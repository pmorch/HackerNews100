<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 11 Aug 2025 20:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Token growth indicates future AI spend per dev (147 pts)]]></title>
            <link>https://blog.kilocode.ai/p/future-ai-spend-100k-per-dev</link>
            <guid>44867312</guid>
            <pubDate>Mon, 11 Aug 2025 17:59:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.kilocode.ai/p/future-ai-spend-100k-per-dev">https://blog.kilocode.ai/p/future-ai-spend-100k-per-dev</a>, See on <a href="https://news.ycombinator.com/item?id=44867312">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Kilo just broke through the 1 trillion tokens a month barrier on OpenRouter for the first time. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!v9-s!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!v9-s!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 424w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 848w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 1272w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!v9-s!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png" width="500" height="187.5" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:546,&quot;width&quot;:1456,&quot;resizeWidth&quot;:500,&quot;bytes&quot;:237722,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://blog.kilocode.ai/i/170429285?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!v9-s!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 424w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 848w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 1272w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Each of the open source family of AI coding tools (Cline, Roo, Kilo) is growing rapidly this month.</p><div><figure><a target="_blank" href="https://x.com/Kilo_Code/status/1953767203175543246" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!IDuV!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 424w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 848w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 1272w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!IDuV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png" width="728" height="248.5" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:497,&quot;width&quot;:1456,&quot;resizeWidth&quot;:728,&quot;bytes&quot;:386444,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:&quot;https://x.com/Kilo_Code/status/1953767203175543246&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.kilocode.ai/i/170429285?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!IDuV!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 424w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 848w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 1272w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>Part of this growth is caused by Cursor and Claude starting to throttle their users. We wrote about </span><a href="https://blog.kilocode.ai/p/cursors-500-requests-unlimited-225" rel="">Cursor at the beginning of July</a><span> and about </span><a href="https://blog.kilocode.ai/p/the-ai-pricing-bait-and-switch" rel="">Claude in the second half of July</a><span>. Their throttling </span><a href="https://www.reddit.com/r/kilocode/comments/1mcdxr4/amazed_by_kilo_or_where_will_all_the_coders_go/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button" rel="">sent users to the open source family of AI coding tools</a><span> causing the increases you see in the graphs above. Cursor and Claude needed to throttle because the industry made a flawed assumption.</span></p><p>The industry expected that because the raw inference costs were coming down fast, the applications inference costs would come down fast as well but this assumption was wrong.</p><p><a href="https://a16z.com/llmflation-llm-inference-cost/" rel="">Raw inference costs did decrease by 10x year-over-year.</a><span> This expectation made startups bet on a business model where companies could afford to sell subscriptions at significant losses, knowing they'd achieve healthy margins as costs plummeted.</span></p><p><a href="https://x.com/dobroslav_dev/status/1952369863344673194" rel="">Cursor's Ultra plan</a><span> exemplified this approach perfectly: charge users $200 while providing at least $400 worth of tokens, essentially operating at -100% gross margin.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!sMJm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!sMJm!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 424w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 848w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 1272w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!sMJm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png" width="542" height="436.5346869712352" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:952,&quot;width&quot;:1182,&quot;resizeWidth&quot;:542,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!sMJm!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 424w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 848w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 1272w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The bet was that by the following year, the application inference would cost 90% less, creating a $160 gross profit (+80% gross margins). But this didn't happen, instead of declining the application inference costs actually grew!</p><p>Application inference costs increased for two reasons: the frontier model costs per token stayed constant and the token consumption per application grew a lot. We'll first dive into the reasons for the constant token price for frontier models and end with explaining the token consumption per application.</p><p><a href="https://ethanding.substack.com/p/ai-subscriptions-get-short-squeezed" rel="">The price per token for the frontier model stayed constant</a><span> because of the </span><a href="https://www.researchgate.net/figure/The-most-popular-large-AI-models-of-recent-years-This-figure-shows-the-main-features-of_fig1_387458379" rel="">increasing size of models</a><span> and more test-time scaling. Test time scaling, also called long thinking, is the </span><a href="https://blogs.nvidia.com/blog/ai-scaling-laws/" rel="">third way to scale AI</a><span> as shown in the graphic below.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!i474!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!i474!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 424w, https://substackcdn.com/image/fetch/$s_!i474!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 848w, https://substackcdn.com/image/fetch/$s_!i474!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!i474!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!i474!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg" width="624" height="351" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:624,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!i474!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 424w, https://substackcdn.com/image/fetch/$s_!i474!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 848w, https://substackcdn.com/image/fetch/$s_!i474!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!i474!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>While the pre- and post-training scaling influenced only the training costs of models. But this test-time scaling increases the cost of inference. Thinking models like OpenAI's o1 series allocate massive computational effort during inference itself. These models can require </span><strong>over 100x compute for challenging queries</strong><span> compared to traditional single-pass inference.</span></p><p><span>Token consumption per application grew a lot because models allowed for </span><a href="https://www.meibel.ai/post/understanding-the-impact-of-increasing-llm-context-windows" rel="">longer context windows</a><span> and bigger suggestions from the models. The combination of a steady price per token and more token consumption caused app inference costs to grow about 10x over the last two years. Market leader Cursor introduced a $200 plan where before $20 was the default. The $200 plan has also been followed by Claude Code and others.</span></p><p>The top end of subscriptions is $200 today but power users find that they are extensively throttled if they use a lot of inference. That throttling comes in the form of rate limiting, using lower quality models, context window compression, and other techniques.</p><p>If you don't want to be throttled you need to pay for inference yourself. The open source family of coding tools (Cline, Roo, Kilo) is based on that principle: “never throttle the user”. Because the users directly see the costs these tools have also led the way in reducing costs by allowing the users to:</p><ol><li><p>Splitting work up in many smaller tasks that can each be run efficiently.</p></li><li><p>Using different modes, in Kilo we have an Orchestrator, Architect, Code, and Debug mode.</p></li><li><p>Combine closed-source models for architecting tasks (e.g. Sonnet 4) and open-source for coding (Qwen3)</p></li><li><p>Enhance the prompt with AI before submitting it</p></li><li><p>Optimize context efficiency with memory banks</p></li><li><p>Enable prompt caching</p></li><li><p>Allow termination of a running task when the model hallucinates</p></li></ol><p>Despite the efforts to reduce costs we do expect them to continue to grow for the power users.</p><p>We expect app inference costs to grow quickly. This is driven by two developments: more parallel agents and more work done before human feedback is needed.</p><p><span>People are </span><a href="https://www.reddit.com/r/ClaudeAI/comments/1kwm4gm/has_anyone_tried_parallelizing_ai_coding_agents/" rel="">experimenting</a><span> </span><a href="https://ainativedev.io/news/how-to-parallelize-ai-coding-agents" rel="">with</a><span> </span><a href="https://google.github.io/adk-docs/agents/workflow-agents/parallel-agents/" rel="">parallel</a><span> AI coding agents today with </span><a href="https://www.warp.dev/" rel="">Warp already having it available to people</a><span>. We expect parallel agents to become the default in the industry and look forward to introduce them in Kilo code sooner rather than later. This will greatly increase token consumption per human hour.</span></p><p>Agents are also able to work longer before needing human feedback. Because they are working more and pausing less this also increases token consumption per human hour.</p><p><span>Both effects together will push costs at the top level to $100k a year. Spending that magnitude of money on software is not without precedent, </span><a href="https://news.ycombinator.com/item?id=26658405" rel="">chip design licenses from Cadence or Synopsys are already $250k a year</a><span>.</span></p><p>While the prospect of $100k+ per year in costs is a lot it can always be worse.</p><p><span>AI costs for most engineers are approximately 1000x smaller than what is happening at the AI training stage. Here the costs for the normal 'inference engineer' is dwarfed by the thousand times bigger impact of the AI ‘training engineer’. The ‘inference engineer’ we talked about above might make $100k and use $100k to be many times more productive than an engineer before AI. A top ‘training engineer’ directs $100m in spend and is paid $100m a year. Top frontier labs spend </span><a href="https://www.datacenterdynamics.com/en/news/openai-training-and-inference-costs-could-reach-7bn-for-2024-ai-startup-set-to-lose-5bn-report/" rel="">billions on AI training</a><span> and this compute work is directed by a handful of people. Mark Zuckerberg is rumored to have offered these people ‘signing bonuses’ of </span><a href="https://www.wsj.com/tech/ai/meta-ai-recruiting-mark-zuckerberg-5c231f75" rel="">$100m</a><span> to </span><a href="https://www.wsj.com/tech/ai/meta-zuckerberg-ai-recruiting-fail-e6107555" rel="">$1b</a><span> with unknown contract lengths. The difference in pay between inference and training engineers is because of their relative impact. You train a model with a handful of people while it is used by millions of people.</span></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wikipedia loses challenge against Online Safety Act (151 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cjr11qqvvwlo</link>
            <guid>44866208</guid>
            <pubDate>Mon, 11 Aug 2025 16:33:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cjr11qqvvwlo">https://www.bbc.com/news/articles/cjr11qqvvwlo</a>, See on <a href="https://news.ycombinator.com/item?id=44866208">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="byline-new" data-component="byline-block"><p><span>Chris Vallance</span></p><p><span>Senior technology reporter</span></p></div><div data-component="text-block"><p>Wikipedia has lost a legal challenge to new Online Safety Act rules which it says could threaten the human rights and safety of its volunteer editors.</p><p>The Wikimedia Foundation - the non-profit which supports the online encyclopaedia - wanted a judicial review of regulations which could mean Wikipedia has to verify the identities of its users.</p><p>But it said despite the loss, <a target="_blank" href="https://www.judiciary.uk/wp-content/uploads/2025/08/Wikimedia-Foundation-and-another-v-Secretary-of-State-for-Science-Innovation-and-Technology.pdf">the judgement</a> "emphasized the responsibility of Ofcom and the UK government to ensure Wikipedia is protected".</p><p>The government told the BBC it welcomed the High Court's judgment, "which will help us continue our work implementing the Online Safety Act to create a safer online world for everyone".</p></div><div data-component="text-block"><p>Judicial reviews challenge the lawfulness of the way in which a decision has been made by a public body.</p><p>In this case the Wikimedia Foundation and a Wikipedia editor tried to challenge the way in which the government decided to make regulations covering which sites should be classed "Category 1" under the Online Safety Act - the strictest rules sites must follow.</p><p>It argued the rules were logically flawed and too broad, meaning a policy intended to impose extra rules on large social media companies would instead apply to Wikipedia.</p><p>In particular the foundation is concerned the extra duties required - if Wikipedia was classed as Category 1 - would mean it would have to verify the identity of its contributors, undermining their privacy and safety.</p><p>The only way it could avoid being classed as Category 1 would be to cut the number of people in the UK who could access the online encyclopaedia by about three-quarters, or disable key functions on the site. </p><p>The government's lawyers argued that ministers had  considered whether Wikipedia should be exempt from the regulations but had reasonably rejected the idea.</p></div><div data-component="text-block"><p>In the end, the court rejected Wikimedia's arguments.</p><p>But Phil Bradley-Schmieg, Lead Counsel at the Wikimedia Foundation, said the judgment did not give Ofcom and the Secretary of State, in Mr Justice Johnson's words, "a green light to implement a regime that would significantly impede Wikipedia's operations".</p><p>And the judgement makes it clear other legal challenges could be possible. </p><p>Wikimedia could potentially challenge Ofcom's decision making if the regulator did ultimately decide to classify the site as Category 1.</p><p>And if the effect of making Wikipedia Category 1 meant it could not continue to operate, then other legal challenges could follow.</p><p>"Wikipedia has been caught in the stricter regulations due to its size and user created content even though it argues (convincingly) that it differs significantly from other user-to-user platforms," said Mona Schroedel, data protection litigation specialist at law firm Freeths.</p><p>"The court's decision has left the door open for Wikipedia to be exempt from the stricter rules upon review."</p><p>The communications regulator Ofcom, which will enforce the act, told the BBC: "We note the court's judgment and will continue to progress our work in relation to categorised services and the associated extra online safety rules for those companies."</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude Is the Drug, Cursor Is the Dealer (103 pts)]]></title>
            <link>https://middlelayer.substack.com/p/i-claude-is-the-drug-cursor-is-the</link>
            <guid>44865813</guid>
            <pubDate>Mon, 11 Aug 2025 16:04:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://middlelayer.substack.com/p/i-claude-is-the-drug-cursor-is-the">https://middlelayer.substack.com/p/i-claude-is-the-drug-cursor-is-the</a>, See on <a href="https://news.ycombinator.com/item?id=44865813">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>There is a rumor floating around tech-twitter that Cursor makes just ten cents for every dollar it spends. Maybe that number is exaggerated but even if it’s directionally correct at some point we are in for a rude awakening in the world of consumer AI apps. </p><p><strong>Many of today’s </strong><em><strong>sexiest</strong></em><strong> AI startups are not product companies but rather distribution arms for someone else’s model.</strong></p><p>These companies are beloved on Twitter, raise at billion-dollar valuations and build beautiful user experiences. The approaching bubble pop? They don’t own the product and they don’t control the supply chain. They live and die by the labs upstream.</p><p>We are witnessing tech’s version of the drug trade: a few powerful labs synthesize the product and startups hustle to get it into users’ hands. Often fronting much of the costs and raising large venture rounds to keep the economics alive. </p><div><p><span>If OpenAI, Anthropic, and Meta are the chemists, then startups like Cursor, Bolt and Lovable are the dealers. </span></p><p><span>The labs develop the substance: </span><strong>the models.</strong></p></div><p><span>The startups distribute the substance: </span><strong>the dealers</strong><span>. </span></p><p>These startups:</p><ul><li><p>Wrap the model in UX</p></li><li><p>Front the inference cost</p></li><li><p>Serve the customer</p></li><li><p>Build a fanbase, raise big VC rounds, grow their ARR fast </p></li></ul><p>But over time, the truth sets in: they don’t control the pricing, they don’t control the roadmap, and their supplier keeps shipping your best features natively.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!kz0V!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!kz0V!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!kz0V!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!kz0V!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!kz0V!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!kz0V!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png" width="455" height="455" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:455,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Generated image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="Generated image" title="Generated image" srcset="https://substackcdn.com/image/fetch/$s_!kz0V!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!kz0V!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!kz0V!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!kz0V!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afc3898-0f19-41cd-a991-6700e4b1bbc7_1024x1024.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><em>Let’s name names. </em></p><ul><li><p><span>Raised </span><strong>$900M</strong><span> at a </span><strong>$9B valuation</strong><span> in 2025.</span></p></li><li><p>Built a great AI coding IDE with full-codebase context and session memory.</p></li><li><p>Runs nearly all inference through OpenAI and Anthropic.</p></li></ul><ul><li><p><span>Raised </span><strong>$150M–$200M</strong><span> at around a </span><strong>$2B valuation</strong><span>.</span></p></li><li><p>Offers low-code app generation powered by Claude, with self-hosting, simplified templates and free trials to get user’s hooked. </p></li></ul><ul><li><p>A lightweight app that wraps GPT and Claude in a clean, keyboard-first interface.</p></li><li><p>Simple UX wrappers on the market.</p></li><li><p>No proprietary model, no infra, no moat if the labs go direct.</p></li></ul><p><span>These companies not only have strong branding and ARR that have investors drooling (</span><em>literally) </em><span>but they are also growing the market. With big marketing budgets and costs that are funded by venture pricing models these companies are revealing just how big each use case may be. </span></p><p><span>But there’s a </span><strong>tradeoff</strong><span>. As they grow adoption, they send a clear signal upstream: </span><em>this is what users want.</em></p><p>And more often than not, the labs take notice. (More on that later.)</p><p>In AI, the story looks quite similar across every new crop of YC startups or venture announcements. What we’ve covered here is just one slice of the market: coding tools. The same dynamics are playing out and will play out in other fast-growing categories: meeting notetakers, AI therapists, creative assistants and beyond.</p><p>From here, the path forks. </p><p><strong>Path 1:</strong><span> labs go direct and wrappers get cut out.</span></p><p><em><span>Illustrated by Foundation Capital’s</span><a href="https://foundationcapital.com/when-model-providers-eat-everything-a-survival-guide-for-service-as-software-startups/" rel="nofollow ugc noopener"> excellent graphic </a><span>below:</span></em><span> </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!E5EQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!E5EQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg 424w, https://substackcdn.com/image/fetch/$s_!E5EQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg 848w, https://substackcdn.com/image/fetch/$s_!E5EQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!E5EQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!E5EQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg" width="1024" height="576" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:576,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!E5EQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg 424w, https://substackcdn.com/image/fetch/$s_!E5EQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg 848w, https://substackcdn.com/image/fetch/$s_!E5EQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!E5EQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298dadf0-c9fc-4bb4-a2bc-d6b1fe811d6f_1024x576.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><p><strong>Path 2:</strong><span> models become commoditized, like generic Advil, and wrappers can win.</span></p><p><span>I’ll explore both of those futures in upcoming posts. For now, the takeaway is simple: unless you own the model, you’re not the product. You’re just the dealer and the lab always has the purer supply.</span></p></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GitHub is no longer independent at Microsoft after CEO resignation (682 pts)]]></title>
            <link>https://www.theverge.com/news/757461/microsoft-github-thomas-dohmke-resignation-coreai-team-transition</link>
            <guid>44865560</guid>
            <pubDate>Mon, 11 Aug 2025 15:47:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/news/757461/microsoft-github-thomas-dohmke-resignation-coreai-team-transition">https://www.theverge.com/news/757461/microsoft-github-thomas-dohmke-resignation-coreai-team-transition</a>, See on <a href="https://news.ycombinator.com/item?id=44865560">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><a href="https://www.theverge.com/authors/tom-warren"><img alt="Tom Warren" data-chromatic="ignore" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/197777/profilephoto.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=48 1x, https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/197777/profilephoto.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96 2x" src="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/197777/profilephoto.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96"></a></p><div><p><span aria-expanded="false" aria-haspopup="true" role="button" tabindex="0"><span id="follow-author-standard_article_details-dmcyOmF1dGhvclByb2ZpbGU6MTY0"><span><span><svg width="9" height="9" viewBox="0 0 9 9" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Follow"><path d="M5 0H4V4H0V5H4V9H5V5H9V4H5V0Z"></path></svg></span></span><span>Tom Warren</span></span></span></p> <p><span>is a senior editor and author of <a href="https://www.theverge.com/notepad-microsoft-newsletter"><i>Notepad</i></a>, who has been covering all things Microsoft, PC, and tech for over 20 years.</span></p></div></div><div id="zephr-anchor"><p>Microsoft is moving GitHub into its CoreAI team, following the resignation of GitHub CEO Thomas Dohmke today. After nearly four years as CEO, Dohmke is leaving GitHub to “become a startup founder again,” and pursue opportunities outside of Microsoft and GitHub.</p><p>GitHub has operated as a separate company ever <a href="https://www.theverge.com/2018/10/26/17954714/microsoft-github-deal-acquisition-complete">since Microsoft acquired it</a> in 2018 for $7.5 billion, but Dohmke’s departure is part of a big shakeup to the way GitHub operates. Microsoft isn’t replacing Dohmke’s CEO position, and GitHub will now be fully part of Microsoft instead of being run as a separate entity.</p><p>“GitHub and its leadership team will continue its mission as part of Microsoft’s CoreAI organization, with more details shared soon,” says Dohmke in <a href="https://github.blog/news-insights/company-news/goodbye-github/">a memo</a> to GitHub employees today. “I’ll be staying through the end of 2025 to help guide the transition and am leaving with a deep sense of pride in everything we’ve built as a remote-first organization spread around the world.”</p><p>Microsoft’s CoreAI team is a new engineering group led by former Meta executive Jay Parikh. It includes Microsoft’s platform and tools division and Dev Div teams, with a focus on building an AI platform and tools for both Microsoft and its customers. Parikh described his vision of an AI agent factory <a href="https://www.theverge.com/notepad-microsoft-newsletter/672598/microsoft-ai-agent-factory-jay-parikh-interview">in an interview with </a><em><a href="https://www.theverge.com/notepad-microsoft-newsletter/672598/microsoft-ai-agent-factory-jay-parikh-interview">Notepad</a> </em>earlier this year, and how he is convincing the developer division of Microsoft to adopt AI.</p><p>“Just like how Bill [Gates] had this idea of Microsoft being a bunch of software developers building a bunch of software, I want our platform, for any enterprise or any organization, to be able to be the thing they turn into their own agent factory,” said Parikh.</p><p>Dohmke only just <a href="https://www.theverge.com/decoder-podcast-with-nilay-patel/720075/github-ceo-thomas-dohmke-ai-coding-copilot-openai-interview">appeared on <em>Decoder </em>last week</a>, discussing Copilot, vibe coding, and what’s next for AI. Dohmke was thinking a lot about the competition and GitHub’s role in the future of software development, and now he’s about to leave to potentially create some more competition for Microsoft’s AI efforts.</p><div><p><span><strong>Follow topics and authors</strong> from this story to see more like this in your personalized homepage feed and to receive email updates.</span></p><ul><li id="follow-author-article_footer-dmcyOmF1dGhvclByb2ZpbGU6MTY0"><span aria-expanded="false" aria-haspopup="true" role="button" tabindex="0"><span><span><svg width="9" height="9" viewBox="0 0 9 9" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Follow"><path d="M5 0H4V4H0V5H4V9H5V5H9V4H5V0Z"></path></svg></span><span>Tom Warren</span></span></span></li><li></li><li></li><li></li><li></li></ul></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Auf Wiedersehen, GitHub (143 pts)]]></title>
            <link>https://github.blog/news-insights/company-news/goodbye-github/</link>
            <guid>44864929</guid>
            <pubDate>Mon, 11 Aug 2025 15:01:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.blog/news-insights/company-news/goodbye-github/">https://github.blog/news-insights/company-news/goodbye-github/</a>, See on <a href="https://news.ycombinator.com/item?id=44864929">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
	
<p>Over a decade ago, my family and I made the leap to move from Germany to the United States after the sale of my startup to Microsoft. In the years since, I’ve had the privilege of working with many exceptional human beings, including Hubbers, Microsofties, customers, partners, our GitHub Stars, open-source maintainers, and developers around the world who’ve helped us shape GitHub. From building mobile developer tools, to running the acquisition of GitHub alongside Nat Friedman, to becoming GitHub’s CEO and guiding us into the age of Copilot and AI, it has been the ride of a lifetime.</p>



<p><strong>Still, after all this time, my startup roots have begun tugging on me and I’ve decided to leave GitHub to become a founder again.</strong> GitHub and its leadership team will continue its mission as part of Microsoft’s CoreAI organization, with more details shared soon. I’ll be staying through the end of 2025 to help guide the transition and am leaving with a deep sense of pride in everything we’ve built as a remote-first organization spread around the world.</p>



<p>With more than 1B repos and forks, and over 150 million developers, GitHub has never been stronger than it is today. We have seen more open-source projects with more contributions every year. AI projects have doubled in the last year alone. And our presence in companies of any size is unmatched in the market. The GitHub platform has continued to lead with incredible momentum. We massively improved accessibility and availability, brought GitHub to the EU, Australia, and back to the US for FedRAMP certification, and fixed a ton of small and not-so-small papercuts. GitHub Advanced Security transformed the industry toward “found means fixed” with the power of AI, reducing mean time to remediation by 60% and enabling teams to fix vulnerabilities 3x faster. GitHub Actions has firmly matured into the world’s leading CI solution, now powering 3 billion minutes per month — up 64% year-over-year — fueled by many key ships and stability improvements.</p>



<p>And of course, together, we launched and scaled Copilot from a simple, but magical autocompletion tool to conversational coding with Copilot Chat &amp; Voice, to reviewing and fixing code, to full-stack app creation with GitHub Spark. Today, GitHub Copilot is the leader of the most successful and thriving market in the age of AI, with over 20 million users and counting. We did this by innovating ahead of the curve and showing grit and determination when challenged by the disruptors in our space. In just the last year, GitHub Copilot became the first multi-model solution at Microsoft, in partnership with Anthropic, Google, and OpenAI. We enabled Copilot Free for millions and introduced the synchronous agent mode in VS Code as well as the asynchronous coding agent native to GitHub.&nbsp;</p>



<p><strong>Because of your relentless work, GitHub Copilot has introduced the greatest change to software development since the advent of the personal computer.</strong></p>



<p>While I’m certainly proud of our hard-earned business growth, technology built for its own sake means nothing but vanity unless it serves a greater purpose. We only succeed when the world succeeds, too. By launching this new age of developer AI, we’ve made it possible for anyone — no matter what language they speak at home or how fluent they are in programming — to take their spark of creativity and transform it into something real. I am more convinced than ever that the world will soon see one billion developers enabled by billions of AI agents, each imprinting human ingenuity into a new gold rush of software. When that day comes, we’ll know where the path began: with GitHub.&nbsp;</p>



<p>Thank you, Hubbers. Being your colleague and your leader has been a great honor and I will cherish our many beautiful moments. Together, we’ve bent the arc of technology for the better.</p>



<p>So long, and thanks for all the fish,</p>



<p><em>Thomas</em></p>

		<div>
	<h2>
		Written by	</h2>
	
			<article>
	<div>
					<div>
				<picture>
					<source srcset="https://secure.gravatar.com/avatar/f9a3d6bee42f4503d3169861b9ecdfab5b2faebc73eb6344d2c3fa15727da799?s=200&amp;d=mm&amp;r=g" width="120" height="120" media="(min-width: 768px)">
					<img src="https://secure.gravatar.com/avatar/f9a3d6bee42f4503d3169861b9ecdfab5b2faebc73eb6344d2c3fa15727da799?s=200&amp;d=mm&amp;r=g" alt="Thomas Dohmke" width="80" height="80" loading="lazy" decoding="async">
				</picture>
			</div>
				
					<p>Fascinated by software development since his childhood in Germany, Thomas Dohmke has built a career building tools to accelerate developer happiness. Currently, Thomas is the Chief Executive Officer of GitHub, where he has overseen the rise of the world’s most widely adopted AI developer tools – including the launches of GitHub Copilot, Copilot Workspace, and GitHub Models. Thomas is a celebrated TED speaker and holds a PhD in mechanical engineering from University of Glasgow, UK.</p>
			</div>
</article>
	</div>
</section><section>
	<h2>
		Related posts	</h2>
	<div>
	<article>
	<div>
			<h3>
				<a href="https://github.blog/open-source/maintainers/we-need-a-european-sovereign-tech-fund/" target="_self">
					We need a European Sovereign Tech Fund				</a>
			</h3>
			<p>Open source software is critical infrastructure, but it’s underfunded. With a new feasibility study, GitHub’s developer policy team is building a coalition of policymakers and industry to close the maintenance funding gap.</p>
		</div>
</article>
<article>
	
</article>
<article>
	
</article>
</div>
</section><div>
	<h2>
		Explore more from GitHub	</h2>
	<div>
		<div>
		<p><img src="https://github.blog/wp-content/uploads/2024/07/Icon-Circle.svg" width="44" height="44" alt="Docs"></p><h3>
			Docs		</h3>
		<p>Everything you need to master GitHub, all in one place.</p>
					<p>
				<a data-analytics-click="Blog, click on module, text: Go to Docs; ref_location:bottom recirculation;" href="https://docs.github.com/" target="_blank" aria-label="Go to Docs">
					Go to Docs											<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>
									</a>
			</p>
			</div>
<div>
		<p><img src="https://github.blog/wp-content/uploads/2024/07/Icon_95220f.svg" width="44" height="44" alt="GitHub"></p><h3>
			GitHub		</h3>
		<p>Build what’s next on GitHub, the place for anyone from anywhere to build anything.</p>
					<p>
				<a data-analytics-click="Blog, click on module, text: Start building; ref_location:bottom recirculation;" href="https://github.com/" target="_blank" aria-label="Start building">
					Start building											<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>
									</a>
			</p>
			</div>
<div>
		<p><img src="https://github.blog/wp-content/uploads/2024/07/Icon_da43dc.svg" width="44" height="44" alt="Customer stories"></p><h3>
			Customer stories		</h3>
		<p>Meet the companies and engineering teams that build with GitHub.</p>
					<p>
				<a data-analytics-click="Blog, click on module, text: Learn more; ref_location:bottom recirculation;" href="https://github.com/customer-stories" target="_blank" aria-label="Learn more">
					Learn more											<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>
									</a>
			</p>
			</div>
<div>
		<p><img src="https://github.blog/wp-content/uploads/2024/04/Universe24-North_Star.svg" width="44" height="44" alt="GitHub Universe 2025"></p><h3>
			GitHub Universe 2025		</h3>
		<p>Last chance: Save $700 on your IRL pass to Universe and join us on Oct. 28-29 in San Francisco.</p>
					<p>
				<a data-analytics-click="Blog, click on module, text: Register now; ref_location:bottom recirculation;" href="https://githubuniverse.com/?utm_source=Blog&amp;utm_medium=GitHub&amp;utm_campaign=module" target="_blank" aria-label="Register now">
					Register now											<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>
									</a>
			</p>
			</div>
	</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta Leaks Part 1: Israel and Meta (335 pts)]]></title>
            <link>https://archive.org/details/meta_leaks_part_1</link>
            <guid>44864419</guid>
            <pubDate>Mon, 11 Aug 2025 14:22:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://archive.org/details/meta_leaks_part_1">https://archive.org/details/meta_leaks_part_1</a>, See on <a href="https://news.ycombinator.com/item?id=44864419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent">
          <!--//.container-ia-->
      
    <div id="theatre-ia-wrap">
        

          
    
        

  
  <h2>
    Bookreader Item Preview
  </h2>

  <!--//#theatre-ia-->
  
</div><!--//.container-ia-->
<!--/.container-ia-->

<div>
            <section>
      <p itemprop="interactionStatistic" itemscope="" itemtype="http://schema.org/InteractionCounter">
                  
        
        <span itemprop="userInteractionCount">0</span>

        Views      </p>

      <p>
                  <span>1</span>
          <span>Favorite</span>
              </p>

          </section>
    
                                <section>
      <h2>
        DOWNLOAD OPTIONS
      </h2>

      
                        
                                <div>
                        <p>
                                          DAISY                          
                          </p><p>For users with print-disabilities</p>
                      </div>
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                            
      
                </section>
        
    

              <section>
      <p>
        Uploaded by
                  <a href="https://archive.org/details/@icw_nru">
            icw_nru          </a>
        
                  on <time>August 11, 2025</time>
              </p>
    </section>
          </div><!--//.container-ia-->
  <div id="also-found" data-identifier="meta_leaks_part_1" data-host-name="www14.us.archive.org" data-mediatype="texts">
                <h2>SIMILAR ITEMS (based on metadata)</h2>
        
      </div><!--//.container-ia-->

  <!--/.container-->
              
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Trump Orders National Guard to Washington, D.C., and Takeover of City’s Police (164 pts)]]></title>
            <link>https://www.nytimes.com/live/2025/08/11/us/trump-news</link>
            <guid>44864192</guid>
            <pubDate>Mon, 11 Aug 2025 14:04:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/live/2025/08/11/us/trump-news">https://www.nytimes.com/live/2025/08/11/us/trump-news</a>, See on <a href="https://news.ycombinator.com/item?id=44864192">Hacker News</a></p>
Couldn't get https://www.nytimes.com/live/2025/08/11/us/trump-news: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Claude Code is all you need (340 pts)]]></title>
            <link>https://dwyer.co.za/static/claude-code-is-all-you-need.html</link>
            <guid>44864185</guid>
            <pubDate>Mon, 11 Aug 2025 14:03:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dwyer.co.za/static/claude-code-is-all-you-need.html">https://dwyer.co.za/static/claude-code-is-all-you-need.html</a>, See on <a href="https://news.ycombinator.com/item?id=44864185">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div>
            <h2>Claude Code<br>Is All You<br>Need</h2>
            <p>How I use Claude Code for work, fun, and as a text editor</p>
        </div>

    <div>
            <p><img src="https://dwyer.co.za/static/img/claude-code-header.png" alt="Claude Code"></p><p>I installed Claude Code in June. I'd tried Cursor and Cline and Zed and a few others, but all of them felt clunky to me because I'm used to doing nearly everything in vanilla vim and my terminal. Claude Code was the first tool I tried that felt like it fit into my workflows perfectly rather than needing me to adapt to new tools.</p>
            <p>It also worked really, really well.</p>
            <p>I quickly cancelled my GPT subscription and put the $20/month towards Anthropic instead. Losing GPT advanced voice mode and dealing with the extra UI lag and lack of polish in Claude Desktop and Mobile apps was a bit of an adjustment to make but the terminal tool was fun enough that I didn't care.</p>
            <p>Within a few days I'd upgraded to the $100/month MAX plan to try out Opus and to stop hitting limits.</p>
            <p>Here's a description of some of the things I've used it for so far. Mainly fun stuff while I figure out how to use it, but I'm starting to use it more and more for 'real' work stuff too.</p>

            <div>
                
                
                <p>Some projects include an experimental 'autonomous startup builder', a (one shot) SplitWise replacement, an AI poster maker, a browser plugin to rate HN comments, a basic trello alternative, and some well organized bank statements</p>
            </div>

            <!-- Modal for image overlay -->
            <div id="imageModal" onclick="closeModal()">
                <p><img id="modalImage" src="" alt="">
                </p>
            </div>

            <p>I'll describe most of these in more detail below but my main takeaways from the last few weeks are:</p>

            <p>1) Have faith (always run it with 'dangerously skip permissions', even on important resources like your production server and your main dev machine. If you're from infosec, you might want to stop reading now—the rest of this article isn't going to make you any happier. Keep your medication close at hand if you decide to continue).</p>
            <p>2) Give it a lot of input. The more input you give it, the better its output is. It's a magic tool, but you'd still better be damn good at communicating, either by typing thousands of words into text files or the interactive window, or using TTS (I haven't tried this because I hate the sound of my own voice, but others have reported great results).</p>
            <p>3) It's surprisingly good at UI design given that it's mainly a text model.</p>
        </div>

    <div>
            <a href="#vibe-code" id="vibe-code">
                <h2><span>#</span>Let's vibe code<br>some CRUD</h2>
            </a>
            <p>Let's try some vibe coding.</p>
            
            <p><strong>Aside:</strong> The definition of vibe coding is still in flux, but to me it means creating software without looking at or editing code. You don't really care what languages or frameworks are used under the hood and develop the code only by chatting with a model.</p>

            <p>We're going to put Claude Code through its paces by developing a basic SplitWise clone. A lot of people use Trello or a Todo list as the basic example of a CRUD app. I like using Splitwise because it's simple enough but it's a bit less cliched and probably not as deeply embedded in the models themselves.</p>

            <p>Building a basic splitwise clone is still mainly 'regurgitation' from some people's point of view, but it has some interesting edge cases that people and models tend to get wrong. Specifically around inviting new users, but letting previous users already start adding expenses and assigning them to users with a pending invitation.</p>

            <p>The simplest form of vibe coding is 'one-shot vibe coding' where you want the model to generate a fully working application after only a single prompt, without needing to give it any further inputs about what to fix, add, remove or change.</p>

            <p>I cheated a bit because the prompt I used to one-shot this is based a bit on earlier attempts where the model did things that I didn't want, but the app shown below and at <a href="https://smartsplit.verysmall.site/" target="_blank">smartsplit.verysmall.site</a> is the output of <code>claude -p "Read the SPEC.md file and implement it"</code> My SPEC.md file is about 500 words (shown in full a bit later).</p>

            <p><img src="https://dwyer.co.za/static/img/smartsplit-php-demo.png" alt="Working Smartsplit PHP demo"></p><p>Depending on how much you've been using LLMs for coding in the last few weeks or months, you'll probably either be surprised or unimpressed that we can get a fully working CRUD application with moderately complicated functionality in one prompt. You can see in the screenshot above that it has some nice touches like filling in names automatically for registered users, but falling back to their email address for non-registered ones.</p>

            <p>I haven't extensively tested it, but the few cases I tried and spot checked worked flawlessly.</p>

            <p>If you're surprised that it works this well, then you should know that a) these models are still inconsistent — they can perform wildly differently based on the same or similar inputs, and b) they're very sensitive to the quality and quantity of input.</p>

            <p>For example, here's a version that's completely broken, with not even basic registration working. The prompt I used for this version was nearly identical, but contained a bit less guidance about what technology stack to use, so the model decided to go overboard and overcomplicate everything to the point where it couldn't even build basic functionality.</p>

            <p><img src="https://dwyer.co.za/static/img/smartsplit-js-fail.png" alt="Failed SmartSplit JavaScript version"></p><a href="#tale-two-codebases" id="tale-two-codebases">
                <h2><span>#</span>A tale of two vibe-coded codebases</h2>
            </a>

            <p>Let's take a look at these two projects and the prompt that created them. The working version is a 900 line index.php file that contains the entire app. The broken version is a NodeJS project split into a client and a server. It's not much longer in terms of lines of code - about a 1000 lines of non-dependency code split up over 15 files. But after you run <code>npm i</code> on the broken version it pulls in 500MB (!!) of dependencies.</p>

            <p><img src="https://dwyer.co.za/static/img/smartsplit-tree.png" alt="SmartSplit project structure comparison"></p><p>Here's the full SPEC.md. This prompt I gave Claude Code is a SPEC.md file. It's nearly the same in both cases, except for the PHP one I tell it to keep things simple, stay away from frameworks, and just write raw SQL. In the broken version, I let it do whatever it wants.</p>

            <div>
<pre>smartsplit-php [master+] $ cat SPEC.md
SmartSplit is a basic CRUD application like SplitWise that lets users split expenses and figure out who needs to pay what to who.

Specifically it has the following features

* A user can sign up with a name, email address, and password
* A user can create a new SmartSplit and give it a name
* A user can add expenses which have a name, an optional description, and an amount
* When adding an expense, a user can specify who paid for it, and who it should be split with
* IMPORTANT: a user can specify that other users are the person who paid, not only the logged in user
* IMPORTANT A user can add others users as the payer or as people to split with even if they haven't joined yet
* For users who haven't joined yet, the user can select them by the invited email address
* Invites are not sent by email, there is no email. THey're just used for unique usernames to manage access
* Once they've joined and added their name, the name should be shown everywhere instead of the email address
* When adding a new expense, the default is that it's split between all users (joined and invited)
* The adder can remove some users if some of them did not take part in that expense
* All splits are always even for simplicity, divided equally between all people specified for that expense
* A user can invite another user to a SmartSplit by specifying their email address.
* Each SmartSplit gets a unique 8-digit alphanumeric code that makes easy to share URLs
* Any user can create a new smartsplit, or join an existing one if they're logged in with an email address that was invited
* Even if that user doesn't yet have an aaccount on SmartSplit, they'll have access to any SmartSplits they're added to after signing up (This bit is important). If the invitee has added their email address, they should have access automatically.
* Any user can add, remove, or edit any expenses within a SmartSplit that they have access to
* Any user can press 'Tally up' which should calculate who needs to make what payments to who to split everything

## Implementation details

* Email addresses are usernames, all registration and login is done with only an email and password
* Passwords are hashed but no extra secrutiy like length or weak passwords or confirmed passwords is applied
* Once a user has registered, they are automatically logged in
* The login and register flow is the same, but the user is registered if they don't have an account and logged in if they do

<span>## Techncial details

* Use a single index.php script for the entire app.
* SQLite for all database functionality.
* No frameworks, just vanilla javascript and css
* No ORMs, use raw SQL
* use a clean minimalist elegant design that's mobile responsive</span></pre>
            </div>

            <p>Those last five bullet points are the only difference between the two prompts, so in some sense they represent a transformation of 500MB of broken code into 30KB of working code.</p>

            <p>Yes, it's a toy example and some people will say that the JavaScript one scales better or something. I'm not here to fight. I hate PHP too, but I'm using it more often for fully vibe coded apps because LLMs are very good at it. Frameworks and abstractions are for humans in the end of the day, not robots, and often they get in the way of Vibecoding instead of being helpful.</p>

            <p><img src="https://dwyer.co.za/static/img/levels-simple.png" alt="Levels.io tweet about simple tech stacks"></p><p><a href="https://x.com/levelsio/status/1945125163793609032" target="_blank">@levelsio on keeping things simple</a></p>
        </div>

    <div>
            <a href="#autonomous-startup" id="autonomous-startup">
                <h2><span>#</span>Building an<br>Autonomous Startup</h2>
            </a>
            <p>The thing about Claude Code is that it isn't really a magic model. It's still using Sonnet or Opus under the hood, which are great, but they're not going to do the things that Claude Code can do. Claude Code's magic is like a magician's trick—it looks incredible, but it's surprisingly simple once you see how it's done.</p>
            <p>I've always told people that coding is just conditional logic and looping (which is just conditional logic). So if you want to be a programmer, learn what an if statement is and build stuff.</p>
            <p>It's an exaggeration but it's also kind of true. And Claude Code demonstrates this well. What makes it work much better than other tools I've tried seems to be a clever but simple combination of looping and conditionals, repeated calls to an LLM with context-specific instructions. This lets it run in a useful loop with limited human input between prompts.</p>
            <a href="#root-vps" id="root-vps">
                <h2><span>#</span>Giving a root VPS to a robot and saying 'go fetch'</h2>
            </a>
            <p>The first thing I thought of doing was extending that loop—maybe infinitely? How far could Claude Code go if it was given a few resources, like a root VPS, and some minimal instructions to never terminate and to just go forever?</p>
            <p><img src="https://dwyer.co.za/static/img/micromonitor.png" alt="Micromonitor startup"></p><p>Spoilers: this is what it built. You can visit the startup it built at <a href="https://claude.dwyer.co.za/" target="_blank">claude.dwyer.co.za</a> or see the GitHub project at <a href="https://github.com/sixhobbits/claude-experiments" target="_blank">github.com/sixhobbits/claude-experiments</a>.</p>
            <p>I fired up a cheap VPS on Hetzner, installed and authenticated Claude Code and messed around for a while trying to get it to write its own prompt about building an autonomous startup and its own looping logic to keep running forever without my input.</p>
            <p>I had some issues getting it to understand that it wasn't meant to terminate, so I instead told it to write a basic bash script that calls claude with the <code>-p</code> flag and "please continue" whenever it detects its not running.</p>
            <p>Here's the script:</p>
            <p><img src="https://dwyer.co.za/static/img/monitor-claude.png" alt="Claude monitoring"></p><div>
                <p><strong>Aside:</strong> I hit a small snag where Anthropic decides that running Claude as root with --dangerously-skip-permissions / yolo-mode is not allowed. You can get past this dumb nanny-state stuff by running:</p>
                <p><code>export IS_SANDBOX=1 &amp;&amp; claude --dangerously-skip-permissions</code></p><p><em>Not financial advice</em></p>
            </div>
            <p>It wrote its own prompt (<a href="https://github.com/sixhobbits/claude-experiments/blob/main/CLAUDE.md" target="_blank">link</a>) (I can't remember the exact prompt I gave it to write this file, but it was a lot shorter and just outlined the basic goals of building an autonomous startup), <a href="https://github.com/sixhobbits/claude-experiments/blob/main/IDEAS.md" target="_blank">evaluated a bunch of startup ideas</a>, rated them, and got to work.</p>
            <p>I watched it code for a while by looking at the new commits coming through to GitHub. I realised I still needed a way to steer it a bit as it was coding without giving me any way to run the app. I added the idea of a HUMAN_INPUT file which it needs to check on each loop, and told it to make sure the app was available and working before continuing with more feature development.</p>
            <p>The idea it came up with (server monitoring) doesn't make any sense at all and it never realised it. It's a web app, so the only server it can monitor is the one it's running on, but from the copy it seems to think it's a SaaS tool you can sign up to and monitor your own servers. You can't.</p>
            <p>BUT this is still seriously impressive stuff. It configured a fully working full stack web application, including Nginx, certificates, etc etc. It's doing real (if misguided) development work, with nearly no input from me at all.</p>
            <div>
                <p><strong>Aside:</strong> Most people I know would criticize this in the same way that AI has always been criticized. "It's not real bro, it's just pattern matching. It's seen stuff like that before. It's not even working properly. An intern could do that with a bit of time."</p>
                <p>The thing about these criticisms is I've been hearing them since I got into character-based neural networks in 2015. The criticisms never change, it's just the line that moves.</p>
                <p><strong>AI (noun)</strong> – something that can do whatever humans can do, but AI can't do
                </p>
                <p>Or</p>
                <p><strong>AI (noun)</strong> – something that doesn't possess abstract human qualities like 'consciousness', 'creativity', or 'a soul' – anything about humans that we can't make any falsifiable claims about.
                </p>
                <p>Whatever, I don't want to get into the debate too much here, but a) I am impressed. b) I would never have predicted an artificial system that could do this 10 years ago or even 6 months ago, and c) anyone else who claims otherwise is likely lying or has ulterior motives. Have a nice day.</p>
            </div>
            <a href="#hitting-snag" id="hitting-snag">
                <h2><span>#</span>Hitting a Snag: the model builders are also the police now</h2>
            </a>
            <p>Most of the time I could interact with my startup builder just by:</p>
            <ul>
                <li>Seeing the changes it made to the production website</li>
                <li>Seeing the outputs it added to GitHub in the various note files and what human help it asked for</li>
                <li>Adding stuff to HUMAN_INPUT.md</li>
            </ul>
            <p>I never needed to SSH into the VPS until it stopped working. After 6 hours of no commits I had to login to check what was happening:</p>
            <pre>[Fri 25 Jul 2025 02:29:41 AM UTC] Starting Claude process...
API Error: Claude Code is unable to respond to this request,
which appears to violate our Usage Policy
(https://www.anthropic.com/legal/aup). Please double press esc
to edit your last message or start a new session for Claude Code
to assist with a different task.
[Fri 25 Jul 2025 02:29:47 AM UTC] Claude process exited with
status: 1
Waiting 3 hours before restart..</pre>
            <p>Uh oh. We're getting blocked again and I've heard Anthropic has a reputation for shutting down even paid accounts with very few or no warnings.</p>
            <p>I read the User policy and saw that my recent inputs telling it to go ahead and market the startup to get users had probably tripped some big brother switch. The user policy (which obviously I had also read before when I signed up. I always read all small print before using software and you should too. But somehow I had forgotten this bit) states that automatically published content needs a human-in-the-loop and Claude was trying to promote the startup on Hackernews without my sign off. (I'm not sure if it would have the motivation or capability to actually go create an HN account and start posting, but it wasn't willing to try.)</p>
            <p><img src="https://dwyer.co.za/static/img/aup-human.png" alt="Anthropic usage policy"></p><p>So I tweaked the prompt a bit to say that it must follow those regulations, and it needs to ask me to approve and post stuff instead of trying to do it itself.</p>

            <p>Then I posted its stuff to <a href="https://news.ycombinator.com/item?id=44689210" target="_blank">Hacker News</a> and Reddit. Luckily I didn't get banned by either for spamming but I did get ignored by both.</p>

            <p>I watched the autonomous startup builder a bit more. It started talking a lot about user acquisition and conversion metrics, which I think were mainly hallucinated though it was taking some stuff from the nginx logs and the database.</p>

            <p>It got lost trying to monetize through a free trial and social proof stuff, which was directionally correct even if completely non-sensical in context and then decided to turn it off so I could save my (still limited, even under the max plan) usage for some more useful stuff.</p>
            <p>The project has 100 commits, so if you want to see exactly what it did, you can take a look at <a href="https://github.com/sixhobbits/claude-experiments/commits/main/" target="_blank">each of those</a>.</p>
            <p>(More in a later section, but it's also my text editor. Look at me writing this article in Claude Code.)</p>
            <p><img src="https://dwyer.co.za/static/img/editor.png" alt="Writing in Claude Code">
        </p></div>

    <div>
            <a href="#production-migration" id="production-migration">
                <h2><span>#</span>Migrating a Real<br>Production Project</h2>
            </a>
            <p>The first opportunity I had to try Claude Code on something where the stakes were a bit higher was when I got a DM from my friend <a href="http://n1c.dev/" target="_blank">Nic</a> on Slack.</p>
            <blockquote>
                "You have any luck with a place to host Sboj?"
            </blockquote>
            <p>I'd recently taken over the ZATech.co.za Slack community from Nic. A related project was Sboj - a reverse job board (squint at the name a bit) that was integrated into the Slack community for recruitment.</p>
            <p><a href="https://zatech.co.za/" target="_blank"><img src="https://dwyer.co.za/static/img/zatech.png" alt="ZATech Slack community"></a>
                <a href="https://sboj.dev/" target="_blank"><img src="https://dwyer.co.za/static/img/sboj.png" alt="Sboj reverse job board"></a>
            </p>
            <p><a href="https://zatech.co.za/" target="_blank">ZATech</a> and <a href="https://sboj.dev/" target="_blank">Sboj</a>, two projects I'd taken over and was battling to find the time to give them the attention they deserved.</p>
            <p>It's a Laravel/PHP app with MySQL and a bunch of other helper stuff that I had little-to-no familiarity with (my choice of poison is usually Python and Postgres), and it was running on an expensive hosting service that was overkill for the amount of traffic and users.</p>
            <p>I wanted to migrate it to my standard set up of a cheap hetzner VPS with nginx and lets encrypt, but Nic had been the founder and solo dev of this project so it didn't have a detailed README of how someone else could get started with it, and I didn't have the time to go spelunking into this code base and figure out all the dependencies and set up steps.</p>
            <p>Claude Code? Yes please.</p>
            <p>Because this isn't an experiment anymore we need to be a bit more careful. A safe step was to clone the code base locally and ask Claude to generate a README file of dependencies and set up instructions.</p>
            <p><img src="https://dwyer.co.za/static/img/readme.png" alt="Generated README"></p><p>It was great at analysing the code base and telling me about the dependencies that I need to check I have access to.</p>
            <p>It listed things I expected (like some worker queue stuff) and stuff I didn't (like Cloudflare Turnstyle that I'd never even heard of).</p>
            <p>I spot checked the dependencies it mentioned and the rest of the README. It seemed accurate enough, so I decided Claude Code had earned the right to try actually set up and run this project. Once again, my starting point was a brand new VPS.</p>
            <p>I manually installed and authorized Claude, cloned the repo, and got a database dump file. Then I fired up Claude and told it to get started.</p>
            <p><code>lfg</code> I said.</p>
            <p>Command not found. Oh yeah, VPS, no shortcuts. <code>export IS_SANDBOX=1 &amp;&amp; claude --dangerously-skip-permissions</code> it is.</p>
            <p>It set up everything I needed, and got the app running at a temporary domain. In the meantime I had to do some old-fashioned non-AI work in the background getting access to the various accounts and switching 2FA to mine.</p>
            <p>It had a few issues restoring the database and I was glad I was watching more closely now as it tried to manually create new SQL dump files that were only excerpts of the actual dump I'd given it to get around not having the correct permissions. After telling it to rather just give itself super admin permissions on the SQL database and restoring the dump from there, it was fine.</p>

            <p><strong>Aside:</strong> It <em>did</em> drop the soon-to-be production database once when I really wasn't expecting it to and it was not the appropriate thing to do based on what it was trying to do. Remember before when I said you needed faith? That also means expecting stuff like this to happen and forgiving, forgetting and moving on instead of trying to hype up the internet and the media about <a href="https://www.reddit.com/r/OpenAI/comments/1m4lqvh/replit_ai_went_rogue_deleted_a_companys_entire/" target="_blank">'AI GOING ROGUE'</a>.</p>
            <p>I haven't done this migration manually so I have no idea how much time I saved, but I'd guess at least 16-32 hours of learning enough about a new stack to get everything running and have confidence that it was doing what I thought it was doing.</p>
            <p>I certainly saved a lot of time in leaning on Claude Code to find relevant logs, debug Turnstyle errors, turn off Turnstyle temporarily while we figured out how to migrate a Cloudflare account, and starting up the Laravel worker processes to do the background analytics stuff. It also migrated the email sending to Resend and then when I hit the 100 emails/day free limit, it was another easy migration to Brevo which offers 300 emails/day for free. (If you're hiring technical people and are able to hire from South Africa, check out Sboj.dev, we've got some great talent and hopefully a bunch of upgrades and QOL improvements coming to the platform soon).</p>
        </div>

    <div>
            <a href="#build-things" id="build-things">
                <h2><span>#</span>You can just build things</h2>
            </a>
            <p>These are only a few things I've built with Claude Code since using it. Most of them are experimental and I've read some reports of it not doing as well on massive real-world code bases, but from what I've seen I'd be surprised it wasn't still useful in those contexts given enough guidance. I'm still surprised by how much better it is as a tool when given a lot of context and input. Here are a few other toy projects I've had it spit out - all things that I've wanted to build for months or years but never found the time. Now you can do stuff like this in a few minutes or hours instead of days or weeks.</p>

            <h3>Building a HackerNews comment ranker plugin</h3>
            <p>I've often been annoyed by comments on HackerNews that are not at all about the article they're commenting on. "Bitcoin adopts a new FlibbityGippity Protocol and can now handle 2.3 transactions per day" and someone will comment that all Crypto projects are scams or something. Note that I don't care about the quality of the comment, or whether or not I agree with it, but I'd wanted a visual way to skip over the 'noise' comments that aren't actually about the article at all.</p>

            <p>I tried to build this before but got distracted by more important stuff, so I figured I'd start over with Claude Code.</p>

            <p><img src="https://dwyer.co.za/static/img/hn-plugin-demo.png" alt="HackerNews comment ranking plugin demo"></p><p>It took a few tries before it could actually display the badges correctly within HN's (pretty simple) HTML structure, but after a few rounds of 'no try again' or 'add more debugging so I can paste the errors to you', it created almost exactly what I had envisioned.</p>

            <p>I was surprised by how good it looked (much better than my normal hacky frontends), and the details it had added unprompted (like the really nice settings page, even with a nod to the HN orange theme).</p>

            <p><img src="https://dwyer.co.za/static/img/hn-plugin-settings.png" alt="HackerNews plugin settings page"></p><p>The actual ranking (which I'm using OpenAI for, not Anthropic) is not that good. It could probably be improved with a better prompt and some more examples of what I think is a '1' comment or a '5' comment, but it works and looks at least directionally accurate so far.</p>

            <h3>Building Poster Maker - A Minimal Canva Replacement</h3>
            <p>AI is getting good at graphic design, and I knew people who were using it to generate basic posters. They liked that the AI could choose good background images, and generally make things look nice with well-sized fonts etc, but they were frustrated that the AI was still only 80% good at generating images of text, and often had spelling errors or other artifacts.</p>
            <p>I was going to tell them to use Canva or Slides.new or another alternative. I tried them out so I could do a quick tutorial on how to use them and realised they were all kinda bad. Either enshittified to death, or lacking the basic AI features, or too complicated for non-technical people to use.</p>
            <p>LFG!</p>
            <p>This was the project that felt a bit more like engineering and less like vibe coding than the others. I knew what I wanted: a really simple interface to combine images and text and get an A4 PDF out. I'd tried to build something like this before and looked at different PDF creation libraries, HTML→PDF flows, and seen that it's not the easiest problem to solve.</p>
            <p>Last time I solved a similar problem (in 2018) <a href="https://www.codementor.io/@garethdwyer/create-pdf-files-from-templates-with-python-and-google-scripts-p63kal1vb" target="_blank">I ended up hacking in Google Docs</a> to create A4 PDFs, but that was more of a templating problem and Google Docs isn't great for layout stuff.</p>
            <p>So I built <a href="https://posters.dwyer.co.za/" target="_blank">posters.dwyer.co.za</a> - it lets you generate the background image with AI (I used Claude Code to build everything, but I told it to use GPT for image generation as that's what I'd used before and I think it's better? I don't even know if Anthropic has image generation APIs to be honest and it seemed easier to just use what I knew).</p>
            <p><img src="https://dwyer.co.za/static/img/poster-maker.png" alt="Poster Maker interface"></p><p><strong>Aside:</strong> Another small snag, it seems like OpenAI blocks Anthropic crawlers so Claude couldn't go read the OpenAI API docs and figure out how to image gen. I had to save the file locally and tell it to reference that.</p>
            <p>This project took a few hours of back and forth. I was really impressed with some of Claude's UI knowledge (it one shotted the font selection when I told it what I wanted) and also saw the limitations in other aspects (it kept overlaying elements in a very un-userfriendly way, sidebar would hide and show and move everything around, it clearly has no idea what it's like to be a human and use something like this).</p>
            <p>But after telling it exactly where to put elements and what they should do, I got more or less exactly what I had envisioned. I was surprised at how well the PDF export worked after the 6th or 7th attempt of blank files or cut off files - now it seems really great at giving me a PDF that is exactly like the preview version which for anyone not in tech seems like a really basic piece of functionality and anyone who has actually tried to do it before knows is like the XKCD bird problem:</p>
            <div>
                <p><a href="https://xkcd.com/1425/">
                    <img src="https://imgs.xkcd.com/comics/tasks.png" alt="XKCD: Tasks">
                </a></p><p>XKCD #1425: Why seemingly simple tasks can be surprisingly hard for computers</p>
            </div>

            <h3>Doing admin with Claude Code</h3>
            <p>This isn't really a project I built, but I'm using Claude Code more and more to do non-coding related tasks. I needed to upload bank statements for my accountant, but my (shitty) South African banks don't name the files well. I can download each month from the web app, but it calls them all "Unknown (5)" or whatever with no extension so it's a pain to go and name them correctly.</p>

            <p>I asked Claude to rename all the files and I could go do something else while it churned away, reading the files and figuring out the correct names.</p>

            <p>I then took it a step further and told it to merge them all into a single CSV file (which also involved extracting random header tabs off the badly formatted XLSX files that my bank provides), and classifying all expenses into broad and specific categories. I told it a few things like the roles of specific people in the team and I think it one-shotted that too. I'm not going to fire my bookkeepers yet, but if I were a bookkeeper I'd definitely make sure to be upskilling with AI tooling right now.</p>

            <p><img src="https://dwyer.co.za/static/img/bank-statements.png" alt="Bank statements renaming tool"></p><h4>Using Claude Code as my Text Editor</h4>

            <p>I'm a die-hard vanilla vim user for all writing, coding, configuration and anything else that fits. I've tried nearly every IDE and text editor out there, and I was certainly happy to have a real IDE when I was pushing production Java for AWS, but vim is what I've always come back to.</p>

            <p>Switching to Claude Code has opened a lot of new design possibilities. Before (did I mention I suck at front end coding), I was restricted to whatever output was produced by static site generators or pandoc templates. Now I can just tell Claude to write an article (like the one you're currently reading) and give it some pointers regarding how I want it to look, and it can generate any custom HTML and CSS and JavaScript I want on the fly.</p>

            <p><img src="https://dwyer.co.za/static/img/this-article.png" alt="This article being written"></p><p>I wrote this entire article in the Claude Code interactive window. The TUI flash (which I've read is a problem with the underlying library that's hard to fix) is really annoying, but it's a really nice writing flow to type stream of consciousness stuff into an editor, mixing text I want in the article, and instructions to Claude, and having it fix up the typos, do the formatting, and build the UX on the fly.</p>

            <p>Nearly every word, choice of phrase, and the overall structure is still manually written by me, a human. I'm still on the fence about whether I'm just stuck in the old way by preferring to hand-craft my words, or if models are generally not good at writing.</p>

            <p>When I read answers to questions I've asked LLMs, or the long research-style reports they create, the writing style is pretty good and I've probably read more LLM-generated words than human-generated words in the last few months.</p>

            <p>But whenever I try to get them to produce the output I want to produce, they fail hard unless I spend as much effort on the prompt as I would have on writing the output myself.</p>

            <p>Simon Willison calls them <a href="https://simonwillison.net/2023/Apr/2/calculator-for-words/" target="_blank">'word calculators'</a> and this is still mainly how I think of them. Great at moving content around (if you want a summary of this now very long article, an LLM will probably do a great job) but pretty useless at generating new stuff.</p>

            <p>Maybe us writers will be around for a while still - let's see, and lfg.</p>
        </div>

    

    


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[I tried every todo app and ended up with a .txt file (554 pts)]]></title>
            <link>https://www.al3rez.com/todo-txt-journey</link>
            <guid>44864134</guid>
            <pubDate>Mon, 11 Aug 2025 13:59:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.al3rez.com/todo-txt-journey">https://www.al3rez.com/todo-txt-journey</a>, See on <a href="https://news.ycombinator.com/item?id=44864134">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p>I’ve tried them all. Notion, Todoist, Things 3, OmniFocus, Asana, Trello, Any.do, TickTick. I even built my own todo app once (spoiler: I never finished it). After years of productivity app hopping, I’m back to where I started: a plain text file called <code>todo.txt</code>.</p>
<p>I’m not alone in this. Jeff Huang wrote about his <a href="https://jeffhuang.com/productivity_text_file/">“never-ending .txt file”</a> that he’s used for over 14 years. Reading his post validated everything I’d discovered on my own.</p>
<h2 id="the-endless-search">The Endless Search</h2>
<p>My productivity journey started like everyone else’s. I’d devour blog posts about getting things done or spot a cool app and think “this is it, this will finally organize me.” I’d burn hours building the perfect system, creating categories, tags, projects, labels. Setting it up felt like work.</p>
<p>Then reality hits. The app wants $9.99/month. The sync breaks. The company sells out and dies. Or worse - I waste more time managing the system than working.</p>
<h2 id="what-actually-happened-with-each-app">What Actually Happened With Each App</h2>
<p><strong>Notion</strong>: Built an entire life operating system. Spent three weeks perfecting it. Used it for two days. Now it’s a graveyard of abandoned databases.</p>
<p><strong>Todoist</strong>: Great until I realized I was gaming the points system instead of doing actual work. Turns out completing “drink water” 8 times a day doesn’t make you productive.</p>
<p><strong>Things 3</strong>: Beautiful. Expensive. Tricked me into thinking I had my life together. But I kept forgetting to check it.</p>
<p><strong>Trello</strong>: Turned my todo list into a board with columns. Realized I’m not a startup. I’m just one person trying to remember to buy milk.</p>
<p><strong>OmniFocus</strong>: So powerful I needed a manual to use it. Spent more time learning OmniFocus than finishing my actual projects.</p>
<h2 id="the-breaking-point">The Breaking Point</h2>
<p>One day my phone died and I couldn’t check my tasks. I grabbed a sticky note and scribbled:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>- finish report</span></span>
<span><span>- call mom</span></span>
<span><span>- gym</span></span>
<span><span>- buy groceries</span></span></code></pre>
<p>And you know what? I crushed all four things. No tags, no priorities, no due dates. Just four things written down.</p>
<h2 id="my-current-system-one-text-file">My Current System: One Text File</h2>
<p>Now I run everything through a single text file. That’s it. Here’s what it looks like:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>2025-08-11</span></span>
<span><span>10am review pull requests</span></span>
<span><span>- check the auth changes specifically</span></span>
<span><span>write blog post about todo apps</span></span>
<span><span>2pm meeting with team</span></span>
<span><span>- discuss sprint planning</span></span>
<span><span>- bring up the deployment issue</span></span>
<span><span>3:30pm call with client</span></span>
<span><span>figure out dinner plans</span></span>
<span><span>read that article Sarah sent</span></span>
<span><span>fix that annoying bug in the navbar</span></span></code></pre>
<p>Every night, I check tomorrow’s calendar. I dump everything into the next day’s section. Scheduled items get times in front. Sub-bullets hold notes or reminders. Finished tasks? I delete them or add what happened. Still on the list? Not done yet. That’s it.</p>
<p>This transforms into a living document throughout the day. I scribble notes right next to tasks as I work:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>2025-08-11</span></span>
<span><span>10am review pull requests</span></span>
<span><span>- check the auth changes specifically</span></span>
<span><span>- merged 3 PRs, waiting on Bob for the 4th</span></span>
<span><span>write blog post about todo apps - drafted, need to proofread</span></span>
<span><span>2pm meeting with team</span></span>
<span><span>- discuss sprint planning</span></span>
<span><span>- bring up the deployment issue</span></span>
<span><span>- decided to push release to Thursday</span></span>
<span><span>3:30pm call with client - rescheduled to tomorrow</span></span>
<span><span>figure out dinner plans - ordered pizza</span></span>
<span><span>read that article Sarah sent</span></span>
<span><span>fix that annoying bug in the navbar - was a CSS specificity issue</span></span></code></pre>
<p>Every few days I start fresh with a new date. The old sections stay. They transform into my journal. I search back to find when I did something, who I met, what we decided. Todo list and work log in one file.</p>
<h2 id="why-this-actually-works">Why This Actually Works</h2>
<p><strong>It’s always there</strong>: The file sits on my desktop. It stares at me every time I open my laptop. No app to launch, no subscription to manage.</p>
<p><strong>It’s instant</strong>: My keyboard shortcut launches my todo.txt in a floating window. Doesn’t matter what I’m doing, my todos are one key press away. No switching between apps, no waiting for things to load, just boom - there’s my list.</p>
<p><strong>AI helps but isn’t needed</strong>: With Cursor/Claude Code or Neovim + Supermaven, I can write my entire day’s schedule in 5 minutes. The AI completes my sentences, predicts meeting times, memorizes how I write tasks. But if all these AI companies disappear tomorrow, my system still works. It’s just a text file. The AI makes it faster, not required.</p>
<p><strong>It’s fast</strong>: Adding a task burns 2 seconds. No clicking through menus or selecting projects.</p>
<p><strong>It’s searchable</strong>: Cmd+F and I find anything instantly. “When did I last call the dentist?” Search for “dentist”. Done.</p>
<p><strong>It’s mine</strong>: No company can kill it. No updates can destroy it. No algorithm decides what I should see.</p>
<p><strong>It’s honest</strong>: I can’t hide behind fancy features. Either I did the thing or I didn’t.</p>
<p><strong>It lasts forever</strong>: A text file is the most basic thing a computer can read. It’ll work after every software update, every company shutdown, every app that stops working. Text files from 20 years ago still open perfectly. Try that with your Notion workspace.</p>
<h2 id="the-secret-sauce">The Secret Sauce</h2>
<p>Productivity isn’t about finding the perfect app. It’s about:</p>
<ol>
<li>Dumping things onto paper so your brain can forget them</li>
<li>Checking the list regularly</li>
<li>Executing the tasks</li>
</ol>
<p>That’s it. Everything else is procrastination dressed up as organization.</p>
<h2 id="what-about-insert-feature-here">What About [Insert Feature Here]?</h2>
<p>“But what about reminders?” - I use my calendar for time-specific stuff.</p>
<p>“But what about projects?” - I add a note like <code>[PROJECT]</code> if I need to.</p>
<p>“But what about collaboration?” - I use work tools for work. This is for my life.</p>
<p>“But what about mobile?” - The file syncs through Dropbox. Any text editor works.</p>
<h2 id="the-plot-twist">The Plot Twist</h2>
<p>I’m more productive now than when I had all those fancy apps. Turns out the best productivity system is the one you actually use. And I use this one because there’s nothing to figure out. It’s just a list.</p>
<h2 id="try-it-yourself">Try It Yourself</h2>
<p>Ready to ditch the productivity app hamster wheel? Do this:</p>
<ol>
<li>Create a file called <code>todo.txt</code></li>
<li>Write down what you need to do tomorrow</li>
<li>Do those things</li>
<li>Add notes as you work</li>
<li>Start a new date section when needed</li>
</ol>
<p>Give it a week. Simple beats sophisticated every time.</p>
<p>And if it doesn’t work? Well, there’s always another shiny new app launching next week.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wikimedia Foundation Challenges UK Online Safety Act Regulations (619 pts)]]></title>
            <link>https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/</link>
            <guid>44863487</guid>
            <pubDate>Mon, 11 Aug 2025 12:38:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/">https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/</a>, See on <a href="https://news.ycombinator.com/item?id=44863487">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>UPDATE: On Monday, 11 August, the High Court of Justice dismissed the Wikimedia Foundation’s <a href="https://medium.com/wikimedia-policy/wikipedias-nonprofit-host-brings-legal-challenge-to-new-online-safety-act-osa-regulations-0f9153102f29" target="_blank" rel="noreferrer noopener">challenge to the UK’s Online Safety Act</a> (OSA) Categorisation Regulations. While the decision does not provide the immediate legal protections for Wikipedia that we hoped for, the Court’s ruling emphasized the responsibility of <a href="https://www.ofcom.org.uk/" target="_blank" rel="noreferrer noopener">Ofcom</a> and the UK government to ensure Wikipedia is protected as the OSA is implemented.&nbsp;</p>



<p>The judge recognized the “significant value” of Wikipedia, its safety for users, as well as the damages that wrongly-assigned OSA categorisations and duties could have on the human rights of Wikipedia’s volunteer contributors. The Court stressed that this ruling “does not give Ofcom and the Secretary of State a green light to implement a regime that would significantly impede Wikipedia’s operations”,&nbsp; and indicated they could face legal repercussions if they fail to protect Wikipedia and the rights of its users.&nbsp;In order to achieve that outcome, he suggested that Ofcom may need to find a particularly flexible interpretation of the rules in question, or that the rules themselves may need amendment in Parliament.</p>



<p>If the ruling stands, the first categorization decisions from Ofcom are expected this summer. The Foundation will continue to seek solutions to protect Wikipedia and the rights of its users as the OSA continues to be implemented.</p>







<hr>



<p>17 July 2025&nbsp;— Next week, on 22 and 23 July 2025, the High Court of Justice in London will hear the Wikimedia Foundation’s&nbsp;<a href="https://medium.com/wikimedia-policy/wikipedias-nonprofit-host-brings-legal-challenge-to-new-online-safety-act-osa-regulations-0f9153102f29" target="_blank" rel="noreferrer noopener">legal challenge</a>&nbsp;to the&nbsp;<a href="https://www.legislation.gov.uk/uksi/2025/226/regulation/3/made" target="_blank" rel="noreferrer noopener">Categorisation Regulations</a>&nbsp;of the United Kingdom (UK)’s Online Safety Act (OSA).<strong>&nbsp;</strong></p>



<p>The Wikimedia Foundation, the non-profit that operates Wikipedia and other <a href="https://wikimediafoundation.org/our-work/wikimedia-projects/" target="_blank" rel="noreferrer noopener">Wikimedia projects</a>, announced its legal challenge earlier this year, arguing that the regulations endanger Wikipedia and the global community of volunteer contributors who create the information on the site.</p>



<blockquote>
<p>“The Court has an opportunity in this case to set a global precedent for protecting public interest projects online,” said <a href="https://wikimediafoundation.org/profile/stephen-laporte/" target="_blank" rel="noreferrer noopener">Stephen LaPorte</a>, General Counsel at the Wikimedia Foundation. “Wikipedia is the backbone of knowledge on the internet. It’s the only top-ten website operated by a non-profit and one of the highest-quality datasets used in training Large Language Models (LLMs). We trust the Court will protect Wikipedia—a vital encyclopedic resource—from rules crafted for the internet’s riskiest commercial sites and, in doing so, safeguard the open internet for everyone”.</p>
</blockquote>



<p>Information on Wikipedia is written and curated by a global community of nearly 260,000 volunteer contributors. These volunteers set and enforce policies to ensure that information on the platform is fact-based, neutral, and attributed to reliable sources. Over the last 25 years, this human-centered content moderation model has established Wikipedia as an unparalleled resource for reliable information in over 300 languages; its 65 million articles are viewed more than 15 billion times per month worldwide.</p>



<p>The Wikimedia Foundation shares the UK government’s commitment to promoting online environments where everyone can safely participate. The organization is not bringing a general challenge to the OSA as a whole, nor to the existence of the Category 1 duties themselves. Rather, the legal challenge focuses solely on the new Categorisation Regulations that risk imposing Category 1 duties (the OSA’s most stringent obligations) on Wikipedia.</p>



<p>If enforced on Wikipedia, Category 1 demands would undermine the privacy and safety of Wikipedia’s volunteer contributors, expose the encyclopedia to manipulation and vandalism, and divert essential resources from protecting people and improving Wikipedia, one of the world’s most trusted and widely used <a href="https://wikimediafoundation.org/news/2025/02/12/wikipedia-recognized-as-a-digital-public-good/" target="_blank" rel="noreferrer noopener">digital public goods</a>.</p>



<p>For example, the Foundation would be required to verify the identity of many Wikipedia contributors, undermining the privacy that is central to keeping Wikipedia volunteers safe. In addition to being exceptionally burdensome, this requirement—which is just one of several Category 1 demands—could expose contributors to data breaches, stalking, lawsuits, or even imprisonment by authoritarian regimes. Additional details about the concerning impacts of the Category 1 duties on Wikipedia are available in this <a href="https://medium.com/wikimedia-policy/wikipedias-nonprofit-host-brings-legal-challenge-to-new-online-safety-act-osa-regulations-0f9153102f29" target="_blank" rel="noreferrer noopener">blog post</a>.</p>
</div><div>
<p>The Wikimedia Foundation will be joined in the case by longtime UK-based volunteer Wikipedia contributor <a href="https://en.wikipedia.org/wiki/User:Zzuuzz" target="_blank" rel="noreferrer noopener">User:Zzuuzz</a> as a joint claimant. Their voluntary participation highlights what is at stake in this case for the everyday people who read and contribute to Wikimedia projects. It presents the perspective of a Wikipedia volunteer on how the OSA Categorisation Regulations directly threaten the ability of contributors to participate in knowledge sharing on Wikipedia, as well as compromising their rights to privacy, safety, free speech, and association.&nbsp;</p>



<p>The legal challenge is the first to be issued against the OSA’s Categorisation Regulations, as well as the first with a volunteer Wikipedia editor participating as a joint claimant. It follows years of dialogue with regulators and policymakers, in which the Foundation expressed its concerns, as well as <a href="https://www.theyworkforyou.com/lords/?id=2025-02-24a.1524.0&amp;s=wikipedia" target="_blank" rel="noreferrer noopener">warnings from the UK Parliament</a> and <a href="https://wikimedia.org.uk/2023/06/online-safety-bill-open-letter/" target="_blank" rel="noreferrer noopener">civil society</a>.</p>



<blockquote>
<p>“Our concerns on the looming threats to Wikipedia and its contributors remain unaddressed”, said Phil Bradley-Schmieg, Lead Counsel at the Wikimedia Foundation. “We are taking action now to protect Wikipedia’s volunteers, as well as the global accessibility and integrity of free knowledge. We call on the Court to defend the privacy and safety of Wikipedia’s volunteer contributors from flawed legislation”.</p>
</blockquote>



<p>Wikipedia and other Wikimedia projects are safe and important resources through which people across the UK—and the wider world—learn, share knowledge, collaborate, and gain media literacy. Thousands of volunteer Wikipedia contributors are based in the UK, and Wikipedia hosts content from cultural institutions such as the British Library and Wellcome Collection. Content on Wikipedia and other Wikimedia projects was viewed 776 million times last month in the UK alone. Moreover, Wikipedia is used to preserve and promote cultural heritage in the UK, including Indigenous and minority languages such as Welsh. The <a href="https://cy.wikipedia.org/wiki/Hafan?wprov=wppw2" target="_blank" rel="noreferrer noopener">Welsh language version of Wikipedia</a> is the single most popular Welsh language website in the world and is an <a href="https://digitalanddata.blog.gov.wales/2017/08/07/using-technology-to-promote-welsh-language-wikipedia/" target="_blank" rel="noreferrer noopener">official component of the curriculum</a> in Wales.</p>



<p>The hearings at the Royal Courts of Justice (Administrative courts of the King’s Bench Division) in London, are expected to be open to the public. The case reference is AC-2025-LON-001365, and the courtroom location will be announced <a href="https://www.court-tribunal-hearings.service.gov.uk/summary-of-publications?locationId=109" target="_blank" rel="noreferrer noopener">here</a> shortly before the hearing. The Court will issue its decision following the hearing, though the exact timing of the announcement is not known.</p>



<hr>







<p>The personal identity of User:Zzuuzz, the volunteer joining the challenge, will remain confidential and protected by the law and the Foundation.&nbsp;</p>



<p>For media inquiries, please contact <a href="mailto:press@wikimedia.org" target="_blank" rel="noreferrer noopener">press@wikimedia.org</a>.&nbsp;</p>



<p><a href="https://mailchi.mp/wikimedia/global-advocacy-policy-newsletter" target="_blank" rel="noreferrer noopener">Subscribe to our Global Advocacy newsletter</a> to stay informed on this case and other global advocacy updates from the Wikimedia Foundation.&nbsp;</p>



<h2><strong>About the Wikimedia Foundation</strong></h2>



<p>The <a href="https://wikimediafoundation.org/" target="_blank" rel="noreferrer noopener">Wikimedia Foundation</a> is the nonprofit organization that operates Wikipedia and other Wikimedia free knowledge projects. Our vision is a world in which every single human can freely share in the sum of all knowledge. We believe that everyone has the potential to contribute something to our shared knowledge and that everyone should be able to access that knowledge freely. We host Wikipedia and the Wikimedia projects; build software experiences for reading, contributing, and sharing Wikimedia content; support the volunteer communities and partners who make Wikimedia possible. The Wikimedia Foundation is a United States 501(c)(3) tax-exempt organization with offices in San Francisco, California, USA.</p>
</div><div>

			<p><a alt="" tabindex="-1" href="https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/">
			<img loading="lazy" decoding="async" width="800" height="600" src="https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-Royal_Courts_of_Justice_2019-1.jpg?w=800&amp;h=600&amp;crop=1" alt="" srcset="https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-Royal_Courts_of_Justice_2019-1.jpg?resize=400,300 400w, https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-Royal_Courts_of_Justice_2019-1.jpg?resize=800,600 800w" sizes="auto, (max-width: 800px) 100vw, 800px">		</a></p><div>
					<h3>
				<a href="https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/">
					Wikimedia Foundation Challenges UK Online Safety Act Regulations				</a>
			</h3>
		
					
		
					<p>UPDATE: On Monday, 11 August, the High Court of Justice dismissed the Wikimedia Foundation’s challenge to the UK’s Online Safety Act (OSA) Categorisation Regulations. While the decision does not provide the immediate legal protections for Wikipedia that we hoped for, the Court’s ruling emphasized the responsibility of Ofcom and the UK government to ensure Wikipedia….</p>
		
		

		<p><a href="https://wikimediafoundation.org/news/2025/08/11/wikimedia-foundation-challenges-uk-online-safety-act-regulations/" aria-label="Read more about Wikimedia Foundation Challenges UK Online Safety Act Regulations">
			Read more		</a>
	</p></div>
</div><div>

			<p><a alt="" tabindex="-1" href="https://wikimediafoundation.org/news/2025/07/09/china-block-wikimedia-wipo/">
			<img loading="lazy" decoding="async" width="800" height="600" src="https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-069_United_Nations_Geneva_World_Intellectual_Property_Organization_WIPO_-_Creative_Commons-1.jpg?w=800&amp;h=600&amp;crop=1" alt="" srcset="https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-069_United_Nations_Geneva_World_Intellectual_Property_Organization_WIPO_-_Creative_Commons-1.jpg?resize=400,300 400w, https://wikimediafoundation.org/wp-content/uploads/2025/07/1024px-069_United_Nations_Geneva_World_Intellectual_Property_Organization_WIPO_-_Creative_Commons-1.jpg?resize=800,600 800w" sizes="auto, (max-width: 800px) 100vw, 800px">		</a></p><div>
					<h3>
				<a href="https://wikimediafoundation.org/news/2025/07/09/china-block-wikimedia-wipo/">
					For fifth time, China blocks Wikimedia Foundation as permanent observer to the World Intellectual Property Organization (WIPO)				</a>
			</h3>
		
					
		
					<p>On 9 July 2025, the Wikimedia Foundation was denied permanent observer status at the World Intellectual Property Organization (WIPO).</p>
		
		

		<p><a href="https://wikimediafoundation.org/news/2025/07/09/china-block-wikimedia-wipo/" aria-label="Read more about For fifth time, China blocks Wikimedia Foundation as permanent observer to the World Intellectual Property Organization (WIPO)">
			Read more		</a>
	</p></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pricing Pages – A Curated Gallery of Pricing Page Designs (144 pts)]]></title>
            <link>https://pricingpages.design/</link>
            <guid>44863409</guid>
            <pubDate>Mon, 11 Aug 2025 12:27:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pricingpages.design/">https://pricingpages.design/</a>, See on <a href="https://news.ycombinator.com/item?id=44863409">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Filter by style &amp; or by industry</p><p><a data-w-id="0f548e28-46ec-0449-f2ab-898862391dd9" href="#"><img alt="" src="https://cdn.prod.website-files.com/6244257bf98bf00e37b25f97/6244257bf98bf0e23bb25fec_icon_close-modal.svg"></a></p></div><div><div role="list"><p><label><span fs-cmsfilter-field="Industry" for="Filters">AI &amp; Machine Learning</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Business &amp; Productivity</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Business Intelligence &amp; Analytics</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Communication &amp; Collaboration</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Content &amp; Media</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Crypto</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Design &amp; Creative</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Developer Tools &amp; Infrastructure</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">E-commerce &amp; Retail</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Fintech &amp; Financial Services</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">HR &amp; Recruiting</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Marketing &amp; Sales</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Specialized &amp; Niche</span></label></p><p><label><span fs-cmsfilter-field="Industry" for="Filters">Travel &amp; Transportation</span></label></p></div><div role="list"><p><label><span fs-cmsfilter-field="Styles" for="Filters">Color Coded Tiers</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Comparison Table</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Contact Sales </span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Custom Pricing</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Enterprise option</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Feature Checkmarks</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Feature Lists</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Monthly/Annual Toggle</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Stacked Cards</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Standard Table</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Table</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Tiered Table</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Usage-Based</span></label></p><p><label><span fs-cmsfilter-field="Styles" for="Filters">Usage-Based Calculator</span></label></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenSSH Post-Quantum Cryptography (281 pts)]]></title>
            <link>https://www.openssh.com/pq.html</link>
            <guid>44863242</guid>
            <pubDate>Mon, 11 Aug 2025 12:01:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openssh.com/pq.html">https://www.openssh.com/pq.html</a>, See on <a href="https://news.ycombinator.com/item?id=44863242">Hacker News</a></p>
<div id="readability-page-1" class="page">
<hr>

<p>
OpenSSH supports a number of cryptographic key agreement algorithms
considered to be safe against attacks from quantum computers. 
We recommend that all SSH connections use these algorithms.
</p>

<p>
OpenSSH has offered post-quantum key agreement (<i>KexAlgorithms</i>)
by default since release 9.0 (2022), initially via the
<tt>sntrup761x25519-sha512</tt> algorithm. More recently, in OpenSSH 9.9,
we have added a second post-quantum key agreement <tt>mlkem768x25519-sha256</tt>
and it was made the default scheme in OpenSSH 10.0.
</p>

<p>
To encourage migration to these stronger algorithms, OpenSSH 10.1 will warn
the user when a non post-quantum key agreement scheme is selected. These
warnings are displayed by default but may be disabled via the
<i>WarnWeakCrypto</i> option in
<a href="https://man.openbsd.org/ssh_config.5">ssh_config(5)</a>.
</p>

<h3>Background</h3>

<p>
A quantum computer (QC) is a device capable of performing computations
with information encoded as quantum states. Such a device could quickly solve
particular problems that are intractable for existing "classical" computers.
</p>

<p>
The mathematics that underpin a number of cryptographic algorithms
are among the problems that quantum computers are believed to be able to
effectively solve. This means that a sufficiently-powerful quantum computer
(a.k.a a "cryptographically-relevant" quantum computer) will be able to break
them. Most affected is the cryptography used for key agreement and digital
signatures, both of which play important roles in SSH.
</p>

<p>
Fortunately, quantum computers of sufficient power to break cryptography
have not been invented yet. Estimates for when a cryptographically-relevant
quantum computer will arrive, based on the rate of progress in the field,
range from 5-20 years, with many observers expecting them to arrive
in the mid-2030s.
</p>

<p>
The entire privacy of an SSH connection depends on cryptographic key agreement.
If an attacker can break the key agreement then they are able to decrypt and
view the entire session. The attacker need not perform this attack in real
time; they may collect encrypted SSH sessions now and then decrypt them later
once they have access to a quantum computer.
This is referred to as a "store now, decrypt later" attack (also as
"harvest now, decrypt later").
</p>

<p>
OpenSSH supports post-quantum cryptography to protect user traffic against
this attack.
</p>

<h2>FAQ</h2>

<dl>
<dt><b>I received a warning from ssh that directed me to this page. What should I do?</b></dt>
<dd>
As mentioned above, OpenSSH 10.1 started warning users when connections use
cryptography that is not safe against quantum computers. If you received such
a warning, it means that the server you connected to did not offer one of the
two post-quantum key agreement algorithms that are being standardised for the
SSH protocol:
<a href="https://datatracker.ietf.org/doc/draft-ietf-sshm-mlkem-hybrid-kex/"><tt>mlkem768x25519-sha256</tt></a> and
<a href="https://datatracker.ietf.org/doc/draft-josefsson-ntruprime-ssh/"><tt>sntrup761x25519-sha512</tt></a>
<p>
The ideal solution is to update the server to use an SSH implementation that
supports at least one of these. OpenSSH versions 9.0 and greater support
</p><tt>sntrup761x25519-sha512</tt> and versions 9.9 and greater support
<tt>mlkem768x25519-sha256</tt>. If your server is already running one of these
versions, then check whether the <i>KexAlgorithms</i> option has disabled
their use.
<p>
If you are unable to update the server and/or you prefer to accept the risk
of continuing to use quantum-unsafe cryptography then the warning may be
silenced via the 
<i>WarnWeakCrypto</i> option in
<a href="https://man.openbsd.org/ssh_config.5">ssh_config(5)</a>.
We recommend doing this selectively, for example:
</p><pre>Match host unsafe.example.com
    WarnWeakCrypto no
</pre>
</dd>
<dt><b>Quantum computers don't exist yet, why go to all this trouble?</b></dt>
<dd>
Because of the "store now, decrypt later" attack mentioned above. Traffic
sent today is at risk of decryption unless post-quantum key agreement is used.
</dd>
<dt><b>What about signature algorithms? You said they were at risk too</b></dt>
<dd>
Yes, most currently-used signature algorithms (including RSA and ECDSA) can be
broken by a quantum computer. However, there is no risk to existing traffic
in this situation (i.e. there is no analogous "store now, decrypt later").
The only urgency for signature algorithms is ensuring that all classical
signature keys are retired in advance of cryptographically-relevant computers
becoming a reality. OpenSSH will add support for post-quantum signature
algorithms in the future.
</dd>
<dt><b>I don't believe we'll ever get quantum computers. This is a waste of time</b></dt>
<dd>
Some people consider the task of scaling existing quantum computers up to the
point where they can tackle cryptographic problems to be practically
insurmountable. This is a possibilty. However, it appears that most of the
barriers to a cryptographically-relevant quantum computer are engineering
challenges rather than underlying physics.
<p>
If we're right about quantum computers being practical, then we will have
protected vast quantities of user data. If we're wrong about it, then all
we'll have done is moved to cryptographic algorithms with stronger mathematical
underpinnings.
</p></dd>
<dt><b>These post-quantum algorithms are new, are we sure they aren't broken?</b></dt>
<dd>
We're wary of this too. Though post-quantum key agreement algorithms have
received a lot of concerted cryptographic attention over the last few years,
it's possible that new attacks might be found.
<p>
To defend against this happening we have selected post-quantum algorithms with
good safety margins, this means that even if they turn out to be weaker than
expected they are still likely to be strong enough to be considered fit for
purpose.
</p><p>
Additionally, all the post-quantum algorithms implemented by OpenSSH are
"hybrids" that combine a post-quantum algorithm with a classical
algorithm. For example </p><tt>mlkem768x25519-sha256</tt> combines ML-KEM, a
post-quantum key agreement scheme, with ECDH/x25519, a classical key agreement
algorithm that was formerly OpenSSH's preferred default. This ensures that the
combined, hybrid algorithm is <i>no worse</i> than the previous best
classical algorithm, even if the post-quantum algorithm turns out to be
completely broken by future cryptanalysis.
</dd>
</dl>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-OSS-120B runs on just 8GB VRAM & 64GB+ system RAM (209 pts)]]></title>
            <link>https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/</link>
            <guid>44862542</guid>
            <pubDate>Mon, 11 Aug 2025 10:02:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/">https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/</a>, See on <a href="https://news.ycombinator.com/item?id=44862542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Here is the thing, the expert layers run amazing on CPU  (<del>~17T/s</del> 25T/s on a 14900K) and you can force that with this new llama-cpp option: --cpu-moe .</p>

<p>You can offload just the attention layers to GPU  (requiring about 5 to 8GB of VRAM) for fast prefill.</p>

<ul>
<li>KV cache for the sequence</li>
<li>Attention weights &amp; activations</li>
<li>Routing tables</li>
<li>LayerNorms and other “non-expert” parameters</li>
</ul>

<p>No giant MLP weights are resident on the GPU, so memory use stays low.</p>

<p>This yields an amazing snappy system for a 120B model!  Even something like a 3060Ti would be amazing! GPU with BF16 support would be best (RTX3000+) because all layers except the MOE layers (which are mxfp4) are BF16.</p>

<p>64GB of system ram would be minimum, and 96GB would be ideal. (linux uses mmap so will keep the 'hot' experts in memory even if the whole model doesn't fit in memory)</p>

<blockquote>
<p>prompt eval time = 28044.75 ms / 3440 tokens ( 8.15 ms per token, 122.66 tokens per second)</p>

<p>eval time = 5433.28 ms / 98 tokens ( 55.44 ms per token, 18.04 tokens per second)</p>
</blockquote>

<p>with 5GB of vram usage!</p>

<p>Honestly, I think this is the biggest win of this 120B model. This seems an amazing model to run fast for GPU-poor people. You can do this on a 3060Ti and 64GB of system ram is cheap.</p>

<p>edit: with this latest PR: <a href="https://github.com/ggml-org/llama.cpp/pull/15157">https://github.com/ggml-org/llama.cpp/pull/15157</a></p>

<pre><code>~/build/llama.cpp/build-cuda/bin/llama-server \
    -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \
    --n-cpu-moe 36 \    #this model has 36 MOE blocks. So cpu-moe 36 means all moe are running on the CPU. You can adjust this to move some MOE to the GPU, but it doesn't even make things that much faster.
    --n-gpu-layers 999 \   #everything else on the GPU, about 8GB
    -c 0 -fa \   #max context (128k), flash attention
    --jinja --reasoning-format none \
    --host 0.0.0.0 --port 8502 --api-key "dummy" \



prompt eval time =   94593.62 ms / 12717 tokens (    7.44 ms per token,   134.44 tokens per second)
       eval time =   76741.17 ms /  1966 tokens (   39.03 ms per token,    25.62 tokens per second)
</code></pre>

<p>Hitting above 25T/s with only 8GB VRAM use!</p>

<p>Compared to running 8 MOE layers also on the GPU (about 22GB VRAM used total) :</p>

<pre><code>~/build/llama.cpp/build-cuda/bin/llama-server \
    -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \
    --n-cpu-moe 28 \
    --n-gpu-layers 999 \
    -c 0 -fa \
    --jinja --reasoning-format none \
    --host 0.0.0.0 --port 8502 --api-key "dummy" \

prompt eval time =   78003.66 ms / 12715 tokens (    6.13 ms per token,   163.01 tokens per second)
       eval time =   70376.61 ms /  2169 tokens (   32.45 ms per token,    30.82 tokens per second)
</code></pre>

<p>Honestly, this 120B is the perfect architecture for running at home on consumer hardware. Somebody did some smart thinking when designing all of this!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Faster substring search with SIMD in Zig (159 pts)]]></title>
            <link>https://aarol.dev/posts/zig-simd-substr/</link>
            <guid>44862414</guid>
            <pubDate>Mon, 11 Aug 2025 09:41:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aarol.dev/posts/zig-simd-substr/">https://aarol.dev/posts/zig-simd-substr/</a>, See on <a href="https://news.ycombinator.com/item?id=44862414">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>Published 10.08.2025</span></p><p>I’ve been learning a lot about low-level programming languages lately, and for a long time there has been one thing that has interested me: SIMD (or ‘single instruction, multiple data’) code. I’ve seen a lot of articles about having massive performance gains by utilizing SIMD and wanted to learn how to do it myself.</p><p>This article is a journey into implementing ~60% faster substring searching compared to Zig’s <code>std.mem.indexOf</code> using a SIMD-friendly algorithm.</p><h2 id="baseline">Baseline
<a href="#baseline">#</a></h2><p>This is the baseline function that we will be comparing against:</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>fn</span><span> </span><span>find_substr</span><span>(</span><span>needle</span><span>:</span><span> </span><span>[]</span><span>const</span><span> </span><span>u8</span><span>,</span><span> </span><span>haystack</span><span>:</span><span> </span><span>[]</span><span>const</span><span> </span><span>u8</span><span>)</span><span> </span><span>?</span><span>usize</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>return</span><span> </span><span>std</span><span>.</span><span>mem</span><span>.</span><span>indexOf</span><span>(</span><span>u8</span><span>,</span><span> </span><span>haystack</span><span>,</span><span> </span><span>needle</span><span>);</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></div><p>It’s the closest thing to a substring search function from Zig’s standard library. It returns the first index of a subsequence – or <code>null</code> if not found.</p><h2 id="simd-algorithm">SIMD algorithm
<a href="#simd-algorithm">#</a></h2><p>This algorithm is taken directly from Wojciech Muła’s fantastic article <a href="http://0x80.pl/notesen/2016-11-28-simd-strfind.html#algorithm-1-generic-simd">SIMD-friendly algorithms for substring searching</a>, which seems to have the best algorithms for finding substrings in a large body of text.</p><p>Here’s how the algorithm works: say that we want to find the index of the word “blue” (the <code>needle</code>) in “It was a beautiful, bounteous, blue day” (the <code>haystack</code>). First, we extract the first and last character of the <code>needle</code> (‘b’ and ’e’) and store them in a variable.</p><p>Then we will loop through all of the characters in <code>haystack</code>, loading the next 32 characters (bytes) from memory into a SIMD register and comparing each character (byte) in the register with ‘b’. This will result in a mask containing 32 bytes, <code>1</code> if the character is ‘b’ and <code>0</code> in all other cases.</p><p>We will do the same with the last character, but load the characters with an offset (<code>needle.len - 1</code>).</p><blockquote><p>Without the offset, any match that starts in one 32‑byte chunk and ends in the next would be missed. With this method, we can also check for <code>needles</code> that are longer than 32 characters.</p></blockquote><p>The result will be two bit masks, <code>First</code> and <code>Last</code>, where we can use bit-wise AND (<code>Result = First &amp; Last</code>) to figure out potential substring occurrences.</p><p><code>Result</code> will be <code>1</code> only when there is a ‘b’ at index <code>i</code> followed by an ’e’ at index <code>i+3</code>. We still need to check if those positions actually contain the value “blue”, but this still dramatically reduces the number of checks (= individual memory accesses) that are necessary. We’ll see how this works in practice in the next section.</p><h2 id="implementation-in-zig">Implementation in Zig
<a href="#implementation-in-zig">#</a></h2><p>First, to properly use SIMD, let’s assume that the CPU supports AVX2 (Advanced Vector Extensions 2) and has 256-bit wide registers.</p><blockquote><p>All desktop processors less than 10 years old support AVX2, with newer ones also supporting AVX-512 with 512-bit wide registers.</p></blockquote><p>This allows us to use Zig’s <a href="https://ziglang.org/documentation/0.14.1/#Vectors">@Vector</a> function to make a type:</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>const</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>@Vector</span><span>(</span><span>32</span><span>,</span><span> </span><span>u8</span><span>);</span><span> </span><span>// number of elements, element type (32*8=256)
</span></span></span></code></pre></div><p>By using <code>Block</code>, we are telling the compiler that the operations on this datatype should use SIMD instructions where possible.</p><p>Next, we take the first and last letters of the search word (’needle’) and load them into two SIMD registers, so that every byte of the register is filled with the character. This is handled by another built-in function, <code>@splat</code>:</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>const</span><span> </span><span>first_letter</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>@splat</span><span>(</span><span>needle</span><span>[</span><span>0</span><span>]);</span><span>
</span></span></span><span><span><span></span><span>const</span><span> </span><span>last_letter</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>@splat</span><span>(</span><span>needle</span><span>[</span><span>needle</span><span>.</span><span>len</span><span> </span><span>-</span><span> </span><span>1</span><span>]);</span><span>
</span></span></span></code></pre></div><p>In the main loop, we check that there is enough characters left in <code>haystack</code> so that we can read the next <code>32 + needle.len</code> characters. Inside the block, we load the blocks that we’re going to compare <code>first_letter</code> and <code>last_letter</code> with.</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>const</span><span> </span><span>n</span><span> </span><span>=</span><span> </span><span>haystack</span><span>.</span><span>len</span><span>;</span><span>
</span></span></span><span><span><span></span><span>const</span><span> </span><span>k</span><span> </span><span>=</span><span> </span><span>needle</span><span>.</span><span>len</span><span>;</span><span>
</span></span></span><span><span><span></span><span>var</span><span> </span><span>i</span><span>:</span><span> </span><span>usize</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span></span><span>while</span><span> </span><span>(</span><span>i</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>+</span><span> </span><span>32</span><span> </span><span>&lt;=</span><span> </span><span>n</span><span>)</span><span> </span><span>:</span><span> </span><span>(</span><span>i</span><span> </span><span>+=</span><span> </span><span>32</span><span>)</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>first_block</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>haystack</span><span>[</span><span>i</span><span>..][</span><span>0</span><span>..</span><span>32</span><span>].</span><span>*</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>last_block</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>haystack</span><span>[</span><span>i</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span> </span><span>..][</span><span>0</span><span>..</span><span>32</span><span>].</span><span>*</span><span>;</span><span>
</span></span></span></code></pre></div><p>Now we can make the comparisons and combine them into a mask:</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>	</span><span>// ...
</span></span></span><span><span><span></span><span>    </span><span>const</span><span> </span><span>eq_first</span><span> </span><span>=</span><span> </span><span>first_letter</span><span> </span><span>==</span><span> </span><span>first_block</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>eq_last</span><span> </span><span>=</span><span> </span><span>last_letter</span><span> </span><span>==</span><span> </span><span>last_block</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>var</span><span> </span><span>mask</span><span>:</span><span> </span><span>std</span><span>.</span><span>bit_set</span><span>.</span><span>IntegerBitSet</span><span>(</span><span>32</span><span>)</span><span> </span><span>=</span><span> </span><span>.{</span><span> </span><span>.</span><span>mask</span><span> </span><span>=</span><span> </span><span>@bitCast</span><span>(</span><span>eq_first</span><span> </span><span>&amp;</span><span> </span><span>eq_last</span><span>)</span><span> </span><span>};</span><span>
</span></span></span></code></pre></div><p>Here we can use an <code>IntegerBitSet</code> from Zig’s standard library. We construct it by casting the result of <code>eq_first &amp; eq_last</code> into a 32-bit integer. If the resulting mask is non-zero, there are candidates in the current block.</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>	</span><span>// ...
</span></span></span><span><span><span></span><span>&nbsp; &nbsp; </span><span>while</span><span> </span><span>(</span><span>mask</span><span>.</span><span>findFirstSet</span><span>())</span><span> </span><span>|</span><span>bitpos</span><span>|</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>if</span><span> </span><span>(</span><span>std</span><span>.</span><span>mem</span><span>.</span><span>eql</span><span>(</span><span>u8</span><span>,</span><span> </span><span>haystack</span><span>[</span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span> </span><span>+</span><span> </span><span>1</span><span> </span><span>..][</span><span>0</span><span> </span><span>..</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span>],</span><span> </span><span>needle</span><span>[</span><span>1</span><span>..]))</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>return</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>        </span><span>_</span><span> </span><span>=</span><span> </span><span>mask</span><span>.</span><span>toggleFirstSet</span><span>();</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span></code></pre></div><p>The first and last characters of the substring are checked already, so we don’t need to check their equality again.</p><p>Finally, if there are leftover characters, we can fall back to <code>std.mem.IndexOf</code>.</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>	</span><span>// ...
</span></span></span><span><span><span></span><span>	</span><span>// Fallback to scalar search for the tail
</span></span></span><span><span><span></span><span>	</span><span>if</span><span> </span><span>(</span><span>i</span><span> </span><span>&lt;</span><span> </span><span>n</span><span>)</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>	    </span><span>if</span><span> </span><span>(</span><span>std</span><span>.</span><span>mem</span><span>.</span><span>indexOf</span><span>(</span><span>u8</span><span>,</span><span> </span><span>haystack</span><span>[</span><span>i</span><span>..],</span><span> </span><span>needle</span><span>))</span><span> </span><span>|</span><span>rel_idx</span><span>|</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>	        </span><span>return</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>rel_idx</span><span>;</span><span>
</span></span></span><span><span><span>	    </span><span>}</span><span>
</span></span></span><span><span><span>	</span><span>}</span><span>
</span></span></span><span><span><span>	</span><span>return</span><span> </span><span>null</span><span>;</span><span> </span><span>// no substring found
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></div><h3 id="benchmarks">Benchmarks
<a href="#benchmarks">#</a></h3><p>To properly show the effects of our SIMD algorithm, we’re going to need a large haystack. For this, I’ve chosen to use <a href="https://www.gutenberg.org/ebooks/2701">the entirety Moby Dick</a> in plain text, and a search word ’newsletter’, which appears at the very end of the text.</p><blockquote><p>The code is available <a href="https://github.com/aarol/substr">on GitHub</a></p></blockquote><p>To compile the code, I ran <code>zig build</code> with <code>-Doptimize=ReleaseFast</code>:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>&gt; zig build -Doptimize<span>=</span>ReleaseFast
</span></span></code></pre></div><blockquote><p>Support for <a href="https://github.com/ziglang/zig/pull/24131">bitwise operations on boolean vectors</a> was added in Zig 0.15, which is unreleased as of now (August 2025). If you want to run the code on your system, you need to build Zig from the master branch.</p></blockquote><p>To measure performance and compare against baseline, I’ll use one of my favorite CLI tools, <a href="https://github.com/andrewrk/poop">poop</a>:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>&gt; poop -d <span>10000</span> <span>"./zig-out/bin/substr"</span> <span>"./zig-out/bin/substr --simd"</span>
</span></span></code></pre></div><pre>
<span>Benchmark 1 (6361 runs)</span>: ./zig-out/bin/substr
<span>  measurement          </span><span>mean</span><span> ± </span><span>σ</span><span>            </span><span>min</span><span> … </span><span>max</span><span>           </span><span>outliers</span><span>         delta</span>
  wall_time          <span>1.22</span><span>ms</span> ± <span> 185</span><span>us</span>    <span> 903</span><span>us</span> … <span>5.33</span><span>ms</span>        242 ( 4%)        0%
  peak_rss           <span>1.20</span><span>MB</span> ± <span> 290</span><span>  </span>    <span>1.18</span><span>MB</span> … <span>1.20</span><span>MB</span>          2 ( 0%)        0%
  cpu_cycles         <span>2.15</span><span>M </span> ± <span>40.5</span><span>K </span>    <span>2.10</span><span>M </span> … <span>2.71</span><span>M </span>        312 ( 5%)        0%
  instructions       <span>1.85</span><span>M </span> ± <span>0.75</span><span>  </span>    <span>1.85</span><span>M </span> … <span>1.85</span><span>M </span>         56 ( 1%)        0%
  cache_references   <span>43.8</span><span>K </span> ± <span> 620</span><span>  </span>    <span>38.3</span><span>K </span> … <span>44.9</span><span>K </span>          9 ( 0%)        0%
  cache_misses       <span>19.0</span><span>K </span> ± <span>10.3</span><span>K </span>    <span>4.08</span><span>K </span> … <span>33.6</span><span>K </span>          0 ( 0%)        0%
  branch_misses      <span>48.1</span><span>  </span> ± <span>17.4</span><span>  </span>    <span>  20</span><span>  </span> … <span> 104</span><span>  </span>         97 ( 2%)        0%

<span>Benchmark 2 (10000 runs)</span>: ./zig-out/bin/substr --simd
<span>  measurement          </span><span>mean</span><span> ± </span><span>σ</span><span>            </span><span>min</span><span> … </span><span>max</span><span>           </span><span>outliers</span><span>         delta</span>
  wall_time          <span> 500</span><span>us</span> ± <span>96.9</span><span>us</span>    <span> 397</span><span>us</span> … <span>4.23</span><span>ms</span>        840 ( 8%)        <span>⚡</span><span>- 58.9% ±  0.4%</span>
  peak_rss           <span>1.20</span><span>MB</span> ± <span> 164</span><span>  </span>    <span>1.18</span><span>MB</span> … <span>1.20</span><span>MB</span>          1 ( 0%)          +  0.0% ±  0.0%
  cpu_cycles         <span> 369</span><span>K </span> ± <span>36.1</span><span>K </span>    <span> 340</span><span>K </span> … <span>1.10</span><span>M </span>       <span>1167 (12%)</span>        <span>⚡</span><span>- 82.8% ±  0.1%</span>
  instructions       <span> 578</span><span>K </span> ± <span>0.53</span><span>  </span>    <span> 578</span><span>K </span> … <span> 578</span><span>K </span>          6 ( 0%)        <span>⚡</span><span>- 68.8% ±  0.0%</span>
  cache_references   <span>38.8</span><span>K </span> ± <span> 545</span><span>  </span>    <span>34.1</span><span>K </span> … <span>40.5</span><span>K </span>          6 ( 0%)        <span>⚡</span><span>- 11.4% ±  0.0%</span>
  cache_misses       <span>5.62</span><span>K </span> ± <span>4.97</span><span>K </span>    <span>2.11</span><span>K </span> … <span>27.9</span><span>K </span>       <span>1529 (15%)</span>        <span>⚡</span><span>- 70.3% ±  1.2%</span>
  branch_misses      <span>2.88</span><span>K </span> ± <span>23.4</span><span>  </span>    <span>2.81</span><span>K </span> … <span>3.09</span><span>K </span>        453 ( 5%)        💩<span>+5879.8% ±  1.4%</span>
</pre><p>(Scroll right to see more data)</p><p>As you can see, for a large body of text, the speedup is noticeable: 59% faster with 80% less CPU cycles!</p><blockquote><p>The SIMD version only took 500 microseconds to complete on average, including the overhead of loading the program into memory and printing the result. 500 microseconds is half a millisecond. That’s how fast my laptop searches through a whole book, <strong>200 000 words</strong>, cover to cover. This is why computers are so powerful! How long would it take for a human to do that?</p></blockquote><p>This is quite a large improvement, and proves that the SIMD code is actually working (otherwise the reduction in CPU cycles wouldn’t be so massive). Can we do even better though?</p><h2 id="character-selection">Character selection
<a href="#character-selection">#</a></h2><p>You may notice from the output of <code>poop</code> that the number of branch misses has absolutely blown up – from 48 on average to 2.88k !</p><p>Why does this happen? Well, if you were to count how many times the inner while loop is entered when the mask is non-zero:</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>    </span><span>var</span><span> </span><span>i</span><span>:</span><span> </span><span>usize</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>var</span><span> </span><span>count</span><span>:</span><span> </span><span>usize</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>while</span><span> </span><span>(</span><span>i</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>+</span><span> </span><span>32</span><span> </span><span>&lt;=</span><span> </span><span>n</span><span>)</span><span> </span><span>:</span><span> </span><span>(</span><span>i</span><span> </span><span>+=</span><span> </span><span>32</span><span>)</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>const</span><span> </span><span>block_first</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>haystack</span><span>[</span><span>i</span><span>..][</span><span>0</span><span>..</span><span>32</span><span>].</span><span>*</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>const</span><span> </span><span>block_last</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>haystack</span><span>[</span><span>i</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span> </span><span>..][</span><span>0</span><span>..</span><span>32</span><span>].</span><span>*</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>const</span><span> </span><span>eq_first</span><span> </span><span>=</span><span> </span><span>first</span><span> </span><span>==</span><span> </span><span>block_first</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>const</span><span> </span><span>eq_last</span><span> </span><span>=</span><span> </span><span>last</span><span> </span><span>==</span><span> </span><span>block_last</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>var</span><span> </span><span>mask</span><span>:</span><span> </span><span>std</span><span>.</span><span>bit_set</span><span>.</span><span>IntegerBitSet</span><span>(</span><span>32</span><span>)</span><span> </span><span>=</span><span> </span><span>.{</span><span> </span><span>.</span><span>mask</span><span> </span><span>=</span><span> </span><span>@bitCast</span><span>(</span><span>eq_first</span><span> </span><span>&amp;</span><span> </span><span>eq_last</span><span>)</span><span> </span><span>};</span><span>
</span></span></span><span><span><span>        </span><span>while</span><span> </span><span>(</span><span>mask</span><span>.</span><span>findFirstSet</span><span>())</span><span> </span><span>|</span><span>bitpos</span><span>|</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>count</span><span> </span><span>+=</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>            </span><span>if</span><span> </span><span>(</span><span>std</span><span>.</span><span>mem</span><span>.</span><span>eql</span><span>(</span><span>u8</span><span>,</span><span> </span><span>haystack</span><span>[</span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span> </span><span>+</span><span> </span><span>1</span><span> </span><span>..][</span><span>0</span><span> </span><span>..</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span>],</span><span> </span><span>needle</span><span>[</span><span>1</span><span>..]))</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>                </span><span>std</span><span>.</span><span>debug</span><span>.</span><span>print</span><span>(</span><span>"found match with count: {}</span><span>\n</span><span>"</span><span>,</span><span> </span><span>.{</span><span>count</span><span>});</span><span>
</span></span></span><span><span><span>                </span><span>return</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span>;</span><span>
</span></span></span><span><span><span>            </span><span>}</span><span>
</span></span></span><span><span><span>            </span><span>_</span><span> </span><span>=</span><span> </span><span>mask</span><span>.</span><span>toggleFirstSet</span><span>();</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span></code></pre></div><div><pre tabindex="0"><code data-lang="fallback"><span><span>found match with count: 2792
</span></span></code></pre></div><p>The fact that&nbsp;<code>count</code>&nbsp;is so close to the number of mispredictions suggests that each time the mask is non‑zero we incur a branch miss.</p><p>Unfortunately, there is no obvious way to prevent this with the current algorithm. The state-of-the-art seems to be choosing two bytes in the needle that occur less frequently according to a pre-calculated frequency distribution. This is used in the <a href="https://github.com/BurntSushi/memchr"><code>memchr</code> crate</a> in Rust, as explained by the author in <a href="https://news.ycombinator.com/item?id=44275934">this comment on Hacker News</a>.</p><p>For example, the needle <code>newsletter</code> has the rarest characters <code>w</code> at index <code>2</code> and <code>l</code> at index <code>4</code>.</p><p>The function in <code>memchr</code> can be found <a href="https://github.com/BurntSushi/memchr/blob/3962118774ac511580c5b40fd14323e31629fa52/src/arch/all/packedpair/mod.rs#L163">here</a>. I’ve ported it into Zig, and you can see it <a href="https://github.com/aarol/substr/blob/9392f9557de735929dfb79efa4fc88115341c65d/src/main.zig#L100">here</a>.</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>    </span><span>const</span><span> </span><span>needle_index_pair</span><span> </span><span>=</span><span> </span><span>find_rarest</span><span>(</span><span>needle</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>first_letter</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>@splat</span><span>(</span><span>needle</span><span>[</span><span>needle_index_pair</span><span>[</span><span>0</span><span>]]);</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>first_offset</span><span> </span><span>=</span><span> </span><span>needle_index_pair</span><span>[</span><span>0</span><span>];</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>second_letter</span><span>:</span><span> </span><span>Block</span><span> </span><span>=</span><span> </span><span>@splat</span><span>(</span><span>needle</span><span>[</span><span>needle_index_pair</span><span>[</span><span>1</span><span>]]);</span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span><span>second_offset</span><span> </span><span>=</span><span> </span><span>needle_index_pair</span><span>[</span><span>1</span><span>];</span><span>
</span></span></span></code></pre></div><p>The algorithm is the exact same, but the index for <code>first_letter</code> and <code>second_letter</code> now varies according to the pre-calculated frequency distribution.</p><h3 id="benchmarks">Benchmarks
<a href="#benchmarks">#</a></h3><pre>
<span>Benchmark 1 (10000 runs)</span>: ./zig-out/bin/substr --simd
<span>  measurement          </span><span>mean</span><span> ± </span><span>σ</span><span>            </span><span>min</span><span> … </span><span>max</span><span>           </span><span>outliers</span><span>         delta</span>
  wall_time          <span> 472</span><span>us</span> ± <span>62.9</span><span>us</span>    <span> 400</span><span>us</span> … <span>1.62</span><span>ms</span>        735 ( 7%)        0%
  peak_rss           <span>1.20</span><span>MB</span> ± <span>   0</span><span>  </span>    <span>1.20</span><span>MB</span> … <span>1.20</span><span>MB</span>          0 ( 0%)        0%
  cpu_cycles         <span> 376</span><span>K </span> ± <span>44.7</span><span>K </span>    <span> 347</span><span>K </span> … <span>1.46</span><span>M </span>       <span>1213 (12%)</span>        0%
  instructions       <span> 578</span><span>K </span> ± <span>0.54</span><span>  </span>    <span> 578</span><span>K </span> … <span> 578</span><span>K </span>         10 ( 0%)        0%
  cache_references   <span>38.7</span><span>K </span> ± <span> 715</span><span>  </span>    <span>28.3</span><span>K </span> … <span>40.6</span><span>K </span>         96 ( 1%)        0%
  cache_misses       <span>7.37</span><span>K </span> ± <span>5.83</span><span>K </span>    <span>2.78</span><span>K </span> … <span>27.7</span><span>K </span>       <span>1608 (16%)</span>        0%
  branch_misses      <span>2.88</span><span>K </span> ± <span>23.4</span><span>  </span>    <span>2.82</span><span>K </span> … <span>3.08</span><span>K </span>        415 ( 4%)        0%

<span>Benchmark 2 (10000 runs)</span>: ./zig-out/bin/substr --simdv2
<span>  measurement          </span><span>mean</span><span> ± </span><span>σ</span><span>            </span><span>min</span><span> … </span><span>max</span><span>           </span><span>outliers</span><span>         delta</span>
  wall_time          <span> 429</span><span>us</span> ± <span>75.5</span><span>us</span>    <span> 369</span><span>us</span> … <span>3.85</span><span>ms</span>        393 ( 4%)        <span>⚡</span><span>-  9.1% ±  0.4%</span>
  peak_rss           <span>1.20</span><span>MB</span> ± <span>   0</span><span>  </span>    <span>1.20</span><span>MB</span> … <span>1.20</span><span>MB</span>          0 ( 0%)          -  0.0% ±  0.0%
  cpu_cycles         <span> 304</span><span>K </span> ± <span>28.4</span><span>K </span>    <span> 282</span><span>K </span> … <span>1.07</span><span>M </span>       <span>1140 (11%)</span>        <span>⚡</span><span>- 19.2% ±  0.3%</span>
  instructions       <span> 561</span><span>K </span> ± <span>0.52</span><span>  </span>    <span> 561</span><span>K </span> … <span> 561</span><span>K </span>          5 ( 0%)        <span>⚡</span><span>-  2.9% ±  0.0%</span>
  cache_references   <span>38.7</span><span>K </span> ± <span> 610</span><span>  </span>    <span>29.9</span><span>K </span> … <span>40.3</span><span>K </span>         25 ( 0%)          -  0.1% ±  0.0%
  cache_misses       <span>5.21</span><span>K </span> ± <span>3.53</span><span>K </span>    <span>2.57</span><span>K </span> … <span>27.3</span><span>K </span>       <span>1306 (13%)</span>        <span>⚡</span><span>- 29.3% ±  1.8%</span>
  branch_misses      <span>1.07</span><span>K </span> ± <span>14.0</span><span>  </span>    <span>1.02</span><span>K </span> … <span>1.17</span><span>K </span>        275 ( 3%)        <span>⚡</span><span>- 62.8% ±  0.0%</span>

</pre><p>Comparing to the previous SIMD version, the number of branch misses has dropped by 60%, and it’s 9% faster too. Nice!</p><blockquote><p>The number of branch misses is lower, which can cause faster execution, but I suspect that a much bigger impact is the fact that there are less false positives, which means less byte-by-byte memory accesses and comparisons.</p></blockquote><h2 id="avx-512">AVX-512
<a href="#avx-512">#</a></h2><p>Since AMD <a href="https://en.wikipedia.org/wiki/Zen_4">Zen 4 </a>and Intel <a href="https://en.wikipedia.org/wiki/Cannon_Lake_%28microprocessor%29">Cannon Lake</a>, there has been a new SIMD instruction set, AVX-512 with 512-bit instructions – double the size of AVX2. I don’t have a computer that has AVX-512 right now, but I suspect that changing the Zig code to process 64 characters at once would lead to even better results.</p><h2 id="a-smaller-haystack">A smaller haystack
<a href="#a-smaller-haystack">#</a></h2><p>It’s clear that with a very large haystack, the SIMD version is much faster. But what about a tiny input, like less than a hunder characters?</p><p>I did a bit of benchmarking with <code>poop</code>, but I found that I couldn’t accurately measure the speed, since both versions finish extremely very quickly. I decided to use <a href="https://github.com/hendriknielaender/zBench">zBench</a> to do a microbenchmark. I decided to use a snippet from Moby Dick as seen <a href="https://github.com/aarol/substr/blob/main/src/haystack-small.txt">here</a>.</p><pre>+- run test stderr
benchmark              runs     total time     time/run (avg ± σ)    (min ... max)                p75        p99        p995      
-----------------------------------------------------------------------------------------------------------------------------
find_substr            <span>100000   424.368ms      </span><span>4.243us ± 740ns       </span><span>(3.964us ... 107.923us)      </span><span>4.187us    7.075us    7.245us   </span>
find_substr_simd_v2    <span>100000   147.883ms      </span><span>1.478us ± 186ns       </span><span>(1.417us ... 21.354us)       </span><span>1.483us    1.539us    1.548us   </span>
</pre><p>I was surprised to see that even when processing less than a hundred characters, the SIMD algorithm is still faster! The difference between 4μs vs 1μs is extremely small, but it’s slightly faster nonetheless.</p><h2 id="conclusion">Conclusion
<a href="#conclusion">#</a></h2><p>As you can see, SIMD can be used to make substring searching dramatically faster, for both very large and very small strings.</p><p>But if it’s so much better, then why haven’t I made a pull request to change <code>std.mem.indexOf</code> to use SIMD? Well, the reason is that</p><ol><li><code>std.mem.indexOf</code> is generic over element size, and having a size larger than <code>u8</code> makes the algorithm much slower</li><li>The <a href="https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore%E2%80%93Horspool_algorithm">algorithm</a> used in <code>stdmem.indexOf</code> is cross-platform, while the SIMD code wouldn’t be. (not all platforms have SIMD registers at all, Arm has only 128-bit)</li></ol><p>Substring searching is rarely the bottleneck in programs, especially ones written in a fast language like Zig. That’s why I don’t personally think it would be worth it to add it to the standard library.</p><p>Still, it was great to learn about this advanced optimization technique and see some concrete performance measurements from it!</p><p>The full code is available on GitHub <a href="https://github.com/aarol/substr/">here</a>.</p><h2 id="further-reading">Further reading
<a href="#further-reading">#</a></h2><ul><li>SIMD with Zig <a href="https://www.openmymind.net/SIMD-With-Zig/">https://www.openmymind.net/SIMD-With-Zig/</a></li><li>SIMD-friendly algorithms for substring searching: <a href="http://0x80.pl/notesen/2016-11-28-simd-strfind.html">http://0x80.pl/notesen/2016-11-28-simd-strfind.html</a></li><li><code>memchr</code> source code: <a href="https://github.com/BurntSushi/memchr">https://github.com/BurntSushi/memchr</a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hand-picked selection of articles on AI fundamentals/concepts (194 pts)]]></title>
            <link>https://aman.ai/primers/ai/</link>
            <guid>44862112</guid>
            <pubDate>Mon, 11 Aug 2025 08:59:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aman.ai/primers/ai/">https://aman.ai/primers/ai/</a>, See on <a href="https://news.ycombinator.com/item?id=44862112">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  <header>
    
  </header>

  <article>
  <h2 id="overview">Overview</h2>

<ul>
  <li>Here’s a hand-picked selection of articles on AI fundamentals/concepts that cover the entire process of building neural nets to training them to evaluating results.</li>
</ul>

<h2 id="algorithmsarchitecture">Algorithms/Architecture</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/linear-logistic-regression">Linear and Logistic Regression</a></li>
  <li><a href="https://aman.ai/primers/ai/k-nearest-neighbors">k-Nearest Neighbors</a></li>
  <li><a href="https://aman.ai/primers/ai/clustering">Clustering</a></li>
  <li><a href="https://aman.ai/primers/ai/support-vector-machines">Support Vector Machines (SVM)</a></li>
  <li><a href="https://aman.ai/primers/ai/naive-bayes">Naive Bayes</a></li>
  <li><a href="https://aman.ai/primers/ai/decision-trees-and-ensemble-methods">Decision Trees and Ensemble Methods</a></li>
  <li><a href="https://aman.ai/primers/ai/ml-comp">ML Algorithms Comparative Analysis</a></li>
  <li><a href="https://aman.ai/primers/ai/dl-comp">DL Architectures Comparative Analysis</a></li>
  <li><a href="https://aman.ai/primers/ai/prompt-engineering">Prompt Engineering</a></li>
  <li><a href="https://aman.ai/primers/ai/gan">Generative Adversarial Networks (GANs)</a></li>
  <li><a href="https://aman.ai/primers/ai/diffusion-models">Diffusion Models</a></li>
  <li><a href="https://aman.ai/primers/ai/gnn">Graph Neural Networks</a></li>
  <li><a href="https://aman.ai/primers/ai/attention">Attention</a></li>
  <li><a href="https://aman.ai/primers/ai/separable-convolutions">Separable Convolutions</a></li>
  <li><a href="https://aman.ai/primers/ai/inductive-bias">Inductive Bias</a></li>
  <li><a href="https://aman.ai/primers/ai/cnn">Convolutional Neural Networks</a></li>
  <li><a href="https://aman.ai/primers/ai/reinforcement-learning">Reinforcement Learning</a></li>
  <li><a href="https://aman.ai/primers/ai/mixture-of-experts">Mixture-of-Experts</a></li>
  <li><a href="https://aman.ai/primers/ai/state-space-models">State Space Models</a></li>
  <li><a href="https://aman.ai/primers/ai/agents">Agents</a></li>
  <li><a href="https://aman.ai/primers/ai/flashattention">FlashAttention</a>
<!-- - [Quantization](../ai/quantization) --></li>
  <li><a href="https://aman.ai/primers/ai/model-acceleration">Model Acceleration</a></li>
  <li><a href="https://aman.ai/primers/ai/speculative-decoding">Speculative Decoding</a></li>
  <li><a href="https://aman.ai/primers/ai/cross-validation">Cross Validation</a></li>
</ul>

<h2 id="datatraining">Data/Training</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/data-sampling">Data Sampling</a></li>
  <li><a href="https://aman.ai/primers/ai/data-imbalance">Data Imbalance</a></li>
  <li><a href="https://aman.ai/primers/ai/standardization-vs-normalization">Standardization vs. Normalization</a></li>
  <li><a href="https://aman.ai/primers/ai/learning-paradigms">Learning Paradigms</a></li>
  <li><a href="https://aman.ai/primers/ai/xavier-init">Xavier Initialization</a></li>
  <li><a href="https://aman.ai/primers/ai/padding-and-packing">Padding and Packing</a></li>
  <li><a href="https://aman.ai/primers/ai/regularization">Regularization</a></li>
  <li><a href="https://aman.ai/primers/ai/gradient-descent-and-backprop">Gradient Descent and Backprop</a></li>
  <li><a href="https://aman.ai/primers/ai/activation-functions">Activation Functions</a></li>
  <li><a href="https://aman.ai/primers/ai/loss">Loss Functions</a></li>
  <li><a href="https://aman.ai/primers/ai/activation">Activation Functions</a></li>
  <li><a href="https://aman.ai/primers/ai/fine-tuning-models">Fine-tuning Models</a></li>
  <li><a href="https://aman.ai/primers/ai/data-split">Splitting Datasets</a></li>
  <li><a href="https://aman.ai/primers/ai/batchnorm">Batchnorm</a></li>
  <li><a href="https://aman.ai/primers/ai/dropout">Dropout</a></li>
  <li><a href="https://aman.ai/primers/ai/double-descent">Double Descent</a></li>
  <li><a href="https://aman.ai/primers/ai/fine-tune-and-eval-BERT">Fine-Tuning and Evaluating BERT</a>
<!-- - [Debugging Deep Learning Projects](../ai/debugging-dl-projects) --></li>
  <li><a href="https://aman.ai/primers/ai/train-val-loss">Training Loss &gt; Validation Loss?</a></li>
  <li><a href="https://aman.ai/primers/ai/svm-kernel-trick">SVM Kernel/Polynomial Trick</a></li>
  <li><a href="https://aman.ai/primers/ai/bias-variance-tradeoff">Bias Variance Tradeoff</a></li>
  <li><a href="https://aman.ai/primers/ai/grad-accum-checkpoint">Gradient Accumulation and Checkpointing</a></li>
  <li><a href="https://aman.ai/primers/ai/parameter-efficient-fine-tuning">Parameter Efficient Fine-Tuning</a></li>
  <li><a href="https://aman.ai/primers/ai/hypernetworks">Hypernetworks</a></li>
  <li><a href="https://aman.ai/primers/ai/distributed-training-parallelism">Distributed Training Parallelism</a></li>
</ul>

<h2 id="speech">Speech</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/speech-processing">Speech Processing</a></li>
</ul>

<h2 id="vision">Vision</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/vit">Vision Transformer (ViT)</a></li>
  <li><a href="https://aman.ai/primers/ai/receptive-field">Receptive Field</a></li>
  <li><a href="https://aman.ai/primers/ai/skip-connections">Residual Networks/Skip Connections</a></li>
  <li><a href="https://aman.ai/primers/ai/gpt4o-native-image-generation">GPT-4o Native Image Generation</a></li>
</ul>

<h2 id="nlp">NLP</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/word-vectors">Word Vectors/Embeddings</a></li>
  <li><a href="https://aman.ai/primers/ai/nlp-tasks">NLP Tasks</a></li>
  <li><a href="https://aman.ai/primers/ai/preprocessing">Preprocessing</a></li>
  <li><a href="https://aman.ai/primers/ai/tokenizer">Tokenization</a></li>
  <li><a href="https://aman.ai/primers/ai/data-sampling">Data Sampling</a></li>
  <li><a href="https://aman.ai/primers/ai/architectures">Neural Architectures</a></li>
  <li><a href="https://aman.ai/primers/ai/attention">Attention</a></li>
  <li><a href="https://aman.ai/primers/ai/transformers">Transformers</a></li>
  <li><a href="https://aman.ai/primers/ai/token-sampling">Token Sampling Methods</a></li>
  <li><a href="https://aman.ai/primers/ai/encoder-vs-decoder-models">Encoder vs. Decoder vs. Encoder-Decoder Models</a>
<!-- - [Language Models](../ai/language-model) --></li>
  <li><a href="https://aman.ai/primers/ai/LLM">Overview of Large Language Models (LLMs)</a></li>
  <li><a href="https://aman.ai/primers/ai/reinforcement-finetuning">Reinforcement Fine-Tuning</a></li>
  <li><a href="https://aman.ai/primers/ai/preference-optimization">Preference Optimization</a></li>
  <li><a href="https://aman.ai/primers/ai/translation">Machine Translation</a></li>
  <li><a href="https://aman.ai/primers/ai/knowledge-graphs">Knowledge Graphs</a></li>
  <li><a href="https://aman.ai/primers/ai/hallucination">Hallucination Detection and Mitigation</a></li>
  <li><a href="https://aman.ai/primers/ai/AIDetect">AI Text Detection Techniques</a></li>
  <li><a href="https://aman.ai/primers/ai/ner">Named Entity Recognition</a></li>
  <li><a href="https://aman.ai/primers/ai/textual-entailment">Textual Entailment</a></li>
  <li><a href="https://aman.ai/primers/ai/RAG">Retrieval Augmented Generation (RAG)</a></li>
  <li><a href="https://aman.ai/primers/ai/context-length-extension">LLM Context Length Extension</a></li>
  <li><a href="https://aman.ai/primers/ai/document-intelligence">Document Intelligence</a>
<!-- - [Personalizing Large Language Models](../ai/personalize-LLMs) --></li>
  <li><a href="https://aman.ai/primers/ai/code-mixing-switching">Code Mixing and Switching</a></li>
  <li><a href="https://aman.ai/primers/ai/LLMOps">Large Language Model Ops (LLMOps)</a></li>
  <li><a href="https://aman.ai/primers/ai/benchmarks">LLM/VLM Benchmarks</a></li>
</ul>

<h2 id="multimodal">Multimodal</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/VLM">Overview of Vision-Language Models (VLMs)</a></li>
  <li><a href="https://aman.ai/primers/ai/vision-language-models">VLM Architectures</a></li>
  <li><a href="https://aman.ai/primers/ai/computer-control">Computer Control</a></li>
</ul>

<h2 id="models">Models</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/bert">BERT</a></li>
  <li><a href="https://aman.ai/primers/ai/gpt">GPT</a></li>
  <li><a href="https://aman.ai/primers/ai/CLIP">CLIP</a></li>
  <li><a href="https://aman.ai/primers/ai/meena">Meena</a></li>
  <li><a href="https://aman.ai/primers/ai/chatGPT">ChatGPT</a></li>
  <li><a href="https://aman.ai/primers/ai/GPT-4">GPT-4</a></li>
  <li><a href="https://aman.ai/primers/ai/LLaMA">LLaMA</a></li>
  <li><a href="https://aman.ai/primers/ai/alpaca">Alpaca</a></li>
  <li><a href="https://aman.ai/primers/ai/gemini">Gemini</a></li>
  <li><a href="https://aman.ai/primers/ai/toolformer">Toolformer</a></li>
  <li><a href="https://aman.ai/primers/ai/visualChatGPT">Visual ChatGPT</a></li>
  <li><a href="https://aman.ai/primers/ai/TaskMatrix">TaskMatrix.AI</a></li>
  <li><a href="https://aman.ai/primers/ai/bigbird">BigBird</a></li>
  <li><a href="https://aman.ai/primers/ai/o1">OpenAI o1</a></li>
  <li><a href="https://aman.ai/primers/ai/deepseek-R1">DeepSeek R1</a></li>
  <li><a href="https://aman.ai/primers/ai/deepseek-janus-pro">DeepSeek Janus-Pro</a></li>
  <li><a href="https://aman.ai/primers/ai/gemma-3n">Gemma 3n</a></li>
</ul>

<h2 id="offlineonline-evaluation">Offline/Online Evaluation</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/evaluation-metrics">Evaluation Metrics</a></li>
  <li><a href="https://aman.ai/primers/ai/f-beta">F-Beta Score</a></li>
  <li><a href="https://aman.ai/primers/ai/ab-testing">A/B Testing</a></li>
</ul>

<h2 id="mlops">MLOps</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/drift">Data Drift</a></li>
  <li><a href="https://aman.ai/primers/ai/mlops-tooling">MLOps Tooling</a></li>
  <li><a href="https://aman.ai/primers/ai/mlops-testing">MLOps Testing</a></li>
</ul>

<h2 id="on-device-ai">On-Device AI</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/model-compression">Model Compression</a></li>
  <li><a href="https://aman.ai/primers/ai/pii">Personally Identifiable Information (PII)</a></li>
  <li><a href="https://aman.ai/primers/ai/federated-learning">Federated Learning</a></li>
  <li><a href="https://aman.ai/primers/ai/differential-privacy">Differential Privacy</a></li>
  <li><a href="https://aman.ai/primers/ai/on-device-transformers">On-device Transformers</a></li>
</ul>

<h2 id="project-planning-scheduling-execution">Project Planning, Scheduling, Execution</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/okr">Objectives and Key Results (OKRs)</a></li>
  <li><a href="https://aman.ai/primers/ai/rice-framework">RICE Framework</a></li>
  <li><a href="https://aman.ai/primers/ai/gantt-charts">Gantt Charts</a></li>
  <li><a href="https://aman.ai/primers/ai/project-management">Project Management</a></li>
</ul>

<h2 id="miscellaneous">Miscellaneous</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/top-30-papers">Ilya Sutskever’s Top 30</a></li>
  <li><a href="https://aman.ai/primers/ai/model-debugging">Debugging Model Training</a></li>
  <li><a href="https://aman.ai/primers/ai/ml-runtimes">ML Runtimes</a></li>
  <li><a href="https://aman.ai/primers/ai/chain-rule">Chain Rule</a></li>
  <li><a href="https://aman.ai/primers/ai/bayes-theorem">Bayes’ Theorem</a></li>
  <li><a href="https://aman.ai/primers/ai/probability-calibration">Probability Calibration</a></li>
  <li><a href="https://aman.ai/primers/ai/multiclass-vs-multilabel-classification">Multiclass vs. Multilabel Classification</a></li>
  <li><a href="https://aman.ai/primers/ai/matmul">N-Dimensional Tensor Product</a></li>
  <li><a href="https://aman.ai/primers/ai/pytorch-vs-tensorflow">PyTorch vs. TensorFlow</a></li>
  <li><a href="https://aman.ai/primers/ai/ann-similarity-search">Approximate Nearest Neighbors – Similarity Search</a></li>
  <li><a href="https://aman.ai/primers/ai/transferability-estimation">Transferability Estimation</a></li>
  <li><a href="https://aman.ai/primers/ai/tensorboard">TensorBoard</a></li>
  <li><a href="https://aman.ai/primers/ai/cnns-for-text-classification">Convolutional Neural Networks for Text Classification</a></li>
  <li><a href="https://aman.ai/primers/ai/hmm-and-naive-bayes">Relationship between Hidden Markov Models and Naive Bayes</a></li>
  <li><a href="https://aman.ai/primers/ai/maximum-entropy-markov-models-and-logistic-reg">Maximum Entropy Markov Models</a></li>
  <li><a href="https://aman.ai/primers/ai/conditional-random-fields">Conditional Random Fields</a>
<!-- - [Information Retrieval Methods](../ai/info-retrieval-methods) --></li>
</ul>

<h2 id="hyperparameters">Hyperparameters</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/hyperparameter-tuning">Hyperparameter Tuning</a></li>
  <li><a href="https://aman.ai/primers/ai/hyperparameter-logging">Hyperparameter Logging</a></li>
</ul>

<h2 id="practice">Practice</h2>

<ul>
  <li><a href="https://aman.ai/primers/ai/interview">Interview Questions</a></li>
</ul>

  </article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AOL to discontinue dial-up internet (118 pts)]]></title>
            <link>https://www.nytimes.com/2025/08/11/business/aol-dial-up-internet.html</link>
            <guid>44861521</guid>
            <pubDate>Mon, 11 Aug 2025 07:15:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/08/11/business/aol-dial-up-internet.html">https://www.nytimes.com/2025/08/11/business/aol-dial-up-internet.html</a>, See on <a href="https://news.ycombinator.com/item?id=44861521">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/08/11/business/aol-dial-up-internet.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Google paid a $250K reward for a bug (453 pts)]]></title>
            <link>https://issues.chromium.org/issues/412578726</link>
            <guid>44861106</guid>
            <pubDate>Mon, 11 Aug 2025 05:56:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://issues.chromium.org/issues/412578726">https://issues.chromium.org/issues/412578726</a>, See on <a href="https://news.ycombinator.com/item?id=44861106">Hacker News</a></p>
<div id="readability-page-1" class="page"><header><div ng-non-bindable="" data-ogsr-up="" id="gb"><p><a aria-label="Sign in" href="https://accounts.google.com/ServiceLogin?passive=1209600&amp;osid=1&amp;continue=https%3A%2F%2Fissues.chromium.org%2Fissues%2F412578726&amp;followup=https%3A%2F%2Fissues.chromium.org%2Fissues%2F412578726&amp;ec=GAZAkwI" target="_top"><span>Sign in</span></a></p></div></header></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Basic Social Skills Guide (211 pts)]]></title>
            <link>https://www.improveyoursocialskills.com/basic-social-skills-guide</link>
            <guid>44860932</guid>
            <pubDate>Mon, 11 Aug 2025 05:19:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.improveyoursocialskills.com/basic-social-skills-guide">https://www.improveyoursocialskills.com/basic-social-skills-guide</a>, See on <a href="https://news.ycombinator.com/item?id=44860932">Hacker News</a></p>
Couldn't get https://www.improveyoursocialskills.com/basic-social-skills-guide: Error: Request failed with status code 521]]></description>
        </item>
        <item>
            <title><![CDATA[Going faster than memcpy (128 pts)]]></title>
            <link>https://squadrick.dev/journal/going-faster-than-memcpy</link>
            <guid>44860847</guid>
            <pubDate>Mon, 11 Aug 2025 04:59:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://squadrick.dev/journal/going-faster-than-memcpy">https://squadrick.dev/journal/going-faster-than-memcpy</a>, See on <a href="https://news.ycombinator.com/item?id=44860847">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      



<p>While profiling <a href="https://github.com/squadrick/shadesmar">Shadesmar</a> a couple of
weeks ago, I noticed that for large binary unserialized messages (&gt;512kB) most
of the execution time is spent doing copying the message (using <code>memcpy</code>)
between process memory to shared memory and back.</p>

<p>I had a few hours to kill last weekend, and I tried to implement a faster way
to do memory copies.</p>

<hr>

<h3 id="autopsy-of-memcpy">Autopsy of memcpy</h3>

<p>Here’s the dumb of <a href="https://perf.wiki.kernel.org/index.php/Main_Page"><code>perf</code></a>
when running pub-sub for messages of sizes between 512kB and 2MB.</p>

<div><pre><code> Children      Self  Shared Object      Symbol
+  99.86%     0.00%  libc-2.27.so       [.] __libc_start_main
+  99.86%     0.00%  [unknown]          [k] 0x4426258d4c544155
+  99.84%     0.02%  raw_benchmark      [.] main
+  98.13%    97.12%  libc-2.27.so       [.] __memmove_avx_unaligned_erms
+  51.99%     0.00%  raw_benchmark      [.] shm::PublisherBin&lt;16u&gt;::publish
+  51.98%     0.01%  raw_benchmark      [.] shm::Topic&lt;16u&gt;::write
+  47.64%     0.01%  raw_benchmark      [.] shm::Topic&lt;16u&gt;::read
</code></pre></div>

<p><code>__memmove_avx_unaligned_erms</code> is an implementation of <code>memcpy</code> for unaligned
memory blocks that uses AVX to copy over 32 bytes at a time. Digging into the
<code>glibc</code> source code, I found this:</p>

<div><pre><code><span>#if IS_IN (libc)
# define VEC_SIZE                32
# define VEC(i)                  ymm##i
# define VMOVNT                  vmovntdq
# define VMOVU                   vmovdqu
# define VMOVA                   vmovdqa
# define SECTION(p)              p##.avx
# define MEMMOVE_SYMBOL(p,s)     p##_avx_##s
</span>
<span># include "memmove-vec-unaligned-erms.S"
#endif
</span></code></pre></div>

<p>Breaking down this function:</p>

<p><code>memmove</code>: <code>glibc</code> implements <code>memcpy</code> as a <code>memmove</code> instead, here’s the
relevant source code:</p>

<div><pre><code><span># define SYMBOL_NAME memcpy
# include "ifunc-memmove.h"
</span>
<span>libc_ifunc_redirected</span> <span>(</span><span>__redirect_memcpy</span><span>,</span> <span>__new_memcpy</span><span>,</span>
		       <span>IFUNC_SELECTOR</span> <span>());</span>
</code></pre></div>

<p>Here’s the difference between the two: With <code>memcpy</code>, the destination cannot
overlap the source at all. With <code>memmove</code> it can. Initially, I wasn’t sure why
it was implemented as <code>memmove</code>. The reason for this will become clearer as
the post proceeds.</p>

<p><code>erms</code>: <em>E</em>nhanced <em>R</em>ep <em>M</em>ov<em>s</em> is a hardware optimization for a loop that
does a simple copy. In simple pseudo-code, this is what the loop implementation
looks like for copying a single byte at a time (<code>REP MOVSB</code>).</p>

<div><pre><code><span>void</span> <span>rep_movsb</span><span>(</span><span>void</span> <span>*</span><span>dest</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>src</span><span>,</span> <span>size_t</span> <span>len</span><span>)</span> <span>{</span>
  <span>const</span> <span>uint8_t</span><span>*</span> <span>s</span> <span>=</span> <span>(</span><span>uint8_t</span><span>*</span><span>)</span><span>src</span><span>;</span>
  <span>uint8_t</span><span>*</span> <span>d</span> <span>=</span> <span>(</span><span>uint8_t</span><span>*</span><span>)</span><span>dest</span><span>;</span>

  <span>while</span> <span>(</span><span>len</span><span>--</span><span>)</span>
    <span>*</span><span>d</span><span>++</span> <span>=</span> <span>*</span><span>s</span><span>++</span><span>;</span>

  <span>return</span> <span>dest</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Since the loop copies data pointer by pointer, it can handle the case of
overlapping data.</p>

<p><code>vec</code>: For the above loop rather than copying around single bytes, it uses x86
vectorized instructions to copy multiple bytes in a single loop iteration
(technically single instruction). <code>vmov*</code> are assembly instructions for AVX
which is the latest instruction set that the CPU on my laptop supports. With
<code>VEC_SIZE = 32</code>, it copies 32 bytes at a time.</p>

<p><code>unaligned</code>: This is a generic version of <code>memmove</code> that can copy between any
pointer locations irrespective of their alignment. Unaligned pointers increase
complexity for the copy loop when using vectorized instructions. The unaligned
preceeding and trailing memory locations must be copied separately before hitting the
optimized loop.</p>

<p><code>memmove-vec-unaligned-erms.S</code> holds the actual implementation in assembly. A
few things that the implementation does:</p>

<ol>
  <li>
    <p>It uses <code>REP MOVS</code> only if the data is greater than 4kB. For smaller values it uses the SSE2 optimization.</p>
  </li>
  <li>For handling <code>unaligned</code> pointers, it uses the following blocks:
    <ul>
      <li>16 to 31: <code>vmovdqu</code></li>
      <li>15 to 8: <code>movq</code></li>
      <li>7 to 4: <code>movl</code></li>
      <li>3 to 2: <code>movzwl</code> and <code>movw</code></li>
    </ul>
  </li>
  <li><code>VMOVNT</code> defined above is for doing non-temporal(NT) moves. NT instructions
are used when there is an overlap between destination and source since
destination may be in cache when source is loaded. Uses <code>prefetcht0</code> to load
data into cache (all levels: t0). In the current iteration, we prefetch the
data for 2 iterations later. The data is copied (via cache) into registers. The
data (via NT) is copied from registers into destination.</li>
</ol>

<div><pre><code><span>L</span><span>(</span><span>loop_large_forward</span><span>):</span>
	<span>; Copy 4 * VEC a time forward with non-temporal stores.</span>
	<span>PREFETCH_ONE_SET</span> <span>(</span><span>1</span><span>,</span> <span>(</span><span>%</span><span>rsi</span><span>),</span> <span>PREFETCHED_LOAD_SIZE</span> <span>*</span> <span>2</span><span>)</span>
	<span>PREFETCH_ONE_SET</span> <span>(</span><span>1</span><span>,</span> <span>(</span><span>%</span><span>rsi</span><span>),</span> <span>PREFETCHED_LOAD_SIZE</span> <span>*</span> <span>3</span><span>)</span>
  <span>; PREFETCH 256b from rsi+256 to rsi+511</span>

	<span>VMOVU</span>	<span>(</span><span>%</span><span>rsi</span><span>),</span> <span>%</span><span>VEC</span><span>(</span><span>0</span><span>)</span>
	<span>VMOVU</span>	<span>VEC_SIZE</span><span>(</span><span>%</span><span>rsi</span><span>),</span> <span>%</span><span>VEC</span><span>(</span><span>1</span><span>)</span>
	<span>VMOVU</span>	<span>(</span><span>VEC_SIZE</span> <span>*</span> <span>2</span><span>)(</span><span>%</span><span>rsi</span><span>),</span> <span>%</span><span>VEC</span><span>(</span><span>2</span><span>)</span>
	<span>VMOVU</span>	<span>(</span><span>VEC_SIZE</span> <span>*</span> <span>3</span><span>)(</span><span>%</span><span>rsi</span><span>),</span> <span>%</span><span>VEC</span><span>(</span><span>3</span><span>)</span>
  <span>; mov 128b from rsi to rsi+127 -&gt; 4 ymm registers (cache)</span>
  <span>; 2 loops later, we hit the prefetched values</span>

	<span>addq</span>	<span>$</span><span>PREFETCHED_LOAD_SIZE</span><span>,</span> <span>%</span><span>rsi</span>  <span>; advance to rsi+128 in next loop</span>
	<span>subq</span>	<span>$</span><span>PREFETCHED_LOAD_SIZE</span><span>,</span> <span>%</span><span>rdx</span>

	<span>VMOVNT</span>	<span>%</span><span>VEC</span><span>(</span><span>0</span><span>),</span> <span>(</span><span>%</span><span>rdi</span><span>)</span>
	<span>VMOVNT</span>	<span>%</span><span>VEC</span><span>(</span><span>1</span><span>),</span> <span>VEC_SIZE</span><span>(</span><span>%</span><span>rdi</span><span>)</span>
	<span>VMOVNT</span>	<span>%</span><span>VEC</span><span>(</span><span>2</span><span>),</span> <span>(</span><span>VEC_SIZE</span> <span>*</span> <span>2</span><span>)(</span><span>%</span><span>rdi</span><span>)</span>
	<span>VMOVNT</span>	<span>%</span><span>VEC</span><span>(</span><span>3</span><span>),</span> <span>(</span><span>VEC_SIZE</span> <span>*</span> <span>3</span><span>)(</span><span>%</span><span>rdi</span><span>)</span>
  <span>; mov 128b from 4 ymm register -&gt; rdi to rdi+127 (no cache)</span>

	<span>addq</span>	<span>$</span><span>PREFETCHED_LOAD_SIZE</span><span>,</span> <span>%</span><span>rdi</span>  <span>; advance to rdi+128 in next loop</span>
	<span>cmpq</span>	<span>$</span><span>PREFETCHED_LOAD_SIZE</span><span>,</span> <span>%</span><span>rdx</span>
	<span>ja</span>	<span>L</span><span>(</span><span>loop_large_forward</span><span>)</span>
</code></pre></div>

<hr>

<h3 id="method-1-basic-rep-movsb">Method 1: Basic REP MOVSB</h3>

<p>Before getting into more exotic implementations, I wanted to first implement a
super simple version of ERSB to see how well it would perform. I used inline
assembly to write out the loop.</p>

<div><pre><code><span>void</span> <span>_rep_movsb</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>)</span> <span>{</span>
  <span>asm</span> <span>volatile</span><span>(</span><span>"rep movsb"</span>
               <span>:</span> <span>"=D"</span><span>(</span><span>d</span><span>),</span> <span>"=S"</span><span>(</span><span>s</span><span>),</span> <span>"=c"</span><span>(</span><span>n</span><span>)</span>
               <span>:</span> <span>"0"</span><span>(</span><span>d</span><span>),</span> <span>"1"</span><span>(</span><span>s</span><span>),</span> <span>"2"</span><span>(</span><span>n</span><span>)</span>
               <span>:</span> <span>"memory"</span><span>);</span>
<span>}</span>
</code></pre></div>

<p>This does the same as the pseudo-code attached above, but I wrote it in
assembly to prevent any compiler optimization, and rely only on the hardware
ERMS optimization.</p>

<h3 id="alternate-2-aligned-avx">Alternate 2: Aligned AVX</h3>

<p>One of the complexities in <code>glibc</code>’s  implementation is getting it to work for
unaligned pointers. Since I control the memory allocation, I figured I could
recreate the implementation focused solely on aligned pointer and sizes. I’m
using AVX intrinsics for 32-byte vectors (AVX):</p>

<div><pre><code><span>void</span> <span>_avx_cpy</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>)</span> <span>{</span>
  <span>// d, s -&gt; 32 byte aligned</span>
  <span>// n -&gt; multiple of 32</span>
  <span>auto</span> <span>*</span><span>dVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>__m256i</span> <span>*&gt;</span><span>(</span><span>d</span><span>);</span>
  <span>const</span> <span>auto</span> <span>*</span><span>sVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>const</span> <span>__m256i</span> <span>*&gt;</span><span>(</span><span>s</span><span>);</span>
  <span>size_t</span> <span>nVec</span> <span>=</span> <span>n</span> <span>/</span> <span>sizeof</span><span>(</span><span>__m256i</span><span>);</span>
  <span>for</span> <span>(;</span> <span>nVec</span> <span>&gt;</span> <span>0</span><span>;</span> <span>nVec</span><span>--</span><span>,</span> <span>sVec</span><span>++</span><span>,</span> <span>dVec</span><span>++</span><span>)</span> <span>{</span>
    <span>const</span> <span>__m256i</span> <span>temp</span> <span>=</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span><span>);</span>
    <span>_mm256_store_si256</span><span>(</span><span>dVec</span><span>,</span> <span>temp</span><span>);</span>
  <span>}</span>
<span>}</span>
</code></pre></div>

<p>The logic is identical to the previous <code>REP MOVSB</code> loop instead operating on 32
bytes at a time.</p>

<h3 id="method-3-stream-aligned-avx">Method 3: Stream aligned AVX</h3>

<p><code>_mm256_load_si256</code> and <code>_mm256_store_si256</code> go through the cache, which incurs
additional overhead. AVX instruction set has <code>_stream_</code> load and store
instructions that skip the cache. The performance of this copy is dependant
on:</p>
<ol>
  <li>Quantity of data to copy</li>
  <li>Cache size</li>
</ol>

<p>Non-temporal moves may bog down the performance for smaller copies (that can
fit into L2 cache) compared to regular moves.</p>

<div><pre><code><span>void</span> <span>_avx_async_cpy</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>)</span> <span>{</span>
  <span>// d, s -&gt; 32 byte aligned</span>
  <span>// n -&gt; multiple of 32</span>
  <span>auto</span> <span>*</span><span>dVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>__m256i</span> <span>*&gt;</span><span>(</span><span>d</span><span>);</span>
  <span>const</span> <span>auto</span> <span>*</span><span>sVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>const</span> <span>__m256i</span> <span>*&gt;</span><span>(</span><span>s</span><span>);</span>
  <span>size_t</span> <span>nVec</span> <span>=</span> <span>n</span> <span>/</span> <span>sizeof</span><span>(</span><span>__m256i</span><span>);</span>
  <span>for</span> <span>(;</span> <span>nVec</span> <span>&gt;</span> <span>0</span><span>;</span> <span>nVec</span><span>--</span><span>,</span> <span>sVec</span><span>++</span><span>,</span> <span>dVec</span><span>++</span><span>)</span> <span>{</span>
    <span>const</span> <span>__m256i</span> <span>temp</span> <span>=</span> <span>_mm256_stream_load_si256</span><span>(</span><span>sVec</span><span>);</span>
    <span>_mm256_stream_si256</span><span>(</span><span>dVec</span><span>,</span> <span>temp</span><span>);</span>
  <span>}</span>
  <span>_mm_sfence</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>Exact code as before but using non-temporal moves instead. There’s an extra
<code>_mm_sfence</code> which guarantees that all stores in the preceding loop are 
visible globally.</p>

<h3 id="method-4-stream-aligned-avx-with-prefetch">Method 4: Stream aligned AVX with prefetch</h3>

<p>In the previous method, we skipped the cache entirely. We can squeeze a bit
more performance by prefetching the source data into the cache for the next
iteration in the current iteration. Since all prefetches work on cache-lines
(64-bytes), each loop iteration copies 64-bytes from source to data.</p>

<div><pre><code><span>void</span> <span>_avx_async_pf_cpy</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>)</span> <span>{</span>
  <span>// d, s -&gt; 64 byte aligned</span>
  <span>// n -&gt; multiple of 64</span>

  <span>auto</span> <span>*</span><span>dVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>__m256i</span> <span>*&gt;</span><span>(</span><span>d</span><span>);</span>
  <span>const</span> <span>auto</span> <span>*</span><span>sVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>const</span> <span>__m256i</span> <span>*&gt;</span><span>(</span><span>s</span><span>);</span>
  <span>size_t</span> <span>nVec</span> <span>=</span> <span>n</span> <span>/</span> <span>sizeof</span><span>(</span><span>__m256i</span><span>);</span>
  <span>for</span> <span>(;</span> <span>nVec</span> <span>&gt;</span> <span>2</span><span>;</span> <span>nVec</span> <span>-=</span> <span>2</span><span>,</span> <span>sVec</span> <span>+=</span> <span>2</span><span>,</span> <span>dVec</span> <span>+=</span> <span>2</span><span>)</span> <span>{</span>
    <span>// prefetch the next iteration's data</span>
    <span>// by default _mm_prefetch moves the entire cache-lint (64b)</span>
    <span>_mm_prefetch</span><span>(</span><span>sVec</span> <span>+</span> <span>2</span><span>,</span> <span>_MM_HINT_T0</span><span>);</span>

    <span>_mm256_stream_si256</span><span>(</span><span>dVec</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span><span>));</span>
    <span>_mm256_stream_si256</span><span>(</span><span>dVec</span> <span>+</span> <span>1</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span> <span>+</span> <span>1</span><span>));</span>
  <span>}</span>
  <span>_mm256_stream_si256</span><span>(</span><span>dVec</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span><span>));</span>
  <span>_mm256_stream_si256</span><span>(</span><span>dVec</span> <span>+</span> <span>1</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span> <span>+</span> <span>1</span><span>));</span>
  <span>_mm_sfence</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>The load from source pointer to register should <strong>not</strong> skip the cache since
that data is explicitly prefetched into the cache, non-stream
<code>_mm256_load_si256</code> must be used instead.</p>

<p>This also unrolls the loop for 2 copies at a time instead of a single copy.
This is to guarantee that each loop iteration’s prefetch coincides the copy.
Prefetch the next 64-bytes and copy the current 64-bytes.</p>

<hr>

<h2 id="alternate-avenues">Alternate avenues</h2>

<h3 id="unrolling">Unrolling</h3>

<p>In the previous section, most of the changes were in the actual underlying
load, store instructions used. Another avenue of exploration is to unroll the
loop for a certain number of iterations. This reduces the number of branch
statements by the factor of unrolling.</p>

<p>In the <code>glibc</code> implementation the unrolling factor is 4 which is what I’ll use
as well. A very simple way to implement this is to increase the alignment 
required by 4x and treat each loop as 4 instructions that copy 4x data.</p>

<p>A more complicated version would be trying to implement an unrolled loop
without increasing alignment size. We’ll need to copy using a regular fully
rolled loop till we hit a pointer location that is aligned to the size expected
by our unrolled loop.</p>

<p>Unrolling the aligned AVX copy:</p>

<div><pre><code><span>void</span> <span>_avx_cpy_unroll</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>)</span> <span>{</span>
  <span>// d, s -&gt; 128 byte aligned</span>
  <span>// n -&gt; multiple of 128</span>

  <span>auto</span> <span>*</span><span>dVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>__m256i</span> <span>*&gt;</span><span>(</span><span>d</span><span>);</span>
  <span>const</span> <span>auto</span> <span>*</span><span>sVec</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>const</span> <span>__m256i</span> <span>*&gt;</span><span>(</span><span>s</span><span>);</span>
  <span>size_t</span> <span>nVec</span> <span>=</span> <span>n</span> <span>/</span> <span>sizeof</span><span>(</span><span>__m256i</span><span>);</span>
  <span>for</span> <span>(;</span> <span>nVec</span> <span>&gt;</span> <span>0</span><span>;</span> <span>nVec</span> <span>-=</span> <span>4</span><span>,</span> <span>sVec</span> <span>+=</span> <span>4</span><span>,</span> <span>dVec</span> <span>+=</span> <span>4</span><span>)</span> <span>{</span>
    <span>_mm256_store_si256</span><span>(</span><span>dVec</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span><span>));</span>
    <span>_mm256_store_si256</span><span>(</span><span>dVec</span> <span>+</span> <span>1</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span> <span>+</span> <span>1</span><span>));</span>
    <span>_mm256_store_si256</span><span>(</span><span>dVec</span> <span>+</span> <span>2</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span> <span>+</span> <span>2</span><span>));</span>
    <span>_mm256_store_si256</span><span>(</span><span>dVec</span> <span>+</span> <span>3</span><span>,</span> <span>_mm256_load_si256</span><span>(</span><span>sVec</span> <span>+</span> <span>3</span><span>));</span>
  <span>}</span>
<span>}</span>
</code></pre></div>

<h3 id="multithreading">Multithreading</h3>

<p>The operation of copying data is super easy to parallelize across multiple
threads. The total data to be transferred can be segmented into (almost)
equal chunks, and then copied over using one of the above methods. This will
make the copy super-fast especially if the CPU has a large core count.</p>

<hr>

<h2 id="shadesmar-api">Shadesmar API</h2>

<p>To make it easy to integrate custom memory copying logic into the library,
I introduced the concept of <code>Copier</code> in <a href="https://github.com/Squadrick/shadesmar/commit/22dc762ca658d1396f3c00366e80e4f695189df9">this commit</a>.
For a new copying algorithm, an abstract class <code>Copier</code> must be implemented.</p>

<p>Here’s the definition of <code>Copier</code>:</p>

<div><pre><code><span>class</span> <span>Copier</span> <span>{</span>
 <span>public:</span>
  <span>virtual</span> <span>void</span> <span>*</span><span>alloc</span><span>(</span><span>size_t</span><span>)</span> <span>=</span> <span>0</span><span>;</span>
  <span>virtual</span> <span>void</span> <span>dealloc</span><span>(</span><span>void</span> <span>*</span><span>)</span> <span>=</span> <span>0</span><span>;</span>
  <span>virtual</span> <span>void</span> <span>shm_to_user</span><span>(</span><span>void</span> <span>*</span><span>,</span> <span>void</span> <span>*</span><span>,</span> <span>size_t</span><span>)</span> <span>=</span> <span>0</span><span>;</span>
  <span>virtual</span> <span>void</span> <span>user_to_shm</span><span>(</span><span>void</span> <span>*</span><span>,</span> <span>void</span> <span>*</span><span>,</span> <span>size_t</span><span>)</span> <span>=</span> <span>0</span><span>;</span>
<span>};</span>
</code></pre></div>

<p>The original reason for introducing this construct was to allow cross-device
usage, where a custom copier would be implemented to tranfer between CPU and
GPU. E.g.: using <code>cudaMemcpy</code> for Nvidia GPUs.</p>

<p>For a single device use case the implementation of <code>shm_to_user</code> and 
<code>user_to_shm</code> are identical. The implementation of a copier that uses
<code>std::memcpy</code>:</p>

<div><pre><code><span>class</span> <span>DefaultCopier</span> <span>:</span> <span>public</span> <span>Copier</span> <span>{</span>
 <span>public:</span>
  <span>void</span> <span>*</span><span>alloc</span><span>(</span><span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span> <span>return</span> <span>malloc</span><span>(</span><span>size</span><span>);</span> <span>}</span>

  <span>void</span> <span>dealloc</span><span>(</span><span>void</span> <span>*</span><span>ptr</span><span>)</span> <span>override</span> <span>{</span> <span>free</span><span>(</span><span>ptr</span><span>);</span> <span>}</span>

  <span>void</span> <span>shm_to_user</span><span>(</span><span>void</span> <span>*</span><span>dst</span><span>,</span> <span>void</span> <span>*</span><span>src</span><span>,</span> <span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span>
    <span>std</span><span>::</span><span>memcpy</span><span>(</span><span>dst</span><span>,</span> <span>src</span><span>,</span> <span>size</span><span>);</span>
  <span>}</span>

  <span>void</span> <span>user_to_shm</span><span>(</span><span>void</span> <span>*</span><span>dst</span><span>,</span> <span>void</span> <span>*</span><span>src</span><span>,</span> <span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span>
    <span>std</span><span>::</span><span>memcpy</span><span>(</span><span>dst</span><span>,</span> <span>src</span><span>,</span> <span>size</span><span>);</span>
  <span>}</span>
<span>};</span>
</code></pre></div>

<p>I also created an adapter <code>MTCopier</code> that adds multithreading support to other
copiers:</p>

<div><pre><code><span>template</span> <span>&lt;</span><span>class</span> <span>BaseCopierT</span><span>&gt;</span> 
<span>class</span> <span>MTCopier</span> <span>:</span> <span>public</span> <span>Copier</span> <span>{</span>
<span>public:</span>
  <span>explicit</span> <span>MTCopier</span><span>(</span><span>uint32_t</span> <span>threads</span> <span>=</span> <span>std</span><span>::</span><span>thread</span><span>::</span><span>hardware_concurrency</span><span>())</span>
      <span>:</span> <span>base_copier</span><span>(</span><span>base_copier</span><span>),</span> <span>nthreads</span><span>(</span><span>threads</span><span>)</span> <span>{}</span>

  <span>void</span> <span>*</span><span>alloc</span><span>(</span><span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span> <span>return</span> <span>base_copier</span><span>.</span><span>alloc</span><span>(</span><span>size</span><span>);</span> <span>}</span>

  <span>void</span> <span>dealloc</span><span>(</span><span>void</span> <span>*</span><span>ptr</span><span>)</span> <span>override</span> <span>{</span> <span>base_copier</span><span>.</span><span>dealloc</span><span>(</span><span>ptr</span><span>);</span> <span>}</span>

  <span>void</span> <span>_copy</span><span>(</span><span>void</span> <span>*</span><span>d</span><span>,</span> <span>void</span> <span>*</span><span>s</span><span>,</span> <span>size_t</span> <span>n</span><span>,</span> <span>bool</span> <span>shm_to_user</span><span>)</span> <span>{</span>
    <span>std</span><span>::</span><span>vector</span><span>&lt;</span><span>std</span><span>::</span><span>thread</span><span>&gt;</span> <span>threads</span><span>;</span>
    <span>threads</span><span>.</span><span>reserve</span><span>(</span><span>nthreads</span><span>);</span>

    <span>ldiv_t</span> <span>per_worker</span> <span>=</span> <span>div</span><span>((</span><span>int64_t</span><span>)</span><span>n</span><span>,</span> <span>nthreads</span><span>);</span>

    <span>size_t</span> <span>next_start</span> <span>=</span> <span>0</span><span>;</span>
    <span>for</span> <span>(</span><span>uint32_t</span> <span>thread_idx</span> <span>=</span> <span>0</span><span>;</span> <span>thread_idx</span> <span>&lt;</span> <span>nthreads</span><span>;</span> <span>++</span><span>thread_idx</span><span>)</span> <span>{</span>
      <span>const</span> <span>size_t</span> <span>curr_start</span> <span>=</span> <span>next_start</span><span>;</span>
      <span>next_start</span> <span>+=</span> <span>per_worker</span><span>.</span><span>quot</span><span>;</span>
      <span>if</span> <span>(</span><span>thread_idx</span> <span>&lt;</span> <span>per_worker</span><span>.</span><span>rem</span><span>)</span> <span>{</span>
        <span>++</span><span>next_start</span><span>;</span>
      <span>}</span>
      <span>uint8_t</span> <span>*</span><span>d_thread</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>uint8_t</span> <span>*&gt;</span><span>(</span><span>d</span><span>)</span> <span>+</span> <span>curr_start</span><span>;</span>
      <span>uint8_t</span> <span>*</span><span>s_thread</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>uint8_t</span> <span>*&gt;</span><span>(</span><span>s</span><span>)</span> <span>+</span> <span>curr_start</span><span>;</span>

      <span>if</span> <span>(</span><span>shm_to_user</span><span>)</span> <span>{</span>
        <span>threads</span><span>.</span><span>emplace_back</span><span>(</span><span>&amp;</span><span>Copier</span><span>::</span><span>shm_to_user</span><span>,</span> <span>&amp;</span><span>base_copier</span><span>,</span> <span>d_thread</span><span>,</span>
                             <span>s_thread</span><span>,</span> <span>next_start</span> <span>-</span> <span>curr_start</span><span>);</span>
      <span>}</span> <span>else</span> <span>{</span>
        <span>threads</span><span>.</span><span>emplace_back</span><span>(</span><span>&amp;</span><span>Copier</span><span>::</span><span>user_to_shm</span><span>,</span> <span>&amp;</span><span>base_copier</span><span>,</span> <span>d_thread</span><span>,</span>
                             <span>s_thread</span><span>,</span> <span>next_start</span> <span>-</span> <span>curr_start</span><span>);</span>
      <span>}</span>
    <span>}</span>
    <span>for</span> <span>(</span><span>auto</span> <span>&amp;</span><span>thread</span> <span>:</span> <span>threads</span><span>)</span> <span>{</span>
      <span>thread</span><span>.</span><span>join</span><span>();</span>
    <span>}</span>
    <span>threads</span><span>.</span><span>clear</span><span>();</span>
  <span>}</span>

  <span>void</span> <span>shm_to_user</span><span>(</span><span>void</span> <span>*</span><span>dst</span><span>,</span> <span>void</span> <span>*</span><span>src</span><span>,</span> <span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span>
    <span>_copy</span><span>(</span><span>dst</span><span>,</span> <span>src</span><span>,</span> <span>size</span><span>,</span> <span>true</span><span>);</span>
  <span>}</span>

  <span>void</span> <span>user_to_shm</span><span>(</span><span>void</span> <span>*</span><span>dst</span><span>,</span> <span>void</span> <span>*</span><span>src</span><span>,</span> <span>size_t</span> <span>size</span><span>)</span> <span>override</span> <span>{</span>
    <span>_copy</span><span>(</span><span>dst</span><span>,</span> <span>src</span><span>,</span> <span>size</span><span>,</span> <span>false</span><span>);</span>
  <span>}</span>

<span>private:</span>
  <span>BaseCopierT</span> <span>base_copier</span><span>;</span>
  <span>uint32_t</span> <span>nthreads</span><span>;</span>
<span>};</span>
</code></pre></div>

<p>Currently this only works for <code>memcpy</code> and <code>_rep_movsb</code> since the
implementation expects the memory copy to work for unaligned memory.</p>

<hr>

<h2 id="benchmark">Benchmark</h2>

<p>I used Google’s <a href="https://github.com/google/benchmark">Benchmark</a> for timing
the performance of copying data ranging from size of 32kB to 64MB. All the
benchmarks were run on my PC with the following specifications:</p>
<ol>
  <li>AMD Ryzen 7 3700X</li>
  <li>2x8GB DDR4 RAM @ 3600Mhz</li>
</ol>










<h3 id="conclusion">Conclusion</h3>

<p>Stick to <code>std::memcpy</code>. It delivers great performance while also adapting to
the hardware architecture, and makes no assumptions about the memory alignment.</p>

<p>If performance truly matters, then you might want to consider using a more
specific non-genetic implementation with alignment requirements. The streaming
prefetching copy works the best for larger copies (&gt;1MB), but the performance
for small sizes is abyssal, but <code>memcpy</code> matches its performance. For small to
medium sizes Unrolled AVX absolutely dominates, but as for larger messages, it
is slower than the streaming alternatives. The regular <code>RepMovsb</code> is by far the
worst overall performer as excepted.</p>

<p>Unrolling definitely improves performance in most cases by about 5-10%. The
only case where the unrolled version is slower than rolled version is for
<code>AvxCopier</code> with data size of 32B, which the unrolled version is 25% slower.
The rolled version will do a single AVX-256 load/store and a conditional check.
The unrolled version will do 4 AVX-256 load/stores and a conditional check.</p>

<h3 id="code">Code</h3>

<p>Code for all the methods is included in the library conforming to the above
mentioned API. To actively warn about the danger of using these custom copiers
I have named this file <a href="https://github.com/Squadrick/shadesmar/blob/master/include/shadesmar/memory/dragons.h"><code>dragons.h</code></a>,
with an apt message: <em>Here be dragons</em>.</p>


<p><span>
  Written on
  
  May
  24th,
  2020
  by
  
    Dheeraj R Reddy
  
</span>

    </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vanishing from Hyundai’s data network (400 pts)]]></title>
            <link>http://techno-fandom.org/~hobbit/cars/ev/offnet.html</link>
            <guid>44860139</guid>
            <pubDate>Mon, 11 Aug 2025 01:55:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://techno-fandom.org/~hobbit/cars/ev/offnet.html">http://techno-fandom.org/~hobbit/cars/ev/offnet.html</a>, See on <a href="https://news.ycombinator.com/item?id=44860139">Hacker News</a></p>
<div id="readability-page-1" class="page"><div width="85%">
<tbody><tr><td>&nbsp;</td><td>
The Yuppie Button page talks about making lots of light.&nbsp;
Now I needed to do the opposite, by "going dark" -- to vanish completely
from Hyundai's data network, and avoid having the car being tracked
or actively interfered with outside of my control.&nbsp;
See, this is one of the showstopping problems I have with Tesla -- they
*insist* that you have your car online all the time, talking to Tesla's
cloud and sending telematic data.&nbsp;
Thank you, NO.&nbsp;
The <a href="https://www.hyundaiusa.com/bluelink/index.aspx">
range of things</a>
that Hyundai's BlueLink setup is able to
do remotely to someone's car given only a VIN is totally scary.&nbsp;
Not only did I want no parts of that, we all have every right to not
participate in that nonsense if we so choose.
<p>

As a first step I refused to let the dealer sign me for BlueLink, telling
them that I could handle signup later myself if I wanted to.&nbsp;
But I knew there was more to it, since the car as it came was still able
to make a cellular data connection and send information about itself.&nbsp;
The obvious question was to find and disable the cellular communication
facility, or "telematics unit" as it is implemented in many vehicles.
</p></td></tr></tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla remotely deactivates rapper's vehicle for singing about the Cybertruck (219 pts)]]></title>
            <link>https://www.threads.com/@brittainforsenate/post/DNMcEZ9yOxk</link>
            <guid>44859807</guid>
            <pubDate>Mon, 11 Aug 2025 00:56:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.threads.com/@brittainforsenate/post/DNMcEZ9yOxk">https://www.threads.com/@brittainforsenate/post/DNMcEZ9yOxk</a>, See on <a href="https://news.ycombinator.com/item?id=44859807">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Compiling a Lisp: Lambda lifting (152 pts)]]></title>
            <link>https://bernsteinbear.com/blog/compiling-a-lisp-12/</link>
            <guid>44858892</guid>
            <pubDate>Sun, 10 Aug 2025 22:35:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bernsteinbear.com/blog/compiling-a-lisp-12/">https://bernsteinbear.com/blog/compiling-a-lisp-12/</a>, See on <a href="https://news.ycombinator.com/item?id=44858892">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p><span data-nosnippet="">
<em><a href="https://bernsteinbear.com/blog/compiling-a-lisp-0/">first</a></em> – <em><a href="https://bernsteinbear.com/blog/compiling-a-lisp-11/">previous</a></em>
</span></p>

<p><em>EDIT: /u/thunderseethe correctly points out that this is closure conversion,
not lambda lifting, so I have adjusted the post title from “lambda lifting” to
“closure conversion” accordingly. Thanks!</em></p>

<p>I didn’t think this day would come, but I picked up the <a href="https://bernsteinbear.com/assets/img/11-ghuloum.pdf">Ghuloum
tutorial</a> (PDF) again and I got a little bit further. There’s just
one caveat: I have rewritten the implementation in Python. It’s available in
the <a href="https://github.com/tekknolagi/ghuloum">same repo</a> in
<a href="https://github.com/tekknolagi/ghuloum/blob/trunk/compiler.py">compiler.py</a>.
It’s brief, coming in at a little over 300 LOC + tests (compared to the C
version’s 1200 LOC + tests).</p>

<p>I guess there’s another caveat, too, which is that the Python version has no
S-expression reader. But that’s fine: consider it an exercise for you, dear
reader. That’s hardly the most interesting part of the tutorial.</p>

<p>Oh, and I also dropped the instruction encoding. I’m doing text assembly now.
Womp womp.</p>

<p>Anyway, converting the lambdas as required in the paper requires three things:</p>

<ul>
  <li>Keeping track of which variables are bound</li>
  <li>Keeping track of which variables are free in a given lambda</li>
  <li>Keeping a running list of <code>code</code> objects that we create as we recurse</li>
</ul>

<p>We have two forms that can bind variables: <code>let</code> and <code>lambda</code>. This means that
we need to recognize the names in those special expressions and modify the
environment. What environment, you ask?</p>

<h3 id="the-closure-converter">The closure converter</h3>

<p>Well, I have this little <code>LambdaConverter</code> class.</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>labels</span><span>:</span> <span>dict</span><span>[</span><span>str</span><span>,</span> <span>list</span><span>]</span> <span>=</span> <span>{}</span>

    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>:</span> <span>set</span><span>[</span><span>str</span><span>],</span> <span>free</span><span>:</span> <span>set</span><span>[</span><span>str</span><span>]):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span>case</span> <span>_</span><span>:</span>
                <span>raise</span> <span>NotImplementedError</span><span>(</span><span>expr</span><span>)</span>

<span>def</span> <span>convert_lambdas</span><span>(</span><span>expr</span><span>):</span>
    <span>conv</span> <span>=</span> <span>LambdaConverter</span><span>()</span>
    <span>expr</span> <span>=</span> <span>conv</span><span>.</span><span>convert</span><span>(</span><span>expr</span><span>,</span> <span>set</span><span>(),</span> <span>set</span><span>())</span>
    <span>labels</span> <span>=</span> <span>[[</span><span>name</span><span>,</span> <span>code</span><span>]</span> <span>for</span> <span>name</span><span>,</span> <span>code</span> <span>in</span> <span>conv</span><span>.</span><span>labels</span><span>.</span><span>items</span><span>()]</span>
    <span>return</span> <span>[</span><span>"labels"</span><span>,</span> <span>labels</span><span>,</span> <span>expr</span><span>]</span>
</code></pre></div>

<p>We keep the same <code>labels</code> dict for the entire recursive traversal of the
program, but we modify <code>bound</code> at each binding site and <code>free</code> only at lambdas.</p>

<p>To illustrate how they are used, let’s fill in some sample expressions: <code>3</code>,
<code>'a</code>, and <code>#t</code>:</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span>case</span> <span>int</span><span>(</span><span>_</span><span>)</span> <span>|</span> <span>Char</span><span>():</span>  <span># bool(_) is implied by int(_)
</span>                <span>return</span> <span>expr</span>
            <span># ...
</span>
<span>class</span> <span>LambdaTests</span><span>(</span><span>unittest</span><span>.</span><span>TestCase</span><span>):</span>
    <span>def</span> <span>test_int</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>(</span><span>3</span><span>),</span> <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>3</span><span>])</span>

    <span>def</span> <span>test_bool</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>(</span><span>True</span><span>),</span> <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>True</span><span>])</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>(</span><span>False</span><span>),</span> <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>False</span><span>])</span>

    <span>def</span> <span>test_char</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>(</span><span>Char</span><span>(</span><span>"a"</span><span>)),</span> <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>Char</span><span>(</span><span>"a"</span><span>)])</span>
</code></pre></div>

<p>Well, okay, sure, we don’t actually need to think about variable names when we
are dealing with simple constants.</p>

<p>So let’s look at variables:</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span># ...
</span>            <span>case</span> <span>str</span><span>(</span><span>_</span><span>)</span> <span>if</span> <span>expr</span> <span>in</span> <span>bound</span><span>:</span>
                <span>return</span> <span>expr</span>
            <span>case</span> <span>str</span><span>(</span><span>_</span><span>):</span>
                <span>free</span><span>.</span><span>add</span><span>(</span><span>expr</span><span>)</span>
                <span>return</span> <span>expr</span>
            <span># ...
</span>
<span>class</span> <span>LambdaTests</span><span>(</span><span>unittest</span><span>.</span><span>TestCase</span><span>):</span>
    <span># ...
</span>    <span>def</span> <span>test_freevar</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>(</span><span>"x"</span><span>),</span> <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>"x"</span><span>])</span>
</code></pre></div>

<p>We don’t want to actually transform the variable uses, just add some metadata
about their uses. If we have some variable <code>x</code> bound by a <code>let</code> or a <code>lambda</code>
expression, we can leave it alone. Otherwise, we need to mark it.</p>

<div><pre><code><span>(</span><span>let</span> <span>((</span><span>x</span> <span>5</span><span>))</span>
  <span>(</span><span>+</span> <span>x</span>        <span>; bound</span>
     <span>y</span><span>))</span>      <span>; free</span>
</code></pre></div>

<p>There’s one irritating special case here which is that we don’t want to
consider <code>+</code> (for example) as a free variable: it is a special language
primitive. So we consider <code>+</code> and the others as always bound.</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span># ...
</span>            <span>case</span> <span>str</span><span>(</span><span>_</span><span>)</span> <span>if</span> <span>expr</span> <span>in</span> <span>BUILTINS</span><span>:</span>
                <span>return</span> <span>expr</span>
            <span># ...
</span>
<span>class</span> <span>LambdaTests</span><span>(</span><span>unittest</span><span>.</span><span>TestCase</span><span>):</span>
    <span># ...
</span>    <span>def</span> <span>test_plus</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>(</span><span>"+"</span><span>),</span> <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>"+"</span><span>])</span>
</code></pre></div>

<p>Armed with this knowledge, we can do our first recursive traversal: <code>if</code>
expressions. Since they have recursive parts and don’t bind any variables, they
are the second-simplest form for this converter.</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span># ...
</span>            <span>case</span> <span>[</span><span>"if"</span><span>,</span> <span>test</span><span>,</span> <span>conseq</span><span>,</span> <span>alt</span><span>]:</span>
                <span>return</span> <span>[</span><span>"if"</span><span>,</span>
                        <span>self</span><span>.</span><span>convert</span><span>(</span><span>test</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>),</span>
                        <span>self</span><span>.</span><span>convert</span><span>(</span><span>conseq</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>),</span>
                        <span>self</span><span>.</span><span>convert</span><span>(</span><span>alt</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>)]</span>
            <span># ...
</span>
<span>class</span> <span>LambdaTests</span><span>(</span><span>unittest</span><span>.</span><span>TestCase</span><span>):</span>
    <span># ...
</span>    <span>def</span> <span>test_if</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>([</span><span>"if"</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>]),</span>
                         <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>[</span><span>"if"</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>]])</span>
</code></pre></div>

<p>This test doesn’t tell us much yet (other than adding an empty <code>labels</code> and not
raising an exception). But it will soon.</p>

<h3 id="lambda">Lambda</h3>

<p>Let’s think about what <code>lambda</code> does. It’s a bunch of features in a trench
coat:</p>

<ul>
  <li>bind names</li>
  <li>allocate code</li>
  <li>capture outside environment</li>
</ul>

<p>To handle the closure conversion, we have to reason about all three.</p>

<p>First, the lambda binds its parameters as new names. In fact, those are the
<em>only</em> bound variables in a lambda. Consider:</p>



<p><code>x</code> is a free variable in that lambda! We’ll want to transform that lambda
into:</p>

<div><pre><code><span>;                  +-parameters</span>
<span>;                  |  +-freevars</span>
<span>;                  v  v</span>
<span>(</span><span>labels</span> <span>((</span><span>f0</span> <span>(</span><span>code</span> <span>()</span> <span>(</span><span>x</span><span>)</span> <span>x</span><span>)))</span>
  <span>(</span><span>closure</span> <span>f0</span> <span>x</span><span>))</span>
</code></pre></div>

<p>Even if <code>x</code> were bound by some <code>let</code> outside the lambda, it would be free in
the lambda:</p>

<div><pre><code><span>(</span><span>let</span> <span>((</span><span>x</span> <span>5</span><span>))</span>
  <span>(</span><span>lambda</span> <span>()</span> <span>x</span><span>))</span>
</code></pre></div>

<p>That means we don’t thread through the <code>bound</code> parameter to the lambda body; we
don’t care what names are bound <em>outside</em> the lambda.</p>

<p>We also want to keep track of the set of variables that are free inside the
lambda: we’ll need them to create a <code>code</code> form. Therefore, we also pass in a
new set for the lambda body’s <code>free</code> set.</p>

<p>So far, all of this environment wrangling gives us:</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span># ...
</span>            <span>case</span> <span>[</span><span>"lambda"</span><span>,</span> <span>params</span><span>,</span> <span>body</span><span>]:</span>
                <span>body_free</span> <span>=</span> <span>set</span><span>()</span>
                <span>body</span> <span>=</span> <span>self</span><span>.</span><span>convert</span><span>(</span><span>body</span><span>,</span> <span>set</span><span>(</span><span>params</span><span>),</span> <span>body_free</span><span>)</span>
                <span>free</span><span>.</span><span>update</span><span>(</span><span>body_free</span> <span>-</span> <span>bound</span><span>)</span>
                <span># ...
</span>                <span>return</span> <span># ???
</span>            <span># ...
</span></code></pre></div>

<p>There’s also <code>free.update(body_free - bound)</code> in there because any variable
free in a lambda expression is also free in the current expression—well,
except for the variables that are currently bound.</p>

<p>Last, we’ll make a <code>code</code> form and a <code>closure</code> form. The <code>code</code> gets appended
to the global list with a new label and the label gets threaded through to the
<code>closure</code>.</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span>def</span> <span>push_label</span><span>(</span><span>self</span><span>,</span> <span>params</span><span>,</span> <span>freevars</span><span>,</span> <span>body</span><span>):</span>
        <span>result</span> <span>=</span> <span>f</span><span>"f</span><span>{</span><span>len</span><span>(</span><span>self</span><span>.</span><span>labels</span><span>)</span><span>}</span><span>"</span>
        <span>self</span><span>.</span><span>labels</span><span>[</span><span>result</span><span>]</span> <span>=</span> <span>[</span><span>"code"</span><span>,</span> <span>params</span><span>,</span> <span>freevars</span><span>,</span> <span>body</span><span>]</span>
        <span>return</span> <span>result</span>

    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span># ...
</span>            <span>case</span> <span>[</span><span>"lambda"</span><span>,</span> <span>params</span><span>,</span> <span>body</span><span>]:</span>
                <span>body_free</span> <span>=</span> <span>set</span><span>()</span>
                <span>body</span> <span>=</span> <span>self</span><span>.</span><span>convert</span><span>(</span><span>body</span><span>,</span> <span>set</span><span>(</span><span>params</span><span>),</span> <span>body_free</span><span>)</span>
                <span>free</span><span>.</span><span>update</span><span>(</span><span>body_free</span> <span>-</span> <span>bound</span><span>)</span>
                <span># vvvv new below this line vvvv
</span>                <span>body_free</span> <span>=</span> <span>sorted</span><span>(</span><span>body_free</span><span>)</span>
                <span>label</span> <span>=</span> <span>self</span><span>.</span><span>push_label</span><span>(</span><span>params</span><span>,</span> <span>body_free</span><span>,</span> <span>body</span><span>)</span>
                <span>return</span> <span>[</span><span>"closure"</span><span>,</span> <span>label</span><span>,</span> <span>*</span><span>body_free</span><span>]</span>
            <span># ...
</span></code></pre></div>

<p>This is finicky! I think my first couple of versions were subtly wrong for
different reasons. Tests help a lot here. For every place in the code where I
mess with <code>bound</code> or <code>free</code> in a recursive call, I tried to have a test that
would fail if I got it wrong.</p>

<div><pre><code><span>class</span> <span>LambdaTests</span><span>(</span><span>unittest</span><span>.</span><span>TestCase</span><span>):</span>
    <span># ...
</span>    <span>def</span> <span>test_lambda_no_params_no_freevars</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>([</span><span>"lambda"</span><span>,</span> <span>[],</span> <span>3</span><span>]),</span>
                         <span>[</span><span>"labels"</span><span>,</span> <span>[</span>
                             <span>[</span><span>"f0"</span><span>,</span> <span>[</span><span>"code"</span><span>,</span> <span>[],</span> <span>[],</span> <span>3</span><span>]],</span>
                         <span>],</span> <span>[</span><span>"closure"</span><span>,</span> <span>"f0"</span><span>]])</span>

    <span>def</span> <span>test_nested_lambda</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>([</span><span>"lambda"</span><span>,</span> <span>[</span><span>"x"</span><span>],</span>
                                       <span>[</span><span>"lambda"</span><span>,</span> <span>[</span><span>"y"</span><span>],</span>
                                        <span>[</span><span>"+"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]]),</span>
                         <span>[</span><span>"labels"</span><span>,</span>
                          <span>[[</span><span>"f0"</span><span>,</span> <span>[</span><span>"code"</span><span>,</span> <span>[</span><span>"y"</span><span>],</span> <span>[</span><span>"x"</span><span>],</span> <span>[</span><span>"+"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]],</span>
                           <span>[</span><span>"f1"</span><span>,</span> <span>[</span><span>"code"</span><span>,</span> <span>[</span><span>"x"</span><span>],</span> <span>[],</span> <span>[</span><span>"closure"</span><span>,</span> <span>"f0"</span><span>,</span> <span>"x"</span><span>]]]],</span>
                          <span>[</span><span>"closure"</span><span>,</span> <span>"f1"</span><span>]])</span>
    <span># ... and many more, especially interacting with `let`
</span></code></pre></div>

<p>Now let’s talk about the other binder.</p>

<h3 id="let">Let</h3>

<p>Let’s think about what <code>let</code> does by examining a confusing let expression:</p>

<div><pre><code><span>(</span><span>let</span> <span>((</span><span>wolf</span> <span>5</span><span>)</span>
      <span>(</span><span>x</span> <span>wolf</span><span>))</span>
  <span>wolf</span><span>)</span>
</code></pre></div>

<p>In this expression, there are two <code>wolf</code>s. One of them is bound inside the let,
but the other is free inside the let! This is because <code>let</code> evaluates all of
its bindings without access to the bindings as they are being built up (for
that, we would need <code>let*</code>).</p>

<div><pre><code><span>(</span><span>let</span> <span>((</span><span>wolf</span> <span>5</span><span>)</span>   <span>; new binding  &lt;-------------+</span>
      <span>(</span><span>x</span> <span>wolf</span><span>))</span>  <span>; some other variable; free! |</span>
  <span>wolf</span><span>)</span>          <span>; bound to ------------------+</span>
</code></pre></div>

<p>So this must mean that:</p>

<ul>
  <li>we need to convert all of the bindings using the original <code>bound</code> and <code>free</code>,
then</li>
  <li>only for the let body, add the new bindings (and use the original <code>free</code>)</li>
</ul>

<p>Which gives us, in code:</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span># ...
</span>            <span>case</span> <span>[</span><span>"let"</span><span>,</span> <span>bindings</span><span>,</span> <span>body</span><span>]:</span>
                <span>new_bindings</span> <span>=</span> <span>[]</span>
                <span>for</span> <span>name</span><span>,</span> <span>val_expr</span> <span>in</span> <span>bindings</span><span>:</span>
                    <span>new_bindings</span><span>.</span><span>append</span><span>([</span><span>name</span><span>,</span> <span>self</span><span>.</span><span>convert</span><span>(</span><span>val_expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>)])</span>
                <span>names</span> <span>=</span> <span>{</span><span>name</span> <span>for</span> <span>name</span><span>,</span> <span>_</span> <span>in</span> <span>bindings</span><span>}</span>
                <span>new_body</span> <span>=</span> <span>self</span><span>.</span><span>convert</span><span>(</span><span>body</span><span>,</span> <span>bound</span> <span>|</span> <span>names</span><span>,</span> <span>free</span><span>)</span>
                <span>return</span> <span>[</span><span>"let"</span><span>,</span> <span>new_bindings</span><span>,</span> <span>new_body</span><span>]</span>
            <span># ...
</span>
<span>class</span> <span>LambdaTests</span><span>(</span><span>unittest</span><span>.</span><span>TestCase</span><span>):</span>
    <span># ...
</span>    <span>def</span> <span>test_let</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>([</span><span>"let"</span><span>,</span> <span>[[</span><span>"x"</span><span>,</span> <span>5</span><span>]],</span> <span>"x"</span><span>]),</span>
                         <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>[</span><span>"let"</span><span>,</span> <span>[[</span><span>"x"</span><span>,</span> <span>5</span><span>]],</span> <span>"x"</span><span>]])</span>

    <span>def</span> <span>test_let_lambda</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>([</span><span>"let"</span><span>,</span> <span>[[</span><span>"x"</span><span>,</span> <span>5</span><span>]],</span>
                                       <span>[</span><span>"lambda"</span><span>,</span> <span>[</span><span>"y"</span><span>],</span>
                                        <span>[</span><span>"+"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]]),</span>
                         <span>[</span><span>"labels"</span><span>,</span>
                          <span>[[</span><span>"f0"</span><span>,</span> <span>[</span><span>"code"</span><span>,</span> <span>[</span><span>"y"</span><span>],</span> <span>[</span><span>"x"</span><span>],</span> <span>[</span><span>"+"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]]],</span>
                          <span>[</span><span>"let"</span><span>,</span> <span>[[</span><span>"x"</span><span>,</span> <span>5</span><span>]],</span> <span>[</span><span>"closure"</span><span>,</span> <span>"f0"</span><span>,</span> <span>"x"</span><span>]]])</span>

    <span>def</span> <span>test_let_inside_lambda</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>([</span><span>"lambda"</span><span>,</span> <span>[</span><span>"x"</span><span>],</span>
                                       <span>[</span><span>"let"</span><span>,</span> <span>[[</span><span>"y"</span><span>,</span> <span>6</span><span>]],</span>
                                        <span>[</span><span>"+"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]]),</span>
                         <span>[</span><span>"labels"</span><span>,</span>
                          <span>[[</span><span>"f0"</span><span>,</span> <span>[</span><span>"code"</span><span>,</span> <span>[</span><span>"x"</span><span>],</span> <span>[],</span>
                                   <span>[</span><span>"let"</span><span>,</span> <span>[[</span><span>"y"</span><span>,</span> <span>6</span><span>]],</span>
                                    <span>[</span><span>"+"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]]]],</span>
                          <span>[</span><span>"closure"</span><span>,</span> <span>"f0"</span><span>]])</span>

    <span>def</span> <span>test_paper_example</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>([</span><span>"let"</span><span>,</span> <span>[[</span><span>"x"</span><span>,</span> <span>5</span><span>]],</span>
                                         <span>[</span><span>"lambda"</span><span>,</span> <span>[</span><span>"y"</span><span>],</span>
                                          <span>[</span><span>"lambda"</span><span>,</span> <span>[],</span>
                                           <span>[</span><span>"+"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]]]),</span>
                         <span>[</span><span>"labels"</span><span>,</span> <span>[</span>
                             <span>[</span><span>"f0"</span><span>,</span> <span>[</span><span>"code"</span><span>,</span> <span>[],</span>
                               <span>[</span><span>"x"</span><span>,</span> <span>"y"</span><span>],</span> <span>[</span><span>"+"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]],</span>
                             <span>[</span><span>"f1"</span><span>,</span> <span>[</span><span>"code"</span><span>,</span> <span>[</span><span>"y"</span><span>],</span> <span>[</span><span>"x"</span><span>],</span>
                               <span>[</span><span>"closure"</span><span>,</span> <span>"f0"</span><span>,</span> <span>"x"</span><span>,</span> <span>"y"</span><span>]]],</span>
                           <span>],</span>
                          <span>[</span><span>"let"</span><span>,</span> <span>[[</span><span>"x"</span><span>,</span> <span>5</span><span>]],</span> <span>[</span><span>"closure"</span><span>,</span> <span>"f1"</span><span>,</span> <span>"x"</span><span>]]])</span>
    <span># ... and many more, especially interacting with `lambda`
</span></code></pre></div>

<h3 id="function-calls">Function calls</h3>

<p>Last, and somewhat boringly, we have function calls. The only thing to call out
is again handling these always-bound primitive operators like <code>+</code>, which we
don’t want to have a <code>funcall</code>:</p>

<div><pre><code><span>class</span> <span>LambdaConverter</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>convert</span><span>(</span><span>self</span><span>,</span> <span>expr</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>):</span>
        <span>match</span> <span>expr</span><span>:</span>
            <span># ...
</span>            <span>case</span> <span>[</span><span>func</span><span>,</span> <span>*</span><span>args</span><span>]:</span>
                <span>result</span> <span>=</span> <span>[]</span> <span>if</span> <span>isinstance</span><span>(</span><span>func</span><span>,</span> <span>str</span><span>)</span> <span>and</span> <span>func</span> <span>in</span> <span>BUILTINS</span> <span>else</span> <span>[</span><span>"funcall"</span><span>]</span>
                <span>for</span> <span>e</span> <span>in</span> <span>expr</span><span>:</span>
                    <span>result</span><span>.</span><span>append</span><span>(</span><span>self</span><span>.</span><span>convert</span><span>(</span><span>e</span><span>,</span> <span>bound</span><span>,</span> <span>free</span><span>))</span>
                <span>return</span> <span>result</span>
            <span># ...
</span>
<span>class</span> <span>LambdaTests</span><span>(</span><span>unittest</span><span>.</span><span>TestCase</span><span>):</span>
    <span># ...
</span>    <span>def</span> <span>test_call</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>assertEqual</span><span>(</span><span>convert_lambdas</span><span>([</span><span>"f"</span><span>,</span> <span>3</span><span>,</span> <span>4</span><span>]),</span> <span>[</span><span>"labels"</span><span>,</span> <span>[],</span> <span>[</span><span>"funcall"</span><span>,</span> <span>"f"</span><span>,</span> <span>3</span><span>,</span> <span>4</span><span>]])</span>
</code></pre></div>

<p>Now that we have these new <code>funcall</code>, and <code>closure</code> forms we have to compile
them into assembly.</p>

<h3 id="compiling-closure">Compiling <code>closure</code></h3>

<p>Compiling closure forms is very similar to allocating a string or a vector. In
the first cell, we want to put a pointer to the code that backs the closure
(this will be some label like <code>f12</code>). We can get a reference to that using
<code>lea</code>, since it will be a label in the assembly. Then we write it to the heap.</p>

<p>Then for each free variable, we go find out where it’s defined. Since we know
by construction that these are all strings, we don’t need to worry about having
weird recursion issues around keeping track of a moving heap pointer. Instead,
we know it’s always going to be an indirect from the stack or from the current
closure. Then we write that to the heap.</p>

<p>Then, since a closure is an object, we need to give it a tag. So we tag it with
<code>lea</code> because I felt cute. You could also use <code>or</code> or <code>add</code>. We store the
result in <code>rax</code> because that’s our compiler contract.</p>

<p>Last, we bump the heap pointer by the size of the closure.</p>

<div><pre><code><span>def</span> <span>compile_expr</span><span>(</span><span>expr</span><span>,</span> <span>code</span><span>,</span> <span>si</span><span>,</span> <span>env</span><span>):</span>
    <span>match</span> <span>expr</span><span>:</span>
        <span># ...
</span>        <span>case</span> <span>[</span><span>"closure"</span><span>,</span> <span>str</span><span>(</span><span>lvar</span><span>),</span> <span>*</span><span>args</span><span>]:</span>
            <span>comment</span><span>(</span><span>"Get a pointer to the label"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"lea rax, </span><span>{</span><span>lvar</span><span>}</span><span>"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"mov </span><span>{</span><span>heap_at</span><span>(</span><span>0</span><span>)</span><span>}</span><span>, rax"</span><span>)</span>
            <span>for</span> <span>idx</span><span>,</span> <span>arg</span> <span>in</span> <span>enumerate</span><span>(</span><span>args</span><span>):</span>
                <span>assert</span> <span>isinstance</span><span>(</span><span>arg</span><span>,</span> <span>str</span><span>)</span>
                <span>comment</span><span>(</span><span>f</span><span>"Load closure cell #</span><span>{</span><span>idx</span><span>}</span><span>"</span><span>)</span>
                <span># Just a variable lookup; guaranteed not to allocate
</span>                <span>compile_expr</span><span>(</span><span>arg</span><span>,</span> <span>code</span><span>,</span> <span>si</span><span>,</span> <span>env</span><span>)</span>
                <span>emit</span><span>(</span><span>f</span><span>"mov </span><span>{</span><span>heap_at</span><span>((</span><span>idx</span><span>+</span><span>1</span><span>)</span><span>*</span><span>WORD_SIZE</span><span>)</span><span>}</span><span>, rax"</span><span>)</span>
            <span>comment</span><span>(</span><span>"Tag a closure pointer"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"lea rax, </span><span>{</span><span>heap_at</span><span>(</span><span>CLOSURE_TAG</span><span>)</span><span>}</span><span>"</span><span>)</span>
            <span>comment</span><span>(</span><span>"Bump the heap pointer"</span><span>)</span>
            <span>size</span> <span>=</span> <span>align</span><span>(</span><span>WORD_SIZE</span> <span>+</span> <span>len</span><span>(</span><span>args</span><span>)</span><span>*</span><span>WORD_SIZE</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"add </span><span>{</span><span>HEAP_BASE</span><span>}</span><span>, </span><span>{</span><span>size</span><span>}</span><span>"</span><span>)</span>
        <span># ...
</span></code></pre></div>

<p>So <code>(lambda (x) x)</code> compiles to:</p>

<div><pre><code><span>.intel_syntax</span>
<span>.global</span> <span>scheme_entry</span>

<span>f0:</span>
<span>mov</span> <span>rax</span><span>,</span> <span>[</span><span>rsp</span><span>-</span><span>8</span><span>]</span>
<span>ret</span>

<span>scheme_entry:</span>
<span>#</span> <span>Get</span> <span>a</span> <span>pointer</span> <span>to</span> <span>the</span> <span>label</span>
<span>lea</span> <span>rax</span><span>,</span> <span>f0</span>
<span>mov</span> <span>[</span><span>rsi</span><span>+</span><span>0</span><span>],</span> <span>rax</span>
<span>#</span> <span>Tag</span> <span>a</span> <span>cl</span><span>osure</span> <span>pointer</span>
<span>lea</span> <span>rax</span><span>,</span> <span>[</span><span>rsi</span><span>+</span><span>6</span><span>]</span>
<span>#</span> <span>Bump</span> <span>the</span> <span>heap</span> <span>pointer</span>
<span>add</span> <span>rsi</span><span>,</span> <span>16</span>
<span>ret</span>
</code></pre></div>

<p>and if we had a closure variable, for example <code>(let ((y 5)) (lambda () y))</code>:</p>

<div><pre><code><span>.intel_syntax</span>
<span>.global</span> <span>scheme_entry</span>

<span>f0:</span>
<span>mov</span> <span>rax</span><span>,</span> <span>[</span><span>rdi</span><span>+</span><span>2</span><span>]</span>
<span>ret</span>

<span>scheme_entry:</span>
<span>#</span> <span>Code</span> <span>for</span> <span>y</span>
<span>mov</span> <span>rax</span><span>,</span> <span>20</span>
<span>#</span> <span>Store</span> <span>y</span> <span>on</span> <span>the</span> <span>stack</span>
<span>mov</span> <span>[</span><span>rsp</span><span>-</span><span>8</span><span>],</span> <span>rax</span>
<span>#</span> <span>Get</span> <span>a</span> <span>pointer</span> <span>to</span> <span>the</span> <span>label</span>
<span>lea</span> <span>rax</span><span>,</span> <span>f0</span>
<span>mov</span> <span>[</span><span>rsi</span><span>+</span><span>0</span><span>],</span> <span>rax</span>
<span>#</span> <span>Load</span> <span>cl</span><span>osure</span> <span>cell</span> <span>#</span><span>0</span>
<span>mov</span> <span>rax</span><span>,</span> <span>[</span><span>rsp</span><span>-</span><span>8</span><span>]</span>
<span>mov</span> <span>[</span><span>rsi</span><span>+</span><span>8</span><span>],</span> <span>rax</span>
<span>#</span> <span>Tag</span> <span>a</span> <span>cl</span><span>osure</span> <span>pointer</span>
<span>lea</span> <span>rax</span><span>,</span> <span>[</span><span>rsi</span><span>+</span><span>6</span><span>]</span>
<span>#</span> <span>Bump</span> <span>the</span> <span>heap</span> <span>pointer</span>
<span>add</span> <span>rsi</span><span>,</span> <span>16</span>
<span>ret</span>
</code></pre></div>

<p>One nicety of emitting text assembly is that I can add inline comments very
easily. That’s what my <code>comment</code> function is for: it just prefixes a <code>#</code>.</p>

<p>…wait, hold on, why are we reading from <code>rdi+2</code> for a closure variable? That
doesn’t make any sense, right?</p>

<p>That’s because while we are reading off the closure, we are reading from a
tagged pointer. Since we know the index into the closure and also the tag at
compile-time, we can fold them into one neat indirect.</p>

<div><pre><code><span>def</span> <span>compile_lexpr</span><span>(</span><span>lexpr</span><span>,</span> <span>code</span><span>):</span>
    <span>match</span> <span>lexpr</span><span>:</span>
        <span>case</span> <span>[</span><span>"code"</span><span>,</span> <span>params</span><span>,</span> <span>freevars</span><span>,</span> <span>body</span><span>]:</span>
            <span>env</span> <span>=</span> <span>{}</span>
            <span>for</span> <span>idx</span><span>,</span> <span>param</span> <span>in</span> <span>enumerate</span><span>(</span><span>params</span><span>):</span>
                <span>env</span><span>[</span><span>param</span><span>]</span> <span>=</span> <span>stack_at</span><span>(</span><span>-</span><span>(</span><span>idx</span><span>+</span><span>1</span><span>)</span><span>*</span><span>WORD_SIZE</span><span>)</span>
            <span># vvvv New for closures vvvv
</span>            <span>for</span> <span>idx</span><span>,</span> <span>fvar</span> <span>in</span> <span>enumerate</span><span>(</span><span>freevars</span><span>):</span>
                <span>env</span><span>[</span><span>fvar</span><span>]</span> <span>=</span> <span>indirect</span><span>(</span><span>CLOSURE_BASE</span><span>,</span> <span>(</span><span>idx</span><span>+</span><span>1</span><span>)</span><span>*</span><span>WORD_SIZE</span> <span>-</span> <span>CLOSURE_TAG</span><span>)</span>
            <span># ^^^^ New for closures ^^^^
</span>            <span>compile_expr</span><span>(</span><span>body</span><span>,</span> <span>code</span><span>,</span> <span>si</span><span>=-</span><span>(</span><span>len</span><span>(</span><span>env</span><span>)</span><span>+</span><span>1</span><span>)</span><span>*</span><span>WORD_SIZE</span><span>,</span> <span>env</span><span>=</span><span>env</span><span>)</span>
            <span>code</span><span>.</span><span>append</span><span>(</span><span>"ret"</span><span>)</span>
        <span>case</span> <span>_</span><span>:</span>
            <span>raise</span> <span>NotImplementedError</span><span>(</span><span>lexpr</span><span>)</span>
</code></pre></div>

<p>Now let’s call some closures…!</p>

<h3 id="compiling-funcall">Compiling <code>funcall</code></h3>

<p>I’ll start by showing the code for <code>labelcall</code> because it’s a good stepping
stone toward <code>funcall</code> (nice job, Dr Ghuloum!).</p>

<p>The main parts are:</p>

<ul>
  <li>save space on the stack for the return address</li>
  <li>compile the args onto the stack</li>
  <li>adjusting the stack pointer above the locals</li>
  <li>call</li>
  <li>bringing the stack pointer back</li>
</ul>

<p>I think in my last version (the C version) I did this recursively because
looping felt challenging to do neatly in C with the data structures I had
built but since this is Python and the wild west, we’re looping.</p>

<div><pre><code><span>def</span> <span>compile_expr</span><span>(</span><span>expr</span><span>,</span> <span>code</span><span>,</span> <span>si</span><span>,</span> <span>env</span><span>):</span>
    <span>match</span> <span>expr</span><span>:</span>
        <span># ...
</span>        <span>case</span> <span>[</span><span>"labelcall"</span><span>,</span> <span>str</span><span>(</span><span>label</span><span>),</span> <span>*</span><span>args</span><span>]:</span>
            <span>new_si</span> <span>=</span> <span>si</span> <span>-</span> <span>WORD_SIZE</span>  <span># Save a word for the return address
</span>            <span>for</span> <span>arg</span> <span>in</span> <span>args</span><span>:</span>
                <span>compile_expr</span><span>(</span><span>arg</span><span>,</span> <span>code</span><span>,</span> <span>new_si</span><span>,</span> <span>env</span><span>)</span>
                <span>emit</span><span>(</span><span>f</span><span>"mov </span><span>{</span><span>stack_at</span><span>(</span><span>new_si</span><span>)</span><span>}</span><span>, rax"</span><span>)</span>
                <span>new_si</span> <span>-=</span> <span>WORD_SIZE</span>
            <span># Align to one word before the return address
</span>            <span>si_adjust</span> <span>=</span> <span>abs</span><span>(</span><span>si</span><span>+</span><span>WORD_SIZE</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"sub rsp, </span><span>{</span><span>si_adjust</span><span>}</span><span>"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"call </span><span>{</span><span>label</span><span>}</span><span>"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"add rsp, </span><span>{</span><span>si_adjust</span><span>}</span><span>"</span><span>)</span>
        <span># ...
</span></code></pre></div>

<p>A lot of this carries over exactly to <code>funcall</code>, with a couple differences:</p>

<ul>
  <li>save space on the stack for the return address <em>and the closure pointer</em></li>
  <li>compile the function expression, which can be arbitrarily complex and results
in a closure pointer</li>
  <li>save the current closure pointer</li>
  <li>set up the new closure pointer</li>
  <li>call through the new closure pointer</li>
  <li>restore the old closure pointer</li>
</ul>

<p>I think the stack adjustment math was by and away the most irritating thing to
get right here. Oh, and also remembering to untag the closure when trying to
call it.</p>

<div><pre><code><span>def</span> <span>compile_expr</span><span>(</span><span>expr</span><span>,</span> <span>code</span><span>,</span> <span>si</span><span>,</span> <span>env</span><span>):</span>
    <span>match</span> <span>expr</span><span>:</span>
        <span># ...
</span>        <span>case</span> <span>[</span><span>"funcall"</span><span>,</span> <span>func</span><span>,</span> <span>*</span><span>args</span><span>]:</span>
            <span># Save a word for the return address and the closure pointer
</span>            <span>clo_si</span> <span>=</span> <span>si</span> <span>-</span> <span>WORD_SIZE</span>
            <span>retaddr_si</span> <span>=</span> <span>clo_si</span> <span>-</span> <span>WORD_SIZE</span>
            <span>new_si</span> <span>=</span> <span>retaddr_si</span>
            <span># Evaluate arguments
</span>            <span>for</span> <span>arg</span> <span>in</span> <span>args</span><span>:</span>
                <span>compile_expr</span><span>(</span><span>arg</span><span>,</span> <span>code</span><span>,</span> <span>new_si</span><span>,</span> <span>env</span><span>)</span>
                <span>emit</span><span>(</span><span>f</span><span>"mov </span><span>{</span><span>stack_at</span><span>(</span><span>new_si</span><span>)</span><span>}</span><span>, rax"</span><span>)</span>
                <span>new_si</span> <span>-=</span> <span>WORD_SIZE</span>
            <span>compile_expr</span><span>(</span><span>func</span><span>,</span> <span>code</span><span>,</span> <span>new_si</span><span>,</span> <span>env</span><span>)</span>
            <span># Save the current closure pointer
</span>            <span>emit</span><span>(</span><span>f</span><span>"mov </span><span>{</span><span>stack_at</span><span>(</span><span>clo_si</span><span>)</span><span>}</span><span>, </span><span>{</span><span>CLOSURE_BASE</span><span>}</span><span>"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"mov </span><span>{</span><span>CLOSURE_BASE</span><span>}</span><span>, rax"</span><span>)</span>
            <span># Align to one word before the return address
</span>            <span>si_adjust</span> <span>=</span> <span>abs</span><span>(</span><span>si</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"sub rsp, </span><span>{</span><span>si_adjust</span><span>}</span><span>"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"call </span><span>{</span><span>indirect</span><span>(</span><span>CLOSURE_BASE</span><span>,</span> <span>-</span><span>CLOSURE_TAG</span><span>)</span><span>}</span><span>"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"add rsp, </span><span>{</span><span>si_adjust</span><span>}</span><span>"</span><span>)</span>
            <span>emit</span><span>(</span><span>f</span><span>"mov </span><span>{</span><span>CLOSURE_BASE</span><span>}</span><span>, </span><span>{</span><span>stack_at</span><span>(</span><span>clo_si</span><span>)</span><span>}</span><span>"</span><span>)</span>
        <span># ...
</span></code></pre></div>

<p>So <code>((lambda (x) x) 3)</code> compiles to:</p>

<div><pre><code><span>.intel_syntax</span>
<span>.global</span> <span>scheme_entry</span>

<span>f0:</span>
<span>mov</span> <span>rax</span><span>,</span> <span>[</span><span>rsp</span><span>-</span><span>8</span><span>]</span>
<span>ret</span>

<span>scheme_entry:</span>
<span>#</span> <span>Evaluate</span> <span>arguments</span>
<span>mov</span> <span>rax</span><span>,</span> <span>12</span>
<span>mov</span> <span>[</span><span>rsp</span><span>-</span><span>24</span><span>],</span> <span>rax</span>
<span>#</span> <span>Get</span> <span>a</span> <span>pointer</span> <span>to</span> <span>the</span> <span>label</span>
<span>lea</span> <span>rax</span><span>,</span> <span>f0</span>
<span>mov</span> <span>[</span><span>rsi</span><span>+</span><span>0</span><span>],</span> <span>rax</span>
<span>#</span> <span>Tag</span> <span>a</span> <span>cl</span><span>osure</span> <span>pointer</span>
<span>lea</span> <span>rax</span><span>,</span> <span>[</span><span>rsi</span><span>+</span><span>6</span><span>]</span>
<span>#</span> <span>Bump</span> <span>the</span> <span>heap</span> <span>pointer</span>
<span>add</span> <span>rsi</span><span>,</span> <span>16</span>
<span>#</span> <span>Save</span> <span>the</span> <span>current</span> <span>cl</span><span>osure</span> <span>pointer</span>
<span>mov</span> <span>[</span><span>rsp</span><span>-</span><span>16</span><span>],</span> <span>rdi</span>
<span>mov</span> <span>rdi</span><span>,</span> <span>rax</span>
<span>#</span> <span>Align</span> <span>to</span> <span>one</span> <span>word</span> <span>before</span> <span>the</span> <span>return</span> <span>address</span>
<span>sub</span> <span>rsp</span><span>,</span> <span>8</span>
<span>call</span> <span>[</span><span>rdi</span><span>-</span><span>6</span><span>]</span>
<span>#</span> <span>Rest</span><span>ore</span> <span>stack</span> <span>and</span> <span>cl</span><span>osure</span>
<span>add</span> <span>rsp</span><span>,</span> <span>8</span>
<span>mov</span> <span>rdi</span><span>,</span> <span>[</span><span>rsp</span><span>-</span><span>16</span><span>]</span>
<span>ret</span>
</code></pre></div>

<p>Not bad for a 300 line compiler!</p>

<h3 id="wrapping-up">Wrapping up</h3>

<p>I think that’s all there is for today, folks. We got closures, free variable
analysis, and indirect function calls. That’s pretty good.</p>

<p>Happy hacking!</p>

<h4 id="mini-table-of-contents">Mini Table of Contents</h4>

<ul>































    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-0/"><span>Overture</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-1/"><span>The smallest program</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-2/"><span>Integers</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-3/"><span>Booleans, characters, nil</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-4/"><span>Primitive unary functions</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-5/"><span>Primitive binary functions</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-6/"><span>Reader</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-7/"><span>Let</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-8/"><span>If</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-9/"><span>Heap allocation</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-10/"><span>Instruction encoding interlude</span></a>
        
    </li>



    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-11/"><span>Labelled procedure calls</span></a>
        
    </li>











































































































    <li>
        <a href="https://bernsteinbear.com/blog/compiling-a-lisp-12/"><span>Closure conversion</span></a>
         <span><i>(this page)</i></span> 
    </li>


</ul>


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I tried coding with AI, I became lazy and stupid (127 pts)]]></title>
            <link>https://thomasorus.com/i-tried-coding-with-ai-i-became-lazy-and-stupid</link>
            <guid>44858641</guid>
            <pubDate>Sun, 10 Aug 2025 21:54:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thomasorus.com/i-tried-coding-with-ai-i-became-lazy-and-stupid">https://thomasorus.com/i-tried-coding-with-ai-i-became-lazy-and-stupid</a>, See on <a href="https://news.ycombinator.com/item?id=44858641">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
            

<p>Around April 2025, my boss at $dayjob insisted we try AI tools for coding. It wasn't toxic pressure or anything like <em>"20% of your code needs to be AI"</em>, just a concern from him that we could miss on something. I understand why he asked that and I don't blame him. We are in difficult economic period even for software, and we have salaries to pay. If AI can increase productivity or our margins, it should be at least put on the table of negotiations. I am not happy about this coming, but <em>I get it</em>.</p>

<details><summary>My personal stance of AI</summary> I have personal reasons to dislike LLMs. My partner lost their writing job due to chatGPT convincing their manager writers were now useless. A lot of artist friends struggle because of LLMs. We recently had an intern who lost her translator role due to LLMs. And even outside my personal experience, LLMs are based on stolen content, don't respect consent, waste huge amount of electricity and water, and are overall a new weapon for the capitalists in the <a href="https://danmcquillan.org/ai_thatcherism.html">class warfare</a>. </details>

<p>The other reason why I folded comes from a toxic relationship I built with my job when I became a developer. I detailed in a previous blog post how choosing this career came with very high stakes which triggered a shift in my brain that hasn't left me since:</p>

<figure><blockquote cite="/every-web-stack-is-a-product-now">When I started web development seven years ago, I was in survival mode after years of low paying wages and unemployment. It <em>had</em> to work, and for it to work, I <em>had</em> to always learn more, read and listen about web development all the time, monitor the field, socialize as much as possible with my peers. This way, I would not get disposable and lose my job. I would build a network. I would be safe.</blockquote><figcaption>— I, <a href="https://thomasorus.com/every-web-stack-is-a-product-now">In a blog post from 2022</a></figcaption></figure>

<p>10 years and 3 burnouts later, one can tell this mindset, even if it worked out for a while, wasn't sane or desirable. I had managed to put aside this fear of being disposable, but LLMs triggered it back big time. What if AI vendors were right? What if a future company I apply to requires you to use it? Am I going to lose my job? I'm almost 40, what will I do?</p>

<p>So I tried AI. First at my day job, because I wanted answers. But outside fixing TypeScript types errors, generate inaccessible template code, or review my code for errors, I couldn't find a <em>life changing</em> use out of it that all AI influencers talk about. I asked my colleagues about their own experiments, and lots of them came to the same conclusion: it doesn't seem to help me help our clients achieve their goals.</p>

<p>When July came I was starting the image processing part of my new CMS that powers this website. Still stressed I couldn't get a real shot at coding with an LLM, and very tired by different personal events that fogged my brain, I decided it was the right task to try it seriously and get answers.</p>

<p>After setting up everything in VS Code, opening the AI panel, giving access to the codebase and detailing my needs in a prompt, the LLM produced around 200 lines of code. Mostly functions using dependencies to convert, resize, process images. It wasn't perfect but after a few changes, the task was done and it had taken around 30 minutes, far less than if I had made it by hand.</p>

<p>I was impressed. It really felt like I had superpowers! But then I had the idea to audit the code the LLM just produced, like I did at my $dayjob for a Vue application. Feeling that uploading files could be a source of security issues, I asked the same LLM to focus on this specific topic.</p>

<p>It found several dangers: directory traversal attacks, file size limits, system file overwrite, etc. I had no idea the initial code was this unsafe. I had reviewed the code, but without enough experience in backend development, how could I identify issues I didn't know existed? And why, if it knew about all those dangers, did the LLM produced unsafe code in the first place?</p>

<p>When I tried to fix the security issues, I quickly realized how this whole thing was a trap. Since I didn't wrote it, I didn't have a good bird's eye view of the code and what it did. I couldn't make changes quickly, which started to frustrated me. The easiest route was asking the LLM to do the fixes for me, so I did. More code was changed and added. It worked, but again I could not tell if it was good or not.</p>

<p>That's when I stopped the experiment.</p>

<p>I was shocked by how easily I had slipped into this slacker way of programming. The LLM had given me shitty code, made me ignorant about my own code base, and too lazy to try to fix it myself. And at the same time, the whole experience felt smooth, frictionless, empowering. On the moment I felt smarter, more productive, in control. But it was all an illusion.</p>

<p>I knew about this as we had studies showing <a href="https://www.researchgate.net/publication/392560878_Your_Brain_on_ChatGPT_Accumulation_of_Cognitive_Debt_when_Using_an_AI_Assistant_for_Essay_Writing_Task">LLM use makes us dumb</a>, and that <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/">self-reported productivity gains are false</a>. But experiencing it for myself was a totally different feeling.</p>

<p>It gave me a whole different perspective and answered my initial question: will I get replaced by AI soon?</p>

<p>The answer is no. I don't think AI will take my job anytime soon because it's smarter and more productive than I am. I also don't think AI will make me <a href="https://colton.dev/blog/curing-your-ai-10x-engineer-imposter-syndrome/">10 times more productive</a>. If I lose my job due to AI, it will be because I used it so much it made me lazy and stupid to the point another human has to replace me and I become unemployable.</p>

<p>I shouldn't invest time in AI. I should invest more time studying new things that interest me. That's probably the only way to keep doing this job and, you know, <em>be safe</em>.</p>


            
    

        </article><p><small><strong>Initially published: </strong>08 Aug 2025 at 20:03</small><br>
        <small><strong>Modified and/or rebuilt: </strong>08 Aug 2025 at 20:17</small>
    </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[1910: The year the modern world lost its mind (333 pts)]]></title>
            <link>https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost</link>
            <guid>44858154</guid>
            <pubDate>Sun, 10 Aug 2025 20:48:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost">https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost</a>, See on <a href="https://news.ycombinator.com/item?id=44858154">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><p><em>“Automobilism is an illness, a mental illness. This illness has a pretty name: speed... [Man] can no longer stand still, he shivers, his nerves tense like springs, impatient to get going once he has arrived somewhere because it is not somewhere else, somewhere else, always somewhere else.” </em></p><p><em>- Octave Mirbeau, French novelist, 1910</em></p></div><p><em><strong>About today’s piece: When we hear about technological change and social crisis in the 21st century, it is easy to imagine that we are living through a special period of history. But many eras have grappled with the problems that seem to uniquely plague our own. The beginning of the 20th century was a period of speed and technological splendor (the automobile! the airplane! the bicycle!), shattered nerves, mass anxiety, and a widespread sense that the world had been forever knocked off its historical axis: a familiar stew of ideas. I think we can learn a lot about the present by studying historical periods whose challenges rhyme with our own.</strong></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!QkGQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!QkGQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 424w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 848w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg" width="580" height="394" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:394,&quot;width&quot;:580,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;1910 Model T ad&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="1910 Model T ad" title="1910 Model T ad" srcset="https://substackcdn.com/image/fetch/$s_!QkGQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 424w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 848w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Welcome back to The Sunday Morning Post!</p><p><span>My favorite period of history is the 30- to 40-year span between the end of the 19th century and the early innings of the 20th century. It was an era of incredible change. From </span><em>Abundance</em><span>:</span></p><blockquote><p>Imagine going to sleep in 1875 in New York City and waking up thirty years later. As you shut your eyes, there is no electric lighting, Coca-Cola, basketball, or aspirin. There are no cars or “sneakers.” The tallest building in Manhattan is a church. </p><p>When you wake up in 1905, the city has been remade with towering steel-skeleton buildings called “skyscrapers.” The streets are filled with novelty: automobiles powered by new internal combustion engines, people riding bicycles in rubber-soled shoes—all recent innovations. The Sears catalog, the cardboard box, and aspirin are  new arrivals. People have enjoyed their first sip of Coca-Cola and their first bite of what we now call an American hamburger. The Wright brothers have flown the first airplane. When you passed into slumber, nobody had taken a picture with a Kodak camera or used a machine that made motion pictures, or bought a device to play recorded music. By 1905, we have the first commercial versions of all three—the simple box camera, the cinematograph,  and the phonograph. </p></blockquote><p><span>No book on turn-of-the-century history has influenced me more, or brought me more joy, than </span><em><a href="https://www.amazon.com/Vertigo-Years-Europe-1900-1914/dp/0465020291" rel="">The Vertigo Years: Europe 1900-1914</a></em><span> by Philipp Blom. I think it might be the most underrated history book ever written.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-170457512" href="https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost#footnote-1-170457512" target="_self" rel="">1</a></span><span> In my favorite chapters focusing on the years around 1910</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-170457512" href="https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost#footnote-2-170457512" target="_self" rel="">2</a></span><span>, Blom describes how turn-of-the-century technology changed the way people thought about art and human nature and how it contributed to a nervous breakdown across the west. Disoriented by the speed of modern times, Europeans and Americans suffered from record-high rates of anxiety and a sense that our inventions had destroyed our humanity. Meanwhile, some artists channeled this disorientation to create some of the greatest art of all time.</span></p><p><span>In today’s TSMP, I want to share with you my favorite passages and lessons from </span><em>The Vertigo Years</em><span>, most of which come from the chapter on the year 1910. Great history books remind us that while history never repeats itself, its themes never stop rhyming, and we would all do well to listen with open ears. I’ve tried to limit my summary to areas of overlap between the early 1900s and the 2020s, but I’m not going to press the similarities too hard throughout the piece. You’re going to have to recognize them for yourself.</span></p><p>Transportation technology remade the west in a few short decades between the 1880s and 1910. A “bicycle craze” swept America in the 1890s. The Wright Brothers took flight in 1903. The first Model Ts rolled off Ford’s production lines in 1908. In Europe, cars quickly transformed the physical environment. The number of automobiles in France increased from about 3,000 in 1900 to 100,000 by 1914. That year, Ford's factory in Detroit produced and sold more than 300,000 Model Ts. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!3gex!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!3gex!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 424w, https://substackcdn.com/image/fetch/$s_!3gex!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 848w, https://substackcdn.com/image/fetch/$s_!3gex!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!3gex!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!3gex!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg" width="1456" height="978" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:978,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!3gex!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 424w, https://substackcdn.com/image/fetch/$s_!3gex!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 848w, https://substackcdn.com/image/fetch/$s_!3gex!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!3gex!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Model T, Wikimedia Commons</figcaption></figure></div><p><span>Speed was a physical experience, Blom writes, and cultural critics of the early 1900s were confident that it was unnatural for people to move so quickly through space—women, in particular. A woman on a bicycle was a thing to be feared. She signified a high-velocity freedom that was often associated with moral and sexual deviancy. Physicians warned that </span><a href="https://pessimistsarchive.org/list/bicycle/clippings/1897/sc-178" rel="">"diseases of the wheel"</a><span> came by "the almost universal use of the bicycle" and that </span><a href="https://pessimistsarchive.org/list/bicycle/clippings/1897/m-sc-192-184" rel="">"serious evils"</a><span> might befall the youth who rode without restraint.  Moralists condemned women who “pedaled along gleefully, having discarded their corsets and put on more practical clothing, including trousers.” </span></p><p><span>Critics and novelists considered technological speed to be a vice, and they warned that our lust for celerity might turn into literal lust; that cars and bicycles would beckon us into carnal sin. In </span><em>Le surmale</em><span> (1902), the book’s hero wins a 100,000-mile bike race and then celebrates with an act of love-making that makes one character exclaim, “This is not a man, but a machine!” The idea that cars, planes, and bicycles were turning people into “machines” was most entertainingly summarized by a 1905 article in the journal </span><em>Je sais tout</em><span> (“I know all”), which calculated just how tall a human being would have to be to naturally walk at the pace that our new machines traveled. To equal the speed of a bicycle, for example, it was calculated that a person have to be more than 40 feet tall. Blom:</span></p><blockquote><p>Comparisons with other forms of transportation showed that in a fast train, a voyager would be effectively 51 meters tall, while the chauffeur of a racing car would almost dwarf Notre Dame Cathedral in Paris. Technology had created a new race of giants — in both senses of the term — and it changed the experience of space and time itself.</p></blockquote><p>“The growing speed of daily life, of news and work and play was a fetish of artists and industrialists alike,” Blom writes. “Never before had so much social change occurred so quickly.” As daily life sped up, people in the west started to break down.</p><p><span>Around the turn of the century, a nervous disorder first diagnosed in the U.S. gradually made its way across the Atlantic. The doctor George Miller Beard had called it “neurasthenia,” or nervous exhaustion. Europeans sometimes referred to it as “American Nervousness.” According to Beard, the affliction was most common among “the in-door classes of civilized countries” and the sufferers could be found “in nearly every brain-working household.”</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-170457512" href="https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost#footnote-3-170457512" target="_self" rel="">3</a></span><span> </span></p><p>As Blom points out, those afflicted tended to be white-collar workers working at the “frontiers of technology,” as “telephone operators, typesetters on new, faster machines, railway workers, engineers, [or] factory workers handling fast machines. One 1893 hospital survey of neurasthenia found that among nearly 600 cases, “there were almost 200 businessmen, 130 civil servants, 68 teachers, 56 students and eleven farmers.” Notably, no manual workers were counted at the clinic. Neurasthenia seemed to disproportionately affect white-collar workers, who were “overwhelmed” by their labor. “Overwork was a common theme in patients’ histories,” Blom writes.</p><p>It is tempting to write off this phenomenon as just another case of the “worried well.” But the scale of the west’s mental health distress in this period was striking. Blom: </p><blockquote><p><strong>In Germany, 40,375 patients were registered in mental hospitals in 1870. The number rose to 115,882 in 1900 and 220,881 in 1910</strong><span>. Over the same period, the proportion of patients admitted to general hospitals for illnesses of the nervous system rose from 44 to 60 percent. While these numbers include those suffering from many and varied mental conditions, not just neurasthenia, they do not include the huge number of sufferers who preferred going for cures or long stays in private sanatoriums, spas or other paramedical establishments in which a doctor would look after the guests — as in the one described by Thomas Mann in </span><em>The Magic Mountain</em><span>. </span></p></blockquote><p>“Artists were fascinated by this accelerated reality and its possibilities,” Blom writes. The novelists, painters, and musicians of the era could not stop talking about the changes they saw around them and their duty to use art to enter into a dialogue with those changes. Blom: </p><blockquote><p>Their view of things was shaped by reading about races in fast machines and in children’s magazines, by over-hearing adult whispers about nervous breakdowns and fast women … their imagination was alert to the fact that an age had ended and a new one — by turns a promise and a menace — was busting onto the scene, visible as yet only in flashes and fragmented visions. </p></blockquote><p>Blom deeply considers three artistic icons of the era: the composer Igor Stravinsky and the painters Vassily Kandinsky and Pablo Picasso. Each sought to make art that felt simultaneously cutting-edge and primal. Each responded to the modern age by reaching for inspiration in the past.</p><p><span>Blom begins with Stravinsky, whose famous orchestral work </span><em>The Rite of Spring</em><span> was inspired by ancient Russian dance rituals. A melange of old folk music and arresting dissonance, the piece’s first performance in Paris 1913 triggered one of the most infamously violent reactions of any concert-hall audience in history. As Blom puts it bluntly, “all hell broke loose”: </span></p><blockquote><p>“During the first two minutes the public remained quiet,' Monteux [a musician] later recalled, “then there were boos and hissing from the upper circle, soon after from the stalls. People sitting next to one another began to hit one another on the head with fists and walking sticks, or whatever else they had to hand. Soon, their anger was turned against the dancers and especially against the orchestra... Everything to hand was thrown at them, but we continued playing. The chaos was complete when members of the audience turned on one another, on anyone supporting the other side. A heavily bejewelled lady was seen slapping her neighbour before storming off, while another one spat in her detractor's face. Fights broke out everywhere and challenges to duels were issued.”</p></blockquote><p><span>Some music critics now consider The Rite of Spring </span><a href="https://www.pittsburgh-theater.com/shows/heinz-hall/pittsburgh-symphony-orchestra-the-rite-of-spring" rel="">“undoubtedly the most famous composition of the early 20th century.”</a></p><p>As classical music disintegrated in the concert halls, visual art was undergoing its own revolution, which may have been technological in origin. For thousands of years before the turn-of-the-century, the ability to perfectly represent nature been a rare skill possessed only by the most talented painters and drawers among us. But the Kodak camera (invented in 1888, with sales accelerating into the 1900s) turned the ability to capture realist images into a consumerist trifle. It cannot be a coincidence that the rise of abstract art coincided so perfectly with the proliferation of cheap camera technology that debased the value of perfect renderings of the natural world. </p><p>In the early 1900s, Vassily Kandinsky, one of the great pioneers of abstract art, pushed back against the mind-blurring speed of modernity. Kandinsky drew inspiration from the shamans of the Ural Mountains and the sound of their drums, according to Blom, and his abstraction sought to capture their primary music in images. By turning sound into image, Kandinsky’s art sought to achieve an act of synesthesia that no Kodak machine could ever match. Other art historians are less certain about what inspired Kandinsky’s first abstract watercolors, which he painted around 1910. All that is certain is that paintings like this one reject any effort to depict the natural world as it might be seen through a retina or camera lens. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!EFhk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!EFhk!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 424w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 848w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!EFhk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg" width="1456" height="1151" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1151,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Untitled First Abstract Watercolor, 1910–1913, Centre Pompidou, Paris[19]&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="Untitled First Abstract Watercolor, 1910–1913, Centre Pompidou, Paris[19]" title="Untitled First Abstract Watercolor, 1910–1913, Centre Pompidou, Paris[19]" srcset="https://substackcdn.com/image/fetch/$s_!EFhk!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 424w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 848w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Untitled </span><em>First Abstract Watercolor</em><span>, 1910–1913</span></figcaption></figure></div><p>Kandinsky is one of my favorite artists. But the critical response to the dawn of abstract painting was about as brutal as it gets. One German review that Blom cites includes the following passage:</p><blockquote><p>Looked at as painting they are the end of art, a prank. But they show a more nefarious side. The modern phrase that the object of art is indifferent, if abused here in a truly malevolent way... What is presented to us breathes the poison breath of the darkest places of vice of the big city and shows the constitution of the artists, which can only be understood in terms of pathology.</p></blockquote><p><span>Around the same time that Kandinsky was putting his mark on abstraction, Pablo Picasso was pioneering his own rejection of purely representative art, with primitivism. Drawing inspiration from African masks and carvings from West Africa, Picasso’s art “did everything to hide its underlying technical and compositional virtuosity,” Blom writes. Picasso’s 1907 classic </span><em>Les Demoiselles d'Avignon</em><span> is “a large canvas of brutal and disturbing bluntness.” While Picasso was indifferent to the actual “significance and symbolism” of the African styles he drew on, Blom writes, critics have said his aim was to represent the “unchanging structure of the human condition” in the face of civilizational change.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!9OXU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!9OXU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 424w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 848w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!9OXU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg" width="1280" height="1326" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1326,&quot;width&quot;:1280,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;undefined&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="undefined" title="undefined" srcset="https://substackcdn.com/image/fetch/$s_!9OXU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 424w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 848w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Independent of one another, Stravinsky, Kandinsky, and Picasso each reacted to the modern world and “the alienation of the human mind from its own emotions” by pulling pre-modern styles and atavistic images into their art. What we call Modernism today was in most cases a reaction to modernity. It was an effort to excavate something ancient and honest about humanity in an age obsessed with and overrun by novelty.</p><p>Blom closes his chapter “1910: Human Nature Changed” by considering two intellectual giants of the time: the sociologist Max Weber and the psychoanalyst Sigmund Freud, whose International Psychoanalytic Association was founded in 1910. The tension between their theories of human nature are profoundly relevant today.</p><p><span>In his famous work </span><em>The Protestant Ethic and the Spirit of Capitalism</em><span>, Weber, a German sociologist, argued that certain Protestant—especially Calvinist—traditions supported habits that aligned with the development of modern capitalism. He argued that the Protestant tradition of northern European worshippers cultivated a disciplined approach to work, savings, and investment that proved valuable in commerce, while the Calvinist doctrine of divine grace “could lead believers to read worldly success as a possible sign of God’s favor,” as Blom summarizes. Weber believed that Protestantism not only encouraged followers to pour their energies into labor (hence the allusion to </span><em>Work Ethic</em><span> in the book’s title) but also helped create a culture of trade and investment that supported the rise of modern capitalism.</span></p><p><span>“It is easy to see how Freud’s analysis follows on from Weber’s,” Blom writes. To Freud, human nature was at risk of being fully dissolved by capitalism and modern society, like chalk dropped in acid. Beneath the polite masks demanded by modern society, he said, there lurked a more atavistic and instinctual self. Freud saw our psyche as a tug-of-war between the id (our animal urges) and superego (the voice in our head that internalizes society’s rules), with the ego stuck in the middle trying to negotiate an authentic identity in the face of mass inauthenticity. One of Freud’s most fantastic insights was that some people can channel or redirect their most raw and unacceptable urges toward productive and acceptable work. His name for this bit of psychological alchemy was </span><em>sublimation</em><span>. </span></p><p>Modern capitalism, in Freudian terms, was the sublimation of self-interest—or, one might even say, the sublimation of greed. “The suppression of natural urges is a necessary precondition for capitalist success,” Blom writes in summary, “but while it is productive for the group and its wealth, such an approach will eventually exact its revenge on the individual.” By this interpretation, the mass anxiety of the early 1900s—whether you call it neurasthenia, American Nervousness, or Newyorkitis—was price of modernity, technological development, and even capitalism itself.</p><p><span>There is little evidence that Freud and Weber ever debated one another. Yet when you set their theories side by side, it’s hard not to hear a conversation that still shapes much modern commentary. Weber wrote that modern capitalism evolved from religious doctrines that fit our nature, while Freud argued that human nature is unfit for a modern world that distorts and represses our basic urges. </span><em>Are our most impressive inventions the ultimate expression of our humanity, or are they the ultimate threat to it? </em><span>This is the question that every generation must answer for itself, including our own. It is a question equally worthy of the automobile and artificial intelligence. The troubling answer—for Weber and for Freud; for 1910 and for 2025—is: perhaps, both.</span></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[One Million Screenshots (258 pts)]]></title>
            <link>https://onemillionscreenshots.com/?q=random</link>
            <guid>44858067</guid>
            <pubDate>Sun, 10 Aug 2025 20:30:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://onemillionscreenshots.com/?q=random">https://onemillionscreenshots.com/?q=random</a>, See on <a href="https://news.ycombinator.com/item?id=44858067">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><a href="https://onemillionscreenshots.com/"><h2><span>One</span><span>Million</span><span>Screenshots</span></h2></a><p>Zoom into the web's top homepages</p></div></div>]]></description>
        </item>
    </channel>
</rss>