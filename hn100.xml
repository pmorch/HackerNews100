<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 14 Sep 2024 12:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Intel Solidifies $3.5B Deal to Make Chips for Military (148 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-09-13/intel-solidifies-3-5-billion-deal-to-make-chips-for-us-military</link>
            <guid>41536131</guid>
            <pubDate>Fri, 13 Sep 2024 23:29:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-09-13/intel-solidifies-3-5-billion-deal-to-make-chips-for-us-military">https://www.bloomberg.com/news/articles/2024-09-13/intel-solidifies-3-5-billion-deal-to-make-chips-for-us-military</a>, See on <a href="https://news.ycombinator.com/item?id=41536131">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My 71 TiB ZFS NAS After 10 Years and Zero Drive Failures (308 pts)]]></title>
            <link>https://louwrentius.com/my-71-tib-zfs-nas-after-10-years-and-zero-drive-failures.html</link>
            <guid>41536088</guid>
            <pubDate>Fri, 13 Sep 2024 23:21:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://louwrentius.com/my-71-tib-zfs-nas-after-10-years-and-zero-drive-failures.html">https://louwrentius.com/my-71-tib-zfs-nas-after-10-years-and-zero-drive-failures.html</a>, See on <a href="https://news.ycombinator.com/item?id=41536088">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              <p>My <a href="https://louwrentius.com/71-tib-diy-nas-based-on-zfs-on-linux.html">4U 71 TiB ZFS NAS</a> built with twenty-four 4 TB drives is over 10 years old and still going strong.</p>
<p><img alt="my nas" src="https://louwrentius.com/static/images/zfsnas01.jpg"></p>
<p>Although now on its second motherboard and power supply, the system has yet to experience a single drive failure (knock on wood).</p>
<p>Zero drive failures in ten years, how is that possible?</p>
<h2>Let's talk about the drives first</h2>
<p>The 4 TB HGST drives have roughly 6000 hours on them after ten years. You might think something's off and you'd be right. That's only about 250 days worth of runtime. And therein lies the secret of drive longevity (I think):</p>
<p><strong>Turn the server off when you're not using it.</strong></p>
<hr>
<p>According to people on <a href="https://news.ycombinator.com/item?id=41536088">Hacker News</a> I have my bearings wrong. The chance of having zero drive failures over 10 years for 24 drives is much higher than I thought it was. So this good result may not be related to turning my NAS off and keeping it off most off the time.</p>
<hr>
<p>My NAS is turned off by default. I only turn it on (remotely) when I need to use it. I use a script to turn the IoT power bar on and once the BMC (Baseboard Management Controller) is done booting, I use <a href="https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface">IPMI</a> to turn on the NAS itself. But I could have used <a href="https://en.wikipedia.org/wiki/Wake-on-LAN">Wake-on-Lan</a> too as an alternative.</p>
<p>Once I'm done using the server, I run a small script that turns the server off, wait a few seconds and then turn the wall socket off. </p>
<p>It wasn't enough for me to just turn off the server, but leave the motherboard, and thus the BMC powered, because that's just a constant 7 watts (about two Raspberry Pis at idle) being wasted (24/7).</p>
<p>This process works for me because I run other services on low-power devices such as Raspberry Pi4s or servers that use <em>much less power</em> when idling than my 'big' NAS. </p>
<p>This proces reduces my energy bill considerably (primary motivation) and also seems great for hard drive longevity.</p>
<p>Although zero drive failures to date is awesome, N=24 is not very representative and I could just be very lucky. Yet, it was the same story with the <a href="https://louwrentius.com/20-disk-18-tb-raid-6-storage-based-on-debian-linux.html">predecessor of this NAS</a>, a machine with 20 drives (1 TB Samsung Spinpoint F1s (remember those?)) and I also had zero drive failures during its operational lifespan (~5 years).</p>
<h2>The motherboard (died once)</h2>
<p>Although the drives are still ok, I had to replace the motherboard a few years ago. The failure mode of the motherboard was interesting: it was impossible to get into the BIOS and it would occasionally fail to boot. I tried the obvious like removing the CMOS battery and such but to no avail.</p>
<p>Fortunately, the [motherboard]<sup id="fnref:same"><a href="#fn:same">1</a></sup> was still available on Ebay for a decent price so that ended up not being a big deal. </p>
<h2>ZFS</h2>
<p>ZFS worked fine for all these years. I've switched operating systems over the years and I never had an issue importing the pool back into the new OS install.
If I would build a new storage server, I would definitely use ZFS again.</p>
<p>I run a zpool scrub on the drives a few times a year<sup id="fnref:longduration"><a href="#fn:longduration">2</a></sup>. The scrub has <em>never found a single checksum error</em>. I must have run so many scrubs, more than a <em>petabyte</em> of data must have been read from the drives (all drives combined) and ZFS didn't have to kick in.</p>
<p>I'm not surprised by this result at all. Drives tend to fail most often in two modes:</p>
<ol>
<li>Total failure, drive isn't even detected</li>
<li>Bad sectors (read or write failures)</li>
</ol>
<p>There is a third failure mode, but it's extremely rare: <strong>silent data corruption</strong>. Silent data corruption is 'silent' because a disk isn't aware it delivered corrupted data. Or the SATA connection didn't detect any checksum errors. </p>
<p>However, due to all the low-level checksumming, this risk is extremely small. It's a real risk, don't get me wrong, but it's a small risk. To me, it's a risk you mostly care about at scale, in datacenters<sup id="fnref:enterprise"><a href="#fn:enterprise">4</a></sup> but for residential usage, it's totally reasonable to accept the risk<sup id="fnref:risk"><a href="#fn:risk">3</a></sup>. </p>
<p>But ZFS is not that difficult to learn and if you are well-versed in Linux or FreeBSD, it's absolutely worth checking out. Just <a href="https://louwrentius.com/the-hidden-cost-of-using-zfs-for-your-home-nas.html">remember</a>!</p>
<p><img alt="inside" src="https://louwrentius.com/static/images/nano/topview.jpg"></p>
<h2>Sound levels (It's Oh So Quiet)</h2>
<p>This NAS is <em>very</em> quiet for a NAS (<a href="https://youtu.be/LS3cfl-7n-4">video with audio</a>). </p>
<p>But to get there, I had to do some work.</p>
<p>The chassis contains three sturdy 12V fans that cool the 24 drive cages. These fans are extremely loud if they run at their default speed. But because they are so beefy, they are fairly quiet when they run at idle RPM, yet they still provide enough airflow, most of the time. But running at idle speeds was not enough as the drives would heat up eventually, especially when they are being read from / written to.</p>
<p>Fortunately, the particular Supermicro motherboard I bought at the time allows all fan headers to be controlled through Linux. So I decided to create a <a href="https://github.com/louwrentius/storagefancontrol">script</a> that sets the fan speed according to the temperature of the hottest drive in the chassis.</p>
<p>I actually visited a math-related subreddit and asked for an algorithm that would best fit my need to create a silent setup and also keep the drives cool.
Somebody recommended to use a "<a href="https://en.wikipedia.org/wiki/Proportional%E2%80%93integral%E2%80%93derivative_controller">PID controller</a>", which I knew nothing about. So I wrote some Python, stole some example Python PID controller code, and tweaked the parameters to find a balance between sound and cooling performance.</p>
<p>The script has worked very well over the years and kept the drives at 40C or below. PID controllers are awesome and I feel it should be used in much more equipment that controls fans, temperature, and so on, instead of 'dumb' on/of behaviour or less 'dumb' lookup tables.</p>
<h2>Networking</h2>
<p>I started out with quad-port gigabit network controllers and I used <a href="https://louwrentius.com/achieving-450-mbs-network-file-transfers-using-linux-bonding.html">network bonding</a> to get around 450 MB/s network transfer speeds between various systems. This setup required a ton of UTP cables so eventually I got bored with that and I bought some cheap <a href="https://louwrentius.com/using-infiniband-for-cheap-and-fast-point-to-point-networking.html">Infiniband</a> cards and that worked fine, I could reach around 700 MB/s between systems. As I decided to move away from Ubuntu and back to Debian, I faced a problem: the Infiniband cards didn't work anymore and I could not figure out how to fix it. So I decided to buy some second-hand 10Gbit Ethernet cards and those work totally fine to this day.</p>
<h2>The dead power supply</h2>
<p>When you turn this system on, all drives spin up at once (no staggered spinup) and that draws around 600W for a few seconds. I remember that the power supply was rated for 750W and the 12 volt rail would have been able to deliver enough power, but it would sometimes cut out at boot nonetheless.</p>
<h2>UPS (or lack thereof)</h2>
<p>For many years, I used a beefy UPS with the system, to protect against power failure, just to be able to shutdown cleanly during an outage. This worked fine, but I noticed that the UPS used another 10+ watts on top of the usage of the server and I decided it had to go.</p>
<p>Losing the system due to power shenanigans is a risk I accept.</p>
<h2>Backups (or a lack thereof)</h2>
<p>My most important data is backed up trice. But a lot of data stored on this server isn't important enough for me to backup. I rely on replacement hardware and ZFS protecting against data loss due to drive failure.</p>
<p>And if that's not enough, I'm out of luck. I've accepted that risk for 10 years. Maybe one day my luck will run out, but until then, I enjoy what I have.</p>
<h2>Future storage plans (or lack thereof)</h2>
<p>To be frank, I don't have any. I built this server back in the day because I didn't want to shuffle data around due to storage space constraints and I still have ample space left.</p>
<p>I have a spare motherboard, CPU, Memory and a spare HBA card so I'm quite likely able to revive the system if something breaks.</p>
<p>As hard drive sizes have increased tremendously, I may eventually move away from the 24-drive bay chassis into a smaller form-factor. It's possible to create the same amount of redundant storage space with only 6-8 hard drives with RAIDZ2 (RAID 6) redundancy. Yet, storage is always expensive. </p>
<p>But another likely scenario is that in the coming years this system eventually dies and I decide not to replace it at all, and my storage hobby will come to an end.</p>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Legend of Holy Sword: An Immersive Experience for Concentration Enhancement (103 pts)]]></title>
            <link>https://arxiv.org/abs/2408.16782</link>
            <guid>41536003</guid>
            <pubDate>Fri, 13 Sep 2024 23:07:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2408.16782">https://arxiv.org/abs/2408.16782</a>, See on <a href="https://news.ycombinator.com/item?id=41536003">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2408.16782">View PDF</a>
    <a href="https://arxiv.org/html/2408.16782v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Concentration is significant for maximizing potential in any activity. However, traditional methods to improve it often lack direct and natural feedback. We propose an innovative and inspiring VR system to experience and improve concentration. In the experience of pulling out the holy sword, which cannot be achieved simply by force, the player receives multimodal concentration feedback in visual, auditory, and haptic senses. We believe that this experience will help the user confront his/her concentration and improve the ability to control it consciously.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Nanami Kotani [<a href="https://arxiv.org/show-email/e7e21ea5/2408.16782">view email</a>]      <br>    <strong>[v1]</strong>
        Sun, 18 Aug 2024 05:29:50 UTC (16,432 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI o1 Results on ARC-AGI-Pub (178 pts)]]></title>
            <link>https://arcprize.org/blog/openai-o1-results-arc-prize</link>
            <guid>41535694</guid>
            <pubDate>Fri, 13 Sep 2024 22:14:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arcprize.org/blog/openai-o1-results-arc-prize">https://arcprize.org/blog/openai-o1-results-arc-prize</a>, See on <a href="https://news.ycombinator.com/item?id=41535694">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<h3 id="arc-prize-testing-and-notes-on-openais-new-o1-model">ARC Prize testing and notes on OpenAI's new o1 model</h3>

<p>Over the past 24 hours, we got access to OpenAI's newly released <code>o1-preview</code> and <code>o1-mini</code> models specially trained to emulate reasoning. These models are given extra time to generate and refine reasoning tokens before giving a final answer.</p>

<p><a href="https://x.com/arcprize/status/1834348966663856577" target="_blank">Hundreds of people</a> have asked how o1 stacks up on ARC Prize. So we put it to test using the same baseline testing harness we've used to assess Claude 3.5 Sonnet, GPT-4o, and Gemini 1.5. Here are the results:</p>

<p><img src="https://cdn.zappy.app/9b60c2cb230af28f29a5f2a0506dbff1.png" alt="ARC-AGI-Pub Leaderboard"></p>

<p>Is o1 a new paradigm towards AGI? Will it scale up? What explains the massive difference between o1's performance on IOI, AIME, and many other <a href="https://openai.com/index/learning-to-reason-with-llms/" target="_blank">impressive benchmark scores</a> compared to only modest scores on ARC-AGI?</p>

<p>There's a lot to talk about.</p>

<hr>

<h3 id="chain-of-thought">Chain-of-Thought</h3>

<p>o1 fully realizes the "let's think step by step" chain-of-thought (CoT) paradigm by applying it at both training time <em>and</em> test time inference.</p>

<figure>
  <img src="https://arcprize.org/media/images/blog/openai-o1-performance.png" alt="OpenAI o1 Performance Chart">
  <figcaption>Source: OpenAI "<a href="https://openai.com/index/learning-to-reason-with-llms/" target="_blank">Learning to Reason with LLMs</a>".</figcaption>
</figure>

<p>In practice, o1 is significantly less likely to make mistakes when performing tasks where the sequence of intermediate steps is well-represented in the synthetic CoT training data.</p>

<p>At training time, OpenAI says they've built a new reinforcement learning (RL) algorithm and a highly data-efficient process that leverages CoT.</p>

<p>The implication is that the foundational source of o1 training is still a fixed set of pre-training data. But OpenAI is also able to generate tons of synthetic CoTs that emulate human reasoning to further train the model via RL. An unanswered question is how OpenAI selects which generated CoTs to train on?</p>

<p>While we have few details, reward signals for RL were likely achieved using verification (over formal domains like math and code) and human labeling (over informal domains like task breakdown and planning.)</p>

<p>At inference time, OpenAI says they're using RL to enable o1 to hone its CoT and refine the strategies it uses. We can speculate the reward signal here is some kind of actor + critic system similar to ones OpenAI <a href="https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/" target="_blank">previously published</a>. And that they're applying search or backtracking over the generated reasoning tokens at inference time.</p>

<hr>

<h3 id="test-time-compute">Test-Time Compute</h3>

<p>The most important aspect of o1 is that it shows a working example of applying CoT reasoning search to informal language as opposed to formal languages like math, code, or Lean.</p>

<p>While added train-time scaling using CoT is notable, the big new story is test-time scaling.</p>

<p>We believe iterated CoT genuinely unlocks greater generalization. Automatic iterative re-prompting enables the model to better adapt to novelty, in a way similar to test-time fine-tuning leveraged by the MindsAI team.</p>

<p>If we only do a single inference, we are limited to reapplying memorized programs. But by generating intermediate output CoTs, or programs, for each task, we unlock the ability to compose learned programs components, achieving adaption. This technique is one way to surmount the #1 issue of large language model generalization: the ability to adapt to novelty. Though like test-time fine-tuning it does ultimately remain limited.</p>

<p>When AI systems are allowed a variable amount of test-time compute (e.g., the amount of reasoning tokens or the time to search), there is no objective way to report a single benchmark score because it's relative to the allowed compute. That is what <a href="https://arcprize.org/media/images/blog/openai-o1-performance.png">this chart</a> shows.</p>

<p>More compute means more accuracy.</p>

<p>When OpenAI released o1 they could have allowed developers to specify the amount of compute or time allowed to refine CoT at test-time. Instead, they've "hard coded" a point along the test-time compute continuum and hid that implementation detail from developers.</p>

<p>With varying test-time compute, we can no longer just compare the output between two different AI systems to assess relative intelligence. We need to also compare the compute <em>efficiency</em>.</p>

<p>While OpenAI's announcement did not share efficiency numbers, it's exciting we're now entering a period where efficiency will be a focus. Efficiency is critical to the <a href="https://arcprize.org/arc#agi-definition">definition of AGI</a> and this is why ARC Prize enforces an efficiency limit on winning solutions.</p>

<p><strong>Our prediction</strong>: expect to see way more benchmark charts comparing accuracy vs test-time compute going forward.</p>

<hr>

<h3 id="arc-agi-pub-model-baselines">ARC-AGI-Pub Model Baselines</h3>

<p>OpenAI <code>o1-preview</code> and <code>o1-mini</code> both outperform <code>GPT-4o</code> on the ARC-AGI public evaluation dataset. <code>o1-preview</code> is about on par with Anthropic's Claude 3.5 Sonnet in terms of accuracy but takes about 10X longer to achieve similar results to Sonnet.</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Score <small>(public eval)</small></th>
      <th>Verification Score <small>(semi-private eval)</small></th>
      <th>Avg Time/Task <small>(mins)</small></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>o1-preview</td>
      <td>21.2%</td>
      <td>18%</td>
      <td>4.2</td>
    </tr>
    <tr>
      <td>Claude 3.5</td>
      <td>21%</td>
      <td>14%</td>
      <td>0.3</td>
    </tr>
    <tr>
      <td>o1-mini</td>
      <td>12.8%</td>
      <td>9.5%</td>
      <td>3.0</td>
    </tr>
    <tr>
      <td>GPT-4o</td>
      <td>9%</td>
      <td>5%</td>
      <td>0.3</td>
    </tr>
    <tr>
      <td>Gemini 1.5</td>
      <td>8%</td>
      <td>4.5%</td>
      <td>1.1</td>
    </tr>
  </tbody>
</table>

<p>To get the baseline model scores on the ARC-AGI-Pub leaderboard, we're using the <a href="https://www.kaggle.com/code/gregkamradt/using-frontier-models-on-arc-agi-via-langchain?scriptVersionId=184611945" target="_blank">same baseline prompt</a> we used to test <code>GPT-4o</code>. When we test and report results on pure models like o1, our intention is to get a measurement of base model performance, as much as that is possible, without layering on any optimization.</p>

<p>Others may discover better ways to prompt CoT-style models in the future, and we are happy to add those to the leaderboard if verified.</p>

<p>o1's performance increase did come with a time cost. It took 70 hours on the 400 public tasks compared to only 30 minutes for <code>GPT-4o</code> and Claude 3.5 Sonnet.</p>

<p>You can use our <a href="https://www.kaggle.com/code/gregkamradt/testing-o1-preview-on-arc-agi" target="_blank">open source Kaggle notebook</a> as a baseline testing harness or starting point for your own approach. The SOTA submission on the public leaderboard is the result of clever techniques in addition to cutting edge models.</p>

<p>Maybe you can figure out how to leverage o1 as a foundational component to achieve a higher score in a similar way!</p>



<hr>

<h2 id="is-agi-here">Is AGI Here?</h2>

<p>On <a href="https://arcprize.org/media/images/blog/openai-o1-performance.png">this chart</a>, OpenAI shows a log-linear relationship between accuracy and test-time compute on AIME. In other words, with exponentially increasing compute, accuracy goes up linearly.</p>

<p>A new question many are asking: how far does this scale?</p>

<p>The only conceptual limit to the approach is the <a href="https://en.wikipedia.org/wiki/Decidability_(logic)" target="_blank">decidability</a> of the problem posed to the AI. So long as the search process has an external verifier which does contain the answer, you will see accuracy scale up logarithmically with compute.</p>

<p>In fact, the reported results are extremely similar to one of ARC Prize's <a href="https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt" target="_blank">top approaches</a> by Ryan Greenblatt. He achieved a score of 43% by having <code>GPT-4o</code> generate k=2,048 solution programs per task and deterministically verifying them against the task demonstrations.</p>

<p>Then he assessed how accuracy varied for different values of k.</p>

<figure>
  <img src="https://arcprize.org/media/images/blog/o1-top-3-accuracy-vs-k.jpg" alt="Top 3 Accuracy vs k">
  <figcaption>Source: Ryan Greenblatt.</figcaption>
</figure>

<p>Ryan found an identical log-linear relationship between accuracy and test-time compute on ARC-AGI.</p>

<p>Does all this mean AGI is here if we just scale test-time compute? Not quite.</p>

<p>You can see similar exponential scaling curves by looking at any brute force search which is O(x^n). In fact, we know at least 50% of ARC-AGI can be solved via brute force and zero AI.</p>

<p>To beat ARC-AGI this way, you'd need to generate over 100 million solution programs per task. Practicality alone rules out O(x^n) search for scaled up AI systems.</p>

<p>Moreover, we know this is not how humans beat ARC tasks. Humans do not generate thousands of potential solutions, instead we use the perception network in our brains to "see" a handful of potential solutions and determnistically check them with system 2-style thinking.</p>

<p>We can get smarter.</p>

<hr>

<h2 id="new-ideas-are-needed">New Ideas Are Needed</h2>

<p>Intelligence can be measured by looking at how well a system converts information to action over a space of situations. It's a conversion ratio and thus approaches a limit. Once you have perfect intelligence, the only way to progress is to go collect new information.</p>

<p>There are a couple ways a less intelligent system could appear more intelligent without actually being more intelligent.</p>

<p>One way is a system that just memorizes the best action. This kind of system would be very brittle, appearing intelligent in one domain, but fall over easily in another.</p>

<p>Another way is through trial and error. A system might appear intelligent if it eventually gets the right answer, but not if it needs 100 guesses first.</p>

<p>We should expect future test-time compute research to look at how to scale search and refinement more efficiently, perhaps using deep learning to guide the search process.</p>

<p>That said, we don't believe this alone explains the big gap between o1's performance on ARC-AGI and other objectively difficult benchmarks like IOI or AIME.</p>

<p>A more sufficient way to explain this is that o1 still operates primarily within distribution of its pre-training data, but now inclusive of all the newly generated synthetic CoTs.</p>

<p>The additional synthetic CoT data increases focus on the distribution of CoTs as opposed to just the distribution of answers (more compute is spent on how to get the answer vs what is the answer). We expect to see systems like o1 do better on benchmarks that involve reusing well-known emulated reasoning templates (programs) but will still struggle to crack problems that require synthesizing brand new reasoning on the fly.</p>

<p>Test-time refinement on CoT can only correct reasoning mistakes so far. This also suggests why o1 is so impressive in certain domains. Test-time refinement on CoT gets an additional boost when the base model is pre-trained in a similar way.</p>

<p>Either approach alone would not get you the big leap.</p>

<p>In summary, o1 represents a paradigm shift from "memorize the answers" to "memorize the reasoning" but is not a departure from the broader paradigm of fitting a curve to a distribution in order to boost performance by making everything in-distribution.</p>

<p>We still need new ideas for AGI.</p>

<h3 id="take-action">Take Action</h3>

<p>Deep CoT itegration is an exciting, new direction and there's much more to be discovered. ARC Prize exists to bring this energy to open source so that all can benefit from open AGI in our lifetime.</p>

<p>Do you have ideas on how to push these new ideas further? What about CoT with multi-modal, CoT with code generation, or combining program search with CoT?</p>

<p>Now is the time to jump into the action by participating in ARC Prize. Win top score prizes and paper awards and the attention and respect of millions.</p>

<p>Great ideas can come from anywhere. Maybe you?</p>

<p>Try hacking on o1 yourself starting with our default <a href="https://www.kaggle.com/code/gregkamradt/testing-o1-preview-on-arc-agi" target="_blank">Kaggle notebook</a>.</p>

<p><a href="#" data-modal-id="newsletter">Sign up for ARC Prize</a> to get notified when new high scores hit the leaderboard.</p>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lisp implemented in Rust macros (234 pts)]]></title>
            <link>https://github.com/RyanWelly/lisp-in-rs-macros</link>
            <guid>41535354</guid>
            <pubDate>Fri, 13 Sep 2024 21:31:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/RyanWelly/lisp-in-rs-macros">https://github.com/RyanWelly/lisp-in-rs-macros</a>, See on <a href="https://news.ycombinator.com/item?id=41535354">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto"><code>lisp-in-rs-macros</code></h2><a id="user-content-lisp-in-rs-macros" aria-label="Permalink: lisp-in-rs-macros" href="#lisp-in-rs-macros"></a></p>
<p dir="auto">A simple, lexically scoped Lisp interpreter that operates fully in Rust's declarative macros. The <code>lisp!</code> macro expands to the lisp value computed by the code, and then stringifies it. This means that <code>lisp!(CAR (CONS (QUOTE A) (QUOTE (B))))</code> expands to the string "A" and that all this computation happens at compile time by rustc expanding macros.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why</h2><a id="user-content-why" aria-label="Permalink: Why" href="#why"></a></p>
<p dir="auto">It's a lisp interpreter written fully in Rust's macros, I think that's pretty cool. It's also less than 250 lines, which is neat.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Example</h2><a id="user-content-example" aria-label="Permalink: Example" href="#example"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="let output = lisp!(CAR (LIST (QUOTE A) (QUOTE B) (QUOTE C)));
assert_eq!(output, &quot;A&quot;); 

lisp!(PROGN
(DEFINE message (LAMBDA () (QUOTE &quot;hello there&quot;)))
(DISPLAY (message))
(DEFINE NOT (LAMBDA (X) (COND (X NIL) (TRUE TRUE))) )
(DISPLAY (NOT NIL))
); // will print &quot;hello there&quot; and &quot;TRUE&quot;
// &quot;DISPLAY&quot; forms first evaluate their arguments, then expand to a println!(&quot;{}&quot;, stringify!(evaled_argument))
"><pre><span>let</span> output = <span>lisp</span><span>!</span><span>(</span><span>CAR</span> <span>(</span><span>LIST</span> <span>(</span><span>QUOTE</span> <span>A</span><span>)</span> <span>(</span><span>QUOTE</span> <span>B</span><span>)</span> <span>(</span><span>QUOTE</span> <span>C</span><span>)</span><span>)</span><span>)</span><span>;</span>
<span>assert_eq</span><span>!</span><span>(</span>output, <span>"A"</span><span>)</span><span>;</span> 

<span>lisp</span><span>!</span><span>(</span><span>PROGN</span>
<span>(</span><span>DEFINE</span> message <span>(</span><span>LAMBDA</span> <span>(</span><span>)</span> <span>(</span><span>QUOTE</span> <span>"hello there"</span><span>)</span><span>)</span><span>)</span>
<span>(</span><span>DISPLAY</span> <span>(</span>message<span>)</span><span>)</span>
<span>(</span><span>DEFINE</span> <span>NOT</span> <span>(</span><span>LAMBDA</span> <span>(</span><span>X</span><span>)</span> <span>(</span><span>COND</span> <span>(</span><span>X</span> <span>NIL</span><span>)</span> <span>(</span><span>TRUE</span> <span>TRUE</span><span>)</span><span>)</span><span>)</span> <span>)</span>
<span>(</span><span>DISPLAY</span> <span>(</span><span>NOT</span> <span>NIL</span><span>)</span><span>)</span>
<span>)</span><span>;</span> <span>// will print "hello there" and "TRUE"</span>
<span>// "DISPLAY" forms first evaluate their arguments, then expand to a println!("{}", stringify!(evaled_argument))</span></pre></div>
<p dir="auto">As another fun example, here is a quine:</p>
<div dir="auto" data-snippet-clipboard-copy-content="lisp!
       ((LAMBDA (s) (LIST s (LIST (QUOTE QUOTE) s)))
       (QUOTE (LAMBDA (s) (LIST s (LIST (QUOTE QUOTE) s)))));"><pre><span>lisp</span><span>!</span>
       <span>(</span><span>(</span><span>LAMBDA</span> <span>(</span>s<span>)</span> <span>(</span><span>LIST</span> s <span>(</span><span>LIST</span> <span>(</span><span>QUOTE</span> <span>QUOTE</span><span>)</span> s<span>)</span><span>)</span><span>)</span>
       <span>(</span><span>QUOTE</span> <span>(</span><span>LAMBDA</span> <span>(</span>s<span>)</span> <span>(</span><span>LIST</span> s <span>(</span><span>LIST</span> <span>(</span><span>QUOTE</span> <span>QUOTE</span><span>)</span> s<span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span>;</span></pre></div>
<p dir="auto">This code expands to:</p>
<div dir="auto" data-snippet-clipboard-copy-content="stringify!(((LAMBDA (s) (LIST s (LIST (QUOTE QUOTE) s)))
       (QUOTE (LAMBDA (s) (LIST s (LIST (QUOTE QUOTE) s))))));"><pre><span>stringify</span><span>!</span><span>(</span><span>(</span><span>(</span><span>LAMBDA</span> <span>(</span>s<span>)</span> <span>(</span><span>LIST</span> s <span>(</span><span>LIST</span> <span>(</span><span>QUOTE</span> <span>QUOTE</span><span>)</span> s<span>)</span><span>)</span><span>)</span>
       <span>(</span><span>QUOTE</span> <span>(</span><span>LAMBDA</span> <span>(</span>s<span>)</span> <span>(</span><span>LIST</span> s <span>(</span><span>LIST</span> <span>(</span><span>QUOTE</span> <span>QUOTE</span><span>)</span> s<span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span>;</span></pre></div>
<p dir="auto">In other words, the code evaluates to itself. Isn't that wonderful?</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Recursion</h2><a id="user-content-recursion" aria-label="Permalink: Recursion" href="#recursion"></a></p>
<p dir="auto">This lisp does not currently support any explicit form of recursion. Luckily, explicit recursion is not needed, all we need is lambda.</p>
<p dir="auto">You can write a simple function that appends two lists by using self application:</p>
<div dir="auto" data-snippet-clipboard-copy-content="lisp!(PROGN
(DEFINE append 
    (LAMBDA (self X Y) 
        (COND 
            ((EQ X NIL) Y) 
            (TRUE (CONS (CAR X) (self self (CDR X) Y))) 
        )))
(append append (QUOTE (A B)) (QUOTE (C D)))

)"><pre><span>lisp</span><span>!</span><span>(</span><span>PROGN</span>
<span>(</span><span>DEFINE</span> append 
    <span>(</span><span>LAMBDA</span> <span>(</span><span>self</span> <span>X</span> <span>Y</span><span>)</span> 
        <span>(</span><span>COND</span> 
            <span>(</span><span>(</span><span>EQ</span> <span>X</span> <span>NIL</span><span>)</span> <span>Y</span><span>)</span> 
            <span>(</span><span>TRUE</span> <span>(</span><span>CONS</span> <span>(</span><span>CAR</span> <span>X</span><span>)</span> <span>(</span><span>self</span> <span>self</span> <span>(</span><span>CDR</span> <span>X</span><span>)</span> <span>Y</span><span>)</span><span>)</span><span>)</span> 
        <span>)</span><span>)</span><span>)</span>
<span>(</span>append append <span>(</span><span>QUOTE</span> <span>(</span><span>A</span> <span>B</span><span>)</span><span>)</span> <span>(</span><span>QUOTE</span> <span>(</span><span>C</span> <span>D</span><span>)</span><span>)</span><span>)</span>

<span>)</span></pre></div>
<p dir="auto">This results in "(A B C D)". The append function does not mention <code>append</code> in its body, yet we can call it recursively. Wonderful!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Notes for use</h2><a id="user-content-notes-for-use" aria-label="Permalink: Notes for use" href="#notes-for-use"></a></p>
<p dir="auto">The lisp! macro only evaluates a single expression; if you want to evaluate multiple expressions, use <code>(PROGN expr1 expr2 expr3)</code>. This evaluates all the expressions, and returns the value of the last expression. The DISPLAY form evaluates a single expression, then generates a <code>println!("{}", stringify!(...))</code> statement which prints the stringified version of the tokens. The empty list is not self evaluating, you can use <code>NIL</code> or <code>(QUOTE ())</code> to obtain an empty list value. The empty list is the sole "falsy" object.
Dotted lists aren't supported, cons assumes its last argument is a list. The define form can be used anywhere and evaluates to the empty list, but does not support recursion. TRUE is the only self evaluating atom (that isn't a function).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported forms</h2><a id="user-content-supported-forms" aria-label="Permalink: Supported forms" href="#supported-forms"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="DEFINE 
QUOTE 
LAMBDA 
LET
PROGN 
CAR 
CDR 
CONS
LIST
EQ
ATOM
APPLY"><pre><span>DEFINE</span> 
<span>QUOTE</span> 
<span>LAMBDA</span> 
<span>LET</span>
<span>PROGN</span> 
<span>CAR</span> 
<span>CDR</span> 
<span>CONS</span>
<span>LIST</span>
<span>EQ</span>
<span>ATOM</span>
<span>APPLY</span></pre></div>
<p dir="auto">Note: dotted lists are not supported, CONS assumes its latter argument is a list. Define does not handle recursive definitions, it's more like internal definitions in Scheme than a true lispy define.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Metacircular interpreter</h2><a id="user-content-metacircular-interpreter" aria-label="Permalink: Metacircular interpreter" href="#metacircular-interpreter"></a></p>
<p dir="auto">Here is a lisp interpreter written in my lisp:</p>
<div dir="auto" data-snippet-clipboard-copy-content="lisp!(PROGN
            // Y &quot;combinator&quot; for two arguments
        (DEFINE Y2 
                        (LAMBDA (h)
                            ((LAMBDA (x) (h (LAMBDA (a b) ((x x) a b))))
                                (LAMBDA (x) (h (LAMBDA (a b) ((x x) a b)))))))
        
        (DEFINE CADR (LAMBDA (X) (CAR (CDR X))))
        (DEFINE CAAR (LAMBDA (X) (CAR (CAR X))))
        (DEFINE CADAR (LAMBDA (X) (CAR (CDR (CAR X)))))
        (DEFINE CADDR (LAMBDA (X) (CAR (CDR (CDR X)))))
        (DEFINE CADDAR (LAMBDA (X) (CAR (CDR (CDR (CAR X))))))
        (DEFINE CAADAR (LAMBDA (X) (CAR (CAR (CDR (CAR X))))))

        (DEFINE ASSOC (Y2 (LAMBDA (ASSOC) (LAMBDA (X ENV) 
                        (IF (EQ (CAAR ENV) X) (CADAR ENV) (ASSOC X (CDR ENV)))
                    )))
                )


            
        (DEFINE eval (Y2 (LAMBDA (EVAL) (LAMBDA (E A) 
                (COND
                    ((ATOM E) (ASSOC E A))
                    ((ATOM (CAR E)) 
                        (COND 
                            ((EQ (CAR E) (QUOTE quote)) (CADR E))
                            ((EQ (CAR E) (QUOTE atom)) (ATOM (EVAL (CADR E) A)))
                            ((EQ (CAR E) (QUOTE car)) (CAR (EVAL (CADR E) A)))
                            ((EQ (CAR E) (QUOTE cdr)) (CDR (EVAL (CADR E) A)))
                            ((EQ (CAR E) (QUOTE equal)) (EQ (EVAL (CADR E) A) (EVAL (CADDR E) A)))
                            ((EQ (CAR E) (QUOTE cons)) (CONS (EVAL (CADR E) A) (EVAL (CADDR E) A)))
                            (TRUE (EVAL (CONS (ASSOC (CAR E) A) (CDR E)) A)) 
                        )
                    )
                    ((EQ (CAAR E) (QUOTE lambda)) (EVAL (CADDAR E) (CONS (LIST (CAADAR E) (EVAL (CADR E) A)) A)  )) //Evaluate the inner expression of the lambda, in the environment with the argument bound to the parameter
                
                )
            ))))

        (eval (QUOTE (quote (A))) NIL)
        // (eval (QUOTE (atom (quote A))) NIL )
        // (eval (QUOTE (cdr (cdr (quote (A B))))) NIL)
        // (eval (QUOTE (cons (quote a) (quote (a)))) NIL)
        // (eval (QUOTE ((lambda (x) (quote a)) (quote b))) NIL)
        (eval (QUOTE ((lambda (X) X) (quote a))) NIL)

        );"><pre><span>lisp</span><span>!</span><span>(</span><span>PROGN</span>
            <span>// Y "combinator" for two arguments</span>
        <span>(</span><span>DEFINE</span> <span>Y2</span> 
                        <span>(</span><span>LAMBDA</span> <span>(</span>h<span>)</span>
                            <span>(</span><span>(</span><span>LAMBDA</span> <span>(</span>x<span>)</span> <span>(</span>h <span>(</span><span>LAMBDA</span> <span>(</span>a b<span>)</span> <span>(</span><span>(</span>x x<span>)</span> a b<span>)</span><span>)</span><span>)</span><span>)</span>
                                <span>(</span><span>LAMBDA</span> <span>(</span>x<span>)</span> <span>(</span>h <span>(</span><span>LAMBDA</span> <span>(</span>a b<span>)</span> <span>(</span><span>(</span>x x<span>)</span> a b<span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span>
        
        <span>(</span><span>DEFINE</span> <span>CADR</span> <span>(</span><span>LAMBDA</span> <span>(</span><span>X</span><span>)</span> <span>(</span><span>CAR</span> <span>(</span><span>CDR</span> <span>X</span><span>)</span><span>)</span><span>)</span><span>)</span>
        <span>(</span><span>DEFINE</span> <span>CAAR</span> <span>(</span><span>LAMBDA</span> <span>(</span><span>X</span><span>)</span> <span>(</span><span>CAR</span> <span>(</span><span>CAR</span> <span>X</span><span>)</span><span>)</span><span>)</span><span>)</span>
        <span>(</span><span>DEFINE</span> <span>CADAR</span> <span>(</span><span>LAMBDA</span> <span>(</span><span>X</span><span>)</span> <span>(</span><span>CAR</span> <span>(</span><span>CDR</span> <span>(</span><span>CAR</span> <span>X</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span>
        <span>(</span><span>DEFINE</span> <span>CADDR</span> <span>(</span><span>LAMBDA</span> <span>(</span><span>X</span><span>)</span> <span>(</span><span>CAR</span> <span>(</span><span>CDR</span> <span>(</span><span>CDR</span> <span>X</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span>
        <span>(</span><span>DEFINE</span> <span>CADDAR</span> <span>(</span><span>LAMBDA</span> <span>(</span><span>X</span><span>)</span> <span>(</span><span>CAR</span> <span>(</span><span>CDR</span> <span>(</span><span>CDR</span> <span>(</span><span>CAR</span> <span>X</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span>
        <span>(</span><span>DEFINE</span> <span>CAADAR</span> <span>(</span><span>LAMBDA</span> <span>(</span><span>X</span><span>)</span> <span>(</span><span>CAR</span> <span>(</span><span>CAR</span> <span>(</span><span>CDR</span> <span>(</span><span>CAR</span> <span>X</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span>

        <span>(</span><span>DEFINE</span> <span>ASSOC</span> <span>(</span><span>Y2</span> <span>(</span><span>LAMBDA</span> <span>(</span><span>ASSOC</span><span>)</span> <span>(</span><span>LAMBDA</span> <span>(</span><span>X</span> <span>ENV</span><span>)</span> 
                        <span>(</span><span>IF</span> <span>(</span><span>EQ</span> <span>(</span><span>CAAR</span> <span>ENV</span><span>)</span> <span>X</span><span>)</span> <span>(</span><span>CADAR</span> <span>ENV</span><span>)</span> <span>(</span><span>ASSOC</span> <span>X</span> <span>(</span><span>CDR</span> <span>ENV</span><span>)</span><span>)</span><span>)</span>
                    <span>)</span><span>)</span><span>)</span>
                <span>)</span>


            
        <span>(</span><span>DEFINE</span> eval <span>(</span><span>Y2</span> <span>(</span><span>LAMBDA</span> <span>(</span><span>EVAL</span><span>)</span> <span>(</span><span>LAMBDA</span> <span>(</span><span>E</span> <span>A</span><span>)</span> 
                <span>(</span><span>COND</span>
                    <span>(</span><span>(</span><span>ATOM</span> <span>E</span><span>)</span> <span>(</span><span>ASSOC</span> <span>E</span> <span>A</span><span>)</span><span>)</span>
                    <span>(</span><span>(</span><span>ATOM</span> <span>(</span><span>CAR</span> <span>E</span><span>)</span><span>)</span> 
                        <span>(</span><span>COND</span> 
                            <span>(</span><span>(</span><span>EQ</span> <span>(</span><span>CAR</span> <span>E</span><span>)</span> <span>(</span><span>QUOTE</span> quote<span>)</span><span>)</span> <span>(</span><span>CADR</span> <span>E</span><span>)</span><span>)</span>
                            <span>(</span><span>(</span><span>EQ</span> <span>(</span><span>CAR</span> <span>E</span><span>)</span> <span>(</span><span>QUOTE</span> atom<span>)</span><span>)</span> <span>(</span><span>ATOM</span> <span>(</span><span>EVAL</span> <span>(</span><span>CADR</span> <span>E</span><span>)</span> <span>A</span><span>)</span><span>)</span><span>)</span>
                            <span>(</span><span>(</span><span>EQ</span> <span>(</span><span>CAR</span> <span>E</span><span>)</span> <span>(</span><span>QUOTE</span> car<span>)</span><span>)</span> <span>(</span><span>CAR</span> <span>(</span><span>EVAL</span> <span>(</span><span>CADR</span> <span>E</span><span>)</span> <span>A</span><span>)</span><span>)</span><span>)</span>
                            <span>(</span><span>(</span><span>EQ</span> <span>(</span><span>CAR</span> <span>E</span><span>)</span> <span>(</span><span>QUOTE</span> cdr<span>)</span><span>)</span> <span>(</span><span>CDR</span> <span>(</span><span>EVAL</span> <span>(</span><span>CADR</span> <span>E</span><span>)</span> <span>A</span><span>)</span><span>)</span><span>)</span>
                            <span>(</span><span>(</span><span>EQ</span> <span>(</span><span>CAR</span> <span>E</span><span>)</span> <span>(</span><span>QUOTE</span> equal<span>)</span><span>)</span> <span>(</span><span>EQ</span> <span>(</span><span>EVAL</span> <span>(</span><span>CADR</span> <span>E</span><span>)</span> <span>A</span><span>)</span> <span>(</span><span>EVAL</span> <span>(</span><span>CADDR</span> <span>E</span><span>)</span> <span>A</span><span>)</span><span>)</span><span>)</span>
                            <span>(</span><span>(</span><span>EQ</span> <span>(</span><span>CAR</span> <span>E</span><span>)</span> <span>(</span><span>QUOTE</span> cons<span>)</span><span>)</span> <span>(</span><span>CONS</span> <span>(</span><span>EVAL</span> <span>(</span><span>CADR</span> <span>E</span><span>)</span> <span>A</span><span>)</span> <span>(</span><span>EVAL</span> <span>(</span><span>CADDR</span> <span>E</span><span>)</span> <span>A</span><span>)</span><span>)</span><span>)</span>
                            <span>(</span><span>TRUE</span> <span>(</span><span>EVAL</span> <span>(</span><span>CONS</span> <span>(</span><span>ASSOC</span> <span>(</span><span>CAR</span> <span>E</span><span>)</span> <span>A</span><span>)</span> <span>(</span><span>CDR</span> <span>E</span><span>)</span><span>)</span> <span>A</span><span>)</span><span>)</span> 
                        <span>)</span>
                    <span>)</span>
                    <span>(</span><span>(</span><span>EQ</span> <span>(</span><span>CAAR</span> <span>E</span><span>)</span> <span>(</span><span>QUOTE</span> lambda<span>)</span><span>)</span> <span>(</span><span>EVAL</span> <span>(</span><span>CADDAR</span> <span>E</span><span>)</span> <span>(</span><span>CONS</span> <span>(</span><span>LIST</span> <span>(</span><span>CAADAR</span> <span>E</span><span>)</span> <span>(</span><span>EVAL</span> <span>(</span><span>CADR</span> <span>E</span><span>)</span> <span>A</span><span>)</span><span>)</span> <span>A</span><span>)</span>  <span>)</span><span>)</span> <span>//Evaluate the inner expression of the lambda, in the environment with the argument bound to the parameter</span>
                
                <span>)</span>
            <span>)</span><span>)</span><span>)</span><span>)</span>

        <span>(</span>eval <span>(</span><span>QUOTE</span> <span>(</span>quote <span>(</span><span>A</span><span>)</span><span>)</span><span>)</span> <span>NIL</span><span>)</span>
        <span>// (eval (QUOTE (atom (quote A))) NIL )</span>
        <span>// (eval (QUOTE (cdr (cdr (quote (A B))))) NIL)</span>
        <span>// (eval (QUOTE (cons (quote a) (quote (a)))) NIL)</span>
        <span>// (eval (QUOTE ((lambda (x) (quote a)) (quote b))) NIL)</span>
        <span>(</span>eval <span>(</span><span>QUOTE</span> <span>(</span><span>(</span>lambda <span>(</span><span>X</span><span>)</span> <span>X</span><span>)</span> <span>(</span>quote a<span>)</span><span>)</span><span>)</span> <span>NIL</span><span>)</span>

        <span>)</span><span>;</span></pre></div>
<p dir="auto">It appears to work, but trying to evaluate <code>((lambda (X) X) (quote a))</code> in the interpreter takes more than 30 seconds and generates far more than 1 million+ tokens before cargo gets sigkilled. Using the explicit y combinator for recursion isn't particularly efficient here! To fix this, I should add an explicit recursion primitive.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Technical explanation</h2><a id="user-content-technical-explanation" aria-label="Permalink: Technical explanation" href="#technical-explanation"></a></p>
<p dir="auto">Look at EXPLANATION.md. The macro essentially simulates a SECD machine, which is a simple stack-basd abstract machine for evaulating lambda calculus terms.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Awesome resources</h2><a id="user-content-awesome-resources" aria-label="Permalink: Awesome resources" href="#awesome-resources"></a></p>
<ul dir="auto">
<li>Functional Programming: Application and Implementation by Peter Henderson</li>
<li>Ager, Mads Sig, et al. "A functional correspondence between evaluators and abstract machines." Proceedings of the 5th ACM SIGPLAN international conference on Principles and practice of declaritive programming. 2003.</li>
<li>The Implementation of Functional Programming Languages by Simon Peyton Jones</li>
<li>Anything Matt Might has ever written about lisp on his blog (<a href="https://matt.might.net/" rel="nofollow">https://matt.might.net</a>)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">TODO</h2><a id="user-content-todo" aria-label="Permalink: TODO" href="#todo"></a></p>
<ul dir="auto">
<li>Add letrec</li>
<li>Add recursive defines</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CrowdStrike ex-employees: 'Quality control was not part of our process' (445 pts)]]></title>
            <link>https://www.semafor.com/article/09/12/2024/ex-crowdstrike-employees-detail-rising-technical-errors-before-july-outage</link>
            <guid>41534716</guid>
            <pubDate>Fri, 13 Sep 2024 20:17:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.semafor.com/article/09/12/2024/ex-crowdstrike-employees-detail-rising-technical-errors-before-july-outage">https://www.semafor.com/article/09/12/2024/ex-crowdstrike-employees-detail-rising-technical-errors-before-july-outage</a>, See on <a href="https://news.ycombinator.com/item?id=41534716">Hacker News</a></p>
Couldn't get https://www.semafor.com/article/09/12/2024/ex-crowdstrike-employees-detail-rising-technical-errors-before-july-outage: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Janet Jackson had the power to crash laptop computers (2022) (164 pts)]]></title>
            <link>https://devblogs.microsoft.com/oldnewthing/20220816-00/?p=106994</link>
            <guid>41534483</guid>
            <pubDate>Fri, 13 Sep 2024 19:44:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devblogs.microsoft.com/oldnewthing/20220816-00/?p=106994">https://devblogs.microsoft.com/oldnewthing/20220816-00/?p=106994</a>, See on <a href="https://news.ycombinator.com/item?id=41534483">Hacker News</a></p>
Couldn't get https://devblogs.microsoft.com/oldnewthing/20220816-00/?p=106994: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI threatens to revoke o1 access for asking it about its chain of thought (469 pts)]]></title>
            <link>https://twitter.com/SmokeAwayyy/status/1834641370486915417</link>
            <guid>41534474</guid>
            <pubDate>Fri, 13 Sep 2024 19:43:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/SmokeAwayyy/status/1834641370486915417">https://twitter.com/SmokeAwayyy/status/1834641370486915417</a>, See on <a href="https://news.ycombinator.com/item?id=41534474">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[I won't be renewing my Pinboard subscription (168 pts)]]></title>
            <link>https://notes.kateva.org/2024/09/the-end-times-have-come-for-pinboardin.html</link>
            <guid>41533958</guid>
            <pubDate>Fri, 13 Sep 2024 18:47:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://notes.kateva.org/2024/09/the-end-times-have-come-for-pinboardin.html">https://notes.kateva.org/2024/09/the-end-times-have-come-for-pinboardin.html</a>, See on <a href="https://news.ycombinator.com/item?id=41533958">Hacker News</a></p>
Couldn't get https://notes.kateva.org/2024/09/the-end-times-have-come-for-pinboardin.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Meta fed its AI on everything adults have publicly posted since 2007 (214 pts)]]></title>
            <link>https://www.theverge.com/2024/9/12/24242789/meta-training-ai-models-facebook-instagram-photo-post-data</link>
            <guid>41533060</guid>
            <pubDate>Fri, 13 Sep 2024 17:05:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/9/12/24242789/meta-training-ai-models-facebook-instagram-photo-post-data">https://www.theverge.com/2024/9/12/24242789/meta-training-ai-models-facebook-instagram-photo-post-data</a>, See on <a href="https://news.ycombinator.com/item?id=41533060">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Meta has acknowledged that all text and photos that adult Facebook and Instagram users have publicly published since 2007 have been fed into its artificial intelligence models. <a href="https://www.abc.net.au/news/2024-09-11/facebook-scraping-photos-data-no-opt-out/104336170">Australia’s ABC News reports</a> that Meta’s global privacy director, Melinda Claybaugh, initially rejected claims about user data from 2007 being leveraged for AI training during a local government inquiry about AI adoption before relenting after additional questioning.</p><p>“The truth of the matter is that unless you have consciously set those posts to private since 2007, Meta has just decided that you will scrape all of the photos and all of the texts from every public post on Instagram or Facebook since 2007 unless there was a conscious decision to set them on private,” Green Party senator David Shoebridge pushed in the inquiry. “That’s the reality, isn’t it?”</p><p>“Correct,” Claybaugh responded.</p><p>Meta’s <a href="https://www.facebook.com/privacy/guide/generative-ai/">privacy center</a> and <a href="https://about.fb.com/news/2023/09/privacy-matters-metas-generative-ai-features/">blog posts</a> acknowledge hoovering up public posts and comments from Facebook and Instagram to train generative AI:</p><div><blockquote><p>We use public posts and comments on Facebook and Instagram to train generative AI models for these features and for the open source community.</p><p>We don’t use posts or comments with an audience other than Public for these purposes.</p></blockquote></div><p>But the company has been vague about how data is used, when it started scraping, and how far back its collection goes. Asked by <em>The New York Times</em> in June, Meta didn’t answer, other than to confirm that setting posts <a href="https://www.nytimes.com/2024/06/07/technology/meta-ai-scraping-policy.html">to anything besides “public” will prevent future scraping</a>. That still won’t delete data that has already been collected — and people posting back in 2007 (who may have been minors at the time) wouldn’t have known their photos and posts would be used in this way.</p><p>Claybaugh said that Meta doesn’t scrape data from users who are under the age of 18. When Labor Party senator Tony Sheldon asked if Meta would scrape the public photos of his children on his own account, Claybaugh confirmed it would and was unable to clarify if the company also scraped adult accounts that were created when the user was still a child.</p><p>European users can opt out <a href="https://www.theverge.com/2024/6/14/24178591/meta-ai-assistant-europe-ireland-privacy-objections">due to local privacy regulations</a>, and Meta was recently <a href="https://www.theverge.com/2024/7/3/24191405/meta-anpd-stop-training-ai-on-brazilian-facebook-instagram-data">banned from using Brazilian personal data</a> for AI training, but the billions of Facebook and Instagram users in other regions can’t opt out if they want to keep their posts public. Claybaugh was unable to say if Australian users (or anyone else) would be given a choice to opt out in the future, arguing that the option was given to European users because of <a href="https://www.theverge.com/2024/7/18/24201041/meta-multimodal-llama-ai-model-launch-eu-regulations">uncertainty regarding its regulatory landscape</a>.</p><p>“Meta made it clear today that if Australia had these same laws Australians’ data would also have been protected,” Shoebridge said to ABC News. “The government’s failure to act on privacy means companies like Meta are continuing to monetize and exploit pictures and videos of children on Facebook.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Added SVG Maps to My Travel Posts (102 pts)]]></title>
            <link>https://cyberb.space/notes/2024/how-i-added-maps-to-my-travel-posts/</link>
            <guid>41532958</guid>
            <pubDate>Fri, 13 Sep 2024 16:51:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cyberb.space/notes/2024/how-i-added-maps-to-my-travel-posts/">https://cyberb.space/notes/2024/how-i-added-maps-to-my-travel-posts/</a>, See on <a href="https://news.ycombinator.com/item?id=41532958">Hacker News</a></p>
Couldn't get https://cyberb.space/notes/2024/how-i-added-maps-to-my-travel-posts/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Zero-Click Calendar invite – Critical zero-click vulnerability chain in macOS (402 pts)]]></title>
            <link>https://mikko-kenttala.medium.com/zero-click-calendar-invite-critical-zero-click-vulnerability-chain-in-macos-a7a434fc887b</link>
            <guid>41532946</guid>
            <pubDate>Fri, 13 Sep 2024 16:50:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mikko-kenttala.medium.com/zero-click-calendar-invite-critical-zero-click-vulnerability-chain-in-macos-a7a434fc887b">https://mikko-kenttala.medium.com/zero-click-calendar-invite-critical-zero-click-vulnerability-chain-in-macos-a7a434fc887b</a>, See on <a href="https://news.ycombinator.com/item?id=41532946">Hacker News</a></p>
Couldn't get https://mikko-kenttala.medium.com/zero-click-calendar-invite-critical-zero-click-vulnerability-chain-in-macos-a7a434fc887b: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Defend against vampires with 10 gbps network encryption (210 pts)]]></title>
            <link>https://www.synacktiv.com/en/publications/defend-against-vampires-with-10-gbps-network-encryption</link>
            <guid>41531699</guid>
            <pubDate>Fri, 13 Sep 2024 14:42:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.synacktiv.com/en/publications/defend-against-vampires-with-10-gbps-network-encryption">https://www.synacktiv.com/en/publications/defend-against-vampires-with-10-gbps-network-encryption</a>, See on <a href="https://news.ycombinator.com/item?id=41531699">Hacker News</a></p>
<div id="readability-page-1" class="page"><div property="schema:text"><p>Let's say you have a fiber optic line running between two buildings, or between two spaces you rent in the same building. You use trunk ports on the switches connected to the fiber, in order to "stretch" your L2 Ethernet network and its different VLANs, so computers, servers, printers, cameras, etc. in both offices easily communicate with their peers.</p>
<p>But there is next to no physical security in the shared wiring closets and common cabling paths, and so you are concerned about someone <em>tapping</em>&nbsp;into the fiber line and capturing data.</p>
<p>First you may ask, is that really possible? And if so, is it easy to do? Would you need NSA-level hardware and skills or could anyone do it?</p>
<p>Turns out, it is both possible and easy, so you're right to worry! But there is a solution...</p>

<h3>Tapping on copper cables<br>
&nbsp;</h3>
<p>It's so simple to tap on copper network cables that back in the 80s, it was the official way to expand your 10BASE5 Ethernet network. IT people&nbsp;used so-called "vampire taps" that truly <em>bit</em> through the insulation of the cable to make contact with the conductors inside, enabling a new connection without interrupting ongoing data transmission.</p>
<figure role="group">
<img alt="photo of a 80s vampire tap showing bite marks on the cable" data-entity-type="file" data-entity-uuid="239417dc-b316-4b38-8a2e-a7a6589df581" src="https://www.synacktiv.com/sites/default/files/inline-images/vampiretap.jpg">
<figcaption>Vampire tap and 10BASE5 Ethernet cable showing "bite marks"</figcaption>
</figure>
<p><a href="https://www.tiktok.com/@davidbombal/video/7246772712696335643">This&nbsp;TikTok video</a> shows one of these devices in action.</p>
<p>Nowadays, people might still use copper cables instead of fiber for distances &lt; 100m, and <a href="https://en.wikipedia.org/wiki/Network_tap#Gigabit_Ethernet_issues">it's a bit more difficult to tap on 1000BASE-T modern cables</a>. Anyway, since our typical LAN-to-LAN scenario rather involves optical fibers running along not-well-protected cable runways, let's see if optical fibers are&nbsp;vulnerable too.</p>
<h3><br>
Tapping on optical fiber<br>
&nbsp;</h3>
<p>For a number of years it was commonly believed that tapping on optical fibers was possible, but that it needed both expertise and expensive hardware.</p>
<p>But in 2015, the late <a href="https://www.zdnet.com/article/infamous-hacker-kevin-mitnick-sniffs-fiber-reads-email/">Kevin Mitnick demonstrated just how easy it is to tap into a fiber and sniff the traffic</a>, using a 200$ optical "clip-on coupler".</p>
<p>These&nbsp;couplers are originally designed to be used by line technicians to talk to one another over long distances using so-called "optical talk sets", and&nbsp;coordinate with one another while installing these&nbsp;fibers, even when they have no access to the ends of the fiber.</p>
<p>They work by exposing a portion of the fiber core and bending it slightly,&nbsp;reflecting some 2-3% of the light inside the fiber (but still 100% of its data) and also allowing to inject light in it. Of course, it's better to have some practice and a steady hand in order to expose the fiber core without damaging the fiber.</p>
<p>In true hacker fashion, these&nbsp;couplers are creatively used by attackers to read data from the fiber, and inject data of their own.&nbsp;</p>
<figure role="group">
<img alt="clip-on coupler like the one used by Mitnick" data-entity-type="file" data-entity-uuid="f8ad2517-2643-41eb-b7d6-358848d41033" src="https://www.synacktiv.com/sites/default/files/inline-images/clip_on_coupler_small.jpg">
<figcaption>The&nbsp;FOD5516 Clip-on Coupler that can detect and inject light in (singlemode) optical fiber</figcaption>
</figure>
<p>This opens many classical man-in-the-middle attack scenarios such as forcing the downgrade&nbsp;of crypto protocols, redirecting traffic, etc. on top of simple sniffing.</p>

<h2>Encrypt ALL the things !<br>
&nbsp;</h2>
<p>So it seems there's not much you can do to prevent vampire tapping onto your easily-accessible cable paths. In fact, they might already be there...</p>
<p>As with any confidentiality issue, the best solution lies in encryption. If your whole network traffic is encrypted, the data vampires might still suck on your network cables, but instead of draining the very blood of your company, they will only gather encrypted gibberish of no use to them.</p>
<p>But how exactly can you encrypt "everything" in such a LAN-to-LAN scenario? And at what cost?</p>
<p>If you had routers on both ends of the fiber, with each site having its own dedicated IP zones with no overlap, you could easily setup a VPN tunnel like IPSec or Wireguard between the two routers and solve your problem.<br>
That said, if you used a "stretched L2" approach like in our scenario, it would not be easy to go back and re-segment&nbsp;all your networking, moving&nbsp;from bridging&nbsp;to&nbsp;routing traffic.<br>
You could try instead to enforce the policy that every applicative stream on your network, even print jobs, realtime video, DNS, etc. uses only a "secure" (encrypted) version of its protocol. It's certainly an impressive feat if you've managed to achieve that in a typical office environment ☺️.&nbsp;Even so, you'd likely still be interested into setting some kind of encrypted tunnel on the fiber, just in case someone sets up accidentaly some unsecure applicative stream...</p>
<p>The idea we had to solve such a problem, with the least disruption to existing networks and protocols, was to mix "802.1q trunk links" and VPN-like encryption between the two ends of the fiber. So&nbsp;instead of plugging the fiber directly into your switches, you plug them to an equipment with two network interfaces, that will on one end "swallow" all your 802.1q traffic (VLANs and all) and then transfer them over the fiber, through an encrypted tunnel, to a second "mirror" equipment that will decrypt the packets and transform them again into 802.1q frames and "spit them out" to its&nbsp;local network. This was dubbed the "wormhole" project.</p>
<h2>The quest for the encrypted trunk : MACsec</h2>
<p>If you search for ways to "encrypt/secure a 802.1q trunk" you will probably read about MACsec, aka the Cisco-designed 802.1ae standard, which on paper seems to do exactly what we want, with the added benefit that if you use MACsec-capable switches on each end of the fiber, you don't need additionnal equipements to do the secure tunneling.</p>
<p>MACsec creates point-to-point Secure Channels pairs (one for Tx, one for Rx) between two devices over an untrusted connection, using first a negotiation protocol called MKA (MACsec Key Agreement) and for example pre-shared keys (or a PKI). Over these Secure Channels, MACSec Frames are sent, which are only slightly modified Ethernet frames with their layer-3+ payload encrypted using GCM-AES-128 (or GCM-AES-256 in newer hardware). Since they are essentially Ethernet frames, MACSec can natively support 802.1q VLAN headers for example.</p>
<p>In order to test these promises, we bought a pair of the smallest Cisco switch that supports MACsec : the Catalyst 3560 CX WS-3560CX-8XPD-S, at ~1600€ each.</p>
<h3><br>
Testing MACsec</h3>
<p>Unfortunately, during our testing we found MACsec underwhelming for our LAN-to-LAN scenario.</p>
<p>First, it's difficult to find accurate MACsec documentation since it supports a variety of use cases (securing a link between a workstation and a switch, securing a link between two switches...) and as usual with Cisco, there are configuration directives that may or may not exist on a given Cisco switch, depending on its IOS image, feature level, generation, etc. So it was a bit of a pain to get it working for our trunk ports.</p>
<p>Then, we found out something that in hindsight was pretty clear in the MACsec specification&nbsp;: MACsec <em>does not hide the real MAC addresses of devices talking on your network</em> to the eavesdropping attacker. It may make sense, in order for traffic to be able go through MACsec-unaware bridges, to preserve the original MAC adresses of (for example) two computers talking to each other, but we expected to be able to see only the MAC Adresses of the two switches directly connected to the untrusted network and doing the MACsec tunnel. This could have been considered a minor inconvenience, but as MAC addresses are assigned by device manufacturers, it gives an attacker quite a good intel on what brand of computer, printer, appliances, etc. devices you have on your network, so that may help them plan a targeted attack. And if you're worried about industrial espionage, maybe you don't want your spying concurrent to know what brand of components you use on your R&amp;D VLANs either.</p>
<p>Last but not least, on several occasions during our offensive testing, we were able to mess with the MKA/MACsec traffic enough (using not-so-transparent software bridges and resetting MKA sessions on the switches ) so that half of the traffic (corresponding to one of the two secure channels) was being sent fully unencrypted, despite the <code>linksec policy must-secure</code> settings on the ports specifying to never send traffic in the clear on this interface according to Cisco documentation ("Must-Secure imposes that only MACsec encrypted traffic can flow. Hence, until the MKA session is secured, traffic is dropped."). Moreover, the switches were not reporting errors at all and happily continued sending/receiving clear traffic on MACsec must-secure interfaces. The end-user devices (laptops) had no way to know their traffic was being read, since their communications continued as usual (minus the loss of a few ICMP or UDP packets during MKA renegociation).</p>
<figure role="group">
<img alt="wireshard capture showing half the traffic in clear" data-entity-type="file" data-entity-uuid="7101fb11-ea1d-4bcb-81c4-10ca1c3fa7fb" src="https://www.synacktiv.com/sites/default/files/inline-images/pcap_macsec_clair.png">
<figcaption>Wireshark traffic capture : the ping replies are sent in clear over the MACsec link, although the ping echo requests are still sent encrypted.</figcaption>
</figure>
<p>This behaviour was observed with the most recent IOS firmware at that time, and although it then required both 1) an attacker on the fiber doing Man-in-the-Middle and dropping some frames, and 2) an admin action on the switch CLI itself (<code>clear mka sessions</code>), given that the MKA sessions have an expiry and must be renegociated after some time or when a port goes down, we think it's likely that there's a way to trigger that behavior only by "sitting on the wire" with no CLI access to the switch.</p>
<p>The bug tracker from Cisco showed bugs and behaviour that, although apparently not applying to our model, looked close to what we found:</p>
<ul>
<li><a href="https://bst.cisco.com/quickview/bug/CSCvx83835">"MACsec access-control must-secure is allowing the unencrypted traffic to pass through a link"</a></li>
<li><a href="https://bst.cisco.com/quickview/bug/CSCvw36505">"MACsec ports in Auth-pending state after changing to should secure policy with Empty keychain"</a></li>
<li><a href="https://bst.cisco.com/quickview/bug/CSCus74990">"MKA sessions struck in "pending" state after clear MKA sess"</a></li>
</ul>
<p>We had to drop there our experiments with our Cisco switches since the goal of this mission was to "find a reliable way to encrypt LAN to LAN fiber links at high speed", not to "break MACsec", but we may go back to it sometime in the future :)</p>
<h2>Native Linux solution : VXLAN+Wireguard</h2>
<p>Having set MACsec aside, we decided to PoC something that would use only native and well-known Linux kernel features on commodity hardware.</p>
<p>Since Wireguard is the state-of-the-art, in-kernel tunneling, was there a way to shove 802.1q L2 traffic inside a wireguard tunnel, and what speeds could be reached on customer-level hardware costing approximately the same as the Cisco switches we just tested ?</p>
<p>The "missing bit" to go from an L2 trunk to a L3 tunnel was solved by using VXLAN.&nbsp;</p>
<h3>VXLAN</h3>
<p>VXLAN is a protocol used to carry over L2 frames using UDP encapsulation (port 4789 or 8472 depending on the implementation) to a distant endpoint (called VTEP, for "VXLAN termination endpoint"). It works by hooking onto L2 forwarding tables (like on a Linux bridge), and sending to the remote VTEP the frames that must be broadcasted on the segment, along with the frames that have no local destination ("flood and learn"). The remote VTEP decapsulates the L2 frame from the UDP packet and sends it to its local network.</p>
<p>VXLAN real-world use case mostly involves "datacenter bridging", so that VMs from Datacenter 1 could behave like they are "on the same Ethernet segment" than VMs from Datacenter 2 so they can really share the same IP subnet (respond to ARP "who-has", etc).&nbsp;</p>
<p>Despite its name, VXLAN is only "inspired" by the concept of L2 VLANs : it is not something that will, out-of-the box, listen on a trunk port and carry 802.1q tagged frames to the remote endpoint. But it's very possible to do so using Linux wonderful networking stack ! You just need to to map each&nbsp;802.1q VLAN ID to a VXLAN "vid" and do the same thing in reverse on the other VTEP.</p>
<p>Most of the documentation you find online about injecting L2 VLAN info into a VXLAN&nbsp;tunnel has you creating a Linux bridge + a VLAN interface + a VXLAN interface <em>per VLAN </em>that&nbsp;you want to transmit.&nbsp;<br>
If you use all 4096 VLANs, or want to be able to add/drop VLANs on your network without having to do so many steps each time, you can use iproute2 commands <code>bridge</code> and <code>ip</code>&nbsp;with some recent feature flags (<code>vlan_filtering, vlan_default_pvid,&nbsp;vlan_tunnel + tunnel_info</code>)&nbsp;to spare yourself some time and have less clutter on your Linux "wormhole" boxes. The <code>vlan_default_pvid&nbsp;</code>flag is very important to be able to keep 802.1q headers "inside the bridge" and have them reach the vxlan interface where they will then be mapped to a vxlan <code>vid</code>.</p>
<p>With these commands you can have a single bridge and VXLAN interface that handles every VLAN coming its way.</p>
<pre><code># Create a Linux bridge with the right options
/sbin/ip link add br0 type bridge vlan_filtering 1 vlan_default_pvid 0 vlan_stats_enabled 1 vlan_stats_per_port 1

# Enslave the trunk eth interface (connected to the switch trunk port)
# to the bridge
/sbin/ip link set dev ${TRUNK_IFACE} master br0

# Create a vxlan interface and enslave it to the same bridge
/sbin/ip link add vxlan0 type vxlan vni ${VXLAN_DEFAULT_VNI} local ${VTEP_LOCALIP} remote ${VTEP_REMOTEIP} dstport 4789
/sbin/ip link set dev vxlan0 master br0

# Activate vlan tunneling !
/sbin/bridge link set dev vxlan0 vlan_tunnel on</code></pre>
<p>Then, adding a specific VLAN ID to the bridge so its extracted and mapped to a VXLAN vid requires these 3 commands</p>
<pre><code>/sbin/bridge vlan add vid $vid dev ${TRUNK_IFACE}
/sbin/bridge vlan add vid $vid dev vxlan0
/sbin/bridge vlan set dev vxlan0 vid $vid tunnel_info id $vid</code></pre>
<p>You can easily run a little script to do the mapping once and for all for every possible VLAN ID :</p>
<pre><code># extract vlans from trunk, map them to same vxlan vid
for vid in $(seq 2 4095); do
/sbin/bridge vlan add vid $vid dev ${TRUNK_IFACE}
/sbin/bridge vlan add vid $vid dev vxlan0
/sbin/bridge vlan set dev vxlan0 vid $vid tunnel_info id $vid
done</code></pre>

<h3>Wireguard</h3>
<p><br>
Plugging this VXLAN configuration to a wireguard tunnel is surprisingly easy. We won't be covering setting up a wireguard tunnel between two Linux hosts since there are many great resources online (and it just&nbsp;works out of the box).</p>
<p>Indeed, if you've already set up a wg0 interface over the (insecure) fiber connection, with (secure) IP addresses for both of your "wormholes", you can specify the remote peer's wireguard IP address directly as ${VTEP_REMOTEIP}. Linux in-kernel networking will then do its magic and dutifully forward over the wire your 802-1q frames, encapsulated in UDP VXLAN, and encapsulated again in UDP Wireguard.</p>
<p>Having read that, you might worry like we did at the potential performance cost of such many-levels of encapsulation and encryption on top of it.</p>

<h2>Performance&nbsp;</h2>
<p><img alt="scheme of the multiple layers of encapsulation between the wormholes" data-entity-type="file" data-entity-uuid="3c48c427-11f2-416a-b2fe-c228fc8c3a54" src="https://www.synacktiv.com/sites/default/files/inline-images/max-encap.png"></p><p>So, the final packets that will transit "on the wire" between our wormholes will end up looking something&nbsp;&nbsp;like&nbsp;<code>Eth/IP/UDP/WG/IP/UDP/VXLAN/Eth/802.1q/IP/Payload</code>. That's quite an overhead indeed!</p>
<p>But this overhead is only present during the&nbsp;transit on the fiber, which on our&nbsp;scenario is a local, short-distance (read : low latency) optical fiber, typically using at least&nbsp;10 Gbps SFP+ optical transceivers. So the latency and bandwidth should be good, and we will be able to maximize the Maximum Transfer Unit (MTU) on the fiber interface ports, so that a typical Ethernet data payload of 1500 bytes will be able to&nbsp;"sit" easily in the payload of our jumbo wireguard+vxlan frame :&nbsp;&nbsp;there will be&nbsp;no need for segmentation and retransmits.&nbsp;</p>
<p>Regarding wireguard encryption, we did a little research and felt confident after reading&nbsp;<a href="https://restoreprivacy.com/optimizations-in-wireguard-achieve-record-10gbit-sec-throughput/">resources</a> &nbsp;that we could reach high speeds with the right hardware offloads, altough 10 Gbps seemed like the "record".</p>
<p>The corresponding <code>ethtool</code> vars that matched offloads that enabled good performance for VXLAN+wireguard were determined to be :&nbsp;<code>tx-udp_tnl-segmentation,&nbsp;generic-segmentation-offload,&nbsp;generic-receive-offload,&nbsp;rx-vlan-offload</code> and&nbsp;<code>tx-vlan-offload</code>.</p>
<p>The next step was to find server hardware that had&nbsp;10 Gbps SFP+ ports with&nbsp;<strong>UDP Segmentation Offload</strong>&nbsp;(Generic Segmentation Offload),&nbsp;<strong>UDP Receive Coalescing</strong>&nbsp;(Generic Receive Offload) and <strong>VXLAN offloading</strong> capabilites</p>
<p>The search was over when we found out about SuperMicro SuperServer&nbsp;5019D-4C-FN8TP, that had everything we needed with an&nbsp;Intel Xeon D-2123IT&nbsp;SoC that directly handles 2x 10Gbps SFP+ and 2x 10Gbps base-T ports, both the CPU and NICs supporting the aforementioned offload instructions. It costed about 1300€ (you then have to add ECC RAM and local hard drive yourself).</p>
<p><img alt="picture of chosen supermicro server" data-entity-type="file" data-entity-uuid="ff622867-34b4-409c-b904-49fc587e2ef8" src="https://www.synacktiv.com/sites/default/files/inline-images/supermicro.png"></p><h3>Test setup</h3>
<p>Our test setup was like this :</p>
<p><img alt="schema of our test setup described below" data-entity-type="file" data-entity-uuid="592259de-dcc4-40cd-bac4-bb474ba78e6b" src="https://www.synacktiv.com/sites/default/files/inline-images/maquette_0.png"></p><p>We planned to measure throughput using <code>iperf3</code> in the following conditions:</p>
<ul>
<li>Between the Supermico wormholes on the untrusted fiber</li>
<li>Between the&nbsp;Supermico wormholes through the wireguard tunnel built over the untrusted fiber (to measure wireguard encryption penalty)</li>
<li>Between a pair of two 1 Gbps laptops, each on one side&nbsp;of the wormholes, to measure end-device to end-device performance</li>
<li>Between <em>two</em> pairs of such laptops, each pair on a different VLAN and simultaneously trying to use their 1 Gbps max bandwidth, just&nbsp;to be sure we were scaling... and this was quite a good intuition to do this test, as you will read further.</li>
</ul>
<p>We also measured the base performance of the four laptops with a classical setup (just the network switches linked by a trunk port, no vxlan or wireguard, no "wormholing") : they were able to reach 942 Mbps.</p>
<p>Just out-of-the box, with no particular tuning, here were the first&nbsp;tests results:</p>
<ul>
<li>9.81 Gbits/sec between the&nbsp;Supermicros on&nbsp;untrusted fiber (no crypto) - so it seems we really did buy 10 Gbps-capable NICs!&nbsp;nice</li>
<li>8.18 Gbits/sec between the Supermicros on&nbsp;wg0 (AES) - a 17% performance penalty, seemed to be expected from encrypting...</li>
<li>874 Mbits/sec between two&nbsp;laptops (compared to 942 Mbps in a classical setup) - a 8% performance "end-user" penalty, did not seem so bad</li>
<li>But only 658 Mbits/sec when there&nbsp;were 4 laptops each connected in pairs (again compared to 942 Mbps for each pair) - oops, something seemed off!&nbsp;</li>
</ul>
<p>It looked like we were stalling somewhere around a ~1 Gbps shared bandwith for every end-user devices. Looked like a waste of our "next to 10 Gbps" bandwidth, surely there was something to do about it.</p>
<h3>Tuning Linux networking</h3>
<p>So we activated all the network-related tuning we had thought off beforehand:</p>
<pre><code># enable Jumbo frames (9000 bytes MTU) on 10 Gbps fiber
/sbin/ip li set dev ${UNTRUSTED_IFACE} mtu 9000

# https://cromwell-intl.com/open-source/performance-tuning/ethernet.html
/sbin/ip link set dev ${UNTRUSTED_IFACE} txqueuelen 13888 
/sbin/ethtool -G ${UNTRUSTED_IFACE} rx 4096 tx 4096

# setup a 8020 MTU on wg0 interface to account for the 80 bytes wireguard headers overhead
# 20-byte IPv4 header or 40 byte IPv6 header,&nbsp;8-byte UDP header&nbsp;&nbsp;4-byte type,&nbsp;4-byte key index,&nbsp;8-byte nonce,&nbsp;16-byte authentication tag)
/sbin/ip li set dev wg0 mtu 8020</code></pre>
<p>we added some <code>sysctl</code> tuning for 10Gbps ethernet as well:</p>
<pre><code># Maximum receive socket buffer size
net.core.rmem_max = 134217728 

# Maximum send socket buffer size
net.core.wmem_max = 134217728 

# Minimum, initial and max TCP Receive buffer size in Bytes
net.ipv4.tcp_rmem = 4096 87380 134217728 

# Minimum, initial and max buffer space allocated
net.ipv4.tcp_wmem = 4096 65536 134217728 

# Maximum number of packets queued on the input side
net.core.netdev_max_backlog = 300000 

# Auto tuning
net.ipv4.tcp_moderate_rcvbuf =1

# Don't cache ssthresh from previous connection
net.ipv4.tcp_no_metrics_save = 1

# If you are using jumbo frames set this to avoid MTU black holes.
net.ipv4.tcp_mtu_probing = 1</code></pre>

<p>After that we had the following test results:</p>
<ul>
<li>9.91&nbsp;Gbits/sec between the&nbsp;Supermicros on&nbsp;untrusted fiber (no crypto) - 100 Mbps better than&nbsp;9.81 Gbits/sec !</li>
<li>8.41&nbsp;Gbits/sec between the Supermicros on&nbsp;wg0 (wireguard) - more than 200 Mbps better than&nbsp;8.18 Gbits/sec !!</li>
<li>942 Mbits/sec between two&nbsp;laptops - now equivalent to the legacy setup without wormholes - yay !</li>
<li>still 658 Mbits/sec when they were 4 laptops each connected in pairs - something's still off...</li>
</ul>
<p>Having tuned everything network-related we could think of, it then occured to us that a good part of the networking ( bridging, forwarding, encapsulating) was really done in-kernel (read: "in-memory") and not on NICs. So surely some default Linux performance setting was stalling us around ~ 1 Gbps .</p>
<h3>Last tunables</h3>
<p>We then set the CPU governor to <code>performance</code> and tuned virtual memory <code>sysctl</code>s to match what RedHat's <code>tuned</code> tool does when setting the profile <code>throughput-performance</code>:</p>
<pre><code>vm.dirty_ratio = 40
vm.dirty_background_ratio = 10
vm.swappiness=10
# set cpu/power options
governor=performance
energy_perf_bias=performance
min_perf_pct=100</code></pre>
<p>And<em> lo&nbsp;and behold</em>, here were the tests results:</p>
<ul>
<li>9.86&nbsp;Gbits/sec between the&nbsp;Supermicros on&nbsp;untrusted fiber (no crypto) - bit lower than previous test</li>
<li><strong>9.71 Gbits/sec</strong> between the Supermicros on&nbsp;wg0 &nbsp;- the crypto impact was becoming negligible!</li>
<li>942 Mbits/sec between two&nbsp;laptops (still equivalent to the legacy setup without wormholes)</li>
<li>finally 942 Mbits/sec even when they were 4 laptops each connected in pairs - job done!</li>
</ul>
<p>In fact, the performance was so great we felt the need to vampire tap the&nbsp;fiber to make sure everythink was still encrypted - and it&nbsp;still was ;-)</p>
<h2>Conclusion</h2>
<p>So, we were able to build a fully open-source pair of appliances that will strongly encrypt a 10 Gbps 802.1q trunk at almost wire-speed (less than 2% performance penalty), defeating any spying vampire tapping onto the underlying network link. And this appliance costs less than a flagship smartphone.</p>
<p>This truly speaks levels about the performance of today's affordable server hardware&nbsp;and the maturity of Linux networking stack. You can chain network technologies like trunking, bridging, routing, VXLAN and Wireguard almost like you chain CLI commands in true UNIX fashion, and the kernel makes it "just work". It just takes quite a bit of time and trial &amp; error to find the right feature flags and tuning settings so you can get the most out of it. We hope this blog article will do its part as well for future researchers.</p>
<p>If you liked this article and building secure-yet-performant infrastructure, speak French and are living near Paris, note that we are hiring an experienced profile to join&nbsp;our Infrastructure team ! More info (in French) :&nbsp;<a href="https://www.synacktiv.com/apt-search-sysadmin">https://www.synacktiv.com/apt-search-sysadmin</a></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Porting SBCL to the Nintendo Switch (356 pts)]]></title>
            <link>https://reader.tymoon.eu/article/437</link>
            <guid>41530783</guid>
            <pubDate>Fri, 13 Sep 2024 12:53:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reader.tymoon.eu/article/437">https://reader.tymoon.eu/article/437</a>, See on <a href="https://news.ycombinator.com/item?id=41530783">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      
        <header>
          
        </header>
        <div id="article-body">
          <blockquote><article><figure><a href="https://filebox.tymoon.eu//file/TWpjNU5nPT0=" target="_blank"><img alt="https://filebox.tymoon.eu//file/TWpjNU5nPT0=" src="https://filebox.tymoon.eu//file/TWpjNU5nPT0="></a></figure><p>For the past two years Charles Zhang and I have been working on getting my game engine, Trial, running on the Nintendo Switch. The primary challenge in doing this is porting the underlying Common Lisp runtime to work on this platform. We knew going into this that it was going to be hard, but it has proven to be quite a bit more tricky than expected. I'd like to outline some of the challenges of the platform here for posterity, though please also understand that due to Nintendo's NDA I can't go into too much detail.</p><h2 id="current status">Current Status</h2><p>I want to start off with where we are at, at the time of writing this article. We managed to port the runtime and compiler to the point where we can compile and execute arbitrary lisp code directly on the Switch. We can also interface with shared libraries, and I've ported a variety of operating system portability libraries that Trial needs to work on the Switch as well.</p><p>The above photo shows Trial's REPL example running on the Switch devkit. Trial is setting up the OpenGL context, managing input, allocating shaders, all that good stuff, to get the text shown on screen; the Switch does not offer a terminal of its own.</p><figure><iframe width="100%" height="240" frameborder="no" allowfullscreen="yes" src="//www.youtube.com/embed/zLoEkvnoGKY?"></figure><p>Unfortunately it also crashes shortly after as SBCL is trying to engage its garbage collector. The Switch has some unique constraints in that regard that we haven't managed to work around quite yet. We also can't output any audio yet, since the C callback mechanism is also broken. And of course, there's potentially a lot of other issues yet to rear their head, especially with regards to performance.</p><p>Whatever the case, we've gotten pretty far! This work hasn't been free, however. While I'm fine not paying myself a fair salary, I can't in good conscience have Charles invest so much of his valuable time into this for nothing. So I've been paying him on a monthly basis for all the work he's been doing on this port. Up until now that has cost me ~17'000 USD. As you may or may not know, I'm self-employed. All of my income stems from sales of <a class="external-link" href="https://kandria.com">Kandria</a> and donations from generous supporters on <a class="external-link" href="https://patreon.com/shinmera">Patreon</a>, <a class="external-link" href="https://github.com/sponsors/shinmera">GitHub</a>, and <a class="external-link" href="https://ko-fi.com/shinmera">Ko-Fi</a>. On a good month this totals about 1'200 USD. On a bad month this totals to about 600 USD. That would be hard to get by in a cheap country, and it's practically impossible in Zürich, Switzerland.</p><p>I manage to get by by living with my parents and being relatively frugal with my own personal expenses. Everything I actually earn and more goes back into hiring people like Charles to do cool stuff. Now, I'm ostensibly a game developer by trade, and I am working on a currently unannounced project. Games are very expensive to produce, and I do not have enough reserves to bankroll it anymore. As such, it has become very difficult to decide what to spend my limited resources on, and especially a project like this is much more likely to be axed given that I doubt Kandria sales on the Switch would even recoup the porting costs.</p><p>To get to the point: if you think this is a cool project and you would like to help us make the last few hurdles for it to be completed, please consider supporting me on <a class="external-link" href="https://patreon.com/shinmera">Patreon</a>, <a class="external-link" href="https://github.com/sponsors/shinmera">GitHub</a>, or <a class="external-link" href="https://ko-fi.com/shinmera">Ko-Fi</a>. On Patreon you get news for every new library I release (usually at least one a month) and an exclusive monthly roundup of the current development progress of the unannounced game. Thanks!</p><h2 id="an overview">An Overview</h2><p>First, here's what's publicly known about the Switch's environment: user code runs on an ARM64 Cortex-A57 chip with four cores and 4 GB RAM, and on top of a proprietary microkernel operating system that was initially developed for the Nintendo 3Ds.</p><p>SBCL already has an ARM64 Linux port, so the code generation side is already solved. Kandria also easily fits into 4GB RAM, so there's no issues there either. The difficulties in the port reside entirely in interfacing with the surrounding proprietary operating system of the switch. The system has some constraints that usual PC operating systems do not have, which are especially problematic for something like Lisp as you'll see in the next section.</p><p>Fortunately for us, and this is the reason I even considered a port in the first place, the Switch is also the only console to support the OpenGL graphics library for rendering, which Trial is based upon. Porting Trial itself to another graphics library would be a gigantic effort that I don't intend on undertaking any time soon. The Xbox only supports DirectX, though supposedly there's an OpenGL -&gt; DirectX layer that Microsoft developed, so that <em>might</em> be possible. The Playstation on the other hand apparently still sports a completely proprietary graphics API, so I don't even want to think about porting to that platform.</p><p>Anyway, in order to get started developing I had to first get access. I was lucky enough that Nintendo of Europe is fairly accommodating to indies and did grant my request. I then had to buy a devkit, which costs somewhere around 400 USD. The devkit and its SDK only run on Windows, which isn't surprising, but will also be a relevant headache later.</p><p>Before we can get on to the difficulties in building SBCL for the Switch, let's first take a look at how SBCL is normally built on a PC.</p><h2 id="building sbcl">Building SBCL</h2><p>SBCL is primarily written in Lisp itself. There is a small C runtime as well, which you use a usual C compiler to compile, but before it can do that, there's some things it needs to know about the operating system environment it compiles for. The runtime also doesn't have a compiler of its own, so it can't compile any Lisp code. In order to get the whole process kicked off, SBCL requires another Lisp implementation to bootstrap with, ideally another version of itself.</p><p>The build then proceeds in roughly five phases:</p><ol><li value="1"><p><code>build-config</code><br/>This step just gathers whatever build configuration options you want for your target and spits them out into a readable format for the rest of the build process.</p></li><li value="2"><p><code>make-host-1</code></p><p>Now we build the cross-compiler with the host Lisp compiler, and at the same time emit C header files describing Lisp object layouts in memory as C structs for the next step.</p></li><li value="3"><p><code>make-target-1</code></p><p>Next we run the target C compiler to create the C runtime. As mentioned, this uses a standard C compiler, which can itself be a cross-compiler. The C runtime includes the garbage collector and other glue to the operating system environment. This step also produces some constants the target Lisp compiler and runtime needs to know about by using the C compiler to read out relevant operating system headers.</p></li><li value="4"><p><code>make-host-2</code></p><p>With the target runtime built, we build the target Lisp system (compiler and the standard library) using the Lisp cross-compiler built by the Lisp host compiler in <code>make-host-1</code>. This step produces a &quot;cold core&quot; that the runtime can jump into, and can be done purely on the host machine. This cold core is not complete, and needs to be executed on the target machine with the target runtime to finish bootstrapping, notably to initialize the object system, which requires runtime compilation. This is done in</p></li><li value="5"><p><code>make-target-2</code></p><p>The cold core produced in the last step is loaded into the target runtime, and finishes the bootstrapping procedure to compile and load the rest of the Lisp system. After the Lisp system is loaded into memory, the memory is dumped out into a &quot;warm core&quot;, which can be loaded back into memory in a new process with the target runtime. From this point on, you can load new code and dump new images at will.</p></li></ol><p>Notable here is the need to run Lisp code on the <em>target machine</em> itself. We can't cross-compile &quot;purely&quot; on the host, not in the least because user Lisp code cannot be compiled without also being run like batch-compiled C code can, and when it is run it assumes that it is in the target environment. So we really don't have much of a choice in the matter.</p><p>In order to deploy an application, we proceed similar to <code>make-target-2</code>: We compile in Lisp code incrementally and then when we have everything we need we dump out a core with the runtime attached to it. This results in a single binary with a data blob attached.</p><p>When the SBCL runtime starts up it looks for a core blob, maps it into memory, marks pages with code in them as executable, and then jumps to the entry function the user designated. This all is a problem for the Switch.</p><h2 id="building for the switch">Building for the Switch</h2><p>The Switch is not a PC environment. It doesn't have a shell, command line, or compiler suite on it to run the build as we usually do. Worse still, its operating system does not allow you to create executable pages, so even if we could run the compilation steps on there we couldn't incrementally compile anything on it like we usually do for Lisp code.</p><p>But all is not lost. Most of the code is not platform dependent and can simply be compiled for ARM64 as usual. All we need to do is make sure that anything that touches the surrounding environment in some way knows that we're actually trying to compile for the Switch, then we can use another ARM64 environment like Linux to create our implementation.</p><p>With that in mind, here's what our steps look like:</p><ol><li value="1"><p><code>build-config</code><br/>We run this on some host system, using a special flag to indicate that we're building for the Switch. We also enable the <code>fasteval</code> contrib. We need <code>fasteval</code> to step in for any place where we would usually invoke the compiler at runtime, since we absolutely cannot do that on the Switch.</p></li><li value="2"><p><code>make-host-1</code></p><p>This step doesn't change. We just get different headers that prep for the Switch platform.</p></li><li value="3"><p><code>make-target-1</code></p><p>Now we use the C compiler the Nintendo SDK provides for us, which can cross-compile for the Switch. Unfortunately the OS is not POSIX compliant, so we had to create a custom runtime target in SBCL that stubs out and papers over the operating system environment differences that we care about, like dynamic linking, mapping pages, and so on.<br/>Here is where things get a bit weird. We are now moving on to compiling Lisp code, and we want to do so on a Linux host system. So we have to...</p></li><li value="4"><p><code>build-config</code> (2)</p><p>We now create a normal ARM64 Linux system with the same feature set as for the Switch. This involves the usual steps as before, though with a special flag to inform some parts of the Lisp process that we're going to ultimately target the Switch.</p></li><li value="5"><p><code>make-host-1</code> (2)</p></li><li value="6"><p><code>make-target-1</code> (2)</p></li><li value="7"><p><code>make-host-2</code></p></li><li value="8"><p><code>make-target-2</code></p><p>With all of this done we now have a slightly special SBCL build for Linux ARM64. We can now move on to compiling user code.</p></li><li value="9"><p>For user code we now perform some tricks to make it think it's running on the Switch, rather than on Linux. In particular we modify <code>*features*</code> to include <code>:nx</code> (the Switch code name) and not <code>:linux</code>, <code>:unix</code>, or <code>:posix</code>. Once that is set up and ASDF has been neutered, we can compile our program (like Trial) &quot;as usual&quot; and at the end dump out a new core.</p></li></ol><p>We've solved the problem of actually compiling the code, but we still need to figure out how to get the code started on the Switch, since it does not allow us to do the usual core-mapping strategy. As such, attaching the new core to the runtime we made for the Switch won't work.</p><p>To make this work, we make use of two relatively unknown features of SBCL: immobile-code, and elfination. Usually when SBCL compiles code at runtime, it sticks it into a page somewhere, and marks that page executable. The code itself however could become unneeded at some point, at which point we'd like to garbage collect it. We can then reclaim the space it took up, and to do so compact the rest of the code around it. The immobile-code feature allows SBCL to take up a different strategy, where code is put into special reserved code pages and remains there. This means it can't be garbage collected, but it instead can take advantage of more traditional operating system support. Typically executables have pre-marked sections that the operating system knows to contain code, so it can take care of the mapping when the program is started, rather than the program doing it on its own like SBCL usually does.</p><p>OK, so we can generate code and prevent it from being moved. But we still have a core at the end of our build that we now need to transform into the separate code and data sections needed for a typical executable. This is done with the elfination step.</p><p>The elfinator looks at a core and performs assembly rewriting to make the code position-independent (a requirement for Address Space Layout Randomisation), and then tears it out into two separate files, a pure code assembly file, and a pure data payload file.</p><p>We can now take those two files and link them together with the runtime that the C compiler produced and get a completed SBCL that runs like any other executable would. So here's the last steps of the build process:</p><ol><li value="10"><p>Run the elfinator to generate the assembly files</p></li><li value="11"><p>Link the final binary</p></li><li value="12"><p>Run the Nintendo SDK's authoring tools to bundle metadata, shared libraries, assets, and the application binary into one final package</p></li></ol><p>That's quite an involved build setup. Not to mention that we need at least an ARM64 Linux machine to run most of the build on, as well as either an AMD64 Windows machine (or an AMD64 Linux machine with Wine) to run the Nintendo SDK compiler and authoring tools.</p><p>I usually use an AMD64 Linux machine, so there's a total of three machines involved: The AMD64 &quot;driver,&quot; the ARM64 build host, and a Windows VM to talk to the devkit with.</p><p>I wrote a special build system with all sorts of messed up caching and cross-machine synchronisation logic to automate all of this, which was quite a bit of work to get going, especially since the build should also be drivable from an MSYS2/Windows setup. Lots of fun with path mangling!</p><p>So now we have a full Lisp system, including user code, compiling for and being able to run on the Switch. Wow! I've skipped over a lot of the nitty-gritty dealing with getting the build properly aware of which target it's building for, making the elfinator and immobile-code working on ARM64, and porting all of the support libraries like pathname-utils, libmixed, cl-gamepad, etc. Again, most of the details we can't openly talk about due to the NDA. However, we have upstreamed what work we could, and all of the Lisp libraries don't have a private fork.</p><p>It's worth noting though that elfination wasn't initially designed to produce position independent executable Lisp code, which is usually full of absolute pointers. So we needed to do a lot of work in the SBCL compiler and runtime to support load time relocation of absolute pointers and make sure code objects (which usually contain code constants) no longer have absolute pointers, as the GC can't modify executable sections. Not even the OS loader is allowed to modify executable sections to relocate absolute pointer. We did this by relocating absolute pointers like code constants outside of the text space into a read-writable space close enough to rewrite constant references in code to load from this r/w space instead, which the loader and the moving GC can fixup pointers at.</p><p>Instead of interfacing directly with the Nintendo SDK, I've opted to create my own C libraries that have a custom interface the Lisp libraries interface with in order to access the operating system functionality it needs. That way I can at least publish the Lisp bits openly, and only keep the small C library private. Anyway, now that we can run stuff we're not done yet. Our system actually needs to keep running, too, and that brings us to</p><h2 id="the garbage collector">The Garbage Collector</h2><p>Garbage collection is a huge topic in itself and there's a ton of different techniques to make it work efficiently. The standard GC for SBCL is called &quot;gencgc&quot;, a Generational Garbage Collector. Generational meaning it keeps separate &quot;generations&quot; of objects and scans the generations in different frequencies, copying them over to another generation's location to compact the space. None of this is inherently an issue for the Switch, if it weren't for multithreading.</p><p>When multiple threads are involved, we can't just move objects around, as another thread could be accessing it at any time. The easiest way to resolve this conflict is to park all threads before engaging garbage collection. So the question becomes: when a thread wants to start garbage collection, how does it get the other threads to park?</p><p>On Unix systems a pretty handy trick is used: we can use the signalling mechanism to send a signal to the other threads, which then take that hint to park.</p><p>On the Switch we don't have any signal mechanism. In fact, we can't interrupt threads at all. So we instead need to somehow get each thread to figure out that it should park on its own. The typical strategy for this is called &quot;safepoints&quot;.</p><p>Essentially we modify the compiler a little bit to inject some extra code that checks whether the thread should park or not. This strategy has some issues, namely:</p><ul><li><p>Adding a check isn't free. So we want to check as little as possible</p></li><li><p>If we don't check frequently enough, we are going to stall all the other threads because GC can't begin until they're all parked</p></li><li><p>If we have to inject a lot of instructions for a check, it is going to disrupt CPU cache lines and pipelining optimisations</p></li></ul><p>The current safepoint system in SBCL was written for Windows, which similarly does not have inter-process signal handlers. However, unlike the Switch, it <em>does</em> still have signal handling for the current thread. So the current safepoint implementation was written with this strategy:</p><p>Each thread keeps a page around that a safepoint just writes a word to. When GC is engaged, those pages are marked as read-only, so that when the safepoint is hit and the other thread tries to write to the page, a segmentation fault is triggered and the thread can park. This is efficient, since we only need a single instruction to write into the page.</p><p>On the Switch we can't use this trick either, so we have to actually insert a more complex check, which can be tricky to get working as intended, as all parallel algorithms tend to be.</p><p>Since safepoints aren't necessary on any other platform than Windows, it also hasn't been tested anywhere else, so aside from modifying it for this new platform it's also just unstable. It is apparently quite a big mess in the code base and would ideally be redone from scratch, but hopefully we don't have to go quite that far.</p><p>I'd also like to give special mention to the issue that CLOS presents. Usually SBCL defers compilation of the &quot;discriminating function&quot; that is needed to dispatch to methods to the first call of the generic function. This is done because CLOS is highly dynamic and allows adding and removing methods pretty much at any time, and there's usually no good point in time that the system knows it is complete. Of course, on the Switch we can't invoke the compiler, so we can't really do this. For now our strategy has been to instead rely on the fast evaluator. We stub out the <code>compile</code> function to create a lambda that executes the code via the evaluator instead. This has the advantage of working with any user code that relies on <code>compile</code> as well, though it is obviously much slower for execution than it would be if we could actually compile.</p><p>This neatly brings us to</p><h2 id="future work">Future Work</h2><p>The fasteval trick is mostly a fallback. Ideally I'd like to explore options to freeze as much of CLOS in place as possible right before the final image is dumped and compile as much as possible ahead of time. I'd also like to investigate the block compilation mode that Charles restored some years back more closely.</p><p>It's very possible that the Switch's underpowered processor will also force us to implement further optimisations, especially on the side of my engine and the code in Kandria itself. Up until now I've been able to get away with comparatively little optimisation, since even computers of ten years ago are more than fast enough to run what I need for the game. However, I'm not so sure that the Switch could match up to that even if it didn't also introduce additional constraints on performance with its lack of operating system support.</p><p>First, though, we need to get the garbage collector running fully. It runs enough to boot up and get into Trial's main loop, but as soon as it hits multi-generation compaction, it falls flat on its face.</p><p>Next we need to get callbacks from C working again. Apparently this is a part of the SBCL codebase that can only be described as &quot;a mess,&quot; involving lots of hand-rolled assembly routines, which probably need some adjustments to work correctly with immobile-code and elfination. Callbacks fortunately are relatively rare, Trial only needs them for sound playback via libmixed.</p><p>There's also been some other issues that we've kept in the back of our heads but don't require our immediate attention, as well as some extra portability features I know I'll have to work on in Trial before its selftest suite fully passes on the Switch.</p><h2 id="conclusion">Conclusion</h2><p>I'll be sure to add an addendum here should the state of the port significantly change in the future. Some people have also asked me if the work could be made public, or if I'd be willing to share it.</p><p>The answer to that is that while I would desperately like to share it all publicly, the NDA prevents us from doing so. We still upstream and publicise whatever we can, but some bits that tie directly into the Nintendo SDK cannot be shared with anyone that hasn't <em>also</em> signed the NDA. So, in the very remote possibility that someone other than me is crazy enough to want to publish a Common Lisp game on the Nintendo Switch, they can reach out to me and I'll happily give them access to our porting work once the NDA has been signed.</p><p>Naturally, I'll also keep people updated more closely on what's going on in the monthly updates for Patrons. With that all said, I once again plead with you to consider supporting me on <a class="external-link" href="https://patreon.com/shinmera">Patreon</a>, <a class="external-link" href="https://github.com/sponsors/shinmera">GitHub</a>, or <a class="external-link" href="https://ko-fi.com/shinmera">Ko-Fi</a>. All the income from these will, for the foreseeable future, be going towards funding the SBCL port to the Switch as well as the current game project.</p><p>Thank you as always for reading, and I hope to share more exciting news with you soon!</p></article></blockquote>
          <div class="notice">
            Written by <a href="https://user.tymoon.eu/shinmera/" id="author-avatar" title="shinmera">shinmera</a>
          </div>
        </section>
      
      <footer>
        <nav id="move">
          
            <a class="prev" href="https://reader.tymoon.eu/article/436">SRS, Zürich 2024 Edition</a>
          
          
        </nav>
        <nav id="default-linkage">
          <ul/>
        </nav>
      </footer>
    </article>
  </body>
</html>
</iframe></figure></article></blockquote></div></article></div>]]></description>
        </item>
    </channel>
</rss>