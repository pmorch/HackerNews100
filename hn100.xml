(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 21 Apr 2025 05:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Pete Hegseth shared Yemen attack details in second Signal chat (102 pts)]]></title>
            <link>https://www.theguardian.com/us-news/2025/apr/20/pete-hegseth-signal-chat-yemen-attack</link>
            <guid>43747310</guid>
            <pubDate>Sun, 20 Apr 2025 23:27:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/2025/apr/20/pete-hegseth-signal-chat-yemen-attack">https://www.theguardian.com/us-news/2025/apr/20/pete-hegseth-signal-chat-yemen-attack</a>, See on <a href="https://news.ycombinator.com/item?id=43747310">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Before the US launched military strikes on <a href="https://www.theguardian.com/world/yemen" data-link-name="in body link">Yemen</a> in March, <a href="https://www.theguardian.com/us-news/pete-hegseth" data-link-name="in body link">Pete Hegseth</a>, the defense secretary, sent detailed information about the planned attacks to a private Signal group chat that he created himself, which included his wife, his brother and about a dozen other people, <a href="https://www.nytimes.com/2025/04/20/us/politics/hegseth-yemen-attack-second-signal-chat.html?smid=nytcore-ios-share&amp;referringSource=articleShare" data-link-name="in body link">the New York Times reported</a> on Sunday.</p><p>The Guardian has independently confirmed the existence of Hegseth’s own private group chat.</p><p>According to unnamed sources familiar with the chat who spoke to the Times, Hegseth sent the private group of his personal associates some of the same information, including the flight schedules for the F/A-18 Hornets that would strike Houthi rebel targets in Yemen, that he also shared with another Signal group of top officials that was created by Mike Waltz, the national security adviser.</p><p>The existence of the Signal group chat created by Waltz, in which <a href="https://www.theguardian.com/us-news/2025/mar/24/journalist-trump-yemen-war-chat-reaction" data-link-name="in body link">detailed attack plans were divulged</a> by Hegseth to other Trump administration officials on the private messaging app, was <a href="https://www.theatlantic.com/politics/archive/2025/03/trump-administration-accidentally-texted-me-its-war-plans/682151/" data-link-name="in body link">made public last month</a> by Jeffrey Goldberg of the Atlantic, who had been accidentally added to the group by Waltz.</p><figure id="06b68203-5ef2-4f56-bfd6-b86c6e7646cb" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:4,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Exclusive: how the Atlantic’s Jeffrey Goldberg got added to the White House Signal group chat&quot;,&quot;elementId&quot;:&quot;06b68203-5ef2-4f56-bfd6-b86c6e7646cb&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/us-news/2025/apr/06/signal-group-chat-leak-how-it-happened&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0}}"></gu-island></figure><p>The fact that Hegseth also shared the plans in a second Signal group chat, according to “people familiar with the matter” who spoke to the Times, is likely to add to <a href="https://www.politico.com/news/2025/04/18/defense-secretary-chief-of-staff-joe-kasper-departure-00299508" data-link-name="in body link">growing criticism</a> of the former Fox weekend anchor’s ability to manage the Pentagon, a massive organization which operates in matters of life and death around the globe.</p><p>According to the Times, the private chat also included two senior advisers to Hegseth – Dan Caldwell and Darin Selnick – who were fired last week after being accused of leaking unauthorized information.</p><p>Hegseth has <a href="https://www.youtube.com/watch?v=sYLUIBBUNJ0" data-link-name="in body link">previously been criticized</a> for including his wife, Jennifer, a former Fox News producer, <a href="https://www.theguardian.com/us-news/2025/mar/29/pete-hegseth-wife-jennifer-foreign-defense-official-meetings" data-link-name="in body link">in sensitive meetings with foreign leaders</a>, including a discussion of the war in Ukraine with British military leaders. Phil Hegseth, the secretary’s younger brother, was hired as a senior Pentagon adviser and is the defense department’s liaison to the Department of Homeland Security. It is unclear why either would need to know about the details of strikes plans in advance.</p><p>According to the Times, Hegseth used his private phone, rather than a government device, to access the Signal chat with his family and friends.</p><p>CNN <a href="https://www.cnn.com/2025/04/20/politics/hegseth-second-signal-chat-military-plans/index.html" data-link-name="in body link">reported later on Sunday</a> that three sources familiar with Hegseth’s private Signal group confirmed to the broadcaster that he had used it to share Yemen attack plans before the strikes were launched.</p><p>A person familiar with the contents and those who received the messages, confirmed the second chat to the Associated Press ands said that it included 13 people.</p><p>Shortly after the news of the second Signal chat broke, Politico published an opinion article by Hegseth’s former press secretary, John Ullyot, <a href="https://www.politico.com/news/magazine/2025/04/20/pentagon-chaos-ullyot-hegseth-00205594" data-link-name="in body link">who wrote</a>: “It’s been a month of total chaos at the Pentagon. From leaks of sensitive operational plans to mass firings, the dysfunction is now a major distraction for the president – who deserves better from his senior leadership”.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Find the Odd Disk (102 pts)]]></title>
            <link>https://colors2.alessandroroussel.com/</link>
            <guid>43745868</guid>
            <pubDate>Sun, 20 Apr 2025 19:17:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://colors2.alessandroroussel.com/">https://colors2.alessandroroussel.com/</a>, See on <a href="https://news.ycombinator.com/item?id=43745868">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Click the disk that's a different color. Use your eyes only! Make sure to disable any blue-light filter on your screen.
		<br>Clique sur le disque qui est d'une couleur différente. Utilise seulement tes yeux ! Assurez-vous de désactiver tout filtre de lumière bleue sur votre écran.
		<br>Haz clic en el disco que tiene un color diferente. ¡Usa solo tus ojos! Asegúrate de desactivar cualquier filtro de luz azul en tu pantalla.</p><div id="end-screen">
		<h2>Game Over!</h2>
		
		<p>Thank you for participating. Please feel free to play again, the more data the better!
			<br>Merci pour ta participation. N'hésite pas à rejouer, plus il y a de données, mieux c’est !
			<br>Gracias por participar. ¡No dudes en jugar de nuevo, entre más datos tengamos, mejor!</p>
		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: JuryNow – Get an anonymous instant verdict from 12 real people (178 pts)]]></title>
            <link>https://jurynow.app/</link>
            <guid>43745554</guid>
            <pubDate>Sun, 20 Apr 2025 18:32:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jurynow.app/">https://jurynow.app/</a>, See on <a href="https://news.ycombinator.com/item?id=43745554">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. citizen in Arizona detained by immigration officials for 10 days (188 pts)]]></title>
            <link>https://news.azpm.org/p/news-articles/2025/4/18/224512-us-citizen-in-arizona-detained-by-immigration-officials-for-10-days/</link>
            <guid>43745469</guid>
            <pubDate>Sun, 20 Apr 2025 18:20:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.azpm.org/p/news-articles/2025/4/18/224512-us-citizen-in-arizona-detained-by-immigration-officials-for-10-days/">https://news.azpm.org/p/news-articles/2025/4/18/224512-us-citizen-in-arizona-detained-by-immigration-officials-for-10-days/</a>, See on <a href="https://news.ycombinator.com/item?id=43745469">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img src="https://media.azpm.org/master/image/2019/10/23/hero/ice.jpg" alt="ICE arrests">
<span>A March 2018 photo of U.S. Immigration and Customs Enforcement (ICE) officers.</span></p><p>ICE/Flickr</p>
</div><div>




<p>19-year-old Jose Hermosillo, who is visiting Tucson from Albuquerque, says he was lost and walking near the Border Patrol headquarters when an agent arrested him for illegally entering the country. Hermosillo was not carrying identification.</p>
<p>Court documents say a Border Patrol agent arrested Hermosillo “at or near Nogales, Arizona, without proper immigration documents” and that Hermosillo admitted to illegally entering the U.S.</p>
<p>Hermosillo and his girlfriend, who have a 9-month-old child together, live in Albuquerque, New Mexico, and are visiting family in Tucson. He says he has never been to Nogales. </p>
<p>His girlfriend’s aunt Grace Layva says she and her family made numerous calls looking for him before they found out he was being detained in the Florence Correctional Center, which Immigration and Customs Enforcement uses to detain people. </p>
<p>Another family member drove to the detention center, about 70 miles northwest of Tucson, but said officials wouldn’t provide any information or release him. </p>
<p>ICE did not respond to a request for comment about the wrongful detention. </p>
<p>The family later provided officials with his birth certificate and social security card.</p>
<p>“He did say he was a U.S. citizen, but they didn't believe him,” Layva said. “I think they would have kept him. I think they would have if they would have not got that information yesterday in the court and gave that to ICE and the Border Patrol. He probably would have been deported already to Mexico.”</p>
<p>A magistrate judge in Tucson dismissed his case on Thursday, and family says he was released much later that night. </p>
<p>There have been other recent cases of U.S. citizens being wrongly detained by immigration officials, including a man wrongly held in Florida after being pulled over during his commute to work. </p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First hormone-free male birth control pill enters human trials (180 pts)]]></title>
            <link>https://scitechdaily.com/99-effective-first-hormone-free-male-birth-control-pill-enters-human-trials/</link>
            <guid>43745296</guid>
            <pubDate>Sun, 20 Apr 2025 17:54:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scitechdaily.com/99-effective-first-hormone-free-male-birth-control-pill-enters-human-trials/">https://scitechdaily.com/99-effective-first-hormone-free-male-birth-control-pill-enters-human-trials/</a>, See on <a href="https://news.ycombinator.com/item?id=43745296">Hacker News</a></p>
Couldn't get https://scitechdaily.com/99-effective-first-hormone-free-male-birth-control-pill-enters-human-trials/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The Movie Mistake Mystery from "Revenge of the Sith" (361 pts)]]></title>
            <link>https://fxrant.blogspot.com/2025/04/the-movie-mistake-mystery-from-revenge.html</link>
            <guid>43745141</guid>
            <pubDate>Sun, 20 Apr 2025 17:29:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fxrant.blogspot.com/2025/04/the-movie-mistake-mystery-from-revenge.html">https://fxrant.blogspot.com/2025/04/the-movie-mistake-mystery-from-revenge.html</a>, See on <a href="https://news.ycombinator.com/item?id=43745141">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Movies are handmade, and just like any other art form, sometimes the seams that hold movies together become visible to the audience. For movie fans, these moments are very exciting. Catching a glimpse behind the scenes is an exhilarating experience. My favorite kind of “movie mistake” is the kind that is hiding in plain sight... but the casual viewer missed it upon first viewing. Or perhaps even the second viewing, or even the third.&nbsp;</p><p>I’m particularly obsessed with moments that reveal the craft and artistry of the magic trick of a shot that slightly shatters the illusion of cinema. These revealing moments have been in movies since the dawn of cinema, and are everywhere (if you know exactly where to look).</p><p>One of my favorite films of all time also has one of the funniest revealing mistakes I've seen. Edward Zwick's "Glory" (1989) takes place during the American Civil War, and this scene has a blink-and-you'll-miss-it reminder of the film's very modern production:</p><p>Because the audiences' eyes are firmly fixed on Morgan Freeman's character in the center of frame, very few will ever pick up the little kid with the extremely modern wristwatch that enters frame on far screen right. Sometimes the on-set teams that work with featured extras—as well as the costume department that dress the extras—will occasionally miss a modern piece of jewelry on an actor.</p><p>Here's a fun one from Martin Scorsese's masterpiece "Goodfellas" (1990), in one of the closing shots of a nail-bitingly tense scene where Karen nearly walks into an ambush:</p><p>The period-appropriate "movie" license plate dramatically dangles then completely falls off the car in the middle of the take, revealing the actual 1990-era license plate of the car used for the scene. This is an accidental and hilarious glimpse into the detailed hard work that goes into making a Hollywood period piece (this portion of the film takes place in 1980), where every license plate of every car in the movie needed special, detailed work to make them period-appropriate.&nbsp;</p><div><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMcj2VC4Ju_5PCmfKdkGkSC6r2UzvK2qzh0d6NcxdtSdPSk24l8m5oTt3Yeq5GLp0wPZqjUlq1pHDiutRKWpMZucWukSa3qBfHw8J8KOu-dGExAWf97ym5Ikpif9BbZnKqLOwef_z1I2FXFFradn3C8BOr2GkMU8SfJzmBafEDtnv_4d8FeV4/s1908/illustration.png"><img data-original-height="1080" data-original-width="1908" height="226" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMcj2VC4Ju_5PCmfKdkGkSC6r2UzvK2qzh0d6NcxdtSdPSk24l8m5oTt3Yeq5GLp0wPZqjUlq1pHDiutRKWpMZucWukSa3qBfHw8J8KOu-dGExAWf97ym5Ikpif9BbZnKqLOwef_z1I2FXFFradn3C8BOr2GkMU8SfJzmBafEDtnv_4d8FeV4/w400-h226/illustration.png" width="400"></a></p><p>The finale of James Cameron's epic "Aliens" (1986) features the android Bishop (Lance Henriksen) getting severed in half, but still functioning enough to save Newt (Carrie Henn) from getting sucked into the vacuum of space. The action-packed scene features an absolutely wonderful accidental reveal of how the cut-in-half android was accomplished on the set:</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHvAdErohL2BD-qytmJ7mGVvxdRGRuzMOiwXiI8kR6y0Pw707rqoLC58rp3AWp3gHhAt-O5YeR-thdq5AbnQrlhTtoc44TQsDctIyA2Ykga8UQVB7gGQphV1-wPNOChutR7MIFC2dzrBfxkRyeWXr0NjwW9kwU8N28sIgHlotDkprZNKZP9EY/s1920/bishop.png"><img data-original-height="1036" data-original-width="1920" height="216" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHvAdErohL2BD-qytmJ7mGVvxdRGRuzMOiwXiI8kR6y0Pw707rqoLC58rp3AWp3gHhAt-O5YeR-thdq5AbnQrlhTtoc44TQsDctIyA2Ykga8UQVB7gGQphV1-wPNOChutR7MIFC2dzrBfxkRyeWXr0NjwW9kwU8N28sIgHlotDkprZNKZP9EY/w400-h216/bishop.png" width="400"></a></p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrvAm_i6yGBzc5507CmHJY6q_LcLnMS-eTcXVQhyphenhyphenA8uYrX4dXL80CVd4EwJOAEbJZPgeGawDEUn2pdc5KqlBuZJLuaw7fWJdzoG-r2a1vy_uQU_neb-5N5Yz8AaiDR0UtxciF4GhUhY3Tc6-0VamANpf-FePIL0YbeFnucGGA4QEKyVhMau7E/s433/alienshole.2025-04-17%2012_43_03.gif"><img data-original-height="227" data-original-width="433" height="210" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrvAm_i6yGBzc5507CmHJY6q_LcLnMS-eTcXVQhyphenhyphenA8uYrX4dXL80CVd4EwJOAEbJZPgeGawDEUn2pdc5KqlBuZJLuaw7fWJdzoG-r2a1vy_uQU_neb-5N5Yz8AaiDR0UtxciF4GhUhY3Tc6-0VamANpf-FePIL0YbeFnucGGA4QEKyVhMau7E/w400-h210/alienshole.2025-04-17%2012_43_03.gif" width="400"></a></p><p><i><span>"Aliens" (1986)</span></i></p><p>The amazing makeup effects applied to Henriksen's body covers the bottom half of his body which is hidden through a hole in the set. But in order to get that little bit of extra athletic stretch to grab Newt, Henriksen popped his body out of the hole a little too far, revealing the classic stage trick. However, I'd gather that 99% of the audience has never noticed this little reveal of stagecraft since our eyes are fixed on Newt on screen right, sliding toward the airlock, and not on the ground contact of Bishop's half-body, which had already been firmly established in the scene.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU9LeqQEqiQZjf4bniZIvtLsXvLwZa6loSJneVo01NSsgsU60jw0nlF-YB-lQmdd3o-Uq1GOWb19_ctP1J5GMakIBHgCWrFCIlg2OQ1QS49k1ZIFyPaqOCOhpnfLCJCnWn3rWQ-QmSqboHlgpw6WxJPAy-OnoK0rnsMiPOX5SqBjT9fLJOQRc/s1045/circled.png"><img data-original-height="568" data-original-width="1045" height="217" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU9LeqQEqiQZjf4bniZIvtLsXvLwZa6loSJneVo01NSsgsU60jw0nlF-YB-lQmdd3o-Uq1GOWb19_ctP1J5GMakIBHgCWrFCIlg2OQ1QS49k1ZIFyPaqOCOhpnfLCJCnWn3rWQ-QmSqboHlgpw6WxJPAy-OnoK0rnsMiPOX5SqBjT9fLJOQRc/w400-h217/circled.png" width="400"></a></p><p>Avoiding reflections of the crew appearing to camera is a constant struggle for filmmakers. In Steven Spielberg's first masterpiece "Duel" (1971), David (Dennis Weaver) gets into a phone booth to make a call, with the front glass face of the booth aimed directly at the camera, and if the audience's gaze drifted off of Weaver's face, they could catch a glimpse of the crew:</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiF2nMOIJJcigCUwmwYiNCUFK3LSPyqR98CGn7XdriPNtxl1cJ-wHIb11RxPjm3u9LVV8LmypcMzbLLQl8DTTNwrp5eGMx6rs-PPazN7hPp2PihyphenhyphenZrkldyEB49amd3N0gr4tlT-KCokNOIhysaS60VIYxoVWxvTWUoNJR6QUAzH99MOtfshAHY/s442/duel_short.gif"><img data-original-height="249" data-original-width="442" height="225" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiF2nMOIJJcigCUwmwYiNCUFK3LSPyqR98CGn7XdriPNtxl1cJ-wHIb11RxPjm3u9LVV8LmypcMzbLLQl8DTTNwrp5eGMx6rs-PPazN7hPp2PihyphenhyphenZrkldyEB49amd3N0gr4tlT-KCokNOIhysaS60VIYxoVWxvTWUoNJR6QUAzH99MOtfshAHY/w400-h225/duel_short.gif" width="400"></a></p><p><i><span>"Duel" (1971)</span></i></p><p>In the reflection, we see a few crew members on screen left, the camera itself, and director Spielberg on the right (he's the one shuffling left and right, who lowers his head in the middle of the take). Again, like all the examples I'm providing in this article, hardly anyone would ever notice these moments. When a viewer catches these brief moments, the illusion of the movie is briefly broken, but for fans of the filmmaking process, it's a joyful reminder of the overall magic trick. The most intimate movie scene with only two characters in a desolate, isolated environment actually was created by dozens and dozens of crew members standing slightly out of frame.</p><p>Look for another accidental 'crew caught on camera' moment in the reflection in a car window in the 'leave the gun, take the cannoli' scene from "The Godfather" (1972), one that very few people ever notice.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgASHi1vEZJq9xexlwe6N6XnaKUFAH4aOLXmHKCv9BVNxVMiPwJ_Pj_nEabuKrD-nJqZDhITBBmRoNdyYmO3jKQEgLzkHPChAr50lpMRd5yJYt6whAJlaIVY7wOBLK18qVEOOpCbj4b7YeAIAjvItvDNC0tKFlLCk2P8s3NtQRy7Ahvy9kMCe8/s1920/duel_circle.png"><img data-original-height="1080" data-original-width="1920" height="225" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgASHi1vEZJq9xexlwe6N6XnaKUFAH4aOLXmHKCv9BVNxVMiPwJ_Pj_nEabuKrD-nJqZDhITBBmRoNdyYmO3jKQEgLzkHPChAr50lpMRd5yJYt6whAJlaIVY7wOBLK18qVEOOpCbj4b7YeAIAjvItvDNC0tKFlLCk2P8s3NtQRy7Ahvy9kMCe8/w400-h225/duel_circle.png" width="400"></a></p><p>Here's a super quick revealing mistake from "The Dark Knight" (2008) that is a true "you'll never see this in real time" moment:</p><div><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg914mIcnatARLUxXQIIv3YV-sMpkCbW5HIt6fqjlMW70OT5z6_Rn39zeenC42Sq_UYQk-eFX6DwW4aAUj9td-cqk0pzWhx0utnQOq8yuzxDn2hSJ2ptXYxhfydZaTz-F8Dho6lDeOgrdNuegiP2B0cHQe8UuDjRSX4EhWDSoU_HilsML5NXIA/s526/darkknight_crew_BRIGHTfull.2025-04-17%2012_56_27.gif"><img data-original-height="219" data-original-width="526" height="166" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg914mIcnatARLUxXQIIv3YV-sMpkCbW5HIt6fqjlMW70OT5z6_Rn39zeenC42Sq_UYQk-eFX6DwW4aAUj9td-cqk0pzWhx0utnQOq8yuzxDn2hSJ2ptXYxhfydZaTz-F8Dho6lDeOgrdNuegiP2B0cHQe8UuDjRSX4EhWDSoU_HilsML5NXIA/w400-h166/darkknight_crew_BRIGHTfull.2025-04-17%2012_56_27.gif" width="400"></a></p><p><i><span>"The Dark Knight" (2008)</span></i></p></div><p>Although "The Dark Knight" example gives the audience a much clearer look at the camera operator, the focus puller(?) and the camera itself reflected in the interrogation room’s mirrors, the shot is a lot harder to see the crew members and equipment in real time due to the chaotic and energetic camera movement, as opposed to the locked off nature of the "Duel" example.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0YNfNS0gG-xlaqmH-9KJDvwwpsHkIMycqul7SW8X2hE-LNZf0RHBM3LIXuQRlqQo0xD4sj5xzUW_Cu-A2gMXeBUmN55gZwHMlDuBL3nuxqCt0rJDDSGM2hTUzrkuWXxuxIg73mSaclQudBt75y_w_X8YsuWQRFC7iPc5NDSb6dQnQ2D3JxAQ/s1918/darkknight_compare.png"><img data-original-height="800" data-original-width="1918" height="166" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0YNfNS0gG-xlaqmH-9KJDvwwpsHkIMycqul7SW8X2hE-LNZf0RHBM3LIXuQRlqQo0xD4sj5xzUW_Cu-A2gMXeBUmN55gZwHMlDuBL3nuxqCt0rJDDSGM2hTUzrkuWXxuxIg73mSaclQudBt75y_w_X8YsuWQRFC7iPc5NDSb6dQnQ2D3JxAQ/w400-h166/darkknight_compare.png" width="400"></a></p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibaiynJTHf2-1u0HH7_do8IU4-CU7kzWOzAUP4syOn9_7YRAVLSowGvKwEPI52YA3rk4xAUpd4BzXJbBpfnADRmYZoijN4Vkim_OHVJjFrp1S1e8p2AokM1GJDAPSojCoRP4DgcbKlGzmjGpRLrPrZoADLBab4haCQzNsrzxSHo7ThaySW63w/s500/abyss_wipe.gif"><img data-original-height="281" data-original-width="500" height="225" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibaiynJTHf2-1u0HH7_do8IU4-CU7kzWOzAUP4syOn9_7YRAVLSowGvKwEPI52YA3rk4xAUpd4BzXJbBpfnADRmYZoijN4Vkim_OHVJjFrp1S1e8p2AokM1GJDAPSojCoRP4DgcbKlGzmjGpRLrPrZoADLBab4haCQzNsrzxSHo7ThaySW63w/w400-h225/abyss_wipe.gif" width="400"></a></p><p><i><span>"The Abyss" (1989)</span></i></p><p>Amazingly, many folks who watch that clip from the dramatic drowning sequence cannot consciously see the bit of filmmaking that literally blocks the actors in an intimate moment. This is my favorite example of a movie's incredible emotional power — the scene is so dramatic and intense that most viewers cannot consciously see a giant cloth wiping away water from the lens of the camera in the middle of a shot.</p><p>Incidentally, <b>some of these revealing mistakes are being erased from cinema history</b> due to overzealous restoration projects — the process of “cleaning up” a film for newer formats like Blu-ray and 4K — which is deeply wrong. This is a much bigger topic on which I have very strong thoughts and the hottest of takes. Just look at what modern restorations have done to two of these revealing mistakes from "Goodfellas" and "Aliens":</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHamPBMDtOZiH_6g10yv7e4Pp9vRyA26atkaG2cW-uzswK3zQqOUHwgGiH_mvWTXKWIvddFqc4RKdsaanqjgxHOtGHlUZa79k81xaf_i-YV1AeIpbNzfCx1DBMMSeHkBYMQWsyuKrevfCYObg-z5MoMRfn6k5uVnxSqPtvZFIqBlwmClkwU80/s640/LicensePlate%20_restoration.gif"><img data-original-height="362" data-original-width="640" height="226" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHamPBMDtOZiH_6g10yv7e4Pp9vRyA26atkaG2cW-uzswK3zQqOUHwgGiH_mvWTXKWIvddFqc4RKdsaanqjgxHOtGHlUZa79k81xaf_i-YV1AeIpbNzfCx1DBMMSeHkBYMQWsyuKrevfCYObg-z5MoMRfn6k5uVnxSqPtvZFIqBlwmClkwU80/w400-h226/LicensePlate%20_restoration.gif" width="400"></a></p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjczOajV2AciOuOHRI9NXzBIfGyIKJbjRdfinr7TKVb_TTBP-uQQtDCWqGuDeoaY1FJElsr6HXU_I8sqhXHx7gAdNKKbpk8WHL6ByirFITWb_Vhqsq_TL042SKHjqftXYhZSbYXDGxA73zmjHaF3vBPtHDp_POCUDligVeWAi5N5G8XYRKL_A/s500/alienshole_fullmoviewithtextandcompare.gif"><img data-original-height="270" data-original-width="500" height="216" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjczOajV2AciOuOHRI9NXzBIfGyIKJbjRdfinr7TKVb_TTBP-uQQtDCWqGuDeoaY1FJElsr6HXU_I8sqhXHx7gAdNKKbpk8WHL6ByirFITWb_Vhqsq_TL042SKHjqftXYhZSbYXDGxA73zmjHaF3vBPtHDp_POCUDligVeWAi5N5G8XYRKL_A/w400-h216/alienshole_fullmoviewithtextandcompare.gif" width="400"></a></p><p>Painting out these movie mistakes as part of a restoration is wrong. <i>What's in the movie is in the movie, </i>and altering the movie to this extent is a form of revisionist history. Cinema is worse off when over-aggressive restorations alter the action within the frame. To me, this is equivalent to swapping out an actor's performance with a different take, or changing the music score during an action sequence, or replacing a puppet creature with a computer graphics version of the same creature decades after release. But I digress.</p><p>• &nbsp;• &nbsp;• &nbsp;•</p><p>Like I said at the start, movies are handmade, and that's true even in today's landscape where digital visual effects are a prominent part of filmmaking. In the same way that physical crews use physical tools to build sets, construct costumes and craft props, visual effects artists use digital tools to craft an image. And with the hand-made nature of any art form, the lack of clinical accuracy lends to its charm and sometimes offers an accidental peek behind the scenes of how the art was constructed.</p><p>Every few years, a "Star Wars" revealing mistake bubbles up on the internet, one from the Mustafar sequence from Episode III, "Revenge of the Sith" (2005). But the bizarre moment in the single shot was not as easily explainable as the examples I've shown above.</p><p>Being in the privileged position of currently working at Industrial Light &amp; Magic, the visual effects company that made the visual effects for the movie (and having worked on that movie [and that sequence!]), I took it upon myself to try and solve the mystery.</p><p>Please enjoy the story, written by Ian Kintzle, of how I investigated the mystery of the "Force Ghost" in "Revenge of the Sith", as it originally appeared in the Star Wars Celebration Program for Japan 2025.</p><p>• &nbsp;• &nbsp;• &nbsp;• &nbsp;• &nbsp;• &nbsp;• &nbsp;•</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEb2mtF1FNE5KEKPJjSs9hyphenhyphend5cAatilKzhr5B1E8cS-zRT1QI3XEXcX4SBRgLql4-C9ilaCiSSETbvWEl6SlxcbW495GsA6SbcjvulKOl6ZhyynisOsIH22CMDP-JmdMXb2opBnT-iOGoHglz_E6rgqXc8HSvqgbgm4Ibihz-P1RVnOKewPmQ/s1918/finalshot_singleframeA.jpg"><img data-original-height="816" data-original-width="1918" height="170" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEb2mtF1FNE5KEKPJjSs9hyphenhyphend5cAatilKzhr5B1E8cS-zRT1QI3XEXcX4SBRgLql4-C9ilaCiSSETbvWEl6SlxcbW495GsA6SbcjvulKOl6ZhyynisOsIH22CMDP-JmdMXb2opBnT-iOGoHglz_E6rgqXc8HSvqgbgm4Ibihz-P1RVnOKewPmQ/w400-h170/finalshot_singleframeA.jpg" width="400"></a></p><p><i><b><span>THE FORCE GHOST IN THE MACHINE</span></b></i></p><p><i><span>By Ian Kintzle</span></i></p><p><i><span>April 2025, for Star Wars Celebration Japan</span></i></p><p><span>It was spring 2005, and Industrial Light &amp; Magic (ILM)— George Lucas’ dream factory—had just completed two years of work on one of its most ambitious projects yet: "Star Wars: Revenge of the Sith". A massive undertaking, "Sith" required a herculean effort from hundreds of artists and technicians at ILM, crafting 367 computer- generated models, hundreds of 3D and 2D environments, 47 practical miniature setups, and 13,000,000 renders and composites across 2,151 effects shots.</span></p><p><span>Out of all of the effects sequences in the picture, perhaps none was more challenging than the operatic duel between Darth Vader (Hayden Christensen) and Obi-Wan Kenobi (Ewan McGregor) on the volcanic planet of Mustafar. The battle starts within the Klegger Corp Mining Facility situated high on the rocky banks of a vast lava river, and progresses through the facility onto a heat-collection arm stretching over a fast- moving river of boiling magma, and then onto a pair of lava skiffs and panning droids. The battle finally ends on a bank with Vader severely burned and maimed.</span></p><p><span>For the Mustafar sequence, ILM’s team of compositors, led by Compositing Supervisor Pat Tubach and Sequence Supervisor Michael Conte, were faced with the daunting challenge of seamlessly blending all of these live-action plates, computer-generated imagery, and miniature effects, into one cohesive sequence. But with so many individual elements, mistakes happen, and in the case of Revenge of the Sith, a peculiar anomaly slipped through the cracks at precisely 1 hour, 59 minutes, and 2 seconds into the film.</span></p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9Y3af6L5NqwRyByVmK5V-h8fEwV8KjiN0r1UbtqwggsMCVMTciUmSBPrJO9YHUushdOESEJ2UsYi4113wq3UGQo2_Q1LRMccUwyI3_lR7dpLbGn3D0EQoVvELQYDjg8yHCrD9b6mN-evcA8cmvodXq1CS13glDu4Yw_rgxKZMR53P3gDJHGc/s500/finalsequence.gif"><span><img data-original-height="213" data-original-width="500" height="170" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9Y3af6L5NqwRyByVmK5V-h8fEwV8KjiN0r1UbtqwggsMCVMTciUmSBPrJO9YHUushdOESEJ2UsYi4113wq3UGQo2_Q1LRMccUwyI3_lR7dpLbGn3D0EQoVvELQYDjg8yHCrD9b6mN-evcA8cmvodXq1CS13glDu4Yw_rgxKZMR53P3gDJHGc/w400-h170/finalsequence.gif" width="400"></span></a></p><p><span>The internet, ever vigilant, began to take notice of this curious artifact around 2015 – a blink-and-you’ll-miss- it moment of a ghostly-robed figure with dark hair that appears behind Anakin Skywalker for only a frame or two just as he leaps from the panning droid to meet Obi-Wan on the lava skiff. The strange figure sparked countless theories and speculation. Was it a “Force ghost”? An easter egg from a mischievous ILM artist?</span></p><p><span>Todd Vaziri, a seasoned veteran at ILM who also worked on the film as a compositor, was intrigued by the mystery. “Just before the release of The Force Awakens, I started to see this ‘easter egg’ bubble up on social media from time to time of what appears to be a Force ghost on Mustafar,” Vaziri says. “The discourse would really get going. Somebody would spot the artifact and go, ‘What the heck was this?’ And another would say, ‘What do you mean? I don’t see anything.’ And only when you step through the scene, frame-by-frame, do you see what looks like a ghostly face behind Anakin in the shot where he jumps up from the panning droid to continue the lightsaber duel on the lava skiff. And honestly, in-motion, nobody can spot this.”</span></p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYvOBxrHhHq0CiZlSavot6K1cDeaDHKK-OxHOqGpiSBTKMahcyR_4sUwab0y8EJscd2r8mJQ75Diy1xT2gfg4jxRGa6TTmg6dmtG7WUF2u08Se3LSXIRzPUQBE9ItefL8dKzv8yAISsj9fjyVb3VtciyuWcKQDtQpefp2SocuSAFUSMAxeL_M/s500/animation.gif"><span><img data-original-height="213" data-original-width="500" height="170" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYvOBxrHhHq0CiZlSavot6K1cDeaDHKK-OxHOqGpiSBTKMahcyR_4sUwab0y8EJscd2r8mJQ75Diy1xT2gfg4jxRGa6TTmg6dmtG7WUF2u08Se3LSXIRzPUQBE9ItefL8dKzv8yAISsj9fjyVb3VtciyuWcKQDtQpefp2SocuSAFUSMAxeL_M/w400-h170/animation.gif" width="400"></span></a></p><p><span>Getting to the bottom of the mystery would prove difficult. ILM works on dozens of motion pictures and television shows per year, and as older projects are moved offline into their archives, the steps to bring them back to the servers are involved. Revenge of the Sith was no exception. It would require scavenging through terabytes of unaltered greenscreen photography that hasn’t been touched in years. So Vaziri put it behind him – for a time. But in 2024 when the discourse regarding the “Force ghost” roared alive again on social media, Vaziri decided that enough was enough. But in order to locate the anomaly, he would need to spelunk into the film’s digital archives at ILM which had since gone dark.</span></p><p><span>“I think it took 24 hours to unearth the footage and put it back on our servers. I was so excited, my heart was pounding out of my chest. No one had seen the original greenscreen footage for nearly twenty years,” Vaziri says. “The problem was I didn’t remember exactly what these plates looked like, both because it wasn’t my shot, and it was two decades prior. So I dug, and I dug, and finally I found the plate photography. I couldn’t believe it. There on set was a man—likely a stunt rigger—wearing not a robe, but a peculiar shirt that resembled one, standing behind Hayden, manually puppeteering the greenscreen lava skiff that he and Ewan were fighting on. His face and the “Force ghost” matched up frame-for-frame.” During this excavation process, Vaziri was also able to uncover a variety of in-progress versions of the shot composited with very basic layering. In those early takes, the robed man was not present. This meant one thing: the compositor had done some articulates to remove the mystery man, but the green screen extraction wasn’t quite done yet.</span></p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxmX6_02BHHum6AiA3Hz9X3NDxDsyzBCfeyj6Rs-rmTDZoZEw78OEj1hvTlxbHn1-G1R-Ur0Qdy7G2vhbuy_AP1tza0uAlHr216oz-MXNbrp0WWbLEGNjVDALO9UZJG8WYd1jKz2yv5HAic4dNDS4SpXNTxr1dWCdI4rmtyb8VLFBnqDvRw74/s1175/greenscreen_compare.png"><span><img data-original-height="1175" data-original-width="1024" height="400" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxmX6_02BHHum6AiA3Hz9X3NDxDsyzBCfeyj6Rs-rmTDZoZEw78OEj1hvTlxbHn1-G1R-Ur0Qdy7G2vhbuy_AP1tza0uAlHr216oz-MXNbrp0WWbLEGNjVDALO9UZJG8WYd1jKz2yv5HAic4dNDS4SpXNTxr1dWCdI4rmtyb8VLFBnqDvRw74/w349-h400/greenscreen_compare.png" width="349"></span></a></p><p><span>“We have to do frame-by-frame tweaking by hand, which means creating new garbage mattes in order to paint details into the motion- blurred edges,” Vaziri explains. “At some point during the process of refining the edges of the green screen extraction—which required new garbage mattes—the stunt rigger’s head was inadvertently revealed again in that paint process—but because you can’t see it unless you are stepping through it frame-by-frame— it was deemed finished by the artist, by the compositing supervisor, by the visual effects supervisor, by the editors, and by George Lucas himself. Nobody that was part of this process ever caught that and that’s how it made it in the movie. But in a way, I think it’s really wonderful. Plenty of my shots have mistakes in them, and as the saying goes: perfect is the enemy of good. We want our shots to be as perfect as they can be, but we can’t hit everything. In the last 20 years, we have evolved what we call the “Final Check” process, which is our way of scrutinizing shots before they leave ILM. An extra step of quality control, if you will. The bottom line is that we put human hands on every single one of the thousands of shots that you see in Star Wars. This world is handmade, and little things like this become part of ILM history.”</span></p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTXZ4c7qoZ-d8aejJQ29QoqgvIpqihCiRAzFobTxRC5Wd_fJxQThJNkviLOd6L1Af4jhMZrRBRq4ZqoqelEiwSrtaeucjjLlMbdIxhRj_JY9uZccfDtynD7uPPTj-leY98QlCpz2uyy8cFBtkyQiBRHMF2a7EsFY6sfBlL6gDr9XFN2VCBI7E/s1593/greenscreen_singleframeBCROP.jpeg"><span><img data-original-height="783" data-original-width="1593" height="196" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTXZ4c7qoZ-d8aejJQ29QoqgvIpqihCiRAzFobTxRC5Wd_fJxQThJNkviLOd6L1Af4jhMZrRBRq4ZqoqelEiwSrtaeucjjLlMbdIxhRj_JY9uZccfDtynD7uPPTj-leY98QlCpz2uyy8cFBtkyQiBRHMF2a7EsFY6sfBlL6gDr9XFN2VCBI7E/w400-h196/greenscreen_singleframeBCROP.jpeg" width="400"></span></a></p><p><span><span><i>detail of the original greenscreen footage</i></span></span></p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMNJa-NicVw7dMWKVk8mXjvd0wilCDs1xhqFR8-_L8uo2WD7_q9AcNWKUmY6eO8KNuHB1RflBg_z2UnTAJV-EB4BUJJrkEqzWcr4SrauZGkSAK2TqA9LHHvNJ1-6oiNdIYZVlsaVrE4AV4l4uIqArxzA8HfgyNJkzXgHnPHUvWw2eBUa0SJzg/s1918/finalshot_singleframeB.jpg"><span><img data-original-height="816" data-original-width="1918" height="170" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMNJa-NicVw7dMWKVk8mXjvd0wilCDs1xhqFR8-_L8uo2WD7_q9AcNWKUmY6eO8KNuHB1RflBg_z2UnTAJV-EB4BUJJrkEqzWcr4SrauZGkSAK2TqA9LHHvNJ1-6oiNdIYZVlsaVrE4AV4l4uIqArxzA8HfgyNJkzXgHnPHUvWw2eBUa0SJzg/w400-h170/finalshot_singleframeB.jpg" width="400"></span></a></p><p><span><span><i>the final shot as it appears in the film</i></span></span></p><p><span>So there you have it, readers. Another Star Wars mystery solved. In this case it wasn’t a Force ghost, but a stunt rigger who slipped into the shot during the compositing process, providing a wonderful look at the technical seams and handmade nature of the world of visual effects. Star Wars: Revenge of the Sith is streaming now on Disney+.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Turing-Drawings (105 pts)]]></title>
            <link>https://github.com/maximecb/Turing-Drawings</link>
            <guid>43744609</guid>
            <pubDate>Sun, 20 Apr 2025 16:00:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/maximecb/Turing-Drawings">https://github.com/maximecb/Turing-Drawings</a>, See on <a href="https://news.ycombinator.com/item?id=43744609">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>GitHub Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_product_navbar&quot;}" href="https://github.com/security/advanced-security">
      
      <div>
        <p>GitHub Advanced Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code Review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
        <p>Code Search</p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      Events &amp; Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      
      <div>
              <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      
      <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
        <p>Enterprise platform</p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:maximecb/Turing-Drawings" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="3YKtS4VAeyYHAsm0bjOfkutVs8KS6Kz20IdUN0dE9UAf9sBR_WBCjDgJ5dfVP3sRX3JJWVG-BJLmXS4gKCKHEg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="maximecb/Turing-Drawings" data-current-org="" data-current-owner="maximecb" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=maximecb%2FTuring-Drawings" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/maximecb/Turing-Drawings&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="eaf56557bc40b1f72873b38d98a046e468db80c4a47f3d9beddd2981638813f5" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a>

              
          
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Things Zig comptime won't do (343 pts)]]></title>
            <link>https://matklad.github.io/2025/04/19/things-zig-comptime-wont-do.html</link>
            <guid>43744591</guid>
            <pubDate>Sun, 20 Apr 2025 15:57:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matklad.github.io/2025/04/19/things-zig-comptime-wont-do.html">https://matklad.github.io/2025/04/19/things-zig-comptime-wont-do.html</a>, See on <a href="https://news.ycombinator.com/item?id=43744591">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <article>
        <h2>
          Things Zig comptime Won’t Do <time datetime="2025-04-19">Apr 19, 2025</time>
        </h2>

        <figure>
          <blockquote>
            <p>
              Es el disco de Odín. Tiene un solo lado. En la tierra no hay otra
              cosa que tenga un solo lado.
            </p>
          </blockquote>
        </figure>
        <p>
          Zig’s comptime feature is most famous for what it can do: generics!,
          <a href="https://mitchellh.com/writing/zig-comptime-conditional-disable">conditional compilation</a>!,
          <a href="https://mitchellh.com/writing/zig-comptime-tagged-union-subset">subtyping</a>!, serialization!,
          <a href="https://matklad.github.io/2025/03/19/comptime-zig-orm.html">ORM</a>! That’s fascinating, but, to be fair, there’s a bunch of
          languages with quite powerful compile time evaluation capabilities
          that can do equivalent things. What I find more interesting is that
          Zig comptime is actually quite restrictive, by design, and won’t do
          many things! It manages to be very expressive <em>despite</em> being
          pretty limited. Let’s see!
        </p>
        <section id="No-Host-Leakage">
          <h2>
            <a href="#No-Host-Leakage">No Host Leakage </a>
          </h2>
          <p>
            When you execute code at compile time, on which machine does it
            execute? The natural answer is “on your machine”, but it is wrong!
            The code might not run on your machine, it can be cross compiled!
            For overall development sanity, it is important that <code>comptime</code> code observes the same behavior as the runtime
            code, and doesn’t leak details about the host on which the code is
            compiled. Zig doesn’t give comptime code access to host architecture
            (host — machine on which you compile code). Consider this Zig
            program:
          </p>

          <figure>
            <pre><code><span><span>const</span> std = <span>@import</span>(<span>"std"</span>);</span>
<span></span>
<span><span>comptime</span> {</span>
<span>    <span>const</span> x: <span>usize</span> = <span>0xbeef</span>;</span>
<span>    <span>const</span> xs: []<span>const</span> <span>u8</span> = std.mem.asBytes(<span>&amp;</span>x);</span>
<span>    <span>for</span> (xs) <span>|</span>byte<span>|</span> {</span>
<span>        <span>@compileLog</span>(byte);</span>
<span>    }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            I get the following output when compiling normally, on my computer
            for my computer:
          </p>

          <figure>
            <pre><code><span>λ ~/zig-0.14/zig build-lib main.zig</span>
<span>@as(u8, 239)</span>
<span>@as(u8, 190)</span>
<span>@as(u8, 0)</span>
<span>@as(u8, 0)</span>
<span>@as(u8, 0)</span>
<span>@as(u8, 0)</span>
<span>@as(u8, 0)</span>
<span>@as(u8, 0)</span></code></pre>
          </figure>
          <p>
            But if I cross compile to a 32 bit big-endian architecture, comptime
            observes correct <code>usize</code>:
          </p>

          <figure>
            <pre><code><span>λ ~/zig-0.14/zig build-lib -target thumbeb-freestanding-none main.zig</span>
<span>@as(u8, 0)</span>
<span>@as(u8, 0)</span>
<span>@as(u8, 190)</span>
<span>@as(u8, 239)</span></code></pre>
          </figure>
          <p>
            My understanding is that Jai, for example, doesn’t do this, and runs
            comptime code on the host.
          </p>
          <p>
            Rust’s declarative macros and const-fn don’t observe host
            architecture, but procedural macros do.
          </p>
        </section>
        <section id="No-eval">
          <h2>
            <a href="#No-eval">No #eval </a>
          </h2>
          <p>
            Many powerful compile-time meta programming systems work by allowing
            you to inject arbitrary strings into compilation, sort of like <code>#include</code> whose argument is a shell-script that generates the
            text to include dynamically. For example, D mixins work that way:
            <a href="https://dlang.org/articles/mixin.html">https://dlang.org/articles/mixin.html</a>
          </p>
          <p>
            And Rust macros, while technically producing a token-tree rather
            than a string, are more or less the same. In contrast, there’s
            absolutely no facility for dynamic source code generation in Zig.
            You just can’t do that, the feature isn’t!
          </p>
          <p>
            Zig has a completely different feature, partial
            evaluation/specialization, which, none the less, is enough to cover
            most of use-cases for dynamic code generation. Let’s see an
            artificial example:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> f</span>(x: <span>u32</span>, y: <span>u32</span>) <span>u32</span> {</span>
<span>    <span>if</span> (x <span>==</span> <span>0</span>) <span>return</span> y <span>+</span> <span>1</span>;</span>
<span>    <span>if</span> (x <span>==</span> <span>1</span>) <span>return</span> y <span>*</span> <span>2</span>;</span>
<span>    <span>return</span> y;</span>
<span>}</span></code></pre>
          </figure>
          <p>
            This is a normal function that dispatches on the first argument to
            select an operation to apply to the second argument. Nothing fancy!
            Now, the single feature that Zig has is marking the first argument
            with <code>comptime</code>
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> f</span>(<span>comptime</span> x: <span>u32</span>, y: <span>u32</span>) <span>u32</span> {</span>
<span>    <span>if</span> (x <span>==</span> <span>0</span>) <span>return</span> y <span>+</span> <span>1</span>;</span>
<span>    <span>if</span> (x <span>==</span> <span>1</span>) <span>return</span> y <span>*</span> <span>2</span>;</span>
<span>    <span>return</span> y;</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The restriction here is that now, of course, when you call <code>f</code>, the first argument must be comptime-known. You can <span><code>f(92, user_input())</code>,</span> but you can’t
            <span><code>f(user_input(), 92)</code>.</span>
          </p>
          <p>
            The carrot you’ll get in exchange is a guarantee that, for each
            specific call with a particular value of <code>x</code>, the
            compiler will partially evaluate <code>f</code>, so only one branch
            will be left.
          </p>
          <p>
            Zig is an imperative language. Not everything is a function, there’s
            also control flow expressions, and they include partially-evaluated
            variations. For example, <code>for(xs)</code> is a normal runtime
            for loop over a slice, <code>comptime for(xs)</code> evaluates the
            entire loop at compile time, requiring that <code>xs</code> is
            comptime-known, and <code>inline for(xs)</code> requires that just
            the length of <code>xs</code> is known at comptime.
          </p>
          <p>
            Let’s apply specialization to the classic problem solved by
            code-generation — printing. You can imagine a proc-macro style
            solution that prints a struct by reflecting on which fields it has
            and emitting the code to print each field.
          </p>
          <p>
            In Zig, the same is achieved by specializing a recursive <code>print</code> function on the value of type:
          </p>

          <figure>
            <pre><code><span><span>const</span> S = <span>struct</span> {</span>
<span>    int: <span>u32</span>,</span>
<span>    string: []<span>const</span> <span>u8</span>,</span>
<span>    nested: <span>struct</span> {</span>
<span>        int: <span>u32</span>,</span>
<span>    },</span>
<span>};</span>
<span></span>
<span><span>pub</span> <span>fn</span><span> main</span>() <span>void</span> {</span>
<span>    <span>const</span> s: S = .{</span>
<span>        .int = <span>1</span>,</span>
<span>        .string = <span>"hello"</span>,</span>
<span>        .nested = .{ .int = <span>2</span> },</span>
<span>    };</span>
<span>    print(S, s);</span>
<span>}</span>
<span></span>
<span><span>fn</span><span> print</span>(<span>comptime</span> T: <span>type</span>, value: T) <span>void</span> {</span>
<span>    <span>if</span> (T <span>==</span> <span>u32</span>) <span>return</span> print_u32(value);</span>
<span>    <span>if</span> (T <span>==</span> []<span>const</span> <span>u8</span>) <span>return</span> print_string(value);</span>
<span>    <span>switch</span> (<span>@typeInfo</span>(T)) {</span>
<span>        .@<span>"struct"</span> =&gt; <span>|</span>info<span>|</span> {</span>
<span>            print_literal(<span>"{"</span>);</span>
<span>            <span>var</span> space: []<span>const</span> <span>u8</span> = <span>""</span>;</span>
<span>            <span>inline</span> <span>for</span> (info.fields) <span>|</span>field<span>|</span> {</span>
<span>                print_literal(space);</span>
<span>                space = <span>", "</span>;</span>
<span></span>
<span>                print_literal(field.name);</span>
<span>                print_literal(<span>" = "</span>);</span>
<span>                <span>const</span> field_value = <span>@field</span>(value, field.name);</span>
<span>                print(field.<span>type</span>, field_value);</span>
<span>            }</span>
<span>            print_literal(<span>"}"</span>);</span>
<span>        },</span>
<span>        <span>else</span> =&gt; <span>comptime</span> <span>unreachable</span>,</span>
<span>    }</span>
<span>}</span>
<span></span>
<span><span>fn</span><span> print_u32</span>(value: <span>u32</span>) <span>void</span> {</span>
<span>    std.debug.print(<span>"{d}"</span>, .{value});</span>
<span>}</span>
<span></span>
<span><span>fn</span><span> print_string</span>(value: []<span>const</span> <span>u8</span>) <span>void</span> {</span>
<span>    std.debug.print(<span>"<span>\"</span>{s}<span>\"</span>"</span>, .{value});</span>
<span>}</span>
<span></span>
<span><span>fn</span><span> print_literal</span>(literal: []<span>const</span> <span>u8</span>) <span>void</span> {</span>
<span>    std.debug.print(<span>"{s}"</span>, .{literal});</span>
<span>}</span></code></pre>
          </figure>
          <p>
            Our <code>print</code> is set up exactly as our <code>f</code>
            before — the first argument is a comptime-known dispatch parameter.
            If <code>T</code> is an int or a string, the compiler calls <code>print_u32</code> or <code>print_string</code> directly.
          </p>
          <p>
            The third case is more complicated. First, we use <code>@typeInfo</code> to get a comptime value describing our type, and,
            in particular, the list of fields it has. Then, we iterate this list
            and recursively print each field. Note that although the list of
            fields is known in its entirety, we can’t <code>comptime for</code>
            it, we need <code>inline for</code>. This is because the <em>body</em> of our loop depends on the runtime
            <code>value</code>, and can’t be fully evaluated at compile time.
            This might be easier to see if you think in terms of functions. The
            <code>for</code> loop is essentially a map:
          </p>

          <figure>
            <pre><code><span>map :: [a] -&gt; (a -&gt; b) -&gt; [b]</span>
<span>map xs f = ...</span></code></pre>
          </figure>
          <p>
            If both <code>xs</code> and <code>f</code> are comptime-known, you
            can evaluate the entire loop at compile time. But in our case <code>f</code> actually closes over a runtime value, so we can’t evaluate
            everything. Still, we can specialize on the first argument, which
            <em>is</em> known at compile time. This is precisely the difference
            between <code>comptime</code> and <code>inline</code>
            <code>for</code>.
          </p>
        </section>
        <section id="No-DSLs">
          <h2>
            <a href="#No-DSLs">No DSLs </a>
          </h2>
          <p>
            Many meta programming systems, like macros in Lisp or Rust, not only
            <em>produce</em> arbitrary code, but also take arbitrary custom
            syntax as input, as long as parentheses are matched:
          </p>

          <figure>
            <pre><code><span><span>use</span> inline_python::python;</span>
<span></span>
<span><span>let</span> <span>who</span> = <span>"world"</span>;</span>
<span><span>let</span> <span>n</span> = <span>5</span>;</span>
<span>python! {</span>
<span>    <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>'n</span>):</span>
<span>        <span>print</span>(i, <span>"Hello"</span>, <span>'who</span>)</span>
<span>    <span>print</span>(<span>"Goodbye"</span>)</span>
<span>}</span></code></pre>
          </figure>
          <p>
            Zig doesn’t have any extension points for custom syntax. Indeed, you
            can’t pass Zig <em>syntax</em> (code) to comptime functions at all!
            Everything operates on Zig values. That being said, Zig is very
            lightweight when it comes to describing free-form data, so this
            isn’t much of a hindrance. <em>And</em>
            in any case, you can always pass your custom syntax as a comptime
            string. This is exactly how “printf” works:
          </p>

          <figure>
            <pre><code><span><span>pub</span> <span>fn</span><span> print</span>(<span>comptime</span> fmt: []<span>const</span> <span>u8</span>, args: <span>anytype</span>) <span>void</span></span></code></pre>
          </figure>
          <p>
            Here, <code>fmt</code> is an embedded DSL, which is checked at
            compile time to match the arguments.
          </p>
        </section>
        <section id="No-RTTI">
          <h2>
            <a href="#No-RTTI">No RTTI </a>
          </h2>
          <p>
            Zig printing code looks suspiciously close to how you’d do this sort
            of thing in a dynamic language like Python. In fact, it is <em>precisely</em> that same code, except that it is specialized over
            runtime type information Python has to enable this sort of thing.
            Furthermore, Zig actually <em>requires</em> that all type meta
            programming is specialized away. Types as values <em>only</em> exist
            at compile time. Still, looking at our print, we might be concerned
            over code size — we are effectively generating a fresh copy of <code>print</code> for any data structure. Our code will be smaller, and
            will compile faster if there’s just a single <code>print</code> that
            takes an opaque pointer and runtime parameter describing the type of
            the value (its fields and offsets). So let’s roll our own runtime
            type information. For our example, we support ints, strings, and
            structs with fields. For fields, our RTTI should include their names
            and offsets:
          </p>

          <figure>
            <pre><code><span><span>const</span> RTTI = <span>union</span>(<span>enum</span>) {</span>
<span>    <span>u32</span>,</span>
<span>    string,</span>
<span>    @<span>"struct"</span>: []<span>const</span> Field,</span>
<span></span>
<span>    <span>const</span> Field = <span>struct</span> {</span>
<span>        name: []<span>const</span> <span>u8</span>,</span>
<span>        offset: <span>u32</span>,</span>
<span>        rtti: RTTI,</span>
<span>    };</span>
<span>};</span></code></pre>
          </figure>
          <p>
            The printing itself is not particularly illuminating, we just need
            to cast an opaque pointer according to RTTI:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> print_dyn</span>(T: RTTI, value: <span>*</span><span>const</span> anyopaque) <span>void</span> {</span>
<span>    <span>switch</span> (T) {</span>
<span>        .<span>u32</span> =&gt; {</span>
<span>            <span>const</span> value_u32: <span>*</span><span>const</span> <span>u32</span> =</span>
<span>                <span>@alignCast</span>(<span>@ptrCast</span>(value));</span>
<span>            print_u32(value_u32.<span>*</span>);</span>
<span>        },</span>
<span>        .string =&gt; {</span>
<span>            <span>const</span> value_string: <span>*</span><span>const</span> []<span>const</span> <span>u8</span> =</span>
<span>                <span>@alignCast</span>(<span>@ptrCast</span>(value));</span>
<span>            print_string(value_string.<span>*</span>);</span>
<span>        },</span>
<span>        .@<span>"struct"</span> =&gt; <span>|</span>info<span>|</span> {</span>
<span>            print_literal(<span>"{"</span>);</span>
<span>            <span>var</span> space: []<span>const</span> <span>u8</span> = <span>""</span>;</span>
<span>            <span>for</span> (info) <span>|</span>field<span>|</span> {</span>
<span>                print_literal(space);</span>
<span>                space = <span>", "</span>;</span>
<span></span>
<span>                print_literal(field.name);</span>
<span>                print_literal(<span>" = "</span>);</span>
<span>                <span>const</span> field_ptr: <span>*</span><span>const</span> anyopaque =</span>
<span>                    <span>@as</span>([<span>*</span>]<span>const</span> <span>u8</span>, <span>@ptrCast</span>(value)) <span>+</span> field.offset;</span>
<span>            }</span>
<span>            print_literal(<span>"}"</span>);</span>
<span>        },</span>
<span>    }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            Finally, we need to compute <code>RTTI</code>, which amounts to
            taking comptime-only Zig type info and extracting important bits
            into an <code>RTTI</code> struct which is computed at compile time,
            but can exist at runtime as well:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> reflect</span>(<span>comptime</span> T: <span>type</span>) RTTI {</span>
<span>    <span>comptime</span> {</span>
<span>        <span>if</span> (T <span>==</span> <span>u32</span>) <span>return</span> .<span>u32</span>;</span>
<span>        <span>if</span> (T <span>==</span> []<span>const</span> <span>u8</span>) <span>return</span> .string;</span>
<span>        <span>switch</span> (<span>@typeInfo</span>(T)) {</span>
<span>            .@<span>"struct"</span> =&gt; <span>|</span>info<span>|</span> {</span>
<span>                <span>var</span> fields: [info.fields.len]Field = <span>undefined</span>;</span>
<span>                <span>for</span> (<span>&amp;</span>fields, info.fields) <span>|</span><span>*</span>slot, field<span>|</span> {</span>
<span>                    slot.<span>*</span> = .{</span>
<span>                        .name = field.name,</span>
<span>                        .offset = <span>@offsetOf</span>(T, field.name),</span>
<span>                        .rtti = reflect(field.<span>type</span>),</span>
<span>                    };</span>
<span>                }</span>
<span>                <span>const</span> fields_frozen = fields;</span>
<span>                <span>return</span> .{ .@<span>"struct"</span> = <span>&amp;</span>fields_frozen };</span>
<span>            },</span>
<span>            <span>else</span> =&gt; <span>unreachable</span>,</span>
<span>        }</span>
<span>    }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The call site is illustrative: we need <code>comptime</code> to <em>compute</em> the type information, but then we reify it as some
            real bytes in the binary, and use it as runtime value when calling
            <code>print_dyn</code>.
          </p>

          <figure>
            <pre><code><span><span>pub</span> <span>fn</span><span> main</span>() <span>void</span> {</span>
<span>    <span>const</span> s: S = .{</span>
<span>        .int = <span>1</span>,</span>
<span>        .string = <span>"hello"</span>,</span>
<span>        .nested = .{ .int = <span>2</span> },</span>
<span>    };</span>
<span>    print_dyn(<span>comptime</span> RTTI.reflect(S), <span>&amp;</span>s);</span>
<span>}</span></code></pre>
          </figure>
        </section>
        <section id="No-New-API">
          <h2>
            <a href="#No-New-API">No New API </a>
          </h2>
          <p>
            You can use Zig comptime to create new types. That’s how a <a href="https://matklad.github.io/2025/03/19/comptime-zig-orm.html">Zig ORM</a> can work. However, it is impossible to add methods to
            generated types, they must be inert bundles of fields. In Rust, when
            you use a derive macro, it can arbitrarily extend type’s public API,
            and you need to read proc macro docs (or look at the generated code)
            to figure out what’s available. In Zig, types’s API is always hand
            written, but it can use comptime reflection internally.
          </p>
          <p>
            So, if you are building a JSON serialization library in Zig, you
            can’t add <code>.to_json</code> method to user-types. You’ll
            necessarily have to supply a normal top-level function like
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> to_json</span>(<span>comptime</span> T: <span>type</span>, value: T, writer: Writer) <span>!</span><span>void</span> {</span>
<span>    ...</span>
<span>}</span></code></pre>
          </figure>
          <p>
            If you want to make sure that types explicitly opt-in JSON
            serialization, you need to ask the user to mark types specially:
          </p>

          <figure>
            <pre><code><span><span>const</span> Person = <span>struct</span> {</span>
<span>    first_name: []<span>const</span> <span>u8</span>,</span>
<span>    last_name: []<span>const</span> <span>u8</span>,</span>
<span></span>
<span>    <span>pub</span> <span>const</span> JSONOptions = .{</span>
<span>        .style = .camelCase,</span>
<span>    };</span>
<span>}</span></code></pre>
          </figure>
          <p>
            With this setup, <code>to_json</code> can only allow primitives and
            types with <code>JSONOptions</code>.
          </p>
        </section>
        <section id="No-IO">
          <h2>
            <a href="#No-IO">No IO </a>
          </h2>
          <p>
            Last but not least, Zig comptime does not allow any kind of input
            output. There isn’t even any kind of sandbox, as there are no IO
            facilities in the first place. So, while compiling the code, you
            can’t talk to your database to generate the schema. In exchange,
            compile time evaluation is hermetic, reproducible, safe, and
            cacheable.
          </p>
          <p>
            If you do need to talk to the database at build time, you can still
            do that, just through the build system! Zig’s <code>build.zig</code>
            is a general purpose build system, which easily supports the
            use-case of running an arbitrary Zig program to generate arbitrary
            Zig code which can then be normally
            <code>@import</code>ed.
          </p>
        </section>
        <section id="El-Disco">
          <h2>
            <a href="#El-Disco">El Disco </a>
          </h2>
          <p>
            <a href="https://www.tedinski.com/2018/01/30/the-one-ring-problem-abstraction-and-power.html">Any abstraction has two sides</a>. Powerful abstractions are useful
            because they are more expressive. But the flip-side is that
            abstraction-using code becomes harder to reason about, because the
            space of what it can possibly do is so vast. This dependency is not
            zero sum. A good abstraction can be simultaneously more powerful and
            easier to reason about than a bad one.
          </p>
          <p>
            Meta programming is one of the more powerful abstractions. It is
            very capable in Zig, and comes at a significant cost — Zig doesn’t
            have declaration-site type checking of comptime code. That being
            said, I personally find Zig’s approach to be uniquely tidy, elegant,
            and neat! It does much less than alternative systems, but ends
            extremely ergonomic in practice, and <em>relatively</em> easy to
            wrap ones head around.
          </p>
        </section>
      </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The skill of the future is not 'AI', but 'Focus' (164 pts)]]></title>
            <link>https://www.carette.xyz/posts/focus_will_be_the_skill_of_the_future/</link>
            <guid>43744394</guid>
            <pubDate>Sun, 20 Apr 2025 15:28:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.carette.xyz/posts/focus_will_be_the_skill_of_the_future/">https://www.carette.xyz/posts/focus_will_be_the_skill_of_the_future/</a>, See on <a href="https://news.ycombinator.com/item?id=43744394">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>If you frequent Hacker News regurlarly, you have likely noticed the buzz around engineers using AI
(specifically Large Language Models, or LLMs) to tackle Computer Science problems.</p>
<p>I want to be clear: <strong>I’m not against LLMs</strong>.<br>
LLMs are incredibly powerful tools, and <strong>can</strong> be a huge boon to engineers.
They can automate repetitive tasks, generate code snippets, help with brainstorming, assist in debugging, …
and this can frees up engineers’ time and mental energy, which could be channeled into more complex, creative problem-solving.<br>
But, like any tool, LLMs should be used <strong>wisely</strong>.<br>
LLMs can hallucinate, exhibit inconsistencies (especially with self-reflection models), and harbor biases. These limitations mean that LLM outputs require careful review before they can be trusted.</p>
<p>A key concern with LLMs lies in their training data.<br>
The data can be biased, contradictory sometimes, but those data contain solutions to known problems.<br>
If an engineer wants to “reinvent the wheel,” an LLM might offer a solution (good or bad, depending on the prompt). But when faced with truly <em>novel</em> problems, LLMs often provide unreliable responses, placing the burden of error detection squarely on the engineer.</p>
<p>This reliance on readily available solutions, particularly for familiar problems, creates a real risk: engineers may inadvertently atrophy their own problem-solving skills, hindering their ability to tackle truly novel challenges.<br>
The solution lies is <strong>balance</strong>, and a focus on the “why”, not just the “what”.<br>
Engineers should strive to understand the <em>reasoning</em> behind LLM-generated solutions, not simply accept them blindly.  Blind acceptance shifts the focus from <em>solving</em> problems to merely <em>obtaining</em> a solution.  Crucially, solving complex problems often depends on mastering simpler and foundational skills, which the engineer might lose quickly.</p>
<p>This idea summarizes why I disagree with those who equate the LLM revolution to the rise of search engines, like Google in the 90s.
Search enginers offer a good choice between <em>Exploration</em> (crawl through the list and pages of results) and
<em>Exploitation</em> (click on the top result).<br>
LLMs, however, do not give this choice, and tend to encourage immediate exploitation instead.
Users may explore if the first solution <strong>does not work</strong>, but the first
choice is <strong>always</strong> to exploit.<br>
<em>Exploitation</em> and <em>exploration</em> are complementary. Remove the exploration and you will introduce more and more instability into the exploitation process.</p>
<p>Computer Science emerged because Humans needed <strong>tools</strong> to solve problems faster and wanted to <strong>focus</strong> on the real problems, not repetitive tasks.
Humans built machines to accelerate problem-solving, but engineers remained the masters of the algorithms.<br>
I fear we’re losing our grip on this mastery.
Not because engineers are becoming less and less intelligent, but because the pressure to deliver solutions quickly is paramount.<br>
In embracing these “fast-paced solutions”, we risk losing a fundamental skill: <em>focus</em>. Because focus, like any skill, requires practice.</p>
<p>This is a worrying trend. If engineers become less adept at solving complex problems, what does the future hold?
Will our ability to tackle complex challenges rest solely on self-reflecting AIs, rather than human ingenuity?</p>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jagged AGI: o3, Gemini 2.5, and everything after (188 pts)]]></title>
            <link>https://www.oneusefulthing.org/p/on-jagged-agi-o3-gemini-25-and-everything</link>
            <guid>43744173</guid>
            <pubDate>Sun, 20 Apr 2025 14:55:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.oneusefulthing.org/p/on-jagged-agi-o3-gemini-25-and-everything">https://www.oneusefulthing.org/p/on-jagged-agi-o3-gemini-25-and-everything</a>, See on <a href="https://news.ycombinator.com/item?id=43744173">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>Amid today’s AI boom, it’s disconcerting that we still don’t know how to measure how smart, creative, or empathetic these systems are. Our tests for these traits, never great in the first place, were made for humans, not AI. </span><a href="https://ai-analytics.wharton.upenn.edu/generative-ai-labs/research-and-technical-reports/tech-report-prompt-engineering-is-complicated-and-contingent/" rel="">Plus, our recent paper testing prompting techniques</a><span> finds that AI test scores can change dramatically based simply on how questions are phrased. Even famous challenges like the Turing Test, where humans try to differentiate between an AI and another person in a text conversation, were designed as thought experiments at a time when such tasks seemed impossible. But now that </span><a href="https://arxiv.org/pdf/2503.23674" rel="">a new paper shows that AI passes the Turing Test</a><span>, we need to admit that we really don’t know what that actually means. </span></p><p><span>So, it should come as little surprise that one of the most important milestones in AI development, Artificial General Intelligence, or AGI, is badly defined and much debated. Everyone agrees that it has something to do with the ability of AIs to perform human-level tasks, though no one agrees whether this means expert or average human performance, or how many tasks and which kinds an AI would need to master to qualify. Given the definitional morass surrounding AGI, illustrating its nuances and history from its precursors to its initial coining by Shane Legg, Ben Goertzel and Peter Voss to today is challenging. As an experiment in both substance and form (and speaking of potentially intelligent machines) I delegated the work entirely to AI. I had Google Deep Research put together a </span><a href="https://docs.google.com/document/d/1VJ-OzBRJUChgUB0L0--dbdNjU2e-14PXnoTqNFYgXNQ/edit?tab=t.0" rel="">really solid 26 page summary on the topic</a><span>. I then had HeyGen turn it into a video podcast discussion between a twitchy AI-generated version of me and an AI-generated host. It’s not actually a bad discussion (though I don’t fully agree with AI-me), but every part of it, from the research to the video to the voices is 100% AI generated.</span></p><p><span>Given all this, it was interesting to see </span><a href="https://marginalrevolution.com/marginalrevolution/2025/04/o3-and-agi-is-april-16th-agi-day.html" rel="">this post </a><span>by influential economist and close AI observer Tyler Cowen declaring that o3 is AGI. Why might he think that?</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg" width="370" height="368.8787878787879" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1316,&quot;width&quot;:1320,&quot;resizeWidth&quot;:370,&quot;bytes&quot;:243143,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.oneusefulthing.org/i/161512556?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65e30c6c-abb0-46e9-9699-cbe7c4211185_1320x1507.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>First, a little context. Over the past couple of weeks, two new AI models, Gemini 2.5 Pro from Google and o3 from OpenAI were released. These models, along with a set of slightly less capable but faster and cheaper models (Gemini 2.5 Flash, o4-mini, and Grok-3-mini), represent a pretty</span><a href="https://epoch.ai/data/ai-benchmarking-dashboard" rel=""> large leap in benchmarks</a><span>. But benchmarks aren’t everything, as Tyler pointed out. For a real-world example of how much better these models have gotten, we can turn to </span><a href="https://a.co/d/2qRbAxA" rel="">my book</a><span>. To illustrate a chapter on how AIs can generate ideas, a little over a year ago I asked ChatGPT-4 to come up with marketing slogans for a new cheese shop:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png" width="389" height="304.7465437788018" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:510,&quot;width&quot;:651,&quot;resizeWidth&quot;:389,&quot;bytes&quot;:159134,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.oneusefulthing.org/i/161510512?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>Today I gave the latest successor to GPT-4, o3, an </span><em>ever so slightly</em><span> more involved version of the same prompt: “</span><strong>Come up with 20 clever ideas for marketing slogans for a new mail-order cheese shop. Develop criteria and select the best one. Then build a financial and marketing plan for the shop, revising as needed and analyzing competition. Then generate an appropriate logo using image generator and build a website for the shop as a mockup, making sure to carry 5-10 cheeses that fit the marketing plan.” </strong><span>With that single prompt, in less than two minutes, the AI not only provided a list of slogans, but ranked and selected an option, did web research, developed a logo, built marketing and financial plans, and launched a demo website for me to react to. The fact that my instructions were vague, and that common sense was required to make decisions about how to address them, was not a barrier.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png" width="1456" height="723" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:723,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2019800,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.oneusefulthing.org/i/161510512?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>In addition to being, presumably, a larger model than GPT-4, o3 also works as a </span><a href="https://www.oneusefulthing.org/p/a-new-generation-of-ais-claude-37" rel="">Reasoner </a><span>- you can see its “thinking” in the initial response. It also is an agentic model, one that can use tools and decide how to accomplish complex goals. You can see how it took multiple actions with multiple tools, including web searches and coding, to come up with the extensive results that it did.</span></p><p><span>And this isn’t the only extraordinary examples, o3 can also do an impressive job guessing locations from photos if you just give it an image and prompt </span><strong>“be a geo-guesser”</strong><span> (with some quite profound privacy implications). Again, you can see the agentic nature of this model at work, as it zooms into parts of the picture, adds web searches, and does multi-step processes to get the right answer.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png" width="525" height="434.4951923076923" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1205,&quot;width&quot;:1456,&quot;resizeWidth&quot;:525,&quot;bytes&quot;:3215925,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.oneusefulthing.org/i/161510512?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Or I gave o3 a large dataset of historical machine learning systems as a spreadsheet and asked </span><strong>“figure out what this is and generate a report examining the implications statistically and give me a well-formatted PDF with graphs and details”</strong><span> and got a full analysis with a single prompt. (I did give it some feedback to make the PDF better, though, as you can see).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png" width="1456" height="669" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:669,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:694441,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.oneusefulthing.org/i/161512556?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>This is all pretty impressive stuff and you should experiment with these models on your own. </span><a href="https://gemini.google.com/u/1/app" rel="">Gemini 2.5 Pro</a><span> is free to use and as “smart” as o3, though it lacks the same full agentic ability. If you haven’t tried it or o3, take a few minutes to do it now. Try </span><a href="https://x.com/emollick/status/1910534521998487709" rel="">giving Gemini an academic paper and asking it to turn the paper into a game</a><span> or have it brainstorm with you for startup ideas, or just ask for the AI</span><a href="https://g.co/gemini/share/b2ce16d04017" rel=""> to impress you</a><span> (and then keep saying “more impressive”). Ask the Deep Research option to do a research report on your industry, or to research a purchase you are considering, or to </span><a href="https://bsky.app/profile/emollick.bsky.social/post/3lmdminw4m22o" rel="">develop a marketing plan for a new product</a><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png" width="374" height="253.2753623188406" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:514,&quot;width&quot;:759,&quot;resizeWidth&quot;:374,&quot;bytes&quot;:169048,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.oneusefulthing.org/i/161512556?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>You might find yourself “feeling the AGI” as well. Or maybe not. Maybe the AI failed you, even when you gave it the exact same prompt I used. If so, you just encountered the jagged frontier.</p><p><a href="https://www.oneusefulthing.org/p/centaurs-and-cyborgs-on-the-jagged" rel="">My co-authors and I coined the term “Jagged Frontier”</a><span> to describe the fact that AI has surprisingly uneven abilities. An AI may succeed at a task that would challenge a human expert but fail at something incredibly mundane. For example, consider this puzzle, a variation on a classic old brainteaser (a concept first explored by </span><a href="https://x.com/colin_fraser" rel="">Colin Fraser</a><span> and expanded by </span><a href="https://x.com/goodside/status/1790912819442974900" rel="">Riley Goodside</a><span>): </span><em>"A young boy who has been in a car accident is rushed to the emergency room. Upon seeing him, the surgeon says, "I can operate on this boy!" How is this possible?"</em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png" width="443" height="461.75132275132273" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a0b6c099-fc6c-42c2-b78b-380972895420_945x985.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:985,&quot;width&quot;:945,&quot;resizeWidth&quot;:443,&quot;bytes&quot;:66935,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.oneusefulthing.org/i/161512556?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span> o3 insists the answer is “the surgeon is the boy’s mother,” which is wrong, as a careful reading of the brainteaser will show. Why does the AI come up with this incorrect answer? Because that is the answer to the classic version of the riddle, meant to expose unconscious bias: </span><em>“A father and son are in a car crash, the father dies, and the son is rushed to the hospital. The surgeon says, 'I can't operate, that boy is my son,' who is the surgeon?”</em><span> The AI has “seen” this riddle in its training data so much that even the smart o3 model fails to generalize to the new problem, at least initially. And this is just one example of the kinds of issues and hallucinations that even advanced AIs can fall prey to, showing how jagged the frontier can be.</span></p><p><span>But the fact that the AI often messes up on this particular brainteaser does not take away from the fact that it can </span><a href="https://scale.com/leaderboard/enigma_eval" rel="">solve much harder brainteasers</a><span>, or that it can do the other impressive feats I have demonstrated above. That is the nature of the Jagged Frontier. In some tasks, AI is unreliable. In others, it is superhuman. You could, of course, say the same thing about calculators, but it is also clear that AI is different. It is already demonstrating general capabilities and performing a wide range of intellectual tasks, including those that it is not specifically trained on. Does that mean that o3 and Gemini 2.5 are AGI? Given the definitional problems, I really don’t know, but I do think they can be credibly seen as a form of “Jagged AGI” - superhuman in enough areas to result in real changes to how we work and live, but also unreliable enough that human expertise is often needed to figure out where AI works and where it doesn’t. Of course, models are likely to become smarter, and a good enough Jagged AGI may still beat humans at every task, including in ones the AI is weak in.</span></p><p><span>Returning to Tyler’s post, you will notice that, despite thinking we have achieved AGI, he doesn’t think that threshold </span><a href="https://marginalrevolution.com/marginalrevolution/2025/02/why-i-think-ai-take-off-is-relatively-slow.html" rel="">matters much to our lives in the near term</a><span>. That is because, as many people have pointed out, technologies do not instantly change the world, no matter how compelling or powerful they are. Social and organizational structures change much more slowly than technology, and technology itself takes time to diffuse. Even if we have AGI today, we have years of trying to figure out how to integrate it into our existing human world.</span></p><p><span>Of course, that assumes that AI acts like </span><a href="https://knightcolumbia.org/content/ai-as-normal-technology" rel="">a normal technology</a><span>, and one whose jaggedness will never be completely solved. There is the possibility that this may not be true. The agentic capabilities we're seeing in models like o3, like the ability to decompose complex goals, use tools, and execute multi-step plans independently, might actually accelerate diffusion dramatically compared to previous technologies. If and when AI can effectively navigate human systems on its own, rather than requiring integration, we might hit adoption thresholds much faster than historical precedent would suggest.</span></p><p>And there's a deeper uncertainty here: are there capability thresholds that, once crossed, fundamentally change how these systems integrate into society? Or is it all just gradual improvement? Or will models stop improving in the future as LLMs hit a wall? The honest answer is we don't know.</p><p><span>What's clear is that we continue to be in uncharted territory. The latest models represent something qualitatively different from what came before, whether or not we call it AGI. Their agentic properties, combined with their jagged capabilities, create a genuinely novel situation with few clear analogues. It may be that history continues to be the best guide, and that figuring out how to successfully apply AI in a way that shows up in the economic statistics may be a process measured in decades. Or it might be that we are on the edge of some sort of </span><a href="https://www.nytimes.com/2025/04/03/technology/ai-futures-project-ai-2027.html" rel="">faster take-off</a><span>, where AI-driven change sweeps our world suddenly. Either way, those who learn to navigate this jagged landscape now will be best positioned for what comes next… whatever that is.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://www.oneusefulthing.org/p/on-jagged-agi-o3-gemini-25-and-everything?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.oneusefulthing.org/p/on-jagged-agi-o3-gemini-25-and-everything?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png" width="349" height="219.13953488372093" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/afdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:864,&quot;width&quot;:1376,&quot;resizeWidth&quot;:349,&quot;bytes&quot;:2622025,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.oneusefulthing.org/i/161512556?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why on Earth is OpenAI buying Windsurf? (197 pts)]]></title>
            <link>https://theahura.substack.com/p/tech-things-openai-buys-windsurf</link>
            <guid>43743993</guid>
            <pubDate>Sun, 20 Apr 2025 14:28:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf">https://theahura.substack.com/p/tech-things-openai-buys-windsurf</a>, See on <a href="https://news.ycombinator.com/item?id=43743993">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>The quiet news of the last few days was the leak/announcement of a $3 billion OpenAI acquisition of Windsurf. That's not the largest private acquisition ever made — that honor goes to Google's $30 billion acquisition of Wiz a few months prior</span><strong> </strong><span>— but man, it's up there! $3B is the kind of exit startup founders dream about. Especially for a startup that's been around for 2 years, </span><a href="https://news.ycombinator.com/item?id=42127882" rel="">with its current branding for about 5 months</a><span>.</span></p><p>I assume most people don't know what Windsurf is, which is fair because it has so few users that when you try to Google for that information you get data about the sport.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png" width="1040" height="409" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:409,&quot;width&quot;:1040,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:117182,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://theahura.substack.com/i/161689970?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Thanks Gemini!</figcaption></figure></div><p><span>Maybe that’s unfair, supposedly the company </span><a href="https://www.latent.space/p/windsurf?open=false" rel="">has over a million users</a><span>. But I'm always a bit skeptical of numbers like that. A person who uses Windsurf every day is obviously in a different category than one that installed the tool to play around for five minutes and quickly discarded it. This slipperiness has always been one of the benefits of working in the world of privately held companies. Anecdotally I know only one person who uses Windsurf, and I only kinda sorta know that person because he's just a guy that I met at an </span><a href="https://theahura.substack.com/p/notes-from-the-sf-party-scene" rel="">SF house party</a><span>.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-1-161689970" target="_self" rel="">1</a></span></p><p>If you aren't familiar with Windsurf, you may know it by its previous name, Codeium. And if you aren't familiar with Codeium, you may know its primary competition, a company called Cursor. And if you don't know what Cursor is, a) you might know what GitHub Copilot is, and b) how did you find my blog?</p><p>All of these products are roughly in the same category of "AI tools for software engineers". They all have basically the same form factor too — they integrate AI models directly into your coding workflow. And traditionally they operate on three levels of granularity:</p><ul><li><p>Auto-complete. As you type the AI will suggest the rest of the line or function, which you can generally accept with a single button press.</p></li><li><p>Sidebar Q&amp;A. The code window itself will have an integrated sidebar where you can ask models to modify a few files. You'll get a diff, which you can then choose to apply or modify.</p></li><li><p>Agentic flows. The term "agent" is wildly underspecified, but in the AI coding space the term has generally come to mean "an AI model operates in a loop over an entire code base, often with a very vague or high level prompt as a starting point, and is equipped with tools to write, run, and otherwise analyze code and the computer system".</p></li></ul><p>Different companies aim to be best in class along different verticals. Some are better at the auto complete (Copilot), others at the agent flow (Claude Code). Some aim to be the best for non-technical people (Bolt or Replit), others for large enterprises (again, Copilot). Still, all of this "differentiation" ends up making a 1-2% difference in product. In fact, I can't stress enough how much the UX and core functionality of these tools is essentially identical. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png" width="1159" height="692" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:692,&quot;width&quot;:1159,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Cursor Download Free (Windows) - 0.48.9 | Softpedia&quot;,&quot;title&quot;:&quot;Cursor Download Free (Windows) - 0.48.9 | Softpedia&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="Cursor Download Free (Windows) - 0.48.9 | Softpedia" title="Cursor Download Free (Windows) - 0.48.9 | Softpedia" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>A screencap of cursor. This has become the ‘canonical’ UX for vibe coding — a sidebar with integrated chat that is able to call out to tools and directly change code using diffs.</figcaption></figure></div><p>These all would have pejoratively been known as "GPT wrappers" just two years ago, because they do not actually compete on the model layer but rather allow users to choose and switch between any of the big LLM providers. To really emphasize how interchangeable these AI code assistants all are, I use an even lesser known tool called Avante, an entirely free and open source neovim plugin. It does the same things as all the other tools. I like it because I don't have to leave vim.</p><p><span>But the similarities of these tools does not take away from how game changing they are. Once you get used to a form of AI powered coding, you cannot go back. The real issue with all of these products is that they are</span><em> </em><span>too</span><em> </em><span>easily verticalized. Anyone who wants to spin up a version of Windsurf with one slight change that targets a tiny market segment can do so fairly easily — again, I'm on Avante entirely because it supports vim. That means the addressable market for any of these companies may actually go </span><em>down</em><span> over time as more competitors and free alternatives enter the market, even as the number of "programmers" goes up.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-2-161689970" target="_self" rel="">2</a></span><span> In point of fact, even though I love the ingenuity behind Cursor (which really spearheaded the current AI coding paradigm) I have openly said that their long term opportunities are slim. Even though Cursor had significant first mover advantage, they have no moat or stickiness. As with the rest of the AI market, switching cost remains extremely low, and there is simply no reason to use Cursor when you can use a free version or one with better enterprise support.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-3-161689970" target="_self" rel="">3</a></span><span> Cursor isn't even living on its own platform — it's a fork of VSCode. I am personally convinced that their only long term exit opportunity is an acquisition by Microsoft, and even that seems less and less likely as Satya puts more resources into the already-VSCode-native Copilot as a real competitor.</span></p><p><span>All of which makes the $3B price tag for Windsurf seem eye wateringly high. Compared to Cursor, Windsurf has fewer users, has been around for less time, has less brand recognition, and has diminishing prospects for future growth. It’s not as tied to VSCode, which is a plus, I guess. But it all begs the question: why on </span><em>earth</em><span> is OpenAI paying so much?</span></p><p><span>This is especially strange in the context of OpenAI's financial situation. </span><a href="https://www.thealgorithmicbridge.com/p/google-is-winning-on-every-ai-front" rel="">Smart</a><span> </span><a href="https://www.wired.com/story/google-openai-gemini-chatgpt-artificial-intelligence/" rel="">observers</a><span> </span><a href="https://www.reddit.com/r/singularity/comments/1hh03ri/google_is_winning_the_ai_race/" rel="">have</a><span> </span><a href="https://www.pymnts.com/news/artificial-intelligence/2025/google-debuts-touted-gemini-winner-take-all-ai-model-race" rel="">caught</a><span> </span><a href="https://medium.com/artificial-corner/how-google-quietly-took-the-lead-in-the-ai-race-with-gemini-2-5-c98dfb58b6a1" rel="">on</a><span> </span><a href="https://venturebeat.com/ai/from-catch-up-to-catch-us-how-google-quietly-took-the-lead-in-enterprise-ai/" rel="">to</a><span> Google's inherent advantages in the space, something that I first publicly called out as early as </span><a href="https://theahura.substack.com/p/tech-things-gpt-pro-and-the-state?" rel="">December</a><span> and more fully </span><a href="https://theahura.substack.com/p/tech-things-gemini-25-and-the-bull?utm_source=publication-search" rel="">two weeks ago</a><span>. OpenAI needs to shore up both its access to compute and its access to data in order to compete. But it's once-sterling relationship with its previous patron, Microsoft, has frayed significantly. This has essentially forced the company to go to SoftBank (yes, that SoftBank) for additional capital.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-4-161689970" target="_self" rel="">4</a></span><span> </span></p><p><span>It's true that OpenAI managed to get $40bn committed, and it's also true that this is the largest amount of capital ever raised by a privately held company. But they're going against </span><em>Google</em><span>, one of the most valuable companies in the entire world, and extremely profitable to boot. That's a hell of a war chest to compete with.</span></p><p><span>In that light, the decision to spend 3 out of 40 of those billions is even harder to rationalize.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-5-161689970" target="_self" rel="">5</a></span><span> Even worse, it's not yet clear OpenAI actually </span><em>has </em><span>$40bn to spend — so far they've only got $10bn actually lined up, with the rest being held by SoftBank contingent on OpenAI actually becoming a for-profit company by the end of the year.</span></p><p>It seems pretty obvious that Windsurf will not help OpenAI get more compute. Maybe Windsurf is providing OpenAI access to data? There's certainly some possibility that this is the case — though it makes me wonder just how bad OpenAI's relationship with Microsoft has gotten if they no longer have access to GitHub, which surely dwarfs any amount of code that Windsurf could provide.</p><p><span>The other possibility is that this is entirely a long term distribution play, akin to Facebook buying WhatsApp or Instagram. People criticized those deals for being overpriced too. OpenAI may think that Windsurf will be a crown jewel in how people access GPT models. There's some sense in this — OpenAI has also announced a social media project, likely also an attempt at maintaining lines to unique data sources while providing more native ways to improve distribution and "</span><a href="https://gwern.net/complement" rel="">commoditize their complement</a><span>".</span></p><p><span>But the issue that they will inevitably run into with Windsurf is that GPT just isn't the best in class for programming. Everyone who's using Windsurf is almost definitely using Claude or Gemini. Even though the "GPT wrapper" term was always meant as an insult, it is in practice a huge table stakes feature to be able to wrap around many different LLM providers. That flexibility is what allows a company like Windsurf to ride the machine learning wave, buoyed along by everyone else's investments. Cursor really only took off when Claude suddenly got really good at programming, after all. If Windsurf ends up being tied exclusively to GPT, many of its users may leave the platform simply because it is now a worse platform. But if there isn't any vendor lock in, we're back to square one — what is the </span><em>point</em><span>?</span></p><p>Personally, I don't get it. Maybe someone smarter than I am (or more connected than I am) can help me figure it out. But for now, I'm chalking this particular check size as a symptom of the AI market being way too hot right now.</p><p>The other quiet news of the last few days is the dawning realization of just how quiet it has been. The last two weeks saw the release of 3 new OpenAI models — o3, o4-mini, and GPT 4.1 — as well as the new Llama 4 model family from Meta and Grok-3 from Grok. And…nothing. It's just crickets. In past months, a release calendar like this would have had headlines blazing. The hype train should be chugging at ridiculous speeds. But compared to what I'd expect, there's nothing.</p><p>The reason is obvious: Google is still in the lead. Take a look at these two charts.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png" width="1456" height="529" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7e34249c-afab-4751-87ae-0932f234e111_1584x575.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:529,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:137731,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://theahura.substack.com/i/161689970?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg" width="1456" height="1067" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1067,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Imagen&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="Imagen" title="Imagen" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Even though this chart is only two weeks old, it is already out of date. </figcaption></figure></div><p>The former is the current state of the LMSYS chatbot arena; the latter maps chatbot arena performance against price. The over under? There's no headlines because there's nothing to write about. "OpenAI takes second place" no one cares!</p><p><span>It’s still too early to write about ‘general consensus’ since the OpenAI models were released only a few days ago. And to their credit, those models </span><em>do </em><span>top many of the LLM benchmarks.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-6-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-6-161689970" target="_self" rel="">6</a></span><span> But so far, the </span><a href="https://www.reddit.com/r/ChatGPTCoding/comments/1k10ehv/openais_o3_and_o4mini_just_dethroned_gemini_25_pro/" rel="">reception has been extremely muted</a><span>, with many saying something like:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png" width="867" height="195" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:195,&quot;width&quot;:867,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:37315,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://theahura.substack.com/i/161689970?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Even though some of the new OpenAI models are quite powerful, they are simply too expensive and too slow for not enough extra juice. </p><p><span>I've already </span><a href="https://theahura.substack.com/p/tech-things-gemini-25-and-the-bull" rel="">written extensively about Google's Gemini 2.5 release</a><span>, which quickly became the go-to model for just about everything. What I didn't originally clock was just how much Google had shored up its model offerings all over the price/performance curve. Put bluntly: at every price point, the best model is a Google model.</span></p><p><span>That's not all. I mentioned rumors that Google has disallowed new publications; that is now confirmed as of </span><a href="https://arstechnica.com/ai/2025/04/deepmind-is-holding-back-release-of-ai-research-to-give-google-an-edge/" rel="">earlier this month</a><span>:</span></p><blockquote><p>Among the changes in the company’s publication policies is a six-month embargo before “strategic” papers related to generative AI are released. Researchers also often need to convince several staff members of the merits of publication, said two people with knowledge of the matter.</p></blockquote><p><span>Google has also apparently started offering </span><a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/google-accused-of-paying-employees-to-do-nothing-for-up-to-a-year-to-stifle-ai-talent-migration" rel="">extremely generous non-compete deals</a><span> to researchers, preferring to keep them on payroll doing nothing than have them go to competitors and leak secrets:</span></p><blockquote><p><a href="https://www.tomshardware.com/tag/google" rel="">Google</a><span> is making use of aggressive noncompete clauses and extended notice periods, contends former GoogDeepMinder Nando de Freitas in a recent post on X. In some cases, Google DeepMind’s employment contracts may lock an AI developer into doing nothing for as long as a year, notes </span><a href="https://www.businessinsider.com/google-deepmind-ai-talent-war-aggressive-noncompetes-2025-4" rel="">Business Insider</a><span>, to prevent its AI talent from moving to competing firms. That’s a long time away from working on the cutting edge in the rapidly developing world of AI.</span></p></blockquote><p>And finally, Google has continued to release and improve its TPU offerings on GCP, giving them yet another method to profit off the back of the AI boom — even if they don't win on the model, they can win by providing the underlying hardware.</p><p><span>With almost no fanfare, we all just woke up one day to a Google-dominated AI landscape. I have been </span><a href="https://theahura.substack.com/p/tech-things-eu-ai-act-biden-admin?utm_source=publication-search" rel="">critical of Sundar in the past</a><span>, but I have to hand it to him — sometimes the showmanship really is just a distraction from executing a slow but precise strategy.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-7-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-7-161689970" target="_self" rel="">7</a></span><span> Google is clearly now on a war footing. They are relentlessly poaching employees while trying to close up their shop as much as possible. The AI industry as a whole owes more to Google than any other organization. It's unclear how many other players in that industry will survive when cut off from Google's research. It's also unclear how much this will last in the face of a continuing DOJ antitrust suit. More on that in a different article, though.</span></p><p><span>One last thought. I've always been a staunch defender of capitalism and free markets, even though that's historically been an unpopular opinion in my particular social circle. Watching the LLM market, I can't help but feel extremely vindicated. Over the last 5 years, the cost per token has been driven down relentlessly even as model quality has skyrocketed. The brutal and bruising competition between the tech giants has left nothing but riches for the average consumer. There's an alternative world where all of this is priced so high that only the wealthiest businesses can justify a "GPT license", or where the government ends up keeping all the best AI technology for themselves. That world would objectively suck — not only would most people not be able to access the technology, there would also be significantly less interest in or ability to innovate. Just look at Google, which has finally risen like a beast from slumber to show the world what it means to innovate once more.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-8-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-8-161689970" target="_self" rel="">8</a></span><span> </span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-9-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-9-161689970" target="_self" rel="">9</a></span></p><p>Since we've been talking about things that didn't happen, I want to talk about one last notable absence: where the hell is Apple?</p><p><a href="https://theahura.substack.com/p/tech-things-gpt-pro-and-the-state" rel="">Something</a><span> </span><a href="https://theahura.substack.com/p/tech-things-deepseek-r1-and-the-arrival?utm_source=publication-search" rel="">I've</a><span> </span><a href="https://theahura.substack.com/p/tech-things-deepseek-but-make-it?utm_source=publication-search" rel="">said</a><span> </span><a href="https://theahura.substack.com/p/tech-things-gemini-25-and-the-bull?utm_source=publication-search" rel="">repeatedly</a><span> is that the LLM market has strong winner-take-all effects, and players in the market are heavily dependent on access to scientists, compute, and data. Apple is an extraordinarily wealthy company, so they have no problem getting access to scientists. But it seems like they have had a ton of issues on both of the latter two categories.</span></p><p><span>On the compute side, it seems like Apple sorta own goaled themselves? From the </span><a href="https://www.nytimes.com/2025/04/11/technology/apple-issues-trump-tariffs.html" rel="">NYT</a><span>:</span></p><blockquote><p>The A.I. stumble was set in motion in early 2023. Mr. Giannandrea, who was overseeing the effort, sought approval from the company’s chief executive, Tim Cook, to buy more A.I. chips, known as graphics processing units, or GPUs, five people with knowledge of the request said. The chips, which can perform hundreds of computations at the same time, are critical to building the neural networks of A.I. systems, like chatbots, that can answer questions or write software code.</p><p>At the time, Apple’s data centers had about 50,000 GPUs that were more than five years old — far fewer than the hundreds of thousands of chips being bought at the time by A.I. leaders like Microsoft, Amazon, Google and Meta, these people said.</p><p>Mr. Cook approved a plan to double the team’s chip budget, but Apple’s finance chief, Luca Maestri, reduced the increase to less than half that, the people said. Mr. Maestri encouraged the team to make the chips they had more efficient.</p><p>The lack of GPUs meant the team developing A.I. systems had to negotiate for data center computing power from its providers like Google and Amazon, two of the people said. The leading chips made by Nvidia were in such demand that Apple used alternative chips made by Google for some of its A.I. development.</p></blockquote><p><span>Well, that at least explains why they haven’t been putting out any decent models. Anecdotally, Apple obviously has data centers, but they aren't a cloud provider like Google/Microsoft/Amazon, which at various points have powered DeepMind/OpenAI/Anthropic directly. So Apple is starting way behind on the whole chip thing. Maybe it makes some kind of strategic sense to try and double down on their own unique chip capacity — maybe try to do what Google has done with TPUs — but that's really being extremely generous. The more obvious answer is the simple one: Apple cheaped out, and was penny wise pound foolish. As a result, the company that dominated the mobile wave is all but absent from the AI wave.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-10-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-10-161689970" target="_self" rel="">10</a></span></p><p>On the data side, Apple definitely own goaled themselves. In an environment of data hoarders and open disregard for information safety, Apple struck out as an ardent defender of user privacy. They made a brand out of it! They ran ads on it!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>That all made sense a few years ago, when the data itself was more questionably useful and Apple had full control of the hardware stack. Apple was able to poke fun at Google while </span><a href="https://www.businessinsider.com/facebook-blames-apple-10-billion-loss-ad-privacy-warning-2022-2" rel="">giving Meta a pretty serious black eye</a><span> over their ads policy.</span></p><p>But now, data is almost literally fuel for deep learning models. Worse, there's basically no way to avoid leaking data through the model! People have consistently been able to get models to directly reproduce training data! Google has more or less avoided using any training data because they can, they have the whole Internet already indexed. Meta, xAI, OpenAI, and Anthropic all train on public data — the former two from public posts on their social media platforms, and all four from extremely questionable flouting of copyright law. </p><p><span>Meanwhile, Apple is stuck with the same problem Google had </span><a href="https://theahura.substack.com/p/tech-things-openai-is-an-unaligned-590?utm_source=publication-search" rel="">when they got rid of their "Don't be evil" motto</a><span>. </span></p><blockquote><p><span>"Don't be evil" was a lot of things, and there were a lot of disagreeing interpretations about what it meant. One thing that no one disagreed about: it was hard to get rid of. Execs at Google ended up regretting the "Don't be evil" motto, because no matter what Google did they would get raked over the coals for doing it. "I thought you said you </span><em>wouldn't be evil</em><span>", internet commenters would snidely say. They even </span><a href="https://en.wikipedia.org/wiki/Don%27t_be_evil#Lawsuit" rel="">got sued over it</a><span>!</span></p></blockquote><p>Apple is in a similar boat. Either they use the user data they have and risk serious brand damage that the rest of FAANG is sure to capitalize on, or they handicap themselves in the AI race. Which, really, is less of a race and more of an all out brawl, one in which Apple is fighting with both hands behind its back.</p><p><span>So far, they've taken the "handicap" approach. They've tried to pay their way out of the data access problem by straight up </span><a href="https://www.inc.com/kit-eaton/apple-signs-deal-for-ai-training-data-from-image-service-shutterstock.html" rel="">buying the copyright licenses for data</a><span> they want to train on, but, come on, it's just not anywhere near enough training data.</span></p><p><span>The worst case scenario for Apple is they decide to use user data </span><em>late</em><span>. In that setting, Apple incurs the brand risk while also being miles behind everyone else. That increasingly seems like what will happen, though, because I just can't imagine Apple actually sitting the entire AI race out.</span></p><p>So yeah. All in all, a pretty quiet few weeks for AI.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Joy of Linux Theming in the Age of Bootable Containers (125 pts)]]></title>
            <link>https://blues.win/posts/joy-of-linux-theming/</link>
            <guid>43743784</guid>
            <pubDate>Sun, 20 Apr 2025 13:56:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blues.win/posts/joy-of-linux-theming/">https://blues.win/posts/joy-of-linux-theming/</a>, See on <a href="https://news.ycombinator.com/item?id=43743784">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Having spent a couple of decades in the Linux world, I have always had an interest in Linux desktop environments and how they are themed.
I would often come across a post on <a href="https://reddit.com/r/unixporn">/r/unixporn</a> that inspired me to try to customize the look and feel of my desktop environment. So I would install Xfce, LXQt or Sway and try to recreate components that I like from other users or create my own. I would end up installing different kinds of panels, plugins, docks and launchers as well as random themes, fonts and sounds.</p>
<p>A portion of this process would be documented, initially as random shell scripts in my home directory, before graduating to Ansible playbooks – with a brief detour into Nix that I will not elaborate on. Some of the customizations would live in my home directory, but there were often system-wide modifications to <code>/usr</code> required.</p>
<p>Eventually, the constant churn and randomly broken desktop components such as a panel that mysteriously vanished or a non-functional dock led me to stick with the stock configuration of whatever desktop environment I was using at the time.
The major desktop environments, <a href="https://kde.org/">KDE Plasma</a> and <a href="https://www.gnome.org/">GNOME</a>, are both well polished and great out of the box. The desktop experience that they have delivered over the last few years has contributed to desktop Linux being the best it has ever been, in my opinion.</p>
<p>But the itch to customize and tweak my desktop environment in fun and interesting ways is still there. Eventually, I was introduced to the concept of bootable containers.</p>
<h2 id="bootc-as-a-themers-playground">Bootc As A Themer’s Playground<a href="#bootc-as-a-themers-playground" arialabel="Anchor">⌗</a> </h2>
<p>The <a href="https://github.com/bootc-dev/bootc">bootc</a> project, originally developed by <a href="https://www.redhat.com/">Red Hat</a> but now part of the <a href="https://www.cncf.io/">Cloud Native Computing Foundation</a>, is a core component of the <a href="https://containers.github.io/bootable/">Bootable Containers Initiative</a>. Conceptually, it allows you to define your operating system as a Containerfile:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>FROM quay.io/fedora/fedora-bootc:42
</span></span><span><span>RUN dnf install -y my-custom-theme my-custom-fonts my-custom-panel
</span></span></code></pre></div><p>Once written, you can build the container locally and instruct your bootc-aware system to use the new image.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sudo podman build -f Containerfile -t my-fedora
</span></span><span><span>sudo bootc switch --transport containers-storage localhost/my-fedora:latest
</span></span></code></pre></div><p>After a reboot, the system’s deployment is defined by the new container.</p>
<p>With Fedora Atomic systems, <code>/usr</code> is mounted read-only and because your operating system is defined by an OCI container, it is incredibly easy to revert to a previous tag of that container. I can easily create a throwaway container where I test out ideas for a theme, reboot into the new deployment and test it out on bare-metal. I can roll back to the previous container if necessary or create a new container with follow-up modifications.</p>
<p>One downside is that this reboot-heavy workflow can obviously cause some friction. This can be mitigated somewhat by enabling “Development Mode” with <code>ostree admin unlock</code>, which creates a temporary writable overlayfs on top of <code>/usr</code>. I often find myself using this mode to temporarily install some package, theme or configuration in <code>/usr</code>. After verifying that it works as expected, I can add that functionality to the Containerfile. If it doesn’t work, I can either reboot to get rid of the changes, or (more likely) just forget about the change and it will remove itself whenever I reboot normally. The lack of cruft that accumulates over time in a typical Linux installation is one of the major advantages of this approach.</p>
<p>Of course, there are other ways to achieve similar results without using a bootable container model:</p>
<ul>
<li>You can write shell scripts or Ansible playbooks and hope that they accurately capture changes to the system so that they can be reliably undone. Typically, configuration drift that occurs as software gets updated is not addressed.</li>
<li>With <a href="https://www.freedesktop.org/software/systemd/man/latest/systemd-sysext.html">systemd-sysext(8)</a>, you can create a squashfs image of a filesystem containing your theming changes for <code>/usr</code> and overlay it on top of the root filesystem. An ecosystem around how these images should be created, maintained, deployed and updated has yet to emerge.</li>
<li>You can inscribe your custom theming as runes in an arcane and inscrutable functional language known only to the elders as N̸̘̏͑̕͝į̸̈́̂x̸͙̑̅̒.</li>
</ul>
<p>In my opinion, none of the alternatives provide the same level of flexibility and tooling support as writing a Dockerfile, nor can they achieve bootc’s level of safety and reliability by making it extremely difficult to bork your <code>/usr</code> directory. And if the <code>/usr</code> directory somehow gets borked anyway, rolling back to the previously deployed container is just a reboot away.</p>
<h2 id="what-is-a-distro">What Is A Distro?<a href="#what-is-a-distro" arialabel="Anchor">⌗</a> </h2>

<p>A few weeks ago, an OCI image based on Fedora Xfce Atomic that I made called <a href="https://github.com/winblues/blue95">Blue95</a> was posted to <a href="https://news.ycombinator.com/item?id=43524937">Hacker News</a>. For the most part, the reception was warm but there were some interesting questions that were raised, such as:</p>
<blockquote>
<p>Is it really necessary to spin up an entirely new distro for an XFCE+GTK theme?</p></blockquote>
<p>The original poster made me question the nature of the project I created: is it a distro? In the age of bootc, the distinction between what is considered a Linux distribution and what is simply a Containerfile + CI/CD is, in my opinion, murky at best. Historically, the barrier to entry for creating what has traditionally been called a Linux distribution was orders of magnitude higher than creating and publishing an OCI container. My nights-and-weekends side project of building a bootable container is a far cry from what I imagine a Linux distribution to be.</p>
<p><a href="https://blues.win/95">Blue95</a> is a collection of scripts and YAML files cobbled together to produce a Containerfile, which is built via GitHub Actions and published to the GitHub Container Registry. Which part of this process elevates the project to the status of a Linux distribution? What set of <code>RUN</code> commands in the Containerfile take the project from being merely a Fedora-based OCI image to a full-blown Linux distribution?</p>
<p>Popular bootc-based projects like <a href="https://projectbluefin.io/">Project Bluefin</a> and <a href="https://bazzite.gg/">Bazzite</a> are often labeled as Linux distributions, much to the consternation of their creators and maintainers. But if you’ve ever used Bazzite and booted directly into Steam’s Big Picture Mode, you might agree that it does indeed feel like its own Linux distribution; it is quite distinct from its twin bases of <a href="https://fedoraproject.org/atomic-desktops/silverblue/">Fedora Silverblue</a> and <a href="https://fedoraproject.org/atomic-desktops/kinoite/">Fedora Kinoite</a>.</p>
<p>Maybe the imprecision of the term “Linux distribution” is most evident when arguments arise over what is and isn’t a distro. It has always been problematic to define a distribution as simply a curated collection of software plus a Linux kernel -— but that definition is now especially lacking, as it could just as easily describe any Containerfile for a bootable container. Ultimately, “I know it when I see it” may be the best we can do when deciding whether a project deserves the label Linux distribution or not.</p>
<p>Finally, to address the original question about the necessity of spinning up a new distro just for a theme: creating a bootable container with a consistent visual design and curated set of applications can bring a bit of <strong>joy and levity</strong>. At this moment, my laptop is booted from a container that I have created myself. The operating system being used to draft these words is the product of my own artistic and creative expression – built on the work of countless other human beings. And that brings me joy.</p>
<hr>
<figure>
<a href="https://blues.win/images/ty4reading.png"><img src="https://blues.win/images/ty4reading.png"></a>
</figure>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemma 3 QAT Models: Bringing AI to Consumer GPUs (477 pts)]]></title>
            <link>https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/</link>
            <guid>43743337</guid>
            <pubDate>Sun, 20 Apr 2025 12:22:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/">https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/</a>, See on <a href="https://news.ycombinator.com/item?id=43743337">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    
      
    

    

    

    

    
    <div>
          

<div>
    <p data-block-key="sisye">Last month, we launched Gemma 3, our latest generation of open models. Delivering state-of-the-art performance, Gemma 3 quickly established itself as a leading model capable of running on a single high-end GPU like the NVIDIA H100 using its native BFloat16 (BF16) precision.</p><p data-block-key="4impe">To make Gemma 3 even more accessible, we are announcing new versions optimized with Quantization-Aware Training (QAT) that dramatically reduces memory requirements while maintaining high quality. This enables you to run powerful models like Gemma 3 27B locally on consumer-grade GPUs like the NVIDIA RTX 3090.</p>
</div>   

<div>
        
            <p><img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma3QuantizedChart_RD1_01_1_qlGnyVc.original.png" alt="Chatbot Arena Elo Score - Gemma 3 QAT"></p><p>
                    This chart ranks AI models by Chatbot Arena Elo scores; higher scores (top numbers) indicate greater user preference. Dots show estimated NVIDIA H100 GPU requirements.
                </p>
            
        
    </div>
  <div>
    <h2 data-block-key="sisye">Understanding performance, precision, and quantization</h2><p data-block-key="2a2jr">The chart above shows the performance (Elo score) of recently released large language models. Higher bars mean better performance in comparisons as rated by humans viewing side-by-side responses from two anonymous models. Below each bar, we indicate the estimated number of NVIDIA H100 GPUs needed to run that model using the BF16 data type.</p><p data-block-key="ae0co"><b><br>Why BFloat16 for this comparison?</b> BF16 is a common numerical format used during inference of many large models. It means that the model parameters are represented with 16 bits of precision. Using BF16 for all models helps us to make an apples-to-apples comparison of models in a common inference setup. This allows us to compare the inherent capabilities of the models themselves, removing variables like different hardware or optimization techniques like quantization, which we'll discuss next.</p><p data-block-key="3q45i">It's important to note that while this chart uses BF16 for a fair comparison, deploying the very largest models often involves using lower-precision formats like FP8 as a practical necessity to reduce immense hardware requirements (like the number of GPUs), potentially accepting a performance trade-off for feasibility.</p><h2 data-block-key="8r8jc"><b><br></b>The Need for Accessibility</h2><p data-block-key="dkjr0">While top performance on high-end hardware is great for cloud deployments and research, we heard you loud and clear: you want the power of Gemma 3 on the hardware you already own. We're committed to making powerful AI accessible, and that means enabling efficient performance on the consumer-grade GPUs found in desktops, laptops, and even phones.</p><h2 data-block-key="950c"><b><br></b>Performance Meets Accessibility with Quantization-Aware Training in Gemma 3</h2><p data-block-key="158md">This is where quantization comes in. In AI models, quantization reduces the precision of the numbers (the model's parameters) it stores and uses to calculate responses. Think of quantization like compressing an image by reducing the number of colors it uses. Instead of using 16 bits per number (BFloat16), we can use fewer bits, like 8 (int8) or even 4 (int4).</p><p data-block-key="3a7o0">Using int4 means each number is represented using only 4 bits – a 4x reduction in data size compared to BF16. Quantization can often lead to performance degradation, so we’re excited to release Gemma 3 models that are robust to quantization. We released several quantized variants for each Gemma 3 model to enable inference with your favorite inference engine, such as Q4_0 (a common quantization format) for Ollama, llama.cpp, and MLX.</p><p data-block-key="epsbi"><b><br>How do we maintain quality?</b> We use QAT. Instead of just quantizing the model after it's fully trained, QAT incorporates the quantization process during training. QAT simulates low-precision operations during training to allow quantization with less degradation afterwards for smaller, faster models while maintaining accuracy. Diving deeper, we applied QAT on ~5,000 steps using probabilities from the non-quantized checkpoint as targets. We reduce the perplexity drop by 54% (using llama.cpp perplexity evaluation) when quantizing down to Q4_0.</p><h2 data-block-key="eatdo"><b><br></b>See the Difference: Massive VRAM Savings</h2><p data-block-key="9f9so">The impact of int4 quantization is dramatic. Look at the VRAM (GPU memory) required just to load the model weights:</p><ul><li data-block-key="41cf1"><b>Gemma 3 27B:</b> Drops from 54 GB (BF16) to just <b>14.1 GB</b> (int4)</li></ul><ul><li data-block-key="5uebr"><b>Gemma 3 12B:</b> Shrinks from 24 GB (BF16) to only <b>6.6 GB</b> (int4)</li></ul><ul><li data-block-key="9d985"><b>Gemma 3 4B:</b> Reduces from 8 GB (BF16) to a lean <b>2.6 GB</b> (int4)</li></ul><ul><li data-block-key="bf7p"><b>Gemma 3 1B:</b> Goes from 2 GB (BF16) down to a tiny <b>0.5 GB</b> (int4)</li></ul>
</div>   

<div>
    <p><img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma3QuantizedChart_RD1_02.original.png" alt="Comparison chart of model weights showing VRAM required to load">
        
        
    </p>
</div>
  <div>
    <blockquote data-block-key="7ui9z"><b><sup>Note:</sup></b> <i><sup>This figure only represents the VRAM required to load the model weights. Running the model also requires additional VRAM for the KV cache, which stores information about the ongoing conversation and depends on the context length</sup></i></blockquote><h2 data-block-key="e1hmg"><b><br></b>Run Gemma 3 on Your Device</h2><p data-block-key="4jdc3">These dramatic reductions unlock the ability to run larger, powerful models on widely available consumer hardware:</p><ul><li data-block-key="bqsl5"><b>Gemma 3 27B (int4):</b> Now fits comfortably on a single desktop NVIDIA RTX 3090 (24GB VRAM) or similar card, allowing you to run our largest Gemma 3 variant locally.</li></ul><ul><li data-block-key="c4adc"><b>Gemma 3 12B (int4):</b> Runs efficiently on laptop GPUs like the NVIDIA RTX 4060 Laptop GPU (8GB VRAM), bringing powerful AI capabilities to portable machines.</li></ul><ul><li data-block-key="be3pr"><b>Smaller Models (4B, 1B):</b> Offer even greater accessibility for systems with more constrained resources, including phones and <a href="https://youtu.be/lgsD_wSZ0hI?si=pyQj23bOxNPLrxtL&amp;t=102">toasters</a> (if you have a good one).</li></ul><h2 data-block-key="7c8ds"><b><br></b>Easy Integration with Popular Tools</h2><p data-block-key="2324e">We want you to be able to use these models easily within your preferred workflow. Our official int4 and Q4_0 unquantized QAT models are available on Hugging Face and Kaggle. We’ve partnered with popular developer tools that enable seamlessly trying out the QAT-based quantized checkpoints:</p><ul><li data-block-key="avkt"><a href="https://ollama.com/library/gemma3"><b>Ollama</b></a><b>:</b> Get running quickly – all our Gemma 3 QAT models are natively supported starting today with a simple command.</li></ul><ul><li data-block-key="6sp3o"><a href="https://lmstudio.ai/model/gemma-3-12b-it-qat"><b>LM Studio</b></a><b>:</b> Easily download and run Gemma 3 QAT models on your desktop via its user-friendly interface.</li></ul><ul><li data-block-key="69oug"><a href="https://huggingface.co/collections/mlx-community/gemma-3-qat-68002674cd5afc6f9022a0ae"><b>MLX</b></a><b>:</b> Leverage MLX for efficient, optimized inference of Gemma 3 QAT models on Apple Silicon.</li></ul><ul><li data-block-key="19s6q"><a href="https://www.kaggle.com/models/google/gemma-3/gemmaCpp"><b>Gemma.cpp</b></a><b>:</b> Use our dedicated C++ implementation for highly efficient inference directly on the CPU.</li></ul><ul><li data-block-key="f05gl"><a href="https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b"><b>llama.cpp</b></a><b>:</b> Integrate easily into existing workflows thanks to native support for our GGUF-formatted QAT models.</li></ul><h2 data-block-key="c5073"><b><br></b>More Quantizations in the Gemmaverse</h2><p data-block-key="74vnv">Our official Quantization Aware Trained (QAT) models provide a high-quality baseline, but the vibrant <a href="https://ai.google.dev/gemma/gemmaverse">Gemmaverse</a> offers many alternatives. These often use Post-Training Quantization (PTQ), with significant contributions from members such as <a href="https://huggingface.co/bartowski/google_gemma-3-27b-it-GGUF">Bartowski</a>, <a href="https://huggingface.co/collections/unsloth/gemma-3-67d12b7e8816ec6efa7e4e5b">Unsloth</a>, and <a href="https://huggingface.co/collections/ggml-org/gemma-3-67d126315ac810df1ad9e913">GGML</a> readily available on Hugging Face. Exploring these community options provides a wider spectrum of size, speed, and quality trade-offs to fit specific needs.</p><h2 data-block-key="fnsl0"><b><br></b>Get Started Today</h2><p data-block-key="fdu54">Bringing state-of-the-art AI performance to accessible hardware is a key step in democratizing AI development. With Gemma 3 models, optimized through QAT, you can now leverage cutting-edge capabilities on your own desktop or laptop.</p><p data-block-key="179dk">Explore the quantized models and start building:</p><ul><li data-block-key="6uj07">Use on your PC with <a href="https://ollama.com/library/gemma3">Ollama</a></li></ul><ul><li data-block-key="741fu">Find the Models on <a href="https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b">Hugging Face</a> &amp; <a href="https://www.kaggle.com/models/google/gemma-3/transformers">Kaggle</a></li></ul><ul><li data-block-key="6069i">Run on your phone with <a href="https://developers.googleblog.com/en/gemma-3-on-mobile-and-web-with-google-ai-edge/">Google AI Edge</a></li></ul><p data-block-key="4j3l0">We can't wait to see what you build with Gemma 3 running locally!</p>
</div> 
      </div>
    

    

    
    
    
  </div></div>]]></description>
        </item>
    </channel>
</rss>