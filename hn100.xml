<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 12 Jul 2024 14:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[AT&T says criminals stole phone records of 'nearly all' customers in data breach (169 pts)]]></title>
            <link>https://techcrunch.com/2024/07/12/att-phone-records-stolen-data-breach/</link>
            <guid>40944505</guid>
            <pubDate>Fri, 12 Jul 2024 11:17:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/07/12/att-phone-records-stolen-data-breach/">https://techcrunch.com/2024/07/12/att-phone-records-stolen-data-breach/</a>, See on <a href="https://news.ycombinator.com/item?id=40944505">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">U.S. phone giant AT&amp;T confirmed Friday it will begin notifying millions of consumers about a fresh data breach that allowed cybercriminals to steal the phone records of “nearly all” of its customers, a company spokesperson told TechCrunch.</p>

<p>In a statement, AT&amp;T said that the stolen data contains phone numbers of both cellular and landline customers, as well as AT&amp;T records of calls and text messages — such as who contacted who by phone or text — during a six-month period between May 1, 2022 and October 31, 2022.&nbsp;</p>

	
	


<p>AT&amp;T said some of the stolen data includes more recent records from January 2, 2023 for a smaller but unspecified number of customers.</p>

	
	


<p>The stolen data also includes call records of customers with phone service from other cell carriers that rely on AT&amp;T’s network, the company said.&nbsp;</p>

<p>AT&amp;T said the stolen data “does not contain the content of calls or texts,” but does include calling and texting records that an AT&amp;T phone number interacted with during the six-month period, as well as the total count of a customer’s calls and texts, and call durations — information that is often referred to as metadata. The stolen data does not include the time or date of calls or texts, AT&amp;T said.</p>

<p>Some of the stolen records include cell site identification numbers associated with phone calls and text messages, information that can be used to determine the approximate location of where a call was made or text message sent.</p>

<p>In all, the phone giant said it will notify around 110 million AT&amp;T customers of the data breach, company spokesperson Andrea Huguely told TechCrunch.&nbsp;</p>

	
	


	
	


<p>AT&amp;T published <a href="https://www.att.com/DataIncident" target="_blank" rel="noreferrer noopener nofollow">a website with information for customers</a> about the data incident.<strong> </strong>AT&amp;T also disclosed the data breach in <a rel="nofollow" href="https://www.sec.gov/ix?doc=/Archives/edgar/data/0000732717/000073271724000046/t-20240506.htm">a filing with regulators</a> before the market opened on Friday.</p>

<h2 id="h-breach-linked-to-snowflake">Breach linked to Snowflake</h2>

<p>AT&amp;T said it learned of the data breach on April 19, and that it was <a href="https://techcrunch.com/2024/03/30/att-reset-account-passcodes-customer-data/" target="_blank" rel="noreferrer noopener">unrelated to its earlier security incident</a> in March.&nbsp;</p>

<p>AT&amp;T’s Huguely told TechCrunch that the most recent compromise of customer records were stolen from the cloud data giant Snowflake <a href="https://techcrunch.com/2024/06/10/mandiant-hackers-snowflake-stole-significant-volume-data-customers/" target="_blank" rel="noreferrer noopener">during a recent spate of data thefts</a> targeting Snowflake’s customers.</p>

	
	


<p>Snowflake allows its corporate customers, like tech companies and telcos, to analyze huge amounts of customer data in the cloud. It’s not clear for what reason AT&amp;T was storing customer data in Snowflake, and the spokesperson would not say.</p>

<p>AT&amp;T is the latest company in recent weeks to confirm it had data stolen from Snowflake, <a href="https://techcrunch.com/2024/05/31/live-nation-confirms-ticketmaster-was-hacked-says-personal-information-stolen-in-data-breach/" target="_blank" rel="noreferrer noopener">following Ticketmaster</a> and <a href="https://techcrunch.com/2024/06/07/snowflake-ticketmaster-lendingtree-customer-data-breach/" target="_blank" rel="noreferrer noopener">LendingTree subsidiary QuoteWizard</a>, and others.</p>

	
	


<p>Snowflake blamed the data thefts on its customers for not using multi-factor authentication to secure their Snowflake accounts, a security feature that the cloud data giant did not enforce or require its customers to use.&nbsp;</p>

<p>Cybersecurity incident response firm Mandiant, which Snowflake called in to help with notifying customers, later said <a href="https://techcrunch.com/2024/06/10/mandiant-hackers-snowflake-stole-significant-volume-data-customers/" target="_blank" rel="noreferrer noopener">about 165 Snowflake customers had a “significant volume of data” stolen from their customer accounts</a>.&nbsp;</p>

	
	


<p>Mandiant attributed the breach to an as-yet-uncategorized cybercriminal group tracked only as UNC5537. Mandiant’s researchers say the hackers are financially motivated and have members in North America and at least one member in Turkey.&nbsp;</p>

<p>Some of the other corporate victims of the Snowflake account thefts had data subsequently published on known cybercrime forums. For AT&amp;T’s part, the company said that it does not believe that the data is publicly available at this time.</p>

<p>AT&amp;T’s statement said it was working with law enforcement to arrest the cybercriminals involved in the breach. AT&amp;T said that “at least one person has been apprehended.” AT&amp;T’s spokesperson said that the arrested individual was not an AT&amp;T employee, but deferred questions about the alleged criminals to the FBI. </p>

	
	


<p>An FBI spokesperson confirmed to TechCrunch on Friday that that after the phone giant contacted the agency to report the breach, AT&amp;T, the FBI and the Department of Justice agreed to delay notifying the public and customers on two occasions, citing “potential risks to national security and/or public safety.” </p>

	
	


<p>“AT&amp;T, FBI, and DOJ worked collaboratively through the first and second delay process, all while sharing key threat intelligence to bolster FBI investigative equities and to assist AT&amp;T’s incident response work,” the FBI spokesperson said.</p>

<p>The FBI did not comment on the arrest of one of the alleged cybercriminals.</p>

<p>This is <a href="https://techcrunch.com/2024/06/29/2024-in-data-breaches-1-billion-stolen-records-and-rising/" target="_blank" rel="noreferrer noopener">the second security incident AT&amp;T has disclosed this year</a>. AT&amp;T was forced to reset the account passcodes of millions of its customers after a cache of customer account information — including encrypted passcodes for accessing AT&amp;T customer accounts — was published on a cybercrime forum. A security researcher told TechCrunch at the time that the encrypted passcodes could be easily decrypted, prompting AT&amp;T to <a href="https://techcrunch.com/2024/04/10/att-notifies-regulators-after-customer-data-breach/" target="_blank" rel="noreferrer noopener">take precautionary action to protect customer accounts</a>.</p>

<p><strong>Read more on TechCrunch:</strong></p>

	
	


<ul>
<li><a href="https://techcrunch.com/2024/07/11/mspy-spyware-millions-customers-data-breach/" target="_blank" rel="noreferrer noopener">Data breach exposes millions of mSpy spyware customers</a></li>



<li><a href="https://techcrunch.com/2024/07/10/apple-alerts-iphone-users-in-98-countries-to-mercenary-spyware-attacks/" target="_blank" rel="noreferrer noopener">Apple warns iPhone users in 98 countries of spyware attacks</a></li>



<li><a href="https://techcrunch.com/2024/07/09/evolve-bank-says-ransomware-gang-stole-personal-data-on-millions-of-customers/" target="_blank" rel="noreferrer noopener">Evolve Bank says ransomware gang stole personal data on millions of customers</a></li>



<li><a href="https://techcrunch.com/2024/07/05/openai-breach-is-a-reminder-that-ai-companies-are-treasure-troves-for-hackers/" target="_blank" rel="noreferrer noopener">OpenAI breach is a reminder that AI companies are treasure troves for hackers</a></li>
</ul>

<p><em>Updated with comment from the FBI.</em></p>

<figure></figure>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[As an Employee, You Are Disposable (2023) (223 pts)]]></title>
            <link>https://nelson.cloud/as-an-employee-you-are-disposable/</link>
            <guid>40943436</guid>
            <pubDate>Fri, 12 Jul 2024 07:41:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nelson.cloud/as-an-employee-you-are-disposable/">https://nelson.cloud/as-an-employee-you-are-disposable/</a>, See on <a href="https://news.ycombinator.com/item?id=40943436">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><p>The recent tech layoffs have shown that employees are disposable in the eyes of executives. This isn’t surprising though and I’m definitely not the first person that has written about this. I just want to highlight the current situation.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/tech-layoff-tracker.webp" alt="TrueUp tech layoff tracker"><figcaption><p><em>Source:
<a href="https://www.trueup.io/layoffs?ref=nelson.cloud" target="_blank">TrueUp: Tech Layoff Tracker</a></em></p></figcaption></figure><p>It doesn’t matter if investor expectations are surpassed, layoffs can still take place.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/cnbc-shopify.webp" alt="CNBC Shopfiy headline"><figcaption><p><em>Source:
<a href="https://www.cnbc.com/2023/05/04/shopify-cuts-20percent-of-its-workforce-shares-surge-on-earnings-beat.html?ref=nelson.cloud" target="_blank">CNBC: Shopify cuts 20% of its workforce; shares surge on earnings beat</a></em></p></figcaption></figure><p>It’s somewhat understandable if a company is struggling financially and resorts to layoffs. However, there’s plenty of companies that are profitable and still lay off the people that earned the company those profits.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/microsoft.webp" alt="Polygon Microsoft headline"><figcaption><p><em>Source:
<a href="https://www.polygon.com/23561210/microsoft-layoffs-xbox-bethesda-halo-infinite-343-industries?ref=nelson.cloud" target="_blank">Polygon: Microsoft mass layoffs reportedly impact Bethesda, Halo Infinite teams</a></em></p></figcaption></figure><p>Many companies are not only profitable, but their executives continue to earn huge sums of money amidst layoffs.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/ars-google.webp" alt="Ars Technica headline"><figcaption><p><em>Source:
<a href="https://arstechnica.com/tech-policy/2023/05/googlers-angry-about-ceos-226m-pay-after-cuts-in-perks-and-12000-layoffs/?ref=nelson.cloud" target="_blank">Ars Technica: Googlers angry about CEO’s $226M pay after cuts in perks and 12,000 layoffs</a></em></p></figcaption></figure><p>Aside from layoffs, employees may have their pay frozen even though company revenues are up. That’s what happened at Microsoft. Let’s not forget that Microsoft is a $2.5 trillion dollar company (at the time of this writing).</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/microsoft-pay-freeze.webp" alt="Techradar headline"><figcaption><p><em>Source:
<a href="https://www.techradar.com/pro/microsoft-workers-protest-landmark-year-ceo-memo-following-pay-freeze?ref=nelson.cloud" target="_blank">Techradar: Microsoft workers protest ’landmark year’ CEO memo following pay freeze</a></em></p></figcaption></figure><p>It doesn’t matter how much value you’ve delivered. It doesn’t matter how much impact you’ve had in a company. It doesn’t matter how long you’ve been at a company. You are still disposable.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/jeremy-joslin.webp" alt="Tweet from @jcj"><figcaption><p><em>Source:
<a href="https://twitter.com/jcj/status/1616482322278420481?ref=nelson.cloud" target="_blank">Jeremy Joslin (@jcj) on Twitter</a></em></p></figcaption></figure><p>This article shows the mindset some very wealthy executives have about the average worker/employee.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/tim-gurner.webp" alt="BBC Tim Gurner headline"><figcaption><p><em>Source:
<a href="https://www.bbc.com/news/business-66803279?ref=nelson.cloud" target="_blank">BBC: Tim Gurner apologises over call for more unemployment to fix worker attitudes</a></em></p></figcaption></figure><p>There are some bits I want to highlight:</p><blockquote><p>“There’s been a systematic change where employees feel the employer is extremely lucky to have them,” Mr Gurner said. “We need to remind people they work for the employer, not the other way around.”</p></blockquote><blockquote><p>[Mr Gurner] has previously made headlines by suggesting young people cannot afford homes because they spend too much on avocado toast.</p></blockquote><h2 id="in-conclusion">In Conclusion…</h2><p>It’s okay to like your job and employer. Just understand that, <strong>as an employee, you are disposable</strong>.</p><h2 id="further-reading">Further Reading</h2><p>Here are some articles I’ve come across that share similar sentiments or are very relevant. I highly recommend giving them a read.</p><ul><li><a href="https://www.qword.net/2023/04/30/maybe-you-should-store-passwords-in-plaintext?ref=nelson.cloud" target="_blank">Maybe you should store passwords in plaintext</a></li><li><a href="https://www.mcsweeneys.net/articles/our-company-is-doing-so-well-that-youre-all-fired?ref=nelson.cloud" target="_blank">Our Company Is Doing So Well That You’re All Fired</a></li><li><a href="https://hbr.org/2022/12/what-companies-still-get-wrong-about-layoffs?ref=nelson.cloud" target="_blank">What Companies Still Get Wrong About Layoffs</a></li><li><a href="https://ludic.mataroa.blog/blog/i-accidentally-saved-half-a-million-dollars/?ref=nelson.cloud" target="_blank">I Accidentally Saved Half A Million Dollars</a></li></ul></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using S3 as a Container Registry (179 pts)]]></title>
            <link>https://ochagavia.nl/blog/using-s3-as-a-container-registry/</link>
            <guid>40942732</guid>
            <pubDate>Fri, 12 Jul 2024 04:26:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ochagavia.nl/blog/using-s3-as-a-container-registry/">https://ochagavia.nl/blog/using-s3-as-a-container-registry/</a>, See on <a href="https://news.ycombinator.com/item?id=40942732">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p>For the last four months I’ve been developing a custom container image builder, collaborating with Outerbounds<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. The technical details of the builder itself might be the topic of a future article, but there’s something surprising I wanted to share already: you can use <a href="https://en.wikipedia.org/wiki/Amazon_S3">S3</a> as a container registry! You heard it right. All it takes is to expose an S3 bucket through HTTP and to upload the image’s files to specific paths. With that in place, you can actually <code>docker pull</code> from it. Isn’t that neat?</p>
<p>In the rest of this post I’ll explain how it all works, but let’s start with a demo for the impatient among us. I created a container image that runs <a href="https://en.wikipedia.org/wiki/Cowsay">cowsay</a> and mirrored it to a bucket. Here’s what happens when you pull and run it from the bucket’s url:</p>
<pre tabindex="0"><code>$ docker run --rm pub-40af5d7df1e0402d9a92b982a6599860.r2.dev/cowsay

 _________________________
&lt; This is seriously cool! &gt;
 -------------------------
        \   ^__^
         \  (oo)\_______
            (__)\       )\/\
                ||----w |
                ||     ||
</code></pre><p>Don’t you agree with the cow? Note that, for this demo, I’m using <a href="https://www.cloudflare.com/developer-platform/r2/">R2</a> instead of S3 (because it has free egress 😎). Fortunately, it doesn’t matter whether you use R2 or S3, since they are API-compatible. As a matter of fact, I used the AWS SDK to push my image to R2.</p>
<h3 id="but-why">But why?</h3>
<p>Using S3 is not the traditional approach for hosting container images. You’d normally use a container registry, such as <a href="https://hub.docker.com/">DockerHub</a>, <a href="https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry">GitHub Container Registry</a>, <a href="https://aws.amazon.com/ecr/">ECR</a>, etc. What benefits does S3 bring, then, to make deviating from the trodden, <a href="https://boringtechnology.club/">boring</a>, path worthwhile?</p>
<p>Let’s take a step back. We are developing a custom image builder (or bakery, as we affectionately call it) because of speed. We want to go from requirements to a ready-to-pull image in a few seconds. The easiest container registry to use in our case is ECR, because we are on AWS. However, it turns out there’s a substantial performance difference between S3 and ECR when it comes to upload speed!</p>
<p>I discovered the performance gap somewhat by accident. Since speed is important for us, and the first rule of performance optimization is to measure, I instrumented the code to generate <a href="https://medium.com/jaegertracing/jaeger-tracing-a-friendly-guide-for-beginners-7b53a4a568ca">traces</a>. Having that, I went hunting for optimization opportunities and came across something unexpected: the traces showed that pushing layers to the container registry accounted for a significant amount of time! That felt off, so I decided to run a small benchmark: to upload a 198 MiB layer to ECR and to S3, and observe the difference in duration.</p>
<p>Here’s the outcome:</p>
<table>
<thead>
<tr>
<th></th>
<th><strong>Minimum observed speed</strong></th>
<th><strong>Maximum observed speed</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ECR</strong></td>
<td>24 MiB/s (8.2 s)</td>
<td>28 MiB/s (7.0 s)</td>
</tr>
<tr>
<td><strong>S3</strong></td>
<td>115 MiB/s (1.7 s)</td>
<td>190 MiB/s (1.0 s)</td>
</tr>
</tbody>
</table>
<p>The table shows that S3 is up to 8x faster than ECR, which is almost an order of magnitude<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>! Of course, there are <a href="#caveats">caveats</a>, but “raw” S3 container registries are nevertheless a promising avenue of optimization.</p>
<h3 id="what-makes-s3-faster-than-ecr">What makes S3 faster than ECR?</h3>
<p>The big difference between pushing to ECR and uploading objects to S3 is that the latter allows uploading a single layer’s chunks in parallel. Given enough bandwidth, this yields a massive increase in throughput. In fact, parallel chunked uploads are recommended in the <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-guidelines.html#optimizing-performance-guidelines-scale">AWS docs</a> to maximize bandwidth usage<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.</p>
<p>Why can’t ECR support this kind of parallel uploads? The “problem” is that it implements the <a href="https://github.com/opencontainers/distribution-spec/blob/2291163927cae6f5105a07d32c675c00ff39244c/spec.md">OCI Distribution Spec</a>, which is the standard for container registries (i.e. the reason why you can <code>docker pull</code> and <code>docker push</code> to different registry implementations). According to the specification, a layer push must happen sequentially: even if you upload the layer in chunks, each chunk needs to finish uploading before you can move on to the next one. Needless to say, having a single active connection per layer leaves a significant amount of bandwidth unused!</p>
<p><em>Aside: we also tested the performance of sequential uploads to S3. The result? Throughput went down to ECR-like levels!</em></p>
<h3 id="but-s3-is-not-a-container-registry">But S3 is not a container registry!</h3>
<p>Indeed, S3 is not a container registry in the strict sense of the word. You can’t <code>docker push</code> to it, and the fact that you can <code>docker pull</code> is mostly a happy coincidence. So how does it work?</p>
<p>The answer to our question is revealed by looking at the inner workings of <code>docker pull</code>. Spoiler: it’s HTTP requests all the way down. More specifically, I logged<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> the requests issued by <code>docker pull</code> and saw that they are “just” a bunch of <code>HEAD</code> and <code>GET</code> requests. As an example, see the log of a <code>docker pull my-image:latest</code> at my self-hosted registry (lines starting with <code>#</code> are comments):</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span><span># Check whether the image's manifest is present in the registry</span>
</span></span><span><span>HEAD /v2/my-image/manifests/latest
</span></span><span><span><span># Download the image's manifest</span>
</span></span><span><span>GET /v2/my-image/manifests/latest
</span></span><span><span><span># Re-download the image's manifest, now addressed using the manifest's hash</span>
</span></span><span><span><span># (I think this is a sanity check by docker)</span>
</span></span><span><span>GET /v2/my-image/manifests/sha256:dabf91b69c191a1a0a1628fd6bdd029c0c4018041c7f052870bb13c5a222ae76
</span></span><span><span><span># Download one of the image's blobs (which happens to be the image's metadata)</span>
</span></span><span><span>GET /v2/my-image/blobs/sha256:a606584aa9aa875552092ec9e1d62cb98d486f51f389609914039aabd9414687
</span></span><span><span><span># Download the remaining image's blob (which happens to be its only layer)</span>
</span></span><span><span>GET /v2/my-image/blobs/sha256:ec99f8b99825a742d50fb3ce173d291378a46ab54b8ef7dd75e5654e2a296e99
</span></span></code></pre></div><p>That’s it! A <code>docker pull</code> is merely downloading files through HTTP! Which means… You can pull containers from <em>any</em> static file server, as long as it has the necessary files at the expected paths and sets the right <code>Content-Type</code> header for each request. Since a S3 bucket is capable of both, a carefully crafted bucket can become a container registry!</p>
<p><em>Aside: if you want to know more about “manifests”, “blobs”, and such, check out my article on <a href="https://ochagavia.nl/blog/crafting-container-images-without-dockerfiles/">Crafting container images without dockerfiles</a> and the <a href="https://github.com/opencontainers/image-spec/blob/036563a4a268d7c08b51a08f05a02a0fe74c7268/spec.md">OCI Image Format Specification</a>.</em></p>
<h3 id="caveats">Caveats</h3>
<p>In case it’s not already clear: this is all very experimental. I’m waiting to do more research before making any serious claims. Will it end up in production? Or will you, my dear reader, send me an email explaining how my approach is utterly flawed?</p>
<p>Note that, while I haven’t made a survey of the container registry offerings out there, it’s obvious they come with features that make them more attractive than dumping files on a bucket. For instance: you can trust the images you upload are actually valid (because the registry uses the standard push method); you can run automated security scans against your layers and receive warnings if there’s anything fishy; you can natively specify who has access to private repositories; etc.</p>
<p>Don’t let these caveats discourage you, though. If it all works as well as I’m hoping, maybe we’ll see a new trend of hosting public container images in Cloudflare’s R2! What would you say to free egress?</p>
<h5 id="ps-what-about-the-whale">PS. What about the whale?</h5>
<p>It’s a pun… <a href="https://www.google.com/search?q=docker+logo&amp;hl=en">go have a look</a> at the docker logo 😉</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Floppy8 – A Tiny Computer, in a Floppy Drive (2023) (122 pts)]]></title>
            <link>https://abe.today/blogs/main/floppy8-a-tiny-computer-in-a-floppy-drive</link>
            <guid>40942141</guid>
            <pubDate>Fri, 12 Jul 2024 01:45:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abe.today/blogs/main/floppy8-a-tiny-computer-in-a-floppy-drive">https://abe.today/blogs/main/floppy8-a-tiny-computer-in-a-floppy-drive</a>, See on <a href="https://news.ycombinator.com/item?id=40942141">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
          <p>This post covers the design and creation of the Floppy8, a microcomputer and cartridge system which fits inside a floppy drive.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-23_15-07-54_600x600.png?v=1677186494" alt=""></p>
<p>To see it in action, checkout this video!</p>
<p><iframe src="https://www.youtube.com/embed/zvp-eqWCjGU" title="Floppy8 Demo" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="337" height="600" frameborder="0"></iframe></p>
<p>The Floppy8 plays 4K movies and games on custom cartridges, features wireless controllers, status lights, motorized cartridge ejection and more!</p>
<p>Similar to a Famicon or Super Nintendo, the cartridge sticks out, allowing the label to still be partially visible. The front button safely unmounts and ejects the cart when pressed. The RGB indicator LED flashes different colors for waiting, mounting, running and error states.</p>
<p>The wireless controllers are modified off-the-shelf NES clones which have a beige color and replaced d-pads along with extra added weights for that premium feel.</p>
<h2>How (and why) I Made It</h2>
<p><span>Please note: This post contains affiliate links to many of the parts used in this build!</span></p>
<p>If you're like me, you often find yourself browsing Ebay (the online auction site) for weird items in the middle of the night (the time that isn't day) . One evening, while searching for floppy drives, I came across this little gorgeous little guy</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/s-l1600_240x240.jpg?v=1676999242" alt=""><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/s-l16002_240x240.jpg?v=1676999255" alt=""></p>
<div><p>Strangely, the listing identified it as an <a href="https://www.google.com/search?tbm=isch&amp;q=amiga%201010&amp;tbs=imgo:1" target="_blank" rel="noopener noreferrer">Amiga 1010 disk drive</a>. Maybe it is? But it doesn't look like one and I can not find anything that looks similar to it. The mystery intrigued me slightly, but the beautifully simple industrial design intrigued me more. </p><p> I was led to this disk drive that night by a growing fixation on physical media which had been bubbling inside me for the previous months. I began to realize that when you have the ability to watch any film or play any game in seconds... it removes some of the joy.</p><p> Not to mention, physical media which become tarnished or worn over years of love come to tell a story beyond the media contained within. There's a warmth to holding your favorite movie in your hands and that idea had led me to this drive.</p><p> The longer I laid there, basking in the bright white light of the photos of this mysterious Ebay floppy drive, the more I became obsessed. The texture of the front cover, the smooth shiny beige metal outside, the playful and complex rainbow ribbon cable. Before I knew it, I had spent much too much for a disk drive that had to travel much too far to arrive at my door. </p><p> Luckily, I was on a three week work trip while I waited the three weeks of shipping, so the time passed quickly and the question rattled in my brain "what do I do with such a satisfying device?" After all I don't own an Amiga and I don't intend to start, so what's a lad to do.</p><p> I came to the conclusion that the best use of this device was to pull the innards (as carefully as possible) and replace them with a framework of my own design which allowed the drive to be useful and fun for years to come.</p><p> But in 2022 we don't use floppy disks - they are much too impractical, so we'll need a new media, which means we'll need a new drive mechanism, which means we'll need the spirit of adventure because I have no idea how to do that.</p></div>
<h2>Week 0: The Plan</h2>
<p>I had a simple dream for this project...</p>
<ol>
<li>Design a new physical media format that hit a mix of nostalgia and practicality</li>
<li>Mount some computer inside the case to read that media</li>
<li>Design a drive that had a satisfying tactical feel when inserting a cartridge / floppy</li>
<li>Replace the status LED</li>
<li>Replace the mechanical front-eject button with one to eject the new cart type</li>
<li>Leave all original pieces unmodified so they could be put back if needed</li>
<li>Make it actually practical to use and enjoy!</li>
</ol>
<p>On paper a simple task, in practice this project tested my patience and was one of the hardest things I've ever done. As someone who has never done any really practical 3D printing... or electrical engineering... or anything - we had a lot to tackle.</p>
<h2>Week 1: It's real small</h2>
<p>Once the drive finally arrived, I realized how small it really was (which implies that if it <i>is</i> some Amiga 1010 variant that it's probably a late model due to its scale).</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/tiny_600x600.png?v=1677002508" alt=""></p>
<p>The device shipped from Australia and still has a "Bruining Headlam Computers" sticker on it, I assume this is from when it was last serviced many decades ago. This did not help in solving the mystery of what this drive actually was, but I kept the sticker for nostalgia and to keep with my goal of being able to restore the drive to it's original state if needed.</p>
<p>Although it was about 105mm by 43mm and around 5" deep the real concern for me was the height of the floppy disk hole in the front cover. I knew I needed to retain this part so whatever new disk /&nbsp;cartridge format I invented would need to fit within this narrow slow. Luckily, this drive has a slightly taller than average hole, which allows our new cartridges about 4mm of height to play with.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/floppy8_240x240.png?v=1677002638" alt=""> <img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/floppy8-2_240x240.png?v=1677002653" alt="" width="224" height="180"></p>
<p>The other mind-blowing thing was the amazing condition of the device. It looked almost new, with just a few hints of wear. I was as careful as possible to avoid new scratches, dings or other tarnish as I began work on the device.</p>
<p>Sadly over the weeks of working with this device, some of the paint around the front has some very small chips. This saddens me, but I'll be even more careful in the future.</p>
<h2>Week 2: We need Cartridges</h2>
<div><p>Before I could begin laying out any internals, I needed a sense of how the new physical media would work so designing the new media / floppy / cartridge (whatever you'd like to call it) was the top concern in the first few days. I needed to validate that I could manufacture something which was only 4mm tall and held some form of flash media <i>and</i> fit my aesthetic goals. </p><p> Initially, I thought I might retain the visual elements of a 3.5" floppy. I enlisted my brother to help develop our first idea - SD cards attached to custom circuit boards which looked like floppies but extended the SD card wires to an edge-connected pin set.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/floppy-circuit_240x240.png?v=1677002986" alt=""><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/floppy-circuit-2_240x240.png?v=1677003001" alt="" width="233" height="237"></p>
<div><p>Wow! My brother's designs, visually, looked fantastic. They replicted the original disks very closely and they were easy to manufacture. </p><p> The simplicity was great, but I began to realize I needed something which lended itself well to a high-quality glossy label (more like a video game cart than a floppy) and more importantly, it needed to be made of a fairly soft material. I was worried that the sharp edges of a circuit board could damage the front cover of the drive. This concern, of doing this project non-destructively, held for the whole build. With the exception of the back sticker which was cut to reach a screw, this drive was never harmed and could be restored for historic value.</p><p> I investigated a few ideas to encase these circuits, but with only 4mm of space, I could never conquer an idea which fit all the requirements, so we moved to fully 3D printed carts.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/cart-0_480x480.png?v=1677003044" alt=""></p>
<div><p>The initial prototype was compelling. I used the free tool, <a href="https://penpot.app/">Penpot</a> to modify existing art for a game and printed a label on a cart I designed. Initially, I intended to use small USB flash memory sticks embedded in the cart, but I found that the square edges of a USB make it hard to align into a socket (more on this later). They were also slow when benchmarked and just kinda ugly. I would soon pivot to embedding micro SD cards in the 3D printed carts - which works well. </p><p> This was the first tangible element of the device I sometimes called the Floppy-puter or occasionally Floppy8. I designed the carts in the free browser-based, grade-school focused 3D modeling tool <a href="https://tinkercad.com/">Tinkercad</a>. I used this tool for all 3D models for this project. </p><p> While designing the carts, I purchased a small dye-sub printer, called the <a href="https://amzn.to/3kF3Wcn">Polaroid Hi-print</a> and fit the label indent on the cart to the photo size the printer produced. I also bought a <a href="https://amzn.to/3ZVicy1">small tool to round paper corners</a>. Together along with my existing straight cutter, I was able to produce high-quality labels. Here is an overview of the whole cart creation process.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/cart_480x480.gif?v=1677003075" alt="">&nbsp;</p>
<div><p>The cart design was finalized pretty quickly and aside from small fit tweaks the design stayed the same from early on. It was a nice constant to have and a good motivator to hold and look at during frustrating days. </p><p> Eventually, I would make a few carts of games or films I liked. I wanted to make them out of different filament as I loved the mismatched look of different colored 3.5" floppies and wanted to retain that.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/carts_480x480.png?v=1677003603" alt=""></p>
<p>Around this same time, I also began work on the software which would power the computer. I initially began with a Raspbian base (when I thought I would use a Raspberry Pi) and began customizing it to my liking, reducing the desktop UI, changing splash screens, etc.</p>
<p>I knew I would need a fair bit of logic to recognize the insertion of the carts, mount them, run an autostart, then once the program exited or the eject button was pressed it needed to attempt to safely kill the program (if that failed, non-safely kill it) then unmount the drive and trigger a servo to eject the cart. While this was happening it would also be updating the front LED to indicate the device's status.</p>
<p>Originally, I wrote the code as <a href="https://gist.github.com/abeisgoat/b0567184214297cbfe2b2912d132f848" title="a bash script">a bash script</a> and this worked fine until I had many signals coming from different places (e.g. a program exit or eject button press may trigger the eject process). I eventually gave up and wrote the whole software stack in Node and used <a href="https://www.npmjs.com/package/firmata">firmata</a> to talk to the Arduino hardware.&nbsp;</p>
<p>In order to get the LED status to indicate an inserted, but unmounted cart, I used dmesg to read the kernel logs early and reflect insertion before the cart was mounted. I then used fairly straight forward logic to handle cart, LED, and program status.</p>
<p>The final setup requires each cart to have <strong></strong><em>autostart.sh</em> and an <em>autokill.sh</em> along with any binaries / media it needs. Ideally those scripts will boot the program then safely kill it once the eject button is pressed, but it will also force quit if the program hangs.</p>
<p>The software never changed much after this point, just tweaks to make it more reliable.</p>
<p>However, what did change a lot after this point was my approach to getting a Micro SD card reader to fit in the small gap inside the carts where the edge connector of the Micro SD card sticks out.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-23_16-42-43_480x480.png?v=1677192177" alt=""></p>
<p>After buying and ripping apart a dozen or so SD card readers, I finally found this incredibly tiny USB-C reader which fits perfectly in the gap in the cart. Designing the rest of the drive bay would mean figuring out how to mount this card reader while also aligning the rest of the cart properly so it would smoothly insert when pushed in.</p>
<p>This wasn't <em>that</em> hard but did involve a lot of prototyping as tolerances on 3D printers are not great, so even though something was modeled to fit, that was no guarantee.</p>
<h2>Week 3-6: So many unknowns...</h2>
<p>Once I had a mostly-finished cart design I had to begin prototyping the computer, I knew I had a few big unknowns I had to explore. Some easier than others - but all challenges none-the-less.</p>
<ol>
<li>Is a Raspberry Pi powerful enough to do everything I want to do?</li>
<li>How can I create an eject mechanism for the cartridges?</li>
<li>How can I control the front-facing LED?</li>
<li>Do I need fans?</li>
<li>Will the metal case block wifi?</li>
<li>How will I fit all the wires?</li>
</ol>
<p>Let's start at the beginning...</p>
<h3>Is a Raspberry Pi powerful enough to do everything I want to do?</h3>
<div><p>No - they are not. My initial tests showed the Pi of <i>barely</i> being capable of playing 4K video and definitely not at an acceptable framerate. In my tests, emulator and game performance was terrible for anything but the oldest consoles. This made me question all the Raspberry Pi emulator devices around - either my hardware / software was just terrible or folks aren't really playing those devices much.</p><p> I ended up purchasing a <a href="https://www.lattepanda.com/">Latte Panda 3 Delta</a> - an obscure but really wonderful device. Aside from having a built-in Arduino, it also has headers for USB, power, and more. This made running wires for buttons, ports, lights, etc trivial and despite my constant wiring mistakes the board held up and worked really well.</p><p> In my tests the performance was really reasonable for a small single board computer and in many ways, I don't think the Floppy8 would exist without this device. Sure other single board options exist, but the Latte Panda was really perfect. The only complaint I have is that documentation is minimal, but because the hardware is basically an x86 machine with an embedded Arduino in a tiny package, I did not feel the need for complex docs.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/latte-panda_480x480.png?v=1677003630" alt=""></p>
<p><br> Perhaps the luckiest element of this build was the fact that the Latte Panda just barely fit into the floppy drive. I printed this rose-gold plastic piece with the internal dimensions of the case to see the fit and as you can see, it is snug length-wise, but it sure fits.</p>
<h3>How can I create an eject mechanism for the cartridges?</h3>
<p>I knew early on that I wanted the device to be able to "eject" carts. I experimented with many types of spring-loaded mechanisms, latches, etc but in the end I ended up simply placing a servo with a cog which rotates and pushes the cart out. The actual implementation was simple, but getting power to the servo was tricky. I tried various electircal-engineery ways to get a high amperage 5v line for the servo, but nothing worked. At one point I thought I'd put a battery pack inside the device which would be kept charged and used only for the high-amp spikes for the servo, but somehow in testing I managed to miswire that and burn up the servo. So my final decision was to pull from the 5v line on the LattePanda but put a 0.4amp fuse to protect the board in the case of a jammed servo causing an amperage spike and harming the board. In practice this has worked perfectly fine and despite jams / etc the fuse has never tripped and the board has never shown any issue.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/servo_480x480.png?v=1677003666" alt=""></p>
<h3>How can I control the front-facing LED?</h3>
<p>Controlling an RGB LED is, apparently, trivial.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/led_480x480.png?v=1677003693" alt=""></p>
<p>After soldering up some resistors (for some reason, idk it's what the internet said) you can control the colors straight from a couple PWM pins on an Arduino. This worked quickly and reliably. The terrible mess pictured above was replaced by a nicer heat-shrunk LED later, but the first attempt also worked, it was just ugly.</p>
<h3>Do I need fans?</h3>
<p>Early on, I did some heat stress testing by placing the Latte Panda inside the case and completely blocking the front and back with scrap plastic.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/heat_480x480.png?v=1677003722" alt=""></p>
<div><p>I let the device sit playing 4K video for several hours. The Latte Panda kept a steady 69c internal temp. Warm, but reasonable. The metal case works as a decent radiator and helps expell the warmth into the surrounding air. This was not completely similar to the final build, but it was close enough that I was confident the onboard fan would keep the device cool enough. </p><p> Eventually I printed a heat sensitive cartridge, placed it in the front slot and let the device run for a while.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/heat2_480x480.png?v=1677003743" alt=""></p>
<p>This was not informative, but it was cool to look at.</p>
<h3>Will the metal case block wifi?</h3>
<p>This is a quick one - no. Nice.</p>
<h3>How will I fit all the wires?</h3>
<p>I ended up crimping many custom wires for this project. I invested in some <a href="https://amzn.to/3kA3n3G">nice wire</a> for another project and used it all over this one. I also bought a <a href="https://amzn.to/3Dcs2lh">wire-stripper</a> and a <a href="https://amzn.to/3H1vr7M">crimp kit</a>. The most critical and difficult wire to create was the short USB-C wire which attached the Micro SD reader for the carts to the USB headers on the Latte Panda.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/wire_480x480.png?v=1677003772" alt=""></p>
<p>This required soldering to a small <a href="https://amzn.to/3wrefUm">USB-C female port</a>.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/usbc_240x240.png?v=1677003792" alt=""></p>
<p>Most of the other wires were simple - just short :)</p>
<h2>Week 7-10: 3D Printing!</h2>
<p>As the prototyping of core elements began to subside it became time to begin iterating on the most complex element of the Floppy8, the designs for the internal brackets / drive which would actually hold all the components securely.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/3d-breakout_600x600.png?v=1677003823" alt=""></p>
<div><p>As I mentioned before, I used Tinkercad for all the CAD work. This screenshot shows a small percentage of all the digital artifacts of this build. In total I would design and print 201 STL files for this project. Slowly tweaking and iterating fits, tolerance, feels - not to mention iterating on the overall designs as needs came up. </p><p> I've never 3D printed anything which required multiple parts so figuring out how to make parts which could be screwed together and hold strongly took time. The final internal design uses 11 parts...</p></div>
<ol>
<li>
<b>Button (Dark Grey)</b> - This is used to replace the stock front button with one which has a short throw and flairs at the bottom.</li>
<li>
<b>Button Mount (Orange)</b> - This bracket holds the small circuit board which the front button is mounted to.</li>
<li>
<b>Cover Mount (Red)</b> - The original front cover and the button mount screw to this bracket. It has additional holes to attach it to the cart shifter.</li>
<li>
<b>Cart Shifter (Light Blue)</b> - This L shaped piece has two feet on the backside which the Latte Panda rests on and a slight slope on the front to help shift the cart upward and into the actual bay.</li>
<li>
<b>Drive Bay (Brown)</b> - This holds the actual cart when inserted along with the Micro SD card reader. It also serves as a mount for the Servo bracket (Pink) and LED bracket (Light Purple / Beige).</li>
<li>
<b>Servo Bracket (Pink)</b> - This bracket holds the micro-sized servo and screws to the Drive Bay and Cover Mount to add rigidity.</li>
<li>
<b>LED Bracket (Beige and Light Purple)</b> - This wraps about the LED assembly and holds it against the Cover Mount while being screwed to the Drive Bay.</li>
<li>
<b>Rear Bracket (Yellow)</b> - This bracket has the feet for the rear Latte Panda screw holes and also has a mount point for the power switch in the top left. It also has screw holes for the back cover.</li>
<li>
<b>Back Cover (Green)</b> - This cover / vent serve as the back plate for the device.</li>
<li>
<b>Port Cover (Orange)</b> - This removable port cover has carefully cut holes for the USB-C and HDMI ports on the Latte panda, but because the ports are a bit sunk into the device, the piece pops out in case the cables need to go deeper than the plate allows.</li>
<li>
<b>Metal Rods (Dark Grey)</b> - These pieces are made from 3mm risers which have been screwed together to the proper length, then sealed in heat-shrink tubing. They are included in the model for context.</li>
</ol>
<p>It's worth nothing all the parts are printed without supports except the Cover Mount which has those small corners cut out to allow for the heads of screws to attach to the metal riser rods. As that corner is an unsupported overhand on two sides, it needed supports.</p>
<h2>Week 11: Assembly</h2>
<p>As I printed I would attempt to put pieces together, iterate, print again. I learned to design pieces to be small to save print time and slowly over weeks the device started to emerge.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/near-final-assembly_480x480.png?v=1677003861" alt=""></p>
<p>With some assembly working correctly, I was able to check how the Floppy8 would feel with a cart inserted. I wanted it to resemble a Super Nintendo or a Famicom, with the cart label still visible when inserted.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/fit_480x480.png?v=1677003887" alt=""></p>
<p>One of the last elements to sort out was how to mount the internal switch for the front button (and how to replace the front button with one which matched the plastic of the front cover). I tried various options to recreate the visible element of the button, I tried making a mold and casting it in resin, etc. I eventually just printed it and resigned to sanding it smooth. I also found <a href="https://amzn.to/3EDmlOa">these adorable PCBs</a> on Amazon which are perfect to mount a single button.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-23_15-49-17_480x480.png?v=1677188971" alt=""></p>
<p>I like this close up because it helps to show how fucked up projects are when you look at them up close. In real life the Floppy8 looks fantastic because it fits in the original shell, but up close the internals have lots of bits which were printed bad or screw holes I had to drill out or whatever. These things aren't as perfect as photos make them look, so don't get down on yourself if your project isn't perfect.</p>
<p>Anyway, after getting these last elements sorted the whole thing felt fantastic to me - it felt real. I kept iterating then one day, like the flip of a switch, I realized we were almost done. I reprinted all the existing parts in black to help reduce unintended internal LED glow and began putting together the final bits.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/assembly_600x600.png?v=1677003908" alt=""></p>
<p>Putting all the pieces together, including a lot of wire crimping, we ended up with a functional but unhoused Floppy8.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/assembled_600x600.png?v=1677003936" alt=""></p>
<p>Testing at this point confirmed that our cartridges were read, the servo ejects them, the LED reflects the status, it all worked. Perhaps amazingly it also all fit in the case.</p>
<h2>Week 12: Final Polish and Controllers</h2>
<p>As the final bits and bobs began to line up I was finally able to do some real testing for heat, cartridge seating, and ~vibe~.</p>
<p>Heat wise, amazingly, even with all the junk in the case the Latte Panda stayed cool. However, there are some air vents directly below the cartridge slot which pump a lot of heat up around the cart. Originally the SD cart reader was mounted in the place with hot glue... you can see where this is going.</p>
<p>Aside from dripping glue down onto the Panda (which luckily missed the fan by a few mil) the glue was also prone to shifting over the course of a long gaming session or film. I redesigned the cartridge slot as two parts, one which aligns the carts and one which firmly holds the SD reader.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-23_15-44-05_480x480.png?v=1677188782" alt=""></p>
<p>This redesign made the feel of inserting a cart <em>way</em> better.</p>
<p>I also took this opportunity to work on input devices! I picked up this pair of <a href="https://amzn.to/3Kxryur">clone NES controllers</a>. I also purchase the official Nintendo Switch Bluetooth NES controllers, but the after-market clones are actually beige, not grey like the official ones - so I opted to go with those.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-24_13-25-46_480x480.png?v=1677266875" alt=""></p>
<p>I made a few small modifications to the controllers. I replaced the cheap looking matte d-pad with the one from the official controllers so it had the imprinted arrows and felt more premium. Then I took a note out of the book of toy-makers I added a bunch of weight to the controller to make it feel more real.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-24_13-27-09_480x480.png?v=1677266988" alt=""></p>
<p>I glued in a ton of ball bearings and tbh this makes way more difference than you'd think! The other very small detail was the light bleed from the connection LED.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-24_13-26-48_480x480.png?v=1677266981" alt=""></p>
<p>This small detail highlighted the cheapness / thinness of the plastic, so I used a small piece of heat-shrink tubing around the LED to help focus the light.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-24_13-26-57_480x480.png?v=1677267126" alt=""></p>
<p>It's another super tiny detail, but it makes everything feel more polished.</p>
<p>I also purchased a replacement <a href="https://amzn.to/3XTj6Ja">Fire TV remote</a> (which is just bluetooth and works great with Linux) along with an after-market beige shell for it.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-24_13-34-44_240x240.png?v=1677267398" alt=""></p>
<p>Sadly the shell is shipping from China and has a ~2mo wide delivery window. So it hasn't arrived yet. When it does, I'll look into removing branding from the remote's buttons and replacing the shell so we have a proper remote! I considered buying an actual old beige remote, but they realistically didn't have all the buttons I wanted, but if I find the right one, I may migrate the insides of the Fire TV remote to a whole new home in something truly retro!</p>
<h2>Anyway, we're done...</h2>
<p>Finally after weeks of tweaking, ups and down, frustrations and almost giving up at least once a day for weeks, it was done.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-23_15-10-03_600x600.png?v=1677186628" alt=""></p>
<p>I was able to sit, watch a film, play a game, etc. I made a lot of mistakes, wasted a lot of filament, but at the end of the day I'm very proud that I kept pushing and didn't give up. I'll probably never use the Floppy8 much, but I'm proud to have it on my desk.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/IMG_4722_600x600.jpg?v=1677189226" alt=""></p>
<p>Thanks for reading and happy hacking!</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AuraFlow v0.1: a open source alternative to Stable Diffusion 3 (120 pts)]]></title>
            <link>https://blog.fal.ai/auraflow/</link>
            <guid>40941853</guid>
            <pubDate>Fri, 12 Jul 2024 00:42:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.fal.ai/auraflow/">https://blog.fal.ai/auraflow/</a>, See on <a href="https://news.ycombinator.com/item?id=40941853">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
        <p>Open-source AI is in jeopardy. As community interest in AI models skyrocketed over the past year, we noticed that development of new open-source foundational models came to a halt. Some even boldly announced that open-source AI is dead. Not so fast!</p><p>We are excited to present you the first release of our AuraFlow model series, the largest yet completely open sourced flow-based generation model that is capable of text-to-image generation. AuraFlow is a reaffirmation of the open-source community's resilience and relentless determination.</p><h2 id="how-do-i-use-it">How do I use it?</h2><p>If you want to try out a few quick prompts, go to <a href="https://fal.ai/models/fal-ai/aura-flow/playground?ref=blog.fal.ai">fal’s model gallery</a> to start playing around.</p><p>If you want to build some cool Comfy workflows with the model, get the latest version of <a href="https://github.com/comfyanonymous/ComfyUI?ref=blog.fal.ai" rel="noreferrer">Comfy</a> and download the model weights from our <a href="https://huggingface.co/fal/AuraFlow?ref=blog.fal.ai" rel="noreferrer">HuggingFace page</a>.</p><p>We would love to give a huge shout out to ComfyUI and the HuggingFace 🤗 diffusers 🧨 teams for supporting AuraFlow natively on Comfy and <code>diffusers</code> on day 0!</p><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-1.png" alt="" loading="lazy" width="1257" height="593" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-1.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-1.png 1000w, https://blog.fal.ai/content/images/2024/07/Untitled-1.png 1257w" sizes="(min-width: 720px) 720px"><figcaption><span>A fine selection of AuraFlow v0.1 generations</span></figcaption></figure><h2 id="how-this-collaboration-happened">How this collaboration happened</h2><p><a href="https://x.com/cloneofsimo?ref=blog.fal.ai" rel="noreferrer">Simo</a> is one of our favorite researchers in the wild world of generative media models. You may know him from the amazing <a href="https://github.com/cloneofsimo/lora?ref=blog.fal.ai" rel="noreferrer">adaptation of the LoRA paper</a> for text-to-image models. Few months ago, Simo wanted to implement MMDiT from scratch, and see if he would be able to reproduce it. His initial attempts at <a href="https://github.com/cloneofsimo/minRF?ref=blog.fal.ai">https://github.com/cloneofsimo/minRF</a> and its initial result <a href="https://huggingface.co/cloneofsimo/lavenderflow-5.6B?ref=blog.fal.ai">Lavenderflow-v0</a> came out to be promising. Soon, he found various aspects that could be optimized to train the model on a larger scale more efficiently.</p><p>Timing couldn’t have worked out better. Right around this time, we were convinced that a SOTA open-sourced model is the way forward for this space to move forward. We wanted to bring serious resources and compute to scale up the model. We were aligned very well, and thus begun the collaboration.</p><p>AuraFlow demonstrates that collaborative, transparent AI development is not only alive but thriving, ready to tackle the challenges and opportunities of tomorrow's AI landscape.</p><h2 id="technical-details">Technical Details</h2><p>Here, we wanted to share some initial technical details that stand out. We are planning on following up with a more detailed report and possibly a paper as well.</p><p><strong>1. MFU as a first-class citizen</strong></p><p><strong>Most layers don’t need MMDiT Blocks</strong>: While MMDiT achieved good performance, we found that removing many layers to just be single DiT block were much more scalable and compute efficient way to train these models. With careful search in the small-scale proxy, we’ve removed most of the MMDiT blocks and replaced them with large DiT Encoder blocks. These improved the model flops utilization at 6.8B scale by 15%. </p><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-2.png" alt="" loading="lazy" width="1110" height="710" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-2.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-2.png 1000w, https://blog.fal.ai/content/images/2024/07/Untitled-2.png 1110w" sizes="(min-width: 720px) 720px"><figcaption><span>Number of Double Layers and Optimal Learning Rate</span></figcaption></figure><p><strong>Improved training with torch.compile:</strong> At fal, we are already big fans of Torch Dynamo + Inductor, and build on top of this tooling (with a custom dynamo backend) to run our inference workloads super fast (and efficiently utilizing the underlying hardware). Since PT2’s torch.compile is able to handle both forward and backwards passes, AuraFlow’s training was further optimized with its primitives on each layers forward method, and further able to improve MFU by extra 10% ~ 15% depending on the stage.</p><p>2.<strong> Unlock zero-shot learning rate transfer</strong>It is clear that we are not Meta, and would like to have very good hyperparameters even without sweeping them. Fortunately, we noticed MMDiT architectures were also zero-shot LR transferred with maximal-update-parameterization was utilized.Compared to SP, muP was clearly the winner in terms of predictability of learning rate at scale.</p><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-3.png" alt="" loading="lazy" width="1042" height="668" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-3.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-3.png 1000w, https://blog.fal.ai/content/images/2024/07/Untitled-3.png 1042w" sizes="(min-width: 720px) 720px"><figcaption><span>Standard Parametrization</span></figcaption></figure><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-4.png" alt="" loading="lazy" width="1196" height="774" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-4.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-4.png 1000w, https://blog.fal.ai/content/images/2024/07/Untitled-4.png 1196w" sizes="(min-width: 720px) 720px"><figcaption><span>Maximal Update Parametrization</span></figcaption></figure><p><strong>3. Re-captioned, everything.</strong>It is common trick to recaption everything to make sure there are no faulty text conditions in the dataset. We used our in-house captioner &amp; external captioned dataset to train these models, which improves the quality of the instruction-following significantly. We followed the DALL·E 3 approach to the extreme, and we had no captions that were alt-texts.</p><p><strong>4. Wider, shorter, better!</strong>To further investigate the optimal architecture, we were interested into making a fatter model, i.e., making the architecture overall utilize largest matmul divisible by 256. This lead us into searching for optimal aspect ratio under optimal learning rate found by muP.With these findings, we were confident that aspect ratio of 20 ~ 100 is indeed suitable at larger scale, which was similar with findings from <a href="https://arxiv.org/abs/2010.14701?ref=blog.fal.ai">Scaling Laws for Autoregressive Generative Modeling</a>. We ended up using 3072 / 36, which resulted in model size of 6.8B parameters.</p><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-6.png" alt="" loading="lazy" width="1945" height="1409" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-6.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-6.png 1000w, https://blog.fal.ai/content/images/size/w1600/2024/07/Untitled-6.png 1600w, https://blog.fal.ai/content/images/2024/07/Untitled-6.png 1945w" sizes="(min-width: 720px) 720px"><figcaption><span>Number of Parameters / Loss</span></figcaption></figure><p>In the end, we did the best of our ability to improve and effectively find the optimal configurations for large scale training. Utilizing the findings from above, we were able to train a text-to-image model from scratch in our largest possible settings for 4 week of compute time, including 256x256, 512x512, 1024x1024 pre-training and aspect ratio fine-tuning. Final model achieves a GenEval score of 0.63~0.67 during pretraining, and similarly 0.64 after 1024x1024 pretraining. But with prompt-enhancement pipeline similar to DALL·E 3, we were able to achieve 0.703!</p><div data-kg-toggle-state="close">
            <div>
                <h4><span>Prompt for prompt-enhancement</span></h4>
                </div>
            <p><span>A caption is a way that a person would describe an image separated by commas when necessary. All in lower case. Expand the input below into a more detailed caption without changing the original relative positions or interactions between objects, colors or any other specific attributes if they are disclosed in the original prompt. Clarify positional information, colors, counts of objects, other visual aspects and features. Make sure to include as much detail as possible. Make sure to describe the spatial relationships seen in the image. You can use words like left/right, above/below, front/behind, far/near/adjacent, inside/outside. Make sure to include object interactions like "a table is in front of the kitchen pot" and "there are baskets on the table". Also describe relative sizes of objects seen in the image. Make sure to include counts of prominent objects in the image, especially when there is humans in the image. When its a photograph, include photographic details like bokeh, large field of view etc but dont just say it to say something, do it only when it makes sense. When its art, include details about the style like minimalist, impressionist, oil painting etc. Include world and period knowledge if it makes sense to, like 1950s chevrolet etc.</span></p>
        </div><h3 id="challenges-of-distributed-training-on-multi-modal-data">Challenges of distributed training on multi-modal data</h3><p>One of the harshest realities of training image models is that, unlike LLMs, the modality of the data itself can be a real pain to deal with. During AuraFlow’s training, we leveraged our expertise from dealing with distributed storage as well as managing a large fleet of thousands of GPUs.</p><p>Some of this expertise was directly transferable from production grade inference/fine-tuning systems, where we were able to use open source projects like <a href="https://github.com/juicedata/juicefs?ref=blog.fal.ai">JuiceFS</a> and some were more novel challenges like how do you stream massive amounts of data in and out of multiple nodes while leveraging local NVME space as a staging ground to not to reduce the MFU.</p><p>Be on the lookout for a detailed post on how we choose our storage mediums, where we trained this model, how we evaluated GPU performance and managed large clusters!</p><h2 id="what-is-next">What is next?</h2><p>We are not done training! This model is an initial release to kickstart some community engagement. We will continue training the model and apply our learnings from this first attempt. We also noticed that smaller models or MoE’s might be more efficient for consumer GPU cards which have a limiter amount of compute power, so follow closely for a mini version of model that is still as powerful yet much much faster to run. In the meantime, we encourage the community to experiment with what we are releasing today.</p><p>Our goal is to make this model a standard backbone that other innovative work can be built on top of. We look forward to community contributions. If you want to train finetunes, IP-Adapters, or quantizations of the current model, we are happy to support you in any way we can. There is already a vibrant community around fal and Aura models in our Discord. We <a href="https://discord.gg/fal-ai?ref=blog.fal.ai">invite</a> you to join if you want to get involved.</p><p>For business inquiries, please email us at <a href="mailto:hello@fal.ai">hello@fal.ai</a> 😄</p>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Physics-Based Deep Learning Book (243 pts)]]></title>
            <link>https://physicsbaseddeeplearning.org/intro.html</link>
            <guid>40941056</guid>
            <pubDate>Thu, 11 Jul 2024 22:10:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://physicsbaseddeeplearning.org/intro.html">https://physicsbaseddeeplearning.org/intro.html</a>, See on <a href="https://news.ycombinator.com/item?id=40941056">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="main-content" role="main">
<h2>Welcome …<a href="#welcome" title="Permalink to this headline">#</a></h2>
<figure id="pbdl-logo-large">
<img alt="_images/logo-xl.jpg" src="https://physicsbaseddeeplearning.org/_images/logo-xl.jpg">
</figure>
<p>Welcome to the <em>Physics-based Deep Learning Book</em> (v0.2) 👋</p>
<p><strong>TL;DR</strong>:
This document contains a practical and comprehensive introduction of everything
related to deep learning in the context of physical simulations.
As much as possible, all topics come with hands-on code examples in the
form of Jupyter notebooks to quickly get started.
Beyond standard <em>supervised</em> learning from data, we’ll look at <em>physical loss</em> constraints,
more tightly coupled learning algorithms with <em>differentiable simulations</em>,
training algorithms tailored to physics problems,
as well as
reinforcement learning and uncertainty modeling.
We live in exciting times: these methods have a huge potential to fundamentally
change what computer simulations can achieve.</p>

<hr>
<section id="coming-up">
<h2>Coming up<a href="#coming-up" title="Permalink to this headline">#</a></h2>
<p>As a <em>sneak preview</em>, the next chapters will show:</p>
<ul>
<li><p>How to train networks to infer a fluid flow around shapes like airfoils, and estimate the uncertainty of the prediction. This gives a <em>surrogate model</em> that replaces a traditional numerical simulation.</p></li>
<li><p>How to use model equations as residuals to train networks that represent solutions, and how to improve upon these residual constraints by using <em>differentiable simulations</em>.</p></li>
<li><p>How to more tightly interact with a full simulator for <em>inverse problems</em>. E.g., we’ll demonstrate how to circumvent the convergence problems of standard reinforcement learning techniques by leveraging simulators in the training loop.</p></li>
<li><p>We’ll also discuss the importance of <em>inversion</em> for the update steps, and how higher-order information can be used to speed up convergence, and obtain more accurate neural networks.</p></li>
</ul>
<p>Throughout this text,
we will introduce different approaches for introducing physical models
into deep learning, i.e., <em>physics-based deep learning</em> (PBDL) approaches.
These algorithmic variants will be introduced in order of increasing
tightness of the integration, and the pros and cons of the different approaches
will be discussed. It’s important to know in which scenarios each of the
different techniques is particularly useful.</p>
<div>
<p>Executable code, right here, right now</p>
<p>We focus on Jupyter notebooks, a key advantage of which is that all code examples
can be executed <em>on the spot</em>, from your browser. You can modify things and
immediately see what happens – give it a try by
<a href="https://colab.research.google.com/github/tum-pbs/pbdl-book/blob/main/intro-teaser.ipynb">[running this teaser example in your browser]</a>.</p>
<p>Plus, Jupyter notebooks are great because they’re a form of <a href="https://en.wikipedia.org/wiki/Literate_programming">literate programming</a>.</p>
</div>
</section>
<section id="comments-and-suggestions">
<h2>Comments and suggestions<a href="#comments-and-suggestions" title="Permalink to this headline">#</a></h2>
<p>This <em>book</em>, where “book” stands for a collection of digital texts and code examples,
is maintained by the
<a href="https://ge.in.tum.de/">Physics-based Simulation Group</a> at <a href="https://www.tum.de/">TUM</a>.
Feel free to contact us if you have any comments, e.g., via <a href="mailto:i15ge%40cs.tum.edu">old fashioned email</a>.
If you find mistakes, please also let us know! We’re aware that this document is far from perfect,
and we’re eager to improve it. Thanks in advance 😀!
Btw., we also maintain a <a href="https://github.com/thunil/Physics-Based-Deep-Learning">link collection</a> with recent research papers.</p>
<figure id="divider-mult">
<a href="https://physicsbaseddeeplearning.org/_images/divider-mult.jpg"><img alt="_images/divider-mult.jpg" src="https://physicsbaseddeeplearning.org/_images/divider-mult.jpg"></a>
<figcaption>
<p><span>Fig. 1 </span><span>Some visual examples of numerically simulated time sequences. In this book, we explain how to realize algorithms that use neural networks alongside numerical solvers.</span><a href="#divider-mult" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="thanks">
<h2>Thanks!<a href="#thanks" title="Permalink to this headline">#</a></h2>
<p>This project would not have been possible without the help of many people who contributed. Thanks to everyone 🙏 Here’s an alphabetical list:</p>
<ul>
<li><p><a href="https://ge.in.tum.de/about/philipp-holl/">Philipp Holl</a></p></li>
<li><p><a href="https://ge.in.tum.de/">Maximilian Mueller</a></p></li>
<li><p><a href="https://ge.in.tum.de/about/patrick-schnell/">Patrick Schnell</a></p></li>
<li><p><a href="https://ge.in.tum.de/">Felix Trost</a></p></li>
<li><p><a href="https://ge.in.tum.de/about/n-thuerey/">Nils Thuerey</a></p></li>
<li><p><a href="https://ge.in.tum.de/about/kiwon/">Kiwon Um</a></p></li>
</ul>
<p>Additional thanks go to
Georg Kohl for the nice divider images (cf. <span id="id1">[<a href="https://physicsbaseddeeplearning.org/references.html#id8" title="Georg Kohl, Kiwon Um, and Nils Thuerey. Learning similarity metrics for numerical simulations. International Conference on Machine Learning, 2020. URL: https://ge.in.tum.de/publications/2020-lsim-kohl/.">KUT20</a>]</span>),
Li-Wei Chen for the airfoil data image,
and to
Chloe Paillard for proofreading parts of the document.</p>
</section>
<section id="citation">
<h2>Citation<a href="#citation" title="Permalink to this headline">#</a></h2>
<p>If you find this book useful, please cite it via:</p>
<div><pre><span></span><span>@book</span><span>{</span><span>thuerey2021pbdl</span><span>,</span>
  <span>title</span><span>=</span><span>{</span><span>Physics</span><span>-</span><span>based</span> <span>Deep</span> <span>Learning</span><span>},</span>
  <span>author</span><span>=</span><span>{</span><span>Nils</span> <span>Thuerey</span> <span>and</span> <span>Philipp</span> <span>Holl</span> <span>and</span> <span>Maximilian</span> <span>Mueller</span> <span>and</span> <span>Patrick</span> <span>Schnell</span> <span>and</span> <span>Felix</span> <span>Trost</span> <span>and</span> <span>Kiwon</span> <span>Um</span><span>},</span>
  <span>url</span><span>=</span><span>{</span><span>https</span><span>:</span><span>//</span><span>physicsbaseddeeplearning</span><span>.</span><span>org</span><span>},</span>
  <span>year</span><span>=</span><span>{</span><span>2021</span><span>},</span>
  <span>publisher</span><span>=</span><span>{</span><span>WWW</span><span>}</span>
<span>}</span>
</pre></div>
</section>









</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WebVM is a server-less virtual Linux environment running client-side (538 pts)]]></title>
            <link>https://webvm.io/</link>
            <guid>40940225</guid>
            <pubDate>Thu, 11 Jul 2024 20:16:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webvm.io/">https://webvm.io/</a>, See on <a href="https://news.ycombinator.com/item?id=40940225">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="loginLink">
                    <p><span id="networkStatus">Connect via Tailscale </span>
	              <span id="ipCopied">Copied! </span>
                    </p>
	            <p><img src="https://webvm.io/assets/tailscale.svg" height="35px">
		  </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Karpathy: Let's reproduce GPT-2 (1.6B): one 8XH100 node 24h $672 in llm.c (176 pts)]]></title>
            <link>https://github.com/karpathy/llm.c/discussions/677</link>
            <guid>40939707</guid>
            <pubDate>Thu, 11 Jul 2024 19:21:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/karpathy/llm.c/discussions/677">https://github.com/karpathy/llm.c/discussions/677</a>, See on <a href="https://news.ycombinator.com/item?id=40939707">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="presentation" data-paste-markdown-skip="">
    <tbody data-target-translation-id="6923990" data-target-translation-type="discussion">
        <tr>
    <td>
        <p dir="auto">In this post we are reproducing GPT-2 in llm.c. This is <strong>"the GPT-2"</strong>, the full, 1558M parameter version that was introduced in OpenAI's blog post <a href="https://openai.com/index/better-language-models/" rel="nofollow">Better Language Models and their Implications</a> in February 14, 2019. llm.c does so directly in C/CUDA (total of ~5,000 lines of code), without the typical training stack that would involve the Python interpreter and a significantly more complex deep learning library like PyTorch/JAX, huggingface/transformers, or etc. In 2019, training GPT-2 was an involved project from an entire team and considered a big model run but, ~5 years later, due to improvements in compute (H100 GPUs), software (CUDA, cuBLAS, cuDNN, FlashAttention) and data (e.g. the FineWeb-Edu dataset), we can reproduce this model on a single 8XH100 node in 24 hours, and for $672, which is quite incredible. There are some caveats and asterisks involved though - llm.c is still not perfectly tuned, sufficiently stabilized (we still see loss spikes and bad activation ranges now and then), and our evals are not comprehensive (e.g. we do not carefully eval multilingual, code, math); A lot of this work is still ongoing.</p>
<p dir="auto"><strong>Unicorns</strong>. The natural place to start is probably with unicorns. In the GPT-2 blog post, OpenAI prompted GPT-2 with this unicorn prompt. Let's see what our model thinks about English-speaking unicorns in the Andes mountains:</p>
<p dir="auto"><strong>Prompt</strong>: <em>In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.</em><br>
<strong>Completion</strong>: <em>It may sound like a hoax, or just a dream of the mythical animals, but it appears this may be a real case of the unicorn language. The Andean area has an abundance of minerals and is also very rich in biodiversity. According to the researchers' study, the unicorns' language is called Elveseo. To produce it, the animal forms a consonant from the lips and makes a vowel sound with the teeth, then produces a consonant sound with the mouth and makes an 'ow' sound from its chest. "The question is, what caused this, and what does this tell us about intelligence and communication?" commented study co-author Paul Marek. Speaking in the video, the unicorns had the right way to speak, the animals were able to communicate and talk in perfect English.</em></p>
<p dir="auto">Fun! :) The model is fairly coherent and qualitatively somewhere around the level of GPT-2. You can find 20 samples from both GPT-2 and the llm.c model <a href="http://llmc.s3-us-west-2.amazonaws.com/html/gpt2_vs_llmc30kedu.html" rel="nofollow">here</a>, or generate many more using instructions down below.</p>
<p dir="auto"><strong>Training</strong>. Training a GPT-2 with llm.c is quite simple because it is written in C/CUDA, so there is no need for minconda, Python, PyTorch, etc. You will want an 8XH100 GPU box, I recommend spinning one up from <a href="https://lambdalabs.com/" rel="nofollow">Lambda labs</a>. But llm.c is flexible on its compute - if you have only 1 GPU you can still get your GPT-2, you'll just have to wait 8 days instead of 1. If you have 16 GPUs (e.g. using the new Lambda 1 Click Clusters), you'll be able to train multinode and only have to wait 12 hours. Once you spin up your node, here are the complete instructions to train your GPT-2 (this only takes a ~minute from blank box to start stepping):</p>
<div dir="auto" data-snippet-clipboard-copy-content="# install cudnn so we can use FlashAttention and run fast (optional)
# https://developer.nvidia.com/cudnn-downloads
# for me, CUDA 12 (run `nvcc --version`) running on Linux x86_64 Ubuntu 22.04
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get -y install libcudnn9-dev-cuda-12

# &quot;install&quot; cudnn-frontend to ~/
git clone https://github.com/NVIDIA/cudnn-frontend.git

# install MPI (optional, if you intend to use multiple GPUs)
# (you might also have to install NVIDIA NCCL if it doesn't come with your setup)
sudo apt -y install openmpi-bin openmpi-doc libopenmpi-dev

# download and enter llm.c repo
git clone https://github.com/karpathy/llm.c.git
cd llm.c

# download the &quot;starter pack&quot; (~1GB download)
# contains GPT2-124M weights (used in tests), tokenizer, eval data .bin s
./dev/download_starter_pack.sh

# download the training dataset (FineWeb-Edu 100B token) .bin data shards
# note: this is a total of 1001 data shards. If you only want to test things
# out and don't want to do an actual run, feel free to append the number of
# training shards to download (e.g. for just 10 shards: ./edu_fineweb.sh 10)
# the full dataset is ~200GB, we can store it here in dev/data directory.
cd dev/data
./edu_fineweb.sh

# compile (~1 min 1st time for cuDNN mostly, few sec from then on)
cd ../../
make train_gpt2cu USE_CUDNN=1

# and train! (wait 24 hours here)
mpirun -np 8 ./train_gpt2cu \
	-i &quot;dev/data/edu_fineweb100B/edu_fineweb_train_*.bin&quot; \
	-j &quot;dev/data/edu_fineweb100B/edu_fineweb_val_*.bin&quot; \
	-o &quot;log_gpt2_1558M&quot; \
	-v 250 -s 300000 -g 384 \
	-h 1 \
	-b 16 -t 1024 \
	-d 1048576 \
	-r 0 \
	-z 1 \
	-c 0.1 \
	-k &quot;cosine&quot; \
	-l 0.0006 \
	-q 0.1 \
	-u 700 \
	-n 2000 \
	-x 32000 \
	-ge 1 \
	-y 1 \
	-e &quot;d48&quot;"><pre><span><span>#</span> install cudnn so we can use FlashAttention and run fast (optional)</span>
<span><span>#</span> https://developer.nvidia.com/cudnn-downloads</span>
<span><span>#</span> for me, CUDA 12 (run `nvcc --version`) running on Linux x86_64 Ubuntu 22.04</span>
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get -y install libcudnn9-dev-cuda-12

<span><span>#</span> "install" cudnn-frontend to ~/</span>
git clone https://github.com/NVIDIA/cudnn-frontend.git

<span><span>#</span> install MPI (optional, if you intend to use multiple GPUs)</span>
<span><span>#</span> (you might also have to install NVIDIA NCCL if it doesn't come with your setup)</span>
sudo apt -y install openmpi-bin openmpi-doc libopenmpi-dev

<span><span>#</span> download and enter llm.c repo</span>
git clone https://github.com/karpathy/llm.c.git
<span>cd</span> llm.c

<span><span>#</span> download the "starter pack" (~1GB download)</span>
<span><span>#</span> contains GPT2-124M weights (used in tests), tokenizer, eval data .bin s</span>
./dev/download_starter_pack.sh

<span><span>#</span> download the training dataset (FineWeb-Edu 100B token) .bin data shards</span>
<span><span>#</span> note: this is a total of 1001 data shards. If you only want to test things</span>
<span><span>#</span> out and don't want to do an actual run, feel free to append the number of</span>
<span><span>#</span> training shards to download (e.g. for just 10 shards: ./edu_fineweb.sh 10)</span>
<span><span>#</span> the full dataset is ~200GB, we can store it here in dev/data directory.</span>
<span>cd</span> dev/data
./edu_fineweb.sh

<span><span>#</span> compile (~1 min 1st time for cuDNN mostly, few sec from then on)</span>
<span>cd</span> ../../
make train_gpt2cu USE_CUDNN=1

<span><span>#</span> and train! (wait 24 hours here)</span>
mpirun -np 8 ./train_gpt2cu \
	-i <span><span>"</span>dev/data/edu_fineweb100B/edu_fineweb_train_*.bin<span>"</span></span> \
	-j <span><span>"</span>dev/data/edu_fineweb100B/edu_fineweb_val_*.bin<span>"</span></span> \
	-o <span><span>"</span>log_gpt2_1558M<span>"</span></span> \
	-v 250 -s 300000 -g 384 \
	-h 1 \
	-b 16 -t 1024 \
	-d 1048576 \
	-r 0 \
	-z 1 \
	-c 0.1 \
	-k <span><span>"</span>cosine<span>"</span></span> \
	-l 0.0006 \
	-q 0.1 \
	-u 700 \
	-n 2000 \
	-x 32000 \
	-ge 1 \
	-y 1 \
	-e <span><span>"</span>d48<span>"</span></span></pre></div>
<p dir="auto">I will describe the args in a second. You'll see a bunch of prints scroll through and then the optimization will begin:</p>
<div data-snippet-clipboard-copy-content="num_parameters: 1557686400 => bytes: 3115372800
allocated 2971 MiB for model parameters
batch_size B=16 * seq_len T=1024 * num_processes=8 and total_batch_size=1048576
=> setting grad_accum_steps=8
created directory: log_gpt2_1558M
allocating 40409 MiB for activations
val loss 11.129390
allocating 2971 MiB for parameter gradients
allocating 742 MiB for AdamW optimizer state m
allocating 742 MiB for AdamW optimizer state v
allocating 742 MiB for master copy of params
step    1/32000 | loss 11.133732 (+nanz)| norm 52.9732 (+nanz)| lr 8.57e-07 | 3056.36 ms | 42.6% bf16 MFU | 343080 tok/s
step    2/32000 | loss 10.539388 (+nanz)| norm 43.5996 (+nanz)| lr 1.71e-06 | 2747.19 ms | 47.4% bf16 MFU | 381690 tok/s
step    3/32000 | loss 9.894109 (+nanz)| norm 23.2229 (+nanz)| lr 2.57e-06 | 2753.25 ms | 47.3% bf16 MFU | 381259 tok/s
step    4/32000 | loss 9.566241 (+nanz)| norm 28.4920 (+nanz)| lr 3.43e-06 | 2741.47 ms | 47.5% bf16 MFU | 381690 tok/s
step    5/32000 | loss 9.482848 (+nanz)| norm 23.7817 (+nanz)| lr 4.29e-06 | 2752.07 ms | 47.3% bf16 MFU | 381507 tok/s
step    6/32000 | loss 9.332832 (+nanz)| norm 15.9113 (+nanz)| lr 5.14e-06 | 2751.01 ms | 47.3% bf16 MFU | 381431 tok/s
step    7/32000 | loss 9.165650 (+nanz)| norm 10.5941 (+nanz)| lr 6.00e-06 | 2753.03 ms | 47.3% bf16 MFU | 381327 tok/s
step    8/32000 | loss 9.132234 (+nanz)| norm 16.2733 (+nanz)| lr 6.86e-06 | 2748.91 ms | 47.3% bf16 MFU | 381348 tok/s
step    9/32000 | loss 9.097384 (+nanz)| norm 12.1342 (+nanz)| lr 7.71e-06 | 2748.73 ms | 47.3% bf16 MFU | 381367 tok/s
step   10/32000 | loss 9.072879 (+nanz)| norm 10.5923 (+nanz)| lr 8.57e-06 | 2749.40 ms | 47.3% bf16 MFU | 381369 tok/s
..."><pre><code>num_parameters: 1557686400 =&gt; bytes: 3115372800
allocated 2971 MiB for model parameters
batch_size B=16 * seq_len T=1024 * num_processes=8 and total_batch_size=1048576
=&gt; setting grad_accum_steps=8
created directory: log_gpt2_1558M
allocating 40409 MiB for activations
val loss 11.129390
allocating 2971 MiB for parameter gradients
allocating 742 MiB for AdamW optimizer state m
allocating 742 MiB for AdamW optimizer state v
allocating 742 MiB for master copy of params
step    1/32000 | loss 11.133732 (+nanz)| norm 52.9732 (+nanz)| lr 8.57e-07 | 3056.36 ms | 42.6% bf16 MFU | 343080 tok/s
step    2/32000 | loss 10.539388 (+nanz)| norm 43.5996 (+nanz)| lr 1.71e-06 | 2747.19 ms | 47.4% bf16 MFU | 381690 tok/s
step    3/32000 | loss 9.894109 (+nanz)| norm 23.2229 (+nanz)| lr 2.57e-06 | 2753.25 ms | 47.3% bf16 MFU | 381259 tok/s
step    4/32000 | loss 9.566241 (+nanz)| norm 28.4920 (+nanz)| lr 3.43e-06 | 2741.47 ms | 47.5% bf16 MFU | 381690 tok/s
step    5/32000 | loss 9.482848 (+nanz)| norm 23.7817 (+nanz)| lr 4.29e-06 | 2752.07 ms | 47.3% bf16 MFU | 381507 tok/s
step    6/32000 | loss 9.332832 (+nanz)| norm 15.9113 (+nanz)| lr 5.14e-06 | 2751.01 ms | 47.3% bf16 MFU | 381431 tok/s
step    7/32000 | loss 9.165650 (+nanz)| norm 10.5941 (+nanz)| lr 6.00e-06 | 2753.03 ms | 47.3% bf16 MFU | 381327 tok/s
step    8/32000 | loss 9.132234 (+nanz)| norm 16.2733 (+nanz)| lr 6.86e-06 | 2748.91 ms | 47.3% bf16 MFU | 381348 tok/s
step    9/32000 | loss 9.097384 (+nanz)| norm 12.1342 (+nanz)| lr 7.71e-06 | 2748.73 ms | 47.3% bf16 MFU | 381367 tok/s
step   10/32000 | loss 9.072879 (+nanz)| norm 10.5923 (+nanz)| lr 8.57e-06 | 2749.40 ms | 47.3% bf16 MFU | 381369 tok/s
...
</code></pre></div>
<p dir="auto">We can see that each step is about 2.75 seconds and there are 32,000 of them, so now we wait ~24 hours. At every step, this training run takes a chunk of ~1 million tokens of <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu" rel="nofollow">FineWeb-EDU</a> (these are educational web pages from the internet), and updates the 1558 million weights of the model to be slightly better at predicting the next token in a sequence. By the end we'll have processed 32,000 * 1048576 = 33.6B tokens in total. The loss goes down as we do a better job predicting the next token. The norm will stabilize around 0.1-1, the learning rate is being warmed up over the first few steps. Our model flops utilization (MFU) is around 50%, i.e. quite efficient.</p>
<p dir="auto">Now wait 24 hours for this to finish, then you can visualize the <code>main.log</code> log file using the <a href="https://github.com/karpathy/llm.c/blob/master/dev/vislog.ipynb">dev/vislog.ipynb</a> jupyter notebook. For this you will need to also have Python and matplotlib installed, and you will see the following:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/241138/345149886-0ddc8c19-aa6a-4342-9292-81f40e49d5ad.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MzAxMDUsIm5iZiI6MTcyMDcyOTgwNSwicGF0aCI6Ii8yNDExMzgvMzQ1MTQ5ODg2LTBkZGM4YzE5LWFhNmEtNDM0Mi05MjkyLTgxZjQwZTQ5ZDVhZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzExJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcxMVQyMDMwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wMjBhOGYxYzdhOTY4YTUyMjk0MzRhMmU4NmUyZTBiYzk4ODkzODIyZWI3OTMxMGM2ZWY5YzA1NDRhNDFkN2JiJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.D8gaGy45j3mIysF2wxtmR9MbL8gjVwPGM4Zxxv7wr10"><img width="849" alt="image" src="https://private-user-images.githubusercontent.com/241138/345149886-0ddc8c19-aa6a-4342-9292-81f40e49d5ad.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MzAxMDUsIm5iZiI6MTcyMDcyOTgwNSwicGF0aCI6Ii8yNDExMzgvMzQ1MTQ5ODg2LTBkZGM4YzE5LWFhNmEtNDM0Mi05MjkyLTgxZjQwZTQ5ZDVhZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzExJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcxMVQyMDMwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wMjBhOGYxYzdhOTY4YTUyMjk0MzRhMmU4NmUyZTBiYzk4ODkzODIyZWI3OTMxMGM2ZWY5YzA1NDRhNDFkN2JiJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.D8gaGy45j3mIysF2wxtmR9MbL8gjVwPGM4Zxxv7wr10"></a>
<p dir="auto"><strong>Evals</strong>. On the left we are tracking the loss on FineWeb-EDU validation data. If you simply run the GPT-2 released by OpenAI and evaluate its loss on this split, you get the red horizontal line (loss 2.83). You see that our run outperforms this very very quickly, by step ~5,000. However, this is not a fair comparison because GPT-2 was trained on the never-released WebText dataset, so there is a possibly large distribution shift. So e.g. if you finetune the OpenAI model for 1,000 steps at LR 1e-4, the loss quickly plunges to the blue line (loss 2.61), because it's quickly adapting to the new data statistics. I like to look at the validation loss as a sanity check, but for the actual comparison we'd want to look at fixed, 3rd party evaluations. One of the well-behaved, smooth, common, often-cited evals that also offer early signal is the <a href="https://rowanzellers.com/hellaswag/" rel="nofollow">HellaSwag</a> eval. These are simple common sense scenarios and the model has to pick the correct continuation. We evaluate HellaSwag on the right pane, where we see that we cross over the GPT-2 model around step ~25K (earlier than GPT-2, which is estimated to have been trained on ~100B tokens. This possibly has to do with increased data quality, as we also observed in our earlier <a href="https://github.com/karpathy/llm.c/discussions/481" data-hovercard-type="discussion" data-hovercard-url="/karpathy/llm.c/discussions/481/hovercard">124M run</a>). The green line is the GPT-3 model of the same size, which is pretty much the same model architecture as GPT-2 with minor differences (context length 1024 -&gt; 2048) but trained for 300B tokens (i.e. ~10X more tokens than what we trained on here). I should say that even HellaSwag is not an ideal single point of comparison because it tests simple English and common sense, it does not test e.g. multilingual, math or code. It could have been that the WebText data mixture was a lot heavier on these, and these domains were "stealing" model capacity to some extent, we don't know because it was never released. Lastly, in general, good evals are harder at low model capability like GPT-2 because e.g. the models don't understand multiple choice, and their samples are not high enough quality to make above chance dent into standard math or code evals.</p>
<p dir="auto"><strong>Args guide</strong>. Let's look at the args we passed into the training now in more detail. The GPT-2 release from OpenAI included model weights but very few details, while GPT-3 release had no weights but many details. So in many cases, we follow the GPT-3 paper hyperparameters because the GPT-2 paper has very very little information:</p>
<ul dir="auto">
<li><code>-i -j</code> are training and validation splits token files, downloaded earlier with <code>edu_fineweb.sh</code></li>
<li><code>-o</code> is the output directory to write logs and checkpoints into</li>
<li><code>-v 250</code> asks to evaluate and log the validation loss every 250 steps</li>
<li><code>-s 300000</code> asks to sample some tokens every 300000 steps. Because the total number of steps will be less than this, this is hacky way to turn sampling off and we will only sample a single time at the very end.</li>
<li><code>-g 384</code> sets the number of tokens to be sampled at the end to be 384</li>
<li><code>-h 1</code> asks to evaluate the HellaSwag accuracy</li>
<li><code>-b 16</code> sets the micro-batch size to 16 . If you are running out of memory, decrease this value, e.g. try 8, 4, 2, all the way down to 1 potentially.</li>
<li><code>-t 1024</code> sets the maximum sequence length to 1024, as GPT-2 did</li>
<li><code>-d 1048576</code> asks that the total batch size be 2 to the power 20, following the GPT-3 paper hyperparameters table. The code will make sure to meet this desired total batch size and calculate the needed gradient accumulation "inner loop" steps of the optimization. For example up above, we saw that we have 8 GPUs each doing 16 X 1024 tokens, so that is 8 X 16 X 1024 = 131,072 tokens per micro-step (a single forward backward), so the code calculated gradient accumulation steps of 8 to meet the desired 1M batch size per step. i.e. it does forward+backward 8 times and then a single update.</li>
<li><code>-r 0</code> sets recompute to zero. Recompute is a way to trade off compute and memory. If <code>-r 1</code>, then we recompute a piece of the forward pass (the GeLU) during backward. This means we don't have to cache it and save memory, at the cost of some  more compute. So if you're running out of memory, try -r 1, or -r 2 (also recompute layernorms).</li>
<li><code>-z 1</code> turns on ZeRO-1 (i.e. optimizer state sharding) across multiple GPUs. If you're training with &gt; 1 GPU, this setting is a no-brainer and should basically always be on. On 1 GPU this setting is a no-op.</li>
<li><code>-c 0.1</code> sets the weight decay to 0.1. Only (2D) weights are decayed exactly as in GPT-2, and this number comes from the GPT-3 paper</li>
<li><code>-k "cosine"</code> sets the cosine learning rate schedule, which is the default so this is a bit spurious.</li>
<li><code>-l 0.0006</code> sets the maximum learning rate to 6e-4. The GPT-3 paper says to use 2e-4 for this model size, but here we triple and it and seems to train faster and without any issues. This wasn't tuned very carefully yet.</li>
<li><code>-q 0.1</code> says that we will decay the learning rate to 10% of max LR over the course of training, following GPT-3 paper.</li>
<li><code>-u 700</code> says that we will ramp up the learning rate from 0 to max learning rate over the first 700 iterations, which at total batch size 0.5M is 350M tokens, following GPT-3 paper.</li>
<li><code>-n 2000</code> asks to save model checkpoints every 2000 steps.</li>
<li><code>-x 32000</code> asks for 32K steps in total. I chose this number because it is a nice number, and just fits into 24 hours.</li>
<li><code>-ge 1</code> sets a very recently merged gelu recompute setting for CublasLt (optional)</li>
<li><code>-y 1</code> sets the "resume" flag on. If your training for any reason crashes or hangs, you can CTRL+C and re-run this command, and it will attempt to resume the optimization. llm.c is bitwise-deterministic, so you'll get the identical result as if you didn't crash.</li>
<li><code>-e "d48"</code> asks to initialize, a depth 48 GPT-2 model from scratch.</li>
</ul>
<p dir="auto"><strong>Memory guide.</strong> The biggest constraint most people will probably face is that their GPU doesn't have 80GB. That's okay you should still be able to run everything above if you are patient, it would just run slower. So if the model doesn't fit, what do you play with? The most important one is the micro batch size <code>-b</code>. Try to decrease it but keep it to nice numbers. So e.g. 16 -&gt; 8 -&gt; 4 -&gt; 2 -&gt; 1. From there, try to also play with the recompute setting <code>-r</code> which is 0 (fastest, a lot of memory), 1 (very slightly slower, but a huge memory saving), or 2 (slightly slower, smaller memory saving). The next thing you can do is disable master weights in fp32, which you can do with <code>-w 0</code> (1 is default). We won't maintain fp32 copy of params. Empirically in a few runs before this seems to be okay, likely due to our use of stochastic rounding. If even that doesn't fit (that's unlikely right?), you could try to decrease the maximum sequence length with <code>-t</code>, default is 1024 you can take it down to 512, 256, etc., but now you are making your model worse because you're decreasing its maximum attention span.</p>
<p dir="auto"><strong>Code.</strong> Certainly I feel biased but llm.c is quite beautiful:</p>
<ul dir="auto">
<li>It only requires basic CUDA dependencies to run.</li>
<li>It is a direct, minimal and readable implementation in C/CUDA. llm.c totals about 5,000 lines of C/CUDA code. We try to be mostly C, not C++ to keep it simple. Neural net training is just one while loop of the same, simple arithmetic operations (think +, -, *, /) on a single float array, it really shouldn't be that complicated.</li>
<li>It compiles and runs very quickly (few seconds), so you're doing more stepping and less waiting.</li>
<li>It allocates all of its GPU memory a single time at the start and from then on during training has an exactly constant memory footprint. So once you start stepping, you know you're good for the rest of the run and won't OOM.</li>
<li>It is bitwise deterministic.</li>
<li>It is efficient, at just below ~50% MFU.</li>
</ul>
<p dir="auto">The main entry point and the majority of the code is in the file <a href="https://github.com/karpathy/llm.c/blob/master/train_gpt2.cu">train_gpt2.cu</a>. It contains the GPT-2 model definition and the training loop in ~2,000 LOC, and it imports a bunch of helper files with various utilities and the individual layer implementations from the <code>llmc</code> directory. <code>cloc llmc</code> reports 23 files with 3170 LOC, and <code>cloc train_gpt2.cu</code> is 1353 LOC atm.</p>
<p dir="auto"><strong>Multi-node training</strong>. If you are part of the privileged GPU-rich upper class, llm.c supports multi-node training and the most GPUs I've seen someone train llm.c with is ~500 GPUs. This biggest run I've done personally so far is on Lambda's new 1-click cluster feature with 16XH100 GPUs in 2 nodes. The downsides of unemployment. The lambda team has put up <a href="https://github.com/LambdaLabsML/llm.c-1cc/tree/main">detailed instructions</a> on how you can train llm.c models on their 1-click clusters. E.g. with the 512-GPU H100 cluster for $2,300/hr, you might be able to train your GPT-2 in ~30 minutes. You'd have to increase the total batch size (e.g. to ~8M) and possibly tune the hyperparameters a little. I haven't tried but it probably works and would be very cool :)</p>
<p dir="auto"><strong>PyTorch comparison</strong>. A relatively comparable run in PyTorch would I think look something like this, using our parallel PyTorch implementation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="torchrun --standalone --nproc_per_node=8 train_gpt2.py \
    --input_bin &quot;dev/data/edu_fineweb100B/edu_fineweb_train_*.bin&quot; \
    --input_val_bin &quot;dev/data/edu_fineweb100B/edu_fineweb_val_*.bin&quot; \
    --write_tensors 0 \
    --model d48 \
    --batch_size 8 --sequence_length 1024 --total_batch_size 1048576 \
    --dtype bfloat16 \
    --compile 1 \
    --tensorcores 1 \
    --flash 1 \
    --num_iterations 32000 \
    --warmup_iters 700 \
    --weight_decay 0.1 \
    --overfit_single_batch 0 \
    --learning_rate 0.0006 \
    --zero_stage 1"><pre>torchrun --standalone --nproc_per_node=8 train_gpt2.py \
    --input_bin <span><span>"</span>dev/data/edu_fineweb100B/edu_fineweb_train_*.bin<span>"</span></span> \
    --input_val_bin <span><span>"</span>dev/data/edu_fineweb100B/edu_fineweb_val_*.bin<span>"</span></span> \
    --write_tensors 0 \
    --model d48 \
    --batch_size 8 --sequence_length 1024 --total_batch_size 1048576 \
    --dtype bfloat16 \
    --compile 1 \
    --tensorcores 1 \
    --flash 1 \
    --num_iterations 32000 \
    --warmup_iters 700 \
    --weight_decay 0.1 \
    --overfit_single_batch 0 \
    --learning_rate 0.0006 \
    --zero_stage 1</pre></div>
<p dir="auto">The PyTorch code is meant as a testing reference not an actual implementation, so the training loop is a little bit different in some places (e.g. the dataloader doesn't permute the shards, etc.), but this is still possibly useful as a point of reference. I also hacked the default vocab size to be 50257 -&gt; 50304 to get added efficiency, then the currently PyTorch nightly gives:</p>
<div data-snippet-clipboard-copy-content="step   16/32000 | train loss 8.903997 | norm 8.3474 | lr 1.37e-05 | (3381.88 ms | 310057 tok/s)
step   17/32000 | train loss 8.870140 | norm 3.7936 | lr 1.46e-05 | (3381.95 ms | 310051 tok/s)
step   18/32000 | train loss 8.875732 | norm 9.4993 | lr 1.54e-05 | (3393.09 ms | 309033 tok/s)
step   19/32000 | train loss 8.817432 | norm 2.8345 | lr 1.63e-05 | (3379.75 ms | 310253 tok/s)
step   20/32000 | train loss 8.798056 | norm 4.1234 | lr 1.71e-05 | (3386.53 ms | 309631 tok/s)
step   21/32000 | train loss 8.777574 | norm 2.8010 | lr 1.80e-05 | (3386.05 ms | 309675 tok/s)
..."><pre><code>step   16/32000 | train loss 8.903997 | norm 8.3474 | lr 1.37e-05 | (3381.88 ms | 310057 tok/s)
step   17/32000 | train loss 8.870140 | norm 3.7936 | lr 1.46e-05 | (3381.95 ms | 310051 tok/s)
step   18/32000 | train loss 8.875732 | norm 9.4993 | lr 1.54e-05 | (3393.09 ms | 309033 tok/s)
step   19/32000 | train loss 8.817432 | norm 2.8345 | lr 1.63e-05 | (3379.75 ms | 310253 tok/s)
step   20/32000 | train loss 8.798056 | norm 4.1234 | lr 1.71e-05 | (3386.53 ms | 309631 tok/s)
step   21/32000 | train loss 8.777574 | norm 2.8010 | lr 1.80e-05 | (3386.05 ms | 309675 tok/s)
...
</code></pre></div>
<p dir="auto">Now I wouldn't say I have full confidence that the PyTorch script is maximally tuned, but the following observations can be made. PyTorch seems to be taking a lot more memory (this run is ~80GB), while llm.c is at 57GB (29% improvement). Memory is important because it allows you to crank up the batch size (e.g. llm.c can go up to 24 microbatch here), which goes a bit faster. Second, we're seeing about 3386 vs. 2750ms per iteration, so llm.c is stepping ~19% faster. Some of the gains here have known origin, e.g. llm.c includes optimizations like the Fused classifier that kicks off the backward pass, which is something torch.compile does not do today afaik. But it's also possible that this script isn't fully maximally tuned, but in any case I'm showing the comparison in case 1) others would like to take a look, play with, compare, help tune and 2) to just say that llm.c is quite optimized and fast - in the specific case of GPT-2/3 training.</p>
<p dir="auto"><strong>The final model</strong>. A few links that may be helpful, for posterity:</p>
<ul dir="auto">
<li>The <a href="http://llmc.s3-us-west-2.amazonaws.com/gpt2_1558M/main.log" rel="nofollow">main.log</a> file.</li>
<li>The <a href="http://llmc.s3-us-west-2.amazonaws.com/gpt2_1558M/model_00032000.bin" rel="nofollow">model_00032000.bin</a> llm.c bin model file</li>
<li>The model converted to huggingface transformers GPT-2 model I uploaded here: <a href="https://huggingface.co/karpathy/gpt2_1558M_final2_hf" rel="nofollow">karpathy/gpt2_1558M_final2_hf</a>.</li>
</ul>
<p dir="auto"><strong>Model export</strong>. The model export can be done as follows, for example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python dev/eval/export_hf.py --input log_gpt2_128M/model_00032000.bin --output gpt2_1558M_export"><pre>python dev/eval/export_hf.py --input log_gpt2_128M/model_00032000.bin --output gpt2_1558M_export</pre></div>
<p dir="auto">This then lets you run the Eleuther eval harness, or run the huggingface sampling pipeline to get model samples:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# take model for spin
import torch

output = &quot;./gpt2_1558M_final2_hf&quot;

# set pytorch seeds
torch.manual_seed(42)
torch.cuda.manual_seed(42)

prompt = &quot;In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.&quot;
from transformers import AutoModelForCausalLM, AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(output)
model = AutoModelForCausalLM.from_pretrained(output, attn_implementation=&quot;flash_attention_2&quot;, torch_dtype=torch.bfloat16, device_map='cuda')
model.eval()
tokens = tokenizer.encode(prompt, return_tensors=&quot;pt&quot;)
tokens = tokens.to('cuda')

output = model.generate(tokens, max_new_tokens=500, pad_token_id=tokenizer.eos_token_id, do_sample=True, top_k=50, num_return_sequences=4)
samples = tokenizer.batch_decode(output)
for sample in samples:
    print('-'*30)
    print(sample)"><pre><span># take model for spin</span>
<span>import</span> <span>torch</span>

<span>output</span> <span>=</span> <span>"./gpt2_1558M_final2_hf"</span>

<span># set pytorch seeds</span>
<span>torch</span>.<span>manual_seed</span>(<span>42</span>)
<span>torch</span>.<span>cuda</span>.<span>manual_seed</span>(<span>42</span>)

<span>prompt</span> <span>=</span> <span>"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English."</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>
<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>output</span>)
<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>output</span>, <span>attn_implementation</span><span>=</span><span>"flash_attention_2"</span>, <span>torch_dtype</span><span>=</span><span>torch</span>.<span>bfloat16</span>, <span>device_map</span><span>=</span><span>'cuda'</span>)
<span>model</span>.<span>eval</span>()
<span>tokens</span> <span>=</span> <span>tokenizer</span>.<span>encode</span>(<span>prompt</span>, <span>return_tensors</span><span>=</span><span>"pt"</span>)
<span>tokens</span> <span>=</span> <span>tokens</span>.<span>to</span>(<span>'cuda'</span>)

<span>output</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>tokens</span>, <span>max_new_tokens</span><span>=</span><span>500</span>, <span>pad_token_id</span><span>=</span><span>tokenizer</span>.<span>eos_token_id</span>, <span>do_sample</span><span>=</span><span>True</span>, <span>top_k</span><span>=</span><span>50</span>, <span>num_return_sequences</span><span>=</span><span>4</span>)
<span>samples</span> <span>=</span> <span>tokenizer</span>.<span>batch_decode</span>(<span>output</span>)
<span>for</span> <span>sample</span> <span>in</span> <span>samples</span>:
    <span>print</span>(<span>'-'</span><span>*</span><span>30</span>)
    <span>print</span>(<span>sample</span>)</pre></div>
<p dir="auto">Also have a look at <a href="https://github.com/karpathy/llm.c/tree/master/dev/eval">dev/eval</a> for instructions on how to run the Eleuther Evaluation Harness, the evals from the HuggingFace Open LLM Leaderboard, etc.</p>
<p dir="auto"><strong>400B token run</strong>. I have also made the attempt to train GPT-2 for significantly longer than 33B tokens. In particular, I changed -x to 400,000 to train for 420B tokens (even more than GPT-3 model of this size, which was trained with 300B). This model run looked great until about step 330,000:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/241138/347626140-8708850a-c29e-427e-8e14-fb6ba7d7776a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MzAxMDUsIm5iZiI6MTcyMDcyOTgwNSwicGF0aCI6Ii8yNDExMzgvMzQ3NjI2MTQwLTg3MDg4NTBhLWMyOWUtNDI3ZS04ZTE0LWZiNmJhN2Q3Nzc2YS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzExJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcxMVQyMDMwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1hZTUwOTVmZTZjZjZhYWNmZTg1OTc3YzllYzVjODQzNzk1NWY4MzA4YWVkNjIzYmQ5MTllMDU1MGEwMGJkYmYzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.Bp_T57O0jNGvZvwEP2gCddZAPRmuyJVJ6m8m_VENnxw"><img width="1293" alt="image" src="https://private-user-images.githubusercontent.com/241138/347626140-8708850a-c29e-427e-8e14-fb6ba7d7776a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MzAxMDUsIm5iZiI6MTcyMDcyOTgwNSwicGF0aCI6Ii8yNDExMzgvMzQ3NjI2MTQwLTg3MDg4NTBhLWMyOWUtNDI3ZS04ZTE0LWZiNmJhN2Q3Nzc2YS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzExJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcxMVQyMDMwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1hZTUwOTVmZTZjZjZhYWNmZTg1OTc3YzllYzVjODQzNzk1NWY4MzA4YWVkNjIzYmQ5MTllMDU1MGEwMGJkYmYzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.Bp_T57O0jNGvZvwEP2gCddZAPRmuyJVJ6m8m_VENnxw"></a>
<p dir="auto">This model dramatically beats GPT-2 and GPT-3 of its size on HellaSwag (it gets up to ~61%), but sadly becomes unstable there on and explodes. There are more smaller spikes along the way but the code is configured to detect the more simple instantaneous instability and skips update (I used the flags <code>-sl 5.0 -sg 5.0</code>), which helps mitigate and defers issues. However, I think we're not yet being sufficiently careful with our initialization, activation ranges, and overall model training stability and there are deeper issues that gradually drift the model into instability, especially for larger models and over long training duration. To be continued. If you have ideas or recommendations for stabilizing LLM model training please contribute your experience in the discussion below.</p>
<p dir="auto"><strong>FAQ</strong>:</p>
<ul dir="auto">
<li>Can I <strong>sample</strong> from the model in llm.c? kind of, but it's inefficient and a bit weird, and even more hacky if you'd like to prompt the model. Use the huggingface paths above for now.</li>
<li>Can I <strong>chat</strong> with it? no, this is currently only pretraining, not chat finetuning.</li>
<li>Can you train in <strong>fp8</strong>? No, we're currently mostly training in bf16, but early versions are very much work in progress.</li>
<li>I have a non-NVIDIA GPU can I run llm.c? No, llm.c supports C/CUDA only, but good forks exist (see main README). For example there is an actively maintained <a href="https://github.com/anthonix/llm.c">AMD fork</a> by <a data-hovercard-type="user" data-hovercard-url="/users/anthonix/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/anthonix">@anthonix</a> that is quite good.</li>
</ul>
<p dir="auto"><strong>GPT-2 (124M)</strong>. I wanted to also link to an earlier post on training the <a href="https://github.com/karpathy/llm.c/discussions/481" data-hovercard-type="discussion" data-hovercard-url="/karpathy/llm.c/discussions/481/hovercard">GPT-2 (124M) model</a> in llm.c, which has some more related information to llm.c runs. 124M is a smaller model in the GPT-2 miniseries, only 124M parameters compared to 1558M parameters.</p>
<p dir="auto"><strong>Authors</strong></p>
<p dir="auto">Substantial contributions to llm.c came from what now feels like the llm.c core dev team, in addition to self:</p>
<ul dir="auto">
<li><a data-hovercard-type="user" data-hovercard-url="/users/ngc92/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/ngc92">@ngc92</a> in all aspects of the code base</li>
<li><a data-hovercard-type="user" data-hovercard-url="/users/ademeure/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/ademeure">@ademeure</a> in CUDA kernel optimization, low precision training, cudnn, cublas, ...</li>
<li><a data-hovercard-type="user" data-hovercard-url="/users/gordicaleksa/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/gordicaleksa">@gordicaleksa</a> in all aspects of whatever is next on the TODO list, from algorithms to code to multi-node or etc.</li>
<li><a data-hovercard-type="user" data-hovercard-url="/users/rosslwheeler/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/rosslwheeler">@rosslwheeler</a> in CI and Windows support. If you're happily running llm.c on Windows you should definitely thank Ross :)</li>
<li><a href="https://lambdalabs.com/" rel="nofollow">Lambda labs</a> for sponsoring the GPUs used in the development of llm.c. The history here is that I've happily used Lambda for several years and then a few months ago I pretty please asked if they are open to not charging my account for llm.c dev work and they agreed so here we are thank you for supporting llm.c!</li>
</ul>
<p dir="auto"><strong>Coming up</strong>. Some of the next big steps we are interested in and looking at these days:</p>
<ol dir="auto">
<li>Further optimize GPT-2 training hyperparameters. For some reason, the hyperparameters cited by OpenAI in the GPT-3 paper appear to be quite suboptimal, e.g. @Yuchenj_UW on X found that you can 3X the learning rate and get faster training with no apparent downsides. There might be other similar low-hanging fruit.</li>
<li>Improve training and scaling stability, e.g. more stable optimizers, schedulers, clipping, norming, muP. (Some of these PRs already exist, if you have tips on stabilizing LLM runs please reach out with ideas to try!).</li>
<li>Mixed precision++: training with fp8 (imminent!).</li>
<li>Model inference, e.g. KV cache is the low hanging fruit here.</li>
<li>Finetuning: SFT, RLHF</li>
<li>Multimodal extensions, VQVAE and friends</li>
<li>More modern architectures, support for Llama / Gemma model series.</li>
</ol>
<p dir="auto">The goal of llm.c remains to have a simple, minimal, clean training stack for a full-featured LLM agent, in direct C/CUDA, and companion educational materials to bring many people up to speed in this awesome field.</p>
<p dir="auto">Please feel free to use the Discussions for any FAQ and related, or if you'd like something faster, #llmc on <a href="https://discord.gg/3zy8kqD9Cp" rel="nofollow">Discord</a>, or #llmdotc on CUDA MODE Discord.</p>
<p dir="auto">We'll see you next time!</p>
    </td>
  </tr>

    </tbody>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gene-silencing tool shows promise as a future therapy against prion diseases (181 pts)]]></title>
            <link>https://news.mit.edu/2024/charmed-collaboration-creates-therapy-candidate-fatal-prion-diseases-0627</link>
            <guid>40939703</guid>
            <pubDate>Thu, 11 Jul 2024 19:21:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.mit.edu/2024/charmed-collaboration-creates-therapy-candidate-fatal-prion-diseases-0627">https://news.mit.edu/2024/charmed-collaboration-creates-therapy-candidate-fatal-prion-diseases-0627</a>, See on <a href="https://news.ycombinator.com/item?id=40939703">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
          

            <p>Drug development is typically slow: The pipeline from basic research discoveries that provide the basis for a new drug to clinical trials and then production of a widely available medicine can take decades. But decades can feel impossibly far off to someone who currently has a fatal disease. Broad Institute of MIT and Harvard Senior Group Leader Sonia Vallabh is acutely aware of that race against time, because the topic of her research is a neurodegenerative and ultimately fatal disease — fatal familial insomnia, a type of prion disease — that she will almost certainly develop as she ages.&nbsp;</p><p>Vallabh and her husband, Eric Minikel, switched careers and became researchers after they learned that Vallabh carries a disease-causing version of the prion protein gene and that there is no effective therapy for fatal prion diseases. The two now run a lab at the Broad Institute, where they are working to develop drugs that can prevent and treat these diseases, and their deadline for success is not based on grant cycles or academic expectations but on the ticking time bomb in Vallabh’s genetic code.</p><p>That is why Vallabh was excited to discover, when she entered into a collaboration with Whitehead Institute for Biomedical Research member Jonathan Weissman, that Weissman’s group likes to work at full throttle. In less than two years, Weissman, Vallabh, and their collaborators have developed a set of molecular tools called CHARMs that can turn off disease-causing genes such as the prion protein gene — as well as, potentially, genes coding for many other proteins implicated in neurodegenerative and other diseases — and they are refining those tools to be good candidates for use in human patients. Although the tools still have many hurdles to pass before the researchers will know if they work as therapeutics, the team is encouraged by the speed with which they have developed the technology thus far.</p><p>“The spirit of the collaboration since the beginning has been that there was no waiting on formality,” Vallabh says. “As soon as we realized our mutual excitement to do this, everything was off to the races.”</p><p>Co-corresponding authors Weissman and Vallabh and co-first authors Edwin Neumann, a graduate student in Weissman’s lab, and Tessa Bertozzi, a postdoc in Weissman’s lab, describe CHARM — which stands for Coupled Histone tail for Autoinhibition Release of Methyltransferase — in a <a href="https://doi.org/10.1126/science.ado7082">paper</a> published today in the journal <em>Science</em>.</p><p>“With the Whitehead and Broad Institutes right next door to each other, I don’t think there’s any better place than this for a group of motivated people to move quickly and flexibly in the pursuit of academic science and medical technology,” says Weissman, who is also a professor of biology at MIT and a Howard Hughes Medical Institute Investigator. “CHARMs are an elegant solution to the problem of silencing disease genes, and they have the potential to have an important position in the future of genetic medicines.”</p><p><strong>To treat a genetic disease, target the gene</strong></p><p>Prion disease, which leads to swift neurodegeneration and death, is caused by the presence of misshapen versions of the prion protein. These cause a cascade effect in the brain: the faulty prion proteins deform other proteins, and together these proteins not only stop functioning properly but also form toxic aggregates that kill neurons. The most famous type of prion disease, known colloquially as mad cow disease, is infectious, but other forms of prion disease can occur spontaneously or be caused by faulty prion protein genes.</p><p>Most conventional drugs work by targeting a protein. CHARMs, however, work further upstream, turning off the gene that codes for the faulty protein so that the protein never gets made in the first place. CHARMs do this by epigenetic editing, in which a chemical tag gets added to DNA in order to turn off or silence a target gene. Unlike gene editing, epigenetic editing does not modify the underlying DNA — the gene itself remains intact. However, like gene editing, epigenetic editing is stable, meaning that a gene switched off by CHARM should remain off. This would mean patients would only have to take CHARM once, as opposed to protein-targeting medications that must be taken regularly as the cells’ protein levels replenish.</p><p>Research in animals suggests that the prion protein isn’t necessary in a healthy adult, and that in cases of disease, removing the protein improves or even eliminates disease symptoms. In a person who hasn’t yet developed symptoms, removing the protein should prevent disease altogether. In other words, epigenetic editing could be an effective approach for treating genetic diseases such as inherited prion diseases. The challenge is creating a new type of therapy.</p><p>Fortunately, the team had a good template for CHARM: a research tool called CRISPRoff that Weissman’s group previously developed for silencing genes. CRISPRoff uses building blocks from CRISPR gene editing technology, including the guide protein Cas9 that directs the tool to the target gene. CRISPRoff silences the targeted gene by adding methyl groups, chemical tags that prevent the gene from being transcribed, or read into RNA, and so from being expressed as protein. When the researchers tested CRISPRoff’s ability to silence the prion protein gene, they found that it was effective and stable.</p><p>Several of its properties, though, prevented CRISPRoff from being a good candidate for a therapy. The researchers’ goal was to create a tool based on CRISPRoff that was just as potent but also safe for use in humans, small enough to deliver to the brain, and designed to minimize the risk of silencing the wrong genes or causing side effects.</p><p><strong>From research tool to drug candidate</strong></p><p>Led by Neumann and Bertozzi, the researchers began engineering and applying their new epigenome editor. The first problem that they had to tackle was size, because the editor needs to be small enough to be packaged and delivered to specific cells in the body. Delivering genes into the human brain is challenging; many clinical trials have used adeno-associated viruses (AAVs) as gene-delivery vehicles, but these are small and can only contain a small amount of genetic code. CRISPRoff is way too big; the code for Cas9 alone takes up most of the available space.</p><p>The Weissman lab researchers decided to replace Cas9 with a much smaller zinc finger protein (ZFP). Like Cas9, ZFPs can serve as guide proteins to direct the tool to a target site in DNA. ZFPs are also common in human cells, meaning they are less likely to trigger an immune response against themselves than the bacterial Cas9.</p><p>Next, the researchers had to design the part of the tool that would silence the prion protein gene. At first, they used part of a methyltransferase, a molecule that adds methyl groups to DNA, called DNMT3A. However, in the particular configuration needed for the tool, the molecule was toxic to the cell. The researchers focused on a different solution: Instead of delivering outside DNMT3A as part of the therapy, the tool is able to recruit the cell’s own DNMT3A to the prion protein gene. This freed up precious space inside of the AAV vector and prevented toxicity.</p><p>The researchers also needed to activate DNMT3A. In the cell, DNMT3A is usually inactive until it interacts with certain partner molecules. This default inactivity prevents accidental methylation of genes that need to remain turned on. Neumann came up with an ingenious way around this by combining sections of DNMT3A’s partner molecules and connecting these to ZFPs that bring them to the prion protein gene. When the cell’s DNMT3A comes across this combination of parts, it activates, silencing the gene.</p><p>“From the perspectives of both toxicity and size, it made sense to recruit the machinery that the cell already has; it was a much simpler, more elegant solution,” Neumann says. “Cells are already using methyltransferases all of the time, and we’re essentially just tricking them into turning off a gene that they would normally leave turned on.”</p><p>Testing in mice showed that ZFP-guided CHARMs could eliminate more than 80 percent of the prion protein in the brain, while previous research has shown that as little as 21 percent elimination can improve symptoms.</p><p>Once the researchers knew that they had a potent gene silencer, they turned to the problem of off-target effects. The genetic code for a CHARM that gets delivered to a cell will keep producing copies of the CHARM indefinitely. However, after the prion protein gene is switched off, there is no benefit to this, only more time for side effects to develop, so they tweaked the tool so that after it turns off the prion protein gene, it then turns itself off.</p><p>Meanwhile, a complementary project from Broad Institute scientist and collaborator Benjamin Deverman’s lab, focused on brain-wide gene delivery and <a href="https://www.science.org/doi/10.1126/science.adm8386">published</a> in <em>Science</em> on May 17, has brought the CHARM technology one step closer to being ready for clinical trials. Although naturally occurring types of AAV have been used for gene therapy in humans before, they do not enter the adult brain efficiently, making it impossible to treat a whole-brain disease like prion disease. Tackling the delivery problem, Deverman’s group has designed an AAV vector that can get into the brain more efficiently by leveraging a pathway that naturally shuttles iron into the brain. Engineered vectors like this one make a therapy like CHARM one step closer to reality.</p><p>Thanks to these creative solutions, the researchers now have a highly effective epigenetic editor that is small enough to deliver to the brain, and that appears in cell culture and animal testing to have low toxicity and limited off-target effects.</p><p>“It’s been a privilege to be part of this; it’s pretty rare to go from basic research to therapeutic application in such a short amount of time,” Bertozzi says. “I think the key was forming a collaboration that took advantage of the Weissman lab’s tool-building experience, the Vallabh and Minikel lab’s deep knowledge of the disease, and the Deverman lab’s expertise in gene delivery.”</p><p><strong>Looking ahead</strong></p><p>With the major elements of the CHARM technology solved, the team is now fine-tuning their tool to make it more effective, safer, and easier to produce at scale, as will be necessary for clinical trials. They have already made the tool modular, so that its various pieces can be swapped out and future CHARMs won’t have to be programmed from scratch. CHARMs are also currently being tested as therapeutics in mice.&nbsp;</p><p>The path from basic research to clinical trials is a long and winding one, and the researchers know that CHARMs still have a way to go before they might become a viable medical option for people with prion diseases, including Vallabh, or other diseases with similar genetic components. However, with a strong therapy design and promising laboratory results in hand, the researchers have good reason to be hopeful. They continue to work at full throttle, intent on developing their technology so that it can save patients’ lives not someday, but as soon as possible.</p>        

      </div><div>

    


            
          

            
      

                          <div>
  
  
  

      <header>
      <h2>Press Mentions</h2>
    </header>
  
  
  

  <div><h3>USA Today</h3><p>Sonia Vallabh and Eric Minikel, senior group leaders from the Broad Institute have created a gene-editing tool to combat prion diseases, reports Karen Weintraub for <em>USA Today</em>.&nbsp;The approach “should also work against diseases such as Huntington's, Parkinson's, ALS and even Alzheimer's, which result from the accumulation of toxic proteins,” Weintraub writes.</p></div>


    

  
  

  
  
</div>
           

                <div>
      <h2>Related Links</h2>
      <div><ul><li><a href="https://www.broadinstitute.org/bios/sonia-vallabh" target="_blank">Sonia Vallabh</a></li><li><a href="https://wi.mit.edu/people/member/weissman" target="_blank">Jonathan Weissman</a></li><li><a href="https://wi.mit.edu/" target="_blank">Whitehead Institute for Biomedical Research</a></li><li><a href="https://www.broadinstitute.org/" target="_blank">The Broad Institute of MIT and Harvard</a></li><li><a href="https://biology.mit.edu/" target="_blank">Department of Biology</a></li><li><a href="https://science.mit.edu/" target="_blank">School of Science</a></li></ul></div>

    </div>
      
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Vision Pro U.S. Sales Are All but Dead, Market Analysts Say (121 pts)]]></title>
            <link>https://gizmodo.com/apple-vision-pro-u-s-sales-2000469302</link>
            <guid>40939627</guid>
            <pubDate>Thu, 11 Jul 2024 19:12:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/apple-vision-pro-u-s-sales-2000469302">https://gizmodo.com/apple-vision-pro-u-s-sales-2000469302</a>, See on <a href="https://news.ycombinator.com/item?id=40939627">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                            <p><span>Those still holding on to their </span><a href="https://gizmodo.com/apple-vision-pro-1851249913"><span>Apple Vision Pros</span></a><span> may remain in a </span><a href="https://gizmodo.com/apple-vision-pro-already-forgotten-about-1851426564"><span>rather exclusive club</span></a><span> throughout this year. Market research shows that sales for Apple’s first big, expensive headset will remain low in 2024. The latest reports from those keeping tabs on the Cupertino, California company say AVP will have dropped off 75% by the end of August. The true test for Apple’s spatial dreams may rest on the rumored (slightly) </span><a href="https://gizmodo.com/apple-vision-pro-stopped-production-ar-headset-1851547035"><span>cheaper headset</span></a><span>.</span></p>

 <p><span>The market analyst firm IDC told </span><a href="https://www.bloomberg.com/news/articles/2024-07-11/apple-s-vision-pro-won-t-cross-500-000-sales-this-year-idc-says"><span>Bloomberg</span></a><span> the Apple Vision Pro has yet to sell 100,000 units. It’s an expensive headset, and Apple wasn’t expecting it to sell like an iPhone. Supply chain analysts have reported that <a href="https://gizmodo.com/apple-vision-pro-reality-check-1851429764">Apple cut its sales expectations for its $3,500 “spatial computer” </a></span><span>in April. But this latest report shows that sales will have dropped off a cliff in the U.S. in the third quarter of this year and will continue to slacken through the holidays.</span></p> <p><span>Last month, Apple launched the Vision Pro in international markets, including Europe, the U.K., China, Japan, and Singapore. IDC expects the AVP’s interest in those markets to keep the headset’s sales afloat until next year. The real pick-me-up for Apple’s spatial dreams would be a new, less expensive headset. Those in the know have hinted Apple is working on a “budget” Vision device slated for the latter half of 2025.</span></p>

 <p><span>Even if the <a href="https://gizmodo.com/cheaper-weaker-apple-vision-pro-1851556515">next Vision device</a> costs half the Pro model, it will still cost $1,750 and one of the most expensive consumer-end VR/AR headsets you can buy. Rumors hint that the next device could remove the pointless exterior display to save on manufacturing costs. It could also reduce the FOV and use a less-capable chip than the current M2. Bloomberg hinted that Apple was even considering tethering it to an iPhone or Mac for daily use, which would drastically reduce its portability.</span></p> <p><span>We don’t have pure statistics on how many folks returned their Vision Pro after buying it during the initial hype rush, but analysts have noted that </span><a href="https://gizmodo.com/apple-vision-pro-returns-users-didnt-know-set-it-up-1851293527"><span>many who bought one were confused by</span></a><span> its more complicated setup and what they were supposed to use it for in their daily lives.</span></p>

 <p><span>Sales expectations will put even more pressure on Apple engineers to design something that can compete with devices like the $500 </span><a href="https://gizmodo.com/meta-android-of-vr-1851521814"><span>Meta Quest 3</span></a><span> while justifying the higher price tag. Fans of the Cupertino company are already used to paying an “Apple tax” on their products, but not when the cost is literally thousands of dollars more.&nbsp;</span></p> <p><span>Apple is </span><a href="https://gizmodo.com/everything-announced-at-wwdc-2024-ios-apple-ai-1851529902"><span>working on a visionOS update</span></a><span> to improve the faux-3D spatial photos, add a few new gesture controls, and allow for a panoramic Mac screen mirroring. The latest version of visionOS won’t have a public beta, so we’ll have to wait and see if the changes will give the few on-the-fence customers a reason to pick up the ultra-expensive headset.</span></p>            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Capturing Linux SSL/TLS plaintext without a CA certificate using eBPF (156 pts)]]></title>
            <link>https://github.com/gojue/ecapture</link>
            <guid>40938810</guid>
            <pubDate>Thu, 11 Jul 2024 17:31:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/gojue/ecapture">https://github.com/gojue/ecapture</a>, See on <a href="https://news.ycombinator.com/item?id=40938810">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/gojue/ecapture/blob/master/images/ecapture-logo-400x400.png"><img src="https://github.com/gojue/ecapture/raw/master/images/ecapture-logo-400x400.png" alt=""></a></p>
<p dir="auto"><a href="https://github.com/gojue/ecapture/blob/master/README_CN.md">中文介绍</a> | English | <a href="https://github.com/gojue/ecapture/blob/master/README_JA.md">日本語</a></p>
<p dir="auto"><a href="https://github.com/gojue/ecapture"><img src="https://camo.githubusercontent.com/270192c3bdf7383aaf60e58e1257325923ad8a10e93ce348b011438b31e0e7b9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6a75652f65636170747572652e7376673f6c6162656c3d5374617273266c6f676f3d676974687562" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/gojue/ecapture.svg?label=Stars&amp;logo=github"></a>
<a href="https://github.com/gojue/ecapture"><img src="https://camo.githubusercontent.com/2a1ba577b9b9d6298b29c621597f4adf0705ef187c20677eb75d246494aa1cd5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f676f6a75652f65636170747572653f6c6162656c3d466f726b73266c6f676f3d676974687562" alt="GitHub forks" data-canonical-src="https://img.shields.io/github/forks/gojue/ecapture?label=Forks&amp;logo=github"></a>
<a href="https://github.com/gojue/ecapture/actions/workflows/code-analysis.yml"><img src="https://github.com/gojue/ecapture/actions/workflows/codeql-analysis.yml/badge.svg" alt="CI"></a>
<a href="https://github.com/gojue/ecapture/releases"><img src="https://camo.githubusercontent.com/9441198ce3414f84ecb7fca20a61c221fe44be49f1b2b1acff425972a88a2486/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f676f6a75652f65636170747572653f646973706c61795f6e616d653d74616726696e636c7564655f70726572656c656173657326736f72743d73656d766572" alt="Github Version" data-canonical-src="https://img.shields.io/github/v/release/gojue/ecapture?display_name=tag&amp;include_prereleases&amp;sort=semver"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">eCapture(旁观者): capture SSL/TLS text content without a CA certificate using eBPF.</h3><a id="user-content-ecapture旁观者-capture-ssltls-text-content-without-a-ca-certificate-using-ebpf" aria-label="Permalink: eCapture(旁观者): capture SSL/TLS text content without a CA certificate using eBPF." href="#ecapture旁观者-capture-ssltls-text-content-without-a-ca-certificate-using-ebpf"></a></p>
<blockquote>
<p dir="auto"><strong>Note</strong></p>
<p dir="auto">Supports Linux/Android kernel versions x86_64 4.18 and above, <strong>aarch64 5.5</strong> and above.
Does not support Windows and macOS system.</p>
</blockquote>
<hr>

<ul dir="auto">
<li><a href="#how-ecapture-works">How eCapture works</a></li>
<li><a href="#ecapture-user-manual">eCapture User Manual</a></li>
<li><a href="#getting-started">Getting started</a></li>
<li><a href="#ecapture-architecture">eCapture Architecture</a></li>
<li><a href="#whats-ebpf">What's eBPF</a></li>
<li><a href="#how-to-compile">How to compile</a></li>
<li><a href="#contributing">Contributing</a></li>
</ul>

<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">How eCapture works</h2><a id="user-content-how-ecapture-works" aria-label="Permalink: How eCapture works" href="#how-ecapture-works"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/gojue/ecapture/blob/master/images/how-ecapture-works.png"><img src="https://github.com/gojue/ecapture/raw/master/images/how-ecapture-works.png" alt=""></a></p>
<ul dir="auto">
<li>SSL/TLS plaintext capture, support openssl\libressl\boringssl\gnutls\nspr(nss) libraries.</li>
<li>GoTLS plaintext support go tls library, which refers to encrypted communication in https/tls programs written in the golang language.</li>
<li>bash audit, capture bash command for Host Security Audit.</li>
<li>mysql query SQL audit, support mysqld 5.6\5.7\8.0, and mariadDB.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">eCapture User Manual</h2><a id="user-content-ecapture-user-manual" aria-label="Permalink: eCapture User Manual" href="#ecapture-user-manual"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/gojue/ecapture/blob/master/images/ecapture-help-v0.7.4.png"><img src="https://github.com/gojue/ecapture/raw/master/images/ecapture-help-v0.7.4.png" alt=""></a></p>
<p dir="auto">Youtube video: <a href="https://www.youtube.com/watch?v=CoDIjEQCvvA" title="eCapture User Manual" rel="nofollow">How to use eCapture v0.1.0</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">use ELF binary file</h2><a id="user-content-use-elf-binary-file" aria-label="Permalink: use ELF binary file" href="#use-elf-binary-file"></a></p>
<p dir="auto">Download ELF zip file <a href="https://github.com/gojue/ecapture/releases">release</a> , unzip and use by
command <code>./ecapture --help</code>.</p>
<ul dir="auto">
<li>Linux kernel version &gt;= 4.18 is required.</li>
<li>Enable BTF <a href="https://www.kernel.org/doc/html/latest/bpf/btf.html" rel="nofollow">BPF Type Format (BTF)</a>  (Optional, 2022-04-17)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">use docker image</h2><a id="user-content-use-docker-image" aria-label="Permalink: use docker image" href="#use-docker-image"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# pull docker image
docker pull gojue/ecapture:latest
# run
docker run --rm --privileged=true --net=host -v ${HOST_PATH}:${CONTAINER_PATH} gojue/ecapture ARGS"><pre><span><span>#</span> pull docker image</span>
docker pull gojue/ecapture:latest
<span><span>#</span> run</span>
docker run --rm --privileged=true --net=host -v <span>${HOST_PATH}</span>:<span>${CONTAINER_PATH}</span> gojue/ecapture ARGS</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Command line options</h2><a id="user-content-command-line-options" aria-label="Permalink: Command line options" href="#command-line-options"></a></p>
<blockquote>
<p dir="auto"><strong>Note</strong></p>
<p dir="auto">Need ROOT permission.</p>
</blockquote>
<p dir="auto">eCapture search <code>/etc/ld.so.conf</code> file default, to search load directories of  <code>SO</code> file, and search <code>openssl</code> shard
libraries location. or you can use <code>--libssl</code>
flag to set shard library path.</p>
<p dir="auto">If target program is compile statically, you can set program path as <code>--libssl</code> flag value directly。</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Modules</h2><a id="user-content-modules" aria-label="Permalink: Modules" href="#modules"></a></p>
<p dir="auto">The eCapture tool comprises 8 modules that respectively support plaintext capture for TLS/SSL encryption libraries like OpenSSL, GnuTLS, NSPR, BoringSSL, and GoTLS. Additionally, it facilitates software audits for Bash, MySQL, and PostgreSQL applications.</p>
<ul dir="auto">
<li>bash		capture bash command</li>
<li>gnutls	capture gnutls text content without CA cert for gnutls libraries.</li>
<li>gotls		Capturing plaintext communication from Golang programs encrypted with TLS/HTTPS.</li>
<li>mysqld	capture sql queries from mysqld 5.6/5.7/8.0 .</li>
<li>nss		capture nss/nspr encrypted text content without CA cert for nss/nspr libraries.</li>
<li>postgres	capture sql queries from postgres 10+.</li>
<li>tls		use to capture tls/ssl text content without CA cert. (Support openssl 1.0.x/1.1.x/3.0.x or newer).
You can use <code>ecapture -h</code> to view the list of subcommands.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">OpenSSL Module</h2><a id="user-content-openssl-module" aria-label="Permalink: OpenSSL Module" href="#openssl-module"></a></p>
<p dir="auto">The OpenSSL module supports three capture modes:</p>
<ul dir="auto">
<li><code>pcap</code>/<code>pcapng</code> mode stores captured plaintext data in pcap-NG format.</li>
<li><code>keylog</code>/<code>key</code> mode saves the TLS handshake keys to a file.</li>
<li><code>text</code> mode directly captures plaintext data, either outputting to a specified file or printing to the command line.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pcap Mode</h3><a id="user-content-pcap-mode" aria-label="Permalink: Pcap Mode" href="#pcap-mode"></a></p>
<p dir="auto">You can specify <code>-m pcap</code> or <code>-m pcapng</code> and use it in conjunction with <code>--pcapfile</code> and <code>-i</code> parameters. The default value for <code>--pcapfile</code> is <code>ecapture_openssl.pcapng</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="./ecapture tls -m pcap -i eth0 --pcapfile=ecapture.pcapng tcp port 443"><pre>./ecapture tls -m pcap -i eth0 --pcapfile=ecapture.pcapng tcp port 443</pre></div>
<p dir="auto">This command saves captured plaintext data packets as a pcapng file, which can be viewed using <code>Wireshark</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Keylog Mode</h3><a id="user-content-keylog-mode" aria-label="Permalink: Keylog Mode" href="#keylog-mode"></a></p>
<p dir="auto">You can specify <code>-m keylog</code> or <code>-m key</code> and use it in conjunction with the <code>--keylogfile</code> parameter, which defaults to <code>ecapture_masterkey.log</code>.</p>
<p dir="auto">The captured OpenSSL TLS <code>Master Secret</code> information is saved to <code>--keylogfile</code>. You can also enable <code>tcpdump</code> packet capture and then use <code>Wireshark</code> to open the file and set the <code>Master Secret</code> path to view plaintext data packets.</p>
<div dir="auto" data-snippet-clipboard-copy-content="./ecapture tls -m keylog -keylogfile=openssl_keylog.log"><pre>./ecapture tls -m keylog -keylogfile=openssl_keylog.log</pre></div>
<p dir="auto">You can also directly use the <code>tshark</code> software for real-time decryption and display:</p>
<div dir="auto" data-snippet-clipboard-copy-content="tshark -o tls.keylog_file:ecapture_masterkey.log -Y http -T fields -e http.file_data -f &quot;port 443&quot; -i eth0"><pre>tshark -o tls.keylog_file:ecapture_masterkey.log -Y http -T fields -e http.file_data -f <span><span>"</span>port 443<span>"</span></span> -i eth0</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Text Mode</h3><a id="user-content-text-mode" aria-label="Permalink: Text Mode" href="#text-mode"></a></p>
<p dir="auto"><code>./ecapture tls -m text</code> will output all plaintext data packets. (Starting from v0.7.0, it no longer captures SSLKEYLOG information.)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">GoTLS Module</h2><a id="user-content-gotls-module" aria-label="Permalink: GoTLS Module" href="#gotls-module"></a></p>
<p dir="auto">Similar to the OpenSSL module.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">check your server BTF config：</h3><a id="user-content-check-your-server-btf-config" aria-label="Permalink: check your server BTF config：" href="#check-your-server-btf-config"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="cfc4n@vm-server:~$# uname -r
4.18.0-305.3.1.el8.x86_64
cfc4n@vm-server:~$# cat /boot/config-`uname -r` | grep CONFIG_DEBUG_INFO_BTF
CONFIG_DEBUG_INFO_BTF=y"><pre>cfc4n@vm-server:<span>~</span><span>$#</span> uname -r
4.18.0-305.3.1.el8.x86_64
cfc4n@vm-server:<span>~</span><span>$#</span> cat /boot/config-<span><span>`</span>uname -r<span>`</span></span> <span>|</span> grep CONFIG_DEBUG_INFO_BTF
CONFIG_DEBUG_INFO_BTF=y</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">gotls command</h3><a id="user-content-gotls-command" aria-label="Permalink: gotls command" href="#gotls-command"></a></p>
<p dir="auto">capture tls text context.</p>
<p dir="auto">Step 1:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./ecapture gotls --elfpath=/home/cfc4n/go_https_client --hex"><pre>./ecapture gotls --elfpath=/home/cfc4n/go_https_client --hex</pre></div>
<p dir="auto">Step 2:</p>
<div dir="auto" data-snippet-clipboard-copy-content="/home/cfc4n/go_https_client"><pre>/home/cfc4n/go_https_client</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">more help</h3><a id="user-content-more-help" aria-label="Permalink: more help" href="#more-help"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">bash Module</h2><a id="user-content-bash-module" aria-label="Permalink: bash Module" href="#bash-module"></a></p>
<p dir="auto">capture bash command : <code>ecapture bash</code></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">eCapture Architecture</h2><a id="user-content-ecapture-architecture" aria-label="Permalink: eCapture Architecture" href="#ecapture-architecture"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/gojue/ecapture/blob/master/images/ecapture-architecture.png"><img src="https://github.com/gojue/ecapture/raw/master/images/ecapture-architecture.png" alt=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What's eBPF</h2><a id="user-content-whats-ebpf" aria-label="Permalink: What's eBPF" href="#whats-ebpf"></a></p>
<p dir="auto"><a href="https://ebpf.io/" rel="nofollow">eBPF</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to compile</h2><a id="user-content-how-to-compile" aria-label="Permalink: How to compile" href="#how-to-compile"></a></p>
<p dir="auto">Linux Kernel: &gt;= 4.18.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tools</h2><a id="user-content-tools" aria-label="Permalink: Tools" href="#tools"></a></p>
<ul dir="auto">
<li>golang 1.21 or newer</li>
<li>clang 9.0 or newer</li>
<li>cmake 3.18.4 or newer</li>
<li>clang backend: llvm 9.0 or newer</li>
<li>kernel config:CONFIG_DEBUG_INFO_BTF=y (Optional, 2022-04-17)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">command</h2><a id="user-content-command" aria-label="Permalink: command" href="#command"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">ubuntu</h3><a id="user-content-ubuntu" aria-label="Permalink: ubuntu" href="#ubuntu"></a></p>
<p dir="auto">If you are using Ubuntu 20.04 or later versions, you can use a single command to complete the initialization of the compilation environment.</p>
<div dir="auto" data-snippet-clipboard-copy-content="/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/gojue/ecapture/master/builder/init_env.sh)&quot;"><pre>/bin/bash -c <span><span>"</span><span><span>$(</span>curl -fsSL https://raw.githubusercontent.com/gojue/ecapture/master/builder/init_env.sh<span>)</span></span><span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">other Linux</h3><a id="user-content-other-linux" aria-label="Permalink: other Linux" href="#other-linux"></a></p>
<p dir="auto">In addition to the software listed in the 'Toolchain Version' section above, the following software is also required for the compilation environment. Please install it yourself.</p>
<ul dir="auto">
<li>linux-tools-common</li>
<li>linux-tools-generic</li>
<li>pkgconf</li>
<li>libelf-dev</li>
</ul>
<p dir="auto"><strong>Clone the repository code and compile it</strong></p>
<p dir="auto">Caution: The following <code>make</code> command will install libpcap into the system
directory if <code>libpcap.a</code> does not exist under <code>/usr/local/lib</code>. If you have
installed libpcap in system without <code>libpcap.a</code>, it maybe break your libpcap's
headers.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone --recurse-submodules git@github.com:gojue/ecapture.git
cd ecapture
make
bin/ecapture"><pre>git clone --recurse-submodules git@github.com:gojue/ecapture.git
<span>cd</span> ecapture
make
bin/ecapture</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">compile without BTF</h2><a id="user-content-compile-without-btf" aria-label="Permalink: compile without BTF" href="#compile-without-btf"></a></p>
<p dir="auto">eCapture support BTF disabled with command <code>make nocore</code> to compile at 2022/04/17. It can work normally even on Linux systems that do not support BTF.</p>
<div dir="auto" data-snippet-clipboard-copy-content="make nocore
bin/ecapture --help"><pre>make nocore
bin/ecapture --help</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">cross-compilation</h2><a id="user-content-cross-compilation" aria-label="Permalink: cross-compilation" href="#cross-compilation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Kernel header files</h3><a id="user-content-kernel-header-files" aria-label="Permalink: Kernel header files" href="#kernel-header-files"></a></p>
<p dir="auto">To cross-compile the eCapture tool, you need to install the kernel header files for the target architecture. you need to install the <code>linux-source</code> package.</p>
<div dir="auto" data-snippet-clipboard-copy-content="kernel_ver=`uname -r | cut -d'-' -f 1`
sudo apt-get install -y linux-source-$kernel_ver
cd /usr/src
sudo tar -xf linux-source-${kernel_ver}.tar.bz2
cd /usr/src/linux-source-${kernel_ver}
test -f .config || yes &quot;&quot; | sudo make oldconfig"><pre>kernel_ver=<span><span>`</span>uname -r <span>|</span> cut -d<span><span>'</span>-<span>'</span></span> -f 1<span>`</span></span>
sudo apt-get install -y linux-source-<span>$kernel_ver</span>
<span>cd</span> /usr/src
sudo tar -xf linux-source-<span>${kernel_ver}</span>.tar.bz2
<span>cd</span> /usr/src/linux-source-<span>${kernel_ver}</span>
<span>test</span> -f .config <span>||</span> yes <span><span>"</span><span>"</span></span> <span>|</span> sudo make oldconfig</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">ToolChains</h3><a id="user-content-toolchains" aria-label="Permalink: ToolChains" href="#toolchains"></a></p>
<p dir="auto">To cross-compile binary files for the aarch64 architecture on an amd64 architecture system, you need to install the gcc-aarch64-linux-gnu toolchain. Similarly, to cross-compile binary files for the amd64 architecture on an aarch64 system, you need to install the gcc-x86-64-linux-gnu toolchain.</p>
<ul dir="auto">
<li>amd64 arch: gcc-aarch64-linux-gnu</li>
<li>arm64 arch: gcc-x86-64-linux-gnu</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build Commands</h3><a id="user-content-build-commands" aria-label="Permalink: Build Commands" href="#build-commands"></a></p>
<p dir="auto">To build an <code>arm64</code> artifact on an ubuntu <code>amd64</code> system, you can set the <code>CROSS_ARCH</code> environment variable to achieve cross-compilation.</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Stargazers over time</h2><a id="user-content-stargazers-over-time" aria-label="Permalink: Stargazers over time" href="#stargazers-over-time"></a></p>
<p dir="auto"><a href="https://starchart.cc/gojue/ecapture" rel="nofollow"><img src="https://camo.githubusercontent.com/82f9137fa72bb9a2660aa993e601d83c1c661035ee4b8cfbabbad9160e1334a6/68747470733a2f2f7374617263686172742e63632f676f6a75652f65636170747572652e737667" alt="Stargazers over time" data-canonical-src="https://starchart.cc/gojue/ecapture.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">See <a href="https://github.com/gojue/ecapture/blob/master/CONTRIBUTING.md">CONTRIBUTING</a> for details on submitting patches and the contribution workflow.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-Precision (266 pts)]]></title>
            <link>https://www.together.ai/blog/flashattention-3</link>
            <guid>40938577</guid>
            <pubDate>Thu, 11 Jul 2024 17:06:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.together.ai/blog/flashattention-3">https://www.together.ai/blog/flashattention-3</a>, See on <a href="https://news.ycombinator.com/item?id=40938577">Hacker News</a></p>
<div id="readability-page-1" class="page"><div fs-richtext-element="rich-text"><p>Attention, as a core layer of the ubiquitous Transformer architecture, is a bottleneck for large language models and long-context applications. FlashAttention (and FlashAttention-2) pioneered an approach to speed up attention on GPUs by minimizing memory reads/writes, and is now used by most <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">libraries</a> to accelerate Transformer training and inference. This has contributed to a massive increase in LLM context length in the last two years, from 2-4K (GPT-3, OPT) to 128K (GPT-4), or even 1M (<a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k">Llama 3</a>). However, despite its success, FlashAttention has yet to take advantage of new capabilities in modern hardware, with FlashAttention-2 achieving only 35% utilization of theoretical max FLOPs on the H100 GPU. In this blogpost, we describe three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) incoherent processing that leverages hardware support for FP8 low-precision.</p><p>We’re excited to release FlashAttention-3 that incorporates these techniques. It’s 1.5-2.0x faster than FlashAttention-2 with FP16, up to 740 TFLOPS, i.e., 75% utilization of H100 theoretical max FLOPS. With FP8, FlashAttention-3 reaches close to 1.2 PFLOPS, with 2.6x smaller error than baseline FP8 attention.</p><p>The improvements from FlashAttention-3 will result in:</p><ol role="list"><li><strong>More efficient GPU Utilization</strong>: The new technique uses up to 75% of an H100 GPU's maximum capabilities, up from just 35% before. This results in significantly (1.5-2x) faster than previous versions for training and running of large language models (LLMs).</li></ol><ol start="2" role="list"><li><strong>Better performance with lower precision</strong>: FlashAttention-3 can work with lower precision numbers (FP8) while maintaining accuracy. This allows for even faster processing and potentially lower memory usage, which could lead to cost savings and improved efficiency for customers running large-scale AI operations.</li></ol><ol start="3" role="list"><li><strong>Ability to use longer context in LLMs</strong>: By speeding up the attention mechanism, FlashAttention-3 enables AI models to work with much longer pieces of text more efficiently. This could allow for applications that can understand and generate longer, more complex content without slowing down.</li></ol><p>FlashAttention-3 is available on Github <a href="https://github.com/Dao-AILab/flash-attention">here.</a></p><p>Read the paper <a href="https://github.com/Dao-AILab/flash-attention">here</a>.</p><h2>FlashAttention Recap</h2><p><a href="https://arxiv.org/abs/2205.14135">FlashAttention</a> is an algorithm that reorders the attention computation and leverages tiling and recomputation to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. We use tiling to load blocks of inputs from HBM (GPU memory) to SRAM (fast cache), perform attention with respect to that block, and update the output in HBM. By not writing the large intermediate attention matrices to HBM, we reduce the amount of memory reads/writes, which brings 2-4x wallclock time speedup.</p><p>‍</p><p>Here we show a diagram of FlashAttention forward pass: with tiling and softmax rescaling, we operate by blocks and avoid having to read/write from HBM, while obtaining the correct output with no approximation.</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/668fef78677cfc76424fd0f4_flash_recap_diagram.png" loading="lazy" alt=""></p></figure><h2>New hardware features on Hopper GPUs - WGMMA, TMA, FP8</h2><p>While FlashAttention-2 can achieve up to 70% theoretical max FLOPS on Ampere (A100) GPUs, it does not yet take advantage of new features on Hopper GPUs to maximize performance. We describe some of the new Hopper-specific features here, and why they are important.</p><p>1. WGMMA (Warpgroup Matrix Multiply-Accumulate). This new feature makes use of the new Tensor Cores on Hopper, with much higher throughput <sub>1</sub> than the older mma.sync instruction in Ampere (image from the <a href="https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper?ncid=no-ncid">H100 white paper)</a>.</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/668fef94896a312df22db9bd_h100_wgmma.png" loading="lazy" alt=""></p></figure><p>2. TMA (Tensor Memory Accelerator). This is a special hardware unit that accelerates the transfer of data between global memory and shared memory, taking care of all index calculation and out-of-bound predication. This frees up registers, which is a valuable</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/668ff020c21b55bf4f71c935_h100_tma.png" loading="lazy" alt=""></p></figure><p>3. Low-precision with FP8. This doubles the Tensor Core throughput (e.g. 989 TFLOPS with FP16 and 1978 TFLOPS with FP8), but trades off accuracy by using fewer bits to represent floating point numbers.</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/668ff09c0c34e79f0eb28842_h100_wgmma_fp8.png" loading="lazy" alt=""></p></figure><div><p>FlashAttention-3 makes use of all of these new features of Hopper, using powerful abstractions from <a href="https://github.com/NVIDIA/cutlass">NVIDIA’s CUTLASS</a> library.</p><p>By rewriting FlashAttention to use these new features, we can already significantly speed it up (e.g., from 350 TFLOPS in FlashAttention-2 FP16 forward pass to around 540-570 TFLOPS). However, the asynchronous nature of the new instructions on Hopper (WGMMA and TMA) opens up additional algorithmic opportunities to overlap operations and thereby extract even greater performance. For this blogpost, we’ll explain two such techniques specific to attention. The generic technique of warp specialization, with separate producer and consumer warps doing TMA and WGMMA, is <a href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#warp-specialization">well-covered elsewhere</a> in the context of GEMM and works the same here.</p></div><h2>Asynchrony: Overlapping GEMM and Softmax</h2><p>Why overlap?</p><p>Attention has GEMMs (those matmuls between Q and K and between attention probability P and V) and softmax as its two main operations. Why do we need to overlap them? Isn’t most of the FLOPS in the GEMMs anyway? As long as the GEMMs are fast (e.g., computed using WGMMA instructions), shouldn’t the <a href="https://horace.io/brrr_intro.html">GPU be going <em>brrrr</em></a>?</p><p>The problem is that non-matmul operations are much slower than matmul operations on modern accelerators. Special functions such as exponential (for the softmax) have even lower throughput than floating point multiply-add; they are evaluated by the multi-function unit, a unit separate from floating point multiply-add or matrix multiply-add. As an example, the H100 GPU SXM5 has 989 TFLOPS of FP16 matrix multiply, but only 3.9 TFLOPS (256x less throughput) for special functions <sub>2</sub> ! For head dimension 128, there are 512x more matmul FLOPS than exponential, which means that exponential can take 50% of the time compared to matmul. The situation is even worse for FP8, where the matmul FLOPS are twice as fast yet exponential FLOPS stay the same speed. Ideally we want matmul and softmax to operate in parallel. While the Tensor Cores are busy with matmul, the multi-function units should be calculating exponential!</p><h3>Inter-warpgroup overlapping with pingpong scheduling</h3><p>The first and easiest way to overlap GEMM and softmax is to do nothing at all! The warp schedulers already try to schedule warps so that if some warps are blocked (e.g., waiting for GEMM results), other warps can run. That is, the warp schedulers do some of this overlapping for us, for free.</p><p>However, we can improve on this by doing some of the scheduling manually. As an example, if we have 2 warpgroups (labeled 1 and 2 – each warpgroup is a group of 4 warps), we can use synchronization barriers (bar.sync) so that warpgroup 1 first does its GEMMs (e.g., GEMM1 of one iteration and GEMM0 of the next iteration), and then warpgroup 2 does its GEMMs while warpgroup 1 does its softmax, and so on. This “pingpong” schedule is illustrated in the figure below, where the same color denotes the same iteration.</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/668ff0a001e73d009d1b73fb_pingpong_pipelining.png" loading="lazy" alt=""></p></figure><p>This would allow us to perform the softmax in the shadow of the GEMMs of the other warpgroup. Of course, this figure is just a caricature; in practice the scheduling is not really this clean. Nevertheless, pingpong scheduling can improve FP16 attention forward pass from around 570 TFLOPS to 620 TFLOPS (head dim 128, seqlen 8K).</p><h3>Intra-warpgroup overlapping of GEMM and Softmax</h3><p>Even within one warpgroup, we can have some part of softmax running while the GEMMs of that warpgroup is running. This is illustrated in this figure, where the same color denotes the same iteration.</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/668ff0d6d9fd917e68abc5f3_2_stage_pipelining.png" loading="lazy" alt=""></p></figure><p>This pipelining increases throughput from around 620 TFLOPS to around 640-660 TFLOPS for FP16 attention forward, at the cost of higher register pressure. We need more registers to hold both accumulators of the GEMMs, and the input/output of softmax. Overall, we find this technique to offer a favorable tradeoff.</p><p>Low-precision: reduce quantization error with incoherent processing</p><p>LLM activation can have <a href="https://arxiv.org/abs/2208.07339">outliers</a> with much larger magnitude than the rest of the features. These outliers make it difficult to quantize, producing much larger quantization errors. We leverage incoherent processing, a technique used in the quantization literature (e.g. from <a href="https://arxiv.org/abs/2307.13304">QuIP</a>) that multiplies the query and key with a random orthogonal matrix to “spread out” the outliers and reduce quantization error. In particular, we use the Hadamard transform (with random signs), which can be done per attention head in O(d log d) instead of O(d^2) time, where d is the head dimension. Since the Hadamard transform is memory-bandwidth bound, it can be fused with previous operations such as rotary embedding (also memory-bandwidth bound) “for free”.</p><p>In our experiment where Q, K, V are generated from a standard normal distribution but 0.1% of the entries have large magnitudes (to simulate outliers), we found that incoherent processing can reduce the quantization error by 2.6x. We show numerical error comparison in the table below. Please see the paper for details.</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/668ff0e415c3198e0afba4ab_flash3_numerical_error.png" loading="lazy" alt=""></p></figure><h2>Attention benchmark</h2><p>We show some results with FlashAttention-3, and compare it to FlashAttention-2, as well as the implementation in Triton and cuDNN (both of which already use new hardware features of Hopper GPUs).</p><p>For FP16, we see about 1.6x-1.8x speedup over FlashAttention-2</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/668ff11d2cb1c308a71d0cdf_flash3_fp16_fwd.png" loading="lazy" alt=""></p></figure><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/668ff0f215c3198e0afbad52_flash3_fp16_bwd.png" loading="lazy" alt=""></p></figure><p>For FP8, we can reach close to 1.2 PFLOPS!</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/668ff0e74da9fa32dfc5bacf_flash3_fp8_fwd.png" loading="lazy" alt=""></p></figure><h2>Discussion</h2><p>This blogpost highlights some of the optimizations for FlashAttention available on Hopper GPUs. Other optimizations (e.g., variable length sequences, persistent kernel, and in-kernel transpose for FP8) are covered in the paper. <br>We have seen that designing algorithms that take advantage of the hardware they run on can bring significant efficiency gains and unlock new model capabilities such as long context. We look forward to future work on optimization for LLM inference, as well as generalizing our techniques to other hardware architectures.&nbsp;</p><p>We also look forward to FlashAttention-3 being integrated in a future release of PyTorch.</p><p>‍</p><p><em><sub>Footnotes:</sub></em></p><p><em><sub>1 Without the wgmma instruction, the older mma.sync instruction can only reach about ⅔ the peak throughput of Hopper Tensor Cores: https://arxiv.org/abs/2402.13499v1</sub></em></p><p><em><sub>2 The CUDA programming guide specifies that the throughput for special functions is 16 operations per streaming multiprocessor (SM) per clock cycle. We multiply 16 by 132 SMs and 1830 Mhz (clock speed used to calculate 989 TFLOPS of FP16 matmul) to get 3.9 TFLOPS</sub></em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rulers of the Ancient World: period correct measuring tools (110 pts)]]></title>
            <link>https://www.burn-heart.com/rulers-of-the-ancient-world</link>
            <guid>40938388</guid>
            <pubDate>Thu, 11 Jul 2024 16:41:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.burn-heart.com/rulers-of-the-ancient-world">https://www.burn-heart.com/rulers-of-the-ancient-world</a>, See on <a href="https://news.ycombinator.com/item?id=40938388">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-type="page" data-updated-on="1502683525230" id="canvasWrapper" role="main" data-content-field="main-content"><div data-block-type="2" id="block-c8312fe2defdbd7e48aa">
  <p>The "Rulers of the Ancient World" is a metrology/design/production project, based around producing a range of period correct rulers from various ancient empires.</p><p>Stemming from a personal fascination with the creation and development of systems of measurement, this project seeks to highlight the artfulness of handmade tools and the capability of handwork and CNC milling to complement one another and create a unique, novel product.</p><p>The original series of four rulers (Egyptian Span &amp; Cubit, Roman Cubitus and Japanese Kanejaku)&nbsp;are each locally sourced Hard Maple, hand planed, French polished and waxed, and etched by CNC with a 0.1 mm engraving bit, which is then inked with India Ink, applied by hand. The Limited Edition ruler set, an Egyptian Span, Roman Cubitus and Japanese Shaku, were made with local and historically accurate woods for their respective cultures, and created by similar means. The French "Roubo" Fathom was made with flamed maple, and similarly polished and finished, though etched by hand. Lastly, like the Limited Edition ruler set, the French "Roubo Pied du Roi" ruler is made of European Sycamore, a geographically correct and culturally relevant wood.</p><p>The goals of this project are far-reaching- to illuminate the use of these ancient measurement systems, to enable a physical, tactile engagement with a piece of history, and highlight the possibility of beauty and novelty in toolmaking in the presence of traditional techniques and CNC machinery.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_8_1466106089330_19688">

<p>
  <h2>french "Roubo Toise" Fathom Ruler</h2>
</p>






















</div><div data-block-type="2" id="block-yui_3_17_2_2_1466116632014_31437">
  <p>The French "Roubo" Fathom is based on the pre-metric, 18th century "Pied du Roi," or the King's foot. This foot varies from the modern foot a significant amount, about 51/64" over a foot. This makes each inch (<em>pouce</em>)&nbsp;1.066", each foot (<em>pied</em>) 12.792" and the six foot fathom (<em>toise</em>) 6.396', or 76.752".&nbsp;</p><p>This period measurement finds significance through a historic account of woodworking techniques, "L'Art du Menuisier" by André Roubo. In this book (a beautiful reprint of which can be found over at <a target="_blank" href="https://lostartpress.com/products/l-art-du-menuisier-the-book-of-plates">Lost Art Press</a>) Roubo uses the French inch, foot and fathom as a scale for each drawing. This rule is intended as a study aid/physical representation of this unit, for those seeking to replicate or produce projects from the plates, or better understand the scale on which Roubo's work was done. It is taken directly from a drawing in Roubo's work, plate 100, figure 2.</p><p>Each ruler is made of gorgeous flamed Maine-grown maple, and fully hand etched and stamped using the traditional, preindustrial techniques, along with hand planed edges and card scraped faces. Each is also finished with hand rubbed shellac and wax.&nbsp;</p><p>These techniques give each ruler a unique look, individual variation, though each maintains the accuracy of divider layout and tightly scribed lines. They are 76.75" long, 1.875" wide and a heavy 1/4" thick. Intended for use, though refined for display and collection.</p><p>Plate 100 image from "Roubo on Furniture," used with permission.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_5_1468461924049_21450">

<p>
  <h2>FRENCH "Roubo PIED du roi" Foot Ruler</h2>
</p>






















</div><div data-block-type="2" id="block-yui_3_17_2_5_1468461924049_33367">
  <p>The "Roubo Pied du Roi"&nbsp;ruler is based on the 18th Century French foot.&nbsp;This foot varies from the modern imperial foot a significant amount, about 51/64" over a foot. This makes each inch/thumb (<em>pouce)</em>&nbsp;1.066" and each foot (<em>pied</em>)&nbsp;12.792". The inch is divided into 12 lines (<em>lignes</em>), an interested subdivision used by the French for its divisibility. This adds an interesting feature to this ruler- one edge's subdivisions emphasize 1/6 and 1/3 subdivisions, while the other emphasizes 1/8, 1/4 and 1/2 subdivisions.</p><p>This era in measurement in Europe was one of great variety and decentralization- at one time, dozens of different measures could be found, based on occupation and locale. For instance, on page 100 of Johann Friedrich Krüger's metrological text <a target="_blank" href="https://books.google.com/books?id=iro2AAAAMAAJ&amp;pg=PA100">"Vollständiges handbuch der münzen, masse und gewicht aller länder der erde"</a> (translates roughly to "full manual of coins, mass and weight of all countries of the Earth") shows that even in western Europe (which it seems this text asserts as being "all the countries of the Earth") there were <strong>134 </strong>different foot measurements, ranging from 236 to 480 modern millimeters. While certain countries were better at maintaining a consistent measure, it was exactly this issue that eventually led to the establishment and eventual adoption of the metric system- though I have to say, there is a romantic (if ridiculous) impulse to support this diversity of unit on my part- though adoption of common units of measure have undoubtedly led to an ease in collaboration and documentation of all matter of work. Just ask your local physicist if they use inches!</p><p>This period measurement finds modern significance through a historic account of woodworking techniques, "L'Art du Menuisier" by André Roubo. In this book (a beautiful reprint of which can be found over at <a target="_blank" href="https://lostartpress.com/products/l-art-du-menuisier-the-book-of-plates">Lost Art Press</a>) Roubo uses the French inch, foot and fathom as a scale for each drawing. This rule is intended as a study aid/physical representation of this unit, for those seeking to replicate or produce projects from the plates, or better understand the scale on which Roubo's work was done.</p><p>Each is made in French Sycamore (Acer pseudoplatanus), geographically and chronologically significant to the French culture it came for. It is produced in the same format and fashion as the <a target="_blank" href="http://www.burn-heart.com/shop/lckywlic1idq82fnhcd9w8qlezbuf7">Limited Edition rules</a>, and makes a great edition to that set. It has a hand-rubbed French polish finish, with a hard wax coating, and is handplaned to size and etched by an ultra-precise CNC process.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_6_1455727531539_19194">

<p>
  <h2>Egyptian "Cubit" and "Span" Rulers</h2>
</p>






















</div><div data-block-type="2" id="block-yui_3_17_2_7_1455661458912_11097">
  <p>The Egyptian Cubit Ruler and its half size Span ruler are based on various "Cubit Rods" found in the tombs of famous foremen and architects of Ancient Egypt. These rods, given as ornamental gifts by Pharoahs to nobility, serve as references for modern archaeological and metrological experts in determining the dimensions of measures from this time. This particular ruler is based on those cubit rods found in the Turin Museum, in Turin, Italy, and its dimensions go back to roughly 1500 BC, during the "New Kingdom" era of Egypt.&nbsp;</p><p>My initial excitement for metrology came from reading "<a target="_blank" href="http://www.amazon.com/Secrets-Great-Pyramid-Discoveries-Surrounding/dp/0883659573">Secrets of the Great Pyramid,</a>"&nbsp;a book by metrologist and egyptologist Peter Tompkins. In this book, Tompkins uses projections of the base units of measure in the civilization who built the pyramids to make determinations as to the geometrical basis of the pyramids, and goes on to use these conclusions to further extrapolate possible uses and design elements of the pyramids. This interest was further heightened when in 2010 I was able to visit the Great Pyramid and spend time, alone with only my father and two sisters, in the Queen's Chamber, the heart of the pyramid.&nbsp;</p><p>The two ruler's units are based on the Egyptian "Djeba" or finger, measuring 18.75 mm (0.738"). Each is further subdivided into a "Sheshep" or palm of four fingers, "Drt" or hand of five fingers,&nbsp;the "Pedj-aa" or large span of 14 fingers, which is the Span ruler's full length of 10.332". The larger cubit ruler extends to the "Meh Niswt" or Royal Cubit, which is two spans of 14 fingers, or 20.664". An interesting quirk of these rulers concern the first fourteen fingers, each of which is divided into their respective subdivision, for example the seventh finger is divided in seven, the tenth into ten, so on. This is copied directly from the ancient cubit rod relics, and speaks to both their use and possession of divider-like tools, and their use of fractional, complex mathematics.</p><p>There is a lot of interesting reading concerning these units, the cubit rods, and metrology in general, and were my sources for the research that led to these rulers. Here are a few links:</p><p><a target="_blank" href="http://www.metmuseum.org/pubs/bulletins/1/pdf/3257092.pdf.bannered.pdf">A small article from the Metropolitan Museum of Art on Cubit Rods</a>.</p><p><a target="_blank" href="http://www.egyptorigins.org/index.html">Egypt Origins</a>, an HTML-ey website full of technical and detailed accounts of Egyptian units, cubit rods, and other such information.</p><p>"<a target="_blank" href="http://www.amazon.com/Secrets-Great-Pyramid-Discoveries-Surrounding/dp/0883659573">Secrets of the Great Pyramid</a>" by Peter Tompkins on Amazon.</p><p><a target="_blank" href="http://www.ucl.ac.uk/museums-static/digitalegypt/">Digital Egyptian Museum</a>, another old school HTML website full of good information on the Ancient Egyptian Civilizations.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_7_1455661458912_61673">

<p>
  <h2>Japanese "KaneJaku" Shaku Ruler</h2>
</p>






















</div><div data-block-type="2" id="block-yui_3_17_2_7_1455661458912_62825">
  <p>The Japanese "Kanejaku" or Shaku Ruler is based on the Japanese unit of the "Shaku," an ancient measure derived from the Chinese "Chi," adopted in Japan in 701, though the units used here are based on those in use during Japan's Edo period, from the 17th to 19th centuries.&nbsp;Its original length was likely much shorter than the standardized 303 mm, as it was originally the length from outstretched thumb to middle finger. Several stories attempt to explain the change, my favorite of which tells about its use in taxation, and its elongation due to corrupt officials attempting to lengthen the unit and therefore increase their take.</p><p>The unit standardized in 1891 was based on the "Kanejaku" or carpenter's square measure, as it had seemingly changed the least over time, in part due to its use in temple building. As this is a tool made for (and by) woodworkers, I was inclined to name it after this original unit. The unit was standardized to 10/33 meters, or around 11.93", though its proximity to the foot is happenstance.&nbsp;<span>While the base unit is the "Shaku" of 303 mm (~11.93"), there are further decimal subdivisions. The 寸&nbsp;or "Sun" is 1/10 of a shaku (~1.19"), and the </span><span>分&nbsp;</span><span>decimal or "Bu" is 1/100 of a shaku (~0.119").</span></p><p>An interesting aspect of this ruler from a design standpoint was the question of how to represent the numbers and characters of the Japanese language. With the other rulers, I used the characters that were common at the time of the measurement's use, and do not use any modern characters or anachronistic type or language. However in this case, the unit was adopted in Japan in 701 CE, but changed so much over that time that the only reasonable measurement on which to base it was its 1891 standardized length. So, I opted to use the modern numbering scheme, opting out of using more formal Japanese number system or more primitive. As the unit is still in use today among temple builders and other traditional Japanese makers, this is in part a salute to the relevance and steadfastness of the unit itself. I also took the time to design and redraw my own Japanese characters, both for the numbers and the characters for the Bu, Sun and Shaku.</p><p>An interesting etymological note- the Shakuhachi instrument of Japan takes its name from its length- one shaku and "hachi" (eight) sun, their combination in a etymological blending being shaku-hachi. Just a fun note.</p><p>On another note, there is another "Shaku" unit, called the "Kujirajaku" or Whale Shaku. It is used in the textile industry, much like the yard in the United States. It gets its name from the baleen of a whale, which were used as measurement tools themselves. This ruler is around 379 mm (14.9") inches long, and a great example of using a natural resource with a relatively consistent length as a measurement tool. I'd make a ruler in this size, but its nature as a flexible ruler for cloth would be lost being made of wood, and I'm certainly not interested in acquiring any baleen! There are even more "Shaku" for various trades and special purposes, like the "Gofukujaku" used by traditional Japanese clothiers.</p><p>There's some research in English, though not a ton, on the Shaku. Most interesting is the general use of geometry and measure in Japanese home and temple building. Here are some links:</p><p><a target="_blank" href="http://www.sljfaq.org/afaq/units.html">Japanese FAQ</a>&nbsp;page with a large amount of information on the different units, and a brief history on the units.</p><p><a target="_blank" href="http://www.toolsfromjapan.com/store/index.php?main_page=page&amp;id=12&amp;chapter=5">Tools from Japan</a>'s page on the sizing of Japanese Kanna or handplanes, with an interesting big of information on why the "Kujirajaku" is used, instead of the standard "Kanejaku."</p><p><a target="_blank" href="http://reibo.org/">Reibo</a>, a website about Shakuhachi including some interesting musicological/technical writing on the use of shaku in determining pitch in the traditional flute.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_7_1455661458912_161548">

<p>
  <h2>Roman "Cubitus" Ruler</h2>
</p>






















</div><div data-block-type="2" id="block-yui_3_17_2_7_1455661458912_218416">
  <p>The Roman "Cubitus" Ruler is based on the measure used in Ancient Rome roughly around the time of the Second Triumvariate period, circa 40 BCE. The base unit of the Romans was in fact the "Pes" or foot (296 mm or 11.65"), though they used the cubit as well, measured at 1 1/2 "Pes," or about 444 mm (17.47")&nbsp;. The subdivisions of the foot may be this ruler's most interesting feature- the foot is divided into both 12 "Uncia" or thumbs (etymological root of inch) and 16 "Digiti" or digits. This comes from a duality of systems- the Greek divided their foot into 16 "Daktylos" or fingers, but this older system gave way slowly over the course of the Empire's reign to the "Uncia," a division of twelve. Interestingly, these units coexisted for some time, with different regions using the different subdivisions.</p><p>My inclusion of this ruler and system of measurement in this project is two-fold. For one, this is one of the most widespread measurements of all time, maybe the most widely used before the British foot and Metric system traveled the world. On the other hand, it is very similar to many other measurements around that time, and represents what many coexistent civilizations were using, including the Egyptian and Byzantine Empires of the first few centuries CE. It also is the distant source of the modern English foot, being very similar at 11.65" (though not as close as the Shaku at 11.93", which is simply a coincidence). Furthermore, there is some controversy as to whether the "Pes" refers to an anatomical foot or a shoed or booted foot, and account for the general truth that one's foot is shorter than an actual foot.</p><p>Aside from the "Cubitus" and "Pes," this ruler also features the "Palmus" or width across the palm, and "Palmus Major" or length of the hand from heel of the palm to tip of the middle finger. I wish I could feature some of the longer length measurements, as they get colorful in longer lengths- a step of 2 1/2 feet, a pace 5 feet, a perch of 10 feet, and the famous stadium of 625 feet. I'll have to make some really long rulers to attain these measurements. Maybe some day.</p><p>There is a huge body of research on this subject. Metrologists have studied the Roman foot for centuries, and it's relation to the English foot is a widely discussed topic. This research goes as far back as 1647, when John Greaves, an antiquary, astronomer and mathematician, wrote the snappily named "<a target="_blank" href="https://books.google.com/books?id=LW86AAAAcAAJ">A discourse of the Romane foot and denarius; from whence, as from two principles, the measures and weights used by the ancients may be deduced</a>" in which he writes at some length attempting to determine the true measurement of the Roman foot, by way of measuring various statues and buildings (which,&nbsp;thanks to the magic of the internet, you can read in full at the link above). Others later picked up the research and posited their own conjectured measurements, notably Sir William Smith, a lexicographer who compiled a number of researched sources for his "Dictionary of Greek and Roman Biography, Mythology and Geography."</p><p>There is a wealth of information on the subject, here a few links I found interesting or useful:</p><p><a target="_blank" href="https://books.google.com/books?id=LW86AAAAcAAJ">John Greave's book on the Roman Foot</a>&nbsp;written in 1647, also discusses the denarius, the Roman currency coin.</p><p><a target="_blank" href="http://penelope.uchicago.edu/Thayer/E/Roman/Texts/Frontinus/De_Aquis/text*.html">English Translation of a primary source </a>on the use of digiti and uncia, written by Frontinus in the 1st century CE.</p><p><a target="_blank" href="http://www.unrv.com/culture/distance-measurements.php">UNRV website page on the length measurements</a>, which is brief but the site as a whole is a great resource for Roman history.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_3_1500678874958_62865">

<p>
  <h2>Vedic Aratni Ruler</h2>
</p>






















</div><div data-block-type="2" id="block-yui_3_17_2_3_1500678874958_61700">
  <div><p>Derived from my research into the units used in early Indus Valley civilizations, this ruler's system of measurement comes from unearthed relics from Mohenjo Daro, coupled with research spanning from the indigenous Nepalese systems to Vedic texts like the Arthashastra. This system's base unit, the "Angula," or finger,&nbsp;dates back to around 2500 BCE, making it one of the oldest systems of measurement known. The Angula I've chosen to use is 16.764 mm, a measure deciphered from an unearthed scale etched on a shell, found on the Mohenjo Daro archaeological site in 1930 by Ernest Mackay.</p><p>This ruler is unique, in a few ways. First, it is about half as wide as the standard maple rules- this is largely because it has no subdivisions below the finger (Angula) and could therefore be made smaller- though it is long, at 15.847". The numerals are new, as well, being Devanagari Sanskrit numerals, as would have been used in the time of the measurements. It also has two new "corporeal" units- the "Bow Grip," or Dharnugrah, of four fingers, and the "Fist + Thumb," or Dhanurmishti, of eight fingers.</p></div><p>The ruler is Maine-grown, quarter sawn Hard Maple, with a hand rubbed French polish finish with a hard wax coating, hand rubbed india ink etching and hand planed surfaces. Each ruler is slightly individual, though precisely dimensioned. There will be slight variants in grain and etch depth do to its hand made nature.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_3_1466106089330_48121">

<p>
  <h2>Limited Edition Ruler Set</h2>
</p>






















</div><div data-block-type="2" id="block-yui_3_17_2_3_1466106089330_100972">
  <p>This set of three rulers, in an edition of only ten,&nbsp;is the culmination of research into the native timbers, measurement systems and traditions of three ancient cultures- Dynastic Japan, Pharaonic Egypt and Ancient Rome.</p><p>The first is a Roman Cubitus rule, made in quarter-sawn European Sycamore (<em>Acer pseudoplatanus)</em>, a wood famous in ancient Rome, mentioned and revered by many of the famous Roman historians. &nbsp;It depicts several of the main units of Ancient Rome, the Hand, Span, Foot and Cubit, measured in both Digiti (1/16th of a Roman Foot) and Uncias (1/12th of a Roman Foot), both of which were used at various points of the Roman Empire. This is the longest ruler of the set, and the thinnest, allowing for some flexibility, as the wood is a very strong and sturdy timber.</p><p>The next is a Japanese Shaku made in Japanese Cedar (<em>Cryptomeria japonica</em>) also called sugi (<em>杉)</em>&nbsp;by the Japanese. This wood is planted ornamentally around many of the temples of Japan, and is appropriate for this ruler, as the "Shaku" unit was the base unit of building the temples and shrines of Dynastic Japan.&nbsp;The Japanese "Kanejaku" or Shaku Ruler is based on the Japanese unit of the "Shaku," an ancient measure derived from the Chinese "Chi," adopted in Japan in 701, though the units used here are based on those in use during Japan's Edo period, from the 17th to 19th centuries.</p><p>The last of the set is the Egyptian "Span" ruler in African Blackwood (<em>Dalbergia melanoxylon),&nbsp;</em>called hbny by the Ancient Egyptians (which happens to be the <a target="_blank" href="https://dianabuja.wordpress.com/2012/10/02/modern-words-that-survive-from-ancient-egypt-what-how-and-why/">etymological root of the word ebony</a>). This was a wood of great value in Ancient Egypt, used in ornamental goods and furniture, often for the pharaoh himself.&nbsp;The Egyptian "Span" ruler is based on various "Cubit Rods" found in the tombs of famous foremen and architects of Ancient Egypt. This particular ruler is based on those cubit rods found in the Turin Museum, in Turin, Italy, and its dimensions go back to roughly 1500 BC, during the "New Kingdom" era of Egypt.&nbsp;</p>
</div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Korvus: Single-Query RAG with Postgres (208 pts)]]></title>
            <link>https://github.com/postgresml/korvus</link>
            <guid>40938325</guid>
            <pubDate>Thu, 11 Jul 2024 16:35:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/postgresml/korvus">https://github.com/postgresml/korvus</a>, See on <a href="https://news.ycombinator.com/item?id=40938325">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
   <themed-picture data-catalyst-inline="true"><picture>
     <source media="(prefers-color-scheme: dark)" srcset="https://github.com/postgresml/korvus/assets/19626586/54dda262-861b-4751-a3ce-0790762f3cbe">
     <source media="(prefers-color-scheme: light)" srcset="https://github.com/postgresml/korvus/assets/19626586/f567ce57-35b2-4411-8e43-5f0887a938cb">
     <img alt="Logo" src="https://github.com/postgresml/korvus/raw/main" width="520">
   </picture></themed-picture>
</div>
<p dir="auto"><b>One query to rule them all</b></p>

<p dir="auto">
| <a href="https://postgresml.org/docs/open-source/korvus/" rel="nofollow"><b>Documentation</b></a> | <a href="https://postgresml.org/blog" rel="nofollow"><b>Blog</b></a> | <a href="https://discord.gg/DmyJP3qJ7U" rel="nofollow"><b>Discord</b></a> |
</p>
<hr>
<p dir="auto">Korvus is a search SDK that unifies the entire RAG pipeline in a single database query. Built on top of Postgres with bindings for Python, JavaScript and Rust, Korvus delivers high-performance, customizable search capabilities with minimal infrastructure concerns.</p>
<details open="">
<summary><b>📕 Table of Contents</b></summary>
<ul dir="auto">
<li><a href="#-what-is-korvus">🦅 What is Korvus?</a></li>
<li><a href="#-languages">🔠 Languages</a></li>
<li><a href="#-why-korvus">🏆 Why Korvus?</a></li>
<li><a href="#-key-features">⚡ Key Features</a></li>
<li><a href="#-system-architecture">🧩 System Architecture</a></li>
<li><a href="#-get-started">🚀 Get Started</a></li>
<li><a href="#-the-power-of-sql">🔍 The Power of SQL</a></li>
<li><a href="#-documentation">📘 Documentation</a></li>
<li><a href="#-community">🌐 Community</a></li>
<li><a href="#-contributing">🤝 Contributing</a></li>
</ul>
</details>
<details open="">
  <summary>
    
    <span aria-label="Video description demo.mp4">demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19626586/347521449-2b697dc6-8c38-41a7-8c8e-ef158dacb29b.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MzczMDIsIm5iZiI6MTcyMDczNzAwMiwicGF0aCI6Ii8xOTYyNjU4Ni8zNDc1MjE0NDktMmI2OTdkYzYtOGMzOC00MWE3LThjOGUtZWYxNThkYWNiMjliLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA3MTElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNzExVDIyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ5MjhjYmI2NGM2Y2E1OTVhZTYyYzQ0ZGYyOGI2MGFjOTc4ZmI4MzljZjQyODI1ZDkxMzY1Zjk5MGNhZmE3ZGMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.I4bPw86GlP4MsGPVpOJeUZu36dqW8adeqDfXRXOQcM8" data-canonical-src="https://private-user-images.githubusercontent.com/19626586/347521449-2b697dc6-8c38-41a7-8c8e-ef158dacb29b.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MzczMDIsIm5iZiI6MTcyMDczNzAwMiwicGF0aCI6Ii8xOTYyNjU4Ni8zNDc1MjE0NDktMmI2OTdkYzYtOGMzOC00MWE3LThjOGUtZWYxNThkYWNiMjliLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA3MTElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNzExVDIyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ5MjhjYmI2NGM2Y2E1OTVhZTYyYzQ0ZGYyOGI2MGFjOTc4ZmI4MzljZjQyODI1ZDkxMzY1Zjk5MGNhZmE3ZGMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.I4bPw86GlP4MsGPVpOJeUZu36dqW8adeqDfXRXOQcM8" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">🦅 What is Korvus?</h2><a id="user-content--what-is-korvus" aria-label="Permalink: 🦅 What is Korvus?" href="#-what-is-korvus"></a></p>
<p dir="auto">Korvus is an all-in-one, open-source RAG (Retrieval-Augmented Generation) pipeline built for Postgres. It combines LLMs, vector memory, embedding generation, reranking, summarization and custom models into a single query, maximizing performance and simplifying your search architecture.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19626586/344766695-9ee9d695-7630-4da7-ab2a-386e20ae4a68.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MzczMDIsIm5iZiI6MTcyMDczNzAwMiwicGF0aCI6Ii8xOTYyNjU4Ni8zNDQ3NjY2OTUtOWVlOWQ2OTUtNzYzMC00ZGE3LWFiMmEtMzg2ZTIwYWU0YTY4LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA3MTElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNzExVDIyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZlYmFiNTg2MTA1NWQyNWI5MWMyNTE2OTUyMDg0ZjExNzY1ZGFmODE1Y2VkNDAzNTRlYWQ1ZGViNmZhMDYzMDYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.xD0xJlDKsRn8vbkwcYG5cUY-f42ebDlaYJlHFaZwKR0"><img src="https://private-user-images.githubusercontent.com/19626586/344766695-9ee9d695-7630-4da7-ab2a-386e20ae4a68.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MzczMDIsIm5iZiI6MTcyMDczNzAwMiwicGF0aCI6Ii8xOTYyNjU4Ni8zNDQ3NjY2OTUtOWVlOWQ2OTUtNzYzMC00ZGE3LWFiMmEtMzg2ZTIwYWU0YTY4LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA3MTElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNzExVDIyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZlYmFiNTg2MTA1NWQyNWI5MWMyNTE2OTUyMDg0ZjExNzY1ZGFmODE1Y2VkNDAzNTRlYWQ1ZGViNmZhMDYzMDYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.xD0xJlDKsRn8vbkwcYG5cUY-f42ebDlaYJlHFaZwKR0" alt="korvus-demo" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔠 Languages</h2><a id="user-content--languages" aria-label="Permalink: 🔠 Languages" href="#-languages"></a></p>
<p dir="auto">Korvus provides SDK support for multiple programming languages, allowing you to integrate it seamlessly into your existing tech stack:</p>
<ul dir="auto">
<li>Python: <a href="https://pypi.org/project/korvus/" rel="nofollow">PyPI Package</a></li>
<li>JavaScript: <a href="https://www.npmjs.com/package/korvus" rel="nofollow">npm Package</a></li>
<li>Rust: <a href="https://crates.io/crates/korvus" rel="nofollow">Crates.io Package</a></li>
<li>C: <a href="https://postgresml.org/docs/api/client-sdk/" rel="nofollow">Build from source</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🏆 Why Korvus?</h2><a id="user-content--why-korvus" aria-label="Permalink: 🏆 Why Korvus?" href="#-why-korvus"></a></p>
<p dir="auto">Korvus stands out by harnessing the full power of Postgres for RAG operations:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Postgres-Native RAG</strong>: Korvus leverages Postgres' robust capabilities, allowing you to perform complex RAG operations directly within your database. This approach eliminates the need for external services and API calls, significantly reducing latency and complexity many times over.</p>
</li>
<li>
<p dir="auto"><strong>Single Query Efficiency</strong>: With Korvus, your entire RAG pipeline - from embedding generation to text generation - is executed in a single SQL query. This "one query to rule them all" approach simplifies your architecture and boosts performance.</p>
</li>
<li>
<p dir="auto"><strong>Scalability and Performance</strong>: By building on Postgres, Korvus inherits its excellent scalability and performance characteristics. As your data grows, Korvus grows with it, maintaining high performance even with large datasets.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">⚡ Key Features</h2><a id="user-content--key-features" aria-label="Permalink: ⚡ Key Features" href="#-key-features"></a></p>
<ul dir="auto">
<li><strong>Simplified Architecture</strong>: Replace complex service oriented architectures with a single, powerful query.</li>
<li><strong>High Performance</strong>: Eliminates API calls and data movement for faster processing and greater reliability.</li>
<li><strong>Open Source</strong>: Improve your developer experience with open source software and models that run locally in Docker too.</li>
<li><strong>Multi-Language Support</strong>: Use Korvus with Python, JavaScript and Rust. Open an issue to vote for other language support.</li>
<li><strong>Unified Pipeline</strong>: Combine embedding generation, vector search, reranking, and text generation in one query.</li>
<li><strong>Postgres-Powered</strong>: Under the hood, Korvus operations are powered by efficient SQL queries on a time-tested database platform.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🧩 System Architecture</h2><a id="user-content--system-architecture" aria-label="Permalink: 🧩 System Architecture" href="#-system-architecture"></a></p>
<p dir="auto">Korvus utilizes PostgresML's pgml extension and the pgvector extension to compress the entire RAG pipeline inside of Postgres.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19626586/343950842-53128313-ded8-4b29-91c4-f585db859c23.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MzczMDIsIm5iZiI6MTcyMDczNzAwMiwicGF0aCI6Ii8xOTYyNjU4Ni8zNDM5NTA4NDItNTMxMjgzMTMtZGVkOC00YjI5LTkxYzQtZjU4NWRiODU5YzIzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA3MTElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNzExVDIyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTNiOGYzZjgwZTBmZjJkMGU0ZWFlODdhOWQ1NjVhN2U3YTY0OGE0OGRhYmRiN2VmOTVkMGVmNTdiM2ZiYTRhZjEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.2Dl2cS-glyIQd1W7tP1VXdW3pVSuMlRTSoAtQo2pMp8"><img src="https://private-user-images.githubusercontent.com/19626586/343950842-53128313-ded8-4b29-91c4-f585db859c23.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MzczMDIsIm5iZiI6MTcyMDczNzAwMiwicGF0aCI6Ii8xOTYyNjU4Ni8zNDM5NTA4NDItNTMxMjgzMTMtZGVkOC00YjI5LTkxYzQtZjU4NWRiODU5YzIzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA3MTElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNzExVDIyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTNiOGYzZjgwZTBmZjJkMGU0ZWFlODdhOWQ1NjVhN2U3YTY0OGE0OGRhYmRiN2VmOTVkMGVmNTdiM2ZiYTRhZjEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.2Dl2cS-glyIQd1W7tP1VXdW3pVSuMlRTSoAtQo2pMp8" alt="PostgresML_Old-V-New_Diagram-Update"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Get Started</h2><a id="user-content--get-started" aria-label="Permalink: 🚀 Get Started" href="#-get-started"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">📋 Prerequisites</h3><a id="user-content--prerequisites" aria-label="Permalink: 📋 Prerequisites" href="#-prerequisites"></a></p>
<p dir="auto">To use Korvus, you need a Postgres database with pgml and pgvector installed. You have two options:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Self-hosted</strong>: Set up your own database with pgml and pgvector.</p>
<ul dir="auto">
<li>For instructions, see our <a href="https://postgresml.org/docs/resources/developer-docs/quick-start-with-docker" rel="nofollow">self-hosting guide</a>.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Hosted Service</strong>: Use our managed Postgres service with pgml and pgvector pre-installed.</p>
<ul dir="auto">
<li><a href="https://postgresml.org/signup" rel="nofollow">Sign up for PostgresML Cloud</a>.</li>
</ul>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">🏁 Quick Start</h3><a id="user-content--quick-start" aria-label="Permalink: 🏁 Quick Start" href="#-quick-start"></a></p>
<ol dir="auto">
<li>Install Korvus:</li>
</ol>

<ol start="2" dir="auto">
<li>Set the <code>KORVUS_DATABASE_URL</code> env variable:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="export KORVUS_DATABASE_URL=&quot;{YOUR DATABASE CONNECTION STRING}&quot;"><pre><span>export</span> KORVUS_DATABASE_URL=<span><span>"</span>{YOUR DATABASE CONNECTION STRING}<span>"</span></span></pre></div>
<ol start="3" dir="auto">
<li>Initialize a Collection and Pipeline:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="from korvus import Collection, Pipeline
import asyncio

collection = Collection(&quot;korvus-demo-v0&quot;)
pipeline = Pipeline(
    &quot;v1&quot;,
    {
        &quot;text&quot;: {
            &quot;splitter&quot;: {&quot;model&quot;: &quot;recursive_character&quot;},
            &quot;semantic_search&quot;: {&quot;model&quot;: &quot;Alibaba-NLP/gte-base-en-v1.5&quot;},
        }
    },
)

async def add_pipeline():
    await collection.add_pipeline(pipeline)

asyncio.run(add_pipeline())"><pre><span>from</span> <span>korvus</span> <span>import</span> <span>Collection</span>, <span>Pipeline</span>
<span>import</span> <span>asyncio</span>

<span>collection</span> <span>=</span> <span>Collection</span>(<span>"korvus-demo-v0"</span>)
<span>pipeline</span> <span>=</span> <span>Pipeline</span>(
    <span>"v1"</span>,
    {
        <span>"text"</span>: {
            <span>"splitter"</span>: {<span>"model"</span>: <span>"recursive_character"</span>},
            <span>"semantic_search"</span>: {<span>"model"</span>: <span>"Alibaba-NLP/gte-base-en-v1.5"</span>},
        }
    },
)

<span>async</span> <span>def</span> <span>add_pipeline</span>():
    <span>await</span> <span>collection</span>.<span>add_pipeline</span>(<span>pipeline</span>)

<span>asyncio</span>.<span>run</span>(<span>add_pipeline</span>())</pre></div>
<ol start="4" dir="auto">
<li>Insert documents:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="async def upsert_documents():
    documents = [
        {&quot;id&quot;: &quot;1&quot;, &quot;text&quot;: &quot;Korvus is incredibly fast and easy to use.&quot;},
        {&quot;id&quot;: &quot;2&quot;, &quot;text&quot;: &quot;Tomatoes are incredible on burgers.&quot;},
    ]
    await collection.upsert_documents(documents)

asyncio.run(upsert_documents())"><pre><span>async</span> <span>def</span> <span>upsert_documents</span>():
    <span>documents</span> <span>=</span> [
        {<span>"id"</span>: <span>"1"</span>, <span>"text"</span>: <span>"Korvus is incredibly fast and easy to use."</span>},
        {<span>"id"</span>: <span>"2"</span>, <span>"text"</span>: <span>"Tomatoes are incredible on burgers."</span>},
    ]
    <span>await</span> <span>collection</span>.<span>upsert_documents</span>(<span>documents</span>)

<span>asyncio</span>.<span>run</span>(<span>upsert_documents</span>())</pre></div>
<ol start="5" dir="auto">
<li>Perform RAG</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="async def rag():
    query = &quot;Is Korvus fast?&quot;
    print(f&quot;Querying for response to: {query}&quot;)
    results = await collection.rag(
        {
            &quot;CONTEXT&quot;: {
                &quot;vector_search&quot;: {
                    &quot;query&quot;: {
                        &quot;fields&quot;: {&quot;text&quot;: {&quot;query&quot;: query}},
                    },
                    &quot;document&quot;: {&quot;keys&quot;: [&quot;id&quot;]},
                    &quot;limit&quot;: 1,
                },
                &quot;aggregate&quot;: {&quot;join&quot;: &quot;\n&quot;},
            },
            &quot;chat&quot;: {
                &quot;model&quot;: &quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;,
                &quot;messages&quot;: [
                    {
                        &quot;role&quot;: &quot;system&quot;,
                        &quot;content&quot;: &quot;You are a friendly and helpful chatbot&quot;,
                    },
                    {
                        &quot;role&quot;: &quot;user&quot;,
                        &quot;content&quot;: f&quot;Given the context\n:{{CONTEXT}}\nAnswer the question: {query}&quot;,
                    },
                ],
                &quot;max_tokens&quot;: 100,
            },
        },
        pipeline,
    )
    print(results)

asyncio.run(rag())"><pre><span>async</span> <span>def</span> <span>rag</span>():
    <span>query</span> <span>=</span> <span>"Is Korvus fast?"</span>
    <span>print</span>(<span>f"Querying for response to: <span><span>{</span><span>query</span><span>}</span></span>"</span>)
    <span>results</span> <span>=</span> <span>await</span> <span>collection</span>.<span>rag</span>(
        {
            <span>"CONTEXT"</span>: {
                <span>"vector_search"</span>: {
                    <span>"query"</span>: {
                        <span>"fields"</span>: {<span>"text"</span>: {<span>"query"</span>: <span>query</span>}},
                    },
                    <span>"document"</span>: {<span>"keys"</span>: [<span>"id"</span>]},
                    <span>"limit"</span>: <span>1</span>,
                },
                <span>"aggregate"</span>: {<span>"join"</span>: <span>"<span>\n</span>"</span>},
            },
            <span>"chat"</span>: {
                <span>"model"</span>: <span>"meta-llama/Meta-Llama-3-8B-Instruct"</span>,
                <span>"messages"</span>: [
                    {
                        <span>"role"</span>: <span>"system"</span>,
                        <span>"content"</span>: <span>"You are a friendly and helpful chatbot"</span>,
                    },
                    {
                        <span>"role"</span>: <span>"user"</span>,
                        <span>"content"</span>: <span>f"Given the context<span>\n</span>:{{CONTEXT}}<span>\n</span>Answer the question: <span><span>{</span><span>query</span><span>}</span></span>"</span>,
                    },
                ],
                <span>"max_tokens"</span>: <span>100</span>,
            },
        },
        <span>pipeline</span>,
    )
    <span>print</span>(<span>results</span>)

<span>asyncio</span>.<span>run</span>(<span>rag</span>())</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔍 The Power of SQL</h2><a id="user-content--the-power-of-sql" aria-label="Permalink: 🔍 The Power of SQL" href="#-the-power-of-sql"></a></p>
<p dir="auto">While Korvus provides a high-level interface in multiple programming languages, its core operations are built on optimized SQL queries. This approach offers several advantages:</p>
<ul dir="auto">
<li><strong>Transparency</strong>: Advanced users can inspect and understand the underlying queries.</li>
<li><strong>Customizability</strong>: Extend Korvus's capabilities by modifying or adding to its SQL operations.</li>
<li><strong>Performance</strong>: Benefit from PostgreSQL's advanced query optimization capabilities.</li>
</ul>
<p dir="auto">Don't worry if you're not a SQL expert - Korvus's intuitive API abstracts away the complexity while still allowing you to harness the full power of SQL-based operations.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">📘 Documentation</h2><a id="user-content--documentation" aria-label="Permalink: 📘 Documentation" href="#-documentation"></a></p>
<p dir="auto">For comprehensive documentation, including API references, tutorials, and best practices, visit our <a href="https://postgresml.org/docs/open-source/korvus/" rel="nofollow">official documentation</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🌐 Community</h2><a id="user-content--community" aria-label="Permalink: 🌐 Community" href="#-community"></a></p>
<p dir="auto">Join our community to get help, share ideas, and contribute:</p>
<ul dir="auto">
<li><a href="https://discord.gg/DmyJP3qJ7U" rel="nofollow">Discord</a></li>
<li><a href="https://x.com/postgresml" rel="nofollow">Twitter</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🤝 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 🤝 Contributing" href="#-contributing"></a></p>
<p dir="auto">We welcome contributions to Korvus! Please read our <a href="https://github.com/postgresml/korvus/blob/main/CONTRIBUTING.md">Contribution Guidelines</a> before submitting pull requests.</p>
<hr>
<p dir="auto">Korvus is maintained by <a href="https://postgresml.org/" rel="nofollow">PostgresML</a>. For enterprise support and consulting services, please <a href="https://postgresml.org/contact" rel="nofollow">contact us</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Shining actress Shelley Duvall dies at 75 (239 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cy77p22jr5lo</link>
            <guid>40938092</guid>
            <pubDate>Thu, 11 Jul 2024 16:06:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cy77p22jr5lo">https://www.bbc.com/news/articles/cy77p22jr5lo</a>, See on <a href="https://news.ycombinator.com/item?id=40938092">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div data-testid="byline" data-component="byline-block"><p><time>4 hours ago</time></p><div><p><span data-testid="byline-name">By&nbsp;<!-- -->Ian Youngs<!-- -->,&nbsp;<!-- --></span><span>Culture reporter</span></p></div></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/8562/live/3bd49400-24e6-11ef-baa7-25d483663b8e.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/8562/live/3bd49400-24e6-11ef-baa7-25d483663b8e.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/8562/live/3bd49400-24e6-11ef-baa7-25d483663b8e.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/8562/live/3bd49400-24e6-11ef-baa7-25d483663b8e.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/8562/live/3bd49400-24e6-11ef-baa7-25d483663b8e.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/8562/live/3bd49400-24e6-11ef-baa7-25d483663b8e.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/8562/live/3bd49400-24e6-11ef-baa7-25d483663b8e.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/8562/live/3bd49400-24e6-11ef-baa7-25d483663b8e.jpg.webp" loading="eager" alt="Getty Images Shelley Duvall smiling in front of a bright red umbrella"><span>Getty Images</span></p></div><p data-component="caption-block"><figcaption>Shelley Duvall spent 20 years out of the Hollywood spotlight<!-- --></figcaption></p></figure><div data-component="text-block"><p>US actress Shelley Duvall, known for films like The Shining, Annie Hall and Nashville, has died at the age of 75.<!-- --></p><p>Her partner Dan Gilroy <!-- --><a target="_blank" href="https://www.hollywoodreporter.com/movies/movie-news/shelley-duvall-dead-shining-actress-1235946118/">confirmed the news to The Hollywood Reporter<!-- --></a>. <!-- --></p><p>"My dear, sweet, wonderful life partner and friend left us. Too much suffering lately, now she’s free. Fly away, beautiful Shelley," he said, according to the outlet.<!-- --></p><p>She died in her sleep of complications from diabetes at her home in Texas, Gilroy said.<!-- --></p><p>Duvall's other credits included 1977 drama 3 Women, directed by Robert Altman, for which she won the Cannes Film Festival's best actress award and was nominated for a Bafta.<!-- --></p><p>Three years later, she starred as Olive Oyl opposite Robin Williams in Altman's musical version of Popeye.<!-- --></p><p>But Duvall fell out of favour in Hollywood and was off screens for two decades, before making her comeback in 2023's The Forest Hills.<!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/cc8c/live/4ddb4b30-24e6-11ef-a13a-0b8c563da930.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/cc8c/live/4ddb4b30-24e6-11ef-a13a-0b8c563da930.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/cc8c/live/4ddb4b30-24e6-11ef-a13a-0b8c563da930.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/cc8c/live/4ddb4b30-24e6-11ef-a13a-0b8c563da930.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/cc8c/live/4ddb4b30-24e6-11ef-a13a-0b8c563da930.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/cc8c/live/4ddb4b30-24e6-11ef-a13a-0b8c563da930.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/cc8c/live/4ddb4b30-24e6-11ef-a13a-0b8c563da930.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/cc8c/live/4ddb4b30-24e6-11ef-a13a-0b8c563da930.jpg.webp" loading="lazy" alt="Getty Images Shelley Duvall talking to Woody Allen in a shot from Annie Hall"><span>Getty Images</span></p></div><p data-component="caption-block"><figcaption>Duvall's character went on a date with Woody Allen in Annie Hall<!-- --></figcaption></p></figure><div data-component="text-block"><p>With her large brown eyes and offbeat charisma, Duvall was a distinctive and compelling presence.<!-- --></p><p>She began her career, and her association with Altman, in 1970 dark comedy Brewster McCloud, and the pair reunited for McCabe and Mrs Miller in 1971. <!-- --></p><p>After filming her performance as a woman who falls for a 1930s bank robber in their next movie, Thieves Like Us, Altman told her: "I knew you were good, but I didn't know you were great."<!-- --></p><p>She said that remark was "the reason I stuck with it and became an actress".<!-- --></p><p>The director stuck with her, once saying she "was able to swing all sides of the pendulum: charming, silly, sophisticated, pathetic, even beautiful".<!-- --></p><p>Altman cast her again in 1975's Nashville, his satire of US society, politics and country music.<!-- --></p><p>Their next collaboration, 3 Women, saw Duvall play a talkative, trend-following health spa attendant. <!-- --><a target="_blank" href="https://www.theguardian.com/culture/article/2024/jul/11/shelley-duvall-best-films-ranked">The Guardian's Anne Billson ranked it as<!-- --></a> her best role, and "quite simply one of the greatest performances of the 1970s".<!-- --></p><p>Meanwhile, also in 1977, Duvall memorably played Pam, a Rolling Stone reporter who went on a date with Woody Allen's Alvy in Annie Hall.<!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/0a01/live/26031930-24e6-11ef-80aa-699d54c46324.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/0a01/live/26031930-24e6-11ef-80aa-699d54c46324.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/0a01/live/26031930-24e6-11ef-80aa-699d54c46324.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/0a01/live/26031930-24e6-11ef-80aa-699d54c46324.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/0a01/live/26031930-24e6-11ef-80aa-699d54c46324.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/0a01/live/26031930-24e6-11ef-80aa-699d54c46324.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/0a01/live/26031930-24e6-11ef-80aa-699d54c46324.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/0a01/live/26031930-24e6-11ef-80aa-699d54c46324.jpg.webp" loading="lazy" alt="Getty Images Shelley Duvall screaming as an axe comes through a door in a shot from The Shining"><span>Getty Images</span></p></div><p data-component="caption-block"><figcaption>Filming The Shining took its toll on Duvall<!-- --></figcaption></p></figure><div data-component="text-block"><p>Her best-known role was perhaps Wendy, the wife of Jack Nicholson's terrifying hotel caretaker in Stanley Kubrick's 1980 horror classic The Shining.<!-- --></p><p>Filming was an ordeal. "I had to cry 12 hours a day, all day long, the last nine months straight, five or six days a week," she once recalled.<!-- --></p><p>After that, Duvall's film roles included Terry Gilliam's Time Bandits and Roxanne with Steve Martin.<!-- --></p><p>She also set up her own production companies, and made and hosted beloved 1980s children's TV show Faerie Tale Theatre.<!-- --></p><p>Her acting roles diminished in the 1990s, with Jane Campion’s The Portrait of a Lady the pick of the crop, and she dropped off the radar in 2002.<!-- --></p><p>The New York Times attributed her apparent disappearance to the impact of a 1994 earthquake that damaged her Los Angeles home, and the stress of her brother having cancer.<!-- --></p><p>Discussing her prolonged absence from the screen, <!-- --><a target="_blank" href="https://www.nytimes.com/2024/04/25/style/shelley-duvall.html">she told the paper<!-- --></a> in May she had been the victim of a fickle film industry. "I was a star. I had leading roles. People think it's just ageing, but it's not. It's violence," she said.<!-- --></p><p>Asked to explain, she said: "How would you feel if people were really nice, and then, suddenly, on a dime they turn on you?<!-- --></p><p>"You would never believe it unless it happens to you. That's why you get hurt, because you can't really believe it's true."<!-- --></p></div><p data-component="subheadline-block"><h2>'Ultimate film star'<!-- --></h2></p><div data-component="text-block"><p>Concerns about her health were raised when she appeared on the TV talk show Dr Phil in 2016 and told him: "I'm very sick. I need help."<!-- --></p><p>She also talked about receiving messages from a "shapeshifting" Robin Williams following his death, and talked about malevolent forces who were out to do her harm, the paper said.<!-- --></p><p>Speaking about that period, Gilroy told the New York Times she had become "paranoid and just kind of delusional".<!-- --></p><p>Asked by the paper why she had agreed to return to the screen in The Forest Hills, she replied: "I wanted to act again. And then this guy kept calling, and so I wound up doing it."<!-- --></p><p><a target="_blank" href="https://www.ft.com/content/c1717d96-7169-4223-8b17-cbb666be3a4b">Novelist Nicole Flattery wrote in the Financial Times<!-- --></a> in 2023 that her return showed her magic had remained intact.<!-- --></p><p>In an article dubbing her the "ultimate film star", Flattery summed up her talent, writing: "She’s a master at playing characters who act happy when they’re sad, their daffiness masking depth."<!-- --></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to Survive 3 Years in North Korea as a Foreigner (412 pts)]]></title>
            <link>https://mydiplomaticlife.com/how-to-survive-3-years-in-north-korea-as-a-foreigner/</link>
            <guid>40937973</guid>
            <pubDate>Thu, 11 Jul 2024 15:52:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mydiplomaticlife.com/how-to-survive-3-years-in-north-korea-as-a-foreigner/">https://mydiplomaticlife.com/how-to-survive-3-years-in-north-korea-as-a-foreigner/</a>, See on <a href="https://news.ycombinator.com/item?id=40937973">Hacker News</a></p>
Couldn't get https://mydiplomaticlife.com/how-to-survive-3-years-in-north-korea-as-a-foreigner/: Error: Request failed with status code 409]]></description>
        </item>
    </channel>
</rss>