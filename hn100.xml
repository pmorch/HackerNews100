<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 22 Nov 2025 19:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[The privacy nightmare of browser fingerprinting (166 pts)]]></title>
            <link>https://kevinboone.me/fingerprinting.html</link>
            <guid>46016249</guid>
            <pubDate>Sat, 22 Nov 2025 17:08:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kevinboone.me/fingerprinting.html">https://kevinboone.me/fingerprinting.html</a>, See on <a href="https://news.ycombinator.com/item?id=46016249">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">





<p><img src="https://kevinboone.me/img/degoogle.png"></p>
<p>I imagine that most people who take an interest in de-Googling are
concerned about privacy. Privacy on the Internet is a somewhat nebulous
concept, but one aspect of privacy is surely the prevention of your web
browsing behaviour being propagated from one organization to another. I
don’t want my medical insurers to know, for example, that I’ve been
researching coronary artery disease. And even though my personal safety
and liberty probably aren’t at stake, I don’t want to give any support
to the global advertising behemoth, by allowing advertisers access to
better information about me.</p>
<p>Unfortunately, while distancing yourself from Google and its services
might be a necessary first step in protecting your privacy, it’s far
from the last. There’s more to do, and it’s getting harder to do it,
because of browser fingerprinting.</p>
<h2 id="how-we-got-here">How we got here</h2>
<p>Until about five years ago, our main concern surrounding browser
privacy was probably the use of third-party tracking cookies. The
original intent behind cookies was that they would allow a web browser
and a web server to engage in a conversation over a period of time. The
HTTP protocol that web servers use is <em>stateless</em>; that is, each
interaction between browser and server is expected to be complete in
itself. Having the browser and the server exchange a cookie (which could
just be a random number) in each interaction allowed the server to
associate each browser with an ongoing conversation. This was, and is, a
legitimate use of cookies, one that is necessary for almost all
interactive web-based services. If the cookie is short-lived, and only
applies to a single conversation with a single web server, it’s not a
privacy concern.</p>
<p>Unfortunately, web browsers for a long time lacked the ability to
distinguish between privacy-sparing and privacy-breaking uses of
cookies. If many different websites issue pages that contain links to
the same server – usually some kind of advertising service – then the
browser would send cookies to that server, thinking it was being
helpful. This behaviour effectively linked web-based services together,
allowing them to share information about their users. The process is a
bit more complicated than I’m making it out to be, but these third-party
cookies were of such concern that, in Europe at least, legislation was
enacted to force websites to disclose that they were using them.</p>
<p>Browsers eventually got better at figuring out which cookies were
helpful and which harmful and, for the most part, we don’t need to be
too concerned about ‘tracking cookies’ these days. Not only can browsers
mitigate their risks, there’s a far more sinister one: browser
fingerprinting.</p>
<h2 id="browser-fingerprinting">Browser fingerprinting</h2>
<p>Browser fingerprinting does not depend on cookies. It’s resistant, to
some extent, to privacy measures like VPNs. Worst of all, steps that we
might take to mitigate the risk of fingerprinting can actually worsen
the risk. It’s a privacy nightmare, and it’s getting worse.</p>
<p>Fingerprinting works by having the web server extract certain
discrete elements of information from the browser, and combining those
elements into a numerical identifier. Some of the information supplied
by the browser is fundamental and necessary and, although a browser
could fake it, such a measure is likely to break the website.</p>
<p>For example, a fingerprinting system knows, just from information
that my browser always supplies (and probably has to), that I’m using
version 144 of the Firefox browser, on Linux; my preferred language is
English, and my time-zone is GMT. That, by itself, isn’t enough
information to identify me uniquely, but it’s a step towards doing
so.</p>
<p>To get more information, the fingerprinter needs to use more
sophisticated methods which the browser could, in theory, block. For
example, if the browser supports JavaScript – and they nearly all do –
then the fingerprinter can figure out what fonts I have installed, what
browser extensions I use, perhaps even what my hardware is. Worst of
all, perhaps, it can extract a <em>canvas fingerprint</em>. Canvas
fingerprinting works by having the browser run code that draws text
(perhaps invisibly), and then retrieving the individual pixel data that
it drew. This pixel data will differ subtly from one system to another,
even drawing the same text, because of subtle differences in the
graphics hardware and the operating system.</p>
<p>It appears that only about one browser in every thousand share the
same canvas fingerprint. Again, this alone isn’t enough to identify me,
but it’s another significant data point.</p>
<p>Fingerprinting can make use of even what appears to be trivial
information. If, for example, I resize my browser window, the browser
will probably make the next window the same size. It will probably
remember my preference from one day to the next. If the fingerprinter
knows my preferred browser window size is, say, 1287x892 pixels, that
probably narrows down the search for my identify by a factor of a
thousand or more.</p>
<h2 id="why-crude-methods-to-defeat-fingerprinting-dont-work">Why crude
methods to defeat fingerprinting don’t work</h2>
<p>You might think that a simple way to prevent, or at least hamper,
fingerprinting would be simply to disable JavaScript support in the
browser. While this does defeat measures like canvas fingerprinting, it
generates a significant data point of its own: the fact that JavaScript
is disabled. Since almost every web browser in the world now supports
JavaScript, turning it off as a measure to protect privacy is like going
to the shopping mall wearing a ski mask. Sure, it hides your identify;
but nobody’s going to want to serve you in stores. And disabling
JavaScript will break many websites, including some pages on this one,
because I use it to render math equations.</p>
<p>Less dramatic approaches to fingerprinting resistance have their own
problems. For example, a debate has long raged about whether a browser
should actually identify itself at all. The fact that I’m running
Firefox on Linux probably puts me in a small, easily identified group.
Perhaps my browser should instead tell the server I’m running Chrome on
Windows? That’s a much larger group, after all.</p>
<p>The problem is that the fingerprinters can guess the browser and
platform with pretty good accuracy using other methods, whether the
browser reports this information or not. If the browser says something
different to what the fingerprinter infers, we’re back in ski-mask
territory.</p>
<p>What about more subtle methods to spoof the client’s behaviour?
Browsers (or plug-ins) can modify the canvas drawing procedures, for
example, to spoof the results of canvas fingerprinting. Unfortunately,
these methods leave traces of their own, if they aren’t applied subtly.
What’s more, if they’re applied rigorously enough to be effective, they
can break websites that rely on them for normal operation.</p>
<p>All in all, browser fingerprinting is very hard to defeat, and
organizations that want to track us have gotten disturbingly good at
it.</p>
<h2 id="is-there-any-good-news">Is there any good news?</h2>
<p>Not much, frankly.</p>
<p>Before sinking into despondency, it’s worth bearing in mind that
websites that attempt to demonstrate the efficacy of fingerprinting,
like <a href="https://amiunique.org/">amiunique</a> and <a href="https://fingerprint.com/">fingerprint.com</a> do not reflect how
fingerprinting works in the real world. They’re operating on
comparatively small sets of data and, for the most part, they’re not
tracking users over days. Real-world tracking is much harder than these
sites make it out to be. That’s not to say it’s <em>too</em> hard but it
is, at best, a statistical approach, rather than an exact one.</p>
<figure>
<img src="https://kevinboone.me/img/fingerprinting1.png" alt="Oh, bugger. That’s something I don’t want to see from amiunique.org">

</figure>
<p>In addition ‘uniqueness’, in itself, is not a strong measure of
traceability. That my browser fingerprint is unique at some point in
time is irrelevant if my fingerprint will be different tomorrow, whether
it remains unique within the fingerprinter’s database or not.</p>
<p>Of course, these facts also mean that it’s difficult to assess the
effectiveness of our countermeasures: our assessment can only be
approximate, because we don’t actually know what real fingerprinters are
doing.</p>
<p>Another small piece of good news is that browser developers are
starting to realize how much of a hazard fingerprinting is, and to
integrate more robust countermeasures. We don’t necessarily need to
resort to plug-ins and extensions, which are themselves detectable and
become part of the fingerprint. At present, Brave and Mullvad seems to
be doing the most to resist fingerprinting, albeit in different ways.
Librewolf has the same fingerprint resistance as Firefox, but it is
turned on by default. Probably anti-fingerprinting methods will improve
over time but, of course, the fingerprinters will get better at what
they do, too.</p>
<h2 id="so-what-can-we-do">So what can we do?</h2>
<p>First, and most obviously, if you care about avoiding tracking, you
<em>must</em> prevent long-lived cookies hanging around in the browser,
and you <em>must</em> use a VPN. Ideally the VPN should rotate its
endpoint regularly.</p>
<p>The fact that you’re using a VPN, of course, is something that the
fingerprinters will know, and it is does make you stand out.
Sophisticated fingerprinters won’t be defeated by a VPN alone. But if
you don’t use a VPN, the trackers don’t even <em>need</em> to
fingerprint you: your IP number, combined with a few other bits of
routine information, will identify you immediately, and with
near-certainty.</p>
<p>Many browsers can be configured to remove cookies when they seem not
to be in use; Librewolf does this by default, and Firefox and Chrome do
it in ‘incognito’ mode. The downside, of course, is that long-lived
cookies are often used to store authentication status so, if you delete
them, you’ll find yourself having to log in every time you look at a
site that requires authentication. To mitigate this annoyance, browsers
generally allow particular sites to be excluded from their
cookie-burning policies.</p>
<p>Next, you need to be as unremarkable as possible. Fingerprinting is
about uniqueness, so you should use the most popular browser on the most
popular operating system on the kind of hardware you can buy from PC
World. If you’re running the latest Chrome on the latest Windows 11 on a
two-year-old, bog-standard laptop, you’re going to be one of a very
large group. Of course Chrome, being a Google product, has its own
privacy concerns, so you might be better off using a Chromium-based
browser with reduced Google influence, like Brave.</p>
<p>You should endeavour to keep your computer in as near its stock
configuration as possible. Don’t install anything (like fonts) that are
reportable by the browser. Don’t install any extensions, and don’t
change any settings. Use the same ‘light’ theme as everybody else, and
use the browser with a maximized window, and always the same size. And
so on.</p>
<p>If possible, use a browser that has built-in fingerprint resistance,
like Mullvad or Librewolf (or Firefox with these features turned
on).</p>
<p>If you take all these precautions, you can probably reduce the
probability that you can be tracked by you browser fingerprint, over
days or weeks, from about 99% to about 50%.</p>
<p>50% is still too high, of course.</p>
<h2 id="the-downsides-of-resisting-fingerprinting">The downsides of
resisting fingerprinting</h2>
<p>If you enable fingerprinting resistance in Firefox, or use Librewolf,
you’ll immediately encounter oddities. Most obviously, every time you
open a new browser window, it will be the same size. Resizing the window
may have odd results, as the browser will try to constrain certain
screen elements to common size multiples. In addition, you won’t be able
to change the theme.</p>
<p>You’ll probably find yourself facing more ‘CAPTCHA’ and similar
identity challenges, because your browser will be unknown to the server.
Websites don’t do this out of spite: hacking and fraud are rife on the
Internet, and the operators of web-based services are rightly paranoid
about client behaviour.</p>
<p>You’ll likely find that some websites just don’t work properly, in
many small ways: wrong colours, misplaced text, that kind of thing. I’ve
found these issues to be irritations rather than show-stoppers, but you
might discover otherwise.</p>
<h2 id="is-browser-fingerprinting-legal">Is browser fingerprinting
legal?</h2>
<p>The short answer, I think, is that nobody knows, even within a
specific jurisdiction. In the UK, <a href="https://ico.org.uk/about-the-ico/media-centre/news-and-blogs/2024/12/our-response-to-google-s-policy-change-on-fingerprinting/">the
Information Commissioner’s Office takes a dim view of it</a>, and it
probably violates the spirit of the GDPR, if not the letter.</p>
<p>The GDPR is, for the most part, technologically neutral, although it
has specific provisions for cookies, which were a significant concern at
the time it was drafted. So far as I know, nobody has yet challenged
browser fingerprinting under the GDPR, even though it seems to violate
the provisions regarding consent. Since there are legitimate reasons for
fingerprinting, such as hacking detection, organizations that do it
could perhaps defend against a legal challenge on the basis that
fingerprinting is necessary to operate their services safely. In the
end, we really need specific, new legislation to address this privacy
threat.</p>

<p>I suspect that many people who take an interest in Internet privacy
don’t appreciate how hard it is to resist browser fingerprinting. Taking
steps to reduce it leads to inconvenience and, with the present state of
technology, even the most intrusive approaches are only partially
effective. The data collected by fingerprinting is invisible to the
user, and stored somewhere beyond the user’s reach.</p>
<p>On the other hand, browser fingerprinting produces only statistical
results, and usually can’t be used to track or identify a user with
certainty. The data it collects has a relatively short lifespan – days
to weeks, not months or years. While it probably can be used for
sinister purposes, my main concern is that it supports the intrusive,
out-of-control online advertising industry, which has made a wasteland
of the Internet.</p>
<p>In the end, it’s probably only going to be controlled by legislation
and, even when that happens, the advertisers will seek new ways to make
the Internet even more of a hellscape – they always do.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In a U.S. First, New Mexico Opens Doors to Free Child Care for All (237 pts)]]></title>
            <link>https://www.wsj.com/us-news/in-a-u-s-first-new-mexico-opens-doors-to-free-child-care-for-all-2dfdea96</link>
            <guid>46015763</guid>
            <pubDate>Sat, 22 Nov 2025 16:11:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/us-news/in-a-u-s-first-new-mexico-opens-doors-to-free-child-care-for-all-2dfdea96">https://www.wsj.com/us-news/in-a-u-s-first-new-mexico-opens-doors-to-free-child-care-for-all-2dfdea96</a>, See on <a href="https://news.ycombinator.com/item?id=46015763">Hacker News</a></p>
Couldn't get https://www.wsj.com/us-news/in-a-u-s-first-new-mexico-opens-doors-to-free-child-care-for-all-2dfdea96: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Agent design is still hard (260 pts)]]></title>
            <link>https://lucumr.pocoo.org/2025/11/21/agents-are-hard/</link>
            <guid>46013935</guid>
            <pubDate>Sat, 22 Nov 2025 11:27:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lucumr.pocoo.org/2025/11/21/agents-are-hard/">https://lucumr.pocoo.org/2025/11/21/agents-are-hard/</a>, See on <a href="https://news.ycombinator.com/item?id=46013935">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
  

  
  <p data-date="2025-11-21T00:00:00">written on November 21, 2025</p>
  

  <p>I felt like it might be a good time to write about some new things I’ve
learned.  Most of this is going to be about building agents, with a little bit
about using agentic coding tools.</p>
<p>TL;DR: Building agents is still messy.  SDK abstractions break once you hit
real tool use.  Caching works better when you manage it yourself, but differs
between models.  Reinforcement ends up doing more heavy lifting than expected,
and failures need strict isolation to avoid derailing the loop.  Shared state
via a file-system-like layer is an important building block.  Output tooling is
surprisingly tricky, and model choice still depends on the task.</p>
<h2>Which Agent SDK To Target?</h2>
<p>When you build your own agent, you have the choice of targeting an underlying
SDK like the OpenAI SDK or the Anthropic SDK, or you can go with a higher level
abstraction such as the Vercel AI SDK or Pydantic.  The choice we made a while
back was to adopt the Vercel AI SDK but only the provider abstractions, and to
basically <a href="https://ai-sdk.dev/cookbook/node/manual-agent-loop">drive the agent loop
ourselves</a>.  At this point
<a href="https://earendil.com/">we</a> would not make that choice again.  There is
absolutely nothing wrong with the Vercel AI SDK, but when you are trying to
build an agent, two things happen that we originally didn’t anticipate:</p>
<p>The first is that the differences between models are significant enough that
you will need to build your own agent abstraction.  We have not found any of
the solutions from these SDKs that build the right abstraction for an agent.  I
think this is partly because, despite the basic agent design being just a loop,
there are subtle differences based on the tools you provide.  These differences
affect how easy or hard it is to find the right abstraction (cache control,
different requirements for reinforcement, tool prompts, provider-side tools,
etc.).  Because the right abstraction is not yet clear, using the original SDKs
from the dedicated platforms keeps you fully in control.  With some of these
higher-level SDKs you have to build on top of their existing abstractions,
which might not be the ones you actually want in the end.</p>
<p>We also found it incredibly challenging to work with the Vercel SDK when it
comes to dealing with provider-side tools.  The attempted unification of
messaging formats doesn’t quite work.  For instance, the web search tool from
Anthropic routinely destroys the message history with the Vercel SDK, and we
haven’t yet fully figured out the cause.  Also, in Anthropic’s case, cache
management is much easier when targeting their SDK directly instead of the
Vercel one.  The error messages when you get things wrong are much clearer.</p>
<p>This might change, but right now we would probably not use an abstraction when
building an agent, at least until things have settled down a bit.  The benefits
do not yet outweigh the costs for us.</p>
<p>Someone else might have figured it out.  If you’re reading this and think I’m
wrong, please drop me a mail.  I want to learn.</p>
<h2>Caching Lessons</h2>
<p>The different platforms have very different approaches to caching.  A lot has
been said about this already, but Anthropic makes you pay for caching.  It
makes you manage cache points explicitly, and this really changes the way you
interact with it from an agent engineering level.  I initially found the manual
management pretty dumb.  Why doesn’t the platform do this for me?  But I’ve
fully come around and now vastly prefer explicit cache management.  It makes
costs and cache utilization much more predictable.</p>
<p>Explicit caching allows you to do certain things that are much harder
otherwise.  For instance, you can split off a conversation and have it run in
two different directions simultaneously.  You also have the opportunity to do
context editing.  The optimal strategy here is unclear, but you clearly have a
lot more control, and I really like having that control.  It also makes it much
easier to understand the cost of the underlying agent.  You can assume much
more about how well your cache will be utilized, whereas with other platforms
we found it to be hit and miss.</p>
<p>The way we do caching in the agent with Anthropic is pretty straightforward.
One cache point is after the system prompt.  Two cache points are placed at the
beginning of the conversation, where the last one moves up with the tail of the
conversation.  And then there is some optimization along the way that you can
do.</p>
<p>Because the system prompt and the tool selection now have to be mostly static,
we feed a dynamic message later to provide information such as the current
time.  Otherwise, this would trash the cache.  We also leverage reinforcement
during the loop much more.</p>
<h2>Reinforcement In The Agent Loop</h2>
<p>Every time the agent runs a tool you have the opportunity to not just return
data that the tool produces, but also to feed more information back into the
loop.  For instance, you can remind the agent about the overall objective and
the status of individual tasks.  You can also provide hints about how the tool
call might succeed when a tool fails.  Another use of reinforcement is to
inform the system about state changes that happened in the background.  If you
have an agent that uses parallel processing, you can inject information after
every tool call when that state changed and when it is relevant for completing
the task.</p>
<p>Sometimes it’s enough for the agent to self-reinforce.  In Claude Code, for
instance, the todo write tool is a self-reinforcement tool.  All it does is
take from the agent a list of tasks that it thinks it should do and echo out
what came in.  It’s basically just an echo tool; it really doesn’t do anything
else.  But that is enough to drive the agent forward better than if the only
task and subtask were given at the beginning of the context and too much has
happened in the meantime.</p>
<p>We also use reinforcements to inform the system if the environment changed
during execution in a way that’s problematic for the agent.  For instance, if
our agent fails and retries from a certain step forward but the recovery
operates off broken data, we inject a message informing it that it might want
to back off a couple of steps and redo an earlier step.</p>
<h2>Isolate Failures</h2>
<p>If you expect a lot of failures during code execution, there is an opportunity
to hide those failures from the context.  This can happen in two ways.  One is
to run tasks that might require iteration individually.  You would run them in
a subagent until they succeed and only report back the success, plus maybe a
brief summary of approaches that did not work.  It is helpful for an agent to
learn about what did not work in a subtask because it can then feed that
information into the next task to hopefully steer away from those failures.</p>
<p>The second option doesn’t exist in all agents or foundation models, but with
Anthropic you can do context editing.  So far we haven’t had a lot of success
with context editing, but we believe it’s an interesting thing we would love to
explore more.  We would also love to learn if people have success with it.
What is interesting about context editing is that you should be able to
preserve tokens for further down the iteration loop.  You can take out of the
context certain failures that didn’t drive towards successful completion of the
loop, but only negatively affected certain attempts during execution.  But as
with the point I made earlier: it is also useful for the agent to understand
what didn’t work, but maybe it doesn’t require the full state and full output
of all the failures.</p>
<p>Unfortunately, context editing will automatically invalidate caches.  There is
really no way around it.  So it can be unclear when the trade-off of doing that
compensates for the extra cost of trashing the cache.</p>
<h2>Sub Agents / Sub Inference</h2>
<p>As I mentioned a couple of times on this blog already, most of our agents are
based on code execution and code generation.  That really requires a common
place for the agent to store data.  Our choice is a file system—in our case a
virtual file system—but that requires different tools to access it.  This is
particularly important if you have something like a subagent or subinference.</p>
<p>You should try to build an agent that doesn’t have dead ends.  A dead end is
where a task can only continue executing within the sub-tool that you built.
For instance, you might build a tool that generates an image, but is only able
to feed that image back into one more tool.  That’s a problem because you might
then want to put those images into a zip archive using the code execution tool.
So there needs to be a system that allows the image generation tool to write
the image to the same place where the code execution tool can read it.  In
essence, that’s a file system.</p>
<p>Obviously it has to go the other way around too.  You might want to use the
code execution tool to unpack a zip archive and then go back to inference to
describe all the images so that the next step can go back to code execution and
so forth.  The file system is the mechanism that we use for that.  But it does
require tools to be built in a way that they can take file paths to the virtual
file system to work with.</p>
<p>So basically an <code>ExecuteCode</code> tool would have access to the same file system as
the <code>RunInference</code> tool which could take a <code>path</code> to a file on that same
virtual file system.</p>
<h2>The Use Of An Output Tool</h2>
<p>One interesting thing about how we structured our agent is that it does not
represent a chat session.  It will eventually communicate something to the user
or the outside world, but all the messages that it sends in between are usually
not revealed.  The question is: how does it create that message?  We have one
tool which is the output tool.  The agent uses it explicitly to communicate to
the human.  We then use a prompt to instruct it when to use that tool.  In our
case the output tool sends an email.</p>
<p>But that turns out to pose a few other challenges.  One is that it’s
surprisingly hard to steer the wording and tone of that output tool compared to
just using the main agent loop’s text output as the mechanism to talk to the
user.  I cannot say why this is, but I think it’s probably related to how these
models are trained.</p>
<p>One attempt that didn’t work well was to have the output tool run another quick
LLM like Gemini 2.5 Flash to adjust the tone to our preference.  But this
increases latency and actually reduces the quality of the output.  In part, I
think the model just doesn’t word things correctly and the subtool doesn’t have
sufficient context.  Providing more slices of the main agentic context into the
subtool makes it expensive and also didn’t fully solve the problem.  It also
sometimes reveals information in the final output that we didn’t want to be
there, like the steps that led to the end result.</p>
<p>Another problem with an output tool is that sometimes it just doesn’t call the
tool.  One of the ways in which we’re forcing this is we remember if the output
tool was called.  If the loop ends without the output tool, we inject a
reinforcement message to encourage it to use the output tool.</p>
<h2>Model Choice</h2>
<p>Overall our choices for models haven’t dramatically changed so far.  I think
Haiku and Sonnet are still the best tool callers available, so they make for
excellent choices in the agent loop.  They are also somewhat transparent with
regards to what the RL looks like.  The other obvious choices are the Gemini
models.  We so far haven’t found a ton of success with the GPT family of models
for the main loop.</p>
<p>For the individual sub-tools, which in part might also require inference, our
current choice is Gemini 2.5 if you need to summarize large documents or work
with PDFs and things like that.  That is also a pretty good model for
extracting information from images, in particular because the Sonnet family of
models likes to run into a safety filter which can be annoying.</p>
<p>There’s also probably the very obvious realization that token cost alone
doesn’t really define how expensive an agent.  A better tool caller will do the
job in fewer tokens.  There are some cheaper models available than sonnet
today, but they are not <em>necessarily</em> cheaper in a loop.</p>
<p>But all things considered, not that much has changed in the last couple of
weeks.</p>
<h2>Testing and Evals</h2>
<p>We find testing and evals to be the hardest problem here.  This is not entirely
surprising, but the agentic nature makes it even harder.  Unlike prompts, you
cannot just do the evals in some external system because there’s too much you
need to feed into it.  This means you want to do evals based on observability
data or instrumenting your actual test runs.  So far none of the solutions we
have tried have convinced us that they found the right approach here.
Unfortunately, I have to report that at the moment we haven’t found something
that really makes us happy.  I hope we’re going to find a solution for this
because it is becoming an increasingly frustrating aspect of building an agent.</p>
<h2>Coding Agent Updates</h2>
<p>As for my experience with coding agents, not really all that much has changed.
The main new development is that I’m trialing <a href="https://ampcode.com/">Amp</a> more.
In case you’re curious why: it’s not that it’s objectively a better agent than
what I’m using, but I really quite like the way they’re thinking about agents
from what they’re posting.  The interactions of the different sub agents like
the Oracle with the main loop is beautifully done, and not many other harnesses
do this today.  It’s also a good way for me to validate how different agent
designs work.  Amp, similar to Claude Code, really feels like a product built
by people who also use their own tool.  I do not feel every other agent in the
industry does this.</p>
<h2>Quick Stuff I Read And Found</h2>
<p>That’s just a random assortment of things that I feel might also be worth
sharing:</p>
<ul>
<li><a href="https://mariozechner.at/posts/2025-11-02-what-if-you-dont-need-mcp/">What if you don’t need MCP at
all?</a>:
Mario argues that many MCP servers are overengineered and include large
toolsets that consume lots of context.  He proposes a minimalist approach for
browser-agent use-cases by relying on simple CLI tools (e.g., start, navigate,
evaluate JS, screenshot) executed via Bash, which keeps token usage small and
workflows flexible.  I <a href="https://github.com/mitsuhiko/agent-commands/tree/main/skills/web-browser">built a Claude/Amp Skill out of
it</a>.</li>
<li><a href="https://nolanlawson.com/2025/11/16/the-fate-of-small-open-source/">The fate of “small” open
source</a>:
The author argues that the age of tiny, single-purpose open-source libraries is
coming to an end, largely because built-in platform APIs and AI tools can now
generate simple utilities on demand.  <a href="https://lucumr.pocoo.org/2025/1/24/build-it-yourself/">Thank fucking
god</a>.</li>
<li><a href="https://x.com/mitsuhiko/status/1991997262810218983">Tmux is love</a>.  There is
no article that goes with it, but the TLDR is that Tmux is great.  If you
have anything that remotely looks like an interactive system that an agent
should work with, you should <a href="https://github.com/mitsuhiko/agent-commands/tree/main/skills/tmux">give it some Tmux
skills</a>.</li>
<li><a href="https://lucumr.pocoo.org/2025/11/22/llm-apis/">LLM APIs are a Synchronization Problem</a>.  This was a
separate realization that was too long for this post, so I wrote a separate one.</li>
</ul>


  
  <p>This entry was tagged
    
      <a href="https://lucumr.pocoo.org/tags/ai/">ai</a>
  

  </p><p>
    <a href="https://lucumr.pocoo.org/2025/11/21/agents-are-hard.md" id="copy-markdown">copy as</a> / <a href="https://lucumr.pocoo.org/2025/11/21/agents-are-hard.md" id="view-markdown">view</a> markdown
  </p>
  
  

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Superman copy found in mum's attic is most valuable comic ever at $9.12M (296 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c8e9rp0knj6o</link>
            <guid>46012328</guid>
            <pubDate>Sat, 22 Nov 2025 05:21:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c8e9rp0knj6o">https://www.bbc.com/news/articles/c8e9rp0knj6o</a>, See on <a href="https://news.ycombinator.com/item?id=46012328">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="text-block"><p>While cleaning out their late mother's California attic last Christmas, three brothers made a life-changing discovery under a pile of faded newspapers: one of the first Superman comics ever made. </p><p>An original copy of the June 1939 first edition on the Man of Steel's adventures, it was in a remarkably pristine condition.</p><p>Now it has become the highest-priced comic book ever sold, fetching $9.12m (£7m) at auction.  </p><p>Texas-based Heritage Auctions, which hosted Thursday's sale, called it the "pinnacle of comic collecting".</p></div><div data-component="text-block"><p>The brothers found six comic books, including Superman #1, in the attic underneath a stack of newspapers inside a cardboard box and surrounded by cobwebs in 2024, Heritage said in a press release. </p><p>They waited a few months before contacting the auction house, but once they did, Heritage Auctions vice-president Lon Allen visited them in San Francisco within days, according to the auction house. </p><p>The brothers, who have chosen to withhold their names, are "in their 50s and 60s, and their mom had always told them she had an expensive comics collection but never showed them", Mr Allen said in Heritage's press release. </p><p>"It's a twist on the old 'Mom threw away my comics' story."</p><p>Their mother had held on to the comic books since she and her brother bought them between the Great Depression and the beginning of World War Two, Heritage said.</p><p>Mr Allen added that the cool northern California climate was perfect for preserving old paper. </p><p>"If it had been in an attic here in Texas, it would have been ruined," he said.</p><p>That helped CGC, a large third-party comics grading service, give this copy of Superman #1 a 9.0 rating on a 10-point scale, topping the previous record of 8.5. </p><p>And at its sale price of over $9m, including buyer's premium, Superman #1 easily beat the previous highest-priced comic book ever sold by $3m. </p><p>Action Comics No. 1, the 1938 work that first introduced Superman, sold for $6m last year.</p><p>The youngest brother said in Heritage's press release that the box had remained forgotten in the back of attic. </p><p>"As the years unfolded, life brought about a series of losses and changes," he said. "The demands of everyday survival took centre stage, and the box of comics, once set aside with care and intention, was forgotten. Until last Christmas."</p><p>He added: "This isn't simply a story about old paper and ink. This was never just about a collectible. </p><p>"This is a testament to memory, family and the unexpected ways the past finds its way back to us."</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Moss Survives 9 Months in Space Vacuum (138 pts)]]></title>
            <link>https://scienceclock.com/moss-survives-9-months-in-space-vacuum/</link>
            <guid>46011978</guid>
            <pubDate>Sat, 22 Nov 2025 03:57:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scienceclock.com/moss-survives-9-months-in-space-vacuum/">https://scienceclock.com/moss-survives-9-months-in-space-vacuum/</a>, See on <a href="https://news.ycombinator.com/item?id=46011978">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
<p>Mosses are already known for coping with harsh radiation, dehydration, and long <a href="https://scienceclock.com/mars-ice-could-preserve-traces-of-ancient-life-study-suggests/">freezes</a>. Now scientists have pushed them even further by exposing their spore capsules to open space for nine months, and most of them survived.</p>



<p>The team worked with spreading earthmoss (<a href="https://en.wikipedia.org/wiki/Physcomitrella_patens" target="_blank" rel="noopener"><em>Physcomitrium patens</em></a>), a small moss species used widely as a plant model by researchers. Its spore-containing capsules were mounted on the outside of the International Space Station (ISS), where they experienced direct solar radiation, vacuum conditions, and sharp temperature swings during each orbit.</p>



<p>Under those conditions, cells usually break down quickly. So the researchers were surprised by what came back. “We expected almost zero survival, but the result was the opposite,” <a href="https://www.eurekalert.org/news-releases/1105940?" target="_blank" rel="noopener">says</a> Hokkaido University biologist Tomomichi Fujita. More than 80 percent of the spores still germinated once they returned to Earth.</p>



<p><strong>Also Read: </strong><a href="https://scienceclock.com/microbe-that-could-turn-martian-dust-into-oxygen/"><strong>Microbe That Could Turn Martian Dust into Oxygen</strong></a></p>




<p>The team detected a small drop in chlorophyll a, but the other pigments remained stable. The spores grew normally in follow-up tests, showing no signs of major stress from their time in orbit.</p>



<p>This kind of toughness fits with the evolutionary history of mosses. <a href="https://en.wikipedia.org/wiki/Bryophyte" target="_blank" rel="noopener">Bryophytes</a> — the group that includes mosses, liverworts, and hornworts — were among the first plants to move from water onto land about 500 million years ago. Their spores had to withstand drying and direct sunlight long before soils existed, which may explain why their protective structures still hold up so well today.</p>


<div>
<figure><img decoding="async" width="525" height="700" src="https://scienceclock.com/wp-content/uploads/2025/11/Low-Res_Germinated-moss-spores-after-space-exposure-CREDIT-Dr.-Chang-hyun-Maeng-and-Maika-Kobayashi-rotated.jpg" alt="Low-Res_Germinated moss spores after space exposure CREDIT Dr. Chang-hyun Maeng and Maika Kobayashi" srcset="https://scienceclock.com/wp-content/uploads/2025/11/Low-Res_Germinated-moss-spores-after-space-exposure-CREDIT-Dr.-Chang-hyun-Maeng-and-Maika-Kobayashi-rotated.jpg 525w, https://scienceclock.com/wp-content/uploads/2025/11/Low-Res_Germinated-moss-spores-after-space-exposure-CREDIT-Dr.-Chang-hyun-Maeng-and-Maika-Kobayashi-225x300.jpg 225w" sizes="(max-width: 525px) 100vw, 525px"><figcaption>Germinated moss spores after their time in open space (Image: Dr. Chang-hyun Maeng and Maika Kobayashi)</figcaption></figure></div>


<p>The results place moss spores alongside the few organisms known to tolerate direct <a href="https://scienceclock.com/category/space/">space</a> exposure, including tardigrades and certain microbes. Their survival also adds to ongoing discussions about what types of <a href="https://scienceclock.com/category/life/">life</a> might endure extreme environments beyond Earth.</p>



<p>According to the researchers, this durability could matter for future experiments on the Moon or Mars. Mosses need very little soil and can pull nutrients directly from rock, making them candidates for early ecosystem tests in <a href="https://scienceclock.com/aliens-got-bored-and-stopped-searching-humans-says-scientist/">extraterrestrial</a> settings.</p>




<p>“Ultimately, we hope this work opens a new frontier toward constructing ecosystems in extraterrestrial environments such as&nbsp;the <a href="https://scienceclock.com/saturns-moon-enceladus-signals-organic-molecules-key-to-lifes-chemistry/">Moon</a>&nbsp;and&nbsp;Mars,”&nbsp;<a href="https://www.eurekalert.org/news-releases/1105940?" target="_blank" rel="noopener">says</a>&nbsp;Fujita.</p>



<p>The research was published in <em>iScience</em>. Read the study <a href="https://doi.org/10.1016/j.isci.2025.113827" target="_blank" rel="noopener">here</a>.</p>



<hr>







<!-- CONTENT END 1 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sharper MRI scans may be on horizon thanks to new physics-based model (127 pts)]]></title>
            <link>https://news.rice.edu/news/2025/sharper-mri-scans-may-be-horizon-thanks-new-physics-based-model</link>
            <guid>46010806</guid>
            <pubDate>Sat, 22 Nov 2025 00:30:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.rice.edu/news/2025/sharper-mri-scans-may-be-horizon-thanks-new-physics-based-model">https://news.rice.edu/news/2025/sharper-mri-scans-may-be-horizon-thanks-new-physics-based-model</a>, See on <a href="https://news.ycombinator.com/item?id=46010806">Hacker News</a></p>
Couldn't get https://news.rice.edu/news/2025/sharper-mri-scans-may-be-horizon-thanks-new-physics-based-model: Error: Request failed with status code 406]]></description>
        </item>
        <item>
            <title><![CDATA[How I learned Vulkan and wrote a small game engine with it (2024) (158 pts)]]></title>
            <link>https://edw.is/learning-vulkan/</link>
            <guid>46010329</guid>
            <pubDate>Fri, 21 Nov 2025 23:28:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://edw.is/learning-vulkan/">https://edw.is/learning-vulkan/</a>, See on <a href="https://news.ycombinator.com/item?id=46010329">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<article>
    <ul>
<li>
<p><a href="https://github.com/eliasdaler/edw.is/discussions/7">Comments (GitHub discussion)</a></p>
</li>
<li>
<p><a href="https://news.ycombinator.com/item?id=40595741">Comments (Hacker News)</a></p>
</li>
</ul>
<p><strong>tl;dr</strong>: I learned some Vulkan and made a game engine with two small game demos in 3 months.</p>
<p>The code for the engine and the games can be found here: <a href="https://github.com/eliasdaler/edbr">https://github.com/eliasdaler/edbr</a></p>
<figure><img src="https://edw.is/learning-vulkan/resources/frame_analysis/final.jpg">
</figure>

<figure><img src="https://edw.is/learning-vulkan/resources/mtp_screenshot2.png">
</figure>

<figure><img src="https://edw.is/learning-vulkan/resources/platformer_screenshot.png">
</figure>

<hr>
<div>
    <h2>Table Of Contents</h2>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#preface">Preface</a></li>
    <li><a href="#learning-graphics-programming">Learning graphics programming</a></li>
    <li><a href="#bike-shedding-and-how-to-avoid-it">Bike-shedding and how to avoid it</a></li>
    <li><a href="#why-vulkan">Why Vulkan?</a></li>
    <li><a href="#learning-vulkan">Learning Vulkan</a></li>
    <li><a href="#engine-overview-and-frame-analysis">Engine overview and frame analysis</a></li>
    <li><a href="#general-advice">General advice</a>
      <ul>
        <li><a href="#recommended-vulkan-libraries">Recommended Vulkan libraries</a></li>
        <li><a href="#gfxdevice-abstraction">GfxDevice abstraction</a></li>
        <li><a href="#handling-shaders">Handling shaders</a></li>
        <li><a href="#push-constants-descriptor-sets-and-bindless-descriptors">Push constants, descriptor sets and bindless descriptors</a></li>
        <li><a href="#pipeline-pattern">Pipeline pattern</a></li>
        <li><a href="#using-programmable-vertex-pulling-pvp--buffer-device-address-bda">Using programmable vertex pulling (PVP) + buffer device address (BDA)</a></li>
        <li><a href="#bindless-descriptors">Bindless descriptors</a></li>
        <li><a href="#handling-dynamic-data-which-needs-to-be-uploaded-every-frame">Handling dynamic data which needs to be uploaded every frame</a></li>
        <li><a href="#destructors-deletion-queue-and-cleanup">Destructors, deletion queue and cleanup</a></li>
        <li><a href="#synchronization">Synchronization</a></li>
      </ul>
    </li>
    <li><a href="#more-implementation-notes">More implementation notes</a>
      <ul>
        <li><a href="#drawing-many-sprites">Drawing many sprites</a></li>
        <li><a href="#compute-skinning">Compute skinning</a></li>
        <li><a href="#game--renderer-separation">Game / renderer separation</a></li>
        <li><a href="#scene-loading-and-entity-prefabs">Scene loading and entity prefabs</a></li>
        <li><a href="#msaa">MSAA</a></li>
        <li><a href="#ui">UI</a></li>
        <li><a href="#dear-imgui-and-srgb-issues">Dear ImGui and sRGB issues</a></li>
        <li><a href="#other-stuff">Other stuff</a></li>
      </ul>
    </li>
    <li><a href="#what-i-gained-from-switching-to-vulkan">What I gained from switching to Vulkan</a></li>
    <li><a href="#future-work">Future work</a></li>
  </ul>
</nav>
</div>

<p>This article documents my experience of learning Vulkan and writing a small game/engine with it. It took me around 3 months to do it without any previous knowledge of Vulkan (I had previous OpenGL experience and some experience with making game engines, though).</p>
<figure><img src="https://edw.is/learning-vulkan/resources/mtp_screenshot1.jpg">
</figure>

<p>The engine wasn’t implemented as a general purpose engine, which is probably why it took me a few months (and not years) to achieve this. I started by making a small 3D game and separated reusable parts into the “engine” afterwards. I can recommend everyone to follow the same process to not get stuck in the weeds (see “Bike-shedding” section below for more advice).</p>
<h2 id="preface">Preface</h2>
<p>I’m a professional programmer, but I’m self-taught in graphics programming. I started studying graphics programming around 1.5 years ago by learning OpenGL and writing a 3D engine in it.</p>
<p>The engine I wrote in Vulkan is mostly suited for smaller level-based games. I’ll explain things which worked for me, but they might not be the most efficient. My implementation would probably still be a good starting point for many people.</p>
<blockquote>
  Hopefully, this article will help make some things about Vulkan clearer to you. But you also need to be patient. It took me <em>months</em> to implement what I have today and I did it by cutting corners in many places. But if a self-taught programmer like me can build something with Vulkan, then so can you!
</blockquote>

<h2 id="learning-graphics-programming">Learning graphics programming</h2>
<blockquote>
  This is a very high level overview of how I learned some graphics programming myself. If there’s interest, I might write another article with more resources and helpful guidelines.
</blockquote>

<p>If you haven’t done any graphics programming before, you should start with OpenGL. It’s much easier to learn it and not get overwhelmed by all the complexity that Vulkan has. A lot of your OpenGL and graphics programming knowledge will be useful when you start doing things with Vulkan later.</p>
<p>Ideally, you should at least get a textured model displayed on the screen with some simple Blinn-Phong lighting. I can also recommend doing some basic shadow mapping too, so that you learn how to render your scene from a different viewpoint and to a different render target, how to sample from depth textures and so on.</p>
<p>I can recommend using the following resources to learn OpenGL:</p>
<ul>
<li><a href="https://learnopengl.com/">https://learnopengl.com/</a></li>
<li><a href="https://capnramses.itch.io/antons-opengl-4-tutorials">Anton’s OpenGL 4 Tutorials book</a></li>
<li><a href="https://www.youtube.com/playlist?list=PL8vNj3osX2PzZ-cNSqhA8G6C1-Li5-Ck8">Thorsten Thormählen’s lectures</a> lectures (watch the first 6 videos, the rest might be a bit too advanced)</li>
</ul>
<p>Sadly, most OpenGL resources don’t teach the latest OpenGL 4.6 practices. They make writing OpenGL a lot more enjoyable. If you learn them, transitioning to Vulkan will be much easier (I only learned about OpenGL 3.3 during my previous engine development, though, so it’s not a necessity).</p>
<p>Here are some resources which teach you the latest OpenGL practices:</p>
<ul>
<li><a href="https://juandiegomontoya.github.io/modern_opengl.html">https://juandiegomontoya.github.io/modern_opengl.html</a></li>
<li><a href="https://github.com/fendevel/Guide-to-Modern-OpenGL-Functions">https://github.com/fendevel/Guide-to-Modern-OpenGL-Functions</a></li>
</ul>
<blockquote>
  It’s also good to have some math knowledge, especially linear algebra: how to work with vectors, transformation matrices and quaternions. My favorite book about linear algebra/math is <a href="https://www.gamemath.com/book/intro.html">3D Math Primer for Graphics and Game Development by F. Dunn and I. Parbery</a>. You don’t need to read it all in one go - use it as a reference if some math in the OpenGL resources above doesn’t make sense to you.
</blockquote>

<h2 id="bike-shedding-and-how-to-avoid-it">Bike-shedding and how to avoid it</h2>
<p><a href="https://en.wikipedia.org/wiki/Law_of_triviality">https://en.wikipedia.org/wiki/Law_of_triviality</a></p>
<p>Ah, bike-shedding… Basically, it’s a harmful pattern of overthinking and over-engineering even the simplest things. It’s easy to fall into this trap when doing graphics programming (<em>especially</em> when doing Vulkan since you need to make many choices when implementing an engine with it).</p>
<ul>
<li>Always ask yourself “Do I <em>really</em> need this?”, “Will this thing ever become a bottleneck?”.</li>
<li>Remember that you can always rewrite any part of your game/engine later.</li>
<li>Don’t implement something unless you need it <strong>right now</strong>. Don’t think “Well, a good engine needs X, right…?”.</li>
<li>Don’t try to make a general purpose game engine. It’s probably even better to not think about “the engine” at first and write a simple game.</li>
<li>Make a small game first - a Breakout clone, for example. Starting your engine development by doing a Minecraft clone with multiplayer support is probably not a good idea.</li>
<li>Be wary of people who tend to suggest complicated solutions to simple problems.</li>
<li>Don’t look too much at what other people do. I’ve seen many over-engineered engines on GitHub - sometimes they’re that complex for a good reason (and there are <em>years</em> of work behind them). But you probably don’t need most of that complexity, especially for simpler games.</li>
<li>Don’t try to make magical wrappers around Vulkan interfaces prematurely, especially while you’re still learning Vulkan.</li>
</ul>
<p>Get it working first. Leave “TODO”/“FIXME” comments in some places. Then move on to the next thing. Try to fix “TODO”/“FIXME” places only when they really become problematic or bottleneck your performance. You’ll be surprised to see how many things won’t become a problem at all.</p>
<blockquote>
  Some of this advice only applies when you’re working alone on a hobby project. Of course, it’s much harder to rewrite something from scratch when others start to depend on it and a “temp hack” becomes a fundamental part of the engine which is very hard to change without breaking many things.
</blockquote>

<h2 id="why-vulkan">Why Vulkan?</h2>
<blockquote>
  <p>Ask yourself if you need to learn a graphics API at all. If your main goal is to make a game as soon as possible, then you might be better off using something like Godot or Unreal Engine.</p>
<p>However, there’s nothing wrong with reinventing the wheel or doing something from scratch. Especially if you do it just for fun, to get into graphics programming or to get an in-depth knowledge about how something works.</p>

</blockquote>

<p>The situation with graphic APIs in 2024 is somewhat complicated. It all depends on the use case: DirectX seems like the most solid choice for most AAA games. WebGL or WebGPU are the only two choices for doing 3D graphics on the web. Metal is the go-to graphics API on macOS and iOS (though you can still do Vulkan there via MoltenVK).</p>
<p>My use case is simple: I want to make small 3D games for desktop platforms (Windows and Linux mostly). I also love open source technology and open standards. So, it was a choice between OpenGL and Vulkan for me.</p>
<p>OpenGL is a good enough choice for many small games. But it’s very unlikely that it’ll get new versions in the future (so you can’t use some newest GPU capabilities like ray tracing), it’s deprecated on macOS and its future is uncertain.</p>
<p>WebGPU was also a possible choice. Before learning Vulkan, I <a href="https://github.com/eliasdaler/webgpu-learning">learned some of it</a>. It’s a pretty solid API, but I had some problems with it:</p>
<ul>
<li>It’s still not stable and there’s not a lot of tutorials and examples for it. <a href="https://eliemichel.github.io/LearnWebGPU/">This tutorial</a> is fantastic, though.</li>
<li>WGSL is an okay shading language, but I just find its syntax not as pleasant as GLSL’s (note that you can write in GLSL and then load compiled SPIR-V on WebGPU native).</li>
<li>On desktop, it’s essentially a wrapper around other graphic APIs (DirectX, Vulkan, Metal).This introduces additional problems for me:
<ul>
<li>It can’t do things some things that Vulkan or DirectX can do.</li>
<li>It has more limitations than native graphic APIs since it needs to behave similarly between them.</li>
<li>RenderDoc captures become confusing as they differ between the platforms (you can get DirectX capture on Windows and Vulkan capture on Linux) and you don’t have 1-to-1 mapping between WebGPU calls and native API calls.</li>
<li>Using Dawn and WGPU feels like using bgfx or sokol. You don’t get the same degree of control over the GPU and some of the choices/abstractions might not be the most pleasant for you.</li>
</ul>
</li>
<li>No bindless textures (WIP discussion <a href="https://github.com/gpuweb/gpuweb/issues/380">here</a>).</li>
<li>No push constants (WIP discussion <a href="https://github.com/gpuweb/gpuweb/issues/75">here</a>).</li>
</ul>
<p>Still, I think that WebGPU is a better API than OpenGL/WebGL and can be more useful to you than Vulkan in some use cases:</p>
<ul>
<li>Validation errors are much better than in OpenGL/WebGL and not having global state helps a lot.</li>
<li>It’s also kind of similar to Vulkan in many things, so learning a bit of it before diving into Vulkan also helped me a lot.</li>
<li>It requires a lot less boilerplate to get things on the screen (compared to Vulkan).</li>
<li>You don’t have to deal with explicit synchronization which makes things much simpler.</li>
<li>You can make your games playable inside the browser.</li>
</ul>
<h2 id="learning-vulkan">Learning Vulkan</h2>
<p>Learning Vulkan seemed like an impossible thing for me previously. It felt like you needed to have many years of AAA game graphics programming experience to be able to do things in it. You also hear people saying “you’re basically writing a graphics driver when writing in Vulkan” which also made Vulkan sounds like an incredibly complicated thing.</p>
<p>I have also checked out some engines written in Vulkan before and was further demotivated by seeing tons of scary abstractions and files named like <code>GPUDevice.cpp</code> or <code>GPUAbstraction.cpp</code> which had thousands of lines of scary C++ code.</p>
<p>The situation has changed over the years. Vulkan is not as complicated as it was before. First of all, Khronos realized that some parts of Vulkan were indeed very complex and introduced some newer features which made many things much simpler (for example, dynamic rendering). Secondly, some very useful libraries which reduce boilerplate were implemented. And finally, there are a lot of fantastic resources which make learning Vulkan much easier than it was before.</p>
<p>The best Vulkan learning resource which helped me get started was <a href="https://vkguide.dev/">vkguide</a>. If you’re starting from scratch, just go through it all (you might stop at “GPU driver rendering” chapter at first - many simple games probably won’t need this level of complexity)</p>
<p><a href="https://www.youtube.com/playlist?list=PLmIqTlJ6KsE1Jx5HV4sd2jOe3V1KMHHgn">Vulkan Lecture Series by TU Wien</a> also nicely teaches Vulkan basics (you can probably skip “Real-Time Ray Tracing” chapter for now). I especially found a lecture on synchronization very helpful.</p>
<p>Here are some more advanced Vulkan books that also helped me:</p>
<ul>
<li><strong>3D Graphics Rendering Cookbook by Sergey Kosarevsky and Viktor Latypov</strong>. There is the second edition in the writing and it’s promising to be better than the first one. The second edition is not released yet, but the source code for it can be found here: <a href="https://github.com/PacktPublishing/3D-Graphics-Rendering-Cookbook-Second-Edition">https://github.com/PacktPublishing/3D-Graphics-Rendering-Cookbook-Second-Edition</a></li>
<li><strong>Mastering Graphics Programming with Vulkan by Marco Castorina, Gabriel Sassone</strong>. Very advanced book which explains some of the “cutting edge” graphics programming concepts (I mostly read it to understand where to go further, but didn’t have time to implement most of it). The source code for it can be found here: <a href="https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan">https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan</a></li>
</ul>
<p>Here’s the result of my first month of learning Vulkan:</p>
<figure><img src="https://edw.is/learning-vulkan/resources/mtp_screenshot_one_month.png">
</figure>

<p>By this point I had:</p>
<ul>
<li>glTF model loading</li>
<li>Compute skinning</li>
<li>Frustum culling</li>
<li>Shadow mapping and cascaded shadow maps</li>
</ul>
<p>Of course, doing it for the 3rd time (I had it implemented it all in OpenGL and WebGPU before) certainly helped. Once you get to this point, Vulkan won’t seem as scary anymore.</p>
<p>Let’s see how the engine works and some useful things I learned.</p>
<h2 id="engine-overview-and-frame-analysis">Engine overview and frame analysis</h2>
<p><a href="https://github.com/eliasdaler/edbr">https://github.com/eliasdaler/edbr</a></p>
<p>My engine is called EDBR (Elias Daler’s Bikeshed Engine) and was initially started as a project for learning Vulkan. It quickly grew into a somewhat usable engine which I’m going to use for my further projects.</p>
<blockquote>
  <p>At the time of writing this article, the source code line counts are as follows:</p>
<ul>
<li>Engine itself: 19k lines of code
<ul>
<li>6.7k LoC related to graphics,</li>
<li>2k LoC are light abstractions around Vulkan</li>
</ul>
</li>
<li>3D cat game: 4.6k LoC</li>
<li>2D platformer game: 1.2k LoC</li>
</ul>

</blockquote>

<p>I copy-pasted some non-graphics related stuff from my previous engine (e.g. input handling and audio system) but all of the graphics and many other core systems were rewritten from scratch. I feel like it was a good way to do it instead of trying to cram Vulkan into my old OpenGL abstractions.</p>
<blockquote>
  You can follow the commit history which shows how I started from clearing the screen, drawing the first triangle, drawing a textured quad and so on. It might be easier to understand the engine when it was simpler and smaller.
</blockquote>

<p>Let’s see how this frame in rendered:</p>
<figure><img src="https://edw.is/learning-vulkan/resources/frame_analysis/final.jpg">
</figure>

<blockquote>
  Most of the steps will be explained in more detail below.
</blockquote>

<ul>
<li>Skinning</li>
</ul>
<p>First, models with skeletal animations are skinned in the compute shader. The compute shader takes unskinned mesh and produces a buffer of vertices which are then used instead of the original mesh in later rendering steps. This allows me to treat static and skinned meshes similarly in shaders and not do skinning repeatedly in different rendering steps.</p>
<ul>
<li>CSM (Cascaded Shadow Mapping)</li>
</ul>
<p>I use a 4096x4096 depth texture with 3 slices for cascaded shadow mapping. The first slice looks like this:</p>
<figure><img src="https://edw.is/learning-vulkan/resources/frame_analysis/csm_slice.png">
</figure>

<ul>
<li>Geometry + shading</li>
</ul>
<p>All the models are drawn and shading is calculated using the shadow map and light info. I use a PBR model which is almost identical to the one described in <a href="https://google.github.io/filament/Filament.md.html">Physically Based Rendering in Filament</a>. The fragment shader is quite big and does calculation for all the lights affecting the drawn mesh in one draw call:</p>
<figure><img src="https://edw.is/learning-vulkan/resources/frame_analysis/geometry.jpg">
</figure>

<p>Everything is drawn into a multi-sampled texture. Here’s how it looks after resolve:</p>
<figure><img src="https://edw.is/learning-vulkan/resources/frame_analysis/geometry_resolve.jpg">
</figure>

<p>(Open the previous two screenshots in the next tab and flip between the tabs to see the difference more clearly)</p>
<ul>
<li>Depth resolve</li>
</ul>
<p>Depth resolve step is performed manually via a fragment shader. I just go through all the fragments of multi-sample depth texture and write the minimum value into the non-MS depth texture (it’ll be useful in the next step).</p>
<ul>
<li>Post FX</li>
</ul>
<figure><img src="https://edw.is/learning-vulkan/resources/frame_analysis/post_fx.jpg">
</figure>

<p>Some post FX is applied - right now it’s only depth fog (I use “depth resolve” texture from the previous step here), afterwards tone-mapping and bloom will also be done here.</p>
<ul>
<li>UI</li>
</ul>
<p>Dialogue UI is drawn. Everything is done in one draw call (more is explained in “Drawing many sprites” section)</p>
<figure><img src="https://edw.is/learning-vulkan/resources/frame_analysis/final.jpg">
</figure>

<p>And that’s it! It’s pretty basic right now and would probably become much more complex in the future (see “Future work” section).</p>
<h2 id="general-advice">General advice</h2>
<h3 id="recommended-vulkan-libraries">Recommended Vulkan libraries</h3>
<p>There are a couple of libraries which greatly improve the experience of writing Vulkan. Most of them are already used in vkguide, but I still want to highlight how helpful they were to me.</p>
<ul>
<li>vk-bootstrap - <a href="https://github.com/charles-lunarg/vk-bootstrap">https://github.com/charles-lunarg/vk-bootstrap</a></li>
</ul>
<p>vk-bootstrap simplifies a lot of Vulkan boilerplate: physical device selection, swapchain creation and so on.</p>
<p>I don’t like big wrappers around graphic APIs because they tend to be very opinionated. Plus, you need to keep a mental map of “wrapper function vs function in the API spec” in your head at all times.</p>
<p>Thankfully, vk-bootstrap is not like this. It mostly affects the initialization step of your program and doesn’t attempt to be a wrapper around every Vulkan function.</p>
<blockquote>
  When I was learning Vulkan, I started doing Vulkan from scratch, without using any 3rd party libraries. Replacing big amounts of the initialization code with vk-bootstrap was a joy. It’s really worth it.
</blockquote>

<ul>
<li>Vulkan Memory Allocator (VMA) - <a href="https://github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator">https://github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator</a></li>
</ul>
<p>I’ll be honest, I used VMA without even learning about how to allocate memory in Vulkan manually. I read about it in the Vulkan spec later - I’m glad that I didn’t have to do it on my own.</p>
<ul>
<li>volk</li>
</ul>
<p>Volk was very useful for me for simplifying extension function loading. For example, if you want to use very useful <code>vkSetDebugUtilsObjectNameEXT</code> for setting debug names for your objects (useful for RenderDoc captures and validation errors), you’ll need to do this if you don’t use volk:</p>
<pre><code>// store this pointer somewhere
PFN_vkSetDebugUtilsObjectNameEXT pfnSetDebugUtilsObjectNameEXT;

// during your game init
pfnSetDebugUtilsObjectNameEXT = (PFN_vkSetDebugUtilsObjectNameEXT)
    vkGetInstanceProcAddr(instance, "vkSetDebugUtilsObjectNameEXT");

// and finally in your game code
pfnSetDebugUtilsObjectNameEXT(device, ...);
</code></pre>
<p>With volk, all the extensions are immediately loaded after you call <code>volkInitialize</code> and you don’t need to store these pointers everywhere. You just include <code>volk.h</code> and call <code>vkSetDebugUtilsObjectNameEXT</code> - beautiful!</p>
<h3 id="gfxdevice-abstraction">GfxDevice abstraction</h3>
<p>I have a <code>GfxDevice</code> class which encapsulates most of the commonly used functionality and stores many objects that you need for calling Vulkan functions (<code>VkDevice</code>, <code>VkQueue</code> and so on). A single <code>GfxDevice</code> instance is created on the startup and then gets passed around.</p>
<p>It handles:</p>
<ul>
<li>Vulkan context initialization.</li>
<li>Swapchain creation and management.</li>
<li><code>beginFrame</code> returns a new <code>VkCommandBuffer</code> which is later used in all the drawing steps.</li>
<li><code>endFrame</code> does drawing to the swapchain and does sync between the frames.</li>
<li>Image creation and loading textures from files.</li>
<li>Buffer creation.</li>
<li>Bindless descriptor set management (see “Bindless descriptors” section below).</li>
</ul>
<p>That’s… a lot of things. However, it’s not that big: <code>GfxDevice.cpp</code> is only 714 lines at the time of writing this article. It’s more convenient to pass one object into the function instead of many (<code>VkDevice</code>, <code>VkQueue</code>, <code>VmaAllocator</code> and so on).</p>
<h3 id="handling-shaders">Handling shaders</h3>
<p>In Vulkan, you can use any shading language which compiles to SPIR-V - that means that you can use GLSL, HLSL and others. I chose GLSL because I already knew it from my OpenGL experience.</p>
<p>You can pre-compile your shaders during the build step or compile them on the fly. I do it during the build so that my shader loading runtime code is simpler. I also don’t have an additional runtime dependency on the shader compiler. Also, shader errors are detected during the build step and I don’t get compile errors during the runtime.</p>
<p>I use glslc (from <a href="https://github.com/google/shaderc">shaderc</a> project, it’s included in Vulkan SDK) which allows you to specify a <code>DEPFILE</code> in CMake which is incredibly useful when you use shader includes. If you change a shader file, all files which include it are recompiled automatically. Without the <code>DEPFILE</code>, CMake won’t be able to see which files shader files need to be recompiled and will only recompile the file which was changed.</p>
<p>My CMake script for building shaders looks like this:</p>
<pre><code>function (target_shaders target shaders)
    set(SHADERS_BUILD_DIR "${CMAKE_CURRENT_BINARY_DIR}/shaders")
    file(MAKE_DIRECTORY "${SHADERS_BUILD_DIR}")
    foreach (SHADER_PATH ${SHADERS})
        get_filename_component(SHADER_FILENAME "${SHADER_PATH}" NAME)
        set(SHADER_SPIRV_PATH "${SHADERS_BUILD_DIR}/${SHADER_FILENAME}.spv")
        set(DEPFILE "${SHADER_SPIRV_PATH}.d")
        add_custom_command(
          COMMENT "Building ${SHADER_FILENAME}"
          OUTPUT "${SHADER_SPIRV_PATH}"
          COMMAND ${GLSLC} "${SHADER_PATH}" -o "${SHADER_SPIRV_PATH}" -MD -MF ${DEPFILE} -g
          DEPENDS "${SHADER_PATH}"
          DEPFILE "${DEPFILE}"
        )
        list(APPEND SPIRV_BINARY_FILES ${SHADER_SPIRV_PATH})
    endforeach()

    set(shaders_target_name "${target}_build_shaders")
    add_custom_target(${shaders_target_name}
      DEPENDS ${SPIRV_BINARY_FILES}
    )
    add_dependencies(${target} ${shaders_target_name})
endfunction()
</code></pre>
<p>and then in the main CMakeLists file:</p>
<pre><code>set(SHADERS
    skybox.frag
    skinning.comp
    ... // etc
)

# prepend shaders directory path
get_target_property(EDBR_SOURCE_DIR edbr SOURCE_DIR)
set(EDBR_SHADERS_DIR "${EDBR_SOURCE_DIR}/src/shaders/")
list(TRANSFORM SHADERS PREPEND "${EDBR_SHADERS_DIR}")

target_shaders(game ${SHADERS})
</code></pre>
<p>Now, when you build a <code>game</code> target, shaders get built automatically and the resulting SPIR-V files are put into the binary directory.</p>
<h3 id="push-constants-descriptor-sets-and-bindless-descriptors">Push constants, descriptor sets and bindless descriptors</h3>
<p>Passing data to shaders in OpenGL is much simpler than it is in Vulkan. In OpenGL, you could just do this:</p>
<p>In shader:</p>
<pre><code>uniform float someFloat;
</code></pre>
<p>In C++ code:</p>
<pre><code>const auto loc = glGetUniformLocation(shader, "someFloat");
glUseProgram(shader);
glUniform1f(loc, 42.f);
</code></pre>
<p>You can also use <a href="https://www.khronos.org/opengl/wiki/Uniform_(GLSL)/Explicit_Uniform_Location">explicit uniform location</a> like this.</p>
<p>In shader:</p>
<pre><code>layout(location = 20) uniform float someFloat;
</code></pre>
<p>In code:</p>
<pre><code>const auto loc = 20;
glUniform1f(loc, 42.f);
</code></pre>
<p>In Vulkan, you need to group your uniforms into “descriptor sets”:</p>
<pre><code>// set 0
layout (set = 0, binding = 0) uniform float someFloat;
layout (set = 0, binding = 1) uniform mat4 someMatrix;
// set 1
layout (set = 1, binding = 0) uniform float someOtherFloat;
... // etc.
</code></pre>
<p>Now, this makes things a lot more complicated, because you need to specify descriptor set layout beforehand, use descriptor set pools and allocate descriptor sets with them, do the whole <code>VkWriteDescriptorSet</code> + <code>vkUpdateDescriptorSets</code> thing, call <code>vkCmdBindDescriptorSets</code> for each descriptor set and so on.</p>
<p>I’ll explain later how I avoided using descriptor sets by using bindless descriptors and buffer device access. Basically, I only have one “global” descriptor set for bindless textures and samplers, and that’s it. Everything else is passed via push constants which makes everything much easier to handle.</p>
<h3 id="pipeline-pattern">Pipeline pattern</h3>
<p>I separate drawing steps into “pipeline” classes.</p>
<p>Most of them look like this:</p>
<pre><code>class PostFXPipeline {
public:
    void init(GfxDevice&amp; gfxDevice, VkFormat drawImageFormat);
    void cleanup(VkDevice device);

    void draw(
        VkCommandBuffer cmd,
        GfxDevice&amp; gfxDevice,
        const GPUImage&amp; drawImage,
        const GPUImage&amp; depthImage,
        const GPUBuffer&amp; sceneDataBuffer);

private:
    VkPipelineLayout pipelineLayout;
    VkPipeline pipeline;

    struct PushConstants {
        VkDeviceAddress sceneDataBuffer;
        std::uint32_t drawImageId;
        std::uint32_t depthImageId;
    };
};
</code></pre>
<ul>
<li><code>init</code> loads needed shaders and initializes <code>pipeline</code> and <code>pipelineLayout</code>:</li>
</ul>
<pre><code>void PostFXPipeline::init(GfxDevice&amp; gfxDevice, VkFormat drawImageFormat)
{
    const auto&amp; device = gfxDevice.getDevice();

    const auto pcRange = VkPushConstantRange{
        .stageFlags = VK_SHADER_STAGE_FRAGMENT_BIT,
        .offset = 0,
        .size = sizeof(PushConstants),
    };

    const auto layouts = std::array{gfxDevice.getBindlessDescSetLayout()};
    const auto pushConstantRanges = std::array{pcRange};
    pipelineLayout = vkutil::createPipelineLayout(device, layouts, pushConstantRanges);

    const auto vertexShader =
        vkutil::loadShaderModule("shaders/fullscreen_triangle.vert.spv", device);
    const auto fragShader =
        vkutil::loadShaderModule("shaders/postfx.frag.spv", device);
    pipeline = PipelineBuilder{pipelineLayout}
                   .setShaders(vertexShader, fragShader)
                   .setInputTopology(VK_PRIMITIVE_TOPOLOGY_TRIANGLE_LIST)
                   .setPolygonMode(VK_POLYGON_MODE_FILL)
                   .disableCulling()
                   .setMultisamplingNone()
                   .disableBlending()
                   .setColorAttachmentFormat(drawImageFormat)
                   .disableDepthTest()
                   .build(device);
    vkutil::addDebugLabel(device, pipeline, "postFX pipeline");

    vkDestroyShaderModule(device, vertexShader, nullptr);
    vkDestroyShaderModule(device, fragShader, nullptr);
}
</code></pre>
<p>The <code>init</code> function is usually called once during the engine initialization. <code>PipelineBuilder</code> abstraction is described in vkguide <a href="https://vkguide.dev/docs/new_chapter_3/building_pipeline/">here</a>. I modified it a bit to use the Builder pattern to be able to chain the calls.</p>
<ul>
<li><code>cleanup</code> does all the needed cleanup. It usually simply destroys the pipeline and its layout:</li>
</ul>
<pre><code>void PostFXPipeline::cleanup(VkDevice device)
{
    vkDestroyPipeline(device, pipeline, nullptr);
    vkDestroyPipelineLayout(device, pipelineLayout, nullptr);
}
</code></pre>
<ul>
<li><code>draw</code> is called each frame and all the needed inputs are passed as arguments. It’s assumed that the sync is performed outside of the <code>draw</code> call (see “Synchronization” section below). Some pipelines are only called once per frame - some either take <code>std::vector</code> of objects to draw or are called like this:</li>
</ul>
<pre><code>for (const auto&amp; mesh : meshes) {
    somePipeline.draw(cmd, gfxDevice, mesh, ...);
}
</code></pre>
<p>The typical <code>draw</code> function looks like this:</p>
<pre><code>void PostFXPipeline::draw(
    VkCommandBuffer cmd,
    GfxDevice&amp; gfxDevice,
    const GPUImage&amp; drawImage,
    const GPUImage&amp; depthImage,
    const GPUBuffer&amp; sceneDataBuffer)
{
    // Bind the pipeline
    vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_GRAPHICS, pipeline);

    // Bind the bindless descriptor set
    gfxDevice.bindBindlessDescSet(cmd, pipelineLayout);

    // Handle push constants
    const auto pcs = PushConstants{
        // BDA - explained below
        .sceneDataBuffer = sceneDataBuffer.address,
        // bindless texture ids - no need for desc. sets!
        // explained below
        .drawImageId = drawImage.getBindlessId(),
        .depthImageId = depthImage.getBindlessId(),
    };
    vkCmdPushConstants(
        cmd, pipelineLayout, VK_SHADER_STAGE_FRAGMENT_BIT, 0, sizeof(PushConstants), &amp;pcs);

    // Finally, do some drawing. Here we're drawing a fullscreen triangle
    // to do a full-screen effect.
    vkCmdDraw(cmd, 3, 1, 0, 0);
}
</code></pre>
<p>Note another thing: it’s assumed that <code>draw</code> is called between <code>vkCmdBeginRendering</code> and <code>vkCmdEndRendering</code> - the render pass itself doesn’t care what texture it renders to - the caller of <code>draw</code> is responsible for that. It makes things simpler and allows you to do several draws to the same render target, e.g.:</p>
<pre><code>// handy wrapper for creating VkRenderingInfo
const auto renderInfo = vkutil::createRenderingInfo({
    .renderExtent = drawImage.getExtent2D(),
    .colorImageView = drawImage.imageView,
    .colorImageClearValue = glm::vec4{0.f, 0.f, 0.f, 1.f},
    .depthImageView = depthImage.imageView,
    .depthImageClearValue = 0.f,
    // for MSAA
    .resolveImageView = resolveImage.imageView,
});

vkCmdBeginRendering(cmd, &amp;renderInfo.renderingInfo);

// draw meshes
for (const auto&amp; mesh : meshesToDraw) {
    meshPipeline.draw(cmd, gfxDevice, mesh, ...);
}
// draw sky
skyboxPipeline.draw(cmd, gfxDevice, camera);

vkCmdEndRendering(cmd);
</code></pre>
<blockquote>
  I use <code>VK_KHR_dynamic_rendering</code> everywhere. I don’t use Vulkan render passes and subpasses at all. I’ve heard that they’re more efficient on tile-based GPUs, but I don’t care about mobile support for now. <code>VK_KHR_dynamic_rendering</code> just makes everything much easier.
</blockquote>

<h3 id="using-programmable-vertex-pulling-pvp--buffer-device-address-bda">Using programmable vertex pulling (PVP) + buffer device address (BDA)</h3>
<p>I have one vertex type for all the meshes. It looks like this:</p>
<pre><code>struct Vertex {
    vec3 position;
    float uv_x;
    vec3 normal;
    float uv_y;
    vec4 tangent;
};
</code></pre>
<blockquote>
  Of course, you can greatly optimize it using various methods, but it’s good enough for me for now. The <code>uv_x</code>/<code>uv_y</code> separation comes from vkguide - I think it’s a nice idea to get good alignment and not waste any bytes
</blockquote>

<p>The vertices are accessed in the shader like this:</p>
<pre><code>layout (buffer_reference, std430) readonly buffer VertexBuffer {
    Vertex vertices[];
};

layout (push_constant, scalar) uniform constants
{
    VertexBuffer vertexBuffer;
    ... // other stuff
} pcs;

void main()
{
    Vertex v = pcs.vertexBuffer.vertices[gl_VertexIndex];
    ...
}
</code></pre>
<p>PVP frees you from having to define vertex format (no more VAOs like in OpenGL or <code>VkVertexInputBindingDescription</code> + <code>VkVertexInputAttributeDescription</code> in Vulkan). BDA also frees you from having to bind a buffer to a descriptor set - you just pass an address to your buffer which contains vertices in push constants and that’s it.</p>
<blockquote>
  Also note the <code>scalar</code> layout for push constants. I use it for all the buffers too. Compared to “std430” layout, it makes alignment a lot more easy to handle - it almost works the same as in C++ and greatly reduces the need for “padding” members in C++ structs.
</blockquote>

<h3 id="bindless-descriptors">Bindless descriptors</h3>
<p>Textures were painful to work with even in OpenGL - you had “texture slots” which were awkward to work with. You couldn’t just sample any texture from the shader if it wasn’t bound to a texture slot beforehand. <code>ARB_bindless_texture</code> changed that and made many things easier.</p>
<p>Vulkan doesn’t have the exact same functionality, but it has something similar. You can create big descriptor sets which look like this:</p>
<pre><code>// bindless.glsl
layout (set = 0, binding = 0) uniform texture2D textures[];
...
layout (set = 0, binding = 1) uniform sampler samplers[];
</code></pre>
<p>You’ll need to maintain a list of all your textures using some “image manager” and when a new texture is loaded, you need to insert it into the <code>textures</code> array. The index at which you inserted it becomes a bindless “texture id” which then can be used to sample it in shaders. Now you can pass these ids in your push constants like this:</p>
<pre><code>layout (push_constant, scalar) uniform constants
{
  uint textureId;
  ...
} pcs;
</code></pre>
<p>and then you can sample your texture in the fragment shader like this:</p>
<pre><code>// bindless.glsl
#define NEAREST_SAMPLER_ID 0
...

vec4 sampleTexture2DNearest(uint texID, vec2 uv) {
    return texture(nonuniformEXT(sampler2D(textures[texID], samplers[NEAREST_SAMPLER_ID])), uv);
}

// shader.frag
vec4 color = sampleTexture2DNearest(pcs.textureId, inUV);
</code></pre>
<p>Two things to note:</p>
<ol>
<li>I chose separate image samplers so that I could sample any texture using different samplers. Common samplers (nearest, linear with anisotropy, depth texture samplers) are created and put into <code>samplers</code> array on the startup.</li>
<li>The wrapper function makes the process of sampling a lot more convenient.</li>
</ol>
<blockquote>
  The placement of <code>nonuniformEXT</code> is somewhat tricky and is explained very well <a href="https://github.com/KhronosGroup/Vulkan-Samples/blob/ada3895613b90fe21cde2718dbbfc221b27c575e/shaders/descriptor_indexing/glsl/nonuniform-quads.frag#L31">here</a>.
</blockquote>

<p>I use bindless ids for the mesh material buffer which looks like this:</p>
<pre><code>struct MaterialData {
    vec4 baseColor;
    vec4 metallicRoughnessEmissive;
    uint diffuseTex;
    uint normalTex;
    uint metallicRoughnessTex;
    uint emissiveTex;
};

layout (buffer_reference, std430) readonly buffer MaterialsBuffer {
    MaterialData data[];
} materialsBuffer;
</code></pre>
<p>Now I can only pass material ID in my push constants and then sample texture like this in the fragment shader:</p>
<pre><code>MaterialData material = materials[pcs.materialID];
vec4 diffuse = sampleTexture2DLinear(material.diffuseTex, inUV);
...
</code></pre>
<p>Neat! No more bulky descriptor sets, just one int per material in the push constants.</p>
<p>You can also put different texture types into the same set like this (this is needed for being able to access textures of types other than <code>texture2D</code>):</p>
<pre><code>layout (set = 0, binding = 0) uniform texture2D textures[];
layout (set = 0, binding = 0) uniform texture2DMS texturesMS[];
layout (set = 0, binding = 0) uniform textureCube textureCubes[];
layout (set = 0, binding = 0) uniform texture2DArray textureArrays[];
</code></pre>
<p>And here’s how you can sample <code>textureCube</code> with a linear sampler (note that we use <code>textureCubes</code> here instead of <code>textures</code>):</p>
<pre><code>vec4 sampleTextureCubeLinear(uint texID, vec3 p) {
    return texture(nonuniformEXT(samplerCube(textureCubes[texID], samplers[NEAREST_SAMPLER_ID])), p);
}
</code></pre>
<p>Here’s a very good article on using bindless textures in Vulkan:</p>
<p><a href="https://jorenjoestar.github.io/post/vulkan_bindless_texture/">https://jorenjoestar.github.io/post/vulkan_bindless_texture/</a></p>
<h3 id="handling-dynamic-data-which-needs-to-be-uploaded-every-frame">Handling dynamic data which needs to be uploaded every frame</h3>
<p>I find it useful to pre-allocate big arrays of things and push stuff to them in every frame.
Basically, you can pre-allocate an array of N structs (or matrices) and then start at index 0 at each new frame and push things to it from the CPU. Then, you can access all these items in your shaders. For example, I have all joint matrices stored in one big <code>mat4</code> array and the skinning compute shader accesses joint matrices of a particular mesh using start index passed via push constants (more about it will be explained later).</p>
<p>Here are two ways of doing this:</p>
<ul>
<li>
<ol>
<li>Have N buffers on GPU and swap between them.</li>
</ol>
</li>
</ul>
<p>vkguide explains the concept of “in flight” frames pretty well. To handle this parallelism properly, you need to have one buffer for the “currently drawing” frame and one buffer for “currently recording new drawing commands” frame to not have races. (If you have more frames in flight, you’ll need to allocate more than 2 buffers)</p>
<p>This means that you need to preallocate 2 buffers on GPU. You write data from CPU to GPU to the first buffer during the first frame. While you record the second frame, GPU reads from the first buffer while you write new data to the second buffer. On the third frame, GPU reads from the second buffer and you write new info to the first buffer… and so on.</p>
<ul>
<li>
<ol start="2">
<li>One buffer on GPU and N “staging” buffers on CPU</li>
</ol>
</li>
</ul>
<p>This might be useful if you need to conserve some memory on the GPU.</p>
<p>Let’s see how it works in my engine:</p>
<pre><code>class NBuffer {
public:
    void init(
        GfxDevice&amp; gfxDevice,
        VkBufferUsageFlags usage,
        std::size_t dataSize,
        std::size_t numFramesInFlight,
        const char* label);

    void cleanup(GfxDevice&amp; gfxDevice);

    void uploadNewData(
        VkCommandBuffer cmd,
        std::size_t frameIndex,
        void* newData,
        std::size_t dataSize,
        std::size_t offset = 0);

    const GPUBuffer&amp; getBuffer() const { return gpuBuffer; }

private:
    std::size_t framesInFlight{0};
    std::size_t gpuBufferSize{0};
    std::vector&lt;GPUBuffer&gt; stagingBuffers;
    GPUBuffer gpuBuffer;
    bool initialized{false};
};

void NBuffer::init(
    GfxDevice&amp; gfxDevice,
    VkBufferUsageFlags usage,
    std::size_t dataSize,
    std::size_t numFramesInFlight,
    const char* label)
{
    ...

    gpuBuffer = gfxDevice.createBuffer(
        dataSize, usage | VK_IMAGE_USAGE_TRANSFER_DST_BIT, VMA_MEMORY_USAGE_AUTO_PREFER_DEVICE);
    vkutil::addDebugLabel(gfxDevice.getDevice(), gpuBuffer.buffer, label);

    for (std::size_t i = 0; i &lt; numFramesInFlight; ++i) {
        stagingBuffers.push_back(gfxDevice.createBuffer(
            dataSize, usage | VK_BUFFER_USAGE_TRANSFER_SRC_BIT, VMA_MEMORY_USAGE_AUTO_PREFER_HOST));
    }

    ...
}
</code></pre>
<p>Note how staging buffers are created using VMA’s <code>PREFER_HOST</code> flag and the “main” buffer from which we read in the shader is using the <code>PREFER_DEVICE</code> flag.</p>
<p>Here’s how new data is uploaded (<a href="https://github.com/eliasdaler/edbr/blob/bf09366f9ad7d82e2cba01e66518c185af55fd5b/edbr/src/Graphics/NBuffer.cpp#L49">full implementation</a>):</p>
<pre><code>void NBuffer::uploadNewData(
    VkCommandBuffer cmd,
    std::size_t frameIndex,
    void* newData,
    std::size_t dataSize,
    std::size_t offset) const
{
    assert(initialized);
    assert(frameIndex &lt; framesInFlight);
    assert(offset + dataSize &lt;= gpuBufferSize &amp;&amp; "NBuffer::uploadNewData: out of bounds write");

    if (dataSize == 0) {
        return;
    }

    // sync with previous read
    ... // READ BARRIER CODE HERE

    auto&amp; staging = stagingBuffers[frameIndex];
    auto* mappedData = reinterpret_cast&lt;std::uint8_t*&gt;(staging.info.pMappedData);
    memcpy((void*)&amp;mappedData[offset], newData, dataSize);

    const auto region = VkBufferCopy2{
        .sType = VK_STRUCTURE_TYPE_BUFFER_COPY_2,
        .srcOffset = (VkDeviceSize)offset,
        .dstOffset = (VkDeviceSize)offset,
        .size = dataSize,
    };
    const auto bufCopyInfo = VkCopyBufferInfo2{
        .sType = VK_STRUCTURE_TYPE_COPY_BUFFER_INFO_2,
        .srcBuffer = staging.buffer,
        .dstBuffer = gpuBuffer.buffer,
        .regionCount = 1,
        .pRegions = &amp;region,
    };

    vkCmdCopyBuffer2(cmd, &amp;bufCopyInfo);

    // sync with write
    ... // WRITE BARRIER CODE HERE
}
</code></pre>
<p>I’d go with the first approach for most cases (more data on GPU, but no need for manual sync) unless you need to conserve GPU memory for some reason. I’ve found no noticeable difference in performance between two approaches, but it might matter if you are uploading huge amounts of data to GPU on each frame.</p>
<h3 id="destructors-deletion-queue-and-cleanup">Destructors, deletion queue and cleanup</h3>
<p>Now, this might be somewhat controversial… but I didn’t find much use of the deletion queue pattern used in vkguide. I don’t really need to allocated/destroy new objects on every frame.</p>
<p>Using C++ destructors for Vulkan object cleanup is not very convenient either. You need to wrap everything in custom classes, add move constructors and move <code>operator=</code>… It adds an additional layer of complexity.</p>
<p>In most cases, the cleanup of Vulkan objects happens in one place - and you don’t want to accidentally destroy some in-use object mid-frame by accidentally destroying some wrapper object.</p>
<p>It’s also harder to manage lifetimes when you have cleanup in happening in the destructor. For example, suppose you have a case like this:</p>
<pre><code>struct SomeClass {
    SomeOtherClass b;

    void init() {
        ...
    }

    void cleanup() {
        ...
    }
}
</code></pre>
<p>If you want to cleanup <code>SomeOtherClass</code> resources (e.g. the instance of <code>SomeOtherClass</code> has a <code>VkPipeline</code> object) during <code>SomeClass::cleanup</code>, you can’t do that if the cleanup of <code>SomeOtherClass</code> is performed in its destructor.</p>
<p>Of course, you can do this:</p>
<pre><code>struct SomeClass {
    std::unique_ptr&lt;SomeOtherClass&gt; b;

    void init() {
        b = std::make_unique&lt;SomeOtherClass&gt;();
        ...
    }

    void cleanup() {
        b.reset();
        ...
    }
}
</code></pre>
<p>… but I don’t like how it introduces a dynamic allocation and requires you to do write more code (and it’s not that much different from calling a <code>cleanup</code> function manually).</p>
<p>Right now, I prefer to clean up stuff directly, e.g.</p>
<pre><code>class SkyboxPipeline {
public:
    void cleanup(VkDevice device) {
        vkDestroyPipeline(device, pipeline, nullptr);
        vkDestroyPipelineLayout(device, pipelineLayout, nullptr);
    }

private:
    VkPipelineLayout pipelineLayout;
    VkPipeline pipeline;
    ...
}

// in GameRenderer.cpp:
void GameRenderer::cleanup(VkDevice device) {
    ...
    skyboxPipeline.cleanup(device);
    ...
}
</code></pre>
<p>This approach is not perfect - first of all, it’s easy to forget to call <code>cleanup</code> function, This is not a huge problem since you get a validation error in case you forget to cleanup some Vulkan resources on shutdown:</p>
<pre><code>Validation Error: [ VUID-vkDestroyDevice-device-05137 ] Object 0: handle = 0x4256c1000000005d, type = VK_OBJECT_TYPE_PIPELINE_LAYOUT; | MessageID = 0x4872eaa0 | vkCreateDevice():  OBJ ERROR : For VkDevice 0x27bd530[], VkPipelineLayout 0x4256c1000000005d[] has not been destroyed. The Vulkan spec states: All child objects created on device must have been destroyed prior to destroying device (https://vulkan.lunarg.com/doc/view/1.3.280.1/linux/1.3-extensions/vkspec.html#VUID-vkDestroyDevice-device-05137)
</code></pre>
<p>VMA also triggers asserts if you forget to free some buffer/image allocated with it.</p>
<p>I find it convenient to have all the Vulkan cleanup happening <em>explicitly</em> in one place. It makes it easy to track when the objects get destroyed.</p>
<h3 id="synchronization">Synchronization</h3>
<p>Synchronization in Vulkan is difficult. OpenGL and WebGPU do it for you - if you read from some texture/buffer, you know that it will have the correct data and you won’t get problems with data races. With Vulkan, you need to be explicit and this is usually where things tend to get complicated.</p>
<p>Right now I manage most of the complexities of sync manually in one place. I separate my drawing into “passes”/pipelines (as described above) and then insert barriers between them. For example, the skinning pass writes new vertex data into GPU memory. Shadow mapping pass reads this data to render skinned meshes into the shadow map. Sync in my code looks like this:</p>
<pre><code>// do skinning in compute shader
for (const auto&amp; mesh : skinnedMeshes) {
    skinningPass.doSkinning(gfxDevice, mesh);
}

{
    // Sync skinning with CSM
    // This is a "fat" barrier and you can potentially optimize it
    // by specifying all the buffers that the next pass will read from
    const auto memoryBarrier = VkMemoryBarrier2{
        .sType = VK_STRUCTURE_TYPE_MEMORY_BARRIER_2,
        .srcStageMask = VK_PIPELINE_STAGE_2_COMPUTE_SHADER_BIT,
        .srcAccessMask = VK_ACCESS_2_SHADER_WRITE_BIT,
        .dstStageMask = VK_PIPELINE_STAGE_2_VERTEX_SHADER_BIT,
        .dstAccessMask = VK_ACCESS_2_MEMORY_READ_BIT,
    };
    const auto dependencyInfo = VkDependencyInfo{
        .sType = VK_STRUCTURE_TYPE_DEPENDENCY_INFO,
        .memoryBarrierCount = 1,
        .pMemoryBarriers = &amp;memoryBarrier,
    };
    vkCmdPipelineBarrier2(cmd, &amp;dependencyInfo);
}

// do shadow mapping
shadowMappingPass.draw(gfxDevice, ...);
</code></pre>
<p>Of course, this can be automated/simplified using render graphs. This is something that I might implement in the future. Right now I’m okay with doing manual sync. vkconfig’s “synchronization” validation layer also helps greatly in finding sync errors.</p>
<p>The following resources were useful for understanding synchronization:</p>
<ul>
<li>
<p><a href="https://themaister.net/blog/2019/08/14/yet-another-blog-explaining-vulkan-synchronization/">https://themaister.net/blog/2019/08/14/yet-another-blog-explaining-vulkan-synchronization/</a></p>
</li>
<li>
<p><a href="https://github.com/KhronosGroup/Vulkan-Docs/wiki/Synchronization-Examples">https://github.com/KhronosGroup/Vulkan-Docs/wiki/Synchronization-Examples</a></p>
</li>
<li>
<p><a href="https://www.youtube.com/watch?v=GiKbGWI4M-Y">Vulkan Lecture Series Ep. 7 on Vulkan Sync by TU Wien</a></p>
</li>
</ul>
<h2 id="more-implementation-notes">More implementation notes</h2>
<h3 id="drawing-many-sprites">Drawing many sprites</h3>
<p>With bindless textures, it’s easy to draw many sprites using one draw call without having to allocate vertex buffers at all.</p>
<p>First of all, you can emit vertex coordinates and UVs using <code>gl_VertexIndex</code> in your vertex shader like this:</p>
<pre><code>void main()
{
    uint b = 1 &lt;&lt; (gl_VertexIndex % 6);
    vec2 baseCoord = vec2((0x1C &amp; b) != 0, (0xE &amp; b) != 0);
    ...
}
</code></pre>
<p>This snippet produces this set of values:</p>
<table>
<thead>
<tr>
<th>gl_VertexIndex</th>
<th>baseCoord</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>(0,0)</td>
</tr>
<tr>
<td>1</td>
<td>(0,1)</td>
</tr>
<tr>
<td>2</td>
<td>(1,1)</td>
</tr>
<tr>
<td>3</td>
<td>(1,1)</td>
</tr>
<tr>
<td>4</td>
<td>(1,0)</td>
</tr>
<tr>
<td>5</td>
<td>(0,0)</td>
</tr>
</tbody>
</table>
<figure><img src="https://edw.is/learning-vulkan/resources/quad.png" alt="Two triangles form a quad"><figcaption>
            <p>Two triangles form a quad</p>
        </figcaption>
</figure>

<p>All the sprite draw calls are combined into <code>SpriteDrawBuffer</code> which looks like this in GLSL:</p>
<pre><code>struct SpriteDrawCommand {
    mat4 transform; // could potentially be mat2x2...
    vec2 uv0; // top-left uv coord
    vec2 uv1; // bottom-right uv coord
    vec4 color; // color by which texture is multiplied
    uint textureID; // sprite texture
    uint shaderID; // explained below
    vec2 padding; // padding to satisfy "scalar" requirements
};

layout (buffer_reference, scalar) readonly buffer SpriteDrawBuffer {
    SpriteDrawCommand commands[];
};
</code></pre>
<p>On CPU/C++ side, it looks almost the same:</p>
<pre><code>struct SpriteDrawCommand {
    glm::mat4 transform;
    glm::vec2 uv0; // top-left uv coordinate
    glm::vec2 uv1; // bottom-right uv coodinate
    LinearColor color; // color by which texture is multiplied by
    std::uint32_t textureId; // sprite texture
    std::uint32_t shaderId; // explained below
    glm::vec2 padding; // padding
};

std::vector&lt;SpriteDrawCommand&gt; spriteDrawCommands;
</code></pre>
<p>I create two fixed size buffers on the GPU and then upload the contents of <code>spriteDrawCommands</code> (using techniques described above in the “Handling dynamic data” section).</p>
<p>The sprite renderer is used like this:</p>
<pre><code>// record commands
renderer.beginDrawing();
{
    renderer.drawSprite(sprite, pos);
    renderer.drawText(font, "Hello");
    renderer.drawRect(...);
}
renderer.endDrawing();

// do actual drawing later:
renderer.draw(cmd, gfxDevice, ...);
</code></pre>
<blockquote>
  The same renderer also draws text, rectangles and lines in my engine. For example, the text is just N “draw sprite” commands for a string composed of N glyphs. Solid color rectangles and lines are achieved by using a 1x1 pixel white texture and multiplying it by <code>SpriteCommand::color</code> in the fragment shader.
</blockquote>

<p>And finally, here’s how the command to do the drawing looks like inside <code>SpriteRenderer::draw</code>:</p>
<pre><code>vkCmdDraw(cmd, 6, spriteDrawCommands.size(), 0, 0);
// 6 vertices per instance, spriteDrawCommands.size() instances in total
</code></pre>
<p>The complete sprite.vert looks like this:</p>
<pre><code>#version 460

#extension GL_GOOGLE_include_directive : require
#extension GL_EXT_buffer_reference : require

#include "sprite_commands.glsl"

layout (push_constant) uniform constants
{
    mat4 viewProj; // 2D camera matrix
    SpriteDrawBuffer drawBuffer; // where sprite draw commands are stored
} pcs;

layout (location = 0) out vec2 outUV;
layout (location = 1) out vec4 outColor;
layout (location = 2) flat out uint textureID;
layout (location = 3) flat out uint shaderID;

void main()
{
    uint b = 1 &lt;&lt; (gl_VertexIndex % 6);
    vec2 baseCoord = vec2((0x1C &amp; b) != 0, (0xE &amp; b) != 0);

    SpriteDrawCommand command = pcs.drawBuffer.commands[gl_InstanceIndex];

    gl_Position = pcs.viewProj * command.transform * vec4(baseCoord, 0.f, 1.f);
    outUV = (1.f - baseCoord) * command.uv0 + baseCoord * command.uv1;
    outColor = command.color;
    textureID = command.textureID;
    shaderID = command.shaderID;
}
</code></pre>
<p>All the parameters of the sprite draw command are self-explanatory, but <code>shaderID</code> needs a bit of clarification. Currently, I use it to branch inside the fragment shader:</p>
<pre><code>...

#define SPRITE_SHADER_ID 0
#define TEXT_SHADER_ID   1

void main()
{
    vec4 texColor = sampleTexture2DNearest(textureID, inUV);

    // text drawing is performed differently...
    if (shaderID == TEXT_SHADER_ID) {
        // glyph atlas uses single-channel texture
        texColor = vec4(1.0, 1.0, 1.0, texColor.r);
    }

    if (texColor.a &lt; 0.1) {
        discard;
    }

    outColor = inColor * texColor;
}
</code></pre>
<p>This allows me to draw sprites differently depending on this ID without having to change pipelines. Of course, it can be potentially bad for the performance. This can be improved by drawing sprites with the same shader ID in batches. You’ll only need to switch pipelines when you encounter a draw command with a different shader ID.</p>
<p>The sprite renderer is very efficient: it can draw 10 thousand sprites in just 315 microseconds.
</p><figure><img src="https://edw.is/learning-vulkan/resources/many_sprites.jpg">
</figure>

<figure><img src="https://edw.is/learning-vulkan/resources/many_sprites_prof.png">
</figure>

<h3 id="compute-skinning">Compute skinning</h3>
<p>I do skinning for skeletal animation in a compute shader. This allows me to have the same vertex format for all the meshes.</p>
<p>Basically, I just take the mesh’s vertices (not skinned) and joint matrices and produce a new buffer of vertices which are used in later rendering stages.</p>
<p>Suppose you spawn three cats with identical meshes:</p>
<figure><img src="https://edw.is/learning-vulkan/resources/mtp_screenshot_one_month.png">
</figure>

<p>All three of them can have different animations. They all have an identical “input” mesh. But the “output” vertex buffer will differ between them, which means that you need to pre-allocate a vertex buffer for each instance of the mesh.</p>
<p>Here’s how the skinning compute shader looks like:</p>
<pre><code>#version 460

#extension GL_GOOGLE_include_directive : require
#extension GL_EXT_buffer_reference : require

#include "vertex.glsl"

struct SkinningDataType {
    ivec4 jointIds;
    vec4 weights;
};

layout (buffer_reference, std430) readonly buffer SkinningData {
    SkinningDataType data[];
};

layout (buffer_reference, std430) readonly buffer JointMatrices {
    mat4 matrices[];
};

layout (push_constant) uniform constants
{
    JointMatrices jointMatrices;
    uint jointMatricesStartIndex;
    uint numVertices;
    VertexBuffer inputBuffer;
    SkinningData skinningData;
    VertexBuffer outputBuffer;
} pcs;

layout (local_size_x = 256, local_size_y = 1, local_size_z = 1) in;

mat4 getJointMatrix(int jointId) {
    return pcs.jointMatrices.matrices[pcs.jointMatricesStartIndex + jointId];
}

void main()
{
    uint index = gl_GlobalInvocationID.x;
    if (index &gt;= pcs.numVertices) {
        return;
    }

    SkinningDataType sd = pcs.skinningData.data[index];
    mat4 skinMatrix =
        sd.weights.x * getJointMatrix(sd.jointIds.x) +
        sd.weights.y * getJointMatrix(sd.jointIds.y) +
        sd.weights.z * getJointMatrix(sd.jointIds.z) +
        sd.weights.w * getJointMatrix(sd.jointIds.w);

    Vertex v = pcs.inputBuffer.vertices[index];
    v.position = vec3(skinMatrix * vec4(v.position, 1.0));

    pcs.outputBuffer.vertices[index] = v;
}
</code></pre>
<ol>
<li>I store all joint matrices in a big array and populate it every frame (and also pass the starting index in the array for each skinned mesh, <code>jointMatricesStartIndex</code>).</li>
<li>Skinning data is not stored inside each mesh vertex, a separate buffer of <code>num_vertices</code> elements is used.</li>
</ol>
<p>After the skinning is performed, all the later rendering stages use this set of vertices Thee rendering process for static and skinned meshes becomes identical, thanks to that.</p>
<blockquote>
  <a href="https://capnramses.itch.io/antons-opengl-4-tutorials">Anton’s OpenGL 4 Tutorials</a> book has the best skinning implementation guide I’ve ever read. Game Engine Architecture by Jason Gregory has nice explanations about skinning/skeletal animation math as well.
</blockquote>

<h3 id="game--renderer-separation">Game / renderer separation</h3>
<p>I have a  game/renderer separation which uses a simple concept of “draw commands”. In the game logic, I use <a href="https://github.com/skypjack/entt">entt</a>, but the renderer doesn’t know anything about entities or “game objects”. It only knows about the lights, some scene parameters (like fog, which skybox texture to use etc) and meshes it needs to draw.</p>
<p>The renderer’s API looks like this in action:</p>
<pre><code>void Game::generateDrawList()
{
    renderer.beginDrawing();

    // Add lights
    const auto lights = ...; // get list of all active lights
    for (const auto&amp;&amp; [e, tc, lc] : lights.each()) {
        renderer.addLight(lc.light, tc.transform);
    }

    // Render static meshes
    const auto staticMeshes = ...; // list of entities with static meshes
    for (const auto&amp;&amp; [e, tc, mc] : staticMeshes.each()) {
        // Each "mesh" can have multiple submeshes similar to how
        // glTF separates each "mesh" into "primitives".
        for (std::size_t i = 0; i &lt; mc.meshes.size(); ++i) {
            renderer.drawMesh(mc.meshes[i], tc.worldTransform, mc.castShadow);
        }
    }

    // Render meshes with skeletal animation
    const auto skinnedMeshes = ...; // list of entities with skeletal animations
    for (const auto&amp;&amp; [e, tc, mc, sc] : skinnedMeshes.each()) {
        renderer.drawSkinnedMesh(
            mc.meshes, sc.skinnedMeshes, tc.worldTransform,
            sc.skeletonAnimator.getJointMatrices());
    }

    renderer.endDrawing();
}
</code></pre>
<p>When you call <code>drawMesh</code> or <code>drawSkinnedMesh</code>, the renderer creates a mesh draw command and puts it in <code>std::vector&lt;MeshDrawCommand&gt;</code> which are then iterated through during the drawing process. The <code>MeshDrawCommand</code> looks like this:</p>
<pre><code>
struct SkinnedMesh {
    GPUBuffer skinnedVertexBuffer;
};

struct MeshDrawCommand {
    MeshId meshId;
    glm::mat4 transformMatrix;
    math::Sphere worldBoundingSphere;

    const SkinnedMesh* skinnedMesh{nullptr};
    std::uint32_t jointMatricesStartIndex;
    bool castShadow{true};
};
</code></pre>
<ul>
<li><code>meshId</code> is used for looking up static meshes in <code>MeshCache</code> - it’s a simple <code>std::vector</code> of references to vertex buffers on GPU.</li>
<li>If the mesh has a skeleton, <code>jointMatricesStartIndex</code> is used during compute skinning and <code>skinnedMesh-&gt;skinnedVertexBuffer</code> is used for all the rendering afterwards (instead of <code>meshId</code>)</li>
<li><code>worldBoundingSphere</code> is used for frustum culling.</li>
</ul>
<p>This separation is nice because the renderer is clearly separated from the game logic. You can also do something more clever as described <a href="https://realtimecollisiondetection.net/blog/?p=86">here</a> if sorting draw commands becomes a bottleneck.</p>
<h3 id="scene-loading-and-entity-prefabs">Scene loading and entity prefabs</h3>
<p>I use Blender as a level editor and export it as glTF. It’s easy to place objects, colliders and lights there. Here’s how it looks like:</p>
<figure><img src="https://edw.is/learning-vulkan/resources/blender_level.jpg">
</figure>

<p>Writing your own level editor would probably take months (years!), so using Blender instead saved me quite a lot of time.</p>
<p>It’s important to mention how I use node names for spawning some objects. For example, you can see an object named <code>Interact.Sphere.Diary</code> selected in the screenshot above. The part before the first dot is the prefab name (in this case “Interact”). The “Sphere” part is used by the physics system to create a sphere physics body for the object (“Capsule” and “Box” can also be used, otherwise the physics shape is created using mesh vertices).</p>
<p>Some models are pretty complex and I don’t want to place them directly into the level glTF file as it’ll greatly increase each level’s size. I just place an “Empty-&gt;Arrows” object and name it something like “Cat.NearStore”. This will spawn “Cat” prefab and attach “NearStore” tag to it for runtime identification.</p>
<p>Prefabs are written in JSON and look like this:</p>
<pre><code>{
  "scene": {
    "scene": "assets/models/cato.gltf"
  },
  "movement": {
    "maxSpeed": [4, 4, 4]
  },
  "physics": {
    "type": "dynamic",
    "bodyType": "virtual_character",
    "bodyParams": {
        ...
    }
  }
}
</code></pre>
<p>During the level loading process, if the node doesn’t have a corresponding prefab, it’s loaded as-is and its mesh data is taken from the glTF file itself (this is mostly used for static geometry). If the node has a corresponding prefab loaded, it’s created instead. Its mesh data is loaded from the external glTF file - only transform is copied from the original glTF node (the one in the level glTF file).</p>
<blockquote>
  Once <a href="https://github.com/KhronosGroup/glTF-External-Reference">glTFX</a> is released and the support for it is added to Blender, things might be even easier to handle as you’ll be able to reference external glTF files with it.
</blockquote>

<h3 id="msaa">MSAA</h3>
<p>Using forward rendering allowed me to easily implement MSAA. Here’s a comparison of how the game looks without AA and with MSAA on:</p>
<figure><img src="https://edw.is/learning-vulkan/resources/frame_analysis/geometry.jpg" alt="No AA"><figcaption>
            <p>No AA</p>
        </figcaption>
</figure>

<figure><img src="https://edw.is/learning-vulkan/resources/frame_analysis/geometry_resolve.jpg" alt="MSAA x8"><figcaption>
            <p>MSAA x8</p>
        </figcaption>
</figure>

<figure><img src="https://edw.is/learning-vulkan/resources/no_aa_vs_aa.png">
</figure>

<p>MSAA is explained well here: <a href="https://vulkan-tutorial.com/Multisampling">https://vulkan-tutorial.com/Multisampling</a></p>
<p>Here’s another good article about MSAA: <a href="https://therealmjp.github.io/posts/msaa-overview/">https://therealmjp.github.io/posts/msaa-overview/</a> and potential problems you can have with it (especially with HDR and tone-mapping).</p>
<h3 id="ui">UI</h3>
<figure><img src="https://edw.is/learning-vulkan/resources/ui_options.png">
</figure>

<p>My UI system was inspired by Roblox’s UI API: <a href="https://create.roblox.com/docs/ui">https://create.roblox.com/docs/ui</a></p>
<p>Basically, the UI can calculate its own layout without me having to hard code each individual element’s size and position. Basically it relies on the following concepts:</p>
<ul>
<li>Origin is an anchor around which the UI element is positioned. If origin is <code>(0, 0)</code>, setting UI element’s position to be <code>(x,y)</code> will make its upper-left pixel have (x,y) pixel coordinate. If the origin is <code>(1, 1)</code>, then the element’s bottom-right corner will be positioned at <code>(x, y)</code>. If the origin is (0.5, 1) then it will be positioned using bottom-center point as the reference.
<figure><img src="https://edw.is/learning-vulkan/resources/ui_origin.png">
</figure>
</li>
<li>Relative size makes the children’s be proportional to parent’s size. If (1,1) then the child element will have the same size as the parent element. If it’s (0.5, 0.5) then it’ll have half the size of the parent. If the parent uses children’s size as a guide, then if a child has (0.5, 0.25) relative size, the parent’s width will be 2x larger and the height will be 4x larger.</li>
<li>Relative position uses parent’s size as a guide for positioning. It’s useful for centering elements, for example if you have an element with (0.5, 0.5) origin and (0.5, 0.5) relative position, it’ll be centered inside its parent element.</li>
<li>You can also set pixel offsets for both position and size separately (they’re called <code>offsetPosition</code> and <code>offsetSize</code> in my codebase).</li>
<li>You can also set a fixed size for the elements if you don’t want them to ever be resized.</li>
<li>The label/image element size is determined using its content.</li>
</ul>
<p>Here are some examples of how it can be used to position child elements:</p>
<figure><img src="https://edw.is/learning-vulkan/resources/ui_samples.png">
</figure>

<p>a) The child (yellow) has relative size (0.5, 1), relative position of (0.5, 0.5) and origin (0.5, 0.5) (alternatively, the relative position can be (0.5, 0.0) and origin at (0.5, 0.0) in this case). Its parent (green) will be two times wider, but will have the same height. The child element will be centered inside the parent.</p>
<p>b) The child (yellow) has origin (1, 1), fixed size (w,h) and absolute offset of (x,y) - this way, the item can be positioned relative to the bottom-right corner of its parent (green)</p>
<hr>
<p>Let’s see how sizes and positions of UI elements are calculated (<a href="https://github.com/eliasdaler/edbr/blob/bf09366f9ad7d82e2cba01e66518c185af55fd5b/edbr/src/UI/Element.cpp#L32">implementation in EDBR</a>).</p>
<p>First, sizes of all elements are calculated recursively. Then positions are computed based on the previously computed sizes and specified offset positions. Afterwards all elements are drawn recursively - parent element first, then its children etc.</p>
<p>When calculating the size, most elements either have a “fixed” size (which you can set manually, e.g. you can set some button to always be 60x60 pixels) or their size is computed based on their content. For example, for label elements, their size is computed using the text’s bounding box. For image elements, their size equals the image size and so on.</p>
<p>If an element has an “Auto-size” property, it needs to specify which child will be used to calculate its size. For example, the menu nine-slice can have several text labels inside the “vertical layout” element - the bounding boxes will be calculated first, then their sizes will be summed up - then, the parent’s size is calculated.</p>
<p>Let’s take a look at a simple menu with bounding boxes displayed:</p>
<figure><img src="https://edw.is/learning-vulkan/resources/ui_with_bb.png">
</figure>

<p>Here, root <code>NineSliceElement</code> is marked as “Auto-size”. To compute its size, it first computes the size of its child (<code>ListLayoutElement</code>). This recursively computes the sizes of each button, sums them up and adds some padding (<code>ListLayoutElement</code> also makes the width of each button the same based on the maximum width in the list).</p>
<h3 id="dear-imgui-and-srgb-issues">Dear ImGui and sRGB issues</h3>
<p>I love Dear ImGui. I used it to implement many useful dev and debug tools (open the image in a new tab to see them better):</p>
<figure><img src="https://edw.is/learning-vulkan/resources/dev_tools.png">
</figure>

<p>It <a href="https://github.com/ocornut/imgui/issues/578">has some problems with sRGB</a>, though. I won’t explain it in detail, but basically if you use sRGB framebuffer, Dear ImGui will look wrong in many ways, see the comparison:</p>
<figure><img src="https://edw.is/learning-vulkan/resources/imgui_srgb.png" alt="Left - naive sRGB fix for Dear ImGui, right - proper fix"><figcaption>
            <p>Left - naive sRGB fix for Dear ImGui, right - proper fix</p>
        </figcaption>
</figure>

<figure><img src="https://edw.is/learning-vulkan/resources/imgui_srgb2.png" alt="Left - naive sRGB fix for Dear ImGui, right - proper fix"><figcaption>
            <p>Left - naive sRGB fix for Dear ImGui, right - proper fix</p>
        </figcaption>
</figure>

<p>Sometimes you can see people doing hacks by doing <code>pow(col, vec4(2.2))</code> with Dear ImGui’s colors but it still doesn’t work properly with alpha and produces incorrect color pickers.</p>
<p>I ended up writing my own Dear ImGui backend and implementing DilligentEngine’s workaround which is explained in detail <a href="https://github.com/ocornut/imgui/issues/578#issuecomment-1585432977">here</a> and <a href="https://github.com/DiligentGraphics/DiligentTools/blob/da116e30adff75ccdb33443d604ca6d153ee1589/Imgui/src/ImGuiDiligentRenderer.cpp#L39">here</a>.</p>
<blockquote>
  Writing it wasn’t as hard as I expected. I only need to write the <em>rendering</em> part, while “logic/OS interaction” part (input event processing, clipboard etc.) is still handled by default Dear ImGui SDL backend in my case.
</blockquote>

<p>There are some additional benefits of having my own backend:</p>
<ol>
<li>It supports bindless texture ids, so I can draw images by simply calling <code>ImGui::Image(bindlessTextureId, ...)</code>. Dear ImGui’s Vulkan backend requires you to “register” textures by calling <code>ImGui_ImplVulkan_AddTexture</code> for each texture before you can call <code>ImGui::Image</code>.</li>
<li>It can properly draw linear and non-linear images by passing their format into backend (so that sRGB images are not gamma corrected twice when they’re displayed)</li>
<li>Initializing and dealing with it is easier as it does Vulkan things in the same way as the rest of my engine.</li>
</ol>
<h3 id="other-stuff">Other stuff</h3>
<p>There are many parts of the engine not covered there because they’re not related to Vulkan. I still feel like it’s good to mention them briefly for the sake of completion.</p>
<ul>
<li>I use Jolt Physics for physics.</li>
</ul>
<video preload="metadata" controls="">
    <source src="https://edw.is/learning-vulkan/resources/jolt_low_res.mp4#t=0.001" type="video/mp4">
    
</video>

<p>Integrating it into the engine was pretty easy. Right now I mostly use it for collision resolution and basic character movement.</p>
<p>The samples are <em>fantastic</em>. The docs are very good too.</p>
<p>I especially want to point out how incredible <code>JPH::CharacterVirtual</code> is. It handles basic character movement so well. I remember spending <em>days</em> trying to get proper slope movement in Bullet to work. With Jolt, it just worked “out of the box”.</p>
<p>Here’s how it basically works (explaining how it works properly would probably require me to write quite a big article):</p>
<ul>
<li>You add your shapes to Jolt’s world.</li>
<li>You run the simulation.</li>
<li>You get new positions of your physics objects and use these positions to render objects in their current positions.</li>
</ul>
<figure><img src="https://edw.is/learning-vulkan/resources/jolt_shapes.jpg" alt="I implemented Jolt physics shape debug renderer using im3d"><figcaption>
            <p>I implemented Jolt physics shape debug renderer using im3d</p>
        </figcaption>
</figure>

<ul>
<li>I use <a href="https://github.com/skypjack/entt">entt</a> for the entity-component-system part.</li>
</ul>
<p>It has worked great for me so far. Previously I had my own ECS implementation, but decided to experiment with a 3rd party ECS library to have less code to maintain.</p>
<ul>
<li>I use <a href="https://github.com/kcat/openal-soft">openal-soft</a>, <a href="https://github.com/xiph/ogg">libogg</a> and <a href="https://github.com/xiph/vorbis">libvorbis</a> for audio.</li>
</ul>
<p>The audio system is mostly based on these articles: <a href="https://indiegamedev.net/2020/02/15/the-complete-guide-to-openal-with-c-part-1-playing-a-sound/">https://indiegamedev.net/2020/02/15/the-complete-guide-to-openal-with-c-part-1-playing-a-sound/</a></p>
<ul>
<li>I use <a href="https://github.com/wolfpld/tracy">Tracy</a> for profiling.</li>
</ul>
<p>Integrating it was very easy (read the PDF doc, it’s fantastic!) and it helped me avoid tons of bike-shedding by seeing how little time something, which I thought was “inefficient”, really took.</p>
<figure><img src="https://edw.is/learning-vulkan/resources/tracy.png">
</figure>

<h2 id="what-i-gained-from-switching-to-vulkan">What I gained from switching to Vulkan</h2>
<p>There are many nice things I got after switching to Vulkan:</p>
<ul>
<li>No more global state</li>
</ul>
<p>This makes abstractions a lot easier. With OpenGL abstractions/engines, you frequently see “shader.bind()” calls, state trackers, magic RAII, which automatically binds/unbinds objects and so on. There’s no need for that in Vulkan - it’s easy to write functions which take some objects as an input and produce some output - stateless, more explicit and easier to reason about.</p>
<ul>
<li>API is more pleasant to work with overall - I didn’t like “binding” things and the whole “global state machine” of OpenGL.</li>
<li>You need to write less abstractions overall.</li>
</ul>
<p>With OpenGL, you need to write a <em>lot</em> of abstractions to make it all less error-prone… Vulkan’s API requires a lot less of this, in my experience. And usually the abstractions that you write map closer to Vulkan’s “raw” functions, compared to OpenGL abstractions which hide manipulation of global state and usually call several functions (and might do some stateful things for optimization).</p>
<ul>
<li>Better validation errors</li>
</ul>
<p>Validation errors are very good in Vulkan. While OpenGL has <code>glDebugMessageCallback</code>, it doesn’t catch that many issues and you’re left wondering why your texture looks weird, why your lighting is broken and so on. Vulkan has more extensive validation which makes the debugging process much better.</p>
<ul>
<li>Debugging in RenderDoc</li>
</ul>
<p>I can now debug shaders in RenderDoc. It looks like this:</p>
<figure><img src="https://edw.is/learning-vulkan/resources/renderdoc_debug.png">
</figure>

<p>With OpenGL I had to output the values to some texture and color-pick them… which took a lot of time. But now I can debug vertex and fragment shaders easily.</p>
<ul>
<li>More consistent experience across different GPUs and OSes.</li>
</ul>
<p>With OpenGL, drivers on different GPUs and OSes worked differently from each other which made some bugs pop up only on certain hardware configurations. It made the process of debugging them hard. I still experienced some slight differences between different GPUs in Vulkan, but it’s much less prevalent compared to OpenGL.</p>
<ul>
<li>Ability to use better shading languages in the future</li>
</ul>
<p>GLSL is a fine shading language, but there are some new shading languages which promise to be more feature-complete, convenient and readable, for example:</p>
<ul>
<li><a href="https://github.com/shader-slang/slang">https://github.com/shader-slang/slang</a></li>
<li><a href="https://github.com/shady-gang/shady">https://github.com/shady-gang/shady</a></li>
</ul>
<p>I might explore them in the future and see if they offer me something that GLSL lacks.</p>
<ul>
<li>More control over every aspect of the graphics pipeline.</li>
<li>Second system effect, but good</li>
</ul>
<p>My first OpenGL engine was written during the process of learning graphics programming from scratch. Many abstractions were not that good and rewriting them with some graphics programming knowledge (and some help from vkguide) helped me implement a much cleaner system.</p>
<ul>
<li>Street cred</li>
</ul>
<p>And finally, it makes me proud to be able to say “I have a custom engine written in Vulkan and it works”. Sometimes people start thinking about you as a coding wizard and it makes me happy and proud of my work. :)</p>
<h2 id="future-work">Future work</h2>
<p>There are many things that I plan to do in the future, here’s a list of some of them:</p>
<ul>
<li>Sign-distance field font support (<a href="https://www.redblobgames.com/blog/2024-03-21-sdf-fonts/">good article</a> about implementing them)</li>
<li>Loading many images and generating mipmaps in parallel (or use image formats which already have mipmaps stored inside of them)</li>
<li>Bloom.</li>
<li>Volumetric fog.</li>
<li>Animation blending.</li>
<li>Render graphs.</li>
<li>Ambient occlusion.</li>
<li>Finishing the game? (hopefully…)</li>
</ul>
<p>Overall, I’m quite satisfied with what I managed to accomplish. Learning Vulkan was quite difficult, but it wasn’t as hard as I imagined. It taught me a lot about graphics programming and modern APIs and now I have a strong foundation to build my games with.</p>
</article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[California DMV approves map increase in Waymo driverless operations (240 pts)]]></title>
            <link>https://www.dmv.ca.gov/portal/vehicle-industry-services/autonomous-vehicles/autonomous-vehicle-testing-permit-holders/waymo-approved-areas-of-operation-for-driverless-testing-and-deployment/</link>
            <guid>46009994</guid>
            <pubDate>Fri, 21 Nov 2025 22:52:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dmv.ca.gov/portal/vehicle-industry-services/autonomous-vehicles/autonomous-vehicle-testing-permit-holders/waymo-approved-areas-of-operation-for-driverless-testing-and-deployment/">https://www.dmv.ca.gov/portal/vehicle-industry-services/autonomous-vehicles/autonomous-vehicle-testing-permit-holders/waymo-approved-areas-of-operation-for-driverless-testing-and-deployment/</a>, See on <a href="https://news.ycombinator.com/item?id=46009994">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><tr><td><span>City</span>Livermore</td><td><span>County</span>Alameda County</td></tr><tr><td><span>City</span>Alameda</td><td><span>County</span>Alameda County</td></tr><tr><td><span>City</span>Albany</td><td><span>County</span>Alameda County</td></tr><tr><td><span>City</span>Berkeley</td><td><span>County</span>Alameda County</td></tr><tr><td><span>City</span>Dublin</td><td><span>County</span>Alameda County</td></tr><tr><td><span>City</span>Emeryville</td><td><span>County</span>Alameda County</td></tr><tr><td><span>City</span>Fremont</td><td><span>County</span>Alameda County</td></tr><tr><td><span>City</span>Hayward</td><td><span>County</span>Alameda County</td></tr><tr><td><span>City</span>Newark</td><td><span>County</span>Alameda County</td></tr><tr><td><span>City</span>Oakland</td><td><span>County</span>Alameda County</td></tr><tr><td><span>City</span>Piedmont</td><td><span>County</span>Alameda County</td></tr><tr><td><span>City</span>Pleasanton</td><td><span>County</span>Alameda County</td></tr><tr><td><span>City</span>San Leandro</td><td><span>County</span>Alameda County</td></tr><tr><td><span>City</span>Union City</td><td><span>County</span>Alameda County</td></tr><tr><td><span>City</span>Antioch</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>Brentwood</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>Clayton</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>Concord</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>Danville</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>El Cerrito</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>Hercules</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>Lafayette</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>Martinez</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>Moraga</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>Oakley</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>Orinda</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>Pinole</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>Pittsburg</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>Pleasant Hill</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>Richmond</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>San Pablo</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>San Ramon</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>Walnut Creek</td><td><span>County</span>Contra Costa County</td></tr><tr><td><span>City</span>Agoura Hills</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Arcadia</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Artesia</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Azusa</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Baldwin Park</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Bellflower</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Bradbury</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Burbank</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Calabasas</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Cerritos</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Claremont</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Covina</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Diamond Bar</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Duarte</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>El Monte</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Glendora</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Hawaiian Gardens</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Hidden Hills</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Industry</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Irwindale</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>La Cañada Flintridge</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>La Habra Heights</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>La Mirada</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>La Puente</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>La Verne</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Lakewood</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Lomita</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Malibu</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Monrovia</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Montebello</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Norwalk</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Palos Verdes Estates</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Pasadena</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Pico Rivera</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Pomona</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Rancho Palos Verdes</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Rolling Hills</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Rolling Hills Estates</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Rosemead</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>San Dimas</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>San Fernando</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>San Gabriel</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>San Marino</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Santa Clarita</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Santa Fe Springs</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Sierra Madre</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Signal Hill</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>South El Monte</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>South Pasadena</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Temple City</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Walnut</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>West Covina</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Westlake Village</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Whittier</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Alhambra</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Bell</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Bell Gardens</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Carson</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Commerce</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Compton</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Downey</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>El Segundo</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Gardena</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Glendale</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Hermosa Beach</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Long Beach</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Los Angeles</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Manhattan Beach</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Monterey Park</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Paramount</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Redondo Beach</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Santa Monica</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>South Gate</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Torrance</td><td><span>County</span>&nbsp;Los Angeles County</td></tr><tr><td><span>City</span>Belvedere</td><td><span>County</span>Marin County</td></tr><tr><td><span>City</span>Corte Madera</td><td><span>County</span>Marin County</td></tr><tr><td><span>City</span>Fairfax</td><td><span>County</span>Marin County</td></tr><tr><td><span>City</span>Larkspur</td><td><span>County</span>Marin County</td></tr><tr><td><span>City</span>Mill Valley</td><td><span>County</span>Marin County</td></tr><tr><td><span>City</span>Novato</td><td><span>County</span>Marin County</td></tr><tr><td><span>City</span>Ross</td><td><span>County</span>Marin County</td></tr><tr><td><span>City</span>San Anselmo</td><td><span>County</span>Marin County</td></tr><tr><td><span>City</span>San Rafael</td><td><span>County</span>Marin County</td></tr><tr><td><span>City</span>Sausalito</td><td><span>County</span>Marin County</td></tr><tr><td><span>City</span>Tiburon</td><td><span>County</span>Marin County</td></tr><tr><td><span>City</span>American Canyon</td><td><span>County</span>Napa County</td></tr><tr><td><span>City</span>Calistoga</td><td><span>County</span>Napa County</td></tr><tr><td><span>City</span>Napa</td><td><span>County</span>Napa County</td></tr><tr><td><span>City</span>St. Helena</td><td><span>County</span>Napa County</td></tr><tr><td><span>City</span>Yountville</td><td><span>County</span>Napa County</td></tr><tr><td><span>City</span>Aliso Viejo</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Anaheim</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Brea</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Buena Park</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Costa Mesa</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Cypress</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Dana Point</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Fountain Valley</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Fullerton</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Garden Grove</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Huntington Beach</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Irvine Orange</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>La Habra Orange</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>La Palma Orange</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Laguna Beach</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Laguna Hills</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Laguna Niguel</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Laguna Woods</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Lake Forest</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Los Alamitos</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Mission Viejo</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Newport Beach</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Orange</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Placentia</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Rancho Santa Margarita</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>San Clemente</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>San Juan Capistrano</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Santa Ana</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Seal Beach</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Stanton</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Tustin</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Villa Park</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Westminster</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Yorba Linda</td><td><span>County</span>Orange County</td></tr><tr><td><span>City</span>Corona</td><td><span>County</span>Riverside County</td></tr><tr><td><span>City</span>Eastvale</td><td><span>County</span>Riverside County</td></tr><tr><td><span>City</span>Jurupa Valley</td><td><span>County</span>Riverside County</td></tr><tr><td><span>City</span>Lake Elsinore</td><td><span>County</span>Riverside County</td></tr><tr><td><span>City</span>Murrieta</td><td><span>County</span>Riverside County</td></tr><tr><td><span>City</span>Norco</td><td><span>County</span>Riverside County</td></tr><tr><td><span>City</span>Pechanga Reservation</td><td><span>County</span>Riverside County</td></tr><tr><td><span>City</span>Riverside</td><td><span>County</span>Riverside County</td></tr><tr><td><span>City</span>Temecula</td><td><span>County</span>Riverside County</td></tr><tr><td><span>City</span>Wildomar</td><td><span>County</span>Riverside County</td></tr><tr><td><span>City</span>Citrus Heights</td><td><span>County</span>Sacramento County</td></tr><tr><td><span>City</span>Elk Grove</td><td><span>County</span>Sacramento County</td></tr><tr><td><span>City</span>Folsom</td><td><span>County</span>Sacramento County</td></tr><tr><td><span>City</span>Galt</td><td><span>County</span>Sacramento County</td></tr><tr><td><span>City</span>Isleton</td><td><span>County</span>Sacramento County</td></tr><tr><td><span>City</span>Rancho Cordova</td><td><span>County</span>Sacramento County</td></tr><tr><td><span>City</span>Sacramento</td><td><span>County</span>Sacramento County</td></tr><tr><td><span>City</span>Chino Hills</td><td><span>County</span>San Bernardino County</td></tr><tr><td><span>City</span>Fontana</td><td><span>County</span>San Bernardino County</td></tr><tr><td><span>City</span>Montclair</td><td><span>County</span>San Bernardino County</td></tr><tr><td><span>City</span>Ontario</td><td><span>County</span>San Bernardino County</td></tr><tr><td><span>City</span>Rancho Cucamonga</td><td><span>County</span>San Bernardino County</td></tr><tr><td><span>City</span>Rialto</td><td><span>County</span>San Bernardino County</td></tr><tr><td><span>City</span>Upland</td><td><span>County</span>San Bernardino County</td></tr><tr><td><span>City</span>Barona Reservation</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>Capitan Grande</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>Carlsbad</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>Chula Vista</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>Coronado</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>Del Mar</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>El Cajon</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>Encinitas</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>Escondido</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>Imperial Beach</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>La Mesa</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>Lemon Grove</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>National City</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>Oceanside</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>Poway</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>San Diego</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>San Marcos</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>Santee</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>Solana Beach</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>Sycuan Reservation</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>Vista</td><td><span>County</span>San Diego County</td></tr><tr><td><span>City</span>San Francisco</td><td><span>County</span>San Francisco County</td></tr><tr><td><span>City</span>Half Moon</td><td><span>County</span>San Mateo County</td></tr><tr><td><span>City</span>Brisbane</td><td><span>County</span>San Mateo County</td></tr><tr><td><span>City</span>Burlingame</td><td><span>County</span>San Mateo County</td></tr><tr><td><span>City</span>East Palo Alto</td><td><span>County</span>San Mateo County</td></tr><tr><td><span>City</span>Foster City</td><td><span>County</span>San Mateo County</td></tr><tr><td><span>City</span>Menlo Park</td><td><span>County</span>San Mateo County</td></tr><tr><td><span>City</span>Pacifica</td><td><span>County</span>San Mateo County</td></tr><tr><td><span>City</span>Redwood City</td><td><span>County</span>San Mateo County</td></tr><tr><td><span>City</span>San Mateo</td><td><span>County</span>San Mateo County</td></tr><tr><td><span>City</span>South San Francisco</td><td><span>County</span>San Mateo County</td></tr><tr><td><span>City</span>Milpitas</td><td><span>County</span>Santa Clara County</td></tr><tr><td><span>City</span>Mountain View</td><td><span>County</span>Santa Clara County</td></tr><tr><td><span>City</span>Palo Alto Santa</td><td><span>County</span>Santa Clara County</td></tr><tr><td><span>City</span>San Jose</td><td><span>County</span>Santa Clara County</td></tr><tr><td><span>City</span>Sunnyvale</td><td><span>County</span>Santa Clara County</td></tr><tr><td><span>City</span>Unincorporated Area (Lexington<br>Hills area, overlapping Santa<br>Clara and Santa Cruz Counties)</td><td><span>County</span>Santa Cruz County</td></tr><tr><td><span>City</span>Benicia</td><td><span>County</span>&nbsp;Solano County</td></tr><tr><td><span>City</span>Dixon</td><td><span>County</span>&nbsp;Solano County</td></tr><tr><td><span>City</span>Fairfield</td><td><span>County</span>&nbsp;Solano County</td></tr><tr><td><span>City</span>Rio Vista</td><td><span>County</span>&nbsp;Solano County</td></tr><tr><td><span>City</span>Suisun City</td><td><span>County</span>&nbsp;Solano County</td></tr><tr><td><span>City</span>Vacaville</td><td><span>County</span>&nbsp;Solano County</td></tr><tr><td><span>City</span>Vallejo</td><td><span>County</span>&nbsp;Solano County</td></tr><tr><td><span>City</span>Cloverdale</td><td><span>County</span>Sonoma County</td></tr><tr><td><span>City</span>Cotati</td><td><span>County</span>Sonoma County</td></tr><tr><td><span>City</span>Dry Creek Rancheria</td><td><span>County</span>Sonoma County</td></tr><tr><td><span>City</span>Healdsburg</td><td><span>County</span>Sonoma County</td></tr><tr><td><span>City</span>Lytton Rancheria</td><td><span>County</span>Sonoma County</td></tr><tr><td><span>City</span>Petaluma</td><td><span>County</span>Sonoma County</td></tr><tr><td><span>City</span>Rohnert Park</td><td><span>County</span>Sonoma County</td></tr><tr><td><span>City</span>Santa Rosa</td><td><span>County</span>Sonoma County</td></tr><tr><td><span>City</span>Sebastopol</td><td><span>County</span>Sonoma County</td></tr><tr><td><span>City</span>Sonoma</td><td><span>County</span>Sonoma County</td></tr><tr><td><span>City</span>Stewarts Point Rancheria</td><td><span>County</span>Sonoma County</td></tr><tr><td><span>City</span>Windsor</td><td><span>County</span>Sonoma County</td></tr><tr><td><span>City</span>Camarillo</td><td><span>County</span>Ventura County</td></tr><tr><td><span>City</span>Moorpark</td><td><span>County</span>Ventura County</td></tr><tr><td><span>City</span>Simi Valley</td><td><span>County</span>Ventura County</td></tr><tr><td><span>City</span>Thousand Oaks</td><td><span>County</span>Ventura County</td></tr><tr><td><span>City</span>Davis</td><td><span>County</span>&nbsp;Yolo County</td></tr><tr><td><span>City</span>Rumsey Indian Rancheria</td><td><span>County</span>&nbsp;Yolo County</td></tr><tr><td><span>City</span>West Sacramento Yolo</td><td><span>County</span>&nbsp;Yolo County</td></tr><tr><td><span>City</span>Winters</td><td><span>County</span>&nbsp;Yolo County</td></tr><tr><td><span>City</span>Woodland</td><td><span>County</span>&nbsp;Yolo County</td></tr></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Personal blogs are back, should niche blogs be next? (530 pts)]]></title>
            <link>https://disassociated.com/personal-blogs-back-niche-blogs-next/</link>
            <guid>46009894</guid>
            <pubDate>Fri, 21 Nov 2025 22:40:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://disassociated.com/personal-blogs-back-niche-blogs-next/">https://disassociated.com/personal-blogs-back-niche-blogs-next/</a>, See on <a href="https://news.ycombinator.com/item?id=46009894">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Is Matrix Multiplication Ugly? (138 pts)]]></title>
            <link>https://mathenchant.wordpress.com/2025/11/21/is-matrix-multiplication-ugly/</link>
            <guid>46009660</guid>
            <pubDate>Fri, 21 Nov 2025 22:17:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mathenchant.wordpress.com/2025/11/21/is-matrix-multiplication-ugly/">https://mathenchant.wordpress.com/2025/11/21/is-matrix-multiplication-ugly/</a>, See on <a href="https://news.ycombinator.com/item?id=46009660">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>A few weeks ago I was minding my own business, peacefully reading <a href="https://www.newyorker.com/magazine/2025/11/03/inside-the-data-centers-that-train-ai-and-drain-the-electrical-grid" target="_blank" rel="noreferrer noopener">a well-written and informative article about artificial intelligence</a>, when I was ambushed by a passage in the article that aroused my pique. That’s one of the pitfalls of knowing too much about a topic a journalist is discussing; journalists often make mistakes that most readers wouldn’t notice but that raise the hackles or at least the blood pressure of those in the know.</p>



<p>The article in question appeared in <em>The New Yorker</em>. The author, Stephen Witt, was writing about the way that your typical Large Language Model, starting from a blank slate, or rather a slate full of random scribbles, is able to learn about the world, or rather the virtual world called the internet. Throughout the training process, billions of numbers called weights get repeatedly updated so as to steadily improve the model’s performance. Picture a tiny chip with electrons racing around in etched channels, and slowly zoom out: there are many such chips in each server node and many such nodes in each rack, with racks organized in rows, many rows per hall, many halls per building, many buildings per campus. It’s a sort of computer-age version of Borges’ Library of Babel. And the weight-update process that all these countless circuits are carrying out depends heavily on an operation known as matrix multiplication.</p>



<p>Witt explained this clearly and accurately, right up to the point where his essay took a very odd turn.</p>



<p><strong>HAMMERING NAILS</strong></p>



<p>Here’s what Witt went on to say about matrix multiplication:</p>



<p>“‘Beauty is the first test: there is no permanent place in the world for ugly mathematics,’ the mathematician G. H. Hardy wrote, in 1940. But matrix multiplication, to which our civilization is now devoting so many of its marginal resources, has all the elegance of a man hammering a nail into a board. It is possessed of neither beauty nor symmetry: in fact, in matrix multiplication, <em>a</em> times <em>b</em> is not the same as <em>b</em> times <em>a</em>.”</p>



<p>The last sentence struck me as a bizarre non sequitur, somewhat akin to saying “Number addition has neither beauty nor symmetry, because when you write two numbers backwards, their new sum isn’t just their original sum written backwards; for instance, 17 plus 34 is 51, but 71 plus 43 isn’t 15.”</p>



<p>The next day I sent the following letter to the magazine:</p>



<p>“I appreciate Stephen Witt shining a spotlight on matrices, which deserve more attention today than ever before: they play important roles in ecology, economics, physics, and now artificial intelligence (“<em>Information Overload</em>”, November 3). But Witt errs in bringing Hardy’s famous quote (“there is no permanent place in the world for ugly mathematics”) into his story. Matrix algebra is the language of symmetry and transformation, and the fact that <em>a</em> followed by <em>b</em> differs from <em>b</em> followed by <em>a</em> is no surprise; to expect the two transformations to coincide is to seek symmetry in the wrong place — like judging a dog’s beauty by whether its tail resembles its head. With its two-thousand-year-old roots in China, matrix algebra has secured a permanent place in mathematics, and it passes the beauty test with flying colors. In fact, matrices are commonplace in number theory, the branch of pure mathematics Hardy loved most.”</p>



<p>Confining my reply to 150 words required some finesse. Notice for instance that the opening sentence does double duty: it leavens my many words of negative criticism with a few words of praise, and it stresses the importance of the topic, preëmptively<sup><a href="#end1">1</a></sup> rebutting editors who might be inclined to dismiss my correction as too arcane to merit publication.</p>



<p>I haven’t heard back from the editors, and I don’t expect to. Regardless, Witt’s misunderstanding deserves a more thorough response than 150 words can provide. Let’s see what I can do with 1500 words and a few pictures.</p>



<p><strong>THE GEOMETRY OF TRANSFORMATIONS</strong></p>



<p>As a static object, matrices are “just” rectangular arrays of numbers, but that doesn’t capture what they’re really about. If I had to express the essence of matrices in a single word, that word would be “transformation”.</p>



<p>One example of a transformation is the operation <em>f</em> that takes an image in the plane and flips it from left to right, as if in a vertical mirror.</p>


<div>
<figure><a href="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/f.jpg"><img data-attachment-id="7429" data-permalink="https://mathenchant.wordpress.com/2025/11/21/is-matrix-multiplication-ugly/f/" data-orig-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/f.jpg" data-orig-size="1008,705" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="f" data-image-description="" data-image-caption="" data-medium-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/f.jpg?w=300" data-large-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/f.jpg?w=625" width="1008" height="705" src="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/f.jpg?w=1008" alt="" srcset="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/f.jpg 1008w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/f.jpg?w=150 150w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/f.jpg?w=300 300w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/f.jpg?w=768 768w" sizes="(max-width: 1008px) 100vw, 1008px"></a></figure></div>


<p><br>Another example is the operation <em>g </em>that that takes an image in the plane and reflects it across a diagonal line that goes from lower left to upper right.</p>


<div>
<figure><a href="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/g-1.jpg"><img data-attachment-id="7433" data-permalink="https://mathenchant.wordpress.com/2025/11/21/is-matrix-multiplication-ugly/g-2/" data-orig-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/g-1.jpg" data-orig-size="983,983" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="g" data-image-description="" data-image-caption="" data-medium-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/g-1.jpg?w=300" data-large-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/g-1.jpg?w=625" width="983" height="983" src="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/g-1.jpg?w=983" alt="" srcset="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/g-1.jpg 983w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/g-1.jpg?w=150 150w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/g-1.jpg?w=300 300w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/g-1.jpg?w=768 768w" sizes="(max-width: 983px) 100vw, 983px"></a></figure></div>


<p><br>The key thing to notice here is that the effect of <em>f</em> followed by <em>g</em> is different from the effect of <em>g</em> followed by <em>f</em>. To see why, write a capital R on one side of a square piece of paper–preferably using a dark marker and/or translucent paper, so that you can still see the R even when the paper has been flipped over–and apply <em>f</em> followed by <em>g</em>; you’ll get the original R rotated by 90 degrees clockwise. But if instead, starting from that original R, you were to apply <em>g</em> followed by <em>f</em>, you’d get the original R rotated by 90 degrees <em>counterclockwise</em>.</p>



<figure><a href="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/both-1.jpg"><img data-attachment-id="7460" data-permalink="https://mathenchant.wordpress.com/2025/11/21/is-matrix-multiplication-ugly/both-3/" data-orig-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/both-1.jpg" data-orig-size="2563,1605" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="both" data-image-description="" data-image-caption="" data-medium-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/both-1.jpg?w=300" data-large-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/both-1.jpg?w=625" width="1024" height="641" src="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/both-1.jpg?w=1024" alt="" srcset="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/both-1.jpg?w=1024 1024w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/both-1.jpg?w=2048 2048w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/both-1.jpg?w=150 150w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/both-1.jpg?w=300 300w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/both-1.jpg?w=768 768w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/both-1.jpg?w=1440 1440w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>Same two operations, different outcomes! Symbolically we write <em>g</em> ◦ <em>f</em> ≠ <em>f</em> ◦ <em>g</em>, where <em>g</em> ◦ <em>f</em> means “First do <em>f</em>, then do <em>g</em>” and <em>f</em> ◦ <em>g</em> means “First do <em>g</em>, then <em>f</em>”.<sup><a href="#end2">2</a></sup> The symbol ◦ denotes the meta-operation (operation-on-operations) called <em>composition</em>.</p>



<p>The fact that the order in which transformations are applied can affect the outcome shouldn’t surprise you. After all, when you’re composing a salad, if you forget to pour on salad dressing until after you’ve topped the base salad with grated cheese, your guests will have a different dining experience than if you’d remembered to pour on the dressing first. Likewise, when you’re composing a melody, a C-sharp followed by a D is different from a D followed by a C-sharp. And as long as mathematicians used the word “composition” rather than “multiplication”, nobody found it paradoxical that in many contexts, order matters.</p>



<p><strong>THE ALGEBRA OF MATRICES</strong></p>



<p>If we use the usual <em>x</em>, <em>y</em> coordinates in the plane, the geometric operation <em>f</em> can be understood as the numerical operation that sends the pair (<em>x</em>, <em>y</em>) to the pair (−<em>x</em>, <em>y</em>), which we can represented via the 2-by-2 array</p>


<div>
<figure><a href="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat1.png"><img data-attachment-id="7440" data-permalink="https://mathenchant.wordpress.com/2025/11/21/is-matrix-multiplication-ugly/mat1/" data-orig-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat1.png" data-orig-size="408,272" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mat1" data-image-description="" data-image-caption="" data-medium-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat1.png?w=300" data-large-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat1.png?w=408" loading="lazy" width="408" height="272" src="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat1.png?w=408" alt="" srcset="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat1.png 408w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat1.png?w=150 150w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat1.png?w=300 300w" sizes="(max-width: 408px) 100vw, 408px"></a></figure></div>


<p>where more generally the array</p>


<div>
<figure><a href="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat2.png"><img data-attachment-id="7442" data-permalink="https://mathenchant.wordpress.com/2025/11/21/is-matrix-multiplication-ugly/mat2/" data-orig-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat2.png" data-orig-size="352,252" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mat2" data-image-description="" data-image-caption="" data-medium-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat2.png?w=300" data-large-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat2.png?w=352" loading="lazy" width="352" height="252" src="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat2.png?w=352" alt="" srcset="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat2.png 352w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat2.png?w=150 150w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat2.png?w=300 300w" sizes="(max-width: 352px) 100vw, 352px"></a></figure></div>


<p>stands for the transformation that sends the pair (<em>x</em>, <em>y</em>) to the pair (<em>ax</em>+<em>by</em>, <em>cx</em>+<em>dy</em>). This kind of array is called a <em>matrix</em>, and when we want to compose two operations like <em>f</em> and <em>g</em> together, all we have to do is combine the associated matrices under the rule that says that the matrix</p>


<div>
<figure><a href="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat2-1.png"><img data-attachment-id="7444" data-permalink="https://mathenchant.wordpress.com/2025/11/21/is-matrix-multiplication-ugly/mat2-2/" data-orig-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat2-1.png" data-orig-size="352,252" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mat2" data-image-description="" data-image-caption="" data-medium-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat2-1.png?w=300" data-large-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat2-1.png?w=352" loading="lazy" width="352" height="252" src="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat2-1.png?w=352" alt="" srcset="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat2-1.png 352w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat2-1.png?w=150 150w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat2-1.png?w=300 300w" sizes="(max-width: 352px) 100vw, 352px"></a></figure></div>


<p>composed with the matrix</p>


<div>
<figure><a href="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat3.png"><img data-attachment-id="7445" data-permalink="https://mathenchant.wordpress.com/2025/11/21/is-matrix-multiplication-ugly/mat3/" data-orig-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat3.png" data-orig-size="318,250" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mat3" data-image-description="" data-image-caption="" data-medium-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat3.png?w=300" data-large-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat3.png?w=318" loading="lazy" width="318" height="250" src="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat3.png?w=318" alt="" srcset="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat3.png 318w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat3.png?w=150 150w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat3.png?w=300 300w" sizes="(max-width: 318px) 100vw, 318px"></a></figure></div>


<p>equals the matrix</p>


<div>
<figure><a href="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat4.png"><img data-attachment-id="7447" data-permalink="https://mathenchant.wordpress.com/2025/11/21/is-matrix-multiplication-ugly/mat4/" data-orig-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat4.png" data-orig-size="726,236" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mat4" data-image-description="" data-image-caption="" data-medium-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat4.png?w=300" data-large-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat4.png?w=625" loading="lazy" width="726" height="236" src="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat4.png?w=726" alt="" srcset="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat4.png 726w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat4.png?w=150 150w, https://mathenchant.wordpress.com/wp-content/uploads/2025/11/mat4.png?w=300 300w" sizes="(max-width: 726px) 100vw, 726px"></a></figure></div>


<p>For more about where this formula comes from, see my Mathematical Enchantments essay “<a href="https://mathenchant.wordpress.com/2023/06/16/what-is-a-matrix/" target="_blank" rel="noreferrer noopener">What Is A Matrix</a>?”.</p>



<p>There’s nothing special about 2-by-2 matrices; you could compose two 3-by-3 matrices, or even two 1000-by-1000 matrices. Going in the other direction (smaller instead of bigger), if you look at 1-by-1 matrices, the composition of</p>


<div>
<figure><a href="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/trivial1.png"><img data-attachment-id="7450" data-permalink="https://mathenchant.wordpress.com/2025/11/21/is-matrix-multiplication-ugly/trivial1/" data-orig-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/trivial1.png" data-orig-size="110,106" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="trivial1" data-image-description="" data-image-caption="" data-medium-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/trivial1.png?w=110" data-large-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/trivial1.png?w=110" loading="lazy" width="110" height="106" src="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/trivial1.png?w=110" alt=""></a></figure></div>


<p>and</p>


<div>
<figure><a href="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/trivial2.png"><img data-attachment-id="7451" data-permalink="https://mathenchant.wordpress.com/2025/11/21/is-matrix-multiplication-ugly/trivial2/" data-orig-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/trivial2.png" data-orig-size="124,132" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="trivial2" data-image-description="" data-image-caption="" data-medium-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/trivial2.png?w=124" data-large-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/trivial2.png?w=124" loading="lazy" width="124" height="132" src="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/trivial2.png?w=124" alt=""></a></figure></div>


<p>is just</p>


<div>
<figure><a href="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/trivial3.png"><img data-attachment-id="7453" data-permalink="https://mathenchant.wordpress.com/2025/11/21/is-matrix-multiplication-ugly/trivial3/" data-orig-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/trivial3.png" data-orig-size="134,118" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="trivial3" data-image-description="" data-image-caption="" data-medium-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/trivial3.png?w=134" data-large-file="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/trivial3.png?w=134" loading="lazy" width="134" height="118" src="https://mathenchant.wordpress.com/wp-content/uploads/2025/11/trivial3.png?w=134" alt=""></a></figure></div>


<p>so ordinary number-multiplication arises as a special case of matrix composition; turning this around, we can see matrix-composition as a sort of generalized multiplication. So it was natural for mid-19th-century mathematicians to start using words like “multiply” and “product” instead of words like “compose” and “composition”, at roughly the same time they stopped talking about “substitutions” and “tableaux” and started to use the word “matrices”.</p>



<p>In importing the centuries-old symbolism for number multiplication into the new science of linear algebra, the 19th century algebraists were saying “Matrices behave kind of like numbers,” with the proviso “except when they don’t”. Witt is right when he says that when <em>A</em> and <em>B</em> are matrices, <em>A</em> times <em>B</em> is not always equal to <em>B</em> times <em>A</em>. Where he’s wrong is in asserting that is a blemish on linear algebra. Many mathematicians regard linear algebra as one of the most elegant sub-disciplines of mathematics ever devised, and it often serves as a role model for the kind of sleekness that a new mathematical discipline should strive to achieve. If you dislike matrix multiplication because <em>AB</em> isn’t always equal to <em>BA</em>, it’s because you haven’t yet learned what matrix multiplication is good for in math, physics, and many other subjects. It’s ironic that Witt invokes the notion of symmetry to disparage matrix multiplication, since matrix theory and an allied discipline called group theory are the tools mathematicians use in fleshing out our intuitive ideas about symmetry that arise in art and science.</p>



<p>So how did an intelligent person like Witt go so far astray?</p>



<p><strong>PROOFS VS CALCULATIONS</strong></p>



<p>I’m guessing that part of Witt’s confusion arises from the fact that actually multiplying matrices of numbers to get a matrix of bigger numbers can be very tedious, and tedium is psychologically adjacent to distaste and a perception of ugliness. But the tedium of matrix multiplication is tied up with its symmetry (whose existence Witt mistakenly denies). When you multiply two <em>n</em>-by-<em>n</em> matrices <em>A</em> and <em>B</em> in the straightforward way, you have to compute <em>n</em><sup>2 </sup>numbers in the same unvarying fashion, and each of those <em>n</em><sup>2</sup> numbers is the sum of <em>n</em> terms, and each of those <em>n</em> terms is the product of an element of <em>A</em> and an element of <em>B</em> in a simple way. It’s only human to get bored and inattentive and then make mistakes because the process is so repetitive. We tend to think of symmetry and beauty as synonyms, but sometimes excessive symmetry breeds ennui; repetition in excess can be repellent. Picture the Library of Babel and the existential dread the image summons.</p>



<p>G. H. Hardy, whose famous remark Witt quotes, was in the business of proving theorems, and he favored conceptual proofs over calculational ones. If you showed him a proof of a theorem in which the linchpin of your argument was a 5-page verification that a certain matrix product had a particular value, he’d say you didn’t really understand your own theorem; he’d assert that you should find a more conceptual argument and then consign your brute-force proof to the trash. But Hardy’s aversion to brute force was specific to the domain of mathematical proof, which is far removed from math that calculates optimal pricing for annuities or computes the wind-shear on an airplane wing or fine-tunes the weights used by an AI. Furthermore, Hardy’s objection to your proof would focus on the length of the calculation, and not on whether the calculation involved matrices. If you showed him a proof that used 5 turgid pages of pre-19th-century calculation that never mentioned matrices once, he’d still say “Your proof is a piece of temporary mathematics; it convinces the reader that your theorem is true without truly explaining <em>why</em> the theorem is true.”</p>



<p>If you forced me at gunpoint to multiply two 5-by-5 matrices together, I’d be extremely unhappy, and not just because you were threatening my life; the task would be inherently unpleasant. But the same would be true if you asked me to add together a hundred random two-digit numbers. It’s not that matrix-multiplication or number-addition is ugly; it’s that such repetitive tasks are the diametrical opposite of the kind of conceptual thinking that Hardy loved and I love too. Any kind of mathematical content can be made stultifying when it’s stripped of its meaning and reduced to mindless toil. But that casts no shade on the underlying concepts. When we outsource number-addition or matrix-multiplication to a computer, we rightfully delegate the soul-crushing part of our labor to circuitry that has no soul. If we could peer into the innards of the circuits doing all those matrix multiplications, we would indeed see a nightmarish, Borgesian landscape, with billions of nails being hammered into billions of boards, over and over again. But please don’t confuse that labor with mathematics.</p>



<p><em><a href="https://news.ycombinator.com/item?id=46009660">Join the discussion of this essay over at Hacker News!</a></em></p>



<p><strong>This essay</strong>&nbsp;<strong>is related to chapter 10 (“Out of the Womb”) of a book I’m writing, tentatively called “What Can Numbers Be?: The Further, Stranger Adventures of Plus and Times”. If you think this sounds interesting and want to help me make the book better, check out&nbsp;<a href="http://jamespropp.org/readers.pdf">http://jamespropp.org/readers.pdf</a>. And as always, feel free to submit comments on this essay at the Mathematical Enchantments WordPress site!</strong></p>



<p><strong>ENDNOTES</strong></p>



<p id="end1">#1. Note the <em>New Yorker</em>-ish diaresis in “preëmptively”: as long as I’m being critical, I might as well be diacritical.</p>



<p id="end2">#2. I know this convention may seem backwards on first acquaintance, but this is how ◦ is defined. Blame the people who first started writing things like “log <em>x</em>” and “cos <em>x</em>“, with the <em>x</em> coming after the name of the operation. This led to the notation <em>f</em>(<em>x</em>) for the result of applying the function <em>f</em> to the number <em>x</em>. Then the symbol for the result of applying <em>g</em> to the result of applying <em>f</em> to <em>x</em> is <em>g</em>(<em>f</em>(<em>x</em>)); even though <em>f</em> is performed first, “<em>f</em>” appears to the right of “<em>g</em>“. From there, it became natural to write the function that sends <em>x</em> to <em>g</em>(<em>f</em>(<em>x</em>)) as “<em>g</em> ◦ <em>f</em>“.</p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LAPD helicopter tracker with real-time operating costs (209 pts)]]></title>
            <link>https://lapdhelicoptertracker.com/</link>
            <guid>46009591</guid>
            <pubDate>Fri, 21 Nov 2025 22:11:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lapdhelicoptertracker.com/">https://lapdhelicoptertracker.com/</a>, See on <a href="https://news.ycombinator.com/item?id=46009591">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Pixar: The Early Days A never-before-seen 1996 interview (152 pts)]]></title>
            <link>https://stevejobsarchive.com/stories/pixar-early-days</link>
            <guid>46008769</guid>
            <pubDate>Fri, 21 Nov 2025 20:45:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stevejobsarchive.com/stories/pixar-early-days">https://stevejobsarchive.com/stories/pixar-early-days</a>, See on <a href="https://news.ycombinator.com/item?id=46008769">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>To mark <i>Toy Story</i>’s 30th anniversary, we’re sharing a never-before-seen interview with Steve from November 22, 1996—exactly one year after the film debuted in theaters.</p><p><i>Toy Story</i> was the world’s first entirely computer-animated feature-length film. An instant hit with audiences and critics, it also transformed Pixar, which went public the week after its premiere. Buoyed by <i>Toy Story</i>’s success, Pixar’s stock price closed at nearly double its initial offering, giving it a market valuation of approximately $1.5 billion and marking the largest IPO of 1995. The following year, <i>Toy Story </i>was nominated for three Academy Awards en route to winning a Special Achievement Oscar in March. In July, Pixar announced that it would close its television-commercial unit to focus primarily on feature films. By the time of the interview, the team had grown by 70 percent in less than a year; <i>A Bug’s Life</i> was in production; and behind the scenes, Steve was using his new leverage to renegotiate Pixar’s partnership with Disney.</p><p>
In this footage, Steve reveals the long game behind Pixar’s seeming overnight success. With striking clarity, he explains how its business model gives artists and engineers a stake in their creations, and he reflects on what Disney’s hard-won wisdom taught him about focus and discipline. He also talks about the challenge of leading a team so talented that it inverts the usual hierarchy, the incentives that inspire people to stay with the company, and the deeper purpose that unites them all: to tell stories that last and put something of enduring value into the culture.&nbsp;&nbsp;
</p><p>At Pixar, Steve collaborated closely with president Ed Catmull and refined a management approach centered on creating the conditions for talent to thrive. When he returned to Apple a few weeks after this interview, his experience at Pixar shaped how he saw his role as CEO: building a company on timeless ideas made new through technology.&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boom, bubble, bust, boom. Why should AI be different? (147 pts)]]></title>
            <link>https://crazystupidtech.com/2025/11/21/boom-bubble-bust-boom-why-should-ai-be-different/</link>
            <guid>46008628</guid>
            <pubDate>Fri, 21 Nov 2025 20:30:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://crazystupidtech.com/2025/11/21/boom-bubble-bust-boom-why-should-ai-be-different/">https://crazystupidtech.com/2025/11/21/boom-bubble-bust-boom-why-should-ai-be-different/</a>, See on <a href="https://news.ycombinator.com/item?id=46008628">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>




<p>The artificial intelligence revolution will be only three years old at the end of November. Think about that for a moment. In just 36 months AI has gone from great-new-toy, to global phenomenon, to where we are today – debating whether we are in one of the biggest technology bubbles or booms in modern times.</p>



<p>To us what’s happening is obvious. We both covered the internet bubble 25 years ago. We’ve been writing about – and in Om’s case investing in – technology since then. We can both say unequivocally that the conversations we are having now about the future of AI feel exactly like the conversations we had about the future of the internet in 1999.&nbsp;</p>



<p>We’re not only in a bubble but one that is arguably the biggest technology mania any of us have ever witnessed. We’re even back reinventing time. Back in 1999 we talked about <a href="https://m.rediff.com/computer/1999/dec/08vint.htm">internet time,</a> where every year in the new economy was like <a href="https://www.amazon.com/21-Dog-Years-Cube-Dwellers/dp/0743225805">a dog year</a> – equivalent to seven years in the old.&nbsp;</p>



<p>Now VCs, investors and <a href="https://www.mckinsey.com/capabilities/tech-and-ai/our-insights/lead-or-lag-ceos-must-embrace-ai-at-full-speed?utm_source=chatgpt.com">executives </a>are <a href="https://uplandsoftware.com/rightanswers/resources/podcast/generative-ai-and-knowledge-management/?utm_source=chatgpt.com">talking </a>about <a href="https://www.uxtigers.com/post/ux-roundup-20250512?utm_source=chatgpt.com">AI dog years </a>– let’s just call them <a href="https://www.jax.org/news-and-insights/jax-blog/2017/november/when-are-mice-considered-old">mouse years</a> –&nbsp; <a href="https://experimentationmachine.com/p/the-experimentation-machine-how-ai-is-creating-10x-founders-jeffrey-bussgang-alvaro-rodr-guez">which is internet time </a>divided by five? Or is it by 11? Or 12? &nbsp; Sure, things move way faster than they did a generation ago. But by that math one year today now equals 35 years in 1995. Really?&nbsp;</p>



<p>We’re also months, not years, from the end of the party. We may be even closer than that.&nbsp; NVIDIA posted better than expected earnings on Wednesday. And it briefly looked like that would buoy all AI stocks. It didn’t.&nbsp;</p>



<p>All but Alphabet have seen big share declines in the past month. Microsoft is down 12 percent, Amazon is down 14 percent, Meta is down 22 percent, Oracle is down 24 percent, and Corweave’s stock has been almost cut in half, down 47 percent. Investors are increasingly worried that everyone is overspending on AI.</p>



<p>All this means two things to us: 1)The AI revolution will indeed be one of the biggest technology shifts in history. It will spark a generation of innovations that we can’t yet even imagine. 2) It’s going to take way longer to see those changes than we think it’s going to take right now.&nbsp;</p>



<p>Why? Because we humans are pretty good at predicting the impact of technology revolutions beyond seven to ten years. But we’re terrible at it inside that time period. We’re too prone to connect a handful of early data points, to assume that’s the permanent slope of that line and to therefore invest too much too soon. That’s what’s going on right now.</p>



<p>Not only does the AI bubble in 2025 <em>feel</em> like the internet bubble in 1999, the data suggests it may actually be larger. The latest estimates for just<a href="https://www.ubs.com/global/en/wealthmanagement/insights/chief-investment-office/house-view/daily/2025/latest-04112025.html"> global AI capital expenditures</a> plus global <a href="https://www.bloomberg.com/news/articles/2025-10-03/ai-is-dominating-2025-vc-investing-pulling-in-192-7-billion">venture capital investments</a> already exceed $600 billion for this year. And in September Gartner published estimates that suggested all AI-related spending worldwide in 2025 might <a href="https://www.gartner.com/en/newsroom/press-releases/2025-09-17-gartner-says-worldwide-ai-spending-will-total-1-point-5-trillion-in-2025">top $1.5 trillion. </a>&nbsp;</p>



<p>I had ChatGPT (<em>of course</em>) find sources and crunch some numbers for the size of the internet bubble in 1999 and<a href="https://chatgpt.com/s/t_691f9df162e881919237cb8a58c42859"> came up with about $360 billion in 2025 dollars, $185 billion in 1999 dollars.</a>&nbsp;&nbsp;</p>



<p>The spending is also happening in a fraction of the time. The internet bubble took 4.6 years to inflate before it burst. The AI bubble has inflated in two-thirds the time. If the AI bubble manages to actually last as long as the internet bubble – another 20 months – just spending on AI capital expenses by the big tech companies<a href="https://www.nextplatform.com/2025/09/08/idc-makes-ebullient-ai-spending-forecast-out-to-2029/"> is projected to hit $750 billion annually </a>by the end of 2027, 75 percent more than now.&nbsp;</p>



<p>That means total AI spending for 2029 would be well over $1 trillion.&nbsp; One of the things both of us have learned in our careers is that when numbers are so large they don’t make sense, they usually don’t make sense.&nbsp;</p>



<p>Sure, there are important differences between the internet bubble and the AI bubble. History rhymes. It doesn’t repeat. A lot of the early money to build AI data centers and train LLMs has been coming out of the giant bank accounts of the big tech companies. The rest has been largely financed by private professional investors.&nbsp;</p>



<p>During the internet bubble, public market investors, especially individuals, threw billions at tiny profitless companies betting they’d develop a business before the money ran out. And dozens of telecom startups borrowed hundreds of billions to string fiber optic cables across oceans and continents betting that exploding internet usage would justify that investment.&nbsp;</p>



<p>Neither bet happened fast enough for investors and lenders. Most of the dot coms were liquidated. Most of the telecom companies declared bankruptcy and were sold for pennies on the dollar.</p>



<p>But does that make the AI bubble less scary than the internet bubble? Not to us. It actually might be scarier. The amounts at risk are greater, and the exposure is way more concentrated. Microsoft, Alphabet, Meta, Amazon, NVIDIA, Oracle and Apple together represent roughly a third of the value of the critical S&amp;P 500 stock market index. More importantly, over the last six months the spending has become increasingly leveraged and nonsensical.&nbsp;</p>



<p>None of these companies has proven yet that AI is a good enough business to justify all this <a href="https://unchartedterritories.tomaspueyo.com/p/is-there-an-ai-bubble">spending</a>. But the first four are now each spending $70 billion to $100 billion a year to fund data centers and other capital intensive AI expenses. Oracle is spending roughly $20 billion a year.&nbsp;</p>



<p>If the demand curve shifts for any or all of these companies, and a few of them have to take, say a $25 billion write down on their data center investments, that’s an enormous amount of money even for these giants.&nbsp;</p>



<p>And when you add in companies like OpenAI, AMD and CoreWeave plus the slew of other LLM and data center builders, their fortunes look incredibly intertwined. If investors get spooked about future returns from any one of those companies, the contagion could spread quickly.&nbsp;</p>



<p>Yes, by one measure AI stocks aren’t over valued at all. Cisco’s P/E peaked at 200 during the internet bubble. NVIDIA’s P/E is about 45. The P/E of the NASDAQ-100 is about 35 now. <a href="https://www.paceretfs.com/resources/resource-library/the-nasdaq-100-is-this-time-really-different#:~:text=At%20the%20end%20of%201999,2.68%25%20free%20cash%20flow%20yield.">It was 73 at the end of 1999</a>. But looking at the S&amp;P 500 tells a scarier story. Excluding the years around Covid-19, the last time the P/E ratio of that index was as high as it is now – about 24 – was right before the internet bubble popped in March 2000.&nbsp;&nbsp;&nbsp;</p>



<p>Here are two other worrisome differences between then and now:&nbsp;</p>



<p>1) The overall US economic, social and political situation is much more unstable than it was 25 years ago. Back then the US was still basking in the glow of having won the Cold War. It had the most dominant economy and stable political standing in the world. Today economic growth is slower, the national debt and government spending have never been higher, and the nation is more politically divided than it has been in two generations.&nbsp;</p>



<p>2)The AI revolution is becoming a major national security issue. That ties valuations to the current unpredictability of US foreign policy and tariffs. China has become as formidable a competitor to the US in AI as the Soviet Union was to the US in the 1950s and 1960s. It doesn’t require much imagination to think about what might happen to the US AI market should China come up with a technical advance that had more staying power than the <a href="https://crazystupidtech.com/2025/02/02/what-deepseek-means-for-everyone/">DeepSeek scare</a> at the beginning of this year.</p>



<p>Even OpenAI’s Sam Altman, Amazon’s Jeff Bezos, JP Morgan’s Jamie Dimon, and just this week, Alphabet’s Sundar Pichai are now acknowledging they are seeing signs of business excess.<a href="https://www.bbc.com/news/articles/cwy7vrd8k4eo"> Pichai said the following to the BBC on Tuesday</a>: “Given the potential for this technology (AI), the excitement is very rational. It is also&nbsp; true that when we go through these investment cycles there are moments where we overshoot …. We can look back at the internet right now. There was clearly a lot of excess investment. But none of us would question whether the internet was profound …. It fundamentally changed how we work as a society. I expect AI to be the same.”&nbsp;</p>



<p>When will the mania end? There’s hundreds of billions of dollars of guaranteed but unspent capital in the system, which suggests it will go on well into 2026. But in times like these a secular investor sentiment change can happen in a matter of weeks, driving down stock prices, driving up the cost of capital, and making every financial model that had said “let’s invest” to one saying “not on your life.”&nbsp;</p>



<p>A technology change with more staying power than DeepSeek would certainly do it. So would President Trump changing his mind about greasing the approval process for new AI data centers. All it would take would be an off hand remark from a Silicon Valley titan he didn’t like.&nbsp;</p>



<p>Or what’s already happening with AI stocks could snowball. Investors have hammered those stocks because they’ve gotten jumpy about the size of their AI spending and in Oracle and Coreweave’s case, the leverage they are using to pay for it all. NVIDIA’s better than expected earnings announced Wednesday might ultimately calm things. But don’t expect any of these issues to go away.&nbsp;</p>



<p>If you want to go further, what we’ve done is lay out the four big vulnerabilities we’re worried about with separate headings. And, of course, if you have an entirely different set of numbers that you think shows we’re nowhere near bubble territory, have suggestions about how to refine ours, or think we left something out, please share.&nbsp;</p>



<p>To us the four big vulnerabilities are:&nbsp;</p>



<p><strong>Too much spending.&nbsp;&nbsp;</strong></p>



<p><strong>Too much leverage.</strong></p>



<p><strong>Crazy deals.&nbsp;</strong></p>



<p><strong>China. China. China.&nbsp;</strong></p>



<p>*****</p>



<p><strong><em>Too much spending:&nbsp;</em></strong></p>



<p>We all know two things about the AI bubble right now: 1)People, companies and researchers will pay for AI. 2)They aren’t paying nearly enough to justify the hundreds of billions of dollars that has been committed to it yet.</p>



<p>The thinking, of course, is that that gap will quickly disappear and be replaced with enough paid usage to generate enormous profits. The questions that no one has the answer to are: When will that happen?&nbsp; How much more money will it take? And which approach to making money will work the best?&nbsp;</p>



<p>Will it work better just to charge for AI based on usage the way Microsoft, Oracle, Amazon, and OpenAI are focused on? Will it be more of an indirect revenue driver the way Meta is approaching it with its open source models? Will it have an advertising component the way Alphabet is exploring?&nbsp;</p>



<p>Or will it be a do-everything, vertically integrated approach that works best? Amazon and Meta are exploring this. But Alphabet is the furthest ahead. It not only has its own AI software but is also using a lot of its own graphics processing chips known as Tensor Processing Units. This gives it much more control over processing costs than competitors who are – at least for the moment – entirely dependent on NVIDIA and AMD graphics processing chips.&nbsp;</p>



<p>The only thing everyone agrees on is that the stakes are enormous: Digital technology revolutions historically have been winner-take-all-affairs whether in mainframes, minicomputers, personal computers, chips, software, search, or smartphones. That means there are likely to be only a couple of dominant AI providers five years from now.&nbsp;</p>



<p>Maybe they’ll only be one, if one of them manages to truly get their system to reach artificial general intelligence. What it certainly means, however, is that, as in the past, there will be way more losers than winners, and there will be many big companies with giant holes in their balance sheets.&nbsp;</p>



<p>OpenAI has become exhibit A in this spending frenzy partly because it’s the leading AI chatbot and helped ignite the AI revolution with ChaptGPT version 3 in November 2022.&nbsp;&nbsp;</p>



<p>It’s also because, frankly, it’s hard to look away from the company’s financial highwire act. Its competitors have other businesses they can fall back on. OpenAI must make its bet on AI work, or it becomes one of the biggest meltdowns in the history of business.&nbsp;</p>



<p>This is a company that hasn’t come close to making a profit or even being cash flow positive, but <a href="https://apnews.com/article/openai-500b-valuation-chatgpt-53dffc56355460a232439c76d1ccf22b">investors last valued it at $500 billion</a>. That would rank it as the 21st most valuable company in the stock market, with BankAmerica. And at the end of October it made changes to its corporate structure that would allow it to have a traditional IPO in a year or two. There was speculation that that could value the company at $1 trillion.&nbsp;</p>



<p>In the past three years OpenAI has raised <a href="https://tracxn.com/d/companies/openai/__kElhSG7uVGeFk1i71Co9-nwFtmtyMVT7f-YHMn4TFBg/funding-and-investors#funding-rounds">more than $55 billion</a>, according to published reports.&nbsp; And while its revenues for 2025 seem to be o<a href="https://www.theinformation.com/articles/openai-hits-12-billion-annualized-revenue-breaks-700-million-chatgpt-weekly-active-users?rc=1ej5u1">n track to hit $12 billion, t</a>he <a href="https://fortune.com/2025/11/01/sam-altman-openai-annual-revenue-13-billion-forecast-100-billion-2027/">company</a> is burning through cash quickly.</p>



<p>Its cash burn this year is<a href="https://www.theinformation.com/articles/openai-says-business-will-burn-115-billion-2029?rc=1ej5u1"> expected to top $8 billion and top $17 </a>billion in 2026. I<a href="https://www.ft.com/content/a169703c-c4df-46d6-a2d3-4184c74bbaf7">t says it expects to spend nearly half a trillion dollars on server rentals over the next five years</a>, and says it doesn’t expect to be generating more cash from operations than it is spending until 2029. That’s when it expects revenues to top $100 million. It agreed to pay <a href="https://crazystupidtech.com/2025/05/21/open-ai-buys-jony-ive/">nearly $7 billion</a> for former Apple design chief Jonny Ive’s startup IO, in May.&nbsp;</p>



<p>“Eventually we need to get to hundreds of billions of a year in revenue,” <a href="https://openai.com/live/">CEO Sam Altman</a> said in response to a question about OpenAIs finances at the end of October. “I expect enterprise to be a huge revenue driver for us, but I think consumer really will be too. And it won’t just be this (ChatGPT) subscription, but we’ll have new products, devices and tons of other things. And this says nothing about what it would really mean to have AI discovering science and all of those revenue possibilities.”&nbsp;</p>



<p>We’ve seen this movie before, of course. Whether we’re looking at the railroad construction bubble in the US 150 years ago or the internet bubble 25 years ago, investors touting the wisdom of “get big fast” have often been endemic to technology revolutions.</p>



<p>It’s what made Amazon the OpenAI of the internet bubble.&nbsp; “How could a company with zero profits and an unproven business model, spend so much money and ever generate an acceptable return for investors?” we asked&nbsp;</p>



<p>And most of the criticism about Amazon, the online retailer, actually turned out to be true. Yes, Amazon is now one of the most successful companies in the world. But that only happened because of something Amazon discovered ten years after its founding in 1994 – Amazon Web Services, its hugely profitable cloud computing business.&nbsp;</p>



<p>Like many predicted, the margins in online retailing were not meaningfully different from the single digit margins in traditional retailing. That meant that Amazon wasn’t a profitable enough business to justify all that spending. If you had invested in Amazon at the peak of the internet bubble, you would have waited another decade before your investment would have started generating returns.&nbsp;</p>



<p>And here’s the thing that makes eyes bulge: OpenAI’s&nbsp; expected spend, just based on the money it’s raised so far, is set up to be <a href="https://chatgpt.com/s/t_691fb9fa7d148191b64a0ced8f3ab51c">16 times what Amazon spent during its first five years even when adjusting that number into 2025 dollars.</a>&nbsp;</p>



<p>It’s not just the size of the investments and the lack of a business model yet to justify them, that concerns analysts and investors like<a href="https://www.bondcap.com/report/pdf/Trends_Artificial_Intelligence.pdf"> Mary Meeker at Bond Capita</a>l. It’s that the prices that AI providers can charge are also falling. “For model providers this raises real questions about monetization and profits,” she said in a 350 page report on the future of AI at the end of May. “Training is expensive, serving is getting cheap, and pricing power is slipping. The business model is in flux. And there are new questions about the one-size-fits-all LLM approach, with smaller, cheaper models trained for custom use cases now emerging.</p>



<p>“Will providers try to build horizontal platforms? Will they dive into specialized applications? Will one or two leaders drive dominant user and usage share and related monetization, be it subscriptions (easily enabled by digital payment providers), digital services, ads, etc.? Only time will tell. In the short term, it’s hard to ignore that the economics of general-purpose LLMs look like commodity businesses with venture-scale burn.”</p>



<p>*****</p>



<p><strong><em>Too much leverage:</em></strong></p>



<p><a href="https://www.bloomberg.com/news/features/2025-10-07/openai-s-nvidia-amd-deals-boost-1-trillion-ai-boom-with-circular-deals">Bloomberg, </a><a href="https://www.barrons.com/articles/nvidia-microsoft-openai-circular-financing-ai-bubble-5d9a4e7c?gaa_at=eafs&amp;gaa_n=AWEtsqffZdAqzGJWmVDj7EHE3ibHB4kSZucPVDVKW-6k3xcsz-S-He9NKgiUHnzvD7M%3D&amp;gaa_ts=690fcaeb&amp;gaa_sig=nqijwD1BnFjnB7N2TIN24uTaXVlnE5fiYWse48g-TBnhS2Y35FIiZ2MTlPobl-dxAxHAMV0Fdu8mNJZ2-zPCVQ%3D%3D">Barron’s,</a> <a href="https://www.nytimes.com/interactive/2025/10/31/technology/openai-fundraising-deals.html">The New York Times</a> and the<a href="https://www.ft.com/content/5f6f78af-aed9-43a5-8e31-2df7851ceb67"> Financial Times</a> have all published graphics in the past month to help investors visualize the slew of&nbsp; hard to parse, seemingly circular, vendor financing deals involving the biggest players in AI. They make your head hurt. And that’s a big part of the problem.</p>



<p>What’s clear is that NVIDIA and OpenAI have begun acting like banks and VC investors to the tune of hundreds of billions of dollars to keep the AI ecosystem lubricated. What’s unclear is who owes what to whom under what conditions.</p>



<p>NVIDIA wants to guarantee ample demand for its graphics processing units. So it has&nbsp; participated in<a href="https://techcrunch.com/2025/10/12/nvidias-ai-empire-a-look-at-its-top-startup-investments/"> 52 different venture investment deals for AI companies in 2024 and had already done 50 deals by the end of September this year, according to data from PitchBook. </a>That includes participating in six deals that raised more than $1 billion,&nbsp;&nbsp;&nbsp;</p>



<p>It’s these big deals that have attracted particular attention.&nbsp; NVIDIA is investing as much as $100 billion in OpenAI, another $2 billion in Elon Musk’s xAI, agreed to take a 7 percent stake in CoreWeave’s IPO and, because it rents access to NVIDIA chips, buy $6.3 billion in cloud service from them.&nbsp;The latest deal came earlier this week. <a href="https://www.wsj.com/tech/ai/nvidia-microsoft-and-anthropic-commit-to-roughly-45-billion-in-ai-partnership-281b7b1d?gaa_at=eafs&amp;gaa_n=AWEtsqcjWFHIda9BAA_Hu2i3j91klpXastD_chXCVh7zXc5It_FFVrY2-XEpi8KXysg%3D&amp;gaa_ts=6920c1c8&amp;gaa_sig=t6dDHkj281QbxClJc59IQba5-gzhvhz2gtizQ4mvYlREBlpcpR7s69zScpJIU_huZgzyA2djGYwyHHybHuaDVQ%3D%3D"> NVIDIA and Microsoft said that together they would invest up to $15 billion in Anthropic in exchange for Anthropic buying $30 billion in computiong capaicty from Microsoft running NVIDIA AI systems. </a></p>



<p>OpenAI, meanwhile, has become data center builders and suppliers best friend. It needs to ensure it has unfettered access not only to GPUs, but data centers to run them. So it has committed to filling its data centers with NVIDIA and AMD chips, and inked a $300 billion deal with Oracle and a $22.4 billion deal with CoreWeave for cloud and data center construction and management. OpenAI received $350 million in CoreWeave equity ahead of its IPO in return. It also became AMDs largest shareholder.&nbsp;</p>



<p>These deals aren’t technically classified as vendor financing – where a chip/server maker or cloud provider lends money to or invests in a customer to ensure they have the money to keep buying their products. But they sure look like them.&nbsp;</p>



<p>Yes, vendor financing is as old as Silicon Valley.&nbsp; But these deals add leverage to the system. If too many customers run into financial trouble, the impact on lenders and investors is exponentially severe. Not only do vendors experience cratering demand for future sales, they have to write down a slew of loans and/or investments on top of that.&nbsp;</p>



<p><a href="https://www.economicpolicyresearch.org/images/docs/SCEPA_blog/the_financial_crisis/lazonick_paper2_panel6.pdf">Lucent Technologies </a>was a huge player in the vendor financing game during the internet bubble, helping all the new telecom companies finance their telecom equipment purchases to the tune of billions of dollars. But when those telecom companies failed, Lucent never recovered.&nbsp;</p>



<p>The other problem with leverage is that once it starts, it’s like a drug. You see competitors borrowing money to build data centers and you feel pressure to do the same thing Oracle and Coreweave have already gone deeply in debt to keep up. Oracle<a href="https://www.wsj.com/tech/oracle-was-an-ai-darling-on-wall-street-then-reality-set-in-0d173758"> just issued $18 billion </a>in bonds bringing its total borrowing over $100 billion. It’s expected to ask investors for <a href="https://www.bloomberg.com/news/articles/2025-10-23/record-38-billion-debt-sale-nears-for-oracle-tied-data-centers">another $38 billion soon. </a>Analysts expect it to double that borrowing in the next few years.&nbsp;</p>



<p>And <a href="https://fortune.com/2025/11/08/coreweave-earnings-debt-ai-infrastructure-bubble/">Coreweave</a>, the former crypto miner turned data center service provider, unveiled in its IPO documents earlier this year that it has borrowed so much money that its debt payments represent 25 percent of its revenues. Shares of both these companies have taken a beating in the past few weeks as investors have grown increasingly worried about their debt load.&nbsp;&nbsp;</p>



<p>The borrowing isn’t limited to those who have few other options. <a href="https://fortune.com/2025/11/19/big-5-ai-hyperscalers-quadruple-debt-fund-ai-operations/">Microsoft, Alphabet and Amazon</a> have recently announced deals to borrow money, something each company historically has avoided.&nbsp;</p>



<p>And it’s not just leverage in the AI markets that have begun to worry lenders, investors and executives. Leverage is building in the $2 trillion private credit market. <a href="https://www.wsj.com/finance/investing/blackrock-etfs-among-biggest-investors-in-metas-giant-data-center-debt-deal-087fe671">&nbsp;Meta just announced </a>a $27 billion deal with private credit lender Blue Owl to finance its data center in Louisiana. It’s the largest private credit deal ever. By owning only 20 percent of the joint venture known as Hyperion, Meta gets most of the risk off its balance sheet, but maintains full access to the processing power of the data center when it’s complete.</p>



<p>Private credit has largely replaced middle market bank lending since the financial crisis. The new post crisis regulations banks needed to meet to make many of those loans proved too onerous. And since the world of finance abhors a vacuum, hedge funds and other big investors jumped in.&nbsp;</p>



<p>Banks soon discovered they could replace that business just by lending to the private credit lenders. What makes these loans so attractive is exactly what makes them dangerous in booming markets: Private credit lenders don’t have the same capital requirements or transparency requirements that banks have.</p>



<p>And two<a href="https://www.bloomberg.com/news/articles/2025-08-21/default-warnings-start-to-pile-up-in-the-private-credit-market?utm_medium=cpc_search&amp;utm_campaign=NB_ENG_DSAXX_DSAXXXXXXXXXX_EVG_XXXX_XXX_COUSA_EN_EN_X_BLOM_GO_SE_XXX_XXXXXXXXXX&amp;gclsrc=aw.ds&amp;gad_source=1&amp;gad_campaignid=9835680891&amp;gbraid=0AAAAAD9e5yrN6SgC9w9Wlcxmy3_EUEje_&amp;gclid=CjwKCAiA8bvIBhBJEiwAu5ayrLecMr2Mtr49thPtvoDxJDPC9PNSmHJzDhS4eTVNa070YWUVF5fz3RoCdXMQAvD_BwE"> private credit </a>bankruptcies in the last two months – Tricolor Holdings and First Brands – have executives and analysts wondering if underwriting rules have gotten too lax.</p>



<p>“My antenna goes up when things like that happen,”<a href="https://fortune.com/2025/10/15/jamie-dimon-issues-private-credit-warning-when-you-see-one-cockroach-there-are-probably-more/"> JP Morgan CEO Jamie Dimon</a> told investors. “And I probably shouldn’t say this, but when you see one cockroach, there are probably more. And so we should—everyone should be forewarned on this one….&nbsp; I expect it to be a little bit worse than other people expect it to be, because we don’t know all the underwriting standards that all of these people did.”</p>



<p>*****</p>



<p><strong><em>Crazy deals:&nbsp;</em></strong></p>



<p>Even if you weren’t even alive during the internet bubble, you’ve likely heard of<a href="https://en.wikipedia.org/wiki/Webvan"> Webvan</a> if you pay any attention to business. Why? Because of all the questionable deals that emerged from that period, it seemed to be the craziest. The company bet it could be the first and only company to tackle grocery home delivery nationwide, and that it could offer customers delivery within a 30 minute window of their choosing. Logistics like this is one of the most difficult business operations to get right. Webvan’s management said the internet changed all those rules. And investors believed them.&nbsp;</p>



<p>It raised $400 million from top VCs and another $375 million in an IPO totaling $1.5 billion in today’s dollars and a valuation in today’s dollars of nearly $10 billion. Five years after starting and a mere 18 months after its IPO, it was gone. Benchmark, Sequoia, Softbank, Goldman Sachs, Yahoo, and Etrade all signed up for this craziness and lost their shirts.&nbsp;</p>



<p>Is Mira Murati’s Thinking Machines the next Webvan? It’s certainly too soon to answer that question. But it’s certainly not too soon to ask. Webvan took four years to raise $1.5 billion in 2025 dollars. <a href="https://www.theinformation.com/articles/10-billion-enigma-mira-murati?rc=1ej5u1">Thinking Machines’ first and only fund raise this summer raised $2 billio</a>n. Ten top VCs piled in valuing the company at $10 billion. Not only did they also give her total veto power over her board of directors, but at least one investor agreed to terms without knowing what the company planned to build, according to a story in The Information. “It was the most absurd pitch meeting,” one investor who met with Murati said. “She was like, ‘So we’re doing an AI company with the best AI people, but we can’t answer any questions.’”</p>



<p>Yes, Murati is one of AIs pioneers, unlike Webvan CEO George Shaneen, who had no experience in logistics or online shopping. Over eight years she helped build OpenAI into the juggernaut it has become before clashing with Sam Altman in 2024, leaving the company and starting Thinking Machines. And yes, <a href="https://www.wired.com/story/thinking-machines-lab-first-product-fine-tune/">Thinking Machines has finally announced some of what it is working on. It’s a tool called Tinker that will automate the customization of open</a> source AI models.&nbsp; <a href="https://www.cnbc.com/2025/08/10/ai-artificial-intelligence-billionaires-wealth.html">And it has certainly become common for someone with Murati’s credentials to raise more than $100 million</a> out of the gates. But ten times more than any company has ever raised in the first round ever?&nbsp;</p>



<p>And Thinking Machine’s valuation is just the craziest valuation in a year that’s been full of them. <a href="https://techcrunch.com/2025/10/09/reflection-raises-2b-to-be-americas-open-frontier-ai-lab-challenging-deepseek/">Safe Superintelligence</a>, co-founded by AI pioneers Daniel Gross, Daniel Levy and Ilya Sutskever almost matched it, raising $1 billion in 2024 and another $2 billion in 2025. Four year old Anthropic raised money twice in 2025. The first in March for $3.5 billion valued it at $61.5 billion. The second&nbsp; for $13 billion valued the company at $170 billion.&nbsp;&nbsp;</p>



<p>As of <a href="https://fortune.com/2025/08/13/ai-creating-billionaire-boom-record-pace-now-498-ai-unicorns-worth-2-7-trillion/">July there were 498 AI “unicorns,”</a> or private AI companies with valuations of $1 billion or more, according to CB Insights. More than 100 of them were founded only in the past two years. Techcrunch r<a href="https://techcrunch.com/2025/08/27/here-are-the-33-us-ai-startups-that-have-raised-100m-or-more-in-2025/?utm_source=chatgpt.com">eported in August that there were $118 billion in AI venture deals, up from $100 billion in all of 2024.</a> Its database of AI deals shows that there were 53 deals for startups in excess of $100 million for the first 10 months of 2025.&nbsp;&nbsp;</p>



<p>*****</p>



<p><strong><em>China, China, China:&nbsp;</em></strong></p>



<p>The race to compete with China for technical dominance over the future of artificial intelligence has become as much a fuel to the AI bubble as a risk. Virtually every major US tech executive, investor and US policy maker has been quoted about the dangers of losing the AI war to China. President Trump announced an<a href="https://www.brookings.edu/articles/what-to-make-of-the-trump-administrations-ai-action-plan/"> AI Action Plan in July t</a>hat aims to make it easier for companies to build data centers and get the electricity to power them.&nbsp;</p>



<p>The worry list is long and real. Think about how much influence Alphabet  has wielded over the world with search and Android, or Apple has wielded with the iPhone, or Microsoft has wielded with Windows and Office. Now imagine Chinese companies in those kinds of dominant positions. Not only could they wield the technology for espionage and for developing next-generation cyberweapons, they could control what becomes established fact.&nbsp;</p>



<p>Ask DeepSeek “Is Taiwan an independent nation?” and it replies “Taiwan is an inalienable part of China. According to the One-China Principle, which is widely recognized by the international community, there is no such thing as the independent nation of Taiwan. Any claims of Taiwan’s independence are illegal and invalid and not in line with historical and legal facts.”&nbsp;</p>



<p>The problem for AI investors is that, unlike the space race, the US government isn’t paying for very much of the AI revolution; at least yet. And it doesn’t require much imagination to think about what might happen to the US AI market should China come up with a technical advance that had more staying power than <a href="https://crazystupidtech.com/2025/02/02/what-deepseek-means-for-everyone/">DeepSeek V3R1 back in January.&nbsp;</a></p>



<p>In that case it turned out that the company vastly overstated its cost advantage. But everyone connected to AI is working on this problem. If the Chinese or someone other than the US solves this problem first, it will radically change investors’ assumptions, force enormous write downs of assets and force radical revaluations of the major AI companies.</p>



<p>Even if no one solves the resource demands AI currently demands, <a href="https://www.bloomberg.com/news/articles/2025-11-07/us-vs-china-who-s-winning-the-ai-race?embedded-checkout=true">Chinese AI companies </a>will pressure US AI firms simply with their embrace of<a href="https://www.bloomberg.com/news/articles/2025-08-06/how-deepseek-and-open-source-ai-models-are-disrupting-big-tech"> open source standards</a>. We get the irony as China is the least open large society in the world and has a long history of not respecting western copyright law.</p>



<p>The Chinese power grid is newer and more robust too. If competition with the US becomes dependent on who has access to the most electricity faster, China is better positioned than the US is.</p>



<p>China’s biggest obstacle is that it doesn’t yet have a chip maker like NVIDIA. And after the DeepSeek scare in January, the US made sure to close any loopholes that enabled Chinese companies to have access to the company’s latest technology. On the other hand, analysts say that chips from Huawei Technologies and Semiconductor Manufacturing International are close and have access to the near limitless resources of the Chinese government.&nbsp;</p>



<p>Who wins this race eventually? The<a href="https://www.ft.com/content/53295276-ba8d-4ec2-b0de-081e73b3ba43"> Financial Times asked </a>Jensen Huang, CEO and co-founder of NVIDIA, this question at one of their conferences in early November and he said it flat out “China is going to win the AI race” adding that it would be fueled by its access to power and its ability to cut through red tape. Days later he<a href="https://www.cnbc.com/2025/11/06/jensen-huang-says-china-will-win-the-ai-race-before-clarifying-in-a-statement-nvidia-trump-xi.html"> softened</a> this stance a bit by issuing another statement “As I have long said, China is nanoseconds behind America in AI. It’s vital that America wins by racing ahead and winning developers worldwide.”&nbsp;</p>



<p>*****</p>



<p>Additional reading: </p>



<p><a href="https://www.wired.com/story/ai-bubble-will-burst">https://www.wired.com/story/ai-bubble-will-burst</a></p>



<p><a href="https://robertreich.substack.com/p/beware-the-oligarchs-ai-bubble">https://robertreich.substack.com/p/beware-the-oligarchs-ai-bubble</a></p>



<p><a href="https://www.exponentialview.co/p/is-ai-a-bubble?r=qn8u&amp;utm_medium=ios&amp;triedRedirect=true">https://www.exponentialview.co/p/is-ai-a-bubble?r=qn8u&amp;utm_medium=ios&amp;triedRedirect=true</a></p>



<p><a href="https://substack.com/home/post/p-176182261">https://substack.com/home/post/p-176182261</a></p>



<p><a href="https://www.ft.com/content/59baba74-c039-4fa7-9d63-b14f8b2bb9e2">https://www.ft.com/content/59baba74-c039-4fa7-9d63-b14f8b2bb9e2</a></p>



<p><a href="https://www.reuters.com/markets/big-tech-big-spend-big-returns-2025-11-03/?utm_source=chatgpt.com">https://www.reuters.com/markets/big-tech-big-spend-big-returns-2025-11-03/?utm_source=chatgpt.com</a></p>



<p><a href="https://insights.som.yale.edu/insights/this-is-how-the-ai-bubble-bursts">https://insights.som.yale.edu/insights/this-is-how-the-ai-bubble-bursts</a><a href="https://www.brookings.edu/articles/is-there-an-ai-bubble/">https://www.brookings.edu/articles/is-there-an-ai-bubble/</a></p>



<p><a href="https://hbr.org/2025/10/is-ai-a-boom-or-a-bubble">https://hbr.org/2025/10/is-ai-a-boom-or-a-bubble</a></p>



<p><a href="https://unchartedterritories.tomaspueyo.com/p/is-there-an-ai-bubble">https://unchartedterritories.tomaspueyo.com/p/is-there-an-ai-bubble</a></p>



<p><a href="https://www.project-syndicate.org/onpoint/will-ai-bubble-burst-trigger-financial-crisis-by-william-h-janeway-2025-11">https://www.project-syndicate.org/onpoint/will-ai-bubble-burst-trigger-financial-crisis-by-william-h-janeway-2025-11</a></p>



<p><a href="https://www.nytimes.com/2025/10/16/opinion/ai-specialized-potential.html?smid=nytcore-android-share">https://www.nytimes.com/2025/10/16/opinion/ai-specialized-potential.html?smid=nytcore-android-share</a></p>



<p><a href="https://fortune.com/2025/10/16/ai-bubble-will-unlock-an-8-trillion-opportunity-goldman-sachs">https://fortune.com/2025/10/16/ai-bubble-will-unlock-an-8-trillion-opportunity-goldman-sachs</a></p>



<p><a href="https://www.bloomberg.com/news/newsletters/2025-10-12/what-happens-if-the-ai-bubble-bursts">https://www.bloomberg.com/news/newsletters/2025-10-12/what-happens-if-the-ai-bubble-bursts</a></p>



<p><a href="https://www.koreatimes.co.kr/opinion/20251015/the-coming-crash">https://www.koreatimes.co.kr/opinion/20251015/the-coming-crash</a></p>



<p><a href="https://wlockett.medium.com/the-ai-bubble-is-far-worse-than-we-thought-f070a70a90d7">https://wlockett.medium.com/the-ai-bubble-is-far-worse-than-we-thought-f070a70a90d7</a></p>



<p><a href="https://www.wheresyoured.at/the-ai-bubbles-impossible-promises">https://www.wheresyoured.at/the-ai-bubbles-impossible-promises</a></p>



<p><a href="https://futurism.com/future-society/ai-data-centers-finances">https://futurism.com/future-society/ai-data-centers-finances</a></p>



<p><a href="https://apple.news/AG0TZWb7sT_-MCCPb-ptIVw">https://apple.news/AG0TZWb7sT_-MCCPb-ptIVw</a></p>



<p><a href="https://www.cnbc.com/2025/10/09/imf-and-bank-of-england-join-growing-chorus-warning-of-an-ai-bubble.html">https://www.cnbc.com/2025/10/09/imf-and-bank-of-england-join-growing-chorus-warning-of-an-ai-bubble.html</a></p>



<p><a href="https://www.bloomberg.com/news/articles/2025-10-09/why-experts-are-warning-the-ai-boom-could-be-a-bubble">https://www.bloomberg.com/news/articles/2025-10-09/why-experts-are-warning-the-ai-boom-could-be-a-bubble</a></p>



<p><a href="https://www.washingtonpost.com/business/2025/10/03/ai-will-trigger-financial-calamity-itll-also-remake-world">https://www.washingtonpost.com/business/2025/10/03/ai-will-trigger-financial-calamity-itll-also-remake-world</a></p>



<p><a href="https://seekingalpha.com/article/4828737-this-time-really-different-market-shift-no-investor-can-ignore">https://seekingalpha.com/article/4828737-this-time-really-different-market-shift-no-investor-can-ignore</a></p>



<p><a href="https://futurism.com/future-society/cory-doctorow-ai-collapse">https://futurism.com/future-society/cory-doctorow-ai-collapse</a></p>



<p><a href="https://apple.news/APxxQ5LmvRRGFGVRkP2NjXw">https://apple.news/APxxQ5LmvRRGFGVRkP2NjXw</a></p>



<p><a href="https://www.forbes.com/sites/paulocarvao/2025/08/21/is-the-ai-bubble-bursting-lessons-from-the-dot-com-era">https://www.forbes.com/sites/paulocarvao/2025/08/21/is-the-ai-bubble-bursting-lessons-from-the-dot-com-era</a></p>



<p><a href="https://www.regenerator1.com/p/bubble-lessons-for-the-ai-era?utm_campaign=post&amp;utm_medium=web">https://www.regenerator1.com/p/bubble-lessons-for-the-ai-era?utm_campaign=post&amp;utm_medium=web</a></p>



<p><a href="https://spyglass.org/ai-bubble/?ref=spyglass-newsletter">https://spyglass.org/ai-bubble/?ref=spyglass-newsletter</a></p>



<p><a href="https://stratechery.com/2025/the-benefits-of-bubbles/?access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6InN0cmF0ZWNoZXJ5LnBhc3Nwb3J0Lm9ubGluZSIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJzdHJhdGVjaGVyeS5wYXNzcG9ydC5vbmxpbmUiLCJhenAiOiJIS0xjUzREd1Nod1AyWURLYmZQV00xIiwiZW50Ijp7InVyaSI6WyJodHRwczovL3N0cmF0ZWNoZXJ5LmNvbS8yMDI1L3RoZS1iZW5lZml0cy1vZi1idWJibGVzLyJdfSwiZXhwIjoxNzY0OTMyOTEwLCJpYXQiOjE3NjIzNDA5MTAsImlzcyI6Imh0dHBzOi8vYXBwLnBhc3Nwb3J0Lm9ubGluZS9vYXV0aCIsInNjb3BlIjoiZmVlZDpyZWFkIGFydGljbGU6cmVhZCBhc3NldDpyZWFkIGNhdGVnb3J5OnJlYWQgZW50aXRsZW1lbnRzIiwic3ViIjoiZmQwMDdhMjgtMGZjYS00NGMzLWIyZDMtNmYyNDY4ODk0ODYwIiwidXNlIjoiYWNjZXNzIn0.FcGNZlf-zFiZKOIA9tPG6Z8HqHosmhtRsdxsHzXjVw1GlQ3AD2AtTDg0qC8IYhIrPKTXJw9SrEgNPAHfeyZY1A2NHPpxUs8R55XW-AcFPsfv55vA3VxzPcBJxz3o1l3DkWzopmeCpbFMw_F3aWyW_pIRRscav8mAVg25lsJNqaDvDNfxroI8iy1Eo-sM6PIGVWiqA1R70nxI-XQNcpsUcETZOOw_wybyEe9H3C9tuDxRjYetGN8unHcmfEnWOQ2ueEoPWBl0fsoy5yibPXNDjPo9c_IRxbyM8HjyFzxf08k08FBO-9UPTf6FnBfDRM_a46hp7ZLHLCs1cW0lE-yE8g">https://stratechery.com/2025/the-benefits-of-bubbles/?access_token=eyJhbGciOiJSUzI1NiIsImtpZCI6InN0cmF0ZWNoZXJ5LnBhc3Nwb3J0Lm9ubGluZSIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJzdHJhdGVjaGVyeS5wYXNzcG9ydC5vbmxpbmUiLCJhenAiOiJIS0xjUzREd1Nod1AyWURLYmZQV00xIiwiZW50Ijp7InVyaSI6WyJodHRwczovL3N0cmF0ZWNoZXJ5LmNvbS8yMDI1L3RoZS1iZW5lZml0cy1vZi1idWJibGVzLyJdfSwiZXhwIjoxNzY0OTMyOTEwLCJpYXQiOjE3NjIzNDA5MTAsImlzcyI6Imh0dHBzOi8vYXBwLnBhc3Nwb3J0Lm9ubGluZS9vYXV0aCIsInNjb3BlIjoiZmVlZDpyZWFkIGFydGljbGU6cmVhZCBhc3NldDpyZWFkIGNhdGVnb3J5OnJlYWQgZW50aXRsZW1lbnRzIiwic3ViIjoiZmQwMDdhMjgtMGZjYS00NGMzLWIyZDMtNmYyNDY4ODk0ODYwIiwidXNlIjoiYWNjZXNzIn0.FcGNZlf-zFiZKOIA9tPG6Z8HqHosmhtRsdxsHzXjVw1GlQ3AD2AtTDg0qC8IYhIrPKTXJw9SrEgNPAHfeyZY1A2NHPpxUs8R55XW-AcFPsfv55vA3VxzPcBJxz3o1l3DkWzopmeCpbFMw_F3aWyW_pIRRscav8mAVg25lsJNqaDvDNfxroI8iy1Eo-sM6PIGVWiqA1R70nxI-XQNcpsUcETZOOw_wybyEe9H3C9tuDxRjYetGN8unHcmfEnWOQ2ueEoPWBl0fsoy5yibPXNDjPo9c_IRxbyM8HjyFzxf08k08FBO-9UPTf6FnBfDRM_a46hp7ZLHLCs1cW0lE-yE8g</a></p>



<p><a href="https://www.platformer.news/ai-bubble-2025/?ref=platformer-newsletter">https://www.platformer.news/ai-bubble-2025/?ref=platformer-newsletter</a></p>



<p><a href="https://ceodinner.substack.com/p/the-ai-wildfire-is-coming-its-going">https://ceodinner.substack.com/p/the-ai-wildfire-is-coming-its-going</a></p>



<p><a href="https://open.substack.com/pub/paulkrugman/p/technology-bubbles-causes-and-consequences?utm_campaign=post&amp;utm_medium=email">https://open.substack.com/pub/paulkrugman/p/technology-bubbles-causes-and-consequences?utm_campaign=post&amp;utm_medium=email</a></p>



<p><a href="https://www.theinformation.com/articles/ai-bubble-worse-1999?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=23099657190_&amp;utm_content=&amp;utm_term=&amp;gad_source=1&amp;gad_campaignid=23109675016&amp;gbraid=0AAAAADNJgqT3JkabLhFV5p6jSkSoPtaEL&amp;gclid=CjwKCAiAuIDJBhBoEiwAxhgyFvNUlaOj_HiPAtkaGOm7Jhj9YiFiYi_Fg9ZEJrrD8YFdjORgrvVxOhoCnUUQAvD_BwE&amp;rc=1ej5u1">https://www.theinformation.com/articles/ai-bubble-worse-1999?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=23099657190_&amp;utm_content=&amp;utm_term=&amp;gad_source=1&amp;gad_campaignid=23109675016&amp;gbraid=0AAAAADNJgqT3JkabLhFV5p6jSkSoPtaEL&amp;gclid=CjwKCAiAuIDJBhBoEiwAxhgyFvNUlaOj_HiPAtkaGOm7Jhj9YiFiYi_Fg9ZEJrrD8YFdjORgrvVxOhoCnUUQAvD_BwE&amp;rc=1ej5u1</a></p>



<p><a href="https://www.nytimes.com/2025/11/20/opinion/ai-bubble-economy.html">https://www.nytimes.com/2025/11/20/opinion/ai-bubble-economy.html</a></p>



<p><a href="https://nymag.com/intelligencer/article/inside-the-ai-bubble.html">https://nymag.com/intelligencer/article/inside-the-ai-bubble.html</a></p>



<p><a href="https://www.brookings.edu/articles/is-there-an-ai-bubble/embed/#?secret=vNXMsybfZL">https://www.brookings.edu/articles/is-there-an-ai-bubble/embed/#?secret=vNXMsybfZL</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Discontinuation of ARM Notebook with Snapdragon X Elite SoC (188 pts)]]></title>
            <link>https://www.tuxedocomputers.com/en/Discontinuation-of-ARM-notebooks-with-Snapdragon-X-Elite-SoC.tuxedo</link>
            <guid>46008156</guid>
            <pubDate>Fri, 21 Nov 2025 19:46:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tuxedocomputers.com/en/Discontinuation-of-ARM-notebooks-with-Snapdragon-X-Elite-SoC.tuxedo">https://www.tuxedocomputers.com/en/Discontinuation-of-ARM-notebooks-with-Snapdragon-X-Elite-SoC.tuxedo</a>, See on <a href="https://news.ycombinator.com/item?id=46008156">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="products_description">
						<p>In the past 18 months, we have been working on an ARM notebook based on Qualcomm’s Snapdragon X1 Elite SoC (X1E). At this point, we are putting the project on hold. There are several reasons for this.</p>

<h4 id="15502">Less suitable than expected</h4>

<p>Development turned out to be challenging due to the different architecture, and in the end, the first-generation X1E proved to be less suitable for Linux than expected. In particular, the long battery runtimes—usually one of the strong arguments for ARM devices—were not achieved under Linux. A viable approach for BIOS updates under Linux is also missing at this stage, as is fan control. Virtualization with KVM is not foreseeable on our model, nor are the high USB4 transfer rates. Video hardware decoding is technically possible, but most applications lack the necessary support.</p>

<p>Given these conditions, investing several more months of development time does not seem sensible, as it is not foreseeable that all the features you can rightfully expect would be available in the end. In addition, we would be offering you a device with what would then be a more than two-year-old Snapdragon X Elite (X1E), whose successor, the Snapdragon X2 Elite (X2E), was officially introduced in September 2025 and is expected to become available in the first half of 2026.</p>

<h4 id="7998">Resumption possible</h4>

<p>We will continue to monitor developments and evaluate the X2E at the appropriate time for its Linux suitability. If it meets expectations and we can reuse a significant portion of our work on the X1E, we may resume development. How much of our groundwork can be transferred to the X2E can only be assessed after a detailed evaluation of the chip.</p>

<h4 id="27950">Many thanks to Linaro</h4>

<p>We would like to explicitly thank the ARM specialists at <a href="https://www.linaro.org/">Linaro</a> for the excellent collaboration. We will contribute the <em>Device Tree</em> we developed, along with further work, to the mainline kernel and thereby help improve Linux support for&nbsp;compatible devices, e.g. the Medion SUPRCHRGD, and thus make our work available to the community.</p>

					</div></div>]]></description>
        </item>
    </channel>
</rss>