<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 04 Feb 2024 23:00:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Why Gödel, Escher, Bach is the most influential book in my life (214 pts)]]></title>
            <link>https://philosophygeek.medium.com/why-g%C3%B6del-escher-bach-is-the-most-influential-book-in-my-life-49d785a4e428</link>
            <guid>39253099</guid>
            <pubDate>Sun, 04 Feb 2024 18:48:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://philosophygeek.medium.com/why-g%C3%B6del-escher-bach-is-the-most-influential-book-in-my-life-49d785a4e428">https://philosophygeek.medium.com/why-g%C3%B6del-escher-bach-is-the-most-influential-book-in-my-life-49d785a4e428</a>, See on <a href="https://news.ycombinator.com/item?id=39253099">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://philosophygeek.medium.com/?source=post_page-----49d785a4e428--------------------------------"><div aria-hidden="false"><p><img alt="Mark Johnson" src="https://miro.medium.com/v2/resize:fill:88:88/1*ivAx6b9Z9gnoFsJceGMkFQ.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="2061"><a href="https://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567" rel="noopener ugc nofollow" target="_blank"><em>Gödel, Escher, Bach: An Eternal Golden Braid</em></a><em> </em>(henceforth: GEB), the Pulitzer Prize winning book written in 1978 by Douglas Hofstadter, is described in its cryptic tagline as “a metaphorical fugue on minds and machines in the spirit of Lewis Carroll.”</p><p id="9ab3">I recently reread GEB and got fired up by how brilliantly Hofstadter fuses computation, epistemology, and consciousness. After failed attempts at explaining the book to three of my smartest friends, I decided to write something up.</p><p id="7f72">The problem is that a simple reduction like “GEB is about how complex systems arise from simpler systems” is akin to describing <em>Ulysses</em> as “a day in the life of Leopold Bloom.” More detailed descriptions run the risk of diving into the depth that’s only understandable after having read the book.</p><p id="ca55">This post is a more modest attempt to explain to myself why GEB is important, and focuses on three mental models that have profoundly affected my life: <strong>epistemic limits</strong>,<strong> self-reference</strong>, and <strong>isomorphism</strong>.</p><p id="b371">If it causes you to read or reread it, then all the better.</p><p id="00df">Here we go!</p><figure><figcaption>Kurt and Albert, hanging out at Princeton.</figcaption></figure><p id="d440">The main character of the book is <a href="https://en.wikipedia.org/wiki/Kurt_G%C3%B6del" rel="noopener ugc nofollow" target="_blank">Kurt Gödel</a>, the most important person in the 20th century you’ve never heard of. Gödel is the kind of guy that shows up to <a href="https://en.wikipedia.org/wiki/Albert_Einstein" rel="noopener ugc nofollow" target="_blank">his buddy’s</a> 70th birthday with <a href="https://en.wikipedia.org/wiki/G%C3%B6del_metric" rel="noopener ugc nofollow" target="_blank">an exact solution to the Einstein field equations</a> as a present. Despite being the greatest mathematician of his generation, he wasn’t stuffy in the least: his favorite movie was Snow White and the Seven Dwarves.</p><p id="b348">Gödel is most famous for his Incompleteness Theorems, which established limits on mathematics. For the first chunk of the 20th century, mathematicians were obsessed with formalizing mathematics and then proving meta-theorems <em>about</em> those formal systems. In particular, there was a strongly-held belief that for any well-formed formula (a “grammatically correct” statement in math, e.g., A=B is well-formed whereas AA==+B is not), you could use mathematics to <em>decide</em> whether it was true or false.</p><p id="909f">If you think about it for a second, this makes perfect sense: it <em>seems</em> like you should be able to determine whether any statement is true or false.</p><p id="3010">Nope! Gödel proved in 1931 that mathematics is not decidable, an earth-shattering result. He proved that there are statements in mathematics, which are <em>true but not provable</em> within the system. Worse yet, it turns out that you can’t build a more powerful mathematical system. Once a system becomes sufficiently complex, there will always be statements which are undecidable. You’re left with a choice: either have weak system of mathematics or accept that there will always be theorems out of reach. A rough analogy to incompleteness Heisenberg’s Uncertainty Principle, which shows that physics makes it impossible to determine <em>both</em> the position and velocity of a particle with exact precision.</p><p id="fa72">Wouldn’t it be nice if every question had an answer? That’s a lovely fantasy, but Gödel shows that there are <strong>fundamental epistemic limits to the universe</strong>, things that no genius will help us to know, no alien race could teach us, no machine could be built to solve, and no new kinds of mathematics will uncover. How frustrating.</p><p id="167d">A key feature of powerful mathematical systems (or perhaps, any system that generates complexity…) is that they involve <strong>self-reference</strong>, that is, they contain ways of talking about themselves. “This sentence is true” is an example. Because self-referential systems can manipulate and talk about themselves, they systems are very powerful and immediately run into fun paradoxes. Is the statement “This sentence is false.” true or false? Either way, it doesn’t end well.</p><p id="c684">A third major theme of the book is <strong>isomorphism</strong>, which is unique to Hofstadter’s vernacular. In formal mathematics, “isomorphism” takes on a version of “equivalence.” For example, it turns out that many different formalizations of mathematics are provably isomorphic, like Turing Machines, arithmetic, set theory, and formal logic. Hofstadter deliberately uses the term more loosely to describe two systems that are structurally similar. I find this quite useful because it forces one to define the structures of the system, why they are similar, and why other parts of the system are less important. We might describe the way that planets fly around stars as <em>isomorphic</em> to the way that electrons fly around nuclei.</p><figure><figcaption>Escher’s famous Drawing Hands.</figcaption></figure><p id="8a61">The two minor characters, <a href="https://mcescher.com/" rel="noopener ugc nofollow" target="_blank">M.C.Escher</a> and <a href="https://en.wikipedia.org/wiki/Johann_Sebastian_Bach" rel="noopener ugc nofollow" target="_blank">Johann Sebastian Bach</a>, are reflections of Gödel in art and both liberally use self reference. Escher draws pictures of hands drawing hands (!) and water “falling” in an infinite loop. His images don’t just play tricks on the eye, they force paradoxical conclusions, regardless of your angle of interpretation. On the musical side, Papa Bach was most famous for his complex fugues, which are basically the same melody played on top of each other. Common versions of this you might have sung as a child are “Row, row, row your boat” and “Frère Jacques.” Both Escher and Bach are woven into the story (like a fugue?), providing tangible examples to the more abstruse mathematical concepts.</p><figure><figcaption>A playlist full of Bach, fugues, and other tracks referenced in GEB.</figcaption></figure><p id="b898">Perhaps the most astonishing part of the book is the quality of the writing itself. Each chapter begins with a clever dialog between Achilles and the Tortoise (inspired by Lewis Carroll), and a few of their anthropomorphic friends. They deal with a whole range of bizarre situations, like record player so powerful that it can play any record (including a record that can destroy the record player) and asking a Djinn for a meta-wish (“I wish for 5 more wishes”). Hofstadter’s greatest achievement is his palindromic Crab Canon in Chapter VII, which is a dialog that can be read backwards and forwards. Of course, these aren’t just cute dialogs: each is isomorphic to the themes in the following chapter. Oftentimes a dialog is a more understandable exposition of the chapter’s theme than the chapter itself.</p><p id="440f">And, naturally in a book about self-reference, GEB itself is highly self-referential. Themes are often resolved hundreds of pages later and require going back to appreciate fully the depth of Hofstadter’s argument. Mercifully, he’s a gifted and lucid writer so, even though there are chapters that are dense, it’s always tractable to read.</p><p id="5afb">After 742 pages and even after having written the paragraphs above, I still struggle with a simple answer to the question: “What is this book about?” The best I can come up with is that GEB equips you with mental models to contemplate philosophy.</p><p id="04c7">So to end, a few personal examples about how GEB has influenced my own thinking.</p><p id="1696"><a rel="noopener" href="https://philosophygeek.medium.com/moving-from-com-to-org-34150bea9ade">I recently joined Stand Together</a>, who shares my strong belief in <strong>bottom up solutions</strong>. Perhaps the idea that bottom up solutions are better isn’t just an empirical statement of sociology, but fundamental to the nature of complex systems. Indeed, Hofstadter goes through many examples of how complexity emerges from simpler systems, often which look nothing like the higher-level systems. Consciousness itself doesn’t exist in neurons, and yet neurons as a system create consciousness in humans (this is critical to Hoftstadter’s argument that machines can think). There’s also a fantastical example in a dialog with the Anteater, who has conversations with Aunt Hilary, an ant colony. She is perfectly capable of having a robust conversation with the Anteater, powered by the ants in the colony. Of course, the ants themselves are individuals with their own cares and concerns and has no knowledge of the emergent intelligence, much like Aunt Hilary has no knowledge of her inner workings.</p><p id="0ddf">How DNA expresses as proteins, how the brain functions at multiple levels, how we understand and use words, how programs don’t have access to the underlying transistors, how Aunt Hilary doesn’t know what the ants are doing… all of these are a set of isomorphisms that suggest that bottom up is better than top down. An additional tenet of Stand Together is “believe in people,” which means that the smallest units act intelligently. Like ants or neurons, we make local decisions every day that bubble up into the structure of society, without anyone telling us what to do.</p><p id="fbbe">The idea that epistemic limits exist in something as universal as mathematics has humbled me about the limits of knowledge for complex human systems. Utopian thought experiments often generate useful frames of exploration, but ought not be confused with reality. Utopians often try to pull out the “bugs” from human systems, often which are endemic to the system, or “features,” as we might say in the trade. Bugs might not be desirable, but sometimes, <em>the bugs can’t just be ripped out of the system without destroying the system itself</em>. Our time would be better spent figuring out—within the system—optimize for minimizing the downsides of the “bugs” while maximizing the value of the features. Think about this with respect to capitalism, socialism, and communism…</p><p id="c7f8">A final area where GEB has influenced me is in designing software products. Hugh Dubberly has been my collaborator for years, starting off with our deep dive into cybernetics, the study of feedback loops. We believe that iteration is key to quality; perfection is impossible out of the gate. Further, the system used to generate quality software is a series of feedback loops between customers and the company, product and engineering, and so on. Though specific product frameworks have changed over the years, that obsession with iteration and feedback permeates everything I’ve implemented.</p><p id="ea19">My modest goal in writing this post was to have something I could send to a friend, rather than to spend an hour fumbling a feeble explanation of <em>Gödel, Escher, Bach</em>. I had a secondary goal in the back of my head… if you have a copy of GEB on your shelf collecting dust and you’ve never read more than a chapter or two, dust it off and see how it goes this time.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beyond self-attention: How a small language model predicts the next token (124 pts)]]></title>
            <link>https://shyam.blog/posts/beyond-self-attention/</link>
            <guid>39251909</guid>
            <pubDate>Sun, 04 Feb 2024 16:54:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shyam.blog/posts/beyond-self-attention/">https://shyam.blog/posts/beyond-self-attention/</a>, See on <a href="https://news.ycombinator.com/item?id=39251909">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="single"><p><time datetime="2024-01-29 17:25:21 -0700 -0700">Jan 29, 2024</time>
<span>·</span>
<span>17754 words</span>
<span>·</span>
<span>84 minute read</span></p><div><p>I trained a small (~10 million parameter) <a href="https://en.wikipedia.org/wiki/Transformer_%28machine_learning_model%29" target="_blank" rel="noopener">transformer</a> following <a href="https://karpathy.ai/" target="_blank" rel="noopener">Andrej Karpathy</a>’s excellent tutorial, <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener">Let’s build GPT: from scratch, in code, spelled out</a>. After getting it working, I wanted to understand, as deeply as possible, what it was doing internally and how it produced its results.</p><p>The <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">original paper</a>, as well every transformer tutorial I found, focuses primarily on <a href="https://machinelearningmastery.com/the-transformer-attention-mechanism/" target="_blank" rel="noopener">multi-head self-attention</a>, the mechanism by which transformers learn multiple relationships between tokens without relying on recurrences or convolution. But none of the papers or tutorials I encountered give a satisfying explanation of what happens <em>after attention</em>: <strong>how exactly do the results of the attention computation turn into accurate predictions for the next token?</strong></p><p>I thought I could run a few example prompts through the small but working transformer I’d trained, examine the internal states, and figure this out. What I thought would be a quick investigation turned out to be a 6-month deep dive, but yielded some results I think are worth sharing. Specifically, I have a working theory that explains how the transformer produces its predictions and some empirical evidence that suggests this explanation is at least plausible.</p><p>For those readers familiar with transformers and eager for the punchline, here it is: Each transformer block (containing a multi-head self-attention layer and feed-forward network) learns weights that associate a given prompt with a class of strings found in the training corpus. <strong>The distribution of tokens that follow those strings in the training corpus is, approximately, what the block outputs as its predictions for the next token.</strong> Each block may associate the same prompt with a different class of training corpus strings, resulting in a different distribution of next tokens and thus different predictions. The final transformer output is a linear combination of each block’s predictions.</p><p>I implemented imperative code that does what I’m proposing the transformer is doing. It produces outputs very similar to the transformer, which I’ll review in detail in a <a href="#evaluating-the-approximation">later section</a>.</p><p>In this post, I’m going to briefly introduce the model and training data, demo some evidence for my proposed explanation, give a detailed walkthrough of the imperative code implementation of it, and present the supporting evidence I have for my theory. I’ve tried to keep the main narrative succinct, with links to relevant technical details and justifications in the <a href="#appendices">appendices</a> or other notebooks in the <a href="https://shyam.blog/posts/beyond-self-attention/%28https://github.com/spather/transformer-experiments%29">repo</a>.</p><blockquote><p>This project is my first foray into this type of open-ended ML research. I’m sure I have made errors or omissions that would be obvious to more experienced researchers. I welcome any feedback on this work at <code>shyam.pather at gmail dot com</code>.</p></blockquote><h2 id="the-model-and-setup">The Model and Setup <a href="#the-model-and-setup">🔗</a></h2><blockquote><h3 id="disclaimer">Disclaimer <a href="#disclaimer">🔗</a></h3><p>I want to start by saying upfront: the code for the model I trained isn’t mine. It came from <a href="https://karpathy.ai/" target="_blank" rel="noopener">Andrej Karpathy</a>’s video, <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener">Let’s build GPT: from scratch, in code, spelled out</a> (highly recommend).</p><p>I typed in the code by copying what I saw on the screen as I watched the video. For things that weren’t clear onscreen, I referenced the <a href="https://github.com/karpathy/ng-video-lecture" target="_blank" rel="noopener">GitHub repo for the video</a> and the <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener">nanoGPT repo</a>. After getting it working, I made only minor changes to make it work with the rest of the code in/structure of <a href="https://github.com/spather/transformer-experiments" target="_blank" rel="noopener">my repository</a>, resulting in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer.ipynb" target="_blank" rel="noopener">this implementation</a>. In summary: the core language model is Andrej Karpathy’s work, not mine. The analysis and all the supporting code behind it are my original contributions. I’ll acknowledge and cite influential papers, posts, tutorials, and other resources in the relevant places.</p></blockquote><h3 id="model-overview">Model Overview <a href="#model-overview">🔗</a></h3><p>The model is a 6-block, decoder-only <a href="https://en.wikipedia.org/wiki/Transformer_%28machine_learning_model%29" target="_blank" rel="noopener">transformer</a>:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/a9f2adc6c1c25ebb263caf42df37f4429c4ed44eda0a0a228cba52b7a00aeb9d.png" alt=""></p><p>It’s trained on the <a href="https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt" target="_blank" rel="noopener">TinyShakespeare data set</a> which contains 40,000 lines of Shakespeare’s plays. After about an hour of training on an RTX 4000 GPU, it is able to produce reasonable-looking faux Shakespeare.</p><p>Given a prompt, the model predicts tokens that it thinks should follow. Let’s look at an example: starting with the prompt, <code>ROMEO:</code>, and sampling 500 tokens from the model’s predictions, we get:</p><pre tabindex="0"><code>ROMEO:
If thou wilt triumphant be virtue, and since from any
bold virtue that is made a bawd of earth, then the
duke desires of patience and perish:
take up the other husband, dislike his tent
back.

First Citizen:
Ourself goes, go back: you have no consul, but the disguised gods.

Second Citizen:
We choose him in the world, he did runk itself.

First Citizen:
Sir, I am I a man changed him and thriving, I have heard the
king.

CORIOLANUS:
Consider him!

AUFIDIUS:
Most gracious irice, and you must danc
</code></pre><p>It’s not Shakespeare but structurally, it’s plausible Shakespeare. It looks like the script for a play, the language sounds archaic, the character names/titles come from real Shakespeare plays. Most of the words are English words. Punctuation and capitalization are mostly sensible. Clearly, none of the text actually makes sense, but still, it’s not bad for an hour of training.</p><p>The <strong>tokens in the model are characters</strong>, not words. Given a prompt, the model predicts a probability distribution for the next character. For example, given the prompt <code>'my most gr</code>, the model predicts these probabilities for the next token:</p><pre tabindex="0"><code>'a' 0.819
'e' 0.081
'i' 0.059
'o' 0.036
'u' 0.004
'y' 0.001
'w' 0.000
'r' 0.000
'g' 0.000
's' 0.000
</code></pre><p><a href="#i-model-details">Appendix I</a> provides a few more details about the model. Beyond that, if you want to know more, <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer.ipynb" target="_blank" rel="noopener">the code</a> and <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener">Andrej’s video</a> are the best resources.</p><h3 id="transformer-block-structure">Transformer Block Structure <a href="#transformer-block-structure">🔗</a></h3><p>Each of the 6 blocks in the architecture diagram above contains two significant sub-components: a multi-head self-attention layer and a feed-forward network, wired together via a mix of direct and residual connections as follows:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/72a30adc39ebf5f278c0a257fb46f26e6d666d113736e36ce394db587110260c.png" alt=""></p><p>The <code>Block</code> module implements this wiring in PyTorch:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>Block</span>(nn<span>.</span>Module):
</span></span><span><span>    <span>"""One transformer block"""</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> __init__(self, n_embed, n_head):
</span></span><span><span>        super()<span>.</span>__init__()
</span></span><span><span>        head_size <span>=</span> n_embed <span>//</span> n_head
</span></span><span><span>        self<span>.</span>sa <span>=</span> MultiHeadAttention(n_head, head_size)
</span></span><span><span>        self<span>.</span>ffwd <span>=</span> FeedForward(n_embed)
</span></span><span><span>        self<span>.</span>ln1  <span>=</span> nn<span>.</span>LayerNorm(n_embed)
</span></span><span><span>        self<span>.</span>ln2 <span>=</span> nn<span>.</span>LayerNorm(n_embed)
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span>(self, x):
</span></span><span><span>        x <span>=</span> x <span>+</span> self<span>.</span>sa(self<span>.</span>ln1(x)) <span># The `x +` part is a skip connection</span>
</span></span><span><span>        x <span>=</span> x <span>+</span> self<span>.</span>ffwd(self<span>.</span>ln2(x)) <span># The `x +` part is a skip connection</span>
</span></span><span><span>
</span></span><span><span>        <span>return</span> x
</span></span></code></pre></div><p>While many words have been written and spoken about multi-head attention, comparatively little has been said about the feed-forward network because, it seems, comparatively little is known:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/b8214cdd1f6c9466bb984529984c757d780148fb4fe44bfed7714216e12bff73.png" alt=""></p><p>Screenshot from <a href="https://stats.stackexchange.com/q/485910">https://stats.stackexchange.com/q/485910</a></p><p>I started this investigation wondering what comes after attention. Literally, the feed-forward network does. In the transformer I studied, across all 6 blocks, the feed-forward networks comprise over 65% of the total trainable parameters, so they must play some important role.</p><p>As I’ll show <a href="#transformation-via-vector-addition">later</a>, it turns out that the output of the feed-forward network is the primary factor that determines how a block transforms its input into its output.</p><h2 id="demo-my-proposal-in-action">Demo: My Proposal In Action <a href="#demo-my-proposal-in-action">🔗</a></h2><p>In this section, I’m going to show an example that illustrates what I’m proposing the transformer is doing. In the next section, I’ll go into detail about how this is implemented.</p><p>Imagine we did the following:</p><ul><li>Ran the prompt, <code>'And only l'</code>, through the model and extracted the output value of the feed-forward network in the first transformer block.</li><li>Went back to the training corpus, found all substrings of the same length as our prompt (10-characters), ran all of them through the model, and filtered out just the ones whose feed-forward network outputs in the first block have a cosine similarity of 0.95 or greater when compared to that of the prompt, <code>'And only l'</code>.</li></ul><p>We’d come up with this set of strings:</p><pre tabindex="0"><code>'hat only l'    's sickly l'    ' as\nthey l'   'r kingly l'    're; they l'
'eby they l'    'ar, they l'    'im, only l'    'ling any l'    'life may l'
'nobility l'    'e\nBy any l'   ' as they l'    ', if any l'    ' hastily l'
'tly they l'    ' ghastly l'    '\nMy only l'   'For many l'    'r in any l'
' till my l'    'all they l'    'hen they l'    'at Henry l'    'oolishly l'
'er:\nThey l'   'may they l'    'or stony l'    'ur Henry l'    'l gladly l'
'yet they l'    'y;\nDelay l'   'e, on my l'    'or Henry l'    'I dearly l'
' if they l'    ' she may l'    't\nfairly l'   'ould say l'    'd all my l'
'her they l'    ' Stanley l'    ' and may l'    'uld they l'    'u all my l'
'friendly l'    'h gently l'    'e deadly l'    'f all my l'    'n all my l'
'Ere they l'    'steel my l'    ' tell my l'    'e kingly l'    'learn my l'
'd he say l'    't basely l'    'Thursday l'    'iciously l'    " 'if any l"
' as many l'    'hy glory l'    'not very l'    'a goodly l'    'e surely l'
'quiously l'    ', fairly l'    'lord! my l'    'entle my l'    ', he may l'
'our holy l'    ' worldly l'    ' my only l'    ' all, my l'
'ul, they l'    'o lately l'    's in any l'    ' no lady l'
'ter many l'    'Our holy l'    't vainly l'    'e\nA lady l'
' you may l'    'y greedy l'    'untimely l'    'directly l'
'er on my l'    'e wistly l'    'ng Henry l'    'And only l'
's kindly l'    'KE:\nThey l'   ' of many l'    'o, on my l'
</code></pre><p>There’s a clear pattern across these: they all end in <code>y l</code> and several of them end in <code>ly l</code>. Similarity in the space of feed-forward network outputs seems to correspond to human-interpretable patterns.</p><p>Next, imagine we went back to the training corpus, found each of these strings and built a distribution of all the characters that came after them. We’d find, for example:</p><ul><li><code>'hat only l'</code> is followed by <code>i</code> (“T<code>hat only l</code><strong>i</strong>ke a gulf it did remain”)</li><li><code>'l gladly l'</code> is followed by <code>e</code> (“I’l<code>l gladly l</code><strong>e</strong>arn.”)</li><li><code>'n all my l'</code> is followed by both <code>a</code> and <code>i</code> (“I<code>n all my l</code><strong>a</strong>nds and leases whatsoever” and “never saw you before i<code>n all my l</code><strong>i</strong>fe”)</li></ul><p>Doing this for the complete set of 94 strings, we’d end up with this distribution:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/170aed320bd4ab2e2647d8d1ef50b499b215ce1905cff1b5db6fe78dd83c3df3.png" alt=""></p><p>The various tokens in our model’s vocabulary appear on the x-axis and the normalized frequency of occurrence on the y-axis. This plot shows that <code>i</code> was the most frequent, then <code>o</code>, then <code>a</code>, and finally, <code>e</code>.</p><p>Now let’s look at the final output of the transformer as a whole when given <code>And only l</code> as a prompt:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/08cb64e75759e2a329a0a296931433b06c02c2fb92b6dd0bb440af807cde1d86.png" alt=""></p><p>This is a probability distribution representing the model’s predictions for the next token. Notice that it’s strikingly similar to the normalized frequency distribution shown in the previous plot!</p><p>We can quantify how similar they are. <a href="https://en.wikipedia.org/wiki/Hellinger_distance" target="_blank" rel="noopener">Hellinger distance</a> is a measure of overlap between probability distributions. Given distributions \(P\) and \(Q\), the Hellinger distance between them is:</p><p>$$
H(P, Q) = \frac{1}{\sqrt{2}} \sqrt{\sum_{i=1}^n (\sqrt{p_i} - \sqrt{q_i})^2}
$$</p><p>Or, in code:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>hellinger_distance</span>(
</span></span><span><span>    p: torch<span>.</span>Tensor,
</span></span><span><span>    q: torch<span>.</span>Tensor,
</span></span><span><span>):
</span></span><span><span>    <span>return</span> ((p<span>.</span>sqrt() <span>-</span> q<span>.</span>sqrt())<span>**</span><span>2</span>)<span>.</span>sum(dim<span>=-</span><span>1</span>)<span>.</span>sqrt() <span>/</span> math<span>.</span>sqrt(<span>2</span>)
</span></span></code></pre></div><p>Hellinger distance of 0 means the two distributions are identical and 1 means they have no overlap.</p><p>The Hellinger distance between the two distributions above - the distribution formed from the tokens that follow the strings with similar feed-forward network outputs and the distribution the model predicts - is 0.07: very nearly identical.</p><p>For the sake of keeping the demo brief, I chose an example where the first block’s similar strings alone are enough to produce a distribution that closely matches the final output of the transformer. Typically, we’d need to need to do the same exercise - finding the strings in the training corpus that produce similar feed-forward network outputs to the prompt and building a distribution from the tokens that succeed them - for all 6 transformer blocks, and then calculate a weighted sum of the resulting distributions in order to get a good match. We’ll do that in the next section and see that <strong>across a sample of 20,000 prompts, the average Hellinger distance between distributions computed this way and the corresponding transformer output was just 0.17</strong>.</p><p>This small average Hellinger distances suggests the results produced by this approach are a good approximation for the transformer’s outputs. In addition, as I’ll explain in the <a href="#interpretation-why-does-the-approximation-work">interpretation</a> section, I think the approach itself is a reasonable approximation of what the transformer is actually doing.</p><h2 id="implementation-approximating-the-transformer-output-with-feed-forward-network-outputs">Implementation: Approximating the Transformer Output with Feed-forward Network Outputs <a href="#implementation-approximating-the-transformer-output-with-feed-forward-network-outputs">🔗</a></h2><p>In this section, I’m going to walk through in some detail and with code, the exact procedure I used to approximate the transformer’s output using strings that produced similar feed-forward network outputs. If you’re not interested in the implementation, skip this section and proceed to the <a href="#evaluating-the-approximation">evaluation</a> section.</p><p>To recap, this is the procedure to compute the approximation:</p><ol><li>Run a prompt through the model and save the feed-forward network outputs for each block.</li><li>For each block:<ul><li>Find the strings in the training corpus that produce the most similar feed-forward network outputs to the prompt for that block.</li><li>For each string found, build a frequency distribution of the tokens that come after it in the training corpus.</li><li>Sum the frequency distributions for all strings found for the current block.</li></ul></li><li>Compute a weighted sum of the frequency distributions for each block computed in the previous step.</li><li>Normalize the weighted sum to get a probability distribution.</li></ol><h3 id="procedure-setup">Procedure Setup <a href="#procedure-setup">🔗</a></h3><p>The first step of the procedure - running a prompt through the model and saving the feed-forward network outputs for each block - is straightforward to accomplish with some basic PyTorch hooks. But the first part of step two - finding the strings in the training corpus that produce similar feed-forward network outputs - requires some additional machinery to do efficiently.</p><p>I did all the analysis with length 10 strings for compute and storage efficiency (but I also observed that the results hold for both shorter and longer strings). The 1,115,394-character long training corpus contains 858,923 unique, length 10 substrings. Each feed-forward network output is a 384-dimensional vector of <code>float32</code> values and the model produces 6 of them (one for each block). Comparing the 6 384-dimensional feed-forward outputs for any prompt to 6 * 858,923 = 5,153,538 feed-forward outputs from all the other strings takes a long time. To able to work with this data, I had to pre-compute things. I built the following pipeline:</p><ol><li>I chose 20,000 length 10 strings from the training corpus at random to use as prompts in this experiment.</li><li>Overnight, I ran a process to compute the cosine similarity between the feed-forward network outputs the model produced for the 20,000 prompts and those it produced for the 858,923 unique length 10 substrings of the training corpus. I did this in batches and saved the results to disk.</li><li>Even after pre-computing the cosine similarity results, searching through all of them to find the closest matches took a long time. Experiments showed matches of interest never had a cosine similarity below 0.7, so I ran another step to pre-filter the results of step 2 to just those entries with cosine similarity &gt;= 0.7. This greatly reduced the number of entries to search through.</li></ol><p>The code for this pre-computation and pre-filtering is too much to include in this post, but the implementation is available in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/cosine-sims.ipynb" target="_blank" rel="noopener">the <code>cosine-sims</code> experiment notebook</a>.</p><h3 id="procedure-walkthrough">Procedure Walkthrough <a href="#procedure-walkthrough">🔗</a></h3><p>In this section, we’ll build up the code step by step and run it on one prompt at a time and for just one block. Over the following sections, we’ll extend it to additional blocks, run it across a large number of prompts, and examine the results.</p><p>First, we need to grab 20,000 length 10 strings from the training corpus to use as prompts:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Get all the unique substrings in the text</span>
</span></span><span><span>strings10 <span>=</span> all_unique_substrings(text<span>=</span>ts<span>.</span>text, substring_length<span>=</span><span>10</span>)
</span></span><span><span>
</span></span><span><span>n_prompts <span>=</span> <span>20000</span>
</span></span><span><span>
</span></span><span><span>torch<span>.</span>manual_seed(<span>1337</span>)
</span></span><span><span>indices <span>=</span> torch<span>.</span>randperm(len(strings10))[:n_prompts]
</span></span><span><span>prompts <span>=</span> [strings10[i<span>.</span>item()] <span>for</span> i <span>in</span> indices]
</span></span></code></pre></div><p>As described in the <a href="#procedure-setup">Procedure Setup</a> section, I previously ran all these strings through the model, grabbed the feed-forward network outputs for each block, and pre-computed the cosine similarities to all the unique length 10 substrings in the training corpus. And then I pre-filtered the results to just those with cosine similarity &gt;= 0.7.</p><p>The <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/cosine-sims.ipynb" target="_blank" rel="noopener">the <code>cosine-sims</code> experiment notebook</a> that implements all this also exports a helper function, <code>filter_on_prefiltered_results()</code>, that we can use to find the most similar strings to a given prompt by searching over the pre-filtered results.</p><blockquote><p>If you’re curious about how this works, check out the notebook. It’s pretty straightforward and the unit test provides a simple example that illustrates the shape of the inputs and outputs.</p></blockquote><p>To use <code>filter_on_prefiltered_results()</code>, we just need to tell it how to find the prefiltered files:</p><div><pre tabindex="0"><code data-lang="python"><span><span>prefiltered_threshold<span>=</span><span>0.7</span>
</span></span><span><span>prefiltered_results_folder <span>=</span> environment<span>.</span>data_root <span>/</span> <span>'cosine_sim_results/large_files/slen10'</span> <span>/</span> <span>f</span><span>'prefiltered_</span><span>{</span>prefiltered_threshold<span>}</span><span>'</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>prefiltered_filename</span>(block_idx: int, q_idx: int) <span>-&gt;</span> Path:
</span></span><span><span>    <span>return</span> prefiltered_results_folder <span>/</span> <span>f</span><span>'cosine_sim_ffwd_out_</span><span>{</span>q_idx<span>:</span><span>05d</span><span>}</span><span>_</span><span>{</span>block_idx<span>:</span><span>02d</span><span>}</span><span>.pt'</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>load_prefiltered_data</span>(block_idx: int, q_idx: int):
</span></span><span><span>    <span>return</span> torch<span>.</span>load(prefiltered_filename(block_idx, q_idx))
</span></span></code></pre></div><blockquote><p>Note on the use of <code>q_idx</code> here and in the rest of the code: <code>q_idx</code> refers to “query index”. The job that pre-computes all the cosine similarities takes a set of “queries” or values to compare to. These queries are the feed-forward network outputs the model produces for the prompts. There is a 1:1 correspondence between queries and prompts and so I’ve used the terms interchangeably in the code.</p></blockquote><p>To start, we’ll use the same prompt - <code>'And only l'</code> - we used in the earlier demo. It happens to be the prompt at index 57:</p><pre tabindex="0"><code>'And only l'
</code></pre><p>We’ll find the strings whose feed-forward network outputs in block 0 had a cosine similarity of 0.95 or greater when compared to the block 0 feed forward network output of the prompt.</p><div><pre tabindex="0"><code data-lang="python"><span><span>block_idx <span>=</span> <span>0</span>
</span></span><span><span>similarity_threshold<span>=</span><span>0.95</span>
</span></span><span><span>q_idx <span>=</span> <span>57</span>
</span></span><span><span>similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>    load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>    q_idx_start<span>=</span>q_idx,
</span></span><span><span>    q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>    filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_threshold
</span></span><span><span>)
</span></span><span><span>similar_strings <span>=</span> [
</span></span><span><span>    [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>    <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>]
</span></span><span><span>len(similar_strings[<span>0</span>])
</span></span></code></pre></div><pre tabindex="0"><code>94
</code></pre><p>This produced the 94 similar strings we saw in the demo. We can print them again to be sure:</p><div><pre tabindex="0"><code data-lang="python"><span><span>print(<span>f</span><span>"Original string: </span><span>{</span>repr(prompts[q_idx])<span>}</span><span>"</span>)
</span></span><span><span>print(<span>"Similar strings: </span><span>\n</span><span>"</span>)
</span></span><span><span>
</span></span><span><span>data_columns<span>=</span>[
</span></span><span><span>    [repr(s) <span>for</span> s <span>in</span> similar_strings[<span>0</span>][i : i <span>+</span> <span>20</span>]] <span>for</span> i <span>in</span> range(<span>0</span>, len(similar_strings[<span>0</span>]), <span>20</span>)
</span></span><span><span>]
</span></span><span><span>
</span></span><span><span>print(text_table(
</span></span><span><span>    headers<span>=</span>[],
</span></span><span><span>    data_columns<span>=</span>data_columns,
</span></span><span><span>    col_widths<span>=</span>[<span>18</span> <span>for</span> _ <span>in</span> data_columns]
</span></span><span><span>))
</span></span></code></pre></div><pre tabindex="0"><code>Original string: 'And only l'
Similar strings:

'hat only l'      's sickly l'      ' as\nthey l'     'r kingly l'      're; they l'
'eby they l'      'ar, they l'      'im, only l'      'ling any l'      'life may l'
'nobility l'      'e\nBy any l'     ' as they l'      ', if any l'      ' hastily l'
'tly they l'      ' ghastly l'      '\nMy only l'     'For many l'      'r in any l'
' till my l'      'all they l'      'hen they l'      'at Henry l'      'oolishly l'
'er:\nThey l'     'may they l'      'or stony l'      'ur Henry l'      'l gladly l'
'yet they l'      'y;\nDelay l'     'e, on my l'      'or Henry l'      'I dearly l'
' if they l'      ' she may l'      't\nfairly l'     'ould say l'      'd all my l'
'her they l'      ' Stanley l'      ' and may l'      'uld they l'      'u all my l'
'friendly l'      'h gently l'      'e deadly l'      'f all my l'      'n all my l'
'Ere they l'      'steel my l'      ' tell my l'      'e kingly l'      'learn my l'
'd he say l'      't basely l'      'Thursday l'      'iciously l'      " 'if any l"
' as many l'      'hy glory l'      'not very l'      'a goodly l'      'e surely l'
'quiously l'      ', fairly l'      'lord! my l'      'entle my l'      ', he may l'
'our holy l'      ' worldly l'      ' my only l'      ' all, my l'
'ul, they l'      'o lately l'      's in any l'      ' no lady l'
'ter many l'      'Our holy l'      't vainly l'      'e\nA lady l'
' you may l'      'y greedy l'      'untimely l'      'directly l'
'er on my l'      'e wistly l'      'ng Henry l'      'And only l'
's kindly l'      'KE:\nThey l'     ' of many l'      'o, on my l'
</code></pre><p>Next, we’ll need to build a frequency distribution for the tokens that came after these strings in the text. To make this easy and efficient (we’ll eventually be doing many times), we can pre-compute the next token frequency distributions for all the unique length 10 substrings in the training corpus. The helper function <code>build_next_token_map()</code>, implemented in the <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/common/text-analysis.ipynb" target="_blank" rel="noopener">text-analysis module</a>, does this.</p><div><pre tabindex="0"><code data-lang="python"><span><span>next_token_map10 <span>=</span> build_next_token_map(
</span></span><span><span>    text<span>=</span>ts<span>.</span>text,
</span></span><span><span>    prefix_len<span>=</span><span>10</span>,
</span></span><span><span>    vocab_size<span>=</span>tokenizer<span>.</span>vocab_size,
</span></span><span><span>    stoi<span>=</span>tokenizer<span>.</span>stoi
</span></span><span><span>)
</span></span></code></pre></div><p>The return value stored in <code>next_token_map10</code> is a dictionary that maps each unique length 10 substring in the training corpus to a frequency distribution of the tokens that come after it. Conceptually, it looks something like this:</p><div><pre tabindex="0"><code data-lang="python"><span><span>{
</span></span><span><span>    <span>'the common'</span>: {
</span></span><span><span>        <span>' '</span>: <span>12</span>, <span>"'"</span>: <span>1</span>, <span>','</span>: <span>1</span>, <span>'?'</span>: <span>1</span>, <span>'a'</span>: <span>1</span>, <span>'s'</span>: <span>5</span>, <span>'w'</span>: <span>3</span>
</span></span><span><span>    },
</span></span><span><span>    <span>' the gods '</span>: {
</span></span><span><span>        <span>'b'</span>: <span>1</span>, <span>'c'</span>: <span>1</span>, <span>'d'</span>: <span>2</span>, <span>'f'</span>: <span>1</span>, <span>'g'</span>: <span>1</span>, <span>'h'</span>: <span>2</span>, <span>'k'</span>: <span>2</span>, <span>'s'</span>: <span>2</span>, <span>'t'</span>: <span>1</span>, <span>'w'</span>: <span>2</span>
</span></span><span><span>    },
</span></span><span><span>    <span>' authority'</span>: {
</span></span><span><span>        <span>'</span><span>\n</span><span>'</span>: <span>1</span>, <span>' '</span>: <span>5</span>, <span>','</span>: <span>5</span>, <span>':'</span>: <span>2</span>, <span>';'</span>: <span>1</span>
</span></span><span><span>    },
</span></span><span><span>    <span>...</span>
</span></span><span><span>}
</span></span></code></pre></div><p>In reality, the values are actually tensors of shape <code>(vocab_size,)</code> where <code>vocab_size</code> is the number of unique tokens the vocabulary (65, in our case). The item at index <code>i</code> in the tensor is the count of occurrences of the <code>i</code>th token after the string in that entry’s key. So it looks more like:</p><div><pre tabindex="0"><code data-lang="python"><span><span>{
</span></span><span><span>      <span>'the common'</span>: torch<span>.</span>tensor([
</span></span><span><span>            <span>0</span>, <span>12</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>5</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>3</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>
</span></span><span><span>      ]),
</span></span><span><span>      <span>' the gods '</span>: torch<span>.</span>tensor([
</span></span><span><span>            <span>0</span>, <span>12</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>5</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>3</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>
</span></span><span><span>      ]),
</span></span><span><span>      <span>' authority'</span>: torch<span>.</span>tensor([
</span></span><span><span>          <span>0</span>, <span>12</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>          <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>          <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>          <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>5</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>3</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>
</span></span><span><span>      ]),
</span></span><span><span>    <span>...</span>
</span></span><span><span>}
</span></span></code></pre></div><p>Next, we need to sum the frequency distributions for all the strings we found to have similar feed-forward network outputs to our prompt. Because <code>next_token_map10</code> stores the individual frequency distributions as tensors, this is easy to accomplish:</p><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span></code></pre></div><p>We stack up the distributions for each similar string into a single tensor and then sum across all of them. We can now turn this into a probability distribution by dividing each entry by the sum of all the entries:</p><div><pre tabindex="0"><code data-lang="python"><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum()
</span></span></code></pre></div><p>Finally, we can visualize this distribution:</p><div><pre tabindex="0"><code data-lang="python"><span><span>plot_prob_distribution_for_tokens(prob_distribution, title<span>=</span><span>'Probability distribution using only block 0 similar strings'</span>)
</span></span></code></pre></div><p><img src="https://shyam.blog/posts/beyond-self-attention/images/c04c3fbe83a543ea834691f8ef6c5ecdee18522f3e3e456cd9ea81209eb60b00.png" alt=""></p><p>It’s the same distribution we saw in the demo.</p><p>Now let’s code the comparison to the model output:</p><div><pre tabindex="0"><code data-lang="python"><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>logits<span>.</span>plot_probs(title<span>=</span><span>'Probability distribution from model'</span>)
</span></span></code></pre></div><p><img src="https://shyam.blog/posts/beyond-self-attention/images/08cb64e75759e2a329a0a296931433b06c02c2fb92b6dd0bb440af807cde1d86.png" alt=""></p><p>Again, the two distributions look very similar, and in this example, the approximation uses only values from the first block. To better compare them, we can look at the distributions in text form:</p><div><pre tabindex="0"><code data-lang="python"><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution, tokenizer<span>.</span>itos)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            i: 0.389
o: 0.204            o: 0.250
a: 0.195            a: 0.222
e: 0.160            e: 0.139
</code></pre><p>Finally, we can also compare the Hellinger distance between these distributions:</p><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.0711)
</code></pre><p>By combining the next token frequency distributions of the similar strings from just the first layer of the model, we are able to pretty closely approximate the output of the transformer. Of course, I chose an example that works particularly well.</p><p>Here’s an example where the frequency distribution from just the first layer doesn’t work well:</p><pre tabindex="0"><code>'hing tremb'
</code></pre><p>Using the same method, we can identify 57 strings from the training corpus that produce similar feed-forward network outputs to the prompt:</p><div><pre tabindex="0"><code data-lang="python"><span><span>block_idx <span>=</span> <span>0</span>
</span></span><span><span>similarity_threshold<span>=</span><span>0.95</span>
</span></span><span><span>similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>    load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>    q_idx_start<span>=</span>q_idx,
</span></span><span><span>    q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>    filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_threshold
</span></span><span><span>)
</span></span><span><span>similar_strings <span>=</span> [
</span></span><span><span>    [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>    <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>]
</span></span><span><span>len(similar_strings[<span>0</span>])
</span></span></code></pre></div><pre tabindex="0"><code>57
</code></pre><p>We can look up, sum, and normalize the frequency distributions of tokens that follow these strings in the training corpus, and compare the result to the model outputs, as we did before:</p><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum()
</span></span><span><span>
</span></span><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution, tokenizer<span>.</span>itos)
</span></span><span><span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
l: 0.999            e: 0.543
e: 0.000            l: 0.343
r: 0.000            r: 0.114
</code></pre><p>Unlike the previous example, these distributions are quite different. The top 3 tokens are the same in each, but they’re in the wrong order and their probabilities are far apart. These differences contribute to a large Hellinger distance:</p><div><pre tabindex="0"><code data-lang="python"><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.6305)
</code></pre><p>For the prompt, <code>'hing tremb'</code>, just using the values from the first block results in a poor approximation of the transformer’s output. We’ll soon add the contributions from other blocks and when we do, we’ll get the Hellinger distance between the approximation and the real transformer output for this prompt down from 0.63 to just 0.02.</p><h3 id="similarity-thresholds">Similarity Thresholds <a href="#similarity-thresholds">🔗</a></h3><p>In the preceding examples, I used a similarity threshold of 0.95: I searched for strings whose feed-forward network outputs in block 0 produced values with a cosine similarity of 0.95 or greater when compared to the feed-forward network output of the prompt.</p><p>A different threshold would have yielded different results. For example, doing the same exercise for prompt id 57 (<code>'And only l'</code>) with a threshold of 0.90 finds 612 similar strings, vs the 94 we had before:</p><div><pre tabindex="0"><code data-lang="python"><span><span>block_idx <span>=</span> <span>0</span>
</span></span><span><span>similarity_threshold<span>=</span><span>0.90</span>
</span></span><span><span>q_idx <span>=</span> <span>57</span>
</span></span><span><span>similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>    load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>    q_idx_start<span>=</span>q_idx,
</span></span><span><span>    q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>    filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_threshold
</span></span><span><span>)
</span></span><span><span>similar_strings <span>=</span> [
</span></span><span><span>    [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>    <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>]
</span></span><span><span>len(similar_strings[<span>0</span>])
</span></span></code></pre></div><pre tabindex="0"><code>612
</code></pre><p>If we do the rest of the approximation procedure, we see different (and worse) results:</p><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum()
</span></span><span><span>
</span></span><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution, tokenizer<span>.</span>itos)
</span></span><span><span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            o: 0.584
o: 0.204            i: 0.251
a: 0.195            a: 0.095
e: 0.160            e: 0.066
u: 0.004            u: 0.002
l: 0.000            y: 0.001
</code></pre><p>The top 5 tokens are the same, but when ranked by probability, the approximation has a different ordering than the model. The Hellinger distance is also higher:</p><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.2856)
</code></pre><p>Loosening the similarity threshold introduced strings into the calculation that resulted in a worse approximation. Tightening beyond 0.95 also produces worse results than we got with 0.95, presumably because we’re excluding strings that were needed to produce a good approximation:</p><div><pre tabindex="0"><code data-lang="python"><span><span>block_idx <span>=</span> <span>0</span>
</span></span><span><span>similarity_threshold<span>=</span><span>0.97</span>
</span></span><span><span>q_idx <span>=</span> <span>57</span>
</span></span><span><span>similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>    load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>    q_idx_start<span>=</span>q_idx,
</span></span><span><span>    q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>    filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_threshold
</span></span><span><span>)
</span></span><span><span>similar_strings <span>=</span> [
</span></span><span><span>    [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>    <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>]
</span></span><span><span>len(similar_strings[<span>0</span>])
</span></span></code></pre></div><pre tabindex="0"><code>33
</code></pre><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum()
</span></span><span><span>
</span></span><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution, tokenizer<span>.</span>itos)
</span></span><span><span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            o: 0.278
o: 0.204            i: 0.250
a: 0.195            a: 0.250
e: 0.160            e: 0.222
</code></pre><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.1498)
</code></pre><p>For the first block, 0.95 appears to be a sweet spot. I came up with this threshold through manual tuning: trying different values and binary searching towards one that produced the best results. The full history of this tuning exercise is in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/40_widening_similar_space.ipynb" target="_blank" rel="noopener">the similar space analysis notebook</a>.</p><p>In the end, I found the following thresholds produce the best results for each block:</p><table><thead><tr><th>Block</th><th>Similarity Threshold</th></tr></thead><tbody><tr><td>0</td><td>0.95</td></tr><tr><td>1</td><td>0.94</td></tr><tr><td>2</td><td>0.85</td></tr><tr><td>3</td><td>0.76</td></tr><tr><td>4</td><td>0.81</td></tr><tr><td>5</td><td>0.89</td></tr></tbody></table><blockquote><p>When I first started exploring this space, I assumed the approximation would get better the more similarity I could find. I tried a number of techniques, including experimenting with Euclidean distance vs cosine similarity, searching across strings of different lengths, etc. Every time I succeeded in finding strings with more similar feed-forward network outputs to use in the approximation, the results got worse. I realized that, at least for some blocks, including <em>less</em> similar values in the mix produced better approximations, probably because those blocks had learned to map prompts to broader classes of strings in the training corpus.</p></blockquote><h3 id="going-beyond-the-first-block">Going Beyond the First Block <a href="#going-beyond-the-first-block">🔗</a></h3><p>Thus far, we’ve only considered feed-forward network outputs from the first block. Now we’ll incorporate the contributions from the other blocks.</p><p>First, let’s find the strings that produce similar feed-forward network outputs in each block, using the similarity thresholds listed above. For now, we’ll do this for just one query (index 57, <code>'And only l'</code>):</p><div><pre tabindex="0"><code data-lang="python"><span><span>similarity_thresholds<span>=</span>[<span>0.95</span>, <span>0.94</span>, <span>0.85</span>, <span>0.76</span>, <span>0.81</span>, <span>0.89</span>]
</span></span><span><span>q_idx <span>=</span> <span>57</span>
</span></span><span><span>
</span></span><span><span>similar_strings_per_block <span>=</span> []
</span></span><span><span>
</span></span><span><span><span>for</span> block_idx <span>in</span> range(n_layer):
</span></span><span><span>    similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>        load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>        q_idx_start<span>=</span>q_idx,
</span></span><span><span>        q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>        filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_thresholds[block_idx]
</span></span><span><span>    )
</span></span><span><span>    similar_strings <span>=</span> [
</span></span><span><span>        [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>        <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>    ]
</span></span><span><span>    similar_strings_per_block<span>.</span>append(similar_strings)
</span></span></code></pre></div><p>Let’s summarize how many strings we found for each block based on these thresholds:</p><div><pre tabindex="0"><code data-lang="python"><span><span>print(text_table(
</span></span><span><span>    headers<span>=</span>[<span>"Block Index"</span>, <span>"Similarity Threshold"</span>, <span>"# of Similar Strings"</span>],
</span></span><span><span>    data_columns<span>=</span>[
</span></span><span><span>        [<span>f</span><span>"</span><span>{</span>block_idx<span>:</span><span>&gt;10</span><span>}</span><span>"</span> <span>for</span> block_idx <span>in</span> range(n_layer)],
</span></span><span><span>        [<span>f</span><span>"</span><span>{</span>threshold<span>:</span><span>&gt;19</span><span>}</span><span>"</span> <span>for</span> threshold <span>in</span> similarity_thresholds],
</span></span><span><span>        [<span>f</span><span>"</span><span>{</span>len(similar_strings[<span>0</span>])<span>:</span><span>&gt;19</span><span>}</span><span>"</span> <span>for</span> similar_strings <span>in</span> similar_strings_per_block],
</span></span><span><span>    ],
</span></span><span><span>    col_widths<span>=</span>[<span>14</span>, <span>23</span>, <span>23</span>]
</span></span><span><span>))
</span></span></code></pre></div><pre tabindex="0"><code>Block Index   Similarity Threshold   # of Similar Strings
-----------   --------------------   --------------------
         0                   0.95                     94
         1                   0.94                     47
         2                   0.85                     70
         3                   0.76                    108
         4                   0.81                    175
         5                   0.89                   2237
</code></pre><p>Now that we’ve identified the right strings for each block, we can do the next step of the approximation procedure: build the frequency distributions for the tokens that follow those strings, and sum them up. We’re going to be doing this several times over, so let’s define a function for it:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>frequency_distribution_from_similar_strings</span>(
</span></span><span><span>    similar_strings_per_block: Sequence[Sequence[Sequence[str]]],
</span></span><span><span>    next_token_map: Dict[str, torch<span>.</span>Tensor],
</span></span><span><span>) <span>-&gt;</span> torch<span>.</span>Tensor:
</span></span><span><span>    <span># freqs_per_block_per_query is a list of lists of tensors. The outer list has</span>
</span></span><span><span>    <span># one item per block. The inner list has one item per query. Each</span>
</span></span><span><span>    <span># tensor is the next token frequency distribution for a particular</span>
</span></span><span><span>    <span># block and query.</span>
</span></span><span><span>    freqs_per_block_per_query: List[List[torch<span>.</span>Tensor]] <span>=</span> [[] <span>for</span> _ <span>in</span> range(n_layer)]
</span></span><span><span>
</span></span><span><span>    <span>for</span> block_idx <span>in</span> range(n_layer):
</span></span><span><span>        <span>for</span> similar_strings <span>in</span> similar_strings_per_block[block_idx]:
</span></span><span><span>            freqs_per_block_per_query[block_idx]<span>.</span>append(
</span></span><span><span>                torch<span>.</span>stack([next_token_map[string] <span>for</span> string <span>in</span> similar_strings])<span>.</span>sum(
</span></span><span><span>                    dim<span>=</span><span>0</span>
</span></span><span><span>                )
</span></span><span><span>            )
</span></span><span><span>
</span></span><span><span>    <span># Stack all frequency tensors into a single tensor of shape</span>
</span></span><span><span>    <span># (n_layer, n_queries, vocab_size)</span>
</span></span><span><span>    freqs <span>=</span> torch<span>.</span>stack(
</span></span><span><span>        [
</span></span><span><span>            torch<span>.</span>stack(freqs_per_block_per_query[block_idx])
</span></span><span><span>            <span>for</span> block_idx <span>in</span> range(n_layer)
</span></span><span><span>        ]
</span></span><span><span>    )
</span></span><span><span>
</span></span><span><span>    <span>return</span> freqs
</span></span></code></pre></div><p>This function, <code>frequency_distribution_from_similar_strings()</code>, does the equivalent of this code we looked at earlier:</p><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span></code></pre></div><p>But with two key differences:</p><ul><li>It does this calculation for all the blocks, using the similar strings we found for each block above.</li><li>It allows for more than one query. In the code we’ve looked at so far, we only evaluated the approximation for a single prompt. In the next section, we’ll be running it for lots of prompts so I’ve written the code in a more general form to a allow for this. Specifically, the code allows for <code>similar_strings_per_block</code> to contain not just a single list of strings per block but multiple: one for each query.</li></ul><p>Let’s run this on the <code>similar_strings_per_block</code> we constructed earlier:</p><div><pre tabindex="0"><code data-lang="python"><span><span>freq_distribution <span>=</span> frequency_distribution_from_similar_strings(
</span></span><span><span>    similar_strings_per_block,
</span></span><span><span>    next_token_map10,
</span></span><span><span>)
</span></span><span><span>freq_distribution<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([6, 1, 65])
</code></pre><p>It produces a tensor of shape <code>(6, 1, 65)</code>: 6 blocks, 1 query, 65 tokens in the vocabulary. If we’d been working with more queries, the middle dimension would be larger.</p><p>So now we have a frequency distribution for each block, based on the strings found for each block using the similarity thresholds. We now need to turn this into a probability distribution.</p><p>Earlier, when we just had a single frequency distribution for a single block, we just normalized it. But now we have multiple frequency distributions - one for each block - and need to combine them. In my experiments, I found that a weighted sum of these distributions produced the best results.</p><p>As with the similarity thresholds, I was able to find a set of good weights by trial and error. I also tried a deep-learning approach to find weights, but did not get better results than with the hand-tuned approach. The procedure for both hand-tuning and learning weights is implemented in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/40_widening_similar_space.ipynb" target="_blank" rel="noopener">the similar space notebook</a>, the same one used for tuning thresholds.</p><p>For now, let’s use the optimal weights I found:</p><div><pre tabindex="0"><code data-lang="python"><span><span>weights <span>=</span> torch<span>.</span>tensor([<span>0.01</span>, <span>0.01</span>, <span>0.1</span>, <span>1.5</span>, <span>6</span>, <span>0.01</span>])<span>.</span>unsqueeze(dim<span>=</span><span>1</span>)<span>.</span>unsqueeze(dim<span>=</span><span>2</span>) <span># (n_layer, 1, 1)</span>
</span></span><span><span>total_freq_distribution <span>=</span> (freq_distribution <span>*</span> weights)<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum(dim<span>=-</span><span>1</span>, keepdim<span>=</span><span>True</span>)
</span></span></code></pre></div><p>We multiply the frequency distributions by the weights, sum across all blocks, and then normalize into a probability distribution. We can now look at how the approximation’s distribution compares to the model’s.</p><blockquote><p>Note: in the code below, we have to index into the <code>prob_distribution</code> tensor with <code>[0]</code> because its first dimension is the number of queries. We’re only working with a single query, so we can just take the first element.</p></blockquote><div><pre tabindex="0"><code data-lang="python"><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution[<span>0</span>], tokenizer<span>.</span>itos)
</span></span><span><span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            i: 0.363
o: 0.204            o: 0.265
a: 0.195            a: 0.213
e: 0.160            e: 0.147
u: 0.004            u: 0.011
l: 0.000            y: 0.000
</code></pre><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution[<span>0</span>], logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.0731)
</code></pre><p>In this particular case, adding the other layers didn’t change the approximation much (if anything, it’s very slightly worse based on Hellinger distance). But let’s look at the example that didn’t work well when we considered just the first layer: prompt id 40 (<code>'hing tremb'</code>).</p><div><pre tabindex="0"><code data-lang="python"><span><span>similarity_thresholds<span>=</span>[<span>0.95</span>, <span>0.94</span>, <span>0.85</span>, <span>0.76</span>, <span>0.81</span>, <span>0.89</span>]
</span></span><span><span>q_idx <span>=</span> <span>40</span>
</span></span><span><span>
</span></span><span><span>similar_strings_per_block <span>=</span> []
</span></span><span><span>
</span></span><span><span><span>for</span> block_idx <span>in</span> range(n_layer):
</span></span><span><span>    similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>        load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>        q_idx_start<span>=</span>q_idx,
</span></span><span><span>        q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>        filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_thresholds[block_idx]
</span></span><span><span>    )
</span></span><span><span>    similar_strings <span>=</span> [
</span></span><span><span>        [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>        <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>    ]
</span></span><span><span>    similar_strings_per_block<span>.</span>append(similar_strings)
</span></span><span><span>
</span></span><span><span>freq_distribution <span>=</span> frequency_distribution_from_similar_strings(
</span></span><span><span>    similar_strings_per_block,
</span></span><span><span>    next_token_map10,
</span></span><span><span>)
</span></span><span><span>weights <span>=</span> torch<span>.</span>tensor([<span>0.01</span>, <span>0.01</span>, <span>0.1</span>, <span>1.5</span>, <span>6</span>, <span>0.01</span>])<span>.</span>unsqueeze(dim<span>=</span><span>1</span>)<span>.</span>unsqueeze(dim<span>=</span><span>2</span>) <span># (n_layer, 1, 1)</span>
</span></span><span><span>total_freq_distribution <span>=</span> (freq_distribution <span>*</span> weights)<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum(dim<span>=-</span><span>1</span>, keepdim<span>=</span><span>True</span>)
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>
</span></span><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution[<span>0</span>], tokenizer<span>.</span>itos)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
l: 0.999            l: 0.997
e: 0.000            e: 0.002
r: 0.000            r: 0.000
</code></pre><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor([0.0233])
</code></pre><p>Remember that for this example, when we used just the first layer’s similar strings, the approximation was quite different from the model’s prediction and had a Hellinger distance of &gt;0.63. Now it’s nearly identical and has a Hellinger distance of 0.02. So using the rest of the layers really helped this example.</p><p>In the next section, we’ll extend the code to evaluate the approximation over the whole set of 20,000 prompts. The section after that will look at how well the approximation does across all the prompts.</p><h3 id="extending-to-all-20000-prompts">Extending to All 20,000 Prompts <a href="#extending-to-all-20000-prompts">🔗</a></h3><p>We now have all the pieces we need to run the approximation procedure for all 20,000 prompts. First, let’s find the strings with similar feed-forward network outputs for all the prompts, for all blocks:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Takes about 7 minutes to run</span>
</span></span><span><span>
</span></span><span><span>similarity_thresholds<span>=</span>[<span>0.95</span>, <span>0.94</span>, <span>0.85</span>, <span>0.76</span>, <span>0.81</span>, <span>0.89</span>]
</span></span><span><span>
</span></span><span><span>similar_strings_per_block <span>=</span> []
</span></span><span><span>
</span></span><span><span><span>for</span> block_idx <span>in</span> range(n_layer):
</span></span><span><span>    similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>        load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>        q_idx_start<span>=</span><span>0</span>,
</span></span><span><span>        q_idx_end<span>=</span>n_prompts,
</span></span><span><span>        filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_thresholds[block_idx]
</span></span><span><span>    )
</span></span><span><span>    similar_strings <span>=</span> [
</span></span><span><span>        [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>        <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>    ]
</span></span><span><span>    similar_strings_per_block<span>.</span>append(similar_strings)
</span></span></code></pre></div><p>Next, we compute the frequency distributions for each query based on the strings we found, perform the weighted sum, and normalize to produce a probability distribution.</p><div><pre tabindex="0"><code data-lang="python"><span><span>freq_distribution <span>=</span> frequency_distribution_from_similar_strings(
</span></span><span><span>    similar_strings_per_block,
</span></span><span><span>    next_token_map10,
</span></span><span><span>)
</span></span><span><span>weights <span>=</span> torch<span>.</span>tensor([<span>0.01</span>, <span>0.01</span>, <span>0.1</span>, <span>1.5</span>, <span>6</span>, <span>0.01</span>])<span>.</span>unsqueeze(dim<span>=</span><span>1</span>)<span>.</span>unsqueeze(dim<span>=</span><span>2</span>) <span># (n_layer, 1, 1)</span>
</span></span><span><span>total_freq_distribution <span>=</span> (freq_distribution <span>*</span> weights)<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum(dim<span>=-</span><span>1</span>, keepdim<span>=</span><span>True</span>)
</span></span><span><span>prob_distribution<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([20000, 65])
</code></pre><p>The output is a tensor of shape (20000, 65): one 65-entry distribution for each of 20,000 prompts.</p><p>In order to compare, we need to run all the prompts through the model and get the output probability distributions the model predicts:</p><div><pre tabindex="0"><code data-lang="python"><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_strings(prompts)
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_probs <span>=</span> logits<span>.</span>probs()
</span></span><span><span>model_probs <span>=</span> model_probs[:, <span>-</span><span>1</span>, :] <span># We're only interested in the last token</span>
</span></span></code></pre></div><p>Now we have outputs from the approximation and from the model for all prompts. In the next section, we’ll measure the Hellinger distance between them and evaluate the results.</p><h2 id="evaluating-the-approximation">Evaluating the Approximation <a href="#evaluating-the-approximation">🔗</a></h2><p>In earlier sections, we compared output from the approximation to output from the model for individual prompts. Now that we have both outputs for all prompts, we can compare them and look at aggregate results.</p><p>First, we can compute the Hellinger distance between the approximation and the model’s prediction for each prompt:</p><div><pre tabindex="0"><code data-lang="python"><span><span>h <span>=</span> hellinger_distance(prob_distribution, model_probs)
</span></span><span><span>h<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([20000])
</code></pre><p>This produced 20,000 Hellinger distance scores, one for each prompt. We can start by looking at some basic stats:</p><div><pre tabindex="0"><code data-lang="python"><span><span>h<span>.</span>mean(), h<span>.</span>std(), h<span>.</span>min(), h<span>.</span>max()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(0.1677), tensor(0.1215), tensor(0.0013), tensor(0.9994))
</code></pre><p>The average Hellinger distance is just below 0.17, with a standard deviation of around 0.12, suggesting a distribution that skews low (a good thing). We’ve also got at least one really excellent sample (a min of 0.0013) and at least one really terrible one (max of 0.9994).</p><p>Let’s look at the distribution:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/e9c543696d0748c74bccfac0780e9e6a5cd7610dafc6e650fb5dab2192fc8399.png" alt=""></p><p>Indeed, the distribution is skewed left, indicating most queries have Hellinger distance scores on the lower end.</p><p>The numbers and the distribution graph look promising, but is the approximation really a good one? It’s hard to say without something to compare against and it’s not obvious what a good comparison might be.</p><p>A thought experiment: let’s imagine that for some prompt, the model produced a distribution that looked like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/d01e3755f6278cc6f19ae5656ab3ba6fd7b4ecb59c69303db814b6cc43fb0435.png" alt=""></p><p>The tokens <code>b</code> and <code>d</code> have nearly the same predicted probability (0.49 vs 0.51). The model predicts an approximately equal chance of these tokens coming next. Now imagine our approximation, or another model, predicted this distribution:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/02bddd27aedd082ab84a2b5dd45dacae4e745dc49ed363df5725916e4b370844.png" alt=""></p><p>Nearly the same, but the probabilities are reversed: <code>b</code> has probability 0.51 and <code>d</code> has 0.49. Would we care about this difference? Clearly both distributions are saying that <code>b</code> and <code>d</code> are about equally likely. If used for inference, either distribution would probably produce acceptable results. For most use cases I could imagine, the difference would just be noise.</p><p>The Hellinger distance between the two imagined distributions above is 0.0141. Not zero, but we’re saying it doesn’t matter for practical purposes. If 0.0141 is a Hellinger distance that doesn’t matter much, what about 0.02? Or 0.025? We can imagine there is some threshold Hellinger distance below which we wouldn’t care and above which we would consider distributions to be meaningfully different. What is that threshold value?</p><p>If we knew it, then we could look at how close the average Hellinger distance between our approximation’s predictions and model’s come to this threshold. That would be a measure of the goodness of the approximation.</p><p>I did an experiment to estimate what the threshold is. I trained the same transformer architecture three more times, starting with a different random seed each time and stopping at approximately the same training and validation loss as I did for the original model. This gave me three alternative transformers with roughly the same performance, but with different weights due to the different random initial starting points:</p><table><thead><tr><th>Model</th><th>Seed</th><th>Est. Training Loss</th><th>Est. Validation Loss</th></tr></thead><tbody><tr><td>Original Model</td><td>1337</td><td>0.9334</td><td>1.5063</td></tr><tr><td>Alternate 1</td><td>1442</td><td>0.9293</td><td>1.5038</td></tr><tr><td>Alternate 2</td><td>88</td><td>0.9294</td><td>1.4991</td></tr><tr><td>Alternate 3</td><td>99999</td><td>0.9339</td><td>1.4941</td></tr></tbody></table><blockquote><p>I used the same training/validation sets, hyperparameters, optimizer, etc. for the three alternate models as for the original model. The training code and output for the alternate models is in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/alternate-models.ipynb" target="_blank" rel="noopener">the <code>alternate-models</code> experiment notebook</a>. Training code for the original model is at the end of <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer.ipynb" target="_blank" rel="noopener">the main transformer notebook</a>.</p></blockquote><p>I then ran the same 20,000 prompts through the alternative models and calculated the Hellinger distance between their outputs and that of the original model. <a href="#ii-evaluation-of-main-model-vs-3-alternate-models">Appendix II</a> shows the code used to do this. The table below shows the aggregate results.</p><table><thead><tr><th>Comparison</th><th>Mean Hellinger Distance</th></tr></thead><tbody><tr><td>Original vs Alternate 1</td><td>0.1064 ± 0.0823</td></tr><tr><td>Original vs Alternate 2</td><td>0.1057 ± 0.0817</td></tr><tr><td>Original vs Alternate 3</td><td>0.1053 ± 0.0828</td></tr></tbody></table><p>The original model and the three alternate models are “equivalent” in the sense that they perform about equally well in terms of training and validation loss. I could have used any of them as the basis for this post. In other words, the differences between them likely aren’t meaningful - just noise.</p><p>Across all three alternate models, the average Hellinger distance was ~0.11 ± 0.08. We only have 3 data points, so it’s not a perfect measure, but ~0.11 is probably a reasonable lower bound for the threshold Hellinger distance we are looking for.</p><p>For comparison, the average Hellinger distance between the model and the approximation was ~0.17. A little higher than 0.11, but within a standard deviation.</p><p>Plotting the distributions of the various Hellinger distances shows this nicely:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/d82fe84ec51461c861f5fbc1c2c273935d4d8ac8ceceaeeb292d79cd5fb9ee19.png" alt=""></p><p>There is clearly less deviation between the alternates and the original model than between the approximation and the original model, but it’s not wildly different. I think this result suggests the approximation is quite good. Most of the difference is within the “acceptable noise” threshold.</p><h2 id="interpretation-why-does-the-approximation-work">Interpretation: Why Does the Approximation Work? <a href="#interpretation-why-does-the-approximation-work">🔗</a></h2><p>The analysis in the previous section shows that the outputs of the approximation are quite similar to the transformer’s outputs. But that doesn’t necessarily mean that the approximation procedure is similar to what the transformer is actually doing. The approximation and the transformer might just represent two different ways of computing the same result.</p><p>My intuition is that this is not the case: <strong>I think the approximation is at least something like what the transformer is doing</strong>. In this section, I’ll break down <em>how</em> I think the transformer computes something similar to the approximation and then present some supporting evidence.</p><p>The key ideas are:</p><ul><li>The transformer, as its name suggests*, performs a series of transformations on its embedded input. The transformer blocks transform embeddings within embedding space and the final linear layer at the end transforms from embedding space to logit space.</li><li>Within each transformer block, the transformation from input to output embedding is done via vector addition: the block’s output embedding is its input embedding plus the output of the self-attention layer, plus the output of the feed-forward network. Of the two added components, the feed-forward network output value is dominant in determining the final output.</li><li>Within embedding space, subspaces exist that correspond to specific tokens. An embedding within the subspace for a particular token produces an output distribution in which all the probability is concentrated on that token (that token has probability near 1 and all other tokens have probability near 0). Embeddings that lie between the subspaces for multiple tokens result in outputs that distribute all the probability across those tokens.</li><li>The feed-forward network output at each block is an “adjustment vector” that orients the block output towards the subspaces for the tokens that the approximation procedure would predict: those that follow the strings in the training corpus that produce similar feed-forward network outputs at that block.</li></ul><p>In the subsections below, I’ll go into each of these ideas in more detail.</p><blockquote><p>*It’s unclear whether the name “transformer” alludes to transforming an input sequence to an output sequence (the use case in the original paper was machine translation) or the transformations within the layers of the model.</p></blockquote><h3 id="the-model-is-a-series-of-transformations">The Model is a Series of Transformations <a href="#the-model-is-a-series-of-transformations">🔗</a></h3><p>Once the input to the model has been embedded, we can view the model as a series of transformations:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/c0b6c591002c7e1f931a0bcc794b88454aab02158d1125bef2f8422dbc0e8264.png" alt=""></p><p>The sequence of 6 transformer blocks takes a tensor in embedding space (\(\mathbb{R}^{384}\), since <code>n_embed=384</code>) as input and outputs another tensor in embedding space. In this sense, represents a transformation <em>within</em> embedding space. In fact, each transformer block is itself a transformation within embedding space and the stack of all 6 blocks composes these individual transformations. It isn’t literally implemented this way in code, but its equivalent to:</p><div><pre tabindex="0"><code data-lang="python"><span><span>output_embedding <span>=</span> block6(block5(block4(block3(block2(block1(input_embedding))))))
</span></span></code></pre></div><p>At the end of the sequence of blocks, the model sends the output embedding through a LayerNorm operation and then a linear layer that transforms from embedding space into logit space (\(\mathbb{R}^{65}\), since <code>vocab_size=65</code>). Finally, the softmax layer at the end turns the logits into probabilities for the next token.</p><h3 id="transformation-via-vector-addition">Transformation via Vector Addition <a href="#transformation-via-vector-addition">🔗</a></h3><p>We looked at the internal logic within a transformer block in the earlier <a href="#transformer-block-structure">Transformer Block Structure</a> section. To recap, the <code>forward()</code> method of the <code>Block</code> module looks like this:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>Block</span>(nn<span>.</span>Module):
</span></span><span><span>    <span>"""One transformer block"""</span>
</span></span><span><span>
</span></span><span><span>    <span>...</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span>(self, x):
</span></span><span><span>        x <span>=</span> x <span>+</span> self<span>.</span>sa(self<span>.</span>ln1(x)) <span># The `x +` part is a skip connection</span>
</span></span><span><span>        x <span>=</span> x <span>+</span> self<span>.</span>ffwd(self<span>.</span>ln2(x)) <span># The `x +` part is a skip connection</span>
</span></span><span><span>
</span></span><span><span>        <span>return</span> x
</span></span></code></pre></div><p><a id="block-logic-with-intermediates"></a>
This is equivalent to the following code, which, by using some intermediate local variables, clarifies what’s really going on:</p><div><pre tabindex="0"><code data-lang="python"><span><span>    <span>def</span> <span>forward</span>(self, x):
</span></span><span><span>        sa_out <span>=</span> self<span>.</span>sa(self<span>.</span>ln1(x))
</span></span><span><span>        ffwd_out <span>=</span> self<span>.</span>ffwd(self<span>.</span>ln2(x <span>+</span> sa_out))
</span></span><span><span>
</span></span><span><span>        <span>return</span> x <span>+</span> sa_out <span>+</span> ffwd_out
</span></span></code></pre></div><p><strong>The output of the block is equal to the input (<code>x</code>), plus the self-attention output (<code>sa_out</code>), plus the feed forward network output (<code>ffwd_out</code>).</strong> We can think of the block as taking the input embedding, and then making two adjustments to it.</p><p>These values being added together are vectors in \(\mathbb{R}^{384}\). If we imagine the embedding space reduced to just two dimensions, it might look something like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/12e04e2fdd477aaa55660b4020a4ae6b9a72c55038f98009049330cfecfc4291.png" alt=""></p><p>The red vector represents the input embedding. The green vector represents the self-attention output (<code>sa_out</code> in code), and the blue vector represents the feed-forward network output (<code>ffwd_out</code> in code). The gray arrow represent the final sum, or the output of the first block: where you end up when you arrange the individual vectors tip to tail.</p><p>The plot above shows the additions that happen within just one block. Subsequent blocks add their self-attention outputs and feed-forward network outputs, starting from the output of this block. If we add the vectors from those other blocks to the diagram, it looks like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/24b90fb31e5e330043b12b6a3b0bf9e8bed15bbd4cd57e98f0cbdc4b393fc71f.png" alt=""></p><p>Again, the red arrow represents the input vector, each green arrow represents one block’s self-attention output, each blue arrow represents one block’s feed-forward network output. Arranged tip to tail, their endpoint represents the final output from the stack of 6 blocks, depicted by the gray arrow.</p><p>Though it’s only in two dimensions, the diagram above is based on real data and is drawn “to scale”, in a way: the length of each 2D vector is the same as the \(\mathbb{R}^{384}\) vector it represents for a real query (index 57). In addition, the cosine similarity between each 2D blue / green arrow and the sum of the arrows that precede it is the same as the cosine similarity between the corresponding self-attention/feed-forward network output and the block input in the real data.</p><blockquote><p>Code to generate the 2D representation from real data is in the <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb" target="_blank" rel="noopener">embedding adjustments analysis notebook</a>.</p></blockquote><p>We can observe two interesting patterns:</p><ul><li>The feed-forward network outputs are generally longer than the self-attention outputs (the vectors have larger norms)</li><li>Within a given block, the feed-forward network output and the self-attention output point in roughly the same direction.</li></ul><p>Look at what happens when we eliminate the self-attention outputs from the vector sum, leaving just the feed-forward network outputs:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/7fff50753ede8a54541e69eaf00215ea285f523817e8361d1fe08ac5e0c6cd8a.png" alt=""></p><p>The inner blue curve in the above plot represents the sum of the input vector and only the feed-forward network outputs from each block. The tip to tail arrangement of these vectors ends at a point far from where the previous arrangement (including the self-attention outputs) ended. But notice that the feed-forward-only endpoint (shorter gray arrow) is quite closely aligned in <em>direction</em> with the original endpoint (longer gray arrow).</p><p>This plot shows values for only one query and we lose a lot of information dropping from 384 dimensions to 2. But the pattern does seem to hold in general and in the full, high-dimensional embedding space. The <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb" target="_blank" rel="noopener">embedding adjustments analysis notebook</a> provides a deep-dive into this phenomenon across all 20,000 queries.</p><p>The takeaway is that <strong>simplifying the transformation performed by the blocks to just the contributions of the feed-forward networks results in an output vector that is shorter (has a smaller norm) than the original output but points in roughly the same direction</strong>. And the difference in norms would have no impact on the transformer’s final output, because of the LayerNorm operation after the stack of blocks. That LayerNorm step will adjust the norm of any input vector to similar value regardless of its initial magnitude; the final linear layer that follows it will always see inputs of approximately the same norm (see <a href="#iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">Appendix III</a> for a walk-through of this).</p><p>An important clarification: I’m not suggesting that we could remove the self attention computation from the transformer. The feed-forward networks take the self-attention output as part of their input (<code>ffwd_out = self.ffwd(self.ln2(x + </code><strong><code>sa_out</code></strong><code>))</code>); they would compute very different values were the self-attention outputs removed. What I am saying is that, after all block processing has been completed as normal including the self-attention computations, we get roughly the same result if we consider only the feed-forward network contributions, as our approximation does. This is probably because the feed-forward network outputs pass on some of the information they receive as input from the self-attention output.</p><p>For some additional evidence that an approximation based only on feed-forward network outputs can produce similar outputs to the transformer, see <a href="#iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs">Appendix IV</a>.</p><h3 id="token-subspaces">Token Subspaces <a href="#token-subspaces">🔗</a></h3><p>In the examples we’ve seen so far, the model outputs have been distributions that include significant non-zero probabilities for several tokens. For example:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/a90c56ba1e72733d29a614671ad61789b5196562087c1464273da469843cb54d.png" alt=""></p><p>Though we haven’t seen one yet, we might wonder whether <strong>specific inputs</strong> exist that compel the model to predict a <strong>single token</strong> with <strong>near certainty</strong>. In other words, do some inputs cause the model to output a probability distribution in which just one token has probability very near 1 and all other tokens very near zero? Such a distribution might look like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/ecc30428196c016ef2970ee406b6ea46a79a2a24c1521d8843c2dc90f83ffc83.png" alt=""></p><p>In fact, we can ask this question about any stage of the model. “Input” doesn’t have to refer to the initial input to the model, but could be the input to any layer within the model. For example, consider only the layers that transform the final block’s output embedding to logit space (the final LayerNorm and linear layers):</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/4c4339d5f6612480cf52b0d34f2c1732c99485033c1196076543f86ec0925af5.png" alt=""></p><p>Is there some embedding block 6 might emit that would yield an output probability distribution in which some token, say the letter <code>a</code>, has probability very near 1?</p><h4 id="learning-token-subspaces">Learning Token Subspaces <a href="#learning-token-subspaces">🔗</a></h4><p>With the right math, it may be possible to find this embedding analytically. But it’s also possible to “learn” (in the sense of deep learning) such an embedding. Here’s the basic idea:</p><ul><li>Pick a point in the transformer where the input to subsequent layers is an embedding. This could be the input to any of the transformer blocks, or the point right after the final block (as shown in the diagram above).</li><li>Pick a token to learn an embedding for.</li><li>Create an embedding tensor and initialize it with random values. This tensor is the parameter the learning algorithm will optimize; the weights of the transformer are fixed.</li><li>Execute a forward pass by evaluating the transformer from the selected point, using the embedding as input. This will produce some set of logits.</li><li>Compute <a href="https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/#nll" target="_blank" rel="noopener">negative log likelihood loss</a> relative to the token we’re learning an embedding for.</li><li>Do a backward pass, updating the embedding tensor according to the gradients.</li></ul><p>My implementation of this is in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/learn-embeddings.ipynb" target="_blank" rel="noopener">the learned embeddings notebook</a>. I used it to learn embeddings for all tokens at various stages of the model and saved them. We can load one - a learned embedding that produces a distribution giving token <code>a</code> probability almost 1 - and check that it does what we expect when given to the part of the model shown in the diagram above:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/0dce2cb6b27518fe0a1a26685995c9050cf791b48cc4e407a080182039905dca.png" alt=""></p><p>As expected, all the probability mass is concentrated on <code>a</code>. Inference using this distribution would generate <code>a</code> with near certainty.</p><p>The same procedure can learn embeddings for use at other parts of the model. If we wanted to find an embedding for <code>a</code> that could be input to block 6, we could run the same learning algorithm but use this part of the transformer in the forward pass:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/37991d03eeb45809255009149e32de771715221bb9d315efb6e2aa78b26a897c.png" alt=""></p><p>It’s more computationally expensive to learn embeddings at earlier stages of the model because the optimizer has to contend with a larger computation graph involving operations from all included blocks. Thankfully, as I’ll explain <a href="#use-only-final-subspaces">shortly</a>, we need only the embeddings learned for the part of the transformer after all the blocks (embeddings that go straight into the final LayerNorm layer) to show how the transformer operates like the approximation.</p><h4 id="from-embeddings-to-subspaces">From Embeddings to Subspaces <a href="#from-embeddings-to-subspaces">🔗</a></h4><p>For any token, the procedure described in the previous section can learn an embedding that makes the model predict that token with probability near 1. It turns out <strong>there isn’t just one such embedding for each token.</strong> We can learn many different embeddings that all produce probability distributions that assign a given token nearly all the probability mass. It was easy to learn thousands of unique embeddings for every token in the vocabulary.</p><p>I think <strong>the model has learned a complex, non-linear embedding subspace corresponding to each token</strong>. Any embedding within that subspace results in an output distribution that assigns the token near certain probability. Each embedding I was able to learn is probably a point in the embedding subspace for the corresponding token.</p><p>If we imagine the full embedding space (\(\mathbb{R}^{384}\)) reduced to \(\mathbb{R}^3\) (and the complex subspaces reduced to 2D planes), it might look something like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/04a243b63386cc0e853d350cb0177eeb33f2c15d2aacdaf59ddb4dc38f48b444.png" alt=""></p><p>I don’t know how to determine the exact subspaces for each token mathematically. But I do know how to get a workable approximation of them <strong>if we’re willing to pretend that they are linear</strong>. They are almost certainly not linear, even at the end of the model, because of the non-linear LayerNorm operation. But they are likely <em>closer</em> to linear near the end of the model because the LayerNorm is the only non-linearity. Earlier in the model, each feed-forward network introduces an additional non-linearity via its ReLU operation.</p><blockquote><p><a href="https://www.lesswrong.com/posts/jfG6vdJZCwTQmG7kb/re-examining-layernorm" target="_blank" rel="noopener">This post on LessWrong</a> illustrates of the non-linearity of LayerNorm clearly.</p></blockquote><p>Pretending the subspaces are linear actually works quite well for the part of the model after the transformer blocks. And that is the only part of the model we need to consider for this analysis (as I’ll explain <a href="#use-only-final-subspaces">soon</a>).</p><h4 id="linear-approximations-for-subspaces">Linear Approximations for Subspaces <a href="#linear-approximations-for-subspaces">🔗</a></h4><p>The idea is quite simple: for a given token, we can learn a whole lot of different embeddings, treating each one as a data point. Then we can determine the best fitting line, plane, or other low-dimensional linear subspace that fits the data.</p><p>Again, if we imagine our embedding space reduced to just 3 dimensions, it might look something like the following diagram. The blue dots each represent a learned embedding and the red arrow is the line that minimizes projected distance from each point.</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/a32cb4b311513c8c1bbb0af5cae1d68b1e96efeddfbde076f8e9fca02772d605.png" alt=""></p><p>We can use Singular Value Decomposition (SVD) to find the best fitting linear subspace for the learned embeddings.</p><blockquote><p>To learn more about singular value decomposition in this context, I recommend reading Jeremy Kun’s excellent two-part post. <a href="https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/" target="_blank" rel="noopener">Part 1</a> <a href="https://jeremykun.com/2016/05/16/singular-value-decomposition-part-2-theorem-proof-algorithm/" target="_blank" rel="noopener">Part 2</a>.</p></blockquote><p><a href="#v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace">Appendix V</a> walks through the code that uses SVD to find a linear approximation for the subspace corresponding to one token. I did this for all tokens, using the embeddings I learned for the final stages of the transformer. In every case, I was able to find a single vector (1-D subspace) that approximates the token subspace quite well.</p><blockquote><p>For completeness, I also tried this at earlier stages of the transformer and found, as expected, that the linear approximations, even at higher dimensions, didn’t fit the data as well. The relevant experiments are in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb" target="_blank" rel="noopener">the approximation details notebook</a></p></blockquote><h4 id="mixing-subspace-approximations">Mixing Subspace Approximations <a href="#mixing-subspace-approximations">🔗</a></h4><p>By learning a large number of embeddings for each token and then using SVD on them, we can find one vector for each token that approximates its subspace. Given one of these vectors, any embedding that falls on its span will produce an output distribution that concentrates all the probability mass on the corresponding token. But many of the real transformer outputs we’ve seen distribute the probability mass across several tokens. How do we get from subspaces for individual tokens to embeddings that produce these more diverse distributions?</p><p>We can create embeddings that produce probability distributions where several tokens have substantial probability via linear combinations of the subspace approximation vectors for those tokens. This is the distribution we get when we create an embedding by simply adding the approximation vectors for the subspaces for <code>a</code> and <code>b</code>:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/211fa3aed3f5cbbe1a5adf41e014e27068c39ffaf21aff36c833a851b500e211.png" alt=""></p><p>The sum of subspace approximations vectors for two tokens is an embedding somewhere in between the two subspaces, which results in a final distribution that is the combination of the two tokens.</p><p>Sadly, adding the approximation vectors for <code>a</code> and <code>b</code>, without weighting either one, results in not quite a 50-50 distribution across the two tokens (as shown above). I think there are three reasons for this:</p><ol><li>The approximation vectors are just approximations and not perfect representations of their subspaces.</li><li>The subspace approximation vectors are not perfectly orthogonal. To the extent that <code>a</code>’s vector has a small component that points in the direction of <code>b</code>, the sum results in an overweighting of <code>b</code>.</li><li>The final linear layer of the model produces logits of different magnitudes for different tokens. For example, given the approximation for <code>a</code>, the logit for <code>a</code> is ~18.2. The logit for <code>b</code> from its approximation is ~19.5.</li></ol><p>Together, these errors accumulate and the softmax function at the very end exaggerates even small differences. For more analysis on the reasoning behind the differences and how they might be compensated for, see <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/60_combining_token_subspaces.ipynb" target="_blank" rel="noopener">the combining token subspaces notebook</a>.</p><p>These imperfections aside, I think we can conclude that <strong>it’s possible to derive an embedding that produces a distribution for multiple tokens via a linear combination of the approximation vectors for those tokens’ subspaces</strong>.</p><h3 id="putting-it-all-together">Putting it All Together <a href="#putting-it-all-together">🔗</a></h3><p>To summarize where we are, the preceding sections have shown:</p><ul><li>The transformer blocks perform a series of transformations in embedding space.</li><li>Those transformations can be thought of as moving from one point in embedding space to another by adding the feed-forward network output vector to the input embedding.</li><li>Embedding space contains subspaces corresponding to predicting particular tokens and embeddings between subspaces for multiple tokens result in predictions including all those tokens.</li></ul><p>This section adds the final piece, which is the correspondence between what the transformer is doing and what the approximation is doing:</p><ul><li>Within a block, adding the feed-forward network output vector to the input produces an output embedding that better aligns with the embedding subspaces of specific tokens. And <strong>those tokens are the same ones predicted in the approximation</strong>: they’re the tokens that follow the strings in the training corpus that yield similar feed-forward network outputs to the current prompt.</li></ul><p>Let’s look at an example that shows this. The following is the output distribution predicted by the approximation for the prompt, <code>med me Aut</code> (query index 33), using only the feed-forward network outputs from the final block:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/0a46316fd1ec97000bed4b44242e7aad0809b16acd65a98507ff1b987e313291.png" alt=""></p><p>Based on the strings in the training corpus with similar feed-forward network outputs at the final block, the approximation predicts <code>o</code> is the most likely next token and <code>h</code> is next.</p><p>Next, we need to look at the feed-forward network output for the prompt in this block and determine which token subspaces it’s most oriented towards. I’m going to show a little code here, because I think it’s the best way to explain what’s going on. Readers who aren’t interested in the implementation can focus only on the output.</p><p>First we need to actually grab the feed-forward outputs (we haven’t needed them so far because we’ve been working with precomputed/prefiltered similarity data). We’ll use some <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer-helpers.ipynb" target="_blank" rel="noopener">helper functions</a> that provide easy access to the transformer’s intermediate representations:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Tokenize the strings</span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_strings(prompts)
</span></span><span><span>
</span></span><span><span><span># Embed the tokens</span>
</span></span><span><span>embeddings <span>=</span> accessors<span>.</span>embed_tokens(tokens)
</span></span><span><span>
</span></span><span><span><span># Instantiate TransformerAccessors</span>
</span></span><span><span>accessors <span>=</span> TransformerAccessors(m, device)
</span></span><span><span>
</span></span><span><span><span># Run them through the model with hooks attached that let us look at</span>
</span></span><span><span><span># intermediate values</span>
</span></span><span><span>_, io_accessors <span>=</span> accessors<span>.</span>run_model(embeddings)
</span></span><span><span>
</span></span><span><span><span># Grab the outputs of the ffwd networks at each layer</span>
</span></span><span><span>ffwd_outs <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    io_accessors[block_idx]<span>.</span>output(<span>'ffwd'</span>)[:, <span>-</span><span>1</span>, :]<span>.</span>clone()
</span></span><span><span>    <span>for</span> block_idx <span>in</span> range(n_layer)
</span></span><span><span>])
</span></span><span><span>
</span></span><span><span><span># Free up some memory</span>
</span></span><span><span><span>del</span> io_accessors
</span></span><span><span>_ <span>=</span> gc<span>.</span>collect()
</span></span><span><span>
</span></span><span><span>ffwd_outs<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([6, 20000, 384])
</code></pre><p>To determine which token subspaces the feed-forward network output aligns with, we’ll project it onto the subspace approximation for each token, then determine which projections are most similar to the original vector. To do this, we’ll need to get the projection matrix for the rank 1 approximation to each token subspace:</p><blockquote><p>The code below uses the <code>projection_matrix_for_rank_k_approximation()</code> helper function, defined in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/common/svd-helpers.ipynb" target="_blank" rel="noopener">the SVD helpers notebook</a>.</p></blockquote><blockquote><p>In the case of a rank 1 approximation, the projection isn’t really necessary. We could just take the cosine similarity with the approximation vector, but I wanted to keep this code general because I tried out higher-dimensional approximations in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb" target="_blank" rel="noopener">other places</a>.</p></blockquote><div><pre tabindex="0"><code data-lang="python"><span><span>filename_for_token <span>=</span> FilenameForToken(tokenizer)
</span></span><span><span>subspace_dims <span>=</span> <span>1</span>
</span></span><span><span>projection_matrices <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    projection_matrix_for_rank_k_approximation(
</span></span><span><span>        original_matrix<span>=</span>torch<span>.</span>load(
</span></span><span><span>            learned_embeddings_dir <span>/</span>  <span>'no_blocks'</span> <span>/</span> <span>f</span><span>"</span><span>{</span>filename_for_token(token)<span>}</span><span>.pt"</span>,
</span></span><span><span>            map_location<span>=</span>device,
</span></span><span><span>        )[:, <span>0</span>, :],
</span></span><span><span>        k<span>=</span>subspace_dims,
</span></span><span><span>    )
</span></span><span><span>    <span>for</span> token <span>in</span> tokenizer<span>.</span>chars
</span></span><span><span>])
</span></span></code></pre></div><p>Now we’ll perform the projections and find the top 5 most similar ones to the original feed-forward output vector:</p><div><pre tabindex="0"><code data-lang="python"><span><span>projections <span>=</span> projection_matrices <span>@</span> ffwd_outs[block_idx, q_idx, :]
</span></span><span><span>values, indices <span>=</span> torch<span>.</span>topk(
</span></span><span><span>    F<span>.</span>cosine_similarity(projections, ffwd_outs[block_idx][q_idx], dim<span>=-</span><span>1</span>),
</span></span><span><span>    k<span>=</span><span>5</span>,
</span></span><span><span>    dim<span>=</span><span>0</span>,
</span></span><span><span>)
</span></span><span><span>tokens <span>=</span> [tokenizer<span>.</span>chars[i<span>.</span>item()] <span>for</span> i <span>in</span> indices]
</span></span><span><span>list(zip(tokens, values<span>.</span>tolist()))
</span></span></code></pre></div><pre tabindex="0"><code>[('o', 0.5074884295463562),
 ('h', 0.40787822008132935),
 ('i', 0.26926180720329285),
 ('u', 0.22823508083820343),
 ('y', 0.20325089991092682)]
</code></pre><p>It turns out that <code>o</code> and <code>h</code> are the most similar, indicating that the feed-forward network output is most oriented towards the subspaces for these tokens. And these are the same tokens that the approximation predicted from the strings with similar feed-forward network outputs (see the distribution above).</p><p>Another example, this time looking at query index 36 (<code>if and thy</code>), but staying in the final block:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/fb2e608eb48c085de00f28831641ff4d49f7f3fd63b198fc3fc08d02a3cc7c45.png" alt=""></p><div><pre tabindex="0"><code data-lang="python"><span><span>projections <span>=</span> projection_matrices <span>@</span> ffwd_outs[block_idx, q_idx, :]
</span></span><span><span>values, indices <span>=</span> torch<span>.</span>topk(
</span></span><span><span>    F<span>.</span>cosine_similarity(projections, ffwd_outs[block_idx][q_idx], dim<span>=-</span><span>1</span>),
</span></span><span><span>    k<span>=</span><span>5</span>,
</span></span><span><span>    dim<span>=</span><span>0</span>,
</span></span><span><span>)
</span></span><span><span>tokens <span>=</span> [tokenizer<span>.</span>chars[i<span>.</span>item()] <span>for</span> i <span>in</span> indices]
</span></span><span><span>list(zip(tokens, values<span>.</span>tolist()))
</span></span></code></pre></div><pre tabindex="0"><code>[(' ', 0.5869003534317017),
 ('s', 0.47689366340637207),
 ('\n', 0.38412901759147644),
 ('$', 0.23048195242881775),
 ('a', 0.21783535182476044)]
</code></pre><p>Here <code></code>(space), <code>s</code>, and <code>\n</code> (newline) were the tokens predicted from what follows the strings with similar feed-forward outputs, and indeed these are the token subspaces most aligned with the prompt’s feed-forward output.</p><h4 id="aggregate-performance">Aggregate Performance <a href="#aggregate-performance">🔗</a></h4><p>In the previous section, I purposely picked examples that exhibit strong correlation between the approximation’s predictions and the most aligned subspaces, to illustrate the point most clearly. Of course, there are other examples for which the correlation is less strong. Rather than looking at specific cases, let’s try to get a sense of how well the correlation holds up across all 20,000 prompts.</p><p>This immediately leads to a question: what is the right measure of aggregate performance? Unfortunately, even if the hypothesis - that the prompt’s feed-forward output aligns with the subspaces for tokens predicted from the strings with similar feed forward outputs - is true, a few practical issues make it difficult to demonstrate objectively:</p><ul><li>We don’t have exact definitions of the token subspaces, just imperfect, linear approximations.</li><li>Magnitudes don’t line up: the tokens with the most probability mass in the approximation’s predictions don’t always correspond to the subspaces with the greatest cosine similarity (because of the imperfect approximations, because the adjustment required may be bigger or smaller for some tokens vs others based on the input embedding’s current alignment, because, as explained in the <a href="#mixing-subspace-approximations">Mixing Subspace Approximations</a> section, the model is more “sensitive” to some tokens than others).</li></ul><p>Given these impediments, we can’t just do something simple like normalizing the cosine similarities and computing Hellinger distance with the predicted probability distribution.</p><p>Instead, we need to devise a criterion on which to judge whether the data from a particular prompt supports the hypothesis or not. Then we can evaluate aggregate performance by how many of the 20,000 prompts satisfy the criterion. I experimented with several different approaches and in the end came up with this candidate criterion:</p><p>High-level description: <em>Do the subspaces for the tokens containing 90% of the probability mass in the approximation’s predictions appear in the top half of all token subspaces when ranked by cosine similarity with the prompt’s feed-forward output vector?</em></p><p>Exact definition:</p><ul><li>Define <code>top_n</code> as the number of tokens required to cover at least 90% of the probability mass in the approximation’s predictions for this prompt.</li><li>Define <code>n_subspaces</code> as <code>tokenizer.vocab_size // 2</code> (32, based on our 65-token vocabulary).</li><li>Determine: Are the subspaces for the first <code>top_n</code> tokens predicted by the approximation in the first <code>n_subspaces</code> subspaces ranked by cosine similarity with the prompt’s feed-forward output vector?</li></ul><p>Admittedly, this is an arbitrary definition and reasonable people could debate any of the specifics. But I do think it gives as an indication of whether the data from a particular example prompt supports the hypothesis, while allowing for some of the measurement challenges noted above.</p><p>I evaluated this criteria at three places: the outputs of blocks 6, 5, and 4, using projection matrices derived from learned embeddings at each of these places.</p><blockquote><p>I didn’t evaluate at earlier blocks because the GPU time required to learn embeddings at those blocks became prohibitive. The further back in the model, the bigger the computation graph that the learning algorithm needs to optimize over.</p></blockquote><p>The table below shows the results:</p><blockquote><p>The code that produced these results appears at the end of <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb" target="_blank" rel="noopener">the approximation details notebook</a></p></blockquote><table><thead><tr><th>Block</th><th># of Prompts Satisfying Criterion</th></tr></thead><tbody><tr><td>6</td><td>16357 (81.78%)</td></tr><tr><td>5</td><td>10142 (50.71%)</td></tr><tr><td>4</td><td>7760 (38.80%)</td></tr></tbody></table><p>These numbers aren’t exactly a ringing endorsement. As expected, they get worse the further back we go, probably due to the increased non-linearity.</p><p><a id="use-only-final-subspaces"></a>
What if we always used the subspace approximations from the very end of the transformer (which are likely to be the most linear), even when comparing against feed-forward network outputs from earlier blocks? The results get better:</p><table><thead><tr><th>Block</th><th># of Prompts Satisfying Criterion</th></tr></thead><tbody><tr><td>6</td><td>16357 (81.78%)</td></tr><tr><td>5</td><td>13652 (68.26%)</td></tr><tr><td>4</td><td>11630 (58.15%)</td></tr><tr><td>3</td><td>11469 (57.34%)</td></tr><tr><td>2</td><td>10404 (52.02%)</td></tr><tr><td>1</td><td>9942 (49.71%)</td></tr></tbody></table><blockquote><p>Like many good findings, this one resulted from a bug. I accidentally ran the analysis using the projection matrices for the final part of the transformer with the feed-forward network outputs from earlier blocks and was surprised when the numbers turned out to be so good.</p></blockquote><p>It’s valid to use the subspace approximations (and corresponding projection matrices) from the end of the transformer at earlier stages. All blocks operate in the same embedding space and each one seems to make a small refinement on the output of its predecessors, rather than wild changes in direction. So if any block’s feed-forward network output adjusts an embedding towards the subspaces for a set of tokens as defined at the end of the transformer, it is likely also adjusting it towards whatever the subspaces would be for those same tokens at the block where it operates.</p><blockquote><p>The <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens" target="_blank" rel="noopener">logit lens post</a>, in the section “why? / is this surprising?” provides an explanation that supports this idea. In summary, the residual connections encourage the transformer to learn weights that operate within the same basis across blocks and the use of weight decay in training results in a computation that’s spread out over as many layers as possible, with each layer making only a small, incremental change.</p></blockquote><p>To put these numbers in perspective, I investigated how likely it would be for the criterion I’ve defined here to be satisfied by chance. In other words, if we assume the hypothesis is false and that the cosine similarities between the feed-forward network output and the token subspace approximation vectors are random, rather than expressing meaningful relationships, how likely would it be for the criterion to still be satisfied?</p><p>I ran a simulation of this, taking care to ensure that the distribution of randomly generated cosine similarities matches the real data, among other details. The implementation is at the end of <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb" target="_blank" rel="noopener">the approximation details notebook</a>. The final results are:</p><table><thead><tr><th>Block</th><th>Likely % of Prompts Satisfying Criteria By Chance</th></tr></thead><tbody><tr><td>6</td><td>20.76% ± 0.25%</td></tr><tr><td>5</td><td>20.55% ± 0.26%</td></tr><tr><td>4</td><td>18.37% ± 0.24%</td></tr><tr><td>3</td><td>18.20% ± 0.24%</td></tr><tr><td>2</td><td>17.04% ± 0.23%</td></tr><tr><td>1</td><td>16.31% ± 0.23%</td></tr></tbody></table><p>So the best performance numbers we have are clearly much better than chance. But in fairness, they’re still not a slam dunk.</p><p>Even when we use approximations for the most linear subspaces we have, I think there is still a lot of noise in the measurement, for all the reasons outlined earlier in this section. Personally, I take the numbers to be overall supportive of the hypothesis, at least directionally, though I wish the evidence was more conclusive.</p><h3 id="final-summary-of-correspondence-between-transformer-and-approximation">Final Summary of Correspondence Between Transformer and Approximation <a href="#final-summary-of-correspondence-between-transformer-and-approximation">🔗</a></h3><p>The analyses in this post point towards two ideas. First, that the approximation and the transformer produce similar outputs. Second, that there is a correspondence between the approximation procedure and what the transformer is doing. I think the evidence for the first idea is quite strong. The evidence for the second is less clear cut, but still suggests it’s probably at least partially right.</p><p>To close, I want to provide a high-level summary of what I think that correspondence is, even if I can’t yet demonstrate it more definitively:</p><table><thead><tr><th>Concept</th><th>Transformer</th><th>Approximation</th></tr></thead><tbody><tr><td>Prompts map to classes of strings in the training corpus.</td><td>The transformer learns an embedding scheme, along with weights in its self-attention and feed-forward networks, that cause strings in the training corpus with similar characteristics to produce similar output values. Prompts that share those characteristics also produce similar output values.</td><td>The approximation performs the same mapping as the transformer by examining the feed-forward network outputs from all substrings in the training corpus and identifying the ones similar to the outputs from a given prompt.</td></tr><tr><td>Predictions for the tokens likely to follow a prompt derive from the frequency distribution of tokens that follow strings in the training corpus that produce feed-forward network output values similar to those of the prompt.</td><td>A feed-forward network output is a compressed, latent representation, in the embedding space, of the frequency distribution of the tokens that follow strings in the training corpus that produce similar outputs. The weights in the final linear layer map the latent representation into logit space such that it become the correct probability distribution after applying the softmax operation.</td><td>The approximation reconstructs the same frequency distribution manually, by looking up the strings identified as having similar outputs in the training corpus and counting the tokens that follow them. Normalizing the frequency distribution turns it into a probability distribution.</td></tr><tr><td>Final output is a weighted sum of predictions from each block.</td><td>As shown <a href="#transformation-via-vector-addition">earlier</a>, the transformer output is roughly the vector sum of all feed-forward network outputs and the input embedding. The learned weights in the layers within a block determine the magnitude and direction of the output and thus how much it influences the overall direction of the final sum.</td><td>The approximation performs a weighted sum of the distributions determined for each block. The weights control the degree of influence of any given block and are manually selected to produce results as close to the transformer’s as possible.</td></tr></tbody></table><h3 id="what-about-attention">What About Attention? <a href="#what-about-attention">🔗</a></h3><p>I began this post by observing that most explanations of how transformers work focus on attention but don’t say how attention results turn into the final predictions. I may be guilty of the opposite: I’ve written at length about how the transformers produce their output probabilities and said very little about attention.</p><p>To wrap up the analysis, I’d like to rectify this with a few words about attention. In the mechanism I’ve laid out, whether executed in the form of the approximation or the transformer, a key operation is mapping the prompt to a class of strings from the training corpus at each block. Predictions for the next token follow directly from the distribution of tokens that follow those strings in the training corpus. <strong>Making good predictions depends on mapping the prompt to the right class of training corpus strings. And that is the job of self-attention.</strong></p><p>The self-attention layers learn to identify patterns across the tokens that make up a prompt. Those patterns might be simple, such as a common sequence appearing at the beginning or end of the prompt (for example, as we saw <a href="#demo-my-proposal-in-action">earlier</a>, strings that end in ‘y l’). They can also be more general: instead of matching specific tokens, they might match <em>kinds</em> of tokens, such as vowels or capitals, in specific places. The learned weights in the attention heads determine which patterns they respond to, and thus which strings in the training corpus produce similar values. The output of the self-attention heads, when passed through the feed-forward network, yield representations in embedding space that encode information about the distribution of tokens in the training corpus that follow those strings.</p><p>Because the transformer has multiple blocks and each block has multiple attention heads (6 blocks and 6 heads per block in the one we looked at), it’s possible to evaluate each prompt against a large number of different potential patterns. The richness and diversity of the patterns that the attention heads can identify gives the transformer its predictive power.</p><h2 id="closing-thoughts">Closing Thoughts <a href="#closing-thoughts">🔗</a></h2><p>I started this project because I wanted to understand the transformer architecture. It’s given me a satisfying explanation of what at least one transformer is doing, but has been even more fruitful as an exercise in learning how to learn. This was my first foray into an open-ended ML research project on my own. It taught me how to interrogate the internals of models, how to set up experiments to answer questions, and, perhaps most importantly, how to keep moving the project forward when I felt stuck.</p><p>Language models have always seemed magical to me, from the first time I used ChatGPT. I wondered if finding a reductive explanation for what happens internally would rob them of their magic. In fact, I think the opposite has happened. I’ve come to appreciate the beauty in an elegantly simple mechanism that produces such rich complexity in its outputs.</p><p>I don’t know whether the results I found here have any generality beyond the small transformer I trained or if any of it will be of use to anyone else. Regardless, it’s been a joy to do this work and I’m grateful to have had the things I needed along the way: time, resources, and endless support from my family and mentors.</p><h2 id="appendices">Appendices <a href="#appendices">🔗</a></h2><h3 id="i-model-details">I: Model Details <a href="#i-model-details">🔗</a></h3><p>Some notable specs:</p><ul><li>Vocabulary size: 65 (the unique characters in the TinyShakespeare dataset)</li><li>Embedding size (<code>n_embed</code>): 384</li><li>Number of transformer blocks (<code>n_layer</code>): 6</li><li>Number of attention heads (<code>n_head</code>): 6</li><li>Context window size (<code>block_size</code>): 256</li></ul><p>The feed-forward networks comprise over 65% of the total trainable parameters:</p><div><pre tabindex="0"><code data-lang="python"><span><span>all_trainable_params <span>=</span> [p <span>for</span> p <span>in</span> m<span>.</span>parameters() <span>if</span> p<span>.</span>requires_grad]
</span></span><span><span>n_all_trainable_params <span>=</span> sum([np<span>.</span>prod(p<span>.</span>size()) <span>for</span> p <span>in</span> all_trainable_params])
</span></span><span><span>
</span></span><span><span>ffwd_trainable_params <span>=</span> [
</span></span><span><span>    p
</span></span><span><span>    <span>for</span> block_idx <span>in</span> range(n_layer)
</span></span><span><span>    <span>for</span> p <span>in</span> m<span>.</span>blocks[block_idx]<span>.</span>ffwd<span>.</span>parameters()
</span></span><span><span>    <span>if</span> p<span>.</span>requires_grad
</span></span><span><span>]
</span></span><span><span>n_ffwd_trainable_params <span>=</span> sum([np<span>.</span>prod(p<span>.</span>size()) <span>for</span> p <span>in</span> ffwd_trainable_params])
</span></span><span><span>
</span></span><span><span>print(
</span></span><span><span>    <span>f</span><span>"</span><span>{</span>n_ffwd_trainable_params<span>:</span><span>,</span><span>}</span><span> ffwd params out of </span><span>{</span>n_all_trainable_params<span>:</span><span>,</span><span>}</span><span> total params (</span><span>{</span>n_ffwd_trainable_params <span>/</span> n_all_trainable_params<span>:</span><span>.2%</span><span>}</span><span>)"</span>
</span></span><span><span>)
</span></span></code></pre></div><pre tabindex="0"><code>7,089,408 ffwd params out of 10,788,929 total params (65.71%)
</code></pre><h3 id="ii-evaluation-of-main-model-vs-3-alternate-models">II: Evaluation of Main Model vs 3 Alternate Models <a href="#ii-evaluation-of-main-model-vs-3-alternate-models">🔗</a></h3><p>As described in the <a href="#evaluating-the-approximation">Evaluation section</a>, I trained the same transformer architecture used for the main model in this post three additional times, starting from a different random seed each time. This appendix shows the code used to measure the average Hellinger distance between the output of the main models and each of the three alternates, across the 20,000 sample prompts. The results provide a plausible lower bound for the threshold Hellinger distance that indicates a meaningful change in an output probability distribution.</p><p>First, we instantiate the three alternate models from their saved weights:</p><div><pre tabindex="0"><code data-lang="python"><span><span>alt_models_dir <span>=</span> environment<span>.</span>data_root <span>/</span> <span>'alternate-models/model-training/20240112-training/outputs/'</span>
</span></span><span><span><span>assert</span> alt_models_dir<span>.</span>exists(), <span>"Alternate models directory does not exist. Run the training code in ../experiments/alternate-models.ipynb."</span>
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="python"><span><span><span># Instantiate the three alternative trained models</span>
</span></span><span><span>m_alt1, _ <span>=</span> create_model_and_tokenizer(
</span></span><span><span>    saved_model_filename<span>=</span>alt_models_dir <span>/</span> <span>'shakespeare-20240112-1.pt'</span>,
</span></span><span><span>    dataset<span>=</span>ts,
</span></span><span><span>    device<span>=</span>device,
</span></span><span><span>)
</span></span><span><span>m_alt2, _ <span>=</span> create_model_and_tokenizer(
</span></span><span><span>    saved_model_filename<span>=</span>alt_models_dir <span>/</span> <span>'shakespeare-20240112-2.pt'</span>,
</span></span><span><span>    dataset<span>=</span>ts,
</span></span><span><span>    device<span>=</span>device,
</span></span><span><span>)
</span></span><span><span>m_alt3, _ <span>=</span> create_model_and_tokenizer(
</span></span><span><span>    saved_model_filename<span>=</span>alt_models_dir <span>/</span> <span>'shakespeare-20240112-3.pt'</span>,
</span></span><span><span>    dataset<span>=</span>ts,
</span></span><span><span>    device<span>=</span>device,
</span></span><span><span>)
</span></span></code></pre></div><p>Next, we feed the input tokens into the original model and the three alternate models and get their output probability distributions:</p><div><pre tabindex="0"><code data-lang="python"><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_strings(prompts)
</span></span><span><span>
</span></span><span><span>model_probs <span>=</span> get_model_probs(m, tokens)
</span></span><span><span>alt_model_probs1 <span>=</span> get_model_probs(m_alt1, tokens)
</span></span><span><span>alt_model_probs2 <span>=</span> get_model_probs(m_alt2, tokens)
</span></span><span><span>alt_model_probs3 <span>=</span> get_model_probs(m_alt3, tokens)
</span></span></code></pre></div><p>Now we can compute the Hellinger distance between the outputs from the three alternative models and the outputs from the original model. Remember that each of the model probabilities tensors (<code>model_probs</code> and <code>alt_model_probs*</code>) is a 20,000x65 tensor i.e. 20,000 probability distributions of 65 elements each.</p><p>We’re computing the Hellinger distance between those probability distributions. So for each alternative model, we end up with 20,000 Hellinger distance values. These values tell us, for each prompt, how the probability distribution for the next token predicted by one of the alternate models differed from the probability distribution predicted by the original model.</p><div><pre tabindex="0"><code data-lang="python"><span><span>h_alt1 <span>=</span> hellinger_distance(model_probs, alt_model_probs1)
</span></span><span><span>h_alt2 <span>=</span> hellinger_distance(model_probs, alt_model_probs2)
</span></span><span><span>h_alt3 <span>=</span> hellinger_distance(model_probs, alt_model_probs3)
</span></span></code></pre></div><p>With the Hellinger distances computed, we can look at aggregate stats:</p><div><pre tabindex="0"><code data-lang="python"><span><span>h_alts <span>=</span> torch<span>.</span>stack([h_alt1, h_alt2, h_alt3], dim<span>=</span><span>1</span>)
</span></span><span><span>h_alts<span>.</span>mean(dim<span>=</span><span>0</span>), h_alts<span>.</span>std(dim<span>=</span><span>0</span>), h_alts<span>.</span>min(dim<span>=</span><span>0</span>)<span>.</span>values, h_alts<span>.</span>max(dim<span>=</span><span>0</span>)<span>.</span>values
</span></span></code></pre></div><pre tabindex="0"><code>(tensor([0.1064, 0.1057, 0.1053]),
 tensor([0.0823, 0.0817, 0.0828]),
 tensor([0.0005, 0.0008, 0.0008]),
 tensor([0.8351, 0.7881, 0.8743]))
</code></pre><p>For all three alternate models, the average Hellinger distance was ~0.11 ± 0.08. All had very small minimums (&lt;= 0.0008) and maximums around ~0.80.</p><h3 id="iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">III: The Output Embedding’s Norm Doesn’t Matter Because of the Final LayerNorm <a href="#iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">🔗</a></h3><p>This appendix demonstrates the assertion from the <a href="#transformation-via-vector-addition">Transformation via Vector Addition</a> that the norm of the output embeddings from the final transformer block does not matter, because of the LayerNorm before the final linear layer.</p><p>To begin, let’s grab the final block outputs for the first 1000 prompts:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Get the block outputs for the first 1000 prompts</span>
</span></span><span><span>
</span></span><span><span>tokens_sample <span>=</span> encoding_helpers<span>.</span>tokenize_strings(prompts[:<span>1000</span>])
</span></span><span><span>_, io_accessors_sample <span>=</span> accessors<span>.</span>run_model(accessors<span>.</span>embed_tokens(tokens_sample))
</span></span><span><span>
</span></span><span><span>final_block_outputs <span>=</span> io_accessors_sample[<span>-</span><span>1</span>]<span>.</span>output(<span>'.'</span>)[:, <span>-</span><span>1</span>, :]<span>.</span>clone()
</span></span><span><span>
</span></span><span><span><span>del</span> io_accessors_sample
</span></span><span><span>_ <span>=</span> gc<span>.</span>collect()
</span></span><span><span>
</span></span><span><span>final_block_outputs<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([1000, 384])
</code></pre><p>Next, let’s create a copy of those outputs scaled by a factor of 10:</p><div><pre tabindex="0"><code data-lang="python"><span><span>scaled_final_block_outputs <span>=</span> final_block_outputs <span>*</span> <span>10</span>
</span></span><span><span>scaled_final_block_outputs<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([1000, 384])
</code></pre><p>Comparing average norms, we see that those of the scaled outputs indeed are 10 times bigger:</p><div><pre tabindex="0"><code data-lang="python"><span><span>final_block_outputs<span>.</span>norm(dim<span>=-</span><span>1</span>)<span>.</span>mean(), scaled_final_block_outputs<span>.</span>norm(dim<span>=-</span><span>1</span>)<span>.</span>mean()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(22.8909), tensor(228.9091))
</code></pre><p>Now, let’s put both the original and scaled outputs through the final LayerNorm of the model and calculate the average norm of the results:</p><div><pre tabindex="0"><code data-lang="python"><span><span>layer_normed_original <span>=</span> m<span>.</span>ln_f(final_block_outputs)<span>.</span>detach()
</span></span><span><span>layer_normed_scaled <span>=</span> m<span>.</span>ln_f(scaled_final_block_outputs)<span>.</span>detach()
</span></span><span><span>
</span></span><span><span>layer_normed_original<span>.</span>norm(dim<span>=-</span><span>1</span>)<span>.</span>mean(), layer_normed_scaled<span>.</span>norm(dim<span>=-</span><span>1</span>)<span>.</span>mean()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.1262), tensor(23.1263))
</code></pre><p>They’re virtually identical.</p><p>In the preceding example, the output norms were so close to identical because the two inputs differed only in scale: they had the same direction, or cosine similarity of 1. Vectors that have different norms and different directions will emerge from the LayerNorm with norms that are still quite similar but a little further apart.</p><p>To see an example, we can add a little noise to one of the vectors and then scale it:</p><div><pre tabindex="0"><code data-lang="python"><span><span>original_vector <span>=</span> final_block_outputs[<span>0</span>]
</span></span><span><span>
</span></span><span><span><span># Add some random noise to the original vector</span>
</span></span><span><span>torch<span>.</span>manual_seed(<span>42</span>) <span># keep the noise consistent</span>
</span></span><span><span>comparison_vector <span>=</span> original_vector <span>+</span> torch<span>.</span>randn_like(original_vector) <span>*</span> <span>0.1</span>
</span></span><span><span>
</span></span><span><span><span># And scale it</span>
</span></span><span><span>comparison_vector <span>=</span> comparison_vector <span>/</span> comparison_vector<span>.</span>norm()
</span></span><span><span>comparison_vector <span>*=</span> <span>10</span> <span>*</span> original_vector<span>.</span>norm()
</span></span><span><span>
</span></span><span><span>original_vector<span>.</span>norm(), comparison_vector<span>.</span>norm(), F<span>.</span>cosine_similarity(original_vector, comparison_vector, dim<span>=-</span><span>1</span>)
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.7909), tensor(237.9092), tensor(0.9967))
</code></pre><p>The <code>comparison_vector</code>’s norm is exactly 10x that of <code>original_vector</code>, but they’re not perfectly aligned in direction, though still quite close.</p><div><pre tabindex="0"><code data-lang="python"><span><span>m<span>.</span>ln_f(original_vector)<span>.</span>detach()<span>.</span>norm(), m<span>.</span>ln_f(comparison_vector)<span>.</span>detach()<span>.</span>norm()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.1496), tensor(23.1671))
</code></pre><p>Their norms after layer norm are close but further apart than in the previous example.</p><p>If we add a lot more noise, we’ll end up with two vectors with quite different directions:</p><div><pre tabindex="0"><code data-lang="python"><span><span>original_vector <span>=</span> final_block_outputs[<span>0</span>]
</span></span><span><span>
</span></span><span><span><span># Add some random noise to the original vector</span>
</span></span><span><span>torch<span>.</span>manual_seed(<span>4211</span>) <span># keep the noise consistent</span>
</span></span><span><span>comparison_vector <span>=</span> original_vector <span>+</span> torch<span>.</span>randn_like(original_vector) <span>*</span> <span>2</span>
</span></span><span><span>
</span></span><span><span><span># And scale it</span>
</span></span><span><span>comparison_vector <span>=</span> comparison_vector <span>/</span> comparison_vector<span>.</span>norm()
</span></span><span><span>comparison_vector <span>*=</span> <span>10</span> <span>*</span> original_vector<span>.</span>norm()
</span></span><span><span>
</span></span><span><span>original_vector<span>.</span>norm(), comparison_vector<span>.</span>norm(), F<span>.</span>cosine_similarity(original_vector, comparison_vector, dim<span>=-</span><span>1</span>)
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.7909), tensor(237.9093), tensor(0.5178))
</code></pre><p>But their norms after layer norm are only a little more divergent:</p><div><pre tabindex="0"><code data-lang="python"><span><span>m<span>.</span>ln_f(original_vector)<span>.</span>detach()<span>.</span>norm(), m<span>.</span>ln_f(comparison_vector)<span>.</span>detach()<span>.</span>norm()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.1496), tensor(23.0546))
</code></pre><p>So in summary:</p><ul><li>The LayerNorm will remove substantial differences in input norms.</li><li>Norms of the outputs from the LayerNorm will vary a little depending on how closely aligned the input vectors were.</li></ul><h3 id="iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs">IV: Summary of Experiment on Relative Impact of Self-Attention and Feed Forward Network Outputs <a href="#iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs">🔗</a></h3><p>The <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb" target="_blank" rel="noopener">embedding adjustments analysis notebook</a> contains the implementation of an experiment to understand the relative impact of the self-attention outputs and the feed-forward network outputs on the final output of the transformer. This appendix summarizes the experiment and the results.</p><p>Experiment procedure:</p><ul><li>I ran all 20,000 prompts through the model and captured the final output probability distributions as well as the intermediate self-attention outputs, feed-forward network outputs, and final block outputs for each block.</li><li>For each block, I then ran two tests. First, instead of sending the block output as normally implemented (<code>x + sa_out + ffwd_out</code>, as shown <a href="#block-logic-with-intermediates">earlier</a>) to the next stage of the model, I sent a version that omits the self-attention output i.e. just <code>x + ffwd_out</code>, and saved the final output probability distribution that resulted. Then, I did the same thing but removed the feed-forward network output instead, sending on just <code>x + sa_out</code>.</li><li>I then calculated the Hellinger distance between the probability distribution produced with the regular block output and that produced by each of the two modifications.</li></ul><p>The table below shows the results, averaged across all 20,000 prompts:</p><blockquote><p>For the implementation of this analysis, see <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb" target="_blank" rel="noopener">embedding adjustments analysis notebook</a></p></blockquote><table><thead><tr><th>Block</th><th>H(output(<code>x+sa_out+ffwd_out</code>), output(<code>x+ffwd_out</code>))</th><th>H(output(<code>x+sa_out+ffwd_out</code>), output(<code>x+sa_out</code>))</th></tr></thead><tbody><tr><td>1</td><td>0.11 ± 0.07</td><td>0.70 ± 0.17</td></tr><tr><td>2</td><td>0.07 ± 0.04</td><td>0.19 ± 0.11</td></tr><tr><td>3</td><td>0.09 ± 0.07</td><td>0.15 ± 0.10</td></tr><tr><td>4</td><td>0.06 ± 0.05</td><td>0.13 ± 0.10</td></tr><tr><td>5</td><td>0.04 ± 0.03</td><td>0.14 ± 0.10</td></tr><tr><td>6</td><td>0.03 ± 0.03</td><td>0.17 ± 0.10</td></tr></tbody></table><blockquote><p>Remember that Hellinger distance ranges between 0 and 1, with 0 meaning identical and 1 meaning no overlap. A larger Hellinger distance in this table means a larger divergence between the experiment output and the normal transformer output.</p></blockquote><p>The effect is most pronounced in the first block: omitting the feed-forward network output results in an almost completely different probability distribution (H = 0.70) but omitting the self-attention output results in a very similar distribution (H = 0.11). Across the rest of the layers, the difference is less dramatic, but the feed-forward network output always has the larger impact.</p><blockquote><p>Though I think these results support the notion that an approximation based only on feed-forward network outputs can produce similar results to the transformer, it would be interesting to see if the approximation would improve if we include the self-attention outputs, particularly for some of the intermediate layers. But I’m leaving that as an area for future investigation.</p></blockquote><h3 id="v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace">V: Performing SVD to Get a Linear Approximation of a Token Subspace <a href="#v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace">🔗</a></h3><p>This appendix walks through an example of how we can find a linear approximation for a token subspace using SVD. First, let’s load all the embeddings learned for the token <code>a</code> at the output of the last block of the transformer (input to the final layer norm and linear layer):</p><div><pre tabindex="0"><code data-lang="python"><span><span>learned_embeddings_dir <span>=</span> environment<span>.</span>data_root <span>/</span> <span>'learned_embeddings'</span>
</span></span><span><span>multi_emb_a <span>=</span> torch<span>.</span>load(learned_embeddings_dir <span>/</span> <span>'no_blocks'</span> <span>/</span> <span>'lower_a.pt'</span>, map_location<span>=</span>device)
</span></span><span><span>multi_emb_a<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([100, 1, 384])
</code></pre><p>We’ve got 100 different 384-dimensional embedding vectors. Each one, when given as input to the final blocks in the transformer, produces an output distribution that assigns nearly all the probability mass to the token, <code>a</code>. Each one can be thought of as a point in the subspace for token <code>a</code>.</p><p>We can stack these embeddings form a 100x384 matrix:</p><p>$$
\begin{bmatrix}
e_{1,1} &amp; e_{1,2} &amp; \dots &amp; e_{1,384} \\
e_{2,1} &amp; e_{2,2} &amp; \dots &amp; e_{2,384} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
e_{100,1} &amp; e_{100,2} &amp; \dots &amp; e_{100,384}
\end{bmatrix}
$$</p><p>Next, we can run SVD on this matrix:</p><div><pre tabindex="0"><code data-lang="python"><span><span>_, S, V <span>=</span> torch<span>.</span>linalg<span>.</span>svd(multi_emb_a[:, <span>-</span><span>1</span>, :])
</span></span></code></pre></div><p>For this analysis, we’re only interested in the singular values (<code>S</code>) and the right singular vectors (<code>V</code>). We can plot the singular values:</p><div><pre tabindex="0"><code data-lang="python"><span><span>_ <span>=</span> plt<span>.</span>plot(S<span>.</span>numpy(), <span>'-o'</span>)
</span></span></code></pre></div><p><img src="https://shyam.blog/posts/beyond-self-attention/images/ec5755d7859f77e55c3bf34a432c33226741616e9b493b5eec96c716ac1e7fe5.png" alt=""></p><p>The first singular value is much larger (over 6x) than the next, which suggests the first right singular vector alone might be a good approximation for the subspace that predicts token <code>a</code>. We can test what gets predicted when we use this first right singular vector as an embedding:</p><div><pre tabindex="0"><code data-lang="python"><span><span>v0a <span>=</span> adjust_singular_vector_sign(V[<span>0</span>], multi_emb_a[:, <span>-</span><span>1</span>, :])
</span></span><span><span>logits <span>=</span> LogitsWrapper(accessors<span>.</span>logits_from_embedding(unsqueeze_emb(v0a)), tokenizer)
</span></span><span><span>logits<span>.</span>plot_probs(title<span>=</span><span>'Next Token Probability Distribution from First Right Singular Vector of embeddings for Token "a"'</span>)
</span></span></code></pre></div><p><img src="https://shyam.blog/posts/beyond-self-attention/images/968e8e424522937d5366586abd902715e1cdf771fddbb2d4a36144cbe07746e2.png" alt=""></p><p>In this distribution, <code>a</code> has probability near 1 and every other token has probability near 0. So the first right singular vector is effectively another embedding that produces an output predicting <code>a</code> with near certainty.</p><p>But it’s different from the other 100 learned embeddings in an important way: it’s the vector that is best aligned with <em>all</em> of them. More formally, it’s the vector that minimizes the squared distance to all 100 other embedding vectors. In this way, the first right singular vector is like a good summary of the embedding vectors we started from.</p><p>The first right singular vector is a unit vector (as are all the singular vectors):</p><pre tabindex="0"><code>tensor(1.0000)
</code></pre><p>Any vector along its span will produce an output distribution predicting <code>a</code>, similar to the one above (see <a href="#iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">Appendix III</a> for an explanation of why the transformer output is invariant to the scale of the final embedding). So the span of this vector is a good, linear approximation to the subspace for token <code>a</code>.</p><p>The same results we saw here for token <code>a</code> hold for the other tokens too. For each, if we stack all the learned embeddings and perform SVD, we find that the first right singular vector forms a good linear approximation of the token subspace.</p></div></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Browser extensions are underrated: the promise of hackable software (339 pts)]]></title>
            <link>https://www.geoffreylitt.com/2019/07/29/browser-extensions</link>
            <guid>39251095</guid>
            <pubDate>Sun, 04 Feb 2024 15:43:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.geoffreylitt.com/2019/07/29/browser-extensions">https://www.geoffreylitt.com/2019/07/29/browser-extensions</a>, See on <a href="https://news.ycombinator.com/item?id=39251095">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure>
  <img src="https://www.geoffreylitt.com/images/article_images/legos.jpg?1705878965" alt="Lego bricks">
  <figcaption>Photo by <a href="https://unsplash.com/photos/2FaCKyEEtis">Rick Mason on Unsplash</a></figcaption>
</figure>

<p>Recent conversations about web browser extensions have focused on controversy: <a href="https://arstechnica.com/information-technology/2019/07/dataspii-inside-the-debacle-that-dished-private-data-from-apple-tesla-blue-origin-and-4m-people/">malicious browser extensions capturing web history</a>, and <a href="https://www.wired.com/story/google-chrome-ad-blockers-extensions-api/?verso=true">Google limiting the capabilities used by ad blockers</a>. These are important discussions, but we shouldn’t lose sight of the big picture: browser extensions are a special ecosystem worth celebrating.</p>

<p>Among major software platforms today, <strong>browser extensions are the rare exception that allow and encourage users to modify the apps that we use</strong>, in creative ways not intended by their original developers. On smartphone and desktop platforms, this sort of behavior ranges from unusual to impossible, but in the browser it’s an everyday activity.</p>

<p>Browser extensions remind us what it’s like to have deep control over how we use our computers.</p>

<h2 id="assembling-our-own-software">Assembling our own software</h2>

<p>Once a software platform reaches a certain level of openness, it can fundamentally change the way that normal users relate to their software. By installing four different Gmail extensions that modify everything from the visual design to the core functionality, in some sense, I’ve put together my own email client. <strong>Instead of being a passive user of pre-built applications, I can start assembling my own personalized way of using my computer.</strong></p>

<p>The popularity of browser extensions proves that many people are interested in customizing their software, and it’s not just  a hobby for power users. There are over 180,000 extensions on the Chrome store, and nearly half of all Chrome users have browser extensions installed.<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup> When people have an easy way to extend their software with useful functionality, they apparently take advantage.</p>

<h2 id="hackable-platforms-not-custom-apis">Hackable platforms, not custom APIs</h2>

<p>Browser extensions have remarkably broad use cases. I personally use Chrome extensions that fill in my passwords, help me read Japanese kanji, simplify the visual design of Gmail, let me highlight and annotate articles, save articles for later reading, play videos at 2x speed, and of course, block ads.</p>

<p><strong>The key to this breadth is that most extensions modify applications in ways that the original developers didn’t specifically plan for.</strong> When Japanese newspapers publish articles, they’re not thinking about compatibility with the kanji reading extension. Extension authors gain creative freedom because they don’t need to use application-specific APIs that reflect the original developers’ view of how people might want to extend their application.</p>

<p>The web platform has a few qualities that enable this sort of unplanned extensibility. The foundational one is that the classic web deployment style is to ship all the client code to the browser in human-readable form. (Source maps are a key to preserving this advantage as we ship more code that’s minified or compiled from other languages.) The web’s layout model also promotes extensibility by encouraging standardized semantic markup—my password manager extension works because web pages reliably use form tags for password submissions instead of building their own version.</p>

<p>Even with these advantages, it can still require clever tricks to modify a site in ways that it wasn’t built for. But it’s often a reasonable amount of work, not a years-long reverse engineering effort. The sheer variety of extensions available shows that extension authors are willing to jump through a few hoops to create useful software.</p>

<p>Occasionally there are tensions between website developers and extension authors, but it seems far more common that developers are fine with their sites being extended in creative ways, as long as they don’t have to do any extra work. Extensions can even make life easier for application developers: if there’s a niche request that a small minority of users want, a motivated community member can just build an extension to support it. By building on a hackable platform, developers allow their users to get even more value out of their applications.</p>



<p>Many browser extensions are generic tools designed to enhance my use of all websites. I can use my annotation extension on every website everywhere, instead of needing a different highlighting tool for each article I read. Just like using a physical highlighter with paper articles, I can master the tool once, and get a lot of leverage by applying it in different contexts.</p>

<p>In many software platforms, we think of the operating system as providing the cross-cutting tools, and third parties as providing standalone “apps” that are used in isolation. <strong>With browser extensions, third parties are also adding tools;</strong> a single piece of software has the leverage to change my experience across all the apps I use.</p>

<p>When software is built in small units, it also changes the economics. Most extensions I use are free, and are perhaps too small in their feature set to support a full-blown business. And yet, people still choose to make them, and I benefit immensely from these little bits of software. Browsing the extension store feels more like going to a local flea market than going to a supermarket. Massive software built by huge companies isn’t the only way.</p>

<h2 id="the-origins-of-openness">The origins of openness</h2>

<p>It’s not an accident that this openness emerged on the web platform.</p>

<p>Since the beginning of personal computing, there’s been a philosophical tradition that encourages using computers as an interactive medium where people contribute their own ideas and build their own tools—authorship over consumption. This idea is reflected in systems like Smalltalk, Hypercard, and more recently, <a href="https://dynamicland.org/">Dynamicland</a>.</p>

<p>When Tim Berners-Lee created the World Wide Web, he imagined it fitting into this tradition. “My vision was a system in which sharing what you knew or thought should be as easy as learning what someone else knew.”<sup id="fnref2"><a href="#fn2" rel="footnote">2</a></sup> There were some hiccups along the way<sup id="fnref3"><a href="#fn3" rel="footnote">3</a></sup>, but eventually that vision largely won out, and the Web became a place where anyone can publish their opinions or photos through social media platforms.</p>

<p>Still, there’s a catch. When you’re using Facebook, you’re operating within a confined experience. You’re forced to publish in a certain format, and to use their app in a certain way (that includes, of course, seeing all the ads). There’s more room for authorship than just browsing a news website, but only within the strict lines the app has painted for you.</p>

<p><strong>Browser extensions offer a deeper type of control.</strong> Instead of merely typing into the provided text box, we can color outside the lines and deeply modify the way we use any application on the web. Browser extensions offer a kind of decentralization: large companies building major websites don’t get to dictate all the details of our experience.</p>

<h2 id="improving-on-extensions">Improving on extensions</h2>

<p>We clearly need to work on protecting people from malicious extensions that invade their privacy. But beyond that, here are some bigger picture opportunities I see for improving on extensions:</p>

<p><strong>Accessibility:</strong> Today, it requires a big jump to go from using browser extensions to creating them: you need to learn a fair amount of web development to get started, and you can’t easily develop extensions in the browser itself. What if there were a quick way to get started developing and sharing extensions in the browser? You could imagine smoothly transitioning from editing a website in the developer tools to publishing a small extension.</p>

<p><em>Update</em>: I’ve started working on a system called <a href="https://sdg.csail.mit.edu/projects/wildcard">Wildcard</a> to work towards this vision.</p>

<p><strong>Compatibility:</strong> Because extensions hook into websites in unsupported ways, updates to websites often result in extensions temporarily breaking, and extension authors scrambling to fix them. Can we make it easier for website developers and extension authors to form stable connections between their software, without necessarily resorting to using explicit extension APIs?</p>

<p>There are existing practices that fit into this category already—for example, using clean semantic markup, human-readable CSS, and source maps makes it easier to develop an extension.</p>

<p>A simple change that would allow for more stable extensions would be to give users more control over when they upgrade to new versions of cloud software. If I have a 3 month window to continue using an old version after the new one is released, that would give extension authors more time to upgrade their software for the new version.</p>

<p><strong>Power:</strong> Web extensions are limited in their power by the typical architecture of web applications: they have broad rights to modify the browser client, but the server is off limits. For example, if my social media app’s server only provides an endpoint for querying my posts in chronological order, no browser extension can ever search through all my posts by keyword. How could we rethink the client-server boundary to enable extensions to make even deeper modifications?</p>

<p>This raises tough questions around security and privacy. The modern browser extension API has done a good job balancing extensibility with security, and yet we’re still grappling with the consequences of browser extensions invading people’s privacy. Giving extensions more power would raise the stakes further. Still, we shouldn’t give up in the name of security—we should fight for extensibility as a value and find ways to balance these interests.</p>

<h2 id="the-next-platform">The next platform</h2>

<p>I’m intrigued by a couple projects that are rethinking the web in ways that might make it more extensible:</p>

<p>The <a href="https://beakerbrowser.com/about/">Beaker Browser</a> and the decentralized web community are exploring how the web works without centralized servers. It seems like their proposed architecture would give users fuller control over modifying the “server” side of web applications.</p>

<p>Tim Berners-Lee is working on a new project called <a href="https://inrupt.com/blog/one-small-step-for-the-web">SOLID</a>. I don’t yet understand precisely what they’re up to, but given Tim’s involvement I figure it’s worth paying attention. A key principle is giving users more ownership over their data, which would enable people to use extensions and other software to manipulate their data in flexible ways beyond what application server APIs allow.</p>

<p>Computing is still young, and platforms are changing quickly. Modern browser extensions and smartphone platforms have only been around for about a decade. These platforms will evolve, and there will be new platforms after them, and we will get to collectively decide how open they will be.</p>

<p>Browser extensions give us one example to strive for: a place where we routinely hack the software we use and make it our own. <span>▪</span></p>

<p><a href="https://news.ycombinator.com/item?id=20556382"><em>Discuss on Hacker News</em>
</a></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rolling Airframe Missile (129 pts)]]></title>
            <link>https://www.navalgazing.net/RAM</link>
            <guid>39250896</guid>
            <pubDate>Sun, 04 Feb 2024 15:24:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.navalgazing.net/RAM">https://www.navalgazing.net/RAM</a>, See on <a href="https://news.ycombinator.com/item?id=39250896">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>I've previously discussed <a href="https://www.navalgazing.net/Standard-Part-2">Standard</a>, <a href="https://www.navalgazing.net/Sea-Sparrow">Sea Sparrow</a>, <a href="https://www.navalgazing.net/ESSM">ESSM</a> and <a href="https://www.navalgazing.net/Phalanx">Phalanx</a>, but there is one last air-defense weapon that deserves discussion.  This is the <a href="https://en.wikipedia.org/wiki/RIM-116_Rolling_Airframe_Missile" rel="nofollow">RIM-116 Rolling Airframe Missile</a>, better known as RAM.
</p>
<p><span> <img width="600" src="https://www.navalgazing.net/attach/RAMLaunchGreenBay.jpg?v=1706810557.jpg" alt=""><br><span>A RAM is launched from USS <em>Green Bay</em>  </span></span></p>
<p>Much like the other point-defense systems, RAM's origins trace back to <em><a href="https://www.navalgazing.net/Eilat">Eilat</a></em>, and the panic that it provoked within the USN.  It was conceived to work in pretty much the same niche as Phalanx, providing a last-ditch defense against incoming anti-ship missiles.  Effective as it was, Phalanx had a serious limitation, even while it was still in development.  The use of a gun limited effective range to no more than 1500 yards, which was a serious problem in the face of supersonic missiles.  The available window to engage such a weapon was short, and even if the Phalanx did shoot it down, the debris was likely to strike the defended ship.  The obvious solution was to use a missile, which could engage at significantly longer range.
<a name="break" id="break"></a>
</p>
<p>The initial program that led to RAM was based on the <a href="https://en.wikipedia.org/wiki/FIM-43_Redeye" rel="nofollow">Redeye missile</a>, the first American man-portable SAM system, although it would be fitted with a combined radar/IR seeker to allow it to engage closing targets (which were difficult to engage purely with IR seekers at the time) at reasonable range.  The only real concern was the small size of the missile, 2.75" in diameter and about 18 lb, and Congress directed the Navy to study something the size of Sidewinder, 5" and about 160 lb, instead.  The initial contract was signed with General Dynamics in 1976, with West Germany coming onboard as a development partner.  Development didn't go particularly smoothly, with delays from testing and cost adding up to around 5 years, and both the US and Germany came close to withdrawing from the program at various points.  But things were eventually worked out, and RAM entered in the early 90s.
</p>
<p><span> <img width="600" src="https://www.navalgazing.net/attach/RAMMissileFlightGHWB.jpg?v=1706810558.jpg" alt=""><br><span>A RAM in flight</span></span></p>
<p>RAM is a rather unusual missile.  It gets its name because it is fired from a rifled tube and rolls in flight thanks to the tube and four fins.  The roll allows it to use only two control fins in flight, instead of the usual four.  The basic airframe, motor, fuze and warhead initially came from the <a href="https://en.wikipedia.org/wiki/AIM-9_Sidewinder" rel="nofollow">Sidewinder</a>, while the IR seeker was derived from the <a href="https://en.wikipedia.org/wiki/FIM-92_Stinger" rel="nofollow">Stinger</a>.  Because RAM was expected to be fired at incoming targets, where the hot engine would not be visible, the seeker would need to rely on glint (reflected IR radiation from the sun), which sharply limited range. As a result, initial guidance would be provided by a passive RF system, which could home in on the radar seeker of a typical cruise missile.<a id="fnr1_1" href="#fn1_1"><sup>1</sup></a>  The rolling missile could also get away with a 2-sensor radar inferometer, instead of requiring four sensors, like a more conventional missile.  The accuracy of the RF seeker was limited, hence the inclusion of the IR seeker, but it was in theory possible for the missile to use it all the way to the target if it's dark or cloudy and the IR seeker doesn't work.  The wide cone of the RF sensor also allows the missile to be fired "around the corner", reducing the size of sectors blocked by the ship's structure by 10-15°.
</p>
<p><span> <img width="600" src="https://www.navalgazing.net/attach/SailorLoadingRAMTruman.jpg?v=1706810695.jpg" alt=""><br><span>Sailors load a RAM launcher aboard the <em>Truman</em></span></span></p>
<p>But there were serious concerns about the effectiveness of RAM against missiles that used IR or semi-active homing, so even before the Block 0 missile entered service, work began on Block 1, with an imaging IR seeker that is capable of searching for targets on its own.  It still has the RF seeker, and is capable of using the original dual-sensor mode, homing entirely on IR or switching to IR search if it loses RF track.  Block 1 entered service in 1999, and achieved a 95% success rate in intercepting incoming missiles across 180 trials.  A further upgrade took place in 2002, to give better performance against helicopters, slow aircraft and surface targets.  This upgrade, known as HAS, was implemented entirely as a software upgrade.  In the mid-2000s, a second upgrade, Block 2, was started.  It was a considerably bigger overhaul than Block 1, with a new 6.25" motor<a id="fnr1_2" href="#fn1_2"><sup>2</sup></a> (which increased range significantly, from 6 nm to 10 nm), a 4-fin steering system, and an improved RF seeker.  Block 2 was cleared for service in 2015, and is currently in production.
</p>
<p><span> <img width="600" src="https://www.navalgazing.net/attach/JMSDF_DDH-183_SeaRamIzumo.jpg?v=1706810829.jpg" alt=""><br><span>SeaRAM onboard Japanese helicopter destroyer <em>Izumo</em></span></span></p>
<p>The fire-and-forget nature of RAM meant that it imposed relatively minimal burdens on the firing ship's combat system, particularly given the ability of the missile to search for targets after launch.  The combat system still mattered in terms of firing at the correct time and in the right direction, but it opened up new possibilities for smaller ships that couldn't support Sea Sparrow or the like.  The most extreme version of this was SeaRAM, which was essentially a <a href="https://www.navalgazing.net/Phalanx">Phalanx</a> system with the gun removed and replaced by an 11-round RAM launcher.  Like Phalanx, it is capable of operating independently of the ship that carries it, automatically detecting and engaging incoming missiles.  SeaRAM is primarily carried by the <a href="https://www.navalgazing.net/LCS-Part-1"><em>Independence</em> class LCS</a>, although a few <a href="https://www.navalgazing.net/The-Arleigh-Burke-Class"><em>Burke</em>s</a> are also fitted with the system to provide some defense against cruise missiles when the main radar is in ballistic missile defense mode.
</p>
<p><span> <img height="440" src="https://www.navalgazing.net/attach/RIM-116_Rolling_Airframe_Missile_Launcher_Ozelot.jpg?v=1706810828.jpg" alt=""><br><span>A RAM launcher on the German missile boat <em>Ozelot</em></span></span></p>
<p>But the more common launcher is the 21-round Mk 49, a trainable launcher integrated with the ship's combat system and fitted to a number of ships, including all American aircraft carriers and amphibious ships, as well as the <em>Freedom</em> class LCS.  Germany has equipped all of its warships with RAM, while Egypt, Greece, Japan, Mexico, Qatar, South Korea, Saudi Arabia, Turkey and the UAE have bought the system for their ships as well.  If anything, RAM seems destined for wider service in the years to come.  It is the most minimal system capable of providing protection against anti-ship missiles, a threat that has been graphically demonstrated in the Red Sea in recent months, and which is only likely to continue to proliferate.
</p>
<hr>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Plastic bans work. Billions of plastic bags were avoided in the US alone (202 pts)]]></title>
            <link>https://www.zmescience.com/science/news-science/plastic-bans-work-billions-of-plastic-bags-were-avoided-in-the-us-alone/</link>
            <guid>39250434</guid>
            <pubDate>Sun, 04 Feb 2024 14:28:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.zmescience.com/science/news-science/plastic-bans-work-billions-of-plastic-bags-were-avoided-in-the-us-alone/">https://www.zmescience.com/science/news-science/plastic-bans-work-billions-of-plastic-bags-were-avoided-in-the-us-alone/</a>, See on <a href="https://news.ycombinator.com/item?id=39250434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
								
<p>“The bottom line is that <a href="https://www.zmescience.com/ecology/environmental-issues/congo-bans-plastic-bags-321313/">plastic bag bans</a> work,” said Faye Park, president of the U.S. PIRG Education Fund, in a statement. “People realize quickly it’s easy to live without plastic bags and get used to bringing a bag from home or skipping a bag when they can.”</p>



<figure><a href="https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-scaled.jpg"><picture><source srcset="https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1024x683.webp 1024w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1536x1024.jpg 1536w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-2048x1365.jpg 2048w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-750x500.jpg 750w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1140x760.jpg 1140w" sizes="(max-width: 1024px) 100vw, 1024px" type="image/webp"><img src="https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1024x683.jpg" height="683" width="1024" srcset="https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1024x683.jpg 1024w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1536x1024.jpg 1536w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-2048x1365.jpg 2048w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-750x500.jpg 750w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1140x760.jpg 1140w" sizes="(max-width: 1024px) 100vw, 1024px" alt="plastic bag waste" fetchpriority="high" decoding="async"> </picture></a><figcaption>Image via Unsplash.</figcaption></figure>



<h2 id="plastic-ain-t-all-that-fantastic">Plastic ain’t all that fantastic</h2>



<p>Plastic bags are a victim of their own success. When they <a href="https://www.unep.org/news-and-stories/story/birth-ban-history-plastic-shopping-bag">were first patented in Europe in 1965</a>, society was shocked to see how cheap and durable they could be. Within a decade or two they became mainstream on the continent and in North America, and it wasn’t long before they started being widely used on the entire planet.</p>



<p>But plastics were just a little too durable. They didn’t go away. They started accumulating in landfills and in the oceans. The environmental impact of plastic bags gained attention with the discovery of the Great <a href="https://www.zmescience.com/ecology/pollution-ecology/great-pacific-garbage-patch-06102016/">Pacific Garbage Patch</a> in 1997. Plastic bags (and plastic in general) had left its mark on the planet in an unprecedented form of pollution.</p><!-- Tag ID: zmescience_300x250_InContent -->





<p>Fast forward a couple more decades, and countries started fighting their urge to use <a href="https://www.zmescience.com/research/materials/cheap-fabrics-from-plastic-15032021/">cheap plastics</a> and implement bans or other measures against plastic bags — and finally, there’s some good news.</p>



<p>San Francisco pioneered the movement in the U.S. by passing the <a href="https://www.zmescience.com/ecology/environmental-issues/bottled-water-ban-national-park-43423/">nation’s first plastic bag ban</a> in 2007. Several other U.S. cities and <a href="https://www.zmescience.com/ecology/hawaii-bans-plastic-bags-04062012/">states implemented plastic bag bans</a> or restrictions. By 2023, ten states had statewide bans, with similar laws proposed in others​​. To get a state of how much this of a difference this made, five studied bans resulted in an average elimination of <a href="https://www.zmescience.com/ecology/plastic-bag-tax-uk-22112016/">almost 300 plastic bags</a> per person per year​​. Overall, in the US alone, <a href="https://www.zmescience.com/research/tea-plastic-particles-ocean-234523521/">billions of plastic bags</a> were avoided with anti-plastic bag measures.</p>




<h2 id="h-the-case-against-plastic">The case against plastic</h2>



<p>The case against plastic bags is straightforward.  Plastic pollution kills at least <a href="https://wwf.org.au/blogs/plastic-in-our-oceans-is-killing-marine-mammals/">100,000 marine mammals</a> and <a href="https://sustainabledevelopment.un.org/content/documents/Ocean_Factsheet_Pollution.pdf">1 million seabirds</a> every year and entanglement in plastic and other types of litter kills roughly 1,000 turtles per year. Plastic bags aren’t responsible for all of that, but they make up an important part of the problem.</p>







<blockquote>

</blockquote>



<p>The results, which were published in a report, also highlight that imperfect measures leave loopholes or encourage buyers to opt for other single use bags.</p>



<blockquote>
<p>Well-designed <a href="https://www.zmescience.com/science/news-science/europe-single-use-plastic-24102018/">single-use plastic bag bans</a> across the country have successfully reduced single-use plastic bag consumption, cut down on plastic bag litter and driven consumers to make more sustainable bag choices. Policymakers should pursue these policies at the state and local levels,” the report says.</p>
</blockquote>



<p>The idea isn’t to shift from one type of single-use bag to another type of single-use bag. Paper bags are easier to recycle than plastic, but they take 3-4 times more energy to produce and usually generate more solid waste.</p><!-- Tag ID: zmescience_300x250_InContent_3 -->




<p>Ultimately, the report concludes that regulation is the best current way to address plastic waste and plastic pollution.</p>



<blockquote>
<p>“Grocery stores, restaurants and retail shops should not be permitted to distribute plastic film bags of any thickness at checkout. Stores should be required to charge a fee of at least 10 cents for single-use paper bags. A 10-cent paper bag fee will limit the expected increase in paper bag use after a bag ban is imposed and may even reduce paper bag consumption altogether.”</p>



<p>“Local and state governments should conduct regular enforcement to ensure compliance.”</p>
</blockquote>



<p>You can read the <a href="https://environmentamerica.org/center/resources/plastic-bag-bans-work/">report in its entirety here</a>.</p>

<!-- AI CONTENT END 1 -->
								
								
																	
																	
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Write code for the web - Apple doesn't care about you, Mr. Developer (402 pts)]]></title>
            <link>https://mrmr.io/apple/</link>
            <guid>39250406</guid>
            <pubDate>Sun, 04 Feb 2024 14:24:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mrmr.io/apple/">https://mrmr.io/apple/</a>, See on <a href="https://news.ycombinator.com/item?id=39250406">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tabindex="-1" id="___gatsby"><main><div><h3>Write code for the web</h3><p>This is a yarn of three threads, and it got a bit long. The tldr is</p>
<ol>
<li>
<p>Apple doesn’t care for me as a developer</p>
</li>
<li>
<p>I should write code for the web</p>
</li>
<li>
<p>Nothing is set in stone, really</p>
</li>
</ol>
<p>The story is personal, but I hope readers find something they can relate to in
their own life stream (I guess that's the point of blogs)</p>
<h3>Apple doesn't care for you, Mr. Developer</h3>
<p>Apple cares for me as a customer, but it doesn’t care for me as a developer.
This dynamic had been subconsciously griefing me for years, until I was able to
formulate it consciously recently.</p>
<p>The dependency goes <em>Developer -&gt; Apple</em> and <em>Apple -&gt; Consumer</em>, there is no
reverse arrow from Apple to the developers.</p>
<p>If all developers stopped building for Apple's platforms tomorrow, Apple will
still survive, almost intact, since it doesn’t hurt their core value prop (I'll
expand on this below). Apple has no dependency on individual developers. They
care for and need to collaborate with corporate dev “partners”, but that's
different.</p>
<p>Since companies, especially ginormous multinationals, behave in purely game
theoretic cost/benefit terms, and since Apple has no reason to care about
developers: indeed it doesn’t.</p>
<blockquote>
<p>Just to restate the obvious - such a stance isn't necessarily the case. There
are other multinationals who've figured that developers form a core part of
their strategy.</p>
</blockquote>
<p>This realisation has made me happier since I now know my place. I can like their
products without wanting to develop for them.</p>
<hr>
<p>Google has a bug. I have dynamic light/dark mode, and if I search on Google at
night, the first page shows up in light mode. With the rest of my machine in a
subdued state, the glaring white of the background hurts my eyes.</p>
<p>In the morning, my laptop automatically switches back to light mode. Now when I
search on Google, the results first show up in an unreadable black background.</p>
<p>One would think that of all the leetcode certified staff, there must be someone
there who would know an O(n) algorithm for fixing this bug. But no, this bug has
persisted for years (I've counted), and it is unlikely to be fixed in the future
unless it gets accidentally fixed as part of some overhaul.</p>
<p>Apologies for the snide, I know some great people who work there. My point is
that Google isn't fixing this bug not because it doesn’t know how to, but
because it doesn’t care. This bug has zero impact on its bottom line.</p>
<p>The people like me who use alternate search engines like DDG have already moved
on years ago. The rest of the (overwhelming) majority is stuck with Google. No
matter how bad their search is - UX or results - they have a captive audience.</p>
<blockquote>
<p>I think DDG etc also have a marketing blind spot - they keep pitching
themselves as a more privacy friendly alternative, they never go after the
main course. I don’t use DDG because of its privacy benefits, I use it because
it reminds me of early Google - simple low clutter UX, good quality verbatim
search results, and unobtrusive ads.</p>
</blockquote>
<p>Okay Manav, but I thought you were ranting about Apple, why bring in Google?</p>
<p>Google is an example where I never grieved much because I understood the
dynamics. I know that I, the customer, am not their game theoretic target. So
when I have to invariably use their products and face another user hostile
interaction, I try to shrug it off and move on. I know that Google has entered a
rent seeking phase, and while it is sad that the world is giving all its video
content up to it for even more of a hostage situation in the future, but that’s
for governments to deal with.</p>
<p>With Apple I didn’t understand the dynamics.</p>
<hr>
<p>2009-ish. I struggled to find a computer for my mom. Windows (at that time, I
don't know about now) was just too insecure. Linux required constant tech
support. Eventually I prepared for her an OpenBSD machine running Firefox and a
basic game (Bubbles I think).</p>
<p>Obviously, this was not ideal. It fulfilled some goals - it worked without
requiring any tech support when I wasn't around, and I was ensuring her data
safety and privacy - and she was surprisingly happy with too, but I was not
happy about how this was such a shrivelled parody of what things could be, and
how it limited the ways she could use computers to enrich her life.</p>
<p>Around this time, I joined a new job as a developer for a company that was
making iOS apps. After a week or so of using the mac at work it hit me - <em>this
is the computer I wanted for my mom!</em></p>
<p>That is Apple’s value prop.</p>
<p>As soon as I had saved enough I bought her a MacBook. And heartfeltly thanked
Steve Jobs for engendering it.</p>
<p>The earth has done many a revolutions since then, and Jobs has left earth, and
the form factor my mother uses has changed from a laptop to an iPad. But it
still satisfies that core value prop.</p>
<p>Let's consider a different context. Even if there were no apps in my phone, I
would still buy an iPhone. For myself likely, but most certainly for her.</p>
<p>The people running Apple know all this. In the deep dungeons of Menlo Park when
there are meetings of the core council, after all the sacrificial lambs have
been slain and the blood and gore washed away, out comes the elder spreadsheet
that encodes Apple’s business model, but nowhere in them is any cell, input or
output, which involves developers. Sure, Apple doesn't mind if developers are
also happy. But it knows it doesn’t need to care if they are.</p>
<p>This lack of caring is never expressed out loud. It isn’t some conspiracy, it
just is one of those things - you wouldn’t walk up to someone who you don’t care
about and tell them you don’t care.</p>
<p>Unfortunately all this results in sometimes schizophrenic behaviour on Apple’s
part, as there are many individuals working at Apple who <em>do care</em> about
developers and are trying to make things better.</p>
<hr>
<p>2016-ish. As part of my annual Apple simping I was watching WWDC when they
announced Apple Music APIs.</p>
<p>I was ecstatic. My coworkers were puzzled at my ecstasy, “All this, can’t we
already do with Spotify’s API?”. I didn’t know how to answer that, so I just
repeated how <em>this changes everything</em>.</p>
<p>Of course, and as is usual, I was wrong. It didn’t change everything. In fact,
it didn’t change <em>anything</em>.</p>
<p>Apple’s own music player was, and still is, unusably bad. Even talking about it
makes me angry. The people making it can’t be so incompetent, and it seems to be
working for the rest of the world, so I've never known what to make of this
situation.</p>
<p>With the APIs out, I'd thought maybe things will change. But nothing happened.
Apple’s own player continues to make my blood pressure high anytime I have to
use it. And whatever alternative players I have tried just all seem to go for
the same generic “Spotify” approach to music.</p>
<blockquote>
<p>I think it is because the people who're making these apps were never around in
the Justin Frankel era of Winamp, and haven't seen how a music player can
provide a fast, seamless, endless <em>yet</em> still personal approach to music.</p>
<p>That era is not coming back because it relied on piracy, but luckily we don't
need to anymore - that's the great thing about Apple's music catalog! We can
recreate that experience without needing to sail the high seas.</p>
</blockquote>
<p>So that's the backstory. Now recently I had a bunch of free time, and thought
that I’ll write a music player. Mostly for myself, but I also wanted to write a
tutorial. This is what I wrote in the README in the first commit:</p>
<blockquote>
<p>Here I'll be writing down my notes as I build Flowers. The world needs not one
music player, or two, but many: each of us has our own way of connecting with
music, and so maybe these notes will help others build the flower they want,
nay need.</p>
</blockquote>
<p>Cute, dumb, and in vain.</p>
<p>As I went about it, I realized that 8 years down the line, not only is the API
still buggy, it is also still not public!</p>
<p>Firstly you need to pay Apple. No, not for bulk usage etc, but <em>just to try out
the API</em>. So there there goes my dream of writing a walkthrough – nobody’s going
to pay Apple 100 bucks just to try things out. And it also partially explains
why there has been no innovation.</p>
<p>But that's not even it - Even if you pay them, you get a restricted API.</p>
<p>The point where I disgustedly gave up was when I found out that while I was
jumping all these kafkaesque hoops, instead you could just go to Apple's own web
music player, type <code>MusicKit.getInstance().developerToken</code> in your browser
console, and you’ll get an unrestricted root token for free! </p>
<hr>
<p>All these anecdotes Manav, what does all this mean?</p>

<p>Write code that runs on the web. We’re lucky to have a shared platform that no
single entity owns. Even benevolence can be ruined by incompetence.</p>
<p>The web platform is in a precarious place – overreaching governments, browser
duopolies, a complex developer ecosystem – so it is not a given it’ll remain
thriving. But so far it has survived. And every year longer it survives, the
more the chances that it’ll continue to thrive in the future.</p>
<p>Ironically Google is the good guy here, they’re doing great work for the web. On
the other hand, literally every single workaround I’ve had to write in the
recent past in web related code has been due to Safari's princessness.</p>
<h2>Nothing is set in stone</h2>
<p>Which brings me to the third thread of this story, how all this made me
reevalute my relationship with companies.</p>
<p>Someone I know, someone who has a better grip on living than me, told me once
that it's not useful to put people into the buckets of good and bad. People are
a mix. Bad folks can do good actions sometimes, and vice versa.</p>
<p>I don't know to what extent I've been able to internalize their message, but
that's for another day. What I realized is that the same applies to companies.</p>
<p>Companies are like people. I don't know if they're sentient, but otherwise they
share many attributes with us: they're intelligent (they were the AI before AI),
have personalities, they are born, thrive, live and die, they're even legal
persons.</p>
<p>Just like we can't live without other people, we can't live without companies.
They have many inhumane characteristics, but like them or not, such fractal
conceptions of human organizations will always be around.</p>
<p>By not permanently bucketing companies into good or bad, I can have a more fluid
interaction with them - I can reduce my dependence on them when they try to put
me in a zero sum game, or reengage more with them if they're willing to be more
symbiotic. Nothing is set in stone, really.</p>
<hr>
<blockquote><p>I think most large companies and medium-size companies, and even small
companies, are starting to look at the web as the ultimate direct-to-customer
distribution chain, bypassing all middlemen, going directly from the supplier to
the consumer.</p><p>– Steve Jobs, Make Something Wonderful</p></blockquote></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Inuit Parents Teach Kids To Control Their Anger (2019) (105 pts)]]></title>
            <link>https://www.npr.org/sections/goatsandsoda/2019/03/13/685533353/a-playful-way-to-teach-kids-to-control-their-anger</link>
            <guid>39250304</guid>
            <pubDate>Sun, 04 Feb 2024 14:10:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npr.org/sections/goatsandsoda/2019/03/13/685533353/a-playful-way-to-teach-kids-to-control-their-anger">https://www.npr.org/sections/goatsandsoda/2019/03/13/685533353/a-playful-way-to-teach-kids-to-control-their-anger</a>, See on <a href="https://news.ycombinator.com/item?id=39250304">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storytext">
      <div id="res702589837">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                For more than 30 years, the Inuit welcomed anthropologist Jean Briggs into their lives so she could study how they raise their children. Briggs is pictured during a 1974 visit to Baffin Island.
                <b aria-label="Image credit">
                    
                    Jean Briggs Collection / American Philosophical Society
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Jean Briggs Collection / American Philosophical Society
        
    </span>
</p></div>
   </div>
   <p>Back in the 1960s, a Harvard graduate student made a landmark discovery about the nature of human anger.</p>   <p>At age 34, Jean Briggs traveled above the Arctic Circle and lived out on the tundra for 17 months. There were no roads, no heating systems, no grocery stores. Winter temperatures could easily dip below minus 40 degrees Fahrenheit.</p>   <p>Briggs persuaded an Inuit family to "adopt" her and "try to keep her alive," as the anthropologist <a href="https://link.springer.com/article/10.1007%2FBF02805482">wrote </a>in 1970.</p>   <div id="res701118871">
                  <p>This story is part of a series from NPR's Science desk called <strong><a href="https://www.npr.org/series/688838187/the-other-side-of-anger">The Other Side of Anger.</a></strong> There's no question we are in angry times. It's in our politics, our schools and homes. Anger can be a destructive emotion, but it can also be a positive force.</p>         <p>Join NPR in our exploration of anger and what we can learn from this powerful emotion. <strong><a href="https://www.npr.org/series/688838187/the-other-side-of-anger">Read and listen to stories in the series here. </a></strong></p>
      </div>
   
<!-- END ID="RES701118871" CLASS="BUCKETWRAP LISTTEXT" -->
   <p>At the time, many Inuit families lived similar to the way their ancestors had for thousands of years. They built igloos in the winter and tents in the summer. "And we ate only what the animals provided, such as fish, seal and caribou," says <a href="https://www.eagle-eye.com/Myna-Ishulutak">Myna Ishulutak</a>, a film producer and language teacher who lived a similar lifestyle as a young girl.</p>   <p>Briggs quickly realized something remarkable was going on in these families: The adults had an extraordinary ability to control their anger.</p>   
   <p>"They never acted in anger toward me, although they were angry with me an awful lot," Briggs told the Canadian Broadcasting Corp. in an interview.</p>   <div id="res702588179">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                Myna Ishulutak (upper right, in blue jacket) lived a seminomadic life as a child. Above: photos of the girl and her family in the hunting camp of Qipisa during the summer of 1974.
                <b aria-label="Image credit">
                    
                    Jean Briggs Collection / American Philosophical Society
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Jean Briggs Collection / American Philosophical Society
        
    </span>
</p></div>
   </div>
   <p>Even just showing a smidgen of frustration or irritation was considered weak and childlike, Briggs observed.</p>   <p>For instance, one time someone knocked a boiling pot of tea across the igloo, damaging the ice floor. No one changed their expression. "Too bad," the offender said calmly and went to refill the teapot.</p>   <p>In another instance, a fishing line — which had taken days to braid — immediately broke on the first use. No one flinched in anger. "Sew it together," someone said quietly.</p>   <p>By contrast, Briggs seemed like a wild child, even though she was trying very hard to control her anger. "My ways were so much cruder, less considerate and more impulsive," she told the CBC. "[I was] often impulsive in an antisocial sort of way. I would sulk or I would snap or I would do something that they never did."</p>   <p>Briggs, who died in 2016, wrote up her observations in <a href="http://www.hup.harvard.edu/catalog.php?isbn=9780674608283">her first book</a>, <em>Never in Anger</em>. But she was left with a lingering question: How do Inuit parents instill this ability in their children? How do Inuit take tantrum-prone toddlers and turn them into cool-headed adults?</p>   
   <p>Then in 1971, Briggs found a clue.</p>   
   
<!-- END ID="RES702773512" CLASS="BUCKETWRAP INTERNALLINK MEDIAPROMO PRIMARY" -->
   
   
<!-- END ID="RES702766744" CLASS="BUCKETWRAP INTERNALLINK MEDIAPROMO PRIMARY" -->
   <p>She was walking on a stony beach in the Arctic when she saw a young mother playing with her toddler — a little boy about 2 years old. The mom picked up a pebble and said, "'Hit me! Go on. Hit me harder,'" Briggs remembered.</p>   <p>The boy threw the rock at his mother, and she exclaimed, "Ooooww. That hurts!"</p>   <p>Briggs was completely befuddled. The mom seemed to be teaching the child the opposite of what parents want. And her actions seemed to contradict everything Briggs knew about Inuit culture.</p>   <p>"I thought, 'What is going on here?' " Briggs said in the radio interview.</p>   <p>Turns out, the mom was executing a powerful parenting tool to teach her child how to control his anger — and one of the most intriguing parenting strategies I've come across.</p>   <div id="res702586950">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                Iqaluit, pictured in winter, is the capital of the Canadian territory of Nunavut.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1200.jpg">
        </picture>
    </div>
<div>
        <p>Iqaluit, pictured in winter, is the capital of the Canadian territory of Nunavut.</p>
        <p><span aria-label="Image credit">
            
            Johan Hallberg-Campbell for NPR
            
        </span>
    </p></div>
   </div>
   <h3><strong>No scolding, no timeouts</strong></h3>   <p>It's early December in the Arctic town of Iqaluit, Canada. And at 2 p.m., the sun is already calling it a day. Outside, the temperature is a balmy minus 10 degrees Fahrenheit. A light snow is swirling.</p>   <p>I've come to this seaside town, after reading Briggs' book, in search of parenting wisdom, especially when it comes to teaching children to control their emotions. Right off the plane, I start collecting data.</p>   <p>I sit with elders in their 80s and 90s while they lunch on "country food" —stewed seal, frozen beluga whale and raw caribou. I talk with moms selling hand-sewn sealskin jackets at a high school craft fair. And I attend a parenting class, where day care instructors learn how their ancestors raised small children hundreds — perhaps even thousands — of years ago.</p>   <div id="res702586420">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                The elders of Iqaluit have lunch at the local senior center. On Thursdays, what they call "country food" is on the menu, things like caribou, seal and ptarmigan.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1200.jpg">
        </picture>
    </div>
<div>
        <p>The elders of Iqaluit have lunch at the local senior center. On Thursdays, what they call "country food" is on the menu, things like caribou, seal and ptarmigan.</p>
        <p><span aria-label="Image credit">
            
            Johan Hallberg-Campbell for NPR
            
        </span>
    </p></div>
   </div>
   <p>Across the board, all the moms mention one golden rule: Don't shout or yell at small children.</p>   <p>Traditional Inuit parenting is incredibly nurturing and tender. If you took all the parenting styles around the world and ranked them by their gentleness, the Inuit approach would likely rank near the top.<strong> </strong>(They even have a special kiss for babies, where you put your nose against the cheek and sniff the skin.)</p>   <p>The culture views scolding — or even speaking to children in an angry voice — as inappropriate, says Lisa Ipeelie, a radio producer and mom who grew up with 12 siblings. "When they're little, it doesn't help to raise your voice," she says. "It will just make your own heart rate go up."</p>   
   <p>Even if the child hits you or bites you, there's no raising your voice?</p>   <p>"No," Ipeelie says with a giggle that seems to emphasize how silly my question is. "With little kids, you often think they're pushing your buttons, but that's not what's going on. They're upset about something, and you have to figure out what it is."</p>   <div id="res702585934">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                Traditionally, the women and children in the community eat with an ulu knife.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
   </div>
   <p>Traditionally, the Inuit saw yelling at a small child as demeaning. It's as if the adult is having a tantrum; it's basically stooping to the level of the child, Briggs documented.</p>   <p>Elders I spoke with say intense colonization over the past century is damaging these traditions. And, so, the community is working hard to keep the parenting approach intact.</p>   <p>Goota Jaw is at the front line of this effort. She teaches the parenting class at the <a href="https://www.arcticcollege.ca/programs">Arctic College</a>. Her own parenting style is so gentle that she doesn't even believe in giving a child a timeout for misbehaving.</p>   <p>"Shouting, 'Think about what you just did. Go to your room!' " Jaw says. "I disagree with that. That's not how we teach our children. Instead you are just teaching children to run away."</p>   <p>And you are teaching them to be angry, says clinical psychologist and author Laura Markham. "When we yell at a child — or even threaten with something like 'I'm starting to get angry,' we're training the child to yell," says <a href="https://www.psychologytoday.com/us/experts/laura-markham-phd">Markham</a>. "We're training them to yell when they get upset and that yelling solves problems."</p>   <p>In contrast, parents who control their own anger are helping their children learn to do the same, Markham says. "Kids learn emotional regulation from us."</p>   <p>I asked Markham if the Inuit's no-yelling policy might be their first secret of raising cool-headed kids. "Absolutely," she says.</p>   <h3><strong>Playing soccer with your head</strong></h3>   <p>Now at some level, all moms and dads know they shouldn't yell at kids. But if you don't scold or talk in an angry tone, how do you discipline? How do you keep your 3-year-old from running into the road? Or punching her big brother?</p>   
   <p>For thousands of years, the Inuit have relied on an ancient tool with an ingenious twist: "We use storytelling to discipline," Jaw says.</p>   
   
<!-- END ID="RES703045640" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>Jaw isn't talking about fairy tales, where a child needs to decipher the moral. These are oral stories passed down from one generation of Inuit to the next, designed to sculpt kids' behaviors in the moment.<strong> </strong>Sometimes even save their lives.</p>   <p>For example, how do you teach kids to stay away from the ocean, where they could easily drown? Instead of yelling, "Don't go near the water!" Jaw says Inuit parents take a pre-emptive approach and tell kids a special story about what's inside the water. "It's the sea monster," Jaw says, with a giant pouch on its back just for little kids.</p>   <p>"If a child walks too close to the water, the monster will put you in his pouch, drag you down to the ocean and adopt you out to another family," Jaw says.</p>   <p>"Then we don't need to yell at a child," Jaw says, "because she is already getting the message."</p>   <p>Inuit parents have an array of stories to help children learn respectful behavior, too. For example, to get kids to listen to their parents, there is a story about ear wax, says film producer Myna Ishulutak.</p>   <p>"My parents would check inside our ears, and if there was too much wax in there, it meant we were not listening," she says.</p>   <p>And parents tell their kids: If you don't ask before taking food, long fingers could reach out and grab you, Ishulutak says.</p>   <div id="res702594556">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                Inuit parents tell their children to beware of the northern lights. If you don't wear your hat in the winter, they'll say, the lights will come, take your head and use it as a soccer ball!
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1200.jpg">
        </picture>
    </div>
<div>
        <p>Inuit parents tell their children to beware of the northern lights. If you don't wear your hat in the winter, they'll say, the lights will come, take your head and use it as a soccer ball!</p>
        <p><span aria-label="Image credit">
            
            Johan Hallberg-Campbell for NPR
            
        </span>
    </p></div>
   </div>
   <p>Then there's the story of northern lights, which helps kids learn to keep their hats on in the winter.</p>   <p>"Our parents told us that if we went out without a hat, the northern lights are going to take your head off and use it as a soccer ball," Ishulutak says. "We used to be so scared!" she exclaims and then erupts in laughter.</p>   <p>At first, these stories seemed to me a bit too scary for little children. And my knee-jerk reaction was to dismiss them. But my opinion flipped 180 degrees after I watched my own daughter's response to similar tales — and after I learned more about humanity's intricate relationship with storytelling.</p>   
   <p>Oral storytelling is what's known as a human universal. For tens of thousands of years, it has been a key way that parents teach children about values and how to behave.</p>   <p>Modern hunter-gatherer groups use stories to teach sharing, respect for both genders and conflict avoidance, a recent study <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5717173">reported</a>, after analyzing 89 stories from nine different tribes in Southeast Asia and Africa. With the Agta, a hunter-gatherer population of the Philippines, good storytelling skills are prized more than hunting skills or medicinal knowledge, the study found.</p>   <p>Today many American parents outsource their oral storytelling to screens. And in doing so, I wonder if we're missing out on an easy — and effective — way of disciplining and changing behavior. Could small children be somehow "wired" to learn through stories?</p>   <div id="res702585548">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                Inuit parenting is gentle and tender. They even have a special kiss for kids called <em>kunik</em>. (Above) Maata Jaw gives her daughter the nose-to-cheek Inuit sniff.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
   </div>
   <p>"Well, I'd say kids learn well through narrative and explanations," says psychologist <a href="http://starlabkids.org/deena_weisberg/">Deena Weisberg</a> at Villanova University, who studies how small children interpret fiction. "We learn best through things that are interesting to us. And stories, by their nature, can have lots of things in them that are much more interesting in a way that bare<strong> </strong>statements don't."</p>   <p>Stories with a dash of danger pull in kids like magnets, Weisberg says. And they turn a tension-ridden activity like disciplining into a playful interaction that's — dare, I say it — fun.</p>   <p>"Don't discount the playfulness of storytelling," Weisberg says. "With stories, kids get to see stuff happen that doesn't really happen in real life. Kids think that's fun. Adults think it's fun, too."</p>   <h3><strong>Why don't you hit me?</strong></h3>   <div id="res702583935">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                Inuit filmmaker and language teacher Myna Ishulutak as a little girl. Anthropologist Jean Briggs spent six months with the family in the 1970s documenting the child's upbringing.
                <b aria-label="Image credit">
                    
                    Jean Briggs Collection / American Philosophical Society
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Jean Briggs Collection / American Philosophical Society
        
    </span>
</p></div>
   </div>
   <p>Back up in Iqaluit, Myna Ishulutak is reminiscing about her childhood out on the land. She and her family lived in a hunting camp with about 60 other people. When she was a teenager, her family settled in a town.</p>   <p>"I miss living on the land so much," she says as we eat a dinner of baked Arctic char. "We lived in a sod house. And when we woke up in the morning, everything would be frozen until we lit the oil lamp."</p>   
   <p>I ask her if she's familiar with the work of Jean Briggs. Her answer leaves me speechless.</p>   <p>Ishulutak reaches into her purse and brings out Briggs' second book, <em>Inuit Morality Play, </em>which details the life of a 3-year-old girl dubbed Chubby Maata.</p>   <p>"This book is about me and my family," Ishulutak says. "I am Chubby Maata."</p>   <p>In the early 1970s, when Ishulutak was about 3 years old, her family welcomed Briggs into their home for six months and allowed her to study the intimate details of their child's day-to-day life.</p>   <div id="res702582321">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                Myna Ishulutak today in Iqaluit, Canada. As the mother of two grown boys, she says, "When you're shouting at them all the time they tend to kind of block you. So there's a saying: 'Never shout at them.' "
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
   </div>
   <p>What Briggs documented is a central component to raising cool-headed kids.</p>   <p>When a child in the camp acted in anger — hit someone or had a tantrum — there was no punishment. Instead, the parents waited for the child to calm down and then, in a peaceful moment, did something that Shakespeare would understand all too well: They put on a drama. (As the Bard once wrote, "the play's the thing wherein I'll catch the conscience of the king.")</p>   <p>"The idea is to give the child experiences that will lead the child to develop rational thinking," Briggs told the CBC in 2011.</p>   <p>In a nutshell, the parent would act out what happened when the child misbehaved, including the real-life consequences of that behavior.</p>   <p>The parent always had a playful, fun tone. And typically the performance starts with a question, tempting the child to misbehave.</p>   <p>For example, if the child is hitting others, the mom may start a drama by asking: "Why don't you hit me?"</p>   <p>Then the child has to think: "What should I do?" If the child takes the bait and hits the mom, she doesn't scold or yell but instead acts out the consequences. "Ow, that hurts!" she might exclaim.</p>   <p>The mom continues to emphasize the consequences by asking a follow-up question. For example: "Don't you like me?" or "Are you a baby?" She is getting across the idea that hitting hurts people's feelings, and "big girls" wouldn't hit. But, again, all questions are asked with a hint of playfulness.</p>   
   <p>The parent repeats the drama from time to time until the child stops hitting the mom during the dramas and the misbehavior ends.</p>   <p>Ishulutak says these dramas teach children not to be provoked easily. "They teach you to be strong emotionally," she says, "to not take everything so seriously or to be scared of teasing."</p>   <p>Psychologist <a href="http://www.psychology.illinois.edu/people/pjm">Peggy Miller</a>, at the University of Illinois, agrees: "When you're little, you learn that people will provoke you, and these dramas teach you to think and maintain some equilibrium."</p>   <p>In other words, the dramas offer kids a chance to <em>practice </em>controlling their anger, Miller says, during times when they're not actually angry.</p>   <p>This practice is likely critical for children learning to control their anger. Because here's the thing about anger: Once someone is already angry, it is not easy for that person to squelch it — even for adults.</p>   <p>"When you try to control or change your emotions in the moment, that's a really hard thing to do," says <a href="https://lisafeldmanbarrett.com/">Lisa Feldman Barrett</a>, a psychologist at Northeastern University who studies how emotions work.</p>   <p>But if you <em>practice </em>having a different response or a different emotion at times when you're not angry, you'll have a better chance of managing your anger in those hot-button moments, Feldman Barrett says.</p>   <p>"That practice is essentially helping to rewire your brain to be able to make a different emotion [besides anger] much more easily," she says.</p>   <p>This emotional practice may be even more important for children, says psychologist Markham, because kids' brains are still developing the circuitry needed for self-control.</p>   <p>"Children have all kinds of big emotions," she says. "They don't have much prefrontal cortex yet. So what we do in responding to our child's emotions shapes their brain."</p>   <div id="res702582120">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                A lot has changed in the Arctic since the Canadian government forced Inuit families to settle in towns. But the community is trying to preserve traditional parenting practices.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1200.jpg">
        </picture>
    </div>
<div>
        <p>A lot has changed in the Arctic since the Canadian government forced Inuit families to settle in towns. But the community is trying to preserve traditional parenting practices.</p>
        <p><span aria-label="Image credit">
            
            Johan Hallberg-Campbell for NPR
            
        </span>
    </p></div>
   </div>
   <p>Markham recommends an approach close to that used by Inuit parents. When the kid misbehaves, she suggests, wait until everyone is calm. Then in a peaceful moment, go over what happened with the child. You can simply tell them the story about what occurred or use two stuffed animals to act it out.</p>   <p>"Those approaches develop self-control," Markham says.</p>   <p>Just be sure you do two things when you replay the misbehavior, she says. First, keep the child involved by asking many questions. For example, if the child has a hitting problem, you might stop midway through the puppet show and ask,"Bobby, wants to hit right now. Should he?"</p>   
   <p>Second, be sure to keep it fun. Many parents overlook play as a tool for discipline, Markham says. But fantasy play offers oodles of opportunities to teach children proper behavior.</p>   <p>"Play is their work," Markham says. "That's how they learn about the world and about their experiences."</p>   <p>Which seems to be something the Inuit have known for hundreds, perhaps even, thousands of years.</p>   <div id="res702581804">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                Inuit parents value the playful side of kids even when disciplining them. Above: Maata Jaw and daughter.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1200.jpg">
        </picture>
    </div>
<div>
        <p>Inuit parents value the playful side of kids even when disciplining them. Above: Maata Jaw and daughter.</p>
        <p><span aria-label="Image credit">
            
            Johan Hallberg-Campbell for NPR
            
        </span>
    </p></div>
   </div>
   <h3><strong>Share Your Tips</strong></h3>   <p><em>How do you get your kids to do things without yelling or shouting? Or, how did your parents get you to do things without yelling or scolding? Share your advice, tips and stories, and we may include them in a story for NPR.</em></p>   <p><strong>This submission form is now closed. </strong></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New Linux glibc flaw lets attackers get root on major distros (134 pts)]]></title>
            <link>https://www.bleepingcomputer.com/news/security/new-linux-glibc-flaw-lets-attackers-get-root-on-major-distros/</link>
            <guid>39250076</guid>
            <pubDate>Sun, 04 Feb 2024 13:35:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bleepingcomputer.com/news/security/new-linux-glibc-flaw-lets-attackers-get-root-on-major-distros/">https://www.bleepingcomputer.com/news/security/new-linux-glibc-flaw-lets-attackers-get-root-on-major-distros/</a>, See on <a href="https://news.ycombinator.com/item?id=39250076">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="Linux" height="900" src="https://www.bleepstatic.com/content/hl-images/2023/06/22/Linux.jpg" width="1600"></p>
<p>​Unprivileged attackers can get root access on multiple major Linux distributions in default configurations by exploiting a newly disclosed local privilege escalation (LPE) vulnerability in the GNU C Library (glibc).</p>
<p>Tracked as <a href="https://www.qualys.com/2024/01/30/cve-2023-6246/syslog.txt" target="_blank" rel="nofollow noopener">CVE-2023-6246</a>, this security flaw was found in glibc's __vsyslog_internal() function, called by the widely-used syslog and vsyslog functions for writing messages to the system message logger.</p>
<p>The bug is due to a <a href="https://cwe.mitre.org/data/definitions/122.html" target="_blank" rel="nofollow noopener">heap-based buffer overflow weakness</a> accidentally introduced in glibc 2.37 in August 2022 and later backported to glibc 2.36 when addressing a less severe vulnerability tracked as CVE-2022-39046.</p>
<p>"The buffer overflow issue poses a significant threat as it could allow local privilege escalation, enabling an unprivileged user to gain full root access through crafted inputs to applications that employ these logging functions," Qualys security researchers said.</p>
<p>"Although the vulnerability requires specific conditions to be exploited (such as an unusually long argv[0] or openlog() ident argument), its impact is significant due to the widespread use of the affected library."</p>
<h2>Impacts Debian, Ubuntu, and Fedora systems</h2>
<p>While testing their findings, Qualys confirmed that Debian 12 and 13, Ubuntu 23.04 and 23.10, and Fedora 37 to 39 were all vulnerable to CVE-2023-6246 exploits, allowing any unprivileged user to escalate privileges to full root access on default installations.</p>
<p>Although their tests were limited to a handful of distros, the researchers added that "other distributions are probably also exploitable."</p>
<p>While analyzing glibc for other potential security issues, the researchers also found three other vulnerabilities, two of them—harder to exploit—in the __vsyslog_internal() function (CVE-2023-6779 and CVE-2023-6780) and a third one (a <a href="https://www.qualys.com/2024/01/30/qsort.txt" target="_blank" rel="nofollow noopener">memory corruption issue</a> still waiting for a CVEID) in glibc's qsort () function.</p>
<p>"The recent discovery of these vulnerabilities is not just a technical concern but a matter of widespread security implications,"&nbsp;<a href="https://blog.qualys.com/vulnerabilities-threat-research/2024/01/30/qualys-tru-discovers-important-vulnerabilities-in-gnu-c-librarys-syslog" target="_blank" rel="nofollow noopener">said</a> Saeed Abbasi, Product Manager at Qualys' Threat Research Unit.</p>
<p>"These flaws highlight the critical need for strict security measures in software development, especially for core libraries widely used across many systems and applications."</p>
<h2>Other Linux root escalation flaws found by Qualys</h2>
<p>Over the past few years, researchers at Qualys have found several other Linux security vulnerabilities that can let attackers gain complete control over unpatched Linux systems, even in default configurations.</p>
<p>Vulnerabilities they discovered include a flaw in glibc's ld.so dynamic loader (<a href="https://www.bleepingcomputer.com/news/security/new-looney-tunables-linux-bug-gives-root-on-major-distros/" target="_blank">Looney Tunables</a>), one in Polkit's pkexec component (<a href="https://www.bleepingcomputer.com/news/security/linux-system-service-bug-gives-root-on-all-major-distros-exploit-released/" target="_blank">dubbed PwnKit</a>), another in the Kernel's filesystem layer (<a href="https://www.bleepingcomputer.com/news/security/new-linux-kernel-bug-lets-you-get-root-on-most-modern-distros/" target="_blank">dubbed Sequoia</a>), and in the Sudo Unix program (aka <a href="https://www.bleepingcomputer.com/news/security/new-linux-sudo-flaw-lets-local-users-gain-root-privileges/" target="_blank">Baron Samedit</a>).</p>
<p>Days after the Looney Tunables flaw (<a href="https://access.redhat.com/security/cve/cve-2023-4911" target="_blank" rel="nofollow noopener">CVE-2023-4911</a>) was disclosed, proof-of-concept (PoC) exploits were <a href="https://www.bleepingcomputer.com/news/security/exploits-released-for-linux-flaw-giving-root-on-major-distros/" target="_blank">published online</a>, and threat actors <a href="https://www.bleepingcomputer.com/news/security/hackers-exploit-looney-tunables-linux-bug-steal-cloud-creds/" target="_blank">started exploiting it</a> one month later to steal cloud service provider (CSP) credentials in Kinsing malware attacks.</p>
<p>The Kinsing gang is known for deploying cryptocurrency mining malware on compromised cloud-based systems, including Kubernetes, Docker APIs, Redis, and Jenkins servers.</p>
<p>CISA later <a href="https://www.bleepingcomputer.com/news/security/cisa-orders-federal-agencies-to-patch-looney-tunables-linux-bug/" target="_blank">ordered U.S. federal agencies</a> to secure their Linux systems against CVE-2023-4911 attacks after adding it to its catalog of actively exploited bugs and tagging it as posing "significant risks to the federal enterprise."</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rye: A Vision Continued (125 pts)]]></title>
            <link>https://lucumr.pocoo.org/2024/2/4/rye-a-vision/</link>
            <guid>39249005</guid>
            <pubDate>Sun, 04 Feb 2024 10:15:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lucumr.pocoo.org/2024/2/4/rye-a-vision/">https://lucumr.pocoo.org/2024/2/4/rye-a-vision/</a>, See on <a href="https://news.ycombinator.com/item?id=39249005">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
  

  
  <p>written on Sunday, February 4, 2024
  

  </p><p>In April of last year I released <a href="https://rye-up.com/">Rye</a> to the public.
Rye, both then and now, represents my very personal vision of what an improved
Python packaging and project management solution can look like.
Essentially, it's a comprehensive user experience, designed so that the
only tool a Python programmer would need to interface with is Rye itself
and it gets you from zero to one in a minute.  It is capable of
bootstrapping Python by automatically downloading different Python
versions, it creates virtualenvs, it manages dependencies, and lints and
formats.  Initially developed for my own use, I decided to release it to
the public, and the feedback has been overwhelmingly positive.</p>
<p>When I introduced it, I initiated a discussion thread titled <a href="https://github.com/mitsuhiko/rye/discussions/6">“Should Rye
Exist”</a> referencing the
well known <a href="https://xkcd.com/927/">XKCD #929</a> which humorously comments
on the proliferation of competing standards.  I did not feel well throwing
yet another Python packaging tool into the ring.</p>
<p>Yet it exists now and has user.  This standard issue however I think is
helped a bit by the fact that Rye doesn't actually do any of these things
itself.  It wraps established tools:</p>
<ul>
<li><strong>Downloading Python</strong>: it provides an automated way to get access to
the amazing <a href="https://github.com/indygreg/python-build-standalone/">Indygreg Python Builds</a>
as well as the PyPy binary distributions.</li>
<li><strong>Linting and Formatting</strong>: it bundles <a href="https://github.com/astral-sh/ruff">ruff</a>
and makes it available with <cite>rye lint</cite> and <cite>rye fmt</cite>.</li>
<li><strong>Managing Virtualenvs</strong>: it uses the well established <a href="https://virtualenv.pypa.io/en/latest/">virtualenv</a> library under the hood.</li>
<li><strong>Building Wheels</strong>: it delegates that work largely to <a href="https://pypi.org/project/build/">build</a>.</li>
<li><strong>Publishing</strong>: its publish command uses <a href="https://pypi.org/project/twine/">twine</a> to accomplish this task.</li>
<li><strong>Locking and Dependency Installation:</strong> is today implemented by
using <a href="https://pypi.org/project/unearth/">unearth</a> and
<a href="https://github.com/jazzband/pip-tools/">pip-tools</a>.</li>
</ul>
<p>As you can see, Rye is not revolutionary and it's not intended to be.  Rye
itself doesn't do all that much as it delegates all the core functionality
to other tools in the ecosystem.  Rye packages these tools together in a
user-friendly manner, significantly reducing the cognitive load for
developers.  This convenience eliminates the need to learn about various
tools, read extensive documentation, and integrate these components
independently.  Rye lets you get from no Python on a computer to a fully
functioning Python project in under a minute with linting, formatting and
everything in place.  It is sufficiently opinionated that many important
decisions are made for you.  For instance it starts you out with using
<cite>pyproject.toml</cite> and picks a wheel build system for you.  It also picks
the linter and formatter, and the preferred Python distribution and
decides on a build tool.</p>
<div id="defaults-matter">
<h2>Defaults Matter</h2>
<p>Rye is designed to select the best tools for the job — it picks winners.
Why does it do that?  This approach is inspired by my admiration for the
developer experience in the Rust ecosystem, particularly the seamless
integration of <cite>rustup</cite> and <cite>cargo</cite>.  Their functionality made me long for
a similar experience within the Python community.  Crucially the way this
works in the Rust world does not mean that <cite>cargo</cite> does everything.  When
you run <cite>cargo build</cite> it invokes <cite>rustc</cite>, when you run <cite>cargo doc</cite> it runs
<cite>rustdoc</cite>.  When you invoke <cite>cargo clippy</cite> it runs <cite>clippy</cite> for you and so
worth.  Cargo is a manager that delegates the important work to bespoke
tools that are improved by sometimes entirely different teams.  This also
means that tools can be swapped out if they are found to be not the right
choice any more.  The experience in the Rust world also showed me that
excellent Windows support is just a must have.  That's why Rye is not just
a great experience on macOS and Linux, it's also excellent on Windows.</p>
<p>I am convinced that the Python community is deserving of an excellent
developer experience, and Rye, as it stands today, offers a promising
beginning.  My belief is supported by evidence gathered from conducting
in-person user interviews and demos, where Rye was well received.  In
fact, every individual who I was able to give a guided tour of Rye was
impressed by how swiftly one could start working with Python.  Because it
was demonstrably designed to avoid interference with any pre-existing
Python configurations, Rye allows for a smooth and gradual integration and
the emotional barrier of picking it up even for people who use other tools
was shown to be low.</p>
<p>That said, Rye is a one person project and it does not address the
fundamental challenges of some of the issues we have in the Python
ecosystem.  It does not solve multi version dependencies, it does not
offer better performance for the installation of dependencies.  It does
not help with distributing executables for end user applications or
anything like this.  However I am getting multiple signals that the time
is right for a tool like Rye to not just exist, but also to rally a larger
number of the Python community embrace some of these standardization
ideas.</p>
</div>
<div id="what-s-next">
<h2>What's Next?</h2>
<p><a href="https://github.com/Kwpolska">Chris Warrick</a> recently <a href="https://chriswarrick.com/blog/2024/01/15/python-packaging-one-year-later/">wrote a blog post</a>
where he looked back at the last year of Python packaging that made the
rounds on Twitter.  It laments a bit that we did not make much of a
progress in packaging and it also talks a bit about Rye and correctly
points out that Rye does not have enough contributors (basically just me).
That's not a healthy setup.</p>
<p>I still don't really know if Rye <em>should</em> exist.  It has not yet become
established and there are plenty of rough edges.  I personally really
enjoy using it but at the same time every time I use it, I get reminded
that it would stop existing if I did not invest time into it which in some
sense is what keeps me going on it.</p>
<p>However I would love to see the community converge to a Rye like solution,
no matter where it comes from.</p>
</div>
<div id="learn-more">
<h2>Learn More</h2>
<p>Did I spark your interest?  I would really appreciate it if you give it a
try and give feedback:</p>
<p><em>a 16 minute introduction to Rye</em>
    <iframe width="782" height="441" src="https://www.youtube.com/embed/q99TYA7LnuA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p><ul>
<li><a href="https://rye-up.com/">Project Website</a></li>
<li><a href="https://rye-up.com/guide/">User Guide and Documentation</a></li>
<li><a href="https://github.com/mitsuhiko/rye">GitHub Project</a></li>
<li><a href="https://github.com/mitsuhiko/rye/discussions">Discussion Forums</a></li>
<li><a href="https://discord.gg/drbkcdtSbg">Discord</a></li>
</ul>
</div>


  
  <p>This entry was tagged
    
      <a href="https://lucumr.pocoo.org/tags/announcement/">announcement</a> and 
      <a href="https://lucumr.pocoo.org/tags/python/">python</a>
  

      </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You are what you love (109 pts)]]></title>
            <link>https://gspanos.tech/posts/facts-1/</link>
            <guid>39248931</guid>
            <pubDate>Sun, 04 Feb 2024 09:57:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gspanos.tech/posts/facts-1/">https://gspanos.tech/posts/facts-1/</a>, See on <a href="https://news.ycombinator.com/item?id=39248931">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>  <blockquote>
<p>Facts is a series of articles where I express my foldable opinions. I know, right?</p>
</blockquote>
<h2 id="working-with-emotion">Working with Emotion</h2>
<p>I find it really challenging to talk with people who have completely separated their work from their emotional being. For me, work is part of life and it should be a meaningful one.</p>
<p>I recently had a talk with a close colleague that had to do with working with emotion. Their thesis was that there is no place for emotional thinking and analysis when working. They insisted on saying that the analytical brain should be almost exclusively responsible for how we behave in a work environment.</p>
<p>Not only do I find this contradicting everything that my work ethic consists of, but I also find this mantra unsustainable.</p>
<p>Firstly, let’s make clear that it’s unlikely that you’re good at something you don’t love. If you managed to do that, you’ve probably dedicated the hours to something that does not fulfill you. The goal is not happiness. It’s about fulfillment. After all, you have to do what you love. You <em>are</em> that, how can you be doing anything else?</p>
<p>This belief steers me towards believing that there has to be a clear emotional foundation on how we work. To be great, to achieve things, and to provide, we have to convert emotions into outcomes. Rather than the analytical brain being the fuel, it seems it’s more of a catalyst.</p>
<pre tabindex="0"><code><span><span>function</span><span> ValueOfOutcome</span><span>(</span><span>emotionalDrive</span><span>) {</span></span>
<span><span>	// real life situations modifier</span></span>
<span><span>	const</span><span> situationsModifier;</span></span>
<span><span>	// Value of Outcome is the result of the Analytical Process of an emotionalDrive times the situationModifier</span></span>
<span><span>	return</span><span> AnalyticalProcess</span><span>(emotionalDrive) </span><span>*</span><span> situationsModifier;</span></span>
<span><span>}</span></span></code></pre>
<p>Emotion cannot be separate from work. It has to be a part of it. When working, you’re expressing yourself. You express beliefs, opinions, and strategies, world views. You cannot detach yourself completely from work. I doubt that you ever should.</p>
<h2 id="peer-to-peer-communications">Peer-to-peer communications</h2>
<p>People around you are fully aware when you’re doing something for the sake of doing it. No, you’re not hiding it well enough. No, you don’t convince people that you’re having a great time, when you’re not. No one who pays careful attention to what they experience really believes that you’re doing great, while not loving what you do. Again, you might do ok. And ok can be fine. You have to decide if <em>“ok”</em> is enough for you.</p>
<p>In any context, people do get it when you’re acting. Most of the time.</p>
<p>People who pay attention get it almost every single time.</p>
<h2 id="you-should-be-doing-great">You should be doing great</h2>
<p>Everyone wants to excel, right? Not on everything of course, but on what they love, sure. After all, the positive feedback loop of “love” is a mandatory part of its survival over time.</p>
<p>If you want to enjoy yourself when working, try doing something you love. It’s about extroversion. It’s about getting it out and sharing it. People do notice when you love what you do. The reimbursement is not always fair. Frankly, it usually isn’t. But at least you’re doing what you love and, for most people after a certain living standard, this is reimbursement enough.</p>
<p>If you don’t know what you love, try playing with a couple of things. Try gathering some new experiences. It’ll be fun!</p>
<p>If you know what you love and are afraid to pursue to make a living out of this, stop doing that immediately. Everything else is just a compromise. And while compromise can be mandatory, it cannot constitute a permanent state of being. Compromise is a trap for more compromise. Go out and do what you love. This is who you are.</p>
<p>If you know what you love and already do that for a living, <em>thank you</em>. Your contributions are deeply appreciated.</p>
<p>George Spanos</p>
<p><a href="https://moby-it.com/" rel="nofollow, noopener, noreferrer" target="_blank">Moby IT</a></p>    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Finance worker pays out $25M after VC with deepfake CFO (289 pts)]]></title>
            <link>https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html</link>
            <guid>39248649</guid>
            <pubDate>Sun, 04 Feb 2024 08:43:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html">https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=39248649">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-editable="main" data-track-zone="main" data-reorderable="main">  <article data-uri="cms.cnn.com/_components/article/instances/cls6vbf7p0025a9nramq6c335@published" role="main" data-unselectable="true">
      
  <section data-tabcontent="Content">
    <main>
                <div data-image-variation="image" data-breakpoints="{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300}" data-uri="cms.cnn.com/_components/image/instances/cls6vg79l00063b6hf65f80kr@published" data-name="GettyImages-1437811938.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.6666666666666666" data-original-height="2000" data-original-width="3000" data-url="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=original" data-editable="lede" data-freewheel-lede="true">
       <picture><source height="383" width="680" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=16x9&amp;q=h_383,w_680,c_fill/f_webp" type="image/webp"><source height="653" width="1160" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=16x9&amp;q=h_653,w_1160,c_fill/f_webp" type="image/webp"><source height="605" width="1075" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=16x9&amp;q=h_605,w_1075,c_fill/f_webp" type="image/webp"><source height="833" width="1480" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=16x9&amp;q=h_833,w_1480,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=16x9&amp;q=h_833,w_1480,c_fill" alt="Authorities are increasingly concerned at the damaging potential posed by artificial intelligence technology." onload="this.classList.remove('image__dam-img--loading')" onerror="imageLoadError(this)" height="2000" width="3000"></picture>
    </div>
        
        
            <div data-editable="content" itemprop="articleBody" data-reorderable="content">
                    <p><cite>
      <span data-editable="location"></span>
      <span data-editable="source">CNN</span>
        &nbsp;—&nbsp;
    </cite>
</p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls6vbf7p0024a9nrhngzfc7l@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            A finance worker at a multinational firm was tricked into paying out $25 million to fraudsters using deepfake technology to pose as the company’s chief financial officer in a video conference call, according to Hong Kong police.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls732ubh00063d5vtonp5jy8@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The elaborate scam saw the worker duped into attending a video call with what he thought were several other members of staff, but all of whom were in fact deepfake recreations, Hong Kong police said at a briefing on Friday.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls72x77f00043d5v9kft5o5o@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “(In the) multi-person video conference, it turns out that everyone [he saw] was fake,”  senior superintendent Baron Chan Shun-ching told the city’s public broadcaster RTHK.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls73gcs300083d5vqrgwlj90@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Chan said the worker had grown suspicious after he received a message that was purportedly from the company’s UK-based chief financial officer. Initially, the worker suspected it was a phishing email, as it talked of the need for a secret transaction to be carried out.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls73gdgl000a3d5v2mmdlyua@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            However, the worker put aside his early doubts after the video call because other people in attendance had looked and sounded just like colleagues he recognized, Chan said.
    </p>

<div data-image-variation="image" data-breakpoints="{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300}" data-uri="cms.cnn.com/_components/image/instances/cls6zz14y000w3b6h93sv6453@published" data-name="" data-component-name="image" data-observe-resizes="" data-original-ratio="0.666015625" data-original-height="682" data-original-width="1024" data-url="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=h_682,w_1024,x_0,y_0" data-editable="settings">
       <picture><source height="682" width="1024" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=w_1110,c_fill/f_webp" type="image/webp"><source height="682" width="1024" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=w_1015,c_fill/f_webp" type="image/webp"><source height="682" width="1024" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=w_1160,c_fill/f_webp" type="image/webp"><source height="682" width="1024" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=w_680,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=w_1110,c_fill" alt="This aerial photo taken on December 19, 2018 shows a general view of the skyline of Hong Kong. " onload="this.classList.remove('image__dam-img--loading')" onerror="imageLoadError(this)" height="682" width="1024" loading="lazy"></picture>
    </div>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls73ms2y000h3d5vq6h91nw6@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Believing everyone else on the call was real, the worker agreed to remit a total of $200 million Hong Kong dollars – about $25.6 million, the police officer added.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls73p5qm000k3d5v8dltfh94@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The case is one of several recent episodes in which fraudsters are believed to have used deepfake technology to modify publicly available video and other footage to cheat people out of money.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls74r4vs001h3d5vp6iv426g@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            At the press briefing Friday, Hong Kong police said they had made six arrests in connection with such scams.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls7406x6000o3d5v7tamfvbm@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Chan said that eight stolen Hong Kong identity cards – all of which had been reported as lost by their owners – were used to make 90 loan applications and 54 bank account registrations between July and September last year.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls73p7jl000m3d5v9dn91735@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            On at least 20 occasions, AI deepfakes had been used to trick facial recognition programs by imitating the people pictured on the identity cards, according to police.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls7466h2000w3d5vo93ao08o@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The scam involving the fake CFO was only discovered when the employee later checked with the corporation’s head office.
    </p>

  


    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls745fva000u3d5vtnlcd70u@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Hong Kong police did not reveal the name or details of the company or the worker.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls718rv600163b6hcoz7rplm@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Authorities across the world are growing increasingly concerned at the sophistication of deepfake technology and the nefarious uses it can be put to.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls6yumv1000f3b6hofxm8j8t@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            At the end of January, pornographic, AI-generated images of the American pop star <a href="https://www.cnn.com/2024/01/25/tech/taylor-swift-ai-generated-images/index.html">Taylor Swift</a> spread across social media, underscoring the damaging potential posed by artificial intelligence technology.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls74e05b00153d5vox49fjhu@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The photos - which show the singer in sexually suggestive and explicit positions - were viewed tens of millions of times before being removed from social platforms.
    </p>

                </div>
    </main>
  </section>
</article>

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to Replace Your CPAP in Only 666 Days (314 pts)]]></title>
            <link>https://aphyr.com/posts/368-how-to-replace-your-cpap-in-only-666-days</link>
            <guid>39248631</guid>
            <pubDate>Sun, 04 Feb 2024 08:36:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aphyr.com/posts/368-how-to-replace-your-cpap-in-only-666-days">https://aphyr.com/posts/368-how-to-replace-your-cpap-in-only-666-days</a>, See on <a href="https://news.ycombinator.com/item?id=39248631">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p><em>This story is not practical advice. For me, it’s closing the book on an almost two-year saga. For you, I hope it’s an enjoyable bit of bureaucratic schadenfreude. For Anthem, I hope it’s the subject of a series of painful but transformative meetings. This is not an isolated event. I’ve had dozens of struggles with Anthem customer support, and they all go like this.</em></p>
<p><em>If you’re looking for practical advice: it’s this. Be polite. Document everything. Keep a log. Follow the claims process. Check the laws regarding insurance claims in your state. If you pass the legally-mandated deadline for your claim, call customer service. Do not allow them to waste a year of your life, or force you to resubmit your claim from scratch. Initiate a complaint with your state regulators, and escalate directly to <a href="mailto:gail.boudreaux@elevancehealth.com">Gail Boudreaux’s team</a>–or whoever Anthem’s current CEO is.</em></p>
<p>To start, experience an equipment failure.</p>
<p>Use your CPAP daily for six years. Wake up on day zero with it making a terrible sound. Discover that the pump assembly is failing. Inquire with Anthem Ohio, your health insurer, about how to have it repaired. Allow them to refer you to a list of local durable medical equipment providers. Start calling down the list. Discover half the list are companies like hair salons. Eventually reach a company in your metro which services CPAPs. Discover they will not repair broken equipment unless a doctor tells them to.</p>
<p>Leave a message with your primary care physician. Call the original sleep center that provided your CPAP. Discover they can’t help, since you’re no longer in the same state. Return to your primary, who can’t help either, because he had nothing to do with your prescription. Put the sleep center and your primary in touch, and ask them to talk.</p>
<p>On day six, call your primary to check in. He’s received a copy of your sleep records, and has forwarded them to a local sleep center you haven’t heard of. They, in turn, will talk to Anthem for you.</p>
<p>On day 34, receive an approval letter labeled “confirmation of medical necessity” from Anthem, directed towards the durable medical equipment company. Call that company and confirm you’re waitlisted for a new CPAP. They are not repairable. Begin using your partner’s old CPAP, which is not the right class of device, but at least it helps.</p>
<p>Over the next 233 days, call that medical equipment company regularly. Every time, inquire whether there’s been any progress, and hear “we’re still out of stock”. Ask them you what the manufacturer backlog might be, how many people are ahead of you in line, how many CPAPs they <em>do</em> receive per month, or whether anyone has ever received an actual device from them. They won’t answer any questions. Realize they are never going to help you.</p>
<p>On day 267, realize there is no manufacturer delay. The exact machine you need is in stock on CPAP.com. Check to make sure there’s a claims process for getting reimbursed by Anthem. Pay over three thousand dollars for it. When it arrives, enjoy being able to breathe again.</p>
<p>On day 282, follow CPAP.com’s documentation to file a claim with Anthem online. Include your prescription, receipt, shipping information, and the confirmation of medical necessity Anthem sent you.</p>
<p>On day 309, open the mail to discover a mysterious letter from Anthem. They’ve received your appeal. You do not recall appealing anything. There is no information about what might have been appealed, but something will happen within 30-60 days. There is nothing about your claim.</p>
<p>On day 418, emerge from a haze of lead, asbestos, leaks, and a host of other home-related nightmares; remember Anthem still hasn’t said anything about your claim. Discover your claim no longer appears on Anthem’s web site. Call Anthem customer service. They have no record of your claim either. Ask about the appeal letter you received. Listen, gobsmacked, as they explain that they decided your claim was in fact an appeal, and transferred it immediately to the appeals department. The appeals department examined the appeal and looked for the claim it was appealing. Finding none, they decided the appeal was moot, and rejected it. At no point did anyone inform you of this. Explain to Anthem’s agent that you filed a claim online, not an appeal. At their instruction, resign yourself to filing the entire claim again, this time using a form via physical mail. Include a detailed letter explaining the above.</p>
<p>On day 499, retreat from the battle against home entropy to call Anthem again. Experience a sense of growing dread as the customer service agent is completely unable to locate either of your claims. After a prolonged conversation, she finds it using a different tool. There is no record of the claim from day 418. There was a claim submitted on day 282. Because the claim does not appear in her system, there is no claim. There is a claim. There is no claim. Experience the cognitive equivalent of the Poltergeist hallway shot as the agent tells you “Our members are not eligible for charges for claim submission”.</p>
<p>Hear the sentence “There is a claim”. Hear the sentence “There is no claim”. Write these down in the detailed log you’ve been keeping of this unfurling Kafkaesque debacle. Ask again if there is anyone else who can help. There is no manager you can speak to. There is no tier II support. “I’m the only one you can talk to,” she says. Write that down.</p>
<p>Call CPAP.com, which has a help line staffed by caring humans. Explain that contrary to their documentation, Anthem now says members cannot file claims for equipment directly. Ask if they are the provider. Discover the provider for the claim is probably your primary care physician, who has no idea this is happening. Leave a message with him anyway. Leave a plaintive message with your original sleep center for good measure.</p>
<p>On day 502, call your sleep center again. They don’t submit claims to insurance, but they confirm that some people <em>do</em> successfully submit claims to Anthem using the process you’ve been trying. They confirm that Anthem is, in fact, hot garbage. Call your primary, send them everything you have, and ask if they can file a claim for you.</p>
<p>On day 541, receive a letter from Anthem, responding to your inquiry. You weren’t aware you filed one.</p>
<blockquote>
<p>Please be informed that we have received your concern. Upon review we have noticed that there is no claim billed for the date of service mentioned in the submitted documents, Please provide us with a valid claim. If not submitted,provide us with a valid claim iamge to process your claim further.</p>
</blockquote>
<p>Stare at the letter, typos and all. Contemplate your insignificance in the face of the vast and uncaring universe that is Anthem.</p>
<p>On day 559, steel your resolve and call Anthem again. Wait as this representative, too, digs for evidence of a claim. Listen with delight as she finds your documents from day 282. Confirm that yes, a claim definitely exists. Have her repeat that so you can write it down. Confirm that the previous agent was lying: members can submit claims. At her instruction, fill out the claim form a third time. Write a detailed letter, this time with a Document Control Number (DCN). Submit the entire package via registered mail. Wait for USPS to confirm delivery eight days later.</p>
<p>On day 588, having received no response, call Anthem again. Explain yourself. You’re getting good at this. Let the agent find a reference number for an appeal, but not the claim. Incant the magic DCN, which unlocks your original claim.  “I was able to confirm that this was a claim submitted form for a member,” he says. He sees your claim form, your receipts, your confirmation of medical necessity. However: “We still don’t have the claim”.</p>
<p>Wait for him to try system after system. Eventually he confirms what you heard on day 418: the claims department transferred your claims to appeals. “Actually this is not an appeal, but it was denied as an appeal.” Agree as he decides to submit your claim manually again, with the help of his supervisor. Write down the call ref number: he promises you’ll receive an email confirmation, and an Explanation of Benefits in 30-40 business days.</p>
<p>“I can assure you this is the last time you are going to call us regarding this.”</p>
<p>While waiting for this process, recall insurance is a regulated industry. Check the Ohio Revised Code. Realize that section 3901.381 establishes deadlines for health insurers to respond to claims. They should have paid or denied each of your claims within 30 days–45 if supporting documentation was required. Leave a message with the Ohio Department of Insurance’s Market Conduct Division. File an insurance complaint with ODI as well.</p>
<p>Grimly wait as no confirmation email arrives.</p>
<p>On day 602, open an email from Anthem. They are “able to put the claim in the system and currenty on processed [sic] to be applied”. They’re asking for more time. Realize that Anthem is well past the 30-day deadline under the Ohio Revised Code for all three iterations of your claim.</p>
<p>On day 607, call Anthem again. She explains that the claim will be received and processed as of your benefits. She asks you to allow 30-45 days from today. Quote section 3901.381 to her. She promises to expedite the request; it should be addressed within 72 business hours. Like previous agents, she promises to call you back. Nod, knowing she won’t.</p>
<p>On day 610, email the Ohio Department of Insurance to explain that Anthem has found entirely new ways to avoid paying their claims on time. It’s been 72 hours without a callback; call Anthem again. She says “You submitted a claim and it was received” on day 282. She says the claim was expedited. Ask about the status of that expedited resolution. “Because on your plan we still haven’t received any claims,” she explains. Wonder if you’re having a stroke.</p>
<p>Explain that it has been 328 days since you submitted your claim, and ask what is going on. She says that since the first page of your mailed claim was a letter, that might have caused it to be processed as an appeal. Remind yourself Anthem told you to enclose that letter. Wait as she attempts to refer you to the subrogation department, until eventually she gives up: the subrogation department doesn’t want to help.</p>
<p>Call the subrogation department yourself. Allow Anthem’s representative to induce in you a period of brief aphasia. She wants to call a billing provider. Try to explain there is none: you purchased the machine yourself. She wants to refer you to collections. Wonder why on earth Anthem would want money from <em>you</em>. Write down “I literally can’t understand what she thinks is going on” in your log. Someone named Adrian will call you by tomorrow.</p>
<p>Contemplate alternative maneuvers. Go on a deep Google dive, searching for increasingly obscure phrases gleaned from Anthem’s bureaucracy. Trawl through internal training PDFs for Anthem’s ethics and compliance procedures. Call their compliance hotline: maybe someone cares about the law. It’s a third-party call center for Elevance Health. Fail to realize this is another name for Anthem. Begin drawing a map of Anthem’s corporate structure.</p>
<p>From a combination of publicly-available internal slide decks, LinkedIn, and obscure HR databases, discover the name, email, and phone number of Anthem’s Chief Compliance Officer. Call her, but get derailed by an internal directory that requires a 10-digit extension. Try the usual tricks with automated phone systems. No dice.</p>
<p>Receive a call from an Anthem agent. Ask her what happened to “72 hours”. She says there’s been no response from the adjustments team. She doesn’t know when a response will come. There’s no one available to talk to. Agree to speak to another representative tomorrow. It doesn’t matter: they’ll never call you.</p>
<p>Do more digging. Guess the CEO’s email from what you can glean of Anthem’s account naming scheme. Write her an email with a short executive summary and a detailed account of the endlessly-unfolding Boschian hellscape in which her company has entrapped you. A few hours later, receive an acknowledgement from an executive concierge at Elevance (Anthem). It’s polite, formal, and syntactically coherent. She and promises to look into things. Smile. Maybe this will work.</p>
<p>On day 617, receive a call from the executive concierge. 355 days after submission, she’s identified a problem with your claim. CPAP.com provided you with an invoice with a single line item (the CPAP) and two associated billing codes (a CPAP and humidifier). Explain that they are integrated components of a single machine. She understands, but insists you need a receipt with multiple line items for them anyway. Anthem has called CPAP.com, but they can’t discuss an invoice unless you call them. Explain you’ll call them right now.</p>
<p>Call CPAP.com. Their customer support continues to be excellent. Confirm that it is literally impossible to separate the CPAP and humidifier, or to produce an invoice with two line items for a single item. Nod as they ask what the hell Anthem is doing. Recall that this is the exact same machine Anthem covered for you eight years ago. Start a joint call with the CPAP.com representative and Anthem’s concierge. Explain the situation to her voicemail.</p>
<p>On day 623, receive a letter from ODI. Anthem has told ODI this was a problem with the billing codes, and ODI does not intervene in billing code issues. They have, however, initiated a secretive second investigation. There is no way to contact the second investigator.</p>
<p>Write a detailed email to the concierge and ODI explaining that it took over three hundred days for Anthem to inform you of this purported billing code issue. Explain again that it is a single device. Emphasize that Anthem has been handling claims for this device for roughly a decade.</p>
<p>Wait. On day 636, receive a letter from Anthem’s appeals department. They’ve received your request for an appeal. You never filed one. They want your doctor or facility to provide additional information to Carelon Medical Benefits Management. You have never heard of Carelon. There is no explanation of how to reach Carelon, or what information they might require. The letter concludes: “There is currently no authorization on file for the services rendered.” You need to seek authorization from a department called “Utilization Management”.</p>
<p>Call the executive concierge again. Leave a voicemail asking what on earth is going on.</p>
<p>On day 637, receive an email: she’s looking into it.</p>
<p>On day 644, Anthem calls you. It’s a new agent who is immensely polite. Someone you’ve never heard of was asked to work on another project, so she’s taking over your case. She has no updates yet, but promises to keep in touch.</p>
<p>She does so. On day 653, she informs you Anthem will pay your claim in full. On day 659, she provides a check number. On day 666, the check arrives.</p>
<p>Deposit the check. Write a thank you email to the ODI and Anthem’s concierge. Write this, too, down in your log.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why is the mouse cursor slightly tilted and not straight? (417 pts)]]></title>
            <link>https://ux.stackexchange.com/questions/52336/why-is-the-mouse-cursor-slightly-tilted-and-not-straight</link>
            <guid>39248225</guid>
            <pubDate>Sun, 04 Feb 2024 06:57:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ux.stackexchange.com/questions/52336/why-is-the-mouse-cursor-slightly-tilted-and-not-straight">https://ux.stackexchange.com/questions/52336/why-is-the-mouse-cursor-slightly-tilted-and-not-straight</a>, See on <a href="https://news.ycombinator.com/item?id=39248225">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mainbar" role="main" aria-label="question and answers">
                
<div data-questionid="52336" data-position-on-page="0" data-score="604" id="question">
        

        

<div>
    
    <div itemprop="text">
                
<p>Is this a legacy thing or does a tilted cursor serves a purpose? I can tell that, the angle provides a totally vertical left edge which helps when highlighting text but what else apart from that?</p>

<p>EDIT: When cursor is swapped by the little hand cursor when hovered over buttons, the angle seems to be smaller. Why the difference?</p>
    </div>

        

    <div>
    <div>
        <p>
            asked <span title="2014-02-17 09:41:59Z">Feb 17, 2014 at 9:41</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/7393/thanos"><p><img src="https://www.gravatar.com/avatar/aac452da1312fa333c36015a5e591f01?s=64&amp;d=identicon&amp;r=PG" alt="Thanos's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>
    
</div>




            <p><span itemprop="commentCount">8</span></p>
    </div>



                
                
                <div id="answers">
                    


                                    
<div id="answer-52338" data-answerid="52338" data-parentid="52336" data-score="729" data-position-on-page="1" data-highest-scored="1" data-question-has-accepted-highest-score="1" itemprop="acceptedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
                
<p>This is the historical reason:</p>

<p><img src="https://i.stack.imgur.com/e1zH5.png" alt="Concept drawing of the standard mouse cursor at an angle"></p>

<p>(Concept drawing taken from document: <a href="http://bitsavers.trailing-edge.com/pdf/xerox/parc/techReports/VLSI-81-1_The_Optical_Mouse.pdf">VLSI-81-1_The_Optical_Mouse.pdf</a>)</p>

<p>The mouse, and therefore the mouse cursor, was <a href="http://arstechnica.com/features/2005/05/gui/2/">invented by Douglas Engelbart</a>, and was initially <a href="http://origin.arstechnica.com/images/gui/4-NLSgui.jpg">an arrow pointing up</a>. </p>

<p>When the <a href="http://arstechnica.com/features/2005/05/gui/3/">XEROX PARC</a> machine was built, the cursor changed into a tilted arrow. It was found that, given the low resolution of the screens in those days, drawing a straight line (left edge of arrow) and a line at a 45 degree angle (right edge of arrow) was easier to do and more recognizable than the straight cursor.</p>
    </div>
    <div>
            
            <div>
        <a href="https://ux.stackexchange.com/users/40110/code-maverick"><p><img src="https://www.gravatar.com/avatar/c056c352518943b11095c83a4ef2b31f?s=64&amp;d=identicon&amp;r=PG" alt="Code Maverick's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <div>
        <p>
            answered <span title="2014-02-17 09:47:52Z">Feb 17, 2014 at 9:47</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/5657/bart-gijssens"><p><img src="https://www.gravatar.com/avatar/54b1215ffb534c90dd7ea7f480d28c51?s=64&amp;d=identicon&amp;r=PG" alt="Bart Gijssens's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/5657/bart-gijssens">Bart Gijssens</a><span itemprop="name">Bart Gijssens</span></p><p><span title="reputation score 17,317" dir="ltr">17.3k</span><span>4 gold badges</span><span>49 silver badges</span><span>62 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">9</span></p>
    </div>


                                    
<div id="answer-52370" data-answerid="52370" data-parentid="52336" data-score="393" data-position-on-page="2" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
                    

<p>Take your right hand and point to your question.</p>

<p>There, you see. </p>

<p><img src="https://i.stack.imgur.com/xqGFX.jpg" alt="finger pointing at screen"></p>
    </div>
    <div>
            
            <div>
        <a href="https://ux.stackexchange.com/users/127876/silas-reel"><p><img src="https://lh5.googleusercontent.com/-yjvPGG9oHpw/AAAAAAAAAAI/AAAAAAAAAAA/AAN31DV4PX0I1kJiPjyoOIMz70ejP2SvbA/mo/photo.jpg?sz=64" alt="Silas Reel's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <div>
        <p>
            answered <span title="2014-02-17 18:13:23Z">Feb 17, 2014 at 18:13</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/43246/jturolla"><p><img src="https://www.gravatar.com/avatar/d24c555de0ab090f0b822155f31affe4?s=64&amp;d=identicon&amp;r=PG" alt="jturolla's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/43246/jturolla">jturolla</a><span itemprop="name">jturolla</span></p><p><span title="reputation score " dir="ltr">3,511</span><span>1 gold badge</span><span>10 silver badges</span><span>7 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">27</span></p>
    </div>

                                    
<div id="answer-52349" data-answerid="52349" data-parentid="52336" data-score="189" data-position-on-page="3" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>In addition to <a href="https://ux.stackexchange.com/a/52338/43668">Bart's answer</a>, I'd like to add one more reason. </p>

<p>The reason the arrow was tilted to the left was so that the click position was easier to calculate, because the origin of the cursor's bitmap was in the upper left.  This saved the mouse tracking subroutine a calculation on every click (its not much but it helped on older machines).  </p>

<p><a href="http://www.reddit.com/r/explainlikeimfive/comments/1qhzym/" rel="noreferrer">Source</a></p>
    </div>
    <div>
            
            <div>
        <a href="https://ux.stackexchange.com/users/-1/community"><p><img src="https://www.gravatar.com/avatar/a007be5a61f6aa8f3e85ae2fc18dd66e?s=64&amp;d=identicon&amp;r=PG" alt="Community's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <div>
        <p>
            answered <span title="2014-02-17 14:40:50Z">Feb 17, 2014 at 14:40</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/21591/jameo"><p><img src="https://www.gravatar.com/avatar/939c911747739eeb05c16b6b8a922ed9?s=64&amp;d=identicon&amp;r=PG" alt="Jameo's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/21591/jameo">Jameo</a><span itemprop="name">Jameo</span></p><p><span title="reputation score " dir="ltr">1,853</span><span>1 gold badge</span><span>11 silver badges</span><span>8 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">14</span></p>
    </div>


                                    
<div id="answer-52558" data-answerid="52558" data-parentid="52336" data-score="124" data-position-on-page="4" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<h2>Low level visual cognition</h2>
<p>In addition to the various answers given, there is also sense in a tilted mouse pointer if one considers the visual processes in our brain.</p>
<p>Visual information arriving from our eyes is first processed in the primary visual cortex by the V1 area, then by the V2 area. These two areas recognise low-level visual features (hue, lightness, size, orientation, etc.).</p>
<h2>The popout effect</h2>
<p>As visual information is processed by these areas, some visual irregularities truly pop out (ie, they are highly distinguishable), which greatly helps visual search (trying to find an item in a visually busy field). The popular name for this phenomenon is <strong>the popout effect</strong>.</p>
<p>A famous research from 1988 - <a href="http://www2.psychology.uiowa.edu/faculty/hollingworth/prosem/Treisman_Gormican_88_PR_FeatureAnalysisIn.pdf" rel="noreferrer">A. Treisman, and S. Gormican: Feature analysis in early vision: Evidence from search asymmetries</a> summarises many of these popout effects, and the irregularities they involve.</p>
<h2>Orientation</h2>
<p>One such irregularity is <strong>orientation</strong>, and it is neatly explained by the following illustration:</p>
<p><img src="https://i.stack.imgur.com/4xWaH.png" alt="3 images showing many vertical lines and how a tilted line pops out"></p>
<p>You should find it next to impossible to find the search target in 1 (a straight line in a group of straight lines). But rather easy in 2 - finding a tilted line in a group of straight lines. In 3 it should be equally next to impossible to find the tilted line in a group of tilted lines (of the same angle).</p>
<p>Since vertical and horizontal orientations are the most common ones on screens (and in life in general) a tilted mouse pointer will be more easily found.</p>
<p>More information can be found in Chapter 2 (What we can easily see) of <a href="http://www.amazon.co.uk/Visual-Thinking-Kaufmann-Interactive-Technologies/dp/0123708966/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1384303964&amp;sr=1-1&amp;keywords=visual+thinking+for+design" rel="noreferrer">Visual Thinking for Design</a>, Ware 2008.</p>
    </div>
    <div>
            
            <div>
        <a href="https://ux.stackexchange.com/users/-1/community"><p><img src="https://www.gravatar.com/avatar/a007be5a61f6aa8f3e85ae2fc18dd66e?s=64&amp;d=identicon&amp;r=PG" alt="Community's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <div>
        <p>
            answered <span title="2014-02-19 23:38:31Z">Feb 19, 2014 at 23:38</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/16924/izhaki"><p><img src="https://www.gravatar.com/avatar/35c050eac0eab06a8c3b6fec8c2bb5c0?s=64&amp;d=identicon&amp;r=PG" alt="Izhaki's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/16924/izhaki">Izhaki</a><span itemprop="name">Izhaki</span></p><p><span title="reputation score 32,465" dir="ltr">32.5k</span><span>5 gold badges</span><span>66 silver badges</span><span>99 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">13</span></p>
    </div>

                                    
<div id="answer-52355" data-answerid="52355" data-parentid="52336" data-score="80" data-position-on-page="5" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
                    

<p>I've always thought that the arrow cursor is shaped similarly to your hand if you were point (naturally) at the screen with your (as typically dominant) right hand.</p>

<p>I have no support of this other than my own subjective experience but it strikes me as a natural shape when trying to relate real world interaction into a low resolution computer screen where rendering something resembling a hand would be impossible.</p>

<p>[Edit: Someone stole the only thunder I've ever had on StackAnything. Thanks!]</p>

<p><img src="https://i.stack.imgur.com/qGzNQ.jpg" alt="Hand pointing at screen"></p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-02-17 15:23:08Z">Feb 17, 2014 at 15:23</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/43174/user43174"><p><img src="https://www.gravatar.com/avatar/a69a703c86958e2d50e517ffd86c5e01?s=64&amp;d=identicon&amp;r=PG&amp;f=y&amp;so-version=2" alt="user43174's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/43174/user43174">user43174</a><span itemprop="name">user43174</span></p><p><span title="reputation score " dir="ltr">853</span><span>5 silver badges</span><span>4 bronze badges</span>
        </p>
    </div>
</div>
    
</div>




            <p><span itemprop="commentCount">6</span></p>
    </div>

                                    
<div id="answer-52461" data-answerid="52461" data-parentid="52336" data-score="49" data-position-on-page="6" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>In case anyone wonders : some less known interfaces did use a straight arrow as pointed in <a href="http://www.reddit.com/r/explainlikeimfive/comments/1qhzym/">Reddit</a></p>

<p><img src="https://i.stack.imgur.com/rJmmW.gif" alt="enter image description here"></p>

<p><img src="https://i.stack.imgur.com/ukk6x.gif" alt="enter image description here"></p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-02-19 00:48:06Z">Feb 19, 2014 at 0:48</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/25125/gildas-fr%c3%a9mont"><p><img src="https://i.stack.imgur.com/RHbGy.jpg?s=64&amp;g=1" alt="Gildas Frémont's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>
    
</div>




            <p><span itemprop="commentCount">1</span></p>
    </div>

                                    
<div id="answer-52360" data-answerid="52360" data-parentid="52336" data-score="22" data-position-on-page="7" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>Also, there is another answer to this question. As a rule, the <strong>arrow</strong> mouse cursor must have one sharp tip (vertex) - because it is an arrow :) </p>

<p>On the other hand, it is better for a mouse cursor to look good and slick. </p>

<p>But drawing sharp tip on a rectangular pixel based display is very hard, especially without anti-aliasing. </p>

<p>The 0 degrees (horizontal or vertical) and 45 degrees lines are the only possible lines that look smooth without anti-aliasing. </p>

<p>That is why almost all arrow mouse cursors are based on one straight and one 45 degrees lines. As a result, the bisector line has angle of 45/2 = 22.5 degrees.</p>

<p>The tail of the arrow is much harder to be drawn well, but it is not so important as well. </p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-02-17 16:30:01Z">Feb 17, 2014 at 16:30</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/25879/johnfound"><p><img src="https://i.stack.imgur.com/R81XM.png?s=64&amp;g=1" alt="johnfound's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/25879/johnfound">johnfound</a><span itemprop="name">johnfound</span></p><p><span title="reputation score " dir="ltr">1,116</span><span>8 silver badges</span><span>16 bronze badges</span>
        </p>
    </div>
</div>
    
</div>




            <p><span itemprop="commentCount">4</span></p>
    </div>

                                    
<div id="answer-68326" data-answerid="68326" data-parentid="52336" data-score="7" data-position-on-page="8" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p><strong>It is a right-handed world.</strong> </p>

<p>It used to be that if you switched our right/left click buttons the arrow would point towards the right (opposite of the images cited). </p>

<p>This supports that the arrow mimics a hand pointing while providing angular contrast. Without a reference, it is an extension of the <em>desktop</em> metaphor.</p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-12-03 19:19:22Z">Dec 3, 2014 at 19:19</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/10767/ken"><p><img src="https://i.stack.imgur.com/htBYy.png?s=64&amp;g=1" alt="Ken's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/10767/ken">Ken</a><span itemprop="name">Ken</span></p><p><span title="reputation score " dir="ltr">1,232</span><span>7 silver badges</span><span>10 bronze badges</span>
        </p>
    </div>
</div>
    
</div>




            <p><span itemprop="commentCount">1</span></p>
    </div>

                                    
<div id="answer-97398" data-answerid="97398" data-parentid="52336" data-score="5" data-position-on-page="9" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>The fact that the mouse cursor is slightly tilted to the left makes a lot of sense. 
A very interesting fact:</p>

<p>If it were straight, it would take a nanosecond more to place the cursor on the desired object. Human mind is generally used to perceiving elements from left to the right, that is why the cursor is designed into the opposite direction, anticipating the intent of interaction with the element you are about to click on.</p>

<p>A nanosecond of time optimization is the closest thing to the absolute idea of irrelevance. With that I agree. However, on a perception level, it makes a huge difference. </p>

<p>The tilted cursor becomes similar to an athlete who's always on the start position, ready to take off towards anything you want to click on at any time.</p>

<p>It's a sensation that gives you so much comfort without you realizing why.</p>

<p>Semiotics, Cognitive Science and Psychology are all embedded into the simple and subtle decision of keeping the tilted cursor, just to simplify by a bit your experience.</p>

<p>Why was it tilted in the first place? Well, in its history, it seems like it was only an accident determined by some technical limitations:</p>

<p><a href="http://www.fastcodesign.com/3026625/why-the-mouse-cursor-is-tilted-instead-of-vertical" rel="nofollow">Why Your Mouse Cursor Looks The Way It Does</a></p>
    </div>
    <div>
            
            <div>
    
    <div>
        <a href="https://ux.stackexchange.com/users/54669/devin"><p><img src="https://i.stack.imgur.com/egmb3.jpg?s=64&amp;g=1" alt="Devin's user avatar" width="32" height="32"></p></a>
    </div>
    <div>
        <p><a href="https://ux.stackexchange.com/users/54669/devin">Devin</a></p><p><span title="reputation score 37,762" dir="ltr">37.8k</span><span>15 gold badges</span><span>79 silver badges</span><span>140 bronze badges</span>
        </p>
    </div>
</div>


            <div>
    <div>
        <p>
            answered <span title="2016-07-29 04:24:58Z">Jul 29, 2016 at 4:24</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/86656/mircea"><p><img src="https://graph.facebook.com/10208868140194564/picture?type=large" alt="Mircea's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/86656/mircea">Mircea</a><span itemprop="name">Mircea</span></p><p><span title="reputation score " dir="ltr">522</span><span>3 silver badges</span><span>4 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">1</span></p>
    </div>

                                    
<div id="answer-68302" data-answerid="68302" data-parentid="52336" data-score="3" data-position-on-page="10" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
    
    <div itemprop="text">
<p>The angle, the cursor is inclined at gives a better feeling of pointing something. A cursor straight at 90 degree would not provide a good effect.It provides  improved appearance on low resolution screens.</p>

<p>Also the position calculation would become a lot easier when done from the top left corner of the pixel.</p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-12-03 13:04:10Z">Dec 3, 2014 at 13:04</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/41491/ashu"><p><img src="https://i.stack.imgur.com/3KnoW.jpg?s=64&amp;g=1" alt="ashu's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/41491/ashu">ashu</a><span itemprop="name">ashu</span></p><p><span title="reputation score " dir="ltr">249</span><span>1 silver badge</span><span>9 bronze badges</span>
        </p>
    </div>
</div>
    
</div>

                                    
<div id="answer-101006" data-answerid="101006" data-parentid="52336" data-score="2" data-position-on-page="11" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
    
    <p>A straight cursor would also obscure more of the object underneath raising the same issues when designing for touch interfaces</p>
    <div>
    <div>
        <p>
            answered <span title="2016-11-01 22:07:43Z">Nov 1, 2016 at 22:07</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/47160/mark-c"><p><img src="https://graph.facebook.com/710302166/picture?type=large" alt="Mark C's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/47160/mark-c">Mark C</a><span itemprop="name">Mark C</span></p><p><span title="reputation score " dir="ltr">151</span><span>1 silver badge</span><span>4 bronze badges</span>
        </p>
    </div>
</div>
    
</div>

                                    
<div id="answer-135748" data-answerid="135748" data-parentid="52336" data-score="2" data-position-on-page="12" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
    
    <div itemprop="text">
<p>Well, the cursor is a pointer, and mimics pointer angles from real life (~30-45° to the vertical).</p>
<p><a href="https://i.stack.imgur.com/J8xzk.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/J8xzk.png" alt="Pointers in the real-world"></a></p>
<p>Importantly, that angle serves to <strong>guide the eye down the length of the pointer</strong>, in the direction going "into" the screen, <strong>towards a single point</strong>, in the same way as perspective drawings do:</p>
<p><a href="https://i.stack.imgur.com/mKdC5.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/mKdC5.png" alt="Perspective drawing"></a></p>
<p>On the contrary, a straight arrow seems to point in the general up-direction, targeting no one point in particular. Have you ever used, or seen someone use, a pointer stick vertically upwards? That is indeed awkward, and reserved for moments where the object being pointed to is high up and well beyond the height of the person and the length of the stick combined, and can be vague in conveying what is actually being pointed at.</p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2020-11-26 20:32:45Z">Nov 26, 2020 at 20:32</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/28743/snag"><p><img src="https://www.gravatar.com/avatar/4a6245cc648b214ad6a440bfe18d0152?s=64&amp;d=identicon&amp;r=PG&amp;f=y&amp;so-version=2" alt="SNag's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/28743/snag">SNag</a><span itemprop="name">SNag</span></p><p><span title="reputation score " dir="ltr">9,597</span><span>3 gold badges</span><span>22 silver badges</span><span>26 bronze badges</span>
        </p>
    </div>
</div>
    
</div>


                                    



                            <h2 data-loc="1">
                                
                            </h2>
                </div>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The first amateur radio station on the moon (147 pts)]]></title>
            <link>https://www.arrl.org/news/the-first-amateur-radio-station-on-the-moon-js1ymg-is-now-transmitting</link>
            <guid>39247614</guid>
            <pubDate>Sun, 04 Feb 2024 04:27:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.arrl.org/news/the-first-amateur-radio-station-on-the-moon-js1ymg-is-now-transmitting">https://www.arrl.org/news/the-first-amateur-radio-station-on-the-moon-js1ymg-is-now-transmitting</a>, See on <a href="https://news.ycombinator.com/item?id=39247614">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p><span>02/02/2024</span></p><p>The Japan Aerospace Exploration Agency (JAXA) successfully landed their Smart Lander for Investigating Moon (SLIM) on January 19, 2024. Just before touchdown, SLIM released two small lunar surface probes, LEV-1 and LEV-2.</p>
<p>LEV-2 collects data while moving on the lunar surface, and LEV-1 receives the data.</p>
<p>The JAXA Ham Radio Club (JHRC), JQ1ZVI, secured amateur radio license JS1YMG for LEV-1, which has been transmitting Morse code on 437.41 MHz since January 19. The probe uses a 1 W UHF antenna with circular polarization and is transmitting "matters related to amateur business."</p>
<p>Radio amateurs have been busy analyzing JS1YMG's signal, with&nbsp;<a href="https://destevez.net/2024/01/trying-to-decode-lev-1/" target="_blank">Daniel Estévez's, EA4GPZ, blog</a>&nbsp;introducing the method and extraction results for demodulating Morse code from the signal, as well as extracting the code string.</p>
<p>It's unclear how long signals will be heard. JAXA has said that SLIM was not designed to survive a lunar night, which lasts about 14 days, and is due to return in a few days.</p>
<p>SLIM was launched on September 6, 2023, and landed on January 19, 2024, with the mission of analyzing the composition of rocks to aid research about the origin of the moon. SLIM's landing made Japan the fifth country to achieve a soft touchdown on the moon. The landing was achieved with exceptional precision -- within 180 feet of its targeted touchdown location.</p>
		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AvaloniaUI: Create Multi-Platform Apps with .NET (162 pts)]]></title>
            <link>https://www.avaloniaui.net/</link>
            <guid>39246988</guid>
            <pubDate>Sun, 04 Feb 2024 02:34:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.avaloniaui.net/">https://www.avaloniaui.net/</a>, See on <a href="https://news.ycombinator.com/item?id=39246988">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

            <!-- Heading -->
            <h2>
              A proven path for WPF app modernization.
            </h2>

            <!-- Text -->
            <p>
                <span>Familiar.</span> Considered a spiritual successor to WPF, Avalonia UI provides a <a href="https://docs.avaloniaui.net/docs/next/get-started/wpf/">familiar developer experience</a> allowing you to leverage years of pre-existing knowledge and investments. With a <a href="https://www.avaloniaui.net/XPF#hybrid">Hybrid XPF</a> license, it's possible to use WPF controls within your Avalonia application from vendors, including Actipro, Telerik, Syncfusion and more. 
            </p>

            <!-- Text -->
            <p>
                  <span>Proven.</span> Trusted by companies including JetBrains, KLM, Canon, Schneider Electric, Unity Games and more for modernising their WPF apps. 
            </p>

          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Doom didn't kill the Amiga (158 pts)]]></title>
            <link>https://www.datagubbe.se/afb/</link>
            <guid>39246825</guid>
            <pubDate>Sun, 04 Feb 2024 02:05:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.datagubbe.se/afb/">https://www.datagubbe.se/afb/</a>, See on <a href="https://news.ycombinator.com/item?id=39246825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p><b>A detailed account of the rise and fall of an Amiga zealot</b></p>

<p><i>Early 2024</i></p>

<p>
<img src="https://www.datagubbe.se/afb/pics/dotc.png">
<br>
<i>A scene from the Amiga game Defender of the Crown, released in 1986.
For years, no other home computer came close to screens like these.</i>
</p>

<p>
Ever since I saw an Amiga 500 at a friend's house in what was probably late 1988, I wanted one for myself. Back then, computers were uncommon, especially at home. Even though I went to a school in a fairly affluent neighborhood, few kids had home computers or video games.
</p>

<p>
Gradually, that started to change.
</p>

<p>
I bought my own Amiga in February 1992. It was basically the same exact model as the one I had first seen in 1988: An Amiga 500+ with a 7 MHz 68000 CPU, 1 meg of RAM, 8-bit stereo PCM sound and many various graphics modes, of which 320x256 in 16 or 32 colors was the most common for games.
</p>

<h3>Competing Platforms</h3>

<p>
Buying any other computer was completely out of the question. There were practical reasons of course: I knew how it worked and friends had one, which meant we could copy pirated games from each other. And it ran <a href="https://www.datagubbe.se/dpaint/">Deluxe Paint</a>, an era defining graphics program and a killer application for anyone with artistic inclinations, which I'd convinced myself I had.
</p>

<p>
I had also come into contact with other types of machines. The C64 felt like a thing of the past, with blocky graphics and beepy sound. The Mac was a boring monochrome computer made for writing equally boring documents about tax deductions. I intensely remember seeing a PC for the first time, and how disappointed it made me: it was much worse than the Mac, possibly even than the C64. Downright ugly graphics, terrible sound and a mysterious operating system that required you to learn textual incantations by heart.
</p>

<p>
But the real home computer feud during those days was between the Amiga 500 and the Atari ST, in which the latter always seemed to come out losing; worse sound, worse graphics, worse OS. Such a machine was completely out of the question - it just <i>had</i> to be an Amiga.
</p>

<p>
I don't regret this decision one bit. Few gadgets have given me as much joy and positive experiences as my Amigas, and even in 1992 an Amiga 500 wasn't a bad purchase. The PC was still a fairly boring machine, having a hard time keeping up with the Amiga's sound and graphics without costing an exorbitant amount of money. The revolutionary Amiga architecture, released in 1985 and basically unchanged since then, could still hold its own: a testament to its ingenuity.
</p>

<h3>The Price is Right</h3>

<p>
Around this time, PC:s were showing signs of becoming affordable, and Amiga style platform- and arcade games had begun appearing on the platform. But even the most straightforward of these titles required a fast 386 CPU, plenty of memory and a VGA card to come close to the amount of motion and commotion the Amiga had been capable of for ages. This meant the price to play was still high: In Sweden, around Christmas 1992, a 25 MHz 386/SX with 2 megs of RAM and a 40 meg hard drive was selling for $530. To this came the additional cost of a sound card. A Sound Blaster 2.0, guaranteed to be supported by most games, was available for a whopping $129. While you could hook the Amiga to a cheap 14" TV via RGB SCART, the PC required an expensive VGA monitor, on which you couldn't also watch MacGyver. Bummer!
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/sotb.png">
<br>
<i>Shadow of the Beast, released for the Amiga in 1989. It featured
lush graphics, multiple layers of parallax scrolling, massive sprites
and tough as nails gameplay. It looked like crap on pretty much every
other system, and wasn't even ported to the then inferior PC.</i>
</p>

<p>
This put the cost of an entry level 386 system at $660 without software, compared to the Amiga 600's $499, and the A600's 40 MB hard drive came pre-loaded with Deluxe Paint, a word processor and several decent games. It's true that a 386 PC was much faster than the 7 MHz Amiga in some aspects, but most of those aspects didn't matter for people like me and my friends - at least not yet.
</p>

<h3>More Moore</h3>

<p>
It's hard to convey just how intense the effects of Moore's law were during the 90's. When it came to hardware, December 1992 was a <i>very</i> different time compared to January 1992. During this year, Commodore pulled a stunt by first introducing the A500+ and then swiftly replacing it with the A600 - a low cost model with the advantage of having an integrated IDE interface, only to finish off by making the A600 obsolete with the release of the next-generation A1200. They were widely criticized for this, but in their defense, home computing was moving at breakneck speed and nobody could really keep up. At any given moment, something that had been unfeasible just six months ago was suddenly commonplace. One such something was <b>Doom</b> - which was nowhere, and then suddenly everywhere. Some Amiga fanatics still claim that Doom was what killed the Amiga, but I don't believe that to be true. Doom was a symptom, not the disease proper.
</p>

<p>
Commodore's last sigh, the Amiga 1200, has been touted as a bad machine and an architectural mistake. Its 14 MHz 68020 CPU, 2 megabytes of RAM and <a href="https://en.wikipedia.org/wiki/Amiga_Advanced_Graphics_Architecture">AGA</a> chipset is often, in hindsight, dismissed as too little, too late. Considering this, it still sold fairly well in Europe - because as a home computer, it actually wasn't half bad. It could be equipped with a hard drive, it had an expansion slot for CPU and memory upgrades, and the new graphics modes actually rivalled VGA and even SVGA in many ways. The 1280x512 pixel mode in 18-bit color was too slow for games, but when displaying still pictures it was quite a sight to behold.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/500p.jpg">
<br>
<i>One of the few surviving photos of my Amiga 500+. Here I'm
showing my grandmother something that looks like a shell window,
and she's pretending to be interested. T-shirt sizes were very comfortable
in those days.</i>
</p>

<p>
To me, even in 1994, switching to another architecture still wasn't under consideration. I was now an <i>Amiga fanboy</i> and the platform had become my natural home. And why not? An entry level A1200 hooked to a TV set wasn't the worst of choices for a boy in his early teens and it was of course a natural step up from the A500. My peripherals and nearly all of my software still worked and I could keep using my trusty old 14" TV, on which I could also watch The X-Files without parental interference. It was a platform I was comfortable with, and I got a good second hand deal - it even came with a hard drive, which was what I wanted for my computer most of all at that point.
</p>

<p>
Still, times were changing. PCs were no longer just for very rich kids and office professionals: any serious gamer had to consider one, and not just because of Doom. PC games were perhaps not yet as colorful, zippy and funky as Amiga games, but they were often very complex and relied heavily on a fast CPU to manage that complexity. Most people didn't really have a fast enough PC to fully enjoy Doom at its time of release in 1993 - but just a year later, expectations on home computing had changed. Even though 486 machines hadn't dropped <i>that</i> drastically in price yet, hardly anyone even marketed 386 machines anymore.
</p>

<p>
Memory was cheaper, as were hard drives. Better graphics and sound fidelity was expected, and it seemed every new PC graphics card that came out offered higher resolutions and more colors than the previous. CD quality sound was suddenly a thing, as was putting a CD-ROM reader in your computer. Using a flickering 50 Hz PAL TV instead of a rock solid 60 Hz VGA monitor was no longer seen as clever frugality, but rather as a way of hurting your eyes when trying to play Sim City 2000 in its high resolution 640x480 glory - which the A1200 honestly wasn't quite fast enough to do anyway. Adventure games like Monkey Island II, Simon the Sorcerer and Indiana Jones and the Fate of Atlantis came on eleven (11!), nine (9!) and eleven (11!) floppy disks, respectively. The Amiga 500 had traditionally been a floppy-based system, but a hard drive was now more or less required, even for gamers.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/agony.png">
<br>
<i>A cutscene from the Amiga game Agony (1992). It took a while,
but the PC eventually caught up, graphics wise.</i>
</p>

<p>
In spring of 1994, around the time of Commodore's demise, a 33 MHz 486 PC with an SVGA card, 4 megs of RAM, a 200 meg hard drive <i>and</i> a 14" monitor cost $1300. For an Amiga 1200 to even begin to come close to such a system, you'd have to spend at least as much money. And while the PC still came without a sound card, a high resolution monitor for the Amiga had to be able to auto-switch between 15 kHz (PAL) and 31 kHz (VGA), pushing the price of the Amiga system higher still. Add to that the fact that the PC was a big box machine, with plenty of room for, say, a CD-ROM drive - the hot new thing.
</p>

<h3>The DOS conundrum</h3>

<p>
By 1995, Commodore was well and truly dead. It was during this time I started getting acquainted with other platforms in earnest: DOS, Windows, Linux, newer Macs, even Unix workstations. I had plenty of friends who owned PCs, but I stayed true to my Amiga, adding CPU and RAM upgrades that cost as much as the computer itself. I could have saved money and bought a PC - but I honestly didn't see the point. Linux was nice for surfing the net, but it lacked all of the fun software I craved: games, demos (as in the demo scene), graphics programs, tracker music. It didn't even have the things I wanted for school, such as a reasonable word processor with Swedish spell checking.
</p>

<p>
All of this and more was of course available on PCs, and I was frequently exposed to it when visiting my PC owning friends. Deluxe Paint, my beloved killer app for the Amiga, apparently existed on PC as well. And there were lots of other neat programs that appealed to a young demo scener: Fasttracker and Screamtracker for music, QBasic and Turbo Pascal for programming, QPEG for viewing images, TheDraw for making ANSI graphics, and droves of pictures, music, games and scene demos.
</p>

<p>
The strange thing was that all of the fun PC stuff was made for DOS. I just didn't get it. Command lines no longer put me off, having dabbled quite a bit with them on both Amiga and Linux, but DOS lacked that one crucial thing I had grown ever more accustomed to on my Amiga: Native, effortless, pre-emptive multitasking.
</p>

<p>
The PCs were indeed impressive, hardware-wise. SVGA was now commonplace, as were 486/DX processors running in 33 or even 66 MHz. Doom was not only playable, but enjoyable. PC scene demos were running impressive texture-mapped 3D effects much faster and, with the addition of modern sound cards, offering much better audio fidelity than my Amiga was capable of. Ostensibly simple things like loading JPEG images felt instant compared to my A1200, even after it was upgraded with a 28 MHz 68030 CPU.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/msdos.png">
<br>
<i>Things like these were perfectly reasonable to MS-DOS users. Expanded or extended memory? Or maybe just conventional? Choose wisely, or you won't
be able to fully maximize the incredible potential of running <b>a single program at a time</b>.</i>
</p>

<p>
What I couldn't grasp was the point of having (and paying for) all that raw power if you couldn't utilize it fully. On my Amiga, I could run Amos Professional - an advanced BASIC dialect - and Deluxe Paint <i>at the same time</i>. If I wanted to change an image used in an Amos program, switching between the two applications was instant. If I suddenly wanted to take some notes, I could just fire up a text editor. And I could listen to music at the same time! In DOS, even something as mundane as enjoying tracker music was a full time, full screen activity - not something you could keep doing while working in other programs.
</p>

<p>
This multitasking was and is a big part of my affinity for the Amiga, and it opened up the possibility for several other <a href="https://www.datagubbe.se/ltmag/">ingenious features</a>. Workflows were completely customizeable with powerful scripting languages, the modular design of the OS made it extremely adaptable to things like new file formats and file systems, and every aspect of it could be tweaked and configured to suit personal needs. It was (and still is, in many ways) like running a carefully honed environment from a professional workstation on a cheap home computer. DOS, Atari and Mac just couldn't compare.
</p>

<p>
Furthermore, DOS seemed to require inordinate amounts of tweaking and configuration. Hardware interrupts had to be configured manually, the 640 kB base memory had to be carefully guarded, and you had to select and configure your sound and graphics cards in almost every single game you wanted to play. On the Amiga, everything just worked. If you bought a new peripheral, all you had to do was plug it in and boot your computer. Drivers were needed for some things, of course, but the Amiga's <i>Autoconfig</i> took care of peripheral detection and configuration.
</p>

<p>
When Dial-up Internet became (almost) affordable for the masses, the Amiga delivered there, too. PC users had to load up Windows 3.11, with its questionable cooperative multitasking, but the Amiga felt as smooth as ever. While waiting for a slow FTP download, I could chat with friends on IRC, play (simple) games and listen to music. A fast 486 with plenty of RAM could perhaps do the same, but not yet with my Amiga's inherent elegance.
</p>

<p>
Hence, the Amiga still felt like a superior platform. Its swift multitasking, efficient resource usage and <a href="https://www.datagubbe.se/ltmag/">many clever ideas</a> made both DOS and Windows feel clunky and primitive. Even though Windows 95 had entered the scene, it was more or less unusable without at least 8 megs of RAM. And all the fun stuff on PCs was <i>still</i> being made for DOS.
</p>

<h3>Doom me once, shame on you</h3>

<p>
What about Doom, then? John Carmack himself has allegedly said that he didn't consider the Amiga as being capable of running Doom. As an Amiga zealot, I'd of course like to point out that he was wrong - it's since been ported to the Amiga and runs just fine. But in my heart of hearts, I know that in 1993, he was actually right. The only Amiga available at that time powerful enough to make Doom palatable would have been an Amiga 4000 with a 25 MHz 68040 CPU, costing somewhere around $2500 - without a monitor.
</p>

<p>
Commodore was frequently derided for not producing yet another <i>killer computer</i>, a proper new Amiga model as revolutionary as the Amiga 1000 had been in 1985.  This complaint is somewhat valid, but still, I think, misses its target. It's not that the Commodore engineers didn't have plans - and even prototypes - for much more advanced graphics hardware than the AGA chipset in the A1200. There are just plenty of <i>other</i> reasons for why things might not have gone as expected, even if they had reached market.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/rnt.png">
<br>
<i>Ruff'n'Tumble on Amiga (1994): huge bosses, beautiful graphics and frantic action.</i>
</p>

<p>
VGA was deceptively simple. A framebuffer, more or less, which provided the famous <i>chunky</i> Mode 13h - part of what made Doom possible. No hardware sprites. No blitter for fast memory copying. No <i>copper</i> (co-processor) like the Amiga had, which could change the color of a given palette index every scanline. But you <i>could</i> write once to memory for a single given pixel, and get any color from an indexed palette of 256 values. And you could run spreadsheets in a steady, 60 Hz 640x480.
</p>

<p>
The Amiga had been designed when sprite-based games were the hottest thing since sliced toast, when memory was still stupendously expensive and when the ability to display 80 column text was considered a noteworthy feature on many home computers. As opposed to VGA, the Amiga had <i>planar</i> graphics, requiring multiple writes to memory to produce a single color value - a perfectly reasonable choice for the time, and one that enabled a lot of other nifty programming tricks and visual effects. The Amiga architecture was so tuned for arcade action that even in 1994, games like Ruff'n'Tumble showed that the 7 MHz Amiga 500 could still hold its own against powerful PCs when it came to fast paced 2D shooters. The problem was that after having dominated the market since first showing up in arcade cabinets, games in that style were becoming unfashionable.
</p>

<p>
By now it should be clear that what really drove the PC boom was neither Doom nor chunky graphics: it was cheaper and faster CPUs. Chunky-to-planar conversion on the Amiga does steal a few CPU cycles, but even if the A1200 had been equipped with a chunky mode, its 14 MHz 68020 processor would've been far too slow for Doom. Motorola's 486 equivalent, the 68040, hadn't been subjected to the price drops of mass produced PC clones and competition from rival manufacturers, such as AMD's and Cyrix' 486 compatible offerings. Put simply: Commodore could have crammed an actual VGA card into the A1200, but its CPU would still have been far too slow for Doom. And even if Doom had never materialized, an affordable Motorola CPU still couldn't crunch the numbers needed for increasingly complex simulators and strategy games.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/msfsim.png">
<br>
<i>Microsoft Flight Simulator 5.0 - tremendously slow even on the recommended minimum 386 CPU.</i>
</p>

<h3>The later 90's</h3>

<p>
Some time after 1995, I and many fellow Amiga zealots had started feeling an itch that was hard to scratch. It's true that this itch came, in part, from Doom. Not so much from wanting to play it, but from the desire to show the world that yes, the Amiga <i>was</i> in fact capable of running it. With the correct, expensive upgrades, mind you - but still.
</p>

<p>
Alas, the itch was also caused by other, more pressing matters. The remnants of Commodore had been bought by Escom and then Gateway, but no new Amiga models had materialized. Despite this, we had dutifully upgraded our CPUs, expanded our RAM, and kept true to our machines - but nothing of substance had materialized from the new owners of Commodore's IP.
</p>

<p>
There had been poor attempts at Doom clones on the A1200, but they all somehow lacked the parts of Doom that made it Doom. Surfing the net was now, to be honest, quite painful even on an accelerated Amiga with an expensive dual-sync monitor. The higher horizontal refresh rate (31 kHz) congested the AGA chipset and made 256 color screens unbearably slow. And all those JPEG files took forever to decompress! Some pages didn't look right, either: they were designed for Netscape, a program not available on our platform. In short, we were desperate for the launch of new Amiga hardware.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/iout.png">
<br>
<i>The pinnacle of Amiga zealot humor in the mid-90's. Hyuk, hyuk, ackshually...</i>
</p>

<p>
We were used to walkover victories in comparisons between computer brands, but it was now painfully obvious that our beloved Amiga was lagging behind. We didn't know how to respond and many of us considered remarks or even simple questions about our platform to be personal insults. This made us completely insufferable, and we spent inordinate amounts of time jeering both on- and offline about "Micro$oft", <a href="https://en.wikipedia.org/wiki/Pentium_FDIV_bug">buggy Pentiums</a>, cooperative multitasking and "Plug'n'Pray". Littering IRC with statements like "Windows 95 = Amiga 85" achieved nothing except making us look obnoxious. But teenage frustration mixed with sunk cost is a tough mix of emotions to combat: We had invested too much time, money and prestige to give up now. The Amiga would surely rise again! Except, of course, <i>damn you</i> if you even suggested the use of cheap Intel CPU:s at the core of the platform.
</p>

<p>
In 1997, the Doom source code was released and Amiga ports started cropping up. With a very expensive CPU expansion, it was perfectly playable on an A1200 - but nobody cared about Doom anymore. It was all Duke Nukem 3D and Quake now, and MP3 files, and fast Pentium MMX CPUs or even the impressive Pentium II and dedicated 3D graphics cards.
</p>

<p>
I was now running a 40 MHz 68040 CPU in my A1200, but even that couldn't keep up with the cheapest of PCs. DOS was more or less gone by now, and the fun stuff had started, little by little, to move into Windows.
</p>

<p>
In 1998, I finally caved and bought a powerful Pentium II PC. I was running a dual-boot system with both Windows 95 and Linux, but eventually Windows NT 4 won out. I could surf the web as well as anyone, play Quake III (after a graphics card update), program web pages using Microsoft's Personal Web Server, ASP and Access databases, chat with my friends on IRC and even watch DivX movies.
</p>

<p>
But there was still something missing. The fun just wasn't as fun on Linux and Windows. I guess a lot of people like me felt the same: even in 1999, the best demo scene stuff for PC was still clinging to DOS. Personally, I missed all the clever stuff my Amiga and its OS did, and the <a href="https://www.datagubbe.se/dopus/">great</a> and <a href="https://www.datagubbe.se/mkdem/">familiar</a> software it ran. 
</p>

<p>
So I just didn't stop using my Amiga. In the early 2000's, I upgraded to an impressively fast 50 MHz 68060 CPU, the last in Motorola's 68k series. I even bought an expensive, towerized A1200 with a 24-bit ("SVGA") graphics card and an ethernet card. It was ridiculously expensive and underpowered compared to any off the shelf PC available at the time - and yet, such a beefy Amiga was a paradox. It was a boyhood dream five or ten years too late - and also less of an Amiga than I perhaps cared to admit to myself at the time. The really fun stuff like Deluxe Paint, Amos and scene demos didn't run on the graphics card. It was more of a platform for running AmigaOS than an Amiga proper, and I eventually ended up downgrading to a more modest configuration. While it couldn't keep up as a daily driver, it was still a computer I booted up regularly, just to have <i>fun</i>.
</p>

<p>
It still is, and I still do.
</p>

<p>
<a href="https://www.datagubbe.se/afb/pics/anet.gif"><img src="https://www.datagubbe.se/afb/pics/anet.gif"></a>
<br>
<i>Network configuration on a souped up Amiga 1200, circa 2003.</i>
</p>

<h3>Custom Silicon</h3>

<p>
Doom didn't kill the Amiga - it was more like a measure of the many nails in the coffin of custom hardware home computer platforms. The biggest culprit was economies of scale.
</p>

<p>
Popular 8-bit home computers had, over time and quite naturally, been replaced by 16- and 32-bit machines. Many manufacturers of unique 8-bit machines during the Cambrian explosion of home computers simply went defunct or started making PC clones. By 1995, the only remaining, popular 32-bit home computer system was the PC. Apple regularly tried to muscle in on the home market, especially during the mid-90's with their PowerPC machines, but without much success. During this time period, a Mac was more of a niche office machine than the PC had ever been, and Apple survived mostly by selling systems for magazine and print ad production. They were in fact dangerously close to folding when Steve Jobs stepped back in and managed to secure funding from Microsoft (as "anti-trust insurance") and launch the iMac just in time to ride the wave of the Internet boom.
</p>

<p>
Commodore had been able to sell their successful 8-bit machines and early Amiga models cheaply thanks to vertical integration. This basically meant that they owned the chip fab, MOS Technology, that manufactured the Amiga's custom chips (and even the C64's 6502 CPU). At the time of its release, this made the original Amiga almost bizarrely cheap compared to equivalent Mac and PC offerings with similar performance.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/amad.jpg">
<br>
<i>A Swedish Amiga 4000T advertisement from 1997. For the listed SEK 29990 (roughly $3000 in today's exchange rate), you could get a fully specced out Pentium II machine - with a decent monitor - running in circles around the Amiga. If you forked out this much for an A4000 at this point in time, I dare say you were stupid. On the inside of your brain.</i>
</p>

<p>
But in 1994, when Commodore went bust, the PC clone market had been in full swing for over a decade. Cobbling together a 486 system using mass produced parts proved, in the end, to be far more competitive than designing, prototyping and manufacturing your own complex graphics and sound hardware for a single platform. Besides being cheap, this kind of standardization had more advantages. One was that PCs worked the same all over the world. Since traditional home computers were meant to work with television sets, they had to be timed to either the PAL or NTSC signal standard, which also meant that games (and other software) were timed to this as well. Games made for the European market didn't work on US Amigas without being rewritten, and vice versa. PCs didn't have this problem, providing a global market without added development cost.
</p>

<p>
Commodore's never completed <a href="https://en.wikipedia.org/wiki/Advanced_Amiga_Architecture_chipset">AAA</a> and <a href="https://en.wikipedia.org/wiki/Amiga_Hombre_chipset">Hombre</a> architectures sound impressive on paper, but were still not finished when the A1200 and A4000 were released in 1992. Besides, It's easy to list specs for nonexistent hardware in hindsight. Even if AAA had become what was promised, it wouldn't necessarily have been competitively priced. Even with the AGA machines, several cost cutting measures had been taken and <i>they still struggled</i> when it came to pricing. Would an even more advanced architecture somehow, magically, have sold as cheap or even cheaper? I have my doubts.
</p>

<p>
It's quite possible that the Commodore engineers could've worked a chunky mode into the AGA chipset. But, as discussed above, the A1200 would have needed a much faster CPU if Doom - or any other "killer app" game - was ever going to be a possibility. A 68040 would have been far too expensive, but even a 40 or 50 MHz 68030 CPU would've put the machine at a decidedly different price point. Combined with a hypothetical new graphics architecture, we can only speculate about the cost. The Amiga was known for being <i>good and cheap</i>, and it was proving hard not just for Commodore to combine the two.
</p>

<h3>Falcon Heavy</h3>

<p>
Many of Commodore's mistakes are said to have occurred after its founder, Jack Tramiel, left the company - even though the Amiga, a great success by all accounts, was bought by Commodore after his departure. Perhaps Tramiel's hardline approach to business ("Business is war!") and cost cutting ("Computers for the masses, not the classes!") could have led Commodore and (had they still bought it) the Amiga down a different path, but history tells us otherwise.
</p>

<p>
Tramiel left Commodore for Atari Corporation, a company that abandoned the home computer market in 1993 after their last-ditch effort, the Falcon 030. Like the A1200, the Falcon wasn't really a bad computer - but it cost even more than a souped-up Amiga 1200, and was still, in many aspects, underpowered compared to 486 PCs. It didn't come with the impressive specs listed for Commodore's AAA chipset, but its capabilities were far more advanced than those of the A1200. Its graphics chip, ViDEL, could produce a wide array of impressive resolutions, including a chunky 16-bit truecolor graphics mode. It had 16-bit sound, a faster CPU (16 MHz 68030), and a 32 MHz Motorola 56001 digital signal processor. This, together with both IDE, SCSI and networking hardware made for a very capable machine indeed.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/falc2.jpg">
<br>
<i>The Atari Falcon 030 still had the classic "home computer in a keyboard"
form factor.</i>
</p>

<p>
When considering all of this, the Falcon <i>was</i> actually competitively priced - but that didn't matter. People who needed SCSI or networking in 1993 usually had other budgets and priorities than home users, such as running WordPerfect or Lotus 1-2-3 for DOS. People who wanted great sound and graphics for gaming didn't want to pay extra for stuff they'd never use.
</p>

<p>
All those bells and whistles, together with the need for keeping the BLiTTER and YM2149F chips for backwards compatibility, meant the Falcon was too curious, inflexible and expensive for something that was supposedly a home computer. In the end, all its <i>killer architecture</i> managed to kill was, sadly, Atari itself.
</p>

<h3>Early birds and worms</h3>

<p>
What if Commodore had managed to put out a new, revolutionary Amiga model <i>much earlier</i> than 1992? Perhaps this could have saved the platform, ensuring its continued longevity? Maybe - but even if the Amiga 1000 launched in 1985, Amigas didn't become popular until the release of the cheaper, stabler and more mature Amiga 500 in 1987 - the same year IBM launched VGA. Could Commodore have released a VGA killer in 1988? As discussed in the beginning of this text - at this point in time, <i>the Amiga already was a VGA killer</i>. Consumer level PC:s just weren't fast enough to do something interesting with 256 colors, and VGA cards were much too expensive anyway. To keep a truly competitive cutting edge, the Amiga would have needed not just a new graphics architecture, but probably a CPU upgrade as well - raising the total cost of the machine considerably.
</p>

<p>
Commodore could surely have produced something very impressive, but again, at what price point - and would it have mattered? Most PCs sold at this time were still turbo XT clones with crappy CGA graphics and yet, PC dominance was already well established in the business sector. This also meant it had started seeping into homes where parents saw an opportunity of running spreadsheets and letting the kids play games and do their homework on the same machine, instead of buying two expensive family computers.
</p>

<p>
Commodore was often derided for their poor attempts at marketing the Amiga - but I think that's a bit unfair, too. The Amiga was launched at a huge press event where Andy Warhol and Debbie Harry famously appeared to show off the computer's graphics abilities. This was followed by print and TV ads that clearly and vividly showcased the advanced hard- and software. In Europe, Amigas sold like hotcakes and Commodore UK were very successful in bundling hardware with popular software titles. And, due to its graphics and video capabilities, the Amiga was a well known and popular machine in broadcasting circuits - in large parts thanks to the impressive <a href="https://en.wikipedia.org/wiki/Video_Toaster">Video Toaster</a>.
</p>

<p>
Could the Amiga have muscled in on the office market in a meaningful way? I honestly don't think so - Apple couldn't, Atari couldn't, in fact <i>nothing but IBM compatibles could</i>. The Amiga 3000 - a high end model often considered to be some of Commodore's finest work - is said to have piqued the interest of Sun Microsystems, who wanted to license it as a low end UNIX workstation. The deal fell through, much to the chagrin of Amiga fanatics across the globe - another oft-cited example of Commodore's failure as a company. Even so, Sun machines catered to a different niche market than IBM PCs, and while the notion of a mass produced Amiga UNIX workstation <i>sounds cool</i>, it's questionable if it would somehow have made consumer Amigas cheaper, faster and better - or simply led Commodore onto a path of expensive high end machines competing with the likes of SGI, HP and DEC. At the very least, considering it would've been running UNIX, it probably wouldn't have resulted in more resources allocated for AmigaOS development.
</p>

<p>
The real question here is, I think, if we would actually have wanted the Amiga to become yet another boring office machine. Everything that was fun and great about it was, to me at least, also what made business execs so suspicious of it.
</p>

<h3>Last breaths</h3>

<p>
With the demise of Commodore and Atari, the traditional home computer was basically dead. Acorn stayed in the game until 1998, but their expensive Archimedes line of machines was never very popular in actual homes, surviving by being more or less subsidized by British schools. Their subsequent RiscPC models catered mainly to various niche actors in broadcasting, including the BBC. Their legacy is now carried on by the popular Raspberry Pi computer.
</p>

<p>
Even highly specialized machines, such as Unix workstations, were living precarious lives by the end of the 90's. SGI, Sun, Digital and IBM focused their efforts more and more on servers and less on desktop machines. The last new Unix workstation models were both launched in 2006, by IBM and Sun. Even in this lucrative segment, competition from cheap PC hardware ultimately proved insurmountable.
</p>

<p>
With this in mind, a new <i>killer architecture</i> from Commodore may have been impressive, but <i>it might not even have been an Amiga as we know them</i>. It could have been a completely new machine. It could have been (mostly) compatible with the "classic" Amiga by somehow incorporating the old Amiga into the new one - still synced to PAL or NTSC, with all that entails, and of course adding cost to the machine. It could have become some kind of short-lived UNIX workstation or perhaps a games console. Or, it could have become a new Voodoo style graphics card for PCs - a platform that was also manufactured by Commodore, and quite profitably at that.
</p>

<p>
Both the Atari Falcon and Amiga 1200 were already suffering from minor problems with backwards software compatibility, something that probably made a lot of consumers more open to switching to PC. It's of course impossible to say for sure, but certainly not unthinkable, that the AAA architecture would have failed miserably to run the existing, bare metal banging games (and applications) an upgrading user would have expected to work on an "Amiga".
</p>

<p>
In short: Commodore might have lasted longer as a company - in some form - had they made other decisions. Such decisions, however, may not have been ones guaranteeing the continued existence of a platform recognizable as an Amiga. Apart from the Mac - in many ways thanks to Microsoft promising continued support and providing a $150 million cash injection - <i>no other home or desktop computer platform survived the 1990's PC dominance</i> in any meaningful way. It seems highly unlikely that the Amiga would somehow have propelled itself back into a significant market position thanks to a chunky graphics mode or, considering the fate of the Falcon 030, even a reworked architecture.
</p>

<h3>All but gone</h3>

<p>
The Amiga was an amazing platform, so far ahead of its time it stayed alive for much longer than what seems reasonable. It came out during the end of the Cambrian home computer explosion and remained in production for close to ten consecutive years. Saying it wasn't successful because of Commodore's lack of business savvy is doing it a disservice: Many, many millions of Amigas were sold and it was, for several years, the dominant home machine in Europe, where it shaped a generation of curious, capable and creative computer users.
</p>

<p>
It was, in fact, so popular and successful that even long after Commodore's demise, there were drawn-out efforts to modernize the platform. Here we can glean another of the many nails in the Amiga's coffin: the Amiga fanatics themselves. Most were now identifying so deeply with their platform that, say, suggestions of porting the operating system we loved to cheap and plentiful X86 hardware was considered heresy. It had to be PowerPC or nothing, despite the failure of the <a href="https://en.wikipedia.org/wiki/BeBox">BeBox</a> - which in many ways was more of a modern Amiga than any of the officially sanctioned attempts.
</p>

<p>
This eventually resulted in the AmigaOne series of PowerPC machines launched in the early 2000's. Keen supporters of the platform can nowadays purchase a motherboard with a 2 GHz dual core CPU for a mind-boggling $2000 (Yikes!). Add to that the cost for graphics, sound, memory and everything else. Spending that kind of money will get you a computer mostly useful for finding out why you don't want to run outdated ports of Linux software on an operating system without memory protection.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/a1.png">
<br>
<i>Pay stupid prices, win stupid hardware.</i>
</p>

<p>
It's not without irony that the once cheap, integrated, cutting edge Amiga platform has now become a ridiculously expensive PC-style kit computer, running an OS kernel that seems almost as primitive today as MS-DOS once did when we Amiga zealots smugly bragged about multitasking.
</p>

<p>
A humbling journey, to say the least.
</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vision Pro Teardown – Why those fake eyes look so weird (301 pts)]]></title>
            <link>https://www.ifixit.com/News/90137/vision-pro-teardown-why-those-fake-eyes-look-so-weird</link>
            <guid>39246664</guid>
            <pubDate>Sun, 04 Feb 2024 01:37:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ifixit.com/News/90137/vision-pro-teardown-why-those-fake-eyes-look-so-weird">https://www.ifixit.com/News/90137/vision-pro-teardown-why-those-fake-eyes-look-so-weird</a>, See on <a href="https://news.ycombinator.com/item?id=39246664">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
<figure><p>
<iframe title="Vision Pro Teardown: Behind the Complex and Creepy Tech" width="456" height="257" src="https://www.youtube-nocookie.com/embed/JVJPAYwY8Us?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>The strangest thing about the Vision Pro is also the thing that makes it most uniquely Apple: it’s got a big shiny bubble glass front, which makes it stand out from the aluminum- and plastic-shrouded competition, even when it’s off. And when it’s on, it’s even stranger—instead of being fully transparent, behind the glass, an odd lenticular screen displays a 3D-ish video of the user’s eyes, emulating their gaze. Apple calls it the EyeSight display, and when the user is looking at you, it kind of, sort of, almost looks like you can see through smokey glass.</p>



<p>Tech journalists have called EyeSight “<a href="https://www.businessinsider.com/apple-vision-pro-eyesight-feature-fans-reaction-2024-1">bizarre</a>,” “<a href="https://www.theverge.com/24054862/apple-vision-pro-review-vr-ar-headset-features-price">uncanny</a>,” and “<a href="https://daringfireball.net/2024/01/the_vision_pro">of highly dubious utility</a>.” But from a repair perspective, it seems like an achilles heel. Why introduce another screen, more connectors, and <em>so many </em>more points of failure—all for the sake of a slightly creepy feature? Of course, we had to dig in and figure out how it works.&nbsp;</p>



<p>We knew it would be tough to get inside (it was). We hoped we wouldn’t break anything (we did). But we knew it would be worth it to see all the new technology Apple squeezed into this thing, from the EyeSight display to the sensor array, the external battery back to the R1 chip. We brought in the heavy hitters for this teardown, including x-ray views of the frame and high-resolution microscope shots of the displays.&nbsp;</p>



<p>We’ve got a lot of observations, some opinions, and a couple educated guesses about <em>why</em> we got the Vision Pro we have today on the teardown table. There is a lot in this device, so we’re splitting our analysis into two, with more detail on the lens system and silicon coming in a few days.</p>



<p>Let’s go spelunking into a never-before-explored cave of glass.</p>



<figure><img decoding="async" fetchpriority="high" width="2000" height="1395" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094026/crop-AVP_TD_EDITED_32.jpeg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094026/crop-AVP_TD_EDITED_32.jpeg 2000w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094026/crop-AVP_TD_EDITED_32-1536x1071.jpeg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094026/crop-AVP_TD_EDITED_32-1290x900.jpeg 1290w" sizes="(max-width: 2000px) 100vw, 2000px"></figure>



<figure><img decoding="async" width="7162" height="4029" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094617/AVP_TD_EDITED_46-2.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094617/AVP_TD_EDITED_46-2.jpg 7162w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094617/AVP_TD_EDITED_46-2-1536x864.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094617/AVP_TD_EDITED_46-2-2048x1152.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094617/AVP_TD_EDITED_46-2-1600x900.jpg 1600w" sizes="(max-width: 7162px) 100vw, 7162px"></figure>



<p>The glass panel is glued on, of course, and it took a <em>lot</em> of heat and time, but we removed it without breakage. Granted it didn’t come out unscathed—the glass has a protective plastic film that got a little peeled up and maybe a bit melted. Apple’s retail fixers <em>might</em> have faster hands than us—but they’ll <a href="https://support.apple.com/apple-vision-pro/repair">charge you $799</a> to replace broken front glass.&nbsp;</p>



<figure><img decoding="async" loading="lazy" width="7059" height="3971" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094405/AVP_TD_EDITED_55-1.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094405/AVP_TD_EDITED_55-1.jpg 7059w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094405/AVP_TD_EDITED_55-1-1536x864.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094405/AVP_TD_EDITED_55-1-2048x1152.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094405/AVP_TD_EDITED_55-1-1600x900.jpg 1600w" sizes="(max-width: 7059px) 100vw, 7059px"></figure>



<figure><img decoding="async" loading="lazy" width="2000" height="1500" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03102917/glass-only-edited.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03102917/glass-only-edited.jpg 2000w, https://valkyrie.cdn.ifixit.com/media/2024/02/03102917/glass-only-edited-1536x1152.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03102917/glass-only-edited-1200x900.jpg 1200w" sizes="(max-width: 2000px) 100vw, 2000px"></figure>



<h3><a href="https://drive.google.com/drive/folders/1pSsMhfg7AtK_QktOMHv41PAHvhhDs7kS"><strong></strong></a><strong>Heavy Metal</strong></h3>



<p>At 34 grams, the glass may not be heavy on its own, but fully kitted out with the battery the Vision Pro weighs over a kilogram.&nbsp;</p>



<p>Here’s where Apple has a performed a bit of a sleight of hand. Carefully hidden in most publicity shots is the external battery, which rides along in your pocket rather than on your headset. As in the early days of VR, integrating the battery as it is now would make the device crazy heavy.&nbsp;And hey, we’re big fans of modular batteries, when the battery inevitably stops holding a charge in <a href="https://batteryuniversity.com/article/bu-801b-how-to-define-battery-life">a year or three</a>, you can replace it painlessly. Apple’s hardware team may also be anticipating the <a href="https://repair.eu/news/big-win-for-right-to-repair-with-new-eu-rules-for-batteries-but-legislators-must-get-the-implementation-right/">upcoming EU battery regulation</a>, which will require all electronics to have user-replaceable batteries by 2027. </p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-1536x1025.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-2048x1367.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-1349x900.jpg 1349w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"></figure>



<p>The battery pack alone weighs 353 grams and is made of three iPhone-sized batteries, delivering a grand total of 35.9 Wh, more than double an iPhone 15 Pro’s 17.3 Wh. The cells themselves are 184 g apiece, surprisingly only about half the weight of the full battery pack. To get inside, we had to soften some perimeter adhesive and release a set of single-use metal clips—then twist open Torx screws galore.<a href="https://drive.google.com/drive/folders/1pSsMhfg7AtK_QktOMHv41PAHvhhDs7kS"></a></p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-1536x1025.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-1349x900.jpg 1349w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-450x300.jpg 450w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77.jpg 2000w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption>Three batteries in the aluminum pack, at ~3.8V each in series, 3166 mAh each, supplying 11.34 Volts in total.</figcaption></figure>



<p>Add the weight of the battery pack and the headset together and you get, as mentioned above, over a kilogram—which would be a really heavy pair of glasses. For comparison, the Quest Pro weighs 722 g and the Quest 3 clocks in at 515 g.</p>



<p>But weight isn’t just about how it tips the scales. It’s about balance. The weight of the Vision Pro largely rests on your face, all the tech is at the front and even the Pro Dual Loop Band can’t overcome it all without a counterbalance. <a href="https://www.patentlyapple.com/2023/11/apple-invents-a-battery-mount-on-the-back-of-vision-pros-headband-to-help-counterbalance-the-weight-of-the-hmd-that-causes-n.html">Apple patented a design</a> for a rear-mounted battery pack, which might’ve helped balance out the heavy front—though it’s hard to imagine wanting to wear something 150% as heavy.&nbsp;</p>



<p>So if we’re just counting the weight on your face—the display module, sans battery, in the Meta Quest Pro is 522 grams. The same assembly in the Vision Pro is 532 grams, effectively the same. The key difference in these units is in the weight distribution, and a much heavier pocket battery in the Vision Pro.</p>



<figure><img decoding="async" loading="lazy" width="2158" height="1319" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03102021/ifixit-the-vision-pro-is-not-as-heavy-as-you-think.png" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03102021/ifixit-the-vision-pro-is-not-as-heavy-as-you-think.png 2158w, https://valkyrie.cdn.ifixit.com/media/2024/02/03102021/ifixit-the-vision-pro-is-not-as-heavy-as-you-think-1536x939.png 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03102021/ifixit-the-vision-pro-is-not-as-heavy-as-you-think-2048x1252.png 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03102021/ifixit-the-vision-pro-is-not-as-heavy-as-you-think-1472x900.png 1472w" sizes="(max-width: 2158px) 100vw, 2158px"></figure>



<p>First impressions, though, are pretty good. “The weight isn’t as bad as expected, although it’s definitely on my forehead/cheeks as opposed to my head which feels weird, like someone is pushing on my head to tilt it down,” says iFixit teardown veteran <a href="https://www.ifixit.com/User/524640/Sam+Goldheart">Sam Goldheart</a> from the teardown lab.</p>



<h3><strong>Headbands</strong></h3>



<p>The Vision Pro comes with both a 3D-knitted Solo Knit Band and a Dual Loop Band. These attach to the ends of the stems, just behind the speakers. The now-iconic Solo Knit Band is the one that seen in all the publicity shots, and it does look cool. It wraps around the back of your head, and you adjust the fit with a dial on the side, similar to how you might tighten a bike helmet.&nbsp;</p>



<p>So how does it feel? “The fabrics are sooo nice,” says Sam. There’s a very fine, cushy weave on the Solo Knit Band, and it is stretchy enough to accommodate a ponytail and still support the face unit.&nbsp;&nbsp;</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1.jpg"><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p>The speakers are fixed onto the two rigid bands that join to the main headset. To release these, you use our old friend, the SIM-card removal tool. The holes are inside the temples of the main headset, and the removable bands have a row of electrical contacts, just like Lighting connectors, again. Easily removable parts? Only demands tools you’ve probably already got? We love to see it. This makes us hope that opening the headset might not be as daunting as we first assumed.<a href="https://drive.google.com/drive/folders/1pSsMhfg7AtK_QktOMHv41PAHvhhDs7kS"></a></p>



<p>This modular design is similar to the <a href="https://www.ifixit.com/Teardown/AirPods+Max+Teardown/139369">AirPods Max</a>, which we quite liked. Wearables are so easy to damage that it makes good sense to have easily swappable speaker modules. We tried to go further and pry the speaker out of the silicon frame, and instantly broke the molded cables inside. That’s all right, you’re not going to need to pry the speaker modules open. </p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption>The speakers—not quite as hard to get into as a pair of AirPods Pro, but almost.</figcaption></figure>



<p>The speakers themselves point back towards your ears. This is a pretty clear indication that you’re not meant to wear this anywhere noisy. You can wear your AirPods Pro if you prefer—and if you want lossless, low-latency audio, they’ll have to be the latest USB-C version.</p>



<p>On the left side is the proprietary battery cable connection, which snaps into place with a magnet and then twists to lock. We understand why Apple used a non-standard connector here, even if we don’t love it—at least it can’t be yanked out by a passing child, or when the cord inevitably catches on your chair. But the plug at the other end of the cable is unforgivable. Instead of terminating with a USB-C plug, it connects to the battery pack with what looks like a proprietary oversized Lightning connector, which you release using a paperclip or SIM-removal tool.</p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption>The locking design is great, but why couldn’t it be USB-C Apple? WHY?</figcaption></figure>



<p>This connector means that you can’t just swap in the USB-C battery pack you already own. Lame.</p>


<div>
<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption>Aww, such a cute family snap!</figcaption></figure></div>


<h3><strong>Light Seals and Face Cushions</strong></h3>



<p>Every face is different, and Apple is selling<a href="https://www.reddit.com/r/VisionPro/comments/19ardw5/all_light_seal_sizes/">&nbsp;28 different light-seal parts</a> to cover all the different face sizes and shapes. Your seal size also changes if you need Zeiss lens inserts. That’s because the seals and cushions are also used to make sure you have the correct eye position relative to the stereo screens and eye sensors. This is why Apple is hand-packing every Vision Pro order—there’s just no “standard” setup. </p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption>Guess who was wearing makeup?</figcaption></figure>



<p>The seals attach to the main headset using magnets, which is Apple through-and-through—it’s either glued in place, or extremely easy to swap. This modularity is a brute force attempt to get an ideal fit on your face. It will be interesting to see if this is required long-term, or if future devices find a simpler way to accomplish this. For the time being, magnets are better than velcro because they can snap the seals into exact alignment. Think how MagSafe snatches the charger and lines it up perfectly over the iPhone’s inductive charging coil.</p>



<p>As for cleaning the seals, <a href="https://support.apple.com/en-us/HT213964">Apple recommends</a> water and unscented dish soap, which will help stop these sweat-soaking parts from getting too gross, and will be especially good for anyone wearing makeup. In her <a href="https://www.wsj.com/video/series/joanna-stern-personal-technology/vision-pro-review-24-hours-in-apples-mixed-reality-headset/05CD2E77-897D-49A9-A87E-9B8A93E3E45F">Wall Street Journal video</a> where she selflessly wore the headset for 24 hours, Joanna Stern said her makeup caked the inside of the seals. And our own Sam Goldheart had the exact same problem this morning.</p>



<p>Under the magnetic seals is a permanent seal, also wrapped in a knit fabric, but less likely to get smudged. It also happens to be the way into the interior of the headset. Removing it reveals another surprise: a thin stretchy sheet of plastic. Whether it’s to compensate for gaps in the knit, or to keep particulates out of the innner workings, we’re not to sure. But we are certain this bit looks <em>very</em> masked superhero.</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim.jpg"><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<h3><strong>EyeSight Display</strong></h3>



<p>The front-facing gogglebox is the defining feature of the Vision Pro, and, now that reviews are pouring in, one of its most controversial.</p>



<p>The <a href="https://patentimages.storage.googleapis.com/51/03/12/4b7f034c90465c/US20240012601A1.pdf">patent for the EyeSight</a> describes three display modes: “internal focus,” “external engagement,” and “do not disturb.” The patent has pages and pages of images that might be displayed on the screen—all kinds of cartoon animal eyes, biometric analysis captured by other sensors, hearts when the user is talking to a loved one. The internal camera might read emotional states and project images based on those emotional states.</p>



<figure><img decoding="async" loading="lazy" width="1906" height="766" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03085115/pasted-image-0-1.jpeg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03085115/pasted-image-0-1.jpeg 1906w, https://valkyrie.cdn.ifixit.com/media/2024/02/03085115/pasted-image-0-1-1536x617.jpeg 1536w" sizes="(max-width: 1906px) 100vw, 1906px"></figure>



<p>Cool thought. In practice, the EyeSight display is so dim and low-resolution that reviewers say it’s hard to see much on it. The WSJ’s Joanna Stern <a href="https://www.youtube.com/watch?v=8xI10SFgzQ8&amp;t=211">called it</a> “hard to see,” and Marques Brownlee (aka MKBHD)<a href="https://www.youtube.com/watch?v=dtp6b76pMak&amp;t=1826"> said</a>, “You can barely see my eyes when I’m wearing the headset.”&nbsp;</p>



<p>It turns out that when the EyeSight displays your eyes, it isn’t just displaying a single video feed of your eyes; it’s showing a <em>bunch</em> of videos of your eyes. Exploring inside the glass shell, we found three layers for the front-facing display: a widening layer, a lenticular layer, and the OLED display itself.</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103.jpg"><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p><a href="https://drive.google.com/drive/folders/1pSsMhfg7AtK_QktOMHv41PAHvhhDs7kS"></a><strong>Why Does EyeSight Look So Wonky?</strong></p>



<figure><img decoding="async" loading="lazy" width="3514" height="960" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03115254/Lenticular-explainer-1.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03115254/Lenticular-explainer-1.jpg 3514w, https://valkyrie.cdn.ifixit.com/media/2024/02/03115254/Lenticular-explainer-1-1536x420.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03115254/Lenticular-explainer-1-2048x559.jpg 2048w" sizes="(max-width: 3514px) 100vw, 3514px"></figure>



<p>Apple wanted to achieve something very specific: an animated, 3D-looking face with eyes. They had to make very strategic design choices and compromises to accomplish this.</p>



<p>Human brains are very sensitive to faces and expressions, its why the uncanny valley is a thing, and part of that is depth sensing. Apple needed to create a believable 3D effect. One reason why 3D renderings don’t look truly 3D is because they lack a stereoscopic effect. For something to look 3D, we need to see subtly different images with each eye. The Vision Pro tackles this problem with lenticular lenses.</p>



<figure><video controls="" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03035750/card-movement.mp4"></video></figure>



<p>A lenticular lens displays different images when viewed from different angles. You can use this effect to simulate movement with two frames of an action. Or, you can create a stereoscopic 3D effect with images of the same subject from different angles.</p>



<p>The Vision Pro has a lenticular layer on top of the exterior OLED panel. VisionOS renders multiple face images—call them A and B—slices them up, and displays A from one angle serving your left eye, and  B from another serving your right eye. This creates a 3D face via the stereoscopic effect. And those angles are tiny, and they are legion, it takes a fancy <a href="https://www.evidentscientific.com/en/">Evident Scientific microscope</a> to really see what we mean.</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03035754/AVP_017.jpg"><img decoding="async" loading="lazy" width="2000" height="1106" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03035754/AVP_017.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03035754/AVP_017.jpg 2000w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035754/AVP_017-1536x849.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035754/AVP_017-1627x900.jpg 1627w" sizes="(max-width: 2000px) 100vw, 2000px"></a><figcaption>The curved ridges of the lenticular lens layer.</figcaption></figure>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03120833/EyeSight-no-filter.jpg"><img decoding="async" loading="lazy" width="1880" height="1040" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03120833/EyeSight-no-filter.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03120833/EyeSight-no-filter.jpg 1880w, https://valkyrie.cdn.ifixit.com/media/2024/02/03120833/EyeSight-no-filter-1536x850.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03120833/EyeSight-no-filter-1627x900.jpg 1627w" sizes="(max-width: 1880px) 100vw, 1880px"></a><figcaption>Pixels bending and shining through the lenticular layer</figcaption></figure>



<p>There are compromises to this approach. The horizontal resolution is dramatically reduced, being divided between each of the multiple images. For example, if two images are displayed on a 2000 pixel wide display, each image only has 1000 horizontal pixels to work with. Even through we don’t know the resolution of the display, nor do we know the number of images being interwoven, the resolution is necessarily reduced. And that is a major reason why EyeSight eyes seem blurry.</p>



<p>In front of the lenticular layer is another plastic lens layer, with similarly lenticular ridges. This layer appears to stretch the projected face wide enough to fit the width of the Vision Pro. Removing this layer and booting the Pro showcases some very oddly pinched eyes.</p>



<figure><video controls="" poster="https://valkyrie.cdn.ifixit.com/media/2024/02/03044324/dsc_7495.mov.00_00_27_13.still002_720.jpg" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03103510/Display-Stack_sm.mp4"></video></figure>



<p>Additionally the lens likely limits the effective viewing angle. Limiting the effect to directly in front of the Vision Pro limits artifacting you might see at extreme angles, sort of like a privacy filter. The downside is that you’re passing an already complex, blurry image through yet another layer of lens. This makes it even blurrier and darker.</p>



<h3><strong>Lens Inserts, Stereo Displays</strong></h3>



<p>You can see the outline of the ovoid lens inserts in this x-rays from our illuminous friends at <a href="https://creativeelectron.com/">Creative Electron</a>, who spent $3,500 so you could see this photo.</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray.jpg"><img decoding="async" loading="lazy" width="3546" height="1992" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray.jpg 3546w, https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray-1536x863.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray-2048x1150.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray-1602x900.jpg 1602w" sizes="(max-width: 3546px) 100vw, 3546px"></a></figure>



<p>The Vision Pro itself performs an <a href="https://support.apple.com/guide/apple-vision-pro/important-safety-information-c0c84db82a44/visionos">automatic interpupillary distance adjustment</a> when you first put it on, with motors adjusting the positioning of the lenses. For everything else there’s prescription lenses. </p>



<p>Apple Stores have a machine to determine approximate prescription glasses strength when you come in for a demo.  For users with eye conditions (like strabismus) that might interfere with eye tracking, the Vision Pro offers <a href="https://support.apple.com/en-us/HT213965">alternative interaction controls</a> in the accessibility features. However, we have heard that lenses are not available for people who have astigmatism, which is <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10045990/">40% of the population</a>. If you know anything more about that, leave it in the comments.&nbsp;</p>



<p>The prescription insert lenses themselves require “pairing” with the headset. The decision has already borne poor UI, John Gruber received an <a href="https://daringfireball.net/2024/01/the_vision_pro">incorrect calibration code</a> with his review unit that made eye tracking perform poorly. <a href="https://www.ifixit.com/News/82867/iphone-15-teardown-reveals-software-lockdown">We hate parts pairing</a> on principle, and there’s got to be a way to enable calibration while still allowing third party lenses.</p>



<p>Oh, and Creative Electron was bored after one photo so they shot us a 360 spin. Sweet!</p>



<figure><video autoplay="" controls="" loop="" preload="auto" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03130232/360-color-4.mp4"></video></figure>



<h3><strong>R1 and M2 Chips</strong></h3>



<p>The headset runs on an M2 Mac chip, in tandem with the new R1 chip—which is specifically responsible for processing the input from 12 cameras, the LiDAR sensor, and the TrueDepth camera, all with a minimum of latency. With AR, you need to project the camera view of the real world into the user’s eyes as fast as possible, otherwise their perceived motions won’t match up with what they see, which is a fast ticket to Vomitsville.&nbsp;</p>



<p>To keep up, the R1 <a href="https://www.theverge.com/2023/6/5/23733874/apple-vision-pro-visionos-augmented-reality-os-specs-wwdc-2023">uses</a> a <a href="https://en.wikipedia.org/wiki/Real-time_operating_system">real-time operating system</a>. That means that tasks are always executed in a fixed amount of time. Most of our computers run on a time-sharing operating system, which schedules tasks on the fly, and can result in slowdowns. Think about jittery mouse cursors, or spinning beach balls, and you’ve got the idea. That won’t fly with something as critical as pass-through video and object rendering. Any glitch there would be like a glitch in the Matrix, and would be jarring at best, and utterly nauseating at worst. It might even cause you to stumble and fall.</p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"></figure>



<h3>An Incredible Feat, With One Really Weird Design Decision</h3>



<p>The original iPhone did something similar. When its underpowered chips couldn’t keep up with rendering a fast-scrolling page, it would <a href="https://daringfireball.net/2008/10/iphone_3g">switch to a gray-and-white checkerboard</a>, which kept up with all your flicks and swipes. Apple prioritized responsiveness over graphical fidelity. This time around, they have prioritized graphics fidelity <em>and</em> responsiveness, and taken the hit on battery life, weight, and heat. Given how important the experience is to Apple’s AR experience, this is probably the right choice for a first generation device.</p>



<p>The Vision Pro is insanely ambitious. Yes, it’s heavy, and the glass is fragile, and that tethered battery might get annoying. But Apple has managed to pack the power of a Mac, plus the performance of a new dedicated AR chip, into a computer that you can wear on your face.</p>



<p>Repairability-wise, it’s not great, but on the plus side, some of the connections are quite delightful. You should have seen our teardown team jump up when they realized that the side arms could be popped out using the SIM-removal tool, for example, and the magnetic cushions are yet more user-friendly.</p>



<p>So why, when this thing clearly took years and years to create—and is Apple’s latest bet on the future of computing—did Apple fail to live up to their own standards with the EyeSight screen? </p>



<p>It’s dim, it’s low-resolution, and it adds a lot of bulk, weight, complexity, and expense to the most weight-sensitive part of the headset. Did they finally hit the drop dead date and miss their targeted performance? Could it be a late-stage manufacturing error? Regardless, we’re sure bringing it to market was a difficult decision.</p>



<p>We’ve been disassembling VR headsets since the original <a href="https://www.ifixit.com/Device/Oculus_Rift">Oculus</a>, and they continue to surprise and delight. There is so much fascinating mechanical and optical design packed in here. Apple’s seamless integration of sensors for rock-solid location tracking is just phenomenal, and we’re eager to dive into how they did it.</p>



<p>We’re not done with our analysis: there’s lots more to investigate inside this device. Next time, we’ll dive into the internal displays, sensor arrays and we’ll award a repairability score. </p>



<p>What else are you excited to see? IPD calibration motors, cooling, specific chips or circuitry? Follow along on social media, or check back here in a few shakes, we’ve got plenty more coming.</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1.jpg"><img decoding="async" loading="lazy" width="7360" height="4136" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1.jpg 7360w, https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1-1536x863.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1-2048x1151.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1-1602x900.jpg 1602w" sizes="(max-width: 7360px) 100vw, 7360px"></a></figure>
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nuclear power saved Armenia (130 pts)]]></title>
            <link>https://thebulletin.org/2024/01/how-nuclear-power-saved-armenia/</link>
            <guid>39246563</guid>
            <pubDate>Sun, 04 Feb 2024 01:20:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thebulletin.org/2024/01/how-nuclear-power-saved-armenia/">https://thebulletin.org/2024/01/how-nuclear-power-saved-armenia/</a>, See on <a href="https://news.ycombinator.com/item?id=39246563">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span><picture title="" decoding="async" loading="lazy">
<source type="image/webp" srcset="https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-1024x768.jpg.webp 1024w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-300x225.jpg.webp 300w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-768x576.jpg.webp 768w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-1536x1152.jpg.webp 1536w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-493x370.jpg.webp 493w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image.jpg.webp 1920w" sizes="(max-width: 1024px) 100vw, 1024px">
<img width="1024" height="768" src="https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-1024x768.jpg" alt="Armenia's Metsamor nuclear power plant cooling towers. (Credit: Adam Jones via Wikimedia Commons. CC BY-SA 2.0)" decoding="async" loading="lazy" srcset="https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-1024x768.jpg 1024w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-300x225.jpg 300w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-768x576.jpg 768w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-1536x1152.jpg 1536w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-493x370.jpg 493w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px">
</picture>
Armenia's Metsamor nuclear power plant cooling towers. (Credit: Adam Jones via Wikimedia Commons. CC BY-SA 2.0)</span></p><div><p>The world is currently in the process of reevaluating its past rejection of nuclear power and is increasingly starting to view it as a reliable source of power that allows for greater energy security. This is at least in part due to the energy crisis that befell Europe after the Russian invasion of Ukraine in 2022, vindicating past worries that over-reliance on fossil fuels from autocratic regimes has made the Western countries vulnerable to political blackmail.</p>
<p>It is now clear that Western use of natural gas and petroleum from aggressive dictatorships—which use cash flows from oil and gas sales to reinforce and expand their hold on power—has backfired badly. In this context, the experience of Armenia—a small country that draws 40 percent of its energy from nuclear power—is instructive, showing how nuclear power can be instrumental in building societal reliance and political stability.</p>
<p><strong>Living in the dark cold.</strong> It is the winter of 1992–1993. As I climb the dark stairs in a freezing-cold Soviet apartment building in Yerevan, the capital of Armenia where my family and I live, the water from the two full buckets I carry is splashing down my legs and freezing on the stairs. My sister Shooshan and I, 14 and 15, are carrying water up to our 11<sup>th</sup>-floor apartment. The water to our apartment shut off weeks ago, and we get at most one hour of electricity each day. I estimate that we need exactly seven gallons of water, if we are careful, for our basic daily needs. So, we repeat the trip every day. During the precious hour when we do get electricity, my mother rushes to the kitchen to cook food for the next 24 hours. I run to the bakery, where I stand in a long queue to buy the half pound of bread that the state has rationed for each one of us.</p>
<p>The daily routine, which goes on for the whole winter, is exhausting. But it is also empowering. As teenagers we feel that we are stronger than the disastrous conditions inflicted on us by the combination of the Soviet collapse, the Nagorno-Karabakh war, and the ensuing severe energy crisis.</p><div id="thebu-1161222088"><p><a href="https://thebulletin.org/doomsday-clock/?utm_source=Website&amp;utm_medium=MobileMediumRectangle&amp;utm_campaign=2024DDCAnnouncementWebAds&amp;utm_content=DoomsdayClock_WebsiteAd_01222023#nav_menu" target="_blank" aria-label="2024 Doomsday Clock – Mobile Medium Rectangle"><img loading="lazy" decoding="async" src="https://thebulletin.org/wp-content/uploads/2024/01/2024-Mobile-Banner-Ads-3-1.png" alt="" srcset="https://thebulletin.org/wp-content/uploads/2024/01/2024-Mobile-Banner-Ads-3-1.png 600w, https://thebulletin.org/wp-content/uploads/2024/01/2024-Mobile-Banner-Ads-3-1-300x250.png 300w, https://thebulletin.org/wp-content/uploads/2024/01/2024-Mobile-Banner-Ads-3-1-444x370.png 444w" sizes="(max-width: 600px) 100vw, 600px" width="300" height="250"></a></p></div>
<p>The reasons that my sister and I—and the thousands of other Armenian teenagers like us—had to lug water and plan their lives around the one hour of electricity during that cruel winter go back to the turbulent events that shook Armenia during the preceding decades.</p>
<figure id="attachment_83863" aria-describedby="caption-attachment-83863"><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://thebulletin.org/wp-content/uploads/2021/03/Armenia-map-image-279x300.jpg.webp 279w, https://thebulletin.org/wp-content/uploads/2021/03/Armenia-map-image.jpg.webp 326w" sizes="(max-width: 281px) 100vw, 281px">
<img loading="lazy" decoding="async" src="https://thebulletin.org/wp-content/uploads/2021/03/Armenia-map-image-279x300.jpg" alt="Map of Armenia. Credit: The World Factbook 2021. Central Intelligence Agency." width="281" height="302" srcset="https://thebulletin.org/wp-content/uploads/2021/03/Armenia-map-image-279x300.jpg 279w, https://thebulletin.org/wp-content/uploads/2021/03/Armenia-map-image.jpg 326w" sizes="(max-width: 281px) 100vw, 281px">
</picture>
<figcaption id="caption-attachment-83863">Map of Armenia. Credit: The World Factbook 2021. Central Intelligence Agency.</figcaption></figure>
<p>In the 1970s and 1980s, the Soviet Union—which Armenia was part of—rapidly expanded its fleet of nuclear reactors to support its growing industrial energy needs. As a result, two pressurized water reactors (PWR) of the Soviet VVER-440 type were built in the Armenian town of Metsamor, about 30 kilometers west of Yerevan. Started in 1977 and 1980, respectively, the two reactors quickly covered more than half of the energy needs of the Armenian Soviet Socialist Republic. (The remainder of the electricity was generated by Armenia’s hydroelectric stations and gas-fired power plants.) The Armenia of the 1980s was a tiny but prosperous Soviet republic that prided itself in a highly educated labor force, an array of scientific institutes, and a vibrant electronics industry that produced some of the early Soviet computer mainframe designs.</p>
<p>A series of violent events during the collapse of the Soviet Union would dramatically alter the Armenian dream.</p>
<p><strong>Chernobyl. </strong>On April 26, 1986, one of the Soviet-designed, graphite-moderated RBMK reactors at the Chernobyl nuclear power plant underwent a catastrophic power excursion that ripped the reactor open. The explosion and fire that followed propelled an enormous amount of radioactive matter into the open atmosphere leading to what is now known as the Chernobyl nuclear disaster, with widespread radioactive contamination, hundreds of deaths from acute radiation poisoning, and likely thousands of additional deaths due to radiation-induced cancers in the months and years that followed.</p>
<p>The Chernobyl accident resonated worldwide, dramatically undermining public trust in nuclear power as a safe source of energy. The public perception of danger from nuclear power was magnified by the outrageous lies that the Soviet leadership spread about the disaster, the obvious incompetence and irresponsibility of the Soviet nuclear designers who built and operated the Chernobyl reactor, and the poorly executed cleanup efforts which were compounded by miscalculations and gross mistakes.</p>
<p>Overnight, citizens across the Soviet Union and beyond went from a blissful ignorance about radiation to an understandable—yet irrational—fear of anything radiation-related. People in Armenia, despite living more than 2,000 kilometers away from Chernobyl, started perceiving radioactive threats everywhere, often attributing many of their common ailments to radiation. Physicists, like my parents, tried to explain what radiation is and how natural doses of radiation are not dangerous. But their advice was sometimes met with hostility: Weren’t the builders of Chernobyl also scientists?</p>
<p>In one chilling conversation that I witnessed at a dinner party, one of the guests told my father only half in jest, “You physicists… you should all be shot!” To paraphrase Valery Legasov’s eponymous character from HBO’s <a href="https://www.nytimes.com/2019/05/03/arts/television/review-chernobyl-hbo.html">five-part mini-series “Chernobyl”</a>: The danger of the lies is not that we mistake them for the truth, but that when enough lies are told we lose hope in the truth and start believing in stories. (Legasov was a Soviet chemist who actively worked on the causes and consequences of the Chernobyl disaster. <a href="https://vtoraya-literatura.com/pdf/radio_liberty_report_on_the_ussr_vol01_14_1989__ocr.pdf">Concerned</a> by the lack of nuclear safety in the Soviet nuclear industry, he died by suicide on April 27, 1988.)</p>
<p><strong>An earthquake, the Soviet collapse, and war.</strong> In December 1988, the devastating earthquake of Spitak killed 50,000 people—a harrowing 2 percent of Armenia’s population—and destroyed most of the country’s infrastructure. The two VVER-440 reactors at Metsamor were suddenly in the public eye. Would another earthquake rip them open and turn Armenia’s heartland, where half of Armenia’s population lived, into a Chernobyl-like radioactive wasteland?</p>
<p>To be clear, the PWRs at Metsamor are safer than the shoddily designed, graphite-moderated reactors at Chernobyl. Metsamor’s Soviet reactor design is close to the standard PWR designs that are still the most common reactor technology used in Western countries. And the buildings and the reactor structures were reinforced to account for Armenia’s seismic activity. But none of that mattered. After the Soviet government’s grotesque lies about the Chernobyl disaster, the official assurances that the Metsamor reactors were safe did not convince many. Legasov’s intuition was right: The pursuit for truth was replaced with belief in conspiratorial rumors. An environmentalist movement sprang up, calling for the shutdown of the Metsamor reactors. The authorities backed down, and the two reactors were turned off on February 25 and March 18, 1989.</p>
<p>Shortly after the shutdown, the Soviet Union started to crack, finally collapsing in 1991. In neighboring Azerbaijan, an Armenian minority living in the mountainous Nagorno-Karabakh region, feeling marginalized and discriminated against, had long been fighting to protect their civil rights. With the weakening of Soviet power, the protest movement turned into demands for secession from Azerbaijan. The response in Azerbaijan was <a href="https://armenian.usc.edu/baku-pogroms-in-context-of-the-karabakh-conflict/">a series of brutal anti-Armenian pogroms</a> in the cities of Sumgait and Baku that killed hundreds of Armenian civilians and forced about 300,000 others to flee the country. Fearing retaliation, the Azeri civilians living in Armenia fled en masse to Azerbaijan.</p>
<p>A relatively peaceful political disagreement had suddenly turned into a violent conflict, with Azerbaijan’s pogroms against Armenians escalating to a total war against the Armenian people of Nagorno-Karabakh. As the Armenian government supported the Nagorno-Karabakh secessionists, Azerbaijan retaliated by shutting off some of the natural gas pipelines that led to Armenia. In a sense, Azerbaijan’s authorities did to Armenia what Russian President Vladimir Putin is now doing to Western European countries that support Ukraine’s war effort. With its nuclear reactors and natural gas supply shut down, Armenia was left with a reduced capacity to generate electricity.</p>
<p>Then came the winter of 1992–1993. Mountain rivers froze, hydroelectric dams dried up, and suddenly hydropower too was nearly gone. Armenia was getting barely a trickle of electricity. What followed is a period now known in Armenia as “tsurt u mut tariner,” literally the cold and dark years: severe shortages of electricity, freezing concrete apartment complexes, closed schools, and many other disruptions. The economy collapsed, with Armenia’s gross domestic product contracting by an estimated 50 to 80 percent between 1990 and 1993. Then, a massive exodus followed, shrinking Armenia’s population by a quarter in just a few years.</p>
<p><strong>Nuclear power revival.</strong> The Armenian public quickly realized that, by abandoning nuclear power, it had forfeited the country’s energy independence. That vulnerability was—and still is—very effectively leveraged by its arch-enemy Azerbaijan. Was it too late to restore nuclear power?</p>
<p>Understanding their mistake, the Armenian authorities re-evaluated their past decision. The choice was stark: Either indulge in exaggerated fears of radiation and face unpredictable consequences, or sober up and accept nuclear power as a lesser evil. Ultimately the government chose the sober option. But rather than rushing headfirst to hastily restart the Metsamor nuclear power plant, the authorities decided to make significant safety improvements to the reactors.</p>
<p>One of the Metsamor reactors finally restarted on November 5, 1995, just before the winter season. The desperately needed 400 megawatts flowed again into the small country’s languishing power grid. Almost overnight, lights were turned on, water pumps worked again, and industries revved up to capacity. Children like my sister and I stopped their exhausting routine and Armenia became a net exporter of electricity.</p>
<p>Over the 13 years that followed, Armenia’s economy grew by an unprecedented 700 percent. The difficult decision to restore nuclear power had saved Armenia and had put it on a path of development. In 2020, about 35 percent of <a href="https://www.eia.gov/international/data/country/ARM/electricity/electricity-generation?pd=2&amp;p=00000000000000000000000000000fvu&amp;u=0&amp;f=A&amp;v=mapbubble&amp;a=-&amp;i=none&amp;vo=value&amp;t=C&amp;g=none&amp;l=249--7&amp;s=315532800000&amp;e=1609459200000&amp;">electricity generated in Armenia</a> came from nuclear, 25 percent came from renewables (primarily hydropower), and the remaining 40 percent from fossil fuels. (In 2021, the share of nuclear power temporarily dropped to 26 percent because the Metsamor reactor was shut down longer than usual to perform a thermal annealing of the pressure vessel, a maintenance method aimed at managing aging effects.)</p>
<p>Despite its important contribution to the electricity mix, the nuclear power plant at Metsamor is not without problems. Mainly, like most Soviet-era PWRs, the reactor does not have the external containment building that is common with Western designs. It is also an aging machine. Because of Armenia’s growing energy needs, the Metsamor reactor has been issued <a href="https://world-nuclear.org/information-library/country-profiles/countries-a-f/armenia.aspx">multiple lifetime extensions</a>. Based on current plans, Metsamor’s VVER-440 reactor will shut down permanently by 2036. Meanwhile the Armenian government has been busy exploring replacement alternatives, such as possibly <a href="https://en.armradio.am/2023/05/24/armenian-in-talks-with-several-partners-to-build-a-new-power-plant-pm-pashinyan-comments-on-us-plans-to-build-modular-nuclear-reactors/">US-built small modular reactors</a> (SMRs), seen as a viable replacement. Armenian officials have also entered in <a href="https://www.neimagazine.com/news/newsarmenia-considers-nuclear-options-10933703">discussions with Russia</a> about the possibility of replacing the Soviet-era VVER-440 reactor with the much larger and more modern Russian VVER-1200 design. While the US option is not easy—mainly because of the lack of readiness of most SMR designs—the Russian option is particularly fraught. Armenia is reluctant to further increase its energy dependence on Russia, given Putin’s campaign of neo-Soviet expansionism. This is further exacerbated by the technical and economic difficulty of hosting a 1200-megawatt electric VVER-1200 unit on a grid that on average consumes only about 1,000 megawatts.</p>
<p><strong>Survival in the shadow of petro-dictatorship.</strong> In recent years, social scientists have studied the negative impacts of nuclear power on underprivileged communities, such as the effects of uranium mining on indigenous populations. These studies are important for understanding the social cost of this resource. However very rarely have scholars studied the positive impact that nuclear power has had in helping the victims of oppression.</p>
<p>Most of the three million inhabitants in Armenia trace their lineage to the survivors of the Armenian Genocide of 1915. Most live near the border with the perpetrator state of Turkey, which to this day refuses to acknowledge its crime and in the recent past has actively helped Azerbaijan. Since 1993, Azerbaijan has been ruled by the Aliyev dynasty with an iron fist, strengthened by the cash flows from the export of the country’s large hydrocarbon reserves to Western countries. To further strengthen his hold on power, Azerbaijani President Ilham Aliyev (son of Heydar Aliyev who held power in Azerbaijan for several decades) has tapped into Azerbaijanis’ trauma from the 1990s by demonizing Armenians and blaming all of Azerbaijan’s ill on this minority.</p>
<p>Since he took power in 2003, the regime of Aliyev son has been accused of <a href="https://www.theguardian.com/commentisfree/2023/oct/09/azerbaijani-ethnic-cleansing-armenians-nagorno-karabakh-children">curtailing free speech and ethnic cleansing</a> of Armenians, whereas Azerbaijan’s armed forces have been busy mounting a campaign of <a href="https://news.cornell.edu/stories/2022/09/report-shows-near-total-erasure-armenian-heritage-sites">widespread cultural erasure</a>. These decades of threats culminated in last September with a <a href="https://www.economist.com/leaders/2023/09/28/a-humanitarian-disaster-is-under-way-in-nagorno-karabakh">swift military attack</a> on the Nagorno-Karabakh region, which in just one week brought the 3,000-year-old indigenous Armenian presence there effectively to an end. The situation currently is so severe that Luis Moreno Ocampo, a former chief prosecutor of the International Criminal Court, <a href="https://amp.cnn.com/cnn/2023/08/11/asia/nagorno-karabakh-armenians-genocide-intl-hnk/index.html">has warned</a> that a new genocide may be underway.</p>
<p>Armenians, whose newly budding democracy is under constant threat from the various authoritarian governments in the region, cannot achieve cultural and existential security if they do not have a state that ensures their security. And that includes energy security, to which nuclear power generation is key. Of course, Azerbaijan deserves to have a democratic government, too, something that is being hindered by the Western countries’ over-reliance on fossil fuel exports.</p>
<p><strong>The lessons of small nations. </strong>When it comes to understanding the value of nuclear energy, studies tend to focus on the big nuclear powers such as the United States, China, and Russia. They rarely study the experiences of small countries like Armenia. Still, the study of these “insignificant” players is important in terms of understanding the mistakes made, successes achieved, and lessons learned, which can be relevant for the “big” players as well. In a telling example, Germany is learning the hard way about the dangers of complacency when it comes to choosing between nuclear energy and fossil fuels for its energy mix: Over the last 20 years, German politicians preferred to shut down their “scary”—but nonetheless safe—nuclear power plants and increase their potentially destabilizing—but considered harmless—reliance on Russia’s natural gas. Had German policymakers studied Armenia’s experience of the 1990s, they could probably have avoided the energy crisis the country is currently experiencing.</p>
<p>Sadly, it’s hard to tell whether European leaders have learned anything from Armenia’s struggle for energy security. In a now much-criticized statement from 2022, European Commission President Ursula von der Leyen called Azerbaijan’s dictator Ilham Aliyev “<a href="https://ec.europa.eu/commission/presscorner/detail/da/statement_22_4583">a reliable partner</a>.” This gesture is now believed to have, at least partly, emboldened the Aliyev regime’s brutality toward the Armenian population of Nagorno-Karabakh. At least for now, it is as if Europe is merely switching dictators while maintaining the same dependence on fossil fuels.</p>
<p>Only a full reckoning by Western countries of their over-reliance on fossil fuels can put an end to the authoritarian regimes that exist only because of their hydrocarbon exports. Such a reckoning, along with the development of renewable energy and nuclear power, would lead to net gains for the climate and the environment. It would also help strengthen liberal democracies that are being unprecedently threatened.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The DIY Diggers Who Can't Stop Making 'Hobby Tunnels' (241 pts)]]></title>
            <link>https://www.bloomberg.com/news/features/2024-02-03/dig-if-you-will-the-underground-world-of-hobby-tunneling</link>
            <guid>39245893</guid>
            <pubDate>Sat, 03 Feb 2024 23:39:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/features/2024-02-03/dig-if-you-will-the-underground-world-of-hobby-tunneling">https://www.bloomberg.com/news/features/2024-02-03/dig-if-you-will-the-underground-world-of-hobby-tunneling</a>, See on <a href="https://news.ycombinator.com/item?id=39245893">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TCC RISC-V Compiler Runs in the Web Browser (Thanks to Zig Compiler) (163 pts)]]></title>
            <link>https://lupyuen.codeberg.page/articles/tcc.html</link>
            <guid>39245664</guid>
            <pubDate>Sat, 03 Feb 2024 23:03:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lupyuen.codeberg.page/articles/tcc.html">https://lupyuen.codeberg.page/articles/tcc.html</a>, See on <a href="https://news.ycombinator.com/item?id=39245664">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <!--[if lte IE 8]>
    <div class="warning">
        This old browser is unsupported and will most likely display funky
        things.
    </div>
    <![endif]-->

        <!-- Begin scripts/rustdoc-before.html: Pre-HTML for Custom Markdown files processed by rustdoc, like chip8.md -->

    <!-- Begin Theme Picker -->
    
    
    
    <!-- Theme Picker -->

    <!-- End scripts/rustdoc-before.html -->
    

    
    <nav id="TOC"><ul>
<li><a href="#tcc-in-the-web-browser">1 TCC in the Web Browser</a><ul></ul></li>
<li><a href="#zig-compiles-tcc-to-webassembly">2 Zig compiles TCC to WebAssembly</a><ul></ul></li>
<li><a href="#posix-for-webassembly">3 POSIX for WebAssembly</a><ul></ul></li>
<li><a href="#file-input-and-output">4 File Input and Output</a><ul></ul></li>
<li><a href="#fearsome-fprintf-and-friends">5 Fearsome fprintf and Friends</a><ul></ul></li>
<li><a href="#test-with-apache-nuttx-rtos">6 Test with Apache NuttX RTOS</a><ul></ul></li>
<li><a href="#hello-nuttx">7 Hello NuttX!</a><ul></ul></li>
<li><a href="#whats-next">8 What’s Next</a><ul></ul></li>
<li><a href="#appendix-compile-tcc-with-zig">9 Appendix: Compile TCC with Zig</a><ul></ul></li>
<li><a href="#appendix-javascript-calls-tcc">10 Appendix: JavaScript calls TCC</a><ul></ul></li>
<li><a href="#appendix-pattern-matching">11 Appendix: Pattern Matching</a><ul></ul></li>
<li><a href="#appendix-nuttx-system-call">12 Appendix: NuttX System Call</a><ul></ul></li>
<li><a href="#appendix-build-nuttx-for-qemu">13 Appendix: Build NuttX for QEMU</a><ul></ul></li>
<li><a href="#appendix-missing-functions">14 Appendix: Missing Functions</a><ul></ul></li></ul></nav><p>📝 <em>4 Feb 2024</em></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-title.png" alt="TCC RISC-V Compiler runs in the Web Browser (thanks to Zig Compiler)"></p>
<p><a href="https://lupyuen.github.io/tcc-riscv32-wasm/">(Try the <strong>Online Demo</strong>)</a></p>
<p><a href="https://youtu.be/DJMDYq52Iv8">(Watch the <strong>Demo on YouTube</strong>)</a></p>
<p><em>TCC is a Tiny C Compiler for 64-bit RISC-V (and other platforms)…</em></p>
<p><em>Can we run TCC Compiler in a Web Browser?</em></p>
<p>Let’s do it! We’ll compile <a href="https://github.com/sellicott/tcc-riscv32"><strong>TCC (Tiny C Compiler)</strong></a> from C to WebAssembly with <a href="https://ziglang.org/"><strong>Zig Compiler</strong></a>.</p>
<p>In this article, we talk about the tricky bits of our <strong>TCC ported to WebAssembly</strong>…</p>
<ul>
<li>
<p>We compiled <strong>TCC to WebAssembly</strong> with one tiny fix</p>
</li>
<li>
<p>But we hit some <strong>Missing POSIX Functions</strong></p>
</li>
<li>
<p>So we built minimal <strong>File Input and Output</strong> </p>
</li>
<li>
<p>Hacked up a simple workaround for <strong>fprintf and friends</strong></p>
</li>
<li>
<p>And TCC produces a <strong>RISC-V Binary</strong> that runs OK</p>
<p>(After some fiddling and meddling in RISC-V Assembly)</p>
</li>
</ul>
<p><em>Why are we doing this?</em></p>
<p>Today we’re running <a href="https://lupyuen.codeberg.page/articles/tinyemu2"><strong>Apache NuttX RTOS</strong></a> inside a Web Browser, with WebAssembly + Emscripten + 64-bit RISC-V.</p>
<p>(<strong>Real-Time Operating System</strong> in a Web Browser on a General-Purpose Operating System!)</p>
<p>What if we could <strong>Build and Test NuttX Apps</strong> in the Web Browser…</p>
<ol>
<li>
<p>We type a <strong>C Program</strong> into our Web Browser (pic below)</p>
</li>
<li>
<p>Compile it into an <strong>ELF Executable</strong> with TCC</p>
</li>
<li>
<p>Copy the ELF Executable to the <strong>NuttX Filesystem</strong></p>
</li>
<li>
<p>And <strong>NuttX Emulator</strong> runs our ELF Executable inside the Web Browser</p>
</li>
</ol>
<p>Learning NuttX becomes so cool! This is how we made it happen…</p>
<p><a href="https://youtu.be/DJMDYq52Iv8">(Watch the <strong>Demo on YouTube</strong>)</a></p>
<p><a href="https://research.cs.queensu.ca/home/cordy/pub/downloads/tplus/Turing_Plus_Report.pdf">(Not to be confused with <strong>TTC Compiler</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-web.png" alt="Online Demo of TCC Compiler in WebAssembly"></p>
<p><a href="https://lupyuen.github.io/tcc-riscv32-wasm/"><em>Online Demo of TCC Compiler in WebAssembly</em></a></p>
<h2 id="tcc-in-the-web-browser"><a href="#tcc-in-the-web-browser">1 TCC in the Web Browser</a></h2>
<p>Click this link to try <strong>TCC Compiler in our Web Browser</strong> (pic above)</p>
<ul>
<li>
<p><a href="https://lupyuen.github.io/tcc-riscv32-wasm/"><strong>TCC RISC-V Compiler in WebAssembly</strong></a></p>
<p><a href="https://youtu.be/DJMDYq52Iv8">(Watch the <strong>Demo on YouTube</strong>)</a></p>
</li>
</ul>
<p>This <strong>C Program</strong> appears…</p>
<div><pre><code>// Demo Program for TCC Compiler
int main(int argc, char *argv[]) {
  printf("Hello, World!!\n");
  return 0;
}
</code></pre></div>
<p>Click the “<strong>Compile</strong>” button. Our Web Browser calls TCC to compile the above program…</p>
<div><pre><code>## Compile to RISC-V ELF
tcc -c hello.c
</code></pre></div>
<p>And it downloads the compiled <a href="https://en.wikipedia.org/wiki/Executable_and_Linkable_Format"><strong>RISC-V ELF <code>a.out</code></strong></a>. We inspect the Compiled Output…</p>
<div><pre><code>## Dump the RISC-V Disassembly
## of TCC Output
$ riscv64-unknown-elf-objdump \
    --syms --source --reloc --demangle \
    --line-numbers --wide  --debugging \
    a.out

main():
   ## Prepare the Stack
   0: fe010113  addi   sp, sp, -32
   4: 00113c23  sd     ra, 24(sp)
   8: 00813823  sd     s0, 16(sp)
   c: 02010413  addi   s0, sp, 32
  10: 00000013  nop

   ## Load to Register A0: "Hello World"
  14: fea43423  sd     a0, -24(s0)
  18: feb43023  sd     a1, -32(s0)
  1c: 00000517  auipc  a0, 0x0
  1c: R_RISCV_PCREL_HI20 L.0
  20: 00050513  mv     a0, a0
  20: R_RISCV_PCREL_LO12_I .text

   ## Call printf()
  24: 00000097  auipc  ra, 0x0
  24: R_RISCV_CALL_PLT printf
  28: 000080e7  jalr   ra  ## 24 &lt;main+0x24&gt;

   ## Clean up the Stack and
   ## return 0 to Caller
  2c: 0000051b  sext.w a0, zero
  30: 01813083  ld     ra, 24(sp)
  34: 01013403  ld     s0, 16(sp)
  38: 02010113  addi   sp, sp, 32
  3c: 00008067  ret
</code></pre></div>
<p>Yep the <strong>64-bit RISC-V Code</strong> looks legit! Very similar to our <a href="https://lupyuen.codeberg.page/articles/app#inside-a-nuttx-app"><strong>NuttX App</strong></a>. (So it will probably run on NuttX)</p>
<p>What just happened? We go behind the scenes…</p>
<p><a href="https://gist.github.com/lupyuen/ab8febefa9c649ad7c242ee3f7aaf974">(See the <strong>Entire Disassembly</strong>)</a></p>
<p><a href="https://lupyuen.codeberg.page/articles/app#inside-a-nuttx-app">(About the <strong>RISC-V Instructions</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-zig.jpg" alt="Zig Compiler compiles TCC Compiler to WebAssembly"></p>
<h2 id="zig-compiles-tcc-to-webassembly"><a href="#zig-compiles-tcc-to-webassembly">2 Zig compiles TCC to WebAssembly</a></h2>
<p><em>Will Zig Compiler happily compile TCC to WebAssembly?</em></p>
<p>Amazingly, yes! (Pic above)</p>
<div><pre><code>## Zig Compiler compiles TCC Compiler
## from C to WebAssembly. Produces `tcc.o`
zig cc \
  -c \
  -target wasm32-freestanding \
  -dynamic \
  -rdynamic \
  -lc \
  -DTCC_TARGET_RISCV64 \
  -DCONFIG_TCC_CROSSPREFIX="\"riscv64-\""  \
  -DCONFIG_TCC_CRTPREFIX="\"/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_LIBPATHS="\"{B}:/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_SYSINCLUDEPATHS="\"{B}/include:/usr/riscv64-linux-gnu/include\""   \
  -DTCC_GITHASH="\"main:b3d10a35\"" \
  -Wall \
  -O2 \
  -Wdeclaration-after-statement \
  -fno-strict-aliasing \
  -Wno-pointer-sign \
  -Wno-sign-compare \
  -Wno-unused-result \
  -Wno-format-truncation \
  -Wno-stringop-truncation \
  -I. \
  tcc.c
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/tcc.c">(See the <strong>TCC Source Code</strong>)</a></p>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-compile-tcc-with-zig">(About the <strong>Zig Compiler Options</strong>)</a></p>
<p>We link the TCC WebAssembly with our <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig"><strong>Zig Wrapper</strong></a> (that exports the TCC Compiler to JavaScript)…</p>
<div><pre><code>## Compile our Zig Wrapper `tcc-wasm.zig` for WebAssembly
## and link it with TCC compiled for WebAssembly `tcc.o`
## Generates `tcc-wasm.wasm`
zig build-exe \
  -target wasm32-freestanding \
  -rdynamic \
  -lc \
  -fno-entry \
  -freference-trace \
  --verbose-cimport \
  --export=compile_program \
  zig/tcc-wasm.zig \
  tcc.o

## Test everything with Web Browser
## or Node.js
node zig/test.js
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig">(See the <strong>Zig Wrapper tcc-wasm.zig</strong>)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/test.js">(See the <strong>Test JavaScript test.js</strong>)</a></p>
<p><em>What’s inside our Zig Wrapper?</em></p>
<p>Our Zig Wrapper will…</p>
<ol>
<li>
<p>Receive the <strong>C Program</strong> from JavaScript</p>
</li>
<li>
<p>Receive the <strong>TCC Compiler Options</strong> from JavaScript</p>
</li>
<li>
<p>Call TCC Compiler to <strong>compile our program</strong></p>
</li>
<li>
<p>Return the compiled <strong>RISC-V ELF</strong> to JavaScript</p>
</li>
</ol>
<p>Like so: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L11-L76">tcc-wasm.zig</a></p>
<div><pre><code>/// Call TCC Compiler to compile a
/// C Program to RISC-V ELF
pub export fn compile_program(
  options_ptr: [*:0]const u8, // Options for TCC Compiler (Pointer to JSON Array:  ["-c", "hello.c"])
  code_ptr:    [*:0]const u8, // C Program to be compiled (Pointer to String)
) [*]const u8 { // Returns a pointer to the `a.out` Compiled Code (Size in first 4 bytes)

  // Receive the C Program from
  // JavaScript and set our Read Buffer
  // https://blog.battlefy.com/zig-made-it-easy-to-pass-strings-back-and-forth-with-webassembly
  const code: []const u8 = std.mem.span(code_ptr);
  read_buf = code;

  // Omitted: Receive the TCC Compiler
  // Options from JavaScript
  // (JSON containing String Array: ["-c", "hello.c"])
  ...

  // Call the TCC Compiler
  _ = main(@intCast(argc), &amp;args_ptrs);

  // Return pointer of `a.out` to
  // JavaScript. First 4 bytes: Size of
  // `a.out`. Followed by `a.out` data.
  const slice = std.heap.page_allocator.alloc(u8, write_buflen + 4)   
    catch @panic("Failed to allocate memory");
  const size_ptr: *u32 = @alignCast(@ptrCast(slice.ptr));
  size_ptr.* = write_buflen;
  @memcpy(slice[4 .. write_buflen + 4], write_buf[0..write_buflen]);
  return slice.ptr; // TODO: Deallocate this memory
}
</code></pre></div>
<p>Plus a couple of Magical Bits that we’ll cover in the next section.</p>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-javascript-calls-tcc">(How JavaScript calls our <strong>Zig Wrapper</strong>)</a></p>
<p><em>Zig Compiler compiles TCC without any code changes?</em></p>
<p>Inside TCC, we stubbed out the <a href="https://github.com/lupyuen/tcc-riscv32-wasm/commit/e30454a0eb9916f820d58a7c3e104eeda67988d8"><strong>setjmp / longjmp</strong></a> to make it compile with Zig Compiler.</p>
<p>Everything else compiles OK!</p>
<p><em>Is it really OK to stub them out?</em></p>
<p><a href="https://en.wikipedia.org/wiki/Setjmp.h"><strong>setjmp / longjmp</strong></a> are called to <strong>Handle Errors</strong> during TCC Compilation. Assuming everything goes hunky dory, we won’t need them.</p>
<p>Later we’ll find a better way to express our outrage. (Instead of jumping around)</p>
<p>We probe the Magical Bits inside our Zig Wrapper…</p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-posix.jpg" alt="TCC Compiler in WebAssembly needs POSIX Functions"></p>
<h2 id="posix-for-webassembly"><a href="#posix-for-webassembly">3 POSIX for WebAssembly</a></h2>
<p><em>What’s this POSIX?</em></p>
<p>TCC Compiler was created as a <strong>Command-Line App</strong>. So it calls the typical <a href="https://en.wikipedia.org/wiki/POSIX"><strong>POSIX Functions</strong></a> like <strong>fopen, fprintf, strncpy, malloc,</strong> …</p>
<p>But WebAssembly running in a Web Browser ain’t <strong>No Command Line</strong>! (Pic above)</p>
<p><a href="https://en.wikipedia.org/wiki/C_standard_library">(WebAssembly doesn’t have a <strong>C Standard Library libc</strong>)</a></p>
<p><em>Is POSIX a problem for WebAssembly?</em></p>
<p>We counted <a href="https://lupyuen.codeberg.page/articles/tcc#appendix-missing-functions"><strong>72 POSIX Functions</strong></a> needed by TCC Compiler, but missing from WebAssembly.</p>
<p>Thus we fill in the <a href="https://lupyuen.codeberg.page/articles/tcc#appendix-missing-functions"><strong>Missing Functions</strong></a> ourselves.</p>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-missing-functions">(About the <strong>Missing POSIX Functions</strong>)</a></p>
<p><em>Surely other Zig Devs will have the same problem?</em></p>
<p>Thankfully we can borrow the POSIX Code from other <strong>Zig Libraries</strong>…</p>
<ul>
<li>
<p><a href="https://github.com/marler8997/ziglibc"><strong>ziglibc</strong></a>: Zig implementation of libc</p>
</li>
<li>
<p><a href="https://github.com/ZigEmbeddedGroup/foundation-libc"><strong>foundation-libc</strong></a>: Freestanding implementation of libc</p>
</li>
<li>
<p><a href="https://lupyuen.codeberg.page/articles/lvgl3#appendix-lvgl-memory-allocation"><strong>PinePhone Simulator</strong></a>: For malloc</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L447-L774">(See the <strong>Borrowed Code</strong>)</a></p>
</li>
</ul>
<p><em>72 POSIX Functions? Sounds like a lot of work…</em></p>
<p>We might not need all 72 POSIX Functions. We stubbed out <strong>many of the functions</strong> to identify the ones that are called: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L776-L855">tcc-wasm.zig</a></p>
<div><pre><code>// Stub Out the Missing POSIX
// Functions. If TCC calls them, 
// we'll see a Zig Panic. Then we 
// implement them. The Types don't
// matter because we'll halt anyway.

pub export fn atoi(_: c_int) c_int {
  @panic("TODO: atoi");
}
pub export fn exit(_: c_int) c_int {
  @panic("TODO: exit");
}
pub export fn fopen(_: c_int) c_int {
  @panic("TODO: fopen");
}

// And many more functions...
</code></pre></div>
<p>Some of these functions are especially troubling for WebAssembly…</p>
<blockquote>
<p><img src="https://lupyuen.codeberg.page/images/tcc-posix2.jpg" alt="File Input and Output are especially troubling for WebAssembly"></p>
</blockquote>
<h2 id="file-input-and-output"><a href="#file-input-and-output">4 File Input and Output</a></h2>
<p><em>Why no #include in TCC for WebAssembly? And no C Libraries?</em></p>
<p>WebAssembly runs in a Secure Sandbox. <strong>No File Access</strong> allowed, sorry! (Like for Header and Library Files)</p>
<p>That’s why our Zig Wrapper <strong>Emulates File Access</strong> for the bare minimum 2 files…</p>
<ul>
<li>
<p>Read the <strong>C Program</strong>: <strong><code>hello.c</code></strong></p>
</li>
<li>
<p>Write the <strong>RISC-V ELF</strong>: <strong><code>a.out</code></strong></p>
</li>
</ul>
<p><strong>Reading a Source File <code>hello.c</code></strong> is extremely simplistic: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L104-L118">tcc-wasm.zig</a></p>
<div><pre><code>/// Emulate the POSIX Function `read()`
/// We copy from One Single Read Buffer
/// that contains our C Program
export fn read(fd0: c_int, buf: [*:0]u8, nbyte: size_t) isize {

  // TODO: Support more than one file
  const len = read_buf.len;
  assert(len &lt; nbyte);
  @memcpy(buf[0..len], read_buf[0..len]);
  buf[len] = 0;
  read_buf.len = 0;
  return @intCast(len);
}

/// Read Buffer for read
var read_buf: []const u8 = undefined;
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L26-L32">(<strong>read_buf</strong> is populated at startup)</a></p>
<p><strong>Writing the Compiled Output <code>a.out</code></strong> is just as barebones: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L128-L140">tcc-wasm.zig</a></p>
<div><pre><code>/// Emulate the POSIX Function `write()`
/// We write to One Single Memory
/// Buffer that will be returned to 
/// JavaScript as `a.out`
export fn fwrite(ptr: [*:0]const u8, size: usize, nmemb: usize, stream: *FILE) usize {

  // TODO: Support more than one `stream`
  const len = size * nmemb;
  @memcpy(write_buf[write_buflen .. write_buflen + len], ptr[0..]);
  write_buflen += len;
  return nmemb;
}

/// Write Buffer for fputc and fwrite
var write_buf = std.mem.zeroes([8192]u8);
var write_buflen: usize = 0;
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L62-L78">(<strong>write_buf</strong> will be returned to JavaScript)</a></p>
<p><em>Can we handle Multiple Files?</em></p>
<p>Right now we’re trying to embed the simple <a href="https://github.com/lupyuen/tcc-riscv32-wasm#rom-fs-filesystem-for-tcc-webassembly"><strong>ROM FS Filesystem</strong></a> into our Zig Wrapper.</p>
<p>The ROM FS Filesystem will be preloaded with the Header and Library Files needed by TCC.</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#rom-fs-filesystem-for-tcc-webassembly">(See the updates for <strong>ROM FS Filesystem</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-format.jpg" alt="Our Zig Wrapper uses Pattern Matching to match the C Formats and substitute the Zig Equivalent"></p>
<h2 id="fearsome-fprintf-and-friends"><a href="#fearsome-fprintf-and-friends">5 Fearsome fprintf and Friends</a></h2>
<p><em>Why is fprintf particularly problematic?</em></p>
<p>Here’s the fearsome thing about <strong>fprintf</strong> and friends: <strong>sprintf, snprintf, vsnprintf</strong>…</p>
<ul>
<li>
<p><strong>C Format Strings</strong> are difficult to parse</p>
</li>
<li>
<p><strong>Variable Number of Untyped Arguments</strong> might create Bad Pointers</p>
</li>
</ul>
<p>Hence we hacked up an implementation of <strong>String Formatting</strong> that’s safer, simpler and so-barebones-you-can-make-<em>soup-tulang</em>.</p>
<p><em>Soup tulang? Tell me more…</em></p>
<p>Our Zig Wrapper uses <a href="https://lupyuen.codeberg.page/articles/tcc#appendix-pattern-matching"><strong>Pattern Matching</strong></a> to match the <strong>C Formats</strong> and substitute the <strong>Zig Equivalent</strong> (pic above): <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L189-L207">tcc-wasm.zig</a></p>
<div><pre><code>// Format a Single `%d`
// like `#define __TINYC__ %d`
FormatPattern{

  // If the C Format String contains this...
  .c_spec = "%d",
  
  // Then we apply this Zig Format...
  .zig_spec = "{}",
  
  // And extract these Argument Types
  // from the Varargs...
  .type0 = c_int,
  .type1 = null
}
</code></pre></div>
<p>This works OK (for now) because TCC Compiler only uses <strong>5 Patterns for C Format Strings</strong>: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L189-L207">tcc-wasm.zig</a></p>
<div><pre><code>/// Pattern Matching for C String Formatting:
/// We'll match these patterns when
/// formatting strings
const format_patterns = [_]FormatPattern{

  // Format a Single `%d`, like `#define __TINYC__ %d`
  FormatPattern{
    .c_spec = "%d",  .zig_spec = "{}", 
    .type0  = c_int, .type1 = null
  },

  // Format a Single `%u`, like `L.%u`
  FormatPattern{ 
    .c_spec = "%u",  .zig_spec = "{}", 
    .type0  = c_int, .type1 = null 
  },

  // Format a Single `%s`, like `.rela%s`
  // Or `#define __BASE_FILE__ "%s"`
  FormatPattern{
    .c_spec = "%s", .zig_spec = "{s}",
    .type0  = [*:0]const u8, .type1 = null
  },

  // Format Two `%s`, like `#define %s%s\n`
  FormatPattern{
    .c_spec = "%s%s", .zig_spec = "{s}{s}",
    .type0  = [*:0]const u8, .type1 = [*:0]const u8
  },

  // Format `%s:%d`, like `%s:%d: `
  // (File Name and Line Number)
  FormatPattern{
    .c_spec = "%s:%d", .zig_spec = "{s}:{}",
    .type0  = [*:0]const u8, .type1 = c_int
  },
};
</code></pre></div>
<p>That’s our quick hack for <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L209-L447"><strong>fprintf and friends</strong></a>!</p>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-pattern-matching">(How we do <strong>Pattern Matching</strong>)</a></p>
<p><em>So simple? Unbelievable!</em></p>
<p>Actually we’ll hit more Format Patterns as TCC Compiler emits various <strong>Error and Warning Messages</strong>. But it’s a good start!</p>
<p>Later our Zig Wrapper shall cautiously and meticulously parse all kinds of C Format Strings. Or we do the <a href="https://github.com/marler8997/ziglibc/blob/main/src/printf.c#L32-L191"><strong>parsing in C</strong></a>, compiled to WebAssembly. (160 lines of C!)</p>
<p>(Funny how <strong>printf</strong> is the first thing we learn about C. Yet it’s incredibly difficult to implement!)</p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-nuttx.jpg" alt="Compile and Run NuttX Apps in the Web Browser"></p>
<h2 id="test-with-apache-nuttx-rtos"><a href="#test-with-apache-nuttx-rtos">6 Test with Apache NuttX RTOS</a></h2>
<p><em>TCC in WebAssembly has compiled our C Program to RISC-V ELF…</em></p>
<p><em>Will the ELF run on NuttX?</em></p>
<p><a href="https://nuttx.apache.org/docs/latest/"><strong>Apache NuttX RTOS</strong></a> is a tiny operating system for 64-bit RISC-V that runs on <a href="https://www.qemu.org/docs/master/system/target-riscv.html"><strong>QEMU Emulator</strong></a>. (And many other devices)</p>
<p>We build <a href="https://lupyuen.codeberg.page/articles/tcc#appendix-build-nuttx-for-qemu"><strong>NuttX for QEMU</strong></a> and copy our <a href="https://en.wikipedia.org/wiki/Executable_and_Linkable_Format"><strong>RISC-V ELF <code>a.out</code></strong></a> to the <a href="https://lupyuen.codeberg.page/articles/semihost#nuttx-apps-filesystem"><strong>NuttX Apps Filesystem</strong></a> (pic above)…</p>
<div><pre><code>## Copy RISC-V ELF `a.out`
## to NuttX Apps Filesystem
cp a.out apps/bin/
chmod +x apps/bin/a.out
</code></pre></div>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-build-nuttx-for-qemu">(How we build <strong>NuttX for QEMU</strong>)</a></p>
<p>Then we boot NuttX and run <strong><code>a.out</code></strong>…</p>
<div><pre><code>## Boot NuttX on QEMU 64-bit RISC-V
$ qemu-system-riscv64 \
  -semihosting \
  -M virt,aclint=on \
  -cpu rv64 \
  -smp 8 \
  -bios none \
  -kernel nuttx \
  -nographic

## Run `a.out` in NuttX Shell
NuttShell (NSH) NuttX-12.4.0
nsh&gt; a.out
Loading /system/bin/a.out
Exported symbol "printf" not found
Failed to load program 'a.out'
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#test-tcc-output-with-nuttx">(See the <strong>Complete Log</strong>)</a></p>
<p>NuttX politely accepts the RISC-V ELF (produced by TCC). And says that <strong>printf</strong> is missing.</p>
<p>Which makes sense: We haven’t linked our C Program with the <a href="https://github.com/lupyuen/tcc-riscv32-wasm#how-nuttx-build-links-a-nuttx-app"><strong>C Library</strong></a>!</p>
<p><a href="https://gist.github.com/lupyuen/847f7adee50499cac5212f2b95d19cd3#file-nuttx-elf-loader-log-L882-L1212">(Loading a <strong>RISC-V ELF</strong> should look like this)</a></p>
<p><em>How else can we print something in NuttX?</em></p>
<p>To print something, we can make a <a href="https://lupyuen.codeberg.page/articles/app#nuttx-app-calls-nuttx-kernel"><strong>System Call (ECALL)</strong></a> directly to NuttX Kernel (bypassing the POSIX Functions)…</p>
<div><pre><code>// NuttX System Call that prints
// something. System Call Number
// is 61 (SYS_write). Works exactly
// like POSIX `write()`
ssize_t write(
  int fd,           // File Descriptor (1 for Standard Output)
  const char *buf,  // Buffer to be printed
  size_t buflen     // Buffer Length
);

// Which makes an ECALL with these Parameters...
// Register A0 is 61 (SYS_write)
// Register A1 is the File Descriptor (1 for Standard Output)
// Register A2 points to the String Buffer to be printed
// Register A3 is the Buffer Length
</code></pre></div>
<p>That’s the same NuttX System Call that <strong>printf</strong> executes internally.</p>
<p>Final chance to say hello to NuttX…</p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-ecall.png" alt="TCC WebAssembly compiles a NuttX System Call"></p>
<h2 id="hello-nuttx"><a href="#hello-nuttx">7 Hello NuttX!</a></h2>
<p><em>We’re making a System Call (ECALL) to NuttX Kernel to print something…</em></p>
<p><em>How will we code this in C?</em></p>
<p>We execute the <a href="https://lupyuen.codeberg.page/articles/app#nuttx-app-calls-nuttx-kernel"><strong>ECALL in RISC-V Assembly</strong></a> like this: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/test-nuttx.js#L52-L105">test-nuttx.js</a></p>
<div><pre><code>int main(int argc, char *argv[]) {

  // Make NuttX System Call
  // to write(fd, buf, buflen)
  const unsigned int nbr = 61; // SYS_write
  const void *parm1 = 1;       // File Descriptor (stdout)
  const void *parm2 = "Hello, World!!\n"; // Buffer
  const void *parm3 = 15; // Buffer Length

  // Load the Parameters into
  // Registers A0 to A3
  // Note: This doesn't work with TCC,
  // so we load again below
  register long r0 asm("a0") = (long)(nbr);
  register long r1 asm("a1") = (long)(parm1);
  register long r2 asm("a2") = (long)(parm2);
  register long r3 asm("a3") = (long)(parm3);

  // Execute ECALL for System Call
  // to NuttX Kernel. Again: Load the
  // Parameters into Registers A0 to A3
  asm volatile (

    // Load 61 to Register A0 (SYS_write)
    "addi a0, zero, 61 \n"
    
    // Load 1 to Register A1 (File Descriptor)
    "addi a1, zero, 1 \n"
    
    // Load 0xc0101000 to Register A2 (Buffer)
    "lui   a2, 0xc0 \n"
    "addiw a2, a2, 257 \n"
    "slli  a2, a2, 0xc \n"
    
    // Load 15 to Register A3 (Buffer Length)
    "addi a3, zero, 15 \n"
    
    // ECALL for System Call to NuttX Kernel
    "ecall \n"
    
    // NuttX needs NOP after ECALL
    ".word 0x0001 \n"

    // Input+Output Registers: None
    // Input-Only Registers: A0 to A3
    // Clobbers the Memory
    :
    : "r"(r0), "r"(r1), "r"(r2), "r"(r3)
    : "memory"
  );

  // Loop Forever
  for(;;) {}
  return 0;
}
</code></pre></div>
<p>We copy this into our Web Browser and compile it. (Pic above)</p>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-nuttx-system-call">(Why so complicated? <strong>Explained here</strong>)</a></p>
<p><a href="https://lupyuen.codeberg.page/articles/app#nuttx-kernel-handles-system-call">(Caution: <strong>SYS_write 61</strong> may change)</a></p>
<p><em>Does it work?</em></p>
<p>TCC in WebAssembly compiles the code above to <strong>RISC-V ELF <code>a.out</code></strong>. When we copy it to NuttX and run it…</p>
<div><pre><code>NuttShell (NSH) NuttX-12.4.0-RC0
nsh&gt; a.out
...
## NuttX System Call for SYS_write (61)
riscv_swint:
  cmd: 61
  A0:  3d  ## SYS_write (61)
  A1:  01  ## File Descriptor (Standard Output)
  A2:  c0101000  ## Buffer
  A3:  0f        ## Buffer Length
...
## NuttX Kernel says hello
Hello, World!!
</code></pre></div>
<p>NuttX Kernel prints <strong>“Hello World”</strong> yay!</p>
<p>Indeed we’ve created a C Compiler in a Web Browser, that <strong>produces proper NuttX Apps</strong>!</p>
<p><em>OK so we can build NuttX Apps in a Web Browser… But can we run them in a Web Browser?</em></p>
<p>Yep, a NuttX App built in the Web Browser… Now runs OK with <strong>NuttX Emulator in the Web Browser</strong>! 🎉 (Pic below)</p>
<ul>
<li>
<p><a href="https://youtu.be/DJMDYq52Iv8">Watch the <strong>Demo on YouTube</strong></a></p>
</li>
<li>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#nuttx-app-runs-in-a-web-browser">Find out <strong>How It Works</strong></a></p>
</li>
</ul>
<p><strong>TLDR:</strong> We called <a href="https://github.com/lupyuen/tcc-riscv32-wasm#nuttx-app-runs-in-a-web-browser"><strong>JavaScript Local Storage</strong></a>
to copy the RISC-V ELF <code>a.out</code> from TCC WebAssembly to NuttX Emulator… Then we patched <code>a.out</code> into the <a href="https://github.com/lupyuen/tcc-riscv32-wasm#nuttx-app-runs-in-a-web-browser"><strong>ROM FS Filesystem</strong></a> for NuttX Emulator. Nifty!</p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-emu2.png" alt="NuttX App built in a Web Browser… Runs inside the Web Browser!"></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#nuttx-app-runs-in-a-web-browser"><em>NuttX App built in a Web Browser… Runs inside the Web Browser!</em></a></p>
<h2 id="whats-next"><a href="#whats-next">8 What’s Next</a></h2>
<p>Thanks to the <a href="https://github.com/sellicott/tcc-riscv32"><strong>TCC Team</strong></a>, we have a <strong>64-bit RISC-V Compiler</strong> that runs in the Web Browser…</p>
<ul>
<li>
<p><strong>Zig Compiler</strong> compiles TCC to WebAssembly with one tiny fix</p>
</li>
<li>
<p>But <strong>POSIX Functions</strong> are missing in WebAssembly</p>
</li>
<li>
<p>So we did the bare minimum for <strong>File Input and Output</strong> </p>
</li>
<li>
<p>And cooked up the simplest workaround for <strong>fprintf and friends</strong></p>
</li>
<li>
<p>Finally TCC produces a <strong>RISC-V Binary</strong> that runs OK on Apache NuttX RTOS</p>
</li>
<li>
<p>Now we can <strong>Build and Test NuttX Apps</strong> all within a Web Browser!</p>
</li>
</ul>
<p>How will you use <strong>TCC in a Web Browser</strong>? Please lemme know 🙏</p>
<p><em>(Build and run RISC-V Apps on iPhone?)</em></p>
<p>Many Thanks to my <a href="https://github.com/sponsors/lupyuen"><strong>GitHub Sponsors</strong></a> (and the awesome NuttX and Zig Communities) for supporting my work! This article wouldn’t have been possible without your support.</p>
<ul>
<li>
<p><a href="https://github.com/sponsors/lupyuen"><strong>Sponsor me a coffee</strong></a></p>
</li>
<li>
<p><a href="https://news.ycombinator.com/item?id=39245664"><strong>Discuss this article on Hacker News</strong></a></p>
</li>
<li>
<p><a href="https://github.com/lupyuen/nuttx-ox64"><strong>My Current Project: “Apache NuttX RTOS for Ox64 BL808”</strong></a></p>
</li>
<li>
<p><a href="https://github.com/lupyuen/nuttx-star64"><strong>My Other Project: “NuttX for Star64 JH7110”</strong></a></p>
</li>
<li>
<p><a href="https://github.com/lupyuen/pinephone-nuttx"><strong>Older Project: “NuttX for PinePhone”</strong></a></p>
</li>
<li>
<p><a href="https://lupyuen.codeberg.page/"><strong>Check out my articles</strong></a></p>
</li>
<li>
<p><a href="https://lupyuen.codeberg.page/rss.xml"><strong>RSS Feed</strong></a></p>
</li>
</ul>
<p><em>Got a question, comment or suggestion? Create an Issue or submit a Pull Request here…</em></p>
<p><a href="https://github.com/lupyuen/lupyuen.github.io/blob/master/src/tcc.md"><strong>lupyuen.github.io/src/tcc.md</strong></a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-web.png" alt="Online Demo of TCC Compiler in WebAssembly"></p>
<p><a href="https://lupyuen.github.io/tcc-riscv32-wasm/"><em>Online Demo of TCC Compiler in WebAssembly</em></a></p>
<h2 id="appendix-compile-tcc-with-zig"><a href="#appendix-compile-tcc-with-zig">9 Appendix: Compile TCC with Zig</a></h2>
<p>This is how we run <strong>Zig Compiler to compile TCC Compiler</strong> from C to WebAssembly (pic below)…</p>
<div><pre><code>## Download the (slightly) Modified TCC Source Code.
## Configure the build for 64-bit RISC-V.

git clone https://github.com/lupyuen/tcc-riscv32-wasm
cd tcc-riscv32-wasm
./configure
make cross-riscv64

## Call Zig Compiler to compile TCC Compiler
## from C to WebAssembly. Produces `tcc.o`

## Omitted: Run the `zig cc` command from earlier...
## https://lupyuen.codeberg.page/articles/tcc#zig-compiles-tcc-to-webassembly
zig cc ...

## Compile our Zig Wrapper `tcc-wasm.zig` for WebAssembly
## and link it with TCC compiled for WebAssembly `tcc.o`
## Generates `tcc-wasm.wasm`

## Omitted: Run the `zig build-exe` command from earlier...
## https://lupyuen.codeberg.page/articles/tcc#zig-compiles-tcc-to-webassembly
zig build-exe ...
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/build.sh">(See the <strong>Build Script</strong>)</a></p>
<p><em>How did we figure out the “<code>zig</code> <code>cc</code>” options?</em></p>
<p>Earlier we saw a long list of <a href="https://lupyuen.codeberg.page/articles/tcc#zig-compiles-tcc-to-webassembly"><strong>Zig Compiler Options</strong></a>…</p>
<div><pre><code>## Zig Compiler Options for TCC Compiler
zig cc \
  tcc.c \
  -DTCC_TARGET_RISCV64 \
  -DCONFIG_TCC_CROSSPREFIX="\"riscv64-\""  \
  -DCONFIG_TCC_CRTPREFIX="\"/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_LIBPATHS="\"{B}:/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_SYSINCLUDEPATHS="\"{B}/include:/usr/riscv64-linux-gnu/include\""   \
  ...
</code></pre></div>
<p>We got them from “<strong><code>make</code> <code>--trace</code></strong>”, which reveals the <strong>GCC Compiler Options</strong>…</p>
<div><pre><code>## Show the GCC Options for compiling TCC
$ make --trace cross-riscv64

gcc \
  -o riscv64-tcc.o \
  -c \
  tcc.c \
  -DTCC_TARGET_RISCV64 \
  -DCONFIG_TCC_CROSSPREFIX="\"riscv64-\""  \
  -DCONFIG_TCC_CRTPREFIX="\"/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_LIBPATHS="\"{B}:/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_SYSINCLUDEPATHS="\"{B}/include:/usr/riscv64-linux-gnu/include\""   \
  -DTCC_GITHASH="\"main:b3d10a35\"" \
  -Wall \
  -O2 \
  -Wdeclaration-after-statement \
  -fno-strict-aliasing \
  -Wno-pointer-sign \
  -Wno-sign-compare \
  -Wno-unused-result \
  -Wno-format-truncation \
  -Wno-stringop-truncation \
  -I. 
</code></pre></div>
<p>And we copied above GCC Options to become our <a href="https://lupyuen.codeberg.page/articles/tcc#zig-compiles-tcc-to-webassembly"><strong>Zig Compiler Options</strong></a>.</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/build.sh">(See the <strong>Build Script</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-zig.jpg" alt="Zig Compiler compiles TCC Compiler to WebAssembly"></p>
<h2 id="appendix-javascript-calls-tcc"><a href="#appendix-javascript-calls-tcc">10 Appendix: JavaScript calls TCC</a></h2>
<p>Previously we saw some <strong>JavaScript (Web Browser and Node.js)</strong> calling our TCC Compiler in WebAssembly (pic above)…</p>
<ul>
<li>
<p><a href="https://lupyuen.github.io/tcc-riscv32-wasm/"><strong>TCC WebAssembly in Web Browser</strong></a></p>
</li>
<li>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#zig-compiles-tcc-to-webassembly"><strong>TCC WebAssembly in Node.js</strong></a></p>
</li>
</ul>
<p>This is how we test the TCC WebAssembly in a Web Browser with a <strong>Local Web Server</strong>…</p>
<div><pre><code>## Download the (slightly) Modified TCC Source Code
git clone https://github.com/lupyuen/tcc-riscv32-wasm
cd tcc-riscv32-wasm

## Start the Web Server
cargo install simple-http-server
simple-http-server ./docs &amp;

## Whenever we rebuild TCC WebAssembly...
## Copy it to the Web Server
cp tcc-wasm.wasm docs/
</code></pre></div>
<p>Browse to this URL and our TCC WebAssembly will appear…</p>
<div><pre><code>## Test TCC WebAssembly with Web Browser
http://localhost:8000/index.html
</code></pre></div>
<p>Check the <strong>JavaScript Console</strong> for Debug Messages.</p>
<p><a href="https://gist.github.com/lupyuen/5f8191d5c63b7dba030582cbe7481572">(See the <strong>JavaScript Log</strong>)</a></p>
<p><em>How does it work?</em></p>
<p>On clicking the <strong>Compile Button</strong>, our JavaScript loads the TCC WebAssembly: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L174-L191">tcc.js</a></p>
<div><pre><code>// Load the WebAssembly Module and start the Main Function.
// Called by the Compile Button.
async function bootstrap() {

  // Load the WebAssembly Module `tcc-wasm.wasm`
  // https://developer.mozilla.org/en-US/docs/WebAssembly/JavaScript_interface/instantiateStreaming
  const result = await WebAssembly.instantiateStreaming(
    fetch("tcc-wasm.wasm"),
    importObject
  );

  // Store references to WebAssembly Functions
  // and Memory exported by Zig
  wasm.init(result);

  // Start the Main Function
  window.requestAnimationFrame(main);
}        
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L25-L48">(<strong>importObject</strong> exports our <strong>JavaScript Logger</strong> to Zig)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L6-L25">(<strong>wasm</strong> is our <strong>WebAssembly Helper</strong>)</a></p>
<p>Which triggers the <strong>Main Function</strong> and calls our Zig Function <strong>compile_program</strong>: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L48-L90">tcc.js</a></p>
<div><pre><code>// Main Function
function main() {
  // Allocate a String for passing the Compiler Options to Zig
  // `options` is a JSON Array: ["-c", "hello.c"]
  const options = read_options();
  const options_ptr = allocateString(JSON.stringify(options));
  
  // Allocate a String for passing the Program Code to Zig
  const code = document.getElementById("code").value;
  const code_ptr = allocateString(code);

  // Call TCC to compile the program
  const ptr = wasm.instance.exports
    .compile_program(options_ptr, code_ptr);

  // Get the `a.out` size from first 4 bytes returned
  const memory = wasm.instance.exports.memory;
  const data_len = new Uint8Array(memory.buffer, ptr, 4);
  const len = data_len[0] | data_len[1] &lt;&lt; 8 | data_len[2] &lt;&lt; 16 | data_len[3] &lt;&lt; 24;
  if (len &lt;= 0) { return; }

  // Encode the `a.out` data from the rest of the bytes returned
  // `encoded_data` looks like %7f%45%4c%46...
  const data = new Uint8Array(memory.buffer, ptr + 4, len);
  let encoded_data = "";
  for (const i in data) {
    const hex = Number(data[i]).toString(16).padStart(2, "0");
    encoded_data += `%${hex}`;
  }

  // Download the `a.out` data into the Web Browser
  download("a.out", encoded_data);

  // Save the ELF Data to Local Storage for loading by NuttX Emulator
  localStorage.setItem("elf_data", encoded_data);
};
</code></pre></div>
<p>Our Main Function then downloads the <strong><code>a.out</code></strong> file returned by our Zig Function.</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L90-L112">(<strong>allocateString</strong> allocates a String from Zig Memory)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L162-L174">(<strong>download</strong> is here)</a></p>
<p><em>What about Node.js calling TCC WebAssembly?</em></p>
<div><pre><code>## Test TCC WebAssembly with Node.js
node zig/test.js
</code></pre></div>
<p><strong>For Easier Testing</strong> (via Command-Line): We copied the JavaScript above into a Node.js Script: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/test.js#L46-L78">test.js</a></p>
<div><pre><code>// Allocate a String for passing the Compiler Options to Zig
const options = ["-c", "hello.c"];
const options_ptr = allocateString(JSON.stringify(options));

// Allocate a String for passing Program Code to Zig
const code_ptr = allocateString(`
  int main(int argc, char *argv[]) {
    printf("Hello, World!!\\n");
    return 0;
  }
`);

// Call TCC to compile a program
const ptr = wasm.instance.exports
  .compile_program(options_ptr, code_ptr);
</code></pre></div>
<p><a href="https://gist.github.com/lupyuen/795327506cad9b1ee82206e614c399cd">(See the <strong>Node.js Log</strong>)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/test-nuttx.js">(Test Script for NuttX QEMU: <strong>test-nuttx.js</strong>)</a></p>
<p><a href="https://gist.github.com/lupyuen/55a4d4cae26994aa673e6d8451716b27">(Test Log for NuttX QEMU: <strong>test-nuttx.log</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-format.jpg" alt="Our Zig Wrapper doing Pattern Matching for Formatting C Strings"></p>
<h2 id="appendix-pattern-matching"><a href="#appendix-pattern-matching">11 Appendix: Pattern Matching</a></h2>
<p>A while back we saw our Zig Wrapper doing <strong>Pattern Matching</strong> for Formatting C Strings…</p>
<ul>
<li><a href="https://lupyuen.codeberg.page/articles/tcc#fearsome-fprintf-and-friends"><strong>“Fearsome fprintf and Friends”</strong></a></li>
</ul>
<p>How It Works: We search for <strong>Format Patterns</strong> in the C Format Strings and substitute the <strong>Zig Equivalent</strong> (pic above): <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L189-L207">tcc-wasm.zig</a></p>
<div><pre><code>// Format a Single `%d`
// like `#define __TINYC__ %d`
FormatPattern{

  // If the C Format String contains this...
  .c_spec = "%d",
  
  // Then we apply this Zig Format...
  .zig_spec = "{}",
  
  // And extract these Argument Types
  // from the Varargs...
  .type0 = c_int,
  .type1 = null
}
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L438-L446">(<strong>FormatPattern</strong> is defined here)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L191-L209">(See the <strong>Format Patterns</strong>)</a></p>
<p>To implement this, we call <strong>comptime Functions</strong> in Zig: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L276-L327">tcc-wasm.zig</a></p>
<div><pre><code>/// CompTime Function to format a string by Pattern Matching.
/// Format a Single Specifier, like `#define __TINYC__ %d\n`
/// If the Spec matches the Format: Return the number of bytes written to `str`, excluding terminating null.
/// Else return 0.
fn format_string1(
  ap: *std.builtin.VaList,  // Varargs passed from C
  str:    [*]u8,            // Buffer for returning Formatted String
  size:   size_t,           // Buffer Size
  format: []const u8,       // C Format String, like `#define __TINYC__ %d\n`
  comptime c_spec:   []const u8,  // C Format Pattern, like `%d`
  comptime zig_spec: []const u8,  // Zig Equivalent, like `{}`
  comptime T0:       type,        // Type of First Vararg, like `c_int`
) usize {  // Return the number of bytes written to `str`, excluding terminating null

  // Count the Format Specifiers: `%`
  const spec_cnt   = std.mem.count(u8, c_spec, "%");
  const format_cnt = std.mem.count(u8, format, "%");

  // Check the Format Specifiers: `%`
  // Quit if the number of specifiers are different
  // Or if the specifiers are not found
  if (format_cnt != spec_cnt or
      !std.mem.containsAtLeast(u8, format, 1, c_spec)) {
    return 0;
  }

  // Fetch the First Argument from the C Varargs
  const a = @cVaArg(ap, T0);

  // Format the Argument
  var buf: [512]u8 = undefined;
  const buf_slice = std.fmt.bufPrint(&amp;buf, zig_spec, .{a}) catch {
    @panic("format_string1 error: buf too small");
  };

  // Replace the C Format Pattern by the Zig Equivalent
  var buf2 = std.mem.zeroes([512]u8);
  _ = std.mem.replace(u8, format, c_spec, buf_slice, &amp;buf2);

  // Return the Formatted String and Length
  const len = std.mem.indexOfScalar(u8, &amp;buf2, 0).?;
  assert(len &lt; size);
  @memcpy(str[0..len], buf2[0..len]);
  str[len] = 0;
  return len;
}

// Omitted: Function `format_string2` looks similar,
// but for 2 Varargs (instead of 1)
</code></pre></div>
<p>The function above is called by a <strong>comptime Inline Loop</strong> that applies all the <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L191-L209"><strong>Format Patterns</strong></a> that we saw earlier: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L207-L251">tcc-wasm.zig</a></p>
<div><pre><code>/// Runtime Function to format a string by Pattern Matching.
/// Return the number of bytes written to `str`, excluding terminating null.
fn format_string(
  ap: *std.builtin.VaList,  // Varargs passed from C
  str:    [*]u8,            // Buffer for returning Formatted String
  size:   size_t,           // Buffer Size
  format: []const u8,       // C Format String, like `#define __TINYC__ %d\n`
) usize {  // Return the number of bytes written to `str`, excluding terminating null

  // If no Format Specifiers: Return the Format, like `warning: `
  const len = format_string0(str, size, format);
  if (len &gt; 0) { return len; }

  // For every Format Pattern...
  inline for (format_patterns) |pattern| {

    // Try formatting the string with the pattern...
    const len2 =
      if (pattern.type1) |t1|
      // Pattern has 2 parameters
      format_string2(ap, str, size, format, // Output String and Format String
        pattern.c_spec, pattern.zig_spec,   // Format Specifiers for C and Zig
        pattern.type0, t1 // Types of the Parameters
      )
    else
      // Pattern has 1 parameter
      format_string1(ap, str, size, format, // Output String and Format String
        pattern.c_spec, pattern.zig_spec,   // Format Specifiers for C and Zig
        pattern.type0 // Type of the Parameter
      );

    // Loop until we find a match pattern
    if (len2 &gt; 0) { return len2; }
  }

  // Format String doesn't match any Format Pattern.
  // We return the Format String and Length.
  const len3 = format.len;
  assert(len3 &lt; size);
  @memcpy(str[0..len3], format[0..len3]);
  str[len3] = 0;
  return len3;
}
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L327-L382">(<strong>format_string2</strong> is here)</a></p>
<p>And the above function is called by <strong>fprintf and friends</strong>: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L382-L438">tcc-wasm.zig</a></p>
<div><pre><code>/// Implement the POSIX Function `fprintf`
export fn fprintf(stream: *FILE, format: [*:0]const u8, ...) c_int {

  // Prepare the varargs
  var ap = @cVaStart();
  defer @cVaEnd(&amp;ap);

  // Format the string
  var buf = std.mem.zeroes([512]u8);
  const format_slice = std.mem.span(format);
  const len = format_string(&amp;ap, &amp;buf, buf.len, format_slice);

  // TODO: Print to other File Streams.
  // Right now we assume it's stderr (File Descriptor 2)
  return @intCast(len);
}

// Do the same for sprintf, snprintf, vsnprintf
</code></pre></div>
<p><a href="https://gist.github.com/lupyuen/3e650bd6ad72b2e8ee8596858bc94f36">(See the <strong>Formatting Log</strong>)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#fix-the-varargs-functions">(Without <strong>comptime</strong>: Our code gets <strong>super tedious</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/app-syscall.jpg" alt="NuttX Apps make a System Call to print to the console"></p>
<h2 id="appendix-nuttx-system-call"><a href="#appendix-nuttx-system-call">12 Appendix: NuttX System Call</a></h2>
<p>Just now we saw a huge chunk of C Code that makes a <strong>NuttX System Call</strong>…</p>
<ul>
<li><a href="https://lupyuen.codeberg.page/articles/tcc#hello-nuttx"><strong>“Hello NuttX!”</strong></a></li>
</ul>
<p><em>Why so complicated?</em></p>
<p>We refer to the Sample Code for <a href="https://lupyuen.codeberg.page/articles/app#nuttx-app-calls-nuttx-kernel"><strong>NuttX System Calls (ECALL)</strong></a>. Rightfully this <strong>shorter version</strong> should work…</p>
<div><pre><code>// Make NuttX System Call to write(fd, buf, buflen)
const unsigned int nbr = 61; // SYS_write
const void *parm1 = 1;       // File Descriptor (stdout)
const void *parm2 = "Hello, World!!\n"; // Buffer
const void *parm3 = 15; // Buffer Length

// Execute ECALL for System Call to NuttX Kernel
register long r0 asm("a0") = (long)(nbr);
register long r1 asm("a1") = (long)(parm1);
register long r2 asm("a2") = (long)(parm2);
register long r3 asm("a3") = (long)(parm3);

asm volatile (
  // ECALL for System Call to NuttX Kernel
  "ecall \n"

  // NuttX needs NOP after ECALL
  ".word 0x0001 \n"

  // Input+Output Registers: None
  // Input-Only Registers: A0 to A3
  // Clobbers the Memory
  :
  : "r"(r0), "r"(r1), "r"(r2), "r"(r3)
  : "memory"
);
</code></pre></div>
<p>Strangely TCC generates <a href="https://github.com/lupyuen/tcc-riscv32-wasm#ecall-for-nuttx-system-call"><strong>mysterious RISC-V Machine Code</strong></a> that mashes up the RISC-V Registers…</p>
<div><pre><code>main():
// Prepare the Stack
   0:  fc010113  add     sp, sp, -64
   4:  02113c23  sd      ra, 56(sp)
   8:  02813823  sd      s0, 48(sp)
   c:  04010413  add     s0, sp, 64
  10:  00000013  nop
  14:  fea43423  sd      a0, -24(s0)
  18:  feb43023  sd      a1, -32(s0)

// Correct: Load Register A0 with 61 (SYS_write)
  1c:  03d0051b  addw    a0, zero, 61
  20:  fca43c23  sd      a0, -40(s0)

// Nope: Load Register A0 with 1?
// Mixed up with Register A1! (Value 1)
  24:  0010051b  addw    a0, zero, 1
  28:  fca43823  sd      a0, -48(s0)

// Nope: Load Register A0 with "Hello World"?
// Mixed up with Register A2!
  2c:  00000517  auipc   a0,0x0  2c: R_RISCV_PCREL_HI20  L.0
  30:  00050513  mv      a0,a0   30: R_RISCV_PCREL_LO12_I        .text
  34:  fca43423  sd      a0, -56(s0)

// Nope: Load Register A0 with 15?
// Mixed up with Register A3! (Value 15)
  38:  00f0051b  addw    a0, zero, 15
  3c:  fca43023  sd      a0, -64(s0)

// Execute ECALL with Register A0 set to 15.
// Nope: A0 should be 61!
  40:  00000073  ecall
  44:  0001      nop
</code></pre></div>
<p>Thus we <a href="https://github.com/lupyuen/tcc-riscv32-wasm#ecall-for-nuttx-system-call"><strong>hardcode Registers A0 to A3</strong></a> in RISC-V Assembly: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/test-nuttx.js#L55-L97">test-nuttx.js</a></p>
<div><pre><code>// Load 61 to Register A0 (SYS_write)
addi  a0, zero, 61

// Load 1 to Register A1 (File Descriptor)
addi  a1, zero, 1

// Load 0xc0101000 to Register A2 (Buffer)
lui   a2, 0xc0
addiw a2, a2, 257
slli  a2, a2, 0xc

// Load 15 to Register A3 (Buffer Length)
addi  a3, zero, 15

// ECALL for System Call to NuttX Kernel
ecall

// NuttX needs NOP after ECALL
.word 0x0001
</code></pre></div>
<p>And it prints “Hello World”!</p>
<p><strong>TODO:</strong> Is there a workaround? Do we paste the ECALL Assembly Code ourselves? <a href="https://github.com/lupyuen/tcc-riscv32-wasm#fix-missing-printf-in-nuttx-app"><strong>NuttX Libraries</strong></a> won’t link with TCC</p>
<p><a href="https://gist.github.com/lupyuen/55a4d4cae26994aa673e6d8451716b27">(See the <strong>TCC WebAssembly Log</strong>)</a></p>
<p><em>What’s with the <code>addi</code> and <code>nop</code>?</em></p>
<p>TCC won’t assemble the “<strong><code>li</code></strong>” and “<strong><code>nop</code></strong>” instructions.</p>
<p>So we used this <a href="https://riscvasm.lucasteske.dev/#"><strong>RISC-V Online Assembler</strong></a> to assemble the code above.</p>
<p>“<strong><code>addi</code></strong>” above is the longer form of “<strong><code>li</code></strong>”, which TCC won’t assemble…</p>
<div><pre><code>// Load 61 to Register A0 (SYS_write)
// But TCC won't assemble `li a0, 61`
// So we do this...

// Add 0 to 61 and save to Register A0
addi a0, zero, 61
</code></pre></div>
<p>“<strong><code>lui / addiw / slli</code></strong>” above is our expansion of “<strong><code>li a2, 0xc0101000</code></strong>”, which TCC won’t assemble…</p>
<div><pre><code>// Load 0xC010_1000 to Register A2 (Buffer)
// But TCC won't assemble `li a2, 0xc0101000`
// So we do this...

// Load 0xC0 &lt;&lt; 12 into Register A2 (0xC0000)
lui   a2, 0xc0

// Add 257 to Register A2 (0xC0101)
addiw a2, a2, 257

// Shift Left by 12 Bits (0xC010_1000)
slli  a2, a2, 0xc
</code></pre></div>
<p><em>How did we figure out that the buffer is at 0xC010_1000?</em></p>
<p>We saw this in our <a href="https://gist.github.com/lupyuen/a715e4e77c011d610d0b418e97f8bf5d#file-nuttx-tcc-app-log-L32-L42"><strong>ELF Loader Log</strong></a>…</p>
<div><pre><code>NuttShell (NSH) NuttX-12.4.0
nsh&gt; a.out
...
Read 576 bytes from offset 512
Read 154 bytes from offset 64
1. 00000000-&gt;c0000000
Read 0 bytes from offset 224
2. 00000000-&gt;c0101000
Read 16 bytes from offset 224
3. 00000000-&gt;c0101000
4. 00000000-&gt;c0101010
</code></pre></div>
<p>Which says that the NuttX ELF Loader copied 16 bytes from our NuttX App Data Section (<strong><code>.data.ro</code></strong>) to <strong><code>0xC010_1000</code></strong>.</p>
<p>That’s all 15 bytes of <em>“Hello, World!!\n”</em>, including the terminating null.</p>
<p>Thus our buffer in NuttX QEMU should be at <strong><code>0xC010_1000</code></strong>.</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#nuttx-app-runs-in-a-web-browser">(<strong>NuttX WebAssembly Emulator</strong> uses <strong><code>0x8010_1000</code></strong> instead)</a></p>
<p><a href="https://lupyuen.codeberg.page/articles/app#kernel-starts-a-nuttx-app">(More about the <strong>NuttX ELF Loader</strong>)</a></p>
<p><em>Why do we Loop Forever?</em></p>
<div><pre><code>// Omitted: Execute ECALL for System Call to NuttX Kernel
asm volatile ( ... );

// Loop Forever
for(;;) {}
</code></pre></div>
<p>That’s because NuttX Apps are not supposed to <a href="https://github.com/lupyuen/tcc-riscv32-wasm#fix-missing-printf-in-nuttx-app"><strong>Return to NuttX Kernel</strong></a>.</p>
<p>We should call the NuttX System Call <strong><code>__exit</code></strong> to terminate peacefully.</p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-demo.png" alt="Online Demo of Apache NuttX RTOS"></p>
<p><a href="https://nuttx.apache.org/demo/"><em>Online Demo of Apache NuttX RTOS</em></a></p>
<h2 id="appendix-build-nuttx-for-qemu"><a href="#appendix-build-nuttx-for-qemu">13 Appendix: Build NuttX for QEMU</a></h2>
<p>Here are the steps to build and run <strong>NuttX for QEMU 64-bit RISC-V</strong> (Kernel Mode)</p>
<ol>
<li>
<p>Install the Build Prerequisites, skip the RISC-V Toolchain…</p>
<p><a href="https://lupyuen.codeberg.page/articles/nuttx#install-prerequisites"><strong>“Install Prerequisites”</strong></a></p>
</li>
<li>
<p>Download the RISC-V Toolchain for <strong>riscv64-unknown-elf</strong>…</p>
<p><a href="https://lupyuen.codeberg.page/articles/riscv#appendix-download-toolchain-for-64-bit-risc-v"><strong>“Download Toolchain for 64-bit RISC-V”</strong></a></p>
</li>
<li>
<p>Download and configure NuttX…</p>
<div><pre><code>## Download NuttX Source Code
mkdir nuttx
cd nuttx
git clone https://github.com/apache/nuttx nuttx
git clone https://github.com/apache/nuttx-apps apps

## Configure NuttX for QEMU RISC-V 64-bit (Kernel Mode)
cd nuttx
tools/configure.sh rv-virt:knsh64
make menuconfig
</code></pre></div>
<p>We use <a href="https://lupyuen.codeberg.page/articles/semihost#nuttx-apps-filesystem"><strong>Kernel Mode</strong></a> because it allows loading of NuttX Apps as ELF Files.</p>
<p>(Instead of Statically Linking the NuttX Apps into NuttX Kernel)</p>
</li>
<li>
<p>(Optional) To enable <strong>ELF Loader Logging</strong>, select…</p>
<p>Build Setup &gt; Debug Options &gt; Binary Loader Debug Features:</p>
<ul>
<li>Enable “Binary Loader Error, Warnings and Info”</li>
</ul>
</li>
<li>
<p>(Optional) To enable <strong>System Call Logging</strong>, select…</p>
<p>Build Setup &gt; Debug Options &gt; SYSCALL  Debug Features:</p>
<ul>
<li>Enable “SYSCALL Error, Warnings and Info”</li>
</ul>
</li>
<li>
<p>Save and exit <strong>menuconfig</strong>.</p>
</li>
<li>
<p>Build the <strong>NuttX Kernel and NuttX Apps</strong>…</p>
<div><pre><code>## Build NuttX Kernel
make -j 8

## Build NuttX Apps
make -j 8 export
pushd ../apps
./tools/mkimport.sh -z -x ../nuttx/nuttx-export-*.tar.gz
make -j 8 import
popd
</code></pre></div></li>
</ol>
<p>This produces the NuttX ELF Image <strong><code>nuttx</code></strong> that we may boot on QEMU RISC-V Emulator…</p>
<div><pre><code>## For macOS: Install QEMU
brew install qemu

## For Debian and Ubuntu: Install QEMU
sudo apt install qemu-system-riscv64

## Boot NuttX on QEMU 64-bit RISC-V
qemu-system-riscv64 \
  -semihosting \
  -M virt,aclint=on \
  -cpu rv64 \
  -smp 8 \
  -bios none \
  -kernel nuttx \
  -nographic
</code></pre></div>
<p>NuttX Apps are located in <strong><code>apps/bin</code></strong>.</p>
<p>We may copy our <strong>RISC-V ELF <code>a.out</code></strong> to that folder and run it…</p>
<div><pre><code>NuttShell (NSH) NuttX-12.4.0-RC0
nsh&gt; a.out
Hello, World!!
</code></pre></div>
<p><img src="https://lupyuen.codeberg.page/images/tcc-posix.jpg" alt="POSIX Functions aren’t supported for TCC in WebAssembly"></p>
<h2 id="appendix-missing-functions"><a href="#appendix-missing-functions">14 Appendix: Missing Functions</a></h2>
<p>Remember we said that POSIX Functions aren’t supported in WebAssembly? (Pic above)</p>
<ul>
<li><a href="https://lupyuen.codeberg.page/articles/tcc#posix-for-webassembly"><strong>“POSIX for WebAssembly”</strong></a></li>
</ul>
<p>We dump the <strong>Compiled WebAssembly</strong> of TCC Compiler, and we discover that it calls <strong>72 POSIX Functions</strong>…</p>
<div><pre><code>## Dump the Compiled WebAssembly
## for TCC Compiler `tcc.o`
$ sudo apt install wabt
$ wasm-objdump -x tcc.o

Import:
 - func[0] sig=1  &lt;env.strcmp&gt; &lt;- env.strcmp
 - func[1] sig=12 &lt;env.memset&gt; &lt;- env.memset
 - func[2] sig=1  &lt;env.getcwd&gt; &lt;- env.getcwd
 ...
 - func[69] sig=2  &lt;env.localtime&gt; &lt;- env.localtime
 - func[70] sig=13 &lt;env.qsort&gt;     &lt;- env.qsort
 - func[71] sig=19 &lt;env.strtoll&gt;   &lt;- env.strtoll
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#missing-functions-in-tcc-webassembly">(See the <strong>Complete List</strong>)</a></p>
<p>Do we need all 72 POSIX Functions? We scrutinise the list…</p>
<hr>
<p><strong>Filesystem Functions</strong></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L87-L166"><em>(Implemented here)</em></a></p>
<p>We’ll simulate these functions for WebAssembly, by embedding the simple <a href="https://github.com/lupyuen/tcc-riscv32-wasm#rom-fs-filesystem-for-tcc-webassembly"><strong>ROM FS Filesystem</strong></a> into our Zig Wrapper…</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#rom-fs-filesystem-for-tcc-webassembly">(See the updates for <strong>ROM FS Filesystem</strong>)</a></p>
<hr>
<p><strong>Varargs Functions</strong></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L186-L445"><em>(Implemented here)</em></a></p>
<p>As discussed earlier, Varargs will be <a href="https://lupyuen.codeberg.page/articles/tcc#fearsome-fprintf-and-friends"><strong>tricky to implement</strong></a> in Zig. Probably we should do it in C.</p>
<p><a href="https://github.com/marler8997/ziglibc/blob/main/src/printf.c#L32-L191">(Similar to <strong>ziglibc</strong>)</a></p>
<p>Right now we’re doing simple <a href="https://lupyuen.codeberg.page/articles/tcc#appendix-pattern-matching"><strong>Pattern Matching</strong></a>. But it might not be sufficient when TCC compiles Real Programs…</p>
<hr>
<p><strong>String Functions</strong></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L541-L776"><em>(Implemented here)</em></a></p>
<p>We’ll borrow the String Functions from <a href="https://github.com/marler8997/ziglibc/blob/main/src/cstd.zig"><strong>ziglibc</strong></a>…</p>
<hr>
<p><strong>Semaphore Functions</strong></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L166-L186"><em>(Implemented here)</em></a></p>
<p>Not sure why TCC uses Semaphores? Maybe we’ll understand when we support <strong><code>#include</code></strong> files.</p>
<p>(Where can we borrow the Semaphore Functions?)</p>
<hr>
<p><strong>Standard Library</strong></p>
<p><strong>qsort</strong> isn’t used right now. Maybe for the Linker later?</p>
<p>(Borrow <strong>qsort</strong> from where? We can probably implement <strong>exit</strong>)</p>
<hr>
<p><strong>Time and Math Functions</strong></p>
<p>Not used right now, maybe later.</p>
<p>(Anyone can lend us <strong>ldexp</strong>? How will we do the Time Functions? Call out to JavaScript to <a href="https://lupyuen.codeberg.page/articles/lvgl4#appendix-handle-lvgl-timer"><strong>fetch the time</strong></a>?)</p>
<hr>
<p><strong>Outstanding Functions</strong></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L776-L855"><em>(Implemented here)</em></a></p>
<p>We have implemented (fully or partially) <strong>48 POSIX Functions</strong> from above.</p>
<p>The ones that we haven’t implemented? These <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L776-L855"><strong>24 POSIX Functions will Halt</strong></a> when TCC WebAssembly calls them…</p>

    


</div>]]></description>
        </item>
    </channel>
</rss>