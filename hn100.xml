<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 07 Dec 2025 16:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Goodbye, Microsoft: Schleswig-Holstein Relies on Open Source and Saves Millions (243 pts)]]></title>
            <link>https://www.heise.de/en/news/Goodbye-Microsoft-Schleswig-Holstein-relies-on-Open-Source-and-saves-millions-11105459.html</link>
            <guid>46181491</guid>
            <pubDate>Sun, 07 Dec 2025 13:21:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.heise.de/en/news/Goodbye-Microsoft-Schleswig-Holstein-relies-on-Open-Source-and-saves-millions-11105459.html">https://www.heise.de/en/news/Goodbye-Microsoft-Schleswig-Holstein-relies-on-Open-Source-and-saves-millions-11105459.html</a>, See on <a href="https://news.ycombinator.com/item?id=46181491">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

        

        <p>The state administration of Schleswig-Holstein is making a remarkable U-turn in its IT strategy and <a href="https://www.heise.de/news/Schleswig-Holstein-treibt-flaechendeckenden-Einsatz-von-Open-Source-voran-10177595.html?from-en=1">consistently relying on open source</a>. After the migration from proprietary Microsoft software to free solutions <a href="https://www.heise.de/news/Schleswig-Holsteins-E-Mail-Systeme-auf-Open-Source-umgestellt-10733720.html?from-en=1">was initially accompanied by problems and criticism</a>, Digitalization Minister Dirk Schrödter (CDU) can now report a significant success: According to his ministry, the state will save over 15 million euros in license costs for Windows, Microsoft Office &amp; Co. next year alone. It is expected to be similar in the following years.</p>
<!-- RSPEAK_STOP -->




  


<!-- RSPEAK_START -->

<p>In contrast, there would be one-time investments of nine million euros in 2026, <a href="https://www.kn-online.de/schleswig-holstein/open-source-statt-microsoft-schleswig-holstein-spart-millionen-G7SUXTH42BBE3FGJC7VQQU4LTA.html" rel="external noopener" target="_blank">explained the Ministry of Digitalization to the Kieler Nachrichten</a>. These would have to be made for the conversion of workplaces and the further development of solutions with free software in the next 12 months. Given the annual savings, this sum will pay for itself in less than a year. In the past, the state transferred millions to the US company Microsoft, primarily for the use of office software and other programs.</p>
<p>The department sees the departure from this "vendor lock-in" – the dependence on a single large provider – as a clear signal for greater independence and sustainable digitalization. The financial incentive now underscores that digital sovereignty can be not only a political buzzword but also an economic gain.</p>
<h3 id="nav_almost_80__0">Almost 80 percent of licenses canceled</h3>
<p>The numbers speak for themselves: outside the tax administration, almost 80 percent of workplaces in the state administration have already been switched to the open-source office software LibreOffice. Schrödter thus confirms a course that reduces technical and economic dependence on individual manufacturers. The consequence of the conversion was already evident recently, <a href="https://www.heise.de/hintergrund/Schleswig-Holstein-Fast-80-Prozent-der-Microsoft-Lizenzen-gekuendigt-10960941.html?from-en=1">as Schrödter emphasized in an interview with c't</a>. Regarding the status of Microsoft license cancellations, he said: "We are at almost 80, without the tax administration." For tax matters, the state finance ministers have "given themselves a clear timetable for the switch." Recently, the Christian Democrat also emphasized, according to the Südtiroler Wirtschaftszeitung, that the state has entered a marathon, not just a sprint.</p>
<p>The remaining 20 percent of workplaces are currently still dependent on Microsoft programs such as Word or Excel, as there is a technical dependency on these programs in certain specialized applications. According to Schrödter, however, the successive conversion of these remaining computers is the stated goal.</p>
<h3 id="nav_opposition_sees__1">Opposition sees challenges</h3>
<p>Despite the savings and the almost completed migration in large parts of the administration, the opposition continues to criticize the quality of the conversion. SPD state parliament member Kianusch Stender pointed out to the Kieler Nachrichten: "It may be that on paper 80 percent of workplaces have been converted. But far fewer than 80 percent of employees can now work with them properly." Errors in the migration are "still present." The initial difficulties in introducing the open-source programs have apparently led to ongoing frustration among some employees in certain areas.</p>
<!-- RSPEAK_STOP -->


  



  




<!-- RSPEAK_START -->

<!-- RSPEAK_STOP -->

  




<!-- RSPEAK_START -->

<p>The Green state parliament member Jan Kürschner also admitted in an interview with heise online that such a comprehensive conversion would not go without friction. <a href="https://www.heise.de/hintergrund/Interview-Wie-die-OSS-Umstellung-in-Schleswig-Holstein-laeuft-10629991.html?from-en=1">But he emphasized the long-term nature of the project</a> and the necessity of fundamentally rethinking administrative processes: "With the change, there is an opportunity to truly rethink the administration and free ourselves from old burdens. That is the great added value." If only a one-to-one conversion is made, it might certainly "stumble at one point or another." But those who truly optimize administrative processes will likely find in the end: "Open source is the better way."</p>
<p>The challenge now is to resolve the initial migration problems and acceptance difficulties and to further develop the open-source solutions so that they fully meet the requirements of a modern state administration. The savings achieved give Schleswig-Holstein more financial leeway for this.</p>


<!-- RSPEAK_STOP -->

<!-- RSPEAK_START -->
<p>

<!-- RSPEAK_STOP -->
<span>(<a href="mailto:nico.ernst@gmail.com" title="Nico Ernst">nie</a>)</span>
<!-- RSPEAK_START -->
</p>
<div>
    <p>
      Don't miss any news – follow us on
      <a href="https://www.facebook.com/heiseonlineEnglish">Facebook</a>,
      <a href="https://www.linkedin.com/company/104691972">LinkedIn</a> or
      <a href="https://social.heise.de/@heiseonlineenglish">Mastodon</a>.
    </p>
    <p>
      <em>This article was originally published in
      
        <a href="https://www.heise.de/news/Adieu-Microsoft-Schleswig-Holstein-setzt-auf-Open-Source-und-spart-Millionen-11105389.html">German</a>.
      
      It was translated with technical assistance and editorially reviewed before publication.</em>
    </p>
  </div>



        

        
        <!-- RSPEAK_STOP -->
        

<a-gift has-access="">
    
</a-gift>


        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[At least 50 hallucinated citations found in ICLR 2026 submissions (194 pts)]]></title>
            <link>https://gptzero.me/news/iclr-2026/</link>
            <guid>46181466</guid>
            <pubDate>Sun, 07 Dec 2025 13:16:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gptzero.me/news/iclr-2026/">https://gptzero.me/news/iclr-2026/</a>, See on <a href="https://news.ycombinator.com/item?id=46181466">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><tr><td>Title</td><td>Average Review Rating</td><td>Paper Link</td><td>Citation Check Scan Link</td><td>Example of Verified Hallucination</td><td>Comment</td></tr><tr><td>TamperTok: Forensics-Driven Tokenized Autoregressive Framework for Image Tampering Localization</td><td>8.0</td><td><a href="https://openreview.net/forum?id=WPgaGP4sVS" target="_blank"><span>TamperTok: Forensics-Driven Tokenized Autoregressive Framework for Image Tampering Localization | OpenReview</span></a></td><td><a href="https://app.gptzero.me/documents/4645494f-70eb-40bb-aea7-0007e13f7179/share" target="_blank">https://app.gptzero.me/documents/4645494f-70eb-40bb-aea7-0007e13f7179/share</a></td><td>Chong Zou, Zhipeng Wang, Ziyu Li, Nan Wu, Yuling Cai, Shan Shi, Jiawei Wei, Xia Sun, Jian Wang, and Yizhou Wang. Segment everything everywhere all at once. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, 2023.</td><td>This paper exists, but all authors are wrong.</td></tr><tr><td>MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive Text Sources</td><td>8.0</td><td><a href="https://openreview.net/forum?id=9upf6JVssk" target="_blank"><span>MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive Text Sources | OpenReview</span></a></td><td><a href="https://app.gptzero.me/documents/bfd10666-ea2d-454c-9ab2-75faa8b84281/share" target="_blank">https://app.gptzero.me/documents/bfd10666-ea2d-454c-9ab2-75faa8b84281/share</a></td><td>Dan Hendrycks, Collin Burns, Steven Basart, Andy Critch, Jerry Li, Dawn Ippolito, Aina Lapedriza, Florian Tramer, Rylan Macfarlane, Eric Jiang, et al. Measuring massive multitask language understanding. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.</td><td>The paper and first 3 authors match. The last 7 authors are not on the paper, and some of them do not exist</td></tr><tr><td>Catch-Only-One: Non-Transferable Examples for Model-Specific Authorization</td><td>6.0</td><td><a href="https://openreview.net/forum?id=de9kj1BnlY" target="_blank">Catch-Only-One: Non-Transferable Examples for Model-Specific Authorization | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/9afb1d51-c5c8-48f2-9b75-250d95062521/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/9afb1d51-c5c8-48f2-9b75-250d95062521/share">https://app.gptzero.me/documents/9afb1d51-c5c8-48f2-9b75-250d95062521/share</a></td><td>Dinghuai Zhang, Yang Song, Inderjit Dhillon, and Eric Xing. Defense against adversarial attacks using spectral regularization. In International Conference on Learning Representations (ICLR), 2020.</td><td>No Match</td></tr><tr><td>OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features</td><td>6.0</td><td><a href="https://openreview.net/forum?id=lBctELT2f9" target="_blank">OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/e3f155d7-067a-4720-adf8-65dc9dc714b9/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/e3f155d7-067a-4720-adf8-65dc9dc714b9/share">https://app.gptzero.me/documents/e3f155d7-067a-4720-adf8-65dc9dc714b9/share</a></td><td>Robert Huben, Logan Riggs, Aidan Ewart, Hoagy Cunningham, and Lee Sharkey. Sparse autoencoders can interpret randomly initialized transformers, 2025. URL https://arxiv.org/ abs/2501.17727.</td><td>This paper exists, but all authors are wrong.</td></tr><tr><td>Principled Policy Optimization for LLMs via Self-Normalized Importance Sampling</td><td>5.0</td><td><a href="https://openreview.net/forum?id=HVciz8hi1c" target="_blank">Principled Policy Optimization for LLMs via Self-Normalized Importance Sampling | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/54c8aa45-c97d-48fc-b9d0-d491d54df8d3/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/54c8aa45-c97d-48fc-b9d0-d491d54df8d3/share">https://app.gptzero.me/documents/54c8aa45-c97d-48fc-b9d0-d491d54df8d3/share</a></td><td>David Rein, Stas Gaskin, Lajanugen Logeswaran, Adva Wolf, Oded teht sun, Jackson H. He, Divyansh Kaushik, Chitta Baral, Yair Carmon, Vered Shwartz, Sang-Woo Lee, Yoav Goldberg, C. J. H. un, Swaroop Mishra, and Daniel Khashabi. Gpqa: A graduate-level google-proof q\&amp;a benchmark, 2023</td><td>All authors except the first are fabricated.</td></tr><tr><td>PDMBench: A Standardized Platform for Predictive Maintenance Research</td><td>4.5</td><td><a href="https://openreview.net/forum?id=oJhj8wOCNB" target="_blank">PDMBench: A Standardized Platform for Predictive Maintenance Research | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/5c55afe7-1689-480d-ac44-9502dc0f9229/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/5c55afe7-1689-480d-ac44-9502dc0f9229/share">https://app.gptzero.me/documents/5c55afe7-1689-480d-ac44-9502dc0f9229/share</a></td><td> Andrew Chen, Andy Chow, Aaron Davidson, Arjun DCunha, Ali Ghodsi, Sue Ann Hong, Andy Konwinski, Clemens Mewald, Siddharth Murching, Tomas Nykodym, et al. Mlflow: A platform for managing the machine learning lifecycle. In Proceedings of the Fourth International Workshop on Data Management for End-to-End Machine Learning, pp. 1-4. ACM, 2018. </td><td><span>Authors and conference match this </span><span><a target="_blank" href="https://dl.acm.org/doi/10.1145/3399579.3399867">paper</a></span><span>, but title is somewhat different and the year is wrong.</span></td></tr><tr><td>IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs</td><td>4.5</td><td><a href="https://openreview.net/forum?id=f9wIqr87W9" target="_blank">IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/5461eefd-891e-4100-ba1c-e5419af520c0/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/5461eefd-891e-4100-ba1c-e5419af520c0/share">https://app.gptzero.me/documents/5461eefd-891e-4100-ba1c-e5419af520c0/share</a></td><td>Chen Zhu et al. A survey on efficient deployment of large language models. arXiv preprint arXiv:2307.03744, 2023.</td><td>The arXiv ID is real, but the paper has different authors and a different title.</td></tr><tr><td>C3-OWD: A Curriculum Cross-modal Contrastive Learning Framework for Open-World Detection</td><td>4.5</td><td><a href="https://openreview.net/forum?id=u4vQPWy3Vk" target="_blank">C3-OWD: A Curriculum Cross-modal Contrastive Learning Framework for Open-World Detection | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/c07521cd-2757-40a2-8dc1-41382d7eb11b/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/c07521cd-2757-40a2-8dc1-41382d7eb11b/share">https://app.gptzero.me/documents/c07521cd-2757-40a2-8dc1-41382d7eb11b/share</a></td><td>K. Marino, R. Salakhutdinov, and A. Gupta. Fine-grained image classification with learnable semantic parts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4500-4509, 2019.</td><td><span>Authors and subject match this </span><span><a target="_blank" href="https://arxiv.org/abs/1612.04844">paper</a></span></td></tr><tr><td>TopoMHC: Sequence–Topology Fusion for MHC Binding</td><td>4.5</td><td><a href="https://openreview.net/forum?id=i4BiQK5Ndw" target="_blank">TopoMHC: Sequence–Topology Fusion for MHC Binding | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/8da4f86c-00d8-4d73-81dd-c168c0bfdf4e/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/8da4f86c-00d8-4d73-81dd-c168c0bfdf4e/share">https://app.gptzero.me/documents/8da4f86c-00d8-4d73-81dd-c168c0bfdf4e/share</a></td><td>Yuchen Han, Yohan Kim, Dalibor Petrovic, Alessandro Sette, Morten Nielsen, and Bjoern Peters. Deepligand: a deep learning framework for peptide-mhc binding prediction. Bioinformatics, 39 (1):btac834, 2023. doi: 10.1093/bioinformatics/btac834.</td><td>No Match</td></tr><tr><td>Can Text-to-Video Models Generate Realistic Human Motion?</td><td>4.5</td><td><a href="https://openreview.net/forum?id=LUXsDBYTkp" target="_blank">Can Text-to-Video Models Generate Realistic Human Motion? | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/f52aad2d-2253-44bf-80ba-8e8668df650f/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/f52aad2d-2253-44bf-80ba-8e8668df650f/share">https://app.gptzero.me/documents/f52aad2d-2253-44bf-80ba-8e8668df650f/share</a></td><td>Yugandhar Balaji, Jianwei Yang, Zhen Xu, Menglei Chai, Zhoutong Xu, Ersin Yumer, Greg Shakhnarovich, and Deva Ramanan. Conditional gan with discriminative filter generation for text-to-video synthesis. In Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI), pp. 2155-2161, July 2019. doi: 10.24963/ijcai.2019/276.</td><td><span>This paper </span><span><a target="_blank" href="https://www.ijcai.org/proceedings/2019/0276.pdf">exists</a></span><span>, but the authors and page numbers are wrong.</span></td></tr><tr><td>GRF-LLM: Environment-Aware Wireless Channel Modeling via LLM-Guided 3D Gaussians</td><td>4.0</td><td><a href="https://openreview.net/forum?id=DmCof7cMTc" target="_blank">GRF-LLM: Environment-Aware Wireless Channel Modeling via LLM-Guided 3D Gaussians | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/c3e66b9c-20b4-4c50-b881-e40aba2a514f/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/c3e66b9c-20b4-4c50-b881-e40aba2a514f/share">https://app.gptzero.me/documents/c3e66b9c-20b4-4c50-b881-e40aba2a514f/share</a></td><td>Junting Chen, Yong Zeng, and Rui Zhang. Rfcanvas: A radio frequency canvas for wireless network design. In IEEE International Conference on Communications, pp. 1-6, 2024.<br></td><td><span>Title partially matches this </span><span><a target="_blank" href="https://xyzhang.ucsd.edu/papers/Xingyu.Chen_SenSys24_RFCanvas.pdf">article</a></span><span>.</span></td></tr><tr><td>Listwise Generalized Preference Optimization with Process-aware Signals for LLM Reasoning</td><td>4.0</td><td><a href="https://openreview.net/forum?id=Qc0goZbgZT" target="_blank">Listwise Generalized Preference Optimization with Process-aware Signals for LLM Reasoning | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/bbeecf1c-189a-4311-999b-617aab686ea9/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/bbeecf1c-189a-4311-999b-617aab686ea9/share">https://app.gptzero.me/documents/bbeecf1c-189a-4311-999b-617aab686ea9/share</a></td><td>Kaixuan Zhou, Jiaqi Liu, Yiding Wang, and James Zou. Generalized direct preference optimization. arXiv preprint arXiv:2402.05015, 2024.</td><td>No Match</td></tr><tr><td>IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation</td><td>4.0</td><td><a href="https://openreview.net/forum?id=s6rmpt3DWe" target="_blank">IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/0f12d2fc-403b-4859-8d00-f75fd9f56e39/share" target="_blank">https://app.gptzero.me/documents/0f12d2fc-403b-4859-8d00-f75fd9f56e39/share</a></td><td>Yash Goyal, Anamay Mohapatra, Nihar Kwatra, and Pawan Goyal. A benchmark for compositional text-to-image synthesis. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.</td><td><span>This paper </span><span><a target="_blank" href="https://openreview.net/pdf?id=bKBhQhPeKaF">exists</a></span><span>, but the authors are all wrong.</span></td></tr><tr><td>Resolving the Security-Auditability Dilemma with Auditable Latent Chain-of-Thought</td><td>4.0</td><td><a href="https://openreview.net/forum?id=BaG67KNavy" target="_blank">Resolving the Security-Auditability Dilemma with Auditable Latent Chain-of-Thought | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/5cee5c3a-5e75-4063-a054-1e934a071705/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/5cee5c3a-5e75-4063-a054-1e934a071705/share">https://app.gptzero.me/documents/5cee5c3a-5e75-4063-a054-1e934a071705/share</a></td><td>Yixiang Ma, Ziyi Liu, Zhaoyu Wang, Zhaofeng Xu, Yitao Wang, and Yang Liu. Safechain: A framework for securely executing complex commands using large language models. arXiv preprint arXiv:2402.16521, 2024a.</td><td><span>No match; although this </span><span><a target="_blank" href="https://arxiv.org/abs/2502.12025">paper</a></span><span> is closely related.</span></td></tr><tr><td>ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks</td><td>4.0</td><td><a href="https://openreview.net/forum?id=qSs6tJ5RjC" target="_blank">ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/f3441445-5401-48e9-9617-09a635992ff9/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/f3441445-5401-48e9-9617-09a635992ff9/share">https://app.gptzero.me/documents/f3441445-5401-48e9-9617-09a635992ff9/share</a></td><td>Yunzhu Yang, Shuang Li, and Jiajun Wu. MM-ReAct: Prompting chatgpt to multi-modal chain-ofthought reasoning. arXiv preprint arXiv:2401.04740, 2024.</td><td>No Match</td></tr><tr><td>Taming the Judge: Deconflicting AI Feedback for Stable Reinforcement Learning</td><td>3.5</td><td><a href="https://openreview.net/forum?id=yeKgUkFRsF" target="_blank">Taming the Judge: Deconflicting AI Feedback for Stable Reinforcement Learning | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/80c64df2-eee6-41aa-90cc-3f835b128747/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/80c64df2-eee6-41aa-90cc-3f835b128747/share">https://app.gptzero.me/documents/80c64df2-eee6-41aa-90cc-3f835b128747/share</a></td><td>Chenglong Wang, Yang Liu, Zhihong Xu, Ruochen Zhang, Jiahao Wu, Tao Luo, Jingang Li, Xunliang Liu, Weiran Qi, Yujiu Yang, et al. Gram-r ${ }^{8}$ : Self-training generative foundation reward models for reward reasoning. arXiv preprint arXiv:2509.02492, 2025b.</td><td>All authors except the first are fabricated and the title is altered.</td></tr><tr><td>DANCE-ST: Why Trustworthy AI Needs Constraint Guidance, Not Constraint Penalties</td><td>3.5</td><td><a href="https://openreview.net/forum?id=sokvfawAYu" target="_blank">DANCE-ST: Why Trustworthy AI Needs Constraint Guidance, Not Constraint Penalties | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/3ebd71b4-560d-4fa3-a0d3-ed2fa13c519f/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/3ebd71b4-560d-4fa3-a0d3-ed2fa13c519f/share">https://app.gptzero.me/documents/3ebd71b4-560d-4fa3-a0d3-ed2fa13c519f/share</a></td><td>Sardar Asif, Saad Ghayas, Waqar Ahmad, and Faisal Aadil. Atcn: an attention-based temporal convolutional network for remaining useful life prediction. The Journal of Supercomputing, 78(1): $1-19,2022$.</td><td><span>Two papers with similar titles exist </span><span><a target="_blank" href="https://www.researchgate.net/publication/377037872_An_attention-based_temporal_convolutional_network_method_for_predicting_remaining_useful_life_of_aero-engine">here</a></span><span> and </span><span><a target="_blank" href="https://www.semanticscholar.org/paper/An-attention-based-multi-scale-temporal-network-for-Xu-Zhang/398d6288f030d5af128fb4bf69a6b7a3226a488c">here</a></span><span>, but the authors, journal, and date do not match.</span></td></tr><tr><td>Federated Hierarchical Anti-Forgetting Framework for Class-Incremental Learning with Large Pre-Trained Models</td><td>3.33</td><td><a href="https://openreview.net/forum?id=RvAvRC3cUW" target="_blank">Federated Hierarchical Anti-Forgetting Framework for Class-Incremental Learning with Large Pre-Trained Models | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/ae10437b-c65b-455b-ad22-918742a5ed82/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/ae10437b-c65b-455b-ad22-918742a5ed82/share">https://app.gptzero.me/documents/ae10437b-c65b-455b-ad22-918742a5ed82/share</a></td><td>Arslan Chaudhry, Arun Mallya, and Abhinav Srivastava. Fedclassil: A benchmark for classincremental federated learning. In NeurIPS, 2023.</td><td>No Match</td></tr><tr><td>Chain-of-Influence: Tracing Interdependencies Across Time and Features in Clinical Predictive Modeling</td><td>3.33</td><td><a href="https://openreview.net/forum?id=Xh2QZ0uo5o" target="_blank">Chain-of-Influence: Tracing Interdependencies Across Time and Features in Clinical Predictive Modeling | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/dff2c063-6986-4241-8c20-4327a39d4d4b/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/dff2c063-6986-4241-8c20-4327a39d4d4b/share">https://app.gptzero.me/documents/dff2c063-6986-4241-8c20-4327a39d4d4b/share</a></td><td>Ishita et al. Bardhan. Icu length-of-stay prediction with interaction-based explanations. Journal of Biomedical Informatics, 144:104490, 2024.</td><td>No Match</td></tr><tr><td>TRACEALIGN - Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs</td><td>3.33</td><td><a href="https://openreview.net/forum?id=6mzS7NAiui" target="_blank">TRACEALIGN - Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/4b379aba-8d8a-427b-ac67-d13af5eda8c9/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/4b379aba-8d8a-427b-ac67-d13af5eda8c9/share">https://app.gptzero.me/documents/4b379aba-8d8a-427b-ac67-d13af5eda8c9/share</a></td><td>Lisa Feldman Barrett. Emotions are constructed: How brains make meaning. Current Directions in Psychological Science, 25(6):403-408, 2016.</td><td><span>This </span><span><a target="_blank" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5390700/">article</a></span><span> is similar, but the title, and metadata are different.</span></td></tr><tr><td>MEMORIA: A Large Language Model, Instruction Data and Evaluation Benchmark for Intangible Cultural Heritage</td><td>3.33</td><td><a href="https://openreview.net/forum?id=0a6WXyNxBG" target="_blank">MEMORIA: A Large Language Model, Instruction Data and Evaluation Benchmark for Intangible Cultural Heritage | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/956129a3-11ee-4503-92e3-3ed5db12d2d6/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/956129a3-11ee-4503-92e3-3ed5db12d2d6/share">https://app.gptzero.me/documents/956129a3-11ee-4503-92e3-3ed5db12d2d6/share</a></td><td>Yang Cao, Rosa Martinez, and Sarah Thompson. Preserving indigenous languages through neural language models: Challenges and opportunities. Computational Linguistics, 49(3):567-592, 2023.</td><td>No Match</td></tr><tr><td>Reflexion: Language Models that Think Twice for Internalized Self-Correction</td><td>3.2</td><td><a href="https://openreview.net/forum?id=FDG2G7JDWO" target="_blank">Reflexion: Language Models that Think Twice for Internalized Self-Correction | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/45f2f68d-df09-4bbf-8513-588fe24f26fa/share" target="_blank">https://app.gptzero.me/documents/45f2f68d-df09-4bbf-8513-588fe24f26fa/share</a></td><td>Guang-He Xiao, Haolin Wang, and Yong-Feng Zhang. Rethinking uncertainty in llms: A case study on a fact-checking benchmark. arXiv preprint arXiv:2305.11382, 2023.</td><td>No Match</td></tr><tr><td>ECAM: Enhancing Causal Reasoning in Foundation Models with Endogenous Causal Attention Mechanism</td><td>3.0</td><td><a href="https://openreview.net/forum?id=sJxz6o5jKM" target="_blank">ECAM: Enhancing Causal Reasoning in Foundation Models with Endogenous Causal Attention Mechanism | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/d99a5552-38e0-459b-8746-4e64069b0640/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/d99a5552-38e0-459b-8746-4e64069b0640/share">https://app.gptzero.me/documents/d99a5552-38e0-459b-8746-4e64069b0640/share</a></td><td>Atticus Geiger, Zhengxuan Wu, Yonatan Rozner, Mirac Suzgun Naveh, Anna Nagarajan, Jure Leskovec, Christopher Potts, and Noah D Goodman. Causal interpretation of self-attention in pre-trained transformers. In Advances in Neural Information Processing Systems 36 (NeurIPS 2023), 2023. URL https://proceedings.neurips.cc/paper_files/paper/ 2023/file/642a321fba8a0f03765318e629cb93ea-Paper-Conference.pdf.</td><td>A paper with this title exists at the given URL, but the authors don't match.</td></tr><tr><td>MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding</td><td>3.0</td><td><a href="https://openreview.net/forum?id=phw13aNM08" target="_blank">MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/381ed9a6-b168-4cd0-81ad-1f50139c0737/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/381ed9a6-b168-4cd0-81ad-1f50139c0737/share">https://app.gptzero.me/documents/381ed9a6-b168-4cd0-81ad-1f50139c0737/share</a></td><td>Guy Dove. Language as a cognitive tool to imagine goals in curiosity-driven exploration. Nature Communications, 13(1):1-14, 2022.</td><td><span>An article with this title </span><span><a target="_blank" href="https://papers.neurips.cc/paper_files/paper/2020/file/274e6fcf4a583de4a81c6376f17673e7-Paper.pdf">exists</a></span><span>, but author and publication don't match.</span></td></tr><tr><td>LOSI: Improving Multi-agent Reinforcement Learning via Latent Opponent Strategy Identification</td><td>3.0</td><td><a href="https://openreview.net/forum?id=S0KGzCEhJp" target="_blank">LOSI: Improving Multi-agent Reinforcement Learning via Latent Opponent Strategy Identification | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/53e86e4b-a7e2-48d0-976b-240bfc412836/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/53e86e4b-a7e2-48d0-976b-240bfc412836/share">https://app.gptzero.me/documents/53e86e4b-a7e2-48d0-976b-240bfc412836/share</a></td><td>Jing Liang, Fan Zhou, Shuying Li, Jun Chen, Guandong Zhou, Huaiming Xu, and Xin Li. Learning opponent behavior for robust cooperation in multi-agent reinforcement learning. IEEE Transactions on Cybernetics, 53(12):7527-7540, 2023.</td><td>No Match</td></tr><tr><td>The Dynamic Interaction Field Transformer: A Universal, Tokenizer-Free Language Architecture</td><td>3.0</td><td><a href="https://openreview.net/forum?id=3flsDiwwUs" target="_blank">The Dynamic Interaction Field Transformer: A Universal, Tokenizer-Free Language Architecture | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/80fd90a6-c99e-4c31-af72-0da9e90949f6/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/80fd90a6-c99e-4c31-af72-0da9e90949f6/share">https://app.gptzero.me/documents/80fd90a6-c99e-4c31-af72-0da9e90949f6/share</a></td><td>Kaj Bostrom and Greg Durrett. Byte-level representation learning for multi-lingual named entity recognition. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4617-4627, 2020.</td><td>No Match</td></tr><tr><td>Strategema: Probabilistic Analysis of Adversarial Multi-Agent Behavior with LLMs in Social Deduction Games</td><td>3.0</td><td><a href="https://openreview.net/forum?id=xc9gn0fd19" target="_blank">Strategema: Probabilistic Analysis of Adversarial Multi-Agent Behavior with LLMs in Social Deduction Games | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/1155e8a8-f679-4942-8fd9-c47fb64ad967/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/1155e8a8-f679-4942-8fd9-c47fb64ad967/share">https://app.gptzero.me/documents/1155e8a8-f679-4942-8fd9-c47fb64ad967/share</a></td><td>Tom Eccles, Jeffrey Tweedale, and Yvette Izza. Let's pretend: A study of negotiation with autonomous agents. In 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT), volume 3, pp. 449-452. IEEE, 2009.</td><td>No Match</td></tr><tr><td>Understanding Transformer Architecture through Continuous Dynamics: A Partial Differential Equation Perspective</td><td>3.0</td><td><a href="https://openreview.net/forum?id=75SJoY9gTN" target="_blank">Understanding Transformer Architecture through Continuous Dynamics: A Partial Differential Equation Perspective | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/460a1a23-1a97-482a-9759-ade855a4a0b4/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/460a1a23-1a97-482a-9759-ade855a4a0b4/share">https://app.gptzero.me/documents/460a1a23-1a97-482a-9759-ade855a4a0b4/share</a></td><td>Zijie J Wang, Yuhao Choi, and Dongyeop Wei. On the identity of the representation learned by pre-trained language models. arXiv preprint arXiv:2109.01819, 2021.</td><td>No Match</td></tr><tr><td>Diffusion Aligned Embeddings</td><td>2.8</td><td><a href="https://openreview.net/forum?id=dPTXHBxa4Q" target="_blank">Diffusion Aligned Embeddings | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/3d95a003-06c6-4233-881b-03b1e29b4ba2/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/3d95a003-06c6-4233-881b-03b1e29b4ba2/share">https://app.gptzero.me/documents/3d95a003-06c6-4233-881b-03b1e29b4ba2/share</a></td><td>Yujia Wang, Hu Huang, Cynthia Rudin, and Yaron Shaposhnik. Pacmap: Dimension reduction using pairwise controlled manifold approximation projection. Machine Learning, 110:559-590, 2021.</td><td><span>A similar paper with two matching authors </span><span><a target="_blank" href="https://jmlr.org/papers/v22/20-1061.html">exists</a></span><span>, but the other authors, title, and journal are wrong.</span></td></tr><tr><td>Leveraging NLLB for Low-Resource Bidirectional Amharic – Afan Oromo Machine Translation</td><td>2.5</td><td><a href="https://openreview.net/forum?id=hav7s0ACAI" target="_blank">Leveraging NLLB for Low-Resource Bidirectional Amharic – Afan Oromo Machine Translation | Open Review</a></td><td><a href="https://app.gptzero.me/documents/813da6e2-f7e8-4c95-bdd8-7d29b8e4b641/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/813da6e2-f7e8-4c95-bdd8-7d29b8e4b641/share">https://app.gptzero.me/documents/813da6e2-f7e8-4c95-bdd8-7d29b8e4b641/share</a></td><td>Atnafa L. Tonja, Gebremedhin Gebremeskel, and Seid M. Yimam. Evaluating machine translation systems for ethiopian languages: A case study of amharic and afan oromo. Journal of Natural Language Engineering, 29(3):456-478, 2023.</td><td>No Match</td></tr><tr><td>Certified Robustness Training: Closed-Form Certificates via CROWN</td><td>2.5</td><td><a href="https://openreview.net/forum?id=iie4YsMjUp" target="_blank">Certified Robustness Training: Closed-Form Certificates via CROWN | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/53b60ef5-2ebf-403e-8123-3a9bb2da0f33/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/53b60ef5-2ebf-403e-8123-3a9bb2da0f33/share">https://app.gptzero.me/documents/53b60ef5-2ebf-403e-8123-3a9bb2da0f33/share</a></td><td>Huan Zhang, Hongge Chen, Chaowei Xiao, and Bo Zhang. Towards deeper and better certified defenses against adversarial attacks. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rJgG92A2m</td><td>No Match</td></tr><tr><td>Context-Aware Input Switching in Mobile Devices: A Multi-Language, Emoji-Integrated Typing System</td><td>2.5</td><td><a href="https://openreview.net/forum?id=TIaHBGnU7s" target="_blank">Context-Aware Input Switching in Mobile Devices: A Multi-Language, Emoji-Integrated Typing System | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/68998766-49c3-4269-9eca-3b6a76ed68b4/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/68998766-49c3-4269-9eca-3b6a76ed68b4/share">https://app.gptzero.me/documents/68998766-49c3-4269-9eca-3b6a76ed68b4/share</a></td><td>Ishan Tarunesh, Syama Sundar Picked, Sai Krishna Bhat, and Monojit Choudhury. Machine translation for code-switching: A systematic literature review. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics, pp. 3654-3670, 2021.</td><td><span>Partial match to this </span><span><a target="_blank" href="https://aclanthology.org/2021.acl-long.245.pdf">article</a></span><span>, but authors, title, and metadata is largely wrong.</span></td></tr><tr><td>Five-Mode Tucker-LoRA for Video Diffusion on Conv3D Backbones</td><td>2.5</td><td><a href="https://openreview.net/forum?id=BamabJR7Ed" target="_blank">Five-Mode Tucker-LoRA for Video Diffusion on Conv3D Backbones | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/eb0fd660-ed00-4769-a940-3d093d4f1ec1/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/eb0fd660-ed00-4769-a940-3d093d4f1ec1/share">https://app.gptzero.me/documents/eb0fd660-ed00-4769-a940-3d093d4f1ec1/share</a></td><td>Shengming Chen, Yuxin Wang, et al. Videocrafter: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2305.07932, 2023b.</td><td><span>A paper with the same title </span><span><a target="_blank" href="https://arxiv.org/pdf/2310.19512">exists</a></span><span>, but the authors and arXiv ID are wrong.</span></td></tr><tr><td>Activation-Guided Regularization: Improving Deep Classifiers using Feature-Space Regularization with Dynamic Prototypes</td><td>2.5</td><td><a href="https://openreview.net/forum?id=evNfQ1sqoQ" target="_blank">Activation-Guided Regularization: Improving Deep Classifiers using Feature-Space Regularization with Dynamic Prototypes | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/4031111e-24ef-4e06-908e-18ab99b08932/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/4031111e-24ef-4e06-908e-18ab99b08932/share">https://app.gptzero.me/documents/4031111e-24ef-4e06-908e-18ab99b08932/share</a></td><td>Wentao Cheng and Tong Zhang. Improving deep learning for classification with unknown label noise. In International Conference on Machine Learning, pp. 6059-6081. PMLR, 2023.</td><td><span>A similar paper </span><span><a target="_blank" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7484266/">exists</a></span><span>.</span></td></tr><tr><td>Sparse-Smooth Decomposition for Nonlinear Industrial Time Series Forecasting</td><td>2.5</td><td><a href="https://openreview.net/forum?id=8X0VgAQJMF" target="_blank">Sparse-Smooth Decomposition for Nonlinear Industrial Time Series Forecasting | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/c01ad49e-a788-4916-a6ee-f43314d14676/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/c01ad49e-a788-4916-a6ee-f43314d14676/share">https://app.gptzero.me/documents/c01ad49e-a788-4916-a6ee-f43314d14676/share</a></td><td>Yutian Chen, Kun Zhang, Jonas Peters, and Bernhard Schölkopf. Causal discovery and inference for nonstationary systems. Journal of Machine Learning Research, 22(103):1-72, 2021.</td><td>No Match</td></tr><tr><td>PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling</td><td>2.0</td><td><a href="https://openreview.net/forum?id=vobmXB4xf8" target="_blank">PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/ba257eea-e86c-4276-84c0-08b7465e1e3e/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/ba257eea-e86c-4276-84c0-08b7465e1e3e/share">https://app.gptzero.me/documents/ba257eea-e86c-4276-84c0-08b7465e1e3e/share</a></td><td><br>Xuechen Li, Juntang Zhuang, Yifan Ding, Zhaozong Jin, Yun chen Chen, and Stefanie Jegelka. Scalable gradients for stochastic differential equations. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics (AISTATS 2020), volume 108 of Proceedings of Machine Learning Research, pp. 3898-3908, 2020.</td><td><span>The paper </span><span><a target="_blank" href="https://proceedings.mlr.press/v108/li20i.html">exists</a></span><span> and the first author is correct but all other authors and the page range are wrong</span></td></tr><tr><td>SAFE-LLM: A Unified Framework for Reliable, Safe, And Secure Evaluation of Large Language Models</td><td>2.0</td><td><a href="https://openreview.net/forum?id=ky1IaQxlPh" target="_blank">SAFE-LLM: A Unified Framework for Reliable, Safe, And Secure Evaluation of Large Language Models | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/05ee7ff4-40e2-48b7-b5bd-8c307d7db669/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/05ee7ff4-40e2-48b7-b5bd-8c307d7db669/share">https://app.gptzero.me/documents/05ee7ff4-40e2-48b7-b5bd-8c307d7db669/share</a></td><td>Kuhn, J., et al. Semantic Entropy for Hallucination Detection. ACL 2023.</td><td><span>A similar paper with different authors can be found </span><span><a target="_blank" href="https://papers.miccai.org/miccai-2025/paper/0083_paper.pdf">here</a></span><span>.</span></td></tr><tr><td>PIPA: An Agent for Protein Interaction Identification and Perturbation Analysis</td><td>2.0</td><td><a href="https://openreview.net/forum?id=1fH0nFuvjo" target="_blank">PIPA: An Agent for Protein Interaction Identification and Perturbation Analysis | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/5031a806-1271-4fd3-b333-2554f47cb9fa/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/5031a806-1271-4fd3-b333-2554f47cb9fa/share">https://app.gptzero.me/documents/5031a806-1271-4fd3-b333-2554f47cb9fa/share</a></td><td>Alex Brown et al. Autonomous scientific experimentation at the advanced light source using language-model-driven agents. Nature Communications, 16:7001, 2025.</td><td>No Match</td></tr><tr><td>Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning</td><td>2.0</td><td><a href="https://openreview.net/forum?id=0qgcZvtQx0" target="_blank">Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/9d2e3239-99db-4712-be7f-e032156d92a5/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/9d2e3239-99db-4712-be7f-e032156d92a5/share">https://app.gptzero.me/documents/9d2e3239-99db-4712-be7f-e032156d92a5/share</a></td><td>DeepMind. Gemma scope: Scaling mechanistic interpretability to chain of thought. DeepMind Safety Blog, 2025. URL https://deepmindsafetyresearch.medium.com/ evaluating-and-monitoring-for-ai-scheming-8a7f2ce087f9. Discusses scaling mechanistic interpretability techniques to chain-of-thought and applications such as hallucination detection.</td><td><span>ThA similar URL </span><span><a target="_blank" href="https://deepmindsafetyresearch.medium.com/evaluating-and-monitoring-for-ai-scheming-d3448219a967">exists</a></span><span>, and the title is similar to this </span><span><a target="_blank" href="https://deepmind.google/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/">blog</a></span><span>. However, no exact match exists.</span></td></tr><tr><td>Graph-Based Operator Learning from Limited Data on Irregular Domains</td><td>2.0</td><td><a href="https://openreview.net/forum?id=fJZCNGRxFF" target="_blank">Graph-Based Operator Learning from Limited Data on Irregular Domains | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/6c52217f-fb88-4bd8-85aa-bd546e1fa88c/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/6c52217f-fb88-4bd8-85aa-bd546e1fa88c/share">https://app.gptzero.me/documents/6c52217f-fb88-4bd8-85aa-bd546e1fa88c/share</a></td><td>Liu, Y., Lütjens, B., Azizzadenesheli, K., and Anandkumar, A. (2022). U-netformer: A u-net style transformer for solving pdes. arXiv preprint arXiv:2206.11832.</td><td>No Match</td></tr><tr><td>KARMA: Knowledge-Aware Reward Mechanism Adjustment via Causal AI</td><td>2.0</td><td><a href="https://openreview.net/forum?id=EsumhpzFK9" target="_blank">KARMA: Knowledge-Aware Reward Mechanism Adjustment via Causal AI | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/92b6492c-68ad-41a3-ae35-628d67f053e0/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/92b6492c-68ad-41a3-ae35-628d67f053e0/share">https://app.gptzero.me/documents/92b6492c-68ad-41a3-ae35-628d67f053e0/share</a></td><td>Reinaldo A. C. Bianchi, Luis A. Celiberto Jr, and Ramon Lopez de Mantaras. Knowledge-based reinforcement learning: A survey. Journal of Artificial Intelligence Research, 62:215-261, 2018.</td><td>No Match</td></tr><tr><td>Microarchitecture Is Destiny: Performance and Accuracy of Quantized LLMs on Consumer Hardware</td><td>2.0</td><td><a href="https://openreview.net/forum?id=SzQGRR65c0" target="_blank">Microarchitecture Is Destiny: Performance and Accuracy of Quantized LLMs on Consumer Hardware | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/4504a39a-af72-41ab-9679-6f6a017a3275/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/4504a39a-af72-41ab-9679-6f6a017a3275/share">https://app.gptzero.me/documents/4504a39a-af72-41ab-9679-6f6a017a3275/share</a></td><td>Zhihang Jiang, Dingkang Wang, Yao Li, et al. Fp6-llm: Efficient llm serving through fp6-centric co-design. arXiv preprint arXiv:2401.14112, 2024.</td><td><span>the arXiv ID corresponds with a very similar </span><span><a target="_blank" href="https://arxiv.org/abs/2401.14112">paper</a></span><span>, but the authors are wrong and the title is altered.</span></td></tr><tr><td>Decoupling of Experts: A Knowledge-Driven Architecture for Efficient LLMs</td><td>1.6</td><td><a href="https://openreview.net/forum?id=57Ew3NKsQK" target="_blank">Decoupling of Experts: A Knowledge-Driven Architecture for Efficient LLMs | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/74eade70-da36-4635-8749-5e1d04748b6d/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/74eade70-da36-4635-8749-5e1d04748b6d/share">https://app.gptzero.me/documents/74eade70-da36-4635-8749-5e1d04748b6d/share</a></td><td>H Zhang, Y L, X W, Y Z, X Z, H W, X H, K G, Z W, H W, H C, H L, and J W. Matrix data pile: A trillion-tokenscale datasets for llm pre-training. arXiv preprint arXiv:2408.12151, 2024.</td><td><span>No Match; </span><span><a target="_blank" href="https://arxiv.org/abs/2408.12151">arxiv </a></span><span>is is unrelated</span></td></tr><tr><td>QUART: Agentic Reasoning To Discover Missing Knowledge in Multi-Domain Temporal Data.</td><td>1.5</td><td><a href="https://openreview.net/forum?id=TNqbfqSPoD" target="_blank">QUART: Agentic Reasoning To Discover Missing Knowledge in Multi-Domain Temporal Data. | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/c6f30343-3948-4c07-b7de-6b1407d5daa6/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/c6f30343-3948-4c07-b7de-6b1407d5daa6/share">https://app.gptzero.me/documents/c6f30343-3948-4c07-b7de-6b1407d5daa6/share</a></td><td>Meera Jain and Albert Chen. Explainable ai techniques for medical applications: A comprehensive review. AI in Healthcare, 5:22-37, 2024.</td><td>No Match</td></tr><tr><td>From Physics-Informed Models to Deep Learning: Reproducible AI Frameworks for Climate Resilience and Policy Alignment</td><td>1.5</td><td><a href="https://openreview.net/forum?id=9MWU9TKOLf" target="_blank">From Physics-Informed Models to Deep Learning: Reproducible AI Frameworks for Climate Resilience and Policy Alignment | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/a7ed6c42-4349-4b45-a356-0e325090e5af/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/a7ed6c42-4349-4b45-a356-0e325090e5af/share">https://app.gptzero.me/documents/a7ed6c42-4349-4b45-a356-0e325090e5af/share</a></td><td>MIT Climate Group. A cautionary tale for deep learning in climate science. https://example. com, 2019.</td><td><span>The </span><span><a target="_blank" href="https://www.climatechange.ai/papers/iclr2024/50">title</a></span><span> matches this paper, but the citation is obviously hallucinated.</span></td></tr><tr><td>A superpersuasive autonomous policy debating system</td><td>1.5</td><td><a href="https://openreview.net/forum?id=vGe3cv9NDD" target="_blank">A superpersuasive autonomous policy debating system | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/b792a4de-baa8-47d4-b880-87b330a482ce/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/b792a4de-baa8-47d4-b880-87b330a482ce/share">https://app.gptzero.me/documents/b792a4de-baa8-47d4-b880-87b330a482ce/share</a></td><td>Roy Bar-Haim, Shachar Bhattacharya, Michal Jacovi, Yosi Mass, Matan Orbach, Eyal Sliwowicz, and Noam Slonim. Key point analysis via contrastive learning and extractive argument summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7953-7962, Online and Punta Cana, Dominican Republic, November 2021a. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.629. URL https://aclanthology.org/2021.emnlp-main. 629.</td><td><span>A paper with the same title </span><span><a target="_blank" href="https://aclanthology.org/2021.argmining-1.19/">exists</a></span><span>, but the authors and URL are wrong.</span></td></tr><tr><td>AnveshanaAI: A Multimodal Platform for Adaptive AI/ML Education Through Automated Question Generation and Interactive Assessment</td><td>1.5</td><td><a href="https://openreview.net/forum?id=jm5tfxAcMP" target="_blank">AnveshanaAI: A Multimodal Platform for Adaptive AI/ML Education Through Automated Question Generation and Interactive Assessment | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/720d6d24-2223-4e0e-95b9-6dfce674f8c7/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/720d6d24-2223-4e0e-95b9-6dfce674f8c7/share">https://app.gptzero.me/documents/720d6d24-2223-4e0e-95b9-6dfce674f8c7/share</a></td><td>Shiyang Liu, Hongyi Xu, and Min Chen. Measuring and reducing perplexity in large-scale llms. arXiv preprint arXiv:2309.12345, 2023.</td><td>No Match</td></tr><tr><td>AI-Assisted Medical Triage Assistant</td><td>1.0</td><td><a href="https://openreview.net/forum?id=I06xiJR4ZL" target="_blank">AI-Assisted Medical Triage Assistant | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/391b5d76-929a-4f3f-addf-31f6993726f2/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/391b5d76-929a-4f3f-addf-31f6993726f2/share">https://app.gptzero.me/documents/391b5d76-929a-4f3f-addf-31f6993726f2/share</a></td><td>[3] K. Arnold, J. Smith, and A. Doe. Variability in triage decision making. Resuscitation, 85:12341239, 2014.</td><td>No Match</td></tr><tr><td>Deciphering Cross-Modal Feature Interactions in Multimodal AIGC Models: A Mechanistic Interpretability Approach</td><td>0.67</td><td><a href="https://openreview.net/forum?id=OClG6Kns1j" target="_blank">Deciphering Cross-Modal Feature Interactions in Multimodal AIGC Models: A Mechanistic Interpretability Approach | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/d4102812-01c4-45b2-aea8-59e467d31fd4/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/d4102812-01c4-45b2-aea8-59e467d31fd4/share">https://app.gptzero.me/documents/d4102812-01c4-45b2-aea8-59e467d31fd4/share</a></td><td>Shuyang Basu, Sachin Y Gadre, Ameet Talwalkar, and Zico Kolter. Understanding multimodal llms: the mechanistic interpretability of llava in visual question answering. arXiv preprint arXiv:2411.17346, 2024.</td><td><span>A paper with this title </span><span><a target="_blank" href="https://arxiv.org/abs/2411.10950">exists</a></span><span>, but the authors and arXiv ID are wrong.</span></td></tr><tr><td>Scalable Generative Modeling of Protein Ligand Trajectories via Graph Neural Diffusion Networks</td><td>0.5</td><td><a href="https://openreview.net/forum?id=xmlbDYVkWJ" target="_blank">Scalable Generative Modeling of Protein Ligand Trajectories via Graph Neural Diffusion Networks | OpenReview</a></td><td><a href="https://app.gptzero.me/documents/32d43311-6e69-4b88-be99-682e4eb0c2cc/share" target="_blank"><span></span></a><a target="_blank" href="https://app.gptzero.me/documents/32d43311-6e69-4b88-be99-682e4eb0c2cc/share">https://app.gptzero.me/documents/32d43311-6e69-4b88-be99-682e4eb0c2cc/share</a></td><td>E. Brini, G. Jayachandran, and M. Karplus. Coarse-graining biomolecular simulations via statistical learning. J. Chem. Phys., 154:040901, 2021.</td><td><span> There is no match for the title and authors, but the journal, volume, and year match this </span><span><a target="_blank" href="https://watermark02.silverchair.com/040901_1_5.0028249.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAABbMwggWvBgkqhkiG9w0BBwagggWgMIIFnAIBADCCBZUGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMSHoTXjV2Z9ks--65AgEQgIIFZst1jTOg4HX0CzOu904xLIDpl_Ilao4er21h8DWqIvmPg2WQA9ASKD1zYnWU7BGpj2JuRFqE-zRiMvHCIbnKju6WwyQAvmJatiNOzCTNYAgGorJvnyD-6Jgk9H--dA3-NJtYSJjtgG_HyzrDlurXQgW2kzGgSMZnO9sJhNXuMIA3uPzpSaDB33u7YYyu2iwp_FBCxoTRwSw4J3qnx5y_9nry9DsDb3GsjCRjkvP2lThLtGcPR3ABkIhxCMPWFz-Jj1psIuR3bgWak2yIkFrr6nzkUZZbPksTYAclXwKK-RyS8AT7qd7dUL99yRqHweqEmJNujY5n8UiU36dU5ROots6rhE-yvDhqIuRy-nX9vsq1gqGSiuK0JjT73ejUQpbp1LuUnG1YqiLCrXcooSD7Zlv6PWh14TIJSH6-4wlrUiDUgks0WWPMTYrTziXYBmdqjbzs7up2eYfySLQZTnCaeAQZzIhGr0NR35gtqPL5coukOiBpY9mkEbFm1cn7Ap2CPk2NplRojN9pdNTDJi-J1QDrqnhHYkVxB6cPb1oGEPeLWrsMcsoBiG-NpknUm0ggLkHhihbYeSQd12-6O9g_ayrMnLtc0XmuKRC2vdx63Sg5FltcnyLCVo8IGznue_kx7ESzGZbIFlbEYr93JudUMlZSbH9FZMCj3TnqjfFTC7gQ85SOPa448CNbNMkc6oouZiBPt5Y0yxoEUF8S-ovzZDAez3OadsxXZqhaCvkG-VxuLUBuedxewLCSSZNXpi5y_5CjyZuFmnqW5FywLiN4xxR_NDA3sRmGq-qtJ-d_EL3sivFhR1uoXdmGV4cAXV8JTsjUOviBOyJciI6WrgoLJNcHjG-D1zT8gQNccFvycRCc1UMvEyQZ2ovaeHtp0IzcBsel0vMVtGMBJtWMMy6fP6wiInZkbk36WkDS2-Htwt_IZmx8O2rtFy9PwfO6ElITs63TXey5aDcP5CVWRHWPYmPwWy-Iw5IbpDXu7nyNtVI2XQK2Fsjr9izhv-Jn_UiGweSLUL65953L1VsJYVPY_Sqw7ezlrY5u4Zd1wEhh7cVHlANG6tcXk8fA2ZgRkL8dhV4wtg2dc5soudXrsYiMq9UjD4ZLR18fJ4sqME_8GrCb01WlUTcU8FFjR2oGHXtfSo6TJ6TlXE55kC-eUqXbVmK6goGq5uJ-b1ATLyjpi1FsS9dPd3E5oVVgrkIV6BUSysQYRI-iJ_HUCNQip-9yTRx7Og3Mco6wgOJXSE4cZsEYRIpwiQWmdOPNB66jE-zwNB806bKagp9VtNCHXI6LCtnETjc1F8psigF2zYKLFGGoTCme-dCDr20pFi-CDPv12TvEh_0rZ56kw5WeJP8e6tZxQ8ULwSjZEIeA7BM5MWLa04gFgf8b3S6AvsBCwAIh4BROzjbAQjY-1dagpD52h9Tvg5h5-vuSkowgpWRZ4aJ1ephdAaHZhST8azQFfflZmvbX5oO7g0pyeqzDz_vhj5i6rZoCMJOTntJg4o6W79T-CZqUP_7rNXiigMpWx74nmDiIXfGZsUTk72P2dbHfKGfx_xsd4wfk9xj4BHrc46m4X4fklIbDsusMfsDbW8KsI4asi6XNtVYeYPwJC6tvLbWZOB_brE1ww9KGmmte5kuDZqXMdibGSLSpOoN-lPyA-wGLh-8_rXHLak7gqoK1Dn2RAES54WzUVokeslJsXpwp2nc9qypIpoaUNa8_LMNj3X9Svohc50tbUQ4_Rf3Dexr_kXbTKWnj8wkWx0fXzsML3ExxLfFcC3AFFI4rVfc_mHvH2IF3YGAgI38PbENuI9PwM0YB_-md-8lzQ0oKnGrMdXS2GCCq">article</a></span></td></tr></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Titans architecture, helping AI have long-term memory (126 pts)]]></title>
            <link>https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/</link>
            <guid>46181231</guid>
            <pubDate>Sun, 07 Dec 2025 12:23:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/">https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/</a>, See on <a href="https://news.ycombinator.com/item?id=46181231">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-gt-publish-date="20251204">
                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="il1w2">The <a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning)" target="_blank" rel="noopener noreferrer">Transformer architecture</a> revolutionized <a href="https://medium.com/machine-learning-basics/sequence-modelling-b2cdf244c233" target="_blank" rel="noopener noreferrer">sequence modeling</a> with its introduction of <a href="https://en.wikipedia.org/wiki/Attention_%28machine_learning%29" target="_blank" rel="noopener noreferrer">attention</a>, a mechanism by which models look back at earlier inputs to prioritize relevant input data. However, computational cost increases drastically with sequence length, which limits the ability to scale Transformer-based models to extremely long contexts, such as those required for full-document understanding or genomic analysis.</p><p data-block-key="36kb5">The research community explored various approaches for solutions, such as efficient linear <a href="https://www.d2l.ai/chapter_recurrent-modern/index.html" target="_blank" rel="noopener noreferrer">recurrent neural networks</a> (RNNs) and <a href="https://huggingface.co/blog/lbourdois/get-on-the-ssm-train" target="_blank" rel="noopener noreferrer">state space models</a> (SSMs) like <a href="https://arxiv.org/pdf/2405.21060" target="_blank" rel="noopener noreferrer">Mamba-2</a>. These models offer fast, linear scaling by compressing context into a fixed-size. However, this fixed-size compression cannot adequately capture the rich information in very long sequences.</p><p data-block-key="40m00">In two new papers, <a href="https://arxiv.org/abs/2501.00663" target="_blank" rel="noopener noreferrer"><i>Titans</i></a> and <a href="https://arxiv.org/pdf/2504.13173" target="_blank" rel="noopener noreferrer"><i>MIRAS</i></a>, we introduce an architecture and theoretical blueprint that combine the speed of RNNs with the accuracy of transformers. Titans is the specific architecture (the tool), and MIRAS is the theoretical framework (the blueprint) for generalizing these approaches. Together, they advance the concept of test-time memorization, the ability of an AI model to maintain long-term memory by incorporating more powerful “surprise” metrics (i.e., unexpected pieces of information) while the model is running and without dedicated offline retraining.</p><p data-block-key="eic3n">The MIRAS framework, as demonstrated by Titans, introduces a meaningful shift toward real-time adaptation. Instead of compressing information into a static state, this architecture actively learns and updates its own parameters as data streams in. This crucial mechanism enables the model to incorporate new, specific details into its core knowledge instantly.</p>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="Titans: Learning new context on the fly">
    


    <p>
        
            
                <h2>Titans: Learning new context on the fly</h2>
            
        
        
    </p>



    <p data-block-key="il1w2">An effective learning system requires distinct yet interconnected memory modules, mirroring the <a href="https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/">human brain's separation of short-term and long-term memory</a>.</p><p data-block-key="dpe7v">While attention mechanisms excel for precise, short-term memory, Titans introduces a novel neural <a href="https://arxiv.org/abs/2306.07174#:~:text=LongMem%20can:%20*%20Memorize%20long%20past%20context,Yan%20*%20Jianfeng%20Gao%20*%20Furu%20Wei" target="_blank" rel="noopener noreferrer">long-term memory module</a>, that, unlike the fixed-size vector or matrix memory in traditional RNNs, acts as a deep neural network (specifically, a <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron" target="_blank" rel="noopener noreferrer">multi-layer perceptron</a>). This memory module provides significantly higher expressive power, allowing the model to summarize large volumes of information without losing important context. The model isn't simply taking notes; it's understanding and synthesizing the entire story.</p><p data-block-key="e95op">Crucially, Titans doesn’t just passively store data. It actively learns <i>how</i> to recognize and retain important relationships and conceptual themes that connect tokens across the entire input. A key aspect of this ability is what we call the “surprise metric”. In human psychology, we know we quickly and easily forget routine, expected events but remember things that break the pattern — unexpected, surprising, or highly emotional events.</p>
</div>

                    
                    
    




                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="il1w2">In the context of Titans, the "surprise metric" is the model detecting a large difference between what it currently remembers and what the new input is telling it.</p><ul><li data-block-key="a9lns"><i>Low surprise</i>: If the new word is "cat" and the model's memory state already expects an animal word, the gradient (surprise) is low. It can safely skip memorizing the word "cat" in its permanent long-term state.</li><li data-block-key="2t2sa"><i>High surprise</i>: If the model's memory state is summarizing a serious financial report, and the new input is a picture of a banana peel (the unexpected event), the gradient (surprise) will be very high. This signals that the new input is important or anomalous, and it must be prioritized for permanent storage in the long-term memory module.</li></ul><p data-block-key="djj22">The model uses this internal error signal (the gradient) as a mathematical equivalent of saying, "This is unexpected and important!" This allows the Titans architecture to selectively update its long-term memory only with the most novel and context-breaking information, keeping the overall process fast and efficient.</p><p data-block-key="dm2am">Titans refines this mechanism by incorporating two critical elements:</p><ol><li data-block-key="bb101"><i>Momentum</i>: The model considers both "momentary surprise" (the current input) and "past surprise" (the recent context flow). This ensures relevant subsequent information is also captured, even if those tokens are not individually surprising.</li><li data-block-key="b269a"><i>Forgetting (weight decay)</i>: To manage the finite capacity of the memory when dealing with extremely long sequences, Titans employ an adaptive weight decay mechanism. This acts as a forgetting gate, allowing the model to discard information that is no longer needed.</li></ol>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="MIRAS: A unified view of sequence modeling">
    


    <p>
        
            
                <h2>MIRAS: A unified view of sequence modeling</h2>
            
        
        
    </p>



    <p data-block-key="il1w2">Every major breakthrough in sequence modeling — from modern transformers to the new, lightning-fast linear RNNs — is essentially the same thing under the hood: a highly complex <a href="https://www.geeksforgeeks.org/computer-organization-architecture/associative-memory/" target="_blank" rel="noopener noreferrer">associative memory</a> module.</p><p data-block-key="d91su">Accordingly, what makes MIRAS both unique and practical is the way it views AI modeling. Instead of seeing diverse architectures, it sees different methods of solving the same problem: efficiently combining new information with old memories without letting the essential concepts be forgotten<b>.</b></p><p data-block-key="7u78e">MIRAS defines a sequence model through four key design choices:</p><ul><li data-block-key="abcem"><i>Memory architecture</i>: The structure that stores information (e.g., a vector, matrix, or a deep multi-layer perceptron, like in Titans).</li><li data-block-key="5s0u1"><i>Attentional bias</i>: The internal learning objective the model optimizes that determines what it prioritizes.</li><li data-block-key="4qd03"><i>Retention gate</i>: The memory regularizer. MIRAS reinterprets "forgetting mechanisms" as specific forms of <a href="https://dev.to/nareshnishad/day-27-regularization-techniques-for-large-language-models-llms-4af3" target="_blank" rel="noopener noreferrer">regularization</a> that balance new learning against retaining past knowledge.</li><li data-block-key="9savd"><i>Memory algorithm</i>: The optimization algorithm used to update the memory.</li></ul>
</div>

                    
                    
    




                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="Transcending the mean squared error paradigm">
    


    <p>
        
            
                <h3>Transcending the mean squared error paradigm</h3>
            
        
        
    </p>



    <p data-block-key="il1w2">Virtually all successful existing sequence models rely on <a href="https://en.wikipedia.org/wiki/Mean_squared_error" target="_blank" rel="noopener noreferrer">mean squared error</a> (MSE) or <a href="https://medium.com/advanced-deep-learning/understanding-vector-similarity-b9c10f7506de" target="_blank" rel="noopener noreferrer">dot-product similarity</a> for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.</p><p data-block-key="1cust">MIRAS transcends this limitation by providing a generative framework to explore a more rich design space informed by the literature in optimization and statistics. This allows for the creation of novel architectures with <a href="https://en.wikipedia.org/wiki/Non-Euclidean_geometry" target="_blank" rel="noopener noreferrer">non-Euclidean objectives</a> and regularization.</p><p data-block-key="40qp3">Using MIRAS, we created three specific attention-free models:</p><ul><li data-block-key="fpeib"><i>YAAD</i>: We designed this MIRAS variant to be less sensitive to major errors or "outliers" (like a single typo in a large document). It uses a gentler math penalty (<a href="https://en.wikipedia.org/wiki/Huber_loss" target="_blank" rel="noopener noreferrer">Huber loss</a>) for mistakes, so it doesn't overreact to one-off issues. This makes the model more robust when the input data is messy or inconsistent.</li><li data-block-key="28vrl"><i>MONETA</i>: This model explores the use of more complex and strict mathematical penalties (called <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)" target="_blank" rel="noopener noreferrer">generalized norms</a>). It investigates whether using these more disciplined rules for both what the model attends to and what it forgets can lead to a more powerful and stable long-term memory system overall.</li><li data-block-key="d4e49"><i>MEMORA</i>: This model focuses on achieving the best possible memory stability by forcing its memory to act like a strict probability map. By using this constraint, it ensures that every time the memory state is updated, the changes are controlled and balanced. This guarantees a clean, stable process for integrating new information.Virtually all successful existing sequence models rely on <a href="https://en.wikipedia.org/wiki/Mean_squared_error" target="_blank" rel="noopener noreferrer">mean squared error</a> (MSE) or <a href="https://medium.com/advanced-deep-learning/understanding-vector-similarity-b9c10f7506de" target="_blank" rel="noopener noreferrer">dot-product similarity</a> for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.</li></ul>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="Experiments and results">
    


    <p>
        
            
                <h2>Experiments and results</h2>
            
        
        
    </p>



    <p data-block-key="il1w2">We rigorously compared Titans along with MIRAS variants (YAAD, MONETA, MEMORA) against leading architectures, including <a href="https://arxiv.org/abs/2003.04974" target="_blank" rel="noopener noreferrer">Transformer++</a>, <a href="https://arxiv.org/pdf/2405.21060" target="_blank" rel="noopener noreferrer">Mamba-2</a>, and <a href="https://arxiv.org/pdf/2412.06464" target="_blank" rel="noopener noreferrer">Gated DeltaNet</a>. We further validated versatility by testing Titans on genomic modeling (DNA) and time-series forecasting, proving the architecture generalizes effectively beyond text.</p><p data-block-key="a9v6c">Across both standard language modeling datasets (<a href="https://c4model.com/" target="_blank" rel="noopener noreferrer">C4</a>, <a href="https://huggingface.co/datasets/Salesforce/wikitext" target="_blank" rel="noopener noreferrer">WikiTex</a>t) and <a href="https://medium.com/@hetzer2807/zero-shot-reasoning-unleashed-the-magic-of-large-language-models-4e877dfe470e" target="_blank" rel="noopener noreferrer">zero-shot reasoning tasks</a> (<a href="https://arxiv.org/abs/1905.07830" target="_blank" rel="noopener noreferrer">HellaSwag</a>, PIQA), our models consistently demonstrated higher accuracy and <a href="https://en.wikipedia.org/wiki/Perplexity" target="_blank" rel="noopener noreferrer">perplexity</a> (a measure of how surprised an LLM is when looking at a piece of text).</p>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="The power of deep memory">
    


    <p>
        
            
                <h3>The power of deep memory</h3>
            
        
        
    </p>



    <p data-block-key="il1w2">Ablation studies clearly show that the depth of the memory architecture is crucial. When comparing long-term memory modules of the same size but different depths, modules with deeper memories consistently achieve lower perplexity in language modeling. Furthermore, they exhibit better scaling properties, maintaining performance as the sequence length increases significantly.</p>
</div>

                    
                    
    




                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="Language modeling and efficiency">
    


    <p>
        
            
                <h3>Language modeling and efficiency</h3>
            
        
        
    </p>



    <p data-block-key="il1w2">In language modeling and commonsense reasoning tasks, Titans architectures outperform state-of-the-art linear recurrent models (such as Mamba-2 and Gated DeltaNet) and Transformer++ baselines of comparable sizes. The novel MIRAS variants (MONETA, YAAD, MEMORA) also achieve improved performance compared to these baselines, validating the benefit of exploring robust, non-MSE optimization mechanisms. Importantly, these models maintain efficient, parallelizable training and fast linear inference speeds.</p>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="Extreme long-context recall">
    


    <p>
        
            
                <h3>Extreme long-context recall</h3>
            
        
        
    </p>



    <p data-block-key="il1w2">The most significant advantage of these new architectures is their ability to handle extremely long contexts. This is highlighted in the <a href="https://github.com/booydar/babilong" target="_blank" rel="noopener noreferrer">BABILong benchmark</a>, a task requiring reasoning across facts distributed in extremely long documents. In this challenging setting, Titans outperforms all baselines, including extremely large models like GPT-4, despite having many fewer parameters. Titans further demonstrates the capability to scale effectively to context window sizes larger than 2 million tokens.</p>
</div>

                    
                    
    




                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="Conclusion">
    


    <p>
        
            
                <h2>Conclusion</h2>
            
        
        
    </p>



    <p data-block-key="il1w2">The introduction of Titans and the MIRAS framework marks a significant advancement in sequence modeling. By employing deep neural networks as memory modules that learn to memorize as data is coming in, these approaches overcome the limitations of fixed-size recurrent states. Furthermore, MIRAS provides a powerful theoretical unification, revealing the connection between online optimization, associative memory, and architectural design. By moving beyond the standard Euclidean paradigm, this research opens the door to a new generation of sequence models that combine the efficiency of RNNs with the expressive power needed for the era of long-context AI.</p>
</div>

                    
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Java Hello World, LLVM Edition (101 pts)]]></title>
            <link>https://www.javaadvent.com/2025/12/java-hello-world-llvm-edition.html</link>
            <guid>46181076</guid>
            <pubDate>Sun, 07 Dec 2025 11:51:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.javaadvent.com/2025/12/java-hello-world-llvm-edition.html">https://www.javaadvent.com/2025/12/java-hello-world-llvm-edition.html</a>, See on <a href="https://news.ycombinator.com/item?id=46181076">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							
<p>After exploring Java bytecode in previous years (<a href="https://www.javaadvent.com/2022/12/jvm-hello-world.html">2022</a>, <a href="https://www.javaadvent.com/2023/12/my-first-compiler.html">2023</a>, <a href="https://www.javaadvent.com/2024/12/peering-through-the-peephole-build-a-peephole-optimiser-using-the-new-java-class-file-api.html">2024</a>), this year we’ll take an unexpected detour for a Java advent: instead of generating Java bytecode, we’ll use Java to build and execute <a href="https://llvm.org/docs/LangRef.html">LLVM IR</a>, the intermediate language behind compilers like clang.</p>



<p>Using Java’s <a href="https://docs.oracle.com/en/java/javase/22/core/foreign-function-and-memory-api.html">Foreign Function &amp; Memory (FFM) API</a>, we’ll call the LLVM C API, generate a “Hello, World!” program, and even JIT-compile it to native code – all from Java.</p>



<p>The task is simple: create a program that simply prints “Hello, World!”. But we must do this from Java via LLVM.</p>



<h2>What is LLVM?</h2>



<p>The <a href="https://llvm.org/">LLVM Project</a>, a collection of modular compiler and toolchain technologies, began as a research project over 20 years ago at the University of Illinois. It has grown significantly, underpinning many compilers and tools like clang.</p>



<p>The core libraries provide a source &amp; target independent optimizer along with code generation for a multitude of target machines. They are built around the <a href="https://llvm.org/docs/LangRef.html">LLVM IR</a>, an intermediate representation, which we’ll generate &amp; execute from Java.</p>



<h2>Installing LLVM</h2>



<p>To use the LLVM C API from Java, we’ll need LLVM’s shared libraries and headers installed locally. There is an automatic installation script available to easily install LLVM on Ubuntu/Debian systems, for example to install LLVM 20:</p>


<div><pre title="">$ wget https://apt.llvm.org/llvm.sh
$ chmod +x llvm.sh
$ ./llvm.sh 20
</pre></div>


<p>Once we have LLVM installed we can use the LLVM tooling to execute textual-form LLVM IR and we’ll also be able to use the LLVM C API in Java via the FFM API.</p>



<h2>LLVM IR</h2>



<p>LLVM IR is a strongly-typed, SSA-based intermediate language. It abstracts away most machine-specific details, making it easier to represent high-level constructs in a compiler-friendly format. There are three equivalent representations of the IR: an in-memory format, a bitcode format for serialisation and<a href="http://llvm.org/docs/LangRef.html"> a human readable assembly language representation</a>.</p>



<p>The textual form of the LLVM IR for our “Hello, World!” looks like this:</p>


<div><pre title="">@str = private constant [14 x i8] c"Hello, World!\00"

declare i32 @puts(ptr)

define i32 @main() {
  call i32 @puts(ptr @str)
  ret i32 0
}
</pre></div>


<p>Eventually, we’ll generate this via Java but, for now, if you save this in a file called helloworld.ll you can try executing it with the LLVM interpreter, <a href="https://llvm.org/docs/CommandGuide/lli.html">lli</a>:</p>


<div><pre title="">$ lli helloworld.ll
Hello, World!
</pre></div>


<p>There are a few types of entities used in the helloworld.ll example:</p>



<ul>
<li>A global variable containing the string “Hello World!”</li>



<li>A declaration of the external <a href="https://man7.org/linux/man-pages/man3/puts.3.html">libc puts</a> function</li>



<li>A definition of the main function</li>



<li>Instructions to call puts and return an integer exit code</li>
</ul>



<p>You can dive deeper into the <a href="https://jameshamilton.eu/programming/llvm-hello-world">LLVM “Hello, World!” example here</a> if you like before continuing to the next section, where we’ll start using the Java FFM API.</p>



<h2>What is the Java FFM API?</h2>



<p>The <a href="https://docs.oracle.com/en/java/javase/22/core/foreign-function-and-memory-api.html">Foreign Function and Memory (FFM) API</a> enables Java programs to interoperate with code and data outside the Java runtime. The API is a replacement for the older JNI API that enables Java programs to call native libraries in a safer way. The API can be used to call foreign functions and safely access foreign memory that is not managed by the JVM.</p>



<p>A companion to the FFM API is a tool named <a href="https://docs.oracle.com/en/java/javase/21/core/call-native-functions-jextract.html">jextract</a> that can automatically generate Java bindings from a C header file. <code>jextract</code> parses C header files and automatically generates the Java source code with method handles and type-safe FFM bindings.</p>



<p>We’ll use the <code>jextract</code> tool to generate bindings for the LLVM C API and those bindings will allow us to call the LLVM API from Java.</p>



<h2>Getting started</h2>



<p>First, let’s create a simple project to start. We’ll use maven to build our project but you can use another build tool if you like, it’s not important:</p>


<div><pre title="">$ mvn archetype:generate -DgroupId=com.example -DartifactId=jvm-llvm-helloworld -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false
</pre></div>


<p>Once you have a project skeleton, update the pom.xml file to set the Java version &gt;= 22:</p>


<div><pre title=""> &lt;properties&gt;
    &lt;maven.compiler.source&gt;25&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;25&lt;/maven.compiler.target&gt;
 &lt;/properties&gt;
</pre></div>


<p>Then build and run the program to check everything is OK:</p>


<div><pre title="">$ mvn clean install
$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App
Hello World!
</pre></div>


<p>The maven generated sample already printed “Hello, World!” but that’s too easy! We’ll remove that and generate it via LLVM in the following sections.</p>



<p>Let’s now create the LLVM bindings using <code>jextract</code> so that we can use the LLVM API.</p>



<h2>Creating LLVM bindings</h2>



<p>We’ll use jextract to generate bindings from the LLVM C API header files. Make sure LLVM is available on your system (see Installing LLVM above) and you’ll also need to download <a href="https://docs.oracle.com/en/java/javase/21/core/call-native-functions-jextract.html">jextract</a>.</p>



<p>The following jextract command (on Linux) will create Java bindings for the specified LLVM C headers, placing the generated code into the <code>com.example.llvm</code> package within the <code>src/main/java</code> directory, with the main header class named <code>LLVM</code>.</p>


<div><pre title="">$ jextract -l LLVM-20 -I /usr/include/llvm-c-20 \
     -I /usr/include/llvm-20 \
     -t com.example.llvm \
     --output src/main/java \
     --header-class-name LLVM \
     /usr/include/llvm-c-20/llvm-c/Core.h \
     /usr/include/llvm-c-20/llvm-c/Support.h \
     /usr/include/llvm-c-20/llvm-c/ExecutionEngine.h \
     /usr/include/llvm-c-20/llvm-c/Target.h \
     /usr/include/llvm-c-20/llvm-c/TargetMachine.h
</pre></div>


<p>To test the generated bindings, let’s print the LLVM version using the static method generated for LLVM version string constant: edit the sample’s App.java file to print the version using the following:</p>



<p>If you run this, you’ll see the LLVM version printed:</p>


<div><pre title="">$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar --enable-native-access=ALL-UNNAMED com.example.App
LLVM version: 20.0.0
</pre></div>


<p>Note the use of <code>--enable-native-access=ALL-UNNAMED</code> to prevent warnings about native code access; I’ll omit this for brevity in later commands.</p>



<h2>Memory Segments</h2>



<p>The <code>LLVM_VERSION_STRING</code> method returns a <a href="https://docs.oracle.com/en/java/javase/22/docs/api/java.base/java/lang/foreign/MemorySegment.html">MemorySegment</a> rather than a Java String. In the FFM API, a <code>MemorySegment</code> represents a contiguous region of memory—either on or off the Java heap—enabling safe, structured access to native memory.</p>



<p>Let’s take a look at the implementation in the generated source file:</p>


<div><pre title="">   public static MemorySegment LLVM_VERSION_STRING() {
    class Holder {
      static final MemorySegment LLVM_VERSION_STRING
         = LLVM.LIBRARY_ARENA.allocateFrom("20.0.0");
    }
    return Holder.LLVM_VERSION_STRING;
  }
</pre></div>


<p>This method allocates memory containing the version string that contains the version number. The allocated MemorySegment is returned from the method and to get the String back into Java-land we need to call <code>getString(0)</code> on the memory segment which reads a null-terminated string at the given offset (<code>0</code>), using the UTF-8 charset.</p>



<p>Memory segments are managed through arenas (such as the <code>LLVM.LIBRARY_ARENA</code> in the code above), which bridge Java’s managed heap and foreign memory spaces by applying familiar resource management patterns like try-with-resources.</p>



<p>Since we’ll need to allocate native memory, let’s declare an <a href="https://docs.oracle.com/en/java/javase/22/docs/api/java.base/java/lang/foreign/Arena.html">Arena</a>:</p>


<div><pre title=""> public static void main(String[] args)
 {
    try (Arena arena = Arena.ofConfined()) {
       // TODO
    }
 }
</pre></div>


<h2>Creating an LLVM module</h2>



<p>As a reminder, we need to recreate the following LLVM IR via the LLVM C API:</p>


<div><pre title="">declare i32 @puts(ptr)

@str = constant [14 x i8] c"Hello, World!\00"

define i32 @main() {
  call i32 @puts(ptr @str)
  ret i32 0
}
</pre></div>


<p>Let’s start by creating an LLVM module – the container for all functions and globals – and print it so that we can run it through the LLVM interpreter:</p>


<div><pre title="">public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // TODO: Fill in the module
            
  	var llvmIrCharPtr = LLVMPrintModuleToString(module);

    try {
      System.out.println(llvmIrCharPtr.getString(0));
    } catch (Exception e) {
      System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    }

	   // Clean up LLVM resources
     LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeModule(module);
  }
}
</pre></div>


<p>If we execute this now, we’ll see an empty IR module:</p>


<div><pre title="">$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App
; ModuleID = 'hello'
source_filename = "hello"
</pre></div>


<p>If you pass this output through the LLVM interpreter, you’ll see that it tries to execute the module but cannot find the entry point main function:</p>


<div><pre title="">$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App | lli
Symbols not found: [ main ]
</pre></div>


<p>We now have an LLVM module, but it has no executable code – the interpreter rightly complains that main is missing; so let’s add the main function.</p>



<h2>Adding a main function</h2>



<p>The entry point to our program is the function named main which takes no parameters and returns an integer exit code, where a non-negative integer denotes success. We can add a function to the module using the <a href="https://llvm.org/doxygen/group__LLVMCCoreModule.html#gaaf70ab92a261e636dc0b2cf30cfede9a">LLVMAddFunction</a> function, along with the <a href="https://llvm.org/doxygen/classllvm_1_1FunctionType.html">LLVMFunctionType</a> and <a href="https://llvm.org/doxygen/group__LLVMCCoreTypeInt.html#ga71ee1444644798c8750ffb5be6a06819">LLVMInt32Type</a> functions to create the function type.&nbsp;</p>



<p>Notice that all of these functions return a <code>MemorySegment</code> and all 3 <code>LLVMAddFunction</code> parameters are <code>MemorySegment</code>s.</p>


<div><pre title="">public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

    // TODO: Add the code

  	var llvmIrCharPtr = LLVMPrintModuleToString(module);

    try {
      System.out.println(llvmIrCharPtr.getString(0));
    } catch (Exception e) {
      System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    }

	   // Clean up LLVM resources
     LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeModule(module);
  }
}
</pre></div>


<p>If you execute this now you’ll see a declaration of the main function but it has no body so the LLVM interpreter will produce the same error:</p>


<div><pre title="">$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App
; ModuleID = 'hello'
source_filename = "hello"

declare i32 @main()

$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App|lli
Symbols not found: [ main ]
</pre></div>


<p>Next we’ll add some instructions to the body of the function.</p>



<h2>Adding an entry basic block</h2>



<p>In order to add code to a function we need to add at least 1 basic block – the entry block. A basic block is a sequence of instructions within a function that executes straight through from start to finish, with no branches in the middle. These blocks form the nodes of the Control-Flow Graph (CFG), and they connect to each other based on how control flows between them.</p>



<p>Basic blocks can be added to a function with the <a href="https://llvm.org/doxygen/group__LLVMCCoreValueBasicBlock.html#gaf1760061b837b6b255224f243cfe94c8">LLVMAppendBasicBlock</a> function:</p>


<div><pre title="">public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 
	  // TODO: Add the instructions

  	var llvmIrCharPtr = LLVMPrintModuleToString(module);

    try {
      System.out.println(llvmIrCharPtr.getString(0));
    } catch (Exception e) {
      System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    }

	   // Clean up LLVM resources
     LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeModule(module);
  }
}
</pre></div>


<p>If you run the program through <code>lli</code> now, you’ll see a different error:</p>


<div><pre title="">$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App | lli
lli: &lt;stdin&gt;:6:1: error: expected instruction opcode
}
</pre></div>


<p>That makes sense, we don’t yet have any instructions in our function!</p>



<h2>Building instructions</h2>



<p>To add instructions, we first create an instruction builder using the <a href="https://llvm.org/doxygen/group__LLVMCCoreInstructionBuilder.html#ga0b336d71db0aa80eef35fe0572ca69bb">LLVMCreateBuilder</a> function. This gives us an LLVMBuilder that we can use to insert new instructions into a basic block.</p>



<p>We’ll also use the <a href="https://llvm.org/doxygen/group__LLVMCCoreInstructionBuilder.html#gab9bdbf21d7fd0bc5a2ee669b333ced2a">LLVMPositionBuilderAtEnd</a> function to position the builder at the end of the entry block and <a href="https://llvm.org/doxygen/group__LLVMCCoreInstructionBuilder.html#gab246fd9294b3801060f0ceb972c262d0">LLVMBuildRet</a> to build a return instruction:</p>


<div><pre title="">public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 	  var builder = LLVMCreateBuilder();
    LLVMPositionBuilderAtEnd(builder, entry);

    // TODO: Call puts “Hello, World!”

	  // Return 0
    LLVMBuildRet(builder, LLVMConstInt(int32Type, 0, 0));

  	var llvmIrCharPtr = LLVMPrintModuleToString(module);

    try {
      System.out.println(llvmIrCharPtr.getString(0));
    } catch (Exception e) {
      System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    }

	   // Clean up LLVM resources
     LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeBuilder(builder);
     LLVMDisposeModule(module);
  }
}
</pre></div>


<p>If you run the program and pass the output through <code>lli</code> now, you’ll see nothing happen:</p>


<div><pre title="">$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App | lli
</pre></div>


<p>Great news – the errors are gone! Checking the return code confirms the program exited successfully, returning 0.</p>





<p>Try changing the 0 to some other number to confirm that the value is indeed coming from the exit code returned by the LLVM IR program!</p>



<h2>Global variables</h2>



<p>A global variable, defined at the top-level in LLVM IR, defines a region of memory with a fixed address that is allocated when the program is loaded, rather than dynamically at runtime. Globals can be declared as constant if their values will never change.</p>



<p>We’ll add the string “Hello, World!” to our LLVM program as a global constant.</p>


<div><pre title="">public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 	  var builder = LLVMCreateBuilder();
    LLVMPositionBuilderAtEnd(builder, entry);

	  // Create a global string constant containing "Hello, World!"
    var helloStr = 
           LLVMBuildGlobalStringPtr(builder,
                arena.allocateFrom("Hello, World!"),
                arena.allocateFrom("hello_str"));

    // TODO: Call puts “Hello, World!”

	  // Return 0
    LLVMBuildRet(builder, LLVMConstInt(int32Type, 0, 0));

  	var llvmIrCharPtr = LLVMPrintModuleToString(module);

    try {
      System.out.println(llvmIrCharPtr.getString(0));
    } catch (Exception e) {
      System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    }

	   // Clean up LLVM resources
     LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeBuilder(builder);
     LLVMDisposeModule(module);
  }
}
</pre></div>


<p>We don’t use the <code>hello_str</code> yet so running <code>lli</code> would produce the same as before, but you can see the string is now declared in the LLVM IR (prefixed with @ because it is a global, like the main function):</p>


<div><pre title="">$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App 
; ModuleID = 'hello'
source_filename = "hello"

@hello_str = private unnamed_addr constant [14 x i8] c"Hello, World!\00", align 1

define i32 @main() {
entry:
  ret i32 0
}
</pre></div>


<p>Let’s add the final instruction next – a call to <code>puts</code> to print the string.</p>



<h2>Calling functions</h2>



<p>Before we can call the libc puts function we must declare it in the module by first building the function type and then calling <code>LLVMAddFunction</code> to add it to the module:</p>


<div><pre title="">public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 	  var builder = LLVMCreateBuilder();
    LLVMPositionBuilderAtEnd(builder, entry);

	  // Create a global string constant containing "Hello, World!"
    var helloStr = 
           LLVMBuildGlobalStringPtr(builder,
                arena.allocateFrom("Hello, World!"),
                arena.allocateFrom("hello_str"));

    // Create puts function type: int puts(char*)
    var putsParamTypes = arena.allocate(ADDRESS, 1);
    var charPtrType = LLVMPointerType(LLVMInt8Type(), 0);
    putsParamTypes.set(ADDRESS, 0, charPtrType);
    var putsType = LLVMFunctionType(int32Type, putsParamTypes, 1, 0);
    // Add puts function to the module
    var putsFunc = LLVMAddFunction(module, arena.allocateFrom("puts"), putsType);

    // TODO: Call puts “Hello, World!”

	  // Return 0
    LLVMBuildRet(builder, LLVMConstInt(int32Type, 0, 0));

  	var llvmIrCharPtr = LLVMPrintModuleToString(module);

    try {
      System.out.println(llvmIrCharPtr.getString(0));
    } catch (Exception e) {
      System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    }

	   // Clean up LLVM resources
     LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeBuilder(builder);
     LLVMDisposeModule(module);
  }
}
</pre></div>


<p>Now that we’ve declared the function we can call it with the <code>@hello_str</code> global as a parameter using the <a href="https://llvm.org/doxygen/group__LLVMCCoreInstructionBuilder.html#ga40cf5b22d9d28f1f82e76048a69d537a">LLVMBuildCall2</a> function:</p>


<div><pre title="">public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 	  var builder = LLVMCreateBuilder();
    LLVMPositionBuilderAtEnd(builder, entry);

	  // Create a global string constant containing "Hello, World!"
    var helloStr = 
           LLVMBuildGlobalStringPtr(builder,
                arena.allocateFrom("Hello, World!"),
                arena.allocateFrom("hello_str"));

    // Create puts function type: int puts(char*)
    var putsParamTypes = arena.allocate(ADDRESS, 1);
    var charPtrType = LLVMPointerType(LLVMInt8Type(), 0);
    putsParamTypes.set(ADDRESS, 0, charPtrType);
    var putsType = LLVMFunctionType(int32Type, putsParamTypes, 1, 0);
    // Add puts function to the module
    var putsFunc = LLVMAddFunction(module, arena.allocateFrom("puts"), putsType);

    // Create puts function call
    var callArgs = arena.allocate(ADDRESS, 1);
    callArgs.set(ADDRESS, 0, helloStr);
    LLVMBuildCall2(builder, putsType, putsFunc, callArgs, 1, arena.allocateFrom("puts"));

	  // Return 0
    LLVMBuildRet(builder, LLVMConstInt(int32Type, 0, 0));

  	var llvmIrCharPtr = LLVMPrintModuleToString(module);

    try {
      System.out.println(llvmIrCharPtr.getString(0));
    } catch (Exception e) {
      System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    }

	   // Clean up LLVM resources
     LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeBuilder(builder);
     LLVMDisposeModule(module);
  }
}
</pre></div>


<p>Running the program’s output through <code>lli</code> will finally display the expected result: “Hello, World!”:</p>


<div><pre title="">$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App | lli
Hello, World!
</pre></div>


<p>Congratulations, you’ve successfully used the Java FFM API to call the LLVM C API to build an LLVM module that contains code to print “Hello, World!”.</p>



<h2>Just-in-time (JIT) Compilation</h2>



<p>So far, we’ve been printing LLVM IR and letting <code>lli</code> execute it. But LLVM also exposes a JIT compiler API, allowing us to generate and execute machine code in-memory. Let’s see how to JIT our “Hello, World!” directly from Java.</p>



<p>LLVM IR is target independent but once we start compiling to native code we must know which machine we are targeting. We’ll target x86 Linux in the following code; if you’re using ARM, Mac or Windows you’ll need to adjust the code for your machine.</p>



<p>The first step is to initialise and create an LLVM JIT compiler for the target machine:</p>


<div><pre title="">public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 	  var builder = LLVMCreateBuilder();
    LLVMPositionBuilderAtEnd(builder, entry);

	  // Create a global string constant containing "Hello, World!"
    var helloStr = 
           LLVMBuildGlobalStringPtr(builder,
                arena.allocateFrom("Hello, World!"),
                arena.allocateFrom("hello_str"));

    // Create puts function type: int puts(char*)
    var putsParamTypes = arena.allocate(ADDRESS, 1);
    var charPtrType = LLVMPointerType(LLVMInt8Type(), 0);
    putsParamTypes.set(ADDRESS, 0, charPtrType);
    var putsType = LLVMFunctionType(int32Type, putsParamTypes, 1, 0);
    // Add puts function to the module
    var putsFunc = LLVMAddFunction(module, arena.allocateFrom("puts"), putsType);

    // Create puts function call
    var callArgs = arena.allocate(ADDRESS, 1);
    callArgs.set(ADDRESS, 0, helloStr);
    LLVMBuildCall2(builder, putsType, putsFunc, callArgs, 1, arena.allocateFrom("puts"));

	  // Return 0
    LLVMBuildRet(builder, LLVMConstInt(int32Type, 0, 0));

    // Initialize LLVM JIT + x86 Target
    LLVMLinkInMCJIT();
    LLVMInitializeX86Target();
    LLVMInitializeX86TargetInfo();
    LLVMInitializeX86TargetMC();
    LLVMInitializeX86AsmPrinter();
    LLVMInitializeX86AsmParser();

    // Create JIT execution engine
    var jitCompiler = arena.allocate(ADDRESS);
    var jitErrorMsgPtrPtr = arena.allocate(ADDRESS);
    LLVMCreateJITCompilerForModule(jitCompiler, module, /* optimization level = */ 2, jitErrorMsgPtrPtr);

    // Disable the IR printing now
  	// var llvmIrCharPtr = LLVMPrintModuleToString(module);
    //
    // try {
    //  System.out.println(llvmIrCharPtr.getString(0));
    // } catch (Exception e) {
    //   System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    // }

	   // Clean up LLVM resources
     // LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeBuilder(builder);
     LLVMDisposeModule(module);
  }
}
</pre></div>


<p><code>LLVMCreateJITCompilerForModule</code> sets up a JIT execution engine to compile an LLVM module to native machine code. <code>LLVMCreateJITCompilerForModule</code> will return a 1 upon failure and then we can check the error message string for more information but to simplify things we’ll ignore error handling for now.&nbsp;</p>



<p>Requesting the address of the main function triggers its compilation – LLVM generates the machine code only when it’s first needed, hence the name Just-In-Time compilation. We can retrieve a pointer to the compiled function using <code>LLVMGetPointerToGlobal</code>:</p>


<div><pre title="">public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 	  var builder = LLVMCreateBuilder();
    LLVMPositionBuilderAtEnd(builder, entry);

	  // Create a global string constant containing "Hello, World!"
    var helloStr = 
           LLVMBuildGlobalStringPtr(builder,
                arena.allocateFrom("Hello, World!"),
                arena.allocateFrom("hello_str"));

    // Create puts function type: int puts(char*)
    var putsParamTypes = arena.allocate(ADDRESS, 1);
    var charPtrType = LLVMPointerType(LLVMInt8Type(), 0);
    putsParamTypes.set(ADDRESS, 0, charPtrType);
    var putsType = LLVMFunctionType(int32Type, putsParamTypes, 1, 0);
    // Add puts function to the module
    var putsFunc = LLVMAddFunction(module, arena.allocateFrom("puts"), putsType);

    // Create puts function call
    var callArgs = arena.allocate(ADDRESS, 1);
    callArgs.set(ADDRESS, 0, helloStr);
    LLVMBuildCall2(builder, putsType, putsFunc, callArgs, 1, arena.allocateFrom("puts"));

	  // Return 0
    LLVMBuildRet(builder, LLVMConstInt(int32Type, 0, 0));

    // Initialize LLVM JIT + x86 Target
    LLVMLinkInMCJIT();
    LLVMInitializeX86Target();
    LLVMInitializeX86TargetInfo();
    LLVMInitializeX86TargetMC();
    LLVMInitializeX86AsmPrinter();
    LLVMInitializeX86AsmParser();

    // Create JIT execution engine
    var jitCompiler = arena.allocate(ADDRESS);
    var jitErrorMsgPtrPtr = arena.allocate(ADDRESS);
    LLVMCreateJITCompilerForModule(jitCompiler, module, /* optimization level = */ 2, jitErrorMsgPtrPtr);

    var executionEngine = jitCompiler.get(ADDRESS, 0);
    var addressOfMainFunc = LLVMGetPointerToGlobal(executionEngine, mainFunc);

    // Disable the IR printing now
  	// var llvmIrCharPtr = LLVMPrintModuleToString(module);
    //
    // try {
    //  System.out.println(llvmIrCharPtr.getString(0));
    // } catch (Exception e) {
    //   System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    // }

	   // Clean up LLVM resources
     // LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeBuilder(builder);
     LLVMDisposeModule(module);
  }
}
</pre></div>


<p>Now that we’ve compiled the function, we need a way to invoke it from Java. To do this, we use the foreign linker to create a <code>MethodHandle</code> for the JIT-compiled main function. This handle acts as a callable reference to the native code:</p>


<div><pre title="">public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 	  var builder = LLVMCreateBuilder();
    LLVMPositionBuilderAtEnd(builder, entry);

	  // Create a global string constant containing "Hello, World!"
    var helloStr = 
           LLVMBuildGlobalStringPtr(builder,
                arena.allocateFrom("Hello, World!"),
                arena.allocateFrom("hello_str"));

    // Create puts function type: int puts(char*)
    var putsParamTypes = arena.allocate(ADDRESS, 1);
    var charPtrType = LLVMPointerType(LLVMInt8Type(), 0);
    putsParamTypes.set(ADDRESS, 0, charPtrType);
    var putsType = LLVMFunctionType(int32Type, putsParamTypes, 1, 0);
    // Add puts function to the module
    var putsFunc = LLVMAddFunction(module, arena.allocateFrom("puts"), putsType);

    // Create puts function call
    var callArgs = arena.allocate(ADDRESS, 1);
    callArgs.set(ADDRESS, 0, helloStr);
    LLVMBuildCall2(builder, putsType, putsFunc, callArgs, 1, arena.allocateFrom("puts"));

	  // Return 0
    LLVMBuildRet(builder, LLVMConstInt(int32Type, 0, 0));

    // Initialize LLVM JIT + x86 Target
    LLVMLinkInMCJIT();
    LLVMInitializeX86Target();
    LLVMInitializeX86TargetInfo();
    LLVMInitializeX86TargetMC();
    LLVMInitializeX86AsmPrinter();
    LLVMInitializeX86AsmParser();

    // Create JIT execution engine
    var jitCompiler = arena.allocate(ADDRESS);
    var jitErrorMsgPtrPtr = arena.allocate(ADDRESS);
    LLVMCreateJITCompilerForModule(jitCompiler, module, /* optimization level = */ 2, jitErrorMsgPtrPtr);

    var executionEngine = jitCompiler.get(ADDRESS, 0);
    var addressOfMainFunc = LLVMGetPointerToGlobal(executionEngine, mainFunc);

    // Create method handle to the int main() function that
    // we just created and compiled.
    var functionHandle = Linker.nativeLinker().downcallHandle(
        addressOfMainFunc,
        FunctionDescriptor.of(/* returnType = */ JAVA_INT)
    );

    // Disable the IR printing now
  	// var llvmIrCharPtr = LLVMPrintModuleToString(module);
    //
    // try {
    //  System.out.println(llvmIrCharPtr.getString(0));
    // } catch (Exception e) {
    //   System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    // }

	   // Clean up LLVM resources
     // LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeBuilder(builder);
     LLVMDisposeModule(module);
  }
}
</pre></div>


<p>The <code>downcallHandle</code> method tells Java how to interpret the native function’s signature – in this case, a function that takes no arguments and returns an int.</p>



<p>Now we can invoke the compiled native function directly from Java, just like a regular method call:</p>


<div><pre title="">public static void main(String[] args)
{
  try (Arena arena = Arena.ofConfined()) {
    var module = LLVMModuleCreateWithName(arena.allocateFrom("hello"));
          
    // Create main function signature: int main()
    var int32Type = LLVMInt32Type();
    var mainType = LLVMFunctionType(int32Type, NULL, 0, 0);
    var mainName = arena.allocateFrom("main");
    var mainFunc = LLVMAddFunction(module, mainName, mainType);

	  var entry = LLVMAppendBasicBlock(mainFunc, arena.allocateFrom("entry"));
 	  var builder = LLVMCreateBuilder();
    LLVMPositionBuilderAtEnd(builder, entry);

	  // Create a global string constant containing "Hello, World!"
    var helloStr = 
           LLVMBuildGlobalStringPtr(builder,
                arena.allocateFrom("Hello, World!"),
                arena.allocateFrom("hello_str"));

    // Create puts function type: int puts(char*)
    var putsParamTypes = arena.allocate(ADDRESS, 1);
    var charPtrType = LLVMPointerType(LLVMInt8Type(), 0);
    putsParamTypes.set(ADDRESS, 0, charPtrType);
    var putsType = LLVMFunctionType(int32Type, putsParamTypes, 1, 0);
    // Add puts function to the module
    var putsFunc = LLVMAddFunction(module, arena.allocateFrom("puts"), putsType);

    // Create puts function call
    var callArgs = arena.allocate(ADDRESS, 1);
    callArgs.set(ADDRESS, 0, helloStr);
    LLVMBuildCall2(builder, putsType, putsFunc, callArgs, 1, arena.allocateFrom("puts"));

	  // Return 0
    LLVMBuildRet(builder, LLVMConstInt(int32Type, 0, 0));

    // Initialize LLVM JIT + x86 Target
    LLVMLinkInMCJIT();
    LLVMInitializeX86Target();
    LLVMInitializeX86TargetInfo();
    LLVMInitializeX86TargetMC();
    LLVMInitializeX86AsmPrinter();
    LLVMInitializeX86AsmParser();

    // Create JIT execution engine
    var jitCompiler = arena.allocate(ADDRESS);
    var jitErrorMsgPtrPtr = arena.allocate(ADDRESS);
    LLVMCreateJITCompilerForModule(jitCompiler, module, /* optimization level = */ 2, jitErrorMsgPtrPtr);

    var executionEngine = jitCompiler.get(ADDRESS, 0);
    var addressOfMainFunc = LLVMGetPointerToGlobal(executionEngine, mainFunc);

    // Create method handle to the int main() function that
    // we just created and compiled.
    var functionHandle = Linker.nativeLinker().downcallHandle(
        addressOfMainFunc,
        FunctionDescriptor.of(/* returnType = */ JAVA_INT)
    );

    // Execute the main function via the method handle.
    try {
      int result = (int) functionHandle.invoke();
      System.out.println("main() returned: " + result);
    } catch (Throwable e) {
      System.err.println("Error calling JIT function: " + e.getMessage());
    }

    // Disable the IR printing now
  	// var llvmIrCharPtr = LLVMPrintModuleToString(module);
    //
    // try {
    //  System.out.println(llvmIrCharPtr.getString(0));
    // } catch (Exception e) {
    //   System.err.println("Failed to write LLVM IR: failed to get error message: " + e.getMessage());
    // }

	   // Clean up LLVM resources
     // LLVMDisposeMessage(llvmIrCharPtr);
     LLVMDisposeBuilder(builder);
     LLVMDisposeModule(module);
  }
}
</pre></div>


<p>When <code>functionHandle.invoke()</code> runs, Java crosses into the native world and calls the machine code that was just compiled by the LLVM JIT compiler.</p>



<p>And that’s it, you can now run the Java application without the LLVM interpreter and see the resulting “Hello, World!”:</p>


<div><pre title="">$ java -cp target/jvm-llvm-helloworld-1.0-SNAPSHOT.jar com.example.App 
Hello, World!
</pre></div>


<p>Congratulations, you’ve now JIT-compiled Hello World, with the help of Java’s FFM API calling LLVM’s C API.</p>



<h2>Next steps</h2>



<p>In this Java advent we built and executed native machine code from pure Java and a little help from LLVM – no JNI, no C glue, just memory segments, method handles, and a modern FFI. By the end, we had just a simple program that prints “Hello, World!” but it shows the potential of the Java FFM API and the things you can do when Java and native code work together.</p>



<p>Now see what else you can do, for example, try generating other instructions: print more text, do simple calculations, or even build tiny programs entirely in LLVM from Java.</p>



<p>The full code for this post is available <a href="https://github.com/mrjameshamilton/java-llvm-helloworld">on GitHub over here</a>.</p>

		<div>
		<p><img alt="" src="https://secure.gravatar.com/avatar/ba9492d96e5b0bf5ea269360a8a81e6df6a18d89d79ff77831450e1e74232f27?s=80&amp;d=retro&amp;r=g" srcset="https://secure.gravatar.com/avatar/ba9492d96e5b0bf5ea269360a8a81e6df6a18d89d79ff77831450e1e74232f27?s=160&amp;d=retro&amp;r=g 2x" height="80" width="80">
		</p>
		<div>
			<h4>Author: <span><a href="https://www.javaadvent.com/author/jhamilton">James Hamilton</a></span></h4><p>I’m a senior software engineer working at Diffblue where we build a tool for automated Java unit test generation using code analysis techniques and reinforcement learning.  I previously worked at Guardsquare on JVM/Android related tools &amp; libraries including ProGuardCORE, ProGuard and DexGuard.
		</p></div>
	
	
	</div>
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Discovering the indieweb with calm tech (150 pts)]]></title>
            <link>https://alexsci.com/blog/calm-tech-discover/</link>
            <guid>46178892</guid>
            <pubDate>Sun, 07 Dec 2025 03:26:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alexsci.com/blog/calm-tech-discover/">https://alexsci.com/blog/calm-tech-discover/</a>, See on <a href="https://news.ycombinator.com/item?id=46178892">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-role="main"><a href="https://alexsci.com/blog/">Blog Home</a><hr><p>When social media first entered my life, it came with a promise of connection.
Facebook connected college-aged adults in a way that was previously impossible, helping to shape our digital generation.
Social media was our super-power and we wielded it to great effect.</p><p>Yet social media today is a noisy, needy, mental health hazard.
They push distracting notifications, constantly begging us to “like and subscribe”, and trying to trap us in endless scrolling.
They have become <a href="https://en.wikipedia.org/wiki/Siren_(mythology)">sirens</a> that lure us into their ad-infested shores with their saccharine promise of dopamine.</p><figure><a href="https://commons.wikimedia.org/w/index.php?curid=6574249"><img src="https://alexsci.com/blog/calm-tech-discover/FacebookSiren.JPG" alt="The Siren (1888) by Edward Armitage with the text 'Connect with friends and the world around you on Facebook' added."></a><figcaption><h4>Beware the siren's call</h4></figcaption></figure><p>How can we defeat these monsters that have invaded deep into our world, while still staying connected?</p><h2 id="streetpass-for-mastodon">StreetPass for Mastodon</h2><p>A couple weeks ago I stumbled into a great browser extension, <a href="https://streetpass.social/">StreetPass for Mastodon</a>.
The creator, <a href="https://mastodon.social/@tvler">tvler</a>, built it to help people find each other on Mastodon.
StreetPass autodiscovers Mastodon verification links as you browse the web, building a collection of Mastodon accounts from the blogs and personal websites you’ve encountered.</p><p>StreetPass is a beautiful example of <a href="https://en.wikipedia.org/wiki/Calm_technology">calm technology</a> .
When StreetPass finds Mastodon profiles it doesn’t draw your attention with a notification, it quietly adds the profile to a list, knowing you’ll check in when you’re ready.</p><figure><img src="https://alexsci.com/blog/calm-tech-discover/StreetPassPopup.png" alt="A screenshot showing the StreetPass extension's popup window open. It lists several Mastodon profiles and the timestamps they were discovered"></figure><p>StreetPass recognizes that there’s no need for an immediate call to action.
Instead it allows the user to focus on their browsing, enriching their experience in the background.
The user engages with StreetPass when they are ready, and on their own terms.</p><p>StreetPass is <a href="https://github.com/tvler/streetpass">open source</a> and available for <a href="https://addons.mozilla.org/en-US/firefox/addon/streetpass-for-mastodon/">Firefox</a>, <a href="https://chrome.google.com/webstore/detail/streetpass-for-mastodon/fphjfedjhinpnjblomfebcjjpdpakhhn">Chrome</a>, and <a href="https://apps.apple.com/us/app/streetpass-for-mastodon/id6446224821">Safari</a>.</p><p>Inspired by StreetPass, I applied this technique to RSS feed discovery.</p><h2 id="blog-quest">Blog Quest</h2><p>Blog Quest is a web browser extension that helps you discover and subscribe to blogs.
Blog Quest checks each page for auto-discoverable RSS and Atom feeds (using <code>rel="alternate"</code> links) and quietly collects them in the background.
When you’re ready to explore the collected feeds, open the extension’s drop-down window.</p><figure><img src="https://alexsci.com/blog/calm-tech-discover/BlogQuestPopup.png" alt="A browser extension popup showing several feeds it discovered"></figure><p>The extension integrates with several feed readers, making subscription management nearly effortless.</p><p>Blog Quest is available for both <a href="https://addons.mozilla.org/en-US/firefox/addon/blog-quest/">Firefox</a> and <a href="https://chromewebstore.google.com/detail/blog-quest/ghmfhadmoephkndjiahchiobgclmkkpi">Chrome</a>.
The project is <a href="https://github.com/robalexdev/blog-quest">open source</a> and I encourage you to build your own variants.</p><p>I reject the dead Internet theory: I see a vibrant Internet full of humans sharing their experiences and seeking connection.
Degradation of the engagement-driven web is well underway, accelerated by AI slop.
But the independent web works on a different incentive structure and is resistant to this effect.
Humans inherently create, connect, and share: we always have and we always will.
If you choose software that works in your interest you’ll find that it’s possible to make meaningful online connections without mental hazard.</p><p>Check out <a href="https://streetpass.social/">StreetPass</a> and <a href="https://github.com/robalexdev/blog-quest">Blog Quest</a> to discover a decentralized, independent Internet that puts you in control.</p><p><small>You can't drown out the noise of social media by shouting louder, you've got to whisper.</small></p><h3 id="image-credits">Image credits</h3><ul><li>Edward Armitage: <a href="https://commons.wikimedia.org/w/index.php?curid=6574249">The Siren (1888)</a></li></ul><hr></div><p><strong>Hello!</strong>
I'm Robert Alexander, a DevSecOps consultant
<a href="https://robalexdev.com/">available for contract work</a>.
This blog features some of my work and thoughts on software, the cloud, and security.
You can subscribe to my posts
<a href="https://alexsci.com/blog/rss.xml">with your favorite RSS client</a>.</p><p><small>Statements are my own and do not represent the positions or opinions of my employer.</small></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Z2 – Lithographically fabricated IC in a garage fab (255 pts)]]></title>
            <link>https://sam.zeloof.xyz/second-ic/</link>
            <guid>46178789</guid>
            <pubDate>Sun, 07 Dec 2025 03:03:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sam.zeloof.xyz/second-ic/">https://sam.zeloof.xyz/second-ic/</a>, See on <a href="https://news.ycombinator.com/item?id=46178789">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-2342">
	
	<!-- .entry-header -->

	<div>
		<p><em>Homemade&nbsp;1000+ transistor array chip&nbsp;</em></p>
<p>In 2018 I made the <a href="https://sam.zeloof.xyz/first-ic/">first lithographically fabricated integrated circuits</a>&nbsp;in my garage fab. I was a senior in high school&nbsp;when I made the Z1 amplifier, and now I’m a senior in college so there are some long overdue improvements to the amateur silicon process.<a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9414_an.jpg"><br>
</a><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9414ano.jpg"><img fetchpriority="high" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9414ano-1024x759.jpg" alt="DSC_9414ano" width="660" height="489" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9414ano-1024x759.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9414ano-300x222.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9414ano-768x569.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a><br>
The Z1 had&nbsp;6 transistors and was a great test chip to develop all the processes and equipment.&nbsp;The Z2 has 100 transistors on a 10µm <a href="http://www.intel4004.com/sgdm.htm">polysilicon gate</a> process – same technology as <a href="https://en.wikipedia.org/wiki/Intel_4004">Intel’s first processor</a>. My chip is a simple 10×10 array of transistors to test, characterize, and tweak the process but this is a huge step closer to more advanced DIY computer chips. The Intel 4004 has 2,200 transistors and I’ve now made 1,200&nbsp;on the same piece of silicon.</p>
<p><iframe title="YouTube video player" src="https://www.youtube.com/embed/IS5ycm7VfXg" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p><img decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/Screen-Shot-2021-08-12-at-4.28.35-PM-1024x628.png" alt="Screen Shot 2021-08-12 at 4.28.35 PM" width="660" height="405" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/Screen-Shot-2021-08-12-at-4.28.35-PM-1024x628.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/Screen-Shot-2021-08-12-at-4.28.35-PM-300x184.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/Screen-Shot-2021-08-12-at-4.28.35-PM-768x471.png 768w" sizes="(max-width: 660px) 100vw, 660px"></p>
<figure id="attachment_2440" aria-describedby="caption-attachment-2440"><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/mooreslaw-2.png"><img loading="lazy" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/mooreslaw-2-1024x768.png" alt="Only half joking" width="660" height="495" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/mooreslaw-2-1024x768.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/mooreslaw-2-300x225.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/mooreslaw-2-768x576.png 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/mooreslaw-2.png 1280w" sizes="(max-width: 660px) 100vw, 660px"></a><figcaption id="caption-attachment-2440">Only half joking</figcaption></figure>
<p>Previously, I made chips&nbsp;with a <a href="https://en.wikipedia.org/wiki/Metal_gate">metal gate process</a>. The aluminum gate has a large work function difference with the silicon channel beneath it which results in a high threshold voltage (&gt;10V). I used these metal gate transistors in a few fun projects like a <a href="https://twitter.com/szeloof/status/1280249239495479297">guitar distortion pedal</a>&nbsp;and a <a href="https://twitter.com/szeloof/status/1263940735923093505">ring oscillator LED blinker</a>&nbsp;but both of these required one or two 9V batteries to run the circuit due to high Vth. By switching to a polysilicon gate process, I get a ton of performance benefits (self aligned gate means lower overlap capacitances) including a much lower Vth which makes these chips compatible with 2.5V and 3.3V logic levels. The new FETs have excellent characteristics:</p>
<pre><strong>NMOS Electrical Properties:</strong>
Vth             = 1.1 V
Vgs MAX         = 8 V
Cgs             = &lt;0.9 pF
Rise/fall time  = &lt;10 ns
On/off ratio    = 4.3e6
Leakage current = 932 pA (Vds=2.5V)
</pre>
<p>I was particularly surprised by the super low leakage current. This value goes up about 100x in ambient room lighting.</p>
<div id="gallery-1"><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9375.jpg"><img loading="lazy" decoding="async" width="660" height="505" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9375-1024x784.jpg" alt="" aria-describedby="gallery-1-2394" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9375-1024x784.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9375-300x230.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9375-768x588.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-1-2394">
				NMOS, 0.5V Vgs steps
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9439.jpg"><img loading="lazy" decoding="async" width="660" height="517" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9439-1024x802.jpg" alt="" aria-describedby="gallery-1-2395" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9439-1024x802.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9439-300x235.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9439-768x601.jpg 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9439.jpg 1979w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-1-2395">
				Diode curve
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9442.jpg"><img loading="lazy" decoding="async" width="660" height="524" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9442-1024x813.jpg" alt="" aria-describedby="gallery-1-2396" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9442-1024x813.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9442-300x238.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9442-768x609.jpg 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9442.jpg 1879w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-1-2396">
				C-V showing Vth = 1.1V
				</figcaption></figure>
		</div>

<p>Now we know that it’s possible to make really good transistors with impure chemicals, no cleanroom, and homemade equipment. Of course, yield and process repeatability are&nbsp;diminished. I’ll do more testing to collect data on the statistics and variability of FET properties but it’s looking good!</p>
<div id="gallery-2"><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9447-e1628800541548.jpg"><img loading="lazy" decoding="async" width="660" height="493" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9447-e1628800541548-1024x765.jpg" alt="" aria-describedby="gallery-2-2398" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9447-e1628800541548-1024x765.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9447-e1628800541548-300x224.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9447-e1628800541548-768x574.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-2-2398">
				1MHz into 50Ω load
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9446.jpg"><img loading="lazy" decoding="async" width="660" height="492" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9446-1024x763.jpg" alt="" aria-describedby="gallery-2-2397" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9446-1024x763.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9446-300x224.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9446-768x572.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-2-2397">
				20MHz into 50Ω load
				</figcaption></figure>
		</div>

<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9419-1.jpg"><img loading="lazy" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9419-1-1024x678.jpg" alt="DSC_9419" width="660" height="437" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9419-1-1024x678.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9419-1-300x199.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/DSC_9419-1-768x509.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>The chip is small, about one quarter the die area of my previous ICs (2.4mm^2) which makes it&nbsp;hard to probe.&nbsp;There’s a simple 10×10 array of N-channel FETs on each chip which will give me a lot of characterization data. Since it’s such a&nbsp;simple&nbsp;design, I was able to lay&nbsp;it out&nbsp;using Photoshop. Columns of 10 transistors share a common gate connection and each row is strung together in series with adjacent transistors sharing a source/drain terminal. It’s similar to NAND flash but I only did this to&nbsp;keep the metal pads large enough so I can reasonably probe them, if every FET had 3 pads for itself they would be too small.</p>
<p><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video id="video-2342-1" width="660" height="371" preload="metadata" controls="controls"><source type="video/mp4" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/probing.m4v?_=1"><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/probing.m4v">http://sam.zeloof.xyz/wp-content/uploads/2021/08/probing.m4v</a></video></p>
<p>It’s hard to convey the excitement of seeing a good FET curve displayed on the curve tracer after dipping a shard of rock into chemicals all day.</p>
<div id="gallery-3"><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/1r.png"><img loading="lazy" decoding="async" width="300" height="169" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/1r-300x169.png" alt="" aria-describedby="gallery-3-2362" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/1r-300x169.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/1r-768x432.png 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/1r-1024x576.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/1r.png 1920w" sizes="(max-width: 300px) 100vw, 300px"></a>
			</p>
				<figcaption id="gallery-3-2362">
				Source/drain
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/2r.png"><img loading="lazy" decoding="async" width="300" height="169" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/2r-300x169.png" alt="" aria-describedby="gallery-3-2363" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/2r-300x169.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/2r-768x432.png 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/2r-1024x576.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/2r.png 1920w" sizes="(max-width: 300px) 100vw, 300px"></a>
			</p>
				<figcaption id="gallery-3-2363">
				Poly gate
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/3r.png"><img loading="lazy" decoding="async" width="300" height="169" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/3r-300x169.png" alt="" aria-describedby="gallery-3-2364" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/3r-300x169.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/3r-768x432.png 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/3r-1024x576.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/3r.png 1920w" sizes="(max-width: 300px) 100vw, 300px"></a>
			</p>
				<figcaption id="gallery-3-2364">
				Contact
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/4r.png"><img loading="lazy" decoding="async" width="300" height="169" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/4r-300x169.png" alt="" aria-describedby="gallery-3-2365" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/4r-300x169.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/4r-768x432.png 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/4r-1024x576.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/4r.png 1920w" sizes="(max-width: 300px) 100vw, 300px"></a>
			</p>
				<figcaption id="gallery-3-2365">
				Metal
				</figcaption></figure>
		</div>

<p>A single 10µm NMOS transistor can be see below, with slight misalignment in the metal layer (part of the left contact is uncovered). Red outline is polycrystalline silicon, blue is the source/drain.</p>
<div id="gallery-4"><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002.jpg"><img loading="lazy" decoding="async" width="660" height="495" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-1024x768.jpg" alt="" aria-describedby="gallery-4-2403" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-1024x768.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-300x225.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-768x576.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-4-2403">
				Single NMOS transistor
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-copy.jpg"><img loading="lazy" decoding="async" width="660" height="495" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-copy-1024x768.jpg" alt="" aria-describedby="gallery-4-2402" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-copy-1024x768.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-copy-300x225.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0002-copy-768x576.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-4-2402">
				Single NMOS transistor
				</figcaption></figure>
		</div>

<p>So far I’ve made an opamp (Z1) and a memory-like array (Z2). More interesting&nbsp;circuits are definitely possible even with this low transistor density.&nbsp;The process needs&nbsp;some tweaking&nbsp;but now that I’m able to consistently make good quality transistors I should be able to design more complex digital and analog circuits. Testing each chip is very&nbsp;tedious&nbsp;so I am trying to automate the process and I’ll post more data then. I’ve made 15 chips (1,500 transistors) and know there’s at least one completely functional chip and at least two “mostly functional”, meaning ~80% of the transistors work instead of 100%. No proper yield data yet. The most common defect is a drain or source shorted to the bulk silicon channel, not a leaky or shorted gate like on my Z1 process.</p>
<figure id="attachment_2409" aria-describedby="caption-attachment-2409"><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/IMG_5821.jpg"><img loading="lazy" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/IMG_5821.jpg" alt="Profilometer scan of gate" width="772" height="590" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/IMG_5821.jpg 772w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/IMG_5821-300x229.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/IMG_5821-768x587.jpg 768w" sizes="(max-width: 772px) 100vw, 772px"></a><figcaption id="caption-attachment-2409">Profilometer scan of gate layer (y axis in angstrom, x axis is micron)</figcaption></figure>
<p>I said before that the gate used to be made out of aluminum and now it’s silicon which makes the chips work a lot better. Silicon comes in three varieties that we care about: amorphous, polycrystalline, and monocrystalline.&nbsp;From left to right, these become more electrically conductive but also much harder to deposit.&nbsp;In fact,&nbsp;monocrystalline Si can’t be deposited, you can only grow it in contact with another mono-Si layer as a seed (epitaxy). Since the gate must be deposited on top of an insulating dielectric, poly is the best we can do. We can heavily dope the polysilicon anyway to make it more conductive.</p>
<div id="gallery-5"><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0003.jpg"><img loading="lazy" decoding="async" width="660" height="495" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0003-1024x768.jpg" alt="" aria-describedby="gallery-5-2404" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0003-1024x768.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0003-300x225.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0003-768x576.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-5-2404">
				2 FETs sharing gate
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001.jpg"><img loading="lazy" decoding="async" width="660" height="495" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001-1024x768.jpg" alt="" aria-describedby="gallery-5-2405" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001-1024x768.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001-300x225.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001-768x576.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-5-2405">
				Neighbors share source/drain
				</figcaption></figure>
		</div>

<p>A typical self-aligned polysilicon gate process&nbsp;requires silane, a&nbsp;toxic and explosive gas, to&nbsp;deposit polycrystalline silicon layers. It may also be possible by sputtering or <a href="https://sci-hub.st/https://sid.onlinelibrary.wiley.com/doi/abs/10.1002/sdtp.10835">evaporating amorphous silicon and annealing with a laser</a>.&nbsp;A major theme of this DIY silicon process is to circumvent expensive, difficult, or dangerous steps. So, I came up with a modified process flow. It’s a variation on the standard self-aligned&nbsp;methods&nbsp;to allow doping via high temperature diffusion rather than ion implantation. The effect is that I’m able to buy a silicon wafer with the&nbsp;polysilicon already deposited on it&nbsp;from the factory and pattern it to make transistors instead of putting my own polysilicon down halfway through the process. This is a nice short term workaround but it would be best to design a polysilicon deposition process using the laser anneal method mentioned above.</p>
<p>Wafers are available with all kinds of materials deposited on them already, so I just had to find one with a thin layer of SiO2 (gate oxide, ~10nm) followed by a thicker polysilicon (300nm). I found a lot of 25 200mm (EPI, prime, [1-0-0], p-type) wafers on eBay for $45 which is essentially a lifetime supply, so email me if you want one. The gate oxide is the most&nbsp;fragile layer and requires the most care during fabrication. Since I bought the wafer with a nice high quality oxide on it already that was capped off and kept clean by the thick polysilicon layer,&nbsp;I was able to eliminate all the&nbsp;aggressive&nbsp;cleaning chemicals (sulfuric acid, etc) from the process and still&nbsp;make great transistors. Minimal process chemicals and tools are listed below.</p>
<pre><strong>Chemicals used in home poly-gate process:
</strong>-Water
-Alcohol
-Acetone
-Phosphoric acid
-Photoresist
-Developer (2% KOH)
-N type dopant (filmtronics P509)
-HF (1%) or CF4/CHF3 RIE
-HNO3 for poly etch or SF6 RIE</pre>
<pre><strong>Equipment used in home poly-gate process:</strong>
-Hotplate
-Tube furnace
-<a href="https://sam.zeloof.xyz/maskless-photolithography/">Lithography apparatus
</a>-Microscope
-Vacuum chamber to deposit metal</pre>
<p>Z2 “gate first” process (similar to standard self-aligned process but without a field oxide):</p>
<div id="gallery-6"><figure>
			<p><img loading="lazy" decoding="async" width="660" height="431" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p1-1024x669.png" alt="" aria-describedby="gallery-6-2412" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p1-1024x669.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p1-300x196.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p1-768x502.png 768w" sizes="(max-width: 660px) 100vw, 660px">
			</p>
				<figcaption id="gallery-6-2412">
				Buy wafer
				</figcaption></figure><figure>
			<p><img loading="lazy" decoding="async" width="660" height="431" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p2-1024x669.png" alt="" aria-describedby="gallery-6-2413" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p2-1024x669.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p2-300x196.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p2-768x502.png 768w" sizes="(max-width: 660px) 100vw, 660px">
			</p>
				<figcaption id="gallery-6-2413">
				Etch active
				</figcaption></figure><figure>
			<p><img loading="lazy" decoding="async" width="660" height="431" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p3-1024x669.png" alt="" aria-describedby="gallery-6-2414" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p3-1024x669.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p3-300x196.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p3-768x502.png 768w" sizes="(max-width: 660px) 100vw, 660px">
			</p>
				<figcaption id="gallery-6-2414">
				Dope source/drain
				</figcaption></figure><figure>
			<p><img loading="lazy" decoding="async" width="660" height="431" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p4-1024x669.png" alt="" aria-describedby="gallery-6-2415" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p4-1024x669.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p4-300x196.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p4-768x502.png 768w" sizes="(max-width: 660px) 100vw, 660px">
			</p>
				<figcaption id="gallery-6-2415">
				Etch poly gate
				</figcaption></figure><figure>
			<p><img loading="lazy" decoding="async" width="660" height="431" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p5-1024x669.png" alt="" aria-describedby="gallery-6-2416" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p5-1024x669.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p5-300x196.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p5-768x502.png 768w" sizes="(max-width: 660px) 100vw, 660px">
			</p>
				<figcaption id="gallery-6-2416">
				Deposit dielectric
				</figcaption></figure><figure>
			<p><img loading="lazy" decoding="async" width="660" height="431" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p6-1024x669.png" alt="" aria-describedby="gallery-6-2417" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p6-1024x669.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p6-300x196.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p6-768x502.png 768w" sizes="(max-width: 660px) 100vw, 660px">
			</p>
				<figcaption id="gallery-6-2417">
				Etch contact
				</figcaption></figure><figure>
			<p><img loading="lazy" decoding="async" width="660" height="431" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p7-1024x669.png" alt="" aria-describedby="gallery-6-2418" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p7-1024x669.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p7-300x196.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p7-768x502.png 768w" sizes="(max-width: 660px) 100vw, 660px">
			</p>
				<figcaption id="gallery-6-2418">
				Deposit metal
				</figcaption></figure><figure>
			<p><img loading="lazy" decoding="async" width="660" height="431" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p8-1024x669.png" alt="" aria-describedby="gallery-6-2419" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/p8-1024x669.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p8-300x196.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/p8-768x502.png 768w" sizes="(max-width: 660px) 100vw, 660px">
			</p>
				<figcaption id="gallery-6-2419">
				Etch metal
				</figcaption></figure>
		</div>

<p>I snapped one of the test chips in half (functional Z2 but with bad layer alignment and thin metal, about 300nm) and put it in my <a href="https://sam.zeloof.xyz/category/electron-microscope/">SEM</a> for a cross section:</p>
<div id="gallery-7"><figure>
			<p><a href="https://sam.zeloof.xyz/second-ic/snap1/"><img loading="lazy" decoding="async" width="660" height="987" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/snap1-e1629399647701-685x1024.jpg" alt="" aria-describedby="gallery-7-2451" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/snap1-e1629399647701-685x1024.jpg 685w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/snap1-e1629399647701-201x300.jpg 201w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/snap1-e1629399647701-768x1149.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-7-2451">
				…snap
				</figcaption></figure><figure>
			<p><a href="https://sam.zeloof.xyz/second-ic/s/"><img loading="lazy" decoding="async" width="660" height="977" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/s-692x1024.jpg" alt="" aria-describedby="gallery-7-2450" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/s-692x1024.jpg 692w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/s-203x300.jpg 203w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/s-768x1137.jpg 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/s.jpg 925w" sizes="(max-width: 660px) 100vw, 660px"></a>
			</p>
				<figcaption id="gallery-7-2450">
				Tilted SEM view
				</figcaption></figure>
		</div>

<p>Find&nbsp;the dust particle in the red circle below, use that to get oriented in the coming cross section views.</p>
<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/xsecloc.jpg"><img loading="lazy" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/xsecloc-1024x515.jpg" alt="xsecloc" width="660" height="332" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/xsecloc-1024x515.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/xsecloc-300x151.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/xsecloc-768x387.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection-1.png"><img loading="lazy" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection-1-1024x550.png" alt="Xsection (1)" width="660" height="354" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection-1-1024x550.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection-1-300x161.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection-1-768x412.png 768w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<figure id="attachment_2453" aria-describedby="caption-attachment-2453"><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection_ano.png"><img loading="lazy" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection_ano-1024x550.png" alt="NMOS cross section" width="660" height="354" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection_ano-1024x550.png 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection_ano-300x161.png 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/Xsection_ano-768x412.png 768w" sizes="(max-width: 660px) 100vw, 660px"></a><figcaption id="caption-attachment-2453">NMOS cross section</figcaption></figure>
<p>Because I bought the wafer already with gate oxide and polysilicon on it, I can’t grow a field oxide. These thick oxide layers are typically used to mask dopants and require a long high temperature step which would oxidize all of my poly and there would be none remaining. So, my modified process uses an additional masking step (the “gate” mask is typically not found in a self-aligned process) that allows me to use the polysilicon itself as a dopant mask and hard-baked photoresist as the field dielectric. This alternative processing results in the stepped structure you can see in the orange region on the NMOS cross section above.&nbsp;This process subtlety&nbsp;is mentioned here, <a href="https://twitter.com/szeloof/status/1426534655197646857?s=20">read this twitter thread</a>.</p>
<figure id="attachment_2460" aria-describedby="caption-attachment-2460"><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/gatemeasure-1.jpg"><img loading="lazy" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/gatemeasure-1-1024x692.jpg" alt="Gate length measurement" width="660" height="446" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/gatemeasure-1-1024x692.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/gatemeasure-1-300x203.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/gatemeasure-1-768x519.jpg 768w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/gatemeasure-1.jpg 1370w" sizes="(max-width: 660px) 100vw, 660px"></a><figcaption id="caption-attachment-2460">Gate length measurement</figcaption></figure>
<p>This process isn’t ideal and I want to make some changes so it’s CMOS compatible but it simplifies fabrication and makes it possible with a minimal set of tools. The 1µm dielectric layer (orange) would ideally be CVD SiO2 (it’s possible to build a&nbsp;TEOS oxide reactor at home) but I used a photoresist instead. Most photoresists can be baked around 250°C to form a hard permanent dielectric layer that is&nbsp;an easy alternative to CVD or PECVD oxide. A spin-on-glass/sol-gel could also be used here. SiO2 etching is done with a <a href="https://sam.zeloof.xyz/sio2-patterning/">buffered HF solution made from rust stain remover</a>&nbsp;or RIE.</p>
<p>Huge composite stitched die image:</p>
<p><a href="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001_stitch.jpg"><img loading="lazy" decoding="async" src="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001_stitch-1024x958.jpg" alt="0001_stitch" width="660" height="617" srcset="https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001_stitch-1024x958.jpg 1024w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001_stitch-300x281.jpg 300w, https://sam.zeloof.xyz/wp-content/uploads/2021/08/0001_stitch-768x718.jpg 768w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>Thanks for following my work and feel free to contact me with your thoughts!</p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Eurydice: a Rust to C compiler (yes) (144 pts)]]></title>
            <link>https://jonathan.protzenko.fr/2025/10/28/eurydice.html</link>
            <guid>46178442</guid>
            <pubDate>Sun, 07 Dec 2025 01:41:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jonathan.protzenko.fr/2025/10/28/eurydice.html">https://jonathan.protzenko.fr/2025/10/28/eurydice.html</a>, See on <a href="https://news.ycombinator.com/item?id=46178442">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Perhaps the greatest surprise of the last two years was, for me, the realization
that people not only care about compiling C to Rust (for obvious reasons, such
as, ahem, memory safety) – they also care about compiling Rust to C! Wait,
what?</p>

<p>I <a href="https://jonathan.protzenko.fr/2024/01/05/eurydice.html">wrote about this</a> briefly a couple years
ago, but the level of interest for the project, I must say, took me somewhat by
surprise. So let’s talk about compiling Rust to C a little more today.</p>

<h2 id="barriers-to-rust-adoption">Barriers to Rust adoption</h2>

<p>Rust is making big progress in terms of adoption, and represents a great value
proposition, especially for new code. Both my <a href="https://microsoft.com/">former
employer</a> and my <a href="https://google.com/">new employer</a>, like
pretty much everyone else these days, have big projects that are written in pure
Rust or can have Rust components. Even <a href="https://techcommunity.microsoft.com/blog/windowsdriverdev/towards-rust-in-windows-drivers/4449718">Windows kernel drivers can be written
in
Rust</a>
now. Amazing stuff.</p>

<p>However, if your project is, say, an open-source library that gets compiled on a
wonderfully diverse set of target architectures, OSes, distributions and
toolchains, well, chances are… one of these is not going to support Rust. Think of a
crypto library: there <strong>will</strong> be people out there with an obscure compiler for a weird
embedded target, and they really want to compile your library, because they’ve
been told not to roll out their own crypto. Or perhaps you have a format library
ridden with memory errors and you want to port it to Rust. Or maybe your company
has an in-house analysis that only runs on C code. Regardless of the scenario,
there will always be that one legacy use-case that prevents you from switching
to Rust until it’s 2035, all those LTS versions (looking at you RHEL) are
finally retired, and you yourself are too close to retirement to even care
anymore.</p>

<p>That is, unless you’re willing to use a Rust to C compiler.</p>

<h2 id="why">Why?</h2>

<p>Having a backwards-compat scenario where Rust can be compiled to C serves
several purposes.</p>

<ol>
  <li>It allows for a gradual transition. The codebase can be ported to Rust,
and refactored / cleaned up / rewritten to use all the nice Rust things (data
types, pattern-matching, polymorphism, memory safety), thus making you and
your developers much, much happier. Meanwhile, the C version co-exists so
that you don’t alienate your userbase.</li>
  <li>It only requires maintaining a single version. The Rust code is
authoritative; the C code is derived from it automatically, either on CI, or
at least with a CI job that checks that the two are in sync.</li>
  <li>It allows for a census of problematic scenarios. By making the Rust version
the default (and putting the fallback C behind a <code>--write-us-an-email</code> flag),
there is finally a way to enumerate those mythical users who cannot switch to
Rust just yet.</li>
</ol>

<p>If that sounds appealing, meet Eurydice.</p>



<p>Eurydice is a compiler from Rust to C that aims to produce <em>readable</em> C code. Of
course, readability is subjective; also, seeing that Rust relies on
whole-program monomorphization, the C code is bound to be more verbose than the
Rust code. But you can judge for yourself: here’s the result of <a href="https://github.com/AeneasVerif/eurydice/blob/9b14f74c05228b8335700efcffb55bf82a991975/out/test-libcrux/libcrux_mlkem_portable.c#L936">compiling
libcrux to
C</a>.</p>

<p>The output of the test suite is under version control, and there are <a href="https://github.com/AeneasVerif/eurydice/tree/9b14f74c05228b8335700efcffb55bf82a991975/out">a lot more
tests</a>
to peruse. See for instance <a href="https://github.com/AeneasVerif/eurydice/blob/9b14f74c05228b8335700efcffb55bf82a991975/out/test-symcrust/symcrust.c">this
bit</a>,
compared to the <a href="https://github.com/AeneasVerif/eurydice/blob/9b14f74c05228b8335700efcffb55bf82a991975/test/symcrust.rs">Rust
original</a>.</p>

<h2 id="the-design-of-eurydice">The design of Eurydice</h2>

<p>Eurydice plugs in directly at the MIR level, using
<a href="https://github.com/AeneasVerif/charon/">Charon</a> to avoid reimplementing the
wheel and paying the price of interacting with the guts of <code>rustc</code>. <a href="https://arxiv.org/abs/2410.18042">Our
paper</a> on Charon says more about its
architecture.</p>

<p>The advantage of plugging in at the MIR level is that i) we do not have to
interpret syntactic sugar, which means our translation is more faithful to the
Rust semantics, and ii) we have way fewer constructs that need compiling to C. Even then,
it’s no easy feat to translate Rust to C.</p>

<p>There is naturally, the need to perform whole-program monomorphization, over
types and const-generic arguments; the compilation of pattern matches into
tagged unions; recognizing instances of iterators that can be compiled to native
C <code>for</code>-loops. Then, there are more subtle things, such as compiling array
repeat expressions sensibly – zero-initializers when possible, initializer
lists otherwise, unless it generates too much code, in which case <code>for</code>-loops are
preferable. And finally, there are all the rules about visibility, <code>static</code>,
<code>inline</code>, etc. that are very C-specific and depend on how you want to lay out
your C files.</p>

<p>The translation is complicated by the constraint that the generated code
ought to be readable: for instance, we compile Rust structs to
C structs, including
<a href="https://doc.rust-lang.org/reference/dynamically-sized-types.html">DST</a>s, by
relying on <a href="https://en.cppreference.com/w/c/language/struct.html">flexible array
members</a>.
We also
work hard to avoid using the fully-generic tagged union pattern when possible,
instead eliminating the tag when e.g. the Rust enum only has a single case.
Additionally, we rely on Charon to reconstruct control-flow, rather than compile
the MIR <a href="https://en.wikipedia.org/wiki/Control-flow_graph">CFG</a> to C code ridden
with <code>goto</code>s; again, this is for code quality.</p>

<p>At a low-level, there were many interesting tidbits.</p>
<ul>
  <li>Because arrays in Rust are values, we wrap them within C structs to give them
value semantics in C, too; concretely, <code>[u32; 8]</code> becomes <code>struct {
uint32_t data[8]; }</code>. (A previous version of Eurydice would emit <code>uint32_t *</code>,
and rely on various <code>memcpy</code>s to implement value semantics, but this produced
a translation that was not type-generic, and there were plenty of finicky
corner cases. We revamped the compilation scheme recently.)</li>
  <li>The notion of <code>lvalue</code> in C means we need to insert more variable declarations
than in Rust – for instance, you can’t trivially compile <code>&amp;[0u32; 1]</code> without
naming the array.</li>
  <li>The fact that the evaluation order is so loosely defined in C means that
intermediary computations need to be stored in intermediary variables to
enforce the evaluation order.</li>
  <li>Rust relies on whole-program monomorphization; this means that the C code is
inevitably going to contains multiple copies of the same types and functions,
but for different choices of type and const generic argumnets. This is
currently done with a builtin phase in Eurydice (for historical reasons), but
in the long run, we want to rely on Charon’s support for monomorphization.</li>
  <li>There are plenty of peephole optimizations that are required for good code
quality, such as recognizing <code>array::from_fn</code> and generating sensible code
that initializes the array in-place (instead of relying on the fully-general
compilation scheme for closures), or recognizing instances of the <code>Eq</code>
trait that deserve dedicated treatment (such as using <code>memcmp</code> for arrays and
slices of flat data).</li>
</ul>

<p>A final design choice is that for now, Eurydice may define more behaviors than
Rust – for instance, Rust panics on integer overflow, but Eurydice-compiled
code does not. This is because we assume the input code is verified, and
therefore has been shown to be free of panics. This design choice can be easily
changed, though.</p>

<p>In practice, as soon as you use traits, the C code becomes more voluminous than
the Rust code. We rely on a configuration file mechanism to control the
placement of monomorphized instances of a given function, rather than put
everything in one big C file. This currently requires a lot of manual
intervention to give good results on large projects.</p>

<h2 id="implementing-of-eurydice">Implementing of Eurydice</h2>

<p>Eurydice starts by compiling the MIR AST obtained out of Charon into
<a href="https://github.com/FStarLang/karamel/">KaRaMeL</a>’s internal AST. This is ~3000
lines of OCaml code, so that’s already pretty involved. A lot of the work
revolves around trait methods and their monomorphization, given Rust’s
expressive trait system.</p>

<p>Then, about 30 nanopasses simplify the KaRaMeL AST until it becomes eligible for
compilation to C. Of those, a handful were originally written for KaRaMeL and
were somewhat reusable; this includes compilation of data types, as well as
monomorphization. The rest was written from scratch for Eurydice, and totals
about ~5000 lines of OCaml code.</p>

<p>A particularly gnarly phase was eliminating MIR’s variable assignments as much
as possible: in MIR, every variable starts out uninitialized at the beginning of
the function; then, <em>in lieu</em> of the variable declaration, we have an assignment
with the initial value. Naturally, having a variable declaration in the right
spot is better for code quality, so an initial phase tries to reconstruct these
assignments. That’s a drawback of using MIR, but we still firmly believe that
sticking to something that has clear semantics is ultimately better.</p>

<p>Fun fact: because there are so many peephole optimizations, I got tired of
maintaining <a href="https://github.com/AeneasVerif/eurydice/blob/29a05cc79df4d63d6a0a3816f1617a3bba4814e2/lib/Cleanup2.ml#L616-L655">enormous
pattern-matches</a>
that would try to catch every flavor of
Rust iterator that can be compiled to a C for-loop. Instead, a <a href="https://github.com/AeneasVerif/eurydice/blob/94cdf3b2ea4541b658dff74e4307ce01041fcc22/cremepat">custom OCaml syntax
extension</a> allows writing <a href="https://github.com/AeneasVerif/eurydice/blob/94cdf3b2ea4541b658dff74e4307ce01041fcc22/lib/Cleanup2.ml#L609-L626">concrete
syntax</a>
for the internal KaRaMeL language in OCaml patterns. Those magic patterns then get
compiled at compile-time to OCaml AST nodes for an actual OCaml pattern that
matches the (deeply-embedded) syntax of KaRaMeL’s AST. This relies on a <code>ppx</code>
that lexes, parses and compiles the concrete syntax.</p>

<h2 id="deploying-eurydice-generated-code">Deploying Eurydice-generated code</h2>

<p>Eurydice-generated code expects some hand-written glue that contains macros and
<code>static inline</code> functions; sometimes, it’s simply more convenient to write a
single macro that uses a type, rather than have Eurydice generate N copies of a
polymorphic function that gets specialized each time. A typical example is
compiling the Eq trait for arrays: it’s nicer to emit <code>Eurydice_array_eq(a1, a2,
len, t)</code>, which macro-expands to <code>!(memcmp(a1, a2, len*sizeof(t)))</code>, rather than
have N such functions, each containing a for-loop specialized for different
values of <code>t</code>.</p>

<p>Eurydice generates code that is either (C11 and C++20-compatible) or (C++-17
compatible, but not C-compatible). The reason for this is that Rust allows enum
values (e.g. <code>Foo { bar: baz }</code>) in any expression position. For simplicity,
Eurydice emits a compound initializer <code>(Foo) { .tag = bar, .value = { .case_Foo
= { .bar = baz }}}</code>, or a C++20 aggregate that uses designated initializers,
relying on a macro (not shown here) to hide the syntax differences between the
two. But C++17 does not have designated initializers, so there is an option for
Eurydice to emit different code that <a href="https://github.com/AeneasVerif/eurydice/blob/94cdf3b2ea4541b658dff74e4307ce01041fcc22/include/eurydice_glue.h#L37">relies on member pointers</a> to achieve
sensibly the <a href="https://github.com/AeneasVerif/eurydice/blob/94cdf3b2ea4541b658dff74e4307ce01041fcc22/out/testxx-result/result.cc#L34">same effect</a>.</p>

<h2 id="limitations-of-eurydice">Limitations of Eurydice</h2>

<p>Naturally, there are many limitations to this approach. Here are the
main ones that come to mind:</p>
<ul>
  <li>we cannot guarantee that the layout of objects will be the same in C as in
Rust; conceivably, one could parse the layout information from MIR, then emit
compiler-specific alignment directives to keep the two identical, but this is
not done currently;</li>
  <li>the generated code <a href="https://github.com/AeneasVerif/eurydice/blob/main/out/test-dst/dst.c#L15">violates strict
aliasing</a>,
because creating a user-defined DST involves casting one pointer type (a
struct containing an array) to another (a struct with a flexible array
member instead); I’m not sure what the best fix is, so for now, please compile your
code with <code>-fno-strict-aliasing</code>;</li>
  <li>the code that Eurydice sees is MIR <em>after</em> applying <code>cfg</code> tweaks; this means
that for code that is intended to be multi-platform, <a href="https://github.com/AeneasVerif/eurydice/pull/260">some
tricks</a> need to be applied,
otherwise, Eurydice will only “see” one version of the code (AVX2, or ARM64,
or something else)</li>
  <li>because monorphization is so pervasive, the configuration language needs to
express things such as “types that reference <code>__m256i</code>, an AVX2-only type,
need to go into a separate file to be compiled with <code>-mavx2</code>”; this can get
tedious <a href="https://github.com/AeneasVerif/eurydice/blob/94cdf3b2ea4541b658dff74e4307ce01041fcc22/test/libcrux/c.yaml">real
fast</a>
but I’m not sure I know how to do better.</li>
</ul>

<h2 id="whats-next">What’s next?</h2>

<p>There is ongoing work to integrate Eurydice-generated code for both
<a href="https://www.microsoft.com/en-us/research/blog/rewriting-symcrypt-in-rust-to-modernize-microsofts-cryptographic-library/">Microsoft</a>
and
<a href="https://boringssl-review.googlesource.com/c/boringssl/+/77027?tab=comments">Google</a>’s
respective crypto libraries.</p>

<p>The community grew recently, with wonderful contributions by GitHub users
@ssyram and @lin23299. There are more in the pipeline, and I look forward to
seeing the supported subset of Rust grow even more. Next on the horizon is
support for <code>dyn</code> traits via vtables, and relying on Charon’s monomorphization
to get MIR exactly as the Rust compiler would monomorphize it, intead of relying
on a custom procedure in Eurydice.</p>

<p>An ambitious goal is for the whole standard library of Rust to be extractable
via Eurydice in 2026. This is non-trivial, but I believe this achievement is
within reach. Stay tuned.</p>

<h2 id="ps-why-the-name">PS: Why the name?</h2>

<p>People keep asking about the name; because the project shares a large amount of
infrastructure with <a href="https://github.com/AeneasVerif/aeneas">Aeneas</a> and
<a href="https://github.com/AeneasVerif/charon">Charon</a>, I had to follow the Greek
mythology theme. Specifically, the myth of
<a href="https://en.wikipedia.org/wiki/Eurydice">Eurydice</a> resonated with me: I thought
I was saved from the hell of <a href="https://jonathan.protzenko.fr/2019/01/04/behind-the-scenes.html">generating C code</a>, and was going to go back to the world of the
living, but alas, no.</p>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using LLMs at Oxide (550 pts)]]></title>
            <link>https://rfd.shared.oxide.computer/rfd/0576</link>
            <guid>46178347</guid>
            <pubDate>Sun, 07 Dec 2025 01:17:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rfd.shared.oxide.computer/rfd/0576">https://rfd.shared.oxide.computer/rfd/0576</a>, See on <a href="https://news.ycombinator.com/item?id=46178347">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>LLM use varies widely, and the ramifications of those uses vary
accordingly; it’s worth taking apart several of the (many) uses for LLMs.</p><div><h3 data-sectnum="2.1."><a href="#_llms_as_readers">LLMs as readers<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16" role="img"><g fill="currentColor"><path d="m6.586 12.243 1.59-1.591a.75.75 0 0 1 1.061 0l.354.353a.75.75 0 0 1 0 1.06L8 13.658A4 4 0 0 1 2.343 8l1.591-1.591a.75.75 0 0 1 1.06 0l.354.354a.75.75 0 0 1 0 1.06l-1.59 1.591a2 2 0 1 0 2.828 2.829M12.066 9.591a.75.75 0 0 1-1.06 0l-.354-.354a.75.75 0 0 1 0-1.06l1.59-1.591a2 2 0 1 0-2.828-2.829l-1.59 1.591a.75.75 0 0 1-1.061 0l-.354-.353a.75.75 0 0 1 0-1.06L8 2.342A4 4 0 0 1 13.657 8z"></path><path d="M9.945 5.702a.75.75 0 0 0-1.061 0L5.702 8.884a.75.75 0 0 0 0 1.06l.353.354a.75.75 0 0 0 1.061 0l3.182-3.182a.75.75 0 0 0 0-1.06z"></path></g></svg></a></h3><div><p>LLMs are superlative at reading comprehension, able to process and meaningfully
comprehend documents effectively instantly.  This can be extraordinarily
powerful for summarizing documents — or of answering more specific questions
of a large document like a datasheet or specification.  (Ironically, LLMs are
especially good at evaluating documents to assess the degree that an LLM
assisted their creation!)</p><p>While use of LLMs to assist comprehension has little downside, it does come
with an important caveat:  when uploading a document to a hosted LLM (ChatGPT,
Claude, Gemini, etc.), there must be assurance of <strong>data privacy</strong> — and
specifically, assurance that the model will not use the document to train
future iterations of itself.  Note that this may be opt-out (that is, by
default, a model may reserve the right to train on uploaded documents), but can
generally be controlled via preferences — albeit occasionally via euphemism.
(OpenAI shamelessly calls this checked-by-default setting "Improve the model
for everyone", making anyone who doesn’t wish the model to train on their data
feel as if they suffer from a kind of reactionary avarice.)</p><p>A final cautionary note:  using LLMs to assist comprehension should not
substitute for actually reading a document where such reading is socially
expected.  More concretely:  while LLMs can be a useful tool to assist in the
evaluating of candidate materials per <a href="#rfd3">[rfd3]</a>, their use should be restricted
to be as a tool, not as a substitute for human eyes (and brain!).</p></div></div><div><h3 data-sectnum="2.2."><a href="#_llms_as_editors">LLMs as editors<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16" role="img"><g fill="currentColor"><path d="m6.586 12.243 1.59-1.591a.75.75 0 0 1 1.061 0l.354.353a.75.75 0 0 1 0 1.06L8 13.658A4 4 0 0 1 2.343 8l1.591-1.591a.75.75 0 0 1 1.06 0l.354.354a.75.75 0 0 1 0 1.06l-1.59 1.591a2 2 0 1 0 2.828 2.829M12.066 9.591a.75.75 0 0 1-1.06 0l-.354-.354a.75.75 0 0 1 0-1.06l1.59-1.591a2 2 0 1 0-2.828-2.829l-1.59 1.591a.75.75 0 0 1-1.061 0l-.354-.353a.75.75 0 0 1 0-1.06L8 2.342A4 4 0 0 1 13.657 8z"></path><path d="M9.945 5.702a.75.75 0 0 0-1.061 0L5.702 8.884a.75.75 0 0 0 0 1.06l.353.354a.75.75 0 0 0 1.061 0l3.182-3.182a.75.75 0 0 0 0-1.06z"></path></g></svg></a></h3><div><p>LLMs can be excellent editors.  Engaging an LLM late in the creative process
(that is, with a document already written and broadly polished), allows for
LLMs to provide helpful feedback on structure, phrasing, etc. — all without
danger of losing one’s own voice.  A cautionary note here: LLMs are infamous
pleasers — and you may find that the breathless praise from an LLM is in fact
more sycophancy than analysis.  This becomes more perilous the earlier one uses
an LLM in the writing process:  the less polish a document already has, the
more likely it is that an LLM will steer to something wholly different — at
once praising your groundbreaking genius while offering to rewrite it for you.</p></div></div><div><h3 data-sectnum="2.3."><a href="#_llms_as_writers">LLMs as writers<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16" role="img"><g fill="currentColor"><path d="m6.586 12.243 1.59-1.591a.75.75 0 0 1 1.061 0l.354.353a.75.75 0 0 1 0 1.06L8 13.658A4 4 0 0 1 2.343 8l1.591-1.591a.75.75 0 0 1 1.06 0l.354.354a.75.75 0 0 1 0 1.06l-1.59 1.591a2 2 0 1 0 2.828 2.829M12.066 9.591a.75.75 0 0 1-1.06 0l-.354-.354a.75.75 0 0 1 0-1.06l1.59-1.591a2 2 0 1 0-2.828-2.829l-1.59 1.591a.75.75 0 0 1-1.061 0l-.354-.353a.75.75 0 0 1 0-1.06L8 2.342A4 4 0 0 1 13.657 8z"></path><path d="M9.945 5.702a.75.75 0 0 0-1.061 0L5.702 8.884a.75.75 0 0 0 0 1.06l.353.354a.75.75 0 0 0 1.061 0l3.182-3.182a.75.75 0 0 0 0-1.06z"></path></g></svg></a></h3><div><p>While LLMs are adept at reading and can be terrific at editing, their writing
is much more mixed.  At best, writing from LLMs is hackneyed and cliché-ridden;
at worst, it brims with tells that reveal that the prose is in fact
automatically generated.</p><p>What’s so bad about this?  First, to those who can recognize an LLM’s reveals
(an expanding demographic!), it’s just embarrassing — it’s as if the writer is
walking around with their
<a href="https://bcantrill.dtrace.org/2025/12/05/your-intellectual-fly-is-open/">intellectual
fly open</a>.  But there are deeper problems:  LLM-generated writing undermines
the authenticity of not just one’s writing but of the thinking behind it as
well.  If the prose is automatically generated, might the ideas be too?  The
reader can’t be sure — and increasingly, the hallmarks of LLM generation cause
readers to turn off (or worse).</p><p>Finally, LLM-generated prose undermines a social contract of sorts:  absent
LLMs, it is presumed that of the reader and the writer, it is the writer that
has undertaken the greater intellectual exertion.  (That is, it is more work to
write than to read!)  For the reader, this is important:  should they struggle
with an idea, they can reasonably assume that the writer themselves understands
it — and it is the least a reader can do to labor to make sense of it.</p><p>If, however, prose is LLM-generated, this social contract becomes ripped up:
a reader cannot assume that the writer understands their ideas because they
might not so much have read the product of the LLM that they tasked to write it.
If one is lucky, these are LLM hallucinations: obviously wrong and quickly
discarded.  If one is unlucky, however, it will be a kind of LLM-induced
cognitive dissonance: a puzzle in which pieces don’t fit because there is in
fact no puzzle at all.  This can leave a reader frustrated:  why should they
spend more time reading prose than the writer spent writing it?</p><p>This can be navigated, of course, but it is truly perilous:  our writing
is an important vessel for building trust — and that trust can be quickly
eroded if we are not speaking with our own voice.  For us at Oxide, there
is a more mechanical reason to be jaundiced about using LLMs to write:
because our hiring process very much selects for writers, we know that
everyone at Oxide <strong>can</strong> write — and we have the luxury of demanding of
ourselves the kind of writing that we know that we are all capable of.</p><p>So our guideline is to generally not use LLMs to write, but this shouldn’t
be thought of as an absolute — and it doesn’t mean that an LLM can’t be
used as part of the writing process.  Just please: consider your
responsibility to yourself, to your own ideas — and to the reader.</p></div></div><div><h3 data-sectnum="2.4."><a href="#_llms_as_code_reviewers">LLMs as code reviewers<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16" role="img"><g fill="currentColor"><path d="m6.586 12.243 1.59-1.591a.75.75 0 0 1 1.061 0l.354.353a.75.75 0 0 1 0 1.06L8 13.658A4 4 0 0 1 2.343 8l1.591-1.591a.75.75 0 0 1 1.06 0l.354.354a.75.75 0 0 1 0 1.06l-1.59 1.591a2 2 0 1 0 2.828 2.829M12.066 9.591a.75.75 0 0 1-1.06 0l-.354-.354a.75.75 0 0 1 0-1.06l1.59-1.591a2 2 0 1 0-2.828-2.829l-1.59 1.591a.75.75 0 0 1-1.061 0l-.354-.353a.75.75 0 0 1 0-1.06L8 2.342A4 4 0 0 1 13.657 8z"></path><path d="M9.945 5.702a.75.75 0 0 0-1.061 0L5.702 8.884a.75.75 0 0 0 0 1.06l.353.354a.75.75 0 0 0 1.061 0l3.182-3.182a.75.75 0 0 0 0-1.06z"></path></g></svg></a></h3><div><p>As with reading comprehension and editing, LLMs can make for good code
reviewers.  But they can also make nonsense suggestions or otherwise miss
larger issues.  LLMs should be used for review (and can be very helpful when
targeted to look for a particular kind of issue), but that review should not
be accepted as a human substitute.</p></div></div><div><h3 data-sectnum="2.5."><a href="#_llms_as_debuggers">LLMs as debuggers<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16" role="img"><g fill="currentColor"><path d="m6.586 12.243 1.59-1.591a.75.75 0 0 1 1.061 0l.354.353a.75.75 0 0 1 0 1.06L8 13.658A4 4 0 0 1 2.343 8l1.591-1.591a.75.75 0 0 1 1.06 0l.354.354a.75.75 0 0 1 0 1.06l-1.59 1.591a2 2 0 1 0 2.828 2.829M12.066 9.591a.75.75 0 0 1-1.06 0l-.354-.354a.75.75 0 0 1 0-1.06l1.59-1.591a2 2 0 1 0-2.828-2.829l-1.59 1.591a.75.75 0 0 1-1.061 0l-.354-.353a.75.75 0 0 1 0-1.06L8 2.342A4 4 0 0 1 13.657 8z"></path><path d="M9.945 5.702a.75.75 0 0 0-1.061 0L5.702 8.884a.75.75 0 0 0 0 1.06l.353.354a.75.75 0 0 0 1.061 0l3.182-3.182a.75.75 0 0 0 0-1.06z"></path></g></svg></a></h3><div><p>LLMs can be surprisingly helpful debugging problems, but perhaps only because
our expectations for them would be so low.  While LLMs shouldn’t be relied upon
(clearly?) to debug a problem, they can serve as a kind of animatronic
<a href="https://en.wikipedia.org/wiki/Rubber_duck_debugging">rubber duck</a>, helping to
inspire the next questions to ask.  (And they can be surprising:  LLMs have been
known to debug I2C issues from the screenshot of a scope capture!)  When
debugging a vexing problem one has little to lose by using an LLM — but
perhaps also little to gain.</p></div></div><div><h3 data-sectnum="2.6."><a href="#_llms_as_programmers">LLMs as programmers<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16" role="img"><g fill="currentColor"><path d="m6.586 12.243 1.59-1.591a.75.75 0 0 1 1.061 0l.354.353a.75.75 0 0 1 0 1.06L8 13.658A4 4 0 0 1 2.343 8l1.591-1.591a.75.75 0 0 1 1.06 0l.354.354a.75.75 0 0 1 0 1.06l-1.59 1.591a2 2 0 1 0 2.828 2.829M12.066 9.591a.75.75 0 0 1-1.06 0l-.354-.354a.75.75 0 0 1 0-1.06l1.59-1.591a2 2 0 1 0-2.828-2.829l-1.59 1.591a.75.75 0 0 1-1.061 0l-.354-.353a.75.75 0 0 1 0-1.06L8 2.342A4 4 0 0 1 13.657 8z"></path><path d="M9.945 5.702a.75.75 0 0 0-1.061 0L5.702 8.884a.75.75 0 0 0 0 1.06l.353.354a.75.75 0 0 0 1.061 0l3.182-3.182a.75.75 0 0 0 0-1.06z"></path></g></svg></a></h3><div><p>LLMs are amazingly good at writing code — so much so that there is borderline
mass hysteria about LLMs entirely eliminating software engineering as a craft.
As with using an LLM to write prose, there is obvious peril here!  Unlike
prose, however (which really should be handed in a polished form to an LLM to
maximize the LLM’s efficacy), LLMs can be quite effective writing code <em>de
novo</em>.  This is especially valuable for code that is experimental or auxiliary
or otherwise throwaway.  The closer code is to the system that we ship, the
greater care needs to be shown when using LLMs.  Even with something that seems
natural for LLM contribution (e.g., writing tests), one should still be
careful:  it’s easy for LLMs to spiral into nonsense on even simple tasks.
Still, they can be extraordinarily useful — and can help to provide an entire
spectrum of utility in writing software; they shouldn’t be dismissed out of
hand.</p><p>Wherever LLM-generated code is used, it becomes the responsibility of the
engineer.  As part of this process of taking responsibility, <strong>self-review</strong>
becomes essential:  LLM-generated code should not be reviewed by others if the
responsible engineer has not themselves reviewed it.  Moreover, once in the
loop of peer review, generation should more or less be removed:  if code review
comments are addressed by wholesale re-generation, iterative review becomes
impossible.</p><p>In short, where LLMs are used to generate code, responsibility, rigor, empathy
and teamwork must remain top of mind.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Trains cancelled over fake bridge collapse image (226 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cwygqqll9k2o</link>
            <guid>46178108</guid>
            <pubDate>Sun, 07 Dec 2025 00:37:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cwygqqll9k2o">https://www.bbc.com/news/articles/cwygqqll9k2o</a>, See on <a href="https://news.ycombinator.com/item?id=46178108">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><header></header><nav data-testid="level1-navigation-container" id="main-navigation-container"><section><nav><ul><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li><li data-testid="mainNavigationItemStyled"></li></ul></nav></section></nav><main id="main-content"><article><div data-testid="byline-new" data-component="byline-block"><p><span data-testid="byline-new-contributors"><p><span>Zoe Toase<!-- -->,</span><span data-testid="byline-new-contributors-contributor-0-role-location">North West</span><span>and</span></p><p><span>Laura O'Neill<!-- -->,</span><span data-testid="byline-new-contributors-contributor-1-role-location">North West</span></p></span></p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251203-121739-f954f14c69-web-2.35.1-2/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/5e92/live/bc1e9fa0-d1fd-11f0-a892-01d657345866.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/5e92/live/bc1e9fa0-d1fd-11f0-a892-01d657345866.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/5e92/live/bc1e9fa0-d1fd-11f0-a892-01d657345866.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/5e92/live/bc1e9fa0-d1fd-11f0-a892-01d657345866.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/5e92/live/bc1e9fa0-d1fd-11f0-a892-01d657345866.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/5e92/live/bc1e9fa0-d1fd-11f0-a892-01d657345866.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/5e92/live/bc1e9fa0-d1fd-11f0-a892-01d657345866.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/5e92/live/bc1e9fa0-d1fd-11f0-a892-01d657345866.jpg.webp" loading="eager" alt="BBC/Network Rail A side-by-side photo showing a damaged bridge on the right. A section of the barriers that run along the top of the bridge appears to have collapsed and a pile of rubble can be seen underneath. A large hole can be seen in front of the bridge. The left is a photo of the bridge taken today showing it is undamaged."><span>BBC/Network Rail</span></p></div><p data-component="caption-block"><figcaption>A photo taken by a BBC North West Tonight reporter showed the bridge is undamaged </figcaption></p></figure><div data-component="text-block"><p>Trains were halted after a suspected AI-generated picture that seemed to show major damage to a bridge appeared on social media following an earthquake.</p><p>The tremor, <a target="_self" href="https://www.bbc.co.uk/news/articles/cgjn8wg53q7o">which struck on Wednesday night</a>, was felt across Lancashire and the southern Lake District.</p><p>Network Rail said it was made aware of the image which appeared to show major damage to Carlisle Bridge in Lancaster at 00:30 GMT and stopped rail services across the bridge while safety inspections were carried out.</p><p>A BBC journalist ran the image through an AI chatbot which identified key spots that may have been manipulated.</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251203-121739-f954f14c69-web-2.35.1-2/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/9c38/live/0b5ac430-d1e1-11f0-ba14-cf9dc7308cae.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/9c38/live/0b5ac430-d1e1-11f0-ba14-cf9dc7308cae.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/9c38/live/0b5ac430-d1e1-11f0-ba14-cf9dc7308cae.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/9c38/live/0b5ac430-d1e1-11f0-ba14-cf9dc7308cae.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/9c38/live/0b5ac430-d1e1-11f0-ba14-cf9dc7308cae.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/9c38/live/0b5ac430-d1e1-11f0-ba14-cf9dc7308cae.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/9c38/live/0b5ac430-d1e1-11f0-ba14-cf9dc7308cae.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/9c38/live/0b5ac430-d1e1-11f0-ba14-cf9dc7308cae.jpg.webp" loading="lazy" alt="Network Rail A photo showing damage to a bridge. A section of the barriers that run along the top of the bridge appears to have collapsed and a pile of rubble can be seen underneath. A large hole can be seen in front of the bridge"><span>Network Rail</span></p></div><p data-component="caption-block"><figcaption>Network Rail said it was made aware that the image was on social media</figcaption></p></figure><div data-component="text-block"><p>Network Rail said the railway line was fully reopened at around 02:00 GMT and it has urged people to "think about the serious impact it could have" before creating or sharing hoax images.</p><p>"The disruption caused by the creation and sharing of hoax images and videos like this creates a completely unnecessary delay to passengers at a cost to the taxpayer," a spokesperson said.</p><p>"It adds to the high workload of our frontline teams, who work extremely hard to keep the railway running smoothly," the spokesperson said.</p><p>"The safety of rail passengers and staff is our number one priority and we will always take any safety concerns seriously."</p><p>The British Transport Police said it was "made aware" of the situation but there was no ongoing investigation into the incident.</p><p>Network Rail said 32 services including passenger and freight trains were delayed because of hoax. </p><p>A spokesperson for the rail provider said a mix of passenger and freight train would have been impacted.</p><p>They said some of them would have been directly stopped or slowed while it  checked the lines, but a lot of the trains were delayed as a result of earlier services still being in their path. </p><p>The spokesperson said many of them would have been local but because of the length of the West Coast Main Line some trains were delayed as far north as Scotland.</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251203-121739-f954f14c69-web-2.35.1-2/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/ff4b/live/2050ccf0-d1fe-11f0-b6c9-7decc25e6290.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/ff4b/live/2050ccf0-d1fe-11f0-b6c9-7decc25e6290.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/ff4b/live/2050ccf0-d1fe-11f0-b6c9-7decc25e6290.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/ff4b/live/2050ccf0-d1fe-11f0-b6c9-7decc25e6290.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/ff4b/live/2050ccf0-d1fe-11f0-b6c9-7decc25e6290.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/ff4b/live/2050ccf0-d1fe-11f0-b6c9-7decc25e6290.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/ff4b/live/2050ccf0-d1fe-11f0-b6c9-7decc25e6290.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/ff4b/live/2050ccf0-d1fe-11f0-b6c9-7decc25e6290.jpg.webp" loading="lazy" alt="A photo showing the bridge is undamaged "></p></div><p data-component="caption-block"><figcaption>A BBC North West reporter visited the bridge today and confirmed it was undamaged</figcaption></p></figure><div data-component="text-block"><p>Railway expert Tony Miles said due to the timing of the incident, very few passengers will have been impacted by the hoax as the services passing through at that time were primarily freight and sleeper trains.</p><p>"They generally go slow so as not to disturb the passengers trying to sleep - this means they have a bit of leeway to go faster and make up time if they encounter a delay," he said.</p><p>"It's more the fact that Network Rail will have had to mobilise a team to go and check the bridge which could impact their work for days."</p><p>He urged people to consider hoaxes like this could have on real people.</p><p>"If they actually did delay a train it could have impacted someone who had to get to a medical appointment, or a flight or a funeral.</p><p>"It may seem like a game, but anyone who's thinking of doing this should consider how it will impact real people."</p></div></article></main><hr data-testid="main-footer-divider"></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kilauea erupts, destroying webcam [video] (463 pts)]]></title>
            <link>https://www.youtube.com/watch?v=TK2N99BDw7A</link>
            <guid>46177645</guid>
            <pubDate>Sat, 06 Dec 2025 23:39:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=TK2N99BDw7A">https://www.youtube.com/watch?v=TK2N99BDw7A</a>, See on <a href="https://news.ycombinator.com/item?id=46177645">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Saving Japan's exceptionally rare 'snow monsters' (117 pts)]]></title>
            <link>https://www.bbc.com/future/article/20251203-japans-disappearing-snow-monsters</link>
            <guid>46177418</guid>
            <pubDate>Sat, 06 Dec 2025 23:06:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/future/article/20251203-japans-disappearing-snow-monsters">https://www.bbc.com/future/article/20251203-japans-disappearing-snow-monsters</a>, See on <a href="https://news.ycombinator.com/item?id=46177418">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div data-component="image-block"><figure><div><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251203-121739-f954f14c69-web-2.35.1-2/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0mlb7w7.jpg.webp 160w, https://ichef.bbci.co.uk/images/ic/240xn/p0mlb7w7.jpg.webp 240w, https://ichef.bbci.co.uk/images/ic/320xn/p0mlb7w7.jpg.webp 320w, https://ichef.bbci.co.uk/images/ic/480xn/p0mlb7w7.jpg.webp 480w, https://ichef.bbci.co.uk/images/ic/640xn/p0mlb7w7.jpg.webp 640w, https://ichef.bbci.co.uk/images/ic/800xn/p0mlb7w7.jpg.webp 800w, https://ichef.bbci.co.uk/images/ic/1024xn/p0mlb7w7.jpg.webp 1024w, https://ichef.bbci.co.uk/images/ic/1376xn/p0mlb7w7.jpg.webp 1376w, https://ichef.bbci.co.uk/images/ic/1920xn/p0mlb7w7.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0mlb7w7.jpg.webp" loading="lazy" alt="Getty Images Dozens of snow-covered trees appear like figures, with the tree in the foreground appearing particularly like a monster. A blue sky is seen behind them (Credit: Getty Images)"><span>Getty Images</span></p></div></figure></div><div data-component="layout-block"><p><b id="a-unique-natural-wonder-is-being-eroded.-can-japan-bring-its-breathtaking-&quot;juhyo&quot;-back-from-the-brink?">A unique natural wonder is being eroded. Can Japan bring its breathtaking "juhyo" back from the brink?</b></p><p>Each winter, the upper slopes of Mount Zao in northern Japan – one of the country's best-known ski areas – are transformed. Fir trees coated in thick frost and snow swell into ghostly figures known as "juhyo" or "snow monsters".</p><p>Juhyo form only under exceptionally rare atmospheric conditions, emerging when strong, persistent winter winds carry supercooled water droplets that freeze on contact with the local evergreen Aomori todomatsu trees, gradually layering into <a target="_blank" href="https://www.weather.gov/otx/Rime">rime ice</a>.</p><p>At Mount Zao, these formations <a target="_blank" href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2011JD017042">occur during sustained westerly winds</a> of up to 26m per second (85ft per second), with surface air temperatures between -6.3C to -0.1C (21-31F) and unusually high cloud liquid water content. Under these precise conditions, the rime thickens on the windward side of trees into overlapping ridges known as "shrimp tails", the distinctive shapes that cluster together to form the towering juhyo figures.</p><p>"Because such precise meteorological and ecological conditions align in very few places, Zao's snow monsters are a phenomenon almost unique to northern Japan," says Fumitaka Yanagisawa, an emeritus professor of geochemistry who studies the juhyo at Yamagata University.</p></div><div data-component="layout-block"><p>The snow monsters are the biggest winter draw of the Zao area, a mountain range which lies between Japan's Yamagata and Miyagi prefectures and attracts <a target="_blank" href="https://www.dbj.jp/upload/investigate/docs/8d6c809df40abe87105e30e8797b3f78.pdf">tens of thousands</a> of visitors annually.</p><p>But recent research indicates that the monsters are becoming slimmer.</p><figure><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251203-121739-f954f14c69-web-2.35.1-2/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0mlb8rv.jpg.webp 160w, https://ichef.bbci.co.uk/images/ic/240xn/p0mlb8rv.jpg.webp 240w, https://ichef.bbci.co.uk/images/ic/320xn/p0mlb8rv.jpg.webp 320w, https://ichef.bbci.co.uk/images/ic/480xn/p0mlb8rv.jpg.webp 480w, https://ichef.bbci.co.uk/images/ic/640xn/p0mlb8rv.jpg.webp 640w, https://ichef.bbci.co.uk/images/ic/800xn/p0mlb8rv.jpg.webp 800w, https://ichef.bbci.co.uk/images/ic/1024xn/p0mlb8rv.jpg.webp 1024w, https://ichef.bbci.co.uk/images/ic/1376xn/p0mlb8rv.jpg.webp 1376w, https://ichef.bbci.co.uk/images/ic/1920xn/p0mlb8rv.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0mlb8rv.jpg.webp" loading="lazy" alt="Getty Images Mount Zao's snow monsters are a phenomenon almost unique to northern Japan (Credit: Getty Images)"><span>Getty Images</span></p><figcaption>Mount Zao's snow monsters are a phenomenon almost unique to northern Japan (Credit: Getty Images)</figcaption></figure><p><a target="_blank" href="https://www.yamagata-u.ac.jp/jp/files/3517/5436/7737/03_.pdf">In August 2025</a>, a research team led by Yanagisawa announced findings that quantified what <a target="_blank" href="https://www.yamagata-u.ac.jp/jp/files/9717/4107/2854/362.pdf">locals have long observed</a>. By analysing identical-angle photographs of Zao's summit taken since 1933, the team measured the thickness of the figures on a six-point scale. The findings (which have not yet been published in a scientific journal) indicate a widespread shrinking of the juhyo.</p><p>"In the 1930s, we saw juhyo five to six metres [16-20ft] across," Yanagisawa says. "By the postwar decades, they were often two to three metres [7-10ft]. Since 2019, many are half a metre [1.6ft] or less. Some are barely columns."</p></div><div data-component="layout-block"><p>The cause is twofold, says Yanagisawa: a warming climate and a forest under attack. The host tree, Aomori todomatsu, <a target="_blank" href="https://www.mdpi.com/2072-4292/13/2/260">suffered a moth outbreak in 2013 that stripped its needles</a>. <a target="_blank" href="https://www.mdpi.com/2079-3197/10/4/63">Bark beetles followed in 2015,</a> boring into weakened trunks. <a target="_blank" href="https://www.pref.yamagata.jp/documents/2446/morishia_vol26-6-7.pdf">Yamagata officials report</a> that around 23,000 firs, about a fifth of the prefectural side's stands, have died. With fewer branches and leaves, there is little surface for snow and ice to cling to.</p><p>Another <a target="_blank" href="https://www.jstage.jst.go.jp/article/jcsir/2019/0/2019_231/_pdf">2019 study</a> found that in nearby Yamagata City, average temperatures from December to March have risen by <a target="_blank" href="http://sci.kj.yamagata-u.ac.jp/~zao/documents/no21s-3.pdf">about 2C (3.6F) over the past 120 years</a>. The lower altitude limit of juhyo formation has shifted upward in step with this warming, it found, while the juhyo also last for fewer days of the year.</p><p>"Unique landscapes are already being lost to climate change," says Akihiko Ito, an ecologist who specialises in forests and climate change at the University of Tokyo.</p><figure><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251203-121739-f954f14c69-web-2.35.1-2/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0mlb92z.jpg.webp 160w, https://ichef.bbci.co.uk/images/ic/240xn/p0mlb92z.jpg.webp 240w, https://ichef.bbci.co.uk/images/ic/320xn/p0mlb92z.jpg.webp 320w, https://ichef.bbci.co.uk/images/ic/480xn/p0mlb92z.jpg.webp 480w, https://ichef.bbci.co.uk/images/ic/640xn/p0mlb92z.jpg.webp 640w, https://ichef.bbci.co.uk/images/ic/800xn/p0mlb92z.jpg.webp 800w, https://ichef.bbci.co.uk/images/ic/1024xn/p0mlb92z.jpg.webp 1024w, https://ichef.bbci.co.uk/images/ic/1376xn/p0mlb92z.jpg.webp 1376w, https://ichef.bbci.co.uk/images/ic/1920xn/p0mlb92z.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0mlb92z.jpg.webp" loading="lazy" alt="Yanagisawa Fumitaka Snow monsters forming on the same tree on the Jizōdake summit of Mount Zao in 2008, 2019 and 2022 (Credit: Yanagisawa Fumitaka)"><span>Yanagisawa Fumitaka</span></p><figcaption>Snow monsters forming on the same tree on the Jizōdake summit of Mount Zao in 2008, 2019 and 2022 (Credit: Yanagisawa Fumitaka)</figcaption></figure><p>Research shows that Japan's warming climate and extreme weather are <a target="_blank" href="https://www.sciencedirect.com/science/article/abs/pii/S0006320712002935">already damaging many of its high mountain forests</a>. "Seasonal shifts in spring and autumn can harm leaves, and insect outbreaks are expanding. These stresses may reduce forest growth and density," Ito says.</p></div><div data-component="layout-block"><p>Across Japan's alpine zones, <a target="_blank" href="https://www.jstage.jst.go.jp/article/ger/10/2/10_10-2_03/_article/-char/en">temperatures have been rising faster than the global average</a> since the 1980s. "In scenarios where climate change continues to advance significantly by the end of this century, it is possible that in warmer-than-usual winters, juhyo may no longer form at all," Ito says.</p><p>The threat has prompted action across Yamagata. In March 2023, the prefecture launched the <a target="_blank" href="https://yamagatayama.com/jyuhyo/about/#kaigi_1">Juhyo Revival Conference</a> – a permanent council bringing together researchers, officials, local businesses and residents to coordinate long-term efforts to restore the fir forests and preserve Mount Zao's snow monsters.</p><p>Juhyo are not only a natural spectacle but also a pillar of the local economy. "The influx of tourists supports hotels, restaurants and souvenir shops throughout the area," says Genji Akiba, deputy director of the Zao Onsen Tourism Association. "If the juhyo disappear, it would be a huge blow."</p><p>"Revival is a strong wish of our citizens," says Yoko Honma, a conservation specialist at Yamagata Prefecture's nature division. Since 2019, the local forest office has <a target="_blank" href="https://yamagatayama.com/jyuhyo/wp-content/uploads/2024/03/%E3%88%AA-3_%E4%BC%9A%E8%AD%B0%E8%B3%87%E6%96%99.pdf">transplanted more than 190 naturally regenerated saplings</a> from lower slopes to the summit zone near the ropeway station. "Because it takes 50 to 70 years for these firs to mature, the key is sustaining conservation across generations," says Honma. "We need patience and continuity."</p><figure><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251203-121739-f954f14c69-web-2.35.1-2/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0mlb99m.jpg.webp 160w, https://ichef.bbci.co.uk/images/ic/240xn/p0mlb99m.jpg.webp 240w, https://ichef.bbci.co.uk/images/ic/320xn/p0mlb99m.jpg.webp 320w, https://ichef.bbci.co.uk/images/ic/480xn/p0mlb99m.jpg.webp 480w, https://ichef.bbci.co.uk/images/ic/640xn/p0mlb99m.jpg.webp 640w, https://ichef.bbci.co.uk/images/ic/800xn/p0mlb99m.jpg.webp 800w, https://ichef.bbci.co.uk/images/ic/1024xn/p0mlb99m.jpg.webp 1024w, https://ichef.bbci.co.uk/images/ic/1376xn/p0mlb99m.jpg.webp 1376w, https://ichef.bbci.co.uk/images/ic/1920xn/p0mlb99m.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0mlb99m.jpg.webp" loading="lazy" alt="Yanagisawa Fumitaka/ Tohoku Regional Forest Office An area of Mount Zao's Jizōdake summit in 2010 (top left), 2013 (top right), 2020 (lower left) and 2025 (lower right) (Credit: Yanagisawa Fumitaka/ Tohoku Regional Forest Office)"><span>Yanagisawa Fumitaka/ Tohoku Regional Forest Office</span></p><figcaption>An area of Mount Zao's Jizōdake summit in 2010 (top left), 2013 (top right), 2020 (lower left) and 2025 (lower right) (Credit: Yanagisawa Fumitaka/ Tohoku Regional Forest Office)</figcaption></figure></div><div data-component="layout-block"><p>In Murayama, about 20km (12 miles) north-west of Zao, students from a forestry and environmental science course at Murayama Technical High School have also taken up the challenge of reviving the firs.</p><p><b id="more-like-this:">More like this:</b></p><p>• <a target="_self" href="https://www.bbc.com/future/article/20221213-why-christmas-trees-may-be-good-for-the-environment">The overlooked benefits of real Christmas trees</a></p><p><a target="_blank" href=""></a>• <a target="_self" href="https://www.bbc.com/future/article/20251120-a-remote-and-dangerous-amazon-river-finally-spills-its-secrets">The secrets of the Amazon's most mysterious river</a></p><p><a target="_blank" href=""></a>• <a target="_self" href="https://www.bbc.com/future/article/20251125-the-mysterious-black-fungus-from-chernobyl-that-appears-to-eat-radiation">The mysterious black fungus from Chernobyl that may eat radiation</a></p><p>Since 2022, the students have been planting Aomori todomatsu trees and studying how to propagate and protect the species. Together with staff from the Yamagata Forest Office, they visit Mount Zao to collect young fir saplings and bring them back to their school for research. There, they cultivate stems through cuttings and experiment with methods for artificially propagating and efficiently producing seedlings.</p><p>"It's been challenging," says Rin Oizumi, a second-year student in the course. "When the seeds we sowed in heavy rain finally sprouted, I felt both relief and excitement. But it was heartbreaking to find that some plots had been damaged by field mice, which had eaten the young shoots." The students have also conducted preliminary experiments using branches of a related fir species, which have shown successful germination.</p><figure><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251203-121739-f954f14c69-web-2.35.1-2/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0mlb9p9.jpg.webp 160w, https://ichef.bbci.co.uk/images/ic/240xn/p0mlb9p9.jpg.webp 240w, https://ichef.bbci.co.uk/images/ic/320xn/p0mlb9p9.jpg.webp 320w, https://ichef.bbci.co.uk/images/ic/480xn/p0mlb9p9.jpg.webp 480w, https://ichef.bbci.co.uk/images/ic/640xn/p0mlb9p9.jpg.webp 640w, https://ichef.bbci.co.uk/images/ic/800xn/p0mlb9p9.jpg.webp 800w, https://ichef.bbci.co.uk/images/ic/1024xn/p0mlb9p9.jpg.webp 1024w, https://ichef.bbci.co.uk/images/ic/1376xn/p0mlb9p9.jpg.webp 1376w, https://ichef.bbci.co.uk/images/ic/1920xn/p0mlb9p9.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0mlb9p9.jpg.webp" loading="lazy" alt="Harada Takato Efforts are being made to transplant of Aomori-todomatsu saplings to Mount Zao in an attempt to revive the forests (Credit: Harada Takato)"><span>Harada Takato</span></p><figcaption>Efforts are being made to transplant of Aomori-todomatsu saplings to Mount Zao in an attempt to revive the forests (Credit: Harada Takato)</figcaption></figure></div><div data-component="layout-block"><p>Kanon Taniai, Oizumi's classmate, recalls seeing more and more fallen or dead trees as she and other students neared the summit one day in July 2024. "It made me feel really sad," she says. "Growing seedlings is hard work, but we want to do what we can to help bring Mount Zao back to life."</p><p>For Taniai, protecting the Juhyo means passing their legacy to the next generation. "They are called snow monsters because nothing else looks like them," she says. "I want the world to see them, and to feel how special Japan's nature is."</p><p>--</p><p><i id="for-essential-climate-news-and-hopeful-developments-to-your-inbox,-sign-up-to-the">For essential climate news and hopeful developments to your inbox, sign up to the&nbsp;</i><a target="_self" href="https://cloud.email.bbc.com/FutureEarth_Newsletter_Signup?&amp;at_bbc_team=studios&amp;at_medium=Onsite&amp;at_objective=acquisition&amp;at_ptr_name=bbc.com&amp;at_link_origin=futurearticle&amp;at_campaign=futureearth&amp;at_campaign_type=owned&amp;&amp;"><i id="future-earth-newsletter,">Future Earth newsletter,</i></a><i id="while">&nbsp;while&nbsp;</i><a target="_self" href="https://cloud.email.bbc.com/SignUp10_08?&amp;at_bbc_team=studios&amp;at_medium=Onsite&amp;at_objective=acquisition&amp;at_ptr_name=bbc.com&amp;at_link_origin=featuresarticle&amp;at_campaign=essentiallist&amp;at_campaign_type=owned"><i id="the-essential-list">The Essential List</i></a><i id="delivers-a-handpicked-selection-of-features-and-insights-twice-a-week.">&nbsp;delivers a handpicked selection of features and insights twice a week.&nbsp;</i></p><p><i id="for-more-science,-technology,-environment-and-health-stories-from-the-bbc,-follow-us-on">For more science, technology, environment and health stories from the BBC, follow us on </i><a target="_blank" href="https://www.facebook.com/BBCFuture/"><i id="facebook">Facebook</i></a><i id="and">&nbsp;and </i><a target="_blank" href="https://www.instagram.com/bbcfuture_official/"><i id="instagram">Instagram</i></a><i id=".">.</i></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[United States Antarctic Program Field Manual (2024) [pdf] (113 pts)]]></title>
            <link>https://www.usap.gov/usapgov/travelAndDeployment/documents/Continental-Field-Manual-2024.pdf</link>
            <guid>46177132</guid>
            <pubDate>Sat, 06 Dec 2025 22:26:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.usap.gov/usapgov/travelAndDeployment/documents/Continental-Field-Manual-2024.pdf">https://www.usap.gov/usapgov/travelAndDeployment/documents/Continental-Field-Manual-2024.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=46177132">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Screenshots from developers: 2002 vs. 2015 (2015) (380 pts)]]></title>
            <link>https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/</link>
            <guid>46176905</guid>
            <pubDate>Sat, 06 Dec 2025 21:55:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/">https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/</a>, See on <a href="https://news.ycombinator.com/item?id=46176905">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

	<article>

		

		<div>
			<p>In 2002 I asked a number of developers/Unix people for screenshots of their desktops. I recently <a href="https://anders.unix.se/2015/10/28/screenshots-from-developers--unix-people-2002/">republished them</a>, and, seeing <a href="https://news.ycombinator.com/item?id=10469824">the</a> <a href="https://www.reddit.com/r/linux/comments/3qlyf6/screenshots_from_developers_unix_people_2002/">interest</a> this generated, I thought it’d be fun to ask the same people* again 13 years later. To my delight I managed to reach many of them.</p>

<p><small>* Sans Dennis Ritchie and itojun, who are no longer with us.</small></p>

<p>So, without further ado:</p>

<div>

<div>
<figure>
    <a href="https://anders.unix.se/images/bwk_desktop.jpg">
        <img src="https://anders.unix.se/images/bwk_desktop.jpg">
    </a>
    
    <figcaption>
        July 2002
        
    </figcaption>
    
</figure>


<blockquote><p>my desktop is pretty boring, since it consists of xterm windows to whatever unix system i am using at the moment.  the machine itself is likely to be running some x-window server like exceed on some flavor of windows, though for many years i just used an x terminal.</p></blockquote>

</div>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_bwk_2015.png">
        <img src="https://anders.unix.se/images/desktop_bwk_2015.png.thumb.jpg">
    </a>
    
    <figcaption>
        October 2015
        
    </figcaption>
    
</figure>


<blockquote><p>If you thought it was boring last time, check this out!</p></blockquote>

</div>
</div>

<hr>

<div>

<div>
<p>2002:</p>
<blockquote><p>
I don’t know how to make a screenshot, because I normally use my computer in text-mode. I have X and GNOME installed, but I use them only occasionally.
</p></blockquote>

</div>
<div>
<p>2015:</p>
<blockquote><p>
Under X, I use the standard environment of Trisquel, but mostly I type at Emacs in a console.
</p></blockquote>

</div>
</div>

<hr>

<div>

<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_b_moolenaar.png">
        <img src="https://anders.unix.se/images/desktop_b_moolenaar.png.thumb.jpg">
    </a>
    
    <figcaption>
        September 2002
        
    </figcaption>
    
</figure>


<blockquote><p>
Well, my desktop is quite boring. I mostly work with four xterms and a few Netscape windows. The KDE bar hides automatically, you can only see a thin grey line at the bottom.
</p></blockquote>

</div>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_b_moolenaar_2015.png">
        <img src="https://anders.unix.se/images/desktop_b_moolenaar_2015.png.thumb.jpg">
    </a>
    
    <figcaption>
        November 2015
        
    </figcaption>
    
</figure>


<blockquote><p>
Here is the new one.  You'll see that, like before, I have lots of xterms where I work on Vim, Zimbu and email.  Now using the Chrome browser, showing off the Zimbu homepage.  But clearly everything has become bigger!
</p></blockquote>

</div>
</div>

<hr>

<div>

<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_rasmus_lerdorf.png">
        <img src="https://anders.unix.se/images/desktop_rasmus_lerdorf.png.thumb.jpg">
    </a>
    
    <figcaption>
        September 2002
        
    </figcaption>
    
</figure>


<blockquote><p>
Linux (2.4.20-pre5), Gnome2, vim, Pine.
</p></blockquote>

</div>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_rasmus_lerdorf_2015.png">
        <img src="https://anders.unix.se/images/desktop_rasmus_lerdorf_2015.png.thumb.jpg">
    </a>
    
    <figcaption>
        October 2015
        
    </figcaption>
    
</figure>


<blockquote><p>
Not that much has changed in 13 years. Still using Linux. Still just a browser window and a ton of terminals hiding behind them.  The main change is that switched from Pine to Thunderbird for email at some point. The OS on my laptop here is Ubuntu with Unity although there are a lot of Debian packages installed so it is a bit of a hybrid at this point. Oh, and yes, my son Carl is a lot older now.
</p></blockquote>

</div>
</div>

<hr>

<div>

<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_warren_toomey.gif">
        <img src="https://anders.unix.se/images/desktop_warren_toomey.gif">
    </a>
    
    <figcaption>
        August 2002
        
    </figcaption>
    
</figure>


<blockquote>
<p>Ah, my desktop is pretty boring, I used fvwm 1.24 as my window manager and I try to have no more than 1 or 2 windows open per virtual desktop.  I use FreeBSD 4-STABLE as my operating system. I first came across Unix when I got an account on a Pyramid 90x running OSx. This had a dual-universe setup: both AT&amp;T and BSD-style environments, chosen by an environment variable. Initially I was given the AT&amp;T environment, but my friends convinced me to ``come over” to BSD. Since then I’ve been a BSD afficionado.</p>

<p>After OSx, SunOS 3.5 and later SunOS releases, until 386BSD 0.1 came out and I started to run BSD at home. Then when 386BSD transmogrified to FreeBSD, I went with FreeBSD.</p>

<p>In terms of desktop, I’m a command-line guy, always will be. My favourite editor is vi, my favourite shell is tcsh (but kudos to rc for elegance).  So I don’t really feel the need for GUI things like Gnome or KDE :-)</p>
</blockquote>

</div>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_warren_toomey_2015.jpg">
        <img src="https://anders.unix.se/images/desktop_warren_toomey_2015.jpg">
    </a>
    
    <figcaption>
        October 2015
        
    </figcaption>
    
</figure>


<blockquote>
<p>How things have (and have not changed). I'm still a command-line junkie with at least two xterm windows open. I'm still using a 3x3 virtual desktop. However, instead of fvwm, it is now LXDE. I've also switched from FreeBSD to Linux and I'm running Lubuntu as my distribution.</p>

<p>There are a lot of indispensable GUI tools that I use. These include Firefox, lyx, Gimp, KeepassX, Shutter, viking, dia, Wireshark, calibre, audacity, Handbrake and VLC. But where possible I still prefer to script things. My main development languages are still shell, Perl and C.</p>

<p>My shell is now bash. The vi keystrokes are burned into my fingertips and, as long as vim can be ported to new systems, that will be my text editor until I pass on. My mail client is now mutt (definitely not a web client) and my mail is stored locally, not on someone else's server.</p>

<p>The only issue I have is that, since a job change, I now have to deal with Windoze things. Thus, I have VirtualBox, libreoffice and Wine to help me do that.</p>

<p>I started with Unix on a Pyramid 90x. I now have a smart phone that blows the 90x out of the water on performance, RAM and storage. But I'm so very happy that, somewhere down underneath, there is still a Bourne shell and an operating system that does open(), close(), read(), write(), fork() and exec()!</p>
</blockquote>

</div>
</div>

<hr>

<div>
<p><a href="https://en.wikipedia.org/wiki/Jordan_Hubbard">Jordan Hubbard</a> (FreeBSD co-founder, later Director of UNIX Technology at Apple; now CTO of iXsystems):
</p>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_jordan_hubbard.jpg">
        <img src="https://anders.unix.se/images/desktop_jordan_hubbard.jpg">
    </a>
    
    <figcaption>
        July 2002
        
    </figcaption>
    
</figure>

</div>
<div>
<figure>
    <a href="https://anders.unix.se/images/desktop_jordan_hubbard_2015.png">
        <img src="https://anders.unix.se/images/desktop_jordan_hubbard_2015.png.thumb.jpg">
    </a>
    
    <figcaption>
        November 2015
        
    </figcaption>
    
</figure>


<blockquote>
<p>You’ll probably be sad (or perhaps not) to hear that my desktop hasn’t really changed much at all - still OS X, though because OS X has virtual desktops now I have multiple “desktops” (6 of them) where Mail.app runs on one, Safari on another, Calendar, Slack, etc - all on separate desktops.  This makes it a bit boring, but here’s the one I probably spend the most time in - the terminal window desktop. :)</p>
</blockquote>

</div>
</div>

<hr>



<hr>

<p>Discussion: <a href="https://news.ycombinator.com/item?id=10722536">Hacker News</a>; reddit: <a href="https://www.reddit.com/r/programming/comments/3wg48k/screenshots_from_developers_2002_vs_2015/">/r/programming</a>, <a href="https://www.reddit.com/r/linux/comments/3w83ta/screenshots_from_developers_2002_vs_2015/">/r/linux</a></p>

		</div>

		 


	</article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The past was not that cute (310 pts)]]></title>
            <link>https://juliawise.net/the-past-was-not-that-cute/</link>
            <guid>46176893</guid>
            <pubDate>Sat, 06 Dec 2025 21:53:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://juliawise.net/the-past-was-not-that-cute/">https://juliawise.net/the-past-was-not-that-cute/</a>, See on <a href="https://news.ycombinator.com/item?id=46176893">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		<main id="main">
		
				
		
<article id="post-1585">

		
	
		
	<div>
		
<p>I was excited when <a href="https://en.wikipedia.org/wiki/Cottagecore">cottagecore</a> became a thing. Maybe my interest in retro clothes and handicrafts would be less embarrassing now!</p>


<div>
<figure><img data-recalc-dims="1" fetchpriority="high" decoding="async" width="925" height="515" src="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?resize=925%2C515&amp;ssl=1" alt="" srcset="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?w=2111&amp;ssl=1 2111w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?resize=300%2C167&amp;ssl=1 300w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?resize=1024%2C570&amp;ssl=1 1024w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?resize=768%2C427&amp;ssl=1 768w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?resize=1536%2C855&amp;ssl=1 1536w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?resize=2048%2C1140&amp;ssl=1 2048w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?resize=1320%2C735&amp;ssl=1 1320w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-3.18.30-PM-edited.png?w=1850&amp;ssl=1 1850w" sizes="(max-width: 925px) 100vw, 925px"><figcaption>Cottagecore, Pinterest 2025</figcaption></figure>
</div>


<p>I still enjoy it. But in spaces focused on old-fashioned vibes, you encounter a lot of people who believe that the past was <em>actually</em> this charming.</p>



<figure>
<figure><img data-recalc-dims="1" decoding="async" width="680" height="1000" data-id="1602" src="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/MV5BODZjMGRkZmMtYzRhMC00NTMyLTg4MjAtNGIxZTIzYjY0MjNmXkEyXkFqcGc%40._V1_.jpg?resize=680%2C1000&amp;ssl=1" alt="" srcset="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/MV5BODZjMGRkZmMtYzRhMC00NTMyLTg4MjAtNGIxZTIzYjY0MjNmXkEyXkFqcGc%40._V1_.jpg?w=680&amp;ssl=1 680w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/MV5BODZjMGRkZmMtYzRhMC00NTMyLTg4MjAtNGIxZTIzYjY0MjNmXkEyXkFqcGc%40._V1_.jpg?resize=204%2C300&amp;ssl=1 204w" sizes="(max-width: 680px) 100vw, 680px"><figcaption>1879s farmers through the eyes of the 1970s.</figcaption></figure>



<figure><img decoding="async" width="678" height="948" data-id="1605" src="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-4.39.45-PM.png?fit=678%2C948&amp;ssl=1" alt="" srcset="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-4.39.45-PM.png?w=678&amp;ssl=1 678w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-4.39.45-PM.png?resize=215%2C300&amp;ssl=1 215w" sizes="(max-width: 678px) 100vw, 678px"><figcaption>Actual farmer, <a href="https://commons.wikimedia.org/wiki/File:Wife_of_Tenant_Farmer.jpg">Texas 1937</a>.</figcaption></figure>
</figure>



<p> <a href="https://en.wikipedia.org/wiki/Laura_Ingalls_Wilder">Laura Ingalls Wilder</a>‘s <em>Little House on the Prairie</em> books are problematic, and also I will always love them. She wrote about the beauty of family and hard work, but she wrote them because she spent her whole life supporting disabled family members. She and her daughter beautified her “pioneer girl” history to make good books. Her daughter describes the reality: &nbsp;“It took seven successive years of complete crop failure, with work, weather and sickness that wrecked [my father’s] health permanently, and interest rates of 36 percent on money borrowed to buy food, to dislodge us from that land.”</p>





<p>My own version of this mistake was thinking that people’s personalities were different in the past. I grew up listening to folk music and imagining a past where nice boys would admire a nice quiet girl like me, and I wouldn’t have to figure out dating because everything would just unfold, probably on a May morning. My mother pointed out that a lot of the <a href="https://en.wikipedia.org/wiki/Down_by_Blackwaterside">songs</a> along the lines of “my own true love proved false to me” were about unplanned pregnancies.</p>



<p>I also assumed the bonny lasses in these songs would be wholesome and nice. But were popular girls of the past nicer people than they are now?</p>



<figure>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="925" height="925" data-id="1591" src="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?resize=925%2C925&amp;ssl=1" alt="" srcset="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?resize=1024%2C1024&amp;ssl=1 1024w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?resize=1536%2C1536&amp;ssl=1 1536w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?resize=80%2C80&amp;ssl=1 80w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?resize=1320%2C1320&amp;ssl=1 1320w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?w=1900&amp;ssl=1 1900w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/star-of-the-county-down.jpg?w=1850&amp;ssl=1 1850w" sizes="auto, (max-width: 925px) 100vw, 925px"></figure>



<figure><img loading="lazy" decoding="async" width="1164" height="1312" data-id="1594" src="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-2.42.31-PM.png?fit=908%2C1024&amp;ssl=1" alt="" srcset="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-2.42.31-PM.png?w=1164&amp;ssl=1 1164w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-2.42.31-PM.png?resize=266%2C300&amp;ssl=1 266w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-2.42.31-PM.png?resize=908%2C1024&amp;ssl=1 908w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Screenshot-2025-12-06-at-2.42.31-PM.png?resize=768%2C866&amp;ssl=1 768w" sizes="auto, (max-width: 925px) 100vw, 925px"></figure>
</figure>



<p>Some of my picture came from growing up in the Anglo-American folk dance and music community: it had a lot of aging hippies with graduate degrees. So I came away imagining a past with a lot of the kind of people who become engineers and English teachers. A more accurate picture would have been “Imagine a small town where the same 19 kids form your entire group of peers and potential partners.”</p>


<div>
<figure><img loading="lazy" decoding="async" width="1415" height="792" src="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Belle.webp?fit=925%2C518&amp;ssl=1" alt="" srcset="https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Belle.webp?w=1415&amp;ssl=1 1415w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Belle.webp?resize=300%2C168&amp;ssl=1 300w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Belle.webp?resize=1024%2C573&amp;ssl=1 1024w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Belle.webp?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/juliawise.net/wp-content/uploads/2025/12/Belle.webp?resize=1320%2C739&amp;ssl=1 1320w" sizes="auto, (max-width: 925px) 100vw, 925px"></figure>
</div>


<p>Bookish girls like Belle didn’t really go to live in enchanted castles with huge libraries. They stayed in villages where everyone thought they were weird and their best option was Gaston.</p>





<p>Maybe my favorite podcast episode ever is <a href="https://www.econtalk.org/rachel-laudan-on-the-history-of-food-and-cuisine/">Rachel Laudan on food history</a>: “I did have the extraordinary good fortune to grow up eating what I think the romantic movement dreams of. We had milk fresh from the cow; I never had pasteurized milk until I went to school. We had fish from the river, pheasant from the farm. The food was extremely good. . . . everything was fresh from the garden. So, I&nbsp;<em>do</em>&nbsp;romanticize—some of that because the taste was often extraordinary. And then I tweak myself and I say, ‘Look, Rachel, your mother spent all day, every day gardening or cooking.’ Essentially. As well as doing other chores. And she said to you, ‘Rachel, it’s servitude. I want you to have a life I didn’t have.’&nbsp;“</p>



<p>I love living in a time and place where we get to choose aesthetics. I have bread rising in my kitchen right now, and I’m looking forward to baking it in an electric oven that doesn’t require me stacking wood or putting smoke into my house.</p>



<p>So I’ll continue to enjoy retro vibes, and draw on the past for lessons on how to be a human. (For example, making music together is one of life’s great experiences, and it’s a mistake to entirely substitute recorded music for that.) But I’ll enjoy doing so with indoor plumbing, dental care, and a desk job. </p>








	
	</div>
	
	
	
			


</article>
<!-- #comments -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>		
				
		</main>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Coffee linked to slower biological ageing among those with severe mental illness (180 pts)]]></title>
            <link>https://www.kcl.ac.uk/news/coffee-linked-to-slower-biological-ageing-among-those-with-severe-mental-illness-up-to-a-limit</link>
            <guid>46176766</guid>
            <pubDate>Sat, 06 Dec 2025 21:33:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.kcl.ac.uk/news/coffee-linked-to-slower-biological-ageing-among-those-with-severe-mental-illness-up-to-a-limit">https://www.kcl.ac.uk/news/coffee-linked-to-slower-biological-ageing-among-those-with-severe-mental-illness-up-to-a-limit</a>, See on <a href="https://news.ycombinator.com/item?id=46176766">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>New research from King’s College London finds that coffee consumption within the NHS recommended limit is linked to longer telomere lengths – a marker of biological ageing – among people with bipolar disorder and schizophrenia. The effect is comparable to roughly five years younger biological age.</p><div><div><p>Telomeres are structures that protect DNA. As people get older, their telomeres shorten as part of the natural human ageing process. This process has been shown to be accelerated among people with severe mental illness, such as bipolar disorder and schizophrenia, who have an average life expectancy 15 years shorter than the general population.</p>
<p>Previous research shows that coffee possesses health benefits. It may reduce oxidative stress in the general population, helping slow biological ageing processes like telomere shortening. The new study, published in <a href="https://mentalhealth.bmj.com/content/28/1/e301700" target="_blank" rel="noopener">BMJ Mental Health</a>, explores whether coffee consumption could slow this ageing process among those with severe mental illness.</p>
<p>Researchers at the Institute of Psychiatry, Psychology &amp; Neuroscience measured the effects of coffee consumption on telomere length among 436 participants aged 18 to 65 with schizophrenia, bipolar disorder or major depressive disorder with psychosis.</p>
<p>They found that coffee consumption of up to four cups per day was linked to longer telomeres, comparable to a biological age five years younger than non-coffee drinkers.</p>
<p>The longest telomeres were seen among those who consumed three to four cups per day. Too much coffee reduced this positive effect, with participants who consumed more than four cups having shorter telomeres than those who consumed between three and four cups.</p></div><p><img src="https://www.kcl.ac.uk/newimages/ioppn/news-spotlights/v-mlakar-coffee-and-telomere-length-figure.x4fa2266f.jpeg?f=webp" alt="V Mlakar et all 2025 figure: As coffee consumption (X axis) increases up to 3-4 cups, telomere length (Y axis) increases."></p><figcaption>Figure from Vid Mlakar et al. 2025: As coffee consumption increases up to 3-4 cups, telomere length increases. At 5+ cups, telomere length begins to shorten again.</figcaption><p>These effects remained after accounting for variations in age, sex, ethnicity, medication and tobacco use.</p><div><blockquote><p>We know that coffee can help slow biological ageing in the general population, but little is known about its effect on people with severe mental illness – a population whose lifespan is already shortened, in part due to age-related diseases. Our study shows that up to four cups of coffee per day is linked to longer telomeres among people with bipolar disorder and schizophrenia. This is comparable to a biological age of five years younger than non-coffee drinkers.</p><cite>Vid Mlakar, PhD student at King’s College London and first author of the study</cite></blockquote></div><div><blockquote><p>Coffee is a beverage that many people consume daily. On one hand, we know that excessive coffee consumption can have negative effects on health, such as reducing sleep quality. However, our new study suggests that coffee consumption up to a certain point may have benefits for biological ageing. Many of the factors that are known to affect biological ageing, such as genetics and negative stressful life experiences, are beyond our control. Lifestyle factors like coffee consumption are something we can actively modify, making research like this particularly valuable.</p><cite>Dr Monica Aas, MRC Research Fellow at King’s College London and senior author of the study</cite></blockquote></div><div><p>Dr Aas added: "Studies such as this also support the idea that we should move away from viewing coffee as simply “good or bad”, and instead consider a more balanced view. Still, these results need to be confirmed in other independent studies and longitudinal research before we can determine if this is a causal effect."</p>
<p>Data were from the Norwegian TOP study, collected between 2007 and 2018. The researchers included participants who had available data on mental health diagnosis (assessed using the Structured Clinical Interview for DSM-IV), telomere length (measured by extracting DNA from blood samples) and self-reported coffee consumption.</p>
<p>The researchers note that the study did not have information on the type of coffee consumed (instant versus filter) or the caffeine concentration of each cup. The NHS advises limiting caffeine intake to 400 mg/day (approximately four cups of coffee).</p>
<p>The study was funded by the Research Council of Norway, the KG Jebsen Stiftelsen and an Medical Research Council Fellowship. The team has recently received funding from the British Medical Association’s Margaret Temple grant to investigate telomere shortening in a longitudinal cohort of patients with psychosis. This project will allow them to explore further how several lifestyle factors, as well as stress, influence the rate of telomere shortening over time.</p>
<p><a href="https://mentalhealth.bmj.com/content/28/1/e301700" target="_blank" rel="noopener">"Coffee intake is associated with telomere length in severe mental disorders"</a> (Vid Mlakar et al.) was published in BMJ Mental Health. DOI: 10.1136/bmjment-2025-301700&nbsp;</p>
<p>For more information, please contact <a href="mailto:ioppn-pr@kcl.ac.uk">Milly Remmington</a> (School of Mental Health &amp; Psychological Sciences Communications Manager).</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zebra-Llama – Towards efficient hybrid models (103 pts)]]></title>
            <link>https://arxiv.org/abs/2505.17272</link>
            <guid>46176289</guid>
            <pubDate>Sat, 06 Dec 2025 20:15:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2505.17272">https://arxiv.org/abs/2505.17272</a>, See on <a href="https://news.ycombinator.com/item?id=46176289">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2505.17272">View PDF</a>
    <a href="https://arxiv.org/html/2505.17272v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>With the growing demand for deploying large language models (LLMs) across diverse applications, improving their inference efficiency is crucial for sustainable and democratized access. However, retraining LLMs to meet new user-specific requirements is prohibitively expensive and environmentally unsustainable. In this work, we propose a practical and scalable alternative: composing efficient hybrid language models from existing pre-trained models. Our approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models by combining State Space Models (SSMs) and Multi-head Latent Attention (MLA) layers, using a refined initialization and post-training pipeline to efficiently transfer knowledge from pre-trained Transformers. Zebra-Llama achieves Transformer-level accuracy with near-SSM efficiency using only 7-11B training tokens (compared to trillions of tokens required for pre-training) and an 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down to 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants, respectively-while preserving 100%, 100%, and &gt;97% of average zero-shot performance on LM Harness tasks. Compared to models like MambaInLLaMA, X-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive or superior accuracy while using significantly fewer tokens, smaller teachers, and vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses Minitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens, over 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves 2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context length. We will release code and model checkpoints upon acceptance.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Mehdi Rezagholizadeh [<a href="https://arxiv.org/show-email/f03618b0/2505.17272" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 22 May 2025 20:39:57 UTC (12,646 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The general who refused to crush Tiananmen's protesters (128 pts)]]></title>
            <link>https://www.economist.com/china/2025/12/04/the-general-who-refused-to-crush-tiananmens-protesters</link>
            <guid>46176072</guid>
            <pubDate>Sat, 06 Dec 2025 19:47:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/china/2025/12/04/the-general-who-refused-to-crush-tiananmens-protesters">https://www.economist.com/china/2025/12/04/the-general-who-refused-to-crush-tiananmens-protesters</a>, See on <a href="https://news.ycombinator.com/item?id=46176072">Hacker News</a></p>
Couldn't get https://www.economist.com/china/2025/12/04/the-general-who-refused-to-crush-tiananmens-protesters: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[OMSCS Open Courseware (198 pts)]]></title>
            <link>https://sites.gatech.edu/omscsopencourseware/</link>
            <guid>46175826</guid>
            <pubDate>Sat, 06 Dec 2025 19:14:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sites.gatech.edu/omscsopencourseware/">https://sites.gatech.edu/omscsopencourseware/</a>, See on <a href="https://news.ycombinator.com/item?id=46175826">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
	<p><a href="#content">Skip to content</a></p><nav id="navbar-secondary" aria-label="secondary">
	<div>
				
			<p><a href="http://www.gatech.edu/">
					<img alt="Georgia Institute of Technology" src="https://sites.gatech.edu/omscsopencourseware/wp-content/themes/gatech-flex/img/gt-logo-oneline-white.svg" width="245px" height="42px">
				</a>
			</p>

			</div>
</nav>
	
		<!-- #wrapper-navbar end -->

	
		<header id="hero-main" aria-label="page title and basic information">
		

		
						<p><img width="850" height="478" src="https://sites.gatech.edu/omscsopencourseware/files/2024/08/CoC-Article-2018-04-13-Three-of-GT-Computing-Awards-Luncheon-1.png" alt="" decoding="async" fetchpriority="high" srcset="https://sites.gatech.edu/omscsopencourseware/files/2024/08/CoC-Article-2018-04-13-Three-of-GT-Computing-Awards-Luncheon-1.png 850w, https://sites.gatech.edu/omscsopencourseware/files/2024/08/CoC-Article-2018-04-13-Three-of-GT-Computing-Awards-Luncheon-1-300x169.png 300w, https://sites.gatech.edu/omscsopencourseware/files/2024/08/CoC-Article-2018-04-13-Three-of-GT-Computing-Awards-Luncheon-1-768x432.png 768w" sizes="(max-width: 850px) 100vw, 850px">						</p>
						
						
	</header>
	
<div id="page-wrapper">

			<main id="main">
									
<article class="page" id="post-7">
			<!-- .page-header -->

		
	<div>
		
		
<p>Georgia Tech’s Online Master of Science in Computer Science (OMSCS) program is proud to make the course content* for many of its courses publicly available through Ed Lessons. Select a course below to view the public content for that course.</p>



<p>Note that students enrolled in OMSCS should access their course content through Canvas, as the for-credit versions of these courses may include graded components or recent content updates not available through OMSCS Open Courseware.</p>



<p>*<em>Course content typically includes things such as lecture videos and exercises; it will not include things like homeworks, projects quizzes, exams, or other graded assignments.</em></p>

























































		
			</div><!-- .entry-content -->

	</article><!-- #post-## -->

												</main><!-- #main -->

			<!-- Do the right sidebar check -->
			
</div><!-- #page-wrapper -->


<!-- wrapper end -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Perl's decline was cultural (312 pts)]]></title>
            <link>https://www.beatworm.co.uk/blog/computers/perls-decline-was-cultural-not-technical</link>
            <guid>46175112</guid>
            <pubDate>Sat, 06 Dec 2025 17:42:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.beatworm.co.uk/blog/computers/perls-decline-was-cultural-not-technical">https://www.beatworm.co.uk/blog/computers/perls-decline-was-cultural-not-technical</a>, See on <a href="https://news.ycombinator.com/item?id=46175112">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

		      
			  
		      


		      <div id="content">
			    <h3>According to the Discourse, somebody killed perl</h3><p>There's been a flurry of discussion <a href="https://news.ycombinator.com/item?id=45977900">on Hacker News</a> and <a href="https://lobste.rs/s/0m6yln/what_killed_perl">other tech forums</a> about what killed Perl. I wrote a lot of Perl in the mid 90s and subsequently worked on some of the most trafficked sites on the web in mod_perl in the early 2000s, so I have some thoughts. My take: it was mostly baked into the culture. Perl grew amongst a reactionary community with conservative values, which prevented it from evolving into a mature general purpose language ecosystem. Everything else filled the gap. </p><h3>I remember Perl</h3><p>Something to keep in mind, is that although this is my personal take, and therefore entirely an opinion piece, I <em>was</em> there at the time. I stopped doing Perl properly when I left Amazon, I think this would have been around 2005. It's based on the first hand impressions of somebody who was very deeply involved in Perl in its heyday, and moved on. I have a lot of experience, from both inside and outside the tent. </p><h3>Perl's roots are sysadmin</h3><p>What culture? Perl always had a significant amount of what you might call "BOFH" culture, which came from its old UNIX sysadmin roots. All of those passive aggressive idioms and in jokes like <em>"RTFM"</em>, <em>"lusers"</em>, <em>"wizards"</em>, <em>"asking for help the wrong way"</em> etc.  None of this is literally serious, but it does encode and inform social norms that are essentially tribal and introverted. There implicitly is a privileged population, with a cost of entry to join. Dues must be paid. Cultural conservatism as a first principle. </p><p>This stems from the old locked-down data centre command culture. When computer resource was expensive, centralised, fragile, and manually operated, it was rigidly maintained by gatekeepers, defending against inappropriate use. I started my career as an apprentice programmer at the very end of this era, (late 80s) pre-web, and before microcomputers had made much inroads, and this really was the prevailing view from inside the fort. (This is a drawback about fort-building. Once you live in a fort, it's slightly too easy to develop a siege mentality). Computers are special, users are inconvenient, disruption is the main enemy. </p><p>An unfortunate feedback loop in this kind of "perilous" environment is that it easily turns prideful. It's difficult to thrive here, if you survive and do well you are skilled; you've performed feats; you <em>should</em> mark your rites of passage. This can become a dangerous culture trap. If you're not careful about it, you may start to think of the hazards and difficulties, the "foot guns", as <em>necessary</em> features - they teach you those essential survival skills that mark you out. More unkindly, they keep the stupid folk out, and help preserve the high status of those who survived long enough to be assimilated. Uh-oh, now you've invented class politics. </p><p>The problem with this thinking is that it's self-reinforcing. Working hard to master system complexities was genuinely rewarding - you really were doing difficult things and doing them well. This is actually the same mechanism behind what eventually became known as 'meritocracy'<sup id="ref1"><a href="#fn1">1</a></sup>, but the core point is simpler - if difficulty itself becomes a badge of honour, you've created a trap: anything that makes the system more approachable starts to feel like it's cheapening what you achieved. You become invested in preserving the barriers you overcame. </p><p>(This is the same mentality that built leetcode interview pipelines BTW, but let's leave that sidebar alone for now) </p><p>So the UNIX operator culture tended to operate as a tribal meritocracy (as opposed to the UNIX <em>implementer</em> culture, which fell out of a different set of cultural norms, quite an interesting side bar itself<sup id="ref2"><a href="#fn2">2</a></sup>), a cultural priesthood, somewhat self-regarding, rewarding of cleverness and knowledge hoarding, prone to feats of bravado, full of lore, with a defensive mentality of keeping the flame aloft, keeping the plebs happy and fed, and warding off the barbarians. As we entered the 90s it was already gently in decline, because centralised computing was giving way to the rise of the microcomputer, but the sudden explosive growth of the WWW pulled internet / Unix culture suddenly back into the mainstream with an enormous and public opportunity vacuum. Everyone suddenly has an urgent need to write programs that push text off UNIX file-systems (and databases) and into web pages, and Perl is uniquely positioned to have a strong first-mover advantage in this suddenly vital, novel ecosystem. But it's culture and values are very much pulled across from this previous era. </p><p>(Springing out of this, Perl had an, at best grudging, tolerance for 'difficult genius' types, alongside this baseline culture. Unfortunately, this kind of toxic personality tends to thrive in the type of culture I've described, and they do set to help the tone. I'm not here to call out people specifically, because I'm trying to make a point rather than feed a culture war, or dig up gossip, but there were several significant examples, you can probably find lore if you like. I think the kindest way I can describe the compounding effect of this is that there was a strong cultural norm along the lines of "It's OK to be rude, as long as it's for a good cause".) </p><h3>A fort within a fort</h3><p>I remember this tension as always being tangibly there. Perl IRC and mailing lists were quite cliquey and full of venerated experts and in-jokes, rough on naivety, keen on robust, verbose debate, and a little suspicious of newcomers. And very cult-like. The "<a href="https://perl.fandom.com/wiki/TIMTOWTDI">TIMTOWTDI</a>" rule, although ostensibly liberal, literally means 'there is more than one way to do it <em>in Perl</em>' - and you can perhaps infer from that that there's little to no reason to do it using anything else. Elevating extreme flexibility like this is paradoxically also an engine of conservatism. If Perl can already do anything, flexibly, in multiple ways, then the language itself doesn't need to change - 'we already have one of those here, we don't need new things'. This attitude determined how Perl intended to handle evolution: the core language would remain stable (a fort inside a fort, only accessible to high level wizards), while innovation was pushed outward to CPAN. You could add features outside of core by writing and consuming third party libraries, you could bend language behaviour with pragmas without modifying Perl itself. The very best CPAN modules could theoretically be promoted into core, allowing the language to evolve conservatively from proven, widely-used features. </p><p>On paper, this sounds reasonable. In practice, I think it encoded a fundamental conflict of interest into the community early on, and set the stage for many of the later growth problems.  I'm not going to pretend that Perl <em>invented</em> dependency hell, but I think it turned out to be another one of those profound misfeatures that their cultural philosophy lead them to mistake for virtue, and embrace. </p><p>An interesting thing I think has been missed discussing the context of the original blog piece, about whether Perl 6 significantly impacted Perl growth, is the fact that Perl 6 itself manifested out of ongoing arguments. Perl 6 is a schism. Here's a oft-cited note from Larry Wall himself about the incident that sparked Perl 6, at <strike> YAPC</strike>  <a href="https://whitecamel.org/p/jon_orwant.html">OSCON 2000</a> </p><blockquote><p>We spent the first hour gabbing about all sorts of political and organizational issues of a fairly boring and mundane nature. Partway through, Jon Orwant comes in, and stands there for a few minutes listening, and then he very calmly walks over to the coffee service table in the corner, and there were about 20 of us in the room, and he picks up a coffee mug and throws it against the other wall and he keeps throwing coffee mugs against the other wall, and he says "we are f-ed unless we can come up with something that will excite the community, because everyone's getting bored and going off and doing other things". </p></blockquote><p>(Pause a second and ask yourself about the sort of social culture that both allows this kind of behaviour at public events, and then chooses to embrace it as a key piece of cultural lore) </p><h3>The impact of Perl 6</h3><p>Perl 6 was really a <em>schism</em>. Perl was already under a great amount of strain trying to accommodate the modernising influx of post dot-com mainstream web application building, alongside the entrenched conservatism of the core maintainers, and the maintenance burden of a few years exponential growth of third-party libraries, starting to build a fractal mess of slightly differentiating, incompatible approaches of those multiple ways to do things that were effectively now table-stakes language features, as the deployment landscape started to tiptoe towards a more modern, ubiquitous WWW<sup id="ref3"><a href="#fn3">3</a></sup>. </p><p>So, while I agree that it's wrong to generalise that 'Perl 6 killed Perl', I would say that Perl 6 was a symptom of the irreconcilable internal forces that killed Perl. Although, I also intend to go on to point out that Perl isn't dead, nothing has actually <em>killed</em> Perl. Killed Perl is a very stupid way to frame the discussion, but here we are. </p><p>So... Perl 6 is created as a valve to offset that pressure, and it kind of works. Up to a point. Unfortunately I think the side effect really is that the two branches of the culture, in the process of forking, double down on their encoded norms. Perl 5.x beds down as the practical, already solved way to do all the same things, with little need to change. Any requirements for more <em>modern</em> application patterns that are emerging in the broader web development environment, like idk, Unicode, REST clients, strict data structures, asynchronous I/O, whatever? That can <em>either</em> wait for Perl6 or you can pull things together using the CPAN if you want to move right now. Perl 6 leans the other way - they don't need to ship immediately, we have Perl 5 already here for doing things, Perl 6 is going to innovate on <em>everything</em>, and spend it's time getting there, designing up-front.<sup id="ref4"><a href="#fn4">4</a></sup> They spend at least two years writing high level requirement specs. They even spin out a side-project trying to build a universal virtual machine to run all dynamic programming languages that never delivers<sup id="ref5"><a href="#fn5">5</a></sup> </p><p>This is the landscape where Perl's central dominance of 'back end' web programming continues to slip. Unfortunately, alongside the now principled bias toward cultural conservatism, Perl 5 has an explicit excuse for it. The future is over there, and exciting, and meanwhile we're working usefully, and getting paid, and getting stuff done. Kind of OK from inside the fort. Some day we'll move to the newer fort, but right now <em>this is fine</em>. Not very attractive to newcomers though, really. And this is also sort of OK, because Perl doesn't really want those sort of newcomers, does it? The kind that turns up on IRC or forums and asks basic questions about Perl 6 and sadly often gets treated with open contempt. </p><h3>Meanwhile, over there</h3><p>Ruby has sprouted "Ruby on Rails", and it's taken the dynamic web building world by storm. Rails is a second generation web framework, that's proudly an 'opinionated web framework'. Given that the web application architecture is starting to stabilise into a kind of three-tier system , with a client as a web browser, a middle tier as a monolithic application server, and a persistence layer as a relational database , and a split server architecture serving static and dynamic content from different routes, here is just one way to do that, with hugely developer friendly tooling turning this into a cookie-cutter solution for the 80% core, and a plugin and client-side decoration approach that allows for the necessary per-site customisation. </p><p>Ruby is interesting as well. Ruby is kind of a Perl6 really. More accurately it's <a href="https://ruby-doc.org/docs/ruby-doc-bundle/FAQ/FAQ.html">a parallel universe Perl5</a> Ruby comes from Japan, and has developed as an attempt to build something similar to Perl, but it's developed much later, by programming language enthusiasts, and for the first ten years or so, it's mostly only used in Japan. To my line of thinking this is probably important. Ruby does not spring from decades of sysadmin or sysop culture. Ruby is a language for programmers, and is at this point an sensible candidate for building something like Rails with - a relatively blank canvas for dynamic programming, with many of the same qualities as Perl, with less legacy cruft, and more modern niceties, like an integrated object system, exceptions, straightforward data structures. Ruby also has adopted 'friendliness' as a core value, and the culture over there adopts a principled approach to aggressively welcoming newcomers, promoting easiness, and programmer happiness and convenience as strong first class principles. </p><p>Rails is a <em>huge</em> hit. At this point, which is around about the time I stopped significantly using Perl (2004-2005) (because I quit my job, not out of any core animosity toward it, in fact, in my day, I was really quite a Perl <em>fan</em>), Rails is the most appealing place to start as a new web programmer. Adoption rate is high, community is great, velocity of development is well-paced, and there's a lovely , well-lit, onboarding pipeline for how to start. You don't even really need to know ruby. It has a one-shot install tool, and generates working websites from templates, almost out of the box. It's an obvious starting point. </p><p>Perl being Perl, develops several analogue frameworks to Rails, all of them interdependently compatible and incompatible with each other and each other's dependencies, all of them designed to be as customisable and as user configurable as they possibly can be<sup id="ref6"><a href="#fn6">6</a></sup> </p><h3>PHP</h3><p>There are also the other obvious contenders. PHP has been there all along, and it's almost coming up from entirely the opposite cultural background of Perl. PHP is <em>a users language</em>. It's built to be deployed by copying script files to your home directory, with minimal server side impact or privileges. It's barely designed at all, but it encounters explosive growth all the way through the first (and through into the second) web era, almost entirely because it makes the barrier to onboarding so low as to be non-existent. PHP gets a couple of extra free shots in the arm </p><ol><li>Because it's architecture is so amenable to shared-server hosting, it is adopted as the primary implementation language of the blogging boom. An entire generation of web developers is born of installing and customising WordPress and text-pattern et. al by installing it directly into your home directory on a rented CPanel host account. It's the go-to answer for 'I'm not a programmer really but how do I get a personal web site'<sup id="ref7"><a href="#fn7">7</a></sup> This zero gate-keeping approach keeps the PHP stack firmly on the table of 'basic' web programmers all through the history of the web up to the current day. </li><li>Because of these initially lightweight deployment targets, PHP scales like little else, mostly because it's execution model leans strongly towards idempotent execution, with each web request tearing up and tearing down the whole environment. In a sense, this is slower than keeping hot state around, but it does lend itself extremely well to shared-nothing horizontal scaling, which as the web user base increases gigantically throughout the 2000s era, is the simplest route to scaling out. Facebook famously, is built in PHP at this point in time. </li></ol><h3>Python</h3><p>There is of course one other big horse in the race in this era, and it's a particularly interesting one in many ways, certainly when contrasted with Perl. This is of course, Python. Python is a close contemporary of Perl's but once again, it's roots are somewhere very different. Python doesn't come from UNIX culture either. Python comes from academia, and programming language culture. It's kind of a forgotten footnote, but Python was originally built for the <a href="https://en.wikipedia.org/wiki/Amoeba_(operating_system">Amoeba operating system</a>, and it's intention was to be a straightforward programming language for scripting this<sup id="ref8"><a href="#fn8">8</a></sup>. The idea was to build a language that could be the 'second programming language' for programmers. Given that this is the 1980s, early 1990s, the programmers would be expected to be mostly using C / C++ ,perhaps Pascal. Python was intended to allow faster development for lighter weight programs or scripting tasks. I suppose the idea was to take something that you might want to build in a shell script, but provide enough high level structured support that you could cleanly build the kind of things that quickly become a problem in shell scripts. So, it emphasises data structures, and scoped variables, and modules, and prioritises making it possible to extend the language with modules. Typical things that experienced programmers would want to use. The language was also designed to be portable between the different platforms programmers would use, running on the desktops of the day, but also on the server. As a consequence, it had a broad standard library of common <em>portable</em> abstractions around standard system features - file-systems, concurrency, time, FFI. For quite a long time, one of python's standard mottoes was 'batteries included'. </p><p>Python never set the world on fire at any particular moment, but it remained committed to a clear evolutionary incremental development, and clean engineering principles. Again, I think the key element here is cultural tone. Python is kind of boring, not trying to be anyone's best language, or even a universal language. Python was always a little fussy, maybe snobby, slightly abstracted away from the real world. It's almost as old as Perl and it just kept incrementally evolving, picking up users, picking up features, slowly broadening the standard library. The first time I saw Python pick up an undeniable mainstream advantage would also have been around the early 2000s, when Google publicly adopted it as one of their house standard languages. Never radical, just calmly evolving in it's environs. </p><h3>Nature abhors a vacuum</h3><p>When I sketch out this landscape, I remain firmly convinced that most of Perl's impedance to continued growth were cultural. Perl's huge moment of relevance in the 90s was because it cross-pollinated two diverging user cultures. Traditional UNIX / database / data-centre maintenance and admin users, and enthusiastic early web builders and scalers. It had a cultural shock phase from extremely rapid growth, the centre couldn't hold, and things slowly fell apart. </p><p>Circling back though, it's time to address the real elephant in the room. Perl manifestly did not die. It's here right now. It's installed I think by default, on almost every single computer I own and operate, without me doing a single thing to make that happen. It's still used every day by millions of people on millions of systems (even if that isn't deliberate). It's still used by many people <em>entirely deliberately</em> for building software, whether that's because they know it and like it and it works, or because they're interfacing with or working on legacy Perl systems (of which there are still many), or maybe they're using it still in it's original intentional role - A capable POSIX-native scripting language, with much better performance and a broader feature-set than any shell or awk. I still occasionally break it out myself, for small scripts I would like to use more than once, or as parts of CLI pipelines. </p><p>What I don't do any more is reach for Perl <em>first</em> to make anything new. In my case, it's just because I typically am spoilt for options that are a better fit for most tasks, depending on whatever it is I'm trying to achieve. By the time I came to Perl, (1998-ish), I was already on my third career phase, I had a strong UNIX background, and had already built real things in lisp, java, pascal, visual basic and C++. My attitude to languages was already informed by picking a tool to fit the task at hand. Boy did I love Perl for a few years. The product/market-fit for those early web days was just beautiful. The culture did have too much of the negative tropes I've been pointing at, but that wasn't really a problem personally for me, I'd grown up amongst the BOFHs inside the data centres already, it wasn't too hard for me to assimilate, nor pick up the core principles. I did occasionally bounce off a couple of abrasive characters in the community, but mostly this just kept me loosely coupled, I enjoyed how the language solved the problems I needed solving quickly, I enjoyed the flexibility, and I also enjoyed the way that it made me feel smart, and en-route to my wizard's robes and hat, when i used it to solve harder problems in creative ways, or designed ways around bugs and gremlins. For a good 3-4 years I would have immediately picked it as my favourite language. </p><p>So as I say, I didn't fall out of it with any sense of pique, I just naturally moved to different domains, and picked up tools that best fit. After Amazon, I spent t a lot of time concentrating on OS X and audio programming, and that involved a lot of objective C, C++. The scripting tools in that domain were often in ruby, sometimes python. For personal hacking, I picked up lisp again<sup id="ref9"><a href="#fn9">9</a></sup> (which I'd always enjoyed in school). I dipped in and out of Perl here and there for occasional contract work, but I tended to gravitate more towards larger database stuff, where I typically found C, java and python. The next time I was building web things, it was all Rails and ruby, and then moving towards the web services / REST / cloud era, the natural fits were go, and of course node and JavaScript or Typescript. I've always been a polyglot, and I've always been pretty comfortable moving between programming languages. The truth of the matter is, that the majority of programming work is broadly similar, and the specific implementation details of the language you use don't matter all that much, if it's a good fit for the circumstances. </p><p>I can't imagine Perl disappearing entirely in my lifetime. I can remember entire programming environments and languages that are much, much deader than I can ever see Perl becoming. </p><ul><li>Pascal used to be <em>huge</em> for teaching and also for desktop development in the 8/16 bit era</li><li>Objective C - only really useful inside the Apple ecosystem, and they're hell bent on phasing it out.</li><li>Before I got into the Internet, I used to build application software for 16 bit Windows (3.11) which was a vast market, in a mixture of database 4GLs (like PowerBuilder, Gupta/Centura SQLWindows) and Win16 C APIs. This entire universe basically no longer exists, and is fully obsolete. There must be many similar cases.</li><li>I mean who the hell realistically uses common lisp any more outside of legacy or enthusiast markets? Less people than Perl I'm sure.</li></ul><p>Perl also got to be if not first, then certainly early to dominate a new market paradigm. Plenty of things never manage that. It's hard to see Perl as anything other than an enormous success on these terms. Perl innovated and influenced languages that came after in some truly significant ways. </p><ul><li>Tightly embedding regular expressions and extending regular expressions (the most commonly used regular expression dialect in other tools is Perl)</li><li>CPAN, for package/library distribution via the internet, with dependency resolution - and including important concepts like supply chain verification with strong package signatures</li><li>A huge emphasis on testing, automated test harnesses, and CI. Perl test format (TAP) is also widely found in other CI/harness systems</li><li>Blending the gap between shell / scripting / and system programming in a single tool. I suppose this is debatable, but the way Perl basically integrated all the fundamental POSIX/libc as native built-ins with broadly the same semantics, but with managed memory and shell conventions was really revolutionary. Before this, most languages I had ever seen broadly tended to sit in one box, afterwards, most languages tended to span across several.</li><li>Amazing integrated documentation, online, in-tool and also man pages. POD is maybe the most successful ever implementation of literate programming ideas (although most of the real docs don't intertwingle the documentation very much iirc)</li></ul><p>Just these points, and I'm sure there are many others that could be made, are enough of a legacy to be proud of. </p><p>Counterfactuals are stupid (but also fun). If I squint, I can imagine that a Perl with a less reactionary culture, and a healthier acceptance of other ideas and environmental change might have been able to evolve alongside the other tools in the web paradigm shift, and still occupy a more central position in today's development landscape. That's not the Perl we have though, and that didn't happen. And I'm very confident that without the Perl we did have, the whole of modern software practice would be differently shaped. I do think Perl now lives in a legacy role, with a declining influence, but that's really nothing to feel shame or regret for. Nobody is going to forcibly take Perl away as long as POSIX exists, and so far as I can see, that means forever. In 2025 too, I can see the invisible hand creeping up on some of these other systems I've mentioned. Rust is slowly absorbing C and C++. Ruby (and of course Rails) is clearly in decline, in a way that probably consigns it to become a similar legacy state. From a certain angle, it looks a lot like Typescript is slowly supplanting Python. I won't be entirely surprised if that happens, although at my age I kind of doubt I'll live to see the day. </p><h3>Footnotes</h3><p><a id="fn1" href="#ref1">1</a> : Meritocracy is a fun word. It was originally coined as a pejorative term to describe a dystopian mechanism by which modern i.e. Western / British society entrenches and justifies an unfair and unequal distribution of privilege </p><p><a id="fn2" href="#ref2">2</a> : The UNIX <em>implementer</em> culture, is scientific/academic and fell out of Bell Labs. I guess you could extend this school of thought as a cultural sweep towards building abstracted cloud operations, toward plan 9/ Inferno / go </p><p><a id="fn3" href="#ref3">3</a> : Web 2.0 was first <a href="https://www.webdesignmuseum.org/web-design-history/web-2-0-1999">defined in 1999</a> by <a href="http://darcyd.com/fragmented_future.pdf">Darcy DiNucci in a print article</a> , the term didn't become mainstream until it was picked up and promoted by Tim O'Reilly (then owner/operator of perl.com, trivia fans), an astute inside observer of the forces driving web development </p><p><a id="fn4" href="#ref4">4</a>: Another unfortunate bit of luck here. Right at the point of time that <em>'agile'</em> starts getting some traction as a more natural way to embrace software development - i.e. iterating in small increments against a changing environment and requirements, Perl 6 decides to do perhaps the most waterfall open source development process ever attempted. . It is fifteen years before Perl 6 ships something resembling a usable programming language.<br>
</p><p><a id="fn5" href="#ref5">5</a> : <a href="http://www.parrot.org/">The Parrot VM</a>, a lovely quixotic idea, which sadly fizzled out, after even Perl 6 stopped trying to target it. Interestingly enough, both python and ruby both made relatively high profile ports to the JVM that were useful enough to be used for production deploys in certain niches. </p><p><a id="fn6" href="#ref6">6</a> : A side effect of this degree of abstraction, is that as well as being very hard to get started, it's easy to fall foul of performance overhead. </p><p><a id="fn7" href="#ref7">7</a> : This ubituitious ecosystem of small footprint wordpress custom installs gives birth to the web agency model of commercial website building / small ecommerce sites, which thrives and is suprisingly healthy today. Recent, and slighly optimistic surveys have pitched WordPress as powering over 40% of all websites today. Now this is certainly inflated, but even if the realistic number is half of that, that's still pretty damn healthy. </p><p><a id="fn8" href="#ref8">8</a> : It's often repeated that Python was designed as a teaching language, but as far as I know, that's not actually the case. The designer of Python, Guido Van Rossum <a href="https://www.artima.com/articles/the-making-of-python">was previously working on a project</a> that <em>was</em> a intended as training language, called ABC, and many of ABC's syntax and structural features influenced or made their way into Python. </p><p><a id="fn9" href="#ref9">9</a> : Common lisp is a better answer to an infinitely flexible 'everything' chainsaw language than perl, IMHO </p>
			    <section>
			      <span>posted by <a rel="me" href="https://www.beatworm.co.uk/">cms</a></span><span> on <time datetime="2025-11-20">2025-11-20</time></span>
			    </section>
			  <p>tagged as</p>
		      </div>

              </article></div>]]></description>
        </item>
    </channel>
</rss>