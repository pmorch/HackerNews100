<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 27 Aug 2024 18:30:11 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[80% of AI Projects Crash and Burn, Billions Wasted Says Rand Report (126 pts)]]></title>
            <link>https://salesforcedevops.net/index.php/2024/08/19/ai-apocalypse/</link>
            <guid>41368935</guid>
            <pubDate>Tue, 27 Aug 2024 15:55:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://salesforcedevops.net/index.php/2024/08/19/ai-apocalypse/">https://salesforcedevops.net/index.php/2024/08/19/ai-apocalypse/</a>, See on <a href="https://news.ycombinator.com/item?id=41368935">Hacker News</a></p>
Couldn't get https://salesforcedevops.net/index.php/2024/08/19/ai-apocalypse/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Sainsbury Wing contractors find 1990 letter from donor (161 pts)]]></title>
            <link>https://www.theartnewspaper.com/2024/08/27/sainsbury-wing-contractors-find-1990-letter-from-donor-anticipating-their-demolition-of-false-columns</link>
            <guid>41368866</guid>
            <pubDate>Tue, 27 Aug 2024 15:49:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theartnewspaper.com/2024/08/27/sainsbury-wing-contractors-find-1990-letter-from-donor-anticipating-their-demolition-of-false-columns">https://www.theartnewspaper.com/2024/08/27/sainsbury-wing-contractors-find-1990-letter-from-donor-anticipating-their-demolition-of-false-columns</a>, See on <a href="https://news.ycombinator.com/item?id=41368866">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody"><p itemprop="text">A “time capsule” has been discovered at London’s National Gallery, buried deep in a column in the foyer of the Sainsbury Wing. It is a letter recording that one of the wing’s funders, John Sainsbury (Lord Sainsbury of Preston Candover), believed the architects had committed a serious “mistake”. The 1990 letter, typed on Sainsbury’s supermarket notepaper, has recently been deposited in the gallery’s archive as an historic document.</p><p itemprop="text">John Sainsbury is critical in the letter of the American post-Modernist architect Robert Venturi and his professional partner and wife Denise Scott Brown for inserting two large false columns in the gallery’s foyer that served no structural purpose. Other than the false columns, John Sainsbury was happy with the Venturi and Scott Brown design.</p><p itemprop="text">While building work was under way, Sainsbury gained access to the site and dropped his letter into a concrete column that was under construction. The letter, protected in a plastic folder, was discovered last year, when the foyer was being reconfigured.</p><p itemprop="text">The Sainsbury letter of 26 July 1990 was addressed “To those who find this note”—who turned out to be the 2023 demolition workers.</p><p itemprop="text">The note, typed in capital letters, continues:</p><p itemprop="text">IF YOU HAVE FOUND THIS NOTE YOU MUST BE ENGAGED IN DEMOLISHING ONE OF THE FALSE COLUMNS THAT HAVE BEEN PLACED IN THE FOYER OF THE SAINSBURY WING OF THE NATIONAL GALLERY. I BELIEVE THAT THE FALSE COLUMNS ARE A MISTAKE OF THE ARCHITECT AND THAT WE WOULD LIVE TO REGRET OUR ACCEPTING THIS DETAIL OF HIS DESIGN.</p><p itemprop="text">LET IT BE KNOWN THAT ONE OF THE DONORS OF THIS BUILDING IS ABSOLUTELY DELIGHTED THAT YOUR GENERATION HAS DECIDED TO DISPENSE WITH THE UNNECESSARY COLUMNS.</p><figure><img alt="" srcset="https://cdn.sanity.io/images/cxgd3urn/production/e483264430dfee3ca8ed67838ae3808fb9b4bbae-1400x1989.jpg?rect=0,0,1399,1989&amp;w=750&amp;h=1066&amp;fit=crop&amp;auto=format 1x, https://cdn.sanity.io/images/cxgd3urn/production/e483264430dfee3ca8ed67838ae3808fb9b4bbae-1400x1989.jpg?w=1920&amp;h=2728&amp;fit=crop&amp;auto=format 2x" src="https://cdn.sanity.io/images/cxgd3urn/production/e483264430dfee3ca8ed67838ae3808fb9b4bbae-1400x1989.jpg?w=1920&amp;h=2728&amp;fit=crop&amp;auto=format" width="644" height="914.9399999999999" decoding="async" data-nimg="1" loading="lazy"><figcaption><p>John Sainsbury's letter of 26 July 1990 was discovered last year, protected in a plastic folder, during building works to reconfigure the foyer of the Sainsbury Wing of London's National Gallery  <span>Courtesy of the National Gallery and by permission of the Sainsbury family</span></p></figcaption></figure><p itemprop="text">John and his wife Anya presumably never imagined that the demolition of the Sainsbury Wing foyer might take place during their lifetimes. John, one of the most generous UK donors to the arts, died in 2022, aged 94. His widow Anya, a former ballerina, was present when her husband’s note was removed. “I was so happy for John’s letter to be rediscovered after all these years,” she says, “and I feel he would be relieved and delighted for the gallery’s new plans and the extra space they are creating.”</p><p itemprop="text">The Sainsbury Wing was opened by Queen Elizabeth II in 1991, just under a year after John wrote his time capsule letter. It was entirely funded by John and his two Sainsbury brothers: Simon (who died in 2006) and Timothy (a former Conservative minister, now aged 92). It was their great-grandfather who established the London grocery shop which has now become the UK’s second largest supermarket chain, after Tesco.</p><div><blockquote>I was so happy for John’s letter to be rediscovered after all these years, and I feel he would be relieved and delighted for the gallery’s new plans and the extra space they are creating <p>Anya Sainsbury</p></blockquote></div><p itemprop="text">Neil MacGregor, the director of the National Gallery when the Sainsbury Wing was planned and built, tells <em>The Art Newspaper</em>: “Venturi wanted the foyer to have the feel of a mighty crypt, leading upstairs to the galleries, so it was a subsidiary space—the beginning of a journey, not a destination. John Sainsbury argued that sightlines should be as unencumbered as possible, thinking the extra columns would conceal the entrance to the lecture theatre and temporary exhibition galleries, confusing the visitor.”</p><figure><img alt="" srcset="https://cdn.sanity.io/images/cxgd3urn/production/0a733d1463e90e5134c5bfa604bb0d91ffbfdf6c-1468x1101.jpg?rect=1,0,1467,1101&amp;w=750&amp;h=563&amp;fit=crop&amp;auto=format 1x, https://cdn.sanity.io/images/cxgd3urn/production/0a733d1463e90e5134c5bfa604bb0d91ffbfdf6c-1468x1101.jpg?w=1920&amp;h=1440&amp;fit=crop&amp;auto=format 2x" src="https://cdn.sanity.io/images/cxgd3urn/production/0a733d1463e90e5134c5bfa604bb0d91ffbfdf6c-1468x1101.jpg?w=1920&amp;h=1440&amp;fit=crop&amp;auto=format" width="644" height="483" decoding="async" data-nimg="1" loading="lazy"><figcaption><p>Anya Sainsbury (centre) holds her late husband John’s 1990 letter after being shown it on site in the Sainsbury Wing by the National Gallery’s director, Gabriele Finaldi (left) and chairman, John Booth, last year<br><span>Photo: Sarah Butler-Sloss</span></p></figcaption></figure><p itemprop="text">MacGregor ultimately concluded in the late 1980s that the false columns were acceptable: “Although there were drawbacks, Venturi had a coherent idea of the organic link between entrance hall, staircase and main galleries. I felt that, on balance, we should let the architect be the architect.”</p><p itemprop="text">The size of the Sainsbury family’s original donation has never been officially revealed, but <em>The Art Newspaper</em> understands that it was around £40m (equivalent to £90m today). It might be considered exceedingly gracious of John Sainsbury to have donated a third of this sum—and not to have insisted that his wishes relating to the false columns should be respected.</p><h4>Sainsbury family is largest contributor to £85m  upgrade of Sainsbury Wing</h4><p itemprop="text">Last year the National Gallery embarked on a £85m project to upgrade the Sainsbury Wing and develop new facilities in the adjacent part of its main building. The main improvement in the Sainsbury Wing will be a more open and welcoming foyer, to cope with double the number of visitors that had been envisaged in the 1980s.</p><p itemprop="text">This scheme, designed by the architect Annabelle Selldorf, included the demolition of the two non-structural columns. These were located on the ground floor near the former cloakroom, halfway between the street entrance and the stairs and lift leading down to the basement. Three adjacent structural columns have needed to be retained.</p><p itemprop="text">Demolition of the two columns was criticised by the Twentieth Century Society, which argued that they “contribute to the sense of weight and the lobby’s function as an anticipatory space”. The National Gallery recently took a different view, pointing out that wayfinding is hindered by columns which “restrict views to the lifts and obscures the entrance to the [lecture] theatre and temporary exhibition spaces”. Westminster City Council gave planning permission for the National Gallery’s plan—and the columns in the listed building were demolished last year.</p><p itemprop="text">Venturi died in 2018. His partner Scott Brown has vociferously opposed the redesign of the foyer.</p><p itemprop="text">The Sainsbury family is the largest financial contributor to the present project. Although the gallery has not released the figures, the Linbury Trust (set up by John and Anya) and the Headley Trust (set up by Timothy and his wife Susan) have each committed £5m. Their £10m joint contribution represents one of the largest donations to a UK museum in recent years.</p><p itemprop="text">Building work has taken longer than originally anticipated, but the newly refurbished Sainsbury Wing is now due to reopen in May next year. Visitors will then be able to make their own judgement on the architectural controversy.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Eazel, ex-Apple led Linux startup (115 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Eazel</link>
            <guid>41368363</guid>
            <pubDate>Tue, 27 Aug 2024 15:04:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Eazel">https://en.wikipedia.org/wiki/Eazel</a>, See on <a href="https://news.ycombinator.com/item?id=41368363">Hacker News</a></p>
Couldn't get https://en.wikipedia.org/wiki/Eazel: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[New 0-Day Attacks Linked to China's 'Volt Typhoon' (144 pts)]]></title>
            <link>https://krebsonsecurity.com/2024/08/new-0-day-attacks-linked-to-chinas-volt-typhoon/</link>
            <guid>41367964</guid>
            <pubDate>Tue, 27 Aug 2024 14:31:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://krebsonsecurity.com/2024/08/new-0-day-attacks-linked-to-chinas-volt-typhoon/">https://krebsonsecurity.com/2024/08/new-0-day-attacks-linked-to-chinas-volt-typhoon/</a>, See on <a href="https://news.ycombinator.com/item?id=41367964">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
												<p>Malicious hackers are exploiting a zero-day vulnerability in <strong>Versa Director</strong>, a software product used by many Internet and IT service providers. Researchers believe the activity is linked to <strong>Volt Typhoon</strong>, a Chinese cyber espionage group focused on infiltrating critical U.S. networks and laying the groundwork for the ability to disrupt communications between the United States and Asia during any future armed conflict with China.</p>
<div id="attachment_68507"><p><img aria-describedby="caption-attachment-68507" decoding="async" src="https://krebsonsecurity.com/wp-content/uploads/2024/08/ss-dissolvingsphere.png" alt="" width="840" height="508" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/08/ss-dissolvingsphere.png 840w, https://krebsonsecurity.com/wp-content/uploads/2024/08/ss-dissolvingsphere-768x464.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/08/ss-dissolvingsphere-782x473.png 782w" sizes="(max-width: 840px) 100vw, 840px"></p><p id="caption-attachment-68507">Image: Shutterstock.com</p></div>
<p>Versa Director systems are primarily used by Internet service providers (ISPs), as well as managed service providers (MSPs) that cater to the IT needs of many small to mid-sized businesses simultaneously. In <a href="https://versa-networks.com/blog/versa-security-bulletin-update-on-cve-2024-39717-versa-director-dangerous-file-type-upload-vulnerability/" target="_blank" rel="noopener">a security advisory</a> published Aug. 26, Versa urged customers to deploy a patch for the vulnerability (<a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2024-39717" target="_blank" rel="noopener">CVE-2024-39717</a>), which the company said is fixed in <em>Versa Director 22.1.4</em> or later.</p>
<p>Versa said the weakness allows attackers to upload a file of their choosing to vulnerable systems. The advisory placed much of the blame on Versa customers who “failed to implement system hardening and firewall guidelines…leaving a management port exposed on the internet that provided the threat actors with initial access.”</p>
<p>Versa’s advisory doesn’t say how it learned of the zero-day flaw, but its vulnerability listing at mitre.org acknowledges “there are reports of others based on backbone telemetry observations of a 3rd party provider, however these are unconfirmed to date.”</p>
<p>Those third-party reports came in late June 2024 from <strong>Michael Horka</strong>, senior lead information security engineer at <strong>Black Lotus Labs</strong>, the security research arm of <strong>Lumen Technologies</strong>, which operates one of the global Internet’s largest backbones.</p>
<p>In an interview with KrebsOnSecurity, Horka said Black Lotus Labs identified a web-based backdoor on Versa Director systems belonging to four U.S. victims and one non-U.S. victim in the ISP and MSP sectors, with the earliest known exploit activity occurring at a U.S. ISP on June 12, 2024.</p>
<p>“This makes Versa Director a lucrative target for advanced persistent threat (APT) actors who would want to view or control network infrastructure at scale, or pivot into additional (or downstream) networks of interest,” Horka <a href="https://blog.lumen.com/taking-the-crossroads-the-versa-director-zero-day-exploitation/" target="_blank" rel="noopener">wrote</a> in a blog post published today.</p>
<p>Black Lotus Labs said it assessed with “medium” confidence that Volt Typhoon was responsible for the compromises, noting the intrusions bear the hallmarks of the Chinese state-sponsored espionage group — including zero-day attacks targeting IT infrastructure providers, and Java-based backdoors that run in memory only.</p>
<p>In May 2023, the <strong>National Security Agency</strong> (NSA), the <strong>Federal Bureau of Investigation</strong> (FBI), and the <strong>Cybersecurity Infrastructure Security Agency</strong> (CISA) issued <a href="https://media.defense.gov/2023/May/24/2003229517/-1/-1/0/CSA_Living_off_the_Land.PDF" target="_blank" rel="noopener">a joint warning</a> (PDF) about Volt Typhoon, also known as “<strong>Bronze Silhouette</strong>” and “<strong>Insidious Taurus</strong>,” which described how the group uses small office/home office (SOHO) network devices to hide their activity.</p>
<p>In early December 2023, Black Lotus Labs <a href="https://blog.lumen.com/routers-roasting-on-an-open-firewall-the-kv-botnet-investigation/" target="_blank" rel="noopener">published its findings</a> on “<strong>KV-botnet</strong>,” thousands of compromised SOHO routers that were chained together to form a covert data transfer network supporting various Chinese state-sponsored hacking groups, including Volt Typhoon.</p>
<p>In January 2024, the <strong>U.S. Department of Justice</strong> disclosed the FBI had <a href="https://www.justice.gov/opa/pr/us-government-disrupts-botnet-peoples-republic-china-used-conceal-hacking-critical" target="_blank" rel="noopener">executed a court-authorized takedown</a> of the KV-botnet shortly before Black Lotus Labs released its December report.<span id="more-68493"></span></p>
<p>In February 2024, CISA again joined the FBI and NSA in <a href="https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-038a" target="_blank" rel="noopener">warning</a> Volt Typhoon had compromised the IT environments of multiple critical infrastructure organizations — primarily in communications, energy, transportation systems, and water and wastewater sectors — in the continental and non-continental United States and its territories, including Guam.</p>
<p>“Volt Typhoon’s choice of targets and pattern of behavior is not consistent with traditional cyber espionage or intelligence gathering operations, and the U.S. authoring agencies assess with high confidence that Volt Typhoon actors are pre-positioning themselves on IT networks to enable lateral movement to OT [operational technology] assets to disrupt functions,” that alert warned.</p>
<p>In a speech at Vanderbilt University in April, FBI Director <strong>Christopher Wray</strong> <a href="https://www.reuters.com/technology/cybersecurity/fbi-says-chinese-hackers-preparing-attack-us-infrastructure-2024-04-18/" target="_blank" rel="noopener">said</a> China is developing the “ability to physically wreak havoc on our critical infrastructure at a time of its choosing,” and that China’s plan is to “land blows against civilian infrastructure to try to induce panic.”</p>
<p><strong>Ryan English</strong>, an information security engineer at Lumen, said it’s disappointing his employer didn’t at least garner an honorable mention in Versa’s security advisory. But he said he’s glad there are now a lot fewer Versa systems exposed to this attack.</p>
<p>“Lumen has for the last nine weeks been very intimate with their leadership with the goal in mind of helping them mitigate this,” English said. “We’ve given them everything we could along the way, so it kind of sucks being referenced just as a third party.”</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nuclear reactors a mile underground promise safe, cheap power (116 pts)]]></title>
            <link>https://newatlas.com/energy/underground-nuclear-reactors/</link>
            <guid>41366436</guid>
            <pubDate>Tue, 27 Aug 2024 11:36:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/energy/underground-nuclear-reactors/">https://newatlas.com/energy/underground-nuclear-reactors/</a>, See on <a href="https://news.ycombinator.com/item?id=41366436">Hacker News</a></p>
Couldn't get https://newatlas.com/energy/underground-nuclear-reactors/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Why has Japan been hit with rice shortages despite normal crops? (115 pts)]]></title>
            <link>https://mainichi.jp/english/articles/20240823/p2a/00m/0bu/024000c</link>
            <guid>41366304</guid>
            <pubDate>Tue, 27 Aug 2024 11:19:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mainichi.jp/english/articles/20240823/p2a/00m/0bu/024000c">https://mainichi.jp/english/articles/20240823/p2a/00m/0bu/024000c</a>, See on <a href="https://news.ycombinator.com/item?id=41366304">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<!-- cxenseparse_start -->

<div>
<figure>
<div>
<a data-href="https://cdn.mainichi.jp/vol1/2024/08/23/20240823p2a00m0bu021000p/9.jpg?1" data-lightbox="photos" data-title="Shortages of Japanese rice, pictured in this file photo taken on Aug. 14, 2024, have continued in Japan. (Mainichi/Shiro Sakamaki)">
<span>
<img src="https://cdn.mainichi.jp/vol1/2024/08/23/20240823p2a00m0bu021000p/6.jpg?1" alt="">

</span>

</a>
</div>
<figcaption>Shortages of Japanese rice, pictured in this file photo taken on Aug. 14, 2024, have continued in Japan. (Mainichi/Shiro Sakamaki)</figcaption>
</figure>
</div>
<p>
    TOKYO -- Shortages of rice have recently been seen across Japan, and the price of the staple food is soaring. But close to 100% of Japan's rice is domestically produced and the yield of crops appears normal, so why is this happening? 
</p>
<!-- cxenseparse_end -->

<!-- cxenseparse_start -->
<p>
    The Mainichi Shimbun spoke with Kazuhito Yamashita, a former bureaucrat at Japan's Ministry of Agriculture, Forestry and Fisheries and a research director at the Canon Institute for Global Studies, about the background to the rice commotion, shedding light on the reality of rice policies that have disregarded consumers. Questions and answers from the interview follow:
</p>
<p>
    <b>No poor harvest</b>
</p>
<p>
    Question: Why is there a shortage of rice and why are prices high?
</p>
<p>
    Answer: Some have said that it's due to lean crops as a result of last year's scorching summer, or from an increase in inbound tourism, but neither of these is the main reason.
</p>
<p>
    The crop situation index for rice grown in 2023, indicating the amount of the rice harvest, was 101 -- around the same as an average year. In contrast, the crop situation index for 1993, which led to the so-called "rice riots of the Heisei era," was 74. Some suggest that a lean harvest of high-quality rice caused a shortage of the crop, but the harvest was not bad.
</p>
<p>
    As for the suggestion that inbound visitors are consuming more, we cannot say this is a major factor. Even if around 3 million visitors were to stay in Japan each month for a week and eat rice for breakfast, lunch and dinner like many Japanese people, it would still only account for around 0.5% of total consumption. And in actual fact, not many visitors have rice for all three meals, so their consumption must be even lower.
</p>
<p>
    <b>A mechanism that has lasted for over 50 years</b>
</p>
<p>
    Q: What is the main factor, then?
</p>
<p>
    A: The reason there is a shortage of rice is because of the acreage reduction policy which decreases the amount of land devoted to cultivation. Under acreage reduction, rice production is cut to raise market prices, and the government provides subsidies to rice farmers who switch to other crops such as wheat or soybeans. Japan has continued this policy for over 50 years.
</p>

<div>
<figure>
<div>
<a data-href="https://cdn.mainichi.jp/vol1/2024/08/23/20240823p2a00m0bu022000p/9.jpg?1" data-lightbox="photos" data-title="Kazuhito Yamashita, a research director at the Canon Institute for Global Studies, speaks in an interview with the Mainichi Shimbun in Tokyo's Chiyoda Ward on Aug. 6, 2024. (Mainichi/Megumi Udagawa)">
<span>
<img src="https://cdn.mainichi.jp/vol1/2024/08/23/20240823p2a00m0bu022000p/7.jpg?1" alt="">

</span>

</a>
</div>
<figcaption>Kazuhito Yamashita, a research director at the Canon Institute for Global Studies, speaks in an interview with the Mainichi Shimbun in Tokyo's Chiyoda Ward on Aug. 6, 2024. (Mainichi/Megumi Udagawa)</figcaption>
</figure>
</div>
<p>
    Because consumption of bread, pasta and other alternative foods is increasing, if farmers produced the same amount of rice as before, there would be a surplus, causing the price of rice to drop. To avoid such a situation, production has been cut year by year, and recently only about 60% of rice paddies are in use, with the others set aside under the acreage reduction policy. Production has thus been kept at under half of the peak of 14.45 million metric tons annually.
</p>
<p>
    Because production has been managed so tightly, when there is even a slight increase in demand, such as from inbound tourists, it can quickly lead to a shortage, and prices surge as a result. This is the essence of what is happening now.
</p>
<p>
    <b>Misleading idea of 'abolition of acreage reduction' under Abe government</b>
</p>
<p>
    Q: Wasn't the rice acreage reduction policy abolished in 2018 under the administration of then Prime Minister Shinzo Abe?
</p>
<p>
    A: That was a trick of the Abe administration. They actually only abolished the rice production targets, and the policy of providing subsidies if farmers reduced their rice production remained in place. In fact, I asked an official from the Ministry of Agriculture, Forestry and Fisheries at the time if they were really abolishing the policy to reduce acreage, and they firmly stated, "Absolutely not. We have never said we would abolish it."
</p>
<p>
    At the time, the prime minister's office probably wanted to display colors of reform by coming out with "the abolishment of acreage reduction" and use that to buoy the administration. The truth is that this was well received by the public, so the Abe administration kept on claiming that the rice acreage reduction policy had been abolished.
</p>
<p>
    <b>No protests occurred</b>
</p>
<p>
    Q: So, the public was deceived?
</p>
<p>
    A: If the rice acreage policy had really been abolished, rice production would have increased, and prices would have plummeted. This in turn would have stirred massive protests from farmers. But have rice prices dropped? On the contrary, we're seeing a commotion now because of the high prices.
</p>
<p>
    No other country has maintained a crop acreage reduction policy for as long as Japan has. The United States and the European Union temporarily introduced such a policy when there was a surplus of crops in order to maintain prices. But now they have all stopped doing this. That's because they know that producing more and exporting it brings greater benefits than reducing the amount produced.
</p>
<p>
    Japan should also completely abolish the rice acreage reduction policy, produce more rice and actively start exporting it. This would also increase the food self-sufficiency rate.
</p>

<div>
<figure>
<div>
<a data-href="https://cdn.mainichi.jp/vol1/2024/08/23/20240823p2a00m0bu023000p/9.jpg?1" data-lightbox="photos" data-title="Ripe rice plants are pictured in Miyagi Prefecture in this file photo taken on Aug. 28, 2017. (Mainichi)">
<span>
<img src="https://cdn.mainichi.jp/vol1/2024/08/23/20240823p2a00m0bu023000p/6.jpg?1" alt="">

</span>

</a>
</div>
<figcaption>Ripe rice plants are pictured in Miyagi Prefecture in this file photo taken on Aug. 28, 2017. (Mainichi)</figcaption>
</figure>
</div>
<p>
    <b>The 'Rolls-Royce' of rice</b>
</p>
<p>
    Q: Is Japanese rice competitive internationally?
</p>
<p>
    A: People across the world say, "It's the best-tasting rice in the world, so why isn't more of it being exported?" Japanese agricultural officials often comment, "We can't compete with cheap rice produced in Thailand and elsewhere," but that's not the case. Just like you have luxury and standard cars, there are different kinds of rice. Luxury cars won't lose out to ordinary ones even if the price is higher.
</p>
<p>
    The British Rolls-Royce is an ultra-luxury vehicle, and Japanese rice, too, should be marketed as the Rolls-Royce of rice, so there's no need to compete with cheaper rice. With the right pricing, any amount will sell. In actual fact, in California, Koshihikari rice produced in the state is now selling for a higher price than in Japanese supermarkets.
</p>
<p>
    <b>A terrible policy</b>
</p>
<p>
    Q: It's said that the decline in rice consumption is serious, but if the amount produced increased and the price went down, people would eat more, wouldn't they?
</p>
<p>
    A: Exactly. Rice acreage reduction is an absolutely terrible policy. The government spends over 300 billion yen (about $2.06 billion) in subsidies annually to decrease the amount of rice produced, thus going out of its way to raise the price and increasing the burden on consumers. In the medical field, for example, the government spends money to reduce the financial burden on citizens, but the rice acreage reduction does the opposite -- it is using taxpayer money to make consumers suffer.
</p>
<p>
    Furthermore, due to the acreage reduction policy, the development of rice varieties that increase yield per unit area has been halted. By unit area, Californian rice now yields 1.6 times more than Japan's. Chinese rice too used to yield only half as much as Japan's but now yields more.
</p>
<p>
    The acreage reduction policy does not benefit consumers or the agricultural industry. If the abolition of the acreage reduction policy causes rice prices to drop and creates difficulties for full-time farmers whose main income is from agriculture, direct payments from the government, like in Western countries, would be the solution.
</p>
<p>
    <b>Increased production needed for food security</b>
</p>
<p>
    Q: Japan's rice is the only grain of which almost all is produced domestically. From the point of food security, it's important to increase the amount produced, right?
</p>
<p>
    A: Japan currently harvests just under 7 million tons annually, but if it were to do away with the rice acreage policy and introduce high-yield rice varieties, it would have the ability to produce 17 million tons a year. If it were to yield that much and export 10 million tons, it would have great security benefits. For example, if a maritime blockade was imposed in the event of an emergency situation in Taiwan and imports and exports were cut off, Japan would be able to use the 10 million tons that it had exported for its people. Exports serve as a kind of reserve for emergencies. All countries are aware of this and are advancing their food policies accordingly.
</p>
<p>
    <b>Shortages, high prices likely to recur</b>
</p>
<p>
    Q: Are the rice shortages and high prices that we're seeing likely to recur in the future?
</p>
<p>
    A: As long as the policy to reduce rice acreage remains in place, similar situations will occur again. That's because the environment where even a small movement in consumption can lead to rice shortages and high prices remains unchanged.
</p>
<p>
    Today, the world's largest rice exporter is India, with its exports reaching 10 million to 20 million tons annually. If Japan completely abolished rice acreage reduction and exported 10 million tons a year, it would become one of the world's largest rice suppliers and could also contribute to global food security. Despite such opportunities, officials have paid no attention, begging the question of how long Japan intends to continue to pour effort into maintaining high domestic rice prices.
</p>
<p>
    (Interviewed by Megumi Udagawa, Opinion Group)
</p>
<p>
    <b>Profile: Kazuhito Yamashita</b> 
</p>
<p>
    Kazuhito Yamashita was born in western Japan's Okayama Prefecture in 1955. He graduated from the University of Tokyo's Faculty of Law in 1977 and joined the former Ministry of Agriculture, Forestry and Fisheries (MAFF) in 1977. He served as vice chairman of the OECD Committee for Agriculture and deputy director-general of MAFF's Rural Development Bureau. His expertise includes food and agricultural policy. He has authored many books, including "Nihon ga Ueru! Sekai Shokuryo Kiki no Shinjitsu" (Japan is starving! The truth of the global food crisis).
</p>
<!-- cxenseparse_end -->

<!--| tools BGN |-->

<!--| tools END |-->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The possibilities for dark matter have shrunk (115 pts)]]></title>
            <link>https://www.sciencenews.org/article/dark-matter-wimps-lz</link>
            <guid>41366064</guid>
            <pubDate>Tue, 27 Aug 2024 10:37:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencenews.org/article/dark-matter-wimps-lz">https://www.sciencenews.org/article/dark-matter-wimps-lz</a>, See on <a href="https://news.ycombinator.com/item?id=41366064">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		

<article id="post-3142967">
	<header>
	<div>
			


			
<p>
	The LZ experiment reports no signs of dark matter in their latest search</p>



		</div>

		<figure>
		<p><img width="1030" height="580" src="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/08/082324_EC_dark-matter_feat.jpg?fit=1030%2C580&amp;ssl=1" alt="An array of circular photomultiplier tubes that is part of the LZ experiment." decoding="async" fetchpriority="high" srcset="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/08/082324_EC_dark-matter_feat.jpg?w=1440&amp;ssl=1 1440w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/08/082324_EC_dark-matter_feat.jpg?resize=680%2C383&amp;ssl=1 680w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/08/082324_EC_dark-matter_feat.jpg?resize=800%2C450&amp;ssl=1 800w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/08/082324_EC_dark-matter_feat.jpg?resize=330%2C186&amp;ssl=1 330w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/08/082324_EC_dark-matter_feat.jpg?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/08/082324_EC_dark-matter_feat.jpg?resize=1030%2C580&amp;ssl=1 1030w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/08/082324_EC_dark-matter_feat.jpg?resize=1380%2C776&amp;ssl=1 1380w" sizes="(max-width: 1030px) 100vw, 1030px" data-attachment-id="3142969" data-permalink="https://www.sciencenews.org/082324_ec_dark-matter_feat" data-orig-file="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/08/082324_EC_dark-matter_feat.jpg?fit=1440%2C810&amp;ssl=1" data-orig-size="1440,810" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;Sanford Underground Research Facility&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="082324_EC_dark-matter_feat" data-image-description="" data-image-caption="<p style=&quot;font-weight: 400&quot;>The LZ experiment is designed to detect light from interactions of dark matter particles using photomultiplier tubes (shown).</p>
" data-medium-file="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/08/082324_EC_dark-matter_feat.jpg?fit=680%2C383&amp;ssl=1" data-large-file="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/08/082324_EC_dark-matter_feat.jpg?fit=800%2C450&amp;ssl=1">		</p>

					<figcaption>
									<span>
						<p>The LZ experiment is designed to detect light from interactions of dark matter particles using photomultiplier tubes (shown).</p>
					</span>
				
									<span>
						<p>Matthew Kapust/Sanford Underground Research Facility</p>
					</span>
							</figcaption>
			</figure>
	</header>

	

		
		
	<div data-component="video-embed">
				




<p>Scientists have just slashed the potential hiding spaces for dark matter particles.</p>



<p>The LUX-ZEPLIN, or LZ, experiment has searched for and ruled out the existence of dark matter particles with a wide swath of properties, researchers report August 26 at two conferences. Dark matter is a substance whose influence can be seen on the scale of galaxies and galaxy clusters, but which has never been directly detected.&nbsp;</p>



<p>LZ searches for a hypothetical type of dark matter particle called a weakly interacting massive particle, specifically WIMPs with masses above 9 billion electron volts. (For comparison, a proton has a mass of around 1 billion electron volts). The LZ detector, filled with 10 metric tons of liquid xenon,&nbsp;<a href="https://www.sciencenews.org/article/dark-matter-lz-experiment-physics">monitors for atomic nuclei recoiling</a>&nbsp;when WIMPs plow into the liquid (<em>SN: 7/7/22</em>).&nbsp;</p>





<p>The researchers characterize WIMPs by their cross section — the probability that a particle will interact. The result shrinks the maximum possible cross section to about a fifth that allowed by previous results, LZ researchers report at the&nbsp;<a href="https://indico.uchicago.edu/event/427/contributions/1325/" target="_blank" rel="noopener">TeV Particle Astrophysics meeting</a>&nbsp;in Chicago and at the&nbsp;<a href="https://indico.cern.ch/event/1390649/contributions/6061507/" target="_blank" rel="noopener">Light Detection in Noble Elements meeting</a>&nbsp;in São Paulo.</p>



<p>“We are making massive strides into new territory,” says physicist Chamkaur Ghag of University College London, spokesperson of LZ.&nbsp;</p>



<p>The study was performed with 280 days’ worth of data. LZ’s final results will be based on 1,000 days of data, and it’s expected to further carve away at the dark matter’s possibilities — or find evidence of it.</p>



			</div>
</article><!-- #post-## -->


<section>
	<h3 id="carousel-heading">
		More Stories from Science News on <a href="https://www.sciencenews.org/topic/particle-physics">Particle Physics</a>
	</h3>
	

</section>

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zuckerberg claims regret on caving to White House pressure on content (258 pts)]]></title>
            <link>https://www.politico.com/news/2024/08/26/zuckerberg-meta-white-house-pressure-00176399</link>
            <guid>41365868</guid>
            <pubDate>Tue, 27 Aug 2024 09:50:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.politico.com/news/2024/08/26/zuckerberg-meta-white-house-pressure-00176399">https://www.politico.com/news/2024/08/26/zuckerberg-meta-white-house-pressure-00176399</a>, See on <a href="https://news.ycombinator.com/item?id=41365868">Hacker News</a></p>
Couldn't get https://www.politico.com/news/2024/08/26/zuckerberg-meta-white-house-pressure-00176399: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Tinybox of tinygrad by George Hotz is finally entering production (170 pts)]]></title>
            <link>https://twitter.com/realgeorgehotz/status/1828197925874463166</link>
            <guid>41365637</guid>
            <pubDate>Tue, 27 Aug 2024 08:54:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/realgeorgehotz/status/1828197925874463166">https://twitter.com/realgeorgehotz/status/1828197925874463166</a>, See on <a href="https://news.ycombinator.com/item?id=41365637">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Notes on Buttondown.com (117 pts)]]></title>
            <link>https://jmduke.com/posts/microblog/buttondown-dot-com/</link>
            <guid>41364783</guid>
            <pubDate>Tue, 27 Aug 2024 05:25:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jmduke.com/posts/microblog/buttondown-dot-com/">https://jmduke.com/posts/microblog/buttondown-dot-com/</a>, See on <a href="https://news.ycombinator.com/item?id=41364783">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <main>
                <p>We spent $85,000 for <code>buttondown.com</code> in April; this was the biggest capital expenditure I've ever made, and though it was coming from cash flow generated by Buttondown rather than my own checking account it was by rough estimation the largest non-house purchase I've ever made.</p>
<p>As of August, we're officially migrated over from <code>buttondown.email</code> to <code>buttondown.com</code>. I'm sure I'll do a more corporate blog post on the transition in the future, but for now I want to jot down some process notes:</p>
<ul>
<li>The entire process was made much more painful due to Buttondown's architecture, which is a hybrid of Vercel/Next (for the marketing site and docs site) and Django/Heroku (for the core app) managed by a HAProxy load balancer to route requests. We ended up using <a href="https://hurl.dev/">hurl</a> as a test harness around HAProxy, something we probably should have done three years ago.</li>
<li>I went in expecting SEO traffic to be hit as Google renegotiates legions of canonical URLs; it hasn't, at least thus far. Instead, everything seems to have just <em>bumped</em> fairly healthily.</li>
<li>I expected <em>more</em> production issues to come up than actually did. I credit this to a fairly clear scope: the goal was "to migrate all web traffic to .com", which meant that a) we didn't need to re-map any paths and b) we didn't need to worry about mapping SMTP traffic (which still runs through <code>buttondown.email</code>).</li>
<li>The hardest part of the process was the stuff you can't grep for. URLs on other sites, OAuth redirect URLs, that sort of thing.</li>
<li>Starting with isolated domains (the documentation site, the demo site) that weren't tied to the aforementioned HAProxy load balancer gave me some good early confidence that the migration would be smooth.</li>
</ul>
<p>Overall: very happy with how it turned out. I would describe the project roughly as "three months of fretting/planning, one week of grepping, and one week of fallout."</p>
<p>Was it worth it? Yes, I think so. Most theoretical capital expenditures Buttondown can make right now have a non-trivial ongoing cost associated with them (buy another newsletter company or content vertical and now you have to run it on a day-to-day basis; do a big marketing build-out and you have to manage it; etc.) — this was a sharp but fixed cost, and it's something that I knew I wanted to do in the fullness of time. (And, most importantly, people stop referring to Buttondown as "Buttondown Email", a personal pet peeve of mine.)</p>

            </main>
        </div><p>
  © 2024 Justin Duke · All rights reserved · have a nice day.
</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sovereign Tech Fund to Invest €686k in FreeBSD Infrastructure Modernization (154 pts)]]></title>
            <link>https://freebsdfoundation.org/blog/sovereign-tech-fund-to-invest-e686400-in-freebsd-infrastructure-modernization/</link>
            <guid>41364776</guid>
            <pubDate>Tue, 27 Aug 2024 05:23:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://freebsdfoundation.org/blog/sovereign-tech-fund-to-invest-e686400-in-freebsd-infrastructure-modernization/">https://freebsdfoundation.org/blog/sovereign-tech-fund-to-invest-e686400-in-freebsd-infrastructure-modernization/</a>, See on <a href="https://news.ycombinator.com/item?id=41364776">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
              
<article id="post-16252">
  <!-- .post-header -->

  
  <div>
    <h5>August 26, 2024</h5><section>
<p><strong>Investment to accelerate zero trust builds, SBOM, security tooling, and developer experience</strong></p>
</section>

<section>
<p><strong>Boulder, CO – August 26, 2024—</strong>The FreeBSD Foundation, dedicated to advancing the open source FreeBSD operating system and supporting the community, announced that Germany’s Sovereign Tech Fund (STF) has agreed to invest €686,400 in the FreeBSD project to drive improvements in infrastructure, security, regulatory compliance, and developer experience.</p>
</section>

<section>
<p>The work, organized and managed by the FreeBSD Foundation, will begin in August 2024 and continue through 2025. It will focus on five key projects:</p>
</section>

<section>
<ul><section>
<li><strong>Zero Trust Builds</strong>: Enhance tooling and processes</li>
</section>

<section>
<li><strong>CI/CD Automation</strong>: Streamline software delivery and operations</li>
</section>

<section>
<li><strong>Reduce Technical Debt</strong>: Implement tools and processes to keep technical debt low</li>
</section>

<section>
<li><strong>Security Controls</strong>: Modernize and extend security artifacts, including the FreeBSD Ports and Package Collection, to assist with regulatory compliance</li>
</section>

<section>
<li><strong>SBOM Improvements</strong>: Enhance and implement new tooling and processes for FreeBSD SBOM</li>
</section></ul>
</section>

<section>
<p>Developers are the lifeblood of every open source project. The Sovereign Tech Fund’s investment in FreeBSD infrastructure will ensure a world-class developer experience while preserving and extending the security and digital sovereignty for which FreeBSD is renowned.&nbsp;</p>
</section>

<section>
<p>The work commissioned by STF also aligns closely with the recent <a href="https://www.whitehouse.gov/oncd/briefing-room/2024/08/09/fact-sheet-biden-harris-administration-releases-end-of-year-report-on-open-source-software-security-initiative-2/">August 9, 2024 summary report</a> released by the <a href="https://www.whitehouse.gov/oncd/">U.S. Office of the National Cyber Director</a> (ONCD), consolidating feedback from the 2023 request for information on key priorities for securing the open source software ecosystem. By enhancing security controls and SBOM tooling, the FreeBSD Foundation is helping to keep FreeBSD at the forefront of improved vulnerability disclosure mechanisms and secure software foundations.&nbsp;</p>
</section>

<section>
<p>“The Sovereign Tech Fund is pleased to support the FreeBSD project,” said Fiona Krakenbürger, co-founder of STF. “This investment in critical digital infrastructure will accelerate modernization of FreeBSD, enhance security hygiene, and improve developer experiences. The widespread prevalence of FreeBSD means that these improvements will have a far-reaching impact on the global public sector and the research sector, as well as commercial users. We are excited to contribute to its continued modernization in a way that best serves the public interest as well as the FreeBSD community.”</p>
</section>

<section>
<p>“We are deeply grateful for this significant investment from the Sovereign Tech Fund, which will further enhance security and infrastructure for FreeBSD developers and users,” said Deb Goodkin, Executive Director of the FreeBSD Foundation. “As it has for thirty years, the FreeBSD project is again positioning itself at the vanguard of open source security, resilience, and reliability. The world’s governments recognize the key role open source projects like FreeBSD play in our shared digital infrastructure. This STF-commissioned work will provide the necessary visibility, auditability, and trust for commercial FreeBSD users facing new regulations as well as public sector, academic, and individual users.”&nbsp;</p>
</section>

<section>
<p>The Sovereign Tech Fund (<a href="https://www.sovereigntechfund.de/">https://www.sovereigntechfund.de</a>) supports the development, improvement, and maintenance of open digital infrastructure in the public interest. Its goal is to strengthen the open source ecosystem sustainably, focusing on security, resilience, technological diversity, and the people behind the code. STF is funded by the German Federal Ministry for Economic Affairs and Climate Action (BMWK) and hosted at and supported by the German Federal Agency for Disruptive Innovation GmbH (SPRIND).</p>
</section>

<section>
<p><br><strong>How to Get Involved:&nbsp;</strong></p>
</section>

<section>
<p>The FreeBSD Foundation is committed to transparent and collaborative communication. All announcements and updates will be made through established public channels. For questions or interest in participating in potential Advisory Committees to provide feedback and guidance on STF-funded work, please contact partnerships@freebsdfoundation.org.</p>
</section>

<section>
<p><strong>About The FreeBSD Foundation</strong></p>
</section>

<section>
<p>The FreeBSD Foundation is a 501(c)(3) non-profit organization supporting the FreeBSD Project and community. Accepting donations from individuals and businesses, the Foundation uses funds to develop features, employ software engineers, improve build and test infrastructure, advocate for FreeBSD through in-person and online events, and provide training and educational material. Representing the FreeBSD Project in legal affairs, the Foundation is the recognized entity for contracts, licenses, and other legal arrangements and is entirely donation supported. Learn more at <a href="https://freebsdfoundation.org/">freebsdfoundation.org</a></p>
</section><p><span><a href="https://freebsdfoundation.org/blog/freebsd-ports-and-packages-what-you-need-to-know/" rel="prev">Previous</a></span><span></span>  </p></div><!-- .post-content -->

</article><!-- #post-16252 -->
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anthropic publishes the 'system prompts' that make Claude tick (221 pts)]]></title>
            <link>https://techcrunch.com/2024/08/26/anthropic-publishes-the-system-prompt-that-makes-claude-tick/</link>
            <guid>41364637</guid>
            <pubDate>Tue, 27 Aug 2024 04:45:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/08/26/anthropic-publishes-the-system-prompt-that-makes-claude-tick/">https://techcrunch.com/2024/08/26/anthropic-publishes-the-system-prompt-that-makes-claude-tick/</a>, See on <a href="https://news.ycombinator.com/item?id=41364637">Hacker News</a></p>
Couldn't get https://techcrunch.com/2024/08/26/anthropic-publishes-the-system-prompt-that-makes-claude-tick/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Box64 and RISC-V in 2024: What It Takes to Run the Witcher 3 on RISC-V (318 pts)]]></title>
            <link>https://box86.org/2024/08/box64-and-risc-v-in-2024/</link>
            <guid>41364549</guid>
            <pubDate>Tue, 27 Aug 2024 04:23:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://box86.org/2024/08/box64-and-risc-v-in-2024/">https://box86.org/2024/08/box64-and-risc-v-in-2024/</a>, See on <a href="https://news.ycombinator.com/item?id=41364549">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

			
<p>It’s been over a year since <a href="https://box86.org/2023/05/box64-and-risc-v/">our last update</a> on the state of the RISC-V backend, and we recently successfully ran The Witcher 3 on an RISC-V PC, which I believe is the first AAA game ever to run on an RISC-V machine. So I thought this would be a perfect time to write an update, and here it comes.</p>



<figure><p>
<iframe title="The Witcher 3 running On RiSC-V" width="580" height="326" src="https://www.youtube.com/embed/5UMUEM0gd34" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p><figcaption>The Witcher 3 Running on RISC-V via Box64, Wine, and DXVK.</figcaption></figure>



<h2>The Story</h2>



<p>A year ago, RV64 DynaRec could only run some relatively “easy-to-run” native Linux games, such as Stardew Valley, World of Goo, etc.</p>



<p>On the one hand, this was because after a large number of new x86_64 instructions were implemented quickly in RISC-V, there were many bugs left in the DynaRec. Things won’t work if you don’t implement the x86_64 ISA correctly. But the most important factor is that we had no RISC-V device could be plugged into an AMD graphics card at the time, and the IMG integrated graphics cards on VisionFive 2 and LicheePi 4A did not support OpenGL, only OpenGL ES.</p>



<p>We can get a certain level of OpenGL support using gl4es, which allows games like Stardew Valley to run, but it is not enough for other more serious Linux games, as well as all Windows games in general.</p>



<p>So this became a hard barrier for us to test more x86 programs in the wider world, until both ptitSeb and I received the Milk-V Pioneer from Sophgo, which is a 64-core RISC-V PC, and of course, it also has a PCIe slot for a graphics card. Many thanks to Sophgo!</p>



<p>In addition, another core contributor xctan also found a way to “plug” an AMD graphics card into VisionFive 2 via the M.2 interface. With that, we were exposed to the wider world and we’ve since fixed a ton of RV64 DynaRec bugs and also added a ton of new x86 instructions. Changing in quantity leads to changes in quality, more and more games were working, and finally, we tried running The Witcher 3 for the first time, and it just worked!</p>



<p>That’s the story of running The Witcher 3 on RISC-V.</p>



<h2>What is the Current Status of RISC-V DynaRec?</h2>



<p>The x86 instruction set is very very big. According to rough statistics, the ARM64 backend implements more than 1,600 x86 instructions in total, while the RV64 backend implements about 1,000 instructions. Among them, more than 300 of these instructions are newly supported AVX ones that we haven’t implemented at all in RISC-V. Anyway, still need some catching up.</p>



<p>Also, for SSE instructions, we use scalar instructions for implementation, while AArch64 uses the Neon extension and LoongArch64 uses the LSX extension. So the performance is quite poor compared to the other two backends.</p>



<p>However, things are not set in stone. RISC-V has a vector extension called the Vector extension. Yeah I know, so I will call it RVV from now on.</p>



<p>There are already some devices that support RVV on the market, such as the Milk-V Pioneer mentioned above, which supports the xtheadvector extension, which is a variant of RVV version 0.7.1 (things are a bit complicated). In addition, the SpacemiT K1/M1 SoC released not long ago supports the ratified version of RVV 1.0. Currently, the Banana Pi F3 and Milk-V Jupiter equipped with this SoC are already available for purchase.</p>



<p>With these devices available, recently we have added basic RVV support to box64 and implemented several common SSE instructions. However, this work is still very early, so it will not help the performance for now. But the future is promising, right?</p>



<p>Next, let’s talk about the two dark clouds hanging over the RISC-V backend. These are the stuff where I feel RISC-V is most lacking in x86 emulation over the past year.</p>



<h2>The Most Wanted Instructions for x86 Emulation</h2>



<p>At least in the context of x86 emulation, among all 3 architectures we support, RISC-V is the least expressive one. Compared with AArch64 and LoongArch64, RISC-V lacks many convenient instructions, which means that we have to use more instructions to emulate the same behavior, so the translation efficiency will be lower.</p>



<p>Among them, two instructions are the most critical ones — the ability to pick a range of bits from one register into another; and the ability to insert some bits from one register into a range of another register.</p>



<p>Both LoongArch64 and AArch64 have equivalent instructions, but the RISC-V world has no counterparts for these two instructions, whether official or vendor extensions. It’s not some complex instructions that break the RISC philosophy, so it’s a shame they do not exist on RISC-V.</p>



<p>But why it’s so important for x86 emulation? Because the x86 ISA tends to preserve the unchanged bits.</p>



<p>For example, for an <code>ADD AH, BL</code> instruction, box64 needs to extract the lowest byte from RBX, added to the second lowest byte of RAX, and then insert it back into the second lowest byte of RAX <strong>while keeping all other bytes in RAX unchanged</strong>.</p>



<p>On LoongArch64, we have <code>BSTRPICK.D</code> to pick the bits, and <code>BSTRINS.D</code> to insert the bits, so the implementation would be:</p>



<pre><code>BSTRPICK.D scratch1, xRAX, 15, 8
BSTRPICK.D scratch2, xRBX, 7, 0
ADD scratch1, scratch1, scratch2
BSTRINS.D xRAX, scratch1, 15, 8</code></pre>



<p>Simple and intuitive, right? And it would be as simple on ARM64, with <code>UBFX</code> and <code>BFI</code> opcodes. On RISC-V, however, we have to do this:</p>



<pre><code># extract the second lowest byte of RAX
SRLI scratch1, xRAX, 8
ANDI scratch1, scratch1, 0xFF
# extract the lowest byte of RBX
ANDI scratch2, xRBX, 0xFF
# do the addition
ADD scratch1, scratch1, scratch2
# fill scratch3 with mask 0xFFFF_FFFF_FFFF_00FF
LUI	scratch3, 0xFFFF0
ADDIW   scratch3, scratch3, 0xFF
# insert it back
AND xRAX, xRAX, scratch3
ANDI scratch1, scratch1, 0xFF
SLLI scratch1, scratch1, 8
OR xRAX, xRAX, scratch1</code></pre>



<p>So a whole of 10 instructions for a simple byte add and this is by no means an isolated case! There are many similar instructions in x86, and their implementation on RISC-V is more cumbersome.</p>



<h2>The Frustration of 16-byte Atomic Instructions</h2>



<p>x86 has LOCK prefixed instructions for lock-free atomic operations, and box64 mainly uses LR/SC sequence to emulate these. LR/SC is short for Load-Reserved / Store-Conditionally.</p>



<p>For example, for <code>LOCK ADD [RAX], RCX</code>, we generate the following code:</p>



<pre><code>MARKLOCK:
LR.D scratch1, (xRAX)
ADD scratch2, scratch1, xRCX
SC.D scratch3, scratch2, (xRAX)
BNEZ scratch3, MARKLOCK</code></pre>



<p>If the address in RAX is unaligned, things become a bit more complex, but in general, this works really well.</p>



<p>Except for the <code>LOCK CMPXCHG16B</code> instruction, which compares <code>RDX:RAX</code> with 16 bytes of memory and exchanges <code>RCX:RBX</code> to the memory address. While some 16-byte atomic instructions in AArch64 and LoongArch64 can be used to implement this, again, there are no counterparts in RISC-V whatsoever, unfortunately.</p>



<p>Therefore, we cannot implement this instruction as perfectly as other architectures, and even more unfortunately, many programs use this instruction, such as Unity games.</p>



<h2>The End</h2>



<p>In the end, and despite all those short-comming, The Witcher 3 actually runs, at up to 15 fps in-game and full speed on the main menu! So not that bad for a machine never designed to run AAA games!</p>



<figure><img decoding="async" width="1024" height="576" src="https://box86.org/wp-content/uploads/2024/08/witcher3menu_riscv-1024x576.png" alt="The Witcher 3 Menu with DXVK_HUD running on RiSC-V" srcset="https://box86.org/wp-content/uploads/2024/08/witcher3menu_riscv-1024x576.png 1024w, https://box86.org/wp-content/uploads/2024/08/witcher3menu_riscv-300x169.png 300w, https://box86.org/wp-content/uploads/2024/08/witcher3menu_riscv-768x432.png 768w, https://box86.org/wp-content/uploads/2024/08/witcher3menu_riscv-1536x864.png 1536w, https://box86.org/wp-content/uploads/2024/08/witcher3menu_riscv-1200x675.png 1200w, https://box86.org/wp-content/uploads/2024/08/witcher3menu_riscv.png 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Harvard and MIT's $800M Mistake: The Triple Failure of 2U, EdX, and Axim (217 pts)]]></title>
            <link>https://www.classcentral.com/report/2u-edx-bankruptcy/</link>
            <guid>41363549</guid>
            <pubDate>Tue, 27 Aug 2024 00:34:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.classcentral.com/report/2u-edx-bankruptcy/">https://www.classcentral.com/report/2u-edx-bankruptcy/</a>, See on <a href="https://news.ycombinator.com/item?id=41363549">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p><img loading="lazy" src="https://www.classcentral.com/report/wp-content/uploads/2024/08/2u-bankruptcy-banner-triple-failure.png" alt="" width="1536" height="768" srcset="https://www.classcentral.com/report/wp-content/uploads/2024/08/2u-bankruptcy-banner-triple-failure.png 1536w, https://www.classcentral.com/report/wp-content/uploads/2024/08/2u-bankruptcy-banner-triple-failure-300x150.png 300w, https://www.classcentral.com/report/wp-content/uploads/2024/08/2u-bankruptcy-banner-triple-failure-1024x512.png 1024w, https://www.classcentral.com/report/wp-content/uploads/2024/08/2u-bankruptcy-banner-triple-failure-768x384.png 768w" sizes="(max-width: 1536px) 100vw, 1536px"></p>
<p>In 2021, the unprofitable 2U bought edX, an unprofitable non-profit, for a staggering $800 million. Even to my untrained eye, it seemed like a questionable decision.</p>
<p>In my <a href="https://www.classcentral.com/report/2u-edx-acquisition-analysis/">analysis of the acquisition</a>, I had highlighted the financial strain this acquisition would place on 2U, noting the additional $42 million of annual interest expense due to the loan taken to finance the purchase.</p>
<p>Last month, <a href="https://www.wsj.com/business/2u-ed-tech-company-files-chapter-11-bankruptcy-24ca1017">2U filed for bankruptcy</a>, primarily due to the significant debt it took on, particularly to finance the acquisition of edX.</p>
<p>Fast forward to 2024, and we’re faced with a harsh reality: 2U is bankrupt, edX seems to have stagnated, and Axim Collaborative (the&nbsp; non-profit from the sale that retained the $800 million) has barely made a peep.</p>
<p>As one of the few voices who expressed skepticism from the outset (in a business sense), I feel compelled to break down this triple failure and the unfulfilled promises that accompanied it.</p>
<h2>From Ivy League to Bankruptcy: A Timeline</h2>
<p>2021:</p>
<ul>
<li aria-level="1">June/July: 2U announces <a href="https://www.classcentral.com/report/2u-edx-acquisition-analysis/">acquisition of edX for $800 million</a></li>
<li aria-level="1">November: Acquisition finalized</li>
</ul>
<p>2022:</p>
<ul>
<li aria-level="1">February: <a href="https://www.classcentral.com/report/2u-stock-drop/">2U stock drops 50</a>% after Q1 earnings report, market cap falls below edX purchase price</li>
<li aria-level="1">July/August: 2U announces strategy shift centered around edX, including <a href="https://www.classcentral.com/report/2u-layoffs/">layoffs and cost reductions</a></li>
<li aria-level="1">2U implements “platform strategy” to unify operations under edX brand</li>
</ul>
<p>2023:</p>
<ul>
<li aria-level="1">March: 2U investor day outlines <a href="https://www.classcentral.com/report/edx-catalog-subscriptions/">plans for edX</a> including new subscription models and “funnel builders”</li>
<li aria-level="1"><a href="https://www.classcentral.com/report/axim-collaborative/">Axim Collaborative announced</a> with a new CEO</li>
<li aria-level="1">Q3:
<ul>
<li aria-level="2">2U announces <a href="https://www.classcentral.com/report/2u-edx-disastrous-quarter/">disastrous Q3 results</a></li>
<li aria-level="2">Market cap falls to less than $80 million</li>
<li aria-level="2">USC partnership terminated (costing USC $40 million “break fee”)</li>
<li aria-level="2"><a href="https://www.classcentral.com/report/2u-layoffs-2023/">Additional layoffs implemented</a></li>
<li aria-level="2"><a href="https://2u.com/newsroom/2u-inc-announces-leadership-transition/">CEO Chip Paucek steps down</a>, replaced by CFO Paul Lalljie</li>
</ul>
</li>
<li aria-level="1">Q4: 2U takes over programs from <a href="https://www.classcentral.com/report/2u-edx-disastrous-quarter/">Pearson’s OPM business</a></li>
</ul>
<p>2024:</p>
<ul>
<li aria-level="1">January: Another wave of <a href="https://www.classcentral.com/report/2u-layoffs-2024/">layoffs at 2U/edX</a></li>
<li aria-level="1">July: <a href="https://www.wsj.com/business/2u-ed-tech-company-files-chapter-11-bankruptcy-24ca1017">2U files for Bankruptcy</a></li>
</ul>
<h2>2U’s Costly Gamble</h2>
<blockquote><p>As we make all of these changes, I’m proud to say that by year’s end we’ll have answered the question from our IPO eight years ago. Yes, you can build a sustainable education business in higher ed at scale.</p></blockquote>
<p>This quote, made almost a year ago by 2U CEO Chip Paucek, came as he announced a major layoff and the consolidation of different 2U brands under edX. Three months later, he stepped down as 2U CEO and board member.</p>
<p>This type of hubris is something I’ve observed since I began following 2U after their acquisition of edX. 2U envisions a reality that doesn’t exist but acts as if it does.</p>
<p>Here’s an example. In response to a <a href="https://www.wsj.com/articles/that-fancy-university-course-it-might-actually-come-from-an-education-company-11657126489">Wall Street Journal article about 2U’s practices</a> (one of many such articles), Paucek wrote a blog post titled “<a href="https://2u.com/newsroom/dont-let-the-skeptic-win/">Don’t Let the Skeptic Win</a>.” One line caught my attention: “We’ve helped our partners dramatically lower the cost of their degrees”.</p>
<table>
<caption>Cost summary of 2U Degrees</caption>
<thead>
<tr>
<td></td>
<td>Masters</td>
<td>Doctorates</td>
</tr>
</thead>
<tbody>
<tr>
<td>Total</td>
<td>99</td>
<td>12</td>
</tr>
<tr>
<td>Median Price</td>
<td>$66.5K</td>
<td>$106K</td>
</tr>
<tr>
<td>Min Price</td>
<td>$34K</td>
<td>$56K</td>
</tr>
<tr>
<td>Max Price</td>
<td>$126K</td>
<td>$201K</td>
</tr>
</tbody>
</table>
<p>This claim surprised me because a few months later, I investigated the <a href="https://www.classcentral.com/report/2u-online-degrees-cost/">cost of 2U-powered degrees</a>. I found that 20% of these degrees would cost the learner over $100,000, with the most expensive exceeding $200,000.</p>
<p>I would argue the opposite—2U’s business model required degrees to be as expensive as on-campus degrees. Acquiring students is a significant cost for online degrees, and as a company tries to grow to satisfy investors, it spends more to acquire students.</p>
<p>In 2021, 2U spent $456M on sales and marketing, accounting for almost 40% of its expenses. This is why it acquired edX: to optimize these costs and reduce their reliance on paid advertising.</p>
<figure id="attachment_75860" aria-describedby="caption-attachment-75860"><a href="https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-marketing-engine.png"><img loading="lazy" src="https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-marketing-engine.png" alt="2U’s plan: Combining edX marketplace with 2U’s marketing engine" width="1324" height="748" srcset="https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-marketing-engine.png 1324w, https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-marketing-engine-300x169.png 300w, https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-marketing-engine-1024x579.png 1024w, https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-marketing-engine-768x434.png 768w" sizes="(max-width: 1324px) 100vw, 1324px"></a><figcaption id="caption-attachment-75860">2U’s plan: Combining edX marketplace with 2U’s marketing engine</figcaption></figure>
<p>Essentially, edX’s role after the acquisition was to help reduce the cost of acquiring learners for 2U’s various programs: GetSmarter Executive/Professional Education, Trilogy Bootcamps, and 2U-powered online degrees.</p>
<p>Before acquiring edX, 2U’s cost per enrollment was approximately $3,900. They anticipated reducing this cost by 10% by marketing their programs to edX users and would lead to $40–$60M in annual savings.</p>
<p>In my analysis of <a href="https://www.classcentral.com/report/2u-edx-acquisition-analysis/">2U’s acquisition of edX</a>, I had talked about why this model might not work. I highlighted two key issues: the mismatch between 2U’s high-cost programs and edX’s largely international user base, and edX’s own poor track record in converting learners to their more affordable online degrees</p>
<figure id="attachment_87874" aria-describedby="caption-attachment-87874"><a href="https://www.classcentral.com/report/wp-content/uploads/2023/10/edx-fce-2023.jpeg"><img loading="lazy" src="https://www.classcentral.com/report/wp-content/uploads/2023/10/edx-fce-2023.jpeg" alt="" width="2330" height="1340" srcset="https://www.classcentral.com/report/wp-content/uploads/2023/10/edx-fce-2023.jpeg 2330w, https://www.classcentral.com/report/wp-content/uploads/2023/10/edx-fce-2023-300x173.jpeg 300w, https://www.classcentral.com/report/wp-content/uploads/2023/10/edx-fce-2023-1024x589.jpeg 1024w, https://www.classcentral.com/report/wp-content/uploads/2023/10/edx-fce-2023-768x442.jpeg 768w, https://www.classcentral.com/report/wp-content/uploads/2023/10/edx-fce-2023-1536x883.jpeg 1536w, https://www.classcentral.com/report/wp-content/uploads/2023/10/edx-fce-2023-2048x1178.jpeg 2048w" sizes="(max-width: 2330px) 100vw, 2330px"></a><figcaption id="caption-attachment-87874">Degree enrollments drop as edX embraces new marketing strategy (<a href="https://s26.q4cdn.com/441000616/files/doc_financials/2023/q2/2023-08-08-FINAL-2Q-23_Earnings_Presentation.pdf">Source</a>)</figcaption></figure>
<p>These anticipated savings never materialized. In fact, 2U’s enrollments continued to decline after they implemented the “new marketing framework.”</p>
<p>While they were able to reduce their marketing and sales expenses, this resulted in fewer enrollments. Furthermore, any savings were offset by the annual interest payments on the debt incurred to purchase edX.</p>
<p>This financial burden was so significant that it led to three rounds of layoffs. To generate more cash at the end of last year, 2U resorted to the drastic measure of “Portfolio Management,” terminating certain degree partnerships in exchange for breakup fees. Agreements totaling approximately $150 million were finalized by the end of 2023.</p>
<p>When 2U acquired edX, it painted a rosy picture of reduced student acquisition costs and a “platform strategy” that would revolutionize online education. Reality could not have been more different.</p>
<p>The bankruptcy filing in 2024 was the final nail in the coffin, proving that the edX acquisition was not just a misstep, but a catastrophic error that sank the entire company.</p>
<h2>EdX’s Stagnation</h2>
<p>“2U’s people, technology, and scale will expand edX’s ability to deliver on our mission and enable all learners to unlock their potential. #freetodegree”<span>&nbsp;</span></p>
<ul>
<li aria-level="1"><em>Anant Agarwal (@agarwaledu)<a href="https://twitter.com/agarwaledu/status/1409840807155224576?ref_src=twsrc%5Etfw"> June 29, 2021</a></em></li>
</ul>
<p>As misguided as it was, 2U has been very specific about how edX fits into its plans: reducing their user acquisition costs. That’s why 2U spent $800 million in cash to buy edX. EdX is a marketing channel for 2U.</p>
<p>EdX’s reason, on the other hand, was a bit vague. 2U has some marketing expertise that would somehow help edX catch up to Coursera.</p>
<p>Even before the pandemic, Coursera was increasing the gap between itself and edX. The gap only widened after the pandemic. C<a href="https://www.classcentral.com/report/the-second-year-of-the-mooc/">oursera gained almost as many learners in 2020</a> alone as edX did since its launch nine years ago.</p>
<figure id="attachment_75863" aria-describedby="caption-attachment-75863"><a href="https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-broad-strokes-e1625475120473.png"><img loading="lazy" src="https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-broad-strokes-e1625475120473.png" alt="Broad strokes of the 2U + edX future plans" width="1100" height="611" srcset="https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-broad-strokes-e1625475120473.png 1100w, https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-broad-strokes-e1625475120473-300x167.png 300w, https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-broad-strokes-e1625475120473-1024x569.png 1024w, https://www.classcentral.com/report/wp-content/uploads/2021/07/2u-broad-strokes-e1625475120473-768x427.png 768w" sizes="(max-width: 1100px) 100vw, 1100px"></a><figcaption id="caption-attachment-75863">Broad strokes of the 2U + edX future plans</figcaption></figure>
<p>I had argued that edX’s rationale for the acquisition was flawed. 2U’s “marketing engine” is built around higher-priced programs involving high-touch activities like sales. This approach is unlikely to benefit the majority of edX’s lower-priced course offerings or help it catch up to Coursera.</p>
<p>Competing with Coursera would require significant investment – potentially hundreds of millions of dollars – which 2U may struggle to provide given its new debt from the acquisition. Even if 2U had the money, it’s unclear why they would take that risk.</p>
<p>2U acquired edX primarily to leverage its marketplace (the website itself) and popular brand to market expensive programs like boot camps and degrees. This is exactly what happened.</p>
<figure id="attachment_92482" aria-describedby="caption-attachment-92482"><img loading="lazy" src="https://www.classcentral.com/report/wp-content/uploads/2024/08/2U-changes-to-edx-seo-leads.png" alt="The image shows two slides from 2U's Investor Day in 2023, focusing on edX's strategy to grow its user base and the impact on organic prospect volume. The left slide lists URLs for various learning pages on edX's website, such as sustainability, marketing, artificial intelligence, public health, and others, emphasizing the strategy of publishing new pages to attract high-intent audiences. The right slide features a line chart titled &quot;Bootcamp Prospect Volume from edX SEO,&quot; displaying a black line indicating the volume of edX organic prospects over time, with the note &quot;edX Adding 30% Incremental Organic Vol&quot; showing a steady increase from December 2021 to February 2023. The chart highlights a rise in domestic U.S. prospect volume, demonstrating edX's efforts to enhance its SEO performance and attract more users." width="2048" height="1024" srcset="https://www.classcentral.com/report/wp-content/uploads/2024/08/2U-changes-to-edx-seo-leads.png 2048w, https://www.classcentral.com/report/wp-content/uploads/2024/08/2U-changes-to-edx-seo-leads-300x150.png 300w, https://www.classcentral.com/report/wp-content/uploads/2024/08/2U-changes-to-edx-seo-leads-1024x512.png 1024w, https://www.classcentral.com/report/wp-content/uploads/2024/08/2U-changes-to-edx-seo-leads-768x384.png 768w, https://www.classcentral.com/report/wp-content/uploads/2024/08/2U-changes-to-edx-seo-leads-1536x768.png 1536w" sizes="(max-width: 2048px) 100vw, 2048px"><figcaption id="caption-attachment-92482"><em>2U’s 2023 Investor Day slides reveal edX’s SEO strategy and bootcamp lead growth.</em></figcaption></figure>
<p>Most programs now featured on edX’s homepage were previously 2U offerings. In fact, 2U rebranded their bootcamps as “<a href="https://press.edx.org/edx-boot-camps">edX bootcamps</a>.”</p>
<p>The rebranding of Trilogy Education’s (acquired for $750M) boot camps as “edX bootcamps” is now backfiring, as <a href="https://feweek.co.uk/ofsted-slates-us-firm-with-5m-dfe-bootcamps-contract/">recent Ofsted reports</a> criticized the program’s management and outcomes, leading to negative publicity. These shortcomings have led to low course completion rates and few learners securing jobs.</p>
<p>This mismanagement reflects poorly on the edX brand, potentially eroding its reputation for quality education. The association of the edX name with these problematic bootcamps amplifies the negative impact.</p>
<p>From an outside perspective, it appeared that a significant portion of edX’s efforts went into promoting 2U programs. Unfortunately for both 2U and edX, <a href="https://www.classcentral.com/report/2u-stock-drop/">edX’s declining SEO performance</a> made it increasingly difficult for 2U to monetize its $800 million acquisition effectively.</p>
<figure id="attachment_92494" aria-describedby="caption-attachment-92494"><img loading="lazy" src="https://www.classcentral.com/report/wp-content/uploads/2024/08/anant-linkedin-bio-2u.png" alt="" width="1024" height="512" srcset="https://www.classcentral.com/report/wp-content/uploads/2024/08/anant-linkedin-bio-2u.png 1024w, https://www.classcentral.com/report/wp-content/uploads/2024/08/anant-linkedin-bio-2u-300x150.png 300w, https://www.classcentral.com/report/wp-content/uploads/2024/08/anant-linkedin-bio-2u-768x384.png 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption id="caption-attachment-92494">Anant Agarwal LinkedIn bio.</figcaption></figure>
<p>With each round of layoffs and reorganization, edX’s CEO, Anant Agarwal, received a new title at 2U, progressing from Chief Open Education Officer to Chief Platform Officer and finally to Chief Academic Officer. It’s unclear how many of the original edX crew remain. I know that many of them have left or been laid off.</p>
<p>While trying to locate the post about the acquisition on edX’s blog, I realized that the entire edX blog has been removed. However, you can still read the post on the <a href="https://web.archive.org/web/20221207072316/https://blog.edx.org/our-shared-mission-a-strong-foundation-for-an-exciting-future">Internet Web Archive</a>.</p>
<p>The very qualities that made edX unique—its non-profit status—were eroded, leaving it a mere shadow of its former self. Its future now rests in the hands of 2U’s creditors.</p>
<h2>Axim Collaborative’s Hollow Legacy</h2>
<figure id="attachment_92492" aria-describedby="caption-attachment-92492"><img loading="lazy" src="https://www.classcentral.com/report/wp-content/uploads/2024/08/axim-homepage.png" alt="" width="1024" height="512" srcset="https://www.classcentral.com/report/wp-content/uploads/2024/08/axim-homepage.png 1024w, https://www.classcentral.com/report/wp-content/uploads/2024/08/axim-homepage-300x150.png 300w, https://www.classcentral.com/report/wp-content/uploads/2024/08/axim-homepage-768x384.png 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption id="caption-attachment-92492">Axim Collaborative homepage.</figcaption></figure>
<p><a href="https://www.axim.org/">Axim Collaborative</a> is the non-profit organization that retained the ~$800 million from the 2U acquisition. It’s the only cash-rich entity in this story, yet three years later, it has little to show for its wealth.</p>
<p>To clarify the somewhat confusing sequence of events:</p>
<ol>
<li aria-level="1">The original non-profit edX sold its brand and most assets to 2U.</li>
<li aria-level="1">The remaining non-profit entity was temporarily renamed “The Center for Reimagining Learning.”</li>
<li aria-level="1">Last year, this organization was officially named Axim Collaborative and appointed a new CEO.</li>
</ol>
<p>Organizationally, Axim Collaborative is essentially the continuation of the original edX non-profit, minus the assets sold to 2U. It kept the sale proceeds and the Open edX platform, which wasn’t part of the acquisition.</p>
<p>Notably, some of the same MIT and Harvard leaders who made the decision to sell edX now sit on Axim’s board. These are the same individuals responsible for the disastrous sale.</p>
<p>During the original acquisition announcement, the non-profit was <a href="https://news.mit.edu/2021/mit-harvard-transfer-edx-2u-0629">positioned ambitiously</a>:</p>
<p>Backed by these substantial resources, the nonprofit will focus on overcoming persistent inequities in online learning, in part through exploring how to apply artificial intelligence to enable personalized learning that responds and adapts to the style and needs of the individual learner.<span>&nbsp;</span></p>
<p>However, <a href="https://projects.propublica.org/nonprofits/organizations/460807740">recent tax returns</a> suggest another reality. According to its 2023 Tax Return (fiscal year ending in June 2023), Axim is sitting on $735 million. In FY 2022, it made $15 million from investment income and had expenses of $9 million.</p>
<p>The promised focus on artificial intelligence and personalized learning seems to have evaporated, especially ironic given the recent AI boom.</p>
<p>Axim appears to have become primarily a grant-giving organization. Besides supporting Open edX, there’s little evidence of using its “substantial resources” for innovation as initially promised.</p>
<p>For perspective, Axim’s current assets exceed the total amount edX spent during its entire non-profit phase.</p>
<p>Instead of being an innovator, Axim Collaborative seems to be a non-entity in the edtech space, its promises of innovation and equity advancement largely unfulfilled.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Predicting the Future of Distributed Systems (155 pts)]]></title>
            <link>https://blog.colinbreck.com/predicting-the-future-of-distributed-systems/</link>
            <guid>41363499</guid>
            <pubDate>Tue, 27 Aug 2024 00:26:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.colinbreck.com/predicting-the-future-of-distributed-systems/">https://blog.colinbreck.com/predicting-the-future-of-distributed-systems/</a>, See on <a href="https://news.ycombinator.com/item?id=41363499">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        



<main id="site-main">
<article>

    <header>


        


        

            <figure>
                <img srcset="https://blog.colinbreck.com/content/images/size/w300/2024/08/tuscan-doors-1.jpg 300w,
                            https://blog.colinbreck.com/content/images/size/w600/2024/08/tuscan-doors-1.jpg 600w,
                            https://blog.colinbreck.com/content/images/size/w1000/2024/08/tuscan-doors-1.jpg 1000w,
                            https://blog.colinbreck.com/content/images/size/w2000/2024/08/tuscan-doors-1.jpg 2000w" sizes="(min-width: 1400px) 1400px, 92vw" src="https://blog.colinbreck.com/content/images/size/w2000/2024/08/tuscan-doors-1.jpg" alt="Predicting the Future of Distributed Systems">
            </figure>

    </header>

    <section>
        <p>There are significant changes happening in distributed systems. Object storage is becoming the database, tools for transactional processing and analytical processing are becoming one in the same, and there are new programming models that promise some combination of superior security, portability, management of application state, or simplification. These changes will influence how systems are operated in addition to how they are programmed. While I want to embrace many of these innovations, it can be hard to pick a path forward.</p>
<p>If a new technology only provides incremental value, most people will question the investment. Even when a technology promises a step change in value, it can be difficult to adopt if there is no migration path, and risky if it will be difficult to change if it ends up being the wrong investment. Many transactional and analytical systems are starting to use object storage because there is a clear step-change in value, and optionality is mitigating many of the risks. However, while I appreciate the promise of new programming models, the path forward is a lot harder to grasp. If people can’t rationalize the investment, most will keep doing what they already know. As the saying goes, nobody gets fired for buying IBM.</p>
<p>I have been anticipating changes in transactional and analytical systems for a few years, especially around object storage and programming models, not because I’m smarter or better at predicting the future, but because I have been lucky enough to be exposed to some of them. While I cannot predict the future, I will share how I’m thinking about it.</p>
<h2 id="one-way-door-and-two-way-door-decisions">One-Way-Door and Two-Way-Door Decisions</h2>
<p>On <a href="https://www.youtube.com/watch?v=DcWqzZ3I2cY&amp;t=3565s">Lex Fridman’s podcast</a>,<sup><a href="#fn1" id="fnref1">[1]</a></sup> Jeff Bezos described how he manages risk from the perspective of one-way-door decisions and two-way-door decisions. A one-way-door decision is final, or takes significant time and effort to change. For these decisions, it is important to slow down, ensure you are involving the right people, and gather as much information as possible. These decisions should be guided by executive leadership.<sup><a href="#fn2" id="fnref2">[2]</a></sup></p>
<blockquote>
<p>Some decisions are so consequential and so important and so hard to reverse that they really are one-way-door decisions. You go in that door, you’re not coming back. And those decisions have to be made very deliberately, very carefully. If you can think of yet another way to analyze the decision, you should slow down and do that.<br>
—Jeff Bezos</p>
</blockquote>
<p>A two-way-door decision is less consequential. If you make the wrong decision, you can always come back and choose another door. These decisions should be made quickly, often by individuals or small teams. Executives should not waste time on two-way-door decisions. In fact, doing so destroys agency.<sup><a href="#fn3" id="fnref3">[3]</a></sup></p>
<blockquote>
<p>What can happen is that you have a one-size-fits-all decision-making process where you end up using the heavyweight process on all decisions, including the lightweight ones—the two-way-door decisions.<br>
—Jeff Bezos</p>
</blockquote>
<p>It is critical that organizations correctly identify one-way-door and two-way-door decisions. It is costly to apply a heavyweight decision-making process to two-way-door decisions, but even more costly is a person or a small team making what they think is a two-way-door decision when it is really a one-way-door decision now imposed on the whole organization for years to come. Many technology choices are one-way-door decisions because they require significant investments and are costly and time-consuming to change.<sup><a href="#fn4" id="fnref4">[4]</a></sup></p>
<h2 id="object-storage">Object Storage</h2>
<p>Cloud object storage is almost two decades old. While it is very mature and incredibly reliable and durable, it continues to see a lot of innovation.<sup><a href="#fn5" id="fnref5">[5]</a></sup> With the amount of attention to backwards compatibility, systems integration, and interoperability, almost every investment in object storage feels like a two-way-door decision, and this will continue to accelerate adoption, investment, and innovation.<sup><a href="#fn6" id="fnref6">[6]</a></sup></p>
<p>Over a decade ago, I built a durable message queue for sharing industrial data between enterprises using Azure Blob Storage page blobs. Because the object storage provided object leases, atomic writes, durability, and replication, it allowed us to build a simple and reliable system without having to worry about broker leadership election, broker quorums, data replication, data synchronization, or other challenges. The read side was independent of the write side, stateless, and could be scaled completely independently. While the company I was working for couldn’t figure out how to leverage this infrastructure to its fullest extent, I knew that relying on object storage was an architecture that had many advantages and I expected to encounter it again.<sup><a href="#fn7" id="fnref7">[7]</a></sup></p>
<p>Fast forward to today and there are many systems—everything from relational databases, time-series databases, message queues, data warehouses, and services for application metrics—using object storage as a core part of their architecture, including transactional workloads and not just analytical workloads, archival storage, or batch processing.<sup><a href="#fn8" id="fnref8">[8]</a></sup> In addition, object storage features have expanded to include cross-region replication, immutability, object versioning, tiered storage, backup, read-after-write consistency, conditional writes,<sup><a href="#fn9" id="fnref9">[9]</a></sup> encryption, metadata, authorization, and more. These features can be used to address industry regulation, compliance, cost optimization, data lifecycle management, disaster recovery, and much more, and in a standard way across services, without having to build it directly into each application. So not only is object storage attractive from an architectural perspective, it is a win for simplicity and consistency.</p>
<p>While I can’t predict exactly how object storage will evolve, I expect the popularity of object storage to increase, especially for transactional and analytical systems. For example, storing data in Parquet files in Amazon S3 feels like a pretty safe bet. I expect read performance will continue to improve through reduced latency, increased bandwidth, improved caching, or better indexing, because it is something that will benefit the huge numbers of applications using S3.<sup><a href="#fn10" id="fnref10">[10]</a></sup> If another storage format becomes more attractive than Parquet, I trust I can use an open table format, like Apache Iceberg or Delta Lake, to manage this evolution if I don’t want to reprocess the historical data. If I do want to reprocess the data, I can rely on the elasticity of cloud infrastructure to reprocesses files when they are accessed, or as a one-time batch job. I’m not worried about choosing an open table format, because they all seem excellent, they are converging on a similar set of features, and they will undoubtedly support interoperability and migration. Similarly, if I rely on an embedded library for query optimization and processing, like DuckDB or Apache DataFusion,<sup><a href="#fn11" id="fnref11">[11]</a></sup> I expect them to continue to improve and share similar features.<sup><a href="#fn12" id="fnref12">[12]</a></sup> In other situations, I might rely on Amazon Athena, Trino, Apache Spark, Pandas, or Polars for data processing. Tools will continue to improve for importing data from, or exporting data to, relational databases, data warehouses, and time-series databases. If I want to run the same services using another cloud provider, or in my own datacenter, there are other object storage services that have S3-compatible APIs.<sup><a href="#fn13" id="fnref13">[13]</a></sup> In other words, lots and lots of two-way doors. Actually, it is an embarrassment of riches.</p>
<p>Object storage is also a very simple storage abstraction. Embedded data processing libraries, like DuckDB and Apache DataFusion, can use the local file system interchangeably with object storage. This opens up the opportunity to move workloads from distributed cloud computing infrastructure and embed them directly in a single server, or move them client-side, embedded in a web browser, or even embedded into IoT devices or industrial equipment controlling critical infrastructure.<sup><a href="#fn14" id="fnref14">[14]</a></sup> The ability to move workloads around to meet changing requirements for availability, scalability, cost, locality, durability, latency, privacy, and security opens up even more two-way doors. With object storage, it’s two-way doors all the way down.</p>
<h2 id="programming-models">Programming Models</h2>
<p>The most disruptive change in the next decade may be how we program systems—a fundamental change in how software is developed and operated—and even what we view as software and what we view as infrastructure—that most people have yet to grasp.<sup><a href="#fn15" id="fnref15">[15]</a></sup><sup><a href="#fn16" id="fnref16">[16]</a></sup> Many fail to see the value, and almost everyone is skeptical of how we get from here to there. While I believe the eventual outcomes are clear, the path forward is anything but. The fact that everything seems like a one-way door is hindering adoption.</p>
<p>I have been anticipating a shift in programming models for many years, not through any great insight of my own, but through my experiences building systems with Akka, a toolkit for distributed computing, including actor-model programming and stream processing. I saw how these primitives solved the challenges I had been working on for fifteen years in industrial computing—flow control, bounded resource constraints, state management, concurrency, distribution, scaling, and resiliency—and not just in logical ways, but from first principles. For example, actors can provide a means of modelling entities, like IoT devices, and managing state, but leave the execution and distribution of those entities up to the run-time, and in a thread-safe way. Reactive Streams provides a way to interface and interoperate systems, expressing the logic of the program, while letting the run-time handle the system dynamics in a reliable way. I could see how these models would logically extend to stateful functions and beyond, as I described in my keynote talk <a href="https://www.youtube.com/watch?v=Ifaqjop1gzU">From Fast-Data to a Key Operational Technology for the Enterprise</a> in 2018.</p>
<p>Today, there are many systems trying to solve these challenges from one perspective or another. If you squint, they break down into roughly three categories. The first category are systems that abstract the most difficult parts of distributed systems, like managing state, workflows, and partial failures. These are systems like Kalix, Dapr, Temporal, Restate, and a few others. These systems generally involve adopting the platform APIs in your programming language of choice. In the second category, in addition to abstracting some of the difficult parts of distributed systems, the platform will execute arbitrary code in the form of a binary, a container, or WebAssembly. Included in this category are wasmCloud, NATS Execution Engine,<sup><a href="#fn17" id="fnref17">[17]</a></sup> Spin, AWS Fargate, and others. The final category are the somewhat uncategorizable because they are so unique, like Golem, which, if I understand correctly, uses the stack-based WebAssembly virtual machine to execute programs durably,<sup><a href="#fn18" id="fnref18">[18]</a></sup> and Unison, which is an entirely new programing language and run-time environment.</p>
<p>However attractive or well engineered these solutions are, ten years from now, not all of these technologies, or the companies developing them, will exist. Even with the promise of solving important problems and accelerating organizations, it is nearly impossible to pick a technology because of this huge investment risk. Furthermore, so much of what matters is the quality and maturity of the tools for building, deploying, static analysis, debugging, performance analysis and all the rest, and most engineers are uncomfortable giving up control over the whole stack.<sup><a href="#fn19" id="fnref19">[19]</a></sup> Adding to the skepticism are questions about how AWS, Azure, Cloudflare, and the other cloud service providers will enter this market with their own integrated and potentially ubiquitous solutions. At the moment, it seems like one-way door after one-way door.</p>
<p>As I see it, the biggest opportunity for a new programming model is extracting the majority of the code from an application and moving it into the infrastructure instead. The second biggest opportunity is for the remaining code—what people refer to as the business logic, the essence of the program—to be portable and secure. A concrete example will help demonstrate how I’m thinking about the future.</p>
<p>In addition to the business logic, embedded in almost all modern programs are HTTP or gRPC servers for client requests, libraries for logging and metrics, clients for interfacing with databases, object storage, message queues, and lots more. Depending on when each application was last updated, built, and deployed, there will be many versions of this auxiliary code running in production. To patch a critical security vulnerability, just finding the affected services can be an enormous undertaking.<sup><a href="#fn20" id="fnref20">[20]</a></sup> Most organizations do not have mature software inventories, but even if they do, the inventory only helps with identifying the services, they still need to be updated, built, tested, and redeployed. Instead of embedding HTTP servers and logging libraries and database clients and all the rest into an application binary, if this code can move down into the infrastructure, then these resources can be isolated, secured, monitored, scaled, inventoried, and patched independently from application code, very similar to how monitoring, upgrading, securing, and patching servers underneath a Kubernetes cluster is transparent to the application developer today.<sup><a href="#fn21" id="fnref21">[21]</a></sup> If the business logic can be described and executed like this, then it also becomes possible to move code between environments, like between the cloud and the IoT edge, or between service providers.<sup><a href="#fn22" id="fnref22">[22]</a></sup></p>
<p>To encourage adoption, new programming models must find ways to transform the one-way-door decisions into two-way-door decisions. WebAssembly may help with this. WebAssembly offers a secure way to run portable code, and the WebAssembly Component Model could be the basis of a standard set of interfaces that more than one platform can provide.<sup><a href="#fn23" id="fnref23">[23]</a></sup> There may be other ways these platforms can encourage adoption by lowering risk, but the two most important things to me are: 1) not having to rewrite every application—in other words, some kind of migration path, rather than only greenfield adoption<sup><a href="#fn24" id="fnref24">[24]</a></sup> and 2) not being locked into a single provider should I want to move to a different platform, or move workloads from the cloud to my own datacenter, or into embedded IoT.</p>
<h2 id="what-is-the-future">What is the Future?</h2>
<p>There are major shifts happening in the software industry. In the future, distributed systems will look different. The decomposition of databases, transactional systems, and operational technology to incorporate object storage is well underway thanks to many two-way doors. New programming models could be very disruptive, but with so many one-way doors, the challenge of picking the technology winners and losers has never been harder. It is easier to keep doing what we already know.</p>
<blockquote>
<p>In a distributed system, there is no such thing as a perfect failure detector.<br>
We can’t hide the complexity...our abstractions are going to leak.<br>
—<a href="https://www.youtube.com/watch?v=A-frep1y80s&amp;t=286s">Peter Alvaro</a></p>
</blockquote>
<p>Programming a distributed system is hard because of the challenge of partial failures. Arguably, the success of object storage is partly due to abstractions that don’t hide all of the complexity. It remains to be seen how well new programming models can deal with partial failures without contorting the programming model itself. But these new systems are promising because they are getting back to basics, just with the lines of abstraction drawn in different places. This should result in systems that are simpler, more modular, with better separation of concerns, that are much easier to build, operate, maintain, secure, and scale. Perhaps the biggest question is, will the early adopters out-compete the others? Or will the rest of the industry catch up quickly once the new programming and operational models become clear? How safe is it to just keep doing what we already know?</p>
<blockquote>
<p>Abstractions are going to leak, so make the abstractions fluid.<br>
—<a href="https://bravenewgeek.com/abstraction-considered-harmful/">Peter Alvaro</a></p>
</blockquote>
<p>It is impossible to predict the future and I’m not going to pretend I can foresee it better than anyone else. However, I am confident in the macro trends of continued investment in object storage and, some day, the widespread adoption of new programming models that move more code down into the infrastructure. It will be fun to look back in a few years.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>This is an excellent podcast on the leadership of complex organizations and products. Another highlight for me is Jeff <a href="https://www.youtube.com/watch?v=DcWqzZ3I2cY&amp;t=7171s">describing his meeting culture</a>. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>The <a href="https://s2.q4cdn.com/299287126/files/doc_financials/annual/2015-Letter-to-Shareholders.PDF">2015 Amazon Letter to Shareholders</a> also described the concept of one-way and two-way doors. Thanks to my friend John Mayerhofer for suggesting I incorporate this idea in this essay. <a href="#fnref2">↩︎</a></p>
</li>
<li id="fn3"><p>For more on agency, see my essay <a href="https://blog.colinbreck.com/the-importance-of-agency/">The Importance of Agency</a>. <a href="#fnref3">↩︎</a></p>
</li>
<li id="fn4"><p>I’ve been through many platform and service migrations. Even when they are the right or necessary, they can take years. In his book <em>An Elegant Puzzle</em>, Will Larson notes that growing organizations will always be in the middle of a migration—“growth makes migrations a way of life”—and it is important to be good at them: “If you don’t get effective at software and systems migrations, you’ll end up languishing in technical debt.” <a href="#fnref4">↩︎</a></p>
</li>
<li id="fn5"><p>For a deep dive into the history and incredible engineering of Amazon S3, see Andy Warfield’s article and talk: <a href="https://www.allthingsdistributed.com/2023/07/building-and-operating-a-pretty-big-storage-system.html">Building and operating a pretty big storage system called S3</a>. Marc Olson also published a deep dive into block storage: <a href="https://www.allthingsdistributed.com/2024/08/continuous-reinvention-a-brief-history-of-block-storage-at-aws.html">Continuous reinvention: A brief history of block storage at AWS</a>. <a href="#fnref5">↩︎</a></p>
</li>
<li id="fn6"><p>Because of the volume of data stored in cloud object storage, it is difficult and expensive to migrate to alternative solutions. In a very positive way, this forces vendors to pay a lot of attention to migration paths and backwards compatibility. <a href="#fnref6">↩︎</a></p>
</li>
<li id="fn7"><p>For more information on this architecture, see <a href="https://blog.colinbreck.com/shared-nothing-architectures-for-server-replication-and-synchronization/">Shared-Nothing Architectures for Server Replication and Synchronization</a>. <a href="#fnref7">↩︎</a></p>
</li>
<li id="fn8"><p>Examples include Amazon Aurora, InfluxDB, WarpStream, Snowflake, and Grafana Mimir. See Chris Riccomini’s article <a href="https://materializedview.io/p/databases-are-falling-apart">Databases Are Falling Apart: Database Disassembly and Its Implications</a> for a comprehensive exploration of this topic. <a href="#fnref8">↩︎</a></p>
</li>
<li id="fn9"><p>Azure Blob Storage has supported conditional writes for many years. <a href="https://aws.amazon.com/about-aws/whats-new/2024/08/amazon-s3-conditional-writes/">Amazon S3 just added this feature</a>. <a href="#fnref9">↩︎</a></p>
</li>
<li id="fn10"><p>For example, Amazon recently introduced S3 Express One Zone which offers millisecond latency, the AWS Common Runtime (CRT) libraries which can saturate network bandwidth for S3 file transfer, and Mountpoint for Amazon S3 for mounting S3 buckets on the local file system. It is also possible to rely on the cloud provider’s infrastructure and optimizations, many of which are only possible at scale, rather than solving these problems yourself. See <a href="https://www.youtube.com/watch?v=sc3J4McebHE&amp;t=1333s">this example</a> from Andy Warfield. <a href="#fnref10">↩︎</a></p>
</li>
<li id="fn11"><p>DuckDB and Apache DataFusion are both incredibly high-quality open-source projects and both are lead by incredibly good engineers. These libraries are blurring what can be done inside a database versus outside a database. Databases and object stores are starting to meet in the middle. See the talks <a href="https://www.youtube.com/watch?v=bZOvAKGkzpQ">DuckDB Internals</a> by <a href="https://mytherin.github.io/">Mark Raasveldt</a> and <a href="https://www.youtube.com/watch?v=I-Z7kFGsYRI">Building InfluxDB 3.0 with Apache Arrow, DataFusion, Flight and Parquet</a> by <a href="http://andrew.nerdnetworks.org/">Andrew Lamb</a>. <a href="#fnref11">↩︎</a></p>
</li>
<li id="fn12"><p>There can be too much focus on performance, rather than reliability, security, ease of use, and other important factors. Databases and query optimizers tend to converge on performance, because as soon as one discovers a performance optimization, the others also implement it. In addition, most innovations in databases and query optimizers outside of SQL tend to eventually be implemented in SQL. For more on these topics, see the article <a href="https://motherduck.com/blog/perf-is-not-enough/">Perf Is Not Enough</a> by Jordan Tigani and the talk <a href="https://www.youtube.com/watch?v=-wCzn9gKoUk">A Short Summary of the Last Decades of Data Management</a> by Hannes Mühleisen. <a href="#fnref12">↩︎</a></p>
</li>
<li id="fn13"><p>For example, Cloudflare R2 and MinIO. <a href="#fnref13">↩︎</a></p>
</li>
<li id="fn14"><p>I expand on this topic in my position paper <a href="https://blog.colinbreck.com/object-storage-and-in-process-databases-are-changing-distributed-systems/">Object Storage and In-Process Databases are Changing Distributed Systems</a>. <a href="#fnref14">↩︎</a></p>
</li>
<li id="fn15"><p>Software developers are often accused of wanting to try the latest fads and tinker with their code until it is perfect. In an industry that changes rapidly, it is important to evaluate trends, but for the vast majority of developers I have worked with, they are all focused on shipping—satisfaction only comes when their work is in the hands of customers and providing value. Perhaps because I’ve always worked on industrial software, the engineers I have worked with have also preferred simple and practical solutions that maximize reliability. If I reflect on the small number of people I have seen struggle—people who could be accused of tinkering or preferring complexity—it is because either the objectives of the work were not clear, or the individuals lacked engineering skills to complement their programming skills. <a href="#fnref15">↩︎</a></p>
</li>
<li id="fn16"><p>One of the last big changes in infrastructure was the emergence of Kubernetes over Mesos, YARN, Ansible, Chef, and other technologies for managing the infrastructure itself. See the <a href="https://x.com/jboner/status/783727538652614656">Deployment Coolness Specturm</a> from Jay Kreps. Kubernetes has accelerated many organizations by decoupling the management of infrastructure from the development of software, especially when managing infrastructure across multiple environments or teams. <a href="#fnref16">↩︎</a></p>
</li>
<li id="fn17"><p>My impression is NATS included containers not to be the next container run-time, but because people are not yet ready for the leap to WebAssembly. <a href="#fnref17">↩︎</a></p>
</li>
<li id="fn18"><p>Golem can make imperative code resilient because Golem itself uses event sourcing for the deterministic WebAssembly instructions. It remains to be seen how Golem will adapt to multi-threaded WebAssembly. See the talk <a href="https://www.youtube.com/watch?v=fHPYetd3q2g">Building Durable Microservices with WebAssembly by John A. De Goes</a> for more information. <a href="#fnref18">↩︎</a></p>
</li>
<li id="fn19"><p>In considering these new programming models, we would be well served not to forget Joel Spolsky’s essay <a href="https://www.joelonsoftware.com/2002/02/13/the-iceberg-secret-revealed/">The Iceberg Secret, Revealed</a>. <a href="#fnref19">↩︎</a></p>
</li>
<li id="fn20"><p>This is what organizations experienced as they scrambled to patch the Log4J <a href="https://en.wikipedia.org/wiki/Log4Shell">Log4Shell</a> remote code execution vulnerability. <a href="#fnref20">↩︎</a></p>
</li>
<li id="fn21"><p>This point is better illustrated visually: <a href="https://www.youtube.com/watch?v=oRuSX-FYybU&amp;t=336s">see this part of my talk from S4</a>. <a href="#fnref21">↩︎</a></p>
</li>
<li id="fn22"><p>It also becomes possible to experiment with techniques for continuous improvement to optimize performance or cost. This is a common technique in the process industries that I wrote about in <a href="https://blog.colinbreck.com/observations-on-observability/">Observations on Observability</a>. <a href="#fnref22">↩︎</a></p>
</li>
<li id="fn23"><p>The interfaces could include object storage, relational databases, key-value stores, message queues, application logs, service metrics, authentication, and authorization. <a href="#fnref23">↩︎</a></p>
</li>
<li id="fn24"><p>I manged to make it this far into an essay about the future of programming distributed systems without mentioning generative artificial intelligence (AI) or large language models (LLMs). Perhaps they have a place in rewriting or porting code as recently described by <a href="https://x.com/ajassy/status/1826608791741493281">Andy Jassy</a>. <a href="#fnref24">↩︎</a></p>
</li>
</ol>
</section>

    </section>


</article>
</main>





    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The End of Finale (120 pts)]]></title>
            <link>https://www.finalemusic.com/blog/end-of-finale-new-journey-dorico-letter-from-president/</link>
            <guid>41363231</guid>
            <pubDate>Mon, 26 Aug 2024 23:34:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.finalemusic.com/blog/end-of-finale-new-journey-dorico-letter-from-president/">https://www.finalemusic.com/blog/end-of-finale-new-journey-dorico-letter-from-president/</a>, See on <a href="https://news.ycombinator.com/item?id=41363231">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
			<main id="main" role="main">

			
<article id="post-23000">
	

	<section>
                <p><img width="1024" height="512" src="https://www.finalemusic.com/wp-content/uploads/2024/08/Finale_Sunset_Blog@2x-1.png" alt="" decoding="async" fetchpriority="high" srcset="https://www.finalemusic.com/wp-content/uploads/2024/08/Finale_Sunset_Blog@2x-1.png 2801w, https://www.finalemusic.com/wp-content/uploads/2024/08/Finale_Sunset_Blog@2x-1.png?resize=300,150 300w, https://www.finalemusic.com/wp-content/uploads/2024/08/Finale_Sunset_Blog@2x-1.png?resize=768,384 768w, https://www.finalemusic.com/wp-content/uploads/2024/08/Finale_Sunset_Blog@2x-1.png?resize=1024,512 1024w, https://www.finalemusic.com/wp-content/uploads/2024/08/Finale_Sunset_Blog@2x-1.png?resize=1536,768 1536w, https://www.finalemusic.com/wp-content/uploads/2024/08/Finale_Sunset_Blog@2x-1.png?resize=2048,1024 2048w" sizes="(max-width: 1024px) 100vw, 1024px">        </p>
        
		<p><em>Updates on this announcement are ongoing. Check the Updates section at the bottom of this post for the most up-to-date information.</em></p>

<p>35 years ago, Coda Music Technologies, now MakeMusic, released the first version of Finale, a groundbreaking and user-centered approach to notation software. For over four decades, our engineers and product teams have passionately crafted what would quickly become the gold standard for music notation.</p>
<p>Four decades is a very long time in the software industry. Technology stacks change, Mac and Windows operating systems evolve, and Finale’s millions of lines of code add up. This has made the delivery of incremental value for our customers exponentially harder over time.</p>
<p>Today, Finale is no longer the future of the notation industry—a reality after 35 years, and I want to be candid about this. Instead of releasing new versions of Finale that would offer only marginal value to our users, we’ve made the decision to end its development.</p>
<p>Effective immediately, we are announcing these changes:</p>
<ul>
<li>There will be no further updates to Finale, or any of its associated tools (PrintMusic, Notepad, Songwriter)</li>
<li>It is no longer possible to purchase or upgrade Finale in the MakeMusic eStore</li>
<li>Finale will continue to work on devices where it is currently installed (barring OS changes)</li>
</ul>
<p>After one year, beginning August 2025, these changes will go into effect:</p>
<ul>
<li>It will not be possible to authorize Finale on any new devices, or reauthorize Finale</li>
<li>Technical support for Finale v27 or any other version of Finale will no longer be available</li>
</ul>
<h2>A new journey with Dorico</h2>
<p>There is, however, a new future for the notation industry: Dorico, developed by Steinberg, the creators of Cubase and a subsidiary of Yamaha.</p>
<p>Many have competed with Finale over the past four decades, enabling positive and healthy stimulation leading to incremental innovations, but when Dorico launched in 2016, it set a brand new bar for the industry. The dozens of quick releases since then have demonstrated the Dorico team’s passion, dedication, expertise, and long-term commitment—qualities that have been the foundation of Finale’s DNA and success.</p>
<p>While Finale development has come to an end, we know your musical journey has not. I want to sincerely express our warm and deep gratitude to all of our loyal and passionate users. Our entire organization thanks you for your trust, and we believe you have a bright new phase of creativity, productivity, and efficiency ahead with Dorico.</p>
<p>To ensure that Finale users continue to have access to the most advanced and efficient tools available, MakeMusic has partnered with Steinberg to offer an exclusive discount on Dorico Pro.</p>
<p><img decoding="async" src="http://www.finalemusic.com/wp-content/uploads/2024/08/Finale_Dorrico_color@2x.png"><br>
<b>For a limited time, users of any version of Finale or PrintMusic can purchase Dorico Pro – the highest tier of the product – for just $149 (retail price $579).</b></p>
<p>Dorico is the best home for Finale users. We know that migrating to a new software will come with its own challenges, which is why we are happy to provide this <a href="https://makemusic.zendesk.com/hc/en-us/articles/25843888130839-Finale-Sunset-FAQ">FAQ</a> that will assist you as you transition from Finale.</p>
<p>To receive your discount on Dorico Pro 5, <a href="https://store.makemusic.com/Store/">please log into your MakeMusic account</a>, where you will see the discount available for purchase.</p>
<h3>Updates</h3>
<p>8/27/2024:</p>
<p><strong>Clarifications on the initial announcement</strong></p>
<ol>
<li aria-level="1"><b>Finale development has ended, but the Finale installer for any previously purchased version can still be downloaded from your </b><a href="https://store.makemusic.com/MyAccount/MySoftware.aspx"><b>eStore</b></a><b> account.</b><span> If your computer crashes or you need to install Finale on a new device, you’re not left without options.&nbsp;</span></li>
<li aria-level="1"><span>We are committed to keeping the authorization process functional for a year. <strong>We’ve heard your concerns and are actively exploring ways to extend flexibility in the weeks ahead.</strong></span></li>
<li aria-level="1"><span>We understand that learning Dorico will be a steep learning curve, as it is with any complex notation or professional software. Both our team and Steinberg have developed </span><a href="https://www.youtube.com/playlist?list=PLoyaeouPUsds4tcAcLTR1lrrvloMkGPo_"><span>extensive onboarding videos</span></a><span> to guide you through the transition.&nbsp;</span></li>
</ol>
    </section>

	<!-- .entry-footer -->
</article><!-- #post-## -->
				
				
			
			</main>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Erasure Coding for Distributed Systems (266 pts)]]></title>
            <link>https://transactional.blog/blog/2024-erasure-coding</link>
            <guid>41361281</guid>
            <pubDate>Mon, 26 Aug 2024 20:03:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://transactional.blog/blog/2024-erasure-coding">https://transactional.blog/blog/2024-erasure-coding</a>, See on <a href="https://news.ycombinator.com/item?id=41361281">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <hr>
    
    
    
    <div id="toc">

<ul>
<li><a href="#_erasure_coding_basics">Erasure Coding Basics</a></li>
<li><a href="#_applications_in_distributed_systems">Applications in Distributed Systems</a>
<ul>
<li><a href="#_space_and_tail_latency_improvements">Space and Tail Latency Improvements</a></li>
<li><a href="#_quorum_systems">Quorum Systems</a></li>
</ul>
</li>
<li><a href="#_usage_basics">Usage Basics</a></li>
<li><a href="#_usage_not_so_basics">Usage Not So Basics</a>
<ul>
<li><a href="#_decoding_cost_variability">Decoding Cost Variability</a></li>
<li><a href="#_library_differences">Library Differences</a></li>
</ul>
</li>
<li><a href="#_implementing_erasure_codes">Implementing Erasure Codes</a>
<ul>
<li><a href="#_algorithmic_efficiency">Algorithmic Efficiency</a></li>
<li><a href="#_implementation_efficiency">Implementation Efficiency</a></li>
</ul>
</li>
<li><a href="#_references">References</a></li>
</ul>
</div>
<div id="preamble">
  <p>Suppose one has \(N\) servers across which to store a file.  One extreme is to give each of the \(N\) servers a full copy of the file.  Any server can supply a full copy of the file, so even if \(N-1\) servers are destroyed, then the file hasn’t been lost.  This provides the best durability and fault tolerance but is the most expensive in terms of storage space used.  The other extreme is to carve the data up into \(N\) equal-sized chunks, and give each server one chunk.  Reading the file will require reading all \(N\) chunks and reassembling the file.  This will provide the best cost efficiency, as each server can contribute to the file read request while using the minimum amount of storage space.</p>
<p>Erasure codes are the way to more generally describe the space of trade-offs between storage efficiency and fault tolerance.  One can say "I’d like this file carved into \(N\) chunks, such that it can still be reconstructed with any \(M\) chunks destroyed", and there’s an erasure code with those parameters which will provide the minimum-sized chunks necessary to meet that goal.</p>
<p>The simplest intuition for there being middle points in this tradeoff is to consider a file replicated across three servers such that reading from any two should be able to yield the whole contents.  We can divide the file into two pieces, the first half of the file forms the first chunk (\(A\)) and the second half of the file forms the second chunk (\(B\)).  We can then produce a third equal-sized chunk (\(C\)) that’s the exclusive or of the first two (\(A \oplus B = C\)).  By reading any two of the three chunks, we can reconstruct the whole file:</p>
<table>
<colgroup>
<col>
<col>
</colgroup>
<tbody>
<tr>
<th><p>Chunks Read</p></th>
<th><p>Reconstruct Via</p></th>
</tr>
<tr>
<td><p>\(\{A, B\}\)</p></td>
<td><p>\(A :: B\)</p></td>
</tr>
<tr>
<td><p>\(\{A, C\}\)</p></td>
<td><p>\(A :: A \oplus C =&gt; (A \oplus (A \oplus B)) =&gt; A :: B\)</p></td>
</tr>
<tr>
<td><p>\(\{B, C\}\)</p></td>
<td><p>\(B \oplus C :: B =&gt; (B \oplus (A \oplus B)) :: B =&gt; A :: B\)</p></td>
</tr>
</tbody>
</table>
<p>And all erasure codes follow this same pattern of having separate data and parity chunks.</p>
</div>
<h2 id="_erasure_coding_basics">
Erasure Coding Basics
</h2> 
<p>Configuring an erasure code revolves around one formula:</p>

<div>
<table>
<tbody><tr>
<td>
\(k\)
</td>
<td>
<p>The number of pieces the data is split into.  One must read at least this many chunks in total to be able to reconstruct the value.  Each chunk in the resulting erasure code will be \(1/k\) of the size of the original file.</p>
</td>
</tr>
<tr>
<td>
\(m\)
</td>
<td>
<p>The number of parity chunks to generate.  This is the fault tolerance of the code, or the number of reads which can fail to complete.</p>
</td>
</tr>
<tr>
<td>
\(n\)
</td>
<td>
<p>The total number of chunks that are generated.</p>
</td>
</tr>
</tbody></table>
</div>
<p>Erasure codes are frequently referred to by their \(k+m\) tuple.  It is important to note that the variable names are not consistent across all literature.  The only constant is that an erasure code written as \(x+y\) means \(x\) data chunks and \(y\) parity chunks.</p>
<p>Please enjoy a little calculator to show the effects of different \(k\) and \(m\) settings:</p>
<div x-data="{k: 3, m: 2}">
<p>
Each chunk is \(1/k = \)<kbd x-text="(100/k).toFixed(2)"></kbd>% of the size of the original data.  There are \(k + m =\)<kbd x-text="k+m"></kbd> chunks total, and together they are equivalent to \((m + k) / k =\)<kbd x-text="((m+k)/k).toFixed(2)"></kbd> full copies of the data.
</p></div>
<p>Erasure codes are incredibly attractive to storage providers, as they offer a way to fault tolerance at minimal storage overhead.
Backblaze B2 runs with \(17+3\), allowing it to tolerate 3 failures using 1.18x the storage space.  OVH Cloud uses an \(8+4\) code, allowing it to tolerate 4 failures using 1.5x the storage space.  Scaleway uses a \(6+3\) code, tolerating three failures using 1.5x the storage space.  "Cloud storage reliability for Big Data applications"<a id="_sideref_1"></a><sup>[1]</sup> pays significant attention to the subject of erasure coding due to the fundamental role it plays in increasing durability for storage providers at a minimal cost of additional storage space.
<span><a id="_sidedef_1"></a>[1]: Rekha Nachiappan, Bahman Javadi, Rodrigo N. Calheiros, and Kenan M. Matawie. 2017. Cloud storage reliability for Big Data applications. <em>J. Netw. Comput. Appl.</em> 97, C (November 2017), 35–47. <a href="https://scholar.google.com/scholar?cluster=12723199345811969350">[scholar]</a></span></p>
<p>The main trade-off in erasure coding is a reduction in storage space used at the cost of an increase in requests issued to read data.  Rather than issuing one request to read a file-sized chunk from one disk, requests are issued to \(k+m\) disks.  Storage systems meant for infrequently accessed data, form ideal targets for erasure coding.  Infrequent access means issuing more IO operations per second won’t be a problematic tax, and the storage savings are significant when compared to storing multiple full copies of every file.</p>
<p>"Erasure coding" describes a general class of algorithms and not any one algorithm in particular.  In general, Reed-Solomon codes can be used to implement any \(k+m\) configuration of erasure codes.  Due to the prevalence of <a href="https://en.wikipedia.org/wiki/Standard_RAID_levels">RAID</a>, special attention in erasure coding research has been paid to developing more efficient algorithms specialized for implementing these specific subsets of erasure coding. RAID-0 is \(k+0\) erasure coding.  RAID-1 is \(1+m\) erasure coding.  RAID-4 and RAID-5 are slightly different variations of \(k+1\) erasure coding.  RAID-6 is \(k+2\) erasure coding.  Algorithms specifically designed for these cases are mentioned in the implementation section below, but it’s also perfectly fine to not be aware of what exact algorithm is being used to implement the choice of a specific \(k+m\) configuration.</p>
<p>Everything described in this post is about <em>Minimum Distance Separable</em> (MDS) erasure codes, which are only one of many erasure code families.  MDS codes provide the quorum-like property that any \(m\) chunks can be used to reconstruct the full value.  Other erasure codes take other tradeoffs, where some combinations of less than \(m\) chunks can be used to reconstruct the full value, but other combinations require more than \(m\) chunks.  "Erasure Coding in Windows Azure Storage"<a id="_sideref_2"></a><sup>[2]</sup> nicely explains the motivation of why Azure devised Local Reconstruction Codes for their deployment.  "SD Codes: Erasure Codes Designed for How Storage Systems Really Fail"<a id="_sideref_3"></a><sup>[3]</sup> pitches specializing an erasure code towards recovering from sector failures, as the most common failure type.  Overall, if one has knowledge about the expected pattern of failures, then a coding scheme that allow recovering from expected failures with less than \(m\) chunks, and unexpected failures with more than \(m\) chunks would have a positive expected value.
<span><a id="_sidedef_2"></a>[2]: Cheng Huang, Huseyin Simitci, Yikang Xu, Aaron Ogus, Brad Calder, Parikshit Gopalan, Jin Li, and Sergey Yekhanin. 2012. Erasure Coding in Windows Azure Storage. In <em>2012 USENIX Annual Technical Conference (USENIX ATC 12)</em>, USENIX Association, Boston, MA, 15–26. <a href="https://scholar.google.com/scholar?cluster=7930684733311413322">[scholar]</a><br>
         <a id="_sidedef_3"></a>[3]: James S. Plank, Mario Blaum, and James L. Hafner. 2013. SD Codes: Erasure Codes Designed for How Storage Systems Really Fail. In <em>11th USENIX Conference on File and Storage Technologies (FAST 13)</em>, USENIX Association, San Jose, CA, 95–104. <a href="https://scholar.google.com/scholar?cluster=6762112190773483176">[scholar]</a></span></p>
<h2 id="_applications_in_distributed_systems">
Applications in Distributed Systems
</h2> 
<h3 id="_space_and_tail_latency_improvements">
Space and Tail Latency Improvements
</h3> 
<p>The most direct application is in reducing the storage cost and increasing the durability of data in systems with a known, fixed set of replicas.
Think of blob/object storage or NFS storage.  A metadata service maps a file path to a server that stores the file.  Instead of having 3 replicas storing the full file each, have 15 replicas store the chunks of the (10+5) erasure coded file.  Such a coding yields half the total amount of data to store, and more than double the fault tolerance.</p>
<p>More generally, this pattern translates to "instead of storing data across \(X\) servers, consider storing it across \(X+m\) replicas with an \(X+m\) erasure code".  Over on Marc Brooker’s blog, this is illustrated <a href="https://brooker.co.za/blog/2023/01/06/erasure.html">using a caching system</a>.  Instead of using consistent hashing to identify one of \(k\) cache servers to query, one can use a \(k+m\) erasure code with \(k+m\) cache servers and not have to wait for the \(m\) slowest responses.  This provides both a storage space and tail latency improvement.</p>
<p>Again, the space and latency savings do come at a cost, which is an increase in IOPS/QPS, or effectively CPU.  In both cases, we’re betting that the limiting resource which determines how many machines or disks we need to buy is storage capacity, and that we can increase our CPU usage to decrease the amount of data that needs to be stored.  If the system is already pushing its CPU limits, then erasure coding might not be a cost-saving idea.</p>
<h3 id="_quorum_systems">
Quorum Systems
</h3> 
<p>Consider a quorum system with 5 replicas, where one must read from and write to at least 3 of them, a simple majority.  Erasure codes are well matched on the read side, where a \(3+2\) erasure code equally represents that a read may be completed using the results from any 3 of the 5 replicas.  Unfortunately, the rule is that writes are allowed to complete as long as they’re received by any 3 replicas, so one could only use a \(1+2\) code, which is exactly the same as writing three copies of the file.  Thus, there are no trivial savings to be had by applying erasure coding.</p>
<p>RS-Paxos<a id="_sideref_4"></a><sup>[4]</sup> examined the applicability of erasure codes to Paxos, and similarly concluded that the only advantage is when there’s an overlap between two quorums of more than one replica.  A quorum system of 7 replicas, where one must read and write to at least 5 of them would have the same 2 replica fault tolerance, but would be able to apply a \(3+2\) erasure code.  In general, with \(N\) replicas and a desired fault tolerance of \(f\), the best one can do with a fixed erasure coding scheme is \((N-2f)+f\).
<span><a id="_sidedef_4"></a>[4]: Shuai Mu, Kang Chen, Yongwei Wu, and Weimin Zheng. 2014. When paxos meets erasure code: reduce network and storage cost in state machine replication. In <em>Proceedings of the 23rd International Symposium on High-Performance Parallel and Distributed Computing</em> (HPDC '14), Association for Computing Machinery, New York, NY, USA, 61–72. <a href="https://scholar.google.com/scholar?cluster=16520033292975033789">[scholar]</a></span></p>
<p>HRaft<a id="_sideref_5"></a><sup>[5]</sup> explores that there is a way to get the desired improvement from a simple majority quorum, but adapting the coding to match the number of available replicas.  When all 5 replicas are available then we may use a \(3+2\) encoding, when 4 are available then use a \(2+2\) encoding, and when only 3 are available then use a \(1+2\) encoding<a id="_sideref_6"></a><sup>[6]</sup>.  Adapting the erasure code to the current replica availability yields our optimal improvement, but comes with a number of drawbacks.  Each write is optimistic in guessing the number of replicas that are currently available, and writes must be re-coded and resent to all replicas if one replica unexpectedly doesn’t acknowledge the write.  Additionally, one must still provision the system such that a replica storing the full value of every write is possible, so that after two failures, the system running in a \(1+2\) configuration won’t cause unavailability due to lacking disk space or throughput.  However, if failures are expected to be rare and will be recovered from quickly, then HRaft’s adaptive encoding scheme will yield significant improvements.
<span><a id="_sidedef_5"></a>[5]: Yulei Jia, Guangping Xu, Chi Wan Sung, Salwa Mostafa, and Yulei Wu. 2022. HRaft: Adaptive Erasure Coded Data Maintenance for Consensus in Distributed Networks. In <em>2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</em>, 1316–1326. <a href="https://scholar.google.com/scholar?cluster=15724086733201598850">[scholar]</a></span>
<span><a id="_sidedef_6"></a>[6]: And just to emphasize again, a \(1+2\) erasure encoding is just 3 full copies of the data.  It’s the same as not applying any erasure encoding.  The only difference is that it’s promised that only three full copies of the data are generated and sent to replicas.</span></p>
<h2 id="_usage_basics">
Usage Basics
</h2> 
<p>For computing erasure codings, there is a mature and standard <a href="https://jerasure.org/">Jerasure</a>.  If on a modern Intel processor, the Intel <a href="https://www.intel.com/content/www/us/en/developer/tools/isa-l/overview.html">Intelligent Storage Acceleration Library</a> is a SIMD-optimized library consistently towards the top of the benchmarks.</p>
<p>As an example, we can use <a href="https://pypi.org/project/pyeclib/">pyeclib</a> as a way to get easy access to an erasure coding implementation from python, and apply it to specifically to HRaft’s proposed adaptive erasure coding scheme:</p>
<details>
<summary>Python source code</summary>
<div>
<pre><code data-lang="python"><span>#!/usr/bin/env python
# Usage: ./ec.py &lt;K&gt; &lt;M&gt;
</span><span>import</span> <span>sys</span>
<span>K</span> <span>=</span> <span>int</span><span>(</span><span>sys</span><span>.</span><span>argv</span><span>[</span><span>1</span><span>])</span>
<span>M</span> <span>=</span> <span>int</span><span>(</span><span>sys</span><span>.</span><span>argv</span><span>[</span><span>2</span><span>])</span>

<span># Requires running the following to install dependencies:
# $ pip install --user pyeclib
# $ sudo dnf install liberasurecode-devel
</span><span>import</span> <span>pyeclib.ec_iface</span> <span>as</span> <span>ec</span>

<span># liberasurecode_rs_vand is built into liberasurecode, so this
# shouldn't have any other dependencies.
</span><span>driver</span> <span>=</span> <span>ec</span><span>.</span><span>ECDriver</span><span>(</span><span>ec_type</span><span>=</span><span>'liberasurecode_rs_vand'</span><span>,</span>
                     <span>k</span><span>=</span><span>K</span><span>,</span> <span>m</span><span>=</span><span>M</span><span>,</span> <span>chksum_type</span><span>=</span><span>'none'</span><span>)</span>
<span>data</span> <span>=</span> <span>bytes</span><span>([</span><span>i</span> <span>%</span> <span>100</span> <span>+</span> <span>32</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>10000</span><span>)])</span>
<span>print</span><span>(</span><span>f</span><span>"Erasure Code(K data chunks = </span><span>{</span><span>K</span><span>}</span><span>, M parity chunks = </span><span>{</span><span>M</span><span>}</span><span>)"</span>
      <span>f</span><span>" of </span><span>{</span><span>len</span><span>(</span><span>data</span><span>)</span><span>}</span><span> bytes"</span><span>)</span>

<span># Produce the coded chunks.
</span><span>chunks</span> <span>=</span> <span>driver</span><span>.</span><span>encode</span><span>(</span><span>data</span><span>)</span>

<span># There's some metdata that's prefixed onto each chunk to identify
# its position.  This isn't technically required, but there isn't
# an easy way to disable it.  There's also some additional bytes
# which I can't account for.
</span><span>metadata_size</span> <span>=</span> <span>len</span><span>(</span><span>driver</span><span>.</span><span>get_metadata</span><span>(</span><span>chunks</span><span>[</span><span>0</span><span>]))</span>
<span>chunk_size</span> <span>=</span> <span>len</span><span>(</span><span>chunks</span><span>[</span><span>0</span><span>])</span> <span>-</span> <span>metadata_size</span>
<span>print</span><span>(</span><span>f</span><span>"Encoded into </span><span>{</span><span>len</span><span>(</span><span>chunks</span><span>)</span><span>}</span><span> chunks of </span><span>{</span><span>chunk_size</span><span>}</span><span> bytes"</span><span>)</span>
<span>print</span><span>(</span><span>""</span><span>)</span>

<span># This replication scheme is X% less efficient than writing 1 copy
</span><span>no_ec_size</span> <span>=</span> <span>(</span><span>K</span><span>+</span><span>M</span><span>)</span> <span>*</span> <span>len</span><span>(</span><span>data</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>"No EC: </span><span>{</span><span>(</span><span>M</span><span>+</span><span>K</span><span>)</span><span>*</span><span>len</span><span>(</span><span>data</span><span>)</span><span>}</span><span> bytes, </span><span>{</span><span>1</span><span>/</span><span>(</span><span>K</span><span>+</span><span>M</span><span>)</span> <span>*</span> <span>100</span><span>}</span><span>% efficiency"</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>"Expected: </span><span>{</span><span>(</span><span>M</span><span>+</span><span>K</span><span>)</span><span>/</span><span>K</span> <span>*</span> <span>len</span><span>(</span><span>data</span><span>)</span><span>}</span><span> bytes,"</span>
      <span>f</span><span>" </span><span>{</span><span>1</span><span>/</span> <span>(</span><span>1</span><span>/</span><span>K</span> <span>*</span> <span>(</span><span>K</span><span>+</span><span>M</span><span>))</span> <span>*</span> <span>100</span><span>}</span><span>% efficiency"</span><span>)</span>
<span>total_ec_size</span> <span>=</span> <span>chunk_size</span> <span>*</span> <span>len</span><span>(</span><span>chunks</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>"Actual: </span><span>{</span><span>total_ec_size</span><span>}</span><span> bytes,"</span>
      <span>f</span><span>" </span><span>{</span><span>len</span><span>(</span><span>data</span><span>)</span> <span>/</span> <span>total_ec_size</span> <span>*</span> <span>100</span><span>}</span><span>% efficiency"</span><span>)</span>

<span># Validate that our encoded data decodes using minimal chunks
</span><span>import</span> <span>random</span>
<span>indexes</span> <span>=</span> <span>random</span><span>.</span><span>sample</span><span>(</span><span>range</span><span>(</span><span>K</span><span>+</span><span>M</span><span>),</span> <span>K</span><span>)</span>
<span># Prepended metadata is used to determine the chunk part number
# from the data itself.  Other libraries require this to be
# passed in as part of the decode call.
</span><span>decoded_data</span> <span>=</span> <span>driver</span><span>.</span><span>decode</span><span>([</span><span>chunks</span><span>[</span><span>idx</span><span>]</span> <span>for</span> <span>idx</span> <span>in</span> <span>indexes</span><span>])</span>
<span>assert</span> <span>decoded_data</span> <span>==</span> <span>data</span></code></pre>
</div>
</details>
<p>When there are 5/5 replicas available, HRaft would use a \(3+2\) erasure code:</p>
<div>
<pre>$ ./ec.py 3 2
Erasure Code(K data chunks = 3, M parity chunks = 2) of 10000 bytes
Encoded into 5 chunks of 3355 bytes

No EC: 50000 bytes, 20% efficiency
Expected: 16666.666666666668 bytes, 60.00000000000001% efficiency
Actual: 16775 bytes, 59.61251862891207% efficiency</pre>
</div>
<p>When there are 4/5 replicas available, HRaft would use a \(2+2\) erasure code:</p>
<div>
<pre>$ ./ec.py 2 2
Erasure Code(K data chunks = 2, M parity chunks = 2) of 10000 bytes
Encoded into 4 chunks of 5021 bytes

No EC: 40000 bytes, 25% efficiency
Expected: 20000.0 bytes, 50% efficiency
Actual: 20084 bytes, 49.790878311093406% efficiency</pre>
</div>
<p>When there are 3/5 replicas available, HRaft would use a \(1+2\) erasure code:</p>
<div>
<pre>$ ./ec.py 1 2
Erasure Code(K data chunks = 1, M parity chunks = 2) of 10000 bytes
Encoded into 3 chunks of 10021 bytes

No EC: 30000 bytes, 33.33333333333333% efficiency
Expected: 30000.0 bytes, 33.33333333333333% efficiency
Actual: 30063 bytes, 33.263480025280245% efficiency</pre>
</div>
<h2 id="_usage_not_so_basics">
Usage Not So Basics
</h2> 
<p>As always, things aren’t quite perfectly simple.</p>
<h3 id="_decoding_cost_variability">
Decoding Cost Variability
</h3> 
<p>Decoding performance varies with the number of data chunks that need to be recovered.  Decoding a \(3+2\) code from the three data chunks is computationally trivial.  Decoding the same file from two data chunks and one parity chunk involves solving a system of linear equations via Gaussian elimination, and the computational increases as the number of required parity chunks involved increases.  Thus, if using an erasure code as part of a quorum system, be aware that the CPU cost of decoding will vary depending on exactly which replicas reply.</p>
<p>There are a few different papers comparing different erasure code implementations and their performance across varying block size and number of data chunks to reconstruct.  I’ll suggest "Practical Performance Evaluation of Space Optimal Erasure Codes for High Speed Data Storage Systems"<a id="_sideref_7"></a><sup>[7]</sup> as the one I liked the most, from which the following figure was taken:
<span><a id="_sidedef_7"></a>[7]: Rui Chen and Lihao Xu. 2019. Practical Performance Evaluation of Space Optimal Erasure Codes for High-Speed Data Storage Systems. <em>SN Comput. Sci.</em> 1, 1 (December 2019). <a href="https://scholar.google.com/scholar?cluster=9222594581704961566">[scholar]</a></span></p>

<div>

<p><img src="https://transactional.blog/images/blog/2024-erasure-coding/decoding_performance-45d237e9.png" alt="decoding performance">
</p>
</div>
<h3 id="_library_differences">
Library Differences
</h3> 
<p>Liberasurecode abstracts over most common erasure coding implementation libraries, but be aware that does not mean that the implementations are equivalent.  Just because two erasure codes are both \(3+2\) codes doesn’t mean the same math was used to construct them.</p>
<p>Correspondingly, liberasurecode doesn’t <em>just</em> do the linear algebra work, it "helpfully" adds metadata necessary to configure which decoder to use and how, which you can’t disable or modify:</p>
<div>
<p>liberasurecode / erasurecode.h</p>
<div>
<pre><code data-lang="c"><span>struct</span> <span>__attribute__</span><span>((</span><span>__packed__</span><span>))</span>
<span>fragment_metadata</span>
<span>{</span>
    <span>uint32_t</span>    <span>idx</span><span>;</span>                <span>/* 4 */</span>
    <span>uint32_t</span>    <span>size</span><span>;</span>               <span>/* 4 */</span>
    <span>uint32_t</span>    <span>frag_backend_metadata_size</span><span>;</span>    <span>/* 4 */</span>
    <span>uint64_t</span>    <span>orig_data_size</span><span>;</span>     <span>/* 8 */</span>
    <span>uint8_t</span>     <span>chksum_type</span><span>;</span>        <span>/* 1 */</span>
    <span>uint32_t</span>    <span>chksum</span><span>[</span><span>LIBERASURECODE_MAX_CHECKSUM_LEN</span><span>];</span> <span>/* 32 */</span>
    <span>uint8_t</span>     <span>chksum_mismatch</span><span>;</span>    <span>/* 1 */</span>
    <span>uint8_t</span>     <span>backend_id</span><span>;</span>         <span>/* 1 */</span>
    <span>uint32_t</span>    <span>backend_version</span><span>;</span>    <span>/* 4 */</span>
<span>}</span> <span>fragment_metadata_t</span><span>;</span></code></pre>
</div>
</div>
<p>This is just a liberasurecode thing.  Using either Jerasure or ISA-L directly allows access to only the erasure coded data.  It <em>is</em> required as part of the APIs that each chunk must be provided along with if it was the Nth data or parity chunk, so the index must be maintained somehow as part of metadata.</p>
<p>As was noted in the <a href="https://www.youtube.com/watch?v=URAm-bbst-o">YDB talk at HydraConf</a>, Jerasure does a permutation of the output from what one would expect from just the linear algebra.  This means that it’s up to the specific implementation details of a library as to if reads must be aligned with writes — Jerasure cannot read a subset or superset of what was encoded.  ISA-L applies no permutation, so reads may decode unaligned subsets or supersets of encoded data.</p>
<p>Jerasure and ISA-L are, by far, the most popular libraries for erasure coding, but they’re not the only ones.  <a href="https://github.com/tahoe-lafs/zfec">tahoe-lafs/zfec<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a> is also a reasonably well-known implementation.  Christopher Taylor has written at least three MDS erasure coding implementations taking different tradeoffs (<a href="https://github.com/catid/cm256">catid/cm256<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>, <a href="https://github.com/catid/longhair">catid/longhair<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>, <a href="https://github.com/catid/leopard">catid/leopard<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>), and a comparison and discussion of the differences can be found on <a href="https://github.com/catid/leopard/blob/master/Benchmarks.md">leopard’s benchmarking results page</a>.  If erasure coding becomes a bottleneck, a library more optimized for your specific use case can likely be found somewhere, but ISA-L is generally good enough.</p>
<h2 id="_implementing_erasure_codes">
Implementing Erasure Codes
</h2> 
<p>It is entirely acceptable and workable to treat erasure codes as a magic function that turns 1 file into \(n\) chunks and back.  You can stop reading here, and not knowing the details of what math is being performed will not hinder your ability to leverage erasure codes to great effect in distributed systems or databases.  (And if you continue, take what follows with a large grain of salt, as efficient erasure coding is a subject folk have spent years on, and the below is what I’ve collected from a couple of days of reading through papers I only half understand.)</p>
<p>The construction of the \(n\) chunks is some linear algebra generally involving a Galois Field, none of which is important to understand to be able to productively <em>use</em> erasure codes.  Backblaze published <a href="https://www.backblaze.com/blog/reed-solomon/">a very basic introduction</a>.  The best introduction to the linear algebra of erasure coding that I’ve seen is Fred Akalin’s <a href="https://www.akalin.com/intro-erasure-codes">"A Gentle Introduction to Erasure Codes"</a>.  <a href="https://tomverbeure.github.io/2022/08/07/Reed-Solomon.html">Reed-Solomon Error Correcting Codes from the Bottom Up</a> covers Reed-Solomon codes and Galois Field polynomials specifically.  There’s also a plethora of erasure coding-related questions on the Stack Overflow family of sites, so any question over the math that one might have has already likely been asked and answered there.</p>
<p>With the basics in place, there are two main dimensions to investigate: what is the exact MDS encoding and decoding algorithm to implement, and how can one implement that algorithm most efficiently?</p>
<h3 id="_algorithmic_efficiency">
Algorithmic Efficiency
</h3> 
<p>In general, most MDS codes are calculated as a matrix multiplication, where addition is replaced with XOR, and multiply is replaced with a more expensive multiplication over GF(256).  For the special cases of 1-3 parity chunks (\(m \in \{1,2,3\}\)), there are algorithms not derived from Reed-Solomon and which use only XORs:</p>
<div>
<ul>
<li>
<p>\(m=1\) is a trivial case of a single parity chunk, which is just the XOR of all data chunks.</p>
</li>
<li>
<p>\(m=2\) is also known as RAID-6, for which I would recommend Liberation codes<a id="_sideref_8"></a><sup>[8]</sup><a id="_sideref_9"></a><sup>[9]</sup> as <em>nearly</em> optimal with an implementation available as part of <a href="https://jerasure.org/">Jerasure</a>, and HDP codes<a id="_sideref_10"></a><sup>[10]</sup> and EVENODD<a id="_sideref_11"></a><sup>[11]</sup> as notable but patented.  If \(k+m+2\) is prime, then X-Codes<a id="_sideref_12"></a><sup>[12]</sup> are also optimal.</p>
</li>
<li>
<p>\(m=3\) can be done via STAR coding<a id="_sideref_13"></a><sup>[13]</sup>.</p>
</li>
</ul>
</div>

<p>Otherwise and more generally, a form of Reed-Solomon coding is used.  The encoding/decoding matrix is either a \(k \times n\) Vandermonde<a id="_sideref_14"></a><sup>[14]</sup> matrix with the upper \(k \times k\) of it Gaussian eliminated to form an identity matrix, or an \(k \times k\) identity matrix with a \(k \times m\) Cauchy<a id="_sideref_15"></a><sup>[15]</sup> matrix glued onto the bottom.  In both cases, the goal is to form a matrix where the top \(k \times k\) is an identity matrix (so that each data chunk is preserved), and any deletion of \(m\) rows yields an invertible matrix.  Encoding is multiplying by this matrix, and decoding deletes the rows corresponding to erased chunks, and then solves the matrix as a system of linear equations for the missing data.</p>
<p>Gaussian elimination, as used in ISA-L, is the simplest method of decoding, but also the slowest.  For Cauchy matrixes, this can be improved<a id="_sideref_16"></a><sup>[16]</sup>, as done in <a href="https://github.com/catid/cm256">catid/cm256<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>.  The current fastest methods appear to be implemented in <a href="https://github.com/catid/leopard">catid/leopard<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>, which uses Fast Fourier Transforms<a id="_sideref_17"></a><sup>[17]</sup><a id="_sideref_18"></a><sup>[18]</sup> for encoding and decoding.</p>

<h3 id="_implementation_efficiency">
Implementation Efficiency
</h3> 
<p>There are levels of implementation efficiency for erasure codes that function over any \(k+m\) configuration:</p>
<div>
<ol>
<li>
<p>Implement the algorithm in C, and rely on the compiler for auto-vectorization.</p>
<p>This provides the most straightforward and most portable implementation, at acceptable performance.  Usage of <code>restrict</code> and ensuring the appropriate architecture-specific compilation flags have been specified (e.g. <code>-march=native</code>).</p>
</li>
<li>
<p>Rely on a vectorization library or compiler intrinsics to abstract the platform specifics.</p>
<p><a href="https://github.com/google/highway">google/highway<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a> and <a href="https://github.com/xtensor-stack/xsimd">xtensor-stack/xsimd<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a> appear to be reasonably commonly used libraries that try to use the best available SIMD instructions to accomplish general tasks.  There is also the upcoming <a href="https://en.cppreference.com/w/cpp/experimental/simd/simd"><code>std::experimental::simd</code></a>.  C/C++ compilers also offer <a href="https://gcc.gnu.org/onlinedocs/gcc/Vector-Extensions.html">builtins</a> for vectorization support.</p>
<p>The core of encoding and decoding is Galois field multiply and addition.  Optimized libraries for this can be found at <a href="https://github.com/catid/gf256">catid/gf256<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a> and <a href="https://web.eecs.utk.edu/~jplank/plank/papers/CS-07-593/">James Plank’s Fast Galois Field Arithmetic Library</a>.</p>
</li>
<li>
<p>Handwrite a vectorized implementation of the core encoding and decoding functions.</p>
<p>Further discussion of fast GF(256) operations can be found in the PARPAR project: <a href="https://github.com/animetosho/ParPar/blob/master/fast-gf-multiplication.md">fast-gf-multiplication</a> and the <a href="https://github.com/animetosho/ParPar/blob/master/xor_depends/info.md">xor_depends work</a>.  The consensus appears to be that a XOR-only GF multiply should be faster than a table-driven multiply.</p>

</li>
</ol>
</div>
<p>Optimizing further involves specializing the code to one specific \(k+m\) configuration by transforming the matrix multiplication with a constant into a linear series of instructions, and then:</p>
<div>
<ol start="4">
<li>
<p>Find an optimal coding matrix and XOR schedule for the specific GF polynomial and encoding matrix.</p>

</li>
<li>
<p>Apply further operation, memory, and cache optimizations.</p>

<p>The code is publicly available at <a href="https://github.com/yuezato/xorslp_ec">yuezato/xorslp_ec<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>.</p>
</li>
<li>
<p>Programmatically explore an optimized instruction schedule for a specific architecture.</p>

<p>The code is publicly available at <a href="https://github.com/Thesys-lab/tvm-ec">Thesys-lab/tvm-ec<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"></span></a>.</p>
</li>
</ol>
</div>
<p>For a more fully explored treatment of this topic, please see <a href="https://www.usenix.org/conference/fast19/presentation/zhou">"Fast Erasure Coding for Data Storage: A Comprehensive Study of the Acceleration Techniques"</a><a id="_sideref_19"></a><sup>[19]</sup>, which also has a video of the presenter if that’s your preferred medium.
<span><a id="_sidedef_19"></a>[19]: Tianli Zhou and Chao Tian. 2020. Fast Erasure Coding for Data Storage: A Comprehensive Study of the Acceleration Techniques. <em>ACM Trans. Storage</em> 16, 1 (March 2020). <a href="https://scholar.google.com/scholar?cluster=15189943361362749273">[scholar]</a></span></p>
<h2 id="_references">
References
</h2> 
<p><a href="https://transactional.blog/blog/2024-erasure-coding.bib">References as BibTeX</a></p>
<p>And if you’re looking to broadly dive deeper, I’d suggest starting with reviewing <a href="https://dblp.org/pid/07/3005.html">James S. Plank’s publications</a>.</p>
    <!-- TODO: consider https://utteranc.es/ for in-page comments. -->
      <hr>
      
      <p><a href="https://discu.eu/?q=https://transactional.blog/blog/2024-erasure-coding.html&amp;submit_title=Erasure%20Coding%20for%20Distributed%20Systems">See discussion of this page on Reddit, HN, and lobsters.</a></p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Snowden: The arrest of Durov is an assault on the basic human rights (130 pts)]]></title>
            <link>https://twitter.com/Snowden/status/1827695836832334169</link>
            <guid>41360808</guid>
            <pubDate>Mon, 26 Aug 2024 19:24:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/Snowden/status/1827695836832334169">https://twitter.com/Snowden/status/1827695836832334169</a>, See on <a href="https://news.ycombinator.com/item?id=41360808">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
    </channel>
</rss>