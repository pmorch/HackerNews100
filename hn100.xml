<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 27 Sep 2023 17:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Mistral 7B, the most powerful language model for its size to date, Apache 2.0 (274 pts)]]></title>
            <link>https://mistral.ai/news/announcing-mistral-7b/</link>
            <guid>37675496</guid>
            <pubDate>Wed, 27 Sep 2023 14:52:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/announcing-mistral-7b/">https://mistral.ai/news/announcing-mistral-7b/</a>, See on <a href="https://news.ycombinator.com/item?id=37675496">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Mistral AI team is proud to release Mistral 7B, the most powerful language model for its size to date.</p><h2 id="mistral-7b-in-short">Mistral 7B in short</h2><p>Mistral 7B is a 7.3B parameter model that:</p><ul><li>Outperforms Llama 2 13B on all benchmarks</li><li>Outperforms Llama 1 34B on many benchmarks</li><li>Approaches CodeLlama 7B performance on code, while remaining good at English tasks</li><li>Uses Grouped-query attention (GQA) for faster inference</li><li>Uses Sliding Window Attention (SWA) to handle longer sequences at smaller cost</li></ul><p>We’re releasing Mistral 7B under the Apache 2.0 license, it can be used without restrictions.</p><ul><li><a href="https://files.mistral-7b-v0-1.mistral.ai/mistral-7B-v0.1.tar">Download it</a> and use it anywhere (including locally) with <a href="https://github.com/mistralai/mistral-src">our reference implementation</a></li><li>Deploy it on any cloud (AWS/GCP/Azure), using vLLM <a href="https://docs.mistral.ai/cloud-deployment/skypilot">inference server and skypilot</a></li><li>Use it on <a href="https://huggingface.co/mistralai">HuggingFace</a></li></ul><p>Mistral 7B is easy to fine-tune on any task. As a demonstration, we’re providing a model fine-tuned for chat, which outperforms Llama 2 13B chat.</p><h3 id="performance-in-details">Performance in details</h3><p>We compared Mistral 7B to the Llama 2 family, and re-run all model evaluations ourselves for fair comparison.</p><p><img src="https://mistral.ai/images/news/announcing-mistral-7b/230927_bars.png" alt="histograms">
<em>Performance of Mistral 7B and different Llama models on a wide range of benchmarks. For all metrics, all models were re-evaluated with our evaluation pipeline for accurate comparison. Mistral 7B significantly outperforms Llama 2 13B on all metrics, and is on par with Llama 34B (since Llama 2 34B was not released, we report results on Llama 34B). It is also vastly superior in code and reasoning benchmarks.</em></p><p>The benchmarks are categorized by their themes:</p><ul><li>Commonsense Reasoning: 0-shot average of Hellaswag, Winogrande, PIQA, SIQA, OpenbookQA, ARC-Easy, ARC-Challenge, and CommonsenseQA.</li><li>World Knowledge: 5-shot average of NaturalQuestions and TriviaQA.</li><li>Reading Comprehension: 0-shot average of BoolQ and QuAC.</li><li>Math: Average of 8-shot GSM8K with maj@8 and 4-shot MATH with maj@4</li><li>Code: Average of 0-shot Humaneval and 3-shot MBPP</li><li>Popular aggregated results: 5-shot MMLU, 3-shot BBH, and 3-5-shot AGI Eval (English multiple-choice questions only)</li></ul><p><img src="https://mistral.ai/images/news/announcing-mistral-7b/230927_table.png" alt="table"></p><p>An interesting metric to compare how models fare in the cost/performance plane is to compute “equivalent model sizes”. On reasoning, comprehension and STEM reasoning (MMLU), Mistral 7B performs equivalently to a Llama 2 that would be more than 3x its size. This is as much saved in memory and gained in throughput.
<img src="https://mistral.ai/images/news/announcing-mistral-7b/230927_effective_sizes.png" alt="effective_sizes">
<em>Results on MMLU, Commonsense Reasoning, World Knowledge and Reading comprehension for Mistral 7B and Llama 2 (7B/13/70B). Mistral 7B largely outperforms Llama 2 13B on all evaluations, except on knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which restricts the amount of knowledge it can compress).</em></p><p><strong>Note</strong>: Important differences between our evaluation and the LLaMA2 paper’s:</p><ul><li>For MBPP, we use the hand-verified subset</li><li>For TriviaQA, we do not provide Wikipedia contexts</li></ul><h3 id="flash-and-furious-attention-drift">Flash and Furious: Attention drift</h3><p>Mistral 7B uses a sliding window attention (SWA) mechanism (<a href="https://arxiv.org/pdf/1904.10509.pdf">Child et al.</a>, <a href="https://arxiv.org/pdf/2004.05150v2.pdf">Beltagy et al.</a>), in which each layer attends to the previous <code>4,096</code> hidden states.
The main improvement, and reason for which this was initially investigated, is a linear compute cost of O(sliding_window.seq_len). In practice, changes made to <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention</a> and <a href="https://facebookresearch.github.io/xformers">xFormers</a> yield a 2x speed improvement for sequence length of 16k with a window of 4k. A huge thanks to Tri Dao and Daniel Haziza for helping include these changes on a tight schedule.</p><p>Sliding window attention exploits the stacked layers of a transformer to attend in the past beyond the window size: A token <code>i</code> at layer <code>k</code> attends to tokens <code>[i-sliding_window, i]</code> at layer <code>k-1</code>. These tokens attended to tokens <code>[i-2*sliding_window, i]</code>. Higher layers have access to informations further in the past than what the attention patterns seems to entail.</p><p><img src="https://mistral.ai/images/news/announcing-mistral-7b/attention_local.png" alt="Local attention"></p><p>Finally, a fixed attention span means we can limit our cache to a size of <code>sliding_window</code> tokens, using rotating buffers (read more in our <a href="https://github.com/mistralai/mistral-src">reference implementation repo</a>). This saves half of the cache memory for inference on sequence length of <code>8192</code>, without impacting model quality.</p><h2 id="acknowledgements">Acknowledgements</h2><p>We are grateful to CoreWeave for their 24/7 help in marshalling our cluster. We thank the <a href="https://www.cineca.it/">CINECA/EuroHPC</a> team, and in particular the operators of Leonardo, for their resources and help. We thank the maintainers of <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention</a>, <a href="https://github.com/vllm-project/vllm">vLLM</a>, <a href="https://github.com/facebookresearch/xformers">xFormers</a>, <a href="https://github.com/skypilot-org/skypilot">SkyPilot</a>, <a href="https://github.com/huggingface/text-generation-inference">TGI</a> for their precious assistance in implementing new features and integrating their solutions into ours. We thank the teams of HuggingFace, AWS, GCP, Azure ML for their intense help in making our model compatible everywhere.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unions Work (131 pts)]]></title>
            <link>https://werd.io/2023/unions-work</link>
            <guid>37675422</guid>
            <pubDate>Wed, 27 Sep 2023 14:46:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://werd.io/2023/unions-work">https://werd.io/2023/unions-work</a>, See on <a href="https://news.ycombinator.com/item?id=37675422">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Rusty revenant Servo returns to render once more (120 pts)]]></title>
            <link>https://www.theregister.com/2023/09/27/servo_returns/</link>
            <guid>37674519</guid>
            <pubDate>Wed, 27 Sep 2023 13:40:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2023/09/27/servo_returns/">https://www.theregister.com/2023/09/27/servo_returns/</a>, See on <a href="https://news.ycombinator.com/item?id=37674519">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p><span>Open Source Summit</span> A pleasant surprise from Open Source Summit is that Servo, the Rusty rendering engine that Mozilla was working on – until COVID, that is – is showing green shoots of renewed vigor.</p>
<p>Servo has been around for about a decade, so as experimental software projects go, it's a mature one. Igalia developer Manuel Rego presented a <a target="_blank" href="https://osseu2023.sched.com/event/1OGkc/servo-web-rendering-engine-reboot-manuel-rego-igalia" rel="nofollow">talk</a> which reports that the project is back under active development, almost exactly three years after <a target="_blank" href="https://www.theregister.com/2020/08/14/mozilla_google_search/">Mozilla terminated its Rust efforts</a> and laid off the whole Rust team, including the Servo developers.</p>
<p>In November 2020, <a target="_blank" href="https://www.theregister.com/2020/11/18/firefox_83/">the Linux Foundation adopted Servo</a>. However, the global operation has a lot of <a target="_blank" href="https://www.linuxfoundation.org/projects" rel="nofollow">projects</a> – we think we count 625 of them, but we could be wrong. Early this year, it handed the project over to its <a target="_blank" href="https://www.theregister.com/2022/09/20/linux_foundation_europe/">new European division</a>, which has a slightly more manageable <a target="_blank" href="https://linuxfoundation.eu/en/projects" rel="nofollow">list</a> of four, among them the <a target="_blank" href="https://www.theregister.com/2022/09/16/open_standards_digital_wallets/">OpenWallet foundation</a> and the <a target="_blank" href="https://www.theregister.com/2023/06/01/linux_foundation_risc_v/">RISC-V Software Ecosystem</a>. Now this also includes <a target="_blank" href="https://servo.org/" rel="nofollow">Servo</a>.</p>

    

<p>Servo first appeared <a target="_blank" href="https://www.theregister.com/2013/04/03/samsung_helps_mozilla_with_servo/">in tandem with Rust</a> a full decade ago, and by 2016 Mozilla <a target="_blank" href="https://www.theregister.com/2016/03/15/mozilla_to_release_first_version_of_servobased_browser_in_june/">was discussing releasing a prototype</a>. Previews <a target="_blank" href="https://www.theregister.com/2016/03/15/mozilla_to_release_first_version_of_servobased_browser_in_june/">started to appear that July</a>, when as we put it: "If Google has the language of Go, Moz has the language of No: Rust."</p>

        


        

<p>The new engine is quite capable. It supports the <a target="_blank" href="https://www.theregister.com/2011/03/03/webgl_one_dot_o_released/">now elderly WebGL API</a> as well as its <a target="_blank" href="https://www.theregister.com/2017/02/08/apple_webgpu/">more modern successor WebGPU</a>, which is much more powerful. For now, Rego said, it is mainly aimed at Windows, macOS, and desktop Linux, although the team is also testing mobile versions for both Android and more generic Linux, initially being tested on <a target="_blank" href="https://www.theregister.com/2021/12/08/pinephone_ships_developers/">Pine64's PinePhone Pro hardware</a>.</p>
<p>As well as being independent of any browser vendor, it is designed to be embeddable, memory-safe, modular, and parallel. The latter in particular benefits from the concurrency features provided by Rust. So far this year, the project has seen 1,682 commits from 77 developers, compared to just 523 from 22 people in 2022. A big change has been a new layout engine, replacing what is now called the legacy engine.</p>
<ul>

<li><a href="https://www.theregister.com/2023/09/20/gnu_turns_40/">GNU turns 40: Stallman's baby still not ready for prime time, but hey, there's cake</a></li>

<li><a href="https://www.theregister.com/2023/09/19/ubuntu_2310_taking_shape/">Ubuntu's 'Mantic Minotaur' peeks out of the labyrinth</a></li>

<li><a href="https://www.theregister.com/2023/09/14/pc_xt_with_hdmi/">These days you can teach old tech a bunch of new tricks</a></li>

<li><a href="https://www.theregister.com/2023/09/13/linux_mint_debian_edition_hands_on/">Linux Mint Debian Edition 6 hits beta with reassuringly little drama</a></li>
</ul>
<p>It still can't pass the <a target="_blank" href="https://www.theregister.com/2008/03/05/acid_three_browser_flunk/">Web Standards Project ACID tests</a>, which way back in 2008 WebKit was the <a target="_blank" href="https://www.theregister.com/2008/10/01/webkit_acid/">first browser to successfully handle</a>, so there's clearly some way to go yet. Even so, Rego's <a target="_blank" href="https://static.sched.com/hosted_files/osseu2023/be/2023-09-21-open-source-summit-europe-servo.pdf" rel="nofollow">presentation</a> [PDF] illustrates the improvements it's made so far this year.</p>
<p>At least for now, its goals have been scaled back from being a full web browser in its own right. One of the targets, though, is as a web runtime that can be embedded into standalone local web apps. At the moment, <a target="_blank" href="https://www.electronjs.org/" rel="nofollow">Electron.js</a> is the dominant tool in this space, but it's based on the Chromium engine, and is therefore another cog in the giant Google machine. However, for that to work, it also needs a JavaScript runtime, which is something that Servo doesn't include. For that, it depends on Mozilla's <a target="_blank" href="https://spidermonkey.dev/" rel="nofollow">SpiderMonkey</a>, which is also the basis of <a target="_blank" href="https://www.theregister.com/2022/08/17/gnome_project_25/">the GNOME desktop's GJS</a> and accordingly <a target="_blank" href="https://www.theregister.com/2023/03/02/linux_mint_212/">Cinnamon's CJS too</a>.</p>

        

<p>The SpiderMonkey code base is much less modern than Servo itself: it <a target="_blank" href="https://openhub.net/p/spidermonkey/analyses/latest/languages_summary" rel="nofollow">consists</a> of about one half C++ and one quarter C, plus a sprinkling of Java. For Rustaceans, we suspect this counts as embarrassing legacy code. A Rust JavaScript runtime called <a target="_blank" href="https://github.com/boa-dev/boa" rel="nofollow">Boa</a> does exist, but it's still in the early stages of development.</p>
<p>There aren't many modern web-rendering engines in existence, and several of them are close relatives: Chrome's Blink is derived from Apple's WebKit, itself derived from KDE's KHTML. Firefox's Gecko is the principal independent one still standing. Since Mozilla canceled development of its successor, it's good to know that it's in active development again. Much of the world is held together by the web, and if that were entirely controlled by one company, it would be <a target="_blank" href="https://xkcd.com/1118/" rel="nofollow">scary</a>. ®</p>
<p>
  <a href="https://www.youtube.com/watch?v=e3Y1C695CIw&amp;t=4100s" data-media="x-videoplayer">Youtube Video</a>
</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The most copied StackOverflow snippet of all time is flawed (248 pts)]]></title>
            <link>https://programming.guide/worlds-most-copied-so-snippet.html</link>
            <guid>37674139</guid>
            <pubDate>Wed, 27 Sep 2023 13:10:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://programming.guide/worlds-most-copied-so-snippet.html">https://programming.guide/worlds-most-copied-so-snippet.html</a>, See on <a href="https://news.ycombinator.com/item?id=37674139">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      
      <p><span>by Andreas Lundblad, 2019-12-02</span></p>
      <p><strong>In a recent study titled <em>Usage and Attribution of Stack Overflow Code Snippets in GitHub Projects</em>, an <a href="https://stackoverflow.com/a/3758880/276052">answer</a> I wrote almost a decade ago was found to be the most copied snippet on Stack Overflow. Ironically it happens to be buggy.</strong></p>
      <h2>A Long Long Time Ago…</h2>
      <p>Back in 2010 I was sitting in my office and doing what I wasn’t supposed to be doing: code golfing and chasing reputation on Stack Overflow.</p>
      <p>The following question got my attention: How do you print a byte count in a human readable format? That is, how do you format something like 123,456,789 bytes as “123.5&nbsp;MB”.</p>
      <div>
        <p><a href="https://stackoverflow.com/q/3758606/276052"><img alt="How to convert byte size into human-readable format in Java? Like 1024 should become '1 Kb' and 1024*1024 should become '1 Mb'." src="https://programming.guide/the-most-copied-so-snippet/so-screenshot.png"></a></p>
      </div>
      <p>The implicit spec here is that the resulting string should have a value between 1 and 999.9 followed by a suffix with an appropriate magnitude.</p>
      <p>One answer had already been posted. The code in that answer was based on a loop. The idea was simple: Try all magnitudes, going from the largest (EB = 10<sup>18</sup> bytes) down to the smallest (B = 1 byte) and use the first one that is smaller than the number of bytes. In pseudo code it looks something like this:</p>
      <pre><code>suffixes   = [ <span>"EB"</span>, <span>"PB"</span>, <span>"TB"</span>, <span>"GB"</span>, <span>"MB"</span>, <span>"kB"</span>, <span>"B"</span> ]
magnitudes = [ <span>10<sup>18</sup></span>, <span>10<sup>15</sup></span>, <span>10<sup>12</sup></span>, <span>10<sup>9</sup></span>, <span>10<sup>6</sup></span>, <span>10<sup>3</sup></span>, <span>10<sup>0</sup></span> ]
i = <span>0</span>
<span>while</span> (i &lt; magnitudes.length &amp;&amp; magnitudes[i] &gt; byteCount)
    i++
printf(<span>"%.1f %s"</span>, byteCount / magnitudes[i], suffixes[i])
</code></pre>
      <p>Usually when there’s a correct answer posted that already has a positive score, it’s hard to catch up with it. In Stack Overflow lingo it’s referred to as the <a href="https://meta.stackexchange.com/q/9731/147319">Fastest Gun in the West Problem</a>. In this case the existing answer had a few flaws, so I still saw an opportunity to top it. At the very least, the loop based code could be cleaned up significantly.</p>
      <h2>This is Algebra, I know this!</h2>
      <p>Then it hit me. The kB, MB, GB,&nbsp;… suffixes are nothing but powers of 1000 (or 1024 in <a href="https://en.wikipedia.org/wiki/Binary_prefix">IEC standard</a>) which means it should be possible to compute the right suffix using logarithms instead of a loop.</p>
      <p>Based on this idea, I posted the following:</p>
      <pre><code><span>public</span> <span>static</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>humanReadableByteCount</span>(<span>long</span> <span>bytes</span>, <span>boolean</span> <span>si</span>) {
    <span>int</span> <span>unit</span> = <span>si</span> ? <span>1000</span> : <span>1024</span>;
    <span>if</span> (<span>bytes</span> &lt; <span>unit</span>) <span>return</span> <span>bytes</span> + <span>" B"</span>;
    <span>int</span> <span>exp</span> = (<span>int</span>) (<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>bytes</span>) / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>unit</span>));
    <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>pre</span> = (<span>si</span> ? <span>"kMGTPE"</span> : <span>"KMGTPE"</span>).<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#charAt(int)"><span>charAt</span></a>(<span>exp</span>-<span>1</span>) + (<span>si</span> ? <span>""</span> : <span>"i"</span>);
    <span>return</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#format(java.lang.String,java.lang.Object:A)"><span>format</span></a>(<span>"%.1f %sB"</span>, <span>bytes</span> / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>), <span>pre</span>);
}</code></pre>
      <p>Granted it’s not very readable and <span>log / pow</span> probably makes it less efficient than other solutions. But there were no loops and almost no branching which I thought was pretty neat.</p>
      <div>
        <p><strong>The math behind this</strong> is straight forward. The byte count is expressed as <span>byteCount = 1000<sup><em>s</em></sup></span> where <em>s</em> represents the scale. (For binary notation, base 1024 is used.) Solving for <em>s</em> gives <span><em>s</em> = log<sub>1000</sub>(byteCount)</span>.</p>
        <p>There’s no log<sub>1000</sub> readily available in the API, but we can express it in terms of the natural logarithm as follows <span><em>s</em> = log(byteCount) / log(1000)</span>. We then floor <em>s</em> (cast to int) since if we for example have more than one megabyte (but not a full gigabyte) we want to use MB as magnitude.</p>
        <p>At this point if <span><em>s</em> = 1</span> the scale is kilobytes, if <span><em>s</em> = 2</span> the scale is megabytes, and so on. We divide the byteCount with 1000<sup><em>s</em></sup> and slap on the corresponding letter prefix.</p>
      </div>
      <p>All I could do now was to wait and see if the community would appreciate the answer. Little did I know that this would become the most copied snippet on Stack Overflow.</p>
      <h2>A Study on Attribution</h2>
      <p>Fast forward to 2018. A PhD student by the name Sebastian Baltes publishes a paper in the journal of <em>Empirical Software Engineering</em>. The title is <a href="https://doi.org/10.1007/s10664-018-9650-5"><em>Usage and Attribution of Stack Overflow Code Snippets in GitHub Projects</em></a> and it basically tries to answer one question: Is Stack Overflow’s CC BY-SA 3.0 license respected? I.e. to what extent is proper attribution given, when copying code from Stack Overflow.</p>
      <p>As part of their analysis they extracted code snippets from the <a href="https://archive.org/details/stackexchange">Stack Overflow data dump</a> and matched them against code from public GitHub repos. Quoting the abstract:</p>
      <blockquote>
        <p><em>We present results of a large-scale empiricalstudy analyzing the usage and attribution of non-trivial Java code snippetsfrom SO answers in public GitHub (GH) projects.</em></p>
      </blockquote>
      <p>(Spoiler alert: No, most people do not include proper attribution.)</p>
      <p>In the paper, they include the following table:</p>
      
      <p>That answer at the top with id <a href="https://stackoverflow.com/a/3758880/276052">3758880</a> happens to be the answer I had posted eight years earlier. At this point the answer had over a hundreds of thousands of views and over a thousand upvotes.</p>
      <p>A quick search on GitHub indeed shows thousands of occurrences of <code>humanReadableByteCount</code>.</p>
      <p><img src="https://programming.guide/the-most-copied-so-snippet/github-search.png"></p>
      <p>To check if you happen to have the code in a locally checked out repo:</p>
      <pre><code>$ git grep humanReadableByteCount
</code></pre>
      <div>
        <p><strong>Fun side story:</strong> How did I first hear about this study?</p>
        <p>Sebastian had found a match in the OpenJDK repository. There was no attribution and the OpenJDK license is not compatible with CC BY-SA 3.0. He <a href="http://mail.openjdk.java.net/pipermail/jdk9-dev/2016-December/005327.html">reached out to the dev mailing list</a> asking if the code on Stack Overflow was copied from OpenJDK, or if it was the other way around.</p>
        <p>The funny part here is that I used to work at Oracle, on the OpenJDK project, so a former colleague and friend of mine <a href="http://mail.openjdk.java.net/pipermail/jdk9-dev/2016-December/005328.html">replied</a> with the following:</p>
        <div>
          <p>Hi,</p>
          <p>why not ask the author of this SO post (aioobe) directly? He is an OpenJDK contributor and was employed by Oracle at the time this code appeared in the OpenJDK source repos.</p>
          <p>/Claes</p>
        </div>
        <p>Oracle doesn’t take these things lightly. I happen to know that some people at Oracle took a sigh of relief when they read this reply, and saw it as a bit of a triumph after the “accusation”.</p>
        <p>Sebastian then reached out to me to straighten it out, which I did: I had <em>not</em> yet started at Oracle when that commit was merged, and I did <em>not</em> contribute that patch. Jokes on Oracle. Shortly after, an issue was <a href="https://bugs.openjdk.java.net/browse/JDK-8170860">filed</a> and the code was <a href="http://hg.openjdk.java.net/jdk9/jdk9/hotspot/rev/b552b596203f">removed</a>.</p>
      </div>
      <h2>The Bug</h2>
      <p>I bet you’ve already given it some thought. What is that bug in the code snippet?</p>
      <p>Here it is again:</p>
      <pre><code><span>public</span> <span>static</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>humanReadableByteCount</span>(<span>long</span> <span>bytes</span>, <span>boolean</span> <span>si</span>) {
    <span>int</span> <span>unit</span> = <span>si</span> ? <span>1000</span> : <span>1024</span>;
    <span>if</span> (<span>bytes</span> &lt; <span>unit</span>) <span>return</span> <span>bytes</span> + <span>" B"</span>;
    <span>int</span> <span>exp</span> = (<span>int</span>) (<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>bytes</span>) / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>unit</span>));
    <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>pre</span> = (<span>si</span> ? <span>"kMGTPE"</span> : <span>"KMGTPE"</span>).<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#charAt(int)"><span>charAt</span></a>(<span>exp</span>-<span>1</span>) + (<span>si</span> ? <span>""</span> : <span>"i"</span>);
    <span>return</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#format(java.lang.String,java.lang.Object:A)"><span>format</span></a>(<span>"%.1f %sB"</span>, <span>bytes</span> / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>), <span>pre</span>);
}</code></pre>
      <p>After exabytes, 10<sup>18</sup>, comes zettabytes, 10<sup>21</sup>. Could it be that a really large input causes an index out of bounds on the <code>"kMGTPE"</code> string? Nope. The maximum <code>long</code> value is <span>2<sup>63</sup> - 1 ≈ 9.2 × 10<sup>18</sup></span>, so no <code>long</code> value will ever go beyond EB.</p>
      <p>Could it be a mix-up between SI and binary? Nope. There was a mix-up in an early version of the answer, but that was fixed rather quickly.</p>
      <p>Can <code>exp</code> end up being 0 causing <code>charAt(exp-1)</code> to fail? Nope. The first if-statement covers that case. The <code>exp</code> value will always be at least 1.</p>
      <p>Could there be some weird rounding error in the output? Now we’re getting there…</p>
      <h2>Lots of 9’s</h2>
      <p>The solution works all the way up until it approaches 1 MB. When given 999,999 bytes as input, the result (in SI mode) is <code>"1000.0 kB"</code>. While it is true that 999,999 is closer to <span>1,000 × 1000<sup>1</sup></span> than it is to <span>999.9 × 1000<sup>1</sup></span>, the 1,000 “significand” is out of range according to spec. The correct result is <code>"1.0 MB"</code>.</p>
      <p>FWIW, <em>all</em> 22 answers posted, including the ones using Apache Commons and Android libraries, had this bug (or a variation of it) at the time of writing this article.</p>
      <p>So how do we fix this? First of all, we note that the exponent (<code>exp</code>) should change from ‘k’ to ‘M’ as soon as the number of bytes is closer to <span>1 × 1,000<sup>2</sup></span> (1 MB) than it is to <span>999.9 × 1000<sup>1</sup></span> (999.9 k). This happens at 999,950. Similarly, we should switch from ‘M’ to ‘G’ when we pass 999,950,000 and so on.</p>
      <p>To achieve this we calculate this threshold and bump <code>exp</code> if <code>bytes</code> is larger. (For the binary case, this threshold is not an integer, se we need to ceil the result.)</p>
      <pre><code><span>if</span> (<span>bytes</span> &gt;= <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#ceil(double)"><span>ceil</span></a>(<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>) * (<span>unit</span> - <span>0.05</span>)))
    <span>exp</span>++;</code></pre>
      <p>With this change the code works well all the way up until the byte count approaches 1&nbsp;EB.</p>
      <h2>Even More 9’s</h2>
      <p>Given the input 999,949,999,999,999,999 the result is now <code>1000.0 PB</code> while correct result is <code>999.9 PB</code>. Mathematically the code is accurate, so what’s going on here?</p>
      <p>At this point we’re running into the precision limitations of a <code>double</code>.</p>
      <div>
        <h3>Floating Point Arithmetic 101</h3>
        <p>Due to the IEEE 754 representation floating point values close to zero are very dense, and large values are very sparse. In fact, half of all values are found between −1 and 1, and when talking large doubles, a value as large as <code>Long.MAX_VALUE</code> means nothing. Literally.</p>
        <pre><code><span>double</span> <span>a</span> = <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Double.html"><span>Double</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Double.html#MAX_VALUE"><span>MAX_VALUE</span></a>;
<span>double</span> <span>b</span> = <span>a</span> - <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html"><span>Long</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html#MAX_VALUE"><span>MAX_VALUE</span></a>;
<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/System.html"><span>System</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/System.html#err"><span>err</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/PrintStream.html#println(boolean)"><span>println</span></a>(<span>a</span> == <span>b</span>);  <span>// prints true</span>
</code></pre>
        <p>See <a href="https://programming.guide/bits-of-a-floating-point-value.html">Bits of a Floating Point Value</a> for a deep dive.</p>
      </div>
      <p>There are two problematic computations:</p>
      <ul>
        <li>The division in the <code>String.format</code> argument, and</li>
        <li>The threshold for bumping <code>exp</code>.</li>
      </ul>
      <p>We could switch to <code>BigDecimal</code>, but where’s the fun in that?! Besides, it gets messy anyway because there’s no <code>BigDecimal</code> log function in the standard API.</p>
      <h3>Scaling down intermediate values</h3>
      <p>For the first issue we can scale down the <code>bytes</code> value to a range where the the precision is better, and adjust <code>exp</code> accordingly. The end result is rounded anyway, so it doesn’t matter that we’re throwing out the least significant digits.</p>
      <pre><code><span>if</span> (<span>exp</span> &gt; <span>4</span>) {
    <span>bytes</span> /= <span>unit</span>;
    <span>exp</span>--;
}
</code></pre>
      <h3>Adjusting the least significant bits</h3>
      <p>For the second issue, we <em>do</em> care about the least significant bits (999,949,99…9 and 999,950,00…0 should end up with different exponents) so this issue calls for a different solution.</p>
      <p>First we note that there are 12 different possible values for the threshold (6 for each mode), and only one of them ends up faulty. The faulty result can be uniquely identified by the fact that it ends with D00<sub>16</sub>. If this is the case we simply adjust it to the correct value.</p>
      <pre><code><span>long</span> <span>th</span> = (<span>long</span>) <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#ceil(double)"><span>ceil</span></a>(<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>) * (<span>unit</span> - <span>0.05</span>));
<span>if</span> (<span>exp</span> &lt; <span>6</span> &amp;&amp; <span>bytes</span> &gt;= <span>th</span> - ((<span>th</span> &amp; <span>0xFFF</span>) == <span>0xD00</span> ? <span>51</span> : <span>0</span>))
    <span>exp</span>++;
</code></pre>
      <p>Since we rely on specific bit patterns in floating-point results, we slap on <code>strictfp</code> to ensure it works regardless of the hardware running the code.</p>
      <h2>Negative input</h2>
      <p>It’s unclear under what circumstances a negative byte count could be relevant, but since Java doesn’t have unsigned <code>long</code>, we better deal with it. Right now an input such as −10,000 results in <code>-10000 B</code>.</p>
      <p>Let’s introduce <code>absBytes</code>:</p>
      <pre><code><span>long</span> <span>absBytes</span> = <span>bytes</span> == <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html"><span>Long</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html#MIN_VALUE"><span>MIN_VALUE</span></a> ? <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html"><span>Long</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html#MAX_VALUE"><span>MAX_VALUE</span></a> : <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#abs(int)"><span>abs</span></a>(<span>bytes</span>);
</code></pre>
      <p>The complicated expression is due to the fact that <code>-Long.MIN_VALUE == Long.MIN_VALUE</code>. Now we perform all computations related to <code>exp</code> using <code>absBytes</code> instead of <code>bytes</code>.</p>
      <h2>Final Version</h2>
      <p>Here’s the final version of the code, golfed and compacted in the spirit of the original version:</p>
      <pre><code><span>// From: https://programming.guide/worlds-most-copied-so-snippet.html</span>
<span>public</span> <span>static</span> <span>strictfp</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>humanReadableByteCount</span>(<span>long</span> <span>bytes</span>, <span>boolean</span> <span>si</span>) {
    <span>int</span> <span>unit</span> = <span>si</span> ? <span>1000</span> : <span>1024</span>;
    <span>long</span> <span>absBytes</span> = <span>bytes</span> == <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html"><span>Long</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html#MIN_VALUE"><span>MIN_VALUE</span></a> ? <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html"><span>Long</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html#MAX_VALUE"><span>MAX_VALUE</span></a> : <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#abs(long)"><span>abs</span></a>(<span>bytes</span>);
    <span>if</span> (<span>absBytes</span> &lt; <span>unit</span>) <span>return</span> <span>bytes</span> + <span>" B"</span>;
    <span>int</span> <span>exp</span> = (<span>int</span>) (<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>absBytes</span>) / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>unit</span>));
    <span>long</span> <span>th</span> = (<span>long</span>) <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#ceil(double)"><span>ceil</span></a>(<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>) * (<span>unit</span> - <span>0.05</span>));
    <span>if</span> (<span>exp</span> &lt; <span>6</span> &amp;&amp; <span>absBytes</span> &gt;= <span>th</span> - ((<span>th</span> &amp; <span>0xFFF</span>) == <span>0xD00</span> ? <span>51</span> : <span>0</span>)) <span>exp</span>++;
    <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>pre</span> = (<span>si</span> ? <span>"kMGTPE"</span> : <span>"KMGTPE"</span>).<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#charAt(int)"><span>charAt</span></a>(<span>exp</span> - <span>1</span>) + (<span>si</span> ? <span>""</span> : <span>"i"</span>);
    <span>if</span> (<span>exp</span> &gt; <span>4</span>) {
        <span>bytes</span> /= <span>unit</span>;
        <span>exp</span> -= <span>1</span>;
    }
    <span>return</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#format(java.lang.String,java.lang.Object:A)"><span>format</span></a>(<span>"%.1f %sB"</span>, <span>bytes</span> / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>), <span>pre</span>);
}
</code></pre>
      <p>Note that this started out as a challenge to avoid loops and excessive branching. After ironing out all corner cases the code is even less readable than the original version. Personally I would not copy this snippet into production code.</p>
      <p>For <strong>updated code that is of production quality</strong> see separate article: <a href="https://programming.guide/java/formatting-byte-size-to-human-readable-format.html">Formatting byte size to human readable format</a></p>
      <h2>Key Takeaways</h2>
      <ul>
        <li>
          <p>Stack Overflow snippets can be buggy, even if they have thousands of upvotes.</p>
        </li>
        <li>
          <p>Test all edge cases, <em>especially</em> for code copied from Stack Overflow.</p>
        </li>
        <li>
          <p>Floating-point arithmetic is hard.</p>
        </li>
        <li>
          <p>Do include proper attribution when copying code. Someone might just call you out on it.</p>
        </li>
      </ul>
      <div>
        <h2>Comments (11)</h2>
        <div id="c1575414054-aveiSoh5">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/d4a2f69d78ee7a7cd0be47f92ad3a114?d=mp"></p>
          <div>
            <p>Whoa! What a writeup. Thanks for sharing</p>
            <details>
              <summary>
                <span>by Nick&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575431393-aet3OPhu">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/af0c24a85eac0b47ac8027eb36e11e75?d=mp"></p>
          <div>
            <p>This is a really hard problem.</p>
            <details>
              <summary>
                <span>by Ssuching&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575454325-abeCohl4">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/2f86996e30f54a0d42d93e5904b74e8c?d=mp"></p>
          <div>
            <p>Brilliant. Attitude too. Thanks a lot.</p>
            <details>
              <summary>
                <span>by syjmick&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575455440-sho7Gaey">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/a01d535d38a2fe0aff4a981308915203?d=mp"></p>
          <div>
            <div>
              <p>I think key takeaway here is: prefer short and simple loops over math. As you already said, the math is very hard to get exactly right (so, error prone and hard to read). But I believe it is also at least an order of magnitude slower than the loop-based solution.</p>
              <p>Did you run any benchmarks?</p>
            </div>
            <details>
              <summary>
                <span>by Ivan&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
            <div id="c1575475836-aoeu">
              <p><img alt="User avatar" src="https://www.gravatar.com/avatar/99e100243aaa8b1469b1ed4e8bbecb06?d=mp"></p>
              <div>
                <div>
                  <p>In general I agree with you. In this particular case however, one should note that the rounding error, negative input, and floating-point precision problems would apply also to a simple loop solution.</p>
                  <p>I have not done any benchmarking. Would be interesting for sure.</p>
                </div>
                <details>
                  <summary>
                    <span>by Andreas Lundblad&nbsp;|&nbsp;</span> <span>Reply</span>
                  </summary>
                  
                </details>
              </div>
            </div>
          </div>
        </div>
        <div id="c1575468068-Ai0eengu">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/4d400194f4d28fee7487eb826d463d9e?d=mp"></p>
          <div>
            <p>This is fantastically done. Thanks for posting this.</p>
            <details>
              <summary>
                <span>by John Doe&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575532582-etha1ahJ">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/a2452b2d1c9315f8cc62c7290c9a26f2?d=mp"></p>
          <div>
            <div>
              <p>This is very interesting article, I worked with floating point but always had difficulty to grasp the special cases of rounding. This article is good academic view of special cases to consider.</p>
              <p>Thanks! Five stars for this article. ★★★★★</p>
            </div>
            <details>
              <summary>
                <span>by Audiory&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575555484-auGh7hai">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/38b8a18199083b31074a90b47810b1ce?d=mp"></p>
          <div>
            <p>This article was quite a journey! Thanks :)</p>
            <details>
              <summary>
                <span>by swiatek7&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575671337-Il5che5l">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/124aca96dfe03819a6bc6e782e18d006?d=mp"></p>
          <div>
            <p>Awesome! Thanks for sharing!</p>
            <details>
              <summary>
                <span>by Adam&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575719090-ohXah7iu">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/1eaa7f930b2393cba93e21925ca58ab5?d=mp"></p>
          <div>
            <p>Nice article, thanks for sharing this story! So, how is it done in Unix? Some commands like <code>ls</code>, <code>df</code> have the <code>-h</code> human readable option.</p>
            <details>
              <summary>
                <span>by IvanV&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
            <div id="c1575732527-aoeu">
              <p><img alt="User avatar" src="https://www.gravatar.com/avatar/99e100243aaa8b1469b1ed4e8bbecb06?d=mp"></p>
              <div>
                <p>The implementation for coreutils is found in <a href="https://github.com/coreutils/gnulib/blob/master/lib/human.c"><code>human.c</code></a></p>
                <details>
                  <summary>
                    <span>by Andreas Lundblad&nbsp;|&nbsp;</span> <span>Reply</span>
                  </summary>
                  
                </details>
              </div>
            </div>
          </div>
        </div>
        <h3>Add comment</h3>
        
      </div>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Workers AI: serverless GPU-powered inference on Cloudflare’s global network (141 pts)]]></title>
            <link>https://blog.cloudflare.com/workers-ai/</link>
            <guid>37674097</guid>
            <pubDate>Wed, 27 Sep 2023 13:06:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cloudflare.com/workers-ai/">https://blog.cloudflare.com/workers-ai/</a>, See on <a href="https://news.ycombinator.com/item?id=37674097">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post">
    <article>
        


        <p localize="" datetime="2023-09-27T14:00:47+01:00">Loading...</p>
        

        <ul>
            <li>
                <a href="https://blog.cloudflare.com/author/phil/">
                    <img src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/phil.jpeg" alt="Phil Wittig" width="62" height="62">
                </a>
                
            </li>
            <li>
                <a href="https://blog.cloudflare.com/author/rita/">
                    <img src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/05/Rita-Kozlov.jpeg" alt="Rita Kozlov" width="62" height="62">
                </a>
                
            </li>
            <li>
                <a href="https://blog.cloudflare.com/author/rebecca-weekly/">
                    <img src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/04/RWeekly---Retouched-16.jpg" alt="Rebecca Weekly" width="62" height="62">
                </a>
                
            </li>
            <li>
                <a href="https://blog.cloudflare.com/author/celso/">
                    <img src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Celso-Martinho.png" alt="Celso Martinho" width="62" height="62">
                </a>
                
            </li>
        </ul>

        <section>
            <p>11 min read</p>
            <div>
                <figure><img src="https://blog.cloudflare.com/content/images/2023/09/image1-29.png" alt="" loading="lazy" width="1800" height="1014"></figure><p>If you're anywhere near the developer community, it's almost impossible to avoid the impact that AI’s recent advancements have had on the ecosystem. Whether you're using AI in your workflow to improve productivity, or you’re shipping AI based features to your users, it’s everywhere. The focus on AI improvements are extraordinary, and we’re super excited about the opportunities that lay ahead, but it's not enough.</p><p>Not too long ago, if you wanted to leverage the power of AI, you needed to know the ins and outs of machine learning, and be able to manage the infrastructure to power it.</p><p>As a developer platform with over one million active developers, we believe there is so much potential yet to be unlocked, so we’re changing the way AI is delivered to developers. Many of the current solutions, while powerful, are based on closed, proprietary models and don't address privacy needs that developers and users demand. Alternatively, the open source scene is exploding with powerful models, but they’re simply not accessible enough to every developer. Imagine being able to run a model, from your code, wherever it’s hosted, and never needing to find GPUs or deal with setting up the infrastructure to support it.</p><p>That's why we are excited to launch Workers AI - an AI inference as a service platform, empowering developers to run AI models with just a few lines of code, all powered by our global network of GPUs. It's open and accessible, serverless, privacy-focused, runs near your users, pay-as-you-go, and it's built from the ground up for a best in class developer experience.</p><h2 id="workers-aimaking-inference-just-work">Workers AI - making inference <strong>just work</strong></h2><p>We’re launching Workers AI to put AI inference in the hands of every developer, and to actually deliver on that goal, it should <strong>just work</strong> out of the box. How do we achieve that?</p><ul><li>At the core of everything, it runs on the right infrastructure - our world-class network of GPUs</li><li>We provide off-the-shelf models that run seamlessly on our infrastructure</li><li>Finally, deliver it to the end developer, in a way that’s delightful. A developer should be able to build their first Workers AI app in minutes, and say “Wow, that’s kinda magical!”.</li></ul><p>So what exactly is Workers AI? It’s another building block that we’re adding to our developer platform - one that helps developers run well-known AI models on serverless GPUs, all on Cloudflare’s trusted global network. As one of the latest additions to our developer platform, it works seamlessly with Workers + Pages, but to make it truly accessible, we’ve made it platform-agnostic, so it also works everywhere else, made available via a REST API.</p><h2 id="models-you-know-and-love">Models you know and love</h2><p>We’re launching with a curated set of popular, open source models, that cover a wide range of inference tasks:</p><ul><li><strong>Text generation (large language model):</strong> meta/llama-2-7b-chat-int8</li><li><strong>Automatic speech recognition (ASR):</strong> openai/whisper</li><li><strong>Translation:</strong> meta/m2m100-1.2</li><li><strong>Text classification:</strong> huggingface/distilbert-sst-2-int8</li><li><strong>Image classification:</strong> microsoft/resnet-50</li><li><strong>Embeddings:</strong> baai/bge-base-en-v1.5</li></ul><p>You can browse all available models in your Cloudflare dashboard, and soon you’ll be able to dive into logs and analytics on a per model basis!</p><figure><img src="https://blog.cloudflare.com/content/images/2023/09/image4-14.png" alt="" loading="lazy" width="1306" height="832"></figure><p>This is just the start, and we’ve got big plans. After launch, we’ll continue to expand based on community feedback. Even more exciting - in an effort to take our catalog from zero to sixty, we’re announcing a partnership with Hugging Face, a leading AI community + hub. The partnership is multifaceted, and you can read more about it <a href="https://blog.cloudflare.com/best-place-region-earth-inference">here</a>, but soon you’ll be able to browse and run a subset of the Hugging Face catalog directly in Workers AI.</p><h2 id="accessible-to-everyone">Accessible to everyone</h2><p>Part of the mission of our developer platform is to provide <strong>all</strong> the building blocks that developers need to build the applications of their dreams. Having access to the right blocks is just one part of it — as a developer your job is to put them together into an application. Our goal is to make that as easy as possible.</p><p>To make sure you could use Workers AI easily regardless of entry point, we wanted to provide access via: Workers or Pages to make it easy to use within the Cloudflare ecosystem, and via REST API if you want to use Workers AI with your current stack.</p><p>Here’s a quick CURL example that translates some text from English to French:</p><!--kg-card-begin: markdown--><pre><code>curl https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/run/@cf/meta/m2m100-1.2b \
-H "Authorization: Bearer {API_TOKEN}" \
	-d '{ "text": "I'll have an order of the moule frites", "target_lang": "french" }'
</code></pre>
<!--kg-card-end: markdown--><p>And here are what the response looks like:</p><!--kg-card-begin: markdown--><pre><code>{
  "result": {
    "answer": "Je vais commander des moules frites"
  },
  "success": true,
  "errors":[],
  "messages":[]
}
</code></pre>
<!--kg-card-end: markdown--><p>Use it with any stack, anywhere - your favorite Jamstack framework, Python + Django/Flask, Node.js, Ruby on Rails, the possibilities are endless. And deploy.</p><h2 id="designed-for-developers">Designed for developers</h2><p>Developer experience is really important to us. In fact, most of this post has been about just that. Making sure it works out of the box. Providing popular models that just work. Being accessible to all developers whether you build and deploy with Cloudflare or elsewhere. But it’s more than that - the experience should be frictionless, zero to production should be fast, and it should feel good along the way.</p><p>Let’s walk through another example to show just how easy it is to use! We’ll run Llama 2, a popular large language model open sourced by Meta, in a worker.</p><p>We’ll assume you have some of the basics already complete (Cloudflare account, Node, NPM, etc.), but if you don’t <a href="https://developers.cloudflare.com/workers-ai/get-started/local-dev-setup/">this guide</a> will get you properly set up!</p><h3 id="1-create-a-workers-project">1. Create a Workers project</h3><p>Create a new project named workers-ai by running:</p><!--kg-card-begin: markdown--><pre><code>$ npm create cloudflare@latest
</code></pre>
<!--kg-card-end: markdown--><p>When setting up your workers-ai worker, answer the setup questions as follows:</p><ul><li>Enter <strong>workers-ai</strong> for the app name</li><li>Choose <strong>Hello World</strong> script for the type of application</li><li>Select <strong>yes </strong>to using TypeScript</li><li>Select <strong>yes</strong> to using Git</li><li>Select <strong>no</strong> to deploying</li></ul><p>Lastly navigate to your new app directory:</p><!--kg-card-begin: markdown--><pre><code>cd workers-ai
</code></pre>
<!--kg-card-end: markdown--><h3 id="2-connect-workers-ai-to-your-worker">2. Connect Workers AI to your worker</h3><p>Create a Workers AI binding, which allows your worker to access the Workers AI service without having to manage an API key yourself.</p><p>To bind Workers AI to your worker, add the following to the end of your <strong>wrangler.toml</strong> file:</p><!--kg-card-begin: markdown--><pre><code>[ai]
binding = "AI" #available in your worker via env.AI
</code></pre>
<!--kg-card-end: markdown--><p>You can also bind Workers AI to a Pages Function. For more information, refer to <a href="https://developers.cloudflare.com/pages/platform/functions/bindings/#ai">Functions Bindings</a>.</p><h3 id="3-install-the-workers-ai-client-library">3. Install the Workers AI client library</h3><!--kg-card-begin: markdown--><pre><code>npm install @cloudflare/ai
</code></pre>
<!--kg-card-end: markdown--><h3 id="4-run-an-inference-task-in-your-worker">4. Run an inference task in your worker</h3><p>Update the <strong>source/index.ts</strong> with the following code:</p><!--kg-card-begin: markdown--><pre><code>import { Ai } from '@cloudflare/ai'
export default {
  async fetch(request, env) {
    const ai = new Ai(env.AI);
    const input = { prompt: "What's the origin of the phrase 'Hello, World'" };
    const output = await ai.run('@cf/meta/llama-2-7b-chat-int8', input );
    return new Response(JSON.stringify(output));
  },
};
</code></pre>
<!--kg-card-end: markdown--><h3 id="5-develop-locally-with-wrangler">5. Develop locally with Wrangler</h3><p>While in your project directory, test Workers AI locally by running:</p><!--kg-card-begin: markdown--><pre><code>$ npx wrangler dev --remote
</code></pre>
<!--kg-card-end: markdown--><p><strong>Note - </strong>These models currently only run on Cloudflare’s network of GPUs (and not locally), so setting <code>--remote</code> above is a must, and you’ll be prompted to log in at this point.</p><p>Wrangler will give you a URL (most likely localhost:8787). Visit that URL, and you’ll see a response like this</p><!--kg-card-begin: markdown--><pre><code>{
  "response": "Hello, World is a common phrase used to test the output of a computer program, particularly in the early stages of programming. The phrase "Hello, World!" is often the first program that a beginner learns to write, and it is included in many programming language tutorials and textbooks as a way to introduce basic programming concepts. The origin of the phrase "Hello, World!" as a programming test is unclear, but it is believed to have originated in the 1970s. One of the earliest known references to the phrase is in a 1976 book called "The C Programming Language" by Brian Kernighan and Dennis Ritchie, which is considered one of the most influential books on the development of the C programming language.
}
</code></pre>
<!--kg-card-end: markdown--><h3 id="6-deploy-your-worker">6. Deploy your worker</h3><p>Finally, deploy your worker to make your project accessible on the Internet:</p><!--kg-card-begin: markdown--><pre><code>$ npx wrangler deploy
# Outputs: https://workers-ai.&lt;YOUR_SUBDOMAIN&gt;.workers.dev
</code></pre>
<!--kg-card-end: markdown--><p>And that’s it. You can literally go from zero to deployed AI in minutes. This is obviously a simple example, but shows how easy it is to run Workers AI from any project.</p><h2 id="privacy-by-default">Privacy by default</h2><p>When Cloudflare was founded, our value proposition had three pillars: more secure, more reliable, and more performant. Over time, we’ve realized that a better Internet is also a more private Internet, and we want to play a role in building it.</p><p>That’s why Workers AI is private by default - we don’t train our models, LLM or otherwise, on your data or conversations, and our models don’t learn from your usage. You can feel confident using Workers AI in both personal and business settings, without having to worry about leaking your data. Other providers only offer this fundamental feature with their enterprise version. With us, it’s built in for everyone.</p><p>We’re also excited to support data localization in the future. To make this happen, we have an ambitious GPU rollout plan - we’re launching with seven sites today, roughly 100 by the end of 2023, and nearly everywhere by the end of 2024. Ultimately, this will empower developers to keep delivering killer AI features to their users, while staying compliant with their end users’ data localization requirements.</p><h2 id="the-power-of-the-platform">The power of the platform</h2><h4 id="vector-databasevectorize">Vector database - Vectorize</h4><p>Workers AI is all about running Inference, and making it really easy to do so, but sometimes inference is only part of the equation. Large language models are trained on a fixed set of data, based on a snapshot at a specific point in the past, and have no context on your business or use case. When you submit a prompt, information specific to you can increase the quality of results, making it more useful and relevant. That’s why we’re also launching Vectorize, our vector database that’s designed to work seamlessly with Workers AI. Here’s a quick overview of how you might use Workers AI + Vectorize together.</p><p>Example: Use your data (knowledge base) to provide additional context to an LLM when a user is chatting with it.</p><ol><li><strong>Generate initial embeddings:</strong> run your data through Workers AI using an embedding model. The output will be embeddings, which are numerical representations of those words.</li><li><strong><strong><strong>Insert those embeddings into Vectorize: </strong></strong></strong>this essentially seeds the vector database with your data, so we can later use it to retrieve embeddings that are similar to your users’ query</li><li><strong><strong><strong>Generate embedding from user question: </strong></strong></strong>when a user submits a question to your AI app, first, take that question, and run it through Workers AI using an embedding model.</li><li><strong><strong><strong>Get context from Vectorize: </strong></strong></strong>use that embedding to query Vectorize. This should output embeddings that are similar to your user’s question.</li><li><strong><strong><strong>Create context aware prompt:</strong> </strong></strong>Now take the original text associated with those embeddings, and create a new prompt combining the text from the vector search, along with the original question</li><li><strong>Run prompt: </strong>run this prompt through Workers AI using an LLM model to get your final result</li></ol><h4 id="ai-gateway">AI Gateway</h4><p>That covers a more advanced use case. On the flip side, if you are running models elsewhere, but want to get more out of the experience, you can run those APIs through our AI gateway to get features like caching, rate-limiting, analytics and logging. These features can be used to protect your end point, monitor and optimize costs, and also help with data loss prevention. Learn more about AI gateway <a href="https://blog.cloudflare.com/announcing-ai-gateway">here</a>.</p><h2 id="start-building-today">Start building today</h2><p>Try it out for yourself, and let us know what you think. Today we’re launching Workers AI as an open Beta for all Workers plans - free or paid. That said, it’s super early, so…</p><h4 id="warningit%E2%80%99s-an-early-beta">Warning - It’s an early beta</h4><p>Usage is <strong>not currently recommended for production apps</strong>, and limits + access are subject to change.</p><h4 id="limits">Limits</h4><p>We’re initially launching with limits on a per-model basis</p><ul><li>@cf/meta/llama-2-7b-chat-int8: 50 reqs/min globally</li></ul><p>Checkout our <a href="https://developers.cloudflare.com/workers-ai/platform/limits/">docs</a> for a full overview of our limits.</p><h4 id="pricing">Pricing</h4><p>What we released today is just a small preview to give you a taste of what’s coming (we simply couldn’t hold back), but we’re looking forward to putting the full-throttle version of Workers AI in your hands.</p><p>We realize that as you approach building something, you want to understand: how much is this going to cost me? Especially with AI costs being so easy to get out of hand. So we wanted to share the upcoming pricing of Workers AI with you.</p><p>While we won’t be billing on day one, we are announcing what we expect our pricing will look like.</p><p>Users will be able to choose from two ways to run Workers AI:</p><ul><li><strong>Regular Twitch Neurons (RTN) </strong>- running wherever there's capacity at $0.01 / 1k neurons</li><li><strong>Fast Twitch Neurons (FTN)</strong> - running at nearest user location at $1.25 / 1k neurons</li></ul><p>You may be wondering — what’s a neuron?</p><p>Neurons are a way to measure AI output that always scales down to zero (if you get no usage, you will be charged for 0 neurons). To give you a sense of what you can accomplish with a thousand neurons, you can: generate 130 LLM responses, 830 image classifications, or 1,250 embeddings.</p><p>Our goal is to help our customers pay only for what they use, and choose the pricing that best matches their use case, whether it’s price or latency that is top of mind.</p><h3 id="what%E2%80%99s-on-the-roadmap">What’s on the roadmap?</h3><p>Workers AI is just getting started, and we want your feedback to help us make it great. That said, there are some exciting things on the roadmap.</p><h4 id="more-models-please">More models, please</h4><p>We're launching with a solid set of models that just work, but will continue to roll out new models based on your feedback. If there’s a particular model you'd love to see on Workers AI, pop into our<a href="https://discord.cloudflare.com/"> Discord</a> and let us know!</p><p>In addition to that, we're also announcing a<a href="https://blog.cloudflare.com/best-place-region-earth-inference"> partnership with Hugging Face</a>, and soon you'll be able to access and run a subset of the Hugging Face catalog directly from Workers AI.</p><h4 id="analytics-observability">Analytics + observability</h4><p>Up to this point, we’ve been hyper focussed on one thing - making it really easy for any developer to run powerful AI models in just a few lines of code. But that’s only one part of the story. Up next, we’ll be working on some analytics and observability capabilities to give you insights into your usage + performance + spend on a per-model basis, plus the ability to fig into your logs if you want to do some exploring.</p><h4 id="a-road-to-global-gpu-coverage">A road to global GPU coverage</h4><p>Our goal is to be the best place to run inference on Region: Earth, so we're adding GPUs to our data centers as fast as we can.</p><p><strong>We plan to be in 100 data centers by the end this year</strong></p><figure><img src="https://blog.cloudflare.com/content/images/2023/09/image3-28.png" alt="" loading="lazy" width="1801" height="1013"></figure><p><strong>And nearly everywhere by the end of 2024</strong></p><figure><img src="https://blog.cloudflare.com/content/images/2023/09/unnamed-3.png" alt="" loading="lazy" width="1600" height="900"></figure><p><br><strong>We’re really excited to see you build</strong> - head over to <a href="https://developers.cloudflare.com/workers-ai/">our docs</a> to get started.</p><p>If you need inspiration, want to share something you’re building, or have a question - pop into our <a href="https://discord.com/invite/cloudflaredev">Developer Discord</a>.</p>
            </div>
        </section>
    
        









    <div>
            <p>We protect
                <a target="_blank" href="https://www.cloudflare.com/network-services/">entire corporate networks</a>,
                    help customers build
                    <a target="_blank" href="https://workers.cloudflare.com/">Internet-scale applications efficiently</a>,
                    accelerate any
                    <a target="_blank" href="https://www.cloudflare.com/performance/accelerate-internet-applications/">website
                    or Internet application</a>,
                    <a target="_blank" href="https://www.cloudflare.com/ddos/">ward off DDoS
                    attacks</a>, keep
                    <a target="_blank" href="https://www.cloudflare.com/application-security/">hackers at
                    bay</a>,
                    and can help you on
                    <a target="_blank" href="https://www.cloudflare.com/products/zero-trust/">your journey to Zero Trust</a>.</p>
            <p>Visit <a target="_blank" href="https://1.1.1.1/">1.1.1.1</a> from any device to get started with
                our free app that makes your Internet faster and safer.</p>
            <p>To learn more about our mission to help build a better Internet, <a target="_blank" href="https://www.cloudflare.com/learning/what-is-cloudflare/">start here</a>. If you're looking for a
                new career direction, check out <a target="_blank" href="https://cloudflare.com/careers">our open
                    positions</a>.</p>
        </div>

        

        
        

        <a href="https://blog.cloudflare.com/tag/birthday-week/">Birthday Week</a>
        <a href="https://blog.cloudflare.com/tag/workers/">Cloudflare Workers</a>
        <a href="https://blog.cloudflare.com/tag/ai/">AI</a>
        <a href="https://blog.cloudflare.com/tag/developer-platform/">Developer Platform</a>
        <a href="https://blog.cloudflare.com/tag/database/">Database</a>
    </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google judge rules trial documents can be posted by U.S. online (127 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2023-09-26/google-judge-rules-trial-documents-can-be-posted-by-us-online</link>
            <guid>37673413</guid>
            <pubDate>Wed, 27 Sep 2023 12:01:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2023-09-26/google-judge-rules-trial-documents-can-be-posted-by-us-online">https://www.bloomberg.com/news/articles/2023-09-26/google-judge-rules-trial-documents-can-be-posted-by-us-online</a>, See on <a href="https://news.ycombinator.com/item?id=37673413">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First Impressions with GPT-4V(ision) (151 pts)]]></title>
            <link>https://blog.roboflow.com/gpt-4-vision/</link>
            <guid>37673409</guid>
            <pubDate>Wed, 27 Sep 2023 12:01:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.roboflow.com/gpt-4-vision/">https://blog.roboflow.com/gpt-4-vision/</a>, See on <a href="https://news.ycombinator.com/item?id=37673409">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>On September 25th, 2023, <a href="https://help.openai.com/en/articles/6825453-chatgpt-release-notes?ref=blog.roboflow.com">OpenAI announced the rollout of two new features</a> that extend how people can interact with its recent and most advanced model, <a href="https://openai.com/research/gpt-4?ref=blog.roboflow.com">GPT-4</a>: the ability to ask questions about images and to use speech as an input to a query.</p><p>This functionality marks GPT-4’s move into being a <a href="https://blog.roboflow.com/multimodal-models/">multimodal model</a>. This means that the model can accept multiple “modalities” of input – text and images – and return results based on those inputs. Bing Chat, developed by Microsoft in partnership with OpenAI, and Google’s Bard model both support images as input, too. <a href="https://blog.roboflow.com/using-google-bard-with-images/">Read our comparison post to see how Bard and Bing perform with image inputs</a>.</p><p>In this guide, we are going to share our first impressions with the GPT-4V image input feature. We will run through a series of experiments to test the functionality of GPT-4V, showing where the model performs well and where it struggles.</p><p><em>Note: This article shows a limited series of tests our team performed; your results will vary depending on the questions you ask and the images you use in a prompt. Tag us on social media @roboflow with your findings using GPT-4V. We would love to see more tests using the model!</em></p><p>Without further ado, let’s get started!</p><h2 id="what-is-gpt-4v">What is GPT-4V?</h2><p><a href="https://cdn.openai.com/papers/GPTV_System_Card.pdf?ref=blog.roboflow.com">GPT-4V(ision)</a> (GPT-4V) is a multimodal model developed by OpenAI. GPT-4V allows a user to upload an image as an input and ask a question about the image, a task type known as visual question answering (VQA).</p><p>GPT-4V is rolling out as of September 24th and will be available in both the OpenAI ChatGPT iOS app and the web interface. You must have a GPT-4 subscription to use the tool.</p><p>Let’s experiment with GPT-4V and test its capabilities!</p><h2 id="test-1-visual-question-answering">Test #1: Visual Question Answering</h2><p>One of our first experiments with GPT-4V was to inquire about a computer vision meme. We chose this experiment because it allows us to the extent to which GPT-4V understands context and relationships in a given image.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-17.25.07-1.jpg" alt="" loading="lazy" width="590" height="1280"></figure><p>GPT-4V was able to successfully describe why the image was funny, making reference to various components of the image and how they connect. Notably, the provided meme contained text, which GPT-4V was able to read and use to generate a response. With that said, GPT-4V did make a mistake. The model said the fried chicken was labeled “NVIDIA BURGER” instead of “GPU”.</p><p>We then went on to test GPT-4V with currency, running a couple of different tests. First, we uploaded a photo of a United States penny. GPT-4V was able to successfully identify the origin and denomination of the coin:</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/Screenshot-2023-09-26-at-19.36.14-1.png" alt="" loading="lazy" width="1258" height="1224" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/Screenshot-2023-09-26-at-19.36.14-1.png 600w, https://blog.roboflow.com/content/images/size/w1000/2023/09/Screenshot-2023-09-26-at-19.36.14-1.png 1000w, https://blog.roboflow.com/content/images/2023/09/Screenshot-2023-09-26-at-19.36.14-1.png 1258w" sizes="(min-width: 720px) 720px"></figure><p>We then uploaded an image with multiple coins and prompted GPT-4V with the text: “How much money do I have?”</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-17.56.29.jpg" alt="" loading="lazy" width="826" height="1280" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/2023-09-26-17.56.29.jpg 600w, https://blog.roboflow.com/content/images/2023/09/2023-09-26-17.56.29.jpg 826w" sizes="(min-width: 720px) 720px"></figure><p>GPT-4V was able to identify the number of coins but did not ascertain the currency type. With a follow up question, GPT-4V successfully identified the currency type:</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-18.00.56.jpg" alt="" loading="lazy" width="1179" height="939" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/2023-09-26-18.00.56.jpg 600w, https://blog.roboflow.com/content/images/size/w1000/2023/09/2023-09-26-18.00.56.jpg 1000w, https://blog.roboflow.com/content/images/2023/09/2023-09-26-18.00.56.jpg 1179w" sizes="(min-width: 720px) 720px"></figure><p>Moving on to another topic, we decided to try using GPT-4V with a photo from a popular movie: Pulp Fiction. We wanted to know: could GPT-4 answer a question about the movie without being told in text what movie it was?</p><p>We uploaded a photo from Pulp Fiction with the prompt “Is it a good movie?”, to which GPT-4V responded with a description of the movie and an answer to our question. GPT-4V provides a high-level description of the movie and a summary of the attributes associated with the movie considered to be positive and negative.</p><p>We further asked about the IMDB score for the movie, to which GPT-4V responded with the score as of January 2022. This suggests, like other GPT models released by OpenAI, there is a knowledge cutoff after which point the model has no more recent knowledge.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-18.13.51.jpg" alt="" loading="lazy" width="1179" height="848" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/2023-09-26-18.13.51.jpg 600w, https://blog.roboflow.com/content/images/size/w1000/2023/09/2023-09-26-18.13.51.jpg 1000w, https://blog.roboflow.com/content/images/2023/09/2023-09-26-18.13.51.jpg 1179w" sizes="(min-width: 720px) 720px"></figure><p>We then explored GPT-4V’s question answering capabilities by asking a question about a place. We uploaded a photo of San Francisco with the text prompt “Where is this?” GPT-4V successfully identified the location, San Francisco, and noted that the Transamerica Pyramid, pictured in the image we uploaded, is a notable landmark in the city.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/Screenshot-2023-09-26-at-19.39.34.png" alt="" loading="lazy" width="764" height="714" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/Screenshot-2023-09-26-at-19.39.34.png 600w, https://blog.roboflow.com/content/images/2023/09/Screenshot-2023-09-26-at-19.39.34.png 764w" sizes="(min-width: 720px) 720px"></figure><p>Moving over to the realm of plants, we provided GPT-4V with a photo of a peace lily and asked the question “What is that plant and how should I care about it?”:</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-27-13.06.19.jpg" alt="" loading="lazy" width="711" height="1280" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/2023-09-27-13.06.19.jpg 600w, https://blog.roboflow.com/content/images/2023/09/2023-09-27-13.06.19.jpg 711w"></figure><p>The model successfully identified that the plant is a peace lily and provided advice on how to care for the plant. This illustrates the utility of having text and vision combined to create a multi-modal such as they are in GPT-4V. The model returned a fluent answer to our question without having to build our own two-stage process (i.e. classification to identify the plant then GPT-4 to provide plant care advice).</p><h2 id="test-2-optical-character-recognition-ocr">Test #2: Optical Character Recognition (OCR)</h2><p>We conducted two tests to explore GPT-4V’s OCR capabilities: OCR on an image with text on a car tire and OCR on a photo of a paragraph from a digital document. Our intent was to build an understanding of how GPT-4V performs at OCR in the wild, where text may have less contrast and be at an angle, versus digital documents with clear text.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-17.36.09-1.jpg" alt="" loading="lazy" width="590" height="605"></figure><p><br>GPT-4V was unable to correctly identify the serial number in an image of a tire. Some numbers were correct but there were several errors in the result from the model.</p><p>In our document test, we presented text from a web page and asked GPT-4V to read the text in the image. The model was able to successfully identify the text in the image.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/File.jpg" alt="" loading="lazy" width="738" height="1600" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/File.jpg 600w, https://blog.roboflow.com/content/images/2023/09/File.jpg 738w" sizes="(min-width: 720px) 720px"></figure><p>GPT-4V does an excellent job translating words in an image to individual characters in text. A useful insight for tasks related to extracting text from documents.</p><h2 id="test-3-math-ocr">Test #3: Math OCR</h2><p>Math OCR is a specialized form of OCR pertaining specifically to math equations. Math OCR is often considered its own discipline because the syntax of what the OCR model needs to identify extends to a vast range of symbols.</p><p>We presented GPT-4V with a math question. This math question was in a screenshot taken from a document. The question concerns calculating the length of a zip wire given two angles. We presented the image with the prompt “Solve it.”</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-27-13.25.51.jpg" alt="" loading="lazy" width="590" height="1280"></figure><figure><img src="https://blog.roboflow.com/content/images/2023/09/photo_2023-09-27-13.25.55.jpeg" alt="" loading="lazy" width="590" height="1280"></figure><p>The model identified the problem can be solved with trigonometry, identified the function to use, and presented a step-by-step walkthrough of how to solve the problem. Then, GPT-4V provided the correct answer to the question.</p><p>With that said, the GPT-4V system card notes that the model may miss mathematical symbols. Different tests, including tests where an equation or expression is written by hand on paper, may indicate deficiencies in the model's ability to answer math questions. </p><h2 id="test-4-object-detection">Test #4: Object Detection</h2><p><a href="https://blog.roboflow.com/object-detection/">Object detection</a> is a fundamental task in the field of computer vision. We asked GPT-4V to identify the location of various objects to evaluate its ability to perform object detection tasks.</p><p>In our first test, we asked GPT-4V to detect a dog in an image and provide the x_min, y_min, x_max, and y_max values associated with the position of the dog. The bounding box coordinates returned by GPT-4V did not match the position of the dog.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/photo_2023-09-26-18.51.24.jpeg" alt="" loading="lazy" width="590" height="1280"></figure><p>While GPT-4V’s capabilities at answering questions about an image are powerful, the model is not a substitute for fine-tuned <a href="https://roboflow.com/models/object-detection?ref=blog.roboflow.com">object detection models</a> in scenarios where you want to know where an object is in an image.</p><h2 id="test-5-captcha">Test #5: CAPTCHA</h2><p>We decided to test GPT-4V with CAPTCHAs, a task OpenAI studied in their research and wrote about in their <a href="https://cdn.openai.com/papers/GPTV_System_Card.pdf?ref=blog.roboflow.com">system card</a>. We found that GPT-4V was able to identify that an image contained a CAPTCHA but often failed the tests. In a traffic light example, GPT-4V missed some boxes that contained traffic lights.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/photo_2023-09-27-13.01.22.jpeg" alt="" loading="lazy" width="1031" height="1280" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/photo_2023-09-27-13.01.22.jpeg 600w, https://blog.roboflow.com/content/images/size/w1000/2023/09/photo_2023-09-27-13.01.22.jpeg 1000w, https://blog.roboflow.com/content/images/2023/09/photo_2023-09-27-13.01.22.jpeg 1031w" sizes="(min-width: 720px) 720px"></figure><p>In the following crosswalk example, GPT-4V classified a few boxes correctly but incorrectly classified one box in the CAPTCHA as a crosswalk.</p><figure><img src="https://lh4.googleusercontent.com/sUn71XmNZHeS4C9U1KGZm9T12MPiDaWSnjeqqZXSTan3I01VVBMvJ0_8knDTQW6kO1YJS8jLXswk_zEyINNDQz7mwDT60e_NoKrikqwaKuULsM9upmURmKCZ7STF6INGj4FtvEY3jlIjvgpVi1eamCI" alt="" loading="lazy" width="248" height="325"></figure><h2 id="test-6-crosswords-and-sudokus">Test #6: Crosswords and Sudoku's</h2><p>We decided to test how GPT-4V performs on crosswords and sudokus.</p><p>First, we prompted GPT-4V with photos of a crossword with the text instruction "Solve it." GPT-4V inferred the image contained a crossword and attempted to provide a solution to the crossword. The model appeared to read the clues correctly but misinterpreted the structure of the board. As a result, the provided answers were incorrect.</p><figure><img src="https://lh6.googleusercontent.com/bXAg1SiRBcs-huLBicWFzkeKI8NxB5OE1zoa1cAvC8sqfU1aFmZ2MRDKd2PTKxafivJsaY3R189vJYPEx0BzrXyWwy5ta2TEaGU2yKrBrOxqCYiQhAM93N4SDvZu6Wb7S3lCGaB2j9PxUCvuqbWD8os" alt="" loading="lazy" width="272" height="592"></figure><p>This same limitation was exhibited in our sudoku test, where GPT-4V identified the game but misunderstood the structure of the board and thus returned inaccurate results:</p><figure><img src="https://lh4.googleusercontent.com/U9cH5wYei3jZN8mmAA6etp3ngH8Zu0YrpLisXW6CEO0uSDB-FW3UO7PDLm-u5sEwc6Isvvh3BP_qizYEZctgWRUQpt8oP2_ius6vKGvUmTmAdcn6eneWiAOgq1O6n2W1LV7rx6a6hmDXLxrHs7IkxZI" alt="" loading="lazy" width="431" height="936"></figure><h2 id="gpt-4v-limitations-and-safety">GPT-4V Limitations and Safety</h2><p>OpenAI conducted research with an alpha version of the vision model available to a small group of users, as outlined in the official <a href="https://cdn.openai.com/papers/GPTV_System_Card.pdf?ref=blog.roboflow.com">GPT-4V(ision) System Card</a>. During this process, they were able to gather feedback and insights on how GPT-4V works with prompts provided by a range of people. This was supplemented with “red teaming”, wherein external experts were “to qualitatively assess the limitations and risks associated with the model and system”.</p><p>Based on OpenAI’s research, the GPT-4V system card notes numerous limitations with the model such as:</p><ol><li>Missing text or characters in an image</li><li>Missing mathematical symbols</li><li>Being unable to recognize spatial locations and colors</li></ol><p>In addition to limitations, OpenAI identified, researched, and attempted to mitigate several risks associated with the model. For example, GPT-4V avoids identifying a specific person in an image and does not respond to prompts pertaining to hate symbols.</p><p>With that said, there is further work to be done in model safeguarding. For example, OpenAI notes in the model system card that “If prompted, GPT-4V can generate content praising certain lesser known hate groups in response to their symbols.”,</p><h2 id="gpt-4v-for-computer-vision-and-beyond">GPT-4V for Computer Vision and Beyond</h2><p>GPT-4V is a notable movement in the field of machine learning and natural language processing. With GPT-4V, you can ask questions about an image – and follow up questions – in natural language and the model will attempt to ask your question.</p><p>GPT-4V performed well at various general image questions and demonstrated awareness of context in some images we tested. For instance, GPT-4V was able to successfully answer questions about a movie featured in an image without being told in text what the movie was.</p><p>For general question answering, GPT-4V is exciting. While models existed for this purpose in the past, they often lacked fluency in their answers. GPT-4V is able to both answer questions and follow up questions about an image and do so in depth.</p><p>With GPT-4V, you can ask questions about an image without creating a two-stage process (i.e. classification then using the results to ask a question to a language model like GPT). There will likely be limitations to what GPT-4V can understand, hence testing a use case to understand how the model performs is crucial.</p><p>With that said, GPT-4V has its limitations. The model did “hallucinate”, wherein the model returned inaccurate information. This is a risk with using language models to answer questions. Furthermore, the model was unable to accurately return bounding boxes for object detection, suggesting it is unfit for this use case currently.</p><p>We also observed that GPT-4V is unable to answer questions about people. When given a photo of Taylor Swift and asked who was featured in the image, the model declined to answer. OpenAI define this as an expected behavior in the published system card.</p><p>Interested in reading more of our experiments with multi-modal language models and GPT-4’s impact on computer vision? Check out the following guides:</p><ul><li><a href="https://blog.roboflow.com/gpt-4-impact-speculation/">Speculating on How GPT-4 Changes Computer Vision</a> (<a href="https://www.youtube.com/watch?v=aNLl0wEdMq4&amp;ref=blog.roboflow.com">Video</a>)</li><li><a href="https://blog.roboflow.com/how-good-is-bing-gpt-4-multimodality/">How Good Is Bing (GPT-4) Multimodality?</a></li><li><a href="https://blog.roboflow.com/chatgpt-code-interpreter-computer-vision/">ChatGPT Code Interpreter for Computer Vision</a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Be My Eyes’ AI assistant starts rolling out (214 pts)]]></title>
            <link>https://www.bemyeyes.com/blog/announcing-be-my-ai</link>
            <guid>37673300</guid>
            <pubDate>Wed, 27 Sep 2023 11:48:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bemyeyes.com/blog/announcing-be-my-ai">https://www.bemyeyes.com/blog/announcing-be-my-ai</a>, See on <a href="https://news.ycombinator.com/item?id=37673300">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Since 2015, Be My Eyes has worked to connect our 6.9 million volunteers to users to assist them with everyday tasks. Our mission is to make the world more accessible for people who are blind or have low vision, which is why, seven months ago, our team began working with the blind community to incorporate AI into the existing Be My Eyes platform. Since then, over 19,000 blind and low-vision beta testers contributed to the design and function of our new AI feature.</p><p>Today we are thrilled to announce that Be My AI is officially entering an open beta phase for iOS users and in coming weeks will be available for hundreds of thousands of Be My Eyes users worldwide.&nbsp;</p><p>We’ll start releasing Be My AI to all our existing iPhone users this week. The full roll-out will take a few weeks, so be sure to keep your app updated so you will have access to Be My AI as soon as it is available to you.</p><h2>How to access and use Be My AI</h2><p>Using Be My AI in your everyday life is quick and simple. Once you have access, open the Be My Eyes app, click on the ‘Be My AI’ tab, and take a picture. Be My AI will give you a detailed description about it, and you can chat and ask Be My AI further questions to get more information. If you like what Be My AI described, you can send its response and photo to others, or use its description in social media.</p><p>And don’t worry - if Be My AI can’t answer all your questions, if you want to check its results, or if you just need a little more description than Be My AI can provide or crave the magic and humanity of working with people, you still can easily reach one of our dedicated volunteers, just like before. They will always be there, in 150 languages all across the globe.</p><p>If you want to learn more about Be My AI and how to use it at its best, we have collected the most common questions (and answers!) in our <a href="https://support.bemyeyes.com/hc/en-us/articles/18133134809105-How-do-I-use-Be-My-AI-beta-">Help Center</a>. Make sure to check them out!</p><h2>When to use Be My AI</h2><p>You can use Be My AI 24/7 in all those situations when you want quick visual assistance without necessarily calling a human volunteer. Be My AI is perfect for all those circumstances when you want a quick solution or you don’t feel like talking to another person to get visual assistance. You may be amazed that Be My AI knows more than just what’s in the photo – just ask for more context and discover what it can tell you.</p><p>Be My AI also will give deaf-blind users a new way to get information if they use, for example, a braille display. Be My AI's written responses are user-selectable in 29 languages.</p><p>For all of its advantages, though, Be My AI does not and should not replace a white cane, guide dog, or other mobility aid that provides for safe travel.&nbsp;</p><blockquote><em>“I have been using it in several ways: taking my own photos particularly of images in magazines, on Twitter where very few people add descriptions or alt text and on WhatsApp where my family send me photos in groups all the time without any context. I was unsure about using Chat GPT which I’d seen many blind people using on social media, but when I saw that you were adding it I thought I’d give it a go!” - Sarah, Be My AI User</em></blockquote><p>Over the past few months, our blind and low vision beta testers have experimented with Be My AI and discovered many different ways to use it throughout your day from learning how to use new breakfast products in the morning to making sure your light is off before going to bed! We have collected a bunch of real life examples directly from their experiences to inspire you and show you what Be My AI can do:</p><ul role="list"><li>Get information about a popcorn box and access to cooking directions.</li><li>Read buttons on your dishwasher, washing machine, and other appliances with flat-screen controls.</li><li>Re-organize your wardrobe or create the perfect outfit for a night out.</li><li>Read instructions to set up your new laptop, smartphone, or tablet.</li><li>Read comics, books, and magazines.</li><li>Set up your Apple TV, Chromecast, or Amazon Fire Stick.</li><li>Find something that you accidentally dropped on the floor.</li><li>Get descriptions of memes from Facebook, X, Instagram and Mastodon.</li><li>Take pictures of paintings, statues, and other artwork to get detailed descriptions of them. You can also get pictures from your holidays and special events described to you.</li><li>Read the number of your bus at the bus station, or check out the departures screen at the train station or at the airport.</li><li>Read the menu at the restaurant and get relevant information from your receipt.</li><li>Translate text from dozens of different languages.</li><li>Prepare for a university exam or get assistance with your homework.</li><li>Check your makeup and identify beauty products while getting ready to go out.</li></ul><blockquote><em>“There aren’t enough words in the world to express what a truly miraculous, life-changing, day-making thing Be My Eyes and especially Be My AI is. Now I’m actually looking forward to organizing my closet because I won’t need human help. It describes my clothes in a way that makes them all sound gorgeous!” - Aimee, Be My AI User</em></blockquote><h2>What is an “Open Beta”?</h2><p>“Open beta” status just means we are opening up Be My AI to all our iOS users while we continue developing it and making it better based on your feedback. AI image recognition is complicated, and AI in general is still a rapidly evolving technology just in its infancy. Be My Eyes - and OpenAI - are constantly learning how to improve it.</p><p>There will be hiccups. Things may break. You may still get hallucinations, occasional wrong answers, and experience some frustrations. But we will be here to fix the breaks and keep making things better. So please be patient, and keep telling us about your experiences, positive and negative, so we can make this the best possible tool for you.</p><h2>What’s next: Android</h2><p>We started closed beta testing Be My AI for Android devices last week, and our goal is to move it to broadly-available open beta status in the coming months. You can already <a href="https://play.google.com/store/apps/details?id=com.bemyeyes.bemyeyes">sign up on the waitlist for Android beta testing</a> directly in the Be My Eyes app.</p><p>Here at Be My Eyes, the pace of innovation is accelerating. To keep updated about Be My AI, make sure to follow Be My Eyes on <a href="https://www.linkedin.com/company/be-my-eyes/?originalSubdomain=dk">LinkedIn</a>, <a href="https://twitter.com/BeMyEyes?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">X</a>, <a href="https://www.facebook.com/bemyeyesapp/">Facebook</a>, <a href="https://www.instagram.com/bemyeyesapp/?hl=en">Instagram</a>, <a href="https://mastodon.social/@bemyeyes">Mastodon</a> and <a href="https://www.tiktok.com/@bemyeyesapp?lang=en">TikTok</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I got robbed of my first kernel contribution (538 pts)]]></title>
            <link>https://ariel-miculas.github.io/How-I-got-robbed-of-my-first-kernel-contribution/</link>
            <guid>37671991</guid>
            <pubDate>Wed, 27 Sep 2023 08:58:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ariel-miculas.github.io/How-I-got-robbed-of-my-first-kernel-contribution/">https://ariel-miculas.github.io/How-I-got-robbed-of-my-first-kernel-contribution/</a>, See on <a href="https://news.ycombinator.com/item?id=37671991">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <h3 id="context">Context</h3>
<p>Around a year and a half ago, I’ve asked my former company for some time to
work on an issue that was impacting the debugging capabilities in our project:
gdbserver couldn’t debug multithreaded applications running on a PowerPC32
architecture.  The connection to the gdbserver was broken and it couldn’t
control the debug session anymore. Multiple people have already investigated
this problem and I had a good starting point, but we still weren’t sure in
which software component the issue lied: it could have been the toolchain, the
gdbserver, the Linux kernel or the custom patches we applied on top of the
kernel tree. We were quite far away from finding the root cause.</p>

<h3 id="investigating-the-issue">Investigating the issue</h3>
<p>After diving into the existing analysis for this issue and channeling my
google-fu, I’ve had my first breakthrough: an <a href="https://lore.kernel.org/linuxppc-dev/dc38afe9-6b78-f3f5-666b-986939e40fc6@keymile.com/">email
thread</a>
which not only described the same symptoms as our issue, but also pointed to
the <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=v6.6-rc3&amp;id=0c8c0f03e3a292e031596484275c14cf39c0ab7a">exact
commit</a>
which introduced it. The patch that introduced the bug moved the definition of
<code>thread_struct thread</code> from the middle of the <code>task_struct</code> to the end, a
seeminlgy innocuous change.</p>

<p>After debugging the issue, this is what Holger Brunck
<a href="https://lore.kernel.org/linuxppc-dev/e5cbd015-eeb5-31b5-0829-14cc8500dc6d@keymile.com/">observed</a></p>
<blockquote>
  <p>What I see is that gdbserver sends for each thread a SIGSTOP to the kernel and
waits for a response. The kernel does receive all the signals but only respond
to some of them in the error case. Which then matches with my “ps” output as I
see that some threads are not in the state pthread_stop and then the gdbserver
gets suspended.</p>
</blockquote>

<p>The low-level issue was that after interacting with gdbserver, some threads
were in the wrong process state and gdbserver couldn’t control them anymore.</p>

<p>I’ve spent 3-4 days reading commit descriptions related to the PowerPC
architecture and the changes around <code>task_struct</code>, trying to figure out whether
this issue was solved in subsequent kernel versions (spoiler: it was not).
I’ve moved <code>thread_struct thread</code> around to determine when the issue reproduced
and used <a href="https://linux.die.net/man/1/pahole">pahole</a> to inspect
<code>task_struct</code>’s layout. I’ve used
<a href="https://www.kernel.org/doc/html/v5.0/trace/ftrace.html">ftrace</a> to figure out
when the threads of the debugged process were scheduled and that’s how I
realized this could be a memory corruption issue: the threads that were stuck
were only scheduled once, unlike the other ones. I’ve originally dismissed that
this could be a memory corruption issue because in the <a href="https://lore.kernel.org/linuxppc-dev/b78d9e5d-fc2e-3676-a47e-ed5ca7a836e6@keymile.com/">original
thread</a>
it was mentioned that:</p>
<blockquote>
  <p>the content of the buffer is always zero and does not change. So at least no
one is writing non-zero to the buffer.</p>
</blockquote>

<p>That’s what I get for not verifying that the structure isn’t overwritten with
zero bytes (always validate your assumptions).</p>

<p>I remembered that the x86 architecture has <a href="https://en.wikipedia.org/wiki/X86_debug_register">debug
registers</a> that could be used
to trigger data write breakpoints. In fact, this is how I solved a bug back in
my earlier days as a software engineer. Sure enough, PowerPC also implements a
similar capability with the help of the <a href="https://stackoverflow.com/a/327540">DABR register</a>.</p>

<p>I’ve investigated how I could use hardware breakpoints on Linux and I ended up
implementing a linux kernel module based on this <a href="https://stackoverflow.com/a/19755213">excellent stackoverflow
answer</a>. This allowed me to place a
hardware breakpoint on the <a href="https://elixir.bootlin.com/linux/v6.5.5/source/include/linux/sched.h#L746">__state
field</a>
to figure out who on earth writes to it.</p>

<h3 id="finding-the-bug">Finding the bug</h3>
<p>And that’s how I found the issue: my custom kernel module showed the stack
traces from the places where the <code>__state</code> field of <code>task_struct</code> was being
written to.  I’ve noticed an outlier which revealed a buffer overflow in
<code>ptrace_put_fpr</code> (used by the POKEUSER API). This led to important fields from
<code>task_struct</code> getting overwritten, such as <code>__state</code>, which stores the state of
the process and it’s also used by the kernel to keep track of which processes
are stopped by the debugger.</p>

<p>The cause of this overflow? Taking an index meant to be used with an array of
32-bit elements and indexing an array of 64-bit elements. There were 64 indexes
that addressed the FPR, so the total addressable memory was 64 * 8 = 512
bytes. But there were only 32 entries in the fp_state.fpr array, which means
that the available memory was only 32 * 8 = 256 bytes. That allowed the user
(aka gdbserver) to write up to 256 bytes past the end of the array.
<img src="https://ariel-miculas.github.io/images/fpr-overflow.png" alt="fpr-overflow"></p>

<h3 id="sending-the-patch-upstream">Sending the patch upstream</h3>
<p>I’ve sent a patch to the Linux kernel security team (security@kernel.org)
because I wanted to err on the safe side: a memory corruption issue that could
overwrite the memory of the processes’s states could have security
implications. Unfortunately, this mailing list is private so I cannot link to
the original patch I sent.  Michael Ellerman, the PowerPC maintainer, followed
up and told me he will contact me in private to figure this issue out. I have
actually sent him two patches fixing the issue: the original one that I sent to
the security mailing list and <a href="https://lists.ozlabs.org/pipermail/linuxppc-dev/2022-June/244438.html">another
version</a>
(quite different from the first one) which addressed some suggestions received
in reply to my original submission. And the latter patch was actually based on
existing kernel code, which emulated PowerPC32 operations on PowerPC64 (yeah,
they got the FPR indexing right). Neither of those were accepted by Michael
Ellerman, and instead he implemented his <a href="https://lore.kernel.org/all/20220609133245.573565-1-mpe@ellerman.id.au/">own version of the
fix</a>.
I told him that I would really appreciate if he could accept a patch from me,
so that I could receive credit for fixing this issue and become a kernel
contributor. I was also open to working with him, addressing his feedback and
sending subsequent versions of patches. He said (paraphrasing):</p>
<blockquote>
  <p>Sorry, I like my version better. If you want to be a Linux kernel
contributor, here’s an issue you could fix.</p>
</blockquote>

<p>I found this really perplexing and insulting. Instead of getting recognized for
fixing the issue, he wanted to give me more work to do. My company and I should
have received proper credit for solving this issue, especially considering how
much effort we put into it.</p>

<p>I felt it was really unfair to only get a “Reported-by” tag. Here’s the
<a href="https://docs.kernel.org/process/submitting-patches.html#using-reported-by-tested-by-reviewed-by-suggested-by-and-fixes">purpose of the tag</a>:</p>

<blockquote>
  <p>The Reported-by tag gives credit to people who find bugs and report them and it hopefully inspires them to help us again in the future.</p>
</blockquote>

<p>Well, I certainly didn’t feel inspired to get involved with the kernel
community again. On the contrary, I felt belittled and angry that my work
wasn’t properly recognized.</p>

<h3 id="conclusion">Conclusion</h3>
<p>I spent a lot of time and effort doing root cause analysis, fixing the bug,
testing and validating the fix, getting feedback from other engineers at my
company, adapting the fix to the latest kernel version, and sending two
different patches to Michael Ellerman, the PowerPC maintainer. Instead of
accepting my patch or guiding me towards a better solution, he went ahead and
implemented his own fix, giving me credit only for reporting the issue (which
was <a href="https://lore.kernel.org/linuxppc-dev/dc38afe9-6b78-f3f5-666b-986939e40fc6@keymile.com/">already
reported</a>
six years prior to this).</p>

<p>My first contribution to the kernel was a really frustrating and discouraging
experience, dealing with people who do not think it’s important to get proper
recognition for your work.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Essence: A desktop OS built from scratch, for control and simplicity (260 pts)]]></title>
            <link>https://nakst.gitlab.io/essence</link>
            <guid>37671419</guid>
            <pubDate>Wed, 27 Sep 2023 07:44:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nakst.gitlab.io/essence">https://nakst.gitlab.io/essence</a>, See on <a href="https://news.ycombinator.com/item?id=37671419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p>
						Efficient with resources.
					</p>

					

					<p>
						Essence will happily run on low-powered hardware. It can take less than 30MB of drive space, and boot with even less RAM. No tasks run in the background, giving your applications all the space they need.
					</p>

					

					<p><img src="https://nakst.gitlab.io/screenshot4.png">
				</p></div><div>
					<p>
						Open source.
					</p>

					

					<p>
						All the code is made available under the MIT license. You can browse through the source on the <a href="https://gitlab.com/nakst/essence">GitLab repository</a>.
					</p>

					

					<p>
						If you're interested in contributing, join our <a href="https://discord.gg/skeP9ZGDK8">Discord server</a> to discuss ideas with other developers.
					</p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing a debugger from scratch: Breakpoints (221 pts)]]></title>
            <link>https://www.timdbg.com/posts/writing-a-debugger-from-scratch-part-5/</link>
            <guid>37670938</guid>
            <pubDate>Wed, 27 Sep 2023 06:31:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.timdbg.com/posts/writing-a-debugger-from-scratch-part-5/">https://www.timdbg.com/posts/writing-a-debugger-from-scratch-part-5/</a>, See on <a href="https://news.ycombinator.com/item?id=37670938">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><p>(New to this series? Consider starting from <a href="https://www.timdbg.com/posts/writing-a-debugger-from-scratch-part-1">part 1</a>)</p><p>At the end of the <a href="https://www.timdbg.com/posts/writing-a-debugger-from-scratch-part-4">last post</a>, we started to get some interesting functionality with the ability to resolve addresses to names in a module. This was the last functionality missing before we could implement breakpoints! This part adds the ability for DbgRs to set hardware breakpoints.</p><p>The code for this post is in the <a href="https://github.com/TimMisiak/dbgrs/tree/part5">part5 branch on github</a>. You can also view the <a href="https://github.com/TimMisiak/dbgrs/compare/part4...part5">changes from part4</a>. If you see any mistakes or ways to improve the code, feel free to <a href="https://github.com/TimMisiak/dbgrs/issues">create issues</a> on the GitHub repo or submit a PR.</p><h2 id="first-some-cleanup">First, some cleanup</h2><p>I’ve been trying to keep DbgRs as simple as possible, avoiding extra architectural layers to keep the concepts as clear and concise as possible, but now that it has grown a bit, we need a little bit of cleanup to keep things easy to understand. I’ll just cover these changes briefly and then we’ll get to breakpoints.</p><p>The biggest change is that I moved all of the code dealing with <a href="https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-waitfordebugeventex">WaitForDebugEventEx</a> into a new file, <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/event.rs">event.rs</a>. A single public function allows waiting for the next debug event, and returns a new <code>DebugEvent</code> enum instead of the raw win32 <code>DEBUG_EVENT</code> type.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span> <span>enum</span> <span>DebugEvent</span> {
</span></span><span><span>    Exception{first_chance: <span>bool</span>, exception_code: <span>i32</span>},
</span></span><span><span>    CreateProcess{exe_name: Option<span>&lt;</span>String<span>&gt;</span>, exe_base: <span>u64</span>},
</span></span><span><span>    CreateThread{thread_id: <span>u32</span>},
</span></span><span><span>    ExitThread{thread_id: <span>u32</span>},
</span></span><span><span>    LoadModule{module_name: Option<span>&lt;</span>String<span>&gt;</span>, module_base: <span>u64</span>},
</span></span><span><span>    OutputDebugString(String),
</span></span><span><span>    ExitProcess,
</span></span><span><span>    Other(String)
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>struct</span> <span>EventContext</span> {
</span></span><span><span>    <span>pub</span> process_id: <span>u32</span>,
</span></span><span><span>    <span>pub</span> thread_id: <span>u32</span>,
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>fn</span> <span>wait_for_next_debug_event</span>(mem_source: <span>&amp;</span><span>dyn</span> MemorySource) -&gt; (EventContext, DebugEvent) {
</span></span><span><span>    <span>//...
</span></span></span><span><span><span></span>}
</span></span></code></pre></div><p>As a result, the <code>main_debugger_loop</code> function is a bit smaller and can focus on the core debugger loop logic.</p><p>The other change is that I’ve added a file called <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/util.rs">util.rs</a> which has some common helpers for win32 structures, including a thin <code>AutoClosedHandle</code> wrapper for <code>HANDLE</code> and the <code>AlignedContext</code> struct that wraps the win32 <code>CONTEXT</code>. Additionally, this includes some constants that are missing from the windows-rs crate.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>#[repr(align(16))]</span>
</span></span><span><span><span>pub</span> <span>struct</span> <span>AlignedContext</span> {
</span></span><span><span>    <span>pub</span> context: <span>CONTEXT</span>,
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>struct</span> <span>AutoClosedHandle</span>(<span>pub</span> <span>HANDLE</span>);
</span></span></code></pre></div><h2 id="evaluating-symbols">Evaluating symbols</h2><p>When setting a breakpoint, it’s expected that you can use the name of a function, and not just the address. So to start, we need to add the capability for resolving a name to an address. Previously, we had the <code>resolve_address_to_name</code> function in <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/name_resolution.rs#L56">name_resolution.rs</a>, so we’ll add the corresponding <code>resolve_name_to_address</code> function there as well.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span> <span>fn</span> <span>resolve_name_to_address</span>(sym: <span>&amp;</span><span>str</span>, process: <span>&amp;</span><span>mut</span> Process) -&gt; Result<span>&lt;</span><span>u64</span>, String<span>&gt;</span> {
</span></span><span><span>    <span>match</span> sym.chars().position(<span>|</span>c<span>|</span> c <span>==</span> <span>'!'</span>) {
</span></span><span><span>        None <span>=&gt;</span> {
</span></span><span><span>            <span>// Search all modules
</span></span></span><span><span><span></span>            Err(<span>"Not yet implemented"</span>.to_string())
</span></span><span><span>        },
</span></span><span><span>        Some(pos) <span>=&gt;</span> {
</span></span><span><span>            <span>let</span> module_name <span>=</span> <span>&amp;</span>sym[<span>..</span>pos];
</span></span><span><span>            <span>let</span> func_name <span>=</span> <span>&amp;</span>sym[pos <span>+</span> <span>1</span><span>..</span>];
</span></span><span><span>            <span>if</span> <span>let</span> Some(module) <span>=</span> process.get_module_by_name_mut(module_name) {
</span></span><span><span>                <span>if</span> <span>let</span> Some(addr) <span>=</span> resolve_function_in_module(module, func_name) {
</span></span><span><span>                    Ok(addr)
</span></span><span><span>                } <span>else</span> {
</span></span><span><span>                    Err(format!(<span>"Could not find </span><span>{}</span><span> in module </span><span>{}</span><span>"</span>, func_name, module_name))
</span></span><span><span>                }
</span></span><span><span>            } <span>else</span> {
</span></span><span><span>                Err(format!(<span>"Could not find module </span><span>{}</span><span>"</span>, module_name))
</span></span><span><span>            }
</span></span><span><span>        },
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><p>For now, we’ll take only the fully qualified name in <code>module.dll!functionName</code> <a aria-describedby="footnote-label" href="#fully-qualified">form</a>, and allow only exact matches.</p><p>Using this function, we can add symbols to our evaluation grammar in <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/command.rs#L26">command.rs</a></p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>#[rust_sitter::language]</span>
</span></span><span><span>    <span>pub</span> <span>enum</span> <span>EvalExpr</span> {
</span></span><span><span>        Number(<span>#[rust_sitter::leaf(pattern = r</span><span>"(\d+|0x[0-9a-fA-F]+)"</span><span>, transform = parse_int)]</span> <span>u64</span>),
</span></span><span><span>        Symbol(<span>#[rust_sitter::leaf(pattern = r</span><span>"(([a-zA-Z0-9_@#.]+!)?[a-zA-Z0-9_@#.]+)"</span><span>, transform = parse_sym)]</span> String),
</span></span></code></pre></div><p>In order to evaluate symbols, the <code>evaluate_expression</code> function now needs a context that it can use to evaluate symbols against. For this, we’ll just pass in a structure with a reference to the <code>Process</code>.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span> <span>struct</span> <span>EvalContext</span><span>&lt;</span><span>'a</span><span>&gt;</span> {
</span></span><span><span>    <span>pub</span> process: <span>&amp;</span><span>'a</span> <span>mut</span> Process,
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>fn</span> <span>evaluate_expression</span>(expr: <span>EvalExpr</span>, context: <span>&amp;</span><span>mut</span> EvalContext) -&gt; Result<span>&lt;</span><span>u64</span>, String<span>&gt;</span> {
</span></span></code></pre></div><p>Note that it also returns a <code>Result</code> now because the name resolution can fail. Most of the function is unchanged besides passing the context through, and it now handles <code>EvalExpr::Symbol</code> by passing it to <code>name_resolution::resolve_to_address</code>.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>match</span> expr {
</span></span><span><span>        EvalExpr::Number(x) <span>=&gt;</span> Ok(x),
</span></span><span><span>        EvalExpr::Add(x, _, y) <span>=&gt;</span> Ok(evaluate_expression(<span>*</span>x, context)<span>?</span> <span>+</span> evaluate_expression(<span>*</span>y, context)<span>?</span>),
</span></span><span><span>        EvalExpr::Symbol(sym) <span>=&gt;</span> {
</span></span><span><span>            resolve_name_to_address(<span>&amp;</span>sym, context.process)
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><p>We can verify this is working by simply evaluating a symbol to an address and making sure it resolves back to the same symbol.</p><pre tabindex="0"><code>&gt; ? ntdll.dll!NtMapViewOfSection+0x14
 = 0x7FFE7360F154
[11254] ntdll.dll!NtMapViewOfSection+0x14
&gt; ln 0x7FFE7360F154
ntdll.dll!NtMapViewOfSection+0x14
[11254] ntdll.dll!NtMapViewOfSection+0x14
</code></pre><p>Success!</p><h2 id="keeping-track-of-breakpoints">Keeping track of breakpoints</h2><p>With the new functionality in the expression evaluator to evaluate symbols, we can add the commands for setting, clearing, and listing breakpoints. First, we add the breakpoint commands to the command grammar in <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/command.rs#L15">command.rs</a></p><div><pre tabindex="0"><code data-lang="rust"><span><span>        SetBreakpoint(<span>#[rust_sitter::leaf(text = </span><span>"bp"</span><span>)]</span> (), Box<span>&lt;</span>EvalExpr<span>&gt;</span>),
</span></span><span><span>        ListBreakpoints(<span>#[rust_sitter::leaf(text = </span><span>"bl"</span><span>)]</span> ()),
</span></span><span><span>        ClearBreakpoint(<span>#[rust_sitter::leaf(text = </span><span>"bc"</span><span>)]</span> (), Box<span>&lt;</span>EvalExpr<span>&gt;</span>),
</span></span></code></pre></div><p>The implementation of these commands need something to talk to, so we’ll create a new structure called BreakpointManager that keeps track of the breakpoints that should be set in the process, and create this at the start of the <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/main.rs#L81">main_debugger_loop</a></p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>fn</span> <span>main_debugger_loop</span>(process: <span>HANDLE</span>) {
</span></span><span><span>    <span>let</span> <span>mut</span> breakpoints <span>=</span> BreakpointManager::new();
</span></span></code></pre></div><p>We’ll get to the implementation of <code>BreakpointManager</code> in a minute, but first we can just see the simple implementation of <code>bp</code>, <code>bl</code>, and <code>bc</code> calling into the breakpoint manager.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>let</span> cmd <span>=</span> command::read_command();
</span></span><span><span>    <span>//...
</span></span></span><span><span><span></span>    <span>match</span> cmd {
</span></span><span><span>        <span>//...
</span></span></span><span><span><span></span>        CommandExpr::SetBreakpoint(_, expr) <span>=&gt;</span> {
</span></span><span><span>            <span>if</span> <span>let</span> Some(addr) <span>=</span> eval_expr(expr) {
</span></span><span><span>                breakpoints.add_breakpoint(addr);
</span></span><span><span>            }
</span></span><span><span>        }
</span></span><span><span>        CommandExpr::ListBreakpoints(_) <span>=&gt;</span> {
</span></span><span><span>            breakpoints.list_breakpoints(<span>&amp;</span><span>mut</span> process);
</span></span><span><span>        }
</span></span><span><span>        CommandExpr::ClearBreakpoint(_, expr) <span>=&gt;</span> {
</span></span><span><span>            <span>if</span> <span>let</span> Some(id) <span>=</span> eval_expr(expr) {
</span></span><span><span>                breakpoints.clear_breakpoint(id <span>as</span> <span>u32</span>);
</span></span><span><span>            }
</span></span><span><span>        }
</span></span></code></pre></div><p>The <code>BreakpointManager</code> contains the list of the breakpoints that have been requested by the user. It has functions for adding a breakpoint at a specified address, removing a breakpoint given its ID, and listing the breakpoints for the user.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>struct</span> <span>Breakpoint</span> {
</span></span><span><span>    addr: <span>u64</span>,
</span></span><span><span>    id: <span>u32</span>,
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>struct</span> <span>BreakpointManager</span> {
</span></span><span><span>    breakpoints: Vec::<span>&lt;</span>Breakpoint<span>&gt;</span>,
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>impl</span> BreakpointManager {
</span></span><span><span>
</span></span><span><span>    <span>pub</span> <span>fn</span> <span>add_breakpoint</span>(<span>&amp;</span><span>mut</span> self, addr: <span>u64</span>) {
</span></span><span><span>        self.breakpoints.push(Breakpoint{addr, id: <span>self</span>.get_free_id()});
</span></span><span><span>        self.breakpoints.sort_by(<span>|</span>a, b<span>|</span> a.id.cmp(<span>&amp;</span>b.id));
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>pub</span> <span>fn</span> <span>list_breakpoints</span>(<span>&amp;</span>self, process: <span>&amp;</span><span>mut</span> Process) {
</span></span><span><span>        <span>for</span> bp <span>in</span> self.breakpoints.iter() {
</span></span><span><span>            <span>if</span> <span>let</span> Some(sym) <span>=</span> name_resolution::resolve_address_to_name(bp.addr, process) {
</span></span><span><span>                println!(<span>"</span><span>{:3}</span><span> </span><span>{:#018x}</span><span> (</span><span>{}</span><span>)"</span>, bp.id, bp.addr, sym)
</span></span><span><span>            } <span>else</span> {
</span></span><span><span>                println!(<span>"</span><span>{:3}</span><span> </span><span>{:#018x}</span><span>"</span>, bp.id, bp.addr)
</span></span><span><span>            }            
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>pub</span> <span>fn</span> <span>clear_breakpoint</span>(<span>&amp;</span><span>mut</span> self, id: <span>u32</span>) {
</span></span><span><span>        self.breakpoints.retain(<span>|</span>x<span>|</span> x.id <span>!=</span> id)
</span></span><span><span>    }
</span></span></code></pre></div><p>We can test these commands to make sure breakpoints are tracked correctly, although we still need to apply the breakpoints to the target process before they’ll do anything.</p><pre tabindex="0"><code>Command line was: '"C:\git\HelloWorld\hello.exe" '
LoadDll: 7FF7E7420000   hello.exe
[5A70] 0x00007ffdaed4aa40
&gt; g
LoadDll: 7FFDAECF0000   ntdll.dll
[5A70] ntdll.dll!RtlUserThreadStart
&gt; bp ntdll.dll!RtlUserThreadStart
[5A70] ntdll.dll!RtlUserThreadStart
&gt; bl
  0 0x00007ffdaed4aa40 (ntdll.dll!RtlUserThreadStart)
[5A70] ntdll.dll!RtlUserThreadStart
&gt; bc 0
[5A70] ntdll.dll!RtlUserThreadStart
&gt; bl
[5A70] ntdll.dll!RtlUserThreadStart
&gt; 
</code></pre><h2 id="applying-breakpoints">Applying breakpoints</h2><p>Finally, we can get to the fun part where we apply the breakpoints to a process. There are two types of breakpoints, software breakpoints and hardware breakpoints. Of the two, hardware breakpoints are less complicated, so we’ll start with those. On x86 processors the hardware breakpoints are controlled via the <a href="https://wiki.osdev.org/CPU_Registers_x86-64#Debug_Registers">“Debug Registers”</a>. There are four hardware breakpoints available on current CPUs. Debug registers DR0 through DR3 are used to specify the address of the breakpoint. Register DR6 is a status register to determine when a breakpoint is hit. And DR7 is a control register to specify the attributes of each hardware breakpoint. Note that there are a number of fields packed together in DR7, so we’ll use a little helper to set these fields.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>// Helper function to set a value at a specific bit offset.
</span></span></span><span><span><span></span><span>fn</span> <span>set_bits</span><span>&lt;</span>T: <span>PrimInt</span><span>&gt;</span>(val: <span>&amp;</span><span>mut</span> T, set_val: <span>T</span>, start_bit: <span>usize</span>, bit_count: <span>usize</span>) {
</span></span><span><span>    <span>// First, mask out the relevant bits
</span></span></span><span><span><span></span>    <span>let</span> max_bits <span>=</span> std::mem::size_of::<span>&lt;</span>T<span>&gt;</span>() <span>*</span> <span>8</span>;
</span></span><span><span>    <span>let</span> mask: <span>T</span> <span>=</span> T::max_value() <span>&lt;&lt;</span> (max_bits <span>-</span> bit_count);
</span></span><span><span>    <span>let</span> mask: <span>T</span> <span>=</span> mask <span>&gt;&gt;</span> (max_bits <span>-</span> <span>1</span> <span>-</span> start_bit);
</span></span><span><span>    <span>let</span> inv_mask <span>=</span> <span>!</span>mask;
</span></span><span><span>
</span></span><span><span>    <span>*</span>val <span>=</span> <span>*</span>val <span>&amp;</span> inv_mask;
</span></span><span><span>    <span>*</span>val <span>=</span> <span>*</span>val <span>|</span> (set_val <span>&lt;&lt;</span> (start_bit <span>+</span> <span>1</span> <span>-</span> bit_count));
</span></span><span><span>}
</span></span></code></pre></div><p>To manipulate these registers, we’ll use the <code>GetThreadContext</code>/<code>SetThreadContext</code> functions to set the registers to the state needed for the requested breakpoints. Note that the debug registers are maintained for each thread separately, so we could theoretically set different breakpoints for each thread, or filter a breakpoint to a specific thread. That functionality won’t be implemented in DbgRs for now, and we’ll just apply the same breakpoints to all threads. To start, we’ll loop over all of the threads in the process and retrieve each thread’s context:</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>pub</span> <span>fn</span> <span>apply_breakpoints</span>(<span>&amp;</span><span>mut</span> self, process: <span>&amp;</span><span>mut</span> Process, resume_thread_id: <span>u32</span>, _memory_source: <span>&amp;</span><span>dyn</span> MemorySource) {
</span></span><span><span>
</span></span><span><span>        <span>for</span> thread_id <span>in</span> process.iterate_threads() {
</span></span><span><span>            <span>let</span> <span>mut</span> ctx: <span>AlignedContext</span> <span>=</span> <span>unsafe</span> { std::mem::zeroed() };
</span></span><span><span>            ctx.context.ContextFlags <span>=</span> <span>CONTEXT_ALL</span>;            
</span></span><span><span>            <span>let</span> thread <span>=</span> AutoClosedHandle(<span>unsafe</span> {
</span></span><span><span>                OpenThread(
</span></span><span><span>                    <span>THREAD_GET_CONTEXT</span> <span>|</span> <span>THREAD_SET_CONTEXT</span>,
</span></span><span><span>                    <span>FALSE</span>,
</span></span><span><span>                    <span>*</span>thread_id,
</span></span><span><span>                )
</span></span><span><span>            });
</span></span><span><span>            <span>let</span> ret <span>=</span> <span>unsafe</span> { GetThreadContext(thread.handle(), <span>&amp;</span><span>mut</span> ctx.context) };
</span></span></code></pre></div><p>We’ll then loop over the requested breakpoints. We need to set four pieces of information for each one. The three fields to set are the LEN (length), RW (access type), and LE (local enable) configuration for each breakpoint. We’ll set the <a aria-describedby="footnote-label" href="#execute-len">LEN to 0</a>, which indicates 1 byte. We’ll set RW to 0, which means “break on instruction execution” (we would use a value of 1 indicate break on read or a value of 3 to indicate break on read or write). Finally, we’ll set the “local enable” bit to 1 indicating that the breakpoint should be enabled.</p><p><img src="https://www.timdbg.com/dr7.png" alt="DR7 layout"></p><div><pre tabindex="0"><code data-lang="rust"><span><span>
</span></span><span><span>    <span>for</span> idx <span>in</span> <span>0</span><span>..</span><span>4</span> {
</span></span><span><span>        <span>if</span> self.breakpoints.len() <span>&gt;</span> idx {
</span></span><span><span>            
</span></span><span><span>            <span>// The DR7_* variables are a set of constants with the correct offsets and sizes for each
</span></span></span><span><span><span></span>            <span>// field of DR7.
</span></span></span><span><span><span></span>            set_bits(<span>&amp;</span><span>mut</span> ctx.context.Dr7, <span>0</span>, <span>DR7_LEN_BIT</span>[idx], <span>DR7_LEN_SIZE</span>);
</span></span><span><span>            set_bits(<span>&amp;</span><span>mut</span> ctx.context.Dr7, <span>0</span>, <span>DR7_RW_BIT</span>[idx], <span>DR7_RW_SIZE</span>);
</span></span><span><span>            set_bits(<span>&amp;</span><span>mut</span> ctx.context.Dr7, <span>1</span>, <span>DR7_LE_BIT</span>[idx], <span>1</span>);
</span></span></code></pre></div><p>The appropriate DR0-DR3 value will be set to the address of the resolved breakpoint.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>
</span></span><span><span>            <span>match</span> idx {
</span></span><span><span>                <span>0</span> <span>=&gt;</span> ctx.context.Dr0 <span>=</span> self.breakpoints[idx].addr,
</span></span><span><span>                <span>1</span> <span>=&gt;</span> ctx.context.Dr1 <span>=</span> self.breakpoints[idx].addr,
</span></span><span><span>                <span>2</span> <span>=&gt;</span> ctx.context.Dr2 <span>=</span> self.breakpoints[idx].addr,
</span></span><span><span>                <span>3</span> <span>=&gt;</span> ctx.context.Dr3 <span>=</span> self.breakpoints[idx].addr,
</span></span><span><span>                _ <span>=&gt;</span> (),
</span></span><span><span>            }
</span></span></code></pre></div><p>Finally, we’ll make sure to disable any breakpoints that we are not using. Note that the code assumes that the debugger “owns” the debug registers and that the target process is not using them in any way. This is typically true, but there are cases where the target process will be using the debug registers for its own purpose, or are manipulated as an anti-debugging technique. To keep things simple, we won’t worry about that and just clear the local enable (LE) bit.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>        } <span>else</span> {
</span></span><span><span>            <span>// Disable any breakpoints that we aren't using.
</span></span></span><span><span><span></span>            set_bits(<span>&amp;</span><span>mut</span> ctx.context.Dr7, <span>0</span>, <span>DR7_LE_BIT</span>[idx], <span>1</span>);
</span></span><span><span>            <span>break</span>;
</span></span><span><span>        }    
</span></span><span><span>    }
</span></span></code></pre></div><p>This new function, <code>apply_breakpoints</code>, will be called from the <code>main_debugger_loop</code> right before we call <code>ContinueDebugEvent</code>. That will ensure that we set up all thread contexts with the correct breakpoint state. Note that because Windows sends a debug event for thread creation, we’ll have an opportunity to set the breakpoint state for all new threads that are created.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    breakpoints.apply_breakpoints(<span>&amp;</span><span>mut</span> process, event_context.thread_id, mem_source.as_ref());
</span></span><span><span>    
</span></span><span><span>    <span>unsafe</span> {
</span></span><span><span>        ContinueDebugEvent(
</span></span><span><span>            event_context.process_id,
</span></span><span><span>            event_context.thread_id,
</span></span><span><span>            continue_status,
</span></span><span><span>        );
</span></span><span><span>    }
</span></span></code></pre></div><h2 id="handling-breakpoint-exceptions">Handling breakpoint exceptions</h2><p>When the CPU tries to execute an instruction that is marked with a debug register, it generates a debug exception (#DB) as a <a href="https://wiki.osdev.org/Exceptions">fault</a> (It’s important to note this is a fault, and not a trap. More on that later). Windows delivers this to a debugger as an exception event with exception code 0x80000004. The thread context will also have a flag set in DR6 indicating which breakpoint was hit. We’ll add some code in the exception event handler letting the breakpoint manager check if a breakpoint was hit.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>        <span>match</span> debug_event {
</span></span><span><span>            DebugEvent::Exception { first_chance, exception_code } <span>=&gt;</span> {
</span></span><span><span>                <span>//...
</span></span></span><span><span><span></span>                <span>if</span> <span>let</span> Some(bp_index) <span>=</span> breakpoints.was_breakpoint_hit(<span>&amp;</span>ctx.context) {
</span></span><span><span>                    println!(<span>"Breakpoint </span><span>{}</span><span> hit"</span>, bp_index);
</span></span><span><span>                    <span>// It's important to use DBG_CONTINUE with ContinueDebugEvent or else the breakpoint will be treated
</span></span></span><span><span><span></span>                    <span>// as an exception to be handled by the target process.
</span></span></span><span><span><span></span>                    continue_status <span>=</span> <span>DBG_CONTINUE</span>;
</span></span><span><span>                }
</span></span><span><span>                <span>//...
</span></span></span></code></pre></div><p>The breakpoint manager will just check DR6 to see if any of the bits were set that correspond to a hardware breakpoint triggering.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>pub</span> <span>fn</span> <span>was_breakpoint_hit</span>(<span>&amp;</span>self, thread_context: <span>&amp;</span><span>CONTEXT</span>) -&gt; Option<span>&lt;</span><span>u32</span><span>&gt;</span> {
</span></span><span><span>        <span>for</span> idx <span>in</span> <span>0</span><span>..</span>self.breakpoints.len() {
</span></span><span><span>            <span>if</span> get_bit(thread_context.Dr6, <span>DR6_B_BIT</span>[idx]) {
</span></span><span><span>                <span>return</span> Some(idx <span>as</span> <span>u32</span>);
</span></span><span><span>            }
</span></span><span><span>        }
</span></span><span><span>        None
</span></span><span><span>    }
</span></span></code></pre></div><p>Remember how I mentioned that hardware breakpoints trigger debug exceptions as a <a aria-describedby="footnote-label" href="#debug-fault">fault</a>? That’s important because a “fault” exception triggers <em>before</em> the instruction has a chance to execute. That lets us examine state before the instruction executes, which is what we want for a debugger. But since it is a fault, resuming the execution of the program just causes the program to break in again! On some architectures, this might be complicated to get past, but on x86 we simply have to set the “resume flag”, which is one of the bits in the EFlags registers that often gets overlooked. The resume flag causes the processor to ignore instruction breakpoints for a single instruction execution. It is set back to 0 right after the debug registers would have been checked, which makes it a convenient tool for resuming execution. We’ll set this flag on whatever thread caused the debugger to break in, regardless of whether the breakpoint has hit or not. (Some debuggers will only set RF when a breakpoint was hit). We’ll set the resume flag inside the <code>apply_breakpoints</code> function, since it’s already manipulating the register contexts of all threads.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>if</span> <span>*</span>thread_id <span>==</span> resume_thread_id {
</span></span><span><span>        set_bits(<span>&amp;</span><span>mut</span> ctx.context.EFlags, <span>1</span>, <span>EFLAG_RF</span>, <span>1</span>);
</span></span><span><span>    }
</span></span></code></pre></div><h2 id="testing-it-out">Testing it out</h2><p>Now that we can set a breakpoint, apply a breakpoint, and handle a breakpoint exception, we’re ready to test out the new breakpoint functionality. To do that, we’ll just continue execution until kernelbase is loaded, and then set a breakpoint on kernelbase!GetLastError, which is a very frequently used function that should get called almost immediately.</p><pre tabindex="0"><code>Command line was: '"C:\git\HelloWorld\hello.exe" '
LoadDll: 7FF7E7420000   hello.exe
[5CF8] 0x00007ffdaed4aa40
&gt; g
LoadDll: 7FFDAECF0000   ntdll.dll
[5CF8] ntdll.dll!RtlUserThreadStart
&gt; g
LoadDll: 7FFDAD6E0000   C:\Windows\System32\KERNEL32.DLL
[5CF8] ntdll.dll!NtMapViewOfSection+0x14
&gt; g
LoadDll: 7FFDAC0A0000   C:\Windows\System32\KERNELBASE.dll
[5CF8] ntdll.dll!NtMapViewOfSection+0x14
&gt; bp kernelbase.dll!GetLastError
[5CF8] ntdll.dll!NtMapViewOfSection+0x14
&gt; g
Breakpoint 0 hit
[5CF8] C:\Windows\System32\KERNELBASE.dll!GetLastError
&gt; 
</code></pre><p>It works! It’s almost starting to feel like a real debugger. A few very important things are left though. To start with, we can’t see the functions that are on the call stack. Viewing the stack is probably the single most important analysis feature of a debugger. So that’s likely where we’re going next.</p><p>I hope you found this post interesting and informative! Have a question or suggestion? Let me know! You can find me on <a href="https://twitter.com/timmisiak">Twitter</a>, <a href="https://dbg.social/@tim">Mastodon</a>, and <a href="https://bsky.app/profile/timdbg.com">Bluesky</a>.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Arena allocator tips and tricks (231 pts)]]></title>
            <link>https://nullprogram.com/blog/2023/09/27/</link>
            <guid>37670740</guid>
            <pubDate>Wed, 27 Sep 2023 05:59:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nullprogram.com/blog/2023/09/27/">https://nullprogram.com/blog/2023/09/27/</a>, See on <a href="https://news.ycombinator.com/item?id=37670740">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en">
<article>
  
  <time datetime="2023-09-27">
    September 27, 2023
  </time>
  <p>
    nullprogram.com/blog/2023/09/27/
  </p>

  <p>Over the past year I’ve refined my approach to <a href="https://www.rfleury.com/p/untangling-lifetimes-the-arena-allocator">arena allocation</a>.
With practice, it’s effective, simple, and fast; typically as easy to use
as garbage collection but without the costs. Depending on need, an
allocator can weigh just 7–25 lines of code — perfect when <a href="https://nullprogram.com/blog/2023/02/15/">lacking a
runtime</a>. With the core details of my own technique settled, now is a
good time to document and share lessons learned. This is certainly not the
only way to approach arena allocation, but these are practices I’ve worked
out to simplify programs and reduce mistakes.</p>

<p>An arena is a memory buffer and an offset into that buffer, initially
zero. To allocate an object, grab a pointer at the offset, advance the
offset by the size of the object, and return the pointer. There’s a little
more to it, such as ensuring alignment and availability. We’ll get to
that. Objects are not freed individually. Instead, groups of allocations
are freed at once by restoring the offset to an earlier value. Without
individual lifetimes, you don’t need to write destructors, nor do your
programs need to walk data structures at run time to take them apart. You
also no longer need to worry about memory leaks.</p>

<p>A minority of programs inherently require general purpose allocation, at
least in part, that linear allocation cannot fulfill. This includes, for
example, most programming language runtimes. If you like arenas, avoid
accidentally create such a situation through an over-flexible API that
allows callers to assume you have general purpose allocation underneath.</p>

<p>To get warmed up, here’s my style of arena allocation in action that shows
off multiple features:</p>

<div><pre><code><span>typedef</span> <span>struct</span> <span>{</span>
    <span>uint8_t</span>  <span>*</span><span>data</span>
    <span>ptrdiff_t</span> <span>len</span><span>;</span>
<span>}</span> <span>str</span><span>;</span>

<span>typedef</span> <span>struct</span> <span>{</span>
    <span>strlist</span> <span>*</span><span>next</span><span>;</span>
    <span>str</span>      <span>item</span><span>;</span>
<span>}</span> <span>strlist</span><span>;</span>

<span>typedef</span> <span>struct</span> <span>{</span>
    <span>str</span> <span>head</span><span>;</span>
    <span>str</span> <span>tail</span><span>;</span>
<span>}</span> <span>strpair</span><span>;</span>

<span>// Defined elsewhere</span>
<span>void</span>    <span>towidechar</span><span>(</span><span>wchar_t</span> <span>*</span><span>,</span> <span>ptrdiff_t</span><span>,</span> <span>str</span><span>);</span>
<span>str</span>     <span>loadfile</span><span>(</span><span>wchar_t</span> <span>*</span><span>,</span> <span>arena</span> <span>*</span><span>);</span>
<span>strpair</span> <span>cut</span><span>(</span><span>str</span><span>,</span> <span>uint8_t</span><span>);</span>

<span>strlist</span> <span>*</span><span>getlines</span><span>(</span><span>str</span> <span>path</span><span>,</span> <span>arena</span> <span>*</span><span>perm</span><span>,</span> <span>arena</span> <span>scratch</span><span>)</span>
<span>{</span>
    <span>int</span> <span>max_path</span> <span>=</span> <span>1</span><span>&lt;&lt;</span><span>15</span><span>;</span>
    <span>wchar_t</span> <span>*</span><span>wpath</span> <span>=</span> <span>new</span><span>(</span><span>&amp;</span><span>scratch</span><span>,</span> <span>wchar_t</span><span>,</span> <span>max_path</span><span>);</span>
    <span>towidechar</span><span>(</span><span>wpath</span><span>,</span> <span>max_path</span><span>,</span> <span>path</span><span>);</span>

    <span>strpair</span> <span>pair</span> <span>=</span> <span>{</span><span>0</span><span>};</span>
    <span>pair</span><span>.</span><span>tail</span> <span>=</span> <span>loadfile</span><span>(</span><span>wpath</span><span>,</span> <span>perm</span><span>);</span>

    <span>strlist</span> <span>*</span><span>head</span> <span>=</span> <span>0</span><span>;</span>
    <span>strlist</span> <span>**</span><span>tail</span> <span>=</span> <span>&amp;</span><span>head</span><span>;</span>
    <span>while</span> <span>(</span><span>pair</span><span>.</span><span>tail</span><span>.</span><span>len</span><span>)</span> <span>{</span>
        <span>pair</span> <span>=</span> <span>cut</span><span>(</span><span>pair</span><span>.</span><span>tail</span><span>,</span> <span>'\n'</span><span>);</span>
        <span>*</span><span>tail</span> <span>=</span> <span>new</span><span>(</span><span>perm</span><span>,</span> <span>strlist</span><span>,</span> <span>1</span><span>);</span>
        <span>(</span><span>*</span><span>tail</span><span>)</span><span>-&gt;</span><span>item</span> <span>=</span> <span>pair</span><span>.</span><span>head</span><span>;</span>
        <span>tail</span> <span>=</span> <span>&amp;</span><span>(</span><span>*</span><span>tail</span><span>)</span><span>-&gt;</span><span>next</span><span>;</span>
    <span>}</span>
    <span>return</span> <span>head</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Take note of these details, each to be later discussed in detail:</p>

<ul>
  <li>
    <p><code>getlines</code> takes two arenas, “permanent” and “scratch”. The former is
for objects that will be returned to the caller. The latter is for
temporary objects whose lifetime ends when the function returns. They
have stack lifetimes just like local variables.</p>
  </li>
  <li>
    <p>Objects are not explicitly freed. Instead, <strong>all allocations from a
scratch arena are implicitly freed upon return</strong>. This would include
error return paths automatically.</p>
  </li>
  <li>
    <p>The <strong>scratch arena is passed by copy</strong> — i.e. a copy of the “header”
not the <em>memory region</em> itself. Allocating only changes the local copy,
and so cannot survive the return. The semantics are obvious to callers,
so they’re less likely to get mixed up.</p>
  </li>
  <li>
    <p>While <code>wpath</code> could be an automatic local variable, it’s relatively
large for the stack, so it’s allocated out of the scratch arena. A
scratch arena safely permits large, dynamic allocations that would never
be safe on the stack. In other words, <strong>a sane <a href="https://man7.org/linux/man-pages/man3/alloca.3.html"><code>alloca</code></a>!</strong>
Same for variable-length arrays (VLAs). A scratch arena means you’ll
never be tempted to use either of these terrible ideas.</p>
  </li>
  <li>
    <p>The second parameter to <code>new</code> is a type, so it’s obviously a macro. As
you will see momentarily, this is not some complex macro magic, just a
convenience one-liner. There is no implicit cast, and you will get a
compiler diagnostic if the type is incorrect.</p>
  </li>
  <li>
    <p>Despite all the allocation, there is not a single <code>sizeof</code> operator nor
size computation. That’s because <strong>size computations are a major source
of defects.</strong> That job is handled by specialized code.</p>
  </li>
  <li>
    <p><strong>Allocation failures are not communicated by a null return</strong>. Lifting
this burden greatly simplifies programs. Instead such errors are handled
non-locally by the arena.</p>
  </li>
  <li>
    <p>All allocations are <strong>zero-initialized by default</strong>. This makes for
simpler, less error-prone programs. When that’s too expensive, this can
become an opt-out without changing the default.</p>
  </li>
</ul>

<p>See also <a href="https://nullprogram.com/blog/2023/01/18/">u-config</a>.</p>

<h3 id="an-arena-implementation">An arena implementation</h3>

<p>An arena suitable for most cases can be this simple:</p>

<div><pre><code><span>typedef</span> <span>struct</span> <span>{</span>
    <span>char</span> <span>*</span><span>beg</span><span>;</span>
    <span>char</span> <span>*</span><span>end</span><span>;</span>
<span>}</span> <span>arena</span><span>;</span>

<span>void</span> <span>*</span><span>alloc</span><span>(</span><span>arena</span> <span>*</span><span>a</span><span>,</span> <span>ptrdiff_t</span> <span>size</span><span>,</span> <span>ptrdiff_t</span> <span>align</span><span>,</span> <span>ptrdiff_t</span> <span>count</span><span>)</span>
<span>{</span>
    <span>ptrdiff_t</span> <span>avail</span> <span>=</span> <span>a</span><span>-&gt;</span><span>end</span> <span>-</span> <span>a</span><span>-&gt;</span><span>beg</span><span>;</span>
    <span>ptrdiff_t</span> <span>padding</span> <span>=</span> <span>-</span><span>(</span><span>uintptr_t</span><span>)</span><span>a</span><span>-&gt;</span><span>beg</span> <span>&amp;</span> <span>(</span><span>align</span> <span>-</span> <span>1</span><span>);</span>
    <span>if</span> <span>(</span><span>count</span> <span>&gt;</span> <span>(</span><span>avail</span> <span>-</span> <span>padding</span><span>)</span><span>/</span><span>size</span><span>)</span> <span>{</span>
        <span>abort</span><span>();</span>  <span>// one possible out-of-memory policy</span>
    <span>}</span>
    <span>ptrdiff_t</span> <span>total</span> <span>=</span> <span>size</span> <span>*</span> <span>count</span><span>;</span>
    <span>char</span> <span>*</span><span>p</span> <span>=</span> <span>a</span><span>-&gt;</span><span>beg</span> <span>+</span> <span>padding</span><span>;</span>
    <span>a</span><span>-&gt;</span><span>beg</span> <span>+=</span> <span>padding</span> <span>+</span> <span>total</span><span>;</span>
    <span>return</span> <span>memset</span><span>(</span><span>p</span><span>,</span> <span>0</span><span>,</span> <span>total</span><span>);</span>
<span>}</span>
</code></pre></div>

<p>Yup, just a pair of pointers! When allocating, all sizes are signed <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1428r0.pdf">just
as they ought to be</a>. Unsigned sizes are another historically
common source of defects, and offer no practical advantages in return.
Case in point exercise for the reader: Change each <code>ptrdiff_t</code> to <code>size_t</code>
in <code>alloc</code>, find the defect that results, then fix it.</p>

<p>The <code>align</code> parameter allows the arena to handle any unusual alignments,
something that’s surprisingly difficult to do with libc. It’s difficult to
appreciate its usefulness until it’s convenient.</p>

<p>The <code>uintptr_t</code> business may look unusual if you’ve never come across it
before. To align <code>beg</code>, we need to compute the number of bytes to advance
the address (<code>padding</code>) until the alignment evenly divides the address.
The modulo with <code>align</code> computes the number of bytes it’s since the last
alignment:</p>



<p>We can’t operate numerically on an address like this, so in the code we
first convert to <code>uintptr_t</code>. Alignment is always a power of two, which
notably excludes zero, so no worrying about division by zero. That also
means we can compute modulo by subtracting one and masking with AND:</p>

<div><pre><code>extra = addr &amp; (align - 1)
</code></pre></div>

<p>However, we want the number of bytes to advance to the next alignment,
which is the inverse:</p>

<div><pre><code>padding = -addr &amp; (align - 1)
</code></pre></div>

<p>Add the <code>uintptr_t</code> cast and you have the code in <code>alloc</code>.</p>

<p>The <code>if</code> tests if there’s enough memory and simultaneously for overflow on
<code>size*count</code>. If either fails, it invokes the out-of-memory policy, which
in this case is <code>abort</code>. I strongly recommend that, at least when testing,
always having <em>something</em> in place to, at minimum, abort when allocation
fails, even when you think it cannot happen. It’s easy to use more memory
than you anticipate, and you want a reliable signal when it happens.</p>

<p>An alternative policy is to <a href="https://nullprogram.com/blog/2023/02/12/">longjmp to a “handler”</a>, which with
GCC and Clang doesn’t even require runtime support. In that case add a
<code>jmp_buf</code> to the arena:</p>

<div><pre><code><span>typedef</span> <span>struct</span> <span>{</span>
    <span>char</span>  <span>*</span><span>beg</span><span>;</span>
    <span>char</span>  <span>*</span><span>end</span><span>;</span>
    <span>void</span> <span>**</span><span>jmp_buf</span><span>;</span>
<span>}</span> <span>arena</span><span>;</span>

<span>void</span> <span>*</span><span>alloc</span><span>(...)</span>
<span>{</span>
    <span>// ...</span>
    <span>if</span> <span>(</span><span>count</span> <span>&gt;</span> <span>(</span><span>avail</span> <span>-</span> <span>padding</span><span>)</span><span>/</span><span>size</span><span>)</span> <span>{</span>
        <span>__builtin_longjmp</span><span>(</span><span>a</span><span>-&gt;</span><span>jmp_buf</span><span>,</span> <span>1</span><span>);</span>
    <span>}</span>
    <span>// ...</span>
<span>}</span>

<span>bool</span> <span>example</span><span>(...,</span> <span>arena</span> <span>scratch</span><span>)</span>
<span>{</span>
    <span>void</span> <span>*</span><span>jmp_buf</span><span>[</span><span>5</span><span>];</span>
    <span>if</span> <span>(</span><span>__builtin_setjmp</span><span>(</span><span>jmp_buf</span><span>))</span> <span>{</span>
        <span>return</span> <span>0</span><span>;</span>
    <span>}</span>
    <span>scratch</span><span>.</span><span>jmp_buf</span> <span>=</span> <span>jmp_buf</span><span>;</span>
    <span>// ...</span>
    <span>return</span> <span>1</span><span>;</span>
<span>}</span>
</code></pre></div>

<p><code>example</code> returns failure to the caller if it runs out of memory, without
needing to check individual allocations and, thanks to the implicit free
of scratch arenas, without needing to clean up. If callees receiving the
scratch arena don’t set their own <code>jmp_buf</code>, they’ll return here, too. In
a real program you’d probably wrap the <code>setjmp</code> setup in a macro.</p>

<p>Suppose zeroing is too expensive or unnecessary in some cases. Add a flag
to opt out:</p>

<div><pre><code><span>void</span> <span>*</span><span>alloc</span><span>(...,</span> <span>int</span> <span>flags</span><span>)</span>
<span>{</span>
    <span>// ...</span>
    <span>return</span> <span>flag</span><span>&amp;</span><span>NOZERO</span> <span>?</span> <span>p</span> <span>:</span> <span>memset</span><span>(</span><span>p</span><span>,</span> <span>0</span><span>,</span> <span>total</span><span>);</span>
<span>}</span>
</code></pre></div>

<p>Similarly, perhaps there’s a critical moment where you’re holding a
non-memory resource (lock, file handle), or you don’t want allocation
failure to be fatal. In either case, it important that the out-of-memory
policy isn’t invoked. You could request a “soft” failure with another
flag, and then do the usual null pointer check:</p>

<div><pre><code><span>void</span> <span>*</span><span>alloc</span><span>(...,</span> <span>int</span> <span>flags</span><span>)</span>
<span>{</span>
    <span>// ...</span>
    <span>if</span> <span>(</span><span>count</span> <span>&gt;</span> <span>(</span><span>avail</span> <span>-</span> <span>padding</span><span>)</span><span>/</span><span>size</span><span>)</span> <span>{</span>
        <span>if</span> <span>(</span><span>flags</span> <span>&amp;</span> <span>SOFTFAIL</span><span>)</span> <span>{</span>
            <span>return</span> <span>0</span><span>;</span>
        <span>}</span>
        <span>abort</span><span>();</span>
    <span>}</span>
    <span>// ...</span>
<span>}</span>
</code></pre></div>

<p>Most non-trivial programs will probably at least one of these flags.</p>

<p>In case it wasn’t obvious, allocating an arena is simple:</p>

<div><pre><code><span>arena</span> <span>newarena</span><span>(</span><span>ptrdiff_t</span> <span>cap</span><span>)</span>
<span>{</span>
    <span>arena</span> <span>a</span> <span>=</span> <span>{</span><span>0</span><span>};</span>
    <span>a</span><span>.</span><span>beg</span> <span>=</span> <span>malloc</span><span>(</span><span>cap</span><span>);</span>
    <span>a</span><span>.</span><span>end</span> <span>=</span> <span>a</span><span>.</span><span>beg</span> <span>?</span> <span>a</span><span>.</span><span>beg</span><span>+</span><span>cap</span> <span>:</span> <span>0</span><span>;</span>
    <span>return</span> <span>a</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Or make a direct allocation from the operating system, e.g. <code>mmap</code>,
<code>VirtualAlloc</code>. Typically arena lifetime is the whole program, so you
don’t need to worry about freeing it. (Since you’re using arenas, you can
also turn off any memory leak checkers while you’re at it.)</p>

<p>If you need more arenas then you can always allocate smaller ones out of
the first! In multi-threaded applications, each thread may have at least
its own scratch arena.</p>

<h3 id="the-new-macro">The <code>new</code> macro</h3>

<p>I’ve shown <code>alloc</code>, but few parts of the program should be calling it
directly. Instead they have a macro to automatically handle the details. I
call mine <code>new</code>, though of course if you’re writing C++ you’ll need to
pick another name (<code>make</code>? <code>PushStruct</code>?):</p>

<div><pre><code><span>#define new(a, t, n)  (t *)alloc(a, sizeof(t), _Alignof(t), n)
</span></code></pre></div>

<p>The cast is an extra compile-time check, especially useful for avoiding
mistakes in levels of indirection. It also keeps normal code from directly
using the <code>sizeof</code> operator, which is easy to misuse. If you added a
<code>flags</code> parameter, pass in zero for this common case. Keep in mind that
the goal of this macro is to make common allocation simple and robust.</p>

<p>Often you’ll allocate single objects, and so the count is 1. If you think
that’s ugly, you could make variadic version of <code>new</code> that fills in common
defaults. In fact, that’s partly why I put <code>count</code> last!</p>

<div><pre><code><span>#define new(...)            newx(__VA_ARGS__,new4,new3,new2)(__VA_ARGS__)
#define newx(a,b,c,d,e,...) e
#define new2(a, t)          (t *)alloc(a, sizeof(t), alignof(t), 1, 0)
#define new3(a, t, n)       (t *)alloc(a, sizeof(t), alignof(t), n, 0)
#define new4(a, t, n, f)    (t *)alloc(a, sizeof(t), alignof(t), n, f)
</span></code></pre></div>

<p>Not quite so simple, but it optionally makes for more streamlined code:</p>

<div><pre><code><span>thing</span> <span>*</span><span>t</span>   <span>=</span> <span>new</span><span>(</span><span>perm</span><span>,</span> <span>thing</span><span>);</span>
<span>thing</span> <span>*</span><span>ts</span>  <span>=</span> <span>new</span><span>(</span><span>perm</span><span>,</span> <span>thing</span><span>,</span> <span>1000</span><span>);</span>
<span>char</span>  <span>*</span><span>buf</span> <span>=</span> <span>new</span><span>(</span><span>perm</span><span>,</span> <span>char</span><span>,</span> <span>len</span><span>,</span> <span>NOZERO</span><span>);</span>
</code></pre></div>

<p>Side note: If <code>sizeof</code> should be avoided, what about array lengths? That’s
part of the problem! Hardly ever do you want the <em>size</em> of an array, but
rather the <em>number of elements</em>. That includes <code>char</code> arrays where this
happens to be the same number. So instead, define a <code>countof</code> macro that
uses <code>sizeof</code> to compute the value you actually want. I like to have this
whole collection:</p>

<div><pre><code><span>#define sizeof(x)    (ptrdiff_t)sizeof(x)
#define countof(a)   (sizeof(a) / sizeof(*(a)))
#define lengthof(s)  (countof(s) - 1)
</span></code></pre></div>

<p>Yes, you can convert <code>sizeof</code> into a macro like this! It won’t expand
recursively and bottoms out as an operator. <code>countof</code> also, of course,
produces a less error-prone signed count so users don’t fumble around with
<code>size_t</code>. <code>lengthof</code> statically produces null-terminated string length.</p>

<div><pre><code><span>char</span> <span>msg</span><span>[]</span> <span>=</span> <span>"hello world"</span><span>;</span>
<span>write</span><span>(</span><span>fd</span><span>,</span> <span>msg</span><span>,</span> <span>lengthof</span><span>(</span><span>msg</span><span>));</span>

<span>#define MSG "hello world"
</span><span>write</span><span>(</span><span>fd</span><span>,</span> <span>MSG</span><span>,</span> <span>lengthof</span><span>(</span><span>MSG</span><span>));</span>
</code></pre></div>

<h3 id="enhance-alloc-with-attributes">Enhance <code>alloc</code> with attributes</h3>

<p>At least for GCC and Clang, we can further improve <code>alloc</code> with three
function attributes:</p>

<div><pre><code><span>__attribute</span><span>((</span><span>malloc</span><span>,</span> <span>alloc_size</span><span>(</span><span>2</span><span>,</span> <span>4</span><span>),</span> <span>alloc_align</span><span>(</span><span>3</span><span>)))</span>
<span>void</span> <span>*</span><span>alloc</span><span>(...);</span>
</code></pre></div>

<p><code>malloc</code> indicates that the pointer returned by <code>alloc</code> does not alias any
existing object. Enables some significant optimizations that are otherwise
blocked, most often by breaking potential loop-carried dependencies.</p>

<p><code>alloc_size</code> tracks the allocation size for compile-time diagnostics and
run-time assertions (<a href="https://gcc.gnu.org/onlinedocs/gcc/Object-Size-Checking.html"><code>__builtin_object_size</code></a>). This generally
requires a non-zero optimization level. In other words, you will get a
compiler warnings about some out bounds accesses of arena objects, and
with Undefined Behavior Sanitizer you’ll get run-time bounds checking.
It’s a great <a href="https://nullprogram.com/blog/2019/01/25/">complement to fuzzing</a>.</p>

<p>In theory <code>alloc_align</code> may also allow better code generation, but I’ve
yet to observe a case. Consider it optional and low-priority. I mention it
only for completeness.</p>

<h3 id="arena-size-and-growth">Arena size and growth</h3>

<p>How large an arena should you allocate? The simple answer: As much as is
necessary for the program to successfully complete. Usually the cost of
untouched arena memory is low or even zero. Most programs should probably
have an upper limit, at which point they assume something has gone wrong.
Arenas allow this case to be handled gracefully, simplifying recovery and
paving the way for continued operation.</p>

<p>While a sufficient answer for most cases, it’s unsatisfying. There’s a
common assumption that programs should increase their memory usage as much
as needed and let the operating system respond if it’s too much. However,
if you’ve ever tried this yourself, you probably noticed that mainstream
operating systems don’t handle it well. The typical results are system
instability — thrashing, drivers crashing — possibly necessitating a
reboot.</p>

<p>If you insist on this route, on 64-bit hosts you can reserve a gigantic
virtual address space and gradually commit memory as needed. On Linux that
means leaning on overcommit by allocating the largest arena possible at
startup, which will automatically commit through use. <a href="https://nullprogram.com/blog/2019/12/29/">Use <code>MADV_FREE</code> to
decommit.</a></p>

<p>On Windows, <code>VirtualAlloc</code> handles reserve and commit separately. In
addition to the allocation offset, you need a commit offset. Then expand
the committed region ahead of the allocation offset as it grows. If you
ever manually reset the allocation offset, you could decommit as well, or
at least <code>MEM_RESET</code>. At some point commit may fail, which should then
trigger the out-of-memory policy, but the system is probably in poor shape
by that point — i.e. use an abort policy to release it all quickly.</p>

<h3 id="pointer-laundering-filthy-hack">Pointer laundering (filthy hack)</h3>

<p>While allocations out of an arena don’t require individual error checks,
allocating the arena itself at startup requires error handling. It would
be nice if the arena could be allocated out of <code>.bss</code> and punt that job to
the loader. While you <em>could</em> make a big, global <code>char[]</code> array to back
your arena, it’s technically not permitted (strict aliasing). A “clean”
<code>.bss</code> region could be obtained with a bit of assembly — <a href="https://sourceware.org/binutils/docs/as/Comm.html"><code>.comm</code></a>
plus assembly to get the address into C without involving an array. I
wanted a more portable solution, so I came up with this:</p>

<div><pre><code><span>arena</span> <span>getarena</span><span>(</span><span>void</span><span>)</span>
<span>{</span>
    <span>static</span> <span>char</span> <span>mem</span><span>[</span><span>1</span><span>&lt;&lt;</span><span>28</span><span>];</span>
    <span>arena</span> <span>r</span> <span>=</span> <span>{</span><span>0</span><span>};</span>
    <span>r</span><span>.</span><span>beg</span> <span>=</span> <span>mem</span><span>;</span>
    <span>asm</span> <span>(</span><span>""</span> <span>:</span> <span>"+r"</span><span>(</span><span>r</span><span>.</span><span>beg</span><span>));</span>  <span>// launder the pointer</span>
    <span>r</span><span>.</span><span>end</span> <span>=</span> <span>r</span><span>.</span><span>beg</span> <span>+</span> <span>countof</span><span>(</span><span>mem</span><span>);</span>
    <span>return</span> <span>r</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>The <code>asm</code> accepts a pointer and returns a pointer (<code>"+r"</code>). The compiler
cannot “see” that it’s actually empty, and so returns the same pointer.
The arena will be backed by <code>mem</code>, but by laundering the address through
<code>asm</code>, I’ve disconnected the pointer from its origin. As far the compiler
is concerned, this is some foreign, assembly-provided pointer, not a
pointer into <code>mem</code>. It can’t optimize away <code>mem</code> because it’s been given
to a mysterious assembly black box.</p>

<p>While inappropriate for a real project, I think it’s a neat trick.</p>

<h3 id="arena-friendly-container-data-structures">Arena-friendly container data structures</h3>

<p>In my initial example I used a linked list to stores lines. This data
structure is great with arenas. It only takes a few of lines of code to
implement a linked list on top of an arena, and no “destroy” code is
needed. Simple.</p>

<p>What about <a href="https://nrk.neocities.org/articles/hash-trees-and-tries">arena-backed associative arrays</a>? Or arena-backed
dynamic arrays? I have simple, fast, easy solutions for each, but that’s
the subject for my next article!</p>



  
  <ol></ol>

  

  <nav>
  
    
  
  
  </nav>
</article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: XRain – Explore rainfall statistics around the world (103 pts)]]></title>
            <link>https://xrain.info/data/</link>
            <guid>37669706</guid>
            <pubDate>Wed, 27 Sep 2023 03:36:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xrain.info/data/">https://xrain.info/data/</a>, See on <a href="https://news.ycombinator.com/item?id=37669706">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[China is flooding Taiwan with disinformation (194 pts)]]></title>
            <link>https://www.economist.com/asia/2023/09/26/china-is-flooding-taiwan-with-disinformation</link>
            <guid>37667874</guid>
            <pubDate>Wed, 27 Sep 2023 00:18:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/asia/2023/09/26/china-is-flooding-taiwan-with-disinformation">https://www.economist.com/asia/2023/09/26/china-is-flooding-taiwan-with-disinformation</a>, See on <a href="https://news.ycombinator.com/item?id=37667874">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p><span><a href="https://www.economist.com/asia/" data-analytics="sidebar:section"><span>Asia</span></a></span><span> | <!-- -->Strait up lies</span></p><h2>With elections looming, China wants Taiwanese voters to think America is their greatest threat</h2></section><div data-body-id="cp2"><p data-component="paragraph"><span data-caps="initial">I</span><small>n July one</small> of Taiwan’s top newspapers, <i>United Daily News</i>, published a story based on supposedly leaked minutes from a secret government meeting. America had asked <a href="https://www.economist.com/china/2023/06/19/when-it-comes-to-a-war-with-taiwan-many-chinese-urge-caution">Taiwan</a> to manufacture biological weapons at a lab run by the island’s defence ministry, the report claimed. Taiwanese and American officials quickly denied it. The allegedly leaked minutes, it transpired, were not written in the usual style of Taiwanese government records. They were filled with official-sounding phrases used in mainland China, but not Taiwan. This was likely Chinese disinformation, Taiwanese officials said. Yet the story spread to Taiwanese talk shows and influencers. Within weeks it had evolved into a wilder claim: Taiwan was going to collect 150,000 samples of Taiwanese blood and hand them over to the Americans, so that they could develop a virus to kill Chinese people.</p><p data-component="paragraph">This sort of disinformation is so widespread in Taiwan that analysts have given it a moniker: <i>yi mei lun</i>, or the “<small>US</small> scepticism” narrative. Its spread is becoming a major worry for Taiwan’s government and civil society in the run-up to a hugely important presidential election next January. Taiwanese voters will in effect be asked to decide whether Taiwan should remain aligned with America in strengthening deterrence against a possible Chinese invasion, or should move towards building ties with China. The opposition Kuomintang has called the vote a choice between “war and peace”, implying that the ruling Democratic Progressive Party’s hostility towards China will provoke it to attack. Chinese state actors have backed that framing, spreading narratives that portray <a href="https://www.economist.com/briefing/2023/03/09/america-and-china-are-preparing-for-a-war-over-taiwan">America</a>, not China, as the island’s biggest threat.  Much of the disinformation is intended to reinforce that false message.</p><p data-component="paragraph">Lo Ping-chen, a cabinet minister who since 2018 has been leading a government task force against disinformation, says it has “severely infiltrated” Taiwan’s society. “We used to think there was more during election season. But it’s now become normalised. It happens every day.” Most Taiwanese voters have little idea of this. A recent survey by Doublethink Lab, a Taiwanese group that studies disinformation, found that less than 20% of respondents believed the false information spread in Taiwan during elections came from abroad. Puma Shen, who heads Doublethink Lab, worries about the one-fifth of voters who are not aligned with any party and could be a decisive bloc. “Even if only 15% of voters are truly affected by Chinese disinformation, it takes only 7% of voters to change the election results,” he says.</p><p data-component="paragraph">A recent study of <small>US</small>-scepticism narratives by the Information Environment Research Centre (<small>IORG</small>), a Taiwanese research group, found that Chinese actors were helping to spread most of them. But more than half appeared to have Taiwanese origins. That suggests China is “piggybacking” on fissures in Taiwanese society, says Chihhao Yu, the report’s author. He suggests many Taiwanese have an “orphan mentality”: they fear abandonment by outsiders because of Taiwan’s experience of losing American diplomatic recognition in the 1970s.</p><p data-component="paragraph">Chinese actors are exploiting those fears, just as Russian disinformation exploited America’s racial and cultural cracks for the benefit of Donald Trump in 2016. Chinese disinformation in Taiwan also echoes Russian propaganda about the war in Ukraine, which claims America is behind the conflict (and is creating bioweapons in Ukrainian labs).</p><p data-component="paragraph">China has developed systematic means to make falsehoods trend in Taiwan, says Chien Yu-yen, a former journalist and author of a book about Chinese influence on Taiwan’s media. She points to a spurious claim that America “wants to blow up” <small>TSMC, </small>a Taiwanese chipmaker. It originated with a misleading video posted on Douyin, the Chinese version of TikTok, which featured an American lawmaker appearing to discuss the possibility. The following morning, a Taiwanese newspaper published a story about the video. Opposition lawmakers and talk-show hosts whipped up outrage. “The journey from China’s Douyin to Taiwan’s mass media, videos, newspapers and television took less than half a day,” says Ms Chien. Chinese state media amplified the narrative, as if merely commenting from the outside on a Taiwanese debate.</p><p data-component="paragraph">Taiwanese officials believe that many of the Taiwanese launching<small> US-</small>scepticism untruths are “local collaborators” taking orders and payments from China. But that is hard to prove, because the suspected Chinese funding is probably funnelled through Taiwanese businesspeople or public-relations firms. Wang Kun-yi, a local commentator who frequently writes <small>US</small>-scepticism narratives for Chinese media and pro-China Taiwanese media, defends his work as a commercial enterprise. All journalists in Taiwan serve the bias of their newspapers’ bosses, says Mr Wang, who has worked for both pro-independence and pro-unification newspapers. “Everyone just treats it as a job,” he says. “It’s a tool to feed yourself.”</p><p data-component="paragraph">Taiwan has laws against foreign infiltration and election influence, but they are limited to cases of proven state-sponsored activity. It has additional laws against spreading wilful falsehood in broadcast media, but they do not cover print or digital outlets. In 2020 the government revoked the licence of <small>CTI </small>News, a pro-China channel, citing repeated failures to verify information. <small>CTI</small> simply moved online.</p><p data-component="paragraph">The case sparked accusations of censorship, which Taiwan wants to avoid. So the government has resorted to more liberal methods of fighting disinformation. It has tried to improve media literacy, provide faster official clarifications and promote fact-checking organisations. But such means cannot match the speed of Chinese propaganda. In August Meta removed a network of more than 7,000 accounts, pages and groups that were spreading Chinese disinformation. But new accounts are easy to set up, a problem that will only accelerate with artificial intelligence, says Mr Lo.</p><p data-component="paragraph">Chinese disinformation has already distorted Taiwan’s public conversation. Will it move votes? Meta has noted that the Chinese disinformation network it removed was “high volume, low reach”, despite having a veneer of engagement designed to make the accounts look more popular than they were. Studies of Russian disinformation in America have found that it has little impact on voter preferences. Despite all the messaging in Chinese and Taiwanese media against the Democratic Progressive Party, its candidate, William Lai, is leading in the polls. And for all the scepticism about America, Taiwanese are even warier of China. A 2022 survey by Academia Sinica, a Taiwanese research institution, found 34% of respondents agreeing that America is a “credible” country. Only 9% said the same of China.</p><p data-component="paragraph">China has itself to blame. It recently surrounded Taiwan with warplanes and warships, even as its ruling Communist Party unveiled an integration plan promising benefits to Taiwanese people living in Fujian, a southern province near the island. Most Taiwanese know where their real threat comes from. But China’s insidious efforts to mislead them are increasing.<span>■</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deconstructing Go Type Parameters (132 pts)]]></title>
            <link>https://go.dev/blog/deconstructing-type-parameters</link>
            <guid>37667731</guid>
            <pubDate>Wed, 27 Sep 2023 00:05:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://go.dev/blog/deconstructing-type-parameters">https://go.dev/blog/deconstructing-type-parameters</a>, See on <a href="https://news.ycombinator.com/item?id=37667731">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-slug="/blog/deconstructing-type-parameters">
    
    <h2><a href="https://go.dev/blog/">The Go Blog</a></h2>
    

    
      
      
      
      <h2 id="slices-package-function-signatures">slices package function signatures</h2>
<p>The <a href="https://pkg.go.dev/slices#Clone" rel="noreferrer" target="_blank"><code>slices.Clone</code></a> function is
pretty simple: it makes a copy of a slice of any type.</p>
<pre><code>func Clone[S ~[]E, E any](s S) S {
    return append(s[:0:0], s...)
}
</code></pre>
<p>This works because appending to a slice with zero capacity will
allocate a new backing array.
The function body winds up being shorter than the function signature,
which is in part because the body is short, but also because the
signature is long.
In this blog post we’ll explain why the signature is written the way
that it is.</p>
<h2 id="simple-clone">Simple Clone</h2>
<p>We’ll start by writing a simple generic <code>Clone</code> function.
This is not the one in the <code>slices</code> package.
We want to take a slice of any element type, and return a new slice.</p>
<pre><code>func Clone1[E any](s []E) []E {
    // body omitted
}
</code></pre>
<p>The generic function <code>Clone1</code> has a single type parameter <code>E</code>.
It takes a single argument <code>s</code> which is a slice of type <code>E</code>, and it
returns a slice of the same type.
This signature is straightforward for anybody familiar with generics
in Go.</p>
<p>However, there is a problem.
Named slice types are not common in Go, but people do use them.</p>
<pre><code>// MySlice is a slice of strings with a special String method.
type MySlice []string

// String returns the printable version of a MySlice value.
func (s MySlice) String() string {
    return strings.Join(s, "+")
}
</code></pre>
<p>Let’s say that we want to make a copy of a <code>MySlice</code> and then get the
printable version, but with the strings in sorted order.</p>
<pre><code>func PrintSorted(ms MySlice) string {
    c := Clone1(ms)
    slices.Sort(c)
    return c.String() // FAILS TO COMPILE
}
</code></pre>
<p>Unfortunately, this doesn’t work.
The compiler reports an error:</p>
<pre><code>c.String undefined (type []string has no field or method String)
</code></pre>
<p>We can see the problem if we manually instantiate <code>Clone1</code> by
replacing the type parameter with the type argument.</p>
<pre><code>func InstantiatedClone1(s []string) []string
</code></pre>
<p>The <a href="https://go.dev/ref/spec#Assignability" rel="noreferrer" target="_blank">Go assignment rules</a> allow
us to pass a value of type <code>MySlice</code> to a parameter of type
<code>[]string</code>, so calling <code>Clone1</code> is fine.
But <code>Clone1</code> will return a value of type <code>[]string</code>, not a value of
type <code>MySlce</code>.
The type <code>[]string</code> doesn’t have a <code>String</code> method, so the compiler
reports an error.</p>
<h2 id="flexible-clone">Flexible Clone</h2>
<p>To fix this problem, we have to write a version of <code>Clone</code> that
returns the same type as its argument.
If we can do that, then when we call <code>Clone</code> with a value of type
<code>MySlice</code>, it will return a result of type <code>MySlice</code>.</p>
<p>We know that it has to look something like this.</p>
<pre><code>func Clone2[S ?](s S) S // INVALID
</code></pre>
<p>This <code>Clone2</code> function returns a value that is the same type as its
argument.</p>
<p>Here I’ve written the constraint as <code>?</code>, but that’s just a
placeholder.
To make this work we need to write a constraint that will let us write
the body of the function.
For <code>Clone1</code> we could just use a constraint of <code>any</code> for the element
type.
For <code>Clone2</code> that won’t work: we want to require that <code>s</code> be a slice
type.</p>
<p>Since we know we want a slice, the constraint of <code>S</code> has to be a
slice.
We don’t care what the slice element type is, so let’s just call it
<code>E</code>, as we did with <code>Clone1</code>.</p>
<pre><code>func Clone3[S []E](s S) S // INVALID
</code></pre>
<p>This is still invalid, because we haven’t declared <code>E</code>.
The type argument for <code>E</code> can be any type, which means it also has to
be a type parameter itself.
Since it can be any type, its constraint is <code>any</code>.</p>
<pre><code>func Clone4[S []E, E any](s S) S
</code></pre>
<p>This is getting close, and at least it will compile, but we’re not
quite there yet.
If we compile this version, we get an error when we call <code>Clone4(ms)</code>.</p>
<pre><code>MySlice does not satisfy []string (possibly missing ~ for []string in []string)
</code></pre>
<p>The compiler is telling us that we can’t use the type argument
<code>MySlice</code> for the type parameter <code>S</code>, because <code>MySlice</code> does not
satisfy the constraint <code>[]E</code>.
That’s because <code>[]E</code> as a constraint only permits a slice type
literal, like <code>[]string</code>.
It doesn’t permit a named type like <code>MySlice</code>.</p>
<h2 id="underlying-type-constraints">Underlying type constraints</h2>
<p>As the error message hints, the answer is to add a <code>~</code>.</p>
<pre><code>func Clone5[S ~[]E, E any](s S) S
</code></pre>
<p>To repeat, writing type parameters and constraints <code>[S []E, E any]</code>
means that the type argument for <code>S</code> can be any unnamed slice type,
but it can’t be a named type defined as a slice literal.
Writing <code>[S ~[]E, E any]</code>, with a <code>~</code>, means that the type argument
for <code>S</code> can be any type whose underlying type is a slice type.</p>
<p>For any named type <code>type T1 T2</code> the underlying type of <code>T1</code> is the
underlying type of <code>T2</code>.
The underlying type of a predeclared type like <code>int</code> or a type literal
like <code>[]string</code> is just the type itself.
For the exact details, <a href="https://go.dev/ref/spec#Underlying_types" rel="noreferrer" target="_blank">see the language
spec</a>.
In our example, the underlying type of <code>MySlice</code> is <code>[]string</code>.</p>
<p>Since the underlying type of <code>MySlice</code> is a slice, we can pass an
argument of type <code>MySlice</code> to <code>Clone5</code>.
As you may have noticed, the signature of <code>Clone5</code> is the same as the
signature of <code>slices.Clone</code>.
We’ve finally gotten to where we want to be.</p>
<p>Before we move on, let’s discuss why the Go syntax requires a <code>~</code>.
It might seem that we would always want to permit passing <code>MySlice</code>,
so why not make that the default?
Or, if we need to support exact matching, why not flip things around,
so that a constraint of <code>[]E</code> permits a named type while a constraint
of, say, <code>=[]E</code> only permits slice type literals?</p>
<p>To explain this, let’s first observe that a type parameter list like
<code>[T ~MySlice]</code> doesn’t make sense.
That’s because <code>MySlice</code> is not the underlying type of any other type.
For instance, if we have a definition like <code>type MySlice2 MySlice</code>,
the underlying type of <code>MySlice2</code> is <code>[]string</code>, not <code>MySlice</code>.
So either <code>[T ~MySlice]</code> would permit no types at all, or it would be
the same as <code>[T MySlice]</code> and only match <code>MySlice</code>.
Either way, <code>[T ~MySlice]</code> isn’t useful.
To avoid this confusion, the language prohibits <code>[T ~MySlice]</code>, and
the compiler produces an error like</p>
<pre><code>invalid use of ~ (underlying type of MySlice is []string)
</code></pre>
<p>If Go didn’t require the tilde, so that <code>[S []E]</code> would match any type
whose underlying type is <code>[]E</code>, then we would have to define the
meaning of <code>[S MySlice]</code>.</p>
<p>We could prohibit <code>[S MySlice]</code>, or we could say that <code>[S MySlice]</code>
only matches <code>MySlice</code>, but either approach runs into trouble with
predeclared types.
A predeclared type, like <code>int</code> is its own underlying type.
We want to permit people to be able to write constraints that accept
any type argument whose underlying type is <code>int</code>.
In the language today, they can do that by writing <code>[T ~int]</code>.
If we don’t require the tilde we would still need a way to say “any
type whose underlying type is <code>int</code>”.
The natural way to say that would be <code>[T int]</code>.
That would mean that <code>[T MySlice]</code> and <code>[T int]</code> would behave
differently, although they look very similar.</p>
<p>We could perhaps say that <code>[S MySlice]</code> matches any type whose
underlying type is the underlying type of <code>MySlice</code>, but that makes
<code>[S MySlice]</code> unnecessary and confusing.</p>
<p>We think it’s better to require the <code>~</code> and be very clear about when
we are matching the underlying type rather than the type itself.</p>
<h2 id="type-inference">Type inference</h2>
<p>Now that we’ve explained the signature of <code>slices.Clone</code>, let’s see
how actually using <code>slices.Clone</code> is simplified by type inference.
Remember, the signature of <code>Clone</code> is</p>
<pre><code>func Clone[S ~[]E, E any](s S) S
</code></pre>
<p>A call of <code>slices.Clone</code> will pass a slice to the parameter <code>s</code>.
Simple type inference will let the compiler infer that the type
argument for the type parameter <code>S</code> is the type of the slice being
passed to <code>Clone</code>.
Type inference is then powerful enough to see that the type argument
for <code>E</code> is the element type of the type argument passed to <code>S</code>.</p>
<p>This means that we can write</p>
<pre><code>    c := Clone(ms)
</code></pre>
<p>without having to write</p>
<pre><code>    c := Clone[MySlice, string](ms)
</code></pre>
<p>If we refer to <code>Clone</code> without calling it, we do have to specify a
type argument for <code>S</code>, as the compiler has nothing it can use to infer
it.
Fortunately, in that case, type inference is able to infer the type
argument for <code>E</code> from the argument for <code>S</code>, and we don’t have to
specify it separately.</p>
<p>That is, we can write</p>
<pre><code>    myClone := Clone[MySlice]
</code></pre>
<p>without having to write</p>
<pre><code>    myClone := Clone[MySlice, string]
</code></pre>
<h2 id="deconstructing-type-parameters">Deconstructing type parameters</h2>
<p>The general technique we’ve used here, in which we define one type
parameter <code>S</code> using another type parameter <code>E</code>, is a way to
deconstruct types in generic function signatures.
By deconstructing a type, we can name, and constrain, all aspects of
the type.</p>
<p>For example, here is the signature for <code>maps.Clone</code>.</p>
<pre><code>func Clone[M ~map[K]V, K comparable, V any](m M) M
</code></pre>
<p>Just as with <code>slices.Clone</code>, we use a type parameter for the type of
the parameter <code>m</code>, and then deconstruct the type using two other type
parameters <code>K</code> and <code>V</code>.</p>
<p>In <code>maps.Clone</code> we constrain <code>K</code> to be comparable, as is required for
a map key type.
We can constrain the component types any way we like.</p>
<pre><code>func WithStrings[S ~[]E, E interface { String() string }](s S) (S, []string)
</code></pre>
<p>This says that the argument of <code>WithStrings</code> must be a slice type for
which the element type has a <code>String</code> method.</p>
<p>Since all Go types can be built up from component types, we can always
use type parameters to deconstruct those types and constrain them as
we like.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Radar Maps Platform (148 pts)]]></title>
            <link>https://radar.com/blog/introducing-radar-maps-platform</link>
            <guid>37667450</guid>
            <pubDate>Tue, 26 Sep 2023 23:38:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://radar.com/blog/introducing-radar-maps-platform">https://radar.com/blog/introducing-radar-maps-platform</a>, See on <a href="https://news.ycombinator.com/item?id=37667450">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next" data-reactroot=""><div><div><ul><li><a href="https://radar.com/product/geofencing">Product</a></li><li><a href="https://radar.com/solutions/retail">Solutions</a></li><li><a href="https://radar.com/documentation">Docs</a></li><li><a href="https://radar.com/pricing">Pricing</a></li><li><a href="https://radar.com/about">Company</a></li></ul><div><p><a href="https://radar.com/product/geofencing"><svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.132 2a3.648 3.648 0 0 1 3.473 3.641v.156c0 2.152-2.911 5.255-3.216 5.572a.649.649 0 0 1-.425.191.649.649 0 0 1-.424-.19c-.3-.318-3.217-3.42-3.217-5.573V5.64A3.643 3.643 0 0 1 7.797 2h.335ZM6.589 5.64a1.369 1.369 0 1 0 2.738 0 1.369 1.369 0 0 0-2.738 0Zm4.496 4.323a5.5 5.5 0 0 0 .227-.335l.006-.006c.903.455 1.447 1.106 1.447 1.848C12.765 12.91 10.715 14 8 14c-2.714 0-4.765-1.088-4.765-2.53 0-.729.526-1.368 1.399-1.823.066.102.131.204.21.311.166.24.334.46.501.676-.586.263-.92.58-.92.837 0 .532 1.423 1.333 3.569 1.333s3.57-.801 3.57-1.333c0-.263-.353-.592-.963-.855l.058-.076c.139-.181.285-.372.426-.576Z" fill="currentColor"></path></svg>Geofences<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.99 3.23a.75.75 0 0 0 0 1.06L10.698 8 6.99 11.709a.75.75 0 1 0 1.06 1.06l4.198-4.197a.81.81 0 0 0 0-1.145L8.05 3.23a.75.75 0 0 0-1.06 0Z" fill="currentColor"></path></svg><span href="/product/geofencing">Industry-leading accuracy with unlimited geofences, polygon geofences, and more</span></a></p><p><a href="https://radar.com/product/trip-tracking"><svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.2 2H2.8c-.44 0-.8.365-.8.812v11.376c0 .447.36.812.8.812h.167c.44 0 .8-.365.8-.812v-2.436H13.2c.44 0 .8-.365.8-.812V2.812A.808.808 0 0 0 13.2 2Zm-.967 4.872H9.411v2.866H6.59V6.872H3.767V4.006h2.822v2.866H9.41V4.006h2.822v2.866Z" fill="currentColor"></path></svg>Trips<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.99 3.23a.75.75 0 0 0 0 1.06L10.698 8 6.99 11.709a.75.75 0 1 0 1.06 1.06l4.198-4.197a.81.81 0 0 0 0-1.145L8.05 3.23a.75.75 0 0 0-1.06 0Z" fill="currentColor"></path></svg><span href="/product/trip-tracking">Trip tracking, live ETAs, arrival detection, and routing for pickups and deliveries</span></a></p><p><a href="https://radar.com/product/places"><svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.425 2.375A.788.788 0 0 0 11.75 2h-7.5c-.3 0-.525.15-.675.375C2 5.375 2 5.525 2 5.75c0 .825.675 1.5 1.5 1.5v6c0 .45.3.75.75.75h7.5c.45 0 .75-.3.75-.75v-6c.825 0 1.5-.675 1.5-1.5 0-.225 0-.375-1.575-3.375ZM9.5 12.5v-3h-3v3H5V7.025c.225.15.45.225.75.225.45 0 .825-.225 1.125-.525.3.3.675.525 1.125.525.45 0 .825-.225 1.125-.525.3.3.675.525 1.125.525.3 0 .525-.075.75-.225V12.5H9.5Z" fill="currentColor"></path></svg>Places<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.99 3.23a.75.75 0 0 0 0 1.06L10.698 8 6.99 11.709a.75.75 0 1 0 1.06 1.06l4.198-4.197a.81.81 0 0 0 0-1.145L8.05 3.23a.75.75 0 0 0-1.06 0Z" fill="currentColor"></path></svg><span href="/product/places">Points-of-interest (POI) dataset with chains and categories to detect visits to millions of places</span></a></p><p><a href="https://radar.com/product/api"><svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M12.57 6.57A4.578 4.578 0 0 0 8.21 2h-.42a4.573 4.573 0 0 0-4.36 4.57v.195c0 2.702 3.662 6.597 4.037 6.995.15.157.383.24.533.24.15 0 .383-.083.533-.24.383-.398 4.037-4.293 4.037-6.995V6.57ZM7.993 8.29a1.718 1.718 0 1 1 0-3.437c.953 0 1.718.765 1.718 1.718S8.946 8.29 7.993 8.29Z" fill="currentColor"></path></svg>Maps Platform<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.99 3.23a.75.75 0 0 0 0 1.06L10.698 8 6.99 11.709a.75.75 0 1 0 1.06 1.06l4.198-4.197a.81.81 0 0 0 0-1.145L8.05 3.23a.75.75 0 0 0-1.06 0Z" fill="currentColor"></path></svg><span href="/product/api">The cost-effective, all-in-one Google Maps alternative, with geocoding, search, routing, and maps</span></a></p></div></div><div><ul><li><a href="https://radar.com/login">Log in</a></li><li><a href="https://radar.com/contact">Get a demo</a></li></ul></div></div><div><header><nav><ul><li><a href="https://radar.com/blog">Blog Home</a></li><li><a href="https://radar.com/blog/categories/company">Company</a></li><li><a href="https://radar.com/blog/categories/product">Product</a></li><li><a href="https://radar.com/blog/categories/engineering">Engineering</a></li><li><a href="https://radar.com/blog/categories/industry">Industry</a></li><li><a href="https://radar.com/blog/categories/guides">Guides</a></li></ul></nav></header></div><div><h2>It’s time to build</h2><p>See what Radar’s location and geofencing <br>solutions can do for your business.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Philips Hue ecosystem is collapsing into stupidity (883 pts)]]></title>
            <link>https://rachelbythebay.com/w/2023/09/26/hue/</link>
            <guid>37667266</guid>
            <pubDate>Tue, 26 Sep 2023 23:22:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rachelbythebay.com/w/2023/09/26/hue/">https://rachelbythebay.com/w/2023/09/26/hue/</a>, See on <a href="https://news.ycombinator.com/item?id=37667266">Hacker News</a></p>
Couldn't get https://rachelbythebay.com/w/2023/09/26/hue/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Merlin's Wisdom Project (111 pts)]]></title>
            <link>https://github.com/merlinmann/wisdom/blob/master/wisdom.md</link>
            <guid>37666773</guid>
            <pubDate>Tue, 26 Sep 2023 22:38:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/merlinmann/wisdom/blob/master/wisdom.md">https://github.com/merlinmann/wisdom/blob/master/wisdom.md</a>, See on <a href="https://news.ycombinator.com/item?id=37666773">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:merlinmann/wisdom" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="teghlIRbGmLeFdMB8ht8tu2-5DgqV5aGqcJgZZVJvhKYamRwhS-fIARrQ440uzMaOaffwx5f5EHT-Uvtl9zMcw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="merlinmann/wisdom" data-current-org="" data-current-owner="merlinmann" data-logged-in="false">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=merlinmann%2Fwisdom" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/merlinmann/wisdom/blob/master/wisdom.md&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="cad10fc57d3712418bf0d2fbc745147bff856c0d10120e4d23ffe63fe3ec9304" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It's Stanislav Petrov day —40yrs ago he saved world by suppressing a tech glitch (106 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Stanislav_Petrov</link>
            <guid>37666466</guid>
            <pubDate>Tue, 26 Sep 2023 22:13:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Stanislav_Petrov">https://en.wikipedia.org/wiki/Stanislav_Petrov</a>, See on <a href="https://news.ycombinator.com/item?id=37666466">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							<div id="mw-indicator-pp-autoreview"><p><span typeof="mw:File"><a href="https://en.wikipedia.org/wiki/Wikipedia:Protection_policy#pending" title="All edits by unregistered and new users are subject to review prior to becoming visible to unregistered users"><img alt="Page protected with pending changes" src="https://upload.wikimedia.org/wikipedia/en/thumb/b/b7/Pending-protection-shackle.svg/20px-Pending-protection-shackle.svg.png" decoding="async" width="20" height="20" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/b/b7/Pending-protection-shackle.svg/30px-Pending-protection-shackle.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/b/b7/Pending-protection-shackle.svg/40px-Pending-protection-shackle.svg.png 2x" data-file-width="512" data-file-height="512"></a></span></p></div>

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div id="mw-content-text" lang="en" dir="ltr">



<table><tbody><tr><th colspan="2"><p>Stanislav Petrov</p></th></tr><tr><td colspan="2"><span typeof="mw:File/Frameless"><a href="https://en.wikipedia.org/wiki/File:Stanislav_Yevgrafovich_Petrov.webp"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Stanislav_Yevgrafovich_Petrov.webp/220px-Stanislav_Yevgrafovich_Petrov.webp.png" decoding="async" width="220" height="291" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Stanislav_Yevgrafovich_Petrov.webp/238px-Stanislav_Yevgrafovich_Petrov.webp.png 1.5x" data-file-width="238" data-file-height="315"></a></span><p>Petrov circa 1983<sup id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup></p></td></tr><tr><th scope="row">Born</th><td><p>Stanislav Yevgrafovich Petrov</p><br>7 September 1939<br><div><p><a href="https://en.wikipedia.org/wiki/Vladivostok" title="Vladivostok">Vladivostok</a>, <a href="https://en.wikipedia.org/wiki/Russian_Soviet_Federative_Socialist_Republic" title="Russian Soviet Federative Socialist Republic">Russian SFSR</a>, <a href="https://en.wikipedia.org/wiki/Soviet_Union" title="Soviet Union">Soviet Union</a></p></div></td></tr><tr><th scope="row">Died</th><td>19 May 2017 (aged&nbsp;77)<br><div><p><a href="https://en.wikipedia.org/wiki/Fryazino" title="Fryazino">Fryazino</a>, <a href="https://en.wikipedia.org/wiki/Russia" title="Russia">Russia</a></p></div></td></tr><tr><th scope="row">Known&nbsp;for</th><td><a href="https://en.wikipedia.org/wiki/1983_Soviet_nuclear_false_alarm_incident" title="1983 Soviet nuclear false alarm incident">1983 Soviet nuclear false alarm incident</a></td></tr><tr><th scope="row">Spouse</th><td>Raisa Petrova 
(m. 1973; died 1997)</td></tr><tr><th scope="row">Children</th><td>2</td></tr><tr><td colspan="2"></td></tr><tr><td colspan="2">Military career</td></tr><tr><th scope="row">Allegiance</th><td><span><span typeof="mw:File"><span><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Flag_of_the_Soviet_Union.svg/23px-Flag_of_the_Soviet_Union.svg.png" decoding="async" width="23" height="12" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Flag_of_the_Soviet_Union.svg/35px-Flag_of_the_Soviet_Union.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Flag_of_the_Soviet_Union.svg/46px-Flag_of_the_Soviet_Union.svg.png 2x" data-file-width="1200" data-file-height="600"></span></span>&nbsp;</span>Soviet Union</td></tr><tr><th scope="row">Service/<wbr>branch</th><td><a href="https://en.wikipedia.org/wiki/Soviet_Air_Defence_Forces" title="Soviet Air Defence Forces">Soviet Air Defence Forces</a></td></tr><tr><th scope="row">Years&nbsp;of service</th><td>1972–1984</td></tr><tr><th scope="row">Rank</th><td><a href="https://en.wikipedia.org/wiki/Lieutenant_colonel" title="Lieutenant colonel">Lieutenant colonel</a></td></tr></tbody></table>
<p><b>Stanislav Yevgrafovich Petrov</b> (<a href="https://en.wikipedia.org/wiki/Russian_language" title="Russian language">Russian</a>: <span lang="ru">Станисла́в Евгра́фович Петро́в</span>; 7 September 1939 – 19 May 2017) was a <a href="https://en.wikipedia.org/wiki/Lieutenant_colonel" title="Lieutenant colonel">lieutenant colonel</a> of the <a href="https://en.wikipedia.org/wiki/Soviet_Air_Defence_Forces" title="Soviet Air Defence Forces">Soviet Air Defence Forces</a> who played a key role in the <a href="https://en.wikipedia.org/wiki/1983_Soviet_nuclear_false_alarm_incident" title="1983 Soviet nuclear false alarm incident">1983 Soviet nuclear false alarm incident</a>.<sup id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup> On 26 September 1983, three weeks after the Soviet military had shot down <a href="https://en.wikipedia.org/wiki/Korean_Air_Lines_Flight_007" title="Korean Air Lines Flight 007">Korean Air Lines Flight 007</a>, Petrov was the duty officer at the command center for the <a href="https://en.wikipedia.org/wiki/Oko" title="Oko">Oko</a> nuclear early-warning system when the system reported that a missile had been launched from the <a href="https://en.wikipedia.org/wiki/United_States" title="United States">United States</a>, followed by up to five more. Petrov judged the reports to be a <a href="https://en.wikipedia.org/wiki/False_alarm" title="False alarm">false alarm</a>.<sup id="cite_ref-AWC_3-0"><a href="#cite_note-AWC-3">[3]</a></sup> 
</p><p>His subsequent decision to disobey orders, against Soviet military protocol,<sup id="cite_ref-BBC2013_4-0"><a href="#cite_note-BBC2013-4">[4]</a></sup> is credited with having prevented an erroneous <a href="https://en.wikipedia.org/wiki/Second_strike" title="Second strike">retaliatory nuclear attack</a> on the United States and its <a href="https://en.wikipedia.org/wiki/NATO" title="NATO">NATO</a> allies that could have resulted in a large-scale <a href="https://en.wikipedia.org/wiki/Nuclear_warfare" title="Nuclear warfare">nuclear war</a> which could have wiped out half of the population of the countries involved. An investigation later confirmed that the Soviet satellite warning system had indeed malfunctioned. Because of his decision not to launch a retaliatory nuclear strike amid this incident, Petrov is often credited as having "saved the world".<sup id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup><sup id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup><sup id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup>
</p>
<meta property="mw:PageProp/toc">
<h2><span id="Early_life_and_military_career">Early life and military career</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Stanislav_Petrov&amp;action=edit&amp;section=1" title="Edit section: Early life and military career">edit</a><span>]</span></span></h2>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Stanislav_Petrov.png"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/Stanislav_Petrov.png/220px-Stanislav_Petrov.png" decoding="async" width="220" height="159" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/Stanislav_Petrov.png/330px-Stanislav_Petrov.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/Stanislav_Petrov.png/440px-Stanislav_Petrov.png 2x" data-file-width="557" data-file-height="402"></a><figcaption>Colourized photo of Petrov</figcaption></figure>
<p>Petrov was born on 7 September 1939 to a <a href="https://en.wikipedia.org/wiki/Russians" title="Russians">Russian</a> family near <a href="https://en.wikipedia.org/wiki/Vladivostok" title="Vladivostok">Vladivostok</a>. His father, Yevgraf, flew <a href="https://en.wikipedia.org/wiki/Fighter_aircraft" title="Fighter aircraft">fighter aircraft</a> during <a href="https://en.wikipedia.org/wiki/World_War_II" title="World War II">World War II</a>.<sup id="cite_ref-NYT_8-0"><a href="#cite_note-NYT-8">[8]</a></sup> His mother was a <a href="https://en.wikipedia.org/wiki/Nurse" title="Nurse">nurse</a>.<sup id="cite_ref-NYT_8-1"><a href="#cite_note-NYT-8">[8]</a></sup>
</p><p>Petrov enrolled at the <a href="https://en.wikipedia.org/wiki/Kyiv_Military_Aviation_Engineering_Academy" title="Kyiv Military Aviation Engineering Academy">Kyiv Military Aviation Engineering Academy</a> of the <a href="https://en.wikipedia.org/wiki/Soviet_Air_Forces" title="Soviet Air Forces">Soviet Air Forces</a>, and after graduating in 1972 he joined the <a href="https://en.wikipedia.org/wiki/Soviet_Air_Defence_Forces" title="Soviet Air Defence Forces">Soviet Air Defence Forces</a>. In the early 1970s, he was assigned to the organization that oversaw the new early warning system intended to detect <a href="https://en.wikipedia.org/wiki/Ballistic_missile" title="Ballistic missile">ballistic missile</a> attacks from <a href="https://en.wikipedia.org/wiki/NATO" title="NATO">NATO</a> countries.<sup id="cite_ref-NYT_8-2"><a href="#cite_note-NYT-8">[8]</a></sup><sup id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup>
</p><p>Petrov was married to Raisa, and had a son, Dmitri, and a daughter, Yelena. His wife died of cancer in 1997.<sup id="cite_ref-NYT_8-3"><a href="#cite_note-NYT-8">[8]</a></sup>
</p>
<h2><span id="Incident">Incident</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Stanislav_Petrov&amp;action=edit&amp;section=2" title="Edit section: Incident">edit</a><span>]</span></span></h2>

<p>On 26 September 1983, during the Cold War, the Soviet nuclear early warning system Oko reported the launch of one intercontinental ballistic missile with four more missiles behind it, from the United States. Petrov, suspecting a false alarm, decided to wait for a confirmation that never came. According to the Permanent Mission of the Russian Federation to the UN, nuclear retaliation requires that multiple sources confirm an attack.<sup id="cite_ref-Russian_Federation_10-0"><a href="#cite_note-Russian_Federation-10">[10]</a></sup> In any case, the incident exposed a serious flaw in the Soviet early warning system. Petrov has said that he was neither rewarded nor punished for his actions.<sup id="cite_ref-lenta_11-0"><a href="#cite_note-lenta-11">[11]</a></sup>
</p><p>Had Petrov reported incoming American missiles, his superiors might have launched an assault against the United States,<sup id="cite_ref-BBC2013_4-1"><a href="#cite_note-BBC2013-4">[4]</a></sup> precipitating a corresponding nuclear response from the United States. Petrov declared the system's indication a false alarm. Later, it was apparent that he was right: no missiles were approaching and the computer detection system was malfunctioning. It was subsequently determined that the false alarm had been created by a rare alignment of sunlight on <a href="https://en.wikipedia.org/wiki/List_of_cloud_types" title="List of cloud types">high-altitude clouds</a> above North Dakota and the <a href="https://en.wikipedia.org/wiki/Molniya_orbit" title="Molniya orbit">Molniya orbits</a> of the satellites, an error later corrected by cross-referencing a geostationary satellite.<sup id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup><sup id="cite_ref-brink_13-0"><a href="#cite_note-brink-13">[13]</a></sup><sup id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup>
</p><p>Petrov later indicated that the influences on his decision included that he had been told a US strike would be <a href="https://en.wikipedia.org/wiki/Pre-emptive_nuclear_strike" title="Pre-emptive nuclear strike">all-out</a>, so five missiles seemed an illogical start;<sup id="cite_ref-AWC_3-1"><a href="#cite_note-AWC-3">[3]</a></sup> that the launch detection system was new and, in his view, not yet wholly trustworthy; that the message passed through 30 layers of verification too quickly;<sup id="cite_ref-economist2017_15-0"><a href="#cite_note-economist2017-15">[15]</a></sup> and that ground radar failed to pick up corroborating evidence, even after minutes of delay.<sup id="cite_ref-wash_16-0"><a href="#cite_note-wash-16">[16]</a></sup> However, in a 2013 interview, Petrov said at the time he was never sure that the alarm was erroneous. He felt that his civilian training helped him make the right decision. He said that his colleagues were all professional soldiers with purely military training and, following instructions, would have reported a missile launch if they had been on his shift.<sup id="cite_ref-BBC2013_4-2"><a href="#cite_note-BBC2013-4">[4]</a></sup>
</p><p>Petrov underwent intense questioning by his superiors about his judgment. Initially, he was praised for his decision.<sup id="cite_ref-AWC_3-2"><a href="#cite_note-AWC-3">[3]</a></sup> General <a href="https://en.wikipedia.org/wiki/Yury_Votintsev" title="Yury Votintsev">Yury Votintsev</a>, then commander of the Soviet Air Defense's Missile Defense Units, who was the first to hear Petrov's report of the incident (and the first to reveal it to the public in the 1990s), states that Petrov's "correct actions" were "duly noted".<sup id="cite_ref-AWC_3-3"><a href="#cite_note-AWC-3">[3]</a></sup> Petrov himself states he was initially praised by Votintsev and promised a reward,<sup id="cite_ref-AWC_3-4"><a href="#cite_note-AWC-3">[3]</a></sup><sup id="cite_ref-moskovskiye_17-0"><a href="#cite_note-moskovskiye-17">[17]</a></sup> but recalls that he was also reprimanded for improper filing of paperwork because he had not described the incident in the <a href="https://en.wikipedia.org/wiki/War_diary" title="War diary">war diary</a>.<sup id="cite_ref-moskovskiye_17-1"><a href="#cite_note-moskovskiye-17">[17]</a></sup><sup id="cite_ref-BBC_18-0"><a href="#cite_note-BBC-18">[18]</a></sup>
</p><p>He received no reward. According to Petrov, this was because the incident and other bugs found in the missile detection system embarrassed his superiors and the scientists who were responsible for it, so that if he had been officially rewarded, they would have had to be punished.<sup id="cite_ref-AWC_3-5"><a href="#cite_note-AWC-3">[3]</a></sup><sup id="cite_ref-lenta_11-1"><a href="#cite_note-lenta-11">[11]</a></sup><sup id="cite_ref-moskovskiye_17-2"><a href="#cite_note-moskovskiye-17">[17]</a></sup><sup id="cite_ref-BBC_18-1"><a href="#cite_note-BBC-18">[18]</a></sup> He was reassigned to a less sensitive post,<sup id="cite_ref-BBC_18-2"><a href="#cite_note-BBC-18">[18]</a></sup> took early retirement (although he emphasized that he was not "forced out" of the army),<sup id="cite_ref-moskovskiye_17-3"><a href="#cite_note-moskovskiye-17">[17]</a></sup> and suffered a <a href="https://en.wikipedia.org/wiki/Nervous_breakdown" title="Nervous breakdown">nervous breakdown</a>.<sup id="cite_ref-BBC_18-3"><a href="#cite_note-BBC-18">[18]</a></sup>
</p><p>In a later interview, Petrov stated that the famous red button was never made operational, as military psychologists did not want to put the decision about a nuclear war into the hands of one single person.<sup id="cite_ref-FAZ_19-0"><a href="#cite_note-FAZ-19">[19]</a></sup><sup id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup>
</p><p>The incident became known publicly in 1998 upon the publication of Votintsev's memoirs. Widespread media reports since then have increased public awareness of Petrov's actions.<sup id="cite_ref-Votintsev_21-0"><a href="#cite_note-Votintsev-21">[21]</a></sup><sup id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup>
</p><p>There is some confusion as to precisely what Petrov's military role was in this incident. Petrov, as an individual, was not in a position where he could have single-handedly launched any of the Soviet missile arsenal. His sole duty was to monitor satellite surveillance equipment and report missile attack warnings up the chain of command; top Soviet leadership would have decided whether to launch a retaliatory attack against the West. But Petrov's role was crucial in providing information to make that decision.<sup id="cite_ref-Quotes_23-0"><a href="#cite_note-Quotes-23">[23]</a></sup> According to <a href="https://en.wikipedia.org/wiki/Bruce_G._Blair" title="Bruce G. Blair">Bruce G. Blair</a>, a Cold War nuclear strategies expert and nuclear disarmament advocate, formerly with the <a href="https://en.wikipedia.org/wiki/Center_for_Defense_Information" title="Center for Defense Information">Center for Defense Information</a>, "The top leadership, given only a couple of minutes to decide, told that an attack had been launched, would make a decision to retaliate."<sup id="cite_ref-NBC_24-0"><a href="#cite_note-NBC-24">[24]</a></sup><sup id="cite_ref-25"><a href="#cite_note-25">[25]</a></sup> In contrast, nuclear security scholar Pavel Podvig argues that, while Petrov did the right thing, "there were at least three assessment and decision-making layers above the command center of the army that operated the satellites", so that Petrov's report would not have directly led to a nuclear launch. In addition, he states that, even if the US strike was deemed to be real, the USSR would only have commenced its own strike after actual nuclear explosions on its territory.<sup id="cite_ref-26"><a href="#cite_note-26">[26]</a></sup>
</p><p>Petrov later said "I had obviously never imagined that I would ever face that situation. It was the first and, as far as I know, also the last time that such a thing had happened, except for simulated practice scenarios."<sup id="cite_ref-Quotes_23-1"><a href="#cite_note-Quotes-23">[23]</a></sup>
</p>
<h2><span id="Later_career">Later career</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Stanislav_Petrov&amp;action=edit&amp;section=3" title="Edit section: Later career">edit</a><span>]</span></span></h2>
<p>In the aftermath of the incident, the Soviet government investigated the incident and determined that Petrov had insufficiently documented his actions during the crisis. He explained it as "Because I had a phone in one hand and the intercom in the other, and I don't have a third hand"; nevertheless, Petrov received a reprimand.<sup id="cite_ref-NYT_8-4"><a href="#cite_note-NYT-8">[8]</a></sup><sup id="cite_ref-27"><a href="#cite_note-27">[27]</a></sup>
</p><p>In 1984, Petrov left the military and got a job at the research institute that had developed the Soviet Union's early warning system. He later retired so he could care for his wife after she was diagnosed with cancer.<sup id="cite_ref-NYT_8-5"><a href="#cite_note-NYT-8">[8]</a></sup> A BBC report in 1998 stated that Petrov had suffered a mental breakdown and quoted him as saying, "I was made a scapegoat."<sup id="cite_ref-BBC_18-4"><a href="#cite_note-BBC-18">[18]</a></sup><sup id="cite_ref-28"><a href="#cite_note-28">[28]</a></sup>
</p><p>During a visit to the United States for the filming of the documentary <i><a href="https://en.wikipedia.org/wiki/The_Man_Who_Saved_the_World" title="The Man Who Saved the World">The Man Who Saved the World</a></i>, Petrov toured in May 2007 the <a href="https://en.wikipedia.org/wiki/Minuteman_Missile_National_Historic_Site" title="Minuteman Missile National Historic Site">Minuteman Missile National Historic Site</a> and, having retired from USSR military, commented, “he would never have imagined being able to visit one of the enemy’s securest sites.”<sup id="cite_ref-USANPS_29-0"><a href="#cite_note-USANPS-29">[29]</a></sup>
</p>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Stanislaw-jewgrafowitsch-petrow-2016.jpg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Stanislaw-jewgrafowitsch-petrow-2016.jpg/220px-Stanislaw-jewgrafowitsch-petrow-2016.jpg" decoding="async" width="220" height="241" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Stanislaw-jewgrafowitsch-petrow-2016.jpg/330px-Stanislaw-jewgrafowitsch-petrow-2016.jpg 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Stanislaw-jewgrafowitsch-petrow-2016.jpg/440px-Stanislaw-jewgrafowitsch-petrow-2016.jpg 2x" data-file-width="581" data-file-height="636"></a><figcaption>Petrov in 2016</figcaption></figure>
<p>Petrov died on 19 May 2017 from <a href="https://en.wikipedia.org/wiki/Hypostatic_pneumonia" title="Hypostatic pneumonia">hypostatic pneumonia</a>, though it was not widely reported until September. He was 77 years old.<sup id="cite_ref-30"><a href="#cite_note-30">[30]</a></sup><sup id="cite_ref-31"><a href="#cite_note-31">[31]</a></sup><sup id="cite_ref-32"><a href="#cite_note-32">[32]</a></sup>
</p>
<h2><span id="Awards_and_commendations">Awards and commendations</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Stanislav_Petrov&amp;action=edit&amp;section=4" title="Edit section: Awards and commendations">edit</a><span>]</span></span></h2>
<p>On 21 May 2004, the <a href="https://en.wikipedia.org/wiki/San_Francisco" title="San Francisco">San Francisco</a>-based Association of World Citizens gave Petrov its World Citizen Award along with a trophy and $1,000 "in recognition of the part he played in averting a catastrophe."<sup id="cite_ref-award_33-0"><a href="#cite_note-award-33">[33]</a></sup> In January 2006, Petrov travelled to the United States where he was honored in a meeting at the <a href="https://en.wikipedia.org/wiki/United_Nations" title="United Nations">United Nations</a> in <a href="https://en.wikipedia.org/wiki/New_York_City" title="New York City">New York City</a>. There the Association of World Citizens presented Petrov with a second special World Citizen Award.<sup id="cite_ref-mosnews_34-0"><a href="#cite_note-mosnews-34">[34]</a></sup> The next day, Petrov met American journalist <a href="https://en.wikipedia.org/wiki/Walter_Cronkite" title="Walter Cronkite">Walter Cronkite</a> at his <a href="https://en.wikipedia.org/wiki/CBS" title="CBS">CBS</a> office in New York City.
</p><p>In the <a href="https://en.wikipedia.org/wiki/Effective_altruism" title="Effective altruism">effective altruism</a> movement, 26 September is commemorated as Petrov day.<sup id="cite_ref-35"><a href="#cite_note-35">[35]</a></sup>
</p><p>That interview, in addition to other highlights of Petrov's trip to the United States, was filmed for <i><a href="https://en.wikipedia.org/wiki/The_Man_Who_Saved_the_World" title="The Man Who Saved the World">The Man Who Saved the World</a></i>,<sup id="cite_ref-award_33-1"><a href="#cite_note-award-33">[33]</a></sup><sup id="cite_ref-StatementFilm_36-0"><a href="#cite_note-StatementFilm-36">[36]</a></sup> a narrative feature and documentary film, directed by Peter Anthony of Denmark. It premiered in October 2014 at the <a href="https://en.wikipedia.org/wiki/Woodstock_Film_Festival" title="Woodstock Film Festival">Woodstock Film Festival</a> in Woodstock, New York, winning "Honorable Mention: Audience Award Winner for Best Narrative Feature" and "Honorable Mention: James Lyons Award for Best Editing of a Narrative Feature."<sup id="cite_ref-37"><a href="#cite_note-37">[37]</a></sup>
</p><p>For his actions in averting a potential nuclear war in 1983, Petrov was awarded the Dresden Peace Prize in <a href="https://en.wikipedia.org/wiki/Dresden" title="Dresden">Dresden</a>, Germany, on 17 February 2013. The award included €25,000.<sup id="cite_ref-38"><a href="#cite_note-38">[38]</a></sup> On 24 February 2012, he was given the 2011 German Media Award, presented to him at a ceremony in <a href="https://en.wikipedia.org/wiki/Baden-Baden" title="Baden-Baden">Baden-Baden</a>, Germany.<sup id="cite_ref-award_33-2"><a href="#cite_note-award-33">[33]</a></sup><sup id="cite_ref-39"><a href="#cite_note-39">[39]</a></sup><sup id="cite_ref-40"><a href="#cite_note-40">[40]</a></sup>
</p><p>On 26 September 2018, he was posthumously honored in New York with the $50,000 Future of Life Award.<sup id="cite_ref-thebulletin.org_41-0"><a href="#cite_note-thebulletin.org-41">[41]</a></sup>  At a ceremony at the <a href="https://en.wikipedia.org/wiki/National_Museum_of_Mathematics" title="National Museum of Mathematics">National Museum of Mathematics</a> in New York, former <a href="https://en.wikipedia.org/wiki/Ban_Ki-moon" title="Ban Ki-moon">United Nations Secretary General Ban Ki-Moon</a> said: “It is hard to imagine anything more devastating for humanity than all-out nuclear war between Russia and the United States. Yet this might have occurred by accident on September 26, 1983, were it not for the wise decisions of Stanislav Yevgrafovich Petrov. For this, he deserves humanity’s profound gratitude. Let us resolve to work together to realize a world free from fear of nuclear weapons, remembering the courageous judgement of Stanislav Petrov.” As Petrov had died, the award was collected by his daughter, Elena.  Petrov’s son Dmitri missed his flight to New York because the US embassy delayed his visa.<sup id="cite_ref-thebulletin.org_41-1"><a href="#cite_note-thebulletin.org-41">[41]</a></sup><sup id="cite_ref-42"><a href="#cite_note-42">[42]</a></sup>
</p><p>On the same day that Petrov was first honored at the United Nations in New York City, the Permanent Mission of the Russian Federation to the United Nations issued a press release contending that a single person could not have started or prevented a nuclear war, stating in part, "Under no circumstances a decision to use nuclear weapons could be made or even considered in the Soviet Union or in the United States on the basis of data from a single source or a system. For this to happen, a confirmation is necessary from several systems: ground-based radars, early warning satellites, intelligence reports, etc."<sup id="cite_ref-Russian_Federation_10-1"><a href="#cite_note-Russian_Federation-10">[10]</a></sup>
</p><p>But nuclear security expert <a href="https://en.wikipedia.org/wiki/Bruce_G._Blair" title="Bruce G. Blair">Bruce G. Blair</a> has said that at that time, the U.S.–Soviet relationship had deteriorated to the point where "the Soviet Union as a system—not just the Kremlin, not just <a href="https://en.wikipedia.org/wiki/Yuri_Andropov" title="Yuri Andropov">Andropov</a>, not just the <a href="https://en.wikipedia.org/wiki/KGB" title="KGB">KGB</a>—but as a system, was geared to expect an attack and to retaliate very quickly to it. It was on hair-trigger alert. It was very nervous and prone to mistakes and accidents. The false alarm that happened on Petrov's watch could not have come at a more dangerous, intense phase in US–Soviet relations."<sup id="cite_ref-Quotes_23-2"><a href="#cite_note-Quotes-23">[23]</a></sup> At that time, according to <a href="https://en.wikipedia.org/wiki/Oleg_Kalugin" title="Oleg Kalugin">Oleg Kalugin</a>, a former KGB chief of foreign counterintelligence, "The danger was in the Soviet leadership thinking, 'The Americans may attack, so we better attack first.'"<sup id="cite_ref-Kalugin_43-0"><a href="#cite_note-Kalugin-43">[43]</a></sup>
</p><p>Petrov said he did not know whether he should have regarded himself as a hero for what he did that day.<sup id="cite_ref-Quotes_23-3"><a href="#cite_note-Quotes-23">[23]</a></sup> In an interview for the film <i>The Man Who Saved the World</i>, Petrov says, "All that happened didn't matter to me—it was my job. I was simply doing my job, and I was the right person at the right time, that's all. My late wife for 10 years knew nothing about it. 'So what did you do?' she asked me. 'Nothing. I did nothing.'"<sup id="cite_ref-Quotes_23-4"><a href="#cite_note-Quotes-23">[23]</a></sup>
</p><p>The story of the nuclear incident is portrayed in the novel <i>La redención del camarada Petrov</i> by Argentinian writer <a href="https://en.wikipedia.org/wiki/Eduardo_Sguiglia" title="Eduardo Sguiglia">Eduardo Sguiglia</a> (Edhasa, 2023).<sup id="cite_ref-44"><a href="#cite_note-44">[44]</a></sup>
</p>
<h2><span id="See_also">See also</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Stanislav_Petrov&amp;action=edit&amp;section=5" title="Edit section: See also">edit</a><span>]</span></span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/Vasily_Arkhipov" title="Vasily Arkhipov">Vasily Arkhipov</a> – a Soviet naval officer who refused to launch a nuclear torpedo during the 1962 <a href="https://en.wikipedia.org/wiki/Cuban_Missile_Crisis" title="Cuban Missile Crisis">Cuban Missile Crisis</a></li>
<li><a href="https://en.wikipedia.org/wiki/List_of_nuclear_close_calls" title="List of nuclear close calls">List of nuclear close calls</a></li></ul>
<h2><span id="References">References</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Stanislav_Petrov&amp;action=edit&amp;section=6" title="Edit section: References">edit</a><span>]</span></span></h2>
<div>
<ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span><cite id="CITEREFChan2017">Chan, Sewell (18 September 2017). <a rel="nofollow" href="https://www.nytimes.com/2017/09/18/world/europe/stanislav-petrov-nuclear-war-dead.html">"Stanislav Petrov, Soviet Officer Who Helped Avert Nuclear War, Is Dead at 77"</a>. <i>The New York Times</i>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0362-4331">0362-4331</a><span>. Retrieved <span>4 October</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=Stanislav+Petrov%2C+Soviet+Officer+Who+Helped+Avert+Nuclear+War%2C+Is+Dead+at+77&amp;rft.date=2017-09-18&amp;rft.issn=0362-4331&amp;rft.aulast=Chan&amp;rft.aufirst=Sewell&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2017%2F09%2F18%2Fworld%2Feurope%2Fstanislav-petrov-nuclear-war-dead.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-2"><span><b><a href="#cite_ref-2">^</a></b></span> <span><cite id="CITEREFMattern2007">Mattern, Douglas (28 November 2007). Standish, Katerina; Bastet, Tatiyana; Reimer, Laura; Devere, Heather; Simpson, Erika; Talahma, Rula; Loadenthal, Michael (eds.). <a rel="nofollow" href="https://www.tandfonline.com/doi/abs/10.1080/10402650701681194">"Beyond Nuclear Terrorism"</a>. <i>Peace Review: A Journal of Social Justice</i>. <a href="https://en.wikipedia.org/wiki/Washington,_D.C." title="Washington, D.C.">Washington, D.C.</a>, United States of America: <a href="https://en.wikipedia.org/wiki/Peace_and_Justice_Studies_Association" title="Peace and Justice Studies Association">Peace and Justice Studies Association</a> (<a href="https://en.wikipedia.org/wiki/International_Peace_Research_Association" title="International Peace Research Association">International Peace Research Association</a>/<a href="https://en.wikipedia.org/wiki/Georgetown_University" title="Georgetown University">Georgetown University</a>)/<a href="https://en.wikipedia.org/wiki/Taylor_%26_Francis" title="Taylor &amp; Francis">Taylor &amp; Francis</a>. <b>19</b> (4): 563–569. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1080%2F10402650701681194">10.1080/10402650701681194</a>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/1040-2659">1040-2659</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:143511673">143511673</a><span>. Retrieved <span>4 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Peace+Review%3A+A+Journal+of+Social+Justice&amp;rft.atitle=Beyond+Nuclear+Terrorism&amp;rft.volume=19&amp;rft.issue=4&amp;rft.pages=563-569&amp;rft.date=2007-11-28&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A143511673%23id-name%3DS2CID&amp;rft.issn=1040-2659&amp;rft_id=info%3Adoi%2F10.1080%2F10402650701681194&amp;rft.aulast=Mattern&amp;rft.aufirst=Douglas&amp;rft_id=https%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Fabs%2F10.1080%2F10402650701681194&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-AWC-3"><span>^ <a href="#cite_ref-AWC_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-AWC_3-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-AWC_3-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-AWC_3-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-AWC_3-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-AWC_3-5"><sup><i><b>f</b></i></sup></a></span> <span><cite id="CITEREFLebedev2004">Lebedev, Anastasiya (21 May 2004). Mattern, Douglas; Waldow, Rene; Ray, Tom (eds.). <a rel="nofollow" href="https://web.archive.org/web/20110721000030/http://www.worldcitizens.org/petrov2.html">"The Man Who Saved the World Finally Recognized"</a>. <i>MosNews/Association of World Citizens (AWC)</i>. <a href="https://en.wikipedia.org/wiki/San_Francisco,_California" title="San Francisco, California">San Francisco</a>, <a href="https://en.wikipedia.org/wiki/California" title="California">California</a>, United States of America: The Association of World Citizens. Archived from <a rel="nofollow" href="http://www.worldcitizens.org/petrov2.html">the original</a> on 21 July 2011<span>. Retrieved <span>4 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=MosNews%2FAssociation+of+World+Citizens+%28AWC%29&amp;rft.atitle=The+Man+Who+Saved+the+World+Finally+Recognized&amp;rft.date=2004-05-21&amp;rft.aulast=Lebedev&amp;rft.aufirst=Anastasiya&amp;rft_id=http%3A%2F%2Fwww.worldcitizens.org%2Fpetrov2.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-BBC2013-4"><span>^ <a href="#cite_ref-BBC2013_4-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-BBC2013_4-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-BBC2013_4-2"><sup><i><b>c</b></i></sup></a></span> <span><cite id="CITEREFAksenov2013">Aksenov, Pavel (26 September 2013). <a href="https://en.wikipedia.org/wiki/Fran_Unsworth" title="Fran Unsworth">Unsworth, Fran</a>; <a href="https://en.wikipedia.org/wiki/Mary_Hockaday" title="Mary Hockaday">Hockaday, Mary</a>; <a href="https://en.wikipedia.org/wiki/Huw_Edwards" title="Huw Edwards">Edwards, Huw</a> (eds.). <a rel="nofollow" href="https://web.archive.org/web/20140308000459/https://www.bbc.com/news/world-europe-24280831">"Stanislav Petrov: The man who may have saved the world"</a>. <a href="https://en.wikipedia.org/wiki/London" title="London">London</a>, <a href="https://en.wikipedia.org/wiki/England" title="England">England</a>, United Kingdom of Great Britain: <a href="https://en.wikipedia.org/wiki/British_Broadcasting_Corporation" title="British Broadcasting Corporation">British Broadcasting Corporation (BBC)</a>. Archived from <a rel="nofollow" href="https://www.bbc.com/news/world-europe-24280831">the original</a> on 8 March 2014<span>. Retrieved <span>4 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Stanislav+Petrov%3A+The+man+who+may+have+saved+the+world&amp;rft.date=2013-09-26&amp;rft.aulast=Aksenov&amp;rft.aufirst=Pavel&amp;rft_id=https%3A%2F%2Fwww.bbc.com%2Fnews%2Fworld-europe-24280831&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-5"><span><b><a href="#cite_ref-5">^</a></b></span> <span><cite id="CITEREFLong2007">Long, Tony (26 September 2007). <a href="https://en.wikipedia.org/wiki/Chris_Anderson_(writer)" title="Chris Anderson (writer)">Anderson, Chris</a> (ed.). <a rel="nofollow" href="https://web.archive.org/web/20151101002723/https://www.wired.com/2007/09/dayintech-0926-2">"Sept. 26, 1983: The Man Who Saved the World by Doing ... Nothing"</a>. <i><a href="https://en.wikipedia.org/wiki/Wired_(magazine)" title="Wired (magazine)">Wired</a></i>. <a href="https://en.wikipedia.org/wiki/San_Francisco" title="San Francisco">San Francisco</a>, <a href="https://en.wikipedia.org/wiki/California" title="California">California</a>, United States of America: Condé Nast Publications. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/1059-1028">1059-1028</a>. <a href="https://en.wikipedia.org/wiki/OCLC_(identifier)" title="OCLC (identifier)">OCLC</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/oclc/24479723">24479723</a>. Archived from <a rel="nofollow" href="https://www.wired.com/2007/09/dayintech-0926-2">the original</a> on 1 November 2015<span>. Retrieved <span>4 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wired&amp;rft.atitle=Sept.+26%2C+1983%3A+The+Man+Who+Saved+the+World+by+Doing+...+Nothing&amp;rft.date=2007-09-26&amp;rft_id=info%3Aoclcnum%2F24479723&amp;rft.issn=1059-1028&amp;rft.aulast=Long&amp;rft.aufirst=Tony&amp;rft_id=https%3A%2F%2Fwww.wired.com%2F2007%2F09%2Fdayintech-0926-2&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-6"><span><b><a href="#cite_ref-6">^</a></b></span> <span><cite id="CITEREFPedersen2005">Pedersen, Glen (1 July 2005). Smith, Susan; Jordan-Simpson, Emma; Vesely-Flad, Ethan (eds.). <a rel="nofollow" href="https://www.proquest.com/openview/99d8a34911f1c57fa5ebc13ff5286c88">"Stanislav Petrov World Hero"</a>. <i>Fellowship</i>. <a href="https://en.wikipedia.org/wiki/New_York_City" title="New York City">New York City</a>, <a href="https://en.wikipedia.org/wiki/New_York_(state)" title="New York (state)">New York</a>, United States of America: <a href="https://en.wikipedia.org/wiki/Fellowship_of_Reconciliation_(United_States)" title="Fellowship of Reconciliation (United States)">United States Fellowship of Reconciliation</a>. <b>71</b> (7–8): 9–10<span>. Retrieved <span>4 September</span> 2021</span> – via <a href="https://en.wikipedia.org/wiki/ProQuest" title="ProQuest">ProQuest</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Fellowship&amp;rft.atitle=Stanislav+Petrov+World+Hero&amp;rft.volume=71&amp;rft.issue=7%E2%80%938&amp;rft.pages=9-10&amp;rft.date=2005-07-01&amp;rft.aulast=Pedersen&amp;rft.aufirst=Glen&amp;rft_id=https%3A%2F%2Fwww.proquest.com%2Fopenview%2F99d8a34911f1c57fa5ebc13ff5286c88&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-7"><span><b><a href="#cite_ref-7">^</a></b></span> <span><cite id="CITEREFFordenPodvigPostol2000">Forden, Geoffrey; Podvig, Pavel; Postol, Theodore A. (1 March 2000). Hassler, Susan; Land, Susan Kathy; Zorpette, Glenn; Goldstein, Harry; Bretz, Elizabeth A.; Guizzo, Erico (eds.). <a rel="nofollow" href="https://ieeexplore.ieee.org/document/825657">"False alarm, nuclear danger"</a>. <i><a href="https://en.wikipedia.org/wiki/IEEE_Spectrum" title="IEEE Spectrum">IEEE Spectrum</a></i>. <a href="https://en.wikipedia.org/wiki/New_York_City" title="New York City">New York City</a>, <a href="https://en.wikipedia.org/wiki/New_York_(state)" title="New York (state)">New York</a>, United States of America: <a href="https://en.wikipedia.org/wiki/Institute_of_Electrical_and_Electronics_Engineers" title="Institute of Electrical and Electronics Engineers">Institute of Electrical and Electronics Engineers</a>. <b>37</b> (3): 31–39. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1109%2F6.825657">10.1109/6.825657</a>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0018-9235">0018-9235</a><span>. Retrieved <span>4 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Spectrum&amp;rft.atitle=False+alarm%2C+nuclear+danger&amp;rft.volume=37&amp;rft.issue=3&amp;rft.pages=31-39&amp;rft.date=2000-03-01&amp;rft_id=info%3Adoi%2F10.1109%2F6.825657&amp;rft.issn=0018-9235&amp;rft.aulast=Forden&amp;rft.aufirst=Geoffrey&amp;rft.au=Podvig%2C+Pavel&amp;rft.au=Postol%2C+Theodore+A.&amp;rft_id=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F825657&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-NYT-8"><span>^ <a href="#cite_ref-NYT_8-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-NYT_8-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-NYT_8-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-NYT_8-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-NYT_8-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-NYT_8-5"><sup><i><b>f</b></i></sup></a></span> <span><cite id="CITEREFChanKishkovskyMatsnev2017">Chan, Sewell; Kishkovsky, Sophia; Matsnev, Oleg (19 September 2017). <a href="https://en.wikipedia.org/wiki/A.G._Sulzberger" title="A.G. Sulzberger">Sulzberger, A.G.</a>; <a href="https://en.wikipedia.org/wiki/Dean_Baquet" title="Dean Baquet">Baquet, Dean</a>; <a href="https://en.wikipedia.org/wiki/Joseph_Kahn_(journalist)" title="Joseph Kahn (journalist)">Kahn, Joseph</a> (eds.). <a rel="nofollow" href="https://web.archive.org/web/20170919023131/https://www.nytimes.com/2017/09/18/world/europe/stanislav-petrov-nuclear-war-dead.html">"Stanislav Petrov, 77; Soviet Who Helped Avert a Nuclear War"</a>. International news. <i><a href="https://en.wikipedia.org/wiki/The_New_York_Times" title="The New York Times">The New York Times</a></i>. Vol.&nbsp;CLXVI, no.&nbsp;186. <a href="https://en.wikipedia.org/wiki/New_York_City" title="New York City">New York City</a>, <a href="https://en.wikipedia.org/wiki/New_York_(state)" title="New York (state)">New York</a>, United States of America. p.&nbsp;B14. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/1553-8095">1553-8095</a>. <a href="https://en.wikipedia.org/wiki/OCLC_(identifier)" title="OCLC (identifier)">OCLC</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/oclc/1645522">1645522</a>. Archived from <a rel="nofollow" href="https://www.nytimes.com/2017/09/18/world/europe/stanislav-petrov-nuclear-war-dead.html">the original</a> on 19 September 2017<span>. Retrieved <span>4 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=Stanislav+Petrov%2C+77%3B+Soviet+Who+Helped+Avert+a+Nuclear+War&amp;rft.volume=CLXVI&amp;rft.issue=186&amp;rft.pages=B14&amp;rft.date=2017-09-19&amp;rft_id=info%3Aoclcnum%2F1645522&amp;rft.issn=1553-8095&amp;rft.aulast=Chan&amp;rft.aufirst=Sewell&amp;rft.au=Kishkovsky%2C+Sophia&amp;rft.au=Matsnev%2C+Oleg&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2017%2F09%2F18%2Fworld%2Feurope%2Fstanislav-petrov-nuclear-war-dead.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-9"><span><b><a href="#cite_ref-9">^</a></b></span> <span><cite id="CITEREFNagesh2017">Nagesh, Ashitha (18 September 2017). <a href="https://en.wikipedia.org/wiki/Ted_Young_(journalist)" title="Ted Young (journalist)">Young, Ted</a>; <a href="https://en.wikipedia.org/wiki/Jonathan_Harmsworth,_4th_Viscount_Rothermere" title="Jonathan Harmsworth, 4th Viscount Rothermere">Harmsworth, 4th Viscount Rothermere, Jonathan</a>; <a href="https://en.wikipedia.org/wiki/Paul_Dacre" title="Paul Dacre">Dacre, Paul</a> (eds.). <a rel="nofollow" href="https://web.archive.org/web/20170918173759/https://www.metro.co.uk/2017/09/18/stanislav-petrov-the-man-who-quietly-saved-the-world-has-died-aged-77-6937015">"Stanislav Petrov – The man who quietly saved the world – Has died aged 77"</a>. <i><a href="https://en.wikipedia.org/wiki/Metro_(British_newspaper)" title="Metro (British newspaper)">Metro</a></i>. <a href="https://en.wikipedia.org/wiki/London" title="London">London</a>, <a href="https://en.wikipedia.org/wiki/England" title="England">England</a>, <a href="https://en.wikipedia.org/wiki/United_Kingdom" title="United Kingdom">United Kingdom</a>: Associated Newspapers Ltd (<a href="https://en.wikipedia.org/wiki/DMG_Media" title="DMG Media">DMG Media</a>). <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/1469-6215">1469-6215</a>. <a href="https://en.wikipedia.org/wiki/OCLC_(identifier)" title="OCLC (identifier)">OCLC</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/oclc/225917520">225917520</a>. Archived from <a rel="nofollow" href="https://metro.co.uk/2017/09/18/stanislav-petrov-the-man-who-quietly-saved-the-world-has-died-aged-77-6937015">the original</a> on 18 September 2017<span>. Retrieved <span>4 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Metro&amp;rft.atitle=Stanislav+Petrov+%E2%80%93+The+man+who+quietly+saved+the+world+%E2%80%93+Has+died+aged+77&amp;rft.date=2017-09-18&amp;rft_id=info%3Aoclcnum%2F225917520&amp;rft.issn=1469-6215&amp;rft.aulast=Nagesh&amp;rft.aufirst=Ashitha&amp;rft_id=https%3A%2F%2Fmetro.co.uk%2F2017%2F09%2F18%2Fstanislav-petrov-the-man-who-quietly-saved-the-world-has-died-aged-77-6937015&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-Russian_Federation-10"><span>^ <a href="#cite_ref-Russian_Federation_10-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Russian_Federation_10-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFChurkin2006"><a href="https://en.wikipedia.org/wiki/Vitaly_Churkin" title="Vitaly Churkin">Churkin, Vitaly</a> (19 January 2006). <a rel="nofollow" href="https://web.archive.org/web/20060228123249/http://www.un.int/russia/other/060119eprel.pdf">"Press release: On presentation of the world citizens award to Stanislav Petrov"</a> <span>(PDF)</span>. Permanent Mission of the Russian Federation to the United Nations (Press release). <a href="https://en.wikipedia.org/wiki/New_York_City" title="New York City">New York City</a>, <a href="https://en.wikipedia.org/wiki/New_York_(state)" title="New York (state)">New York</a>, United States of America: <a href="https://en.wikipedia.org/wiki/Ministry_of_Foreign_Affairs_(Russia)" title="Ministry of Foreign Affairs (Russia)">Ministry of Foreign Affairs of the Russian Federation</a>. <a href="https://en.wikipedia.org/wiki/United_Nations" title="United Nations">United Nations</a>. Archived from <a rel="nofollow" href="http://www.un.int/russia/other/060119eprel.pdf">the original</a> <span>(PDF)</span> on 28 February 2006<span>. Retrieved <span>4 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Press+release%3A+On+presentation+of+the+world+citizens+award+to+Stanislav+Petrov&amp;rft.place=New+York+City%2C+New+York%2C+United+States+of+America&amp;rft.pub=Ministry+of+Foreign+Affairs+of+the+Russian+Federation&amp;rft.date=2006-01-19&amp;rft.aulast=Churkin&amp;rft.aufirst=Vitaly&amp;rft_id=http%3A%2F%2Fwww.un.int%2Frussia%2Fother%2F060119eprel.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-lenta-11"><span>^ <a href="#cite_ref-lenta_11-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-lenta_11-1"><sup><i><b>b</b></i></sup></a></span> <span><a rel="nofollow" href="http://lenta.ru/news/2006/01/20/petrov/">В Нью-Йорке россиянина наградили за спасение мира</a>. Lenta.ru (in Russian)</span>
</li>
<li id="cite_note-12"><span><b><a href="#cite_ref-12">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.everything2.com/index.pl?node=Molniya%20orbit">"Molniya orbit"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Molniya+orbit&amp;rft_id=http%3A%2F%2Fwww.everything2.com%2Findex.pl%3Fnode%3DMolniya%2520orbit&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-brink-13"><span><b><a href="#cite_ref-brink_13-0">^</a></b></span> <span><cite id="CITEREFBaynesParkerNorrisAdams2008">Henry Chancellor (director and producer), Gina McKee (narrator), Richard Bright (producer), Cherry Brewer (producer), Taylor Downing (producer), Sam Organ (producer) (5 January 2008). Baynes, Jeff; Parker, Owen; Norris, Alice; Adams, Alison; Marcussen, Elizabeth; Farrell, Aidan; Mitchell, Andrew (eds.). <a rel="nofollow" href="https://www.youtube.com/watch?v=tPit2BGNKwo&amp;t=1746s"><i>1983: The Brink of the Apocalypse (Documentary)</i></a>. <a href="https://en.wikipedia.org/wiki/Channel_4" title="Channel 4">Channel 4</a> (Motion picture (television broadcast)). <a href="https://en.wikipedia.org/wiki/London" title="London">London</a>, <a href="https://en.wikipedia.org/wiki/England" title="England">England</a>, United Kingdom: <a href="https://en.wikipedia.org/wiki/Channel_Four_Television_Corporation" title="Channel Four Television Corporation">Channel Four Television Corporation</a>/Flashback Productions/<a href="https://en.wikipedia.org/wiki/Discovery_Channel" title="Discovery Channel">Discovery Channel Pictures</a>.  29:06 minutes in<span>. Retrieved <span>8 September</span> 2021</span> – via <a href="https://en.wikipedia.org/wiki/YouTube" title="YouTube">YouTube</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=1983%3A+The+Brink+of+the+Apocalypse+%28Documentary%29&amp;rft.place=London%2C+England%2C+United+Kingdom&amp;rft.pub=Channel+Four+Television+Corporation%2FFlashback+Productions%2FDiscovery+Channel+Pictures&amp;rft.date=2008-01-05&amp;rft_id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DtPit2BGNKwo%26t%3D1746s&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-14"><span><b><a href="#cite_ref-14">^</a></b></span> <span><cite id="CITEREFLui2017">Lui, Kevin (18 September 2017). <a href="https://en.wikipedia.org/wiki/Edward_Felsenthal" title="Edward Felsenthal">Felsenthal, Edward</a>; Benioff, Marc (eds.). <a rel="nofollow" href="https://web.archive.org/web/20170920035125/https://www.time.com/4947492/stanislav-petrov-soviet-officer-nuclear-war">"The Man Who Saved the World From Possible Nuclear War Has Died Age 77"</a>. <i><a href="https://en.wikipedia.org/wiki/Time_(magazine)" title="Time (magazine)">Time</a></i>. Vol.&nbsp;190, no.&nbsp;10. <a href="https://en.wikipedia.org/wiki/New_York_City" title="New York City">New York City</a>, <a href="https://en.wikipedia.org/wiki/New_York_(state)" title="New York (state)">New York</a>, United States of America: Time USA, LLC (Marc &amp; Lynne Benioff). <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0040-781X">0040-781X</a>. <a href="https://en.wikipedia.org/wiki/OCLC_(identifier)" title="OCLC (identifier)">OCLC</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/oclc/1311479">1311479</a>. Archived from <a rel="nofollow" href="https://www.time.com/4947492/stanislav-petrov-soviet-officer-nuclear-war">the original</a> on 20 September 2017.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Time&amp;rft.atitle=The+Man+Who+Saved+the+World+From+Possible+Nuclear+War+Has+Died+Age+77&amp;rft.volume=190&amp;rft.issue=10&amp;rft.date=2017-09-18&amp;rft_id=info%3Aoclcnum%2F1311479&amp;rft.issn=0040-781X&amp;rft.aulast=Lui&amp;rft.aufirst=Kevin&amp;rft_id=https%3A%2F%2Fwww.time.com%2F4947492%2Fstanislav-petrov-soviet-officer-nuclear-war&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-economist2017-15"><span><b><a href="#cite_ref-economist2017_15-0">^</a></b></span> <span><cite id="CITEREFBeddoesStandageBoro2017"><a href="https://en.wikipedia.org/wiki/Zanny_Minton_Beddoes" title="Zanny Minton Beddoes">Beddoes, Zanny Minton</a>; <a href="https://en.wikipedia.org/wiki/Tom_Standage" title="Tom Standage">Standage, Tom</a>; Boro, Lara Salameh, eds. (30 September 2017). <a rel="nofollow" href="https://web.archive.org/web/20170928220347/https://www.economist.com/news/obituary/21729727-man-who-saved-world-was-77-stanislav-petrov-was-declared-have-died-september-18th">"Obituary: Stanislav Petrov was declared to have died on September 18th"</a>. <i><a href="https://en.wikipedia.org/wiki/The_Economist" title="The Economist">The Economist</a></i>. <a href="https://en.wikipedia.org/wiki/London" title="London">London</a>, <a href="https://en.wikipedia.org/wiki/England" title="England">England</a>, United Kingdom: <a href="https://en.wikipedia.org/wiki/The_Economist_Group" title="The Economist Group">The Economist Group (The Economist Newspaper Limited)</a>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0013-0613">0013-0613</a>. Archived from <a rel="nofollow" href="https://www.economist.com/news/obituary/21729727-man-who-saved-world-was-77-stanislav-petrov-was-declared-have-died-september-18th">the original</a> on 28 September 2017.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Economist&amp;rft.atitle=Obituary%3A+Stanislav+Petrov+was+declared+to+have+died+on+September+18th&amp;rft.date=2017-09-30&amp;rft.issn=0013-0613&amp;rft_id=https%3A%2F%2Fwww.economist.com%2Fnews%2Fobituary%2F21729727-man-who-saved-world-was-77-stanislav-petrov-was-declared-have-died-september-18th&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-wash-16"><span><b><a href="#cite_ref-wash_16-0">^</a></b></span> <span><cite id="CITEREFHoffman1999">Hoffman, David (10 February 1999). <a href="https://en.wikipedia.org/wiki/Donald_Edward_Graham" title="Donald Edward Graham">Graham, Donald E.</a>; <a href="https://en.wikipedia.org/wiki/Leonard_Downie_Jr." title="Leonard Downie Jr.">Downie Jr., Leonard</a> (eds.). <a rel="nofollow" href="https://web.archive.org/web/20181218193954/https://www.washingtonpost.com/wp-srv/inatl/longterm/coldwar/soviet10.htm">"<span></span>'I Had A Funny Feeling in My Gut'<span></span>"</a>. <i><a href="https://en.wikipedia.org/wiki/The_Washington_Post" title="The Washington Post">The Washington Post</a></i>. <a href="https://en.wikipedia.org/wiki/Washington,_D.C." title="Washington, D.C.">Washington, D.C.</a>, United States of America. p.&nbsp;A19. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0190-8286">0190-8286</a>. <a href="https://en.wikipedia.org/wiki/OCLC_(identifier)" title="OCLC (identifier)">OCLC</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/oclc/2269358">2269358</a>. Archived from <a rel="nofollow" href="https://www.washingtonpost.com/wp-srv/inatl/longterm/coldwar/soviet10.htm">the original</a> on 18 December 2018<span>. Retrieved <span>4 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Washington+Post&amp;rft.atitle=%27I+Had+A+Funny+Feeling+in+My+Gut%27&amp;rft.pages=A19&amp;rft.date=1999-02-10&amp;rft_id=info%3Aoclcnum%2F2269358&amp;rft.issn=0190-8286&amp;rft.aulast=Hoffman&amp;rft.aufirst=David&amp;rft_id=https%3A%2F%2Fwww.washingtonpost.com%2Fwp-srv%2Finatl%2Flongterm%2Fcoldwar%2Fsoviet10.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-moskovskiye-17"><span>^ <a href="#cite_ref-moskovskiye_17-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-moskovskiye_17-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-moskovskiye_17-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-moskovskiye_17-3"><sup><i><b>d</b></i></sup></a></span> <span><cite id="CITEREFVasilev">Vasilev, Yuri. Gurevich, Vladimir; <a href="https://en.wikipedia.org/wiki/Arcadi_Gaydamak" title="Arcadi Gaydamak">Gaydamak, Arcadi</a>; Sokolov, Sergey Viktorovich; Bogomolov, Alexey; Chelnokov, Alexey Sergeevich (eds.). <a rel="nofollow" href="https://web.archive.org/web/20041129144726/https://www.flb.ru/info/27637.html">"Тот, который не нажал"</a> [The one that didn't click]. <i><a href="https://en.wikipedia.org/wiki/Moskovskiye_Novosti" title="Moskovskiye Novosti">Moskovskiye Novosti</a></i> (in Russian). <a href="https://en.wikipedia.org/wiki/Moscow" title="Moscow">Moscow</a>, <a href="https://en.wikipedia.org/wiki/Russia" title="Russia">Russia</a>: FLB LLC. Archived from <a rel="nofollow" href="https://www.flb.ru/info/27637.html">the original</a> on 29 November 2004.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Moskovskiye+Novosti&amp;rft.atitle=%D0%A2%D0%BE%D1%82%2C+%D0%BA%D0%BE%D1%82%D0%BE%D1%80%D1%8B%D0%B9+%D0%BD%D0%B5+%D0%BD%D0%B0%D0%B6%D0%B0%D0%BB&amp;rft.aulast=Vasilev&amp;rft.aufirst=Yuri&amp;rft_id=https%3A%2F%2Fwww.flb.ru%2Finfo%2F27637.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-BBC-18"><span>^ <a href="#cite_ref-BBC_18-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-BBC_18-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-BBC_18-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-BBC_18-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-BBC_18-4"><sup><i><b>e</b></i></sup></a></span> <span><cite id="CITEREFLittle1998">Little, Alan (21 October 1998). <a href="https://en.wikipedia.org/wiki/Fran_Unsworth" title="Fran Unsworth">Unsworth, Fran</a>; <a href="https://en.wikipedia.org/wiki/Mary_Hockaday" title="Mary Hockaday">Hockaday, Mary</a>; <a href="https://en.wikipedia.org/wiki/Huw_Edwards" title="Huw Edwards">Edwards, Huw</a> (eds.). <a rel="nofollow" href="https://web.archive.org/web/20061108234104/http://news.bbc.co.uk/2/hi/europe/198173.stm">"<span></span>'How I stopped nuclear war'<span></span>"</a>. <i><a href="https://en.wikipedia.org/wiki/BBC_News" title="BBC News">BBC News</a></i>. <a href="https://en.wikipedia.org/wiki/London" title="London">London</a>, <a href="https://en.wikipedia.org/wiki/England" title="England">England</a>, United Kingdom of Great Britain: <a href="https://en.wikipedia.org/wiki/British_Broadcasting_Corporation" title="British Broadcasting Corporation">British Broadcasting Corporation (BBC)</a>. Archived from <a rel="nofollow" href="http://news.bbc.co.uk/2/hi/europe/198173.stm">the original</a> on 8 November 2006<span>. Retrieved <span>4 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=BBC+News&amp;rft.atitle=%27How+I+stopped+nuclear+war%27&amp;rft.date=1998-10-21&amp;rft.aulast=Little&amp;rft.aufirst=Alan&amp;rft_id=http%3A%2F%2Fnews.bbc.co.uk%2F2%2Fhi%2Feurope%2F198173.stm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-FAZ-19"><span><b><a href="#cite_ref-FAZ_19-0">^</a></b></span> <span><cite id="CITEREFBraunbergerFrankeHeidHüsgen2013">Braunberger, Gerald; Franke, Martin; Heid, Tatjana; Hüsgen, Simon; Kaube, Jürgen; Johannßen, Philipp; Kohler, Berthold; Konstantinidis, Lisa, eds. (18 February 2013). <a rel="nofollow" href="https://web.archive.org/web/20130219202029/https://www.faz.net/aktuell/gesellschaft/menschen/offizier-petrow-im-gespraech-der-rote-knopf-hat-nie-funktioniert-12084911.html">"Officer Petrow im gespräch: "Der rote Knopf hat nie funktioniert"<span></span>"</a> [Conversation with Officer Petrov: "The red button never worked"]. <i>Frankfurter Allgemeine Zeitung</i> (in German). <a href="https://en.wikipedia.org/wiki/Frankfurt" title="Frankfurt">Frankfurt</a>, <a href="https://en.wikipedia.org/wiki/Hesse" title="Hesse">Hesse</a>, <a href="https://en.wikipedia.org/wiki/Germany" title="Germany">Germany</a>: Frankfurter Allgemeine Zeitung GmbH. Archived from <a rel="nofollow" href="https://www.faz.net/aktuell/gesellschaft/menschen/offizier-petrow-im-gespraech-der-rote-knopf-hat-nie-funktioniert-12084911.html">the original</a> on 19 February 2013<span>. Retrieved <span>8 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Frankfurter+Allgemeine+Zeitung&amp;rft.atitle=Officer+Petrow+im+gespr%C3%A4ch%3A+%22Der+rote+Knopf+hat+nie+funktioniert%22&amp;rft.date=2013-02-18&amp;rft_id=https%3A%2F%2Fwww.faz.net%2Faktuell%2Fgesellschaft%2Fmenschen%2Foffizier-petrow-im-gespraech-der-rote-knopf-hat-nie-funktioniert-12084911.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-20"><span><b><a href="#cite_ref-20">^</a></b></span> <span><cite id="CITEREFPeppard2015">Peppard, Michael (20 March 2015). <a rel="nofollow" href="https://www.proquest.com/openview/38a21b9c8ef68715b7067e832fe91061/1.pdf">"Accidental Armaggedon"</a> <span>(PDF)</span>. <i>Commonweal</i>. <a href="https://en.wikipedia.org/wiki/Bolinas" title="Bolinas">Bolinas</a>, <a href="https://en.wikipedia.org/wiki/California" title="California">California</a>, United States of America: Commonweal Foundation. <b>142</b> (6): 6<span>. Retrieved <span>4 September</span> 2021</span> – via <a href="https://en.wikipedia.org/wiki/ProQuest" title="ProQuest">ProQuest</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Commonweal&amp;rft.atitle=Accidental+Armaggedon&amp;rft.volume=142&amp;rft.issue=6&amp;rft.pages=6&amp;rft.date=2015-03-20&amp;rft.aulast=Peppard&amp;rft.aufirst=Michael&amp;rft_id=https%3A%2F%2Fwww.proquest.com%2Fopenview%2F38a21b9c8ef68715b7067e832fe91061%2F1.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-Votintsev-21"><span><b><a href="#cite_ref-Votintsev_21-0">^</a></b></span> <span><cite id="CITEREFSteele2017">Steele, Jonathan (11 October 2017). <a href="https://en.wikipedia.org/wiki/Katharine_Viner" title="Katharine Viner">Viner, Katharine</a>; <a href="https://en.wikipedia.org/wiki/Neil_Berkett" title="Neil Berkett">Berkett, Neil</a> (eds.). <a rel="nofollow" href="https://web.archive.org/web/20171011171747/https://www.theguardian.com/world/2017/oct/11/stanislav-petrov-obituary">"Stanislav Petrov obituary"</a>. <i><a href="https://en.wikipedia.org/wiki/The_Guardian" title="The Guardian">The Guardian</a></i>. <a href="https://en.wikipedia.org/wiki/London" title="London">London</a>, <a href="https://en.wikipedia.org/wiki/England" title="England">England</a>, <a href="https://en.wikipedia.org/wiki/United_Kingdom" title="United Kingdom">United Kingdom</a>: <a href="https://en.wikipedia.org/wiki/Guardian_Media_Group" title="Guardian Media Group">Guardian Media Group plc</a> (<a href="https://en.wikipedia.org/wiki/Scott_Trust_Limited" title="Scott Trust Limited">Scott Trust</a>). <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/1756-3224">1756-3224</a>. <a href="https://en.wikipedia.org/wiki/OCLC_(identifier)" title="OCLC (identifier)">OCLC</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/oclc/60623878">60623878</a>. Archived from <a rel="nofollow" href="https://www.theguardian.com/world/2017/oct/11/stanislav-petrov-obituary">the original</a> on 11 October 2017<span>. Retrieved <span>4 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Guardian&amp;rft.atitle=Stanislav+Petrov+obituary&amp;rft.date=2017-10-11&amp;rft_id=info%3Aoclcnum%2F60623878&amp;rft.issn=1756-3224&amp;rft.aulast=Steele&amp;rft.aufirst=Jonathan&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Fworld%2F2017%2Foct%2F11%2Fstanislav-petrov-obituary&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-22"><span><b><a href="#cite_ref-22">^</a></b></span> <span><cite id="CITEREFBläsiusSiekmann2021">Bläsius, Karl-Hans; Siekmann, Jörg (27 February 2021). Sonntag, Daniel; Pagel, Peter (eds.). <a rel="nofollow" href="https://link.springer.com/content/pdf/10.1007/s13218-021-00710-0.pdf">"Unintended Nuclear War"</a> <span>(PDF)</span>. <i>KI - Künstliche Intelligenz</i>. Gesellschaft für Informatik e.V./<a href="https://en.wikipedia.org/wiki/Springer_Nature" title="Springer Nature">Springer Nature</a>. <b>35</b> (1): 119–121. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1007%2Fs13218-021-00710-0">10.1007/s13218-021-00710-0</a></span>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0933-1875">0933-1875</a><span>. Retrieved <span>4 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=KI+-+K%C3%BCnstliche+Intelligenz&amp;rft.atitle=Unintended+Nuclear+War&amp;rft.volume=35&amp;rft.issue=1&amp;rft.pages=119-121&amp;rft.date=2021-02-27&amp;rft_id=info%3Adoi%2F10.1007%2Fs13218-021-00710-0&amp;rft.issn=0933-1875&amp;rft.aulast=Bl%C3%A4sius&amp;rft.aufirst=Karl-Hans&amp;rft.au=Siekmann%2C+J%C3%B6rg&amp;rft_id=https%3A%2F%2Flink.springer.com%2Fcontent%2Fpdf%2F10.1007%2Fs13218-021-00710-0.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-Quotes-23"><span>^ <a href="#cite_ref-Quotes_23-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Quotes_23-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Quotes_23-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-Quotes_23-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-Quotes_23-4"><sup><i><b>e</b></i></sup></a></span> <span><cite><a rel="nofollow" href="https://www.brightstarsound.com/world_hero/insight.html">"Important Insight"</a>. Bright Star Sound. <q>It is nice of them to consider me a hero. I don't know that I am. Since I am the only one in this country who has found himself in this situation, it is difficult to know if others would have acted differently.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Important+Insight&amp;rft.pub=Bright+Star+Sound&amp;rft_id=https%3A%2F%2Fwww.brightstarsound.com%2Fworld_hero%2Finsight.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-NBC-24"><span><b><a href="#cite_ref-NBC_24-0">^</a></b></span> <span><cite id="CITEREFBensadoun2010">Bensadoun, Daniel (1 October 2010). <a href="https://en.wikipedia.org/wiki/David_Horovitz" title="David Horovitz">Horovitz, David</a>; Ashkenazi, Inbar; Katz, Yaakov (eds.). <a rel="nofollow" href="https://web.archive.org/web/20210302111314/https://www.jpost.com/International/This-week-in-history-Stanislav-Petrov-avoids-nuclear-war">"This week in history: Stanislav Petrov avoids nuclear war"</a>. <i><a href="https://en.wikipedia.org/wiki/The_Jerusalem_Post" title="The Jerusalem Post">The Jerusalem Post</a></i>. <a href="https://en.wikipedia.org/wiki/Jerusalem" title="Jerusalem">Jerusalem</a>, <a href="https://en.wikipedia.org/wiki/Israel" title="Israel">Israel</a>: Palestine Post Ltd./Jpost Inc. (Jerusalem Post Group). <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0021-597X">0021-597X</a>. Archived from <a rel="nofollow" href="https://www.jpost.com/International/This-week-in-history-Stanislav-Petrov-avoids-nuclear-war">the original</a> on 2 March 2021<span>. Retrieved <span>4 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Jerusalem+Post&amp;rft.atitle=This+week+in+history%3A+Stanislav+Petrov+avoids+nuclear+war&amp;rft.date=2010-10-01&amp;rft.issn=0021-597X&amp;rft.aulast=Bensadoun&amp;rft.aufirst=Daniel&amp;rft_id=https%3A%2F%2Fwww.jpost.com%2FInternational%2FThis-week-in-history-Stanislav-Petrov-avoids-nuclear-war&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-25"><span><b><a href="#cite_ref-25">^</a></b></span> <span><cite id="CITEREFKlare2020">Klare, Michael T. (1 April 2020). <a rel="nofollow" href="https://www.proquest.com/openview/b3c456f8a122d0454502842f065af7c5">"<span></span>'Skynet' Revisited: The Dangerous Allure of Nuclear Command Automation"</a> <span>(PDF)</span>. <i>Arms Control Today</i>. <a href="https://en.wikipedia.org/wiki/Washington,_D.C." title="Washington, D.C.">Washington, D.C.</a>, United States of America: <a href="https://en.wikipedia.org/wiki/Arms_Control_Association" title="Arms Control Association">Arms Control Association</a>. <b>50</b> (3): 10–15<span>. Retrieved <span>4 September</span> 2021</span> – via <a href="https://en.wikipedia.org/wiki/ProQuest" title="ProQuest">ProQuest</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Arms+Control+Today&amp;rft.atitle=%27Skynet%27+Revisited%3A+The+Dangerous+Allure+of+Nuclear+Command+Automation&amp;rft.volume=50&amp;rft.issue=3&amp;rft.pages=10-15&amp;rft.date=2020-04-01&amp;rft.aulast=Klare&amp;rft.aufirst=Michael+T.&amp;rft_id=https%3A%2F%2Fwww.proquest.com%2Fopenview%2Fb3c456f8a122d0454502842f065af7c5&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-26"><span><b><a href="#cite_ref-26">^</a></b></span> <span><cite id="CITEREFPodvig2022">Podvig, Pavel (22 October 2022). <a rel="nofollow" href="https://russianforces.org/blog/2022/10/did_stanislav_petrov_save_the_.shtml">"Did Stanislav Petrov save the world in 1983? It's complicated"</a>. <i>Russian Strategic Nuclear Forces</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Russian+Strategic+Nuclear+Forces&amp;rft.atitle=Did+Stanislav+Petrov+save+the+world+in+1983%3F+It%27s+complicated&amp;rft.date=2022-10-22&amp;rft.aulast=Podvig&amp;rft.aufirst=Pavel&amp;rft_id=https%3A%2F%2Frussianforces.org%2Fblog%2F2022%2F10%2Fdid_stanislav_petrov_save_the_.shtml&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-27"><span><b><a href="#cite_ref-27">^</a></b></span> <span><cite id="CITEREFLundgren2013">Lundgren, Carl (27 June 2013). Pollack, Joshua; Horner, Dan; Spector, Leonard S.; Fink, Anya L.; Williams, Heather (eds.). <a rel="nofollow" href="https://www.tandfonline.com/doi/abs/10.1080/10736700.2013.799828">"What are the odds? Assessing the probability of a nuclear war"</a>. <i>The Nonproliferation Review</i>. <a href="https://en.wikipedia.org/wiki/Monterey" title="Monterey">Monterey</a>, <a href="https://en.wikipedia.org/wiki/California" title="California">California</a>, United States of America: James Martin Center for Nonproliferation Studies (Middlebury Institute of International Studies at Monterey)/<a href="https://en.wikipedia.org/wiki/Taylor_%26_Francis" title="Taylor &amp; Francis">Taylor &amp; Francis</a>. <b>20</b> (2): 361–374. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1080%2F10736700.2013.799828">10.1080/10736700.2013.799828</a>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/1073-6700">1073-6700</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:143379126">143379126</a><span>. Retrieved <span>4 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Nonproliferation+Review&amp;rft.atitle=What+are+the+odds%3F+Assessing+the+probability+of+a+nuclear+war&amp;rft.volume=20&amp;rft.issue=2&amp;rft.pages=361-374&amp;rft.date=2013-06-27&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A143379126%23id-name%3DS2CID&amp;rft.issn=1073-6700&amp;rft_id=info%3Adoi%2F10.1080%2F10736700.2013.799828&amp;rft.aulast=Lundgren&amp;rft.aufirst=Carl&amp;rft_id=https%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Fabs%2F10.1080%2F10736700.2013.799828&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-28"><span><b><a href="#cite_ref-28">^</a></b></span> <span><cite id="CITEREFPeppard2015">Peppard, Michael (20 March 2015). Allsman, Arlene; Dodd, Catherine; Bookoff, Steve; Marcotte, Vanessa (eds.). <a rel="nofollow" href="https://www.proquest.com/openview/38a21b9c8ef68715b7067e832fe91061/1.pdf">"Accidental Armaggedon"</a> <span>(PDF)</span>. <i>Commonweal</i>. <a href="https://en.wikipedia.org/wiki/Bolinas" title="Bolinas">Bolinas</a>, <a href="https://en.wikipedia.org/wiki/California" title="California">California</a>, United States of America: Commonweal Foundation. <b>142</b> (6): 6<span>. Retrieved <span>4 September</span> 2021</span> – via <a href="https://en.wikipedia.org/wiki/ProQuest" title="ProQuest">ProQuest</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Commonweal&amp;rft.atitle=Accidental+Armaggedon&amp;rft.volume=142&amp;rft.issue=6&amp;rft.pages=6&amp;rft.date=2015-03-20&amp;rft.aulast=Peppard&amp;rft.aufirst=Michael&amp;rft_id=https%3A%2F%2Fwww.proquest.com%2Fopenview%2F38a21b9c8ef68715b7067e832fe91061%2F1.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-USANPS-29"><span><b><a href="#cite_ref-USANPS_29-0">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.nps.gov/people/stanislav_petrov.htm">"Stanislav Petrov (U.S. National Park Service)"</a>. <i>www.nps.gov</i><span>. Retrieved <span>27 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.nps.gov&amp;rft.atitle=Stanislav+Petrov+%28U.S.+National+Park+Service%29&amp;rft_id=https%3A%2F%2Fwww.nps.gov%2Fpeople%2Fstanislav_petrov.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-30"><span><b><a href="#cite_ref-30">^</a></b></span> <span><cite id="CITEREFMyre2017">Myre, Greg (18 September 2017). Lansing, John (ed.). <a rel="nofollow" href="https://web.archive.org/web/20170918170724/https://www.npr.org/sections/thetwo-way/2017/09/18/551792129/stanislav-petrov-the-man-who-saved-the-world-dies-at-77">"Stanislav Petrov, 'The Man Who Saved The World,' Dies At 77"</a>. <i><a href="https://en.wikipedia.org/wiki/National_Public_Radio" title="National Public Radio">National Public Radio (NPR)</a></i>. <a href="https://en.wikipedia.org/wiki/Washington,_D.C." title="Washington, D.C.">Washington, D.C.</a>, United States of America: National Public Radio, Inc. Archived from <a rel="nofollow" href="https://www.npr.org/sections/thetwo-way/2017/09/18/551792129/stanislav-petrov-the-man-who-saved-the-world-dies-at-77">the original</a> on 18 September 2017<span>. Retrieved <span>4 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=National+Public+Radio+%28NPR%29&amp;rft.atitle=Stanislav+Petrov%2C+%27The+Man+Who+Saved+The+World%2C%27+Dies+At+77&amp;rft.date=2017-09-18&amp;rft.aulast=Myre&amp;rft.aufirst=Greg&amp;rft_id=https%3A%2F%2Fwww.npr.org%2Fsections%2Fthetwo-way%2F2017%2F09%2F18%2F551792129%2Fstanislav-petrov-the-man-who-saved-the-world-dies-at-77&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-31"><span><b><a href="#cite_ref-31">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.bbc.co.uk/news/world-europe-41314948">"Stanislav Petrov, who averted possible nuclear war, dies at 77"</a>. BBC. 19 September 2017<span>. Retrieved <span>19 September</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Stanislav+Petrov%2C+who+averted+possible+nuclear+war%2C+dies+at+77&amp;rft.date=2017-09-19&amp;rft_id=https%3A%2F%2Fwww.bbc.co.uk%2Fnews%2Fworld-europe-41314948&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-32"><span><b><a href="#cite_ref-32">^</a></b></span> <span><cite id="CITEREFSanders-Zakre2017">Sanders-Zakre, Alicia (1 October 2017). <a rel="nofollow" href="https://www.proquest.com/openview/657e5a18a81111d6bef5355be3c7ea7a">"Man Who 'Saved the World'Dies at 77"</a> <span>(PDF)</span>. <i>Arms Control Today</i>. <a href="https://en.wikipedia.org/wiki/Washington,_D.C." title="Washington, D.C.">Washington, D.C.</a>, United States of America: <a href="https://en.wikipedia.org/wiki/Arms_Control_Association" title="Arms Control Association">Arms Control Association</a>. <b>47</b> (8): 31<span>. Retrieved <span>4 September</span> 2021</span> – via <a href="https://en.wikipedia.org/wiki/ProQuest" title="ProQuest">ProQuest</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Arms+Control+Today&amp;rft.atitle=Man+Who+%27Saved+the+World%27Dies+at+77&amp;rft.volume=47&amp;rft.issue=8&amp;rft.pages=31&amp;rft.date=2017-10-01&amp;rft.aulast=Sanders-Zakre&amp;rft.aufirst=Alicia&amp;rft_id=https%3A%2F%2Fwww.proquest.com%2Fopenview%2F657e5a18a81111d6bef5355be3c7ea7a&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-award-33"><span>^ <a href="#cite_ref-award_33-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-award_33-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-award_33-2"><sup><i><b>c</b></i></sup></a></span> <span><cite><a rel="nofollow" href="http://www.brightstarsound.com/">"Stanislav Petrov Averts a Worldwide Nuclear War"</a>. Bright Star Sound<span>. Retrieved <span>27 September</span> 2006</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Stanislav+Petrov+Averts+a+Worldwide+Nuclear+War&amp;rft.pub=Bright+Star+Sound&amp;rft_id=http%3A%2F%2Fwww.brightstarsound.com%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-mosnews-34"><span><b><a href="#cite_ref-mosnews_34-0">^</a></b></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20060206103808/http://www.mosnews.com/news/2006/01/20/petrovaward.shtml">"Russian Colonel Who Averted Nuclear War Receives World Citizen Award"</a>. <i><a href="https://en.wikipedia.org/wiki/Moscow_News" title="Moscow News">Moscow News</a></i>. 20 January 2006. Archived from <a rel="nofollow" href="http://www.mosnews.com/news/2006/01/20/petrovaward.shtml">the original</a> on 6 February 2006<span>. Retrieved <span>27 September</span> 2006</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Moscow+News&amp;rft.atitle=Russian+Colonel+Who+Averted+Nuclear+War+Receives+World+Citizen+Award&amp;rft.date=2006-01-20&amp;rft_id=http%3A%2F%2Fwww.mosnews.com%2Fnews%2F2006%2F01%2F20%2Fpetrovaward.shtml&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-35"><span><b><a href="#cite_ref-35">^</a></b></span> <span><cite id="CITEREFYudkowsky">Yudkowsky, Eliezer. <a rel="nofollow" href="https://www.lesswrong.com/posts/QtyKq4BDyuJ3tysoK/9-26-is-petrov-day">"9/26 is Petrov Day"</a>. <i>www.lesswrong.com</i><span>. Retrieved <span>4 July</span> 2023</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=www.lesswrong.com&amp;rft.atitle=9%2F26+is+Petrov+Day&amp;rft.aulast=Yudkowsky&amp;rft.aufirst=Eliezer&amp;rft_id=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQtyKq4BDyuJ3tysoK%2F9-26-is-petrov-day&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-StatementFilm-36"><span><b><a href="#cite_ref-StatementFilm_36-0">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.statementfilm.com/">"Statement Film website"</a>. Statement Film ApS.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Statement+Film+website&amp;rft.pub=Statement+Film+ApS&amp;rft_id=http%3A%2F%2Fwww.statementfilm.com%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-37"><span><b><a href="#cite_ref-37">^</a></b></span> <span><cite id="CITEREFBernstein2014">Bernstein, Rachel (21 October 2014). <a href="https://en.wikipedia.org/wiki/Jay_Penske" title="Jay Penske">Penske, Jay</a>; Blauvelt, Christian; Eric, Eric (eds.). <a rel="nofollow" href="https://web.archive.org/web/20170710090656/https://www.indiewire.com/2014/10/2014-woodstock-film-festival-honors-darren-aronofsky-announces-audience-awards-68891">"2014 Woodstock Film Festival Honors Darren Aronofsky, Announces Audience Awards"</a>. <i><a href="https://en.wikipedia.org/wiki/IndieWire" title="IndieWire">IndieWire</a></i>. <a href="https://en.wikipedia.org/wiki/Los_Angeles" title="Los Angeles">Los Angeles</a>, <a href="https://en.wikipedia.org/wiki/California" title="California">California</a>, United States of America: <a href="https://en.wikipedia.org/wiki/Penske_Media_Corporation" title="Penske Media Corporation">Penske Media Corporation</a>. Archived from <a rel="nofollow" href="https://www.indiewire.com/2014/10/2014-woodstock-film-festival-honors-darren-aronofsky-announces-audience-awards-68891">the original</a> on 10 July 2017<span>. Retrieved <span>4 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=IndieWire&amp;rft.atitle=2014+Woodstock+Film+Festival+Honors+Darren+Aronofsky%2C+Announces+Audience+Awards&amp;rft.date=2014-10-21&amp;rft.aulast=Bernstein&amp;rft.aufirst=Rachel&amp;rft_id=https%3A%2F%2Fwww.indiewire.com%2F2014%2F10%2F2014-woodstock-film-festival-honors-darren-aronofsky-announces-audience-awards-68891&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-38"><span><b><a href="#cite_ref-38">^</a></b></span> <span><cite><a rel="nofollow" href="http://dresdner-friedenspreis.de/preistraeger/">"Preisträger – Dresden-Preis (Prize winners - Dresden Prize)"</a>. <i>Internationaler Friedenspreis</i> (in German)<span>. Retrieved <span>28 September</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Internationaler+Friedenspreis&amp;rft.atitle=Preistr%C3%A4ger+%E2%80%93+Dresden-Preis+%28Prize+winners+-+Dresden+Prize%29&amp;rft_id=http%3A%2F%2Fdresdner-friedenspreis.de%2Fpreistraeger%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-39"><span><b><a href="#cite_ref-39">^</a></b></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20200728043304/https://www.deutscher-medienpreis.de/deutscher-medienpreis-20011-an-yacoobi-raheb-petrow-mukwege/">"Deutscher Medienpreis 2011 an Dr. Sakena Yacoobi, Dr. Mitri Raheb, Stanislaw Petrow &amp; Dr. Denis Mukwege"</a>. <i>Deutscher Medienpreis</i> (in German). Archived from <a rel="nofollow" href="https://www.deutscher-medienpreis.de/deutscher-medienpreis-20011-an-yacoobi-raheb-petrow-mukwege/">the original</a> on 28 July 2020<span>. Retrieved <span>28 September</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Deutscher+Medienpreis&amp;rft.atitle=Deutscher+Medienpreis+2011+an+Dr.+Sakena+Yacoobi%2C+Dr.+Mitri+Raheb%2C+Stanislaw+Petrow+%26+Dr.+Denis+Mukwege&amp;rft_id=https%3A%2F%2Fwww.deutscher-medienpreis.de%2Fdeutscher-medienpreis-20011-an-yacoobi-raheb-petrow-mukwege%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-40"><span><b><a href="#cite_ref-40">^</a></b></span> <span><cite id="CITEREFBarash">Barash, David. Shermer, Michael; Linse, Pat; Miele, Frank; Bull, William (eds.). <a rel="nofollow" href="https://go.gale.com/ps/i.do?id=GALE%7CA656765713">"Close Calls: When Nuclear Armageddon Threatened to Destroy Civilization"</a>. <i><a href="https://en.wikipedia.org/wiki/Skeptic_(U.S._magazine)" title="Skeptic (U.S. magazine)">Skeptic Magazine</a></i>. <a href="https://en.wikipedia.org/wiki/Altadena,_California" title="Altadena, California">Altadena</a>, <a href="https://en.wikipedia.org/wiki/California" title="California">California</a>, United States of America: <a href="https://en.wikipedia.org/wiki/The_Skeptics_Society" title="The Skeptics Society">The Skeptics Society</a>. <b>26</b> (1): 39–46. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/1063-9330">1063-9330</a><span>. Retrieved <span>4 September</span> 2021</span> – via Gale Academic OneFile.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Skeptic+Magazine&amp;rft.atitle=Close+Calls%3A+When+Nuclear+Armageddon+Threatened+to+Destroy+Civilization&amp;rft.volume=26&amp;rft.issue=1&amp;rft.pages=39-46&amp;rft.issn=1063-9330&amp;rft.aulast=Barash&amp;rft.aufirst=David&amp;rft_id=https%3A%2F%2Fgo.gale.com%2Fps%2Fi.do%3Fid%3DGALE%257CA656765713&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-thebulletin.org-41"><span>^ <a href="#cite_ref-thebulletin.org_41-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-thebulletin.org_41-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFTegmark2018">Tegmark, Max (26 September 2018). Mecklin, John; Bronson, Rachel; Drollette Jr., Dan (eds.). <a rel="nofollow" href="https://web.archive.org/web/20180929123105/https://www.thebulletin.org/2018/09/a-posthumous-honor-for-the-man-who-saved-the-world">"A posthumous honor for the man who saved the world"</a>. <i>Bulletin of the Atomic Scientists</i>. <a href="https://en.wikipedia.org/wiki/Chicago" title="Chicago">Chicago</a>, <a href="https://en.wikipedia.org/wiki/Illinois" title="Illinois">Illinois</a>, United States of America: Educational Foundation for Nuclear Science (<a href="https://en.wikipedia.org/wiki/Taylor_and_Francis" title="Taylor and Francis">Taylor and Francis</a>). <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0096-3402">0096-3402</a>. <a href="https://en.wikipedia.org/wiki/LCCN_(identifier)" title="LCCN (identifier)">LCCN</a>&nbsp;<a rel="nofollow" href="https://lccn.loc.gov/48034039">48034039</a>. <a href="https://en.wikipedia.org/wiki/OCLC_(identifier)" title="OCLC (identifier)">OCLC</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/oclc/470268256">470268256</a>. Archived from <a rel="nofollow" href="https://thebulletin.org/2018/09/a-posthumous-honor-for-the-man-who-saved-the-world">the original</a> on 29 September 2018<span>. Retrieved <span>4 September</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bulletin+of+the+Atomic+Scientists&amp;rft.atitle=A+posthumous+honor+for+the+man+who+saved+the+world&amp;rft.date=2018-09-26&amp;rft_id=info%3Aoclcnum%2F470268256&amp;rft.issn=0096-3402&amp;rft_id=info%3Alccn%2F48034039&amp;rft.aulast=Tegmark&amp;rft.aufirst=Max&amp;rft_id=https%3A%2F%2Fthebulletin.org%2F2018%2F09%2Fa-posthumous-honor-for-the-man-who-saved-the-world&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-42"><span><b><a href="#cite_ref-42">^</a></b></span> <span><cite id="CITEREFShuster2017">Shuster, Samuel (19 September 2017). <a href="https://en.wikipedia.org/wiki/Edward_Felsenthal" title="Edward Felsenthal">Felsenthal, Edward</a>; Benioff, Marc (eds.). <a rel="nofollow" href="https://www.time.com/4947879/stanislav-petrov-russia-nuclear-war-obituary">"Stanislav Petrov, the Russian Officer who averted a nuclear war"</a>. <i><a href="https://en.wikipedia.org/wiki/Time_(magazine)" title="Time (magazine)">Time</a></i>. Vol.&nbsp;19, no.&nbsp;10. <a href="https://en.wikipedia.org/wiki/New_York_City" title="New York City">New York City</a>, <a href="https://en.wikipedia.org/wiki/New_York_(state)" title="New York (state)">New York</a>, United States of America: Time USA, LLC (Marc &amp; Lynne Benioff). pp.&nbsp;3–5. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0040-781X">0040-781X</a>. <a href="https://en.wikipedia.org/wiki/OCLC_(identifier)" title="OCLC (identifier)">OCLC</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/oclc/1311479">1311479</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Time&amp;rft.atitle=Stanislav+Petrov%2C+the+Russian+Officer+who+averted+a+nuclear+war&amp;rft.volume=19&amp;rft.issue=10&amp;rft.pages=3-5&amp;rft.date=2017-09-19&amp;rft_id=info%3Aoclcnum%2F1311479&amp;rft.issn=0040-781X&amp;rft.aulast=Shuster&amp;rft.aufirst=Samuel&amp;rft_id=https%3A%2F%2Fwww.time.com%2F4947879%2Fstanislav-petrov-russia-nuclear-war-obituary&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-Kalugin-43"><span><b><a href="#cite_ref-Kalugin_43-0">^</a></b></span> <span><cite><a rel="nofollow" href="http://hnn.us/articles/1709.html#bombs9-5-03">"The Nuclear War that Almost Happened in 1983"</a>. <i>The Baltimore Sun</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Baltimore+Sun&amp;rft.atitle=The+Nuclear+War+that+Almost+Happened+in+1983&amp;rft_id=http%3A%2F%2Fhnn.us%2Farticles%2F1709.html%23bombs9-5-03&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStanislav+Petrov"></span></span>
</li>
<li id="cite_note-44"><span><b><a href="#cite_ref-44">^</a></b></span> <span>López Girondo, A. La historia del hombre que salvó a la humanidad de una catástrofe nuclear. Tiempo Argentino [Internet]. 23 April 2023. [citado 23 April 2023]. Disponible en: <a rel="nofollow" href="https://www.tiempoar.com.ar/mundo/la-historia-del-hombre-que-salvo-a-la-humanidad-de-una-catastrofe-nuclear/">https://www.tiempoar.com.ar/mundo/la-historia-del-hombre-que-salvo-a-la-humanidad-de-una-catastrofe-nuclear/</a></span>
</li>
</ol></div>
<h2><span id="External_links">External links</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Stanislav_Petrov&amp;action=edit&amp;section=7" title="Edit section: External links">edit</a><span>]</span></span></h2>

<ul><li><a rel="nofollow" href="https://www.brightstarsound.com/">BrightStarSound.com</a> a tribute website, multiple pages with photos and reprints of various articles about Petrov</li>
<li><a rel="nofollow" href="https://www.brightstarsound.com/world_hero/weekendavisen.html">Nuclear War:  Minuteman</a></li>
<li><a rel="nofollow" href="https://www.imdb.com/title/tt2277106/"><i>The Man Who Saved the World</i></a> at <a href="https://en.wikipedia.org/wiki/IMDb_(identifier)" title="IMDb (identifier)">IMDb</a></li></ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Warren Spector – 40 years and I'm still here (155 pts)]]></title>
            <link>https://www.gamedeveloper.com/blogs/my-40-years-in-the-game-industry</link>
            <guid>37665946</guid>
            <pubDate>Tue, 26 Sep 2023 21:18:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gamedeveloper.com/blogs/my-40-years-in-the-game-industry">https://www.gamedeveloper.com/blogs/my-40-years-in-the-game-industry</a>, See on <a href="https://news.ycombinator.com/item?id=37665946">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>To my amazement, this month - September 2023 - marks my 40th year in game development. I’m not sure how I’ve survived so long in a business that more typically burns people out in... somewhat less, let’s just say. That said, I have some thoughts about survival and an urge to tell the story of how I today find myself one of the most long-lived veterans still actively involved in making games. There may be a few older geezers still working, I guess, but most of you who want to chime in with an arthritic “But I’ve been doing this longer than you” or “You’re a young whipper-snapper next to me” are probably big deals on the business side - C-suite types. If you’ve worked more than 40 years and are still actually working with teams to make games, let me know. We can hang out and talk about our lumbago.</p><p>But now I’m just bragging.</p><p>So, I want to share my story. And I want to talk about how things have changed and about how they haven’t. Don’t expect any deep design or creative leadership insights here. I’m going full selfish on you and talking about myself. (Hey, if Cliffy B and John Romero can write their - fascinating - autobiographies, a blog post from me can't hurt.) Maybe you’ll find what I have to say interesting, maybe not. One of the joys of aging is that you kind of stop caring what other people think, so I’m just going to ramble on. You can join me if you want. Or go read someone else's blog. I won't be offended.</p><h3>My story</h3><p>I discovered Dungeons &amp; Dragons back in 1978, just four years after the game made its debut. I bought the white box edition - the one that required players to make up half the rules, a bit of design genius if there ever was one. I mean, once you'd made up a bunch of rules and made the game your own, who was going to give it up for some other set of (possibly better) rules? Modern TTRPG designers may want to take note.</p><p>Anyway, prior to the D&amp;D revelation, I was an obsessive player of little-cardboard-square boardgames with a writer friend of mine - Walton Simons. (The name may sound familiar to Deus Ex fans.) Some of those games came in ziplock bags from a company called Metagames and later from Steve Jackson Games (important to the story later). Walton - better known as "Bud" invited me to play in a D&amp;D game whose Dungeonmaster was an SF writer named Bruce Sterling, before his first publication and before he became one of the fathers of the cyberpunk movement. I played in that campaign for about ten years and if I tell you how it ended I'll end of crying so you'll have to use your imaginations. Let's just say my fellow party members and I started as lowly troublemakers, the Rat Gang, and ended up political, military and individual powers in the river city of Shang. (This was long before TSR came out with Oriental Adventures. See earlier reference to "writing rules to make the game your own.) I played in a bunch of other campaigns as well, but Bruce’s is the one I’ll always remember.</p><p>My personal favorite game was Chill from Pacesetter games, but eventually I created my own set of rules, for a World War 2 RPG. I ran that campaign - about what would have happened in Poland if Nazi occult beliefs were true - for a while before my life changed completely. (As a note, even that Nazi occult stuff was based-in reality as someone saw it, one of the core tenets of the rest of the tabletop RPG's and every digital game I've worked on since.)</p><p>That was the context and only qualification for getting my first professional job in game development. I was a game nerd. I was also a film nerd, working on my PhD at The University of Texas where, as part of that program, I was teaching film history, theory and criticism when the department asked me to give up those classes and help another grad student learn how to teach one of them. I had no idea how I was going to pay my rent.</p><h2>Steve Jackson Games</h2><p>That was when I got a call out of the blue from a gaming friend who was working at Steve Jackson Games asking if I was interested in an Associate Game Editor at Steve Jackson Games, a small developer of tabletop games in my hometown of Austin, Texas. They were looking for a developer on the game side and an editor for Space Gamer and Fantasy Gamer magazines. That was in 1983. The year I dropped out of the PhD program I was in, just a dissertation short of a Doctorate, to make games. My mom cried for ten years. Given the hijinks at the company that might have been justified.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltc021aec5dfa720b5/6511e33890ea2622f11541e9/Steve_Jackson_Warren_Spector_ET_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltc021aec5dfa720b5/6511e33890ea2622f11541e9/Steve_Jackson_Warren_Spector_ET_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltc021aec5dfa720b5/6511e33890ea2622f11541e9/Steve_Jackson_Warren_Spector_ET_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltc021aec5dfa720b5/6511e33890ea2622f11541e9/Steve_Jackson_Warren_Spector_ET_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="5f84eymj7lbd" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltc021aec5dfa720b5/6511e33890ea2622f11541e9/Steve_Jackson_Warren_Spector_ET_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltc021aec5dfa720b5">
        </picture>
        <figcaption>Steve Jackson, me and a surprise guest editor</figcaption></figure><p>I worked on a lot of games at SJG, but the two things that were most noteworthy were Thing in the Darkness and TOON: The Cartoon Roleplaying (both released in 1984). Thing in the Darkness was a solo adventure by Matthew J. Costello which Steve felt needed editing and development. Steve assigned me to do that, but I had no experience flowcharting and revising choose your own adventures and no idea how to do it. Magazine editing I had nailed - I'd done that editing an entertainment magazine insert in The Daily Texan newspaper. Developing a solo adventure? I was clueless. Steve Jackson stepped in and gave me a lesson I needed and will never forget. The Thing in the Darkness appeared on the cover of Fantasy Gamer #3 and was received well. Success!</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd1bf9962bdced534/6511e36a063da84658eaadaf/Thing_in_the_Darkness_2_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd1bf9962bdced534/6511e36a063da84658eaadaf/Thing_in_the_Darkness_2_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd1bf9962bdced534/6511e36a063da84658eaadaf/Thing_in_the_Darkness_2_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd1bf9962bdced534/6511e36a063da84658eaadaf/Thing_in_the_Darkness_2_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="00hscm9yjl3s" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd1bf9962bdced534/6511e36a063da84658eaadaf/Thing_in_the_Darkness_2_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltd1bf9962bdced534">
        </picture>
        <figcaption>Thing in the Darkness – the first published game I worked on (all credit to Matthew J. Costello for a fine foundation and Steve Jackson for giving me an education in flowcharting!)</figcaption></figure><p>TOON was, to this day, a high point in my career. I've always been a cartoon freak and when I saw TOON on a stack of games submitted for magazine publication, I knew we had something special. It was designed by a guy named Greg Costikyan, a well-known, super-talented tabletop designer. (Weirdly, he and I went to the same high school, though we didn't know each other. There must have been something in the water at that school!)</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1bc07117419b515b/6511c42a6805d46c5e6aea30/TOON_Playtest.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1bc07117419b515b/6511c42a6805d46c5e6aea30/TOON_Playtest.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1bc07117419b515b/6511c42a6805d46c5e6aea30/TOON_Playtest.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1bc07117419b515b/6511c42a6805d46c5e6aea30/TOON_Playtest.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="6tldver2w72v" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1bc07117419b515b/6511c42a6805d46c5e6aea30/TOON_Playtest.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt1bc07117419b515b">
        </picture>
        <figcaption>TOON playtest. That’s Allen Varney on the left and Caroline Chase (soon to be Caroline Spector) on the right. If you’re laughing when you playtest a cartoon RPG you’re doing something right.</figcaption></figure><p>Two things struck me about TOON in its original form - it was way too cool to be relegated to magazine publication and it was written in what was called SPI case format. (The latter was the geekiest, dullest albeit maximally effective way of writing rules.) I went to Steve and asked him to let me develop the TOON rules into something simpler and more appropriate to the subject matter. To his credit he told me to run with it. I enlisted the aid of another designer, Allen Varney, who was a great collaborator and deserves more credit for TOON's success than he gets. The game debuted at GENCON IN 1984 - the same day as West End Games' game, Paranoia. The competition for funniest game was on! Both teams won. I never again heard as much laughter at a GENCON. What I'll say is that TOON is still for sale 40 years after Allen and I worked on it. We did something right!</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2ba62833e56cc1f3/6511c55e1d8b4f70eaa511b8/SJG_Crew.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2ba62833e56cc1f3/6511c55e1d8b4f70eaa511b8/SJG_Crew.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2ba62833e56cc1f3/6511c55e1d8b4f70eaa511b8/SJG_Crew.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2ba62833e56cc1f3/6511c55e1d8b4f70eaa511b8/SJG_Crew.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="lfzw5fqywisz" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2ba62833e56cc1f3/6511c55e1d8b4f70eaa511b8/SJG_Crew.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt2ba62833e56cc1f3">
        </picture>
        <figcaption>The SJG crew in the early-80s. That’s me in the vest. My future wife, Caroline, is second from the right. Frequent collaborator, Allen Varney is far right. Steve Jackson is in the jacket and tie in the second row.</figcaption></figure><p>Three years into my tenure as Steve Jackson Games’ Editor-in-Chief (I got promoted!) I got a call out of the blue from someone at TSR. They made me the Godfather offer - $25,000 a year! – so I moved to Lake Geneva, home of D&amp;D.</p><h2>TSR</h2><p>At the time, D&amp;D was perceived in the game development world as yesterday’s news, so bound up with what Gary Gygax and Dave Arneson had come up with they hadn’t innovated in years. Frankly, I thought it was a design problem, that they needed fresh blood, people who wanted to do new things. What I found was something completely different – the design talent up there was prodigious and, frankly, more ambitious than they were allowed to be by the folks in the C-suite. Looking back, you have to understand the executive viewpoint - I mean, would <em>you</em> mess with D&amp;D, the most successful TTRPG on the planet? Well, I would, but no one would mistake me for a business-first sort of guy.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt4d56a701ded622b5/6511e3b847304e6539afec5d/Warren_Spector_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt4d56a701ded622b5/6511e3b847304e6539afec5d/Warren_Spector_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt4d56a701ded622b5/6511e3b847304e6539afec5d/Warren_Spector_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt4d56a701ded622b5/6511e3b847304e6539afec5d/Warren_Spector_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="soq9lmigxigl" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt4d56a701ded622b5/6511e3b847304e6539afec5d/Warren_Spector_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt4d56a701ded622b5">
        </picture>
        <figcaption>Early in my tenure at TSR. Smug bastard, eh? If you can’t read it, the button says “For this we came all the way from Texas?” It was a reference to the weather, not the company!</figcaption></figure><p>At TSR, I worked as a developer, editor or designer on a bunch of games and modules. I was also Manager of the Game Division for a while, working under a talented designer and all-around great guy, Jim Ward. But that was less interesting and fun than working on games! Notably, I worked with Doug Niles on the Top Secret/S.I. RPG (1987), with Jeff Grubb on the Buck Rogers: Battle for the 25th Century Boardgame (1988), and with Zeb Cook on The Bullwinkle &amp; Rocky Party Roleplaying Game (1988). The first of those was well-received and to this day I think the mechanics were really innovative and still relevant. The Buck Rogers game was definitely not received well! I'll defend it to the end – it was a ton of fun and with greater "toy value" than most games at the time.</p><p>I honestly don’t remember if the Bullwinkle and Rockey game attracted any attention at all, but it was for sure zany to the max – hand puppets, spinners, storytelling cards, fake diplomas from Wossamotta U. We tested with all players of all ages, from 7 to 70, and it really did appeal to kids and adults.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt203f7bdee8c5434a/6511e3fa22e09d9d9968bd55/Moose_and_Squirrel_RPG_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt203f7bdee8c5434a/6511e3fa22e09d9d9968bd55/Moose_and_Squirrel_RPG_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt203f7bdee8c5434a/6511e3fa22e09d9d9968bd55/Moose_and_Squirrel_RPG_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt203f7bdee8c5434a/6511e3fa22e09d9d9968bd55/Moose_and_Squirrel_RPG_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="m0mbls4nx107" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt203f7bdee8c5434a/6511e3fa22e09d9d9968bd55/Moose_and_Squirrel_RPG_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt203f7bdee8c5434a">
        </picture>
        <figcaption>Right up there with TOON for the zaniest game I ever worked on!</figcaption></figure><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1b7705263dccef44/6511e44287e59c5946cca70d/Rocky_Bullwinkle_Game_Swag_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1b7705263dccef44/6511e44287e59c5946cca70d/Rocky_Bullwinkle_Game_Swag_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1b7705263dccef44/6511e44287e59c5946cca70d/Rocky_Bullwinkle_Game_Swag_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1b7705263dccef44/6511e44287e59c5946cca70d/Rocky_Bullwinkle_Game_Swag_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="sz2a88mn9416" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt1b7705263dccef44/6511e44287e59c5946cca70d/Rocky_Bullwinkle_Game_Swag_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt1b7705263dccef44">
        </picture>
        <figcaption>And here’s why – Cartoony rules, spineers, hand puppets, and more</figcaption></figure><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt70d4b62b7335d03e/6511c7623741cdc91c1eab8d/GenCon_1988.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt70d4b62b7335d03e/6511c7623741cdc91c1eab8d/GenCon_1988.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt70d4b62b7335d03e/6511c7623741cdc91c1eab8d/GenCon_1988.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt70d4b62b7335d03e/6511c7623741cdc91c1eab8d/GenCon_1988.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="1dxkfv7ks761" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt70d4b62b7335d03e/6511c7623741cdc91c1eab8d/GenCon_1988.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt70d4b62b7335d03e">
        </picture>
        <figcaption>Designers at GenCon 1988 playing with Bullwinkle &amp; Rocky stuff. That’s Mike Pondsmith (yes, the Cyberpunk guy!), Lawrence Schick, me, and Zeb Cook, from left to right.</figcaption></figure><p>These projects were early lessons for me that sales aren't the only success criterion (though it's obviously a hugely important one!). I also got to work on the 2<sup>nd</sup> Edition AD&amp;D Dungeonmasters Guide, which was kind of cool, to say the least, but it shipped after I left.</p><p>And speaking of leaving...</p><p>By late 1988, I had started thinking it might be time to move on. It was cool that I got to write a novel (The Hollow Earth Affair) and a Marvel Superheroes solo book (One Thing After Another) featuring the Thing from the Fantastic Four. It was a thrill getting to type “It’s clobbering’ time!” and years later I even got Stan Lee to autograph a copy. Still, I was feeling like the biggest decision I had to make in my job was whether to use percentile dice or D20s. Let’s just say that wasn’t the most fulfilling thing about game design.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blta66adc7a1a415ed8/6511e48860a66b03009b186b/One_Thing_After_Another_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blta66adc7a1a415ed8/6511e48860a66b03009b186b/One_Thing_After_Another_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blta66adc7a1a415ed8/6511e48860a66b03009b186b/One_Thing_After_Another_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blta66adc7a1a415ed8/6511e48860a66b03009b186b/One_Thing_After_Another_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="zrt97esl01zt" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blta66adc7a1a415ed8/6511e48860a66b03009b186b/One_Thing_After_Another_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blta66adc7a1a415ed8">
        </picture>
        <figcaption>My solo adventure featuring The Thing and the Fantastic Four.</figcaption></figure><figure><grammarly-extension data-grammarly-shadow-root="true"></grammarly-extension><grammarly-extension data-grammarly-shadow-root="true"></grammarly-extension>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltea1b877abe0c6b4a/6511e4c8321f32271c6aad8a/Warren_Spector_Stan_Lee_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltea1b877abe0c6b4a/6511e4c8321f32271c6aad8a/Warren_Spector_Stan_Lee_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltea1b877abe0c6b4a/6511e4c8321f32271c6aad8a/Warren_Spector_Stan_Lee_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltea1b877abe0c6b4a/6511e4c8321f32271c6aad8a/Warren_Spector_Stan_Lee_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="dy9iog6n75mg" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltea1b877abe0c6b4a/6511e4c8321f32271c6aad8a/Warren_Spector_Stan_Lee_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltea1b877abe0c6b4a">
        </picture>
        <figcaption spellcheck="false">And here's Stan the Man autographing a copy of One Thing After Another. I could barely speak. Stan Lee changed my life!</figcaption></figure><figure><grammarly-extension data-grammarly-shadow-root="true"></grammarly-extension><grammarly-extension data-grammarly-shadow-root="true"></grammarly-extension>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt04f43007366b9a4b/6511eba147304ef362afec62/Double_Agent_by_Warren_Spector_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt04f43007366b9a4b/6511eba147304ef362afec62/Double_Agent_by_Warren_Spector_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt04f43007366b9a4b/6511eba147304ef362afec62/Double_Agent_by_Warren_Spector_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt04f43007366b9a4b/6511eba147304ef362afec62/Double_Agent_by_Warren_Spector_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="lodz8d4b08kn" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt04f43007366b9a4b/6511eba147304ef362afec62/Double_Agent_by_Warren_Spector_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt04f43007366b9a4b">
        </picture>
        <figcaption spellcheck="false">My one published novel, part of a two-book combo. It’s said that everyone has a novel that should stay in a closet and never get published. This was mine. But it was fun as hell to write.</figcaption></figure><p>At that time, I figured there were two logical career paths – become a Disney Imagineer or get into computer games, which I was playing obsessively. I talked to some folks at Disney on the phone, but that didn’t lead anywhere. Probably for the best, as it turned out. See, I had never worked on a theme park attraction, and I was playing computer games like they were going out of style as well as testing the new AD&amp;D digital efforts. I respected the efforts of the folks making those games – they were doing an excellent job of bringing the D&amp;D rules to computers - but I felt that wasn’t the right way to approach things. The <em>feeling</em> of playing D&amp;D was what was important, not the rules. The games I loved the most were the ones that felt right which, at the time, meant the Ultima games made by Richard Garriott and his teams at Origin. Their motto was "We Create Worlds" and they lived up to it. They were getting closer than anyone else to making me feel like I was playing D&amp;D, a game defined by telling stories <em>with</em> an author (or Dungeonmaster) rather than being <em>told</em> a story by that person.</p><p>It was a low point for me, but I got lucky again and, in 1988, got a call from someone I'd worked with at Steve Jackson Games who was now at Origin. One day he called me up and said Origin was looking for an Associate Producer and asked if I was interested. That was easy! YES! The opportunity to work with Richard Garriott, the guy who made the early Ultima games and led teams to create later ones, was too enticing to say no to. Richard got it. He saw everything I wanted games to be (within the limits of then current technology).</p><p>I got the job.</p><h2>Origin</h2><p>It was 1989 and the first things I was assigned to work on at Origin were Ultima VI, Richard's latest project, and Paul Neurath's Space Rogue.</p><p>Paul probably doesn't even remember this, but he asked me to work on the plot and flowchart missions. I did that. He junked everything I did and redid it all himself. Probably the right call. I was pretty green and even though I thought “I’m going to teach these computer guys what interactivity is all about” I quickly realized I knew nothing! I had to forget a lot of what I had learned in the tabletop world and learn to exercise some new muscles. Space Rogue was part of my design fitness program. It’s worth noting here that Space Rogue (1989) is an under-appreciated game that ought to be better known than it is. It consisted of first-person space combat, mashed up with an RPG, mashed up with a really fun arcade game. That idea of genre mashup appealed to me. A lot. And I've used that as the foundation of many of my later games. I owe that to Paul.</p><p>But at the time, I had more success working with Richard. He and I spent weeks crafting the story, the characters, the quests, and the puzzles for what became Ultima VI (1990). I learned his techniques and his philosophy of what games could and should be. Quite the education. And quite the game. Still one of my favorite Ultimas. Heck, one of my favorite games, even if I did work on it.</p><p>I worked on a bunch of other things at Origin, notably Chris Roberts' magnum opus, Wing Commander (1990). That project was a result of a unique vision, a dedicated team and Chris' unwillingness to compromise. I mean, I figured if I had ten arguments with Chris in a day and won three of them, that was a very good day indeed. (I could talk about Wing Commander all day, but that'd fill a book and I'm not writing a book here!)</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf9e0344eae7c21ba/6511c93255583f1ac56cfba3/Young_Warren_Spector.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf9e0344eae7c21ba/6511c93255583f1ac56cfba3/Young_Warren_Spector.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf9e0344eae7c21ba/6511c93255583f1ac56cfba3/Young_Warren_Spector.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf9e0344eae7c21ba/6511c93255583f1ac56cfba3/Young_Warren_Spector.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="8f3yp5bg66zp" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf9e0344eae7c21ba/6511c93255583f1ac56cfba3/Young_Warren_Spector.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltf9e0344eae7c21ba">
        </picture>
        <figcaption>Testing Wing Commander at home. Work was pretty all-consuming back then!</figcaption></figure><p>I worked on other games as well – Martian Dreams, Serpent Isle, Wings of Glory… a bunch of stuff. . I’m especially proud of Martian Dreams, an Ultima-engine game even if no one remembers it even came out.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd8a1576f8152971f/6511c9aec603d30d4775bca3/Savage_Empire.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd8a1576f8152971f/6511c9aec603d30d4775bca3/Savage_Empire.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd8a1576f8152971f/6511c9aec603d30d4775bca3/Savage_Empire.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd8a1576f8152971f/6511c9aec603d30d4775bca3/Savage_Empire.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="hhjm786um3bi" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltd8a1576f8152971f/6511c9aec603d30d4775bca3/Savage_Empire.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltd8a1576f8152971f">
        </picture>
        <figcaption>In Savage Empire I was the evil Dr. Spector. In Martian Dreams I was the no-longer-evil-Dr. Spector, the Avatar’s friend. I swear I didn’t ask to be a character in the games – the teams insisted on it.</figcaption></figure><p><br>One game I worked on was particularly special: Ultima Underworld, made by Blue Sky Productions, founded by Paul Neurath who once again entered my life and pushed me further on the path I'm still on today.</p><p>My first encounter with Ultima Underworld came even before Underworld was Underworld. Paul Neurath showed up with a real-time, fully-textured, first-person tech demo - just a demo! I can't speak for anyone else, but all I could think as I watched it was "The world just changed." I saw the opportunity to let players see a world through their own eyes, to be immersed in it, to believe - at least a little - that it was <em>them </em>in the world, not some squidgy puppet they moved around the screen. I went to my boss immediately and begged him to let me work on the game Paul was proposing, but he gave it to another guy! Luckily for me, that other guy left the company a while later. I begged again and this time, I got it. I got to work on Ultima Underworld (1992).</p><figure><grammarly-extension data-grammarly-shadow-root="true"></grammarly-extension><grammarly-extension data-grammarly-shadow-root="true"></grammarly-extension>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb63e45bf3115ead6/6511f644d81f5986e112869e/Underworld_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb63e45bf3115ead6/6511f644d81f5986e112869e/Underworld_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb63e45bf3115ead6/6511f644d81f5986e112869e/Underworld_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb63e45bf3115ead6/6511f644d81f5986e112869e/Underworld_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="hsl55a8lzser" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb63e45bf3115ead6/6511f644d81f5986e112869e/Underworld_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltb63e45bf3115ead6">
        </picture>
        <figcaption spellcheck="false">Denis Loubet’s Underworld box art remains one my favorites to this day. It captures perfectly the feeling of exploration and danger embodied by the game.</figcaption></figure><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte1d409294df86968/6511f68187e59c399ecca720/Ultima_Underworld_copy.png?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte1d409294df86968/6511f68187e59c399ecca720/Ultima_Underworld_copy.png?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte1d409294df86968/6511f68187e59c399ecca720/Ultima_Underworld_copy.png?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte1d409294df86968/6511f68187e59c399ecca720/Ultima_Underworld_copy.png?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="tvfs3e2w93j0" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte1d409294df86968/6511f68187e59c399ecca720/Ultima_Underworld_copy.png?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blte1d409294df86968">
        </picture>
        <figcaption>The first ad we did for Ultima Underworld was strange – we had to explain what a first-person, real-time game was. No one back then knew. I’m not sure the ad did its job, but it was certainly... strange.</figcaption></figure><p>Working with Paul was great, of course, but the real revelation was meeting Doug Church. To this day, Doug is one of the smartest, most talented, most creative programmer/designers I've ever met. A true Secret Master of Gaming if there ever was one. He led the charge on the project in a way he's never credited for having done. And what a team. It was a bunch of MIT-types - even the non-MIT grads and non-grads were MIT-types in my mind! I don't believe there was anyone on the project who had ever worked on a game before. They had no idea what was and wasn't possible, which was key to their success. And, man, were they smart! A bunch of them lived together in a house they called "Deco Morono" - House of Ten Dumb Guys. Right... The first time I set foot in the place, I realized I was the dumbest guy in the room. It was awesome. The game, when it was released, was unlike anything anyone had ever seen. It attempted to empower players in ways that were unexpected and powerful, laying the groundwork for Immersive Sims to come.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt75e6f99d8f9e9326/6511f7d255583f51986cfbc1/Shipping_Ultima_Underworld_copy.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt75e6f99d8f9e9326/6511f7d255583f51986cfbc1/Shipping_Ultima_Underworld_copy.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt75e6f99d8f9e9326/6511f7d255583f51986cfbc1/Shipping_Ultima_Underworld_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt75e6f99d8f9e9326/6511f7d255583f51986cfbc1/Shipping_Ultima_Underworld_copy.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="krj29tz2ogxt" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt75e6f99d8f9e9326/6511f7d255583f51986cfbc1/Shipping_Ultima_Underworld_copy.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt75e6f99d8f9e9326">
        </picture>
        <figcaption>The day we shipped Ultima Underworld. That’s Doug Church on the left, Paul Neurath in the middle, and me on the right. (We were launching a toy soldier into the air to celebrate. Don’t ask me why.)</figcaption></figure><p>Speaking of Sims-to-come, after Underworld and Underworld 2, it was on to System Shock (1994). I'm sure everyone has their own story of how that game came to be and my memory may be faulty, but here's what I remember... I was at Origin, bored to death of making games about heroes in chainmail or plate, looking like the Mighty Thor, saving princesses and slaying evil mages. I'd worked with Chris Roberts on the original Wing Commander game and decided I could take the ideas behind Underworld and make a science fiction game, set in the Wing Commander universe. I called it Alien Commander. I wrote up a concept doc and got ready to pitch it. What I didn't know was that at Looking Glass Technologies, Doug Church (there he is again...) was working on an SF game of his own. Knowing Paul, I'm sure he was involved, too, but it was Doug I was talking to. We compared notes and all I could think was, "Um... Yeah... That's better than Alien Commander." Paul was able to assemble yet another amazing team to work on the game. He has a knack for building teams...</p><p>The game was cyberpunk to the max, with an engine more powerful than Underworld, with a ton of physics going on. And the plot was, well, good. That was in large part thanks to the participation of Austin Grossman, with whom I've had the pleasure of working many times since. (If you want to know something of what it was like at LG, check out his novel, YOU.)</p><figure><grammarly-extension data-grammarly-shadow-root="true"></grammarly-extension><grammarly-extension data-grammarly-shadow-root="true"></grammarly-extension>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte67ea7546d2756c7/6511cc1c23d5752444e92290/System_Shock_Cover.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte67ea7546d2756c7/6511cc1c23d5752444e92290/System_Shock_Cover.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte67ea7546d2756c7/6511cc1c23d5752444e92290/System_Shock_Cover.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte67ea7546d2756c7/6511cc1c23d5752444e92290/System_Shock_Cover.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="ypqu5rcaidyg" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte67ea7546d2756c7/6511cc1c23d5752444e92290/System_Shock_Cover.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blte67ea7546d2756c7">
        </picture>
        <figcaption spellcheck="false">Even the System Shock box spoke of disasters awaiting us in a horrific future of man/machine hybrids and machines run amok.</figcaption></figure><p>The System Shock narrative was communicated in a new way - through videologs and messages from dead folks - everyone was dead on Citadel Station, largely because all existing conversation systems sucked (which they still do). But there was more to the story, as it were. System Shock communicated story through elements in the level - signage, dead body placement, messages scrawled (usually in blood) on the walls. I hesitate to say it, but Shock may have introduced environmental storytelling to gaming.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt33460bff4afe3d75/6511ccce5ea4736398597047/SHODAN.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt33460bff4afe3d75/6511ccce5ea4736398597047/SHODAN.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt33460bff4afe3d75/6511ccce5ea4736398597047/SHODAN.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt33460bff4afe3d75/6511ccce5ea4736398597047/SHODAN.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="utndgy40xjlo" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt33460bff4afe3d75/6511ccce5ea4736398597047/SHODAN.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt33460bff4afe3d75">
        </picture>
        <figcaption>SHODAN, mad AI. Not that anything like that would ever happen...</figcaption></figure><p>In addition, the game featured a mad computer, SHODAN, who's still considered one of the great game villains of all time. The player character had no name - they were just Hacker - so players could create their character based on their own choices and imagination. And D&amp;D-like character stats didn’t matter, either. Shock was all about you, in the world, alone and underpowered to deal with an enemy that was smart and knew exactly what you were up to (or seemed to…) Players had some choices to make about how to play – not just what weapon to use to headshot that zombie. But there was more to come on that score.</p><p>I left Origin and officially joined Looking Glass in 1996. Paul Neurath offered me the opportunity to build my own studio in Austin, Texas. That was too good an opportunity to pass up, so after due consideration, I made the leap.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt408d86234d1898df/6511cd42d81f595a6212867b/Warren_Spector_Red_Shirt.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt408d86234d1898df/6511cd42d81f595a6212867b/Warren_Spector_Red_Shirt.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt408d86234d1898df/6511cd42d81f595a6212867b/Warren_Spector_Red_Shirt.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt408d86234d1898df/6511cd42d81f595a6212867b/Warren_Spector_Red_Shirt.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="8quwsv6fkn8y" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt408d86234d1898df/6511cd42d81f595a6212867b/Warren_Spector_Red_Shirt.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt408d86234d1898df">
        </picture>
        <figcaption>Leaving Origin wasn’t easy, but the opportunity to build a studio from scratch made the difference.</figcaption></figure><p>And then there was Thief (1998). A lot of people give me credit for having “made” that game. Leaving aside that no one person can be said to have made or created any game, let me clarify here and now that I worked on Thief for the middle year of a basically three-year development cycle. I’m proud to have been a part of it and hope I contributed something, but let’s cut out the “created by” stuff. Doug and I guy named Greg Lopiccolo&nbsp;led the Thief team to glory, not me.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt444e792a5db84b52/6511cdbdfac8e1c91ff222a4/Thief.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt444e792a5db84b52/6511cdbdfac8e1c91ff222a4/Thief.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt444e792a5db84b52/6511cdbdfac8e1c91ff222a4/Thief.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt444e792a5db84b52/6511cdbdfac8e1c91ff222a4/Thief.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="m5d24njkns7q" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt444e792a5db84b52/6511cdbdfac8e1c91ff222a4/Thief.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt444e792a5db84b52">
        </picture>
        <figcaption>NOT MY GAME! I WORKED ON IT FOR A WHILE. I’M PROUD OF MY CONTRIBUTION. BUT I DIDN’T “CREATE” IT!</figcaption></figure><p>Okay, that out of the way, there was one critical thing that came out of my involvement in Thief. There was a point where I just found an encounter too tough to sneak past, so I went to some folks in the team and said, “Just let me fight my way past things that are too hard for me.” They looked at me like I was crazy and said, “If we make the player powerful enough to fight, no one will ever sneak.” Of course, they were right for Thief. It’s a jewel of a game, tightly focused and beautifully designed. There’s a reason why it’s still cited as one of the best – maybe <em>the</em> best – stealth games ever made. At the time, though, what went through my head was, “I’ll show them. I’ll make a game where you can decide for yourself whether to fight or sneak past any problem.</p><p>Well, there came a point in 1997 where Boston-based Looking Glass was running short of funds and my Austin, Texas studio was the obvious place to make cuts. I remember telling Paul Neurath to shut us down. I mean, shutting down the mothership didn’t make sense. “I’ll find another deal. We’ll be okay.” That’s what I said. Arrogant? I guess. But I was pretty confident. I had a strong team and knew what game I was going to pitch. That was the point when I dug up a proposal I’d written up in 1995 – a pitch for a game called “Troubleshooter.”</p><h2>Ion Storm</h2><p>Troubleshooter was an attempt to make what I called “the real-world roleplaying game.” The main character was Jake Shooter, ex-CIA operative who the agency came to when they had a case too tough – or too shady – to handle themselves. The gameplay was, well, play Deus Ex and you’ll see. Problem was the real-world part of the plan. See, people know how the real world works – telephones and televisions and cars and everything else are well-understood and people have expectations of how they’ll behave. At the time – even today, I’d argue – we can’t live up to people’s expectations and the last thing you want to do in a game is thwart players expectations. The answer was to move the game into the near future – recognizable but far enough out that players couldn’t say, “That’s not the way a computer works!”</p><div><p>Then there was another consideration. Cool as a noir-like private investigator/spy could have been, it would have been about as silly as another space marine or knight in shining armor. And the kinds of situations the player would find themselves in might be thrilling, but a million other games had the spy/military thing covered. I knew we had to do something different. Cutting to the chase, that “different” looked like it was going to be an RPG variant of Command &amp; Conquer. Talk about cool! I would have done that in a heartbeat if John Romero hadn’t called me and offered me the opportunity of a life-time. <em>Another</em> lucky break! My life’s been full of them.</p><p>John drove down to Austin from Dallas and said “Make the game of your dreams. Biggest budget you’ve ever had. Biggest marketing budget you’ve ever had. And no creative interference.” Who the hell says no to that?! I sure as hell wasn’t about to. So we joined Ion Storm, with Eidos as our publisher. Both were great. Both lived up to every promise John made. I owe John a huge debt. I’ll never be able to repay it, but I hope our friendship and mutual respect is enough.</p></div><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt020ff2b146ea4371/6511ce3bd21fb05e23fdfce1/Ion_Storm_1999.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt020ff2b146ea4371/6511ce3bd21fb05e23fdfce1/Ion_Storm_1999.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt020ff2b146ea4371/6511ce3bd21fb05e23fdfce1/Ion_Storm_1999.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt020ff2b146ea4371/6511ce3bd21fb05e23fdfce1/Ion_Storm_1999.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="pc969xbvg1bh" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt020ff2b146ea4371/6511ce3bd21fb05e23fdfce1/Ion_Storm_1999.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt020ff2b146ea4371">
        </picture>
        <figcaption>What a bunch of reprobates! Tom Hall, John Romero, and me. Ion Storm in 1999.</figcaption></figure><p>But back to the story. We were set, deal-wise, but the remnants of my old Looking Glass team and I still knew our world-building needed work. One of the things I’m committed to is finding things people already care about rather than trying to <em>make</em> them care about something. So the team and I started looking around at what normal humans – not just gamers – were thinking and talking about. What we found was that Y2K was on people’s minds. The rise of AI. Terrorism. Human augmentation. Bioengineering. Unequal distribution of wealth. And there were so many conspiracy theories flying around in the cultural ether you could swat ‘em with a baseball bat with no fear of striking out. Those things would be the heart of our world. (Heck, you could make a game about all that today and it would <em>still</em> work. Which is kind of pathetic…) And, not to get too far ahead of myself, one of our rules was that nothing went into the game unless you could point to the real-world reference for it. We even got blueprints, maps and, of course, photographs of places and tools and so on.</p><p>Anyway, we had the foundation of a world and a clue about what the game was about. I knew I wanted to show those Thief guys that you could make a game that gave players choices about how to solve problems. The game had to be about how clever and creative players were, not how clever and creative we, as developers, were. I needed the right team, folks who bought into the world and the gameplay concept. I found them. I could talk about every person on the team but I’m going to risk alienating people and just mention three – Harvey Smith, the game’s Lead Designer, Chris Norden, the Lead Programmer (and later my Assistant Director) and Sheldon Pacotti, our Lead Narrative Designer. Talk about people who get too little credit!</p><p>Chris was a terrific coder who totally bought into the plan. He was what I call “the guy who told me ‘no.’” I’m kind of a kitchen sink guy and often need to be told to back off. More than anyone else, Chris was never shy about doing that. That said, he had an interesting “tell” when we talked that said “yes.” If I asked for something and Chris said “no,” a lot of the time I knew I’d see it working in a couple of weeks. Heck, sometimes the next day! To say he could make things happen would be an understatement.</p><p>And Harvey. What can I say about the guy? I knew he was special even when he was a tester on System Shock. I remember some great design conversations we had late in the night at Origin! He got the Imm Sim thing even then. I knew I needed him on Deus Ex, took him out to dinner, plied him with guacamole and told him what I wanted to do. Happily, he signed on. There’s a lot more I could say, but I’ll just leave it at this – Harvey was a natural-born design team leader and a critical leader overall whose contributions to Deus Ex were huge and under-appreciated.</p><p>Finally, Sheldon. I wanted him on the team from the minute I read the short stories he submitted when he applied for the narrative job. Terrific writer and super smart. People often talk about how “intelligent” a game Deus Ex is. Lots of people contributed to that – as a team we were committed to it – but Sheldon was The Man when it came to ensuring that our plot, NPCs, dialogue and in-game texts were smarter than a lot of other games. (As a note, I’m really proud of the “smartness” of DX. The name refers to Deus Ex Machina which in literary terms describes a bad narrative approach – look it up. It was also called that because the game was at least in part about the potential and peril of AI, of machines that become sentient – Deus Ex Machina... God from the Machine... get it? Plus, I thought it would be funny if people mispronounced it and had to say “sex.” That last one proves that I’m kind of a nitwit at times! And I will never give a game a name that players can’t pronounce. I’m sure embarrassment cost us some sales!</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2bf4d5bc2e4d0962/6511ceb2e821fe6b8e13800b/Deus_Ex_Team.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2bf4d5bc2e4d0962/6511ceb2e821fe6b8e13800b/Deus_Ex_Team.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2bf4d5bc2e4d0962/6511ceb2e821fe6b8e13800b/Deus_Ex_Team.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2bf4d5bc2e4d0962/6511ceb2e821fe6b8e13800b/Deus_Ex_Team.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="qo9ddsz75wmh" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt2bf4d5bc2e4d0962/6511ceb2e821fe6b8e13800b/Deus_Ex_Team.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt2bf4d5bc2e4d0962">
        </picture>
        <figcaption>The incredible Deus Ex team. One of the best. And, yes, it was that small. That’s Chris Norden on the far left, Harvey next to him and Sheldon on the far right in the second row. Wish I could name them all, but this is long enough already!</figcaption></figure><p>Anyway, there’s a lot more to say about Deus Ex, but I’ll leave it at this: When we were getting ready to ship, I put my head down on my desk and thought, “If people compare our combat to Half-Life, we’re dead; if they compare us to Thief’s stealth, we’re dead; if they compare our RPG elements to Bioware’s latest, we’re dead. But if they get that they can <em>decide</em> how to play, to do <em>any</em> of those they want, we might rule the world.” I’ll leave it to others to decide which of those describes the finished product, but I’m pretty proud of the result.</p><p>Deus Ex was released in 2000. People liked it pretty well. As important, we on the team liked it. It was definitely a high point in my career and, I hope, in the careers of the team that made it possible and, well, made it!</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbcb2b25db752a4ae/6511cf1ac8c41b3f68950379/Deus_Ex_Launch_Day.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbcb2b25db752a4ae/6511cf1ac8c41b3f68950379/Deus_Ex_Launch_Day.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbcb2b25db752a4ae/6511cf1ac8c41b3f68950379/Deus_Ex_Launch_Day.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbcb2b25db752a4ae/6511cf1ac8c41b3f68950379/Deus_Ex_Launch_Day.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="0vwybxf9ji3f" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbcb2b25db752a4ae/6511cf1ac8c41b3f68950379/Deus_Ex_Launch_Day.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltbcb2b25db752a4ae">
        </picture>
        <figcaption>Deus Ex launch day. Yeah we were in a thing called a “software store.” Some of you may remember those...</figcaption></figure><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb2f61903e06d545a/6511d0381d8b4f7fb5a511c8/Harvey_Smith.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb2f61903e06d545a/6511d0381d8b4f7fb5a511c8/Harvey_Smith.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb2f61903e06d545a/6511d0381d8b4f7fb5a511c8/Harvey_Smith.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb2f61903e06d545a/6511d0381d8b4f7fb5a511c8/Harvey_Smith.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="l179o0m0c82j" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltb2f61903e06d545a/6511d0381d8b4f7fb5a511c8/Harvey_Smith.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltb2f61903e06d545a">
        </picture>
        <figcaption>Harvey at the mic, me playing Superman behind him when Deus Ex won some awards at the Game Developers Conference.</figcaption></figure><p>Oh, one last thing. At the time, it was science fiction and people took it as such. I’ve said this before, but I’m not sure I’d make a game like Deus Ex today. Too many people would see it as a documentary. Did I say “pathetic” before. Yep. I did. Now, the idea that games should empower players and give them agency to tell their own stories? That I will always do. But the world and the narrative? Nope. Too dangerous in a world where the things we said about the world in Deus Ex have become our everyday reality. No. I’m not going there. Plus I have other interests and I’m going to indulge them as long as people will continue to fund me.</p><p>After Deus Ex, Ion Storm Austin did some sequels – Deus Ex: Invisible War (2003) and Thief: Deadly Shadows (2004). There are plenty of stories to tell about those, but the critical thing for me is that I’m not much of a sequel guy and so in 2005 I decided to leave the studio and do a start-up: Junction Point.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte6d7be9942d61750/6511d0b6eff67faa4db49a70/Warren_Spector_Awards_Case.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte6d7be9942d61750/6511d0b6eff67faa4db49a70/Warren_Spector_Awards_Case.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte6d7be9942d61750/6511d0b6eff67faa4db49a70/Warren_Spector_Awards_Case.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte6d7be9942d61750/6511d0b6eff67faa4db49a70/Warren_Spector_Awards_Case.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="2s98beysj1di" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blte6d7be9942d61750/6511d0b6eff67faa4db49a70/Warren_Spector_Awards_Case.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blte6d7be9942d61750">
        </picture>
        <figcaption>One of the awards cases from Origin, Looking Glass and Ion Storm days. And yeah, I’m bragging. Wanna make something of it?</figcaption></figure><h2>Junction Point</h2><p>Junction Point got started in 2004 by a small group of Ion Storm folks, who like me, were ready for a new challenge.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf120709c7665b6e0/6511d13b55583f4b146cfbad/Junction_Point.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf120709c7665b6e0/6511d13b55583f4b146cfbad/Junction_Point.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf120709c7665b6e0/6511d13b55583f4b146cfbad/Junction_Point.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf120709c7665b6e0/6511d13b55583f4b146cfbad/Junction_Point.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="ogvvnahcvrmf" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltf120709c7665b6e0/6511d13b55583f4b146cfbad/Junction_Point.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltf120709c7665b6e0">
        </picture>
        <figcaption>Junction Point, day one. We started out small, but we had big dreams</figcaption></figure><p>Most important was Art Min, my partner. I'm not sure I would have had the nerve to do a start-up myself - I'm pretty sure I would have failed - without him. We had worked together before at a couple of studios on several games and I knew he was a terrific coder and a natural-born leader.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltdcd163ba08028017/6511d2210c0dc861800c343a/Art_Min_and_Warren_Spector.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltdcd163ba08028017/6511d2210c0dc861800c343a/Art_Min_and_Warren_Spector.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltdcd163ba08028017/6511d2210c0dc861800c343a/Art_Min_and_Warren_Spector.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltdcd163ba08028017/6511d2210c0dc861800c343a/Art_Min_and_Warren_Spector.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="7n6r01x0oezo" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltdcd163ba08028017/6511d2210c0dc861800c343a/Art_Min_and_Warren_Spector.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltdcd163ba08028017">
        </picture>
        <figcaption>Art Min and me. I wouldn’t – probably couldn’t - have made Junction Point a reality without my partner in crime.</figcaption></figure><p>There were a dozen of us and we met at a local Mexican restaurant to plan our new endeavor. But there was one more piece of the puzzle - Seamus Blackley ("father of the Xbox"). He had been a developer of note but at the time we started Junction Point he had left development to be an agent at Creative Artists Agency and I signed on with him to represent me.</p><p>Seamus and the other game agents at CAA had a plan to remake the business model in gaming to be more advantageous to developers and it sounded to me like exactly what should happen. I won't go into the details here - that's not my story to tell - but I'll just say it really would have changed things.</p><p>Back home, the team and I started in on some new IP and came up with three that showed promise. One was an epic fantasy RPG/Immersive Sim called Sleeping Giants, based on a concept my wife, Caroline Spector, and I had come up with for DC Comics. (They passed...). One was a multiplayer action Imm Sim called Necessary Evil, a multiplayer game about augmented soldiers and independent operatives who could collaborate with or oppose each other, serving the military, businesses or mercenary groups. It was all about what happened when augmented military soldiers mustered out. I still think that’s a cool background for a game. Someone ought to make it so I can play it. The third concept stemmed from Seamus’s fertile imagination. He hooked me up with director, John Woo and together he and I developed an IP called Ninja Gold, designed to be a transmedia thing - movie and game simultaneously, with us collaborating on its creation. That was a trip. I loved working with John and learned a lot about the similarities between movies and games but, more important, some critical differences. That's a blog for another time.</p><p>The team got working on Sleeping Giants and Ninja Gold and did some great concepting and prototyping. Maybe I'll post some of the work they did some time. I have some cool docs and cooler videos... Anyway, we had a movie deal for Ninja Gold but no game deal at that time. The movie deal went away when the studio decided on a change of strategic direction. (A term that would come to haunt me later.) We got a game deal for Sleeping Giants. It went away - strategic direction again. We got another deal. (Seamus was good at his job!) That deal went away. I think we got another deal, but I can't really remember. Whether we got a third deal or not, we ended up with no deal. And no way to keep Junction Point alive. Until Valve called.</p><p>They offered us a lifeline in the form of a Half-Life 2 episode. Needless to say, I said yes, and the team got to work on yet another project. There's a whole story about that, but this is getting long enough that I'll just say it was set in Ravenholm and the team came up with a thing called the Magnet Gun. (You can imagine what it did.) The MG was unlike anything in the Half-Life arsenal and I thought it was incredibly cool - powerful, open to creative uses, and at times even funny. It took us a long time to master the Half-Life tech and just as we started building what I thought were very cool levels, Valve pulled the plug on us. The timing on that was pretty frustrating.</p><p>So in 2006, or thereabouts (my memory’s hazy), I was on the road with Seamus again, pitching three adult, action-oriented, original IP, desperate to keep the studio alive and the staff employed. (Indie studios don't have an easy row to hoe!) I pitched a bunch of publishers and potential funding partners, but didn't have much success. Then one day Seamus said, "Let's pitch Disney."</p><p>I thought he was crazy. "They're not going to be interested in any of these concepts," I said. To which he replied, "They've changed. Let's do it."</p><p>Oooo...kaaaay....</p><p>The day came for the Disney pitch. As I talked about the three concepts, the dozen or so people around the conference room table stopped paying attention and started checking their phones and all that. I was going to kill Seamus when I got out of there. But then the strangest thing happened.</p><p>Graham Hopper, who headed up Disney's game division at the time, asked me if I'd be interested in making any licensed games. Here's where you have to understand that I've been a huge Disney fan for... well... forever. I said, "Yeah. Give me The Night Stalker or Ducktales and I'm in." (I had an idea for a monster-of-the-week game that would have been perfect for ABC’s Night Stalker series and I love Carl Barks and Scrooge McDuck.) Graham looked at me and said, "What about Mickey Mouse?"</p><p>It took me three seconds to think, "Is he kidding? The most recognizable icon on player Earth?" I said, "Yes!" I have to admit that, with Mickey as my star, I could reach a mainstream audience with the Imm Sim game approach, but I probably would have done anything to work with Mickey.</p><p>Graham said, "We have a Mickey concept I'd like to show you. You don't have to use any of it. Just tell us what you think." So Luigi Priore, who's been with Disney forever, pulled up a PowerPoint and pitched a concept dreamed up by some interns. Some very, very creative interns! It had a lot of elements that made it all the way into the shipping Epic Mickey game. Graham reiterated that I didn't have to use any of their pitch but I had a quick reply. "Are you kidding? Why wouldn't I use some of those ideas? You have some pretty creative folks at Disney and that concept is really strong. You've given me an acorn and I want to grow it into an oak tree! I'm in!"</p><h2>The Interregnum</h2><p>Seamus negotiated a concept development deal for Epic Mickey. I spent six months in 2006 working with my old TOON design collaborator, Allen Varney, and Alex Duran, a talented programmer, to flesh out the idea. Together we came up with a pitch. (and by “pitch,” I mean a 240-page document and a PowerPoint deck). I went to Disney and they ushered me into a conference room with two doors. I went in one, gave the pitch, and eventually was ushered out the other to discuss things further. I was later told that if I'd been ushered out through the door where I'd entered, that would have meant the pitch was unsuccessful. Luckily, I went out through door number 2 and discussions continued.</p><p>After going out the good door, Graham said they wanted to move ahead. Sadly, over sushi, a biz dev guy told me shortly after that the only way I would get to make Epic Mickey was if Disney acquired Junction Point. Stupidly, I guess, I said no. I wasn't ready to sell (and if I’m being honest, the deal just wasn’t that good). Cut to a year later, 2007, and Disney came back and said they still wanted Junction Point to make the game but they still had to acquire the studio. I thought to myself, "How many times is Disney going to come back to me?" I said yes, Seamus negotiated the acquisition, and the deal was done. Almost.</p><p>The plan was to announce the deal at E3. The date was set. The time. The room booked. On the day, the press was filing into that room. Unfortunately, I hadn't yet signed the deal! There were a couple of points I wasn't happy with. So there I was outside with Graham who was waving the papers at me waiting for me to sign. I had Seamus on the phone talking me off a cliff. Finally, he just said "Do you want to work with these guys and make this game?” “Yes,” I said. “Then sign the deal" he said. I signed, the announcement went off just a little late, and I officially worked for Disney.</p><p>When I went back to the studio, two things happened: First, I called my mother to tell her the news. Her response tickles me to this day. What she said was "It's about time" - not "Congratulations" or "Are you crazy?" Just "It's about time." I told you I was a Disney fan!</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt632ca178a3a426e0/6511d2b21c83997060635038/Warren_As_A_Baby_1956.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt632ca178a3a426e0/6511d2b21c83997060635038/Warren_As_A_Baby_1956.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt632ca178a3a426e0/6511d2b21c83997060635038/Warren_As_A_Baby_1956.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt632ca178a3a426e0/6511d2b21c83997060635038/Warren_As_A_Baby_1956.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="zb6kvah00aoz" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt632ca178a3a426e0/6511d2b21c83997060635038/Warren_As_A_Baby_1956.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt632ca178a3a426e0">
        </picture>
        <figcaption>My mother and me – was there any doubt I’d end up a Disney fan?</figcaption></figure><p>The other thing was I went back to the team built to make adult Imm Sims and said "We're making a Mickey Mouse game." A great level builder and my narrative lead left because they simply didn’t want to make that kind of game. Best decision they could have made, for themselves and the project. If your team isn’t onboard and bought-in, you’re doomed. The rest stuck around because they were Disney fans like me or felt like they could become Disney fans by the end of the project. Which everyone did.</p><p>One other thing happened. Paul Weaver, one of the best Creative Producers and studio execs I've ever worked with signed on. We'd worked together at Ion Storm and with him onboard, I felt like Junction Point couldn't fail. I had that much confidence.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt14e38c91cc763d9b/6511d42249219b782a12f263/Paul_Weaver.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt14e38c91cc763d9b/6511d42249219b782a12f263/Paul_Weaver.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt14e38c91cc763d9b/6511d42249219b782a12f263/Paul_Weaver.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt14e38c91cc763d9b/6511d42249219b782a12f263/Paul_Weaver.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="bjki9160wvsu" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt14e38c91cc763d9b/6511d42249219b782a12f263/Paul_Weaver.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt14e38c91cc763d9b">
        </picture>
        <figcaption>Paul Weaver. Production and business powerhouse with substantial creative chops.</figcaption></figure><p>Paul brought along a senior designer named Chase Jones. I didn’t know Chase at the time, but he turned out to be one of the most talented designers I've ever worked with, another one of those natural leaders I’m lucky enough to stumble into. I put him in charge of the Epic Mickey design Allen, Alex and some incredibly creative interns had concepted. I functioned as Creative Director, with ultimate authority. (I always reserve one more vote than everyone else on my teams, combined).</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt7d8a692724357662/6511d46ef7552e73e17ccdc3/Chase_Jones.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt7d8a692724357662/6511d46ef7552e73e17ccdc3/Chase_Jones.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt7d8a692724357662/6511d46ef7552e73e17ccdc3/Chase_Jones.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt7d8a692724357662/6511d46ef7552e73e17ccdc3/Chase_Jones.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="l6z15ohn51ux" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt7d8a692724357662/6511d46ef7552e73e17ccdc3/Chase_Jones.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt7d8a692724357662">
        </picture>
        <figcaption>Chase Jones. Super-talented designer and team leader. Disney Epic Mickey and the sequel, The Power of Two wouldn’t have happened without him.</figcaption></figure><p>At the same time I was keeping an eye on another project I still don't want to talk about. And running the studio. And dealing with Disney. Fun times!</p><p>The Epic Mickey story is a long one so I'll glide over that. Suffice to say it was one of the highlights of my career. It took over 300 people, but we got it done and shipped in 2010. I loved its mashup of platformer, shooter, and adventure. I loved that it was what I called an Imm Sim Lite. (Core gamers didn’t get that, which pains me to this day, but normal humans did, which was pretty darn satisfying.)</p><p>Among the coolest aspects of my Disney experience was that me team and I got to reintroduce Oswald the Lucky Rabbit to the world. (Oswald was Walt Disney's first cartoon star, lost to him in a 1928 contract dispute with his distributor. And, man, is there a story there – about the contract and about how Disney got the rights to Oswald back. A story for another time.) We got to give Oswald a girlfriend - Ortensia - who everybody at Disney and the world now thinks was part of Oswald's world back in the '20s. She wasn't - we named her... brought her to the screen... in a game... and no one realizes it. I laugh every time I think about that!</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt95379c4063888af1/6511d4bcc5e74d769ea57d6f/Oswald_and_Ortensia.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt95379c4063888af1/6511d4bcc5e74d769ea57d6f/Oswald_and_Ortensia.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt95379c4063888af1/6511d4bcc5e74d769ea57d6f/Oswald_and_Ortensia.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt95379c4063888af1/6511d4bcc5e74d769ea57d6f/Oswald_and_Ortensia.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="9cqdzlqohc2w" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt95379c4063888af1/6511d4bcc5e74d769ea57d6f/Oswald_and_Ortensia.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt95379c4063888af1">
        </picture>
        <figcaption>Oswald the Lucky Rabbit and Ortensia, his girlfriend.</figcaption></figure><p>The first game did well when it was released in 2010, so we did a sequel, with Chase very much in charge, supported by Paul and with some help from me. It took almost 800 people to make Disney Epic Mickey: The Power of Two in 2012, but we introduced a lot of new things, most notably adding multiplayer and Broadway musical-type songs. I've always wanted to make a musical game and this was a baby step in that direction. Someday I'll make a full-on, interactive, Imm Sim musical game that is far more than just beat-matching. I have ideas about how to do it and I will… Soon as someone's crazy enough to fund it.</p><p>After that, we started working on an Epic Donald game with the Oliver Twins over in the UK. They and their team did a great job. Concepting was going well. Concept art was spectacular. A prototype showed promise. My first clue that things were going south at Disney was when I pitched the game for continued concepting and the project didn't get greenlit. Sixteen people in a room voted no when they were told by a Marketing guy that data showed that Donald didn't test well. I was seething mad about that conclusion! I didn’t and don’t trust data and research much. Specifically, I felt like they hadn’t tested in Europe and Scandinavia, where the ducks are incredibly popular. And even if it were true that the ducks weren’t popular in the States, didn’t that make it our job to change that and put Donald in a position to test better?</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt0da5e4e47b4472b1/6511d52a3da3ec55db4396c1/Ducktales.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt0da5e4e47b4472b1/6511d52a3da3ec55db4396c1/Ducktales.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt0da5e4e47b4472b1/6511d52a3da3ec55db4396c1/Ducktales.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt0da5e4e47b4472b1/6511d52a3da3ec55db4396c1/Ducktales.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="56jxxvax3eev" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt0da5e4e47b4472b1/6511d52a3da3ec55db4396c1/Ducktales.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt0da5e4e47b4472b1">
        </picture>
        <figcaption>I got to write a series of Ducktales comics despite the demise of Epic Donald! For a Carl Barks fan that was both intimidating and incredible!</figcaption></figure><p>But the non-greenlighting of Epic Donald sent us back to the creative and business drawing board.</p><p>Paul and I – mostly Paul - worked up dozens of sku plans and budgets, but none of them got approved by the powers that be. (It’s worth pointing out that “the powers that be” no longer included Graham Hopper. He was a guy who actually cared about games not just about business, but he’d been replaced. So kudos to him for having some vision. Whatever the opposite is of kudos go to the folks who replaced him. That’s all I’ll say about that!)</p><p>The rest of the Disney-Junction Point story is all business nonsense I'll maybe talk about some other time. The important thing is that Disney shut down Junction Point in 2013. Two hundred people lost their jobs. And after seven years, I was no longer a cast member and, for the first time ever, I was out of work.</p><h2>The Denius-Sams Gaming Academy</h2><p>I'm embarrassed to admit I spent several months after the shutdown of Junction Point sitting on a couch with a remote control in my hand watching YouTube videos. My wife finally told me I needed to get up and find something to do. I got lucky <em>again</em>. A call came in late 2013 from the head of the College of Communication at the University of Texas at Austin asking if I wanted to create a game development program. I’d taught a Master Class in Game Development in 2007, I think it was. (Videos of all the sessions are still on You Tube and I think some of the content is still relevant today.) Anyway, he told me the College had funding for three years and wanted to set up something innovative – a certificate program that would be free for 25 students a year. Yeah, it was a great deal for students and sounded like a fun change of pace to me, so I agreed to a three-year run as leader of whatever program I could sell the University on.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt527404b81bd4e2cc/6511d67d62943278f1ac48b2/WS.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt527404b81bd4e2cc/6511d67d62943278f1ac48b2/WS.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt527404b81bd4e2cc/6511d67d62943278f1ac48b2/WS.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt527404b81bd4e2cc/6511d67d62943278f1ac48b2/WS.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="12d9zpgc754s" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt527404b81bd4e2cc/6511d67d62943278f1ac48b2/WS.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt527404b81bd4e2cc">
        </picture>
        <figcaption>Me in front of the Texas Tower on the University of Texas at Austin campus.</figcaption></figure><p>First thing I did was research what all the other university and college game development programs were doing. There were, according to the ESA, 400 institutions of higher learning offering game dev courses or programs. I didn’t see any point duplicating them. What I found was that all of them I could find and research were teaching actual development – the making of games – and often in the context of small, indie-style teams working on very small projects. And I found that many, maybe most, student projects never reached completion. Finishing things is one of the hallmarks of professionalism. I’ll blog about that someday. (Before any educators start berating me, that wasn’t universally true, but definitely the predominant strain of games education.)</p><p>I knew I wanted to do something different, so I decided to focus on creative and business <em>leadership</em> as it related to relatively large teams. I wanted my students working to complete two projects during the year – a simple one in the first semester and a more complex one in the second. Each student would have a specialty – just like on a real, studio dev team. There would be leadership lectures in the morning and development work in the afternoon and evening.</p><p>My faculty would consist of two folks who had real industry experience and had worked on a mainstream PC or console games recently enough that their experience was still relevant. Too many game dev programs are taught by people who’ve been out of the professional world for too long or have never worked on a game at all. Given how quickly things change in the world of gaming (see below!) either was, let’s just say suboptimal... I found my staff in two excellent developer/teachers – Joshua Howard and D.S. Cohen.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt8cbbffce3ff6cce1/6511d7258a0039fbf6bd1618/WS_2.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt8cbbffce3ff6cce1/6511d7258a0039fbf6bd1618/WS_2.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt8cbbffce3ff6cce1/6511d7258a0039fbf6bd1618/WS_2.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt8cbbffce3ff6cce1/6511d7258a0039fbf6bd1618/WS_2.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="ss1i8tethl58" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/blt8cbbffce3ff6cce1/6511d7258a0039fbf6bd1618/WS_2.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="blt8cbbffce3ff6cce1">
        </picture>
        <figcaption>Joshua Howard, me, and D.S. Cohen, instructors at the Denius-Sams Gaming Academy. Here we are preparing to instruct.</figcaption></figure><p>We interviewed dozens and dozens of student candidates, as rigorously as if we were hiring a team of our own. We quizzed them on their development skills but also on their leadership potential. We wanted the best of the best, and I think we found them. The success our graduates have achieved speaks to the quality and qualities of our ex-students as much as – more than – our training of them. I can’t tell you how proud I am of them. I’m as interested in making people as I am in making games at this point in my career (though I have no plans to stop doing either!) and working with the Academy students was a great way to scratch that itch.</p><p>In any event, I haven’t even gotten to the innovative thing the program offered – the real, hands-on opportunity for each student to take on the creative and business leadership opportunities on the projects. Every two weeks, we created a development duo, one playing what might be called the Creative Director role and the other the Producer role. The staff met with these duos daily to talk about what they were doing well and where they needed to do more work. Everyone got to put into practice the leadership lessons learned in the morning lectures.</p><div><p>You’d have to talk to the students to determine how well the program met their needs and prepared them for the real world, but all four semesters resulted in finished games of high quality. And almost all of the students got real-world jobs and, as I said, went on in most cases to do some pretty impressive things.</p><p>I was proud of what we were doing, but mid-way through my three years building and then running the program, I started to feel the itch to get back into development myself. There were still things I wanted to make. As my commitment neared its end, I started thinking about what might come next. I could stay on and be a teacher for the rest of my life. That would have been cool. But I started entertaining the idea of doing a new start-up. I had ideas for games to develop – my One-Block Roleplaying Game... an Interactive Broadway-style musical... An outdoor, multi-player Immersive Sim...</p></div><p>It was 2016 and Paul Neurath entered my life again.</p><h2>OtherSide</h2><p>It was one of those magical, out-of-the-blue calls - the kind of call that has changed the trajectory of my life so many times. This time Paul told me he was doing a start-up of his own, had the boring business stuff under control, and asking if I wanted to join him as a co-founder.</p><p>Hm. Do a start-up where all the HR and business stuff was already in place? Intriguing. The hook was in the water. Then he baited it.</p><div><p>“I have the rights to make a new Underworld game. And I have the rights to make a new System Shock game. I’m going to make the Underworld game up north. You can make the new Shock game. In Austin” (my hometown).</p><p>It was another of those moments – like the Steve Jackson Games job I probably didn’t deserve,,, like the TSR moment... the Origin moment... the Ion Storm moment... the Mickey Mouse moment... It was yet another “Who says no to that?” moment in what’s been a blessed career.</p></div><p>So Paul and I set about building two offices, one in the Boston area, the other in Austin.</p><p>The story of System Shock 3 is long and convoluted. Basically, though, I built a team. I had a concept that included a feature no one had ever seen in a game before. We got funding. We worked at it for a while. The business side of the project went kaflooey. Suddenly, I was looking for a new concept and a new team to build it.</p><p>I had those three ideas for games to make I mentioned earlier. I picked one - Argos: Riders on the Storm (working title), the one that seemed most obviously the next step for Imm Sims. I could have gone for one of the projects I thought would just be cool, personally. But I’m a relentless advocate for Imm Sims and we’re not done defining that genre. Argos would be a next step.</p><p>We’ve built a core team to concept it with me, and prototype it and plan it. That’s really about all I can say right now. Okay, I <em>will</em> say that the team we’ve built so far is one of the strongest I’ve ever worked with. They have the potential to be a team on par with the best in my career – and I’ve been lucky enough to have worked with some great groups. They constantly come back to me with ideas and approaches to realizing the vision better than I could have imagined (which is exactly what you want from a team!) And the concept is... out there. Even I think it’s a little nuts. But if ... <em>when</em>... we pull it off... Man,..</p><p>In addition to being the Creative Director on Argos, I’m the Chief Creative Officer for OtherSide, which means I get my fingers in every game we put in development. My job as CCO involves two things:</p><p>First, to safeguard the studio’s mission. That means empowering players to experiment with deeply interactive worlds and craft their own unique experiences through their play and playstyle choices. In other words, kind of what I’ve tried to do for forty years. Paul Neurath and I are in lockstep on this. No compromises.</p><div><p>Second, it’s my responsibility to help all of our teams, in any way I can, to build the best version of the games they want to make. Tempting as it is to violate this rule, I have to remind myself that it’s <em>their</em> game, not mine. No compromises there either.</p><p>On Argos, I play a central Creative Director role in determining what the game is going to be. But we’re also making another game, Thick as Thieves, which has its own team, its own leaders, and a somewhat different approach to realizing the company mission. Like Argos, and as every game should, Thick as Thieves has some elements that I think will surprise and please players. There’s a great leadership team in place and the rest of the team is strong. I can’t say anything more yet. Sit tight.</p></div><p>In 2023, OtherSide was acquired by a new company called Aonic. It’s early days, but I’m psyched to be a part of their family and I’m looking forward to seeing where our partnership goes. The future looks bright.</p><h3>Ch...ch...ch...changes</h3><p>So I survived 40 years. What's changed in that time? It really is tempting to go with the cliche “everything.” And there's some truth to that:</p><p>For starters, I have a lot more gray hair. But that’s significant only to me. There are plenty of genuinely important changes to talk about.</p><figure>
        <picture>
        <source type="image/webp" media="(max-width: 576px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbec810f0ec89a16b/6511d78ab7734b57dd78dba6/Ion_Storm_2023.jpg?width=480&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(max-width: 767px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbec810f0ec89a16b/6511d78ab7734b57dd78dba6/Ion_Storm_2023.jpg?width=768&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/webp" media="(min-width: 768px)" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbec810f0ec89a16b/6511d78ab7734b57dd78dba6/Ion_Storm_2023.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale">
        <source type="image/jpeg" srcset="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbec810f0ec89a16b/6511d78ab7734b57dd78dba6/Ion_Storm_2023.jpg?width=828&amp;quality=80&amp;format=jpg&amp;disable=upscale">
        <img data-image="asfa50x69sqq" src="https://eu-images.contentstack.com/v3/assets/blt95b381df7c12c15d/bltbec810f0ec89a16b/6511d78ab7734b57dd78dba6/Ion_Storm_2023.jpg?width=828&amp;quality=80&amp;format=webply&amp;disable=upscale" data-sys-asset-uid="bltbec810f0ec89a16b">
        </picture>
        <figcaption>Remember that picture earlier of the three reprobate designers at Ion Storm? Here we are in 2023 at the ceremony where John Romero got his well-deserved Lifetime Achievement Award at GDC. That’s Tom Hall on the left, then John, then me.</figcaption></figure><p>When I started, we had nothing, conceptually, development-wise, technologically or tools. There were no game engines to make life easier. There weren't even any existing genres to borrow from and guide feature development. There were no sound cards. (One major sound card company told us they owed their survival to the first Wing Commander game...) Hell, when I started, we had one color - green - and got way too excited when CGA came along! We were making it up as we went along. which was pretty exciting, let me tell you. That feeling of creating something – an entire medium, not just a single game, was pretty thrilling.</p><p>There was no E3 show. And the game space at CES was silent as a tomb. Until one year, Chris Roberts rented what passed for a big-screen TV and a movie theater sound system to demo Wing Commander. Let’s just say the folks in charge of the show came by to make us turn the volume down! We ignored them. You can blame Chris and Origin for the hearing-damaging volume levels at game conventions!</p><p>Selling 100,000 copies of a game was called "going gold" and when that happened you went and bought yourself a Ferrari.</p><p>There was no digital distribution or online and multiplayer gaming. We sold in baggies or boxes filled with disks. The first time we shipped a game you could install on a 10-megabyte hard drive, we said "No one has a hard drive that big - we'll have to ship the game with a drive in the box. And the first time we saw a CD - a CD mind you - we stared at it like apes seeing the monolith in 2001 and said "no one will ever fill one of these up." (We were wrong about that, by the way.)</p><p>We actually depended on these things called "magazines" for reviews and PR.</p><p>We posted fan letters... on paper... some of them handwritten... on office walls.</p><p>The 3D revolution hadn't even begun. There were no games that featured 3D modeling - some of us spoke at SIGGRAPH about the 3D revolution in games even as we were in the midst of that revolution. And speaking of 3D, remember what I said earlier about the first time I saw Paul Neurath's tech demo of a real-time, fully-textured first-person, 3D engine and thought to myself, "The world just changed."</p><p>Audience expectations have gone way up. When I started, games were something new and sold to a limited audience who just wanted a novel kind of entertainment. They were D&amp;D players and technophiles who’d never seen anything like this new thing called “computer games” or “video games.” They – we really were geeks who reveled in our ability to master a medium normal people didn’t get. Today, the mass audience has jumped on the bandwagon and developers must exploit the capabilities of machines powerful enough to dwarf anything we could have imagined. Graphics and gameplay have to be first-rate. User interfaces that use every key on a keyboard and didn’t even have the option of a mouse have to be user friendly today at a level we didn’t have to achieve. We were pioneers back in the day and people expected hardship. We’re settled city dwellers today and our audience expects comfort, usability and quality in all aspects of their experience.</p><p>There were only two ways to reach an audience – through a publisher with connections with software stores or in ziplock bags sold by small studios or individual developers through the mail. Advertisements in something called “magazines” were a major distribution outlet for us, Today there are so many business models – so many ways to reach an audience and make some money – it’s hard to keep up.</p><p>And I haven't even gotten to the internet, always-on connectivity, a computer in everyone's pocket, free-to-play, microtransactions, designing games to be played forever instead of being satisfactorily completable. And there's this early access stuff that sounds to me like shipping a game before it's ready. Not only did those things not exist - we couldn't even have imagined them.</p><p>Today, anyone who says they’re not a gamer is either an outcast (like gamers used to be) or a liar.</p><p>I could go on, but you get the idea.</p><h3>Same old same old</h3><p>So what hasn't changed?</p><p>Things still change every day, just as they did when I started - new genres, new graphical approaches, new distribution methods, new multiplayer modes, new business models, and so on. That may sound like a contradiction, like a list of things I should have mentioned earlier, in discussing things that have changed. The reason it's here is because there's as much innovation possible today as there ever was. I learn from younger developers every day, and they learn from each other, just like I learned from my peers earlier. That hasn’t changed. Nor has the necessity of being a life-long learner. Yes, it’s a cliche, but it's one I proudly adopt for myself. And you should adopt yourself. My guess is you already have. I know game developers well enough to be comfortable saying that. There's always something new to learn.</p><p>People and teams are as committed, as dedicated and as in love with what they do as they were 40 years ago. I literally cried at work recently because the team I'm working with listened to a high-level vision and found a way to express it that was so much more than I expected. Better. Deeper. I cried out of pride in them and gratitude for the gift they gave me then and give me every day.</p><p>Games are and always will be - or at least have the potential to be - storytelling tools. But not in the way earlier media were. We can empower players to tell their own stories through their play choices. We can turn every player into an author. That was true in the early days and it's even truer today.</p><p>Things to learn, committed developers and games as a unique medium? All true 40 years ago. Still true today.</p><h3>Survival</h3><p>Now that you know my story, how did I survive it? Here's a partial list. Maybe you can relate to some of these, see them in your own life, and hold them close to make <em>your </em>survival more likely.</p><p>I've been lucky enough to be able to fill every role associated with game development that didn't involve engineering, art and sound. I've been on the business side and the creative side. I've worked for publishers and developers. I've been a designer, a game director, a creative director and a chief creative officer. I've worked with internal teams and external teams. If variety is the spice of life, I've had the Thai food of careers. Spicy! I'm sure "never been bored" is part of my equation.</p><p>I’ve been surrounded by people way smarter than I am and I've learned something from them every day. I get a lot of credit for a lot of games, but I’d be nowhere without folks like Steve Jackson, Richard Garriott, Paul Neurath, Doug Church, Harvey Smith, Art Min, John Romero, Seamus Blackley, Paul Weaver, Chase Jones and, of course, my understanding and long-suffering wife who's helped me when the chaos of development got me down. There are too many more to name. These were my friends and partners. All have been my teachers and some of them more than that - mentors. I owe each of them a debt of gratitude I can never repay. Find folks like this and cherish them.</p><p>I've been driven to and been given the opportunity in a variety of forums to be a relentless evangelist for a particular kind of game - the Immersive Simulation. It’s a genre I love and to this day think is critical to our maturation as a medium. Note that there's still plenty of work to be done in that space if any of you want to join me – literally, as team members, or figuratively, as fellow creators and evangelists.</p><p>I get bored easily. That might not sound like a survival trait but think about it. Boredom can spur you on to try new things, to challenge yourself, maybe even to boldly go where no one has gone before. I often joke that I’ve been making the same game over and over again, just a bit better each time. To an extent that’s true. I have no interest in making a game that doesn’t allow each player to create their own unique experience. I’m a dyed-in-the-wool Immersive Sim guy. But look more closely and you’ll see that the content and associated techniques never repeat. I’ve worked on fantasy and SF games, original IP as well as sequels to other people’s work and licenses. I’ve mashed up existing genres to create something new and unique. Look at Deus Ex - part RPG, part Shooter, part Stealth game. Or Disney Epic Mickey - part Adventure game, part Shooter (think about it…), part Platformer. And my new game, well, let’s just say it’s one part something different than anything I’ve worked on before, combined with something else and something else again. (You have no idea what’s coming…). Trust me when I tell you that part of my survival is that I’ve never had to repeat myself.</p><p>People have appreciated the work my teams and I have done and have expressed it loudly and affectionately. I may or may not deserve accolades, but I've been lucky enough to receive them and I'm grateful for all the kind words and well-wishes I've received. Maybe it's just my ego, but what I consider to be my successes have nothing to do with reviews, sales or revenue. Success for me is connection with players (and not in the data-collecting way some of you may be thinking). I've had people send me handmade plush toys based on characters in my games. I've had people send me artwork they were inspired to create. I've had people tell me a game I worked on helped get them through chemotherapy. Autism. Cerebral palsy. I ran into a young woman at Disneyland dressed as Ortensia, in a homemade costume, before the character was a star in the Disney firmament. “I started my company because of your game,” I’ve been told. And “I changed the way I thought about design because of a game you worked on.” Now those are success criteria that have kept me going, even when things got tough.</p><p>I've had the opportunity to help, in a small way, create and define a new art form, an opportunity that comes along only two or three times a century. I’ve always thought games were - or could be - important, artistically and culturally. And I'm just pretentious enough to want to make art and be a minor contributor to a medium that has and will continue to change the world. (As a note, that opportunity still exists - we're not a solved problem like other media. We've barely scratched the surface of what games can and should be.)</p><p>I’ve been exceptionally lucky. Every time I’ve needed, wanted, or benefited from a change, a phone call came my way offering me a new opportunity. I’ve always had new challenges and new things to learn. Hard to get bored when that’s the nature of things. That said, I believe that you make your own luck, to an extent. I read somewhere that in order to get hit by a train you have to stand on the tracks. A silly way to put it, but with a grain of truth to it. What it means to me is that you have to work hard to make sure you’re in a position where luck can come your way. I think I’ve done that. I’ve prepared for change and put myself in positions where people want to call me. So they do.</p><p>I’ve loved my work, even when I've hated it. (You developers know what I mean!). I hope every one of you reading this feels that love every day when you go to the office (those of you who still <em>go </em>to an office...).</p><p>Finally, I’m not qualified to do anything else but make games. Most of you can get jobs somewhere else. I guess I could teach, but basically, I got nothing but this. I've <em>had </em>to survive!</p><h3>Conclusions</h3><p>So what conclusions can I draw from 40 years of making games?</p><p>I couldn’t have survived without my mentors, notably Steve Jackson and Richard Garriott. Steve gave me what I think of as my undergraduate degree in game development. Richard gave me my graduate degree. They were integral to everything I've done and I guess become... As I said before, I’ve had too many teachers and collaborators to name. Whether I worked <em>for</em> them or they worked for me, whether we were peers or partners, I’ve tried to learn something from all of them. It isn't too strong a statement that I’ve been taught more by others than I taught them. Well, at least as much as I hope I've taught them. Mentors, teachers, teammates, friends and family are, in a real and tangible sense, responsible for "my" success. If you don't have people like this in your life, find them.</p><p>What I discovered at Origin, Looking Glass, Ion Storm, Junction Point and most recently OtherSide, is that I'm not the world’s greatest designer. I’m certainly not as good as some of the other designers I've worked with. What I'm best at is coming up with a concept and communicating a core vision clearly and compellingly before finding star performers who make that vision real, in as great a way as I could have expected or, usually, even better.</p><p>I’m also pretty good at identifying people with a strong, clear vision of their own and helping them express that vision better than they could without a few pushes and nudges here and there. One of the most important lessons I’ve learned is that if your game doesn't have a clear vision, and you can't express and "sell" it and if you don't hire people better than you are, you're likely to fail. At best you're on the road to mediocrity. Life is too short to work on games that you know are going to be simply okay.</p><p>Finally, to younger developers (which is basically all of you given my... ahem... experience), to you I say, don't lose sight of a significant part of your job - maybe the most important part of your job. It's up to each and every one of you to make things so amazing and innovative that people forget about guys like me. Seriously. I want you to stun me, to do things I can't even imagine. Don't settle for rehashes of earlier games that differ only by virtue of prettier pictures. Show me things I've never seen or done before. Games are not what I call a "solved problem." Other media, more mature than us, may offer different content and even different development tools and distribution models, but in formal terms, movies and novels, to name just two, haven't changed much in 100 years or more. Editing techniques and narrative structures are well-understood. Games are a mystery, still.</p><p>I'll wrap this up by saying that the coolest thing I've seen in the last 40 years is that we changed the world - not me, not the individual companies I've worked for. But the game business and the medium as a whole and all the intelligent, creative people working in it. We went from games for geeks to games for everyone. Our sales dwarf those in any other medium of expression ever. Ever. Think about that. We’re a powerful cultural force. We've developed an entirely new art form. We’re the only medium in the history of humankind that can turn every consumer into a creator and, astonishingly, we do that through the power of play. Think about <em>that</em>!</p><p>We changed the world, yes, but we’re not done yet. As I said, we’re not a solved problem. Anyone reading this could be the next agent of change, whether you work for a huge company or a small one. There’s no telling where innovation will come from. So get to it! Give me a game unlike any I’ve ever played. Show me what you got! Change the world.<br></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rethinking the Luddites in the Age of A.I (219 pts)]]></title>
            <link>https://www.newyorker.com/books/page-turner/rethinking-the-luddites-in-the-age-of-ai</link>
            <guid>37664682</guid>
            <pubDate>Tue, 26 Sep 2023 19:31:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newyorker.com/books/page-turner/rethinking-the-luddites-in-the-age-of-ai">https://www.newyorker.com/books/page-turner/rethinking-the-luddites-in-the-age-of-ai</a>, See on <a href="https://news.ycombinator.com/item?id=37664682">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="ArticlePageChunks"><p>On December 15, 1811, the London <em>Statesman</em> issued a warning about the state of the stocking industry in Nottingham. Twenty thousand textile workers had lost their jobs because of the incursion of automated machinery. Knitting machines known as lace frames allowed one employee to do the work of many without the skill set usually required. In protest, the beleaguered workers had begun breaking into factories to smash the machines. “Nine Hundred Lace Frames have been broken,” the newspaper reported. In response, the government had garrisoned six regiments of soldiers in the town, in a domestic invasion that became a kind of slow-burning civil war of factory owners, supported by the state, against workers. The article was apocalyptic: “God only knows what will be the end of it; nothing but ruin.”</p><p>The workers destroying the lace frames were the group who called themselves Luddites, after Ned Ludd, a (likely fictional) knitting-frame apprentice near Leicester who was said to have rebelled against his boss by destroying a frame with a hammer. Today, the word “Luddite” is used as an insult to anyone resistant to technological innovation; it suggests ignoramuses, sticks in the mud, obstacles to progress. But a new book by the journalist and author Brian Merchant, titled “<a data-offer-url="https://www.amazon.com/Blood-Machine-Origins-Rebellion-Against/dp/0316487740" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.amazon.com/Blood-Machine-Origins-Rebellion-Against/dp/0316487740&quot;}" href="https://www.amazon.com/Blood-Machine-Origins-Rebellion-Against/dp/0316487740" rel="nofollow noopener" target="_blank">Blood in the Machine</a>,” argues that Luddism stood not against technology per se but for the rights of workers above the inequitable profitability of machines. The book is a historical reconsideration of the movement and a gripping narrative of political resistance told in short vignettes.</p><p>The hero of the story is George Mellor, a young laborer from Huddersfield who worked as a so-called cropper, smoothing the raised surface of rough cloth with shears. He observed the increasing automation of the industry, concluded that it was unjust, and decided to join the insurgent Luddite movement. A physically towering figure, he organized his fellow-workers and led attacks on factories. One factory owner who was targeted was William Horsfall, a local cloth entrepreneur. Horsfall threatened to ride his horse through “Luddite blood” in order to keep his profitable factories going, hiring mercenaries and installing cannons to defend his machines. In the background of the story, figures such as the ineffectual Prince George, a sybaritic regent for his infirm father, George III, and Lord Byron, the poet, who voiced his sympathy for the Luddites in Parliament, debate which side to support: owners or workers. Byron exhorted the workers in his poem “Song for the Luddites” to “die fighting, or live free.”</p><p>Merchant ably demonstrates the dire stakes of the Luddites’ plight. The trades that had sustained livelihoods for generations were disappearing, and their families were starving. A Lancashire weaver’s weekly pay dropped from twenty-five shillings in 1800 to fourteen in 1811. The market was being flooded with cheaper, inferior goods such as “cut-ups,” stockings made from two pieces of cloth joined together, rather than knit as one continuous whole. The government repeatedly failed to intervene on behalf of the workers. What option remained was attacking the boss’s capital by disabling the factories. The secretive captains of the Luddite forces took on the pseudonym General Ludd or King Ludd, which they used to write public letters and to sign threats of attacks. The spectre of violence led some factory owners to abandon their plans for automation. They reverted to manual labor or closed up shop completely. For a time, it seemed that the Luddites were making headway in empowering themselves over the machines.</p><p>The book offers plenty of satisfying imagery for the twenty-first-century reader experiencing techlash. Merchant argues that the message of Luddism is just as relevant today, as our lives become increasingly enmeshed with digital platforms, from TikTok to Uber and Instacart, that translate our labor and attention into profit, “overlaying a sort of psychic factory onto its workers’ lives.” (Who hasn’t at times wished to take a hammer to their MacBook?) The Luddites sought revenge against the innovation that was holding them hostage. In Merchant’s telling, they were activists, punks, and masked celebrities standing up for the skilled working class, the successors to Robin Hood, another product of Nottingham. “Luddite” by that measure sounds like a compliment.</p><p>“Blood in the Machine” is being published just as we are facing a new wave of technological automation centering on <a href="https://www.newyorker.com/science/annals-of-artificial-intelligence/can-we-stop-the-singularity">artificial intelligence</a>—which some, including the consulting firm McKinsey, have labelled the “Fourth Industrial Revolution.” Merchant uses anachronistic terms like “startup” and “tech titan” to describe early factories and entrepreneurs, seeking to draw parallels with the present. (The book’s analytical sections are weaker than its narrative ones.) The “labor-saving technology” of today threatens new categories of jobs: customer service is being performed by chatbots; Amazon is selling e-books written by ChatGPT. Designers and illustrators are losing jobs to image generators; translators are being asked to “clean up” transcripts generated by A.I. The profusion of <a href="https://www.newyorker.com/culture/infinite-scroll/my-ai-writing-robot">dubious A.I.-generated content</a> resembles the badly made stockings of the nineteenth century. At the time of the Luddites, many hoped the subpar products would prove unacceptable to consumers or to the government. Instead, social norms adjusted. Both the mass-manufactured products and the regimented jobs that produced them quickly became entrenched.</p><p>The Luddites watched as sprawling factory buildings rose over their rural towns, concentrating labor that had traditionally been performed independently in the home or small workshops. The working conditions in those factories, often staffed by children, were execrable; the horror stories that emerged, of mangled limbs and bodies, eventually helped encourage reform. The victims of automation today are less immediately obvious. ChatGPT users can’t see the low-paid content moderators in countries such as Kenya who undergird <a href="https://www.newyorker.com/news/the-new-yorker-interview/its-not-possible-for-me-to-feel-or-be-creepy-an-interview-with-chatgpt">the program</a>’s output, performing an onerous psychological task that <a data-offer-url="https://crowd.cs.vt.edu/wp-content/uploads/2021/02/CHI21_final__The_Psychological_Well_Being_of_Content_Moderators-2.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://crowd.cs.vt.edu/wp-content/uploads/2021/02/CHI21_final__The_Psychological_Well_Being_of_Content_Moderators-2.pdf&quot;}" href="https://crowd.cs.vt.edu/wp-content/uploads/2021/02/CHI21_final__The_Psychological_Well_Being_of_Content_Moderators-2.pdf" rel="nofollow noopener" target="_blank">studies have shown</a> can induce P.T.S.D. There is no single machine that can be smashed to disable artificial intelligence. If the physical server farms that host A.I. programs were attacked, the software could simply be hosted elsewhere. What’s more, the foundation of A.I. is the raw material that humanity has already labored to produce: reams of text and images that programs process into patterns and then remix into fresh “content.” Unlike the machines of the first Industrial Revolution, A.I. does not necessarily need more input; it can sustain itself. “Jobs are definitely going to go away, full stop,” Sam Altman, the C.E.O. of OpenAI, recently told <a href="https://www.theatlantic.com/magazine/archive/2023/09/sam-altman-openai-chatgpt-gpt-4/674764/"><em>The Atlantic</em></a>.</p><p>The tragedy of the Luddites is not the fact that they failed to stop industrialization so much as the way in which they failed. In the end, Parliament “sided decisively with the entrepreneurs,” as Merchant writes, and frame-breaking was made a capital offense. Dozens of workers were executed for Luddite activities, including, in January of 1813, fourteen in one brutal day. George Mellor, the Luddite captain, was eventually convicted of assassinating Horsfall, the factory owner, and was hanged, at the age of twenty-three. Human rebellion proved inadequate against the pull of technological advancement.</p><p>“Blood in the Machine” suggests that although the forces of mechanization can feel beyond our control, the way society responds to such changes is not. Regulation of the textile industry could have protected the Luddite workers before they resorted to destruction. One proposal suggested a tax on every yard of cloth made by machine. After a pro-worker bill failed to pass in the House of Lords, Gravener Henson, a frame knitter turned advocate and historian, led an association of workers that demanded higher wages and labor protections, though such “combination” was outlawed at the time in the U.K. Eventually, Luddism faded into a more general political movement. By the late nineteenth century, the majority of Nottingham’s lace production had been mechanized. In the era of A.I., we have another opportunity to decide whether automation will create advantages for all, or whether its benefits will flow only to the business owners and investors looking to reduce their payrolls. One 1812 letter from the Luddites described their mission as fighting against “all Machinery hurtful to Commonality.” That remains a strong standard by which to judge technological gains.&nbsp;♦</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Prophet: Automatic Forecasting Procedure (283 pts)]]></title>
            <link>https://github.com/facebook/prophet</link>
            <guid>37663820</guid>
            <pubDate>Tue, 26 Sep 2023 18:35:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/facebook/prophet">https://github.com/facebook/prophet</a>, See on <a href="https://news.ycombinator.com/item?id=37663820">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-prophet-automatic-forecasting-procedure" dir="auto"><a href="#prophet-automatic-forecasting-procedure">Prophet: Automatic Forecasting Procedure</a></h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/facebook/prophet/workflows/Build/badge.svg"><img src="https://github.com/facebook/prophet/workflows/Build/badge.svg" alt="Build"></a></p>
<p dir="auto"><a href="https://pypi.python.org/pypi/prophet" rel="nofollow"><img src="https://camo.githubusercontent.com/ca04c4e8c3096c6322baf8345d9e49654b14eb653f95940ba94ff65ed179f0d6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f70726f706865742e737667" alt="PyPI Version" data-canonical-src="https://img.shields.io/pypi/v/prophet.svg"></a>
<a href="https://pepy.tech/project/prophet" rel="nofollow"><img src="https://camo.githubusercontent.com/06eeeb6f5fd9a30c5a11817aade564762f59b68a00aecd9a39fbc7dc5b229d4b/68747470733a2f2f706570792e746563682f62616467652f70726f706865742f6d6f6e7468" alt="PyPI Downloads Monthly" data-canonical-src="https://pepy.tech/badge/prophet/month"></a>
<a href="https://pepy.tech/project/prophet" rel="nofollow"><img src="https://camo.githubusercontent.com/0ed6f9de39980c2849f2a31ca727fdc2a9f8f8f142a742732c2dd35c9f9c7132/68747470733a2f2f706570792e746563682f62616467652f70726f70686574" alt="PyPI Downloads All" data-canonical-src="https://pepy.tech/badge/prophet"></a></p>
<p dir="auto"><a href="https://cran.r-project.org/package=prophet" rel="nofollow"><img src="https://camo.githubusercontent.com/2d7a3597b2a92021a4f3d29cf68f35888afa2930867842f20782b54899b58aea/68747470733a2f2f7777772e722d706b672e6f72672f6261646765732f76657273696f6e2f70726f70686574" alt="CRAN Version" data-canonical-src="https://www.r-pkg.org/badges/version/prophet"></a>
<a href="https://cran.r-project.org/package=prophet" rel="nofollow"><img src="https://camo.githubusercontent.com/ba49b200f4ee0e11105953d13633da8aa63412acc86560b95ae95422d8c1675f/68747470733a2f2f6372616e6c6f67732e722d706b672e6f72672f6261646765732f70726f706865743f636f6c6f723d627269676874677265656e" alt="CRAN Downloads Monthly" data-canonical-src="https://cranlogs.r-pkg.org/badges/prophet?color=brightgreen"></a>
<a href="https://cranlogs.r-pkg.org/badges/grand-total/prophet" rel="nofollow"><img src="https://camo.githubusercontent.com/af33df4c8be51a774861f6df5d737d8856e1fae0fe35d6799219077e336a59fa/68747470733a2f2f6372616e6c6f67732e722d706b672e6f72672f6261646765732f6772616e642d746f74616c2f70726f706865743f636f6c6f723d627269676874677265656e" alt="CRAN Downloads All" data-canonical-src="https://cranlogs.r-pkg.org/badges/grand-total/prophet?color=brightgreen"></a></p>
<p dir="auto"><a href="https://anaconda.org/conda-forge/prophet/" rel="nofollow"><img src="https://camo.githubusercontent.com/e71d3c39b99e7977f6a02856c23eefbcf3c71806da569fd9ba4450142eff07a1/68747470733a2f2f616e61636f6e64612e6f72672f636f6e64612d666f7267652f70726f706865742f6261646765732f76657273696f6e2e737667" alt="Conda_Version" data-canonical-src="https://anaconda.org/conda-forge/prophet/badges/version.svg"></a></p>
<hr>
<p dir="auto"><strong>2023 Update:</strong> We discuss our plans for the future of Prophet in this blog post: <a href="https://medium.com/@cuongduong_35162/facebook-prophet-in-2023-and-beyond-c5086151c138" rel="nofollow">facebook/prophet in 2023 and beyond</a></p>
<hr>
<p dir="auto">Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.</p>
<p dir="auto">Prophet is <a href="https://code.facebook.com/projects/" rel="nofollow">open source software</a> released by Facebook's <a href="https://research.fb.com/category/data-science/" rel="nofollow">Core Data Science team</a>. It is available for download on <a href="https://cran.r-project.org/package=prophet" rel="nofollow">CRAN</a> and <a href="https://pypi.python.org/pypi/prophet/" rel="nofollow">PyPI</a>.</p>
<h2 tabindex="-1" id="user-content-important-links" dir="auto"><a href="#important-links">Important links</a></h2>
<ul dir="auto">
<li>Homepage: <a href="https://facebook.github.io/prophet/" rel="nofollow">https://facebook.github.io/prophet/</a></li>
<li>HTML documentation: <a href="https://facebook.github.io/prophet/docs/quick_start.html" rel="nofollow">https://facebook.github.io/prophet/docs/quick_start.html</a></li>
<li>Issue tracker: <a href="https://github.com/facebook/prophet/issues">https://github.com/facebook/prophet/issues</a></li>
<li>Source code repository: <a href="https://github.com/facebook/prophet">https://github.com/facebook/prophet</a></li>
<li>Contributing: <a href="https://facebook.github.io/prophet/docs/contributing.html" rel="nofollow">https://facebook.github.io/prophet/docs/contributing.html</a></li>
<li>Prophet R package: <a href="https://cran.r-project.org/package=prophet" rel="nofollow">https://cran.r-project.org/package=prophet</a></li>
<li>Prophet Python package: <a href="https://pypi.python.org/pypi/prophet/" rel="nofollow">https://pypi.python.org/pypi/prophet/</a></li>
<li>Release blogpost: <a href="https://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/" rel="nofollow">https://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/</a></li>
<li>Prophet paper: Sean J. Taylor, Benjamin Letham (2018) Forecasting at scale. The American Statistician 72(1):37-45 (<a href="https://peerj.com/preprints/3190.pdf" rel="nofollow">https://peerj.com/preprints/3190.pdf</a>).</li>
</ul>
<h2 tabindex="-1" id="user-content-installation-in-r---cran" dir="auto"><a href="#installation-in-r---cran">Installation in R - CRAN</a></h2>
<p dir="auto"><g-emoji alias="warning">⚠️</g-emoji> <strong>The CRAN version of prophet is fairly outdated. To get the latest bug fixes and updated country holiday data, we suggest installing the <a href="#installation-in-r---latest-release">latest release</a>.</strong></p>
<p dir="auto">Prophet is a <a href="https://cran.r-project.org/package=prophet" rel="nofollow">CRAN package</a> so you can use <code>install.packages</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="install.packages('prophet')"><pre>install.packages(<span><span>'</span>prophet<span>'</span></span>)</pre></div>
<p dir="auto">After installation, you can <a href="https://facebook.github.io/prophet/docs/quick_start.html#r-api" rel="nofollow">get started!</a></p>
<h2 tabindex="-1" id="user-content-installation-in-r---latest-release" dir="auto"><a href="#installation-in-r---latest-release">Installation in R - Latest release</a></h2>
<div dir="auto" data-snippet-clipboard-copy-content="install.packages('remotes')
remotes::install_github('facebook/prophet@*release', subdir = 'R')"><pre>install.packages(<span><span>'</span>remotes<span>'</span></span>)
<span>remotes</span><span>::</span>install_github(<span><span>'</span>facebook/prophet@*release<span>'</span></span>, <span>subdir</span> <span>=</span> <span><span>'</span>R<span>'</span></span>)</pre></div>
<h4 tabindex="-1" id="user-content-experimental-backend---cmdstanr" dir="auto"><a href="#experimental-backend---cmdstanr">Experimental backend - cmdstanr</a></h4>
<p dir="auto">You can also choose an experimental alternative stan backend called <code>cmdstanr</code>. Once you've installed <code>prophet</code>,
follow these instructions to use <code>cmdstanr</code> instead of <code>rstan</code> as the backend:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# R
# We recommend running this in a fresh R session or restarting your current session
install.packages(c(&quot;cmdstanr&quot;, &quot;posterior&quot;), repos = c(&quot;https://mc-stan.org/r-packages/&quot;, getOption(&quot;repos&quot;)))

# If you haven't installed cmdstan before, run:
cmdstanr::install_cmdstan()
# Otherwise, you can point cmdstanr to your cmdstan path:
cmdstanr::set_cmdstan_path(path = <your existing cmdstan>)

# Set the R_STAN_BACKEND environment variable
Sys.setenv(R_STAN_BACKEND = &quot;CMDSTANR&quot;)"><pre><span><span>#</span> R</span>
<span><span>#</span> We recommend running this in a fresh R session or restarting your current session</span>
install.packages(c(<span><span>"</span>cmdstanr<span>"</span></span>, <span><span>"</span>posterior<span>"</span></span>), <span>repos</span> <span>=</span> c(<span><span>"</span>https://mc-stan.org/r-packages/<span>"</span></span>, getOption(<span><span>"</span>repos<span>"</span></span>)))

<span><span>#</span> If you haven't installed cmdstan before, run:</span>
<span>cmdstanr</span><span>::</span>install_cmdstan()
<span><span>#</span> Otherwise, you can point cmdstanr to your cmdstan path:</span>
<span>cmdstanr</span><span>::</span>set_cmdstan_path(<span>path</span> <span>=</span> <span>&lt;</span><span>your</span> <span>existing</span> <span>cmdstan</span><span>&gt;</span>)

<span><span>#</span> Set the R_STAN_BACKEND environment variable</span>
Sys.setenv(<span>R_STAN_BACKEND</span> <span>=</span> <span><span>"</span>CMDSTANR<span>"</span></span>)</pre></div>
<h3 tabindex="-1" id="user-content-windows" dir="auto"><a href="#windows">Windows</a></h3>
<p dir="auto">On Windows, R requires a compiler so you'll need to <a href="https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started">follow the instructions</a> provided by <code>rstan</code>. The key step is installing <a href="http://cran.r-project.org/bin/windows/Rtools/" rel="nofollow">Rtools</a> before attempting to install the package.</p>
<p dir="auto">If you have custom Stan compiler settings, install from source rather than the CRAN binary.</p>
<h2 tabindex="-1" id="user-content-installation-in-python---pypi-release" dir="auto"><a href="#installation-in-python---pypi-release">Installation in Python - PyPI release</a></h2>
<p dir="auto">Prophet is on PyPI, so you can use <code>pip</code> to install it.</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m pip install prophet"><pre>python -m pip install prophet</pre></div>
<ul dir="auto">
<li>From v0.6 onwards, Python 2 is no longer supported.</li>
<li>As of v1.0, the package name on PyPI is "prophet"; prior to v1.0 it was "fbprophet".</li>
<li>As of v1.1, the minimum supported Python version is 3.7.</li>
</ul>
<p dir="auto">After installation, you can <a href="https://facebook.github.io/prophet/docs/quick_start.html#python-api" rel="nofollow">get started!</a></p>
<h3 tabindex="-1" id="user-content-anaconda" dir="auto"><a href="#anaconda">Anaconda</a></h3>
<p dir="auto">Prophet can also be installed through conda-forge.</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda install -c conda-forge prophet"><pre>conda install -c conda-forge prophet</pre></div>
<h2 tabindex="-1" id="user-content-installation-in-python---development-version" dir="auto"><a href="#installation-in-python---development-version">Installation in Python - Development version</a></h2>
<p dir="auto">To get the latest code changes as they are merged, you can clone this repo and build from source manually. This is <strong>not</strong> guaranteed to be stable.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/facebook/prophet.git
cd prophet/python
python -m pip install -e ."><pre>git clone https://github.com/facebook/prophet.git
<span>cd</span> prophet/python
python -m pip install -e <span>.</span></pre></div>
<p dir="auto">By default, Prophet will use a fixed version of <code>cmdstan</code> (downloading and installing it if necessary) to compile the model executables. If this is undesired and you would like to use your own existing <code>cmdstan</code> installation, you can set the environment variable <code>PROPHET_REPACKAGE_CMDSTAN</code> to <code>False</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export PROPHET_REPACKAGE_CMDSTAN=False; python -m pip install -e ."><pre><span>export</span> PROPHET_REPACKAGE_CMDSTAN=False<span>;</span> python -m pip install -e <span>.</span></pre></div>
<h3 tabindex="-1" id="user-content-linux" dir="auto"><a href="#linux">Linux</a></h3>
<p dir="auto">Make sure compilers (gcc, g++, build-essential) and Python development tools (python-dev, python3-dev) are installed. In Red Hat systems, install the packages gcc64 and gcc64-c++. If you are using a VM, be aware that you will need at least 4GB of memory to install prophet, and at least 2GB of memory to use prophet.</p>
<h3 tabindex="-1" id="user-content-windows-1" dir="auto"><a href="#windows-1">Windows</a></h3>
<p dir="auto">Using <code>cmdstanpy</code> with Windows requires a Unix-compatible C compiler such as mingw-gcc. If cmdstanpy is installed first, one can be installed via the <code>cmdstanpy.install_cxx_toolchain</code> command.</p>
<h2 tabindex="-1" id="user-content-changelog" dir="auto"><a href="#changelog">Changelog</a></h2>
<h3 tabindex="-1" id="user-content-version-114-20230530" dir="auto"><a href="#version-114-20230530">Version 1.1.4 (2023.05.30)</a></h3>
<h4 tabindex="-1" id="user-content-python" dir="auto"><a href="#python">Python</a></h4>
<ul dir="auto">
<li>We now rely solely on <code>holidays</code> package for country holidays.</li>
<li>Upgraded cmdstan version to 2.31.0, enabling Apple M1 support.</li>
<li>Fixed bug with Windows installation caused by long paths.</li>
</ul>
<h4 tabindex="-1" id="user-content-r" dir="auto"><a href="#r">R</a></h4>
<ul dir="auto">
<li>Updated <code>holidays</code> data based on holidays version 0.25.</li>
</ul>
<h3 tabindex="-1" id="user-content-version-112-20230120" dir="auto"><a href="#version-112-20230120">Version 1.1.2 (2023.01.20)</a></h3>
<h4 tabindex="-1" id="user-content-python-1" dir="auto"><a href="#python-1">Python</a></h4>
<ul dir="auto">
<li>Sped up <code>.predict()</code> by up to 10x by removing intermediate DataFrame creations.</li>
<li>Sped up fourier series generation, leading to at least 1.5x speed improvement for <code>train()</code> and <code>predict()</code> pipelines.</li>
<li>Fixed bug in how warm start values were being read.</li>
<li>Wheels are now version-agnostic.</li>
</ul>
<h4 tabindex="-1" id="user-content-r-1" dir="auto"><a href="#r-1">R</a></h4>
<ul dir="auto">
<li>Fixed a bug in <code>construct_holiday_dataframe()</code></li>
<li>Updated <code>holidays</code> data based on holidays version 0.18.</li>
</ul>
<h3 tabindex="-1" id="user-content-version-111-20220908" dir="auto"><a href="#version-111-20220908">Version 1.1.1 (2022.09.08)</a></h3>
<ul dir="auto">
<li>(Python) Improved runtime (3-7x) of uncertainty predictions via vectorization.</li>
<li>Bugfixes relating to Python package versions and R holiday objects.</li>
</ul>
<h3 tabindex="-1" id="user-content-version-11-20220625" dir="auto"><a href="#version-11-20220625">Version 1.1 (2022.06.25)</a></h3>
<ul dir="auto">
<li>Replaced <code>pystan2</code> dependency with <code>cmdstan</code> + <code>cmdstanpy</code>.</li>
<li>Pre-packaged model binaries for Python package, uploaded binary distributions to PyPI.</li>
<li>Improvements in the <code>stan</code> model code, cross-validation metric calculations, holidays.</li>
</ul>
<h3 tabindex="-1" id="user-content-version-10-20210328" dir="auto"><a href="#version-10-20210328">Version 1.0 (2021.03.28)</a></h3>
<ul dir="auto">
<li>Python package name changed from fbprophet to prophet</li>
<li>Fixed R Windows build issues to get latest version back on CRAN</li>
<li>Improvements in serialization, holidays, and R timezone handling</li>
<li>Plotting improvements</li>
</ul>
<h3 tabindex="-1" id="user-content-version-07-20200905" dir="auto"><a href="#version-07-20200905">Version 0.7 (2020.09.05)</a></h3>
<ul dir="auto">
<li>Built-in json serialization</li>
<li>Added "flat" growth option</li>
<li>Bugfixes related to <code>holidays</code> and <code>pandas</code></li>
<li>Plotting improvements</li>
<li>Improvements in cross validation, such as parallelization and directly specifying cutoffs</li>
</ul>
<h3 tabindex="-1" id="user-content-version-06-20200303" dir="auto"><a href="#version-06-20200303">Version 0.6 (2020.03.03)</a></h3>
<ul dir="auto">
<li>Fix bugs related to upstream changes in <code>holidays</code> and <code>pandas</code> packages.</li>
<li>Compile model during first use, not during install (to comply with CRAN policy)</li>
<li><code>cmdstanpy</code> backend now available in Python</li>
<li>Python 2 no longer supported</li>
</ul>
<h3 tabindex="-1" id="user-content-version-05-20190514" dir="auto"><a href="#version-05-20190514">Version 0.5 (2019.05.14)</a></h3>
<ul dir="auto">
<li>Conditional seasonalities</li>
<li>Improved cross validation estimates</li>
<li>Plotly plot in Python</li>
<li>Bugfixes</li>
</ul>
<h3 tabindex="-1" id="user-content-version-04-20181218" dir="auto"><a href="#version-04-20181218">Version 0.4 (2018.12.18)</a></h3>
<ul dir="auto">
<li>Added holidays functionality</li>
<li>Bugfixes</li>
</ul>
<h3 tabindex="-1" id="user-content-version-03-20180601" dir="auto"><a href="#version-03-20180601">Version 0.3 (2018.06.01)</a></h3>
<ul dir="auto">
<li>Multiplicative seasonality</li>
<li>Cross validation error metrics and visualizations</li>
<li>Parameter to set range of potential changepoints</li>
<li>Unified Stan model for both trend types</li>
<li>Improved future trend uncertainty for sub-daily data</li>
<li>Bugfixes</li>
</ul>
<h3 tabindex="-1" id="user-content-version-021-20171108" dir="auto"><a href="#version-021-20171108">Version 0.2.1 (2017.11.08)</a></h3>
<ul dir="auto">
<li>Bugfixes</li>
</ul>
<h3 tabindex="-1" id="user-content-version-02-20170902" dir="auto"><a href="#version-02-20170902">Version 0.2 (2017.09.02)</a></h3>
<ul dir="auto">
<li>Forecasting with sub-daily data</li>
<li>Daily seasonality, and custom seasonalities</li>
<li>Extra regressors</li>
<li>Access to posterior predictive samples</li>
<li>Cross-validation function</li>
<li>Saturating minimums</li>
<li>Bugfixes</li>
</ul>
<h3 tabindex="-1" id="user-content-version-011-20170417" dir="auto"><a href="#version-011-20170417">Version 0.1.1 (2017.04.17)</a></h3>
<ul dir="auto">
<li>Bugfixes</li>
<li>New options for detecting yearly and weekly seasonality (now the default)</li>
</ul>
<h3 tabindex="-1" id="user-content-version-01-20170223" dir="auto"><a href="#version-01-20170223">Version 0.1 (2017.02.23)</a></h3>
<ul dir="auto">
<li>Initial release</li>
</ul>
<h2 tabindex="-1" id="user-content-license" dir="auto"><a href="#license">License</a></h2>
<p dir="auto">Prophet is licensed under the <a href="https://github.com/facebook/prophet/blob/main/LICENSE">MIT license</a>.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU tells Apple to open everything up to its rivals (507 pts)]]></title>
            <link>https://appleinsider.com/articles/23/09/26/eu-tells-apple-to-open-everything-up-to-its-rivals</link>
            <guid>37663725</guid>
            <pubDate>Tue, 26 Sep 2023 18:28:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://appleinsider.com/articles/23/09/26/eu-tells-apple-to-open-everything-up-to-its-rivals">https://appleinsider.com/articles/23/09/26/eu-tells-apple-to-open-everything-up-to-its-rivals</a>, See on <a href="https://news.ycombinator.com/item?id=37663725">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>
                        <a href="https://photos5.appleinsider.com/gallery/51155-101060-european-union-flag-xl.jpg">
              <img src="https://photos5.appleinsider.com/gallery/51155-101060-european-union-flag-xl.jpg" alt="">
            </a>
          </p>

          
          
          
                    <p>European Commissioner Thierry Breton says the Digital Markets Act is just a beginning, and Apple must open up its whole ecosystem to competitors.
</p><p>The EU's Digital Markets Act (DMA) <a href="https://appleinsider.com/articles/22/07/19/european-council-approves-digital-markets-act-rules">was created</a> to target Big Tech firms like Apple, and make a fairer business environment for all comers. The DMA became law in the EU in <a href="https://appleinsider.com/articles/22/11/01/europe-confirms-its-digital-markets-act-will-go-after-apples-app-store">November 2022</a>, and became applicable from May 2023, though it is still in the process of <a href="https://appleinsider.com/articles/23/09/06/apples-imessage-gets-a-reprieve-from-eu-law">being implemented</a>.
</p><p>According to <em>Reuters</em>, Thierry Breton has now <a href="https://www.reuters.com/technology/eus-breton-tells-apple-ceo-open-its-ecosystem-rivals-2023-09-26/">called on Apple</a> to open up its hardware and software ecosystem.
</p><p>"The next job for Apple and other Big Tech, under the DMA is to open up its gates to competitors," he said. "Be it the electronic wallet, browsers or app stores, consumers using an Apple <a href="https://appleinsider.com/inside/iphone" title="iPhone" data-kpt="1">iPhone</a> should be able to benefit from competitive services by a range of providers."
</p><p>Breton said this after meeting with <a href="https://appleinsider.com/inside/tim-cook" title="Tim Cook" data-kpt="1">Tim Cook</a> in Brussels, where the Apple CEO is certain to have reiterated the company's arguments about security and privacy.
</p><p>"EU regulation fosters innovation, without compromising on security and privacy," Breton told <em>Reuters</em>.
</p><p>Apple has not commented. Cook's trip to Brussels was not announced, and he hasn't referred to his meeting with the EU Commissioner. However, he <a href="https://twitter.com/tim_cook/status/1706609678522658865/photo/1">has tweeted</a> about meeting with Apple Store staff in the city.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPU.zip: side channel attack that exposes visual data processed on the GPU (199 pts)]]></title>
            <link>https://www.hertzbleed.com/gpu.zip/</link>
            <guid>37663601</guid>
            <pubDate>Tue, 26 Sep 2023 18:19:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hertzbleed.com/gpu.zip/">https://www.hertzbleed.com/gpu.zip/</a>, See on <a href="https://news.ycombinator.com/item?id=37663601">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>

        <p>GPU.zip is a new type of side channel that exposes visual data processed on the graphics processing unit (GPU).
This channel exploits an optimization that is data dependent, software transparent, and present in nearly all modern GPUs: graphical data compression.
We present the first security-centric analysis of this optimization and demonstrate that it can be abused to leak visual data.
For example, using GPU.zip, a malicious webpage can leak pixels from another webpage in the latest version of Google Chrome, violating the browser security model.</p>
<h2 id="research-paper">Research Paper</h2>
<p>The GPU.zip paper will appear in the 45th IEEE Symposium on Security and Privacy (San Francisco, 20-23 May 2024) with the following title:</p>
<ul>
<li>GPU.zip: On the Side-Channel Implications of Hardware-Based Graphical Data Compression</li>
</ul>
<p>You can download a preprint from <a href="https://www.hertzbleed.com/gpu.zip/GPU-zip.pdf">here</a> and the BibTeX citation from <a href="https://www.hertzbleed.com/gpu.zip/GPU-zip.bib">here</a>.</p>
<p>The paper is the result of a collaboration between the following researchers:</p>
<ul>
<li><a href="https://www.cs.utexas.edu/~yingchen/">Yingchen Wang</a> (University of Texas at Austin)</li>
<li><a href="https://www.cs.cmu.edu/~rpaccagn/">Riccardo Paccagnella</a> (Carnegie Mellon University)</li>
<li><a href="https://www.linkedin.com/in/zhaogangse/en">Zhao Gang</a> (University of Texas at Austin)</li>
<li><a href="https://wrv.github.io/">Willy R. Vasquez</a> (University of Texas at Austin)</li>
<li><a href="https://homes.cs.washington.edu/~dkohlbre/">David Kohlbrenner</a> (University of Washington)</li>
<li><a href="https://www.cs.utexas.edu/~hovav/">Hovav Shacham</a> (University of Texas at Austin)</li>
<li><a href="https://cwfletcher.github.io/">Christopher Fletcher</a> (University of Illinois Urbana-Champaign)</li>
</ul>
<h2 id="questions-and-answers">Questions and Answers</h2>
<h3 id="am-i-affected-by-gpuzip">Am I affected by GPU.zip?</h3>
<p>Likely, yes.
We tested integrated GPUs from AMD, Apple, Arm, Intel, and Qualcomm and one discrete GPU from Nvidia.
We have at least preliminary results to show that all tested GPUs are affected.</p>
<h3 id="i-am-a-website-developer-how-do-i-protect-my-users">I am a website developer. How do I protect my users?</h3>
<p>If your website displays sensitive information about users, you should configure your website to deny being embedded by cross-origin websites.
For more information on how to do this, we refer to <a href="https://web.dev/security-headers/">this web.dev article</a>.</p>
<h3 id="i-am-a-user-should-i-be-worried">I am a user. Should I be worried?</h3>
<p>Under most circumstances, probably not.
Most sensitive websites already deny being embedded by cross-origin websites.
As a result, they are not vulnerable to the pixel stealing attack we mounted using GPU.zip.
However, some websites remain vulnerable.
For example, if a user who is logged into Wikipedia visits a malicious webpage, that webpage can exploit GPU.zip to learn the user’s Wikipedia username (as we demonstrate in Section 5.4 of the <a href="https://www.hertzbleed.com/gpu.zip/GPU-zip.pdf">paper</a>).</p>
<h3 id="what-makes-gpuzip-different-from-prior-compression-side-channels">What makes GPU.zip different from prior compression side channels?</h3>
<p>GPU.zip exploits <em>software-transparent</em> uses of compression.
This is in contrast to prior compression side channels, which leak because of software-visible uses of compression and can be mitigated by disabling compression in software.
For a more detailed explanation, we refer to the <a href="https://www.hertzbleed.com/gpu.zip/GPU-zip.pdf">paper</a>.</p>
<h3 id="what-exactly-is-gpu-graphical-data-compression">What exactly is GPU graphical data compression?</h3>
<p>GPU graphical data compression is a feature of modern GPUs used to save memory bandwidth and improve performance without any software involvement.
Specifically, modern GPUs compress graphical data losslessly even when software does not request any compression.</p>
<p>Interestingly, the algorithms used by GPUs for graphical data compression vary across vendors and microarchitectures.
Check out the <a href="https://www.hertzbleed.com/gpu.zip/GPU-zip.pdf">paper</a> for a reverse engineering of several proprietary compression algorithms used by Intel and AMD.</p>
<h3 id="when-did-you-disclose-gpuzip">When did you disclose GPU.zip?</h3>
<p>We disclosed our findings and proof-of-concept code to GPU vendors (AMD, Apple, Arm, Intel, Nvidia, and Qualcomm) and to Google in March 2023.</p>
<h3 id="do-gpu-vendors-plan-to-patch">Do GPU vendors plan to patch?</h3>
<p>As of September 2023, no GPU vendor has committed to patching.</p>
<h3 id="does-chrome-plan-to-patch">Does Chrome plan to patch?</h3>
<p>As of September 2023, Google is still deciding whether and how to patch.</p>
<h3 id="what-about-other-browsers">What about other browsers?</h3>
<p>Chrome is vulnerable to the pixel stealing attack demonstrated in the paper because it satisfies the following three criteria:</p>
<ol>
<li>It allows cross-origin iframes to be loaded with cookies.</li>
<li>It allows rendering SVG filters on iframes.</li>
<li>It delegates rendering tasks to the GPU.</li>
</ol>
<p>Other browsers, like Firefox and Safari, do not meet all these criteria and are therefore not vulnerable.</p>
<h3 id="can-i-use-the-logo">Can I use the logo?</h3>
<p>Yes. The GPU.zip logo is free to use under a <a href="https://creativecommons.org/publicdomain/zero/1.0/">CC0</a> license.</p>
<ul>
<li>Download logo: <a href="https://www.hertzbleed.com/gpu.zip/images/GPU-zip-logo.svg">SVG</a>, <a href="https://www.hertzbleed.com/gpu.zip/images/GPU-zip-logo.png">PNG</a></li>
<li>Download logo with text: <a href="https://www.hertzbleed.com/gpu.zip/images/GPU-zip-logo-with-text.svg">SVG</a>, <a href="https://www.hertzbleed.com/gpu.zip/images/GPU-zip-logo-with-text.png">PNG</a></li>
</ul>
<h3 id="did-you-release-the-source-code-of-gpuzip">Did you release the source code of GPU.zip?</h3>
<p>Yes, you can find the source code at the link:
<a href="https://github.com/UT-Security/gpu-zip">https://github.com/UT-Security/gpu-zip</a></p>


      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Causality for Machine Learning (2020) (123 pts)]]></title>
            <link>https://ff13.fastforwardlabs.com/</link>
            <guid>37663523</guid>
            <pubDate>Tue, 26 Sep 2023 18:14:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ff13.fastforwardlabs.com/">https://ff13.fastforwardlabs.com/</a>, See on <a href="https://news.ycombinator.com/item?id=37663523">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p><a href="https://www.cloudera.com/products/fast-forward-labs-research.html"><img alt="Cloudera Fast Forward" src="https://ff13.fastforwardlabs.com/figures/cloudera-fast-forward-logo.png"></a>
          </p>
          
          
<p>FF13 · ©2020 Cloudera, Inc. All rights reserved</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-cover-splash.png" alt="Causality for Machine Learning report cover"><figcaption>Causality for Machine Learning report cover</figcaption></figure>
<p><em>This is an applied research report by <a href="https://www.cloudera.com/products/fast-forward-labs-research.html">Cloudera Fast Forward Labs</a>. We write reports about emerging technologies. Accompanying each report are working prototypes that exhibit the capabilities of the algorithm and offer detailed technical advice on its practical application. Read our full report on causality for machine learning below or <a href="https://ff13.fastforwardlabs.com/FF13-Causality_for_Machine_Learning-Cloudera_Fast_Forward.pdf" target="_blank" id="report-pdf-download">download the PDF</a>. Also be sure to check out the complementary prototype, <a href="https://scene.fastforwardlabs.com/">Scene</a>.</em></p>
<div><ul><li><a href="#introduction">Introduction</a></li><li><a href="#background%3A-causal-inference">Background: Causal Inference</a><ul><li><a href="#why-are-we-interested-in-causal-inference%3F">Why are we interested in causal inference?</a></li><li><a href="#the-ladder-of-causation">The ladder of causation</a></li><li><a href="#from-correlation-to-causation">From correlation to causation</a></li><li><a href="#from-prediction-to-intervention">From prediction to intervention</a></li><li><a href="#how-do-we-know-which-graph-to-use%3F">How do we know which graph to use?</a></li><li><a href="#tl%3Bdr">TL;DR</a></li></ul></li><li><a href="#causality-and-invariance">Causality and Invariance</a><ul><li><a href="#the-great-lie-of-machine-learning">The great lie of machine learning</a></li><li><a href="#dangers-of-spurious-correlations">Dangers of spurious correlations</a></li><li><a href="#invariance">Invariance</a></li><li><a href="#invariant-causal-prediction">Invariant Causal Prediction</a></li><li><a href="#invariant-risk-minimization">Invariant Risk Minimization</a></li><li><a href="#how-irm-works">How IRM works</a></li></ul></li><li><a href="#prototype">Prototype</a><ul><li><a href="#the-wildcam-dataset">The Wildcam dataset</a></li><li><a href="#experimental-setup">Experimental setup</a></li><li><a href="#results">Results</a></li><li><a href="#product%3A-scene">Product: Scene</a></li></ul></li><li><a href="#landscape">Landscape</a><ul><li><a href="#use-cases">Use Cases</a></li><li><a href="#tools">Tools</a></li></ul></li><li><a href="#ethics">Ethics</a><ul><li><a href="#causal-graphs-make-assumptions-explicit">Causal graphs make assumptions explicit</a></li><li><a href="#omitting-protected-attributes-is-not-enough">Omitting protected attributes is not enough</a></li><li><a href="#invariance-as-a-route-to-fairness">Invariance as a route to fairness</a></li></ul></li><li><a href="#future">Future</a><ul><li><a href="#comparable-approaches">Comparable approaches</a></li><li><a href="#looking-ahead">Looking ahead</a></li></ul></li><li><a href="#conclusion">Conclusion</a></li></ul></div>
<h2 id="introduction">Introduction</h2>
<p>In recent years, machine learning has made remarkable progress, providing novel capabilities like the creation of sophisticated, computable representations of text and images. These capabilities have enabled new products, such as image searches based on image content, automatic translation between many languages, and even the synthesis of realistic images and voice. Simultaneously, machine learning has seen widespread adoption in the enterprise for classic use cases (for instance, predicting customer churn, loan defaulting, and manufacturing equipment failure).</p>
<p>Where machine learning has been successful, it has been extraordinarily so.</p>
<p>In many cases, that success can be attributed to supervised learning on large volumes of training data (combined with extensive computation). Broadly, supervised learning systems excel at one task: <em>prediction</em>. When the goal is to predict an outcome, and when we have many examples of that outcome arising, as well as the features associated with it, we may turn to supervised learning.</p>
<p>As machine learning has gained popularity, its sphere of influence in business processes has expanded beyond narrow prediction and into decision making. The results of machine learning systems are routinely used to set credit limits, anticipate manufacturing equipment failures, and curate our various news feeds. As individuals and businesses seek to learn from the information provided by such complex and nonlinear systems, more (and better) methods for interpretability have been developed, and this is both healthy and important.</p>
<p>However, there are fundamental limits to reasoning based on prediction alone. For instance, what will happen if a bank increases a customer’s credit limit? Such questions cannot be answered by a correlative model built on previously observed data, because they involve a possible change in the customer’s choices as a reaction to the change in credit limit. In many cases, the outcome of our decision process is an <em>intervention</em> - an action that changes something in the world. As we’ll demonstrate in this report, purely correlative predictive systems are not equipped for reasoning under such interventions, and hence are prone to biases. For data-informed decision making under intervention, we need causality.</p>
<p>Even for purely predictive systems, which is very much the forte of supervised learning, applying some causal thinking brings benefits. Causal relationships are by their definition <em>invariant</em>, meaning they hold true across different circumstances and environments. This is a very desirable property for machine learning systems, where we often predict on data that we have not seen in training; we need these systems to be adaptable and robust.</p>
<p>The intersection of causal inference and machine learning is a rapidly expanding area of research. It is already yielding capabilities that are ready for mainstream adoption - capabilities which can help us build more robust, reliable, and fair machine learning systems.</p>
<p>This report is an introduction to causal reasoning as it pertains to much data science and machine learning work. We introduce causal graphs, with a focus on removing the <em>conceptual</em> barriers to understanding. We then use this understanding to explore recent ideas around <em>invariant prediction</em>, which brings some of the benefits of causal graphs to high dimensional problems. Along with the accompanying prototype, we show how even classic machine learning problems, like image classification, can benefit from the tools of causal inference.</p>
<h2 id="background%3A-causal-inference">Background: Causal Inference</h2>
<p>In this chapter, we discuss the essentials of causal reasoning (particularly in how it differs from supervised learning) and give an informal introduction to structural causal models. Grasping the basic notions of causal modeling allows for a much richer understanding of invariance and generalization, which we discuss in the next chapter, <a href="#causality-and-invariance">Causality and Invariance</a>.</p>
<h3 id="why-are-we-interested-in-causal-inference%3F">Why are we interested in causal inference?</h3>
<p>Imagine a bank that would like to reduce the number of business loans which default. Historical data and sophisticated supervised learning techniques may be able to accurately identify which loans are likely to default, and interpretability techniques may tell us some features that are correlated with (or predictive of) defaulting. However, to reduce the default rate, we must understand what changes to make, which requires understanding not only <em>which</em> loans default, but <em>why</em> the loans default.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-01.png" alt="A bank would like to decide which business loans to grant based on true, causal relationships."><figcaption>A bank would like to decide which business loans to grant based on true, causal relationships.</figcaption></figure>
<p>It may be that we find small loans are more likely to default than larger loans. One might naively assume that the bank ought to stop making small loans. However, perhaps it is really the case that smaller businesses are more likely to fail than large businesses, and <em>also</em> more likely to apply for small loans. In this case, the true causal relationship is between the size of the <em>business</em> and defaulting, and not between the size of the <em>loan</em> and defaulting. If this is so, our policy decisions should be influenced by business size, rather than loan size.</p>
<p>Unfortunately, supervised learning alone cannot tell us which is true. If we include both loan size and business size as features in our model, we will simply find that they are both related to loan defaulting, to some extent. While that insight is true - as they are both statistically related to defaulting - which <em>causes</em> defaulting is a separate question, and the one to which we want the answer.</p>
<p>Causality gives us a framework to reason about such questions, and recent developments at the intersection of causality and machine learning are making the discovery of such causal relationships easier.</p>
<h4 id="the-shortcomings-of-supervised-learning">The shortcomings of supervised learning</h4>
<p>Supervised machine learning has proved enormously successful at some tasks. This is particularyly true in dealing with tasks that require high-dimensional inputs, such as computer vision and natural language processing. There has been truly remarkable progress over the past two decades, and it should be noted that an acknowledgment of supervised learning’s shortcomings does not in any way diminish that progress.</p>
<p>With success have come inflated expectations that autonomous systems be capable of independent decision-making, and even human-like intelligence. Current machine learning approaches are unable to meet those expectations, owing to fundamental limitations of pattern recognition.</p>
<p>One such limitation is <strong>generalizability</strong> (also called <em>robustness</em> or <em>adaptability</em>), that is, the ability to apply a model learned in one context in a new environment. Many current state-of-the-art machine learning approaches assume that the trained model will be applied to data that looks the same as the training data. These models are trained on highly specific tasks, like recognizing dogs in images or identifying fraud in banking transactions. In real life, though, the data on which we predict is often different from the data on which we train, even when the task is the same. For example, training data is often subject to some form of selection bias, and simply collecting more of it does not mitigate that.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-02.png" alt="The real world is often distributed differently than our training data."><figcaption>The real world is often distributed differently than our training data.</figcaption></figure>
<p>Another limitation is <strong>explainability</strong>, that is, machine learning models remain mostly “black boxes” that are unable to explain the reasons behind their predictions or recommendations, thus eroding users’ trust and impeding diagnosis and repair. For example, a deep learning system can be trained to recognize cancer in medical images with high accuracy, provided it is given plenty of images and compute power, but - unlike a real doctor - it cannot explain why or how a particular image suggests disease. Several methods for understanding model predictions have been developed, and while these are necessary and welcome, understanding the interpretation and limitations of their outputs is a science in itself. While model interpretation methods like <a href="https://arxiv.org/abs/1602.04938">LIME</a> and <a href="https://arxiv.org/abs/1705.07874">SHAP</a> are useful, they provide insight only into how the model works, and not into how the world works.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-03.png" alt="Predictions alone are often not useful unless accompanied by an explanation."><figcaption>Predictions alone are often not useful unless accompanied by an explanation.</figcaption></figure>
<p>And finally, the understanding of <strong>cause-and-effect</strong> connections - a key element of human intelligence - is absent from pattern recognition systems. Humans have the ability to answer “what if” kinds of questions. <em>What if I change something? What if I had acted differently?</em> Such interventional, counterfactual, or retrospective questions are the forte of human intelligence. While imbuing machines with this kind of intelligence is still far-fetched, researchers in deep learning are increasingly recognizing the importance of these questions, and using them to inform their research.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-04.png" alt="Humans use counterfactual reasoning all the time. This is enabled by our unconcious understanding of cause and effect."><figcaption>Humans use counterfactual reasoning all the time. This is enabled by our unconcious understanding of cause and effect.</figcaption></figure>
<p>All of this means that supervised machine learning systems must be used cautiously in certain situations - and if we want to mitigate these restrictions effectively, causation is key.</p>
<h4 id="what-does-causality-bring-to-the-table%3F">What does causality bring to the table?</h4>
<p>Causal inference provides us with tools that allow us to answer the question of <em>why</em> something happens. This takes us a step further than traditional statistical or machine learning approaches that are focused on predicting outcomes and concerned with identifying associations.</p>
<p>Causality has long been of interest to humanity on a philosophical level, but it has only been in the latter half of the 20th century (thanks to the work of pioneering methodologists such as Donald Rubin and Judea Pearl), that a mathematical framework for causality has been introduced. In recent years, the boom of machine learning has enhanced the development of causal inference and attracted new researchers to the area.</p>
<p>Identifying causal effects helps us understand a variety of things: for example, user behavior in online systems,<sup><a href="#fn2" id="fnref2">[2]</a></sup> effect of social policies, risk factors of diseases. Questions of cause-and-effect are also critical for the design of data-driven applications. For instance, how do algorithmic recommendations affect our purchasing decisions? How do they affect a student’s learning outcome or a doctor’s efficacy? All of these are hard questions and require thinking about the counterfactual: what would have happened in a world with a different system, policy, or intervention?  Without causal reasoning, correlation-based methods can lead us astray.</p>
<p>That said, learning causality is a challenging problem. There are broadly two situations in which we could find ourselves: in one case, we are able to actively intervene in the system we are modeling and get experimental data; in the other, we have only observational data.</p>
<p>The gold standard in establishing causal effects is a Randomised Controlled Trial (RCT) and this falls under the experimental data category. In an RCT, we try to engineer similar populations using random assignment (as choosing the populations manually could introduce selection effects that destroy our ability to learn causal relations) and apply an intervention to one population and not the other. From this, we measure the causal effect of changing one variable as a simple difference in the quantity of interest between the two populations.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-05.png" alt="Randomised controlled trials are the gold standard in establishing causal effects."><figcaption>Randomised controlled trials are the gold standard in establishing causal effects.</figcaption></figure>
<p>We can use RCTs to establish whether a particular causal relation holds. However, trials are not always physically possible, and even when they are, they are not always ethical (for instance, it would not be ethical to deny a patient a treatment that is reasonably believed to work, or trial a news aggregation algorithm designed to influence a person’s mood without informed consent).<sup><a href="#fn3" id="fnref3">[3]</a></sup> In some cases, we can find naturally occurring experiments. In the worst cases, we’re left trying to infer causality from observational data alone.</p>
<p>In general, this is not possible, and we must at least impose some modeling assumptions. There are several formal frameworks for doing so. For our purpose of building intuition, we’ll introduce Judea Pearl’s <a href="http://bayes.cs.ucla.edu/BOOK-2K/">Structural Causal Model</a> (SCM) framework in this chapter.<sup><a href="#fn4" id="fnref4">[4]</a></sup></p>
<h3 id="the-ladder-of-causation">The ladder of causation</h3>
<p>In <a href="http://bayes.cs.ucla.edu/WHY/">The Book of Why</a>, Judea Pearl, an author of much foundational work in causality, describes three kinds of reasoning we can perform as rungs on a ladder. These rungs describe when we need causality, and what it buys us.<sup><a href="#fn5" id="fnref5">[5]</a></sup></p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-06.png" alt="The ladder of causation, as described in The Book of Why."><figcaption>The ladder of causation, as described in <a href="http://bayes.cs.ucla.edu/WHY/">The Book of Why</a>.</figcaption></figure>
<p>On the <strong>first rung</strong>, we can do <strong>statistical and predictive reasoning</strong>. This covers most (but not all) of what we do in machine learning. We may make very sophisticated forecasts, infer latent variables in complex deep generative models, or cluster data according to subtle relations. All of these things sit on rung one.</p>
<p><em>Example: a bank wishes to predict which of its current business loans are likely to default, so it can make financial forecasts that account for likely losses.</em></p>
<p>The <strong>second rung</strong> is <strong>interventional reasoning</strong>. Interventional reasoning allows us to predict what will happen when a system is changed. This enables us to describe what characteristics are particular to the exact observations we’ve made, and what should be invariant across new circumstances. This kind of reasoning requires a <em>causal</em> model. Intervening is a fundamental operation in causality, and we’ll discuss both interventions and causal models in this chapter.</p>
<p><em>Example: a bank would like to reduce the number of loans which default, and considers changing its policies. Predicting what will happen as a result of this intervention requires that the bank understand the causal relations which affect loan defaulting.</em></p>
<p>The <strong>third rung</strong> is <strong>counterfactual reasoning</strong>. On this rung, we can talk not only about what has happened, but also what would have happened if circumstances were different. Counterfactual reasoning requires a more precisely specified causal model than intervention. This form of reasoning is very powerful, providing a mathematical formulation of computing in alternate worlds where events were different.</p>
<p><em>Example: a bank would like to know what the likely return on a loan would have been, had they offered different terms than they did.</em></p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-07.png" alt="The ladder of causation describes the kind of question we can answer depending on the sophistication of our causal model."><figcaption>The ladder of causation describes the kind of question we can answer depending on the sophistication of our causal model.</figcaption></figure>
<p>By now, we hopefully agree that there is something to causality, and it has much to offer. However, we have yet to really <em>define</em> causality. We must begin with a familiar refrain: correlation is not causation.</p>
<h3 id="from-correlation-to-causation">From correlation to causation</h3>
<h4 id="spurious-correlations">Spurious correlations</h4>
<p>Very many things display correlation. The rooster crows when the sun rises.<sup><a href="#fn6" id="fnref6">[6]</a></sup> The lights turn off when you flick a switch. Global temperatures have risen alarmingly since the 1800s, and meanwhile pirate numbers have dwindled to almost nothing.<sup><a href="#fn7" id="fnref7">[7]</a></sup></p>
<p>These examples show us that while correlation can <em>appear</em> as a result of causation, as in the case of the light switch, correlation certainly does not always <em>imply</em> causation, as in the case of the pirates.</p>
<p>Correlated things are not always related.<sup><a href="#fn8" id="fnref8">[8]</a></sup> It’s possible to find many correlations with no readily imaginable causal interaction. The internet treasure <a href="https://www.tylervigen.com/spurious-correlations">Spurious Correlations</a> collects many amusing examples of this. These spurious correlations most likely arise as a result of small sample size and coincidences that are bound to happen when making many comparisons. We should not be surprised if we find something that has low probability if we try many combinations.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/spurious-correlation.png" alt="Figure source: Spurious Correlations."><figcaption>Figure source: <a href="https://www.tylervigen.com/spurious-correlations">Spurious Correlations</a>.</figcaption></figure>
<p>In real world systems, spurious correlations can be cause for serious ethical concerns. For instance, certain characteristics may be spuriously associated with individuals or minority groups, and these characteristics may be highly predictive. As such, the model weights them as important during a learning task. This can easily embed bias and unfairness into an algorithm based on the spurious correlations in a given dataset.</p>
<h4 id="the-principle-of-common-cause">The Principle of Common Cause</h4>
<p>In a posthumous 1956 book, <a href="https://www.goodreads.com/book/show/848892.The_Direction_of_Time">The Direction of Time</a>, Hans Reichenbach outlined the principle of common cause. He states the principle this way:</p>
<blockquote>
<p>“If an improbable coincidence has occurred, there must exist a common cause.”</p>
</blockquote>
<p>Our understanding of causality has evolved, but this language is remarkably similar to what we use now. Let’s discuss how correlation may arise from causation.</p>
<p>We will do this in the framework of Structural Causal Models (SCMs). An SCM is a directed acyclic graph of relationships between variables. The nodes represent variables, and the edges between them point from cause to effect. The value of each variable depends only on its direct parents in the graph (the other variables which point directly into it) and a noise variable that encapsulates any environmental interactions we are not modeling. We will examine three fundamental causal structures.</p>
<div>
<h5 id="causal-terminology">Causal Terminology</h5>
<p>A <strong>causal graph</strong> is a directed acyclic graph denoting the dependency between variables.</p>
<p>A <strong>structural causal model</strong> carries more information than a causal graph alone. It also specifies the functional form of dependencies between variables.</p>
<p>Remarkably, it’s possible to do much causal reasoning - including a calculation of the size of causal effects - via the graph alone, without specifying a parametric form for the relationships between causes and effects.</p>
</div>
<h5 id="1.-direct-causation">1. Direct causation</h5>
<p>The simplest way in which correlation between two variables arises is when one variable is a direct cause of the other. We say that one thing causes another when a change in the first thing, while holding everything else constant, results in a change in the second. In the business loan defaulting example discussed earlier, we could create a two node graph with one of the nodes being whether or not a business is small (say “small business” with values 0 or 1) and the other node being “default” indicating whether or not the business defaulted on the loan. In this case, we would expect that a small business increases the chances of it defaulting.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-08.png" alt="Direct causation gives rise to statistical dependence between two variables. In this fictional example, the indicator variable for Small businesses has a direct causal effect on the Loan Defalt indicator variable."><figcaption>Direct causation gives rise to statistical dependence between two variables. In this fictional example, the indicator variable for Small businesses has a direct causal effect on the Loan Defalt indicator variable.</figcaption></figure>
<p>This setup is immediately reminiscent of supervised learning, where we have a dataset of features, X, and targets, Y, and want to learn a mapping between them. However, in machine learning, we typically start with all available features and select those that are most informative about the target. When drawing a causal relationship, only those features we believe have an actual causal effect on the target should be included as direct causes. As we will see below, there are other diagrams that can lead to a predictive statistical relationship between X and Y in which neither directly causes the other.</p>
<h5 id="2.-common-cause">2. Common cause</h5>
<p>A common pattern is for a single variable to be the cause of multiple other variables. If a variable, Z, is a direct cause of both X and Y, we say that Z is a common cause and call the structure a “fork.” For example, unemployment could potentially cause both loan default and reduced consumer spend.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-09.png" alt="Two effects appear statistically dependent, but only because of a common cause. If the common cause, Unemployment, is fixed, then Consumer Spend and Loan Default become statistically independent."><figcaption>Two effects appear statistically dependent, but only because of a common cause. If the common cause, Unemployment, is fixed, then Consumer Spend and Loan Default become statistically independent.</figcaption></figure>
<p>Because both consumer spend and loan default depend on unemployment, they will appear correlated. A given value of unemployment will generate some values of consumer spend and loan default, and when unemployment changes, both consumer spend and loan default will change. As such, in the joint distribution of the SCM, the two dependent variables (consumer spend and loan default) will appear statistically related to one another.</p>
<p>However, if we were to <em>condition</em> on unemployment (for instance, by selecting data corresponding to a fixed unemployment rate), we would see that consumer spend and loan default are independent from one another.</p>
<p>The common cause unemployment <em>confounds</em> the relationship between consumer spend and loan default. We are unable to correctly calculate the relationship between consumer spend and loan default without accounting for unemployment (by conditioning). This is especially dangerous if unnoticed.</p>
<p>Unfortunately, confounders can be tricky or impossible to detect from observational data alone. In fact, if we look only at consumer spend and loan default, we could see the same joint distribution as in the case where consumer spend and loan default are directly causally related. As such, we should think of causal graphs as encoding our <em>assumptions</em> about the system we are studying. We return to this point in <a href="#how-do-we-know-which-graph-to-use%3F">How do we know which graph to use?</a></p>
<h5 id="3.-common-effect">3. Common effect</h5>
<p>The opposite common pattern is for one effect to have multiple direct causes. A node that has multiple causal parents is called a “collider” with respect to those nodes.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-10.png" alt="Variables that share a common effect are independent, until we fix the effect. For a given value of Loan Default, there is an induced dependency between the Number of Liens and Credit Score."><figcaption>Variables that share a common effect are independent, until we fix the effect. For a given value of Loan Default, there is an induced dependency between the Number of Liens and Credit Score.</figcaption></figure>
<p>A collider is a node that depends on more than one cause. In this example, loan defaulting depends on both commercial credit score and number of liens (a “lien” refers to the right to keep possession of property belonging to another entity until a debt owed by that entity is discharged), so we call loan default a <em>collider</em>.</p>
<p>Colliders are different to chains of direct causation and forks because the conditioning behaviour works oppositely. Before any conditioning, commercial credit score and number of liens are unconditionally independent. There is no variable with causal arrows going into both commercial credit score and number of liens, and no arrow linking them directly, so we should not expect a statistical dependency. However, if we condition on the collider, we will induce a conditional dependence between commercial credit score and number of liens.</p>
<p>This may seem a bit unintuitive, but we can make sense of it with a little thought experiment. Loan default depends on both commercial credit score and number of liens, so if either of those changes value, the chance of loan default changes. We fix the value of loan default (say, we look only at those loans that did default). Now, if we were to learn anything about the value of commercial credit score, we would know something about the number of liens too; only certain values of number of liens are compatible with the conditioned value of loan defaulting and observed value of commercial credit score. As such, conditioning on a collider induces a spurious correlation between the parent nodes. Conditioning on a collider is exactly selection bias!</p>
<h4 id="structural-causal-models%2C-in-code">Structural Causal Models, in code</h4>
<div>
<p>The small causal graphs shown above are an intuitive way to reason about causality. Remarkably, we can do much causal reasoning (and calculate causal effects) with these graphs, simply by specifying qualitatively which variables causally influence others. In the real world, causal graphs can be large and complex.</p>
<p>Of course, there are other ways to encode the information. Given the graph, we can easily write down an expression for the joint distribution: it’s the product of probability distributions for each node conditioned on its direct causal parents. In the case of a collider structure, <code>x</code> → <code>z</code> ← <code>y</code>, the joint distribution is simply <code>p(x,y,z) = p(x) p(y) p(z|x,y)</code>. The conditional probability <code>p(z|x,y)</code> is exactly what we’re used to estimating in supervised learning!</p>
<p>If we know more about the system, we can move from this causal graph to a full structural causal model. An example SCM compatible with this graph would be:</p>
<pre><code>from numpy.random import randn

def x():
  return -5 + randn()

def y():
  return 5 + randn()

def z(x, y):
  return x + y + randn()

def sample():
  x_ = x()
  y_ = y()
  z_ = z(x_, y_)
  return x_, y_, z_
</code></pre>
<p>Each of the variables has an independent random noise associated with it, arising from factors not modeled by the graph. These distributions need not be identical, but must be independent. Notice that the structure of the graph encodes the dependencies between variables, which we see as the function signatures. The values of <code>x</code> and <code>y</code> are independent, but <code>z</code> depends on both. We can also see clearly that the model defines a generative process for the data, since we can easily sample from the joint distribution by calling the <code>sample</code> function. Doing so repeatedly allows us to chart the joint distribution, and see that <code>x</code> and <code>y</code> are indeed independent; there’s no apparent correlation in the scatter chart.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/scm-observed.png" alt="Left: Histograms of the observational distributions of x, y and z. Right: Scatter plot of the observational joint distribution of x and y. Since x and y are not causally connected except through the collider z, they are completely uncorrelated."><figcaption>Left: Histograms of the observational distributions of x, y and z. Right: Scatter plot of the observational joint distribution of x and y. Since x and y are not causally connected except through the collider z, they are completely uncorrelated.</figcaption></figure>
<p>Now that we have a model in code, we can see a selection bias effect. If we condition the data to only values of <code>z</code> (the collider node) greater than a cutoff (which we can do easily, if inefficiently, by filtering the samples to those where <code>z &gt; 2.5</code>), the previously independent <code>x</code> and <code>y</code> become negatively correlated.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/scm-conditioned.png" alt="Left: We have conditioned on z > 2.5 by filtering the samples (note the change of scale), which changes the x and y distributions; they’re both shifted right. Right: The conditional joint distribution of x and y, with a line showing a linear fit, which illustrates the induced negative correlation."><figcaption>Left: We have conditioned on z &gt; 2.5 by filtering the samples (note the change of scale), which changes the x and y distributions; they’re both shifted right. Right: The conditional joint distribution of x and y, with a line showing a linear fit, which illustrates the induced negative correlation.</figcaption></figure>
</div>
<h3 id="from-prediction-to-intervention">From prediction to intervention</h3>
<p>Now that we have some understanding of what a causal model is, we can get to the heart of causality: the difference between an observation and an <em>intervention</em>.</p>
<p>When we introduced the ladder of causation, we mentioned the notion of <em>intervention</em>, something that changes the system. This is a fundamental operation, and it is important to understand the difference between intervention and observation. It may not at first seem natural to consider intervening as a fundamental action, evoking a similar sense of confusion to when one first encounters priors in Bayesian statistics. Is an intervention subjective? Who gets to define what an intervention is?</p>
<p>Simply, an intervention is a change to the data generating process. Samples from the joint distribution of the variables in the graph may be obtained by simply “running the graph forward.” For each cause, we sample from its noise distribution and propagate that value through the SCM to calculate the resulting effects. To compute an <em>interventional</em> distribution, we force particular causes (on which we are intervening) to some value, and propagate those values through the equations of the SCM. This introduces a distribution different from the observational distribution with which we usually work.</p>
<p>There is sometimes confusion between an interventional distribution and a conditional distribution. A conditional distribution is generated by filtering an observed distribution to meet some criteria. For instance, we might want to know the loan default rate among the businesses to which we have granted a loan at a particular interest rate. This interest rate would itself likely have been determined by some model, and as such, the businesses with that rate will likely share statistical similarities.</p>
<p>The interventional distribution (when we intervene on interest rate) is fundamentally different. It is the distribution of loan defaulting if we <em>fix</em> the interest rate to a particular value, regardless of other features of the business that may warrant a different rate. This corresponds to removing all the inbound arrows to the interest rate in the causal graph; we’re forcing the value, so it no longer depends on its causal parents.</p>
<p>Clearly, not all interventions are physically possible! While we could intervene to set the interest rate, we of course would not be able to make every business a large one.</p>
<h4 id="interventions-in-code">Interventions in code</h4>
<div>
<p>It is easy to make interventions concrete with code. Returning to the collider example, to compute an interventional distribution, we could define a new sampling function where instead of drawing all variables at random, we intervene to set <code>x</code> to a particular value. Because this is an intervention, not simply conditioning (as earlier), we must make the change, then run the data generating process again.</p>
<pre><code>def sample_intervened():
  x_ = -3
  y_ = y()
  z_ = z(x_, y_)
  return x_, y_, z_
</code></pre>
<p>Performing this intervention results in a new distribution for <code>z</code>, which is different from the observational distribution that we saw earlier. Further, the relationship between x and y has changed; the joint distribution is now simply the marginal distribution of <code>y</code>, since <code>x</code> is fixed. This is a strikingly different relationship than when we simply conditioned the observational distribution.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/scm-intervened.png" alt="Left: We have intervened to fix x in the data generating process, which changes z, but not y. Right: When we intervened on x, the joint distribution of x and y became just the marginal distribution of y."><figcaption>Left: We have intervened to fix x in the data generating process, which changes z, but not y. Right: When we intervened on x, the joint distribution of x and y became just the marginal distribution of y.</figcaption></figure>
</div>
<h4 id="interventions-in-customer-churn">Interventions in customer churn</h4>
<p>In our <a href="https://ff06-2020.fastforwardlabs.com/">interpretability report</a>, we present a customer churn modeling use case. Briefly, given 20 features of the customers of a telco - things like tenure, demographic attributes, whether they have phone and internet services, and whether they have tech support - we must model their likelihood of churning within a fixed time. To do this, we turn to a dataset of customers and whether they churned in the time period. This can be modeled as straightforward binary classification, and we can use the resulting output scores as a measure of how likely a customer is to churn.</p>
<p>The model used to calculate the churn score is an ensemble of a linear model, a random forest, and a simple feed forward neural network. With appropriate hyperparameters and training procedure, such an ensemble is capable of good predictive performance. That performance is gained by exploiting subtle correlations in the data.</p>
<p>To understand the predictions made, we apply <a href="https://arxiv.org/abs/1602.04938">LIME</a>. This returns a feature importance at the local level: which features contributed to each individual prediction. To accompany the analysis, we built Refractor, an interface for exploring the feature importances. Examining these is interesting, and highlights the factors that are <em>correlated</em> with a customer being likely to churn. <a href="https://refractor.fastforwardlabs.com/">Refractor</a> suggests which features most affect the churn prediction, and allows an analyst to change customer features and see the resulting churn prediction.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/refractor.gif" alt="The Refractor prototype"><figcaption>The <a href="https://refractor.fastforwardlabs.com/">Refractor</a> prototype</figcaption></figure>
<p>Because we have a model that provides new predictions when we change the features, it is tempting to believe we can infer from this alone how to reduce churn probability. Aside from the fact that often the most important features cannot be changed by intervention (tenure, for instance), this is an incorrect interpretation of what LIME and our model provide. The correct interpretation of the prediction is the probability of churn for someone who <em>naturally</em> occurred in our dataset with those features, or, for instance, what this same customer’s churn probability will look like next year (when tenure will have naturally increased by one year), assuming none of their other features change.</p>
<p>Of course, there are some features that can be changed in reality. For instance:</p>
<ul>
<li>the telco could reduce the monthly fee for a customer, or</li>
<li>try to convince them to change contract type from monthly to yearly (one does not have to think too hard about why this changes the short-term churn probability), or</li>
<li>upgrade the service from DSL to fiber-optic.</li>
</ul>
<p>Which of these interventions would most decrease the probability that the customer churns? We don’t know. Our model alone - for all its excellent predictive accuracy - can’t tell us that, precisely because it is entirely correlative. Even a perfect model, that 100% accurately predicts which customers will churn, cannot tell us that.</p>
<p>With some common sense, we can see that a causal interpretation is not appropriate here. LIME often reports that having a faster fiber-optic broadband connection increases churn probability, relative to slower DSL. It seems unlikely that faster internet has this effect. In reality, LIME is correctly reporting that there is a <em>correlation</em> between having fiber-optic and churning, likely because of some latent factors - perhaps people who prefer faster internet are also intrinsically more willing to switch providers. This distinction of interpretation is crucial.</p>
<p>The model can only tell us what <strong>statistical dependencies</strong> exist in the dataset we trained it on. The training dataset was purely observational - a snapshot of a window of time with observations about those customers in it. If we select “give the customer access to tech support” in the app, the model can tell us that similar customers who also had access to tech support were less likely to churn. Our model only captures information about customers who happened to have some combination of features. It does not capture information about what happens when we <em>change</em> a customer’s features. This is an important distinction.</p>
<p>To know what would happen when we intervene to change a feature, we must compute the interventional distribution (or a point prediction), which can be very different from the observational distribution. In the case of churn, it’s likely the true causal graph is rather complex.</p>
<p>Interpretability techniques such as LIME provide important insights into models, but they are not causal insights. To make good decisions using the output of any interpretability method, we need to combine it with causal knowledge.</p>
<p>Often, this causal knowledge is not formally specified in a graph, and we simply call it “domain knowledge,” or expertise. We have emphasized what the <em>model</em> cannot do, in order to make the technical point clear, but in reality, anyone working with the model would naturally apply their own expertise. The move from that to a causal model requires formally encoding the assumptions we make all the time and verifying that the expected statistical relationships hold in our observed data (and if possible, experimenting). Doing so would give us an understanding of the cause-effect relationships in our system, and the ability to reason quantitatively about the effect of interventions.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-11.png" alt=""></figure>
<p>Constructing a useful causal model of churn is a complex undertaking, requiring both deep domain knowledge and a detailed technical understanding of causal inference.<sup><a href="#fn9" id="fnref9">[9]</a></sup> In <a href="#causality-and-invariance">Causality and Invariance</a>, we will discuss some techniques that are bridging the gap between a full causal model and the supervised learning setup we use in problems like churn prediction.</p>
<h4 id="when-do-we-need-interventions%3F">When do we need interventions?</h4>
<p>When do we need to concern ourselves with intervention and causality? If all we want to do is predict, and to do so with high accuracy (or whatever model performance metric we care about), then we should use everything at our disposal to do so. That means making use of all the variables that may correlate with the outcome we’re trying to predict, and it doesn’t matter that they don’t cause the outcome. Correlation is not causation, but correlation is still predictive,<sup><a href="#fn10" id="fnref10">[10]</a></sup> and supervised learning excels at discovering subtle correlations.</p>
<p>Some situations in which this pure supervised learning approach is useful:</p>
<ul>
<li>We want to predict when a machine in our factory will fail.</li>
<li>We want to forecast next quarter’s sales.</li>
<li>We want to identify named entities in some text.</li>
</ul>
<p>Conversely, if we want to predict the effect of an intervention, we need causal reasoning. For example:</p>
<ul>
<li>We want to know what to change about our machines to reduce the likelihood of failures.</li>
<li>We want to know how we can increase next quarter’s sales.</li>
<li>We want to know whether longer or shorter article headlines generate more clicks.<sup><a href="#fn11" id="fnref11">[11]</a></sup></li>
</ul>
<h3 id="how-do-we-know-which-graph-to-use%3F">How do we know which graph to use?</h3>
<p>Knowing the true causal structure of a problem is immensely powerful. Earlier in this chapter, we discussed three building blocks of causal graphs (direct causation, forks, and colliders) but for real problems, a graph can be arbitrarily complex.</p>
<p>The graph structure allows us to reason qualitatively about what statistical dependencies ought to hold in our data. In the absence of abundant randomized controlled trials or other experiments, qualitative thinking is necessary for causal inference. We must use our domain knowledge to construct a plausible graph to test against the data we have. It is possible to refute a causal graph by considering the statistical independence relations it implies, and matching those against the expected relations from the causal structure. For example, if two variables are connected by a common cause on which we have not conditioned, we should expect a statistical dependence between them.</p>
<div>
<h5 id="causal-discovery">Causal Discovery</h5>
<p>The independence relationships implied by a graph can be used for causal discovery. Causal discovery is the process of attempting to recover causal graphs from observational data. There are many approaches appropriate for different sets of assumptions about the graph. However, since many causal graphs can imply the same joint distribution, the best we should hope for from causal discovery is a set of plausible graphs, which, if we are fortunate, may contain the true graph. In reality, inferring the direction of causation in even a two variable system is not always possible from data alone.<sup><a href="#fn12" id="fnref12">[12]</a></sup></p>
</div>
<p>It is not, in general, possible to <em>prove</em> a causal graph, since different graphs can result in the same observed and even interventional distributions. The difficulty of confirming a causal relationship means that we should always proceed with caution when making causal claims. It is best to think of causal models as giving results <em>conditional on a set of causal assumptions</em>. Two nodes that are not directly connected in the causal graph are assumed to be independent in the data generating process, except insofar as the causal relations described above (or combinations of them) induce a statistical dependence.</p>
<p>The validity of the results depends on the validity of the assumptions. Of course, we face the same situation in all machine learning work - and it is to be expected that stronger, causal claims require stronger assumptions than merely observational claims.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-23.png" alt="Sometimes it can be difficult to establish the causal direction even in very simple graphs."><figcaption>Sometimes it can be difficult to establish the causal direction even in very simple graphs.</figcaption></figure>
<p>One case in which we may be able to write down the true causal graph is when we have ourselves created the system. For instance, a manufacturing line may have a sufficiently deterministic process that makes it possible to write down a precise graph encoding which parts move from which machine to another. If we were to model the production of faulty parts, that graph would be a good basis for the causal graph, since a machine that has not processed a given faulty part is unlikely to be responsible for the fault, and causal graphs encode exactly these independences.</p>
<h3 id="tl%3Bdr">TL;DR</h3>
<p>Causal graphical models present an intuitive and powerful means of reasoning about systems. If an application requires only pure prediction, this reasoning is not necessary, and we may apply supervised learning to exploit subtle correlations between variables and our predicted quantity of interest. However, when a prediction will be used to inform a decision that changes the system, or we want to predict for the system under intervention, we <em>must</em> reason causally  - or else likely draw incorrect conclusions. That said, behind every causal conclusion there is always a causal assumption that cannot be tested or verified by mere observation.</p>
<p>Even without a formal education in causal inference, there are advantages to the qualitative reasoning enabled by causal graphical models. Trying to write down a causal graph forces us to confront our mental model of a system, and helps to highlight potential statistical and interpretational errors. Further, it precisely encodes the independence assumptions we are making. However, these graphs could be complex and high dimensional and require close collaboration between practitioners and domain experts who have substantive knowledge of the problem.</p>
<p>In many domains, problems such as the large numbers of predictors, small sample sizes, and possible presence of unmeasured causes, remain serious impediments to practical applications of causal inference. In such cases, there is often limited background knowledge to reduce the space of alternative causal hypotheses. Even when experimental interventions are possible, performing the many thousands of experiments that would be required to discover causal relationships between thousands or tens of thousands of predictors is often not practical.</p>
<p>Given these challenges, how do we combine causal inference and machine learning? Many of the researched approaches at the intersection of ML and causal inference are motivated by the ability to apply causal inference techniques to high dimensional data, and in domains where specifying causal relationships could be difficult. In the next chapter, we will bridge this gap between structural causal models and supervised machine learning.</p>
<h2 id="causality-and-invariance">Causality and Invariance</h2>
<p>Supervised machine learning is very good at prediction, but there are useful lessons we can take from causal models even for purely predictive problems.</p>
<p>Relative to recent advancements made in the broader field of machine learning, the intersection of machine learning and causal reasoning is still in its infancy. Nonetheless, there are several emerging research directions. Here, we focus on one particularly promising path: the link between causality and invariance. Invariance is a desirable property for many machine learning systems: a model that is invariant is one that performs well in new circumstances, particularly when the underlying data distribution changes. As we will see in this chapter, invariance also provides a route to some causal insights, even when working only with observational data.</p>
<h3 id="the-great-lie-of-machine-learning">The great lie of machine learning</h3>
<p>In supervised learning, we wish to predict something that we don’t know, based on only the information that we do have. Usually, this boils down to learning a mapping between input and output.</p>
<p>To create that map, we require a dataset of input features and output targets; the number of examples required scales with the complexity of the problem. We can then fit the parameters of a learning algorithm to the dataset to minimize some loss function that we choose. For instance, if we are predicting a continuous number, like temperature, we might seek to minimize the mean squared difference between the prediction and the true measurements.</p>
<p>If we are not careful, we will <em>overfit</em> the parameters of the ML algorithm to the dataset we train on. In this context, an overfit model is one that has learned the idiosyncrasies (the spurious correlations!) of our dataset. The result is that when the model is applied to any other dataset (even one with the same data generating process), the model’s performance is poor, because it is relying on superficial features that are no longer present.</p>
<p>To avoid overfitting, we employ various regularization schemes and adjust the capacity of the model to an appropriate level. When we fit the model, we shuffle and split our data, so we may learn the parameters from one portion of the data, and validate the resulting model’s performance on another portion. This gives us confidence that the learned parameters are capturing something about all the data we have, and not merely a portion of it.</p>
<p>Whatever procedure we use (be it cross-validation, forward chaining for time series, or simpler train-test-validation splits), we are relying on a crucial assumption. The assumption is that the data points are <em>independent and identically distributed</em> ( i.i.d.). By <em>independent</em>, we mean that each data point was generated without reference to any of the others, and by <em>identically distributed</em>, we mean that the underlying distributions in the data generating process are the same for all the data points.</p>
<p>Paraphrasing <a href="https://www.youtube.com/watch?v=x1UByHT60mQ&amp;feature=youtu.be&amp;t=37m34s">Zoubin Ghahramani</a>,</p>
<blockquote>
<p>the i.i.d. assumption is the great lie of machine learning.</p>
</blockquote>
<p>Rarely are data truly independent and identically distributed. What are the ramifications of this misassumption for machine learning systems?</p>
<h3 id="dangers-of-spurious-correlations">Dangers of spurious correlations</h3>
<p>When we train a machine learning system with the i.i.d. assumption, we are implicitly assuming an underlying data generating process for that data. This data generating process defines an <em>environment</em>. Different data generating processes will result in different environments, with different underlying distributions of features and targets.</p>
<p>When the environment in which we predict differs from the environment in which our machine learning system was trained, we should expect it to perform poorly. The correlations between features and the target are different - and, as such, the model we created to map from features to target in one environment will output incorrect values of the target for the features in another environment.</p>
<p>Unfortunately, it’s rarely possible to know whether the data generating process for data at predict time (in a deployed ML system, for instance) will be the same as during training time. Even once the system is predicting in the wild, if we do not or cannot collect ground truth labels to match to the input features on which the prediction was based, we may never know.</p>
<p>This problem is not academic. <a href="https://arxiv.org/abs/1807.04975">Recognition in Terra Incognita</a> points this out in humorous fashion (see also <a href="http://people.csail.mit.edu/torralba/publications/datasets_cvpr11.pdf">Unbiased Look at Dataset Bias</a>). Both of these papers highlight that computer vision systems trained for visual recognition of objects, animals, and people can utterly fail to recognise the same objects in different contexts. A cow on the slopes of an alpine field is easily recognised, but a cow on a beach is not noticed at all, or poorly classified as a generic “mammal.”</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/terra-incognita.png" alt="Figure from Recognition in Terra Incognita, where annotations were provided by ClarifAI.com."><figcaption>Figure from <a href="https://arxiv.org/abs/1807.04975">Recognition in Terra Incognita</a>, where annotations were provided by <a href="https://www.clarifai.com/">ClarifAI.com</a>.</figcaption></figure>
<p>These failures should not come as a surprise to us! Supervised machine learning is <em>designed</em> to exploit correlations between features to gain predictive performance, and cows and alpine pastures are highly correlated. Neural networks are a very flexible class of models that encode the invariants of the dataset on which they’re trained. If cows dominantly appear on grass, we should expect this to be learned.</p>
<div>
<h5 id="when-is-a-correlation-spurious%3F">When is a correlation spurious?</h5>
<p>In supervised learning, we learn to use subtle correlations, possibly in high dimensional spaces like natural images, to make predictions. What distinguishes a genuine correlation from a spurious one? The answer depends on the intended use of the resulting model.</p>
<p>If we intend for our algorithm to work in only one environment, with very similar images, then we should use all the correlations at our disposal, including those that are very specific to our environment. However, if - as is almost always the case - we intend the algorithm to be used on new data outside of the training environment, we should consider any correlation that only holds in the training environment to be spurious. A spurious correlation is a correlation that only appears to be true due to a selection effect (such as selecting a training set!).</p>
<p>In <a href="#background%3A-causal-inference">Background: Causal Inference</a>, we saw that correlation can arise from several causal structures. In the strictest interpretation, any correlation that does not arise from direct causation could be considered spurious.</p>
<p>Unfortunately, given only a finite set of training data, it is often not possible to know which correlations are spurious. The methods in this section are intended to address precisely that problem.</p>
</div>
<p>When a machine learning algorithm relies heavily on spurious correlations for predictive performance, its performance will be poor on data from outside the dataset on which it was trained. However, that is not the only problem with spurious correlations.</p>
<p>There is an important and growing emphasis on interpretability in machine learning. A machine learning system should not only make predictions, but also provide a means of inspecting how those predictions were made. If a model is relying on spurious correlations, the feature importances (such as those calculated by <a href="https://arxiv.org/abs/1602.04938">LIME</a> or <a href="https://arxiv.org/abs/1705.07874">SHAP</a>) will be similarly spurious. No one should make decisions based on spurious explanations!</p>
<h3 id="invariance">Invariance</h3>
<p>To be confident of our predictions outside of our training and testing datasets, we need a model that is robust to distributional shifts away from the training set. Such a model would have learned a representation which ignores dataset-specific correlations, and instead relies upon features that affect the target in all environments.</p>
<p>How can we go about creating such a model? We could simply train our model with data from multiple environments, as we often do in machine learning (playing fast and loose with the i.i.d. assumption). However, doing so naively would provide us with a model that can only generalize to the environments it has seen (and interpolations of them, if we use a robust objective).<sup><a href="#fn13" id="fnref13">[13]</a></sup> We wish our model to generalize beyond the limited set of environments we can access for training, and indeed extrapolate to new and unseen (perhaps unforeseen) environments. The property we are looking for - performing optimally in all environments - is called invariance.</p>
<p>The connection between causality and invariance is well established. In fact, causal relationships are - by their nature - invariant. The way many intuitive causal relationships are established is by observing that the relationship holds all the time, in all circumstances.</p>
<p>Consider how physical laws are discovered. They are found by performing a series of experiments in different conditions, and monitoring which relationships hold, and what their functional form is. In the process of discovering nature’s laws, we will perform some tests that do not show the expected result. In cases where a law does not hold, this gives us information to refine the law to something that is invariant across environments.<sup><a href="#fn14" id="fnref14">[14]</a></sup></p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-12.png" alt="We learn causal relationships by observing under different experimental conditions. Causal relationships are those that are invariant across the environments created by these conditions."><figcaption>We learn causal relationships by observing under different experimental conditions. Causal relationships are those that are invariant across the environments created by these conditions.</figcaption></figure>
<p>For example, water boils at 100° Celsius (212° Fahrenheit). We could observe that everywhere, and write a simple causal graph: temperature → water boiling. We have learned a relationship that is invariant across all the environments we have observed.</p>
<p>Then, a new experiment conducted on top of a tall mountain reveals that on the mountain, water boils at a slightly lower temperature. After some more experimentation, we improve our causal model, by realising that in fact, both temperature and pressure affect the boiling point of water, and the true invariant relationship is more complicated.</p>
<p>The mathematics of causality make the notion of invariance and environments precise. Environments are defined by interventions in the causal graph. Each intervention changes the data generating process, such that the correlations between variables in the graph may be different (see <a href="#from-prediction-to-intervention">From prediction to intervention</a>). However, direct causal relationships are invariant relationships: if a node in the causal graph depends only on three variables, and our causal model is correct, it will depend on those three variables, and in the same way, regardless of any interventions. It may be that an intervention restricts the values that the causal variables take, but the relationship itself is not changed. Changing the arguments to a function does not change the function itself.</p>
<h4 id="invariance-and-machine-learning">Invariance and machine learning</h4>
<p>In the machine learning setting, we are mostly concerned with using features to predict a target. As such, we tend to select features for their predictive performance. In contrast, causal graphs are constructed based on domain knowledge and statistical independence relations, and thus encode a much richer dependency structure. However, we are not always interested in the entire causal graph. We may be interested only in the causes of a particular target variable. This puts us closer to familiar machine learning territory.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-14.png" alt="In supervised learning, we often use all available variables (or a subset selected for predictive performance) to predict an outcome. With structural causal models, we encode a much richer dependency structure between variables."><figcaption>In supervised learning, we often use all available variables (or a subset selected for predictive performance) to predict an outcome. With structural causal models, we encode a much richer dependency structure between variables.</figcaption></figure>
<p>We will now examine two approaches to combining causal invariance and machine learning. The first, invariant causal prediction, uses the notion of invariance to infer the direct causes of a variable of interest. This restricted form of causal discovery (working out the structure of a small part of the graph in which we are interested) is appropriate for problems with well defined variables where a structural causal model (or at least causal graph) could be created - in principle, if not in practice.</p>
<p>Not all problems are amenable to SCMs. In the following section, we describe invariant risk minimization, where we forego the causal graph and seek to find a predictor that is invariant across multiple environments. We don’t learn anything about the graph structure from this procedure, but we do get a predictor with greatly improved out-of-distribution generalization.</p>
<h3 id="invariant-causal-prediction">Invariant Causal Prediction</h3>
<p><a href="https://arxiv.org/abs/1501.01332">Invariant causal prediction</a> (ICP) addresses the task of invariant prediction explicitly in the framework of structural causal models.</p>
<p>Often, the quantity we are ultimately concerned with in a causal analysis is the causal effect of an intervention: what is the difference in the target quantity when another variable is changed?<sup><a href="#fn15" id="fnref15">[15]</a></sup> To calculate that, we either need to hold some other variables constant, or else account for the fact that they have changed. If we are only interested in the causes that affect a particular target, we do not need to construct the whole graph, but rather only determine which factors are the true direct causes of the target. Once we know that, we can answer causal questions, like how strongly each variable contributes to the effect, or the causal effect of changing one of the input variables.</p>
<p>The key insight offered by ICP is that because direct causal relationships are invariant, we can use that to determine the causal parents (the direct causes). The set-up is similar to that of machine learning; we have some input features, and we’d like a model of an output target. The difference from supervised learning is that the goal is not “performance at predicting the target variable.” In ICP, we aim to discover the direct causes of a given variable - the variables that point directly into the target in the causal graph.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-15.png" alt="We are not always interested in the full causal graph, and instead only seek to find the direct causes of a given target variable. This brings some of the advantages of a causal model into the supervised learning paradigm."><figcaption>We are not always interested in the full causal graph, and instead only seek to find the direct causes of a given target variable. This brings some of the advantages of a causal model into the supervised learning paradigm.</figcaption></figure>
<p>To use ICP, we take a target variable of interest, and construct a plausible list of the potential direct causes of that variable. Then we must define environments for the problem: each environment is a dataset. In the language of SCMs, each environment corresponds to data observed when a particular intervention somewhere in the graph was active. We can reason about this even without specifying the whole graph, or even which particular intervention was active, as long as we can separate the data into environments. In practice, we often take an observed variable to be the environment variable, when it could plausibly be so.</p>
<p>For instance, perhaps we are predicting sales volume in retail, and want to discern what store features causally impact sales. The target is sales volume, and the potential causes would be features like store size, number of nearby competitors, level of staffing, and so on.</p>
<p>Environments might be different counties (or even countries) - something that is unlikely to impact the sales directly, but which may impact the <em>features</em> that impact the sales. For instance, different places will have different populations, and population density is a possible cause of sales volume. Importantly, the environment cannot be a descendent of the target variable.<sup><a href="#fn16" id="fnref16">[16]</a></sup></p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-16.png" alt="We fit a model in multiple environments, and monitor which features are consistently predictive."><figcaption>We fit a model in multiple environments, and monitor which features are consistently predictive.</figcaption></figure>
<p>To apply ICP, we first consider a subset of features. We then fit a linear (Gaussian) regression from this subset to the target in each environment we have defined. If the model does not change between environments (which can be assessed either via the coefficients or a check on residuals), we have found a set of features that appear to result in an invariant predictor. We iterate over subsets of features combinatorially. Features that appear in a model that is invariant are plausible causes of the target variable. The intersection of these sets of plausible causes (i.e., the features which are predictive in all environments) is then a subset of the true direct causes.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-17.png" alt="The features that are consistently predictive of a target are likely the causal parents in the (unknown!) causal graph."><figcaption>The features that are consistently predictive of a target are likely the causal parents in the (unknown!) causal graph.</figcaption></figure>
<p>In machine learning terms, ICP is essentially a feature selection method, where the features selected are very likely to be the direct causes of the target. The model built atop those features can be interpreted causally: a high coefficient for a feature means that feature has a high causal effect on the target, and changes in those features should result in the predicted change in the target.</p>
<p>Naturally, there are some caveats and assumptions. In particular, we must assume there is no unobserved confounding between the features and the target (recall that a confounder is a common cause of the feature and target). If there are known confounders, we must make some adjustments to account for them, as detailed in the <a href="https://arxiv.org/abs/1501.01332">ICP paper</a>. The authors provide an R package, <a href="https://cran.r-project.org/web/packages/InvariantCausalPrediction/index.html">InvariantCausalPrediction</a>, implementing the methods.</p>
<p>The restriction of using a linear Gaussian model - and that environments be discrete, rather than defined by the value of a continuous variable - are removed by nonlinear ICP.<sup><a href="#fn17" id="fnref17">[17]</a></sup> In the nonlinear case, we replace comparing residuals or coefficients with conditional independence tests.<sup><a href="#fn18" id="fnref18">[18]</a></sup></p>
<h3 id="invariant-risk-minimization">Invariant Risk Minimization</h3>
<p>When using Invariant Causal Prediction, we avoid writing the full structural causal model, or even the full graph of the system we are modeling, but we must still think about it.</p>
<p>For many problems, it’s difficult to even attempt drawing a causal graph. While structural causal models provide a complete framework for causal inference, it is often hard to encode known physical laws (such as Newton’s gravitation, or the ideal gas law) as causal graphs. In familiar machine learning territory, how does one model the causal relationships between individual pixels and a target prediction? This is one of the motivating questions behind the paper <a href="https://arxiv.org/abs/1907.02893">Invariant Risk Minimization</a> (IRM). In place of structured graphs, the authors elevate invariance to the defining feature of causality.</p>
<p>They also make the connection between invariance and causality well:</p>
<blockquote>
<p>“If both Newton’s apple and the planets obey the same equations, chances are that gravitation is a thing.”
– <a href="https://arxiv.org/abs/1907.02893">IRM</a> authors</p>
</blockquote>
<p>Like ICP, IRM uses the idea of training in multiple environments. However, unlike ICP, IRM is not concerned with retrieving the causal parents of the target in a causal graph. Rather, IRM focusses on out-of-distribution generalization: the performance of a predictive model when faced with a new environment. The technique proposed aims to create a data representation, on which a classifier or regressor can perform optimally in all environments. The paper itself describes the IRM principle:</p>
<blockquote>
<p>“To learn invariances across environments, find a data representation such that the optimal classifier on top of that representation matches for all environments.”
– <a href="https://arxiv.org/abs/1907.02893">IRM</a> authors</p>
</blockquote>
<p>Said differently, the idea is that there is a latent causal structure behind the problem we’re learning, and the task is to recover a representation that encodes the part of that structure that affects the target. This is different from selecting features, as in Invariant Causal Prediction. In particular, it provides a bridge from very low level features (such as individual pixels) to a representation encoding high level concepts (such as cows).</p>
<h4 id="the-causal-direction">The causal direction</h4>
<p>The idea of a latent causal system generating observed features is particularly useful as a view of computer vision problems. Computer vision researchers have long studied the generative processes involved in moving from real world objects to pixel representations.<sup><a href="#fn19" id="fnref19">[19]</a></sup> It’s instructive to inspect the causal structure of a dataset of cow pictures.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-13.png" alt="When the features are the causes of the target, we say we are learning in the causal direction. When effects are the features, we are learning in the anti-causal direction."><figcaption>When the features are the causes of the target, we say we are learning in the causal direction. When effects are the features, we are learning in the anti-causal direction.</figcaption></figure>
<p>In nature, cows exist in fields and on beaches, and we have an intuitive understanding that the cow itself and the ground are different things. A neural network trying to predict the presence of a cow in an image could be called an “anti-causal” learning problem, because the direction of causation is the opposite of the direction of prediction. The presence of a cow causes certain pixel patterns, but pixels are the input to the network, and the presence of a cow is the output.</p>
<p>However, a further sophistication can be added: the dataset on which we train a neural network is not learning from nature, but rather from annotations provided by humans. This changes the causal direction: we are now learning the effect from the cause, since those annotations are caused by the pixels of the image. This is the view taken by IRM,<sup><a href="#fn20" id="fnref20">[20]</a></sup> which thus interprets supervised learning from images as being a causal (rather than anti-causal) problem.<sup><a href="#fn21" id="fnref21">[21]</a></sup></p>
<p>Not all supervised learning problems are causal. Anti-causal supervised learning problems arise when the label is not provided based on the features, but by some other mechanism that causes the features. For example, in medical imaging, we could obtain a label without reference to the image itself by observing the case over time (this is not a recommended approach for treatment, of course).</p>
<p>Learning in the causal direction explains some of the success of supervised learning - there is a chance that it can recover invariant representations without modification. Any supervised learning algorithm is learning how to combine features to predict the target. If the learning direction is causal, each input is a potential cause of the output, and it’s possible that the features learned will be the true causes. The modifications that invariant risk minimization makes to the learning procedure improve the chance by specifically promoting invariance.</p>
<h3 id="how-irm-works">How IRM works</h3>
<p>To learn an invariant predictor, we must provide the IRM algorithm with data from multiple environments. As in ICP, these environments take the form of datasets - and, as such, the environments must be discrete. We need not specify the graphical or interventional structure associated with the environments. The motivating example of the IRM paper asks us to consider a machine learning system to distinguish cows from camels, highlighting a similar problem to that which <a href="https://arxiv.org/abs/1807.04975">Recognition in Terra Incognita</a> does: animals being classified based on their environment, rather than on the animal itself. In this case, cows on sand may be misclassified as camels, due to the spurious correlations absorbed by computer vision systems.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-18.png" alt="In the IRM setup, we feed the algorithm data from multiple environments, and we must be explicit about which environment a data point belongs to."><figcaption>In the IRM setup, we feed the algorithm data from multiple environments, and we must be explicit about which environment a data point belongs to.</figcaption></figure>
<p>Simply providing data from multiple environments is not enough. The problem of learning the optimal classifier in multiple environments is a bi-level constrained optimization problem, in which we must simultaneously find the optimal data representation and optimal classifier across multiple separate datasets. IRM reduces the problem to a single optimization loop, with the trick of using a constant classifier and introducing a new penalty term to the loss function.</p>
<pre><code>IRM loss = sum over environments (error + penalty)
</code></pre>
<p>The <code>error</code> is the usual error we would use for the problem at hand - for example, the cross entropy for a classification problem - calculated on each environment. The technical definition of the new <code>penalty</code> term is the squared gradient norm with respect to a constant classifier, but it has an intuitive explanation. While the error measures how well the model is performing in each environment, the penalty measures how much the performance could be improved in each environment with one gradient step.</p>
<p>By including the penalty term in the loss, we punish high gradients (situations in which a large improvement in an environment would be possible with one more epoch of learning). The result is a model with optimal performance in all environments. Without the IRM penalty, a model could minimize the loss by performing extremely well in just one environment, and poorly in others. Adding a term to account for the model having a low gradient (roughly, it has converged) in each environment ensures that the learning is balanced between environments.</p>
<p>To understand the IRM paradigm, we can perform a thought experiment. Imagine we have a dataset of cows and camels, and we’d like to learn to classify them as such. We separate out the dataset by the geolocation of photos - those taken in grassy areas form one environment, and those taken in deserts form another.</p>
<p>As a baseline, we perform regular supervised learning to learn a binary classifier between cows and camels. The learning principle at work in supervised learning is referred to as <em>empirical risk minimization</em>, (ERM); we’re just seeking to minimize the usual cross-entropy loss.<sup><a href="#fn22" id="fnref22">[22]</a></sup> We’ll surely find that we can get excellent predictive performance on these two environments, because we have explicitly provided data from both.</p>
<p>The trouble arises when we want to identify a cow on snow, and find that our classifier did not <em>really</em> learn to identify a cow; it learned to identify grass. The holdout performance of our model in any new environment we haven’t trained on will be poor.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-19.png" alt="If we rely on empirical risk minimization, we learn spurious correlations between animals and their environments."><figcaption>If we rely on empirical risk minimization, we learn spurious correlations between animals and their environments.</figcaption></figure>
<p>With IRM, we perform the training across (at least) two environments, and include the penalty term for each in the loss. We’ll almost certainly find that our performance in the training environments is reduced. However, because we have encouraged the learning of invariant features that transfer across environments, we’re more likely to be able to identify cows on snow. In fact, the very reason our performance in training is reduced is that we’ve not absorbed so many spurious correlations that would hurt prediction in new environments.</p>
<p>It is impossible to guarantee that a model trained with IRM learns <em>no</em> spurious correlations. That depends entirely on the environments provided. If a particular feature is a useful discriminator in all environments, it may well be learned as an invariant feature, even if in reality it is spurious. As such, access to sufficiently diverse environments is paramount for IRM to succeed.</p>
<p>However, we should not be reckless in labeling something as an environment. Both ICP and IRM note that splitting on arbitrary variables in observational data can create diverse environments while destroying the very invariances we wish to learn. While IRM promotes invariance as the primary feature of causality, it pays to hold a structural model in the back of one’s mind, and ask if an environment definition makes sense as something that would alter the data-generating process.</p>
<h4 id="considerations-for-applying-irm">Considerations for applying IRM</h4>
<p>IRM buys us extrapolation powers to new datasets, where independent and identically distributed supervised learning can (at best) interpolate between them. Using IRM to construct models improves their generalization properties by explicitly promoting performance across multiple environments, and leaves us with a new, closer-to-causal representation of the input features. Of course, this representation may not be perfect (IRM is an optimization-based procedure, and we will never know if we have found the true minimum risk across all environments) but it should be a step towards latent causal structure. This means that we can use our model to predict based on true, causal correlations, rather than spurious, environment-specific correlations.</p>
<p>However, there is no panacea, and IRM does come with some challenges.</p>
<p>Often, the dataset that we use in a machine learning project is collected well ahead of time, and may have been collected for an entirely different purpose. Even when a well-labeled dataset that is amenable to the problem exists, it is seldom accompanied by detailed metadata (by which we mean “information about the information”). As such, we often do not have information about the environment in which the data was collected.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-20.png" alt="Most datasets are collected in a variety of environments, and without the metadata necessary to separate them. This presents a challenge for invariance-based approaches."><figcaption>Most datasets are collected in a variety of environments, and without the metadata necessary to separate them. This presents a challenge for invariance-based approaches.</figcaption></figure>
<p>Another challenge is finding data from sufficiently diverse environments. If the environments are similar, IRM will be unlikely to learn features that generalize to environments that are different. This is both a blessing and a curse - on the one hand, we do not need to have perfectly separated environments to benefit from IRM, but on the other hand, we are limited by the diversity of environments. If a feature appears to be a good predictor in all the environments we have, IRM will not be able to distinguish that from a true causal feature. In general, the more environments we have, and the more diverse they are, the better IRM will do at learning an invariant predictor, and the closer we will get to a causal representation.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-21.png" alt="IRM relies on representative data from diverse environments. If we cannot collect enough data from sufficiently diverse environments, we may still learn spurious correlations."><figcaption>IRM relies on representative data from diverse environments. If we cannot collect enough data from sufficiently diverse environments, we may still learn spurious correlations.</figcaption></figure>
<p>No model is perfect, and whether or not one is appropriate to use depends on the objective. IRM is more likely to produce an invariant predictor, with good out-of-distribution performance, than empirical risk minimization (regular supervised learning), but using IRM will come at the expense of predictive performance in the training environment.</p>
<p>It’s entirely possible that for a given application, we may be very sure that the data in the eventual test distribution (“in the wild”) will be distributed in the same way as our training data. Further, we may know that all we want to do with the resulting model is predict, not intervene. If both these things are true, we should stick to supervised learning with empirical risk minimization and exploit all the spurious correlations we can.</p>
<h2 id="prototype">Prototype</h2>
<p>The promise of Invariant Risk Minimization (greatly improved out-of-distribution generalization using a representation that is closer-to-causal) is tempting. The IRM paper performs some experiments that clearly show the method works when applied to an artificial structural causal model. Further, an experiment in which an artificial spurious correlation is injected into the MNIST dataset (by coloring the images) is detailed, and works.</p>
<p>In order to gain a better understanding of the algorithm and investigate further, we wanted to test the same technique in a less artificial scenario: on a natural image dataset.</p>
<h3 id="the-wildcam-dataset">The Wildcam dataset</h3>
<p>The <a href="https://www.kaggle.com/c/iwildcam-2019-fgvc6">iWildCam 2019 dataset</a> (from The iWildCam 2019 Challenge Dataset) consists of wildlife images taken using camera traps. In particular, the dataset contains the Caltech Camera Traps (CCT) dataset, on which we focus. The CCT dataset contains 292,732 images, with each image labeled as containing one of 13 animals, or none. The images are collected from 143 locations, and feature a variety of weather conditions and all times of day. The challenge is to identify the animal present in the image.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/wildcam-coyote-raccoon.png" alt="Left, a coyote in its natural environment. Right, a raccoon in the same location at night. Image credit: The iWildCam 2019 Challenge Dataset, used under the Community Data License Agreement."><figcaption>Left, a coyote in its natural environment. Right, a raccoon in the same location at night. Image credit: The <a href="https://arxiv.org/abs/1907.07617">iWildCam 2019 Challenge Dataset</a>, used under the <a href="https://cdla.io/permissive-1-0/">Community Data License Agreement</a>.</figcaption></figure>
<h3 id="experimental-setup">Experimental setup</h3>
<p>This setup maps naturally to the environmental splits used in IRM. Each camera trap location is a distinct physical environment which is roughly consistent, allowing for seasonal, weather, and day/night patterns. No two environments are the same, though the camera locations are spread around roughly the same geographic region (the American Southwest).</p>
<p>The objects of interest in the dataset are animals, which are basically invariant across environments: a raccoon looks like a raccoon in the mountains and in your backyard (though the particular raccoon may be different). The images are not split evenly between environments, since there is more animal activity in some places than others. Nor are the animal species evenly distributed among cameras. Some cameras will primarily produce images of one species or another, depending on the animals active in the area.</p>
<p>If we were to naively train a model using empirical risk on a subset of cameras, we could well end up learning exactly those class imbalances. If 99% of the images from camera 1 are labeled as deer, then we could have a 99% accurate classifier by learning to recognize the fallen tree that is present only in camera 1, rather than the deer themselves. Clearly such a classifier has not really learned to recognize deer, and would be useless for predicting in another environment.</p>
<p>We want to learn to recognize the animals themselves. The IRM setup seems ideally suited to address this challenge.</p>
<p>To validate the approach, we restricted our experiment to only three cameras and two animal species, which were randomly chosen. Of the three cameras, two were used as training environments, and one as a held-out environment for testing. The task was binary classification: distinguish coyotes from raccoons. We used <a href="https://arxiv.org/abs/1512.03385">ResNet18</a>, a pretrained classifier trained on the much larger ImageNet dataset, as a feature extractor with a final fully connected layer with sigmoid output, which we tuned to the problem at hand.</p>
<p>Each of the environments contained images of both coyotes and racoons. Even this reduced dataset exhibited several challenges typical to real world computer vision: some images were dark, some were blurred, some were labeled as containing an animal when only the foot of the animal was visible, and some featured nothing but a patch of fur covering the lens. We saw some success simply ignoring these problems, but ultimately manually selected only those images clearly showing an identifiable coyote or raccoon.</p>
<h3 id="results">Results</h3>
<p>When tackling any supervised learning problem, it’s a good idea to set up a simple baseline against which to compare performance. In the case of a binary classifier, an appropriate baseline model is to always predict the majority class of the training set. The three environments had a class balance as shown in the table below. The majority class in both train environments is coyote, so our baseline accuracy is the accuracy if we always predict the animal is a coyote, regardless of environment or input image.</p>
<div><table>
<thead>
<tr>
<th></th>
<th>Train environment 1</th>
<th>Train environment 2</th>
<th>Test</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coyotes</td>
<td>582</td>
<td>512</td>
<td>144</td>
</tr>
<tr>
<td>Raccoons</td>
<td>276</td>
<td>241</td>
<td>378</td>
</tr>
<tr>
<td>Baseline accuracy</td>
<td>68%</td>
<td>68%</td>
<td>28%</td>
</tr>
</tbody>
</table></div>
<p>When we treated the problem with empirical risk minimization (minimizing the cross-entropy between classes), we found good performance in the train environments, but very poor performance in the test environment. We report the metrics over 120 epochs of training in the table below. The best test accuracy is achieved at epoch 40, after which ERM (empirical risk minimization) begins to overfit. In the case of IRM (invariant risk minimization), we paid a small price in train set accuracy, but achieved much better test results - again, reporting the highest test accuracy achieved in 120 epochs (at epoch 120).</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/erm-vs-irm-table.png" alt="Table comparing metrics on the combined train set and test set for empirical risk minimization (ERM) and invariant risk minimization (IRM)."><figcaption>Table comparing metrics on the combined train set and test set for empirical risk minimization (ERM) and invariant risk minimization (IRM).</figcaption></figure>
<p>ERM outperforms the baseline in all environments, but not by too much in the new test environment. This can be attributed to the learning of spurious correlations. The network was able to effectively distinguish between raccoons and coyotes in the training environments, but the features it relied upon to do so were not general enough to help prediction much in the test environment.</p>
<p>In contrast, IRM loses a single percentage point of accuracy in the train environments, but performs almost as well in the test environment. The feature representation IRM constructs has translated between different environments effectively, and proves an effective discriminator.</p>
<p>As a practical point, we found that IRM worked best when the additional IRM penalty term was not added to the loss until the point at which ERM had reached its best performance - in this case the 40th training epoch. As such, ERM and IRM had identical training routines and performance until this point. When we introduced the IRM penalty, the IRM procedure continued to learn and gain out-of-distribution generalization capability, whereas ERM began to overfit. By the 120th epoch, IRM had the accuracy reported above, whereas ERM had achieved 91% in the combined training environments, at the cost of reducing its test accuracy by a few percentage points to 33%.</p>
<h4 id="interpretability">Interpretability</h4>
<p>IRM yields impressive results, especially considering how hard it is to learn from these images. It has a clear and significant improvement in when compared to ERM in a new environment. In this section, we examine a few concrete examples of successes and failures of our prototype model and our speculations as to why they may be.</p>
<p>It would be nice to have a better sense of whether IRM has learned invariant features. By that we mean, whether it has learned to spot a raccoon’s long bushy tail or a coyote’s slender head, instead of the terrain or foliage in the image. Understanding which parts of the image contribute towards IRM’s performance is a powerful proposition. The classification task itself is hard: if you closely look at some of the images in the Wildcam dataset, at a first glance it’s even hard for us, humans, to point out where exactly the animal is. An interpretability technique like Local Interpretable Model-agnostic Explanations (<a href="https://arxiv.org/abs/1602.04938">LIME</a>) provides valuable insights into how that classification is working.</p>
<p>LIME is an explanation technique that can be applied to almost any type of classification model — our report <a href="https://ff06-2020.fastforwardlabs.com/">FF06: Interpretability</a> discusses these possibilities — but here we will consider its application to image data. LIME is a way to understand how different parts of an input affect the output of a model. This is accomplished, essentially, by turning the dials of the input and observing the effect on the output.</p>
<p>Let’s first try and understand how LIME works at a high level - including what inputs we need to provide, and what to expect as output - through a sample image in the test set. The image on the left of the figure below is a sample raw image of a coyote with dimensions height=747 and width=1024, as were all images in the dataset.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/coyote-resized.png" alt="Left: a raw Wildcam image. Right: Having been cropped and scaled to the input dimensions required by ResNet18."><figcaption>Left: a raw Wildcam image. Right: Having been cropped and scaled to the input dimensions required by ResNet18.</figcaption></figure>
<p>To use the IRM model, we must first perform some image transformations like resizing, cropping, and normalization - using the same transformations that we did when training the model. The input image then appears as shown on the right of the figure above, a normalized, 224 * 224 image. The transformed image when scored by the IRM model outputs a probability of 98% (0.98) for the coyote class! So yes, our model is pretty confident of its prediction.</p>
<p>Now, let’s see how LIME works on this image. First, LIME constructs a local linear model, and makes a prediction for the image. For the example image, the predicted score is 0.95, pretty close to the IRM model. When trying to explain the prediction, LIME uses interpretable representations. For images, interpretable representations are basically contiguous patches of similar pixels called superpixels. The superpixels for an image are generated by a standard algorithm, QuickShift, in the LIME implementation. The left panel in the figure below shows all of the 34 superpixels generated by LIME for the example image.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/lime-masks.png" alt="LIME masks random combinations of superpixels, generated by QuickShift, to build a local linear model."><figcaption>LIME masks random combinations of superpixels, generated by QuickShift, to build a local linear model.</figcaption></figure>
<p>It then creates versions of the original image by randomly masking different combinations of the superpixels as shown in the middle and right panes of the above figure. Each random set of masked superpixels is one perturbation of the image. The modeler chooses the number of perturbations; in our case, we used 1000 perturbations of the original image. LIME then builds a regression model on all these perturbed images and determines the superpixels that contributed most towards the prediction, based on their weights.</p>
<p>The figure below shows the superpixel explanations (with the rest of the image grayed out) for the top 12 features that contribute towards the prediction of the coyote classification. While there are quite a few features that are mostly spurious covering the foliage or terrain, one of them covers the entire body of the coyote. Looking at these explanations provides an alternative way of assessing the IRM model and can enhance our trust that the model is learning to rely on sensible features.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/irm-top-12.png" alt="The non-grayed-out pixels correspond to the top 12 superpixels that contribute positively to the Coyote classification for the IRM model."><figcaption>The non-grayed-out pixels correspond to the top 12 superpixels that contribute positively to the Coyote classification for the IRM model.</figcaption></figure>
<p>Now when we generate the top 12 LIME explanations for the same image but based on the ERM model, they seem to capture more of the surroundings, rather than any of the coyote’s body parts.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/erm-top-12.png" alt="The non-grayed-out pixels correspond to the top 12 superpixels that contribute positively to the Coyote classification for the ERM model. In this case, they didn’t catch much of the coyote."><figcaption>The non-grayed-out pixels correspond to the top 12 superpixels that contribute positively to the Coyote classification for the ERM model. In this case, they didn’t catch much of the coyote.</figcaption></figure>
<p>And then there are instances where LIME explanations seem to rely on spurious features. For example, in the figure below, the original image is classified as a coyote by the IRM model with a probability of 72% (0.72), whereas the LIME score is close to 0.53. The superpixels contributing towards the classification for both the IRM and ERM models usually cover the terrain or foliage, though some outline the coyote’s body.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/spurious-coyote.png" alt="In this instance, both models seem to be relying on environmental features to predict Coyote."><figcaption>In this instance, both models seem to be relying on environmental features to predict Coyote.</figcaption></figure>
<p>We observe that the explanations make more intuitive sense when the LIME score is close to the model score.</p>
<p>IRM can only learn to be invariant with respect to the invariants that the environments encode. If there are spurious correlations that are the same across environments, IRM will not distinguish them from invariant features.</p>
<p>One feature that appears invariant in this dataset is the day or night cycle. Raccoons appear exclusively at night, and IRM could well learn that night means raccoon, and rely on it heavily. This correlation is spurious; a raccoon is still a raccoon during the day! However, we would need more environments, including images of raccoons in the daytime, to disentangle that.</p>
<p>The representation that IRM extracts from an environment should theoretically be closer to encoding the latent causal structure of the problem than that which ERM extracts. In our scenario, we might expect that IRM learns to focus more on the actual animal in the picture, since the presence of the animal is the cause of a given annotation. The animals change little between environments, whereas environmental features (like foliage) are completely different at different camera trap locations. Thus, the causal features ought to be invariant between environments.</p>
<p>That said, although the IRM results appear promising for some samples, it is hard to confirm that there is an obvious pattern, and this can be attributed to both the model and the interpretability technique. We chose to train only the last layer of ResNet18 to come up with the IRM model. This choice has an inherent drawback: the capacity for feature learning is low. As such, we wouldn’t expect perfect results, since it’s unlikely that the pretrained ResNet representations map perfectly to raccoons and coyotes.<sup><a href="#fn23" id="fnref23">[23]</a></sup></p>
<p>Further, although an explanation of an image provides some reassurance of the quality of the model, it’s probably still insufficient to provide an overall picture of the <em>kind</em> of features a given model is using, aggregated from all the individual explanations. And even though explanations for multiple images are insightful, these have to be judiciously selected. When it comes to text or tabular data, there are ways to determine the global feature importances, because the features in tabular data or vocabulary stay consistent across all the data points. The superpixels of an image cannot be consistent across all the images, which makes it really hard to assess whether the explanations make sense. Developing tools to understand large image datasets is a worthy endeavour!</p>
<h3 id="product%3A-scene">Product: Scene</h3>
<figure><img src="https://ff13.fastforwardlabs.com/figures/scene.png" alt="The Scene prototype"><figcaption>The <a href="https://scene.fastforwardlabs.com/">Scene prototype</a></figcaption></figure>
<p>To accompany this report, we built a prototype called <a href="https://scene.fastforwardlabs.com/">Scene</a> that takes you on a guided tour through the dataset, models, and results of our experiment. With Scene, we really wanted to give people a feel for the images that make up the dataset. Each panel of the tour features 16 images from the dataset, cropped and resized to the same dimensions of the images that the model is trained on. Many of the images featured are randomly sampled from the dataset when we generate the page, while others we specifically selected to use as examples. We hope that the amount and variety of images shown helps people get an intuitive feel for the dataset.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/scene-all.png" alt="View all the images in the dataset on the all page."><figcaption>View all the images in the dataset on the <a href="https://scene.fastforwardlabs.com/all">all</a> page.</figcaption></figure>
<p>If you want to go even deeper, we included an <a href="https://scene.fastforwardlabs.com/all">all</a> page, which shows all 2,133 images in the dataset, along with the predictions and interpretability visualizations for each model. It’s nice to be able to use these visualizations to check intuitions (like which features are important to each model) with your own eyes. Of course, even having access to all the images doesn’t mean you can see “the big picture.” It’s difficult to hold everything you’ve seen in your head as you scroll through. If you’re not careful, you’ll end up generalizing the patterns you’ve seen most recently to the entire dataset. This is the challenge of visualizing the scale of the data that machine learning systems take in. Other techniques, like embeddings (as seen in our <a href="https://activelearner.fastforwardlabs.com/">Active Learner</a> prototype) can help you visualize patterns, but then you lose some of the detail gained by being able to see the images up close. No one technique can give you the whole picture; data visualization requires a variety of techniques.</p>
<p>Generating such a large number of images, complete with text labels and interpretability overlays, was an interesting technical challenge. Originally, we’d planned to have Scene animate transitions between the original image and the interpretability overlays. To do this efficiently in a browser, you generate a “sprite sheet” - a large image that contains all the different animation states you’ll transition through (a technique borrowed from video games). It was while we were generating the sprite sheets that we decided that, rather than transitioning through them one at a time, it would be more effective to show the entire sheet. Having more images visible together made comparisons easier and the scale of the dataset more clear. We ended up using the <a href="https://activelearner.fastforwardlabs.com/">node-canvas</a> package to crop and place the images, overlay the interpretability layers, and apply the labels through a node script. Since we do all the work of generating images locally, we guarantee the user as snappy an experience as possible. Static site generation has seen renewed interest as a web-development strategy, and could be especially useful for large-scale data-visualization.</p>
<h2 id="landscape">Landscape</h2>
<p>Causality spans a broad area of topics, including using causal insights to improve machine learning methods, adapting it for high-dimensional datasets and applying them for better data-driven decision making in real-world contexts. We also discussed in <a href="#causality-and-invariance">Causality and Invariance</a> how the collected data is rarely an accurate reflection of the population, and hence may fail to generalize in different environments or new datasets. Methods based on invariance show promise in addressing out-of-distribution generalization.</p>
<h3 id="use-cases">Use Cases</h3>
<p>As we demonstrated in the <a href="#prototype">Prototype</a> chapter, Invariant Risk Minimization is particularly well suited to image problems in diverse physical environments. However, an environment need not mean only the scenery in an image, and when it does, it need not be fixed to a single value. Here we suggest some applications in and beyond computer vision.</p>
<h4 id="healthcare">Healthcare</h4>
<p>In healthcare, medical images have to be manually annotated by radiologists to identify abnormalities. These annotated images are then used to train and build diagnostic models. Often the devices (like MRI scanners) which generate these medical images exhibit some kind of variation.<sup><a href="#fn24" id="fnref24">[24]</a></sup> That is, due to mechanical configurations, vendor differences, or any number of other reasons, the images that are generated by one MRI scanner could be systematically different from another for the same patient. As such, a diagnostic model that was built on the images generated by an old MRI scanner may perform poorly when tested on the images generated by a new scanner. One way to solve this problem is to have the radiologist annotate the images generated by the new scanner and then retrain the model. But that could be expensive and time-consuming. Plus, this isn’t a permanent solution; every time there’s a new scanner or changes to the configuration, it would be necessary to retrain the existing model with an entirely new set of images.</p>
<p>A diagnostic model based on invariant prediction that treats scanners as environments could be immune to these noisy device variations and change its decisions accordingly. This could save the time and money needed to annotate images from the new scanner.</p>
<h4 id="robotics">Robotics</h4>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-26.png" alt="Autonomous systems trained in the lab or in limited environments will struggle to adapt to the diversity present in the real world."><figcaption>Autonomous systems trained in the lab or in limited environments will struggle to adapt to the diversity present in the real world.</figcaption></figure>
<p>Autonomous systems need to detect and adapt to different environments. These systems rely on sophisticated sensors, cameras, and large amounts of labeled and diverse real-world datasets (which are difficult to acquire). Take, for example, the task of autonomously following a man-made trail that is traversed by hikers or mountain bikers. This is a mostly unsolved task for robotics, but yet an important one for applications like search and rescue.<sup><a href="#fn25" id="fnref25">[25]</a></sup></p>
<p>While many types of robots (such as the quadrupedal robot) can be efficient at locomotion, successfully navigating real-world forest trails is hard. Apart from the mechanics of the problem, perceiving real-world trails is difficult. The appearance of the wilderness area may vary a lot depending on the location, unpaved roads generally have less structure (and tend to blend in with the surrounding grass areas, vegetation, and such), and trails change over time. It would be impossible to have a comprehensive dataset of all trails, in all weather and lighting conditions.</p>
<p>In such cases, a possible solution is to cast the trail perception problem as an image classification task and adopt an invariance based approach that operates directly on the image’s raw pixel values. Successful application could allow for out-of-distribution generalization to new trails, since the features learned are more transferable than environment-specific signals. Naturally, similar ideas are relevant for autonomous vehicles in urban areas.</p>
<h4 id="activity-recognition-systems">Activity recognition systems</h4>
<p>Smart devices (phones, watches, fitness trackers) carry a large array of sensors: accelerometers, gyroscopes, magnetometers, barometers, ambient lights sensors, and many more. Categorizing this data by the activity being performed at the time of recording - such as sitting, standing, or swimming - has allowed for the development of machine learning-based human-activity recognition systems. Correctly predicting a wearer’s activity enables a host of contextual applications, in particular in (but not restricted to) the health and wellness space.<sup><a href="#fn26" id="fnref26">[26]</a></sup></p>
<p>Unfortunately, it is hard to satisfactorily model this data due to the diversity exhibited in the real world. A single individual can perform a given activity slightly differently day-to-day, or the device may be unusually placed, or held or worn in a variety of orientations. Of course, different users are also physically diverse, and devices have intrinsic differences in their sensors and systems. This means that we either need a labeled dataset that captures the activity for each user and device (which is prohibitively expensive) or another way of identifying attributes that generalize better. Methods based on invariance could be particularly useful and well-suited in this scenario, capturing the essence of “sitting,” rather than the particular sensor activations for a particular user sitting on a particular chair.</p>
<h4 id="natural-language-processing">Natural language processing</h4>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-25.png" alt="Environments are everywhere. For instance, different sources of natural language."><figcaption>Environments are everywhere. For instance, different sources of natural language.</figcaption></figure>
<p>Invariant prediction approaches are of course not restricted exclusively to image problems. In natural language processing, texts from different publication platforms are tricky to analyze due to different contexts, vocabularies, and differences between how authors express themselves. For instance, financial news articles use a vocabulary and tone that differs from culture or society articles. The former is likely terse, whereas the latter may have an entertaining or personal tone. Similarly, online product reviews are linguistically different from tweets. Sentiment classification also relies heavily on context; different words are used to express whether someone likes a book versus an electronic gadget.</p>
<p>Two recent papers, <a href="https://arxiv.org/abs/2004.05007">An Empirical Study of Invariant Risk Minimization</a> and <a href="https://arxiv.org/abs/2003.09772">Invariant Rationalization</a>, apply the idea of IRM to a sentiment classification task, and find it improves out-of-distribution generalization. In particular, invariance acts to remove spurious reliance on single words which correlate highly with the target. Like images, text corpora form very high-dimensional datasets (there are many possible words!), making spurious correlations extremely unlikely to be noticed “manually.” As such, invariance based approaches are especially promising here.</p>
<h4 id="recommender-systems">Recommender systems</h4>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-24.png" alt="The data a recommender system collects is inherently biased by the suggestions it makes. We can untangle this bias with causal inference."><figcaption>The data a recommender system collects is inherently biased by the suggestions it makes. We can untangle this bias with causal inference.</figcaption></figure>
<p>Recommendation systems are algorithms designed to present relevant items to users on the web (for example, suggesting which movie to watch, a book to read, or a product to buy). As such, making good recommendations is an important problem: we want to make relevant recommendations for a user based on a record of their historical activities, from which we must infer their preferences.</p>
<p>The training data is either explicit (e.g., a rating a user left on a book) or implicit (e.g., linger time on a webpage or click data). There is a well-known exposure problem in recommender systems: a user simply cannot click on an item with which they have not been presented. Modeling the data without accounting for this is akin to the assumption of independent and identically distributed data, and is false: users do not select items randomly and independently of one another. For instance, a user may choose between two competing movies to watch, rather than selecting whether to watch each independently.</p>
<p>RecSys are a classic application for causality, which allows us to correct for this exposure bias by treating the selection of items to present to a user as an intervention. Applying causal approaches to recommendation naturally improves generalization to new data,<sup><a href="#fn27" id="fnref27">[27]</a></sup> and it seems likely that methods using invariant prediction could enhance this.</p>
<h3 id="tools">Tools</h3>
<p>The invariance-based approaches to causality we have discussed do not require dedicated tooling - ICP and IRM are procedures that could be implemented with general purpose machine learning frameworks.</p>
<p>Nonetheless, the authors of the ICP papers <sup><a href="#fn28" id="fnref28">[28]</a></sup> provide corresponding R packages: <a href="https://cran.r-project.org/web/packages/InvariantCausalPrediction/index.html">InvariantCausalPrediction</a> and <a href="https://cran.r-project.org/web/packages/nonlinearICP/index.html">nonlinearICP</a>. The packages make the techniques easy to use, and include additional utilities, such as dedicated plots for confidence intervals on causal coefficients. We are not aware of a package for IRM, but the authors have provided a <a href="http://github.com/facebookresearch/InvariantRiskMinimization/">code repository</a> which reproduces the paper results.</p>
<p>Below, we list a handful of open source projects that aid in traditional, SCM-based causal inference.</p>
<h4 id="dowhy">DoWhy</h4>
<p>Microsoft Research is developing the <a href="https://microsoft.github.io/dowhy/">DoWhy</a> python library for causal inference, incorporating elements of both causal graphical models and potential outcomes. The library is oriented around pandas DataFrames, and fits easily into a Python data analysis workflow. In particular, DoWhy makes a separation between four stages of causal inference:</p>
<ol>
<li>Modeling - defining a causal graph, or else the assumptions necessary for a potential outcomes approach (the common causes of the treatment and the outcome variable).</li>
<li>Identification - identifying the expression it is necessary to evaluate, in terms of conditional probability distributions.</li>
<li>Estimation - estimating the treatment effect. There are many estimation methods available in DoWhy, including machine learning-based methods from another of Microsoft’s causal libraries: <a href="https://github.com/microsoft/EconML">EconML</a>.</li>
<li>Refutation - assessing the robustness of the conclusion. Given the reliance of causal inference on modeling assumptions, it is especially important to find ways to test our conclusions. DoWhy provides several methods for this, such as introducing a dummy common cause or replacing the treatment with a random placebo.</li>
</ol>
<p>In addition to the above, DoWhy includes a novel algorithm, the “do-sampler.” In much of causal inference, the quantity of interest is a single number - for instance, the difference in the outcome variable when a binary treatment variable is applied (“what is the average causal effect of smoking on cancer incidence?”). The do-sampler extends the pandas DataFrame API directly, and moves beyond calculating causal effects to allow sampling from the full interventional distribution. Having done so, we can then compute arbitrary statistics under this intervention. The do-sampler is new, but provides a very promising direction for further research, and a potential avenue to making causal inference accessible to many more data science practitioners.</p>
<h4 id="causaldiscoverytoolbox">CausalDiscoveryToolbox</h4>
<p>The <a href="https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/index.html">Causal Discovery Toolbox</a> provides implementations of many algorithms designed for causal discovery - attempting to recover the full causal graph from observational data alone. There are many approaches to causal discovery, and the library is relatively comprehensive, including algorithms for pairwise causal discovery (inferring the direction of causation between a pair of variables), graph skeleton creation (creating an undirected graph of potential causal relationships), and full graphical causal model discovery.</p>
<p>Discovery of entire causal graphs does not yet appear mature enough that we can naively trust its conclusions about the causal structure of a problem. This makes sense, given the difficulty of the task! Inferring the whole causal structure from only observational data is about the hardest imaginable problem we could face with data.</p>
<h4 id="causalnex">CausalNex</h4>
<p><a href="https://causalnex.readthedocs.io/en/latest/">CausalNex</a> is a very recently released (at time of writing) toolkit by QuantumBlack to help data scientists do causal reasoning. It provides both a graph structure learning component to help build the causal graph and tools to fit that graph as a Bayesian network.</p>
<p>The structure learning component is an implementation of <a href="https://arxiv.org/abs/1803.01422">DAGs with NOTEARS</a>, an algorithm that casts structure learning as a continuous optimization problem. In its simplest form, it assumes linear relationships between variables (but unlike some causal discovery methods, does not assume Gaussian noise). Further, the algorithm assumes that all variables are observed (i.e., there is data for all variables). Unfortunately, this is rarely the case in causal problems.</p>
<p>Within these limitations, the algorithm is performant, and allows the user to specify hard constraints (such as, “these variables cannot be child nodes,” or “there is no causal relationship between these two variables”). This facilitates directly encoding domain knowledge into the graph, and using the structure learning component as an aid in places where the causal connection is not known.</p>
<h4 id="pyro">Pyro</h4>
<p>Uber’s <a href="http://pyro.ai/">Pyro</a> probabilistic programming library is primarily intended for implementing deep probabilistic models and fitting them with variational inference. However, in addition to tools for conditioning on observed data, the library implements a do operation to force a variable to take a certain distribution. This allows simulating from interventional distributions, provided the structural causal model (including equations) is known. The intersection of probabilistic programming with causal inference is nascent, but promising!</p>
<h2 id="ethics">Ethics</h2>
<p>Machine learning is playing an increasingly critical role in our society. Decisions that were previously exclusively made by humans are more frequently being made algorithmically. These algorithmic systems govern everything from which emails reach our inboxes, to whether we are approved for credit, to whom we have the opportunity to date – and their impact on our experience of the world is growing. Furthermore, our understanding of how these systems work is still lacking. We can neither explain nor correct them when their predictions are unfairly discriminatory or their outputs are reinforcing existing biases. Causal reasoning gives us a framework for thinking about these problems.</p>
<h3 id="causal-graphs-make-assumptions-explicit">Causal graphs make assumptions explicit</h3>
<p>Even without employing the full machinery of causal inference, when one approaches a new problem, it can be informative to try to write down the causal graph. This forces us to confront our assumptions about a system. It also allows someone else to understand our assumptions, and furnishes a precise framework for debate.</p>
<figure><img src="https://ff13.fastforwardlabs.com/figures/ff13-22.png" alt="Writing down a causal graph provides a principled way to specify and discuss causal assumptions."><figcaption>Writing down a causal graph provides a principled way to specify and discuss causal assumptions.</figcaption></figure>
<p>Making our assumptions explicit aids transparency, which is a win. However, it doesn’t protect against bad assumptions. Establishing causal relationships is hard. Unless we are able to perform sufficient experiments to validate our hypotheses, causal reasoning from observational data is subject to untested (sometimes untestable) assumptions.</p>
<p>We should make any causal claim with humility. As ever, we should be careful of dressing up a bad analysis with additional formalism.</p>
<h3 id="omitting-protected-attributes-is-not-enough">Omitting protected attributes is not enough</h3>
<p>It is unethical, and in many places illegal, to discriminate on the basis of a protected attribute, such as age, race, or disability. Avoiding <em>direct</em> discrimination (whereby some individuals with particular protected attributes are treated unfavourably) is comparatively easy. Appropriately, these protected attributes are frequently omitted from machine learning systems. Using a protected attribute as a feature directly is inviting discrimination based on that attribute.</p>
<p>More difficult to detect and avoid is <em>indirect causal discrimination</em>. Many features that are not themselves protected attributes are nonetheless highly predictive of a protected attribute. For instance, geographic location can correlate very highly with race, religion, and age. In denying loans to any individual with a particular zipcode, a bank could be committing indirect, but very real, discrimination against a protected attribute.</p>
<p>Another sub-category of discrimination is <em>indirect spurious discrimination</em>. These are instances when there are no pathways from causal attributes to the outcome. However, as we saw in <a href="#from-correlation-to-causation">From correlation to causation</a>, correlations can arise from numerous causal structures. As such, merely omitting the protected attribute does not omit its effects. A system is not guaranteed to be non-discriminatory on a protected attribute simply because it does not include that attribute directly. More simply, just because a feature does not cause the target does not mean that it will not be predictive of the target. This presents a particular challenge to algorithmic systems that are designed to find subtle correlations, especially since much historical data on which algorithms are trained is subject to selection bias (and other biases).</p>
<p>Since removing protected attributes is not enough, we must evaluate the resulting model for its discrimination and fairness properties. There are many possible measures of fairness, and it is generally impossible to optimize for all of them.<sup><a href="#fn29" id="fnref29">[29]</a></sup></p>
<p>Several recent papers<sup><a href="#fn30" id="fnref30">[30]</a></sup>, for instance) have proposed causality as a route to understanding and defining fairness and discrimination. In particular, if we have a causal graphical model of a system, we can see which paths are impacted by protected attributes, and correctly account for that impact. There have also been contributions in non-parametric structural causal models that allow one to detect and distinguish the three main discriminations - namely, direct, indirect and spurious.<sup><a href="#fn31" id="fnref31">[31]</a></sup></p>
<p>That said, the difficulty lies in constructing the causal graph. A causal graph could, of course, be used to embed all kinds of biases and prejudices, but at least provides a basis for argument.</p>
<h3 id="invariance-as-a-route-to-fairness">Invariance as a route to fairness</h3>
<p>An interesting idea is proposed in the final section of the IRM paper: treating groups over which we want fairness as the environments. When we seek to learn an invariant model (be that by ICP or IRM), we are explicitly trying to learn a model that performs optimally in different environments. We could construct those environments by separating out groups having different values for protected attributes. Then, by learning a model that seeks to perform optimally in each environment, we are explicitly trying to guarantee the best performance for each protected attribute.</p>
<p>Said differently, invariant features are exactly those that are consistent across groups. Consider again a bank granting loans, this time directly to individuals. The bank does not wish to discriminate on the basis of protected attributes. By treating the protected attributes as the groups, they are looking to learn what impacts loan defaulting invariantly across those groups.</p>
<p>The idea of learning an invariant predictor across environments is that the representation used is capturing something true about the generative process of the data. This representation would be, to some degree, <em>disentangled</em>, in the sense that each dimension of the representation (a vector) should correspond to something meaningful. <a href="https://arxiv.org/abs/1905.13662">On the Fairness of Disentangled Representations</a> shows experimentally that disentangled representations improve fairness in downstream uses.</p>
<h2 id="future">Future</h2>
<p>At the outset, causal reasoning provides a conceptual and technical framework for addressing questions about the effect of real or hypothetical actions or <em>interventions</em>. Once we understand what the effect of an action is, we can turn the question around and ask what action plausibly caused an event. This gives us a formal language to talk about cause-and-effect. That said, not every question about cause is easy to answer. Further, it may not be a trivial task to find an answer or even to interpret it. Causal graphs that we discuss in the <a href="#background%3A-causal-inference">Background: Causal Inference</a> chapter provide a convenient way to discuss these notions, and allow us to reason about statistical dependencies in observed data.</p>
<p>Structural causal models take a step further to this intuitive way of reasoning by making formal assumptions about the parametric form of how the variables interact.</p>
<p>However, causal graphs and SCMs become difficult to construct as the number of variables increases. Some systems are hard to model in this way. How do we draw a causal graph for pixels of an image? Or words in text? The problem gets out of hand quickly.</p>
<p>Fortunately, not all problems require the entire causal graph. Often, we are interested only in the causal relations associated with one particular target variable. This is where methods based on invariance (like IRM) step in to allow the model to capture stable features across environments (that is, different data generating processes). This paradigm enables out-of-distribution generalization. As opposed to causal graphs or structural causal models, where the only way to validate assumptions of the variable interactions is through experimentation, IRM allows us to test them on an unseen test set!</p>
<h3 id="comparable-approaches">Comparable approaches</h3>
<p>So, at this point we probably agree that methods based on invariance are promising. How else might we approach out-of-distribution generalization? In general, there are two families of approaches; those that learn to match the feature distributions (or estimate a data representation) and those that employ some kind of optimization technique.</p>
<h4 id="domain-adaptation">Domain adaptation</h4>
<p>Domain adaptation is a special case of transfer learning. In domain adaptation, the model learns a task in a source domain, which has some feature distribution, and we would like it to be able to perform the same task well in a target domain, where the feature distribution is different. Domains play the same role as environments in invariance-based approaches; a source domain is an environment that was trained in, and a target domain is any environment that was not trained in.</p>
<p>Domain adaptation also enforces a kind of invariance - it seeks a representation that is distributed the same across source and target domains (so, across environments).<sup><a href="#fn32" id="fnref32">[32]</a></sup> However, truly invariant, causal features need not follow the same distribution in different environments. A snowy cow will not generate quite the same pixel distribution as a sandy cow, and the causal feature we wish to represent is the cow itself.</p>
<h4 id="robust-learning">Robust learning</h4>
<p>The idea of learning across multiple environments is not novel to invariance-based approaches. <a href="https://www.aaai.org/Library/AAAI/2005/aaai05-112.php">Robust Supervised Learning</a> is a family of techniques that uses the same multi-environment setup as IRM (but much predates it), with a similar goal of enabling or enhancing out-of-distribution generalization. Said differently, the goal is a predictor that is robust to distributional shifts of the inputs.</p>
<p>The difference from the IRM setup we have covered is the loss function. The key idea is to add environment-specific “baseline” terms to the loss, and try to fix these terms such that particularly noisy environments where the loss may be high do not dominate. Then, minimizing the loss should guarantee good performance across all the known environments. Further, a robust predictor will perform well in new environments that are interpolations of those seen in training. This certainly improves out-of-distribution generalization, but does not allow <em>extrapolation</em> outside of what was seen in training, whereas IRM can extrapolate, thanks to relying on an invariant predictor.</p>
<h4 id="meta-learning">Meta-learning</h4>
<p>Approaches like domain adaptation, robust learning, and (in general) transfer learning try to alleviate the problem of out-of-distribution generalization to some extent. Unfortunately, learning invariant features with varying distributions across environments is still challenging. These approaches are good at interpolation, but not extrapolation.</p>
<p>This is where meta-learning approaches like Model Agnostic Meta Learning (MAML)<sup><a href="#fn33" id="fnref33">[33]</a></sup> come into play. The underlying idea for meta-learners generally is to attempt to learn tasks with a small number of labeled examples. Training meta-learners is a two-step process involving a <em>learner</em> and a <em>trainer</em>. The goal of the learner (model) is to quickly learn new tasks from a small amount of new data; hence, it is sometimes called a <em>fast learner</em>. (A task here refers to any supervised machine learning problem - e.g., predicting a class given a small number of examples.) This learner is trained, by the meta-learner, to be able to learn from a large number of different tasks. The meta-learner accomplishes this by repeatedly showing the learner hundreds and thousands of different tasks.</p>
<p>Learning then, happens at two levels. The first level focuses on quick acquisition of knowledge within each task with a few examples. The second level slowly pulls and digests information across all tasks. In case of MAML (which is optimization-based), the learner (or the first level) can achieve an optimal fast learning on a new task with only a small number of gradient steps because the meta-learner provides a good initialization of a model’s parameters. This approach is close to the problem of learning an optimal classifier in multiple environments, and could be explored further to learn invariant features within the data.</p>
<p>Some recent works have made the connection between causality and meta-learning explicitly.<sup><a href="#fn34" id="fnref34">[34]</a></sup></p>
<h3 id="looking-ahead">Looking ahead</h3>
<p>In this section, we discuss future possibilities with causality in general, as well as with methods based on invariance.</p>
<h4 id="causal-reinforcement-learning">Causal Reinforcement Learning</h4>
<p>Reinforcement learning is the study of how an agent can learn to choose actions that maximize its future rewards in an interactive and uncertain environment. These agents rely on plenty of simulations (and sometimes real data) to learn which actions lead to high reward in a particular context. Causality is also about calculating the effect of actions, and allows us to transfer knowledge to new, unfamiliar situations. These two disciplines have evolved independently with little interaction between them until recently. Integrating them is likely to be a fruitful area of research, and may extend the reach of both causality and reinforcement learning.<sup><a href="#fn35" id="fnref35">[35]</a></sup></p>
<p>There is a natural mapping between the concept of intervention in causal inference and actions taken in reinforcement learning. Throughout an episode of reinforcement learning (an episode is formed of one run of the system, for example, a complete game of chess, or go), an agent takes actions. This defines a data generating process for the reward that the agent ultimately cares about; different sequences of actions will generate different rewards. Since the agent can choose its actions, each of them is an intervention in this data generating process. In making this connection, we can leverage the mathematics of causal inference. For instance, we could use counterfactuals, the third level of the <a href="#the-ladder-of-causation">The ladder of causation</a>, to reason about actions not taken. Applying such causal techniques may reduce the state space the agent needs to consider, or help account for confounders.</p>
<p>Methods based on invariance, like IRM, in principle, learn to discover unknown invariances from multiple environments. We could leverage this attribute in reinforcement learning. An episode of RL consists of all the states that fall in between an initial state and a terminal state. Since each episode is independent of another, in IRM terminology they could be viewed as different environments. An agent could then learn robust policies from each of these episodes that leverage the invariant part of behaviour or actions that lead to reward.</p>
<p>While reinforcement learning itself is still in nascent stages when it comes to commercial applications, combining it with causality offers great potential.<sup><a href="#fn36" id="fnref36">[36]</a></sup> But prior to that, we need to address some questions. For example, how do we combine programming abstractions in causal modeling with reinforcement learning to help find the best decisions? What tools and libraries are necessary to enable commercial applications in this space?</p>
<h4 id="irm-and-environments">IRM and environments</h4>
<p>IRM uses the idea of training in multiple environments to achieve out-of-distribution generalization. Unfortunately, few datasets come with existing environment annotations. There are at least two ways we can try to address this problem.</p>
<p>The first is to be mindful of the environment when collecting data, and collect metadata alongside it. This may be easy (for example, collecting the geo-location of photos in settings where this is possible and does not violate a user’s privacy), or extremely hard (requiring much post-collection manual labeling).</p>
<p>Another compelling but untested option is to try combining IRM with some sort of clustering to segment a single dataset into environments.<sup><a href="#fn37" id="fnref37">[37]</a></sup> The question would be how to cluster in such a way that meaningful and diverse environments are defined. Since existing clustering approaches are purely correlative, and - as such - vulnerable to spurious correlations, this could prove challenging.</p>
<p>Studying the impact of environment selection, and how to create or curate datasets with multiple environments would be a valuable contribution to making invariance-based methods more widely applicable. (The authors of <a href="https://deepai.org/publication/an-empirical-study-of-invariant-risk-minimization">An Empirical Study of Invariant Risk Minimization</a> reach the same conclusion.)</p>
<h4 id="causal-reasoning-for-algorithmic-fairness">Causal reasoning for algorithmic fairness</h4>
<p>In the <a href="#ethics">Ethics</a> chapter, we reviewed some notions of fairness in prediction problems and shared how tools of causal reasoning can be leveraged to address fairness. They depart in the traditional way of wholly relying on data-driven approaches and emphasize the need to require additional knowledge of the structure of the world, in the form of a causal model. This additional knowledge is particularly valuable, as it informs us how changes in variables propagate in a system (be it natural, engineered, or social). Explicit causal assumptions remove ambiguity from methods that just depend upon statistical correlations. Avoiding discrimination through causal reasoning is an active area of research. As efforts to aid more transparency and fairness in machine learning systems grow, causal reasoning will continue to gain significant momentum in guiding algorithms towards fairness.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Structural causal models give us a framework for thinking precisely about cause and effect, and encoding our assumptions about data generating processes. Knowing the complete model for a system is immensely powerful, allowing us to reason about how the system will behave when we intervene in the data generating process, and correct for selection biases.</p>
<p>In machine learning, we’re often concerned only with prediction, for which we do not need causal inference. However, even in this scenario, taking a causal approach brings some benefits. Notably, causal relationships are invariant (they do not change between environments) and when we learn predictors based on them, we get greatly improved out-of-distribution generalization.</p>
<p>For many problems, constructing a causal graph is prohibitively hard, and always relies on assumptions. When working with only observational data, these assumptions are especially important, since they cannot be validated through experiments. Fortunately, by relying on the correspondence between causal relationships and invariance, we can still construct the relevant part of the causal graph for some problems using <a href="#invariant-causal-prediction">Invariant Causal Prediction</a>. For high dimensional inputs like image and text, we can use <a href="#invariant-risk-minimization">Invariant Risk Minimization</a> to learn a predictor that greatly enhances our out-of-distribution performance by learning not to rely on dataset-specifc spurious correlations.</p>
<p>Research at the intersection of causality and machine learning is blossoming, with many major ML conferences hosting dedicated workshops on the topic. Invariance-based approaches are an especially promising development and are ripe for industrial application. As algorithmic systems become increasingly prevalent, and their influence on decisions grows, the need for causal reasoning becomes all the more acute. We think it is important that practitioners have an understanding of causality, and hope to see causal approaches gain significant traction in mainstream data science practice. We hope this report has sparked some causal curiosity in you!</p>
<hr>
<section>
<ol>
<li id="fn1"><p>See for instance, recent works by Yoshua Bengio, like <a href="https://arxiv.org/abs/1901.10912">A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms</a>. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>See, for instance, <a href="https://eng.uber.com/causal-inference-at-uber/">Using Causal Inference to Improve the Uber User Experience</a>. <a href="#fnref2">↩︎</a></p>
</li>
<li id="fn3"><p>Facebook performed <a href="https://www.pnas.org/content/111/24/8788">such an experiment</a> in 2012, and received <a href="https://www.theatlantic.com/technology/archive/2014/06/everything-we-know-about-facebooks-secret-mood-manipulation-experiment/373648/">much criticism</a> as a result. The ethical problem is not so much with the experiment itself, but rather that the subjects had not given informed consent, in violation of basic ethical guidelines for psychological research. <a href="#fnref3">↩︎</a></p>
</li>
<li id="fn4"><p>An alternative popular framework is the Neyman-Reuben causal model, also known as <a href="https://www.cambridge.org/core/books/causal-inference-for-statistics-social-and-biomedical-sciences/71126BE90C58F1A431FE9B2DD07938AB">Potential Outcomes</a>. The frameworks are equivalent in that they can compute the same things, though some causal queries may be easier to reason about in one or the other. <a href="#fnref4">↩︎</a></p>
</li>
<li id="fn5"><p>See also Pearl’s article: <a href="https://cacm.acm.org/magazines/2019/3/234929-the-seven-tools-of-causal-inference-with-reflections-on-machine-learning/fulltext">The Seven Tools of Causal Inference, with Reflections on Machine Learning</a>. <a href="#fnref5">↩︎</a></p>
</li>
<li id="fn6"><p>Some farm-experienced members of the CFF team are keen to point out that roosters crow pretty much <em>all the time</em>. <a href="#fnref6">↩︎</a></p>
</li>
<li id="fn7"><p>See this article in <a href="https://www.forbes.com/sites/erikaandersen/2012/03/23/true-fact-the-lack-of-pirates-is-causing-global-warming/#5cb710453a67">Forbes</a>. <a href="#fnref7">↩︎</a></p>
</li>
<li id="fn8"><p>On a technical note, correlation measures only <em>linear</em> association. For instance, <code>x</code> squared is uncorrelated with <code>x</code>, despite being completely dependent on it. When we say “correlation is not causation,” we really mean “statistical dependence is not causation.” <a href="#fnref8">↩︎</a></p>
</li>
<li id="fn9"><p>Alas, it requires a far more detailed technical knowledge than we can provide in this report. We recommend the textbook <a href="http://bayes.cs.ucla.edu/PRIMER/">Causal Inference in Statistics: A Primer</a> for a succinct introduction to Structural Causal Models. An abbreviated overview, (<a href="https://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf">Causal Inference in Statistics: An Overview</a>) is freely available as a PDF. The textbook <a href="https://mitpress.mit.edu/books/elements-causal-inference">Elements of Causal Inference</a> (available through Open Access) also covers structural causal models, and includes several chapters explicitly drawing connections between causal inference and machine learning. <a href="#fnref9">↩︎</a></p>
</li>
<li id="fn10"><p>We will examine the nuances of this statement in <a href="#causality-and-invariance">Causality and invariance</a>. Correlation is predictive <em>in distribution</em>. <a href="#fnref10">↩︎</a></p>
</li>
<li id="fn11"><p>Adam Kelleher and Amit Sharma have an excellent <a href="https://medium.com/@akelleh/introducing-the-do-sampler-for-causal-inference-a3296ea9e78d">blog post</a> describing this problem, and introducing a new causal sampling technology to make solving it easier. <a href="#fnref11">↩︎</a></p>
</li>
<li id="fn12"><p>See <a href="http://jmlr.org/papers/v17/14-518.html">Distinguishing cause from effect using observational data: methods and benchmarks</a>. <a href="#fnref12">↩︎</a></p>
</li>
<li id="fn13"><p>See <a href="https://www.aaai.org/Library/AAAI/2005/aaai05-112.php">Robust Supervised Learning</a>. <a href="#fnref13">↩︎</a></p>
</li>
<li id="fn14"><p>The scientific process of iterated hypothesis and experimentation can also be applied to constructing a causal model for business purposes. The popular George Edward Box quote is pertinent here: “all models are wrong, but some are useful” (see <a href="https://en.wikipedia.org/wiki/All_models_are_wrong">All models are wrong</a>). <a href="#fnref14">↩︎</a></p>
</li>
<li id="fn15"><p>Judea Pearl’s do-calculus is a set of rules to calculate exactly which variables we must account for - and how - to answer a given causal query in a potentially complicated graph. This is not trivial; often there are unobserved variables in a graph, and we must try to express the query only in terms of those variables for which we have data. <a href="#fnref15">↩︎</a></p>
</li>
<li id="fn16"><p>There is a subtlety here. We said environments were defined by interventions. Naturally, it is impossible to intervene on the country a store is built in once the store is built. This turns out not to matter for the purposes of inferring the direct causal parents of the sales volume, so long as the country is further up the graph, and changing country alters the data generating process. <a href="#fnref16">↩︎</a></p>
</li>
<li id="fn17"><p>(See <a href="https://arxiv.org/abs/1706.08576">Invariant Causal Prediction for Nonlinear Models</a>. <a href="#fnref17">↩︎</a></p>
</li>
<li id="fn18"><p>Nonparametric conditional independence testing is an area of active research, and is generally hard - and made more so by having finite data. The nonlinear ICP paper also introduces the notion of defining sets; sometimes no single set of variables is accepted as the set of causal parents, but there are similar sets differing by only one or two variables that may be related. While the algorithm has failed to find a single consistent model, it is nonetheless conveying useful causal information. <a href="#fnref18">↩︎</a></p>
</li>
<li id="fn19"><p>Longer than you may think! See, for instance, <a href="https://dspace.mit.edu/handle/1721.1/11589">Machine perception of three-dimensional solids</a>, published in 1963. <a href="#fnref19">↩︎</a></p>
</li>
<li id="fn20"><p>The final section of the IRM paper includes a charming socratic dialogue that discusses this distinction, as well as the reason that regular supervised learning is so successful, from an invariance standpoint. <a href="#fnref20">↩︎</a></p>
</li>
<li id="fn21"><p>See <a href="https://arxiv.org/abs/1206.6471">On Causal and Anticausal Learning</a> for a description of the insight considering the causal direction of a problem brings to machine learning. <a href="#fnref21">↩︎</a></p>
</li>
<li id="fn22"><p>Technically, loss is the error on the training set, and risk is the error across the whole data distribution. With finite training data, minimizing the loss on the training set is a proxy for minimizing the risk. <a href="#fnref22">↩︎</a></p>
</li>
<li id="fn23"><p>Imperfect interpretability results notwithstanding, using ResNet as a feature extractor is representative of how CV systems are used in the real world, and the resulting out-of-distribution performance improvements are impressive. <a href="#fnref23">↩︎</a></p>
</li>
<li id="fn24"><p>This example is given in <a href="https://arxiv.org/abs/1812.11806">An introduction to domain adaptation and transfer learning</a>, and an empirical study using transfer learning was reported in <a href="https://ieeexplore.ieee.org/document/6945865">Transfer Learning Improves Supervised Image Segmentation Across Imaging Protocols</a> <a href="#fnref24">↩︎</a></p>
</li>
<li id="fn25"><p><a href="http://rpg.ifi.uzh.ch/docs/RAL16_Giusti.pdf">A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots</a> <a href="#fnref25">↩︎</a></p>
</li>
<li id="fn26"><p><a href="https://ieeexplore.ieee.org/document/8444585">Scaling Human Activity Recognition via Deep Learning-based Domain Adaptation</a> outlines the problem and some applications in this space. <a href="#fnref26">↩︎</a></p>
</li>
<li id="fn27"><p>See <a href="http://www.its.caltech.edu/~fehardt/UAI2016WS/papers/Liang.pdf">Causal Inference for Recommendation</a> and <a href="https://arxiv.org/abs/1808.06581">The Deconfounded Recommender: A Causal Inference Approach to Recommendation</a>. <a href="#fnref27">↩︎</a></p>
</li>
<li id="fn28"><p><a href="https://arxiv.org/abs/1501.01332">Causal inference using invariant prediction: identification and confidence intervals</a> and <a href="https://arxiv.org/abs/1706.08576">Invariant Causal Prediction for Nonlinear Models</a>. <a href="#fnref28">↩︎</a></p>
</li>
<li id="fn29"><p>See <a href="https://arxiv.org/abs/1609.05807">Inherent Trade-Offs in the Fair Determination of Risk Scores</a>. <a href="#fnref29">↩︎</a></p>
</li>
<li id="fn30"><p>See <a href="https://arxiv.org/abs/1805.05859">Causal Reasoning for Algorithmic Fairness</a> and <a href="https://arxiv.org/abs/1706.02744">Avoiding Discrimination through Causal Reasoning</a>. <a href="#fnref30">↩︎</a></p>
</li>
<li id="fn31"><p>See <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16949">Fairness in Decision-Making – The Causal Explanation Formula</a>. <a href="#fnref31">↩︎</a></p>
</li>
<li id="fn32"><p><a href="https://arxiv.org/abs/1505.07818">Domain adversarial training of neural networks</a> <a href="#fnref32">↩︎</a></p>
</li>
<li id="fn33"><p><a href="https://arxiv.org/abs/1703.03400">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a> <a href="#fnref33">↩︎</a></p>
</li>
<li id="fn34"><p>See <a href="https://arxiv.org/abs/1901.10912">A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms</a>. <a href="#fnref34">↩︎</a></p>
</li>
<li id="fn35"><p>There is a nice introduction to causal reinforcement learning in the paper <a href="http://gershmanlab.webfactional.com/pubs/RL_causal.pdf">Reinforcement learning and causal models</a>. The blog post <a href="https://causallu.com/2018/12/31/introduction-to-causalrl/">Introduction to Causal RL</a> contains a shorter description, and also suggests some medical applications. <a href="#fnref35">↩︎</a></p>
</li>
<li id="fn36"><p>We are grateful to David Lopez-Paz (one of the <a href="https://arxiv.org/abs/1907.02893">Invariant Risk Minimization</a> authors) for sharing his thoughts and ideas about possible extensions and applications of IRM with us, including applications to reinforcement learning. <a href="#fnref36">↩︎</a></p>
</li>
<li id="fn37"><p>This idea was also suggested to us by David Lopez-Paz. <a href="#fnref37">↩︎</a></p>
</li>
</ol>
</section>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Unity like game editor running in pure WASM (603 pts)]]></title>
            <link>https://raverie-us.github.io/raverie-engine/</link>
            <guid>37663270</guid>
            <pubDate>Tue, 26 Sep 2023 17:58:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://raverie-us.github.io/raverie-engine/">https://raverie-us.github.io/raverie-engine/</a>, See on <a href="https://news.ycombinator.com/item?id=37663270">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[ROCm is AMD's priority, executive says (277 pts)]]></title>
            <link>https://www.eetimes.com/rocm-is-amds-no-1-priority-exec-says/</link>
            <guid>37663194</guid>
            <pubDate>Tue, 26 Sep 2023 17:54:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eetimes.com/rocm-is-amds-no-1-priority-exec-says/">https://www.eetimes.com/rocm-is-amds-no-1-priority-exec-says/</a>, See on <a href="https://news.ycombinator.com/item?id=37663194">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

					<!--?//php echo do_shortcode('[responsivevoice_button voice="US English Male" buttontext="Listen to Post"]') ?-->
					<!-- EET_Top_Leaderboard -->

<p>SANTA CLARA, CALIF. — “If you think about the product portfolio that AMD has, it’s arguably the broadest in the industry in terms of AI compute,” Vamsi Boppana, senior VP of the AI group at AMD, said in his keynote address at the recent AI Hardware Summit. AMD’s hardware portfolio includes data-center–class CPUs and GPUs, consumer GPUs, FPGAs and the Ryzen 7040, a client CPU with NPU designed for PCs. Software is key to unlocking the performance of these different hardware platforms. But how does AMD compete with its GPU competitors’ strong offerings, given its more diverse hardware?</p>
<p>AMD’s software stacks for each class of product are separate: ROCm (short for Radeon Open Compute platform) targets its Instinct data center GPU lines (and, soon, its Radeon consumer GPUs), Vitis AI targets its FPGAs, and ZenDNN targets its client devices.</p>
<p>How far along is AMD with unifying these stacks?</p>


<p>“We have enormous customer pull coming, and that is dictating quite a bit of our near-term plans,” Boppana told EE Times in an interview after his talk here. “The plane is flying right now, so we cannot disassemble the engine. However, we are absolutely doing things at the foundational level to make more unification happen in our stack.”</p>
		
		<div>
		<div>
				<div>
					<figure>
					<a href="https://www.eetimes.com/briocean-technology-shines-bright-as-it-ranks-8th-on-the-top-asia-pacific-distributor-list/" title="Briocean Technology Shines Bright as It Ranks 8th on the Top Asia Pacific Distributor List&nbsp;"><img data-lazy-fallback="1" loading="lazy" width="62" src="https://www.eetimes.com/wp-content/uploads/Briocean-thumbnail-image.jpg?w=62" alt="Briocean Technology Shines Bright as It Ranks 8th on the Top Asia Pacific Distributor List&nbsp;" data-lazy-src="https://www.eetimes.com/wp-content/uploads/Briocean-thumbnail-image.jpg?w=62&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a>
					</figure>
					
				</div><p>By Briocean Technology&nbsp; 09.26.2023</p>
			</div>
		<div>
				<div>
					<figure>
					<a href="https://www.eetimes.com/unlock-the-benefits-of-endpoint-ai-solutions-for-laptop-computers/" title="Unlock the Benefits of Endpoint AI Solutions for Laptop Computers&nbsp;"><img data-lazy-fallback="1" loading="lazy" width="62" src="https://www.eetimes.com/wp-content/uploads/EE-Times_600x340_.jpg?w=62" alt="Unlock the Benefits of Endpoint AI Solutions for Laptop Computers&nbsp;" data-lazy-src="https://www.eetimes.com/wp-content/uploads/EE-Times_600x340_.jpg?w=62&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a>
					</figure>
					
				</div><p>By Himax Technologies. Ltd.&nbsp; 09.26.2023</p>
			</div>
		<div>
				<div>
					<figure>
					<a href="https://www.eetimes.com/1409388-2/" title="SK hynix’s Evolution in CIS HDR Technology and Future Outlook"><img data-lazy-fallback="1" loading="lazy" width="62" src="https://www.eetimes.com/wp-content/uploads/SK-hynix_CIS-HDR-Technology_Thumbnail.png?w=62" alt="SK hynix’s Evolution in CIS HDR Technology and Future Outlook" data-lazy-src="https://www.eetimes.com/wp-content/uploads/SK-hynix_CIS-HDR-Technology_Thumbnail.png?w=62&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a>
					</figure>
					
				</div><p>By Suram Cha, Technical Leader of Next Gen Biz Team, SK hynix&nbsp; 09.26.2023</p>
			</div></div>
<figure id="attachment_1409408" aria-describedby="caption-attachment-1409408"><a href="https://www.eetimes.com/wp-content/uploads/Kisaco_day2-4767.jpg"><img data-lazy-fallback="1" decoding="async" fetchpriority="high" src="https://www.eetimes.com/wp-content/uploads/Kisaco_day2-4767.jpg?w=640&amp;resize=640%2C427" alt="" width="640" height="427" data-recalc-dims="1" data-lazy-src="https://www.eetimes.com/wp-content/uploads/Kisaco_day2-4767.jpg?w=640&amp;is-pending-load=1#038;resize=640%2C427" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption id="caption-attachment-1409408">AMD’s Vamsi Boppana gives a keynote address at the recent AI Hardware Summit in Santa Clara, Calif. (Source: Kisaco Research &amp; Jeffrey Hosier Photography)</figcaption></figure>
<p>Boppana said that there’s some common infrastructure and tooling underlying all three stacks, including an ongoing effort to make a common quantizer.</p>

<p>“Over time, we want to get to a place where users have one execution provider, and underneath that, you will be able to select [a hardware target],” he said. “In the near term, modules are shared across stacks, and over time, as things like heterogeneous platforms are going to become prevalent, the unified elements start coming through.”</p>
<p>A unified stack would be helpful for heterogeneous systems, Boppana said, especially where partitioning is required. Currently, the Vitis stack handles CPU plus xDNA targets, but he agrees that both automatic and user-driven partitioning will be necessary.</p>
<p>“In that scenario, we need to be able to take a problem statement and cut the graph, such that both parts of the graph get executed on [different parts of the hardware], and they need to inter-operate,” he said.</p>
<h3><strong>‘ROCm has evolved’</strong></h3>
<p>ROCm is less mature than competitors’ GPU software offerings, with <a href="https://www.eetimes.com/nvidia-brings-gpu-acceleration-to-computational-lithography/" target="_blank" rel="noopener">Nvidia</a>’s mature CUDA stack often seen as a big part of the market leader’s competitive advantage.</p>
<p>“Software is a journey,” Boppana said. “Anybody who has written or managed complex pieces of software knows it takes time. The good news is, we have been on the journey…ROCm has evolved.”</p>
<p>AMD has made ROCm the No. 1 priority at the company level in the last year, Boppana said, standing up a new organization that’s brought together assets from all the company’s software contributions.</p>
<p>“We have much larger resources actually working on software, and [AMD CEO Lisa Su] has been very clear that she wants to see significant and continued investments on the software side,” Boppana said. “We have agreed to provide people internally, we have acquired Mipsology, and we are looking to grow talent both organically and inorganically.”</p>
<p>AMD also recently stood up an internal <a href="https://www.eetimes.com/podcasts/demystifying-ai-how-neural-networks-like-transformers-really-work/" target="_blank" rel="noopener">AI models</a> group to increase its experience using its own software stack.</p>
<p>“We want a much tighter feedback loop,” Boppana said.</p>
<h3><strong>Using open source to challenge Nvidia</strong></h3>
<p>AMD has embraced OpenAI’s Triton, an open-source programming language and compiler for GPUs that promises to offer an open-source alternative to Nvidia’s CUDA for developers who want to write high-level code that performs optimally on the hardware.</p>
<p>“There are different personas that are programming [our GPUs],” he said. “[Triton] is a level of abstraction that people are comfortable with. It’s productive. And it gets to hardware in a pretty efficient, cogent fashion. But for other customers, that doesn’t matter; they don’t need to develop new kernels. For them, we can ship libraries. So, it’s just a matter of who wants to use us.”</p>
<p>In contrast to Nvidia’s approach with CUDA, which is mostly proprietary, most of AMD’s ROCm stack is open source.</p>
<p>“We partner with the [AI frameworks] and the people writing the libraries and say, ‘If you have a kernel you want to put together, you can take something that exists from us, but if you find there’s the opportunity for you to optimize source code, [you can]’,” he said. “Then we have so many more people that are willing and able to contribute. So, that’s very important and very powerful for us: We think it’s the right strategic direction for us to take.”</p>
<figure id="attachment_1409409" aria-describedby="caption-attachment-1409409"><a href="https://www.eetimes.com/wp-content/uploads/AMD-Hardware-architectures.jpg"><img data-lazy-fallback="1" decoding="async" src="https://www.eetimes.com/wp-content/uploads/AMD-Hardware-architectures.jpg?w=640&amp;resize=640%2C269" alt="" width="640" height="269" data-recalc-dims="1" data-lazy-src="https://www.eetimes.com/wp-content/uploads/AMD-Hardware-architectures.jpg?w=640&amp;is-pending-load=1#038;resize=640%2C269" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption id="caption-attachment-1409409">AMD has a diverse portfolio of hardware architectures for AI acceleration. But how does the company manage its AI software stack for such diverse architectures? (Source: AMD)</figcaption></figure>
<p>MI300 samples are currently with customers, Boppana said, and both customers and AMD have AI training workloads up and running, with availability coming at the end of this year.</p>
<p>ROCm will be crucial to the success of both the MI300 and <a href="https://www.eetimes.com/can-amds-mi300x-take-on-nvidias-h100/" target="_blank" rel="noopener">MI300X</a>.</p>
<p>“Being candid, we have a few places to grow,” he said. “Allowing the community to contribute [to ROCm] alongside us helps us bridge the gap faster.”</p>
				</div><div>
					<div>
						<p><img width="173" height="164" src="https://www.eetimes.com/wp-content/uploads/Sally-Ward_Foxton_low-res-6-e1678729120589.jpg?fit=173%2C164&amp;is-pending-load=1" alt="" decoding="async" data-lazy-src="https://www.eetimes.com/wp-content/uploads/Sally-Ward_Foxton_low-res-6-e1678729120589.jpg?fit=173%2C164&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7">						</p>
					</div>
											<div>
							<p>Sally Ward-Foxton</p>

							<p><em>Sally Ward-Foxton covers AI for EETimes.com and EETimes Europe magazine. Sally has spent the last 18 years writing about the electronics industry from London. She has written for Electronic Design, ECN, Electronic Specifier: Design, Components in Electronics, and many more news publications. She holds a Masters' degree in Electrical and Electronic Engineering from the University of Cambridge.   <br></em></p>
						</div>
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sidechannel pixel-stealing attack works in Chromium on all modern GPUs (196 pts)]]></title>
            <link>https://arstechnica.com/security/2023/09/gpus-from-all-major-suppliers-are-vulnerable-to-new-pixel-stealing-attack/</link>
            <guid>37663159</guid>
            <pubDate>Tue, 26 Sep 2023 17:52:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/security/2023/09/gpus-from-all-major-suppliers-are-vulnerable-to-new-pixel-stealing-attack/">https://arstechnica.com/security/2023/09/gpus-from-all-major-suppliers-are-vulnerable-to-new-pixel-stealing-attack/</a>, See on <a href="https://news.ycombinator.com/item?id=37663159">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      SAME ORIGIN POLICY SHATTERED    —
</h4>
            
            <h2 itemprop="description">A previously unknown compression side channel in GPUs can expose images thought to be private.</h2>
                    </header>
        <section>
            <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/09/pixels-800x540.jpg" alt="GPUs from all major suppliers are vulnerable to new pixel-stealing attack">
      <figcaption></figcaption>  </figure>

  




<!-- cache hit 16:single/related:697e3f1e00d323130cb53bbec0736ce7 --><!-- empty -->
<p>GPUs from all six of the major suppliers are vulnerable to a newly discovered attack that allows malicious websites to read the usernames, passwords, and other sensitive visual data displayed by other websites, researchers have demonstrated in a paper published Tuesday.</p>
<p>The cross-origin attack allows a malicious website from one domain—say, example.com—to effectively read the pixels displayed by a website from example.org, or another different domain. Attackers can then reconstruct them in a way that allows them to view the words or images displayed by the latter site. This leakage violates a critical security principle that forms one of the most fundamental security boundaries safeguarding the Internet. Known as the <a href="https://developer.mozilla.org/en-US/docs/Web/Security/Same-origin_policy">same origin policy</a>, it mandates that content hosted on one website domain be isolated from all other website domains.</p>
<h2>Optimizing bandwidth at a cost</h2>
<p>GPU.zip, as the proof-of-concept attack has been named, starts with a malicious website that places a link to the webpage it wants to read inside of an <a href="https://www.hostinger.com/tutorials/what-is-iframe/">iframe</a>, a common HTML element that allows sites to embed ads, images, or other content hosted on other websites. Normally, the same origin policy prevents either site from inspecting the source code, content, or final visual product of the other. The researchers found that data compression that both internal and discrete GPUs use to improve performance acts as a <a href="https://en.wikipedia.org/wiki/Side-channel_attack">side channel</a> that they can abuse to bypass the restriction and steal pixels one by one.</p>                                            
                                                        
<p>“We found that modern GPUs automatically try to compress this visual data, without any application involvement,” Yingchen Wang, the lead author and a researcher at the University of Texas at Austin, wrote in an email. “This is done to save memory bandwidth and improve performance. Since compressibility is data dependent, this optimization creates a side channel which can be exploited by an attacker to reveal information about the visual data.”</p>
<p>For GPU.zip to work, a malicious page must be loaded into the Chrome or Edge browsers. Under-the-hood differences in the way Firefox and Safari work prevent the attack from succeeding when those browsers process an attack page. Another requirement is that the page linked to in the iframe must not be configured to deny being embedded by cross-origin websites.</p>
<p>The security threats that can result when HTML is embedded in iframes on malicious websites have been well-known for more than a decade. Most websites <a href="https://web.dev/security-headers/">restrict the cross-origin embedding</a> of pages displaying user names, passwords, or other sensitive content through X-Frame-Options or Content-Security-Policy headers. Not all, however, do. One example is Wikipedia, which shows the usernames of people who log in to their accounts. A person who wants to remain anonymous while visiting a site they don’t trust could be outed if it contained an iframe containing a link to <code>https://en.wikipedia.org/wiki/Main_Page</code>.</p>
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/09/gpu.zip-pixel-theft.jpg" data-height="607" data-width="1554" alt="Pixel stealing PoC for deanonymizing a user, run with other tabs open playing video. “Ground Truth” is the victim iframe (Wikipedia logged in as “Yingchenw”). “AMD” is the attack result on a Ryzen 7 4800U after 30 minutes, with 97 percent accuracy. “Intel” is the attack result for an i7-8700 after 215 minutes with 98 percent accuracy."><img alt="Pixel stealing PoC for deanonymizing a user, run with other tabs open playing video. “Ground Truth” is the victim iframe (Wikipedia logged in as “Yingchenw”). “AMD” is the attack result on a Ryzen 7 4800U after 30 minutes, with 97 percent accuracy. “Intel” is the attack result for an i7-8700 after 215 minutes with 98 percent accuracy." src="https://cdn.arstechnica.net/wp-content/uploads/2023/09/gpu.zip-pixel-theft-640x250.jpg" width="640" height="250" srcset="https://cdn.arstechnica.net/wp-content/uploads/2023/09/gpu.zip-pixel-theft-1280x500.jpg 2x"></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/09/gpu.zip-pixel-theft.jpg" data-height="607" data-width="1554">Enlarge</a> <span>/</span> Pixel stealing PoC for deanonymizing a user, run with other tabs open playing video. “Ground Truth” is the victim iframe (Wikipedia logged in as “Yingchenw”). “AMD” is the attack result on a Ryzen 7 4800U after 30 minutes, with 97 percent accuracy. “Intel” is the attack result for an i7-8700 after 215 minutes with 98 percent accuracy.</p><p>Wang et al.</p></figcaption></figure>
<p>The researchers showed how GPU.zip allows a malicious website they created for their PoC to steal pixels one by one for a user’s Wikipedia username. The attack works on GPUs provided by Apple, Intel, AMD, Qualcomm, Arm, and Nvidia. On AMD’s Ryzen 7 4800U, GPU.zip took about 30 minutes to render the targeted pixels with 97 percent accuracy. The attack required 215 minutes to reconstruct the pixels when displayed on a system running an Intel i7-8700.</p>                                            
                                                        
<p>All of the GPUs analyzed use proprietary forms of compression to optimize the bandwidth available in the memory data bus of the PC, phone, or other device displaying the targeted content. The compression schemes differ from manufacturer to manufacturer and are undocumented, so the researchers reverse-engineered each one. The insights yielded a method that uses the SVG, or the scalable vector graphics image format, to maximize differences in DRAM traffic between black and white target pixels in the presence of compression. While their paper discusses GPU.zip as it applies to iGPUs, or internal GPUs, the technique applies equally to standalone or discrete GPUs as well.</p>
<p>In their paper, the researchers wrote:</p>
<blockquote><p>We demonstrate that an attacker can exploit the iGPU-based compression channel to perform cross-origin pixel stealing attacks in the browser using SVG filters (the latest version of Google Chrome as of April 2023), even though SVG filters are implemented at constant time. The reason is that the attacker can create highly redundant or highly non-redundant patterns depending on a single secret pixel in the browser. As these patterns are processed by the iGPU, their varying degrees of redundancy cause the lossless compression output to depend on the secret pixel. The data-dependent compression output directly translates to data-dependent DRAM traffic and data-dependent cache occupancy. Consequently, we show that, even under the most passive threat model—where an attacker can only observe coarse-grained redundancy information of a pattern using a coarse-grained timer in the browser and lacks the ability to adaptively select input—individual pixels can be leaked. Our proof-of-concept attack succeeds on a range of devices (including computers, phones) from a variety of hardware vendors with distinct GPU architectures (Intel, AMD, Apple, Nvidia). Surprisingly, our attack also succeeds on discrete GPUs, and we have preliminary results indicating the presence of software-transparent compression on those architectures as well.</p></blockquote>

                                                </div>

            
                            <nav>Page: <span>1 <a href="https://arstechnica.com/security/2023/09/gpus-from-all-major-suppliers-are-vulnerable-to-new-pixel-stealing-attack/2/">2</a> <a href="https://arstechnica.com/security/2023/09/gpus-from-all-major-suppliers-are-vulnerable-to-new-pixel-stealing-attack/2/"><span>Next <span>→</span></span></a></span></nav>
            
        </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Tips for Solopreneur? (194 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37662937</link>
            <guid>37662937</guid>
            <pubDate>Tue, 26 Sep 2023 17:39:51 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37662937">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="37662937">
      <td><span></span></td>      <td><center><a id="up_37662937" href="https://news.ycombinator.com/vote?id=37662937&amp;how=up&amp;goto=item%3Fid%3D37662937"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=37662937">Ask HN: Tips for Solopreneur?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_37662937">106 points</span> by <a href="https://news.ycombinator.com/user?id=solo_prono">solo_prono</a> <span title="2023-09-26T17:39:51"><a href="https://news.ycombinator.com/item?id=37662937">3 hours ago</a></span> <span id="unv_37662937"></span> | <a href="https://news.ycombinator.com/hide?id=37662937&amp;goto=item%3Fid%3D37662937">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Tips%20for%20Solopreneur%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=37662937&amp;auth=a4c646cbd0cbd88c190eb0bbc2f21002eeaf249c">favorite</a> | <a href="https://news.ycombinator.com/item?id=37662937">52&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>Yo HN! I have been working on some design tools in my spare time to solve problems I've faced over and over, and I'm thinking about monetizing them.</p><p>I've been to some conferences recently and talked to a lot of people who have these problems as well, and they're keen to try it out. I have collected some emails, been communicating with them a bit and even got beers with one of them recently!</p><p>Here's my list of concerns:</p><p>1. It is just me - is that a red flag? Some people have asked me about my team and I told them it was just me. I got the feeling that it may have turned them off because the conversation kind of ended right there. To be fair, after that I did say that it is just me right now BUTTTTTTTT why that is okay due to my experience and work history. However, yes it is my first time doing a business.</p><p>2. How do I set appropriate milestones for me to reach? Do I think about reaching 100 customers before reaching 5 recurring customers for example?</p><p>3. I'm in a small town in PNW. Does that matter if this will be an online thing anyway? Why or when do people move to big cities like Seattle/SF/NYC/Austin etc.</p><p>4. What are some ways to do marketing? Should I even think about that before I have a few customers who are using my product consistently?</p><p>5. I've been inspired by the Startup School videos. Honestly though I'm not sure about fundraising and all these things, it seems very intimidating to me. What's the difference between those things and starting a company and slowly building it up?</p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[macOS 14 Sonoma firewall bug fixed (108 pts)]]></title>
            <link>https://mullvad.net/en/blog/2023/9/22/macos-14-sonoma-firewall-bug-fixed/</link>
            <guid>37662791</guid>
            <pubDate>Tue, 26 Sep 2023 17:29:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mullvad.net/en/blog/2023/9/22/macos-14-sonoma-firewall-bug-fixed/">https://mullvad.net/en/blog/2023/9/22/macos-14-sonoma-firewall-bug-fixed/</a>, See on <a href="https://news.ycombinator.com/item?id=37662791">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>The firewall bug in macOS 14 Sonoma betas and release candidates that we <a href="https://mullvad.net/blog/2023/9/13/bug-in-macos-14-sonoma-prevents-our-app-from-working/">blogged about last week</a> has been fixed by Apple.</p>

<p>Yesterday Apple released macOS 14 Sonoma Release Candidate 2 (23A344). This version no longer exhibits the invalid firewall rule evaluation that we observed in the earlier release candidate and betas (starting from beta 6). This also means that our VPN app now works fine in latest Sonoma.</p>

<h2>Why we were affected</h2>

<p>Our VPN app is what we call a privacy preserving VPN client. This means its main purpose is not just to establish a tunnel and make sure it works, but also to ensure there are no leaks and no ways to de-anonymize the user.</p>

<p>To uphold the privacy preserving aspect, we do not think it is enough to solely rely on the routing table or Apple’s content filter provider API for making sure traffic that is supposed to go in the VPN tunnel actually does. Because doing so leaves numerous potential leaks, for example <a href="https://mullvad.net/blog/2020/11/16/big-no-big-sur-mullvad-disallows-apple-apps-bypass-firewall/">this one that was introduced in Big Sur.</a> At Mullvad we believe in adding as many safety layers as possible. Denying unwanted traffic at the firewall layer is an obvious design choice for us.</p>

<p>The firewall bugs we saw could only be observed if the rules contained the quick option, meaning they terminate firewall rule evaluation early. Without quick, all network traffic will be evaluated by subsequent rules and anchors injected by Apple or other software on the computer. We see this as a potential risk. While it might be possible to write firewall rules for a VPN without quick, we want our rules to be as final as possible, for security.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Exploring Linux command-line space time (232 pts)]]></title>
            <link>https://fabiensanglard.net/st/index.html</link>
            <guid>37662655</guid>
            <pubDate>Tue, 26 Sep 2023 17:21:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fabiensanglard.net/st/index.html">https://fabiensanglard.net/st/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=37662655">Hacker News</a></p>
<div id="readability-page-1" class="page"><br><center>
    
</center><p>
Sep 26, 2023</p>
<p>Exploring Linux command-line space time</p><hr>

<p>I was curious to explore how long a program takes to run, how much memory is used over time, and what processes/threads are spawned. To provide answers I wrote a tool, which I named <code>st</code>. Here are a few things I looked into.</p>

<p>See details at the bottom if you are interested in how the tool works (and why I did not use <code><a href="https://man7.org/linux/man-pages/man1/time.1.html">time(1)</a></code> and <code>strace(1)</code>).</p>


<p>Fill</p><hr><p>An entertaining way to use <code>st</code> is so to predict the outcome of a command and explore the reasons for discrepancies. I started with a simple C program, <a href="https://fabiensanglard.net/st/fill.c">fill.c</a> allocating 1GiB and setting each byte individually.</p>

<!-- HTML generated using hilite.me -->
<pre><span>#include &lt;stdio.h&gt;</span>
<span>#include &lt;stdlib.h&gt;</span>
<span>#include &lt;stdint.h&gt;</span>

<span>void</span> <span>fill</span>(<span>uint8_t</span><span>*</span> addr, <span>size_t</span> amount, <span>char</span> value) {
    <span>for</span> (<span>size_t</span> i <span>=</span> <span>0</span>; i <span>&lt;</span> amount; i<span>++</span>) {
        <span>*</span>(addr <span>+</span> i) <span>=</span> value;
    }
}

<span>int</span> <span>main</span>(<span>int</span> argc, <span>char</span> <span>**</span>argv) {
    <span>size_t</span> s <span>=</span> <span>1</span><span>&lt;&lt;</span><span>30</span>;
    <span>uint8_t</span><span>*</span> buffer <span>=</span> (<span>uint8_t</span><span>*</span>)malloc(s);
    fill(buffer, s, atoi(argv[<span>0</span>]));
    <span>return</span> EXIT_SUCCESS;
}
</pre>



<p>Let's build and run it via <code>st</code>.</p>

<pre><b>$</b> clang -o fill fill.c
<b>$</b> sudo st fill
<span><span>EXEC</span>:</span> [/home/leaf/fill]
Num threads = 1
Num process = 1
Max PSS: 1,073,762,304 bytes
Walltime: 1,229ms - user-space: 1,109ms - kernel-space: 87ms
</pre>

<pre>  1┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                               ██████┃
   ┃                                                                         ████████████┃
   ┃                                                                    █████████████████┃
   ┃                                                               ██████████████████████┃
   ┃                                                         ████████████████████████████┃
   ┃                                                    █████████████████████████████████┃
   ┃                                              ███████████████████████████████████████┃
   ┫                                         ████████████████████████████████████████████┃
   ┃                                   ██████████████████████████████████████████████████┃
   ┃                             ████████████████████████████████████████████████████████┃
   ┃                        █████████████████████████████████████████████████████████████┃
   ┃                  ███████████████████████████████████████████████████████████████████┃
   ┃            █████████████████████████████████████████████████████████████████████████┃
   ┃       ██████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0GB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0s                                                                                    1</pre>

<p>I expected 1GiB to be filled in 10ms but it took a whole second. The issue is that the program is generating many page faults. I remember Firefox sped up their startup time 2x by solving a similar issue<a name="back_1" href="#footnote_1"><sup>[1]</sup></a>.</p>

<p>Also noteworthy, the memory usage did not increase abruptly from 0 to 1GiB with <code>malloc</code>. Instead we see a staircase pattern which shows that physical RAM consumption increased as virtual pages were written to by the <code>fill</code> function.</p>


<p>fillfill</p><hr><p>Another test program, <a href="https://fabiensanglard.net/st/fillfill.c">fillfill.c</a>, to check the tool is properly tracking PSS, processes, and threads.</p>

<!-- HTML generated using hilite.me -->
<pre><span>#include &lt;stdio.h&gt;</span>
<span>#include &lt;stdlib.h&gt;</span>
<span>#include &lt;unistd.h&gt;</span>
<span>#include &lt;sys/wait.h&gt;</span>
<span>#include &lt;stdint.h&gt;</span>

<span>void</span> <span>malloc_and_fill</span>(<span>size_t</span> s) {
  <span>uint8_t</span><span>*</span> buffer <span>=</span> (<span>uint8_t</span><span>*</span>) malloc(s);
  <span>for</span> (<span>size_t</span> i <span>=</span> <span>0</span>; i <span>&lt;</span> s; i<span>++</span>) {
    <span>*</span>(buffer<span>+</span> i) <span>=</span> <span>'F'</span>;
  }
  free(buffer);
}

<span>int</span> <span>main</span>(<span>int</span> argc, <span>char</span> <span>**</span>argv) {
  malloc_and_fill(<span>1L</span> <span>&lt;&lt;</span> <span>30</span>);
  <span>int</span> pid <span>=</span> fork();
  <span>if</span> (pid <span>!=</span> <span>0</span>) {
    malloc_and_fill(<span>1L</span> <span>&lt;&lt;</span> <span>31</span>);   
  } <span>else</span> {
    waitpid(pid, <span>NULL</span>, <span>0</span>);
  }
  <span>return</span> <span>0</span>;
}
</pre>


<p>First allocate 1GiB, free it, then spawn a process which does the same but with 2GiB.</p>

<pre><b>$</b> sudo st ./fillfill
<span>EXEC</span>: [./fillfill]
Num threads = 2
Num process = 2
Max PSS: 2,147,591,168 bytes
Walltime: 3,608ms - user-space: 3,277ms - kernel-space: 327ms</pre>

<pre>  2┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                                ███  ┃
   ┃                                                                            ███████  ┃
   ┃                                                                         ██████████  ┃
   ┃                                                                     ██████████████  ┃
   ┃                                                                 ███████████████████ ┃
   ┃                                                              ██████████████████████ ┃
   ┃                                                          ██████████████████████████ ┃
   ┫                          █                           ██████████████████████████████ ┃
   ┃                      ██████                       █████████████████████████████████ ┃
   ┃                   █████████                   █████████████████████████████████████ ┃
   ┃               █████████████               █████████████████████████████████████████ ┃
   ┃           █████████████████            ████████████████████████████████████████████ ┃
   ┃        ████████████████████        ████████████████████████████████████████████████ ┃
   ┃    ████████████████████████    ████████████████████████████████████████████████████ ┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0GB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0s                                                                                    3</pre>



<p>Fill -O3</p><hr><p>Let's get back to <a href="https://fabiensanglard.net/st/fill.c">fill.c</a> but this time compile it with full optimization (<code>-O3</code>).
  </p>

<pre><b>$</b> clang -o fillo <span>-O3</span> fill.c
<b>$</b> sudo st fillo
<span><span>EXEC</span>:</span>: [ /home/leaf/fillo]
Num threads = 1
Num process = 1
Max PSS: 164,864 bytes
Walltime: 5ms - user-space: 1ms - kernel-space: 0ms
</pre>
<pre>164┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┫█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0KB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                   0</pre>  

<p>Wow. From 1000ms to 5ms. And only using 164 KiB?! According to Binary Ninja<a name="back_2" href="#footnote_2"><sup>[2]</sup></a>, the compiler discarded both <code>malloc</code> and <code>fill</code>. It makes sense since the program did not read what was allocated and written. Flag <code>-O3</code> effectively turned <code><a href="https://fabiensanglard.net/st/fill.c">fill.c</a></code> into an empty <code>main</code> function.</p>

<pre><span>int</span> main(<span>int</span> argc, char <span>**</span>argv) {
    <span>return</span> EXIT_SUCCESS;
}
</pre>


<p><b><u>Sidenote:</u></b> Exploring compiler <code>-O3</code> outputs with Binary Ninja is a source of endless amazement. Loop to memset substitution (<a href="https://fabiensanglard.net/st/fill.c">fill.c</a>, <a href="https://fabiensanglard.net/st/fill.webp">cc</a>, <a href="https://fabiensanglard.net/st/fillo.webp">cc -O3</a>), <code>strlen</code> caching ...even without <code>-O3</code> (<a href="https://fabiensanglard.net/st/strlen.c">strlen.c</a>, <a href="https://cloud.binary.ninja/embed/44fb8393-c731-4cda-b412-5cee57989840">cc</a>, <a href="https://cloud.binary.ninja/embed/6c81b81f-6c3d-4ac2-8297-6c291d7667bb">cc -O3</a>), and <code>printf("%c", 'x')</code> to <code>putchar('x')</code>!) substitution are only a few among many cool optimizations.</p>




<p>clang helloworld.c</p><hr>
<p>I have written extensively about <a href="https://fabiensanglard.net/dc">compiler drivers</a> by the past so it was a good opportunity to double check that <code>clang</code> behaved as expected when compiling <a href="https://fabiensanglard.net/st/hello.c">hello.c</a>.</p>

<pre><span>#include &lt;stdio.h&gt;</span>
<span>int</span> <span>main</span>() {
   printf(<span>"Hello, World!"</span>);
   <span>return</span> <span>0</span>;
}
</pre>


<pre><b>$</b> sudo st clang -o hello hello.c
<span><span>EXEC</span>:</span>: [ clang -o hello hello.c]
<span><span>EXEC</span>:</span>: [/usr/lib/llvm-14/bin/clang -cc1 -triple aarch64-unknown-linux-gnu -emit-obj -mrelax-all --mrelax-relocations -disable-free -clear-ast-before-backend -disable-llvm-verifier -discard-value-names -main-file-name hello.c -mrelocation-model pic -pic-level 2 -pic-is-pie -mframe-pointer=non-leaf -fmath-errno -ffp-contract=on -fno-rounding-math -mconstructor-aliases -funwind-tables=2 -target-cpu generic -target-feature +neon -target-feature +v8a -target-abi aapcs -fallow-half-arguments-and-returns -mllvm -treat-scalable-fixed-error-as-warning -debugger-tuning=gdb -fcoverage-compilation-dir=/home/leaf -resource-dir /usr/lib/llvm-14/lib/clang/14.0.0 -internal-isystem /usr/lib/llvm-14/lib/clang/14.0.0/include -internal-isystem /usr/local/include -internal-isystem /usr/bin/../lib/gcc/aarch64-linux-gnu/11/../../../../aarch64-linux-gnu/include -internal-externc-isystem /usr/include/aarch64-linux-gnu -internal-externc-isystem /include -internal-externc-isystem /usr/include -fdebug-compilation-dir=/home/leaf]
<span><span>EXEC</span>:</span>: [/usr/bin/ld -pie -EL -z relro --hash-style=gnu --build-id --eh-frame-hdr -m aarch64linux -dynamic-linker /lib/ld-linux-aarch64.so.1 -o hello /lib/aarch64-linux-gnu/Scrt1.o /lib/aarch64-linux-gnu/crti.o /usr/bin/../lib/gcc/aarch64-linux-gnu/11/crtbeginS.o -L/usr/bin/../lib/gcc/aarch64-linux-gnu/11 -L/lib/aarch64-linux-gnu -L/usr/lib/aarch64-linux-gnu -L/usr/lib/llvm-14/bin/../lib -L/lib -L/usr/lib /tmp/hello-df1527.o -lgcc --as-needed -lgcc_s --no-as-needed -lc -lgcc --as-needed -lgcc_s --no-as-needed /usr/bin/../lib/gcc/aarch64-linux-gnu/11/crtendS.o /lib/aarch64-linux-gnu/crtn.o ]
Num threads = 3
Num process = 3
Max PSS: 90,911,744 bytes
Walltime: 71ms - user-space: 24ms - kernel-space: 45ms</pre>

<pre> 63┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                             ████████┃
   ┃                                                                           ██████████┃
   ┃                                                                      ███████████████┃
   ┃                                                                    █████████████████┃
   ┃                                                               ██████████████████████┃
   ┃                                                             ████████████████████████┃
   ┃                                                        █████████████████████████████┃
   ┫                                                 ████████████████████████████████████┃
   ┃                                               ██████████████████████████████████████┃
   ┃                                        █████████████████████████████████████████████┃
   ┃                                   ██████████████████████████████████████████████████┃
   ┃                            █████████████████████████████████████████████████████████┃
   ┃                         ████████████████████████████████████████████████████████████┃
   ┃                  ███████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                  36</pre>

<p>Without surprise, we see two forks. One when the driver invokes the compiler and one to invoke the linker. There is no assembler step since it is built-in <code>clang</code>.




</p><p>gcc helloworld.c</p><hr><p>After clang, let's check <code>gcc</code>.

</p><pre><b>$</b> sudo st gcc -o hello hello.c
<span><span>EXEC</span>:</span>: [ gcc -o hello hello.c]
<span><span>EXEC</span>:</span>: [/usr/lib/gcc/aarch64-linux-gnu/11/cc1 -quiet -imultiarch aarch64-linux-gnu hello.c -quiet -dumpbase hello.c -dumpbase-ext .c -mlittle-endian -mabi=lp64 -fasynchronous-unwind-tables -fstack-protector-strong -Wformat -Wformat-security -fstack-clash-protection -o /tmp/ccmt0cw6.s ]
<span><span>EXEC</span>:</span>: [as -EL -mabi=lp64 -o /tmp/ccSRW1gq.o /tmp/ccmt0cw6.s ]
<span><span>EXEC</span>:</span>: [/usr/lib/gcc/aarch64-linux-gnu/11/collect2 -plugin /usr/lib/gcc/aarch64-linux-gnu/11/liblto_plugin.so -plugin-opt=/usr/lib/gcc/aarch64-linux-gnu/11/lto-wrapper -plugin-opt=-fresolution=/tmp/cceZH2hF.res -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lc -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lgcc_s --build-id --eh-frame-hdr --hash-style=gnu --as-needed -dynamic-linker /lib/ld-linux-aarch64.so.1 -X -EL -maarch64linux --fix-cortex-a53-843419 -pie -z now -z relro -o hello /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/Scrt1.o /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/crti.o /usr/lib/gcc/aarch64-linux-gnu/11/crtbeginS.o -L/usr/lib/gcc/aarch64-linux-gnu/11 -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../../lib -L/lib/aarch64-linux-gnu -L/lib/../lib -L/usr/lib/aarch64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/aarch64-linux-gnu/11/../../.. /tmp/ccSRW1gq.o -lg]
<span><span>EXEC</span>:</span>: [/usr/bin/ld -plugin /usr/lib/gcc/aarch64-linux-gnu/11/liblto_plugin.so -plugin-opt=/usr/lib/gcc/aarch64-linux-gnu/11/lto-wrapper -plugin-opt=-fresolution=/tmp/cceZH2hF.res -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lc -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lgcc_s --build-id --eh-frame-hdr --hash-style=gnu --as-needed -dynamic-linker /lib/ld-linux-aarch64.so.1 -X -EL -maarch64linux --fix-cortex-a53-843419 -pie -z now -z relro -o hello /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/Scrt1.o /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/crti.o /usr/lib/gcc/aarch64-linux-gnu/11/crtbeginS.o -L/usr/lib/gcc/aarch64-linux-gnu/11 -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../../lib -L/lib/aarch64-linux-gnu -L/lib/../lib -L/usr/lib/aarch64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/aarch64-linux-gnu/11/../../.. /tmp/ccSRW1gq.o -lgcc --push-state --as-needed -lg]
Num threads = 5
Num process = 5
Max PSS: 18,597,888 bytes
Walltime: 51ms - user-space: 20ms - kernel-space: 20ms</pre>

<pre> 17┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                          ███████                                    ┃
   ┃                                          ███████                                    ┃
   ┃                                        █████████                                    ┃
   ┃                                        █████████                                    ┃
   ┃                                   ██████████████                                    ┃
   ┃                              ███████████████████                                    ┃
   ┃                         ████████████████████████                                    ┃
   ┫                       ██████████████████████████                            ███████ ┃
   ┃                     ████████████████████████████                          █████████ ┃
   ┃                  ███████████████████████████████                        ███████████ ┃
   ┃                  ███████████████████████████████                   ████████████████ ┃
   ┃              ███████████████████████████████████     █████       ██████████████████ ┃
   ┃           ██████████████████████████████████████  ████████    █████████████████████ ┃
   ┃         ████████████████████████████████████████  ██████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                  36</pre>

<p><code>gcc</code> uses two more steps than <code>clang</code>. One for the self-explanatory assembler <code>as</code> and one for the cryptic <code>collect2</code>. I read the <a href="https://gcc.gnu.org/onlinedocs/gccint/Collect2.html">documentation page</a> but I still don't understand what it does.</p>

<p>Noteworthy, <code>gcc</code> uses four times less memory than <code>clang</code>.</p>

















<p>clang++ hello.cc</p><hr><p>Let's checkout the C++ compiler driver from LLVM suite with <code><a href="https://fabiensanglard.net/st/hello.cc">hello.cc</a></code>.</p>

<pre><span>#include &lt;iostream&gt;</span>

<span>int</span> <span>main</span>() {
    std<span>::</span>cout <span>&lt;&lt;</span> <span>"Hello World!"</span>;
    <span>return</span> <span>0</span>;
}
</pre>


<pre><b>$</b> sudo st clang++ hello.cc
<span>EXEC</span>: [ clang++ /home/leaf/hello.cc]
<span>EXEC</span>: [/usr/lib/llvm-14/bin/clang -cc1 -triple aarch64-unknown-linux-gnu -emit-obj -mrelax-all --mrelax-relocations -disable-free -clear-ast-before-backend -disable-llvm-verifier -discard-value-names -main-file-name hello.cc -mrelocation-model pic -pic-level 2 -pic-is-pie -mframe-pointer=non-leaf -fmath-errno -ffp-contract=on -fno-rounding-math -mconstructor-aliases -funwind-tables=2 -target-cpu generic -target-feature +neon -target-feature +v8a -target-abi aapcs -fallow-half-arguments-and-returns -mllvm -treat-scalable-fixed-error-as-warning -debugger-tuning=gdb -fcoverage-compilation-dir=/home/leaf/repos/st -resource-dir /usr/lib/llvm-14/lib/clang/14.0.0 -internal-isystem /usr/bin/../lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11 -internal-isystem /usr/bin/../lib/gcc/aarch64-linux-gnu/11/../../../../include/aarch64-linux-gnu/c++/11 -internal-isystem /usr/bin/../lib/gcc/aarch64-linux-gnu/11/../../../../include/c++/11/backward -internal-isystem /usr/lib/llvm-14/lib/clang/14.0.0/include -internal-isystem /u]
<span>EXEC</span>: [/usr/bin/ld -pie -EL -z relro --hash-style=gnu --build-id --eh-frame-hdr -m aarch64linux -dynamic-linker /lib/ld-linux-aarch64.so.1 -o a.out /lib/aarch64-linux-gnu/Scrt1.o /lib/aarch64-linux-gnu/crti.o /usr/bin/../lib/gcc/aarch64-linux-gnu/11/crtbeginS.o -L/usr/bin/../lib/gcc/aarch64-linux-gnu/11 -L/lib/aarch64-linux-gnu -L/usr/lib/aarch64-linux-gnu -L/usr/lib/llvm-14/bin/../lib -L/lib -L/usr/lib /tmp/hello-32e667.o -lstdc++ -lm -lgcc_s -lgcc -lc -lgcc_s -lgcc /usr/bin/../lib/gcc/aarch64-linux-gnu/11/crtendS.o /lib/aarch64-linux-gnu/crtn.o ]
Num threads = 3
Num process = 3
Max PSS: 127,156,224 bytes
Walltime: 221ms - user-space: 136ms - kernel-space: 69ms</pre>

<pre>127┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃                                                          █████████████              ┃
   ┃            █                    ██████████████████████████████████████              ┃
   ┃            █           ███████████████████████████████████████████████              ┃
   ┃            █         █████████████████████████████████████████████████      █ ████  ┃
   ┃            █  ████████████████████████████████████████████████████████  ████████████┃
   ┫            ███████████████████████████████████████████████████████████ █████████████┃
   ┃           ██████████████████████████████████████████████████████████████████████████┃
   ┃          ███████████████████████████████████████████████████████████████████████████┃
   ┃        █████████████████████████████████████████████████████████████████████████████┃
   ┃      ███████████████████████████████████████████████████████████████████████████████┃
   ┃    █████████████████████████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                 221</pre>

<p>No surprises here either. Compiling <code>C++</code> is much more costly in terms of memory (2x), and takes much longer than <code><a href="https://fabiensanglard.net/st/hello.c">hello.c</a></code> (4x).</p>


<p>g++ hello.cc</p><hr>
<p>Let's checkout the C++ compiler driver from GNU suite, <code>g++</code>.</p>

<pre><b>$</b> sudo st g++ hello.cc
<span>EXEC</span>: [ g++ /home/leaf/hello.cc]
<span>EXEC</span>: [/usr/lib/gcc/aarch64-linux-gnu/11/cc1plus -quiet -imultiarch aarch64-linux-gnu -D_GNU_SOURCE /home/leaf/hello.cc -quiet -dumpdir a- -dumpbase hello.cc -dumpbase-ext .cc -mlittle-endian -mabi=lp64 -fasynchronous-unwind-tables -fstack-protector-strong -Wformat -Wformat-security -fstack-clash-protection -o /tmp/ccBFHbve.s ]
<span>EXEC</span>: [as -EL -mabi=lp64 -o /tmp/cckOP2RY.o /tmp/ccBFHbve.s ]
<span>EXEC</span>: [/usr/lib/gcc/aarch64-linux-gnu/11/collect2 -plugin /usr/lib/gcc/aarch64-linux-gnu/11/liblto_plugin.so -plugin-opt=/usr/lib/gcc/aarch64-linux-gnu/11/lto-wrapper -plugin-opt=-fresolution=/tmp/ccpZysdX.res -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lc -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lgcc --build-id --eh-frame-hdr --hash-style=gnu --as-needed -dynamic-linker /lib/ld-linux-aarch64.so.1 -X -EL -maarch64linux --fix-cortex-a53-843419 -pie -z now -z relro /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/Scrt1.o /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/crti.o /usr/lib/gcc/aarch64-linux-gnu/11/crtbeginS.o -L/usr/lib/gcc/aarch64-linux-gnu/11 -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../../lib -L/lib/aarch64-linux-gnu -L/lib/../lib -L/usr/lib/aarch64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/aarch64-linux-gnu/11/../../.. /tmp/cckOP2RY.o -lstdc++ -lm]
<span>EXEC</span>: [/usr/bin/ld -plugin /usr/lib/gcc/aarch64-linux-gnu/11/liblto_plugin.so -plugin-opt=/usr/lib/gcc/aarch64-linux-gnu/11/lto-wrapper -plugin-opt=-fresolution=/tmp/ccpZysdX.res -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lgcc -plugin-opt=-pass-through=-lc -plugin-opt=-pass-through=-lgcc_s -plugin-opt=-pass-through=-lgcc --build-id --eh-frame-hdr --hash-style=gnu --as-needed -dynamic-linker /lib/ld-linux-aarch64.so.1 -X -EL -maarch64linux --fix-cortex-a53-843419 -pie -z now -z relro /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/Scrt1.o /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/crti.o /usr/lib/gcc/aarch64-linux-gnu/11/crtbeginS.o -L/usr/lib/gcc/aarch64-linux-gnu/11 -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../../lib -L/lib/aarch64-linux-gnu -L/lib/../lib -L/usr/lib/aarch64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/aarch64-linux-gnu/11/../../.. /tmp/cckOP2RY.o -lstdc++ -lm -lgcc_s -lgcc -lc -lgcc_s -lgc]
Num threads = 5
Num process = 5
Max PSS: 60,565,504 bytes
Walltime: 232ms - user-space: 127ms - kernel-space: 48ms</pre>

<pre> 60┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                 ████                ┃
   ┃                                                             ████████                ┃
   ┃                                                        █████████████                ┃
   ┃                                                    █████████████████                ┃
   ┃                                               ██████████████████████                ┃
   ┃                                            █████████████████████████                ┃
   ┃                                         ████████████████████████████                ┃
   ┫                                      ███████████████████████████████                ┃
   ┃                                 ████████████████████████████████████                ┃
   ┃                            █████████████████████████████████████████                ┃
   ┃                        █████████████████████████████████████████████          █████ ┃
   ┃                    █████████████████████████████████████████████████         ██████ ┃
   ┃                █████████████████████████████████████████████████████       █████████┃
   ┃           ██████████████████████████████████████████████████████████     ███████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                 232</pre>

<p>Alike the C driver, <code>gnu</code>'s version requires twice less RAM than <code>clang++</code>.</p>

<p>rust hello.rs</p><hr><p><code>rustc</code> has a reputation to be slow and memory hungry. Seems appropriate since it ran for nearly 200ms and used 131 MiB of RAM (roughly the same as <code>clang++ hello.cc</code>).</p>

<!-- HTML generated using hilite.me -->
<pre><span>fn</span> main() {
    println<span>!</span>(<span>"Hello World!"</span>);
}
</pre>


<p><code>rustc</code> uses an embedded LLVM backend to generates object files. These are handed to the regular <code>gnu</code> suite with <code>collect2</code> and then the linker <code>ld</code>.

</p><pre><b>$</b> sudo st rustc hello.rs 
<span><span>EXEC</span>:</span>: [rustc hello.rs]
<span><span>EXEC</span>:</span>: [cc /tmp/rustc4fdpuV/symbols.o hello.hello.f76cf86e-cgu.0.rcgu.o hello.hello.f76cf86e-cgu.1.rcgu.o hello.hello.f76cf86e-cgu.2.rcgu.o hello.hello.f76cf86e-cgu.3.rcgu.o hello.hello.f76cf86e-cgu.4.rcgu.o hello.hello.f76cf86e-cgu.5.rcgu.o hello.2pelail77dwjevsg.rcgu.o -Wl,--as-needed -L /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib -Wl,-Bstatic /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib/libstd-1d2bb2d795f2ca05.rlib /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib/libpanic_unwind-f1c586c276421094.rlib /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib/libobject-8060a154fd842f2c.rlib /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib/libmemchr-1f7fc15c78d3bcac.rlib /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib/libaddr2line-965bde82ccc4b5e8.rlib /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib/libgimli-352be989f07a059f.rlib /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib/librustc_demangle-4f461a85c762abbb.rlib /usr/lib/rustlib/aarch64-unknown-linux-gnu/lib/libstd_detect-814ef12b1b7d93c2.rlib /usr/lib/rustlib/aarch64-unknow]
<span><span>EXEC</span>:</span>: [/usr/lib/gcc/aarch64-linux-gnu/11/collect2 -plugin /usr/lib/gcc/aarch64-linux-gnu/11/liblto_plugin.so -plugin-opt=/usr/lib/gcc/aarch64-linux-gnu/11/lto-wrapper -plugin-opt=-fresolution=/tmp/ccXRoDto.res --build-id --eh-frame-hdr --hash-style=gnu --as-needed -dynamic-linker /lib/ld-linux-aarch64.so.1 -X -EL -maarch64linux --fix-cortex-a53-843419 -pie -z now -z relro -o hello /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/Scrt1.o /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/crti.o /usr/lib/gcc/aarch64-linux-gnu/11/crtbeginS.o -L/usr/lib/rustlib/aarch64-unknown-linux-gnu/lib -L/usr/lib/rustlib/aarch64-unknown-linux-gnu/lib -L/usr/lib/gcc/aarch64-linux-gnu/11 -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../../lib -L/lib/aarch64-linux-gnu -L/lib/../lib -L/usr/lib/aarch64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/aarch64-linux-gnu/11/../../.. /tmp/rustc4fdpuV/symbols.o hello.hello.f76cf86e-cgu.0.rcgu.o hello.hello.f76cf86e-c]
<span><span>EXEC</span>:</span>: [/usr/bin/ld -plugin /usr/lib/gcc/aarch64-linux-gnu/11/liblto_plugin.so -plugin-opt=/usr/lib/gcc/aarch64-linux-gnu/11/lto-wrapper -plugin-opt=-fresolution=/tmp/ccXRoDto.res --build-id --eh-frame-hdr --hash-style=gnu --as-needed -dynamic-linker /lib/ld-linux-aarch64.so.1 -X -EL -maarch64linux --fix-cortex-a53-843419 -pie -z now -z relro -o hello /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/Scrt1.o /usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu/crti.o /usr/lib/gcc/aarch64-linux-gnu/11/crtbeginS.o -L/usr/lib/rustlib/aarch64-unknown-linux-gnu/lib -L/usr/lib/rustlib/aarch64-unknown-linux-gnu/lib -L/usr/lib/gcc/aarch64-linux-gnu/11 -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../aarch64-linux-gnu -L/usr/lib/gcc/aarch64-linux-gnu/11/../../../../lib -L/lib/aarch64-linux-gnu -L/lib/../lib -L/usr/lib/aarch64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/aarch64-linux-gnu/11/../../.. /tmp/rustc4fdpuV/symbols.o hello.hello.f76cf86e-cgu.0.rcgu.o hello.hello.f76cf86e-cgu.1.rcgu.o hello.hello.f76cf86]
Num threads = 14
Num process = 4
Max PSS: 131,376,128 bytes
Walltime: 197ms - user-space: 134ms - kernel-space: 67ms</pre>


<pre>133┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                  ████████████████████████████████   ┃
   ┃                                     █████████████████████████████████████████████   ┃
   ┃                         █        ████████████████████████████████████████████████   ┃
   ┃                     █████      ██████████████████████████████████████████████████   ┃
   ┃                   ███████   █████████████████████████████████████████████████████   ┃
   ┃                  █████████████████████████████████████████████████████████████████  ┃
   ┃                 ██████████████████████████████████████████████████████████████████  ┃
   ┫                 ██████████████████████████████████████████████████████████████████  ┃
   ┃               ████████████████████████████████████████████████████████████████████  ┃
   ┃              █████████████████████████████████████████████████████████████████████  ┃
   ┃             ████████████████████████████████████████████████████████████████████████┃
   ┃           ██████████████████████████████████████████████████████████████████████████┃
   ┃        █████████████████████████████████████████████████████████████████████████████┃
   ┃     ████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                 220</pre>

<p>Notice that fourteen threads were created. Ten of then are coming from <code>rustc</code>.</p>




<p>javac helloworld.java</p><hr><pre><b>$</b> cat HelloWorld.java 
class HelloWorld {
    public static void main(String[] args) {
        System.out.println("Hello, World!"); 
    }
}
<b>$</b> sudo st javac HelloWorld.java
<span><span>EXEC</span>:</span>: [ javac HelloWorld.java]
Num threads = 23
Num process = 1
Max PSS: 75,673,600 bytes
Walltime: 272ms - user-space: 472ms - kernel-space: 48ms</pre>


<pre> 75┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                          ███████████┃
   ┃                                                               ██████████████████████┃
   ┃                                                       ██████████████████████████████┃
   ┃                                             ████████████████████████████████████████┃
   ┃                                      ███████████████████████████████████████████████┃
   ┃                               ██████████████████████████████████████████████████████┃
   ┃                        █████████████████████████████████████████████████████████████┃
   ┫                 ████████████████████████████████████████████████████████████████████┃
   ┃             ████████████████████████████████████████████████████████████████████████┃
   ┃          ███████████████████████████████████████████████████████████████████████████┃
   ┃       ██████████████████████████████████████████████████████████████████████████████┃
   ┃       ██████████████████████████████████████████████████████████████████████████████┃
   ┃       ██████████████████████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                 263</pre>


<p>Since the compiler is written in Java, I was expecting to see a second process, execing something like <code>java -cp javac.jar java.tools.Javac ...</code>. Surprisingly, javac is actually a JNI wrapper which acts as a launcher<a name="back_3" href="#footnote_3"><sup>[3]</sup></a>. It instantiates a VM in-process and there is no fork/exec needed.</p>


<p>Also surprising is the number of threads created. Although the next test shows that it is probably just a thread pool created at startup regardless of what the VM actually does.</p>


<p>java Helloworld</p><hr><pre><b>$</b> sudo st java -cp . HelloWorld
<span>EXEC</span>: [ java -cp /home/leaf HelloWorld]
Hello, World!
Num threads = 19
Num process = 1
Max PSS: 30,813,184 bytes
Walltime: 38ms - user-space: 10ms - kernel-space: 21ms</pre>


<pre> 30┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                   █████████████     ┃
   ┃                                                          ██████████████████████     ┃
   ┃                                                   █████████████████████████████     ┃
   ┃                                      ██████████████████████████████████████████     ┃
   ┃                                      ██████████████████████████████████████████     ┃
   ┃                                   █████████████████████████████████████████████     ┃
   ┃                                   █████████████████████████████████████████████     ┃
   ┫                                 ███████████████████████████████████████████████     ┃
   ┃                             ███████████████████████████████████████████████████     ┃
   ┃                          ███████████████████████████████████████████████████████████┃
   ┃                      ███████████████████████████████████████████████████████████████┃
   ┃                    █████████████████████████████████████████████████████████████████┃
   ┃                 ████████████████████████████████████████████████████████████████████┃
   ┃             ████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                  38</pre>

<p>Same elevated number of thread as <code>javac</code>. Likely, <code>javac</code> only created four threads (23-19=4).</p>

<p>go build helloworld.go</p><hr><p><code>go</code> stood up to its reputation of being fast. It generated an executable in 54ms while using only 15MiB of RAM. In this limited helloworld study, it is both the fastest and the least RAM hungry compiler.</p>

<!-- HTML generated using hilite.me -->
<pre><span>package</span> main
<span>import</span> <span>"fmt"</span>
<span>func</span> main() {
    fmt.Println(<span>"hello world"</span>)
}
</pre>


<pre><b>$</b> sudo st go build helloworld.go 
<span><span>EXEC</span>:</span>: [ go build helloworld.go]
Num threads = 9
Num process = 1
Max PSS: 15,591,424 bytes
Walltime: 54ms - user-space: 25ms - kernel-space: 39ms
</pre>


<pre> 15┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                █████████████████████████████████████┃
   ┃                                      ███████████████████████████████████████████████┃
   ┃                                    █████████████████████████████████████████████████┃
   ┃                                █████████████████████████████████████████████████████┃
   ┃                                █████████████████████████████████████████████████████┃
   ┃                           ██████████████████████████████████████████████████████████┃
   ┃                         ████████████████████████████████████████████████████████████┃
   ┫                         ████████████████████████████████████████████████████████████┃
   ┃                      ███████████████████████████████████████████████████████████████┃
   ┃                      ███████████████████████████████████████████████████████████████┃
   ┃                      ███████████████████████████████████████████████████████████████┃
   ┃                █████████████████████████████████████████████████████████████████████┃
   ┃        █████████████████████████████████████████████████████████████████████████████┃
   ┃      ███████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                  53</pre>



<p>Golang relies heavily on goroutines, I was surprised to see nine threads being used on a four core system. Something to look into someday.</p>






<p>Opening Chromium</p><hr><pre><b>$</b> sudo st chromium --headless https://fabiensanglard.net/index.html
<span>EXEC</span>: [ chromium https://fabiensanglard.net/index.html]
<span>EXEC</span>: [ chromium https://fabiensanglard.net/index.html]
<span>EXEC</span>: [/snap/snapd/20102/usr/lib/snapd/snap-seccomp version-info ]
Num threads = 21
Num process = 2
Max PSS: 32,493,568 bytes
Walltime: 51ms - user-space: 639ms - kernel-space: 565ms
</pre>


<pre> 32┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃           █                                                                         ┃
   ┃           █                                                                         ┃
   ┃           █ ████████████████████████████████████████████████████████████████████████┃
   ┫           ██████████████████████████████████████████████████████████████████████████┃
   ┃          ███████████████████████████████████████████████████████████████████████████┃
   ┃     █    ███████████████████████████████████████████████████████████████████████████┃
   ┃   ████ █████████████████████████████████████████████████████████████████████████████┃
   ┃   ████ █████████████████████████████████████████████████████████████████████████████┃
   ┃   ████ █████████████████████████████████████████████████████████████████████████████┃
   ┃  ███████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                 280</pre>

<p>The process spawned by Chromium is for the GPU render. It looks like it also executed a command to check the version of <a href="https://en.wikipedia.org/wiki/Seccomp">seccomp</a> before sandboxing the GPU.</p>


<p>curl</p><hr><pre><b>$</b> sudo st curl https://fabiensanglard.net/index.html 
<span>EXEC</span>: [ curl https://fabiensanglard.net/index.html]
Num threads = 2
Num process = 1
Max PSS: 4,878,336 bytes
Walltime: 356ms - user-space: 47ms - kernel-space: 7ms
</pre>


<pre>  4┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                █████████████████████████████████████████████████████┃
   ┃                             ████████████████████████████████████████████████████████┃
   ┃                         ████████████████████████████████████████████████████████████┃
   ┃                       ██████████████████████████████████████████████████████████████┃
   ┃                       ██████████████████████████████████████████████████████████████┃
   ┃                       ██████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┫   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃  ███████████████████████████████████████████████████████████████████████████████████┃
   ┃  ███████████████████████████████████████████████████████████████████████████████████┃
   ┃  ███████████████████████████████████████████████████████████████████████████████████┃
   ┃ ████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                 356</pre>


<p>wget</p><hr><pre><b>$</b> sudo st wget https://fabiensanglard.net/index.html 
<span>EXEC</span>: [ wget https://fabiensanglard.net/index.html]
Num threads = 1
Num process = 1
Max PSS: 2,942,976 bytes
Walltime: 348ms - user-space: 8ms - kernel-space: 8ms
</pre>


<pre>  2┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                        █████████████████████████████████████████████┃
   ┃                      ███████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃   ██████████████████████████████████████████████████████████████████████████████████┃
   ┫   ██████████████████████████████████████████████████████████████████████████████████┃
   ┃  ███████████████████████████████████████████████████████████████████████████████████┃
   ┃  ███████████████████████████████████████████████████████████████████████████████████┃
   ┃  ███████████████████████████████████████████████████████████████████████████████████┃
   ┃  ███████████████████████████████████████████████████████████████████████████████████┃
   ┃ ████████████████████████████████████████████████████████████████████████████████████┃
   ┃ ████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0ms                                                                                 348</pre>

<p><code>wget</code> seems to use less RAM than <code>curl</code>. Also, it uses one thread instead of two.</p>



<p>git make</p><hr>
<p>Let's build a medium size project, <code>git</code>, with <code>make</code>.</p>

<pre><b>$</b> sudo apt-get install dh-autoreconf libcurl4-gnutls-dev libexpat1-dev gettext libz-dev libssl-dev
<b>$</b> sudo apt-get install asciidoc xmlto docbook2x
<b>$</b> gh repo clone git/git
<b>$</b> cd git
<b>$</b> make configure
<b>$</b> ./configure --prefix=/usr
<b>$</b> sudo st make all
...
</pre>

<pre>Num threads = 3,155
Num process = 3,155
Max PSS: 144,361,472 bytes
Walltime: 95,427ms - user-space: 83,485ms - kernel-space: 10,995ms
</pre>


<pre>144┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃                                   █                                                 ┃
   ┃  █        █                       █                              █                  ┃
   ┫  █        ██       █             ███             █        █      █                  ┃
   ┃  █ █ ███  ███ █    ██ █     ███  ███     █  █    █ █ █ █  ██  █  █ ██  █            ┃
   ┃ ████ ████████ ██ █████████████████████  ███ ██  ██████ ██ █████ █████████   █       ┃
   ┃ ██████████████████████████████████████████████████████████████████████████  ████████┃
   ┃████████████████████████████████████████████████████████████████████████████ ████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0s                                                                                   95</pre>

<p>git make -j8</p><hr><p>Let's observe how parallel compilation, <code>-j4</code>,trades walltime with RAM/cores.</p>

<pre><b>$</b> git clean
<b>$</b> sudo st make <span>-j4</span> all
...
Num threads = 3155
Num process = 3155
Max PSS: 288,531,456 bytes
Walltime: 27,556ms - user-space: 90,311ms - kernel-space: 11,458ms
</pre>


<pre>
288┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃               █                                  █                                  ┃
   ┃               █     █                █           █                                  ┃
   ┃     █       █ █     ██       █    █ ██           █       █      █                   ┃
   ┃    ██ █     ███     ██       █  ██████      █    █ █ █   █      █                   ┃
   ┃   ███ ████ ████     ███      ██ ██████   █  █   ██ █ █  ████  █████  █              ┃
   ┫  ████ ████ █████ █ ███████████████████  ██  █  █████ ██ ██████████████  █           ┃
   ┃  █████████████████████████████████████ ██████  ██████████████████████████   █       ┃
   ┃  ████████████████████████████████████████████████████████████████████████   ██ ███  ┃
   ┃  █████████████████████████████████████████████████████████████████████████  ███████ ┃
   ┃  █████████████████████████████████████████████████████████████████████████ █████████┃
   ┃  █████████████████████████████████████████████████████████████████████████ █████████┃
   ┃ ██████████████████████████████████████████████████████████████████████████ █████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0MB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0s                                                                                   27</pre>

<p>Using four cores, dropped runtime nearly linearly (3.5) and doubled RAM usage.</p>

<p>m</p><hr><p>The latest command I wanted to observe was <code>m</code> which builds AOSP.</p>
<pre><b>$</b> cd aosp_24
<b>$</b> cat &gt; source build/envsetup.sh
source build/envsetup.sh
lunch aosp_arm64-eng
m -j8
<b>$</b> chmod +x make.sh
<b>$</b> sudo st make.sh
...
</pre>
<pre>Num threads = 136,305
Num process = 132,891
Max PSS: 6,152,478,720 bytes
Walltime: 700864ms - user-space: 9942532ms - kernel-space: 900950ms
</pre>


<pre>  6┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
   ┃                                                                                     ┃
   ┃                                                                                     ┃
   ┃                                       █                                             ┃
   ┃                                      ██                                       █     ┃
   ┃                  █████               ███                     █                █     ┃
   ┃                  ███████         ███████                     █ █   █          ██    ┃
   ┃                  ███████████ █ █ ███████                     █ █████          ██    ┃
   ┫                  ███████████████████████                     ████████         ██    ┃
   ┃                 ████████████████████████          ██         ████████         ██    ┃
   ┃          █ █   █████████████████████████        ██████   █   ██████████   █ █ ███ ██┃
   ┃          █ ██ ██████████████████████████        ██████████ █ ██████████████ ████████┃
   ┃  █   ███ ████████████████████████████████       ████████████████████████████████████┃
   ┃ ████ ███████████████████████████████████████   █████████████████████████████████████┃
   ┃ ████████████████████████████████████████████████████████████████████████████████████┃
   ┃█████████████████████████████████████████████████████████████████████████████████████┃
0GB┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
   0s                                                                                  700</pre>

<p>6 GiB and 11 minutes to build a whole OS with 8 cores. Not bad at all!</p>


<p>st internals</p><hr><p>Before writing the tool, I considered relying on <code><a href="https://man7.org/linux/man-pages/man1/time.1.html">time(1)</a></code> and <code>strace(1)</code>. However I was not completely satisfied with them. If <code>time(1)</code> invoked via <code>/usr/bin/time -v</code> could retrieve child processes stats and show peak RSS, I wanted PSS. Also, I wanted to see PSS over time.</p>

<p>Likewise, <code>strace -f</code> could follow child processes but it had a tremendous performance overhead and did not show the command invoked by <code>clone</code> upon fork/thread creation.</p>

<p>In the end, I wrote my own tool and called it <code>st</code> (for space-time). The data sources are as follows.</p>

<ul>
<li>CPU time/ Kernel time relies on <code><a href="https://linux.die.net/man/2/waitpid">waitpid(2)</a></code> and <code><a href="https://man7.org/linux/man-pages/man2/getrusage.2.html">getrusage(2)</a></code>.</li>

<li>PSS over time relies on sampling <code><a href="https://man7.org/linux/man-pages/man5/proc.5.html">proc(5)</a></code>(<code>/proc/PID/smaps</code>) on a regular interval.</li>

<li>Following process/thread creation is done via <code><a href="https://man7.org/linux/man-pages/man7/netlink.7.html">netlink(7)</a></code>.</li>

</ul>

<p>Working with netlink</p><hr><p>Following processes lifecycle via netlink proved more challenging than expected, mainly because of poor documentation (maybe because netlink needs <code>root</code> anyway?). If I managed to dig out <a href="https://fabiensanglard.net/st/process-events.txt">process-events.txt</a> from a mailing list and found a working sample <a href="https://fabiensanglard.net/st/exec-notify.c">exec-notify.c</a>, there wasn't much more around.</p>

<p>One series of articles did stand out. Natan Yellin's blog<a name="back_4" href="#footnote_4"><sup>[4]</sup></a><a name="back_5" href="#footnote_5"><sup>[5]</sup></a><a name="back_6" href="#footnote_6"><sup>[6]</sup></a><a name="back_7" href="#footnote_7"><sup>[7]</sup></a>. is a gem. Among many excellent insights, he clarified<a name="back_8" href="#footnote_8"><sup>[8]</sup></a> the less than intuitive values provided by netlink for parent-gid and parent-pid which saved me a lot of time.

</p>

<p>Hopefully, <code>st</code>'s(<a href="https://github.com/fabiensanglard/st">source code</a>) will add a little extra clarity to <code>netlink</code>.</p>


<p>References</p><hr><p id="paperbox"><table><tbody><tr><td><a name="footnote_1"></a><a href="#back_1">^</a></td><td> [1]</td><td><a href="https://bugzilla.mozilla.org/show_bug.cgi?id=627591">Firefox Bugzilla: Preload dlls on windows</a></td></tr><tr><td><a name="footnote_2"></a><a href="#back_2">^</a></td><td> [2]</td><td><a href="https://binary.ninja/">Binary Ninja</a></td></tr><tr><td><a name="footnote_3"></a><a href="#back_3">^</a></td><td> [3]</td><td><a href="https://gist.github.com/mauricio/2310831">java-launcher.c</a></td></tr><tr><td><a name="footnote_4"></a><a href="#back_4">^</a></td><td> [4]</td><td><a href="https://natanyellin.com/posts/life-and-death-of-a-linux-process">
Life and Death of a Linux Process</a></td></tr><tr><td><a name="footnote_5"></a><a href="#back_5">^</a></td><td> [5]</td><td><a href="https://natanyellin.com/posts/using-linux-audit-to-track-processes">
Using the Linux Audit API to Track Processes</a></td></tr><tr><td><a name="footnote_6"></a><a href="#back_6">^</a></td><td> [6]</td><td><a href="https://natanyellin.com/posts/buggy-netlink-process-connectors">
When Netlink Process Connectors Don’t Process</a></td></tr><tr><td><a name="footnote_7"></a><a href="#back_7">^</a></td><td> [7]</td><td><a href="https://natanyellin.com/posts/tracking-running-processes-on-linux">
The Difficulties of Tracking Running Processes on Linux</a></td></tr><tr><td><a name="footnote_8"></a><a href="#back_8">^</a></td><td> [8]</td><td><a href="https://natanyellin.com/posts/understanding-netlink-process-connector-output">
Understanding Netlink Process Connector Output</a></td></tr></tbody></table></p> <hr>
 <center>*</center></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vizro – toolkit for creating modular data visualization applications (121 pts)]]></title>
            <link>https://github.com/mckinsey/vizro</link>
            <guid>37662561</guid>
            <pubDate>Tue, 26 Sep 2023 17:15:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mckinsey/vizro">https://github.com/mckinsey/vizro</a>, See on <a href="https://news.ycombinator.com/item?id=37662561">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">
<themed-picture data-catalyst-inline="true"><picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/Vizro_Github_Banner_Dark_Mode.png">
  <source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/Vizro_Github_Banner_Light_Mode.png">
  <img alt="Vizro logo" src="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/Vizro_Github_Banner_Dark_Mode.png" width="250">
</picture></themed-picture>
</p>
<p dir="auto"><a href="https://pypi.org/project/vizro/" rel="nofollow"><img src="https://camo.githubusercontent.com/8ca8bc983e08b8a9e2151d9c9ad6f7ee06f67c6dad6b4ea8a587bf6688c163af/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e38253230253743253230332e39253230253743253230332e3130253230253743253230332e31312d626c75652e737667" alt="Python version" data-canonical-src="https://img.shields.io/badge/python-3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11-blue.svg"></a>
<a href="https://badge.fury.io/py/vizro" rel="nofollow"><img src="https://camo.githubusercontent.com/c76ace32362608f82e9626febb37e5364c1936354215f9d5845e67bd54fe5dcf/68747470733a2f2f62616467652e667572792e696f2f70792f76697a726f2e737667" alt="PyPI version" data-canonical-src="https://badge.fury.io/py/vizro.svg"></a>
<a href="https://github.com/mckinsey/vizro/blob/main/LICENSE.md"><img src="https://camo.githubusercontent.com/1698104e976c681143eb0841f9675c6f802bb7aa832afc0c7a4e719b1f3cf955/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d417061636865253230322e302d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/license-Apache%202.0-blue.svg"></a>
<a href="https://vizro.readthedocs.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/c1972d2d3ad60b878b4bd23ce0def2ee99b8cb83d50ffe829f6192ffced2b9ec/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f76697a726f2f62616467652f3f76657273696f6e3d737461626c65" alt="Documentation" data-canonical-src="https://readthedocs.org/projects/vizro/badge/?version=stable"></a>
<a href="https://www.bestpractices.dev/projects/7858" rel="nofollow"><img src="https://camo.githubusercontent.com/fd762386fcca5b58ccfa34fc02cf49f120452d78754c770a6813426b881b0813/68747470733a2f2f7777772e626573747072616374696365732e6465762f70726f6a656374732f373835382f6261646765" alt="OpenSSF Best Practices" data-canonical-src="https://www.bestpractices.dev/projects/7858/badge"></a></p>

<hr>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/example_screens.png"><img src="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/example_screens.png" width="700"></a>
</p>
<p dir="auto">

<b>
Visual Intelligence. Beautifully engineered
</b>

</p>
<p dir="auto">

Vizro is a toolkit for creating modular data visualization applications

</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/tech_logos.png"><img src="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/tech_logos.png" width="270"></a>
</p>
<h2 tabindex="-1" id="user-content-what-is-vizro" dir="auto"><a href="#what-is-vizro">What is Vizro?</a></h2>
<p dir="auto">

Rapidly self-serve the assembly of customised dashboards in minutes - without the need for advanced coding or design experience - to create flexible and scalable, Python enabled data visualization applications

</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/code_dashboard.png"><img src="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/code_dashboard.png" width="1300"></a>
</p>
<p dir="auto">Use a few lines of simple configuration to create complex dashboards, which are automatically assembled utilising libraries such as <a href="https://github.com/plotly/plotly.py"><strong>Plotly</strong></a> and <a href="https://github.com/plotly/dash"><strong>Dash</strong></a>, with inbuilt coding and design best practices</p>
<p dir="auto">Define high level categories within the configuration, including:</p>
<ul dir="auto">
<li><strong>components:</strong> create charts, tables, input/output interfaces, and more</li>
<li><strong>controls</strong>: create filters, parameter inputs, and custom action controllers</li>
<li><strong>pages, layouts and navigation</strong>: create multiple pages, with customisable layouts and flexible navigation across them</li>
<li><strong>actions and interactions</strong>: create interactions between charts, and use pre-defined or customised actions (such as exporting)</li>
</ul>
<p dir="auto">Configuration can be written in multiple formats including <strong>Pydantic models</strong>, <strong>JSON</strong>, <strong>YAML</strong> or <strong>Python dictionaries</strong> for added flexibility of implementation</p>
<p dir="auto">Optional high-code extensions allow almost infinite customisation in a modular way, combining the best of low-code and high-code - for flexible and scalable, Python enabled data visualization applications</p>
<br>
<h3 tabindex="-1" id="user-content-key-benefits" dir="auto"><a href="#key-benefits">Key benefits</a></h3>

<p dir="auto">
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/value_prop_icons.png"><img src="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/value_prop_icons.png" width="900"></a>
</p>
<br>
<h3 tabindex="-1" id="user-content-examples" dir="auto"><a href="#examples">Examples</a></h3>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/dashboard_examples.png"><img src="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/dashboard_examples.png" width="1300"></a>
</p>
<h3 tabindex="-1" id="user-content-live-demo" dir="auto"><a href="#live-demo">Live demo</a></h3>
<p dir="auto">
<a href="http://vizro.mckinsey.com/" rel="nofollow">
<img src="https://raw.githubusercontent.com/mckinsey/vizro/main/.github/images/live_interactive_demo.png" width="525" height="296">  </a>
</p>
<h2 tabindex="-1" id="user-content-installation" dir="auto"><a href="#installation">Installation</a></h2>

<p dir="auto">See the <a href="https://vizro.readthedocs.io/en/latest/pages/user_guides/install/" rel="nofollow">Install guide</a> for more information</p>
<p dir="auto">Please note this repository is a monorepo and the core <code>vizro</code> package can be found in <a href="https://github.com/mckinsey/vizro/tree/main/vizro-core">/vizro-core</a></p>
<h2 tabindex="-1" id="user-content-getting-started" dir="auto"><a href="#getting-started">Getting started</a></h2>
<p dir="auto">See the <a href="https://vizro.readthedocs.io/en/latest/pages/tutorials/first_dashboard/" rel="nofollow">Tutorials</a> for creating your first dashboard</p>
<h2 tabindex="-1" id="user-content-documentation" dir="auto"><a href="#documentation">Documentation</a></h2>
<p dir="auto">See the <a href="https://vizro.readthedocs.io/en/latest/" rel="nofollow">Documentation</a> for more details</p>
<h2 tabindex="-1" id="user-content-community-and-development" dir="auto"><a href="#community-and-development">Community and Development</a></h2>
<p dir="auto">We encourage you to ask and answer technical questions via the <a href="https://github.com/mckinsey/vizro/issues">GitHub Issues</a>. This is also the place where you can submit bug reports or request new features.</p>
<h2 tabindex="-1" id="user-content-contributing" dir="auto"><a href="#contributing">Contributing</a></h2>
<p dir="auto">To learn more about making a contribution,
please see the <a href="https://vizro.readthedocs.io/en/latest/pages/development/contributing/" rel="nofollow">Contributing Guide</a> for more information</p>
<p dir="auto">You can also view current and former <a href="https://vizro.readthedocs.io/en/latest/pages/development/authors/" rel="nofollow">contributors</a></p>
<h2 tabindex="-1" id="user-content-reporting-a-security-vulnerability" dir="auto"><a href="#reporting-a-security-vulnerability">Reporting a Security Vulnerability</a></h2>
<p dir="auto">Please see our <a href="https://github.com/mckinsey/vizro/security/policy">security policy</a></p>
<h2 tabindex="-1" id="user-content-license" dir="auto"><a href="#license">License</a></h2>
<p dir="auto"><code>vizro</code> is distributed under the terms of the <a href="https://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">Apache License 2.0</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[macOS Sonoma is available today (377 pts)]]></title>
            <link>https://www.apple.com/newsroom/2023/09/macos-sonoma-is-available-today/</link>
            <guid>37662510</guid>
            <pubDate>Tue, 26 Sep 2023 17:12:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/newsroom/2023/09/macos-sonoma-is-available-today/">https://www.apple.com/newsroom/2023/09/macos-sonoma-is-available-today/</a>, See on <a href="https://news.ycombinator.com/item?id=37662510">Hacker News</a></p>
<div id="readability-page-1" class="page">


	
    







 
<nav id="ac-localnav" lang="en-US" role="navigation" aria-label="Newsroom" data-analytics-region="local nav" data-sticky="">
	
    
    
        




    
    

</nav>



<main id="main" role="main"> 



<span id="opens-in-new-window">opens in new window</span>

	

<section>
<article data-analytics-activitymap-region-id="article">






    
    
    











    <div>
        

        <div>
                    
                    
                        <span>UPDATE</span>
                    
                    
                        <span>September 26, 2023</span>
                    
                    
                </div>

        <div>
                
                
                
                    <h2>
                        
    
        macOS Sonoma is available today
    

                    </h2>
                
            </div>

        

        
            
    
    
    
    
    

        

    </div>







    
    
    






  
    
    
    
    
      <figure aria-label="Media, macOS Sonoma displayed on MacBook Pro, the 27-inch iMac, and MacBook Air.">
        <div>
             
              
              <div>
                macOS Sonoma makes the Mac experience better than ever — from more ways to personalize with widgets, to big updates to Safari and video conferencing, along with optimized gaming.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2023/09/macos-sonoma-is-available-today/article/Apple-macOS-Sonoma-3up.zip" download="" data-analytics-title="Download image" aria-label="Download media, macOS Sonoma displayed on MacBook Pro, the 27-inch iMac, and MacBook Air."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <div><a href="https://www.apple.com/macos/sonoma/" target="_blank">macOS Sonoma</a> is now available as a free software update, bringing a rich set of new features to the Mac that make work and play even more powerful. With macOS Sonoma, desktop widgets unlock a new way to personalize the Mac and get more done, while stunning new screen savers, big updates to video conferencing and Safari, along with optimized gaming make the Mac experience better than ever.<br>

</div>
                 
             
                 <h2>Widgets and Stunning Screen Savers
</h2>
                 
             
                 <div>With macOS Sonoma, widgets can be placed right on the desktop and blend seamlessly with the wallpaper while other windows are open. Widgets also become interactive so users can complete a reminder, play or pause media, access home controls, and perform various tasks directly from the widget on their desktop. And through the magic of Continuity, users can further customize their Mac with widgets from their iPhone.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, A collection of widgets displayed on MacBook Pro with macOS Sonoma.">
        <div>
             
              
              <div>
                Users can personalize their desktop with widgets, and with Continuity, they can even add their iPhone widgets to their Mac desktop.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2023/09/macos-sonoma-is-available-today/article/Apple-macOS-Sonoma-Widgets.zip" download="" data-analytics-title="Download image" aria-label="Download media, A collection of widgets displayed on MacBook Pro with macOS Sonoma."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>A new selection of screen savers in macOS Sonoma features slow-motion videos of beautiful locations around the world, such as the sweeping skyline of Hong Kong, the sandstone buttes of Monument Valley in Arizona, and the rolling hills of Sonoma in Northern California. And after login, the screen savers seamlessly transition to become the desktop wallpaper.
</div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>A new selection of screen savers in macOS Sonoma shows slow-motion videos of some of the most beautiful locations around the world.</div>
        
            <a aria-label="Download video: macOS Sonoma Screen Saver" data-analytics-title="Download video - macOS Sonoma Screen Saver" download="" href="https://www.apple.com/newsroom/videos/apple-macos-sonoma-screen-saver/downloads/Apple-macOS-Sonoma-screen-saver.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>
             
                 <h2>Powerful Productivity for Video Conferencing
</h2>
                 
             
                 <div>macOS Sonoma brings enhanced video conferencing features that enable users to present and share their work more effectively within any video conferencing app. Presenter Overlay displays users in front of the content they are sharing, and Reactions allow users to share how they feel with simple hand gestures that trigger fun, frame-filling 3D effects like balloons, confetti, hearts, and more.
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="video-conferencing">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-cd3c7cce41feeadca525a4198744a00d" href="#gallery-cd3c7cce41feeadca525a4198744a00d" data-ac-gallery-trigger="gallery-cd3c7cce41feeadca525a4198744a00d"><span>A video conference call on MacBook Pro shows a presenter in front of a document they’re showing.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-73d7ac7d6ab2c5b0311ab299cd09aa3e" href="#gallery-73d7ac7d6ab2c5b0311ab299cd09aa3e" data-ac-gallery-trigger="gallery-73d7ac7d6ab2c5b0311ab299cd09aa3e"><span>A Zoom meeting on MacBook Pro shows fireworks behind a presenter.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-cd3c7cce41feeadca525a4198744a00d" aria-labelledby="gallery-dotnav-cd3c7cce41feeadca525a4198744a00d" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:presenter-overlay">
                                
                                <div>
                                    <div>Video conference calls get more engaging with new features like Presenter Overlay, which displays users in front of the content they are sharing.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2023/09/macos-sonoma-is-available-today/article/Apple-macOS-Sonoma-Zoom-Presenter-Overlay.zip" download="" data-analytics-title="Download image" aria-label="Download media, A video conference call on MacBook Pro shows a presenter in front of a document they’re showing."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-73d7ac7d6ab2c5b0311ab299cd09aa3e" aria-labelledby="gallery-dotnav-73d7ac7d6ab2c5b0311ab299cd09aa3e" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:reactions">
                                
                                <div>
                                    <div>Reactions allow users to share how they feel with simple hand gestures that trigger fun, frame-filling 3D effects like balloons, confetti, hearts, and more. </div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2023/09/macos-sonoma-is-available-today/article/Apple-macOS-Sonoma-Zoom-Reactions.zip" download="" data-analytics-title="Download image" aria-label="Download media, A Zoom meeting on MacBook Pro shows fireworks behind a presenter."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2>Enhanced Browsing with Safari
</h2>
                 
             
                 <div>In Safari, profiles keep browsing separate between topics like work, school, and more so users can quickly switch between them. Private Browsing gets even better with added protection against some of the most advanced techniques used to track users — Private Browsing windows become locked when not in use and known trackers are blocked from loading. Safari users can also now add any website to the Dock as they would with an app, complete with a simplified toolbar and notifications for an app-like experience.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, MacBook Pro displays two profiles: one labeled School and the other labeled Home.">
        <div>
             
              
              <div>
                Profiles keep browsing separate between topics like work, school, and more so users can quickly switch between them.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2023/09/macos-sonoma-is-available-today/article/Apple-macOS-Sonoma-profiles.zip" download="" data-analytics-title="Download image" aria-label="Download media, MacBook Pro displays two profiles: one labeled School and the other labeled Home."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2>An Immersive Gaming Experience<br>

</h2>
                 
             
                 <div>With the power of Apple silicon, tens of millions of Macs can run demanding games with great performance, long battery life, and breathtaking visuals. macOS Sonoma improves the gaming experience even further with Game Mode, providing more consistent frame rates and dramatically reducing input and audio latency with wireless game controllers and AirPods.<sup> </sup>Game Mode works with any game, including recent and upcoming Mac games like DEATH STRANDING DIRECTOR’S CUT, Stray, Layers of Fear, and SnowRunner.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, MacBook Pro displays a screen from the video game Stray.">
        <div>
             
              
              <div>
                In macOS Sonoma, Game Mode provides more consistent frame rates and dramatically reduced input and audio latency with wireless game controllers and AirPods.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2023/09/macos-sonoma-is-available-today/article/Apple-macOS-Sonoma-gaming.zip" download="" data-analytics-title="Download image" aria-label="Download media, MacBook Pro displays a screen from the video game Stray."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Additional macOS Sonoma Updates</strong>
</h2>
                 
             
                 <div><ul>
<li><strong>Notes</strong>:<strong> </strong>Users can view PDFs and scans of presentations, assignments, research papers, and more right inside Notes. They can also create links from one note to another to relate ideas and content.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Passwords</strong>:<strong> </strong>A set of passwords can now be shared among a group. Everyone in a group can add and edit passwords to keep them up to date, and since sharing is through iCloud Keychain, it’s end-to-end encrypted. Additionally, the one-time verification codes received in Mail will now autofill in Safari, making it easy to securely log in without leaving the browser.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Messages</strong>: Search filters and swipe to reply enhance everyday messaging, while all-new Live Stickers can be created and synced across macOS, iOS, and iPadOS.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Reminders</strong>:<strong> </strong>Intelligent grocery lists in Reminders streamline weekly trips to the store by organizing lists into sections and arranging them horizontally using a new column view.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Keyboard</strong>:<strong> </strong>Autocorrect receives a comprehensive update with a transformer language model, a state-of-the-art on-device machine learning language model that improves accuracy. A refreshed design makes corrections easier to fix and inline predictions quickly finish sentences. Dictation brings next-level speech recognition and the ability to move fluidly between voice and typing.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Screen Sharing:&nbsp; </strong>A new high performance mode in the Screen Sharing app delivers incredibly responsive remote access over high-bandwidth connections — enabling creative professionals to accomplish their work remotely.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    


     
     
    
    
        <div>macOS Sonoma is a free software update that is available starting today. Some features may not be available in all regions, languages, or on all devices. For more information and a full list of features, visit <a href="https://www.apple.com/macos/sonoma/" target="_blank">apple.com/macos/sonoma</a>.
</div>
 

    
    
    




    
    
        
    


    
    
    



    
    
    






    
















	
	
	
		















	
	

</article>



</section>
</main>


	

</div>]]></description>
        </item>
    </channel>
</rss>