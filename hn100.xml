<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 25 Jan 2024 13:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Only 90s Web Developers Remember This (258 pts)]]></title>
            <link>https://zachholman.com/posts/only-90s-developers/</link>
            <guid>39127433</guid>
            <pubDate>Thu, 25 Jan 2024 08:27:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zachholman.com/posts/only-90s-developers/">https://zachholman.com/posts/only-90s-developers/</a>, See on <a href="https://news.ycombinator.com/item?id=39127433">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    <marquee><blink>Please sign my <a href="https://github.com/holman/feedback">guestbook</a>.</blink></marquee>

<p>Have you ever shoved a <code>&lt;blink&gt;</code> into a <code>&lt;marquee&gt;</code> tag? Pixar gets all the
accolades today, but in the 90s this was a serious feat of computer animation.
By combining these two tags, you were a trailblazer. A person capable of great
innovation. A human being that all other human beings could aspire to.</p>

<p>You were a web developer in the 1990s.</p>

<p>With that status, you knew you were hot shit. And you brought with you a score
of the most fearsome technological innovations, the likes of which we haven’t
come close to replicating ever since.</p>

<p>Put down the jQuery, step away from the non-relational database: we have more
important things to talk about.</p>

<h2 id="1x1gif">1x1.gif</h2>

<p>1x1.gif should have won a fucking Grammy. Or a Pulitzer. Or <em>Most Improved,
Third Grade Gym Class</em> or something. It’s the most important achievement in
computer science since the linked list. It’s not the future we deserved, but
it’s the future we needed (until the box model fucked it all up).</p>

<p>If you’re not familiar with the humble 1x1.gif trick, here it is:</p>

<!-- lol yeah i'm base64 encoding 1x1.gif isn't that fucking dope? -->
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p>

<p>Can’t see it? Here, <em>enhance</em>:</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" height="100" width="100%"></p>

<p>The 1x1.gif — or spacer.gif, or transparent.gif — is just a one pixel by one
pixel transparent GIF. Just like the most futuristic CSS framework of today <em>but
in a billionth of the file size</em>, 1x1.gif is fully optimized for the
<strong>responsive web</strong>. You had to use these advanced attributes to tap into its
power, though:</p>

<div><pre><code><span>&lt;IMG</span> <span>SRC=</span><span>"/1x1.gif"</span> <span>WIDTH=</span><span>150</span> <span>HEIGHT=</span><span>250</span><span>&gt;</span>
</code></pre></div>

<p>By doing this you can position elements ANYWHERE ON THE PAGE. Combine this with
semantically-appropriate containers and you could do amazing things:</p>

<div><pre><code><span>&lt;TABLE&gt;</span>
  <span>&lt;TR&gt;</span>
    <span>&lt;TD&gt;&lt;IMG</span> <span>SRC=</span><span>"1x1.gif"</span> <span>WIDTH=</span><span>300</span><span>&gt;</span>
    <span>&lt;TD&gt;&lt;FONT</span> <span>SIZE=</span><span>42</span><span>&gt;</span>Hello welcome to my <span>&lt;MARQUEE&gt;</span>Internet Web Home<span>&lt;/MARQUEE&gt;&lt;/FONT&gt;</span>
  <span>&lt;/TR&gt;</span>
  <span>&lt;TR&gt;</span>
    <span>&lt;TD</span> <span>BGCOLOR=</span><span>RED</span><span>&gt;&lt;IMG</span> <span>SRC=</span><span>"/cgi/webcounter.cgi"</span><span>&gt;</span>
  <span>&lt;/TR&gt;</span>
<span>&lt;/TABLE&gt;</span>
</code></pre></div>

<p>1x1.gif let you push elements all around the page effortlessly. To this day it
is the only way to vertically center elements.</p>

<h2 id="nbspnbspnbspnbsp"><code>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;</code></h2>

<p>Are images too advanced for you? <em>HTML For Dummies</em> doesn’t cover the <code>&lt;IMG&gt;</code>
tag until chapter four? Well, you’re in luck: the <code>&amp;nbsp;</code> tag is here!</p>

<p>You may be saying to yourself, “Self, I know all about HTML entity encoding.
What is this dastardly handsome man going on about?”</p>

<p>The answer, dear reasonably attractive reader, is an innovation that youth of
today don’t respect nearly enough: the stacked <code>&amp;nbsp;</code>. Much like the 1x1.gif
trick, you can just arbitrarily scale <code>&amp;nbsp</code>; for whatever needs you may face:</p>

<div><pre><code>PLEASE SIGN  <span>&lt;BR&gt;</span>
<span>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;</span>MY GUESTBOOK BELOW:
<span>&lt;HR&gt;&lt;HR&gt;</span>
</code></pre></div>

<p>If I had a nickel for how many times I wrote <code>&amp;nbsp;</code> in the 90s, I’d have
enough money to cover the monthly overage bills from AOL.</p>

<h2 id="dotted-underlines-border-effects">Dotted underlines, border effects</h2>

<p>Towards the end of the golden era of HTML, CSS appeared on the scene, promising
a world of separating content from style, and we’ve been dealing with that
disaster ever since.</p>

<p>The absolute first thing we did with CSS was use it to stop underlining links.
Overnight, the entire internet converted into this sludge of a medium where text
looked like links and links looked like text. You had no idea where to click,
but hell that didn’t really matter anyway because we had developed cursor
effects (you haven’t lived until your mouse had a trail of twelve fireballs
behind it).</p>

<p>This was such a compelling use of advanced technology that it was literally all
we used CSS for initially. I even have proof from an <code>index.shtml</code> (fuck yes
SSI) file from 2000:</p>

<div><pre><code><span>&lt;style </span><span>type=</span><span>"text/css"</span><span>&gt;</span>
<span>&lt;!</span><span>--</span>
<span>a</span><span>:hover</span> <span>{</span><span>text-decoration</span><span>:</span> <span>none</span><span>;</span> <span>color</span><span>:</span> <span>#000000</span><span>}</span>
<span>--</span><span>&gt;</span>
<span>&lt;/style&gt;</span>
</code></pre></div>

<p>That’s it. That’s the entire — inline, of course — CSS for this file. Make sure
when you hover the link, remove the underline and paint it black. From this,
entire interactive websites are born.</p>

<h2 id="dhtml">DHTML</h2>

<p>As soon as we had the technology to remove underlines from links, we decided to
combine it with the power to show <code>alert("Welcome to my website!")</code> messages on
page load. CSS and JavaScript joined forces to form the Technology of Terror:
DHTML.</p>

<p>DHTML, which stands for “distributed HTML”, was the final feather in our cap of
web development tools. It would stand the test of time, ensuring that we could
make snowflakes fall from the top of the page, or build an accordion menu
animated image map, or building your own custom <code>&lt;marquee&gt;</code> except using
semantic tags like <code>&lt;div&gt;</code>.</p>

<p>DHTML helped transition web development from a hobbyist pastime into a
full-fledged profession. Sites like Dynamic Drive meant that instead of thinking
through creative solutions for problems you face, you could just copy and paste
this 50 line block of code and everything would be fixed. In effect, DHTML was
the Twitter Bootstrap of the time.</p>

<h2 id="pixel-fonts">Pixel fonts</h2>

<p>Computer screens were not large. I mean, they were <em>large</em>, since CRT was the
shit, but they didn’t have a high resolution. Therefore, the best way to
leverage those pixels is to write everything in tiny six-point font.</p>

<p><img src="https://zachholman.com/images/posts/pixel%20font.png" alt=""></p>

<p>Along those lines, web developers aspired to become illustrators when they
looked at these simplistic typefaces and realized they were made up of pixels.
You started to see these weird attempts at isometric pixel illustration on
splash screens, made by developers whose time and money was probably better spent
investing in a .com IPO rather than installing Photoshop.</p>

<h2 id="buttons">Buttons</h2>

<p>It’s come to my attention that people today don’t like Internet Explorer. I can
only believe they hate Internet Explorer because it has devolved from its purest
form, Internet Explorer 4.0.</p>

<p>Internet Explorer 4.0 was perfection incarnate in a browser. It had Active
Desktop. It had Channels. <em>It had motherfucking Channels</em>, the coolest
technology that never reached market adoption ever not even a little bit. IE4,
in general, was so good that you were going to have it installed on your PC
whether you liked it or not.</p>

<p>When you’re part of an elite group of people who fully understand the weight of
perfection, there is a natural tendency to tell everyone you meet that you and
you alone have the gravitas necessary to make these hard decisions. Decisions
like what browser your visitors should use.</p>

<p>So we proudly displayed dozens of 88x31 pixel buttons on our sites:</p>

<center>
  <img src="https://zachholman.com/images/posts/ie4.gif">
</center>

<p>These were everywhere. It’s kind of like the ribbons displayed on a uniform of a
military officer: they told the tale of all the battles the individual had
fought in order to get to where they were today. In other words, which editor
(FrontPage ‘98, obviously), which web server (GeoCities, you moron), and which
web ring you were a part of (whichever listed your site highest, which was none
of them).</p>

<hr>

<p>I miss the good ol’ days. Today we have abstractions on top of abstractions on
top of JavaScript, of all things. Shit doesn’t even know how to calculate math
correctly. It’s amazing we ever got to where we are today, when you think about
it.</p>

<p>So raise a glass proudly, and do us all a favor: paste a shit ton of <code>&amp;nbsp;</code>s
into your next pull request, just to fuck with your team a little bit.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brains are not required to think or solve problems – simple cells can do it (131 pts)]]></title>
            <link>https://www.scientificamerican.com/article/brains-are-not-required-when-it-comes-to-thinking-and-solving-problems-simple-cells-can-do-it/</link>
            <guid>39127028</guid>
            <pubDate>Thu, 25 Jan 2024 07:09:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scientificamerican.com/article/brains-are-not-required-when-it-comes-to-thinking-and-solving-problems-simple-cells-can-do-it/">https://www.scientificamerican.com/article/brains-are-not-required-when-it-comes-to-thinking-and-solving-problems-simple-cells-can-do-it/</a>, See on <a href="https://news.ycombinator.com/item?id=39127028">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p data-block="sciam/paragraph"><span>T</span>he planarian is nobody's idea of a genius. A flatworm shaped like a comma, it can be found wriggling through the muck of lakes and ponds worldwide. Its pin-size head has a microscopic structure that passes for a brain. Its two eyespots are set close together in a way that makes it look cartoonishly confused. It aspires to nothing more than life as a bottom-feeder.</p><p data-block="sciam/paragraph">But the worm has mastered one task that has eluded humanity's greatest minds: perfect regeneration. Tear it in half, and its head will grow a new tail while its tail grows a new head. After a week two healthy worms swim away.</p><p data-block="sciam/paragraph">Growing a new head is a neat trick. But it's the tail end of the worm that intrigues Tufts University biologist Michael Levin. He studies the way <a href="https://www.scientificamerican.com/article/synthetic-morphology-lets-scientists-create-new-life-forms/">bodies develop from single cells</a>, among other things, and his research led him to suspect that the intelligence of living things lies outside their brains to a surprising degree. Substantial smarts may be in the cells of a worm's rear end, for instance. “All intelligence is really collective intelligence, because every cognitive system is made of some kind of parts,” Levin says. An animal that can survive the complete loss of its head was Levin's perfect test subject.</p><p data-block="sciam/paragraph">In their natural state planaria prefer the smooth and sheltered to the rough and open. Put them in a dish with a corrugated bottom, and they will huddle against the rim. But in his laboratory, about a decade ago, Levin trained some planaria to expect yummy bits of liver puree that he dripped into the middle of a ridged dish. They soon lost all fear of the rough patch, eagerly crossing the divide to get the treats. He trained other worms in the same way but in smooth dishes. Then he decapitated them all.</p><p data-block="sciam/paragraph">Levin discarded the head ends and waited two weeks while the tail ends regrew new heads. Next he placed the regenerated worms in corrugated dishes and dripped liver into the center. Worms that had lived in a smooth dish in their previous incarnation were reluctant to move. But worms regenerated from tails that had lived in rough dishes learned to go for the food more quickly. Somehow, despite the total loss of their brains, those planaria had retained the memory of the liver reward. But how? Where?</p><p data-block="sciam/paragraph">It turns out that regular cells—not just highly specialized brain cells such as neurons—have the ability to store information and act on it. Now Levin has shown that the cells do so by using subtle changes in electric fields as a type of memory. These revelations have put the biologist at the vanguard of a new field called basal cognition. Researchers in this burgeoning area have spotted hallmarks of intelligence—learning, memory, problem-solving—outside brains as well as within them.</p><p data-block="sciam/paragraph">Until recently, most scientists held that true cognition arrived with the first brains half a billion years ago. Without intricate clusters of neurons, behavior was merely a kind of reflex. But Levin and several other researchers believe otherwise. He doesn't deny that brains are awesome, paragons of computational speed and power. But he sees the differences between cell clumps and brains as ones of degree, not kind. In fact, Levin suspects that cognition probably evolved as cells started to collaborate to carry out the incredibly difficult task of building complex organisms and then got souped-up into brains to allow animals to move and think faster.</p><p data-block="sciam/paragraph">That position is being embraced by researchers in a variety of disciplines, including roboticists such as Josh Bongard, a frequent Levin collaborator who runs the Morphology, Evolution, and Cognition Laboratory at the University of Vermont. “Brains were one of the most recent inventions of Mother Nature, the thing that came last,” says Bongard, who hopes to build deeply intelligent machines from the bottom up. “It's clear that the body matters, and then somehow you add neural cognition on top. It's the cherry on the sundae. It's not the sundae.”</p><figure data-original-class="image-captioned" data-block="sciam/image"><a aria_label="Open image in new tab" href="https://static.scientificamerican.com/sciam/cache/file/D1F27359-310E-4D2C-A91B7FFAB9ECC010_medium.jpg?w=1536" target="_blank"><img alt="A brown worm with a head on both ends shown on an off-white background." decoding="async" height="1024" loading="lazy" sizes="(min-width: 900px) 900px, (min-resolution: 3dppx) 50vw, (min-resolution: 2dppx) 75vw, 100vw" src="https://static.scientificamerican.com/sciam/cache/file/D1F27359-310E-4D2C-A91B7FFAB9ECC010_medium.jpg?w=900" srcset="https://static.scientificamerican.com/sciam/cache/file/D1F27359-310E-4D2C-A91B7FFAB9ECC010_medium.jpg?w=600 600w, https://static.scientificamerican.com/sciam/cache/file/D1F27359-310E-4D2C-A91B7FFAB9ECC010_medium.jpg?w=750 750w, https://static.scientificamerican.com/sciam/cache/file/D1F27359-310E-4D2C-A91B7FFAB9ECC010_medium.jpg?w=900 900w, https://static.scientificamerican.com/sciam/cache/file/D1F27359-310E-4D2C-A91B7FFAB9ECC010_medium.jpg?w=1000 1000w, https://static.scientificamerican.com/sciam/cache/file/D1F27359-310E-4D2C-A91B7FFAB9ECC010_medium.jpg?w=1200 1200w, https://static.scientificamerican.com/sciam/cache/file/D1F27359-310E-4D2C-A91B7FFAB9ECC010_medium.jpg?w=1350 1350w" width="1536"></a>
<figcaption>Head cells in the flatworm <em>Dugesia japonica</em> have different bioelectric voltages than tail cells do. Switch the voltages around and cut off the tail, and the head will regenerate a second head. Credit:&nbsp;Michael Levin</figcaption></figure><p data-block="sciam/paragraph">In recent years interest in basal cognition has exploded as researchers have recognized example after example of surprisingly sophisticated intelligence at work across life's kingdoms, no brain required. For artificial-intelligence scientists such as Bongard, basal cognition offers an escape from the trap of assuming that future intelligences must mimic the brain-centric human model. For medical specialists, there are tantalizing hints of ways to awaken cells' innate powers of healing and regeneration.</p><p data-block="sciam/paragraph">And for the philosophically minded, basal cognition casts the world in a sparkling new light. Maybe thinking builds from a simple start. Maybe it is happening all around us, every day, in forms we haven't recognized because we didn't know what to look for. Maybe minds are everywhere.</p><p data-block="sciam/paragraph">Although it now seems like a Dark Ages idea, only a few decades ago many scientists believed that nonhuman animals couldn't experience pain or other emotions. Real thought? Out of the question. The mind was the purview of humans. “It was the last beachhead,” says Pamela Lyon of the University of Adelaide, a scholar of basal cognition, who coined the term for the field in 2018. Lyon sees scientists' insistence that human intelligence is qualitatively different as just another doomed form of exceptionalism. “We've been ripped from every central position we've inhabited,” she points out. Earth is not the center of the universe. People are just another animal species. But real cognition—that was supposed to set us apart.</p><p data-block="sciam/paragraph">Now that notion, too, is in retreat as researchers document the rich inner lives of creatures increasingly distant from us. Apes, dogs, dolphins, crows and even insects are proving more savvy than suspected. In his 2022 book <em>The Mind of a Bee</em>, behavioral ecologist Lars Chittka chronicles his decades of work with honeybees, showing that <a href="https://www.scientificamerican.com/article/do-insects-feel-joy-and-pain/">bees can use sign language, recognize individual human faces, and remember and convey the locations of far-flung flowers</a>. They have good moods and bad, and they can be traumatized by near-death experiences such as being grabbed by an animatronic spider hidden in a flower. (Who wouldn't be?)</p><p data-block="sciam/paragraph"><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="315" loading="lazy" src="https://www.youtube.com/embed/8WyWFAS96ac?si=jETTvE853eizqsVl" title="YouTube video player" width="560"></iframe></p><p data-block="sciam/paragraph">But bees, of course, are animals with actual brains, so a soupçon of smarts doesn't really shake the paradigm. The bigger challenge comes from evidence of surprisingly sophisticated behavior in our brainless relatives. “The neuron is not a miracle cell,” says Stefano Mancuso, a University of Florence botanist who has written several books on plant intelligence. “It's a normal cell that is able to produce an electric signal. In plants almost every cell is able to do that.”</p><p data-block="sciam/paragraph">On one plant, the touch-me-not, feathery leaves normally fold and wilt when touched (a defense mechanism against being eaten), but when a team of scientists at the University of Western Australia and the University of Firenze in Italy conditioned the plant by jostling it throughout the day without harming it, it quickly learned to ignore the stimulus. Most remarkably, when the scientists left the plant alone for a month and then retested it, it remembered the experience. Other plants have other abilities. A Venus flytrap can count, snapping shut only if two of the sensory hairs on its trap are tripped in quick succession and pouring digestive juices into the closed trap only if its sensory hairs are tripped three more times.</p><p data-block="sciam/paragraph">These responses in plants are mediated by electric signals, just as they are in animals. Wire a flytrap to a touch-me-not, and you can make the entire touch-me-not collapse by touching a sensory hair on the flytrap. And these and other plants can be knocked out by anesthetic gas. Their electric activity flatlines, and they stop responding as if unconscious.</p><p data-block="sciam/paragraph">Plants can sense their surroundings surprisingly well. They know whether they are being shaded by part of themselves or by something else. They can detect the sound of running water (and will grow toward it) and of bees' wings (and will produce nectar in preparation). They know when they are being eaten by bugs and will produce nasty defense chemicals in response. They even know when their neighbors are under attack: when scientists played a recording of munching caterpillars to a cress plant, that was enough for the plant to send a surge of mustard oil into its leaves.</p><p data-block="sciam/paragraph">Plants' most remarkable behavior tends to get underappreciated because we see it every day: they seem to know exactly what form they have and plan their future growth based on the sights, sounds and smells around them, making complicated decisions about where future resources and dangers might be located in ways that can't be boiled down to simple formulas. As Paco Calvo, director of the Minimal Intelligence Laboratory at the University of Murcia in Spain and author of <em>Planta Sapiens</em>, puts it, “Plants have to plan ahead to achieve goals, and to do so, they need to integrate vast pools of data. They need to engage with their surroundings adaptively and proactively, and they need to think about the future. They just couldn't afford to do otherwise.”</p><p data-block="sciam/paragraph">None of this implies that plants are geniuses, but within their limited tool set, they show a solid ability to perceive their world and use that information to get what they need—key components of intelligence. But again, plants are a relatively easy case—no brains but lots of complexity and trillions of cells to play with. That's not the situation for single-celled organisms, which have traditionally been relegated to the “mindless” category by virtually everyone. If amoebas can think, then humans need to rethink all kinds of assumptions.</p><p data-block="sciam/paragraph">Yet the evidence for cogitating pond scum grows daily. Consider the slime mold, a cellular puddle that looks a bit like melted Velveeta and oozes through the world's forests digesting dead plant matter. Although it can be the size of a throw rug, a slime mold is one single cell with many nuclei. It has no nervous system, yet it is an excellent problem solver. When researchers from Japan and Hungary placed a slime mold at one end of a maze and a pile of oat flakes at the other, the slime mold did what slime molds do, exploring every possible option for tasty resources. But once it found the oat flakes, it retreated from all the dead ends and concentrated its body in the path that led to the oats, choosing the shortest route through the maze (of four possible solutions) every time. Inspired by that experiment, the same researchers then piled oat flakes around a slime mold in positions and quantities meant to represent the population structure of Tokyo, and the slime mold contorted itself into a very passable map of the Tokyo subway system.</p><figure data-responsive-image="responsive-image" data-original-class="article-media" data-block="sciam/image"><div><a aria_label="Open image in new tab" href="https://static.scientificamerican.com/sciam/assets/Image/2023/saw0224Jaco31_d.jpg?w=2000&amp;disable=upscale" target="_blank"><picture><source media="(min-width: 768px) and (max-width: 1023px)" sizes="(min-width: 900px) 900px, (min-resolution: 3dppx) 50vw, (min-resolution: 2dppx) 75vw, 100vw" srcset="https://static.scientificamerican.com/sciam/assets/Image/2023/saw0224Jaco31_d.jpg?w=600 600w, https://static.scientificamerican.com/sciam/assets/Image/2023/saw0224Jaco31_d.jpg?w=750 750w, https://static.scientificamerican.com/sciam/assets/Image/2023/saw0224Jaco31_d.jpg?w=900 900w, https://static.scientificamerican.com/sciam/assets/Image/2023/saw0224Jaco31_d.jpg?w=1000 1000w, https://static.scientificamerican.com/sciam/assets/Image/2023/saw0224Jaco31_d.jpg?w=1200 1200w, https://static.scientificamerican.com/sciam/assets/Image/2023/saw0224Jaco31_d.jpg?w=1350 1350w"><source media="(max-width: 767px)" sizes="(min-resolution: 3dppx) 50vw, (min-resolution: 2dppx) 75vw, 100vw" srcset="https://static.scientificamerican.com/sciam/assets/Image/2023/saw0224Jaco31_m.jpg?w=600 600w, https://static.scientificamerican.com/sciam/assets/Image/2023/saw0224Jaco31_m.jpg?w=750 750w, https://static.scientificamerican.com/sciam/assets/Image/2023/saw0224Jaco31_m.jpg?w=1000 1000w, https://static.scientificamerican.com/sciam/assets/Image/2023/saw0224Jaco31_m.jpg?w=1200 1200w"><img alt="Schematic demonstrates that brain cells are not the only cell groupings in the body that show the ability to think. Xenobots—constructed from frog skin and heart cells—can interpret their environment and share enough info to accomplish simple goals, such as moving in a particular direction." decoding="async" loading="lazy" sizes="(min-width: 900px) 900px, (min-resolution: 3dppx) 50vw, (min-resolution: 2dppx) 75vw, 100vw" src="https://static.scientificamerican.com/sciam/assets/Image/2023/saw0224Jaco31_d.jpg?w=900" srcset="https://static.scientificamerican.com/sciam/assets/Image/2023/saw0224Jaco31_d.jpg?w=900 900w, https://static.scientificamerican.com/sciam/assets/Image/2023/saw0224Jaco31_d.jpg?w=1350 1350w"></picture></a></div>
<figcaption>Credit: Brown Bird Design; Source: “A Scalable Pipeline for Designing Reconfigurable Organisms,” by Sam Kriegman et al., in <em>PNAS</em>, Vol. 117; January 2020 (<em>reference</em>)</figcaption></figure><p data-block="sciam/paragraph">Such problem-solving could be dismissed as simple algorithms, but other experiments make it clear that slime molds can learn. When Audrey Dussutour of France's National Center for Scientific Research placed dishes of oatmeal on the far end of a bridge lined with caffeine (which slime molds find disgusting), slime molds were stymied for days, searching for a way across the bridge like an arachnophobe trying to scooch past a tarantula. Eventually they got so hungry that they went for it, crossing over the caffeine and feasting on the delicious oatmeal, and soon they lost all aversion to the formerly distasteful stuff. They had overcome their inhibitions and learned from the experience, and they retained the memory even after being put into a state of suspended animation for a year.</p><p data-block="sciam/paragraph">Which brings us back to the decapitated planaria. How can something without a brain remember anything? Where is the memory stored? <em>Where is its mind?</em></p><p data-block="sciam/paragraph"><span>T</span>he orthodox view of memory is that it is stored as a stable network of synaptic connections among neurons in a brain. “That view is clearly cracking,” Levin says. Some of the demolition work has come from the lab of neuroscientist David Glanzman of the University of California, Los Angeles. Glanzman was able to transfer a memory of an electric shock from one sea slug to another by extracting RNA from the brains of shocked slugs and injecting it into the brains of new slugs. The recipients then “remembered” to recoil from the touch that preceded the shock. If RNA can be a medium of memory storage, any cell might have the ability, not just neurons.</p><p data-block="sciam/paragraph">Indeed, there's no shortage of possible mechanisms by which collections of cells might be able to incorporate experience. All cells have lots of adjustable pieces in their cytoskeletons and gene regulatory networks that can be set in different conformations and can inform behavior later on. In the case of the decapitated planaria, scientists still don't know for sure, but perhaps the remaining bodies were storing information in their cellular interiors that could be communicated to the rest of the body as it was rebuilt. Perhaps their nerves' basic response to rough floors had already been altered.</p><p data-block="sciam/paragraph">Levin, though, thinks something even more intriguing is going on: perhaps the impression was stored not just within the cells but in their states of interaction through bioelectricity, the subtle current that courses through all living things. Levin has spent much of his career studying how cell collectives communicate to solve sophisticated challenges during morphogenesis, or body building. How do they work together to make limbs and organs in exactly the right places? Part of that answer seems to lie in bioelectricity.</p><p data-block="sciam/paragraph">The fact that bodies have electricity flickering through them has been known for centuries, but until quite recently most biologists thought it was mostly used to deliver signals. Shoot some current through a frog's nervous system, and the frog's leg kicks. Neurons used bioelectricity to transmit information, but most scientists believed that was a specialty of brains, not bodies.</p><p data-block="sciam/paragraph">Since the 1930s, however, a small number of researchers have observed that other types of cells seem to be using bioelectricity to store and share information. Levin immersed himself in this unconventional body of work and made the next cognitive leap, drawing on his background in computer science. He'd supported himself during school by writing code, and he knew that computers used electricity to toggle their transistors between 0 and 1 and that all computer programs were built up from that binary foundation. So as an undergraduate, when he learned that all cells in the body have channels in their membranes that act like voltage gates, allowing different levels of current to pass through them, he immediately saw that such gates could function like transistors and that cells could use this electricity-driven information processing to coordinate their activities.</p><p data-block="sciam/paragraph">To find out whether voltage changes really altered the ways that cells passed information to one another, Levin turned to his planaria farm. In the 2000s he designed a way to measure the voltage at any point on a planarian and found different voltages on the head and tail ends. When he used drugs to change the voltage of the tail to that normally found in the head, the worm was unfazed. But then he cut the planarian in two, and the head end regrew a second head instead of a tail. Remarkably, when Levin cut the new worm in half, both heads grew new heads. Although the worms were genetically identical to normal planaria, the one-time change in voltage resulted in a permanent two-headed state.</p><p data-block="sciam/paragraph">For more confirmation that bioelectricity could control body shape and growth, Levin turned to African clawed frogs, common lab animals that quickly metamorphose from egg to tadpole to adult. He found that he could trigger the creation of a working eye anywhere on a tadpole by inducing a particular voltage in that spot. By simply applying the right bioelectric signature to a wound for 24 hours, he could induce regeneration of a functional leg. The cells took it from there.</p><p data-block="sciam/paragraph">“It's a subroutine call,” Levin says. In computer programming, a subroutine call is a piece of code—a kind of shorthand—that tells a machine to initiate a whole suite of lower-level mechanical actions. The beauty of this higher level of programming is that it allows us to control billions of circuits without having to open up the machine and mechanically alter each one by hand. And that was the case with building tadpole eyes. No one had to micromanage the construction of lenses, retinas, and all the other parts of an eye. It could all be controlled at the level of bioelectricity. “It's literally the cognitive glue,” Levin says. “It's what allows groups of cells to work together.”</p><p data-block="sciam/paragraph">Levin believes this discovery could have profound implications not only for our understanding of the evolution of cognition but also for human medicine. Learning to “speak cell”—to coordinate cells' behavior through bioelectricity—might help us treat cancer, a disease that occurs when part of the body stops cooperating with the rest of the body. Normal cells are programmed to function as part of the collective, sticking to the tasks assigned—liver cell, skin cell, and so on. But cancer cells stop doing their job and begin treating the surrounding body like an unfamiliar environment, striking out on their own to seek nourishment, replicate and defend themselves from attack. In other words, they act like independent organisms.</p><p data-block="sciam/paragraph">Why do they lose their group identity? In part, Levin says, because the mechanisms that maintain the cellular mind meld can fail. “Stress, chemicals, genetic mutations can all cause a breakdown of this communication,” he says. His team has been able to induce tumors in frogs just by forcing a “bad” bioelectric pattern onto healthy tissue. It's as if the cancer cells stop receiving their orders and go rogue.</p><p data-block="sciam/paragraph">Even more tantalizingly, Levin has dissipated tumors by reintroducing the proper bioelectric pattern—in effect reestablishing communication between the breakaway cancer and the body, as if he's bringing a sleeper cell back into the fold. At some point in the future, he speculates, bioelectric therapy might be applied to human cancers, stopping tumors from growing. It also could play a role in regenerating failing organs—kidneys, say, or hearts—if scientists can crack the bioelectric code that tells cells to start growing in the right patterns. With tadpoles, in fact, Levin showed that animals suffering from massive brain damage at birth were able to build normal brains after the right shot of bioelectricity.</p><p data-block="sciam/paragraph"><span>L</span>evin's research has always had tangible applications, such as cancer therapy, limb regeneration and wound healing. But over the past few years he's allowed a philosophical current to enter his papers and talks. “It's been sort of a slow rollout,” he confesses. “I've had these ideas for decades, but it wasn't the right time to talk about it.”</p><p data-block="sciam/paragraph">That began to change with a celebrated 2019 paper entitled “The Computational Boundary of a Self,” in which he harnessed the results of his experiments to argue that we are all collective intelligences <a href="https://blogs.scientificamerican.com/cross-check/the-many-minds-of-marvin-minsky-r-i-p/">built out of smaller, highly competent problem-solving agents</a>. As Vermont's Bongard told the <em>New York Times</em>, “What we are is intelligent machines made of intelligent machines made of intelligent machines all the way down.”</p><p data-block="sciam/paragraph">For Levin, that realization came in part from watching the bodies of his clawed frogs as they developed. In frogs' transformation from tadpole to adult, their faces undergo massive remodeling. The head changes shape, and the eyes, mouth and nostrils all migrate to new positions. The common assumption has been that these rearrangements are hardwired and follow simple mechanical algorithms carried out by genes, but Levin suspected it wasn't so preordained. So he electrically scrambled the normal development of frog embryos to create tadpoles with eyes, nostrils and mouths in all the wrong places. Levin dubbed them “Picasso tadpoles,” and they truly looked the part.</p><p data-block="sciam/paragraph">If the remodeling were preprogrammed, the final frog face should have been as messed up as the tadpole. Nothing in the frog's evolutionary past gave it genes for dealing with such a novel situation. But Levin watched in amazement as the eyes and mouths found their way to the right arrangement while the tadpoles morphed into frogs. The cells had an abstract goal and worked together to achieve it. “This is intelligence in action,” Levin wrote, “the ability to reach a particular goal or solve a problem by undertaking new steps in the face of changing circumstances.” Fused into a hive mind through bioelectricity, the cells achieved feats of bioengineering well beyond those of our best gene jockeys.</p><p data-block="sciam/paragraph">Some of the most intense interest in Levin's work has come from the fields of artificial intelligence and robotics, which see in basal cognition a way to address some core weaknesses. For all their remarkable prowess in manipulating language or playing games with well-defined rules, AIs still struggle immensely to understand the physical world. They can churn out sonnets in the style of Shakespeare, but ask them how to walk or to predict how a ball will roll down a hill, and they are clueless.</p><p data-block="sciam/paragraph">According to Bongard, that's because these AIs are, in a sense, too heady. “If you play with these AIs, you can start to see where the cracks are. And they tend to be around things like common sense and cause and effect, which points toward why you need a body. If you have a body, you can learn about cause and effect because you can <em>cause effects</em>. But these AI systems can't learn about the world by poking at it.”</p><p data-block="sciam/paragraph">Bongard is at the vanguard of the “embodied cognition” movement, which seeks to design robots that learn about the world by monitoring the way their form interacts with it. For an example of embodied cognition in action, he says, look no further than his one-and-a-half-year-old child, “who is probably destroying the kitchen right now. That's what toddlers do. They poke the world, literally and metaphorically, and then watch how the world pushes back. It's relentless.”</p><p data-block="sciam/paragraph">Bongard's lab uses AI programs to design robots out of flexible, LEGO-like cubes that he calls “<em>Minecraft</em> for robotics.” The cubes act like blocky muscle, allowing the robots to move their bodies like caterpillars. The AI-designed robots learn by trial and error, adding and subtracting cubes and “evolving” into more mobile forms as the worst designs get eliminated.</p><figure data-original-class="image-captioned" data-block="sciam/image"><a aria_label="Open image in new tab" href="https://static.scientificamerican.com/sciam/cache/file/D08F1B3D-F9CE-4076-BF68B82EB51BC674_medium.jpg?w=1536" target="_blank"><img alt="Illustration of plants." decoding="async" height="1097" loading="lazy" sizes="(min-width: 900px) 900px, (min-resolution: 3dppx) 50vw, (min-resolution: 2dppx) 75vw, 100vw" src="https://static.scientificamerican.com/sciam/cache/file/D08F1B3D-F9CE-4076-BF68B82EB51BC674_medium.jpg?w=900" srcset="https://static.scientificamerican.com/sciam/cache/file/D08F1B3D-F9CE-4076-BF68B82EB51BC674_medium.jpg?w=600 600w, https://static.scientificamerican.com/sciam/cache/file/D08F1B3D-F9CE-4076-BF68B82EB51BC674_medium.jpg?w=750 750w, https://static.scientificamerican.com/sciam/cache/file/D08F1B3D-F9CE-4076-BF68B82EB51BC674_medium.jpg?w=900 900w, https://static.scientificamerican.com/sciam/cache/file/D08F1B3D-F9CE-4076-BF68B82EB51BC674_medium.jpg?w=1000 1000w, https://static.scientificamerican.com/sciam/cache/file/D08F1B3D-F9CE-4076-BF68B82EB51BC674_medium.jpg?w=1200 1200w, https://static.scientificamerican.com/sciam/cache/file/D08F1B3D-F9CE-4076-BF68B82EB51BC674_medium.jpg?w=1350 1350w" width="1536"></a>
<figcaption>Plants use bioelectricity to communicate and take action. If you brush a sensory hair on a Venus flytrap (<em>right</em>), and the flytrap is wired to a touch-me-not plant (<em>left</em>), leaves on the touch-me-not will fold and wilt. Credit: Natalya Balnova</figcaption></figure><p data-block="sciam/paragraph">In 2020 Bongard's AI discovered how to make robots walk. That accomplishment inspired Levin's lab to use microsurgery to remove live skin stem cells from an African clawed frog and nudge them together in water. The cells fused into a lump the size of a sesame seed and acted as a unit. Skin cells have cilia, tiny hairs that typically hold a layer of protective mucus on the surface of an adult frog, but these creations used their cilia like oars, rowing through their new world. They navigated mazes and even closed up wounds when injured. Freed from their confined existence in a biological cubicle, they became something new and made the best of their situation. They definitely weren't frogs, despite sharing the identical genome. But because the cells originally came from frogs of the genus <em>Xenopus</em>, <a href="https://www.scientificamerican.com/video/these-researchers-used-a-i-to-design-a-completely-new-animal-robot/">Levin and Bongard nicknamed the things “xenobots.”</a> In 2023 they showed similar feats could be achieved by pieces of another species: human lung cells. Clumps of the human cells self-assembled and moved around in specific ways. <a href="https://www.scientificamerican.com/article/robots-made-from-human-cells-can-move-on-their-own-and-heal-wounds/">The Tufts team named them “anthrobots.”</a></p><p data-block="sciam/paragraph">To Levin, the xenobots and anthrobots are another sign that we need to rethink the way cognition plays out in the actual world. “Typically when you ask about a given living thing, you ask, ‘Why does it have the shape it has? Why does it have the behaviors it has?' And the standard answer is evolution, of course. For eons it was selected for. Well, guess what? There have never been any xenobots. There's never been any pressure to be a good xenobot. So why do these things do what they do within 24 hours of finding themselves in the world? I think it's because evolution does not produce specific solutions to specific problems. It produces problem-solving machines.”</p><p data-block="sciam/paragraph">Xenobots and anthrobots are, of course, quite limited in their capabilities, but perhaps they provide a window into how intelligence might naturally scale up when individual units with certain goals and needs come together to collaborate. Levin sees this innate tendency toward innovation as one of the driving forces of evolution, pushing the world toward a state of, as Charles Darwin might have put it, endless forms most beautiful. “We don't really have a good vocabulary for it yet,” he says, “but I honestly believe that the future of all this is going to look more like psychiatry talk than chemistry talk. We're going to end up having a calculus of pressures and memories and attractions.”</p><p data-block="sciam/paragraph">Levin hopes this vision will help us overcome our struggle to acknowledge minds that come in packages bearing little resemblance to our own, whether they are made of slime or silicon. For Adelaide's Lyon, recognizing that kinship is the real promise of basal cognition. “We think we are the crown of creation,” she says. “But if we start realizing that we have a whole lot more in common with the blades of grass and the bacteria in our stomachs—that we are related at a really, really deep level—it changes the entire paradigm of what it is to be a human being on this planet.”</p><p data-block="sciam/paragraph">Indeed, the very act of living is by default a cognitive state, Lyon says. Every cell needs to be constantly evaluating its surroundings, making decisions about what to let in and what to keep out and planning its next steps. Cognition didn't arrive later in evolution. It's what made life possible.</p><p data-block="sciam/paragraph">“Everything you see that's alive is doing this amazing thing,” Lyon points out. “If an airplane could do that, it would be bringing in its fuel and raw materials from the outside world while manufacturing not just its components but also the machines it needs to make those components and doing repairs, <em>all while it's flying!</em> What we do is nothing short of a miracle.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Self-rewarding-lm-PyTorch: Self-Rewarding Language Model from MetaAI (110 pts)]]></title>
            <link>https://github.com/lucidrains/self-rewarding-lm-pytorch</link>
            <guid>39125646</guid>
            <pubDate>Thu, 25 Jan 2024 02:43:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/lucidrains/self-rewarding-lm-pytorch">https://github.com/lucidrains/self-rewarding-lm-pytorch</a>, See on <a href="https://news.ycombinator.com/item?id=39125646">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:lucidrains/self-rewarding-lm-pytorch" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="6ht8uhPiA0bl4btXI1G3jI_seybFSNwRxwcAvWdMgdzXhFkP1gexQaUl_i8oUMo5ZVVOOxkckXGyJ9BjvQ644A" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="lucidrains/self-rewarding-lm-pytorch" data-current-org="" data-current-owner="lucidrains" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=H5W3jE22BVKa0urkDL1EehE%2FCTIN0BziQhQ5OTVQNEJyN%2FqyVDh%2FWMGDiYAe1LmPhul%2BY%2BERRWnLXfvs%2Fp%2Fy5fEQOcPpDDYR0JtRmIKuXF7KjcmRurDp08n1RfbMBKQ9Tkr3%2FUbyrbTgnxbG4Lw6qxoLO%2Fp3AHCHi%2Fgv1WsMolKUE7LdCbr8lPkAhQnuir6z51gQdq1ljJbn1%2FmonQcrGW298TKNvcZm74KnOOXA2JyJ1CjNghrfMDwZ%2BrKjB9cfNHThfHfasuOb5dG8m2GSsE3bZ3sgZeYAWGOuzOzyar%2Bh0yyPlRyei1eDpBXGKtoUIlN85xehwIGPsPSHC8PTp8J2dEN5MD31Y6SyxUIS1wKbux2oO4YeRveIuc%2FcFk73r7Y6aq%2ByHWyqgeDy6AxuYR3uv26POHnSSKNpu2kKjWgWMUgYZc3oNJQ8sWJUSKt7Fez2pnKXjrowF2knEO8gKD6oUX6lyJ2QlfN%2BUI3KNX0VWeuN7cpwvDosUBS%2Bn52bZ6a%2BjsvKJUu6vb0z5wpaR5qvd8JQ5bzEJ4brr7xAJMBUTha2Id0%3D--Fkmqc5UCSMR0RjJ4--PlPsxNdJa4zyd%2Bb2yKVwbw%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=lucidrains%2Fself-rewarding-lm-pytorch" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/self-rewarding-lm-pytorch&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="1918200d3f96f8468ffdde228c2193b0143ebbe86ab1c37643ebb80f8dc0c206" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ollama releases Python and JavaScript Libraries (398 pts)]]></title>
            <link>https://ollama.ai/blog/python-javascript-libraries</link>
            <guid>39125477</guid>
            <pubDate>Thu, 25 Jan 2024 02:17:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ollama.ai/blog/python-javascript-libraries">https://ollama.ai/blog/python-javascript-libraries</a>, See on <a href="https://news.ycombinator.com/item?id=39125477">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <h2>January 23, 2024</h2>
      <section>
        <p><img src="https://ollama.ai/public/blog/libraries.svg" alt="Python &amp; JavaScript Libraries"></p>

<p>The initial versions of the Ollama Python and JavaScript libraries are now available:</p>

<ul>
<li><a href="https://github.com/ollama/ollama-python">Ollama Python Library</a></li>
<li><a href="https://github.com/ollama/ollama-js">Ollama JavaScript Library</a></li>
</ul>

<p>Both libraries make it possible to integrate new and existing apps with Ollama in a few lines of code, and share the features and feel of the Ollama REST API.</p>

<h2>Getting Started</h2>

<p><sub>Python</sub></p>

<pre><code>pip install ollama
</code></pre>

<pre><code>import ollama
response = ollama.chat(model='llama2', messages=[
  {
    'role': 'user',
    'content': 'Why is the sky blue?',
  },
])
print(response['message']['content'])
</code></pre>

<p><sub>JavaScript</sub></p>

<pre><code>npm install ollama
</code></pre>

<pre><code>import ollama from 'ollama'

const response = await ollama.chat({
  model: 'llama2',
  messages: [{ role: 'user', content: 'Why is the sky blue?' }],
})
console.log(response.message.content)
</code></pre>

<h2>Use cases</h2>

<p>Both libraries support Ollama’s full set of features. Here are some examples in Python:</p>

<h4>Streaming</h4>

<pre><code>for chunk in chat('mistral', messages=messages, stream=True):
  print(chunk['message']['content'], end='', flush=True)
</code></pre>

<h4>Multi-modal</h4>

<pre><code>with open('image.png', 'rb') as file:
  response = ollama.chat(
    model='llava',
    messages=[
      {
        'role': 'user',
        'content': 'What is strange about this image?',
        'images': [file.read()],
      },
    ],
  )
print(response['message']['content'])
</code></pre>

<h4>Text Completion</h4>

<pre><code>result = ollama.generate(
  model='stable-code',
  prompt='// A c function to reverse a string\n',
)
print(result['response'])
</code></pre>

<h4>Creating custom models</h4>

<pre><code>modelfile='''
FROM llama2
SYSTEM You are mario from super mario bros.
'''

ollama.create(model='example', modelfile=modelfile)
</code></pre>

<h4>Custom client</h4>

<pre><code>ollama = Client(host='my.ollama.host')
</code></pre>

<p>More examples are available in the GitHub repositories for the <a href="https://github.com/ollama/ollama-python/tree/main/examples">Python</a> and <a href="https://github.com/ollama/ollama-js/tree/main/examples">JavaScript</a> libraries.</p>

<h2>New GitHub handle</h2>

<p><img src="https://ollama.ai/public/blog/github-handle.svg" alt="GitHub handle"></p>

<p>These libraries, and the main Ollama repository now live in a new GitHub organization: <strong><a href="https://github.com/ollama">ollama</a></strong>!  Thank you to all the amazing community members who maintain libraries to interact with Ollama via Dart, Swift, C#, Java, PHP, Rust and more – a full list is available <a href="https://github.com/jmorganca/ollama?tab=readme-ov-file#libraries">here</a> – please don’t hesitate to make a pull request to add a library you’ve built or enjoy using.</p>

<p>Special thank you to <a href="https://github.com/saul-jb">Saul</a>, the original author of <code>ollama-js</code>, and everyone else who has worked on community libraries to make Ollama more accessible from different programming languages.</p>

      </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kagi search reached 20k paying members (331 pts)]]></title>
            <link>https://blog.kagi.com/celebrating-20k</link>
            <guid>39124497</guid>
            <pubDate>Thu, 25 Jan 2024 00:14:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.kagi.com/celebrating-20k">https://blog.kagi.com/celebrating-20k</a>, See on <a href="https://news.ycombinator.com/item?id=39124497">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
            <p>Dear Kagi community,</p>

<p>Today, we’re happy and proud to have reached <strong>20,000 paying members</strong>, including over 1,500 families all over the world, in our community. We have come a very long way since the first few lines of code and the crazy idea that a search company could not only displace giants for every day use, but do it in a sustainable, <a href="https://blog.kagi.com/age-pagerank-over">user-centric</a> way.</p>

<p><a href="https://kagifeedback.org/assets/files/2024-01-24/1706071825-864946-kagi-crossing-1.jpg"><img src="https://kagifeedback.org/assets/files/2024-01-24/1706071825-864946-kagi-crossing-1.jpg" \=""></a>
</p><p><em>Early Kagi team, two years ago</em></p>

<p>The community has always been at the center of our mission. You all have supported us through thick and thin, have sent us so many lovely messages of support at times when it seemed like the mountain was too high to climb, and enabled us to seed a new internet, better for humans.</p>

<blockquote>
<p>Just wanted to drop you a note and let you know that I love Kagi. It’s awesome.</p>
</blockquote>

<p>Thank you Jason, without support from you and 20,000 other Kagi lovers around the world we would not be able to be here today. 🙏</p>

<p>For all of this, we’d like to thank you. As we previously announced, we’re going to launch a merchandise store this year. It’s been quite a journey to set it up, and we’d hoped to align its launch date with today, but unfortunately the paperwork gods have decided otherwise (plus we did not expect we will <a href="https://kagi.com/stats">reach 20k as soon</a>!)…</p>

<p>Nevertheless here is what we’re going to do:</p>

<p>When the merch store goes live in about 8 weeks,  <strong>first 20,000 Kagi members will get a free Kagi T-shirt + sticker pack</strong> (excluding shipping). If there are unclaimed shirts left after a few weeks, we’ll send vouchers to all the new members that joined in the meantime on a first come, first serve basis. Sizes are first come first serve too.</p>

<p>We have four t-shirt designs to pick from.  Here is what they look like (click to enlarge):</p>

<p><a href="https://kagifeedback.org/assets/files/2024-01-23/1706048214-141575-doggo-tshirt-yellow-2.jpg"><img src="https://kagifeedback.org/assets/files/2024-01-23/1706048214-141575-doggo-tshirt-yellow-2.jpg" \=""></a>
<a href="https://kagifeedback.org/assets/files/2024-01-23/1706048213-534940-doggo-tshirt-black-2.jpg"><img src="https://kagifeedback.org/assets/files/2024-01-23/1706048213-534940-doggo-tshirt-black-2.jpg" \=""></a>
<a href="https://kagifeedback.org/assets/files/2024-01-23/1706048213-852364-doggo-tshirt-yellow-1.jpg"><img src="https://kagifeedback.org/assets/files/2024-01-23/1706048213-852364-doggo-tshirt-yellow-1.jpg" \=""></a>
<a href="https://kagifeedback.org/assets/files/2024-01-23/1706048213-259584-doggo-tshirt-black-1.jpg"><img src="https://kagifeedback.org/assets/files/2024-01-23/1706048213-259584-doggo-tshirt-black-1.jpg" \=""></a></p>

<p>And of course, Doggo sticker pack!</p>

<p><a href="https://kagifeedback.org/assets/files/2024-01-23/1706048250-795569-kagi-stickers.png"><img src="https://kagifeedback.org/assets/files/2024-01-23/1706048250-795569-kagi-stickers.png" \=""></a></p>

<p>Setting up this project was a significant challenge, and true to our ethos, Kagi approached it with a ‘hard mode on’ mentality, aiming for high quality and meticulous attention to detail in the t-shirts. We initially engaged with several popular merch providers used by many large companies, but after months of testing, none could meet our standards for precision and quality.</p>

<p>The journey led us back to a modest print shop in a small town near Novi Sad, Serbia, where we first created our remarkable t-shirts years ago. Despite initial doubts about their capacity, they impressively accepted and successfully fulfilled our large order of 20,000 t-shirts on schedule.</p>

<p><a href="https://kagifeedback.org/assets/files/2024-01-23/1706048438-833281-img-0503.jpg"><img src="https://kagifeedback.org/assets/files/2024-01-23/1706048438-833281-img-0503.jpg" \=""></a>
<a href="https://kagifeedback.org/assets/files/2024-01-23/1706048438-913308-img-0500.jpg"><img src="https://kagifeedback.org/assets/files/2024-01-23/1706048438-913308-img-0500.jpg" \=""></a>
<a href="https://kagifeedback.org/assets/files/2024-01-24/1706070615-595554-img-0501.jpg"><img src="https://kagifeedback.org/assets/files/2024-01-24/1706070615-595554-img-0501.jpg" \=""></a>
<a href="https://kagifeedback.org/assets/files/2024-01-23/1706048438-760781-img-0491.jpg"><img src="https://kagifeedback.org/assets/files/2024-01-23/1706048438-760781-img-0491.jpg" \=""></a>
<a href="https://kagifeedback.org/assets/files/2024-01-23/1706048438-993881-img-0492.jpg"><img src="https://kagifeedback.org/assets/files/2024-01-23/1706048438-993881-img-0492.jpg" \=""></a></p>

<p>The process from here involves setting up a business entity in Germany, so we can import the t-shirts, store them in a warehouse, connect inventory logistics and ship them all over the world. This includes building a website and connecting it to a back-end database. So, we basically ended up owning a merch production operation end-to-end, just so that we could ensure premium quality of these t-shirts!</p>

<p>Now, you may ask, why did we go through all this trouble and allocate nearly a third of our <a href="https://blog.kagi.com/safe-round">investor-raised funds</a> to produce and freely distribute 20,000 t-shirts?</p>

<ol>
<li>We would not be here without our early adopters (you!) and we deemed it important to pause, reflect and show gratitude.</li>
<li>We acknowledge that our journey is a marathon, not a sprint. With a long road ahead, supporting our member community is both rewarding and meaningful.</li>
<li>Simply put, wearing the Doggo t-shirt is an incredibly awesome experience.</li>
</ol>

<p><iframe width="800" height="450" src="https://www.youtube-nocookie.com/embed/ZW0kupxsMUU?si=OuFxpIG53rsqGHh7&amp;rel=0&amp;vq=hd1080" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p><p><em>Video courtesy of  <a href="https://www.hankntank.com/info">Matthew Bradford</a>  ❤️ Kagi member from Austin, Texas</em></p>

<p>Thank you for your support, and we are setting the next milestone at <a href="https://twitter.com/vladquant/status/1534745055268773888">50,000</a>!</p>

<p>–</p>

<p>Discuss on <a href="https://kagi.com/discord">Kagi Discord</a> | <a href="https://news.ycombinator.com/item?id=39124497">Hacker News</a></p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Researchers find signs of intelligence among jumping spiders (158 pts)]]></title>
            <link>https://knowablemagazine.org/content/article/mind/2021/are-spiders-intelligent</link>
            <guid>39124256</guid>
            <pubDate>Wed, 24 Jan 2024 23:49:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://knowablemagazine.org/content/article/mind/2021/are-spiders-intelligent">https://knowablemagazine.org/content/article/mind/2021/are-spiders-intelligent</a>, See on <a href="https://news.ycombinator.com/item?id=39124256">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h2>Republish</h2>
<p>Thank you for your interest in republishing! This HTML is pre-formatted to adhere to our <a href="https://knowablemagazine.org/page/republish">guidelines</a>, which include: Crediting both the
author and Knowable Magazine; preserving all hyperlinks; including the canonical link to the
original article in the article metadata. Article text (including the headline) may not be edited
without prior permission from Knowable Magazine staff. Photographs and illustrations are not
included in this license. Please see our <a href="https://knowablemagazine.org/page/republish">full
guidelines</a> for more information.</p>

<p><a href="#">close</a>
</p></div><p>http://instance.metastore.ingenta.com/content/article/mind/2021/are-spiders-intelligent</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Majority of America's underground water stores are drying up, study finds (122 pts)]]></title>
            <link>https://thehill.com/policy/equilibrium-sustainability/4426143-majority-of-americas-underground-water-stores-are-drying-up-study-finds/</link>
            <guid>39122929</guid>
            <pubDate>Wed, 24 Jan 2024 21:38:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thehill.com/policy/equilibrium-sustainability/4426143-majority-of-americas-underground-water-stores-are-drying-up-study-finds/">https://thehill.com/policy/equilibrium-sustainability/4426143-majority-of-americas-underground-water-stores-are-drying-up-study-finds/</a>, See on <a href="https://news.ycombinator.com/item?id=39122929">Hacker News</a></p>
Couldn't get https://thehill.com/policy/equilibrium-sustainability/4426143-majority-of-americas-underground-water-stores-are-drying-up-study-finds/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[BLIS: A BLAS-like framework for basic linear algebra routines (138 pts)]]></title>
            <link>https://github.com/flame/blis</link>
            <guid>39122286</guid>
            <pubDate>Wed, 24 Jan 2024 20:32:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/flame/blis">https://github.com/flame/blis</a>, See on <a href="https://news.ycombinator.com/item?id=39122286">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:flame/blis" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="epy3DeZW0rj1bN585RLNinUajcRivmIIkdzpPA5LSFvZJZgwciGWns50nrH2XwBP9Kra7Kr26Gdp6iJJjGbGqQ" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="flame/blis" data-current-org="flame" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=KLg1Qntr19UsezhPT1bc0YZJp%2BAwYF1xhQRCFyh4UkPU5IDTY7ePJi9ZzUNnTsaqUKO8T02AG%2Fg1cH%2FR8RTBk2MZm%2BqLmxjW9PkWLEA6AOfwMBdaCnaKx8l7h8Mp4aPtU0gjMNxGMT6Buc15X2A9%2BXhxEmkfWEAqqlWHibYGtVwJS%2BWX%2B0E1UsehWY6kya5NbwGGmJuxyO%2B9FTvKTbWLwpvgHRhzoA1cc9DmllbXy%2Fz4hhErKyzLqmOqKzQlYaJacJcPCdYw31sI68ZrvSvDlFBiy0zCjTEBSPOn8V10O98ajlZj%2BuGXMubI%2Boi3gaLAwMAckT55Dw7jUSX0M2VGvpvrqrZZf69xlkIagyStcczTIZHhZISK%2Brry6uP6Xp9i9pvJJTaMWPJs0fAhwjzfwFyDADgXNd9t5Kiz3zFrM5BX0RNqQKZb6uLJVyXxVDLwtwFOHNaWgtTjQTR6Q9Dg48UPUKgrgyTBF3nmaNJ75GmTTiLcc1HdU7KkbMTlDoN9BiwpFLiS--xTc0tR7Wrq7eDvKH--QGbGKE6EtTqD7mLPaCH5CA%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=flame%2Fblis" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/flame/blis&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="04e72a3cfe45d4ff4d31ed0efe7ba2ad318732c0128f2334ede073df628e3c48" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Inside a global phone spy tool monitoring billions (101 pts)]]></title>
            <link>https://www.404media.co/inside-global-phone-spy-tool-patternz-nuviad-real-time-bidding/</link>
            <guid>39122281</guid>
            <pubDate>Wed, 24 Jan 2024 20:32:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/inside-global-phone-spy-tool-patternz-nuviad-real-time-bidding/">https://www.404media.co/inside-global-phone-spy-tool-patternz-nuviad-real-time-bidding/</a>, See on <a href="https://news.ycombinator.com/item?id=39122281">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<p>Hundreds of thousands of ordinary apps, including popular ones such as 9gag, Kik, and a series of caller ID apps, are part of a global surveillance capability that starts with ads inside each app, and ends with the apps’ users being swept up into a powerful mass monitoring tool advertised to national security agencies that can track the physical location, hobbies, and family members of people to build billions of profiles, according to a 404 Media investigation.</p><p>404 Media’s investigation, based on now deleted marketing materials and videos, technical forensic analysis, and research from privacy activists, provides one of the clearest examinations yet of how advertisements in ordinary mobile apps can ultimately lead to surveillance by spy firms and their government clients through the real time bidding data supply chain. The pipeline involves smaller, obscure advertising firms and advertising industry giants like Google. In response to queries from 404 Media, Google and PubMatic, another ad firm, have already cut-off a company linked to the surveillance firm.</p><p>“The pervasive surveillance machine that has been developed for digital advertising now directly enables government mass surveillance. Many businesses, from app publishers to advertisers to big tech, are acting completely irresponsibly. This must end,” Wolfie Christl, the principal of Cracked Labs, an Austrian research institute and co-author of a <a href="https://www.iccl.ie/wp-content/uploads/2023/11/Europes-hidden-security-crisis.pdf?ref=404media.co"><u>paper published last year that researched</u></a> the surveillance tool, told 404 Media.</p><div><p>💡</p><p><b><strong>Do you know anything else about about Patternz, Nuviad, or RTB surveillance? I would love to hear from you. Using a non-work device, you can message me securely on Signal at +44 20 8133 5190. Otherwise, send me an email at joseph@404media.co.</strong></b></p></div><h2 id="patternz"><strong>Patternz</strong></h2><p>The mass monitoring tool in question is called Patternz. In a video uploaded to YouTube in January 2023 that was removed once 404 Media started to make inquiries, Rafi Ton, the CEO of Patternz, says “we analyze behavior of over 600,000 applications.” One slide he brings up during the video says that “the mobile phone becomes the de-facto tracking bracelet,” and suggests tracking can be achieved through “virtually any app that has ads.” The video appears to be a demonstration Ton is giving to potential clients for the Patternz system. The context of the pitch is for Patternz to counter COVID-19, but Ton acknowledges that the platform was built as a “homeland security platform.” In other <a href="https://sovsys.co/wp-content/uploads/2020/04/PATTERNZ-NATIONAL-SECURITY-PATTERN-DETECTION.pdf?ref=404media.co"><u>marketing materials online</u></a>, Patternz pitches itself specifically to “national security agencies.”</p>
</div><div>
  <div>
    <h2>This post is for paid members only</h2>
    <p>Become a paid member for unlimited ad-free access to articles, bonus podcast content, and more.</p>
  </div>
  <div>
    <h2>Sign up for free access to this post</h2>
    <p>Free members get access to posts like this one along with an email round-up of our week's stories.</p>
  </div>
  <p><a href="https://www.404media.co/signup/" data-portal="signup">Subscribe</a></p><p>Already have an account? <a href="https://www.404media.co/signin/" data-portal="signin">Sign in</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CRT Manufacturing (120 pts)]]></title>
            <link>https://vintagetek.org/crt-manufacturing/</link>
            <guid>39122060</guid>
            <pubDate>Wed, 24 Jan 2024 20:13:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vintagetek.org/crt-manufacturing/">https://vintagetek.org/crt-manufacturing/</a>, See on <a href="https://news.ycombinator.com/item?id=39122060">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content" role="main" itemtype="https://schema.org/WebPageElement" itemscope="" itemprop="mainContentOfPage"> <!-- page -->
<article id="post-4965" class="page">
    			<!-- .page-header -->
			    <div>

        <p>This January 4, 1954 Oregonian article features the construction of the new CRT building.</p>
<p><img decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/10/NewTekBldg_Oregonian_Jan_4_1954.jpg" alt="" width="350" height="853" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/10/NewTekBldg_Oregonian_Jan_4_1954.jpg 350w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/10/NewTekBldg_Oregonian_Jan_4_1954-123x300.jpg 123w" sizes="(max-width: 350px) 100vw, 350px"></p>

<p>Here are some early photos of CRT manufacturing at the Sunset plant.</p>



<p>A number of CRT lamps were seen around Tektronix and the museum has a few.&nbsp; This June 6, 1955 TekTalk features an article about the CRT lamp.</p>
<p><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/06/CRT_Lamp_TekTalk_06061955.jpg" alt="" width="350" height="1014" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/06/CRT_Lamp_TekTalk_06061955.jpg 350w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/06/CRT_Lamp_TekTalk_06061955-104x300.jpg 104w" sizes="(max-width: 350px) 100vw, 350px"></p>

<p>This August 5, 1955 TekTalk features a follow-on article about the CRT lamps.</p>
<p><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/06/CRT_Lamp_TekTalk_08051955.jpg" alt="" width="350" height="402" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/06/CRT_Lamp_TekTalk_08051955.jpg 350w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/06/CRT_Lamp_TekTalk_08051955-261x300.jpg 261w" sizes="(max-width: 350px) 100vw, 350px"></p>

<p>This October 1957 TekTalk features another follow-on article about the CRT lamps.</p>
<p><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/09/CRTLamp_TekTalk_Oct1957.jpg" alt="" width="600" height="472" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/09/CRTLamp_TekTalk_Oct1957.jpg 600w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/09/CRTLamp_TekTalk_Oct1957-300x236.jpg 300w" sizes="(max-width: 600px) 100vw, 600px"></p>

<p>This November 14, 1956 TekTalk features three employees in the CRT Department.</p>
<p><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/08/CRT_Assy_TekTalk_11141956.jpg" alt="" width="700" height="1482" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/08/CRT_Assy_TekTalk_11141956.jpg 700w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/08/CRT_Assy_TekTalk_11141956-142x300.jpg 142w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/08/CRT_Assy_TekTalk_11141956-484x1024.jpg 484w" sizes="(max-width: 700px) 100vw, 700px"></p>

<p>This February 1957 TekTalk features the Small Parts group which manufactures the internal components of the CRT gun assembly. Click on the image to view the PDF.</p>
<p><a href="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/08/CRT_TekTalk_Feb1957.pdf"><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/08/CRT_TekTalk_Feb1957-141x300.jpg" alt="" width="141" height="300" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/08/CRT_TekTalk_Feb1957-141x300.jpg 141w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/08/CRT_TekTalk_Feb1957-481x1024.jpg 481w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/08/CRT_TekTalk_Feb1957.jpg 700w" sizes="(max-width: 141px) 100vw, 141px"></a></p>

<p>This May 1957 TekTalk features the Gun Group along with the two photos.</p>
<p><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/ShotFromGuns-TekTalk_May1957.jpg" alt="" width="700" height="1482" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/ShotFromGuns-TekTalk_May1957.jpg 700w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/ShotFromGuns-TekTalk_May1957-142x300.jpg 142w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/ShotFromGuns-TekTalk_May1957-484x1024.jpg 484w" sizes="(max-width: 700px) 100vw, 700px"></p>
<p><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/TekTalk_May1957-1.jpg" alt="" width="700" height="419" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/TekTalk_May1957-1.jpg 700w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/TekTalk_May1957-1-300x180.jpg 300w" sizes="(max-width: 700px) 100vw, 700px"></p>
<p><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/TekTalk_May1957-2.jpg" alt="" width="500" height="431" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/TekTalk_May1957-2.jpg 500w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/TekTalk_May1957-2-300x259.jpg 300w" sizes="(max-width: 500px) 100vw, 500px"></p>

<p>This September 1957 TekTalk features the automatic pin welder for CRT gun assembly.</p>
<p><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/09/CRT_TekTalk_Sept1957.jpg" alt="" width="500" height="754" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/09/CRT_TekTalk_Sept1957.jpg 500w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/09/CRT_TekTalk_Sept1957-199x300.jpg 199w" sizes="(max-width: 500px) 100vw, 500px"></p>

<p>This October 1957 TekTalk features the Bottle Preparation group.</p>
<p><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/09/CRT_TekTalk_Oct1957.jpg" alt="" width="700" height="1464" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/09/CRT_TekTalk_Oct1957.jpg 700w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/09/CRT_TekTalk_Oct1957-143x300.jpg 143w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/09/CRT_TekTalk_Oct1957-490x1024.jpg 490w" sizes="(max-width: 700px) 100vw, 700px"></p>

<p>This January 1958 TekTalk features the Screen Preparation group along with a similar photo.</p>
<p><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/BakersDozen-TekTalk_Jan1958.jpg" alt="" width="700" height="1102" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/BakersDozen-TekTalk_Jan1958.jpg 700w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/BakersDozen-TekTalk_Jan1958-191x300.jpg 191w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/BakersDozen-TekTalk_Jan1958-650x1024.jpg 650w" sizes="(max-width: 700px) 100vw, 700px"></p>
<p><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/TekTalk_Jan1958.jpg" alt="" width="700" height="442" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/TekTalk_Jan1958.jpg 700w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/TekTalk_Jan1958-300x189.jpg 300w" sizes="(max-width: 700px) 100vw, 700px"></p>
<p>This photo is also of the Sunset plant Screen Preparation area. We think the calendar on the wall might indicate 1954. We believe&nbsp;Laura Lusk is standing with pipette, Ella Hansen is standing on the left, and Laine Pettai is seated on the left.&nbsp; If you can confirm these or identify the individual on the right please contact the museum.</p>
<p><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/Sunset-plant-Phosphor-Screening-area.jpg" alt="" width="700" height="562" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/Sunset-plant-Phosphor-Screening-area.jpg 700w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/Sunset-plant-Phosphor-Screening-area-300x241.jpg 300w" sizes="(max-width: 700px) 100vw, 700px"></p>

<p>This CRT brochure is aimed at employees of CRT Manufacturing with an overview of the CRT and the importance of cleanliness and carefulness in manufacturing.&nbsp; It is not dated but features a 576 curve tracer which was introduced in 1969.&nbsp; Click on the image to view the PDF.</p>
<p><a href="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/03/CRTMfgBrochure.pdf"><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/03/CRTMfgBrochure-300x126.jpg" alt="" width="300" height="126" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/03/CRTMfgBrochure-300x126.jpg 300w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/03/CRTMfgBrochure.jpg 700w" sizes="(max-width: 300px) 100vw, 300px"></a></p>

<p>This Winter 1970 TekTalk article features ceramic CRT production. Click on the image to view the PDF.</p>
<p><a href="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/08/Ceramics_TekTalk_Winter1970.pdf"><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/08/Ceramics_TekTalk_Winter1970-228x300.jpg" alt="" width="228" height="300" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/08/Ceramics_TekTalk_Winter1970-228x300.jpg 228w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2019/08/Ceramics_TekTalk_Winter1970.jpg 700w" sizes="(max-width: 228px) 100vw, 228px"></a></p>

<p>This October 25, 1975 TekWeek features an Area Representative overview of the CRT operation with questions and answers.&nbsp; Click on the image to view the PDF.</p>
<p><a href="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2017/12/MakingOurOwnCRTs_TW_10251975.pdf"><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2017/12/MakingOurOwnCRTs_TW_10251975-234x300.jpg" alt="" width="234" height="300" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2017/12/MakingOurOwnCRTs_TW_10251975-234x300.jpg 234w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2017/12/MakingOurOwnCRTs_TW_10251975.jpg 700w" sizes="(max-width: 234px) 100vw, 234px"></a></p>


<p>This April 30, 1976 TekWeek article features a productivity improvement by the CRT Gun Group. Click on the image to view the PDF.</p>
<p><a href="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/CRTGunGrpBillJohnston_TW_04301976.pdf"><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/CRTGunGrpBillJohnston_TW_04301976-229x300.jpg" alt="" width="229" height="300" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/CRTGunGrpBillJohnston_TW_04301976-229x300.jpg 229w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/CRTGunGrpBillJohnston_TW_04301976.jpg 700w" sizes="(max-width: 229px) 100vw, 229px"></a></p>
<p>This image used in the TekWeek article is from a large wall photo donated by Linda and Bill Johnston.</p>
<p><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/CRT-gun-team.jpg" alt="" width="700" height="618" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/CRT-gun-team.jpg 700w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2021/10/CRT-gun-team-300x265.jpg 300w" sizes="(max-width: 700px) 100vw, 700px"></p>

<p>This&nbsp; February 19, 1982 TekWeek article features the commemoration of the manufacture of the 250,000 T465 CRT.&nbsp; Click the image to view the PDF.</p>
<p><a href="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2017/12/465CRTMilestone_TW_02191982.pdf"><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2017/12/465CRTMilestone_TW_02191982-234x300.jpg" alt="" width="234" height="300" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2017/12/465CRTMilestone_TW_02191982-234x300.jpg 234w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2017/12/465CRTMilestone_TW_02191982.jpg 700w" sizes="(max-width: 234px) 100vw, 234px"></a></p>

<p>This April 1, 1983 TekWeek features the high volume CRT line.&nbsp; More information and a film is available on our <a href="https://vintagetek.org/high-volume-crt-line/">High Volume CRT Line</a>&nbsp;page. Click on the image to view the PDF.</p>
<p><a href="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2023/05/HighVolumeLine_TekWeek_04011983.pdf"><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2023/05/HighVolumeLine_TekWeek_04011983_1-232x300.jpg" alt="" width="232" height="300" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2023/05/HighVolumeLine_TekWeek_04011983_1-232x300.jpg 232w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2023/05/HighVolumeLine_TekWeek_04011983_1.jpg 700w" sizes="(max-width: 232px) 100vw, 232px"></a></p>

<p>This August 12, 1983 TekWeek features low cost ceramic CRT manufacturing.&nbsp; Click on the image to view the PDF.</p>
<p><a href="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2017/12/CeramicsSuccess_TW_08121983.pdf"><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2017/12/CeramicsSuccess_TW_08121983-241x300.jpg" alt="" width="241" height="300" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2017/12/CeramicsSuccess_TW_08121983-241x300.jpg 241w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2017/12/CeramicsSuccess_TW_08121983.jpg 700w" sizes="(max-width: 241px) 100vw, 241px"></a></p>

<p>This May 19, 1989 TekWeek features the celebration of the 35th anniversary of CRT manufacturing.&nbsp; Click on the image to view the PDF.</p>
<p><a href="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2017/12/TekCRT35thAnniversary_TW_05191989.pdf"><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2017/12/TekCRT35thAnniversary_TW_05191989-225x300.jpg" alt="" width="225" height="300" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2017/12/TekCRT35thAnniversary_TW_05191989-225x300.jpg 225w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2017/12/TekCRT35thAnniversary_TW_05191989.jpg 700w" sizes="(max-width: 225px) 100vw, 225px"></a></p>

<p>This November 5, 1993 TekWeek article describes the wake for the last DVST CRT produced.</p>
<p><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/01/DVSTWake_TW_11051993.jpg" alt="" width="700" height="1405" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/01/DVSTWake_TW_11051993.jpg 700w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/01/DVSTWake_TW_11051993-149x300.jpg 149w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/01/DVSTWake_TW_11051993-510x1024.jpg 510w" sizes="(max-width: 700px) 100vw, 700px"></p>

<p>This faceplate also marks the end of storage CRT manufacturing.</p>
<p><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/07/Storage-CRT-Faceplate.jpg" alt="" width="700" height="592" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/07/Storage-CRT-Faceplate.jpg 700w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/07/Storage-CRT-Faceplate-300x254.jpg 300w" sizes="(max-width: 700px) 100vw, 700px"></p>

<p>This 1994 pin marks the end of miniscope CRT manufacturing.</p>
<p><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/07/Mini-Tubes-pin-300x300.jpg" alt="" width="300" height="300" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/07/Mini-Tubes-pin-300x300.jpg 300w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/07/Mini-Tubes-pin-150x150.jpg 150w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/07/Mini-Tubes-pin.jpg 660w" sizes="(max-width: 300px) 100vw, 300px"></p>

<p>This April 15, 1994 TekWeek article features the last CRT produced by the operation.</p>
<p><img loading="lazy" decoding="async" src="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/03/T7100CRTRetirement_TW_04151994.jpg" alt="" width="700" height="473" srcset="https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/03/T7100CRTRetirement_TW_04151994.jpg 700w, https://7vmc31.p3cdn1.secureserver.net/wp-content/uploads/2018/03/T7100CRTRetirement_TW_04151994-300x203.jpg 300w" sizes="(max-width: 700px) 100vw, 700px"></p>
    </div><!-- .entry-content -->

</article><!-- #post-4965 -->


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI scrapped a promise to disclose key documents to the public (555 pts)]]></title>
            <link>https://www.wired.com/story/openai-scrapped-promise-disclose-key-documents/</link>
            <guid>39121521</guid>
            <pubDate>Wed, 24 Jan 2024 19:21:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/openai-scrapped-promise-disclose-key-documents/">https://www.wired.com/story/openai-scrapped-promise-disclose-key-documents/</a>, See on <a href="https://news.ycombinator.com/item?id=39121521">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Wealthy tech entrepreneurs including Elon Musk <a href="https://www.wired.com/story/what-openai-really-wants/">launched OpenAI in 2015</a> as <a data-offer-url="https://openai.com/blog/introducing-openai" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://openai.com/blog/introducing-openai&quot;}" href="https://openai.com/blog/introducing-openai" rel="noopener" target="_blank">a nonprofit research lab</a> that they said would involve society and <a href="https://medium.com/backchannel/how-elon-musk-and-y-combinator-plan-to-stop-computers-from-taking-over-17e0e27dd02a#.1mz74bfmo">the public</a> in the development of powerful AI, unlike Google and other giant tech companies working behind closed doors. In line with that spirit, OpenAI’s reports to US tax authorities have from its founding said that any member of the public can review copies of its governing documents, financial statements, and conflict of interest rules.</p><p>But when WIRED requested those records last month, OpenAI said its policy had changed, and the company provided only a narrow financial statement that omitted the majority of its operations.</p><p>"We provide financial statements when requested,” company spokesperson Niko Felix says. “OpenAI aligns our practices with industry standards, and since 2022 that includes not publicly distributing additional internal documents.”</p><p>OpenAI’s abandonment of the long-standing transparency pledge obscures information that could shed light on the recent <a href="https://www.wired.com/story/openai-boardroom-drama-sam-altman-could-mess-up-your-future/">near-implosion</a> of a company with crucial influence over the future of AI and could help outsiders understand its vulnerabilities. In November, OpenAI’s board <a href="https://www.wired.com/story/openai-ceo-sam-altman-is-out-after-losing-confidence-of-board/">fired CEO Sam Altman</a>, implying in a statement that he was untrustworthy and had endangered its mission to ensure AI <a data-offer-url="https://openai.com/blog/openai-announces-leadership-transition" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://openai.com/blog/openai-announces-leadership-transition&quot;}" href="https://openai.com/blog/openai-announces-leadership-transition" rel="noopener" target="_blank">“benefits all humanity.”</a> An employee and investor revolt soon forced the board to <a href="https://www.wired.com/story/sam-altman-openai-back/">reinstate Altman</a> and <a href="https://www.wired.com/story/sam-altman-officially-returns-to-openai-board-seat-microsoft/">eject most of its own members</a>, with an overhauled slate of directors <a data-offer-url="https://openai.com/blog/sam-altman-returns-as-ceo-openai-has-a-new-initial-board" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://openai.com/blog/sam-altman-returns-as-ceo-openai-has-a-new-initial-board&quot;}" href="https://openai.com/blog/sam-altman-returns-as-ceo-openai-has-a-new-initial-board" rel="noopener" target="_blank">vowing</a> to review the crisis and enact structural changes to win back the trust of stakeholders.</p><p>Access to OpenAI’s conflict-of-interest policy could show what power the new board really has over Altman and his outside pursuits, which include personal investments in numerous startups pursuing AI projects and <a href="https://www.reuters.com/technology/openai-ceo-altman-says-davos-future-ai-depends-energy-breakthrough-2024-01-16/">a nuclear reactor maker</a>. His day job and personal projects intermingling played some role in board members’ distrust, according to people involved in the situation but not authorized to discuss it. In 2019, while Altman was at the helm, <a href="https://www.wired.com/story/openai-buy-ai-chips-startup-sam-altman/">OpenAI signed a nonbinding letter of intent</a> to buy $51 million of AI chips from Rain, a startup in which he has invested more than $1 million, WIRED reported last month. OpenAI hasn’t moved forward with a purchase. Felix says Altman is transparent with the board about his investments and follows a process for managing potential conflicts.</p><p>Some sunlight on OpenAI’s governing documents could reveal whether it has made revisions to stabilize an unusual corporate structure and potentially pacify backers such as Microsoft. The company’s founding bylaws, <a href="https://rct.doj.ca.gov/Verification/Web/Search.aspx?facility=Y">publicly available</a> via its 2016 application to the Internal Revenue Service for <a data-offer-url="https://www.irs.gov/charities-non-profits/charitable-organizations/exemption-requirements-501c3-organizations" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.irs.gov/charities-non-profits/charitable-organizations/exemption-requirements-501c3-organizations&quot;}" href="https://www.irs.gov/charities-non-profits/charitable-organizations/exemption-requirements-501c3-organizations" rel="noopener" target="_blank">tax-exempt status</a>, indicate how a fraction of the board could take control and push out Altman. OpenAI’s filings to the IRS through 2022 reported that no “significant changes” had ever been made to its governing documents. But the company almost certainly made updates after Altman’s return to allow it to <a href="https://www.wired.com/story/sam-altman-officially-returns-to-openai-board-seat-microsoft/">give a nonvoting seat</a> on the nonprofit board to Microsoft, whose CEO, Satya Nadella, complained publicly that he had been blindsided by Altman’s firing. Any additional changes made at that time remain a secret.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>WIRED’s request for the documents promised in OpenAI’s IRS filings fell to the counsel for its nonprofit to decide on. Their denial and new policy of withholding those documents extends an existing trend of dwindling openness at a project founded on transparency. OpenAI once published extensive detail about its AI inventions but has more recently guarded the technical details and data behind its most famous tool, ChatGPT. Felix, the company spokesperson, says San Francisco-based OpenAI discloses all the material required by the IRS and California’s attorney general and that it regularly publishes information about <a data-offer-url="https://openai.com/research/weak-to-strong-generalization" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://openai.com/research/weak-to-strong-generalization&quot;}" href="https://openai.com/research/weak-to-strong-generalization" rel="noopener" target="_blank">its research and safety work</a>, while also making its research freely available in the form of tools such as ChatGPT.</p><p>OpenAI’s declining openness has been most notable since 2019, when the nonprofit <a href="https://www.wired.com/story/compete-google-openai-seeks-investorsand-profits/">created a for-profit subsidiary</a> to house most of its AI development and draw outside investment. That opened the way for OpenAI to hitch its future to the largesse of Microsoft, one of the tech giants it was founded to challenge, and also to shroud its finances. OpenAI cofounder turned <a href="https://www.wired.com/story/fast-forward-elon-musks-xai-chatgpt-hallucinating/">competitor</a> Elon Musk <a data-offer-url="https://www.rev.com/transcript-editor/shared/BsSehREdKzBuHCZaWh3v15HZ6nXoc9WvTCBWBttFzNGTE_UDx9xRePx3riSr-gePRS_B_ZekP7fK0TdgmiCkAhmbAUI?loadFrom=PastedDeeplink&amp;ts=65.37" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.rev.com/transcript-editor/shared/BsSehREdKzBuHCZaWh3v15HZ6nXoc9WvTCBWBttFzNGTE_UDx9xRePx3riSr-gePRS_B_ZekP7fK0TdgmiCkAhmbAUI?loadFrom=PastedDeeplink&amp;ts=65.37&quot;}" href="https://www.rev.com/transcript-editor/shared/BsSehREdKzBuHCZaWh3v15HZ6nXoc9WvTCBWBttFzNGTE_UDx9xRePx3riSr-gePRS_B_ZekP7fK0TdgmiCkAhmbAUI?loadFrom=PastedDeeplink&amp;ts=65.37" rel="noopener" target="_blank">said at a <em>New York Times</em> event in November</a> that his former company should be called Super-Closed-Source-for-Maxiumum-Profit-AI.</p><p>Closely Held</p><p>OpenAI’s original nonprofit organization—and its board—retain ultimate control of OpenAI’s activities and technology. Like all US nonprofits, it must publicly share upon request a copy of its annual report to the IRS and indicate in those submissions whether any additional documents like its bylaws or similar or related documents such as a governing constitution or conflict of interest policy were also available to the public during the last year.</p><p>Some notable nonprofits do that, like the Bill &amp; Melinda Gates Foundation, which publishes its <a data-offer-url="https://docs.gatesfoundation.org/documents/bill_and_melinda_gates_foundation_board_governing_principles.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://docs.gatesfoundation.org/documents/bill_and_melinda_gates_foundation_board_governing_principles.pdf&quot;}" href="https://docs.gatesfoundation.org/documents/bill_and_melinda_gates_foundation_board_governing_principles.pdf" rel="noopener" target="_blank">bylaws</a> and rules <a data-offer-url="https://docs.gatesfoundation.org/Documents/conflict_of_interest_policy.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://docs.gatesfoundation.org/Documents/conflict_of_interest_policy.pdf&quot;}" href="https://docs.gatesfoundation.org/Documents/conflict_of_interest_policy.pdf" rel="noopener" target="_blank">on conflicts</a> and <a data-offer-url="https://docs.gatesfoundation.org/documents/personal-relationships-policy.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://docs.gatesfoundation.org/documents/personal-relationships-policy.pdf&quot;}" href="https://docs.gatesfoundation.org/documents/personal-relationships-policy.pdf" rel="noopener" target="_blank">workplace relationships</a>, but it’s not standard practice.</p><p>“It is not common for organizations to make their governing documents or internal policies public,” says Rick Cohen, chief operating and communications officer for <a data-offer-url="https://www.councilofnonprofits.org/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.councilofnonprofits.org/&quot;}" href="https://www.councilofnonprofits.org/" rel="noopener" target="_blank">National Council of Nonprofits</a>, an advocacy group.</p><p>Yet for seven consecutive years, from its founding through 2022, OpenAI stated in its <a href="https://projects.propublica.org/nonprofits/organizations/810861541">annual IRS filings</a> that it made those submissions as well as other files available “upon request.” It’s unclear if anyone ever took OpenAI up on the invitation in the years through 2022—OpenAI won’t say.</p><p>Last month, after two days of waiting on OpenAI communications staff to fulfill an emailed request for its governing documents, conflict rules, and financial statements, WIRED rang the doorbell outside OpenAI’s San Francisco headquarters on December 14 asking to see all those documents. A receptionist said over an intercom that wouldn’t be possible, hung up, and didn’t reengage. OpenAI’s IRS filing for 2023 that would reflect it has changed its previous policy isn’t due until later this year.</p><p>To <a data-offer-url="https://www.irs.gov/charities-non-profits/form-990-resources-and-tools" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.irs.gov/charities-non-profits/form-990-resources-and-tools&quot;}" href="https://www.irs.gov/charities-non-profits/form-990-resources-and-tools" rel="noopener" target="_blank">encourage oversight</a> of nonprofits, <a data-offer-url="https://www.irs.gov/charities-non-profits/exempt-organization-public-disclosure-and-availability-requirements" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.irs.gov/charities-non-profits/exempt-organization-public-disclosure-and-availability-requirements&quot;}" href="https://www.irs.gov/charities-non-profits/exempt-organization-public-disclosure-and-availability-requirements" rel="noopener" target="_blank">US tax law requires</a> them to at least make their annual reports to the IRS, form 990s, available for public inspection <a data-offer-url="https://www.irs.gov/charities-non-profits/public-disclosure-and-availability-of-exempt-organizations-returns-and-applications-disclosures-required" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.irs.gov/charities-non-profits/public-disclosure-and-availability-of-exempt-organizations-returns-and-applications-disclosures-required&quot;}" href="https://www.irs.gov/charities-non-profits/public-disclosure-and-availability-of-exempt-organizations-returns-and-applications-disclosures-required" rel="noopener" target="_blank">at their offices the same day they were requested</a> if they are not posted on an organization’s website. OpenAI doesn’t post its reports on its website but still didn’t provide them when WIRED showed up in person. IRS documentation says violators <a data-offer-url="https://www.irs.gov/charities-non-profits/public-disclosure-and-availability-of-exempt-organizations-returns-and-applications-penalties-for-noncompliance" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.irs.gov/charities-non-profits/public-disclosure-and-availability-of-exempt-organizations-returns-and-applications-penalties-for-noncompliance&quot;}" href="https://www.irs.gov/charities-non-profits/public-disclosure-and-availability-of-exempt-organizations-returns-and-applications-penalties-for-noncompliance" rel="noopener" target="_blank">can face fines of $20 a day</a>, up to $10,000, but the agency declined to comment about OpenAI, citing confidentiality <a href="https://www.law.cornell.edu/uscode/text/26/6103">provisions of tax law</a>. OpenAI hasn’t been accused of wrongdoing, and Felix says OpenAI’s reports are available online through government and research databases.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Even when nonprofits tell the IRS their internal documents are open to all, they can be difficult to access. “When I have asked a group for forms it says are public, I basically never get them unless I <em>cc</em> the IRS and the organization's general counsel,” says Brian Galle, a Georgetown University law professor specializing in nonprofit issues.</p><p>Sunshine Dividends</p><p>The financial statement OpenAI shared in response to WIRED’s request excludes the results of OpenAI’s “affiliated entities,” most crucially the for-profit unit that sells ChatGPT and other services, citing a desire to protect trade secrets. The statement, which covers 2022, shows just $44,000 in revenue and $1.3 million in expenses. That’s accurate for the nonprofit, but OpenAI overall reportedly generated <a data-offer-url="https://www.theinformation.com/articles/openais-annualized-revenue-tops-1-6-billion-as-customers-shrug-off-ceo-drama" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.theinformation.com/articles/openais-annualized-revenue-tops-1-6-billion-as-customers-shrug-off-ceo-drama&quot;}" href="https://www.theinformation.com/articles/openais-annualized-revenue-tops-1-6-billion-as-customers-shrug-off-ceo-drama" rel="noopener" target="_blank">hundreds of millions of dollars in sales last year</a> and <a data-offer-url="https://www.theinformation.com/articles/openais-losses-doubled-to-540-million-as-it-developed-chatgpt" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.theinformation.com/articles/openais-losses-doubled-to-540-million-as-it-developed-chatgpt&quot;}" href="https://www.theinformation.com/articles/openais-losses-doubled-to-540-million-as-it-developed-chatgpt" rel="noopener" target="_blank">spends even more</a> on high-end computers and top-flight researchers.</p><p>Investors in OpenAI’s for-profit arm and employees of the company likely have access to some of the records the company now declines to share publicly, though they are bound by nondisclosure agreements covering internal files. Because OpenAI’s nonprofit takes in virtually no public support, it lacks the incentive that some nonprofits have to be more transparent so as to loosen the purse strings of donor or grant makers.</p><p>OpenAI might win more loyalty from its customers or trust from regulators by opening up, but Altman has said the company didn’t lose any clients during its two weeks of high drama and generally has been greeted warmly by political leaders. The company may feel that the ongoing board review into OpenAI’s governance and what is shared from its findings will be enough.</p><p>Without a reversal in OpenAI’s policy and a follow-through on its promises, so much about an increasingly influential organization may never become publicly known, like whether the new board amends the conflict-of-interest policy to better wrangle Altman and other executives. Back in 2022, it may have been possible to get the answer with just the ring of a doorbell.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How we made an animated movie in 8kB (455 pts)]]></title>
            <link>https://www.ctrl-alt-test.fr/2024/how-we-made-an-animated-movie-in-8kb</link>
            <guid>39121101</guid>
            <pubDate>Wed, 24 Jan 2024 18:46:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ctrl-alt-test.fr/2024/how-we-made-an-animated-movie-in-8kb">https://www.ctrl-alt-test.fr/2024/how-we-made-an-animated-movie-in-8kb</a>, See on <a href="https://news.ycombinator.com/item?id=39121101">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>In November 2022, we set ourselves a challenge: make a real-time animation that looks like a standard short animated movie, with the constraint that it should fit in 8 kilobytes. The goal was to have decent graphics, animations, direction and camera work, and the matching music… Yes, 8 kilobytes, less than half of this post, for everything. It wasn’t clear how much was actually feasible, so we had to try it.</p>



<p>In April 2023, after months of work, we finally released <a href="https://www.ctrl-alt-test.fr/the-sheep-and-the-flower/">The Sheep and the Flower</a>. You can run it by yourself (<a href="https://aduprat.com/pub/The_Sheep_and_the_Flower.zip">download link</a>), or see a YouTube capture of the program running:</p>



<figure><p>
<iframe title="The Sheep and the Flower - Razor 1911 &amp; Ctrl-Alt-Test (8kB demo)" width="625" height="352" src="https://www.youtube.com/embed/-dNqYrP3QHU?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>Many people asked how we were able to create something like this. This article will explain the technical details and design constraints behind this production. We’ve also made the <a href="https://github.com/ctrl-alt-test/mouton">source code public on GitHub</a>.</p>



<h2>Overview</h2>



<p>The result is a Windows executable file. It’s a single .exe file that generates everything. It requires no resource file, no external depency, except for Windows and up-to-date drivers. </p>



<p>Here’s the quick summary of what we used. We’ll explain the details in the rest of the post.</p>



<ul>
<li>All the visuals are computed in real-time on the GPU, using GLSL shaders. This includes the timeline information, camera setup, etc.</li>



<li>The rendering is done with raymarching.</li>



<li>The shaders are minified using my own tool, <a href="https://github.com/laurentlb/shader_Minifier">Shader Minifier</a>.</li>



<li>The music was composed using <a href="https://openmpt.org/">OpenMPT</a> and the <a href="https://github.com/hzdgopher/4klang">4klang</a> synthesizer, which generates an assembly file able to replay the music. The instruments are described procedurally, while the list of notes is simply compressed.</li>



<li>The code was written in C++ using Visual Studio 2022.</li>



<li>To get started with the compiler flags and initialization, we used the <a href="https://github.com/armak/Leviathan-2.0/">Leviathan</a> framework.</li>



<li>The final output was compressed using <a href="https://github.com/runestubbe/Crinkler">Crinkler</a>.</li>
</ul>



<h2>Genesis</h2>



<p>One day, I saw a message from a former colleague, who shared a video he made a long time ago, called <a href="https://www.youtube.com/watch?v=khWXdkryBE4">Capoda</a>. I immediately loved the concept. The content is simple, yet it manages to tell a story efficiently.</p>



<p>I shared the link with some friends and thought it could be a good example of a story suitable for size coding. I was about to add this concept to the list of cool things I’ll probably never do, when Anatole replied:</p>



<blockquote>
<p><em>Looks like a perfect fit for 8kb ! Will you do it? I have always dreamed of making a prod like this, but I’m waiting for the “good and original idea”.</em></p>
</blockquote>



<p>I was excited by the project because I wanted to do more <a href="https://en.wikipedia.org/wiki/Demoscene">demos</a> with story-telling and animations. At the same time, I was intrigued by the challenges of making something in 8kB: I usually <a href="https://64k-scene.github.io/">target 64kB</a>, which is a completely different world, with a different set of rules and challenges. For the music, I knew I could count on my old friend CyborgJeff. Anatole had a lot of experience with 4kB. With him, the project was much more likely to be successful. And that’s how we started working together. I was not certain it would be feasible to fit everything in 8kB, but there’s only one way to know, right?</p>



<blockquote>
<p>Why specifically 8kB? In the demoscene, there are multiple size categories, with 4kB and 64kB being very common. I’ve always enjoyed the techniques used in 4kB intros (an intro is a demo with a tight size limitation), but it always feels too limited for proper storytelling. Revision, the biggest demoparty in the world, added an 8kB competition a few years ago, so it was a good opportunity to try it.</p>
</blockquote>



<h2>Rendering Worlds with Two Triangles</h2>



<p>People familiar with the demoscene already know it, as this is <a href="https://iquilezles.org/articles/nvscene2008/rwwtt.pdf">a standard approach</a> since 2008: we draw a rectangle (so, two triangles) to cover the full screen. Then, we run a GPU program (called a shader) on the rectangle using the GLSL language. The program will compute a color for each pixel and each frame. All we need is a function that takes coordinates (and time) as input, and returns a color. Simple, right?</p>



<figure><img decoding="async" src="https://lh7-us.googleusercontent.com/iQtWmNMHz0rgSV0z-HMtrOHo8vhG1GKz_DNnKAgqdyFf3p2rLQfUQC4YmTBGTAapBcxCEJ7l7-KBbYvWcjh7gsS5Z0aXvR_OOF1r9QJIAoWg5kIZe0gj67bBcwTkx2z4P7Li3ulRo6pe8Y0yOPNgGUc" alt=""><figcaption><br>“Please draw me a sheep!” — Antoine de Saint-Exupéry, The Little Prince</figcaption></figure>



<p>Of course, the big question is: how do we write a function that draws a sheep?</p>



<p>Let’s split the problem in two parts:</p>



<ol>
<li>Represent the scene as a signed distance field.</li>



<li>Use raymarching to convert the distance fields into pixels.</li>
</ol>



<h3>Signed Distance Fields (SDF)</h3>



<p>A distance field is a function that computes the distance between a point in space to the nearest object. For every point in space, we want to know how far the point is to the object. If a point is one the object’s surface, the function should return 0. This is a “signed” function, meaning that it will return a negative number for points inside the object.</p>



<p>In the most basic cases, the function is simple to write. For example, the distance between a point and a sphere is trivial to compute. For a cube, you can also do it in a couple of lines (see <a href="https://www.youtube.com/watch?v=62-pRVZuS5c">The SDF of a Box</a> for explanations). Lots of other building blocks have been documented. Inigo Quilez has a <a href="https://iquilezles.org/articles/distfunctions/">nice collection of simple reusable shapes</a> and the Mercury demo group also <a href="https://mercury.sexy/hg_sdf/">provides a great library</a>.</p>



<p>The true power of distance fields emerges when you combine them. To get the union of two objects, you can simply pick the minimum of the two distances, while the maximum gives the intersection. When crafting organic shapes, mathematical unions may appear coarse, but alternative formulas like the <a href="https://iquilezles.org/articles/smin/">smooth unions</a> can create more organic looks.</p>



<p>Once you have a toolset with building blocks, it feels like playing with legos to create an object or a scene. Thus we made our sheep by assembling simple shapes (for example a cone, a sphere) and merging them, while the body and wool consist of a 3D noise function.</p>



<p>The next task will be to actually render the distance function to the screen.</p>



<h3>Raymarching</h3>



<p>To draw the 3D scene on the screen, we use raymarching. Raymarching is a rendering technique that leverages SDFs to trace rays through a 3D scene. Unlike traditional ray tracing, which mathematically computes the intersection point, raymarching works by marching along the ray’s path. The SDF tells us how far we can walk without colliding with the scene. We can then move the ray origin and compute the new distance. We repeat the operation until the distance is 0 (or close to it).</p>



<p>In the image below, imagine that the ray origin is the camera. It is looking in a certain direction. After multiple iterations, we find the intersection point with the right wall.</p>



<figure><img decoding="async" src="https://lh7-us.googleusercontent.com/iaS66-fRMghoTXo-4spMNYhZl4iXq8mtt1lnNlSojpy_Ut-KCLIEZd5tiMBcBU7yp5ySH47Z_HJhJ2rH9N8sR8fydb43aa5J0hSrPVNZCLCjL0kqQoTZydu3ovgq91QbNtUS8FnGCjUfIt7Usfn1QKU" alt=""><figcaption>For more information, see <a href="https://en.wikipedia.org/wiki/Ray_marching#Sphere_tracing">https://en.wikipedia.org/wiki/Ray_marching#Sphere_tracing</a>.</figcaption></figure>



<p>Once we have the intersection point, we know if the current pixel should be a part of the sheep, the sky, or any other object. For the lighting, we need to know at least the surface normal, which we can <a href="https://iquilezles.org/articles/normalsSDF/">estimate by computing the gradient in the same area</a>. To compute shadows, we can send another ray for the point to the sun and find if an object is in between. Many techniques exist to improve the rendering quality beyond this basic idea.</p>



<p>To better understand the technical details, the tutorial “<a href="https://jamie-wong.com/2016/07/15/ray-marching-signed-distance-functions/">Ray Marching and Signed Distance Functions</a>” is a good start. If you have more time, the video <a href="https://www.youtube.com/watch?v=-pdSjBPH3zM">Live Coding “Greek Temple”</a> is a wonderful in-depth example with detailed explanations.</p>



<figure><img decoding="async" src="https://lh7-us.googleusercontent.com/5a2r0IZxVgs4yLDfSoqmE56uIf0RIKD9Zubweu_PCWaPBgF9JdfopuYMYhhvUI1F6M0Weorsh42mnRF_GdDPTD3qqwLdN0wbJN6pAtC8Maj94oLBbS9u4e6wj-Xgn8ggP5PkAn20MuPIwctLyggvIZs" alt=""><figcaption>Early rendering</figcaption></figure>



<h2>Direction</h2>



<p>Imagine that you have to tell a story, but there is only one character, with no voice or written text, and that character is barely animated. It can only walk (but not turn!), move the head and the eyes. With only this, how would you tell a story and convey emotions?</p>



<p>When creating a demo with such a small size limit, it’s critical to know what is important. We avoided anything that didn’t serve that story. For example, we originally imagined the sheep walking in a desert. It wouldn’t be hard to generate dunes and a sky, but the story didn’t actually need it. We decided to keep the pure white background. We also skipped the textures, except for a few that convey a meaning (like the signs and the eyes).</p>



<p>Keeping the scope of the work relatively small allowed us to focus on things that might not be obvious at first: the details and the polish, the camera work, the editing, and the synchronization. Each shot was manually crafted, each animation was adjusted and went through many iterations to make sure the flow feels right. Not only did we need to make sure the flow felt right (adjusting the length of the shots and the sequencing), but it had to match the music.</p>



<p>To ensure that the narrative resonates with the audience, we employed a number of storytelling techniques, often with redundancy for emphasis. For example, to show the sheep’s excitement, we used cartoonish 2D effects, accelerated the walking pace, accentuated tail movements, exaggerated head swings, and added a dramatic shift in the music. This approach allowed us to convey a spectrum of emotions throughout the story.</p>



<p>The camera itself is also a storytelling tool. We used a wide shot to evoke a sense of solitude when the sheep wanders for hours; an extreme closeup on its eyes to capture the moment of realization when it notices the sign; and a slow zoom on the head to intensify the focus when the sheep stares at a sign.</p>



<figure><img decoding="async" src="https://lh7-us.googleusercontent.com/_jompACpAOizuHB2jEm7xIxqBx42TzshyggyYVHMv-lh4uqNDRZfsCcXcbmHyrqVP2md7scB-HLOwym-TZUJ2BpPwiqf4gKo3eDsF5ZGa2C2Kbvcu5LYkYk8MJakX8Ln2S92EaDHSco7EoBKbF0frxo" alt=""><figcaption>A 2D animated background was used to communicate the appeal of the flower.</figcaption></figure>







<h2>Development process</h2>



<p>You may notice that the source code contains a large number of hard-coded constants. The challenge was to determine these values — how large should the sheep’s eyes be? How fast should the camera move? How long should each shot last? What colors should the flower have?</p>



<p>Given this uncertainty, each constant went through numerous iterations. To iterate quickly and have a short feedback loop, shaders come in handy: we recompile them at runtime to update the graphics within a second.</p>



<p>The other requirement was to have a player: something that allows us to control the time with pause and replay commands. This proved to be invaluable when working on animations and camera control, as we could see the result instantly after a live-reload of the shader. Additionally, music support was a must, so that we can perfectly synchronize the animations to the music.<br>We designed our early prototypes in <a href="https://www.shadertoy.com/">Shadertoy</a>, but we later transitioned to <a href="https://hexler.net/kodelife">KodeLife</a>, and finally moved the code to our own project (in C++, using a small framework called <a href="https://github.com/armak/Leviathan-2.0/">Leviathan</a>).</p>



<h2>Music</h2>



<p>Music is a critical component for storytelling. To fit the story, the music needed multiple parts, with different moods and transitions at specific points in time. We decided to use the same tools as we would in a 4kB intro, while allocating more space to allow a more intricate composition.</p>



<p>To compose the music, I asked my Cyborgjeff, a friend familiar with demoscene techniques, whose musical style resonated with our vision. His tool of choice was the <a href="https://github.com/hzdgopher/4klang">4klang synthesizer</a>, an impressive piece of software developed by Gopher. 4klang comes with a plugin usable from any music software and it has an export button that generates an assembly file. This file is then compiled and linked with the demo. When the demo runs, the synth will run in a separate thread, procedurally generate the wave sound, and send it to the soundcard.</p>



<p>Creating a small music comes with a lot of constraints. The first version of the music was bigger than expected. Imagine telling a musician “Your music is too big, can you reduce it by 500 bytes?” Well, that’s just normal in the demoscene.</p>



<p>We studied the output of 4klang to understand how it packs the data. With advice from Gopher, Cyborgjeff was able to iterate on the music to make it smaller. We made multiple adjustments:</p>



<ul>
<li>The number of instruments was reduced from 16 to 13.</li>



<li>The ending theme was recomposed to align with the tempo of the overall music.</li>



<li>The composition used more repetitions so it compresses better. For example, adjusting the length of a note in the background may be imperceptible to the ear while improving the compression ratio.</li>
</ul>



<p>This allowed us to save some space on the music, while keeping its overall structure, with a minimal loss in terms of quality. For more information on the music, Cyborgjeff wrote a blog post: <a href="https://www.studio-quena.be/cyborgjeff/blog/2023/04/11/8000-octets-un-mouton-et-une-fleur/">8000 octets, un mouton et une fleur</a>.</p>



<h2>Animation &amp; Synchronization</h2>



<p>Everything in the demo is re-evaluated at each frame as nothing is precomputed or cached. While this is really bad for the performance, this is a blessing for animations: anything can depend on the time and vary through the demo.</p>



<p>The demo consists of around 25 manually crafted camera shots. When creating a shot, we describe how each of <a href="https://github.com/ctrl-alt-test/mouton/blob/main/Intro/src/shaders/mouton.vert#L5-L22">18 parameters</a> varies over time, such as the position of each object, the state of the sheep, the position of the camera, the focal, what the camera is looking at, etc.</p>



<p>For example, a single line of code describes the camera during the shot:</p>



<pre><code>camPos = vec3(22., 2., time*0.6-10.);</code></pre>



<p>And we get a linear translation. Here “time” represents the time since the beginning of the shot so that we can easily insert, remove or adjust a shot without affecting the rest of the demo. Absolute times are avoided for code maintenance.</p>



<p>A note of caution on linear interpolations: while functional, they often look poor or robotic. In many cases, we apply the <a href="https://en.wikipedia.org/wiki/Smoothstep">smoothstep</a> function, which leads to a smoother, more natural animation. Smoothstep is an S-shaped smooth interpolation that helps avoid sharp edges and sudden movement changes.<br>The code for the timeline is <a href="https://github.com/ctrl-alt-test/mouton/blob/main/Intro/src/shaders/mouton.vert#L53">in the vertex shader</a>. You may notice that each shot is defined in a similar way, and the code may seem redundant. It’s not a problem, as redundancy leads to better compression.</p>



<h2>Textures &amp; Materials</h2>



<p>In a traditional renderer, textures are 2D images that are applied on a 3D model. One difficulty with the textures is how to compute the texture coordinates and map each pixel in the texture to the 3D surface. With our raymarching approach, we can’t easily compute texture coordinates. Instead, we compute on the fly 3D textures. Once the raymarcher finds the 3D position of the point to render, we pass the 3D coordinates to the corresponding texture function.</p>


<div>
<figure><img decoding="async" src="https://lh7-us.googleusercontent.com/PMwY_GEUekURSgaZZKRVz-cVoSNUckN6wYXdq7LhZlXLabKU89zj-Rl1gC3-Ed5dxLlmrq57c0oo_bmbCI_kJP_fCPyBZ3mUrWpWSfoLJy5ZuKEcs6gUiVFmhaI6S6K7pebMJm5RRqfc6YLWxtVGFOc" alt=""></figure></div>


<p>Let’s take a look at the traffic signs. They use some math to compute a triangle or a square to serve as a border, then the inner content of the sign is achieved by <a href="https://github.com/ctrl-alt-test/mouton/blob/main/Intro/src/shaders/mouton.frag#L624">combining multiple functions</a>. For example, the restaurant symbol is created with 4 black oval-like shapes, and 2 white shapes are added to create the dents.</p>



<p>To make the visuals more interesting, we wanted to have not just textures, but also different materials. Parameters of the lighting equation depend on the material. For example, the sheep hooves have a different reflection (using <a href="https://en.wikipedia.org/wiki/Fresnel_equations">fresnel coefficients</a>).</p>






<div>
<figure><img decoding="async" src="https://lh7-us.googleusercontent.com/6OnCIvQ8M6ONMZRRomTCB2pSDJnxt9zeO4Is3oXyVNMuulGoQUQ7KKMT6B-xTnMDDgh0zv93Avi9IhlRMkND7tu91mTol28XbK9AXt2N3c7Kqo4JxCfbtHWPGmM5cSmBSGQsu2__9o1NyCcwVcOQfHA" alt=""></figure></div>


<h2>Eyes</h2>



<p>For a long time during the development, the eyes looked dull and lifeless. I believe eyes are a big part of the character design. This was one of the first things that we animated, and it was very important as a tool for story-telling. Eyes help show feelings and they enable many transitions.</p>



<figure><img decoding="async" src="https://lh7-us.googleusercontent.com/QGyZLSEmC5EVpGPhOyH1W17dZBkbGqI106q_RYkEOPZsIEhp3IuSI5k1ILwOUqzjx68DswWkgaFM3geC_fsQpluilmWmGbYIGi_vfHYPsKdR8wNKBZY-2sY6ERgVMfNR6cYT83w2pxdLGqDK6EX-yDc" alt=""><figcaption>Early rendering</figcaption></figure>



<p>After searching for reference images on the Internet, I found that most of the cartoon characters have an iris, but the iris doesn’t seem to be strictly required.&nbsp; I’ve also noticed that the pupil is always big. If there’s an iris, the pupil will be big compared to the iris.</p>



<p>But the important part to get sparkling eyes is to have some reflections of the light in the eyes. With the standard lighting equations we used, we couldn’t get a reflection (unless the sun and the camera were at very specific positions). An input of the lighting equations is the normal vector of the surface. Our trick was to modify the vector to increase the probability of getting some reflections from the sun.</p>



<p>On top of that, we created an <a href="https://en.wikipedia.org/wiki/Reflection_mapping">environment mapping</a>. This technique is commonly used in video games: instead of computing perfect reflections in the scene in real time, we can look up in a texture (that simulates the environment). Usually, people use environment mapping to optimize the code as the texture can be a simplification of the real environment. We did the opposite: our environment world is actually a perfect white, but we used a texture to fake and add details.</p>



<figure><img decoding="async" src="https://lh7-us.googleusercontent.com/beQPbrggllClDp6K168goj9fwyz_WQKed1Gc2cZBWC1lZfNCPj904WZ1xrMRg2BZayoZIGDGdrOD63gcfuci6c-nNATQ-wIiS4VdyD5dmbJkF-CQy_6gc4JMIPpdBmpdyzW_QD8UK2Au0YzKdnOH8Hk" alt=""><figcaption>Final result</figcaption></figure>



<p>The reflections in the eyes (both the white and the pupils) are much more complex than they should be in an empty world. There are multiple fake sources of lights, as well as a gradient (to mimic a darker ground and a blue-ish sky).</p>



<h2>Post-processing</h2>



<p>Once everything is done, the final visual touch defining the mood is the post-processing.</p>



<p>Despite being subtle, it helps have a good image quality and set the tone for the story.&nbsp; We used:</p>



<ul>
<li>color grading;</li>



<li>gamma correction;</li>



<li>a bit of vignetting;</li>



<li>finally, a two-pass FXAA filter to avoid aliasing (but you won’t notice it if you only watched the YouTube capture)</li>
</ul>



<p>We also implemented some of the effects in the post-processing step, such as the stars in the eyes or the ending screen effect. Those are made as pure 2D and don’t exist in the 3D world.</p>



<p>Finally, we experimented with other alternative styles. At some point, we tried to give an old-cartoon look, and we implemented contour detection to simulate a hand-drawn image, with a monochrome rendering, grain, and noise. The result looked like this:</p>



<figure><img decoding="async" src="https://lh7-us.googleusercontent.com/l8g7w28ukmvnB9lO6UauAk76tzbtbgv_uwRjEiAouIcA4KDgwCeVYhTeVeAcsjSXK7FN6kJVtA7r3Ka7dPMJGeY3zB4Xvg4z1WGnPovluXJZ9D9iC1QLW5mp8DXywamC2zHaJv_T_RAnILcbDoFXRbg" alt=""></figure>



<figure><img decoding="async" src="https://lh7-us.googleusercontent.com/bG6HYJcungIAKm9LFHBNJCIYlCtGCL1zI2pOLX2Hfc7IP0mUOst90cRbXCZQABuAUXKyxZrpbSILNHwzMIaUoqXOGISLV-rafoX2SMHDrN0se6gCpouoR7JFuctXRo66zRr-LvK1nZP--NqESc9OKIQ" alt=""></figure>



<p>After discussions, we decided to give up on this experiment and focus on the cleaner, modern look.</p>



<h2>Compression</h2>



<p>So far, we’ve seen how we made most of the demo. The main idea is to avoid storing data, instead we use code to describe how to generate the data. For the music, we store a list of notes to play, as well as a list of instructions for each instrument. All of this is relatively small, but does it really fit in 8kB?</p>



<p>Part of the magic comes from <a href="https://github.com/runestubbe/Crinkler">Crinkler</a>, a compression tool specifically designed for the demoscene and intros between 1kB and 8kB. As the executable needs to be self-extractible, Crinkler includes some very small and clever assembly code that can decompress the rest of the executable. It’s optimized for size at the expense of other things: the compression algorithm takes a while, the decompression is relatively slow and uses a lot of RAM (hundreds of megabytes).</p>



<p>Crinkler is an impressive tool, but it doesn’t do everything. We have a total of <a href="https://github.com/ctrl-alt-test/mouton/tree/main/Intro/src/shaders">42kB of shader source code</a> and we need one last ingredient to fit it in the binary.</p>



<h2>Minification</h2>



<p>As the shader source code is included in the final binary, we have to make it as small as possible. It would be possible to minify the code by hand, but then it would cause maintenance issues. It was critical for the success of our project to be able to iterate quickly, without worrying about low-level size optimizations. So we needed a tool to minify the shader.</p>



<p>I have written such a tool, <a href="https://github.com/laurentlb/Shader_Minifier/">Shader Minifier</a>, which has been a side project of mine since 2011. It removes unneeded spaces and comments, it renames variables and can do more. It has been the most popular shader minification tool in the demoscene for many years, but this was not sufficient for us: an 8kB intro contains much more code than a 4kB does, and new problems arise from the larger code size.</p>



<p>I stopped working on the demo for one month to implement all the missing features we needed from Shader Minifier. While it’s easy to write a simple minifier, you need to write a compiler if you want to get good minification — a source-to-source compiler, similar to what Closure Compiler does.</p>



<p>The full <a href="https://github.com/laurentlb/Shader_Minifier/?tab=readme-ov-file#transformations">list of transformations</a> supported by Shader Minifier is getting quite long, but here are a few:</p>



<ul>
<li>Renaming of variables and functions</li>



<li>Inlining of variables</li>



<li>Evaluation of constant arithmetic</li>



<li>Inlining of functions</li>



<li>Dead code elimination</li>



<li>Merging of declarations</li>
</ul>



<p>These transformations reduce the size of the output, but it’s not enough. We need to make sure that the output is compression-friendly. This is a hard problem: it can happen that a transformation reduces the size of the code, while increasing the size of the compressed code. So we constantly need to monitor the compressed size, as we iterate on the demo.</p>



<p>New improvements to Shader Minifier have saved around 600 bytes on the compressed binary. To help review and monitor what exactly goes in the final binary, we store <a href="https://github.com/ctrl-alt-test/mouton/blob/main/Intro/src/shaders/shaders.inl">the minified output</a> in the repository. This proved useful to identify new optimization opportunities. At the end, once minified and compressed, the 42kB of shader code fits in about 5kB, which gives just enough space for the music and the C++ code.</p>



<p>Anyway… you could say that I had to write a compiler in order to make this demo. :)</p>



<h2>The War Between the Sheep and the Flowers</h2>



<blockquote>
<p>It’s not important, the war between the sheep and the flowers? […] Suppose I happen to know a unique flower, one that exists nowhere in the world except on my planet, one that a little sheep can wipe out in a single bite one morning, just like that, without even realizing what he’d doing – that isn’t important? If someone loves a flower of which just one example exists among all the millions and millions of stars, that’s enough to make him happy when he looks at the stars. He tells himself ‘My flower’s up there somewhere…’ But <strong>if the sheep eats the flower, then for him it’s as if, suddenly, all the stars went out</strong>. And that isn’t important?”</p>



<p>— Antoine de Saint-Exupéry, The Little Prince</p>
</blockquote>



<h2>Conclusion</h2>



<p>As you’ve seen, there are lots of advanced and fascinating techniques needed to make this kind of demo. But we didn’t invent everything. We’re building on top of what other people did. The amount of work and research that was done by other people is incredible, from the raymarching techniques, to the music generation software and the compression algorithms. Hopefully the new features added to Shader Minifier will help other people create better demos in the future. The 8kB category is fun and offers more possibilities than the 4kB category; let’s hope it will become more popular.</p>



<p>P.S. For comparison, the text of this article contains around 21,000 characters, so it would take 21kB.</p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Train robbery for Amazon packages? More common than you think (212 pts)]]></title>
            <link>https://www.nytimes.com/2024/01/23/magazine/train-robbery-amazon-packages.html</link>
            <guid>39120773</guid>
            <pubDate>Wed, 24 Jan 2024 18:22:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/01/23/magazine/train-robbery-amazon-packages.html">https://www.nytimes.com/2024/01/23/magazine/train-robbery-amazon-packages.html</a>, See on <a href="https://news.ycombinator.com/item?id=39120773">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/01/23/magazine/train-robbery-amazon-packages.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Startup funding simulator (535 pts)]]></title>
            <link>https://www.fundingsimulator.com/</link>
            <guid>39120647</guid>
            <pubDate>Wed, 24 Jan 2024 18:11:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fundingsimulator.com/">https://www.fundingsimulator.com/</a>, See on <a href="https://news.ycombinator.com/item?id=39120647">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Zed, a collaborative code editor, is now open source (1275 pts)]]></title>
            <link>https://zed.dev/blog/zed-is-now-open-source</link>
            <guid>39119835</guid>
            <pubDate>Wed, 24 Jan 2024 17:15:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zed.dev/blog/zed-is-now-open-source">https://zed.dev/blog/zed-is-now-open-source</a>, See on <a href="https://news.ycombinator.com/item?id=39119835">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><header></header><div><figure><img src="https://zed.dev/img/post/zed-is-now-open-source/open-source-F1.png" alt=""></figure></div>
<p>We're excited to announce that Zed is now an open source project. The code for Zed itself will be made available under a copyleft license to ensure any improvements will benefit the entire community (GPL for the editor, AGPL for server-side components). GPUI, the UI framework that powers Zed, will be distributed under the Apache 2 license, so that you can use it to build high-performance desktop applications and distribute them under any license you choose.</p>
<h2 id="motivations"><span data-br=":Rlquuuujf9la:" data-brr="1">Motivations</span></h2>
<p>Why are we doing this? Most importantly, we believe that making Zed open source will make it the best product. Our mission is to build the world's most advanced code editor and get it into the hands of millions of developers. There's a ton of surface area on that problem, and we'll need all the help we can get. Considering that all of Zed's users are programmers, it makes the most sense sense to open Zed to the maximum pool of talent.</p>
<p>On a more selfish level, we also think going open source will be a lot more fun. One of our favorite aspects of software is connecting with people. We're not only proud of <em>what</em> we've built, but also of <em>how</em> we've built it. We want to share Zed's inner beauty with all of you, and we're confident we'll learn a lot from you in the process to make it even better.</p>
<h2 id="introducing-fireside-hacks"><span data-br=":R1dquuuujf9la:" data-brr="1">Introducing <em>Fireside Hacks</em></span></h2>
<p>Motivated by our desire to connect with you all on a human level, we're launching a new Zed feature called Zed Channels, which will make it easy for developers anywhere in the world to write code together in real time just by sharing a link.</p>
<p>Starting tomorrow, we'll be using Channels to run a new program called <em>Fireside Hacks</em>, in which we'll be streaming into a public channel regularly we work on Zed live with whoever shows up. We'll be experimenting with different formats, but we're hoping these regular sessions give us all an opportunity to get to know each other better, beyond what's possible in a static pull request. Come join the experiment to ask questions, make suggestions, and code with us in real time. We're looking forward to meeting you!</p>
<h2 id="wait-doesnt-cash-rule-everything-around-me"><span data-br=":R25quuuujf9la:" data-brr="1">Wait... Doesn't Cash Rule Everything Around Me?</span></h2>
<p>We strongly believe that the best way to build and maintain the world's best editor is by associating it with a sustainable business model. It's the only way we can continue to invest in a full-time team to spearhead development. Some may wonder whether making Zed open source undermines this objective. We've thought about this a lot, and we don't think that openness is at odds with commercial success.</p>
<p>Rather than selling you a proprietary editor, we'd much prefer to sell you services that seamlessly integrate with your editor to make you and your team more productive. Zed Channels is one example of such a service. It's free for anyone today, but we intend to begin charging for private use after a beta period of experimentation. Providing server-side compute to power AI features is another monetization scheme we're seeing getting traction.</p>
<p>Today, we're open sourcing 100% of the code we've written so far. In the future, however, we may still offer proprietary products targeting commercial and enterprise use cases, though we always intend for proprietary code to be a tiny fraction compared to the code we open source. We also intend to ensure our need to generate revenue never interferes with your need to write software. We're never going to show you a banner ad in your code editor, and if we do, you can always build Zed from source.</p>
<p>We believe in the principle of creating more value than we capture. Going open source is a bet that if we can build a huge movement around Zed, our company will find opportunities to capture some of the value we create.</p>
<h2 id="the-road-to-10"><span data-br=":R3dquuuujf9la:" data-brr="1">The Road to 1.0</span></h2>
<p>So what's next? To some extent, that's up to you! But it's also true that we're still a small team. We want to massively grow adoption in 2024 so that more developers can benefit from Zed, and we've put together a public roadmap based on user feedback that we think can help us get there. If your contributions help us make progress toward completing that roadmap, especially items near the top, we'll be more likely to make time for them.</p>
<p>That said, we're going to figure this out as we go, and we welcome anyone who is excited to contribute and learn. If you'd like to get involved, check out our <a href="https://github.com/zed-industries/zed/blob/main/CONTRIBUTING.md">contribution guide</a>, and come say hello at an upcoming Fireside Hack if you have the time. Developers everywhere need a better code editor, and we're excited for you to join us on our mission to advance the state of the art. See you in the codebase!</p><hr></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IRS Free File is now available for the 2024 filing season (216 pts)]]></title>
            <link>https://www.irs.gov/newsroom/irs-free-file-is-now-available-for-the-2024-filing-season</link>
            <guid>39119668</guid>
            <pubDate>Wed, 24 Jan 2024 17:01:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.irs.gov/newsroom/irs-free-file-is-now-available-for-the-2024-filing-season">https://www.irs.gov/newsroom/irs-free-file-is-now-available-for-the-2024-filing-season</a>, See on <a href="https://news.ycombinator.com/item?id=39119668">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      


<article about="/newsroom/irs-free-file-is-now-available-for-the-2024-filing-season">

  
    

  
            <div><p>IRS Tax Tip 2024-03, Jan. 22, 2024</p>

<p>IRS Free File is now available for the 2024 filing season. With this program, eligible taxpayers can prepare and file their federal tax returns using free tax software from trusted IRS Free File partners.</p>

<h2>Who's eligible for IRS Free File</h2>

<p>Each IRS Free File provider sets its own eligibility rules based on age, income and state residency.</p>

<p>Taxpayers, including active-duty military, with an <a data-entity-substitution="canonical" data-entity-type="node" data-entity-uuid="244a4a3f-281f-4d1d-9fb6-ab530ca7a5e1" href="https://www.irs.gov/e-file-providers/definition-of-adjusted-gross-income" title="Definition of adjusted gross income">adjusted gross income (AGI)</a> of $79,000 or less in 2023 can likely find an offer from an IRS Free File provider that matches their needs. Some providers also offer free state tax return preparation. Those with an AGI over the limit can still file their return for free using Free File Fillable Forms.</p>

<p>To find the right IRS Free File offer, taxpayers can go to the&nbsp;<a data-entity-substitution="canonical" data-entity-type="node" data-entity-uuid="0a2fc2c3-9e78-4514-b1dc-6959b2f5e891" href="https://www.irs.gov/filing/free-file-do-your-federal-taxes-for-free" title="Free File: Do your taxes for free">IRS Free File</a> webpage.</p>

<h2>IRS Free File is safe and secure</h2>

<p>Taxpayer information is protected from unauthorized access while it's sent to the IRS. IRS Free File partner companies can't share or use tax return information for purposes other than tax return preparation without the taxpayer's permission.</p>

<h2>No computer? No problem.</h2>

<p>IRS Free File products support mobile phone access. Taxpayers can do their taxes on their smartphone or tablet.</p>

<h2>IRS Free File participants</h2>

<p>These tax providers are participating in IRS Free File in 2024:</p>

<ul><li>1040Now</li>
	<li>Drake (1040.com)</li>
	<li>ezTaxReturn.com</li>
	<li>FileYourTaxes.com</li>
	<li>On-Line Taxes</li>
	<li>TaxAct</li>
	<li>TaxHawk (FreeTaxUSA)</li>
	<li>TaxSlayer</li>
</ul><p><a data-entity-substitution="canonical" data-entity-type="node" data-entity-uuid="f787f7b3-e72c-4d58-81b2-e47a9a771565" href="https://www.irs.gov/newsroom/subscribe-to-irs-tax-tips" title="Subscribe to IRS Tax Tips">Subscribe to IRS Tax Tips</a></p>
</div>
      
</article>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Good DevEx increases productivity. Here is the data (223 pts)]]></title>
            <link>https://github.blog/2024-01-23-good-devex-increases-productivity/</link>
            <guid>39119141</guid>
            <pubDate>Wed, 24 Jan 2024 16:18:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.blog/2024-01-23-good-devex-increases-productivity/">https://github.blog/2024-01-23-good-devex-increases-productivity/</a>, See on <a href="https://news.ycombinator.com/item?id=39119141">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
  
<p>The wait is over: we finally have data to back up the benefits of developer experience (DevEx).</p>
<p>We’ve always known that providing a good DevEx is a smart business move, as it enables developers to solve complex tasks, collaborate with peers, and unleash their creativity. We hear this from customers all the time.</p>
<p>But despite the benefits, DevEx conversations usually end when execs ask for hard data on what their business will gain by improving it. While we have anecdotal evidence, we haven’t had the data to satisfy this question—until now. New research we just published uses statistical analysis to pinpoint how key DevEx factors—<a href="https://github.blog/2024-01-22-how-to-get-in-the-flow-while-coding-and-why-its-important">flow state</a>, cognitive load, and feedback loops—impact individual, team, and organizational outcomes by improving productivity and innovation.</p>
<p>To conduct this research, we partnered with <a href="https://getdx.com/">DX</a>, which helps teams measure developer experience, to leverage their insights and expertise. Using work design theory, we created a set of research hypotheses, developed questions, and surveyed more than 20 industry-diverse companies. We ran rigorous statistical analyses to see which measures were validated and significant. We also documented our survey questions so others could reuse our work and provided step-by-step guidance on advocating for DevEx and measuring the results.</p>
<p>The result is the following body of evidence-based outcomes and advice—detailed in the summary below and a new report—that business leaders everywhere can use to make the case for investing in DevEx.</p>
<p>Dr. Eirini Kalliamvakou<br>
<em>Staff researcher at GitHub, co-author of the study</em></p>
<p><img decoding="async" src="https://github.blog/wp-content/uploads/2024/01/devex-making-an-impact.png?w=1024&amp;resize=1024%2C576" alt="Graph showing what businesses get with better DevEx: by blocking time for deep work, they get 50% more productivity; by creating intuitive processes, they get 50% more innovation; and by enabling fast code reviews, they get 20% more innovation." width="1024" height="576" loading="lazy" srcset="https://github.blog/wp-content/uploads/2024/01/devex-making-an-impact.png?w=1920 1920w, https://github.blog/wp-content/uploads/2024/01/devex-making-an-impact.png?w=300 300w, https://github.blog/wp-content/uploads/2024/01/devex-making-an-impact.png?w=768 768w, https://github.blog/wp-content/uploads/2024/01/devex-making-an-impact.png?w=1024&amp;resize=1024%2C576 1024w, https://github.blog/wp-content/uploads/2024/01/devex-making-an-impact.png?w=1536 1536w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"></p>
<h2 id="boosting-flow-state">Boosting flow state<a href="#boosting-flow-state" aria-label="Boosting flow state"></a></h2>
<p>The <a href="https://queue.acm.org/detail.cfm?id=3639443">research</a> is clear: <strong>developers who carve out significant time for deep work enjoy a 50% productivity boost</strong>. Minimizing distractions, which can be everything from Slack messages to meetings to peers asking for help is paramount for high-value work. Granted, it’s not always easy to reserve blocks of time, especially in distributed teams across multiple time zones. However, providing your developers an atmosphere where they can maximize their flow state can pay high dividends.</p>
<p>“To optimize building code, you need the right environment,” says Dr. Eirini Kalliamvakou, staff researcher at GitHub and co-author of the study. “Implementing practices that enable your developers to enter and stay in the flow is a winning move.”</p>
<p>Adobe is an example of a company that recognizes the value of providing an effective working environment for developers. CJ Dotson, senior PM of developer productivity at Adobe, notes that “when technology is what you sell, investments in DevEx are not optional. Adobe’s investment in DevEx leads to higher developer satisfaction and better business outcomes.”</p>
<p>Additionally, <strong>developers who find their work engaging feel 30% more productive</strong>, according to the DevEx study. This stat should help your organization rethink the distribution of tasks. Do you have the same developers working on less desirable projects that could lead to burnout? Are your teams regularly engaging in tasks they find boring or divorced from the company’s mission? If you want to optimize their work, ensure that your teams are excited about the projects on their plate, at least most of the time.</p>
<p>“Providing deep work and exciting, engaging projects are some of the biggest things companies can do to improve productivity,” says Dr. Nicole Forsgren, partner researcher at Microsoft and co-author of the study.</p>
<h2 id="reducing-cognitive-load">Reducing cognitive load<a href="#reducing-cognitive-load" aria-label="Reducing cognitive load"></a></h2>
<p>It’s a familiar pattern: <strong>developers who report a high degree of understanding of their code feel 42% more productive than those with low or no understanding</strong>. Low understanding can come from various factors, including poor or outdated documentation, lack of onboarding, or the sheer pace of innovation with AI.</p>
<p>“Every developer has experienced the frustration of not understanding their code or its surrounding context well,” Forsgren says. “Because so much of our code is interconnected and developed by multiple people, understandability is part of why having a good DevEx is so important.”</p>
<p>Kalliamvakou also notes that this is where good tooling comes in: “Certain technologies like <a href="https://github.com/features/copilot">GitHub Copilot</a> can help developers better understand their code and future-proof their productivity.”</p>
<p>Additionally, it should be no surprise that intuitive, easy processes can boost innovation, while cumbersome processes can sink time and create frustration. <strong>Our research shows that developers who have intuitive processes feel they are 50% more innovative.</strong></p>
<p>“And it’s not always about the technology,” Forsgren adds. “If you can find ways to remove friction and blockers for developers, you’ll unlock so many things.”</p>
<p>Dermot Russell, director of engineering at Etsy, agrees: “Etsy’s enablement initiatives have improved developers’ day-to-day experience while also enabling rapid software delivery as our organization has grown.”</p>
<h2 id="improving-feedback-loops">Improving feedback loops<a href="#improving-feedback-loops" aria-label="Improving feedback loops"></a></h2>
<p>In the world of development, efficient feedback loops are critical. According to the research, <strong>developers who report fast code turnaround times feel 20% more innovative than developers who don’t.</strong> “Getting fast feedback allows you to move along quickly while maintaining your curiosity and drive,” Kalliamvakou says. “It allows developers to stay in the flow and create the next great thing.”</p>
<p>Focusing on improving feedback loops can benefit the organization’s overall effectiveness and developer satisfaction. For example, UKG’s VP of developer acceleration, Thomas Newton, says, “it’s a virtuous cycle: by reducing friction and waste from developers’ daily work, developers are able to ship high-quality software faster, while also improving happiness and engagement.”</p>
<p>There’s another benefit to quick feedback loops: <strong>teams that provide faster responses to developers’ questions report 50% less technical debt.</strong> In other words, good documentation pays off. Documenting common developer questions and putting tooling in place that enables them to easily find the responses they need, allows developers to increase their agility.</p>
<p>At the end of the day, nimble developers beget nimble teams and organizations.</p>
<p>“So often, if you’re a developer, you have to wait for feedback,” Forsgren adds. “You get interrupted. You’re constantly stalled. You have to figure out a cumbersome process. But if, instead, you can collaborate quickly, have no interruptions, use intuitive technologies, and stay in the flow—that’s when you can problem-solve, be creative, and get work done, which will benefit the team and entire organization.”</p>
<h2 id="the-path-forward">The path forward<a href="#the-path-forward" aria-label="The path forward"></a></h2>
<p>Software creation is critical for innovation. Across all industries, companies need to build and maintain high-quality software to meet their goals. As such, investing in DevEx is a must.</p>
<p>“If you’re a business leader who’s focused on being profitable and innovative, enabling a good DevEx is one of the key levers at your disposal,” Kalliamvakou says. “The research we published finally gives us the solid data and evidence we need to make the case to the larger community.”</p>


      
  </section><div><h2>Interested in bringing <a href="https://github.com/enterprise" target="_blank" rel="noopener">GitHub&nbsp;Enterprise</a> to your&nbsp;organization?</h2>
<p>Start your <a href="https://github.com/enterprise/trial?ref_cta=free%2520trial&amp;ref_loc=cta&amp;ref_page=blog" target="_blank" rel="noopener">free&nbsp;trial</a> for 30 days and increase your team’s collaboration. $21 per user/month after trial expires.</p>
<p>Curious about <a href="https://github.com/pricing" target="_blank" rel="noopener">other&nbsp;plans?</a></p>
</div><section>
    <h2>
      Explore more from GitHub    </h2>
    <div>
      <div>
          <p><img src="https://github.blog/wp-content/uploads/2022/05/enterprise.svg" width="44" height="44" loading="lazy" alt="Enterprise"></p><h3>Enterprise</h3>
    <p>
      How to deliver great software—at scale.    </p>

          <p>
        <a data-analytics-click="Blog, click on module, text: Learn more; ref_location:bottom recirculation;" href="https://github.blog/category/enterprise/" aria-label="Learn more about: Enterprise">
          Learn more<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16" fill="none"><path fill="currentColor" d="M7.28033 3.21967C6.98744 2.92678 6.51256 2.92678 6.21967 3.21967C5.92678 3.51256 5.92678 3.98744 6.21967 4.28033L7.28033 3.21967ZM11 8L11.5303 8.53033C11.8232 8.23744 11.8232 7.76256 11.5303 7.46967L11 8ZM6.21967 11.7197C5.92678 12.0126 5.92678 12.4874 6.21967 12.7803C6.51256 13.0732 6.98744 13.0732 7.28033 12.7803L6.21967 11.7197ZM6.21967 4.28033L10.4697 8.53033L11.5303 7.46967L7.28033 3.21967L6.21967 4.28033ZM10.4697 7.46967L6.21967 11.7197L7.28033 12.7803L11.5303 8.53033L10.4697 7.46967Z"></path><path stroke="currentColor" d="M1.75 8H11" stroke-width="1.5" stroke-linecap="round"></path></svg>
                  </a>
      </p>
      </div>
<div>
          <p><img src="https://github.blog/wp-content/uploads/2022/05/readme.svg" width="44" height="44" loading="lazy" alt="The ReadME Project"></p><h3>The ReadME Project</h3>
    <p>
      Stories and voices from the developer community.    </p>

          <p>
        <a data-analytics-click="Blog, click on module, text: Learn more; ref_location:bottom recirculation;" href="https://github.com/readme" target="_blank" aria-label="Learn more about: The ReadME Project">
          Learn more<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>
                  </a>
      </p>
      </div>
<div>
          <p><img src="https://github.blog/wp-content/uploads/2022/05/Copilot_Blog_Icon-1.svg" width="44" height="44" loading="lazy" alt="GitHub Copilot"></p><h3>GitHub Copilot</h3>
    <p>
      Don't fly solo. Try 30 days for free.    </p>

          <p>
        <a data-analytics-click="Blog, click on module, text: Learn more; ref_location:bottom recirculation;" href="https://github.com/features/copilot?utm_source=blog&amp;utm_medium=bottomnav&amp;utm_campaign=cta&amp;utm_content=copilot" target="_blank" aria-label="Learn more about: GitHub Copilot">
          Learn more<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>
                  </a>
      </p>
      </div>
<div>
          <p><img src="https://github.blog/wp-content/uploads/2022/05/careers.svg" width="44" height="44" loading="lazy" alt="Work at GitHub!"></p><h3>Work at GitHub!</h3>
    <p><span>Check out our current job openings.</span>    </p>

          <p>
        <a data-analytics-click="Blog, click on module, text: Learn more; ref_location:bottom recirculation;" href="https://github.com/about/careers" target="_blank" aria-label="Learn more about: Work at GitHub!">
          Learn more<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>
                  </a>
      </p>
      </div>
    </div>
  </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI-Powered Nvidia RTX Video HDR Transforms Standard Video into HDR Video (104 pts)]]></title>
            <link>https://blogs.nvidia.com/blog/rtx-video-hdr-remix-studio-driver/</link>
            <guid>39118963</guid>
            <pubDate>Wed, 24 Jan 2024 16:04:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blogs.nvidia.com/blog/rtx-video-hdr-remix-studio-driver/">https://blogs.nvidia.com/blog/rtx-video-hdr-remix-studio-driver/</a>, See on <a href="https://news.ycombinator.com/item?id=39118963">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="bsf_rt_marker"><p><i>Editor’s note: This post is part of our weekly </i><a href="https://blogs.nvidia.com/blog/tag/in-the-nvidia-studio/"><i>In the NVIDIA Studio</i></a><i> series, which celebrates featured artists, offers creative tips and tricks, and demonstrates how </i><a href="https://www.nvidia.com/en-us/studio/"><i>NVIDIA Studio</i></a><i> technology improves creative workflows. We’re also deep diving on new </i><a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/"><i>GeForce RTX 40 Series GPU</i></a><i> features, technologies and resources, and how they dramatically accelerate content creation.</i></p>
<p>RTX Video HDR — first announced at CES — is now available for download through the January Studio Driver. It uses AI to transform standard dynamic range video playing in internet browsers into stunning high dynamic range (HDR) on HDR10 displays.</p>
<p>PC game modders now have a powerful new set of tools to use with the release of the <a href="https://www.nvidia.com/en-us/geforce/rtx-remix/">NVIDIA RTX Remix</a> open beta.</p>
<p>It features full ray tracing, <a href="https://www.nvidia.com/en-us/geforce/technologies/dlss/">NVIDIA DLSS</a>, <a href="https://www.nvidia.com/en-us/geforce/technologies/reflex/">NVIDIA Reflex</a>, modern physically based rendering assets and <a href="https://nvidianews.nvidia.com/news/generative-ai-rtx-pcs-and-workstations">generative AI texture tools</a> so modders can remaster games more efficiently than ever.</p>
<p>Pick up the new <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4070-family/">GeForce RTX 4070 Ti SUPER</a> available from custom board partners in stock-clocked and factory-overclocked configurations to enhance creating, gaming and AI tasks.</p>
<figure id="attachment_69440" aria-describedby="caption-attachment-69440"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/01/geforce-rtx-4070-ti-super-out-now-social-post-1280x720-1.png"><picture fetchpriority="high" decoding="async">
<source type="image/webp" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/01/geforce-rtx-4070-ti-super-out-now-social-post-1280x720-1-672x378.png.webp 672w, https://blogs.nvidia.com/wp-content/uploads/2024/01/geforce-rtx-4070-ti-super-out-now-social-post-1280x720-1-400x225.png.webp 400w, https://blogs.nvidia.com/wp-content/uploads/2024/01/geforce-rtx-4070-ti-super-out-now-social-post-1280x720-1-768x432.png.webp 768w, https://blogs.nvidia.com/wp-content/uploads/2024/01/geforce-rtx-4070-ti-super-out-now-social-post-1280x720-1-800x450.png.webp 800w, https://blogs.nvidia.com/wp-content/uploads/2024/01/geforce-rtx-4070-ti-super-out-now-social-post-1280x720-1-382x215.png.webp 382w, https://blogs.nvidia.com/wp-content/uploads/2024/01/geforce-rtx-4070-ti-super-out-now-social-post-1280x720-1-178x100.png.webp 178w, https://blogs.nvidia.com/wp-content/uploads/2024/01/geforce-rtx-4070-ti-super-out-now-social-post-1280x720-1.png.webp 1280w" sizes="(max-width: 672px) 100vw, 672px">
<img fetchpriority="high" decoding="async" src="https://blogs.nvidia.com/wp-content/uploads/2024/01/geforce-rtx-4070-ti-super-out-now-social-post-1280x720-1-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/01/geforce-rtx-4070-ti-super-out-now-social-post-1280x720-1-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/01/geforce-rtx-4070-ti-super-out-now-social-post-1280x720-1-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/01/geforce-rtx-4070-ti-super-out-now-social-post-1280x720-1-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/01/geforce-rtx-4070-ti-super-out-now-social-post-1280x720-1-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2024/01/geforce-rtx-4070-ti-super-out-now-social-post-1280x720-1-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2024/01/geforce-rtx-4070-ti-super-out-now-social-post-1280x720-1-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2024/01/geforce-rtx-4070-ti-super-out-now-social-post-1280x720-1.png 1280w" sizes="(max-width: 672px) 100vw, 672px">
</picture>
</a><figcaption id="caption-attachment-69440">Get creative superpowers with the GeForce RTX 4070 Ti SUPER available now.</figcaption></figure>
<p>Part of the <a href="https://blogs.nvidia.com/blog/studio-rtx-hdr-video-twitch-obs-istock-getty-super-laptop-desktop/">40 SUPER Series announced at CES</a>, it’s equipped with more CUDA cores than the RTX 4070, a frame buffer increased to 16GB, and a 256-bit bus — perfect for video editing and rendering large 3D scenes. It runs up to 1.6x faster than the RTX 3070 Ti and 2.5x faster with DLSS 3 in the most graphics-intensive games.</p>
<p>And this week’s featured <i>In the NVIDIA Studio</i> technical artist Vishal Ranga shares his vivid 3D scene <i>Disowned </i>— powered by <a href="https://www.nvidia.com/en-us/geforce/rtx/">NVIDIA RTX</a> and Unreal Engine with DLSS.</p>
<h2><b>RTX Video HDR Delivers Dazzling Detail</b></h2>
<p>Using the power of Tensor Cores on GeForce RTX GPUs, RTX Video HDR allows gamers and creators to maximize their HDR panel’s ability to display vivid, dynamic colors, preserving intricate details that may be inadvertently lost due to video compression.</p>
<p><iframe title="Introducing RTX Video HDR: AI-Upscale Video to HDR Quality" width="500" height="281" src="https://www.youtube.com/embed/FHAjydnpos8?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<p>RTX Video HDR and <a href="https://blogs.nvidia.com/blog/rtx-video-super-resolution/">RTX Video Super Resolution</a> can be used together to produce the clearest livestreamed video anywhere, anytime. These features work on Chromium-based browsers such as Google Chrome or Microsoft Edge.</p>
<p>To enable RTX Video HDR:</p>
<ol>
<li><i>Download and install the January Studio Driver.</i></li>
<li><i>Ensure Windows HDR features are enabled by navigating to System &gt; Display &gt; HDR.</i></li>
<li><i>Open the NVIDIA Control Panel and navigate to Adjust video image settings &gt; RTX Video Enhancement — then enable HDR.</i></li>
</ol>
<p>Standard dynamic range video will then automatically convert to HDR, displaying remarkably improved details and sharpness.</p>
<p>RTX Video HDR is among the RTX-powered apps enhancing everyday PC use, productivity, creating and gaming. <a href="https://www.nvidia.com/en-us/geforce/broadcasting/broadcast-app/">NVIDIA Broadcast</a> supercharges mics and cams; <a href="https://www.nvidia.com/en-us/studio/canvas/">NVIDIA Canvas</a> turns simple brushstrokes into realistic landscape images; and <a href="https://www.nvidia.com/en-us/omniverse/creators/">NVIDIA Omniverse</a> seamlessly connects 3D apps and creative workflows. Explore exclusive Studio tools, including industry-leading NVIDIA Studio Drivers — free for <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/">RTX graphics card</a> owners — which support the latest creative app updates, AI-powered features and more.</p>
<p>RTX Video HDR requires an RTX GPU connected to an HDR10-compatible monitor or TV. For additional information, check out the <a href="https://nvidia.custhelp.com/app/answers/detail/a_id/5448/~/rtx-video-super-resolution-faq">RTX Video FAQ</a>.</p>
<h2><b>Introducing the Remarkable RTX Remix Open Beta</b></h2>
<p>Built on <a href="https://www.nvidia.com/en-us/omniverse/creators/">NVIDIA Omniverse</a>, the <a href="https://www.nvidia.com/en-us/geforce/rtx-remix/">RTX Remix</a> open beta is available now.</p>
<figure id="attachment_69437" aria-describedby="caption-attachment-69437"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/01/rtx-remix-studio-vishal-ranga-wk93-vishal-ranga-007-1280w.png"><picture decoding="async">
<source type="image/webp" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/01/rtx-remix-studio-vishal-ranga-wk93-vishal-ranga-007-1280w-672x378.png.webp 672w, https://blogs.nvidia.com/wp-content/uploads/2024/01/rtx-remix-studio-vishal-ranga-wk93-vishal-ranga-007-1280w-400x225.png.webp 400w, https://blogs.nvidia.com/wp-content/uploads/2024/01/rtx-remix-studio-vishal-ranga-wk93-vishal-ranga-007-1280w-768x432.png.webp 768w, https://blogs.nvidia.com/wp-content/uploads/2024/01/rtx-remix-studio-vishal-ranga-wk93-vishal-ranga-007-1280w-800x450.png.webp 800w, https://blogs.nvidia.com/wp-content/uploads/2024/01/rtx-remix-studio-vishal-ranga-wk93-vishal-ranga-007-1280w-382x215.png.webp 382w, https://blogs.nvidia.com/wp-content/uploads/2024/01/rtx-remix-studio-vishal-ranga-wk93-vishal-ranga-007-1280w-178x100.png.webp 178w, https://blogs.nvidia.com/wp-content/uploads/2024/01/rtx-remix-studio-vishal-ranga-wk93-vishal-ranga-007-1280w.png.webp 1280w" sizes="(max-width: 672px) 100vw, 672px">
<img decoding="async" src="https://blogs.nvidia.com/wp-content/uploads/2024/01/rtx-remix-studio-vishal-ranga-wk93-vishal-ranga-007-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/01/rtx-remix-studio-vishal-ranga-wk93-vishal-ranga-007-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/01/rtx-remix-studio-vishal-ranga-wk93-vishal-ranga-007-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/01/rtx-remix-studio-vishal-ranga-wk93-vishal-ranga-007-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/01/rtx-remix-studio-vishal-ranga-wk93-vishal-ranga-007-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2024/01/rtx-remix-studio-vishal-ranga-wk93-vishal-ranga-007-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2024/01/rtx-remix-studio-vishal-ranga-wk93-vishal-ranga-007-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2024/01/rtx-remix-studio-vishal-ranga-wk93-vishal-ranga-007-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px">
</picture>
</a><figcaption id="caption-attachment-69437">The NVIDIA RTX open beta is out now.</figcaption></figure>
<p>It allows modders to easily capture game assets, automatically enhance materials with generative AI tools, reimagine assets via Omniverse-connected apps and <a href="https://www.nvidia.com/en-us/omniverse/usd/">Universal Scene Description (OpenUSD)</a>, and quickly create stunning RTX remasters of classic games with full ray tracing and NVIDIA DLSS technology.</p>
<p><iframe loading="lazy" title="[Open Beta Out Now] RTX Remix: Easily Remaster Classic Games with RTX" width="500" height="281" src="https://www.youtube.com/embed/ELJe1jJfOhs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<p>RTX Remix has already delivered stunning remasters, such as <a href="https://www.nvidia.com/en-us/geforce/news/portal-with-rtx-full-ray-tracing-december-8/"><i>Portal with RTX</i></a> and the modder-made <a href="https://www.nvidia.com/en-us/geforce/news/portal-prelude-rtx-available-now-for-free/"><i>Portal: Prelude RTX</i></a>. Orbifold Studios is now using the technology to develop <a href="https://www.nvidia.com/en-us/geforce/news/half-life-2-rtx-remix-in-development/"><i>Half-Life 2 RTX: An RTX Remix Project</i></a>, a community remaster of one of the <a href="https://www.metacritic.com/browse/games/score/metascore/all/pc/filtered">highest-rated games of all time</a>. Check out the gameplay trailer, showcasing Orbifold Studios’ latest updates to Ravenholm:</p>
<p><iframe loading="lazy" title="Half-Life 2 RTX, An RTX Remix Project - Ravenholm Trailer" width="500" height="281" src="https://www.youtube.com/embed/nIE9gQt6WXQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<p><a href="https://www.nvidia.com/en-us/geforce/news/rtx-remix-open-beta-half-life-2-rtx-trailer">Learn more about the RTX Remix open beta</a> and <a href="https://www.nvidia.com/en-us/geforce/rtx-remix/">sign up to gain access</a>.</p>
<h2><b>Leveling Up With RTX</b></h2>
<p>Vishal Ranga has a decade’s worth of experience in the gaming industry, where he pursues level design.</p>
<p>“I’ve loved playing video games since forever, and that curiosity led me to game design,” he said. “A few years later, I found my sweet spot in technical art.”</p>
<figure id="attachment_69434" aria-describedby="caption-attachment-69434"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-a3-1280w.png"><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-a3-1280w-672x378.png.webp 672w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-a3-1280w-400x225.png.webp 400w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-a3-1280w-768x432.png.webp 768w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-a3-1280w-800x450.png.webp 800w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-a3-1280w-382x215.png.webp 382w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-a3-1280w-178x100.png.webp 178w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-a3-1280w.png.webp 1280w" sizes="(max-width: 672px) 100vw, 672px">
<img loading="lazy" decoding="async" src="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-a3-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-a3-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-a3-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-a3-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-a3-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-a3-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-a3-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-a3-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px">
</picture>
</a><figcaption id="caption-attachment-69434">Ranga specializes in level design.</figcaption></figure>
<p>His stunning scene <i>Disowned </i>was born out of experimentation with Unreal Engine’s new ray-traced global illumination lighting capabilities.</p>
<p>Remarkably, he skipped the concepting process — the entire project was conceived solely from Ranga’s imagination.</p>
<p>Applying the water shader and mocking up the lighting early helped Ranga set up the mood of the scene. He then updated old assets and searched the Unreal Engine store for new ones — what he couldn’t find, like fishing nets and custom flags, he created from scratch.</p>
<figure id="attachment_69431" aria-describedby="caption-attachment-69431"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0005-1280w.png"><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0005-1280w-672x335.png.webp 672w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0005-1280w-400x199.png.webp 400w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0005-1280w-768x383.png.webp 768w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0005-1280w-842x420.png.webp 842w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0005-1280w-406x202.png.webp 406w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0005-1280w-188x94.png.webp 188w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0005-1280w.png.webp 1280w" sizes="(max-width: 672px) 100vw, 672px">
<img loading="lazy" decoding="async" src="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0005-1280w-672x335.png" alt="" width="672" height="335" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0005-1280w-672x335.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0005-1280w-400x199.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0005-1280w-768x383.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0005-1280w-842x420.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0005-1280w-406x202.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0005-1280w-188x94.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0005-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px">
</picture>
</a><figcaption id="caption-attachment-69431">Ranga meticulously organizes assets.</figcaption></figure>
<p>“I chose a GeForce RTX GPU to use ray-traced dynamic global illumination with RTX cards for natural, more realistic light bounces.” — Vishal Ranga</p>
<p>Ranga’s GeForce RTX graphics card unlocked RTX-accelerated rendering for high-fidelity, interactive visualization of 3D designs during virtual production.</p>
<p>Next, he tackled shader work, blending in moss and muck into models of wood, nets and flags. He also created a volumetric local fog shader to complement the assets as they pass through the fog, adding greater depth to the scene.</p>
<figure id="attachment_69428" aria-describedby="caption-attachment-69428"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0004-1280w.png"><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0004-1280w-672x378.png.webp 672w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0004-1280w-400x225.png.webp 400w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0004-1280w-768x432.png.webp 768w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0004-1280w-800x450.png.webp 800w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0004-1280w-382x215.png.webp 382w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0004-1280w-178x100.png.webp 178w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0004-1280w.png.webp 1280w" sizes="(max-width: 672px) 100vw, 672px">
<img loading="lazy" decoding="async" src="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0004-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0004-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0004-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0004-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0004-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0004-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0004-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0004-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px">
</picture>
</a><figcaption id="caption-attachment-69428">Shaders add extraordinary depth and visual detail.</figcaption></figure>
<p>Ranga then polished everything up. He first used a water shader to add realism to reflections, surface moss and subtle waves, then tinkered with global illumination and reflection effects, along with other post-process settings.</p>
<figure id="attachment_69425" aria-describedby="caption-attachment-69425"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0002-1280w.png"><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0002-1280w-672x378.png.webp 672w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0002-1280w-400x225.png.webp 400w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0002-1280w-768x432.png.webp 768w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0002-1280w-800x450.png.webp 800w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0002-1280w-382x215.png.webp 382w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0002-1280w-178x100.png.webp 178w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0002-1280w.png.webp 1280w" sizes="(max-width: 672px) 100vw, 672px">
<img loading="lazy" decoding="async" src="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0002-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0002-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0002-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0002-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0002-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0002-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0002-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-0002-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px">
</picture>
</a><figcaption id="caption-attachment-69425">Materials come together to deliver realism and higher visual quality.</figcaption></figure>
<p>Ranga used Unreal Engine’s internal high-resolution screenshot feature and sequencer to capture renders. This was achieved by cranking up screen resolution to 200%, resulting in crisper details.</p>
<p>Throughout, DLSS enhanced Ranga’s creative workflow, allowing for smooth scene movement while maintaining immaculate visual quality.</p>
<p>When finished with adjustments, Ranga exported the final scene in no time thanks to his RTX GPU.</p>
<p><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video id="video-69412-1" width="1280" height="720" loop="1" autoplay="1" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-swamp_01-1280w.mp4?_=1"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-swamp_01-1280w.mp4">https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-vishal-ranga-wk93-swamp_01-1280w.mp4</a></video></p>

<p>Ranga encourages budding artists who are excited by the latest creative advances but wondering where to begin to “practice your skills, prioritize the basics.”</p>
<p>“Take the time to practice and really experience the highs and lows of the creation process,” he said. “And don’t forget to maintain good well-being to maximize your potential.”</p>
<figure id="attachment_69422" aria-describedby="caption-attachment-69422"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-itns-vishal-ranga-wk93-artist-feature-1280w.png"><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-itns-vishal-ranga-wk93-artist-feature-1280w-672x263.png.webp 672w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-itns-vishal-ranga-wk93-artist-feature-1280w-400x156.png.webp 400w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-itns-vishal-ranga-wk93-artist-feature-1280w-768x300.png.webp 768w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-itns-vishal-ranga-wk93-artist-feature-1280w-842x329.png.webp 842w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-itns-vishal-ranga-wk93-artist-feature-1280w-406x159.png.webp 406w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-itns-vishal-ranga-wk93-artist-feature-1280w-188x73.png.webp 188w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-itns-vishal-ranga-wk93-artist-feature-1280w.png.webp 1280w" sizes="(max-width: 672px) 100vw, 672px">
<img loading="lazy" decoding="async" src="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-itns-vishal-ranga-wk93-artist-feature-1280w-672x263.png" alt="" width="672" height="263" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-itns-vishal-ranga-wk93-artist-feature-1280w-672x263.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-itns-vishal-ranga-wk93-artist-feature-1280w-400x156.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-itns-vishal-ranga-wk93-artist-feature-1280w-768x300.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-itns-vishal-ranga-wk93-artist-feature-1280w-842x329.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-itns-vishal-ranga-wk93-artist-feature-1280w-406x159.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-itns-vishal-ranga-wk93-artist-feature-1280w-188x73.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/01/studio-itns-vishal-ranga-wk93-artist-feature-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px">
</picture>
</a><figcaption id="caption-attachment-69422">3D artist Vishal Ranga.</figcaption></figure>
<p>Check out Ranga’s portfolio on <a href="https://www.artstation.com/vishalranga">ArtStation</a>.</p>
<p><i>Follow NVIDIA Studio on </i><a href="https://www.instagram.com/nvidiastudio/"><i>Instagram</i></a><i>, </i><a href="https://twitter.com/NVIDIAStudio"><i>X</i></a><i> and </i><a href="https://www.facebook.com/NVIDIAStudio/"><i>Facebook</i></a><i>. Access tutorials on the </i><a href="https://www.youtube.com/channel/UCDeQdW6Lt6nhq3mLM4oLGWw"><i>Studio YouTube channel</i></a><i> and get updates directly in your inbox by subscribing to the </i><a href="https://www.nvidia.com/en-us/studio/?nvmid=subscribe-creators-mail-icon"><i>Studio newsletter</i></a><i>.</i><b>&nbsp;</b></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Man Allegedly Raped in Jail After AI Wrongly IDs Him as Suspect Despite Alibi (103 pts)]]></title>
            <link>https://www.ibtimes.sg/texas-man-allegedly-raped-jail-after-ai-wrongly-identifies-him-robbery-suspect-despite-having-73211</link>
            <guid>39118534</guid>
            <pubDate>Wed, 24 Jan 2024 15:35:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ibtimes.sg/texas-man-allegedly-raped-jail-after-ai-wrongly-identifies-him-robbery-suspect-despite-having-73211">https://www.ibtimes.sg/texas-man-allegedly-raped-jail-after-ai-wrongly-identifies-him-robbery-suspect-despite-having-73211</a>, See on <a href="https://news.ycombinator.com/item?id=39118534">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody" id="v_main">
<meta charset="UTF-8"><p>A Texas man is suing Macy's and the parent company of Sunglass Hut after the two companies allegedly relied on error-prone facial recognition technology to falsely accuse him of armed robbery.</p><p>Harvey Murphy Jr., 61, went to a DMV to renew his license in Gonzales, Texas, but didn't realize he would be leaving in handcuffs. His arrest was in connection to an armed robbery — one that occurred when he was on the other side of the country.</p><p><strong>Sunglass Hut Employee Told Police They 'Found Their Guy' Using Facial Recognition</strong></p><p>On Jan. 22, 2022, a Sunglass Hut in Houston was robbed by two armed men. The men stole thousands of dollars from the store, according to a lawsuit. During the robbery, the men ordered two employees into a back room and told them to stay there so they could get away, court records say.</p><p>As police were investigating, they got a call from a loss prevention employee with EssilorLuxottica, which is Sunglass Hut's parent company.</p><p>The employee told police they "could stop their investigation because he found their guy," the lawsuit said. The employee said he worked with Macy's loss prevention using artificial intelligence and facial recognition software to identify the suspect as the 61-year-old man, the lawsuit said.</p><p><strong>Murphy Claims He was Arrested Despite Having a Strong Alibi</strong></p><p>EssilorLuxottica and Macy's took a video from the robbery and used it to determine the man was one of the robbers, and police put out a warrant for his arrest, court records said. However, the man had an alibi at the time of the robbery. He was 2,000 miles away in California, according to his lawyer.</p><p>Still, he was arrested with no idea what was going on. "All he knew was he was being transported to the Harris County Jail because of a felony arrest warrant," court documents said.</p><p><strong>Murphy Alleges Being Sexually Assaulted While in Jail</strong></p><p>At his arraignment, the man learned he was being charged with robbery. Once he learned the date of the incident, his lawyer and the prosecutor were able to confirm his alibi, the lawsuit said. The judge then agreed to dismiss the charges against him.</p><p>However, just hours before he was released from jail, the man says he was brutally assaulted, according to the lawsuit. "He was followed into the bathroom by three violent criminals. He was beaten, forced on the ground, and brutally gang raped. After this violent attack, one of the criminals held a shank against his neck and told him that if he reported the rape to anyone, he would be murdered," the lawsuit said. After the attack, the man crawled into his bunk, faced the wall and prayed he wouldn't be attacked again, the lawsuit said.</p><p>The man was left with permanent injuries from the assault, according to court documents.</p><p><strong>Facial Recognition Led to Wrongful Arrest, Claims Suit<br></strong><br>
Facial recognition is an artificial intelligence technology that analyzes human faces and creates a biometric template, according to the lawsuit. Sunglass Hut collects customers' "biometric identifiers" and "biometric information" with face scanning technology at many of its locations, the court documents said.</p><p>"EssilorLuxottica not only profits from its facial-recognition software, but it uses facial-recognition software to identify people who allegedly steal from the Sunglass Hut," the lawsuit said. The court documents say the technology being used is not accurate in most cases and is what led to the wrongful arrest of the man, who is now seeking $10 million in damages from a jury trial.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Embracing Common Lisp in the modern world (170 pts)]]></title>
            <link>https://www.juxt.pro/blog/common-lisp-in-modern-world/</link>
            <guid>39118438</guid>
            <pubDate>Wed, 24 Jan 2024 15:27:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.juxt.pro/blog/common-lisp-in-modern-world/">https://www.juxt.pro/blog/common-lisp-in-modern-world/</a>, See on <a href="https://news.ycombinator.com/item?id=39118438">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>Enter “Embracing Common Lisp in the Modern World,” a presentation that takes you on a captivating journey through the historical context and evolution of Common Lisp, Clojure, and the Java Virtual Machine (JVM):</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/hPulycyXGp4?si=ffZ4tXCvZGIz_D0U" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
<p>In a digital world where languages and technologies evolve at breakneck speed, it’s crucial to understand the foundations that continue to influence our tech choices. This presentation not only reveals how Clojure is deeply rooted in Common Lisp but also explores the grand vision of the JVM as a universal computing platform. It paints a vivid picture of how the tech landscape has transformed since the late ’90s, offering a compelling comparison between then and now. Additionally, it dives into the efficiency and performance disparities between Common Lisp and Clojure, providing valuable insights for developers navigating the complex realm of programming languages.</p>
<p>Now, let’s consider something we all hold dear – our planet and the technology we use. By embracing Common Lisp over Clojure and the JVM, we’re not only choosing a powerful programming language but also making a greener choice for the environment. In a world where sustainability matters more than ever, our programming decisions can play a significant role. Join us in uncovering the enduring legacy of Common Lisp and its relevance in the ever-evolving modern tech world while making a conscious choice for a greener future.</p>
<p>You can find a summary of the presentation here: <a href="http://juxt.pro/embracing-common-lisp.pdf">http://juxt.pro/embracing-common-lisp.pdf</a>.</p>
<p>Let me know what you think!</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ZX Spectrum Raytracer (203 pts)]]></title>
            <link>https://gabrielgambetta.com/zx-raytracer.html</link>
            <guid>39118349</guid>
            <pubDate>Wed, 24 Jan 2024 15:20:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gabrielgambetta.com/zx-raytracer.html">https://gabrielgambetta.com/zx-raytracer.html</a>, See on <a href="https://news.ycombinator.com/item?id=39118349">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p>I love raytracers; in fact I’ve written <a href="https://gabrielgambetta.com/computer-graphics-from-scratch/02-basic-raytracing.html">half a book</a> about them. Probably less known is my love for the <a href="https://en.wikipedia.org/wiki/ZX_Spectrum">ZX Spectrum</a>, the 1982 home computer I grew up with, and which started my interest in graphics and programming. This machine is so ridiculously underpowered for today’s standards (and even for 1980s standards), the inevitable question is, to what extent could I port the Computer Graphics from Scratch raytracer to a ZX Spectrum?</p>
<p>The ZX Spectrum has a 3.5 MHz Z80 processor (1,000 times slower than current computers) that can’t multiply numbers (!!!), 48 KB of RAM (1,000,000 times smaller), and a 256x176 graphics mode (~200 times lower resolution) capable of displaying 15 colors (1,000,000 times fewer – and with some unusual quirks). This is an interesting setup for a CPU-intensive graphics application!</p>
<p>My plan was to implement this in Sinclair BASIC, the built-in programming language of the Spectrum. This is not just BASIC, but an ancient, very limited dialect of BASIC. For example, the only control structures are <code>FOR</code> and <code>IF</code> (and <code>IF</code> has no <code>ENDIF</code>); all variables are global; there are no function calls, only <code>GO TO</code> and <code>GO SUB</code>; etc. It’s also interpreted, so super slow. But at least it implements multiplications in software! I could always rewrite the raytracer in assembler if I cared about performance.</p>
<p>I set up a minimal environment: I write BASIC code using VS Code, compile it using <a href="https://github.com/speccyorg/bas2tap">BAS2TAP</a>, and run it on the <a href="https://fuse-emulator.sourceforge.net/">FUSE emulator</a>. This gave me a pretty decent iteration speed.</p>
<p>As an aside, I hadn’t written BASIC in something like 30 years, and I was surprised at how quickly it all came back. I was between 4 and 10 when I was doing this, so I guess it sticks in the brain like anything you learn at that age, like languages and accents. Now let’s get coding like it’s 1984!</p>
<h2 id="first-iteration-a-simple-raytracer"><a href="#first-iteration-a-simple-raytracer">First iteration: a simple raytracer</a></h2>
<p>My first iteration was pretty straightforward: I ported the <a href="https://gabrielgambetta.com/computer-graphics-from-scratch/02-basic-raytracing.html#rendering-our-first-spheres">starter CGFS raytracing code</a> to BASIC without much tweaking, outputting a 32x22-block image, and to my surprise, it worked well:</p>
<p><img src="https://gabrielgambetta.com/img/zx-rt-1.png"></p>
<p>The number in the upper left corner, 879.76, is the time it took to render this image, in seconds. Yes, that’s almost 15 minutes. Here’s the same scene rendered by the CGFS raytracer in about a second, using the same scene and feature set:</p>
<center>
<img src="https://gabrielgambetta.com/img/zx-rt-1-cgfs.png">
</center>
<p>The Spectrum version doesn’t look bad, considering the limitations! Let’s take a look at the code:</p>
<pre>   1 BRIGHT 1: CLS

   5 LET ROX = 0
   6 LET ROY = 0
   7 LET ROZ = 0
   8 LET TMIN = 0
   9 LET TMAX = 10000

  10 FOR X = 0 TO 31
  20   FOR Y = 0 TO 21

  30     LET RDX = (X - 16) / 32
  31     LET RDY = (11 - Y) / 32
  32     LET RDZ = 1

  40     GO SUB 1000

  50     PAPER COL
  51     PRINT AT Y, X; " "

 100   NEXT Y
 105   GO SUB 3000: PRINT AT 0, 0; TIME
 110 NEXT X

 120 STOP


1000 REM ===== TraceRay =====
1001 REM Params: (ROX, ROY, ROZ): ray origin; (RDX, RDY, RDZ): ray direction; (TMIN, TMAX): wanted ranges of t
1002 REM Returns: COL: pixel color

1010 LET COL = -1: LET MINT = 0

1100 RESTORE 9000
1101 READ NS
1102 FOR S = 1 TO NS

1110    READ SCX, SCY, SCZ, SRAD, SCOL

1200    LET COX = ROX - SCX
1201    LET COY = ROY - SCY
1202    LET COZ = ROZ - SCZ

1210    LET EQA = RDX*RDX + RDY*RDY + RDZ*RDZ
1211    LET EQB = 2*(RDX*COX + RDY*COY + RDZ*COZ)
1212    LET EQC = (COX*COX + COY*COY + COZ*COZ) - SRAD*SRAD

1220    LET DISC = EQB*EQB - 4*EQA*EQC
1230    IF DISC &lt; 0 THEN GO TO 1500

1240    LET T1 = (-EQB + SQR(DISC)) / 2*EQA
1241    LET T2 = (-EQB - SQR(DISC)) / 2*EQA

1250    IF T1 &gt;= TMIN AND T1 &lt;= TMAX AND (T1 &lt; MINT OR COL = -1) THEN LET COL = SCOL: LET MINT = T1
1300    IF T2 &gt;= TMIN AND T2 &lt;= TMAX AND (T2 &lt; MINT OR COL = -1) THEN LET COL = SCOL: LET MINT = T2

1500 NEXT S

1999 IF COL = -1 THEN LET COL = 0
2000 RETURN

3000 REM ===== Get timestamp in seconds =====
3001 LET TIME = (65536*PEEK 23674 + 256*PEEK 23673 + PEEK 23672) / 50
3002 RETURN

8998 REM ===== Sphere data =====
8999 REM Sphere count, followed by (SCX, SCY, SCZ, SRAD, COLOR)
9000 DATA 4
9001 DATA  0, -1, 4, 1, 2
9002 DATA  2,  0, 4, 1, 1
9003 DATA -2,  0, 4, 1, 4
9004 DATA 0, -5001, 0, 5000, 6</pre>
<p>The structure of the code should look familiar if you’re familiar with raytracers in general, and with the CGFS raytracer in particular, despite being written in an ancient dialect of BASIC. I’ll still walk through the code to point out the quirks of the Spectrum.</p>
<p>First, line numbers. Every line had to have a number, so you could use <code>GO TO</code> or <code>GO SUB</code>. Lines supported multiple statements separated by a colon – especially useful for the <code>IF ... THEN</code> statement, considering there’s no <code>END IF</code>!</p>
<p>You’ll notice the line numbers are all over the place. The Spectrum BASIC editor was line-oriented, so while it was possible to change line numbers, it was very time-consuming. So you’d number your lines in multiples of 10, so you had “space” to add lines in between if needed.</p>
<p>We start with this:</p>
<pre>1 BRIGHT 1: CLS</pre>
<p>The Spectrum has a pretty quirky graphics mode. I’ll get into the details in the next section. For now, let’s just say that <code>BRIGHT 1</code> chooses the bright version of the color palette, and <code>CLS</code> clears the screen. So we’re ready to start drawing something.</p>
<p>Then comes the main loop of the rayrtacer:</p>
<pre>  5 LET ROX = 0
  6 LET ROY = 0
  7 LET ROZ = 0
  8 LET TMIN = 0
  9 LET TMAX = 10000

 10 FOR X = 0 TO 31
 20   FOR Y = 0 TO 21

 30     LET RDX = (X - 16) / 32
 31     LET RDY = (11 - Y) / 32
 32     LET RDZ = 1

 40     GO SUB 1000

 50     PAPER COL
 51     PRINT AT Y, X; " "

100   NEXT Y
105   GO SUB 3000: PRINT AT 0, 0; TIME
110 NEXT X

120 STOP</pre>
<p>Lines 5 to 9 set some of the parameters that are constant throughout the main loop. BASIC had arrays but they were pretty inconvenient to use, so using them to represent points and vectors was a non-starter. So the ray origin <code>RO</code> is represented by the three variables <code>ROX</code>, <code>ROY</code> and <code>ROZ</code>.</p>
<p>Lines 10 to 110 form the main loop, iterating over the canvas (32x22 squares). After each pass of the inner loop, rendering a column of squares, line 105 does the equivalent of a function call: <code>GO SUB 3000</code> transfers control flow to the <em>subroutine</em> at line 3000:</p>
<pre>3000 REM ===== Get timestamp in seconds =====
3001 LET TIME = (65536*PEEK 23674 + 256*PEEK 23673 + PEEK 23672) / 50
3002 RETURN</pre>
<p>Line 3000 starts with <code>REM</code>, short for “remark”. We call them “comments” these days, but the ZX Spectrum is British, the brainchild of mad genius <a href="https://en.wikipedia.org/wiki/Clive_Sinclair">Sir Clive Sinclair</a>. So this line is just a comment.</p>
<p>The magical incantation in line 3001 reads the current timestamp in seconds. How? <code>PEEK</code> takes a memory address and returns its contents. All this line does is read a 24-bit number stored in memory, representing the internal <code>FRAME</code> counter; this counter is incremented every 20ms, so we divide it by 50 to convert it to seconds, and store it in the variable <code>TIME</code>.</p>
<p>Every variable in the program is global, so <code>RETURN</code> in line 3002 just returns flow control to the caller, and the “return value” of the function is implicitly the <code>TIME</code> global variable. This <code>GO SUB</code> / <code>RETURN</code> mechanism is very similar to <code>CALL</code> / <code>RET</code> in assembly.</p>
<p>Finally, line 120 terminates the program.</p>
<p>Now let’s take a look at the inner loop. Lines 30 to 32 convert canvas coordinates to viewport coordinates (<code>CanvasToViewport</code> in CGFS). The ray direction is represented by <code>(RDX, RDY, RDZ)</code>.</p>
<p>Line 40 does another “function call”, this time to the equivalent of <code>TraceRay</code>. When it returns, the variable <code>COL</code> will contain the color of whatever the ray hit.</p>
<p>Lines 50 and 51 finally draw the block. This is done by setting the <code>PAPER</code> (background) color and drawing a space (more on this later).</p>
<p>Now let’s take a look at <code>TraceRay</code> starting at line 1000. It starts with a comment block documenting the implicit inputs and outputs:</p>
<pre>1000 REM ===== TraceRay =====
1001 REM Params: (ROX, ROY, ROZ): ray origin; (RDX, RDY, RDZ): ray direction; (TMIN, TMAX): wanted ranges of t
1002 REM Returns: COL: pixel color</pre>
<p>Because there are no function arguments or return values, everything is global, implicit, and by convention. In this case, the inputs are <code>(ROX, ROY, ROZ)</code>, <code>(RDX, RDY, RDZ)</code>, <code>TMIN</code> and <code>TMAX</code>, and the return value is in the variable <code>COL</code>. This represents an index into the fixed color palette of the ZX Spectrum.</p>
<p>Line 1010 initializes the values we need to keep track of the closest intersection found so far, and the color of the sphere at the intersection:</p>
<pre>1010 LET COL = -1: LET MINT = 0</pre>
<p>Then we start the “for each sphere” loop:</p>
<pre>1100 RESTORE 9000
1101 READ NS
1102 FOR S = 1 TO NS
1110    READ SCX, SCY, SCZ, SRAD, SCOL</pre>
<p>Line 1100 resets a “data pointer” to line 9000, which contains the scene data:</p>
<pre>8998 REM ===== Sphere data =====
8999 REM Sphere count, followed by (SCX, SCY, SCZ, SRAD, COLOR)
9000 DATA 4
9001 DATA  0, -1, 4, 1, 2
9002 DATA  2,  0, 4, 1, 1
9003 DATA -2,  0, 4, 1, 4
9004 DATA  0, -5001, 0, 5000, 6</pre>
<p>The <code>READ</code> statement in line 1101 reads the first value (the number 4 in line 9000) into the variable NS. Then line 1102 starts the “for each sphere” loop, and the first thing we do in line 1110 is read the 5 values defining a sphere into variables. After that first batch of <code>READ</code> statemends the data pointer is now at the first value of line 9002, ready to be read during the next iteration of the loop.</p>
<p>Lines 1200 to 1300 solve a straightforward <a href="https://gabrielgambetta.com/computer-graphics-from-scratch/02-basic-raytracing.html#ray-meets-sphere">ray-sphere intersection equation</a>, with lines 1250 and 1300 keeping track of the closest intersection:</p>
<pre>1200    LET COX = ROX - SCX
1201    LET COY = ROY - SCY
1202    LET COZ = ROZ - SCZ

1210    LET EQA = RDX*RDX + RDY*RDY + RDZ*RDZ
1211    LET EQB = 2*(RDX*COX + RDY*COY + RDZ*COZ)
1212    LET EQC = (COX*COX + COY*COY + COZ*COZ) - SRAD*SRAD

1220    LET DISC = EQB*EQB - 4*EQA*EQC
1230    IF DISC &lt; 0 THEN GO TO 1500

1240    LET T1 = (-EQB + SQR(DISC)) / 2*EQA
1241    LET T2 = (-EQB - SQR(DISC)) / 2*EQA

1250    IF T1 &gt;= TMIN AND T1 &lt;= TMAX AND (T1 &lt; MINT OR COL = -1) THEN LET COL = SCOL: LET MINT = T1
1300    IF T2 &gt;= TMIN AND T2 &lt;= TMAX AND (T2 &lt; MINT OR COL = -1) THEN LET COL = SCOL: LET MINT = T2</pre>
<p>We finish the loop checking if there were no intersections, in which case we set the color to 0 (black), and return:</p>
<pre>1999 IF COL = -1 THEN LET COL = 0
2000 RETURN</pre>
<p>And that’s all there is to it. We get our super slow, super low-res output:</p>
<p><img src="https://gabrielgambetta.com/img/zx-rt-1.png"></p>
<p>I still find it pretty impressive that this only takes 50 lines of relatively straightforward code in an underpowered early 80s machine!</p>
<p>But this is just a start. Why stick to 32x22 when the usable pixel dimensions of the screen are 256x176?</p>
<h2 id="second-iteration-higher-resolution-and-handling-attribute-clashes"><a href="#second-iteration-higher-resolution-and-handling-attribute-clashes">Second iteration: higher resolution, and handling attribute clashes</a></h2>
<p>You might think that increasing the resolution of this raytracer is as simple as changing the outer loop to 256x176 instead of 32x22 and drawing individual pixels using <code>PLOT</code> instead of chunky squares using <code>PRINT</code>. This would be 64 times slower (16 hours instead of 15 minutes) but it would work – except in the quirky graphics mode of the ZX Spectrum!</p>
<p>The first version of the ZX Spectrum had a grand total of 16 KB of RAM, so memory efficiency was absolutely critical (I had the considerably more luxurious 48 KB model). To help save memory, video RAM was split in two blocks: a bitmap block, using one bit per pixel, and an attributes block, using one byte per 8x8 block of pixels. The attributes block would assign two colors to that block, called <code>INK</code> (foreground) and <code>PAPER</code> (background).</p>
<p>So you could use <code>PLOT</code> to set or clear the bit corresponding to a pixel, which would then take one of the two colors assigned to that block. <strong>This means each 8x8-pixel block can show one or two different colors, but never three or more</strong>.</p>
<p>This all worked great for text-based applications, since characters were also 8x8 blocks, but for anything graphic, especially games, it was super limiting. This limitation gives Spectrum games its very characteristic aesthetic, because artists had to work around this, usually by designing screens and sprites aligned to a 8x8 pixel grid, or going full monochrome, or accepting that attribute clash was a fact of life.</p>
<p>Back to the raytracer. Increasing the resolution is easy. Dealing with attribute clash, not so much.</p>
<p>There’s no perfect solution: no matter what I do, each 8x8 block can show up to two colors. So what I did was implement an approximation algorithm. I collect the colors present in the 8x8 block, find the most common and second most common, and draw every pixel using one of the two.</p>
<p>The outer loop changes a bit to reflect the higher resolution and the processing on 8x8-block chunks:</p>
<pre> 10 FOR X = 0 TO 255 STEP 8
 20   FOR Y = 0 TO 175 STEP 8

...

500   NEXT Y
505   GO SUB 3000: PRINT AT 0, 0; TIME
510 NEXT X
520 STOP</pre>
<p>Then we trace the 64 rays, collecting the colors in an array:</p>
<pre> 30      DIM C(64)
 31      LET CI = 1
 32      DIM A(8)

120     REM --- For each 8x8 block, collect the pixel colors and their counts ---
125     FOR U = X TO X+7
126       FOR V = Y TO Y+7

130         LET RDX = (U - 128) / 256
131         LET RDY = (V - 88) / 256
132         LET RDZ = 1

140         GO SUB 1000
141         LET C(CI) = COL
142         LET CI = CI + 1
143         LET A(COL+1) = A(COL+1) + 1

160       NEXT V
161     NEXT U</pre>
<p>Line 30 <code>DIM</code>ensions the variable <code>C</code> as a 64-element array. Array indexes start at 1, so line 31 initializes <code>CI</code> (<code>C</code>-index) to 1. Line 32 creates another array <code>A</code> which will hold the color counts.</p>
<p>Lines 140 to 143 call <code>TraceRay</code> and store the results: the pixel color in <code>C</code>, and the updated color count in <code>A</code>. Colors go from 0 to 7 but indexes go from 1 to 8, so we need to use <code>COL+1</code> as the index.</p>
<p>Next we need to find the most and second-most frequent colors:</p>
<pre>199     REM --- Find the most and second most frequent colors in this 8x8 block ---
201     LET MFC = 0
202     FOR C = 1 TO 8
203       IF A(C) &gt; MFC THEN LET MFC = A(C): LET MFI = C
204     NEXT C
205     LET FCOL = MFI - 1

207     LET II = MFI: LET MFC = 0: LET MFI = 0
208     FOR C = 1 TO 8
209       IF C &lt;&gt; II AND A(C) &gt; MFC THEN LET MFC = A(C): LET MFI = C
210     NEXT C
211     LET SCOL = MFI - 1</pre>
<p>Time to draw some pixels. If all the pixels are the same color, just paint the block:</p>
<pre>259     REM --- If there's only one color, paint the whole block --
260     IF SCOL &lt;&gt; -1 THEN GO TO 300
270     POKE 22528 + X/8 + 32*(21-Y/8), 64 + FCOL * 8
280     GO TO 500</pre>
<p>That <code>POKE</code> requires an explanation. <code>POKE</code> puts a byte in a memory address. The first parameter is the address of this 8x8 block in the attributes block. The second parameter, the byte representing the INK and PAPER values, is the combination of the INK color shifted left 3 bits, plus a bit to turn on the BRIGHT attribute.</p>
<p>If not all pixels are the same color, we need to plot them individually. The PAPER color of the block is set to the most frequent color (so there’s fewer pixels to plot), we go over the array, and any pixel that isn’t the most frequent color is drawn with INK color set to the second most frequent color:</p>
<pre>300     REM --- Otherwise set the PAPER to the most frequent color, and draw everything else in the second most frequent color --
301     LET CI = 1
310     FOR U = X TO X+7
311       FOR V = Y TO Y+7

320         IF C(CI) &lt;&gt; FCOL THEN PLOT INK SCOL; PAPER FCOL; U, V
321         LET CI = CI + 1

350       NEXT V
351     NEXT U</pre>
<p>This works pretty well!</p>
<p><img src="https://gabrielgambetta.com/img/zx-rt-2.png"></p>
<p>Attribute clash still happens. Look at this magnified part:</p>
<center>
<img src="https://gabrielgambetta.com/img/zx-rt-2-clash.png">
</center>
<p>With a grid overlaid to show block boundaries, the problem is easier to see. The two blocks that look “wrong” should have three colors: black, yellow, and either green or red. But the Spectrum can’t do that, so this is what the algorithm above ends up doing.</p>
<p>You can take a look at the <a href="https://gabrielgambetta.com/zx-raytracer-2-src.html">full source code for this iteration</a>.</p>
<p>The next thing to notice is that it’s just ridiculously slow - over 17 hours! Even on the emulator hitting 20,000% speed, it takes a while to render. Can we do better?</p>
<h2 id="third-iteration-performance-improvements"><a href="#third-iteration-performance-improvements">Third iteration: performance improvements</a></h2>
<p>I went for an optimization pass. Here’s what I did:</p>
<ul>
<li><p>For each 8x8 block, trace rays for the 4 corners, and if the color is the same in all, paint the whole block. Most of the time this does 4 rays per block instead of 64, so by itself it speeds up rendering by 16x. Of course if there were small objects that fell fully inside a block, the raytracer would miss them; but for this test scene, it feels like it’s a fair approximation.</p></li>
<li><p>Avoid multiplications and divisions at all costs. The Z80 can’t do multiplication in hardware (let alone division), so BASIC implements it in software, and it’s <em>slow</em>.</p></li>
<li><p>Hardcode some constants based on assumptions. Notably, the ray origin is always (0, 0, 0), <code>t_min</code> is always 0, and <code>t_max</code> is always <code>+inf</code>, so that saves some computation.</p></li>
<li><p>Precompute values when possible. Why store the spehre radius as data and square it, when it can be stored squared to begin with?</p></li>
<li><p>Move computed values to outer loops when possible. For example, values related to X are constant for every Y, and can be computed fewer times.</p></li>
<li><p>Inlined the “most frequent color” subroutine, and specialized the first case to not ignore any color.</p></li>
<li><p>Tweaked the line numbers to make sure <code>GO SUB</code> didn’t land on a <code>REM</code> line; believe it or not, processing a line that contains a comment takes time!</p></li>
<li><p>Used shorter variable names. This BASIC is interpreted, so every time you reference a variable, it’s looked up by name…</p></li>
<li><p>I also tried some optimizations that didn’t work, like reading the <code>DATA</code> into an array first, or putting certain expressions into variables. I have the vague feeling that the <em>order</em> in which variables are defined is important – I need to read more about this.</p></li>
<li><p>There’s some optimizations that help marginally, but hinder readability, so I chose not to implement them.</p></li>
</ul>
<p>All in all, the result is pretty good. The image is pixel-identical, but the runtime is down to 2 hours and a bit:</p>
<p><img src="https://gabrielgambetta.com/img/zx-rt-3.png"></p>
<p>You can take a look at the <a href="https://gabrielgambetta.com/zx-raytracer-3-src.html">full source code for this iteration</a>.</p>
<h2 id="fourth-iteration-light-just-the-one"><a href="#fourth-iteration-light-just-the-one">Fourth iteration: light (just the one)</a></h2>
<p>Initially I had stopped myself here; given the limitations of the environment, I felt like there wasn’t much more that could be done.</p>
<p>The obvious next step is to implement lighting. The <a href="https://gabrielgambetta.com/computer-graphics-from-scratch/03-light.html">lighting equations and algorithms</a> are relatively straightforward, but the main problem is the very limited set of colors the ZX Spectrum can represent. To recap, it’s a fixed set of 7 colors, in normal and bright versions, plus black:</p>
<center>
<img src="https://gabrielgambetta.com/img/zx-colors.png">
</center>
<p>Even if I had the light intensity value at every pixel, I can’t just multiply it by the sphere color to get the shaded color, like I can do trivially in RGB. What to do?</p>
<p>Tradeoffs, that’s what. I can simulate shades of a color by alternating the color and black in the right amounts. I can do this on a 8x8 block basis, setting the <code>INK</code> to the color, <code>PAPER</code> to black. The tradeoff is that there will be attribute clashing.</p>
<p>How to decide whether to plot a pixel or leave it black? My first idea was to use the light intensity, a real number between 0.0 and 1.0, as the <em>probability</em> that a pixel would be plotted with the color (and left black otherwise). This worked, but it looked ugly. There’s something better, called <a href="https://en.wikipedia.org/wiki/Ordered_dithering">ordered dithering</a>. The idea is to have a matrix of thresholds, one per pixel, that helps determine whether to plot the pixel. The thresholds are arranged in such a way that they produce repeatable, pleasing patterns of pixels for any intensity level. There’s a 8x8 dithering matrix, which fits perfectly the 8x8 color blocks I’m processing, so it was surprisingly easy to implement.</p>
<p>For the sake of simplicity, I decided to have just one directional light. Even with ordered dithering, there are not enough shades I can display that will adequately represent the nuances of multiple lights illuminating the same object. For the same reason, I went for diffuse lighting only, no specular component.</p>
<p>So the goal was to render something like this:</p>
<center>
<img src="https://gabrielgambetta.com/img/zx-rt-4-cgfs.png">
</center>
<p>How close could I get to that on a humble ZX Spectrum?</p>
<p>Here are the relevant changes I made to the code:</p>
<p>I could no longer use the 4-rays-per-8x8-block trick, because the light intensity at each pixel could be different. Could have computed one intensity per block, but I didn’t want to lose light resolution. So performance took a big hit compared to the previous iteration. The exception is if the 4 corners of the 8x8 block are black, in which case I can safely ignore it.</p>
<p>The lighting part is pretty simple: in the <code>TraceRay</code> subroutine, I needed to keep track of the index of the closest sphere (so I also had to load the sphere data into an array <code>S</code> at the start of the program). After the sphere loop, if the ray hits any sphere, I compute the intersection between the ray and the sphere, the normal at that point in the sphere, and finally the illumination at that point:</p>
<pre>1601 LET NX = DX*MT - S(CS, 1): LET NY = DY*MT - S(CS, 2): LET NZ = DZ*MT - S(CS, 3)
1610 LET PL = AI
1615 LET NL = (NX*LX + NY*LY + NZ*LZ)
1620 IF NL &gt; 0 THEN LET PL = PL + DI * NL / SQR(NX*NX + NY*NY + NZ*NZ)</pre>
<p>In that fragment, <code>CS</code> is the index of the Closest Sphere; <code>PL</code> is a new output variable representing Pixel Lighting; (<code>LX</code>, <code>LY</code>, <code>LZ</code>), <code>DI</code> and <code>AI</code> are set elsewhere, and represent the direction of the light, its intensity, and the intensity of the ambient light, respectively. For performance reasons, <code>LX</code>, <code>LY</code>, <code>LZ</code> represent a normalized vector, so I can skip the <code>SQR</code> and the division in line 1620.</p>
<p>I don’t need to find the second most frequent color in each 8x8 block anymore, because each block will only display the most frequent color and black.</p>
<p>I added some code to load the Bayer ordered dither matrix into an array:</p>
<pre>   3 GO SUB 7000

   ...

6999 REM ===== Initialize 8x8 Bayer matrix =====
7000 DIM H(64)
7001 RESTORE 7100
7002 FOR I = 1 TO 64
7003   READ H(I): LET H(I) = H(I) / 64
7004 NEXT I
7005 RETURN

7100 DATA  0, 32,  8, 40,  2, 34, 10, 42
7101 DATA 48, 16, 56, 24, 50, 18, 58, 26
7102 DATA 12, 44,  4, 36, 14, 46,  6, 38
7103 DATA 60, 28, 52, 20, 62, 30, 54, 22
7104 DATA  3, 35, 11, 43,  1, 33,  9, 41
7105 DATA 51, 19, 59, 27, 49, 17, 57, 25
7106 DATA 15, 47,  7, 39, 13, 45,  5, 37
7107 DATA 63, 31, 55, 23, 61, 29, 53, 21</pre>
<p>And finally, before plotting a pixel, I compare its light intensity with the corresponding threshold in the Bayer matrix:</p>
<pre>320 IF C(CI) &gt; 0 AND H(CI) &lt;= L(CI) THEN PLOT U, V</pre>
<p>I ran this iteration, and honestly, I stared at it in disbelief for a good minute:</p>
<p><img src="https://gabrielgambetta.com/img/zx-rt-4.png"></p>
<p>First of all, <em>it works</em>!</p>
<p>It’s pretty slow compared to the previous iteration, mostly because the missing 4-rays-per-block trick, plus the additional lighting calculations. But it’s not <em>that bad</em>.</p>
<p>Attribute clashing is still there, and it’s a lot more obvious now. Could this be improved? Maybe. The yellow/red clashes look like could be improved by making the blocks red and yellow, and forgoing the shading detail (because there would be no black). For green/yellow and blue/yellow, looks like black/yellow, blue/yellow and again black/yellow would make it look better. Hmmmm. Maybe I’ll get back to this.</p>
<p>You can take a look at the <a href="https://gabrielgambetta.com/zx-raytracer-4-src.html">full source code for this iteration</a>.</p>
<h2 id="fifth-iteration-shadows"><a href="#fifth-iteration-shadows">Fifth iteration: shadows</a></h2>
<p>At this point I’m feeling pretty comfortable with the environment, I’m coding like it’s 1984, so I want to see how far I can take this. Next step: shadows.</p>
<p>Most of the pieces are already in place. <a href="https://gabrielgambetta.com/computer-graphics-from-scratch/04-shadows-and-reflections.html#shadows">The theory</a> is relatively simple: before computing lighting for a point, need to figure out whether there’s an object between the point and the light, blocking it (i.e.&nbsp;casting a shadow). I just had to implement a specialized version of <code>TraceRay</code> that traces from the intersection of the primary ray and a sphere, in the direction of the directional light, and returns as soon as it finds any intersection:</p>
<pre>2090 REM ----- Specialized TraceRay for shadow checks -----
2091 REM Params: (IX, IY, IZ): ray start; (LX, LY, LZ): ray direction (directional light vector)
2092 REM Returns: H = 1 if the ray intersects any sphere, H = 0 otherwise
2093 REM Optimizations: (TMIN, TMAX) hardcoded to (epsilon, +inf)

2100 LET A = 2*(LX*LX + LY*LY + LZ*LZ)

2110 FOR S = 1 TO NS

2111    LET CX = IX - S(S,1): LET CY = IY - S(S,2): LET CZ = IZ - S(S,3)
2120    LET B = -2*(CX*LX + CY*LY + CZ*LZ)
2130    LET C = (CX*CX + CY*CY + CZ*CZ) - S(S, 4)

2140    LET D = B*B - 2*A*C
2150    IF D &lt; 0 THEN GO TO 2210
2160    LET D = SQR(D)

2170    LET T = (B + D) / A
2180    IF T &gt; 0.01 THEN LET H = 1: RETURN
2190    LET T = (B - D) / A
2200    IF T &gt; 0.01 THEN LET H = 1: RETURN

2210 NEXT S
2220 LET H = 0: RETURN</pre>
<p>This is called right before computing illumination:</p>
<pre>1600 LET IX = DX*MT: LET IY = DY*MT: LET IZ = DZ*MT
1601 LET NX = IX - S(CS, 1): LET NY = IY - S(CS, 2): LET NZ = IZ - S(CS, 3)
1610 LET PL = AI

1612 GO SUB 2100: IF H = 1 THEN RETURN

1615 LET NL = (NX*LX + NY*LY + NZ*LZ)
1620 IF NL &gt; 0 THEN LET PL = PL + DI * NL / SQR(NX*NX + NY*NY + NZ*NZ)</pre>
<p>And here’s what comes out…</p>
<p><img src="https://gabrielgambetta.com/img/zx-rt-5.png"></p>
<p>Compare with the output of the CGFS raytracer:</p>
<center>
<img src="https://gabrielgambetta.com/img/zx-rt-5-cgfs.png">
</center>
<p>Pretty slow due to the extra computation (back to 17 hours), but definitely worth it!</p>
<p>You can take a look at the <a href="https://gabrielgambetta.com/zx-raytracer-5-src.html">full source code for this iteration</a>.</p>
<h2 id="what-next"><a href="#what-next">What next?</a></h2>
<p>The obvious next step would be to implement reflections. But it would be practically impossible to blend colors together in a meaningful way. So objects would be either fully reflective or not reflective at all, and it would just look weird. Recursion would be an interesting problem: the Spectsum supports it, but because there are no local variables, each recursive call would overwrite the global variables, so I’d have to manage my own stack. Doable, but doesn’t sound worth the effort.</p>
<p>Another axis is performance. I could rewrite the whole thing in assembly and see how fast can I make it go. I could control how much precision I need, so maybe fixed-point math would do it (or a less precise version of <code>SQR</code>). Maybe some other time!</p>
<p>Finally, the attribute clash at object boundaries still bothers me. I have a couple of ideas that might improve the situation, although the limitations of the Spectrum are such that it will never be 100% fixed.</p>
<h2 id="nostalgic-rant"><a href="#nostalgic-rant">Nostalgic rant</a></h2>
<p>This was a fun weekend project. Entirely pointless, but fun!</p>
<p>It was nice to write Sinclair BASIC after 30 years. Even though the language is the same, I’m not – I found myself thinking higher-level concepts and then translating them to BASIC. I don’t know whether this is because modern languages give me a better vocabulary to think in, that I can then translate to BASIC, or because I’m not 10 anymore. Could be both.</p>
<p>Specifically, this program makes judicious use of <code>GO TO</code>, which as everyone knows, it’s Considered Harmful™. Back in the day it was pretty much all we had: no function calls, only subroutines using <code>GO SUB</code>; no <code>WHILE</code> or <code>REPEAT</code>; <code>IF</code> doesn’t have <code>END IF</code>; <code>FOR</code> doesn’t have <code>BREAK</code> or <code>CONTINUE</code> (the keywords exist, but they don’t do what you think they do). So using <code>GO TO</code> is unavoidable. And sure, it can lead to spaghetti code, but it doesn’t <em>have to</em>; my own code here, although admittedly simple, is structured cleanly.</p>
<p>I also missed the immediacy and the simplicity of the environment. No frameworks, no dependencies, barely any abstractions (even multiplication is implemented in software!). The ZX Spectrum was fully <em>knowable</em>. The whole Z80 instruction set, the quirks of the ROM and the ALU, everything fits in your head pretty easily. You could reason about peformance down to the processor cycle level – no caches or pipelines or anything else to make your life difficult. I miss all that. Kids These Days™ will never get to experience an environment like this, and that makes me sad.</p>
<pre>9999 STOP</pre>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Euler Did It, by Ed Sandifer (182 pts)]]></title>
            <link>http://eulerarchive.maa.org/hedi/index.html</link>
            <guid>39118314</guid>
            <pubDate>Wed, 24 Jan 2024 15:17:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://eulerarchive.maa.org/hedi/index.html">http://eulerarchive.maa.org/hedi/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=39118314">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><tbody><tr><td>
<i>How Euler Did It</i> is an online MAA column, written by Ed Sandifer of Western Connecticut State University from 2003 to 2010. Each article examines a specific work or concept developed by Leonhard Euler, with the topics ranging from number theory to geography to fluid mechanics. <p> The Euler Archive, in collaboration with the MAA, hosts the article collection for the <i>How Euler Did It</i> series. The series was published in two volumes, both volumes are available from the MAA: <a href="http://www.maa.org/press/maa-reviews/how-euler-did-it"><i>How Euler Did It</i></a> (2007), <a href="http://www.maa.org/press/books/how-euler-did-even-more"><i>How Euler Did Even More</i></a> (2014). 
</p></td></tr></tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An open-source browser engine written in Rust (117 pts)]]></title>
            <link>https://github.com/servo/servo</link>
            <guid>39118121</guid>
            <pubDate>Wed, 24 Jan 2024 15:02:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/servo/servo">https://github.com/servo/servo</a>, See on <a href="https://news.ycombinator.com/item?id=39118121">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:servo/servo" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="7xhNAMcjhiBzrIEJ5JiFv33eIlMNDjwctDb8u8JmfzII_txI8zSaHYv7yFrI_s-94_lb74UF3kJPpy4ARInkFg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="servo/servo" data-current-org="servo" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=qGnd%2FnGmiK%2Fso6cyV7fqCfFN%2FNMCHjCYCp85q6NRdi%2FDAbnSBXjITaQd7dpjSmi20UyLU1CYvyEj3kW%2FxF%2FhNkL3XKgPZSp6szH2ZxKiyKVId%2FoyHNIUwOEs9TsB3tK%2Bj9jtbnarChUj8obbNp0SCdiBORcBT1lDdrB5LlRoWPtvrmiPxMjxlhlfByOxBPNM6QVAT%2FIw8uSCvsK60o1jgz5IdT%2FwzFwKtBzMMR7TzMXPoviv%2BjUynPdLOMynlsk1oTisvsoqkYfehQa2ExEPyu2T6z%2FR7MKlFUjgcUIi5hgMw4WTezWiLnCf2G2DRps8MQHnqGneWEIbRHyk%2BZZmiV%2BTR9FjUrA790cS%2F%2F5jbyMCGdnmA0SFJV%2BCWJlsrNISeLK8PnGMUQAOx10CXUdGoZTLq8V6nP5y%2F0pvuqmiAxWmAmlYGEeejyOHOVz3ttXtn%2F%2FDWH0xynityXEzHeTCdemoxbE%2FX7ArgvGt%2B%2FmLwg7JInlIWhV0jZ5CAtIqc9Tmc4KuEJtM--9x2o3hdhC%2Bh6i%2FG3--GMT46nNLd64nhXnpaxHdng%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=servo%2Fservo" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/servo/servo&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="ad5ac4302cc4d4223804955452886d0c7f26fed23f6ab31459bdfe9dc1b31fee" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Out of bounds memory access in V8 in Google Chrome prior to 120.0.6099.224 (137 pts)]]></title>
            <link>https://nvd.nist.gov/vuln/detail/CVE-2024-0519</link>
            <guid>39117903</guid>
            <pubDate>Wed, 24 Jan 2024 14:46:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nvd.nist.gov/vuln/detail/CVE-2024-0519">https://nvd.nist.gov/vuln/detail/CVE-2024-0519</a>, See on <a href="https://news.ycombinator.com/item?id=39117903">Hacker News</a></p>
<div id="readability-page-1" class="page"><div colspan="2" id="vulnDetailPanel">

						<h2 data-testid="page-header">
							<i></i><span data-testid="page-header-vuln-id">CVE-2024-0519</span>
							Detail
						</h2>
						<div>

							<div>
								
 
								<h3 id="vulnDescriptionTitle" data-testid="vuln-description-title">Description </h3>
									
								<p data-testid="vuln-description">Out of bounds memory access in V8 in Google Chrome prior to 120.0.6099.224 allowed a remote attacker to potentially exploit heap corruption via a crafted HTML page. (Chromium security severity: High)</p><br>
								
								

								
								

								<div id="vulnCvssPanel" data-testid="vuln-cvss-container">
									<h3 title="CVSS is not a measure of risk">Severity</h3>  
									 
									



									<div id="Vuln3CvssPanel" data-testid="vuln-cvss3-panel">
										
										<p><strong>CVSS 3.x Severity and Metrics:</strong></p><div>
											<br>

											<div>
													<p><img src="https://nvd.nist.gov/site-media/images/NVD_NVD_Stack_Plain.svg" alt="NIST CVSS score">


													</p>
													<p><strong>NIST:</strong>&nbsp;<span data-testid="vuln-cvss3-source-nvd">NVD</span>
													</p>
												</div>
											<div>
												<p><span><strong>Base
														Score:</strong>&nbsp;<span> <a id="Cvss3NistCalculatorAnchor" href="https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?name=CVE-2024-0519&amp;vector=AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H&amp;version=3.1&amp;source=NIST" data-testid="vuln-cvss3-panel-score">8.8 HIGH</a>
												</span></span> 

											</p></div>
											<p><span><strong>Vector:</strong>&nbsp;
													<span data-testid="vuln-cvss3-nist-vector">CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H</span></span>
												 

											</p>


										</div>

										
									
										<div id="cvss3FootNote">
										<p>
										 <span id="cvss3FootNoteSection"><i>NVD Analysts use publicly available information to associate vector strings and CVSS scores. We also display any CVSS information provided within the CVE List from the CNA.</i></span></p><p><i>Note: NVD Analysts have published a CVSS score for this CVE based on publicly available information at the time of analysis. The CNA has not provided a score within the CVE List.</i></p></div>
									</div>


									
								</div>



								<div>
									
									
									
									<div id="vulnHyperlinksPanel">
										<h3>References to Advisories, Solutions, and Tools</h3>
										<p>
											By selecting these links, you will be leaving NIST webspace.
											We have provided these links to other web sites because they
											may have information that would be of interest to you. No
											inferences should be drawn on account of other sites being
											referenced, or not, from this page. There may be other web
											sites that are more appropriate for your purpose. NIST does
											not necessarily endorse the views expressed, or concur with
											the facts presented on these sites. Further, NIST does not
											endorse any commercial products that may be mentioned on
											these sites. Please address comments about this page to <a href="mailto:nvd@nist.gov">nvd@nist.gov</a>.
										</p>


										<table data-testid="vuln-hyperlinks-table">
											<thead>
												<tr>
													<th>Hyperlink</th>
													<th>Resource</th>
												</tr>
											</thead>
											<tbody>

												<tr data-testid="vuln-hyperlinks-row-0">
													<td data-testid="vuln-hyperlinks-link-0"><a href="https://chromereleases.googleblog.com/2024/01/stable-channel-update-for-desktop_16.html" target="_blank">https://chromereleases.googleblog.com/2024/01/stable-channel-update-for-desktop_16.html</a></td>

													<td data-testid="vuln-hyperlinks-resType-0">
														<span>
															<span>Release Notes</span>&nbsp;
														</span>
													</td>
												</tr>

												<tr data-testid="vuln-hyperlinks-row-1">
													<td data-testid="vuln-hyperlinks-link-1"><a href="https://crbug.com/1517354" target="_blank">https://crbug.com/1517354</a></td>

													<td data-testid="vuln-hyperlinks-resType-1">
														<span>
															<span>Permissions Required</span>&nbsp;
														</span>
													</td>
												</tr>

												<tr data-testid="vuln-hyperlinks-row-2">
													<td data-testid="vuln-hyperlinks-link-2"><a href="https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/IIUBRVICICWREJQUVT67RS7E4PVZQ5RS/" target="_blank">https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/IIUBRVICICWREJQUVT67RS7E4PVZQ5RS/</a></td>

													<td data-testid="vuln-hyperlinks-resType-2">
														<span>
															<span>Mailing List</span>&nbsp;
														</span>
													</td>
												</tr>

												<tr data-testid="vuln-hyperlinks-row-3">
													<td data-testid="vuln-hyperlinks-link-3"><a href="https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/TNN4SO5UI3U3Q6ASTVT6WMZ4723FYDLH/" target="_blank">https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/TNN4SO5UI3U3Q6ASTVT6WMZ4723FYDLH/</a></td>

													<td data-testid="vuln-hyperlinks-resType-3">
														<span>
															<span>Mailing List</span>&nbsp;
														</span>
													</td>
												</tr>



											</tbody>
										</table>



									</div>

									<div id="vulnCisaExploit">
										<h3>
											This CVE is in CISA's Known Exploited Vulnerabilities Catalog
										</h3>
										<p>
											Reference <a href="https://www.cisa.gov/binding-operational-directive-22-01">
												CISA's BOD 22-01</a> and <a href="https://cisa.gov/known-exploited-vulnerabilities-catalog">Known Exploited Vulnerabilities Catalog</a> for further guidance and requirements.
										</p>
										<table>
											<thead>
												<tr>
													<th>Vulnerability Name</th>
													<th>Date Added</th>
													<th>Due Date</th>
													<th>Required Action</th>
												</tr>
											</thead>
											<tbody>
												<tr>
													<td>Google Chromium V8 Out-of-Bounds Memory Access Vulnerability</td>
													<td>01/17/2024</td>
													<td>02/07/2024</td>
													<td>Apply mitigations per vendor instructions or discontinue use of the product if mitigations are unavailable.</td>
												</tr>
											</tbody>
										</table>
									</div>

									<div id="vulnTechnicalDetailsDiv" data-testid="vuln-technical-details-container">
										<h3>Weakness Enumeration</h3>



										<table data-testid="vuln-CWEs-table">
											<thead>
												<tr>
													<th>CWE-ID</th>
													<th>CWE Name</th>
													<th>Source</th>
												</tr>
											</thead>
											<tbody>

												<tr data-testid="vuln-CWEs-row-0">
													<td data-testid="vuln-CWEs-link-0">
														<a href="http://cwe.mitre.org/data/definitions/787.html" target="_blank">CWE-787</a>
														
													</td>
													<td data-testid="vuln-CWEs-link-0">Out-of-bounds Write</td>

													<td data-testid="vuln-cwes-assigner-0">
														<span data-testid="vuln-cwes-assigner-0-0">
														<img src="https://nvd.nist.gov/site-media/images/NVD_NVD_Stack_Plain.svg" alt="cwe source acceptance level">
														
														<span>NIST  </span>
														</span>
														
														</td>

												</tr>

											</tbody>
										</table>

									</div>



									<div>
										<h3>
											Known Affected Software Configurations <small> <a id="toggleCpeVersion" href="#vulnConfigurationsArea">Switch
													to CPE 2.2</a></small>
										</h3>
										<div>
	
   		<p id="vulnCpeTreeLoading"><h3>CPEs loading, please wait.</h3></p>
        
        <p id="vulnCpeInfo">
            <small>           
                    <i></i> Denotes Vulnerable Software<br>
                    <a href="mailto:cpe_dictionary@nist.gov">Are we missing a CPE here?  Please let us know</a>.<br>
           
            </small>
        </p>
        
        	
	</div>
									</div>
									
									<div id="vulnChangeHistoryDiv" data-testid="vuln-change-history-container">
										<h3 id="VulnChangeHistorySection">Change History</h3>
										<p><small> 4 change records found <a href="#VulnChangeHistorySection" id="changeHistoryToggle">show changes</a>
										</small></p>
										 
									</div>
								
								</div>
							</div>


							<div>
									<h4>Quick Info</h4>
									<p><strong>CVE Dictionary Entry:</strong><br> <a target="_blank" data-testid="vuln-cve-dictionary-entry" href="https://cve.org/CVERecord?id=CVE-2024-0519">CVE-2024-0519</a><br> <strong>NVD
										Published Date:</strong><br> <span data-testid="vuln-published-on">01/16/2024</span><br> <strong>NVD
										Last Modified:</strong><br> <span data-testid="vuln-last-modified-on">01/22/2024</span><br> <strong>
										Source:</strong><br> <span data-testid="vuln-current-description-source">Chrome</span><br>
								</p></div>

						</div>

					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Deep search of all ML papers (105 pts)]]></title>
            <link>https://app.undermind.ai/home/</link>
            <guid>39117876</guid>
            <pubDate>Wed, 24 Jan 2024 14:44:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://app.undermind.ai/home/">https://app.undermind.ai/home/</a>, See on <a href="https://news.ycombinator.com/item?id=39117876">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    



    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Custom CSS -->
    


    
    
    <p>
        <h2>Deep Scientific Search: Solved</h2>

    </p>
    
    

    <p>
        <h3>Our AI agent finds precisely what you ask for, 10-50x better than Google Scholar</h3>
    </p>
    
    

    

    

    
<!-- <div class="container">
    <div class="stats-highlight">
        <h3>Find 10x more relevant works than you could scrolling Google Scholar for hours.</h3>
        <a href="/static/x.pdf" target="_blank" class="stats-link">Read the Stats</a>
    </div>
</div>

Find 10x more than you could scrolling Google Scholar for hours.<a href="/static/x.pdf" target="_blank">Read the Stats</a>

     -->
    
<!-- Add the carousel showing the univerisy users we have -->
    


    


    
<p>
    <h4>Used by researchers at</h4>
</p>
    





    
    
    
    
    
    <div>
        
        
        

        
        <!-- Panel 2 -->
        <div>
            <div>
                    <h2>Radically Complex Searches</h2>
                    <h4>Tackle <strong><em>complex problems</em></strong> with a single search</h4>
                    <p>Our AI agent <strong><em>actually understands</em></strong> your complex goals and conditions. It also looks deep inside the full text of articles to evaluate them, so you don't have to. 
                    </p>
                </div>
            <div>
                    <h4>Find me:</h4>
                    <blockquote>
                        <p><em>"<strong>experiments</strong> that use <strong>tapered optical fibers</strong>
                                to couple light into a <strong>microfabricated waveguide</strong> in the <strong>visible
                                    spectrum</strong>"</em></p>
                    </blockquote>
                </div>
        </div>
        

        <!-- Panel 2 -->
        <div>
            <div>
                    <h2>Comprehensive Discovery</h2>
                    <h4>Provably find <em><strong>every  paper</strong></em> on your topic</h4>
                    <p>Our agent systematically explores the literature like a human scientist. By tracking this process, we can determine when all the papers have been found.</p>
                </div>
            <div>
                <p><img src="https://app.undermind.ai/static/Pitch_Comprehensiveness.png" alt="Convergence rate of paper discovery">
                </p>
            </div>
        </div>
        
        
        <!-- Panel 2 -->
        <div>
            <div>
                    <h2>Stunning Accuracy</h2>
                    <h4>Key papers are highlighted with <em><strong>~98% accuracy</strong></em></h4>
                    <p>Our system strongly filters out irrelevant results and highlights the precise papers you should focus on, giving a clear explanation for each decision.
                                            </p>
                </div>
            <div>
                <p><img src="https://app.undermind.ai/static/Pitch_Categories.png" alt="How Undermind classifies papers">
                </p>
            </div>
        </div>
        
        
        
    </div>

    
    <div>
        <p>
            <h2>A 10-50x better search experience</h2> <!-- Increased top margin -->
        </p>
    </div>

    <div> <!-- This ensures the column is centered in the row -->
            <p><img src="https://app.undermind.ai/static/Pitch_Comparison.png" alt="Comparing Undermind to traditional search">
            </p>
    </div>
    <div> 
            <p>
                <h5>Our AI agent <strong>understands</strong> your complex topic, systematically searches to gather <strong>all precisely relevant papers</strong> for you, and <strong>explains</strong> its reasoning</h5>
            </p>
    </div>
    
    
    





        

        








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pentagon moves to declassify some secret space programs and technologies (162 pts)]]></title>
            <link>https://www.space.com/pentagon-us-military-declassify-secret-space-programs</link>
            <guid>39117609</guid>
            <pubDate>Wed, 24 Jan 2024 14:19:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.space.com/pentagon-us-military-declassify-secret-space-programs">https://www.space.com/pentagon-us-military-declassify-secret-space-programs</a>, See on <a href="https://news.ycombinator.com/item?id=39117609">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<picture><source type="image/webp" alt="a white rocket blasts off during the day, leaving a streak of fire behind it" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-1920-80.jpg.webp 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD.jpg"><source type="image/jpeg" alt="a white rocket blasts off during the day, leaving a streak of fire behind it" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-1920-80.jpg 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD.jpg"><img src="https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-320-80.jpg" alt="a white rocket blasts off during the day, leaving a streak of fire behind it" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD-1920-80.jpg 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD.jpg"></picture>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/RwZQGGyLFkpmsp5Hm3YbmD.jpg">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span>A United Launch Alliance Delta 4 Heavy lifts off from Vandenberg Space Force Base, California as it launches a spy satellite from California, Sept. 24, 2022.</span>
<span itemprop="copyrightHolder">(Image credit: MATT HARTMAN/AFP via Getty Images)</span>
</figcaption>
</div>

<div id="article-body">
<p>The United States Department of Defense (DoD) wants to declassify more space programs in order to boost the nation's military edge in space.</p><p>As the world's superpowers continue to invest in the <a data-analytics-id="inline-link" href="https://www.space.com/space-force-space-flag-simulated-orbit-combat" data-before-rewrite-localise="https://www.space.com/space-force-space-flag-simulated-orbit-combat">militarization of space</a>, some leaders at the Pentagon believe it's time to declassify some of the secretive space programs in the United States' portfolio. To that end, U.S. Deputy Secretary of Defense Kathleen Hicks recently approved a new policy that will reduce the classification level of some highly secret space programs and technologies.</p><p>The policies that have prohibited sharing this information are outdated and are holding back the U.S. when it comes to <a data-analytics-id="inline-link" href="https://www.space.com/darpa-space-weapons-superiority-technologies" data-before-rewrite-localise="https://www.space.com/darpa-space-weapons-superiority-technologies">superiority in space</a>, according to DoD Assistant Secretary for Space Policy John Plumb. "What the classification memo does, generally, is it overwrites — it really completely rewrites — a legacy document that had its roots 20 years ago, and it's just no longer applicable to the current environment that involves national security space," Plumb said last week, <a data-analytics-id="inline-link" href="https://breakingdefense.com/2024/01/dod-completely-rewrites-classification-policy-for-secret-space-programs/" target="_blank" data-url="https://breakingdefense.com/2024/01/dod-completely-rewrites-classification-policy-for-secret-space-programs/">according to Breaking Defense</a>.</p><p>The policy does not mean that these programs and technologies will now be fully unclassified and revealed to the public; instead, it will lower their classification levels in order to share some technologies and programs with private industry and international allies to help the U.S. build an "asymmetric advantage and force multiplier that neither China nor Russia could ever hope to match," Plumb said in a DoD <a data-analytics-id="inline-link" href="https://www.defense.gov/News/News-Stories/Article/Article/3649094/space-policy-official-details-approach-to-maintaining-us-edge/" target="_blank" data-url="https://www.defense.gov/News/News-Stories/Article/Article/3649094/space-policy-official-details-approach-to-maintaining-us-edge/">statement</a>.&nbsp;</p><p><strong>Related: </strong><a data-analytics-id="inline-link" href="https://www.space.com/space-command-conflict-in-space-deterrence-kathleen-hicks" data-before-rewrite-localise="https://www.space.com/space-command-conflict-in-space-deterrence-kathleen-hicks">Space is now 'most essential' domain for US military, Pentagon says</a></p><p>The move would allow each branch of the U.S. armed services to decide their own classification levels, rather than&nbsp;spread a blanket DoD policy over all military space programs and technologies.&nbsp;</p><p>One of the key issues driving this policy change is the use of what are known as Special Access Programs (SAPs), security protocols that severely restrict the sharing of highly sensitive and classified information. Some of these SAPs are acknowledged, meaning their existence is known to the public but their details haven't been revealed. Others, however, are unacknowledged, meaning their mere existence is even a secret.</p><p>Plumb argued that the new policy will remove SAP status from some of the Pentagon's most valuable space programs, giving the U.S. military an edge in what the Department of Defense <a data-analytics-id="inline-link" href="https://www.space.com/space-command-conflict-in-space-deterrence-kathleen-hicks" data-before-rewrite-localise="https://www.space.com/space-command-conflict-in-space-deterrence-kathleen-hicks">now considers the "most essential domain"</a> in terms of national security.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" alt="A white rocket with side boosters lifts off from a seaside launch pad" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/space/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-1200-80.jpg.webp 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk.jpg"><source type="image/jpeg" alt="A white rocket with side boosters lifts off from a seaside launch pad" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/space/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk.jpg"><img src="https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk.jpg" alt="A white rocket with side boosters lifts off from a seaside launch pad" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/space/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk.jpg" srcset="https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/RKdB7tSV8K28EN8EAJCJBk-1200-80.jpg 1200w"></picture></p></div><figcaption itemprop="caption description"><span>An Atlas V rocket carrying the classified Silent Barker multi-payload satellite for the National Reconnaissance Office and Space Force lifts off from Cape Canaveral Space Force Station, Florida on Sept. 10, 2023. </span><span itemprop="copyrightHolder">(Image credit: United Launch Alliance)</span></figcaption></figure><p>"Anything we can bring from a SAP level to a Top Secret level for example, brings massive value to the warfighter, massive value to the department, and frankly, my hope is over time [it] will also allow us to share more information with allies and partners that they might not currently be able to share," Plumb said.</p><p>Some officials at the Pentagon have been calling for such a new classification policy <a data-analytics-id="inline-link" href="https://transforming-classification.blogs.archives.gov/2021/03/08/space-operations-chief-decries-over-classification-and-its-effect-on-operations/" data-url="https://transforming-classification.blogs.archives.gov/2021/03/08/space-operations-chief-decries-over-classification-and-its-effect-on-operations/">for years</a>, arguing that excessive classification has prevented advanced military capabilities from deterring attacks from adversaries, which is one of the main reasons they were created to begin with.</p><p>In a rare show of disclosure, the <a data-analytics-id="inline-link" href="https://www.space.com/us-space-force-history-mission-capabilities" data-before-rewrite-localise="https://www.space.com/us-space-force-history-mission-capabilities">U.S. Space Force</a> and National Reconnaissance Office revealed a set of general capabilities of the <a data-analytics-id="inline-link" href="https://www.space.com/atlas-v-rocket-silent-barker-watchdog-satellite-space-force-launch" data-before-rewrite-localise="https://www.space.com/atlas-v-rocket-silent-barker-watchdog-satellite-space-force-launch">Silent Barker "watchdog" satellite</a> launched by United Launch Alliance in September 2023.&nbsp;</p><p>Before the launch, NRO and Space Force officials told the public that Silent Barker was designed to keep an eye on <a data-analytics-id="inline-link" href="https://www.space.com/24839-satellites.html" data-before-rewrite-localise="https://www.space.com/24839-satellites.html">satellites</a> and spacecraft in <a data-analytics-id="inline-link" href="https://www.space.com/29222-geosynchronous-orbit.html" data-before-rewrite-localise="https://www.space.com/29222-geosynchronous-orbit.html">geosynchronous orbit</a> (GEO). The disclosure was designed to help deter attacks on U.S. satellites, Space Force Lt. General Michael Guetlein, commander of Space Systems Command, said at the time.</p><p>"Not only are we going to maintain custody and the ability to detect what's going on in GEO, but we'll have the indications and warnings to know there's something out of the normal occurring, and that goes a long way towards deterrence," Guetlein said.</p><p>The exact capabilities and specifications of many of the U.S. military's and intelligence community's satellites remain unknown.</p>
</div>
<p><em><a href="https://forums.space.com/">Join our Space Forums</a> to keep talking space on the latest missions, night sky and more! And if you have a news tip, correction or comment, let us know at: <a href="mailto:community@space.com">community@space.com.</a></em></p>
<div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-fm9BYch4S6HrTvfakNzh3U"><section><p>Breaking space news, the latest updates on rocket launches, skywatching events and more!</p></section></div>
<div id="slice-container-authorBio-fm9BYch4S6HrTvfakNzh3U"><p>Brett is curious about emerging technologies, alternative launch concepts, anti-satellite technologies and uncrewed aircraft systems. Brett's work has appeared on Scientific American, The War Zone, Popular Science, the History Channel, Science Discovery and more. Brett has English degrees from Clemson University and the University of North Carolina at Charlotte. In his free time, Brett enjoys skywatching throughout the dark skies of the Appalachian mountains.</p></div>


</section>




<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientific journal publishers and editors say they are being offered bribes (224 pts)]]></title>
            <link>https://www.science.org/content/blog-post/just-bribe-everyone-it-s-only-scientific-record</link>
            <guid>39117525</guid>
            <pubDate>Wed, 24 Jan 2024 14:11:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/blog-post/just-bribe-everyone-it-s-only-scientific-record">https://www.science.org/content/blog-post/just-bribe-everyone-it-s-only-scientific-record</a>, See on <a href="https://news.ycombinator.com/item?id=39117525">Hacker News</a></p>
Couldn't get https://www.science.org/content/blog-post/just-bribe-everyone-it-s-only-scientific-record: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Security researchers collect awards for Tesla exploits at Pwn2Own Automotive (104 pts)]]></title>
            <link>https://www.bleepingcomputer.com/news/security/tesla-hacked-24-zero-days-demoed-at-pwn2own-automotive-2024/</link>
            <guid>39117304</guid>
            <pubDate>Wed, 24 Jan 2024 13:49:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bleepingcomputer.com/news/security/tesla-hacked-24-zero-days-demoed-at-pwn2own-automotive-2024/">https://www.bleepingcomputer.com/news/security/tesla-hacked-24-zero-days-demoed-at-pwn2own-automotive-2024/</a>, See on <a href="https://news.ycombinator.com/item?id=39117304">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="Pwn2Own Tokyo Japan" height="900" src="https://www.bleepstatic.com/content/hl-images/2024/01/24/Pwn2Own_Tokyo-headpic.jpg" width="1600"></p>
<p>Security researchers hacked a Tesla Modem and collected awards of $722,500 on the first day of Pwn2Own Automotive 2024 for three bug collisions and 24 unique zero-day exploits.</p>
<p>Synacktiv Team (<a href="https://twitter.com/Synacktiv" target="_blank" rel="nofollow noopener">@Synacktiv</a>) took home $100,000 after successfully chaining three zero-day bugs <a href="https://twitter.com/thezdi/status/1750009207724916989" target="_blank" rel="nofollow noopener">to get root permissions on a Tesla Modem</a>.</p>
<p>They also used two unique two-bug chains to hack a <a href="https://twitter.com/thezdi/status/1750040225320202441" target="_blank" rel="nofollow noopener">Ubiquiti Connect EV Station</a> and a JuiceBox 40 Smart EV Charging Station, earning an additional $120,000.</p>
<p>A third exploit chain targeting the <a href="https://twitter.com/thezdi/status/1750074555266404374" target="_blank" rel="nofollow noopener">ChargePoint Home Flex EV charger</a> was already known but still brought them $16,000 in cash, with a total of $295,000 in prizes during the first day of the contest.</p>
<p>Security researchers also successfully hacked multiple fully patched EV charging stations and infotainment systems, with the NCC Group EDG team taking the second place on the leaderboard after winning $70,000 for zero-days exploited to hack the <a href="https://twitter.com/thezdi/status/1750032135363383725" target="_blank" rel="nofollow noopener">Pioneer DMH-WT7600NEX infotainment system</a> and the <a href="https://twitter.com/thezdi/status/1750092629885460966" target="_blank" rel="nofollow noopener">Phoenix Contact CHARX SEC-3100 EV charger</a>.</p>
<p>After the zero-day bugs are exploited and reported during the Pwn2Own competition, vendors have 90 days to develop and release security fixes before TrendMicro's Zero Day Initiative publicly discloses them.</p>
<div>
<figure><img alt="Pwn2Own rankings after first day" height="382" src="https://www.bleepstatic.com/images/news/u/1109292/2024/PWn2Own%20rankings%20after%20first%20day.jpg" width="680"><figcaption><em>Leaderboard after the first day of Pwn2Own Automotive</em></figcaption></figure></div>
<p>​The Pwn2Own Automotive 2024 hacking contest focuses on automotive technologies and takes place this week in Tokyo, Japan, during the <a href="https://www.automotiveworld.jp/tokyo/en-gb.html" target="_blank" rel="nofollow noopener">Automotive World</a> auto conference between January 24 and January 26.</p>
<p>Throughout the competition, security researchers will be able to target Tesla in-vehicle infotainment (IVI) systems, electric vehicle (EV) chargers, and car operating systems (i.e., Automotive Grade Linux, BlackBerry QNX, Android Automotive OS).</p>
<p>They'll also demo zero-day exploits targeting Tesla Model 3/Y (Ryzen-based) or Tesla Model S/X (Ryzen-based) systems, including the infotainment system, modem, tuner, wireless, and autopilot.</p>
<p>The top prize will be awarded for VCSEC, gateway, or autopilot zero-days, with a cash award of $200,000 and a Tesla car.</p>
<p>You can find the complete schedule of this year's automotive hacking contest <a href="https://www.zerodayinitiative.com/blog/2024/1/23/pwn2own-automotive-2024-the-full-schedule" target="_blank" rel="nofollow noopener">here</a>. The full schedule for the first day and the results for each challenge are available <a href="https://www.zerodayinitiative.com/blog/2024/1/24/pwn2own-automotive-2024-day-one-results" target="_blank" rel="nofollow noopener">here</a>.</p>
<p>During <a href="https://www.bleepingcomputer.com/news/security/hackers-earn-1-035-000-for-27-zero-days-exploited-at-pwn2own-vancouver/" target="_blank">the Pwn2Own Vancouver 2023 competition</a> in March, security researchers earned $1,035,000 and a Tesla Model 3 car after demoing 27 zero-day (and several bug collisions).</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Duplicity: Encrypted bandwidth-efficient backup (122 pts)]]></title>
            <link>https://duplicity.us/</link>
            <guid>39117155</guid>
            <pubDate>Wed, 24 Jan 2024 13:37:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://duplicity.us/">https://duplicity.us/</a>, See on <a href="https://news.ycombinator.com/item?id=39117155">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
<h2 id="big-title"><a name="ToC2">duplicity</a></h2>
<p>Encrypted bandwidth-efficient backup using the rsync algorithm</p>
<h2><a name="ToC3">What is it?</a></h2>
<p>Duplicity backs directories by producing encrypted tar-format
volumes and uploading them to a remote or local file server. Because
duplicity uses
<a href="https://github.com/librsync/librsync">librsync</a>, the
incremental archives are space efficient and only record the parts of
files that have changed since the last backup. Because duplicity uses
<a href="http://www.gnupg.org/">GnuPG</a> to encrypt and/or sign these
archives, they will be safe from spying and/or modification by the
server.</p>
<p>The duplicity package also includes the <strong>rdiffdir</strong>
utility. Rdiffdir is an extension of librsync's
<strong>rdiff</strong> to directories---it can be used to produce
signatures and deltas of directories as well as regular files. These
signatures and deltas are in GNU tar format.</p>
<h2><a name="ToC4">Current development status</a></h2>
<p>Duplicity is fairly mature software. As with any software, it may still have a few bugs, but
will work for normal usage and is in use now for large personal and corporate backups.
If you have questions try the
<a href="http://lists.nongnu.org/mailman/listinfo/duplicity-talk">mailing list</a>.
Bug reports and bug fixes can be entered through the
<a href="https://gitlab.com/duplicity/duplicity"><b>duplicity's GitLab project and git repository</b></a>.
</p><p>In theory many protocols for connecting to a file server could be
supported; so far
</p><ul>
<li>Amazon S3</li>
<li>Backblaze B2</li>
<li>DropBox</li>
<li>ftp</li>
<li>GIO</li>
<li>Google Docs</li>
<li>Google Drive</li>
<li>HSI</li>
<li>Hubic</li>
<li>IMAP</li>
<li>local filesystem</li>
<li>Mega.co</li>
<li>Microsoft Azure</li>
<li>Microsoft Onedrive</li>
<li>par2</li>
<li>Rackspace Cloudfiles</li>
<li>rclone</li>
<li>rsync</li>
<li>Skylabel</li>
<li>ssh/scp</li>
<li>SwiftStack</li>
<li>Tahoe-LAFS</li>
<li>WebDAV</li>
</ul><p>
have been written. Currently duplicity supports deleted files,
full unix permissions, directories, and symbolic links, fifos, and
device files, but not hard links.</p>
<h2><a name="ToC5">Download</a></h2>
<p>The current release is <strong>2.1.5</strong>, released December 28, 2023.</p>
<ul>
<li>See: <a href="https://gitlab.com/duplicity/duplicity/-/releases/rel.2.1.5">Duplicity-2.1.5 Release</a> for details
and download link</li>
<li>Refer to the <a href="https://duplicity.us/stable/CHANGELOG.html">Changelog</a> for the gory details from git.</li>
</ul>
<p><strong>Warning:</strong> If you are upgrading from your distribution's repository to the tarball version,
or from the tarball version to your distribution's version,
please be sure to remove or purge the distribution's version of duplicity. Failure to do so may result in
confusing results since the repository and tarball versions may install in different locations.</p><p>Older versions are also available for the budding historians in the
<a href="https://launchpad.net/duplicity/+download">downloads area</a>.</p>
<p>There are multiple ways of downloading and installing duplicity:</p>
<ul>
<li>Stable snap builds - sudo snap install duplicity --classic</li>
<li>Latest snap builds - sudo snap install duplicity --classic --edge</li>
<li>Stable pip3 builds - sudo pip3 install duplicity</li>
<li>Latest pip3 builds - sudo pip3 install -pre duplicity</li>
<li>Stable duplicity PPA - <a href="https://code.launchpad.net/~duplicity-team/+archive/ubuntu/duplicity-release-git">duplicity-release</a></li>
<li>Latest duplicity PPA - <a href="https://code.launchpad.net/~duplicity-team/+archive/ubuntu/duplicity-develop-git">duplicity-develop</a></li>
<li>Stable repo - git clone --branch main https://gitlab.com/duplicity/duplicity.git
</li><li>Latest repo - git clone --branch dev https://gitlab.com/duplicity/duplicity.git
</li></ul>
<p><a href="mailto:mike@mterry.name">Michael Terry</a>
maintains and releases deja-dup <a href="https://wiki.gnome.org/Apps/DejaDup/Download">here</a>.</p>
<p><a href="mailto:edso@users.sourceforge.net">Edgar Soldin</a>
maintains <a href="http://duply.net/">duply</a> (formerly known as ftplicity), a shell front end that simplifies the usage by keeping settings for backup jobs in profiles.
It supports pre/post backup scripts and executing multiple commands in a batch mode.</p>
<p>All the code here is GPL'ed (free software). Duplicity is also part of the
<a href="http://fedoraproject.org/">Fedora</a>,
<a href="http://www.debian.org/">Debian</a>, and
<a href="http://www.ubuntu.com/">Ubuntu</a>
distributions of GNU/Linux.</p>
<h2><a name="ToC6">Requirements</a></h2>
<p>Duplicity requires a POSIX-like operating system. It is best used
under GNU/Linux. Refer to the <a href="https://duplicity.us/stable/README.html">README</a> for the full list of
requirements.</p>
</div></div>]]></description>
        </item>
    </channel>
</rss>