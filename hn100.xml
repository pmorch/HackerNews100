<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 07 Mar 2025 10:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Differentiable Logic Cellular Automata (243 pts)]]></title>
            <link>https://google-research.github.io/self-organising-systems/difflogic-ca/?hn</link>
            <guid>43286161</guid>
            <pubDate>Thu, 06 Mar 2025 23:43:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://google-research.github.io/self-organising-systems/difflogic-ca/?hn">https://google-research.github.io/self-organising-systems/difflogic-ca/?hn</a>, See on <a href="https://news.ycombinator.com/item?id=43286161">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><hr id="1a00a320-00ae-80de-a29e-dbb965692ffb"><div id="1a00a320-00ae-80af-b579-fb795f51bafd"><p id="1a00a320-00ae-8011-b4ba-c28643baceb5">AFFILIATIONS:</p><p id="1a00a320-00ae-805b-85c7-f25884be9e9d">Google, Paradigms of Intelligence Team</p></div><hr id="1a00a320-00ae-80bc-abd0-c2cdd760cf39"><p id="1a00a320-00ae-80bb-a273-e899edfd6cf5">Imagine trying to reverse-engineer the complex, often unexpected patterns and behaviors that emerge from simple rules. This challenge has inspired researchers and enthusiasts that work with cellular automata for decades.  In cellular automata, we generally approach things from the bottom-up. We choose local rules, then investigate the resulting emergent patterns. What if we could create systems that, given some complex desired pattern, can, in a fully differentiable fashion, learn the local rules that generate it, while preserving the inherent discrete nature of cellular automata? This is what we'll  explore with you today.</p><div id="17c0a320-00ae-8050-84d9-eed83c749260"><div id="9f03a6f1-88e6-40f7-addb-635001c9dac0"><figure id="17c0a320-00ae-8051-b45b-f1e017e41857"><figcaption>Zoomed view of learned circuit</figcaption></figure></div><div id="078f6020-e34b-49ff-ac61-550d1722c0df"><figure id="17c0a320-00ae-80cd-a191-f479ea5492f4"><a href="https://google-research.github.io/self-organising-systems/difflogic-ca/images/google_logo.gif"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/google_logo.gif"></a><figcaption>"G" being generated by learned circuit</figcaption></figure></div></div><p id="1910a320-00ae-80c4-a7d4-ee00e98e6b47">Prior work has explored learning transition rules using non-differentiable techniques, demonstrating the feasibility of evolving local rules for specific computation<d-cite key="Mitchell1994-mi"></d-cite>. Likewise prior exploration of making one-dimensional cellular automata differentiable exist<d-cite key="Martin2017-dx"></d-cite>. We propose a novel, fully end-to-end differentiable approach, combining two interesting concepts from the world of artificial intelligence: Neural Cellular Automata (NCA) <d-cite key="Mordvintsev2020-oh"></d-cite> and Differentiable Logic Gates Networks <d-cite key="Petersen2022-ai"></d-cite><d-cite key="Petersen2024-rr"></d-cite>. NCA exhibit the ability to learn arbitrary patterns and behaviors, however, they do not inherently operate within a discrete state space. This makes interpretability more challenging, and leaves them stuck in a regime where current hardware must perform costly matrix multiplications to gradually update their continuous internal states. Differentiable Logic Gates Networks, meanwhile, have been used to discover combinatorial logic circuits, blending discrete states with differentiable training signals. But they haven't yet been shown to work  in recurrent settings. NCA, as it were, are recurrent in both space, and time.  Sounds intriguing, right?</p><p id="17b0a320-00ae-80b3-8af8-f8d7b98ddb34">Zooming out, we believe the integration of differentiable logic gates and neural cellular automata is a potential step towards programmable matter - <em>Computronium</em>  <d-cite key="Amato1991-ck"></d-cite> - a theoretical physical substance capable of performing arbitrary computation. Toffoli and Margolus pioneered this direction with  CAM-8, a cellular automata based computing architecture <d-cite key="Margolus1995-hg"></d-cite><d-cite key="Toffoli1991-do"></d-cite>, in theory capable of immense, horizontally scalable computation. However, they faced a fundamental challenge: actually crafting the local rules needed to achieve a desired macroscopic computation, with Amato et al. noting that “other researchers [...] still worry about the difficulty of finding local rules that correspond to real natural systems” <d-cite key="Amato1991-ck"></d-cite>.  What if we could directly learn these local rules, and create models that combine binary logic, the flexibility of neural networks, and the local processing of cellular automata?  We believe our prototypes  offer a glimpse into the future of computing: learnable, local, and discrete.</p><p id="17b0a320-00ae-8046-8ed4-e7cb4deb6da4">This article will walk you through implementing cellular automata using differentiable logic gates, and demonstrate some key results along the way.</p><p id="1820a320-00ae-8036-b527-f6da83547f0a">We're faced with two fundamental questions.<em>                                      </em> </p><figure id="1820a320-00ae-800f-a24a-ec0ee8f611ec"><p><span>❓</span></p><p id="1820a320-00ae-80b1-9312-cf2feaaf9f38"><em>Can a Differentiable Logic CA learn at all?   </em></p></figure><p id="1820a320-00ae-804b-bef7-d0f44f912fad">To answer this, we'll start by attacking Conway's Game of Life - perhaps the most iconic cellular automata,  having captivated researchers for decades. While this first experiment might seem overly simplistic (functionally equivalent to learning a truth table), it will prove the basic learning capability of our setup. The more profound question follows: </p><figure id="1820a320-00ae-806a-8d96-d17225e588f1"><p><span>❓</span></p><p id="1820a320-00ae-8061-9b8f-c36fa2937fc1"><em>Can recurrent-in-space and recurrent-in-time circuits learn complex patterns similar to those generated by traditional NCAs? </em></p></figure><p id="1990a320-00ae-804e-a1ae-fd0f1eca6c2c">While both Differentiable Logic Gate Networks and Neural Cellular Automata (NCAs) have demonstrated trainability, effectively training circuits that exhibit both temporal and spatial recurrence of NCA, within the framework of differentiable logic, remains unexplored. </p><p id="1990a320-00ae-80ec-830f-c68734ecb12e">The second experiment will demonstrate the model's ability to learn recurrent circuits that generate complex patterns similar to the ones generated by traditional NCA. </p><h2 id="17b0a320-00ae-80e7-8662-c6ca0699315a">Recap - Neural Cellular Automata</h2><p id="17b0a320-00ae-8094-8353-cbf1959b13bb">At the heart of this project lies Neural Cellular Automata (NCA), a synthesis of classical cellular automata with modern deep learning techniques. This powerful paradigm, pioneered by Mordvintsev et al. <d-cite key="Mordvintsev2020-oh"></d-cite>, represents a fundamental shift in how we think about computational systems that can grow, adapt, and self-organize.</p><p id="17b0a320-00ae-8058-837d-d64466738811">Traditional cellular automata have long captivated researchers with their ability to generate complex behaviors from simple, local rules. Neural Cellular Automata take this concept further by making these rules learnable through gradient descent. Instead of hand-designing update rules, the system discovers them automatically, opening up entirely new possibilities for self-organizing computational systems.</p><p id="17b0a320-00ae-80ce-a251-c52dd6986d31">What makes this approach particularly elegant is how it preserves the core principles of cellular automata - locality, parallelism, and state-based computation - while introducing the adaptability of neural networks. </p><p id="17b0a320-00ae-802b-b083-ddbd44add2b7">In the following sections, we will summarize the main concepts from the “<strong>Growing Neural Cellular Automata”</strong> work <d-cite key="Mordvintsev2020-oh"></d-cite>, which presents a Neural Cellular Automata developed for morphogenesis. If you are already familiar with it, feel free to skip it.</p><h3 id="17b0a320-00ae-8015-91a4-e90b65fe9a6c">The Structure: A 2D Grid of Intelligent Cells</h3><p id="17b0a320-00ae-80ef-923a-d9cc324efe92">At the heart of the system is a 2D grid, much like classic cellular automata. Each cell contains an <strong>n-dimensional vector</strong> of information which is called the cell's <strong>state (or channels)</strong> and for the specific case of Growing-NCA it is composed by these elements:</p><ul id="17b0a320-00ae-8070-92fb-c1b3f696df56"><li><strong>RGB Colors</strong> (3 channels): These represent the visible properties of the cell, essentially its color.</li></ul><ul id="17b0a320-00ae-80ab-bdc9-f15b1fe22d1d"><li><strong>Alpha (α) Channel</strong> (1 channel): This indicates cell vitality. If the alpha value is greater than 0.1, the cell is considered “alive.”</li></ul><ul id="17b0a320-00ae-8001-88e8-eb07b7d7db40"><li><strong>Hidden Channels</strong> (n minus 4 channels): These allow cells to communicate more complex information about their environment, making interactions richer and more dynamic.</li></ul><p id="17b0a320-00ae-8053-886b-ddf0aadc026e">But the magic doesn’t stop here. What really sets this system apart is how the cells interact and evolve through a two-stage process.</p><h3 id="17b0a320-00ae-80cb-9108-d89841539c77">The Two-Stage Update Mechanism: Perception and Update</h3><ol type="1" id="17b0a320-00ae-804c-a4a8-d1b14e54e541" start="1"><li><strong>The Perception Stage</strong><p id="17b0a320-00ae-80db-bdc0-f7258a4a6278">In the first stage, each cell perceives its environment. Think of it as a cell sensing the world around it. To do this, it uses <strong>Sobel filters</strong>, mathematical tools designed to numerically approximate spatial gradients - "changes across its surroundings". The filters are applied channel-wise, and the result is termed the <strong>perception vector</strong>, which combines the cell’s current state with the information it gathers about its environment. A bit like how biological cells use chemical gradients to sense and react to their surroundings.</p></li></ol><ol type="1" id="17b0a320-00ae-8088-acd2-c653be5101a3" start="2"><li><strong>The Update Stage</strong><p id="17b0a320-00ae-80de-9206-f48a8894d617">Next, the neural network steps in. Each cell uses its perception vector as input to a neural network, which performs identical operations on every cell in the grid. Using around ~<strong>8,000 parameters</strong>, the neural network determines how each cell should change based on the information it has gathered. It’s here that the system evolves, with the cells adapting and responding to environmental changes.</p><figure id="17b0a320-00ae-8098-80a0-e69315e4c00b"><a href="https://google-research.github.io/self-organising-systems/difflogic-ca/images/model.svg"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/model.svg"></a><figcaption>Learning process for Growing NCA, image by Mordvintsev et al.<d-cite key="Mordvintsev2020-oh"></d-cite> </figcaption></figure></li></ol><h3 id="17b0a320-00ae-80d5-9c81-ca1b38c6eef5">The Power of Differentiability</h3><p id="17b0a320-00ae-803f-b0f6-e2eb73004110">What makes this system truly powerful is its <strong>differentiability</strong>. Every operation, from perceiving the environment to updating its state, is fully differentiable. This means we can optimize the entire system through <strong>gradient descent</strong>, just like how neural networks learn from data. As a result, the system isn’t statically pre-defined with some arbitrary rules — it can actually <strong>learn</strong> to grow specific patterns or behaviors, making it a powerful tool for modeling complex systems.</p><figure id="1910a320-00ae-8027-a1a6-db5e3610fa63"><a href="https://google-research.github.io/self-organising-systems/difflogic-ca/images/regen2.gif"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/regen2.gif"></a><figcaption>NCA Growing process, credit to <d-cite key="Mordvintsev2020-oh"></d-cite> </figcaption></figure><p id="17b0a320-00ae-80c0-98fe-e15f2925d588">While the individual components of the system (like Sobel filters and neural networks) are relatively simple, their combination creates something much more sophisticated. It’s a balance between simplicity and complexity, much like biological systems in nature, where local interactions lead to the emergence of surprising, intricate behaviors.</p><p id="17b0a320-00ae-8054-80ce-d80770602c35">This approach doesn’t just push the boundaries of what cellular automata can do, it opens up a world of possibilities for learning, growth, and pattern formation through local interactions alone. Whether you’re a researcher, a developer, or simply someone fascinated by the intersection of AI and complexity, there’s a lot to explore here.  </p><p id="17b0a320-00ae-800a-87f3-f2bfda7a56e3">Other applications of Neural Cellular Automata include Image Segmentation <d-cite key="Sandler2020-nx"></d-cite>, Image classification <d-cite key="Randazzo2020-mh"></d-cite> and many more. </p><div id="17c0a320-00ae-8082-9fe2-d7ba3a948c55"><p id="d21df3d1-8bd1-4991-a135-4d7a1ff782ee"><h2 id="3c1f6714-1e60-4811-a242-1890db2faa19">Recap - Differentiable Logic Gate Networks</h2></p></div><p id="17b0a320-00ae-8032-afd2-d1679ae2ad41">What if we could take the basic building blocks of computation (logic gates like AND, OR, and XOR) and combine them in a learned fashion, to solve some task ? That's exactly what <strong>Deep Differentiable Logic Gate Networks</strong> (DLGNs) achieve, merging the efficiency of digital circuits with the power of machine learning. This approach, developed by Petersen et al. <d-cite key="Petersen2022-ai"></d-cite><d-cite key="Petersen2024-rr"></d-cite>, opens up exciting possibilities, especially in resource-constrained environments like edge computing and embedded systems. </p><figure id="17c0a320-00ae-80e3-9e65-fe8301c314b1"><a href="https://google-research.github.io/self-organising-systems/difflogic-ca/images/diffLogicGateNetwork.png"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/diffLogicGateNetwork.png"></a><figcaption>Convolution Differentiable Logic Gate Network, image by Petersen et al.<d-cite key="Petersen2024-rr"></d-cite></figcaption></figure><h3 id="17b0a320-00ae-80a9-8959-fd52f3a68f89"><strong>How Do Deep Differentiable Logic Gate Networks Work?</strong></h3><h3 id="17b0a320-00ae-803b-ab43-df550941a8e8"><strong>Logic Gates as Neurons</strong></h3><p id="17b0a320-00ae-807a-ada3-f42de63afc1d">At their core, DLGNs use <strong>logic gates</strong> as their building blocks, instead of the traditional artificial neurons found in neural networks. Each node in this case is a logic gate, and instead of performing weighted sums and matrix multiplications, each gate performs simple operations like <strong>AND, OR, XOR</strong>, etc.</p><h3 id="17b0a320-00ae-80fd-b4a9-d40ad5d9e6dc"><strong>The Architecture: </strong></h3><p id="17b0a320-00ae-802a-9d09-ea4fc259be20">The architecture of a DLGN is surprisingly simple:</p><ul id="17b0a320-00ae-80e0-88e6-f540f2557e3c"><li>The network is composed of <strong>layers of gates</strong>. Each gate takes inputs from two gates in the previous layer, resulting in a naturally <strong>sparse </strong>network.</li></ul><ul id="17b0a320-00ae-8051-9e01-d3a37f22065d"><li>The <strong>connections</strong> between gates are <strong>fixed</strong>; they are randomly initialized but do not change during training. The learning process determines what each gate does, not the connections between gates.</li></ul><ul id="17b0a320-00ae-80a2-98b4-d4d6224c522e"><li><strong>During inference</strong>, each gate performs one simple binary operation (think AND or OR) based on the operation it learned. </li></ul><h3 id="17b0a320-00ae-80ea-8efd-f37c705c5ee8"><strong>The Learning Process: Making Discrete Operations Differentiable</strong></h3><p id="17c0a320-00ae-8005-a9e7-d933d0ca3d1a">Instead of learning weights as traditional neural networks do, this network <strong>learns which logic operation each gate should perform</strong>. During training, each node solves a classification task to identify the correct gate to use in order to minimize the objective function.</p><p id="1a00a320-00ae-8075-958f-fad7dc45a4ba">The challenge is that logic gates are inherently <strong>discrete</strong> and <strong>non-differentiable</strong>, making them unsuitable for gradient-based learning. So how do we make them learn? Through two key tricks:</p><ol type="1" id="17b0a320-00ae-80cc-b278-ca1bb0950836" start="1"><li><strong>Continuous Logic Operations</strong><p id="17b0a320-00ae-8006-99e9-e4703974fd0c">During training, each logic operation is replaced by a continuous relaxation, which is a <strong>differentiable version</strong> that operates on continuous values between 0 and 1. For example, instead of a <em>hard</em> AND gate that only accepts 0 or 1, we use a <em>soft</em> AND gate that can handle values between 0 and 1 as inputs, and passes a continuous mix of the two inputs as its output. These continuous relaxations (listed below) allow us to train the network using <strong>gradient descent.</strong></p></li></ol><ol type="1" id="17b0a320-00ae-8058-bcc0-e1969485202e" start="2"><li><strong>Probabilistic Gate Selection</strong><p id="17b0a320-00ae-8038-aecd-d59a11a90eb6">Each gate maintains a <strong>probability distribution</strong> over the 16 possible binary operations for two inputs. This distribution is represented by a 16-dimensional parameter vector, which is then transformed into a probability distribution using <em>softmax</em>. The values of the 16-dimensional vector are modified during the training process: over time, the gate <em>learns</em> to prefer one operation over others.</p></li></ol><table id="17c0a320-00ae-807b-96a5-fc5da4beace8"><tbody><tr id="17c0a320-00ae-8019-b393-cb5454de12fe"><td id=">zdq">Index</td><td id="AewD">Operation</td><td id="yM_;">Continuous Relaxation</td><td id="i>w[">Symbol</td></tr><tr id="17c0a320-00ae-8051-a8a6-ea87f18d390e"><td id=">zdq">0</td><td id="AewD">FALSE</td><td id="yM_;">0</td><td id="i>w["><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/FALSE.png"></td></tr><tr id="17c0a320-00ae-80fc-bd4d-efa3309483f2"><td id=">zdq">1</td><td id="AewD">AND</td><td id="yM_;">a * b</td><td id="i>w["><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/AND.png"></td></tr><tr id="17c0a320-00ae-80be-bc07-e0ee2bce83cb"><td id=">zdq">2</td><td id="AewD">A AND (NOT B)</td><td id="yM_;">a - a*b</td><td id="i>w["><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/AANDNOTB.png"></td></tr><tr id="17c0a320-00ae-8025-9ee0-fea1a0311050"><td id=">zdq">3</td><td id="AewD">A</td><td id="yM_;">a</td><td id="i>w["><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/A.png"></td></tr><tr id="17c0a320-00ae-80f8-b4de-dec5453e190f"><td id=">zdq">4</td><td id="AewD">(NOT A) AND B</td><td id="yM_;">b - a*b</td><td id="i>w["><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/NOTAANDB.png"></td></tr><tr id="17c0a320-00ae-802e-8c3f-f80d00c69053"><td id=">zdq">5</td><td id="AewD">B</td><td id="yM_;">b</td><td id="i>w["><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/B.png"></td></tr><tr id="17c0a320-00ae-80f5-9a84-ed1adc3a7614"><td id=">zdq">6</td><td id="AewD">XOR</td><td id="yM_;">a + b - 2a<em>*</em>b</td><td id="i>w["><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/XOR.png"></td></tr><tr id="17c0a320-00ae-80bd-a29f-ea6d14a536c7"><td id=">zdq">7</td><td id="AewD">OR</td><td id="yM_;">a + b - a*b</td><td id="i>w["><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/OR.png"></td></tr><tr id="17c0a320-00ae-80eb-b6f9-e952034333bc"><td id=">zdq">8</td><td id="AewD">NOR</td><td id="yM_;">1 - (a + b - a*b)</td><td id="i>w["><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/NOR.png"></td></tr><tr id="17c0a320-00ae-802f-a05e-ea21aded15f3"><td id=">zdq">9</td><td id="AewD">XNOR</td><td id="yM_;">1 - (a + b - 2a*b)</td><td id="i>w["><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/XNOR.png"></td></tr><tr id="17c0a320-00ae-80b3-a49c-ebe99a8723f6"><td id=">zdq">10</td><td id="AewD">NOT B</td><td id="yM_;">1 - b</td><td id="i>w["><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/NOTB.png"></td></tr><tr id="17c0a320-00ae-80f8-9123-def2f0786f18"><td id=">zdq">11</td><td id="AewD">A OR (NOT B)</td><td id="yM_;">1 - b + a*b</td><td id="i>w["><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/AORNOTB.png"></td></tr><tr id="17c0a320-00ae-8045-9de1-cfd5a5c32556"><td id=">zdq">12</td><td id="AewD">NOT A</td><td id="yM_;">1 - a</td><td id="i>w["><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/NOTA.png"></td></tr><tr id="17c0a320-00ae-80dc-970c-c24859f19813"><td id=">zdq">13</td><td id="AewD">(NOT A) OR B</td><td id="yM_;">1 - a + a*b</td><td id="i>w["><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/NOTAORB.png"></td></tr><tr id="17c0a320-00ae-8048-aa3f-f188631922c8"><td id=">zdq">14</td><td id="AewD">NAND</td><td id="yM_;">1 - a*b</td><td id="i>w["><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/NAND.png"></td></tr><tr id="17c0a320-00ae-80d2-8d65-ec7ff8d46314"><td id=">zdq">15</td><td id="AewD">TRUE</td><td id="yM_;">1</td><td id="i>w["><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/TRUE.png"></td></tr></tbody></table><p id="17c0a320-00ae-8070-b380-c442ec22ce0a">During <strong>training</strong>, the network uses the continuous relaxations of the logic operations, but once the network is trained, we switch to <strong>pure binary operations</strong> for lightning-fast inference.</p><div><figure id="17c0a320-00ae-807d-a43c-f2e551e9acf8"><a href="https://google-research.github.io/self-organising-systems/difflogic-ca/images/ed21e753-3109-4d2d-9df2-9c4d7a037e61.png"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/ed21e753-3109-4d2d-9df2-9c4d7a037e61.png"></a><figcaption>Sketch illustrating the training of a single Gate</figcaption></figure></div><p id="17b0a320-00ae-80d2-8e9c-c28c10d4b78e">To facilitate training stability, the initial distribution of gates is biased toward the <em>pass-through</em> gate.</p><h3 id="17b0a320-00ae-801d-a985-f6758054bec4"><strong>Training: </strong><strong>L</strong><strong>earning the Gates</strong></h3><p id="17b0a320-00ae-80c1-864f-fa24481561eb">The training process follows a standard forward-backward pass:</p><ol type="1" id="17b0a320-00ae-80f4-9cbb-f210c1ed6a82" start="1"><li><strong>Forward Pass</strong><ul id="17b0a320-00ae-8081-8f58-ee7638036e4c"><li>The input values propagate through the network.</li></ul><ul id="17b0a320-00ae-80bd-8e52-cf1e8133140f"><li>Each gate, given two inputs, computes the results of all 16 possible logic operations using their continuous relaxations.</li></ul><ul id="17b0a320-00ae-803a-94f0-e1c705058759"><li>These results are weighted according to the gate’s probability distribution, and the <strong>weighted sum</strong> becomes the output of the gate.</li></ul></li></ol><ol type="1" id="17b0a320-00ae-8061-b44d-f961bceae50a" start="2"><li><strong>Backward Pass</strong><ul id="17b0a320-00ae-80d2-8ffe-e3e960d4d715"><li>The network computes the <strong>gradients</strong> with respect to the probability distributions, which are then updated using <strong>gradient descent</strong>.</li></ul><ul id="17b0a320-00ae-8050-aed2-d26278b130aa"><li>Over time, each gate’s distribution becomes more squeezed and spontaneously converge on one operation, whether it’s AND, OR, XOR, or another.</li></ul></li></ol><h3 id="17b0a320-00ae-8043-8fec-f86cfb47debb"><strong>Inference: The Magic of Binary Operations</strong></h3><p id="17b0a320-00ae-80c9-a69a-d6e102d0e685">Once training is complete, we can freeze the network. This means that each gate <strong>settles on its most probable operation</strong>, and the continuous versions of the logic operations are discarded. What’s left is a <strong>pure logic circuit</strong> that operates on binary values (0 or 1).</p><p id="17b0a320-00ae-80fb-809b-c3220e533e6f">This final form is incredibly efficient. When it’s time to deploy, the network runs using only <strong>binary operations</strong>, making it exceptionally fast on any hardware.</p><h2 id="17b0a320-00ae-8021-8f9d-eb9f303ac034">Differentiable Logic Cellular Automata</h2><p id="17c0a320-00ae-808e-8573-fdd175f0ac57">The integration of differentiable logic gate networks with neural cellular automata provides a solution for handling discrete states while maintaining differentiability. </p><p id="1a80a320-00ae-804b-abf1-c0ba0aaf46a6">Let's explore this system in depth, examining how it differs from traditional Neural Cellular Automata, while highlighting their common principles and understanding the fundamental role of differentiable logic gates. We'll borrow the terminology of NCA stages, highlighting where our model differs.</p><h3 id="17b0a320-00ae-80d8-853f-e6d5a9fcb909">The Structure: A 2D Grid of binary, intelligent cells</h3><p id="1960a320-00ae-80c7-b58b-d5e6c567ca01">As with NCA, the system is built around a 2D grid of cells, where each cell's state is represented by an <strong>n-dimensional binary vector</strong> This binary state vector acts as the cell's working memory, storing information from previous iterations. Throughout this article, <em>cell state</em> and <em>channels</em> will be used interchangeably.</p><h3 id="17b0a320-00ae-80c6-b0da-fcc71275aac2">The Two-Stage Update Mechanism: Perception and Update</h3><ol type="1" id="17b0a320-00ae-8034-ab51-ff1b4ac57cb3" start="1"><li><strong>The </strong><strong>Perception</strong><strong> Stage</strong><div id="17c0a320-00ae-8061-a387-ecb4b1d2d84f"><div id="76bb245b-f562-4640-b34a-75e3bd6f4507"><p id="17c0a320-00ae-80c8-afeb-d3933761f7ad">In cellular automata systems, each cell must be aware of its environment. While traditional NCA use Sobel filters to model this perception, DiffLogic CA takes a different approach, following <d-cite key="Petersen2024-rr"></d-cite>. Each kernel is a distinct circuit, where connections are fixed with a particular structure, but the gates are learned. The kernels are computed channel-wise. Each circuit employs four layers whose connections are designed to compute  <em>interactions</em> between the central cell and its neighboring cells as in the figure on the right. The output dimension is the number of kernels multiplied by the number of channels. Alternative approaches involve kernels with multiple bits of output per channel, rather than only one, improving convergence in some cases.</p></div><div id="9a000272-c6fe-4248-972e-6ec94af6e057"><figure id="1820a320-00ae-80a0-a05d-fc29118859d4"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/perceive_(4).svg"><figcaption>Each kernel operates channel-wise and computes the interaction between the central cell and its neighbors, emulating the CA's interaction within the Moore neighborhood. This 3x3 patch shows a state dimension of 3. The circuit is wired to process interactions between the central cell and its surrounding cells. The first layer has 8 gates, with each gate taking the central cell as its first input and a neighboring cell as its second input.</figcaption></figure></div></div></li></ol><ol type="1" id="17b0a320-00ae-8055-967a-dbcb60fd9aa9" start="2"><li><strong>The Update Stage</strong><div id="17c0a320-00ae-80f0-a694-ed77a59540bc"><div id="9a9dc48d-e2d6-4bc8-aae3-4a36879bf8fb"><p id="6d38c7a5-9c98-4b70-84df-074f0cbf26e1">The update mechanism follows the NCA paradigm, but employs a Differentiable Logic Network to compute each cell's new state. The network's connections can be either randomly initialized or specifically structured to ensure all inputs are included in the computation. The updated state is determined by applying a Differentiable Logic Gate Network to the concatenation of the cell's previous memory (represented in gray), and the information received from its neighbors (represented in orange). In standard NCA, at this point, one would incrementally update the state, treating the whole system like an ODE. With DiffLogic CAs, we output the new state directly.  </p></div><div id="c392a3cb-bd32-4a40-9cd3-668148e146de"><figure id="1820a320-00ae-807f-bb72-f0fe7f137292"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/update_(2).svg"><figcaption>A representation of the update step given a cell state of dimension 4 and 2 kernels.</figcaption></figure></div></div></li></ol><p id="1a00a320-00ae-80cb-822b-c86cdc13c579">In summary: the perception phase uses a logic gate network to process the binary neighborhood states, replacing traditional convolution filter-based operations, and the update rule is implemented as another logic gate network that takes the perception output and current state as inputs, and outputs the next binary state of the cell.</p><figure id="1a00a320-00ae-80d6-9ac9-c9b7651dc6e6"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/cell_architecture.svg"><figcaption>Schematic representation of a 4x4 DiffLogic CA grid. At each time step, each cell reads and processes the information stored in its neighboring cells' states and then updates its own state</figcaption></figure><p id="1a00a320-00ae-807d-a0d8-cd4ede77f9c4">The diagram above schematically represents a 4x4 DiffLogic CA grid,  each of the small squares is a tiny computer with a dual-memory system. We visualize these two registers as gray and orange, respectively. Every cell in our grid performs a two-step process, which we will later see can be either performed synchronously, or in some cases asynchronously: </p><ol type="1" id="1a00a320-00ae-80cf-95c1-d27d65e00a0d" start="1"><li>Step 1: The Perception Phase<br>First, every cell in our grid becomes a data gatherer. They examine their neighbors' gray registers, process what they observe, and store their results in their orange registers.<br></li></ol><ol type="1" id="1a00a320-00ae-8032-90c0-f862ce0748d1" start="2"><li>Step 2: The Update Phase<br>Right after that, each cell becomes a decision maker. Using the information stored in both its registers (the original gray one and the newly filled orange one), the cell calculates its new state. This new state gets written to the gray register, while the orange register is cleared, ready for the next round of perception.<br></li></ol><p id="1a00a320-00ae-80f3-8b64-dd80800837d7">The system behaves like a network of tiny, independent computers that communicate with their neighbors and make decisions based on their observations. Each cell is  a miniature processor in a vast, interconnected grid, working together to perform complex computations through these simple local interactions. By combining local connections with distributed processing, we've built something that can tackle tasks exploiting the emergence of collective behavior. </p><p id="1a00a320-00ae-80ca-a49e-efb4665081d4">We again find strong kinship with the the work <em>Programmable Matter </em>and <em>Computronium</em> by Toffoli and Margolus, who proposed the CAM-8 <d-cite key="Margolus1995-hg"></d-cite><d-cite key="Toffoli1991-do"></d-cite>, a computer architecture based on cellular automata which is similar to the system above where each cell uses a DRAM chips for state variables and an SRAM for processing.</p><figure id="1a00a320-00ae-80f3-92e0-c43cb1668296"><a href="https://google-research.github.io/self-organising-systems/difflogic-ca/images/CAM-8.png"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/CAM-8.png"></a><figcaption>Cam-8 architecture and image from from Margolus et al.<d-cite key="Margolus1995-hg"></d-cite></figcaption></figure><h2 id="17c0a320-00ae-8043-af6b-d7667381714c">Experiment 1: Learning Game of Life</h2><div id="17c0a320-00ae-8036-975b-df6ed2216f95"><div id="6ecea7c3-689a-4597-b6a9-03aae07f9b1f"><p id="12744ce8-f2bc-4573-9602-f3f251460b90">Conway's Game of Life is a fascinating mathematical simulation that demonstrates how complex patterns can emerge from simple rules. Created by mathematician John Conway in 1970, this <em>game</em> isn't played in the traditional sense - it's a cellular automaton where cells on a grid live or die based on just four basic rules. Despite its simplicity, the Game of Life can produce amazing behaviors, from stable structures to dynamic patterns that seem to take on a life of their own.</p></div><div id="5732a09e-ef35-4104-b416-0f7aafae68de"><figure id="17c0a320-00ae-804f-8698-d5b06774ecdf"><a href="https://google-research.github.io/self-organising-systems/difflogic-ca/images/game_of_life_simulation.gif"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/game_of_life_simulation.gif"></a><figcaption>Simulation of Conway's Game of Life</figcaption></figure></div></div><p id="17c0a320-00ae-8092-b9ea-ed2c070ba6b0">The rules of the game are elegantly simple, focusing on how each cell interacts with its eight neighboring cells:</p><ol type="1" id="17c0a320-00ae-804d-85aa-d9526a7e68af" start="1"><li><em>Birth</em>: A dead cell (whose current value is 0) with exactly three living neighbors springs to life in the next generation, as if by reproduction.</li></ol><ol type="1" id="17c0a320-00ae-8006-930a-c889e1f40943" start="2"><li><em>Survival</em>: A living cell (whose current value is 1) with either two or three living neighbors survives to the next generation, representing a balanced environment.</li></ol><ol type="1" id="17c0a320-00ae-8077-b3bd-f32ad09cc8e6" start="3"><li><em>Underpopulation</em>: A living cell with fewer than two living neighbors dies from isolation in the next generation.</li></ol><ol type="1" id="17c0a320-00ae-80e3-a940-c61f42e07b6b" start="4"><li><em>Overpopulation</em>: A living cell with more than three living neighbors dies from overcrowding in the next generation.</li></ol><p id="17c0a320-00ae-808c-8056-fd1db396f4d4">These four rules, applied simultaneously to every cell in the grid at each step, create a dance of patterns. From these basic interactions emerge complex behaviors: <em>stable structures</em> that never change, <em>oscillators</em> that pulse in regular patterns, and even <em>gliders</em> that appear to move across the grid. It's this emergence of complexity from simplicity that has made the Game of Life a powerful metaphor for self-organization in natural systems, from biological evolution to the formation of galaxies.</p><p id="17c0a320-00ae-8087-89e5-caa7c074f76e">Given its binary and dynamic nature, Game of Life is a good sanity check of the DiffLogic CA. </p><h3 id="17c0a320-00ae-8061-a777-eeca554601ce">State and Parameters</h3><p id="17c0a320-00ae-8043-915b-d32ea04bcf22">Given that we know the rules are independent of previous state iterations, we consider a cell state consisting of 1 bit, meaning the system is essentially memory-less.  The model architecture includes 16 perception circuit-kernels, each of them with the same structure of nodes [8, 4, 2, 1]. The update network instead has 23 layers: first 16 layers have 128 nodes each, and the subsequent layers have [64, 32, 16, 8, 4, 2, 1] nodes, respectively.</p><h3 id="17c0a320-00ae-8050-b070-c81a2f73bfea">Loss function</h3><p id="17c0a320-00ae-80a6-8fa8-d9dff565d888">The loss function is computed by summing the squared differences between the predicted grid and the ground truth grid.</p><figure id="17c0a320-00ae-8006-baad-fd12853ac1a2"><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>N</mi></munderover><mo stretchy="false">(</mo><msub><mi>y</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>−</mo><msub><mover accent="true"><mi>y</mi><mo>~</mo></mover><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sum_{i,j}^N(y_{i,j} - \tilde{y}_{i,j})^2  </annotation></semantics></math></span></span></span></p></figure><h3 id="17c0a320-00ae-800e-8361-d0ed4714fa5e">Training Dataset</h3><p id="17c0a320-00ae-8007-8251-fc1ae5ea91e5">The model was trained on 3x3 periodic grids for a single time step. Given that each cell in the Game of Life interacts with its eight neighbors, and its next state is determined by its current state and the states of its neighbors, there are 512 possible unique configurations for a 3x3 grid. To train the model, we constructed a grid including all 512 possible grid configurations. Learning the next state of grid correctly implies learning the complete Game of Life rule set. The trained parameters were subsequently used to simulate the model's behavior on larger grids.</p><h3 id="17c0a320-00ae-80ef-b8b5-ceb5e36e8b3e"><strong>Results</strong></h3><p id="17c0a320-00ae-80d3-88f2-cef619c26726">On the left, you can observe the loss plot comparing the two representations of logic gates. The <em><mark><strong>soft</strong></mark></em><mark><strong> loss</strong></mark> computes the output of the gates using their continuous approximation as explained in the previous section, while the <em><mark><strong>hard</strong></mark></em><em><mark><strong> loss</strong></mark></em> selects only the most probable gate and uses its discrete output. Both losses fully converge, indicating that we were able to generate a circuit that perfectly simulates the Game of Life.</p><p id="1990a320-00ae-80a8-83cc-f84133c2d11e">Using hard inference (selecting most probable gates), the simulation on the right displays the learned circuit's performance on a larger grid. The emergent patterns capture the structures from Conway's Game of Life: gliders moving across the grid, stable blocks remaining fixed in place, and classic structures like loaves and boats maintaining their distinctive shapes. The successful replication of Game of Life's characteristic patterns demonstrates that our circuit has effectively learned the underlying local rules.</p><div id="17c0a320-00ae-8023-8d93-ca1fe9b614cc"><div id="6374f26a-1ff6-4c15-a6cb-288b0dfb7aff"><figure id="1980a320-00ae-8061-85ea-f591ef1029d7"><a href="https://google-research.github.io/self-organising-systems/difflogic-ca/images/gof_loss.svg"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/gof_loss.svg"></a><figcaption>Training plot for DiffLogic CA learning Game of Life</figcaption></figure></div><div id="c01e6682-51f8-4834-9291-d86c414afc83"><figure id="17b0a320-00ae-806b-955e-daf04e669128"><a href="https://google-research.github.io/self-organising-systems/difflogic-ca/images/gif_game_of_life.gif"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/gif_game_of_life.gif"></a><figcaption>Game of Life simulated by the learned circuit</figcaption></figure></div></div><h3 id="19a0a320-00ae-8063-bd2d-c699f4db50e0">Analysis of the Generated Circuit </h3><p id="19a0a320-00ae-80af-8991-f65fef5f2047">While circuit optimization is not the primary focus of this project, this section provides a brief analysis of the generated circuit.</p><p id="17c0a320-00ae-8066-884e-f2b96588559f">The total number of <em>active</em> gates used (excluding the pass-through gates A and B), is 336. Examining the gate distributions, we observe that the most frequently used gates in both networks are OR and AND.</p><p id="1a70a320-00ae-8064-a718-f9867702f336">Since our final circuit is simply a series of binary gates, we can step even deeper and visualize the entirety of the circuit logic involved! Below is a visualization of most of these 336 gates (some gates are pruned, when we determine they don’t contribute to the output). </p><figure id="1a70a320-00ae-8057-81b6-d5a09ba88fb3"><a href="https://google-research.github.io/self-organising-systems/difflogic-ca/images/gol_circuit_full.png"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/gol_circuit_full.png"></a><figcaption>Complete learned perceive-update circuit implementing Game of Life (<a href="https://google-research.github.io/self-organising-systems/difflogic-ca/gol.html">available interactively</a>)</figcaption></figure><p id="1a70a320-00ae-801a-a4f4-f2b54a03f311">The squares arranged in a three-by-three grid on the left are the input gates, arranged as they would be when viewed from the perspective of a single, central, cell somewhere in the game of life. The wires are colored green when high (1), and red when low (0). Finally, each gate should be somewhat self-explanatory, being one of AND, OR or XOR gates, with small circles on inputs or on outputs to denote NOTs on those particular connections. We've additionally replaced the binary NotB and NotA gates with a unary Not gate, and pruned the unused input, to simplify visualization. Finally, some gates are simply “True” or “False”, and these look almost identical to the inputs, appearing as nested squares, either filled in (True) or empty (False).  </p><p id="1a70a320-00ae-800f-ae77-e6586a7d4263">On the far right, we see the single output channel of this circuit - denoting the new state of the cell in the Game of Life. In this particular configuration in the figure, we see the circuit correctly computing the rule “Any dead cell with exactly three live neighbours becomes a live cell, as if by reproduction.”</p><p id="1a70a320-00ae-800c-8cdb-f6a07a46a6dd">We encourage readers to <a href="https://google-research.github.io/self-organising-systems/difflogic-ca/gol.html">directly interact with the circuit</a><d-cite key="MaterzokUnknown-wb"></d-cite>.</p><h2 id="17c0a320-00ae-806e-83c8-cf2856b2b387">Experiment 2: Pattern Generation</h2><p id="17c0a320-00ae-80a8-a075-f0f39993e28b">Neural Cellular Automata (NCA) have shown remarkable capabilities in pattern generation tasks <d-cite key="Mordvintsev2020-oh"></d-cite>, inspiring us to explore similar capabilities with diffLogic CA. In this task, the system evolves from a random initial state toward a target image, allowing multiple steps of computation. By evaluating the loss function only at the final time-step, we challenge the model to discover the discrete transition rules that guide the system through a coherent sequence of states without step-by-step supervision. </p><p id="1a80a320-00ae-8026-89b3-d3c8f76f6740"><br>Successfully learning to reconstruct images would validate two key aspects: the model's ability to develop meaningful long-term dynamics through learned rules, and its capability to effectively learn <em>stateful, recurrent-in-time, recurrent-in-space</em> circuits. This investigation is particularly significant as it represents, according to the best of our knowledge, the first exploration of differentiable logic gate networks <d-cite key="Petersen2022-ai"></d-cite><d-cite key="Petersen2024-rr"></d-cite> in a recurrent setting.</p><h3 id="17c0a320-00ae-8083-8659-e20f9a41611e">State and Parameters</h3><p id="17c0a320-00ae-80a3-8347-cb14d7db4b62">We consider a cell state (channels) of 8 bits and iterate the DiffLogic CA for 20 steps. The model architecture includes 16 perception circuit-kernels, each with 8, 4, then  2 gates per layer, respectively. The update network has 16 layers: 10 layers with 256 gates each, and then layers with [128, 64, 32, 16, 8, 8] gates, respectively. </p><h3 id="17c0a320-00ae-801f-ab3f-ead7589b668c">Loss function</h3><p id="17c0a320-00ae-80a4-80cd-e1cf54b12061">We define the loss function as the sum of the squared differences between the first channel in the predicted grid and the target grid at the last time step.</p><figure id="17c0a320-00ae-8098-a736-e918eb6b04da"><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>N</mi></munderover><mo stretchy="false">(</mo><msub><mi>y</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><mn>0</mn></mrow></msub><mo>−</mo><msub><mover accent="true"><mi>y</mi><mo>~</mo></mover><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><mn>0</mn></mrow></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sum_{i,j}^N(y_{i,j,0} - \tilde{y}_{i,j,0})^2  </annotation></semantics></math></span></span></span></p></figure><h3 id="17c0a320-00ae-801c-a912-f98499dc9d47">Training Dataset</h3><div id="17c0a320-00ae-8063-aba4-d927930e8cd1"><div id="41027de1-db12-49a7-9b8b-a2e7fe1c3892"><p id="17c0a320-00ae-8012-a208-d2be1d27409c">The model was trained to reconstruct a 16x16 checkerboard pattern within 20 time steps. For each training step, the initial state was randomly sampled. The target checkerboard pattern is shown on the right.</p></div><div id="b2f4fe42-1f3c-4dc2-94f2-efc12224d5be"><figure id="17c0a320-00ae-8048-a627-cbcd662de7ee"><a href="https://google-research.github.io/self-organising-systems/difflogic-ca/images/train_set.png"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/train_set.png"></a><figcaption>Target pattern</figcaption></figure></div></div><h3 id="17c0a320-00ae-80ed-b462-f42a3ae1680b"><strong>Results</strong></h3><p id="17c0a320-00ae-807e-8245-c331ebdc614d">The DiffLogic CA fully converges to the target pattern. The training plot (left) reveals consistent convergence of both soft and hard loss functions. The evolution of the first channel (right), which is used for computing the loss function, shows clear pattern formation. An intriguing emergent property is the directional propagation of patterns from bottom-left to top-right, despite the model having no built-in directional bias.</p><div id="1910a320-00ae-8066-bd37-df2342c6feb9"><div id="1910a320-00ae-80a8-ad71-e34a62674acc"><figure id="17c0a320-00ae-80ba-af4c-fad9791b0ab9"><a href="https://google-research.github.io/self-organising-systems/difflogic-ca/images/checkerboard_loss.svg"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/checkerboard_loss.svg"></a><figcaption>Training plot for DiffLogic CA</figcaption></figure></div><div id="1910a320-00ae-80fd-8d01-f41c9c542454"><figure id="1900a320-00ae-80ab-9996-c2f3bb4703ae"><a href="https://google-research.github.io/self-organising-systems/difflogic-ca/images/checker_board_small.gif"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/checker_board_small.gif"></a><figcaption>Evolution of the diffLogic CA, only considering the first bit in the cell state. </figcaption></figure></div></div><h3 id="17c0a320-00ae-8029-84c0-c0a04827a153">Analysis of the Generated Circuit</h3><p id="17c0a320-00ae-80dc-bddf-e75bba1ae1e5">The total number of active gates used (excluding pass-through gates A and B) is 22. Analysis of the learned logic gates reveals a different distribution of gates between the perception kernels and update networks. The TRUE gate appears to play a key role in perception but not in the update one. </p><div id="17c0a320-00ae-8050-84d9-eed83c749260"><div id="9f03a6f1-88e6-40f7-addb-635001c9dac0"><figure id="17c0a320-00ae-8051-b45b-f1e017e41857"><a href="https://google-research.github.io/self-organising-systems/difflogic-ca/images/checkerboard_sync_perceive_gates.svg"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/checkerboard_sync_perceive_gates.svg"></a><figcaption>Distribution of gate counts across all perception kernel circuits</figcaption></figure></div><div id="078f6020-e34b-49ff-ac61-550d1722c0df"><figure id="17c0a320-00ae-80cd-a191-f479ea5492f4"><a href="https://google-research.github.io/self-organising-systems/difflogic-ca/images/checkerboard_sync_update_gates.svg"><img src="https://google-research.github.io/self-organising-systems/difflogic-ca/images/checkerboard_sync_update_gates.svg"></a><figcaption>Distribution of gate counts across the update circuit.</figcaption></figure></div></div><p id="1a80a320-00ae-804f-bb69-c8a252e2d836">Below, we provide an interactive visualization of the circuit, after pruning. Remarkably, we are left with just six gates - one of which is redundant - an <em>AND</em> between the same input. In other words; the entirety of the procedural checkerboard-generation function learned by the circuit can be implemented using just five logic gates. Likewise, most of the inputs and outputs remain unused. Even more remarkably, the cell's own current visual output isn't even considered in an update step. We encourage readers to interact with the circuit below <d-cite key="MaterzokUnknown-wb"></d-cite>, clicking on and off inputs on the left to observe the effect on the outputs.</p><figure>

<figcaption>Complete learned perceive-update circuit generating checkerboard, interactive</figcaption>
</figure>



</div><p id="1910a320-00ae-8063-b6f3-c3712abdae01">To the naked eye, our solution appears to build the grid iteratively—brick by brick, as it were. However, during training, we only employed one fixed size of the grid. Naturally, we should investigate what happens if we change the grid size: is the rule we learned truly an iterative, procedural solution, or is it overfit to one particular grid size? Let's scale up both the spatial and temporal dimensions by a factor of four—using a grid four times larger and running it for four times as many steps.</p><p id="1900a320-00ae-8012-8000-d24e45e6528c">Success! The circuit works just as well in this new setting. This raises an interesting question as to the inductive biases of this model. In the NCA setting, it was possible to coax behavior invariant to grid size and time, but this required either special spatially invariant loss functions <d-cite key="Niklasson2021-vb"></d-cite> , and in the case of the growing lizard a special "alive/dead"<d-cite key="Mordvintsev2020-oh"></d-cite> regime to prevent overfitting to boundary conditions. Here, our boundary conditions are also fixed, yet the model has learned a "boundary-size-invariant" way to produce the pattern. Could the discretization and minimal circuit size be finding some minimal procedural description for generating patterns of interest? </p><p id="1910a320-00ae-80b6-9642-fda8db6a59db">Given our setting, we tested the system's resilience to damage and its recovery capabilities through two experiments. In the first test (left), we evaluated pattern reconstruction when a large portion of cells were permanently disabled, simulating faulty components. In the second test (right), the disabled cells were reactivated after a set number of steps. The system demonstrated robust behavior in both scenarios: maintaining pattern integrity despite permanent cell damage in the first case, and successfully self-repairing to produce the correct pattern once damaged cells came back online in the second case.</p><p id="1980a320-00ae-80f4-a198-c9d7fa195e69">Robust computing <d-cite key="Ackley2013-im"></d-cite> represents a fundamental shift in system design, prioritizing reliable operation under real-world conditions. In contrast to traditional computing, which relies on precise, error-free components, robust systems are designed to remain functional even in the face of hardware failures, environmental interference, unexpected inputs, or manufacturing variations. While contemporary computing, especially distributed computing, has some affordances around robustness to certain types of failures, it's generally still far more brittle than any similarly complex system in the natural world, and those affordances are usually designed around very specific failure cases that we are unable to control for by other means (think cosmic ray-induced bit flips in RAM). In the example reported above, we observed how the DiffLogic CA learned rules that exhibit both fault tolerance and self-healing behavior, without explicitly designing around these conditions. When some cells fail, the damage is contained, and the system continues to function with a gradual decline rather than experiencing catastrophic failure. This mirrors how biological systems achieve reliability through networks of imperfect components, suggesting a powerful approach for future computing systems that can maintain functionality even under imperfect conditions.</p><p id="1960a320-00ae-8082-a939-e7211db1d6ed">Inspired by the approach used in traditional NCA training <d-cite key="Niklasson2021-ft"></d-cite>, we explored asynchronous updates. Instead of updating all cells simultaneously (which can be likened to a global clock), we randomly select a subset of cells to update in each step. This simulates a scenario where each cell operates with its own internal clock. Within this framework, each cell can be conceptualized as a tiny computational unit operating independently of other cells, making its own decisions.</p><p id="19c0a320-00ae-8002-b441-cebc9f9c150c">We proceeded directly to introducing asynchronicity to training, expecting this to be markedly more difficult than in traditional NCAs. Firstly, the updates at every step must output the full new state, and not just an incremental update. Secondly, a cell must now be able to account for surrounding cells being in any combination of desynchronization. Any given neighbour could be one, two, three, or more steps "behind" or "ahead". This combinatorially increases the possible transitions rules the cell has to effectively learn to deal with. To our surprise -  successful asynchronous training was relatively easy to achieve in the simplest pattern - the checkerboard. Below, we demonstrate three different, unique, reconstructions of the pattern , all starting  from the same initial state but with distinct random seeds to determine the cell update order. Despite the asynchronous nature of these updates and a more complex resulting update rule, the cells correctly reconstruct the target pattern in 50 steps, compared to the original 20.</p><p id="1980a320-00ae-80ff-8aca-ea0fc5505fa0">Furthermore, the learned circuit demonstrated generalization capabilities, exhibiting successful reconstruction on larger grids and resilience to errors -  a self-healing checkerboard..</p><p id="1a80a320-00ae-807e-aecd-d610a20882b2">The biggest surprise came when sanity checking the original synchronously trained rule, but using asynchronous inference. It works! This is surprising and further speaks to the robustness of the circuit originally discovered. </p><p id="1a80a320-00ae-8095-900a-f90ff269de71">This unexpected success with asynchronous inference led us to hypothesize that models trained directly with asynchronous updates would exhibit even greater robustness. To test this, we randomly deactivate a 10x10 pixel square within the image domain at each inference time-step, as shown in the simulations below.    </p><p id="1a80a320-00ae-80ea-a82e-f52164befdcd">The images hint at the difference in resilience to noise - the asynchronous cells recover from the damage slightly more quickly, while the synchronously trained rule appears to be more impacted. By measuring the error as the sum of the absolute difference between the target and reconstructed images, we found that asynchronous training improves robustness considering these perturbations. </p><p id="1900a320-00ae-80e5-b44f-e35e162e2266">For the next experiment, we tested DiffLogic CA's ability to learn arbitrary shapes by training it on the outline of a lizard, in an homage to the original NCA work. This involves more memorization than reproducing a highly-compressible regular pattern like the checkerboard. We use a cell state  of 128 bits and iterate the DiffLogic CA for 12 steps. The model architecture includes four perception circuit-kernels with 8, 4, 2, and 1 gates at each layer, respectively. The update network has 10 layers: eight layers with 512 gates each, and then layers with [256, 128] nodes, respectively. </p><p id="1910a320-00ae-8023-bf3a-e2874f1e8ef2">We trained the model to generate a 20x20 lizard pattern, in 12  time steps. Just as in NCA, the initial condition consists of a central seed to break symmetry, with periodic boundary conditions applied to the grid edges. We employed the same loss function previously used in the checkerboard experiment.</p><p id="1980a320-00ae-809d-a272-d689a49e93d9">To assess the model's generalization capabilities, we evaluated its performance on a larger 40x40 grid. The results demonstrate that the model successfully learned the growth pattern without exploiting boundary conditions.</p><p id="19c0a320-00ae-8003-b682-f2ee76e35047">On the left, the plot shows the convergence of both the soft and hard losses to zero. On the right, the visualization illustrates the successful growth of the lizard within the larger grid.</p><p id="1980a320-00ae-80a2-88ce-e27e102d1872">Below, visualizations of the first 32 hidden states offer a glimpse into the internal dynamics of the model during the growth process.</p><p id="1980a320-00ae-8031-82aa-d59dd76aa6fa">Training DiffLogic CA to generate complex patterns presents significant optimization challenges. The process required extensive hyper-parameter tuning. Future improvements to both the model architecture and circuit topology could enhance convergence speed and stability, potentially reducing the need for such intensive hyper-parameter optimization.</p><p id="1980a320-00ae-80dc-9b30-f003157154a3">A total of 577 active gates were used, excluding pass-through gates A and B.</p><p id="19a0a320-00ae-80ba-a25c-cd71756eeb32">The perception kernels predominantly employed the TRUE gate, while the update circuit employed almost all available gates.</p><p id="1990a320-00ae-8038-868d-e4044bacdd59">Previous experiments have primarily focused on effectively monochrome images, using the last channel for visualization purposes. Wanting to investigate more complex target states,  we trained the model to generate a 16x16 "colored" image, over 15 steps. Using 64 channels per cell state, the model has four perception circuit-kernels, each with four three layers with 8, 4, and 2 gates, respectively. The update network architecture consists of 11 layers: 8 layers of 512 nodes each, and a final sequence of 3 layers with [256, 128, 64] nodes, respectively.</p><p id="1990a320-00ae-80e1-a19d-f20f440dc67b">The model was trained to generate a 16x16 colored letter of the alphabet (that might be reminiscent to some), over 15 steps. The initial state is fully zero, without periodic boundary conditions. Following the convention used in standard NCA <d-cite key="Mordvintsev2020-oh"></d-cite>, the first three channels represent RGB color values. However, in our case, these values are constrained to a binary representation of <span data-token-index="0" contenteditable="false"><span></span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mi>s</mi></mrow><annotation encoding="application/x-tex">0s</annotation></semantics></math></span></span></span><span>﻿</span></span> and <span data-token-index="0" contenteditable="false"><span></span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>s</mi></mrow><annotation encoding="application/x-tex">1s</annotation></semantics></math></span></span></span><span>﻿</span></span>, resulting in a palette of eight possible colors.</p><p id="1990a320-00ae-8077-9efd-db1aaba4e056">The loss function is defined as the sum of the squared differences between the predicted grid and the target grid at the final time-step, considering only the first three channels (0, 1, 2).</p><p id="1990a320-00ae-8015-a847-e48d76781606">The results demonstrate that the model successfully learns this colorful G. On the left, the loss function plots show the convergence of both the soft and hard losses. On the right, the  reconstruction the colorful G in 15 steps is shown.</p><p id="1990a320-00ae-80f7-8ef1-e87febb0c8ce">A total of 927 active gates were used (excluding pass-through gates A and B). Analysis of the learned logic gates revealed distinct distributions across perception and update networks. Notably, TRUE and FALSE gates were extensively employed in both networks, while the OR gate was the most prevalent in the update network. We note that this circuit was more complex than previous experiments, both in the difficulty of finding suitable hyperparameters and in size of the circuit.</p><p id="1a00a320-00ae-809b-b46a-cb16a2bdf0f3">This work introduces DiffLogic CA, a novel NCA architecture and training regime, utilising a fully discrete cell state, updated using a learned, recurrent binary circuit. We replace the neural network components with Deep Differentiable Logic Networks, which bring the flexibility of differentiable training to discrete logic gates. The successful application of differentiable logic gates to cellular automata is demonstrated through two key results: replicating the rules of Conway's Game of Life and generating patterns via learned discrete dynamics. These findings highlights the significant potential of integrating discrete logic within the framework of neural cellular automata and prove that differentiable logic gate networks can be effectively learned in recurrent architectures.  While the current model exhibits promising results in learning patterns, training it to generate more complex shapes and structures presents ongoing challenges. Potential directions for improvement include the exploration of hierarchical NCA architectures and the incorporation of specialized gates designed to facilitate state forgetting. For instance, integrating LSTM-like gating mechanisms into the state update process could enable a richer and diverse combination of past and newly computed candidate states, potentially enhancing the model's dynamics and expressiveness.</p><p id="1a00a320-00ae-809b-b46a-cb16a2bdf0f3">We thank Blaise Aguera y Arcas for his support and the Paradigm of Intelligence Team for the fruitful and inspiring discussions. Many thanks to Marek Materzok, and the contributors to the excellent <a href="https://github.com/tilk/digitaljs">DigitalJS circuit visualization library</a>, which was used to power all the interactive circuits in this article.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why I find diffusion models interesting? (133 pts)]]></title>
            <link>https://rnikhil.com/2025/03/06/diffusion-models-eval</link>
            <guid>43285726</guid>
            <pubDate>Thu, 06 Mar 2025 22:35:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rnikhil.com/2025/03/06/diffusion-models-eval">https://rnikhil.com/2025/03/06/diffusion-models-eval</a>, See on <a href="https://news.ycombinator.com/item?id=43285726">Hacker News</a></p>
Couldn't get https://rnikhil.com/2025/03/06/diffusion-models-eval: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[How to distrust a CA without any certificate errors (120 pts)]]></title>
            <link>https://dadrian.io/blog/posts/sct-not-after/</link>
            <guid>43285671</guid>
            <pubDate>Thu, 06 Mar 2025 22:28:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dadrian.io/blog/posts/sct-not-after/">https://dadrian.io/blog/posts/sct-not-after/</a>, See on <a href="https://news.ycombinator.com/item?id=43285671">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>A “distrust” is when a certification authority (CA) that issues <a href="https://dadrian.io/blog/posts/certificates-explained/">HTTPS
certificates</a> to websites is removed from a root store because
it is no longer trusted to issue certificates. This means certificates issued by
that CA will be treated as invalid, likely causing certificate error
interstitials in any browser that distrusted the CA. Distrusts can happen for
security reasons, compliance reasons, or simply due to a lack of trust in the
operators. In the past, the complexity and user impact of distrust events have
largely been dependent on the size and usage of a CA—the larger the CA, the
<a href="https://security.googleblog.com/2017/09/chromes-plan-to-distrust-symantec.html">longer and more complex the timeline was to distrust it if it
misbehaved</a>, and the more likely users were to encounter
certificate errors. Nowadays, the situation is different.</p>
<p>Most user agents<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> require certificates to be logged to public <a href="https://transparency.dev/">certificate
transparency</a> (CT) logs. Since the introduction of CT, most distrusts are no
longer due to key compromise and domain validation failures. Today, CAs are much
less attractive target to exploit than they were in the early 2010s because any
maliciously issued certificate still needs to be logged to CT<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> to be trusted
in browsers. Each certificate must be served with a set of Signed Certificate
Timestamps (SCTs) that are usually embedded in the certificate, but can be
served in the TLS handshake instead. An SCT is a promise that a certificate will
be included in a CT log. The CT logging requirement means that maliciously
issued certificates via CA key compromise are publicly auditable and visible.
This drastically decreases the value of using a compromised CA key as a vector
for targeted MITM attacks, compared to other <a href="https://securitycryptographywhatever.com/2024/06/24/mdowd/">exploitation methods</a>.</p>
<p>Instead of key compromise, most distrusts today are over a pattern of repeated
failures by a CA to comply with the <a href="https://cabforum.org/working-groups/server/baseline-requirements/">Baseline Requirements</a> (BRs). The BRs
are the standard set of rules that apply to all CAs. The BRs are maintained by
the <a href="https://cabforum.org/">CA/Browser Forum</a> (CABF). Think of the CABF as the <a href="https://www.ietf.org/">IETF</a>, but
for CAs, and the BRs as additional RFCs that define CA behavior, including how
to perform <em>domain control validation</em> (DCV).</p>
<p>Beyond CT, certificates now have much shorter lifetimes than they did in the
2010s. This is good because <a href="https://zanema.com/papers/imc23_stale_certs.pdf">shorter certificate lifetimes are more
secure</a>. The current maximum age of a certificate, set in 2020,
is 398 days (13 months), and there’s a <a href="https://github.com/cabforum/servercert/pull/553/files">ballot</a> in the CABF to gradually
lower it to 47 days, whereas lifetimes were unlimited prior to 2012, and still
up to five years until 2018. This means that any change in how certificates are
issued (e.g.  requiring CT), applies to all time-valid certificates within 13
months. “Legacy” certificates are at most one year old, and we can identify the
full set of time-valid certificates by trawling CT logs.</p>
<p>The most complicated distrusts of the 2010s had neither of these
properties—not all certificates were logged to CT, and there existed
time-valid certificates from at least 2-5 years ago at any given time. This
meant that any distrust at the time had to look backwards at existing
certificates, as well as forwards, at certificates that were not yet issued.  CT
provided an incomplete view of the active set of certificates, and the existence
of time-valid certificates with extremely long lifetimes meant that a CA that
stopped issuing could still have unexpired certificates for years.</p>
<p>Luckily, the Web PKI is in a much better space now than it was ten years
ago<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>. Nowadays, distrusts can be primarily <em>forward looking</em>, meaning that
they don’t need to affect current time-valid certificates, and can instead
distrusts can be applied to <em>any certificate issued by the distrusted CA after
some future date</em>. During this period, time-valid certificates issued before the
cutoff continue to work, even if their validity extends past the cut off. The
threshold only needs to be applied to the <code>NotBefore</code> (start) date, rather than
the <code>NotAfter</code> (expiration) date.</p>
<p>The mechanism for forward-looking, interstitial free distrusts, is
<em>SCTNotAfter</em>. An SCTNotAfter constraint is a mechanism for cryptographic
assurance about the NotBefore date, and can be used to implement distrusts that
“grandfather in” existing certificates, without risking a CA backdating a
certificate to get around the distrust (which <a href="https://wiki.mozilla.org/CA/WoSign_Issues#Issue_S:_Backdated_SHA-1_Certs_(January_2016)">has happened in the
past</a>!). Existing certificates from the distrusted CA will
continue to work without errors. SCTNotAfter works by selecting an “SCTNotAfter
date” for some CA certificate, and then requiring any leaf certificate that
chains through that CA to have at least one SCT with a timestamp from <em>before</em>
the SCTNotAfter date. This means that the certificate was disclosed to at least
one CT log prior to the SCTNotAfter date.  Even if the CA backdates the
NotBefore field, the timestamp in the SCT should<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> still be accurate.</p>
<p>For a CA that’s distrusted over repeated compliance failures, this date can be
in the future. This gives time for the CA to reissue any existing certificates
for their max lifetimes, allowing each customer of the CA a full certificate
lifecycle to transition to a new CA. Not all customers will be paying attention
to whether or not their CA was distrusted, however, presumably the distrusted CA
will stop issuing certificates after the SCTNotAfter date, since they wouldn’t
work in browsers. When renewal time comes around, customers will be unable to
get a new certificate from the distrusted CA, and can migrate to a new CA
instead. With SCTNotAfter, customers have an entire certificate lifecycle after
the cutoff date to plan for a transition to new CA. Once every certificate
issued before the SCTNotAfter date expires, the distrusted CA can be safely
removed from the root store.</p>
<p>In the event of a security incident or key compromise, the SCTNotAfter date can
be set in the past. Let’s say we learn at time T that <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=647959">Honest Achmed’s
CA</a> was compromised at time P, where P is before T. Setting an
SCTNotAfter date of P - 1 allows every existing certificate from before the
compromise to live out its remaining lifecycle, while limiting the distrust to
only certificates from after the compromise. This allows as many certificates as
possible to continue to work, while blocking any potentially malcious
certificate.</p>
<p>The Chrome distrusts of <a href="https://groups.google.com/a/ccadb.org/g/public/c/wRs-zec8w7k/m/MeZgTE4PAgAJ">GLOBALTRUST</a> and
<a href="https://security.googleblog.com/2024/06/sustaining-digital-certificate-security.html">Entrust</a> were both implemented using SCTNotAfter. Unlike the
Symantec distrust, neither distrust has resulted in unexpected user-facing
certificate interstitials. This is an improvement for user security, and user
experience.</p>
<p>While distrusts are sometimes necessary, the best outcome for user security
remains a robust Web PKI where each CA and browser is commited to continuous
improvement and user security. When all participants are commited to user
security, distrusts are few and far between.</p>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using GRPO to Beat o1, o3-mini and R1 at "Temporal Clue" (154 pts)]]></title>
            <link>https://openpipe.ai/blog/using-grpo-to-beat-o1-o3-mini-and-r1-on-temporal-clue</link>
            <guid>43284420</guid>
            <pubDate>Thu, 06 Mar 2025 19:51:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openpipe.ai/blog/using-grpo-to-beat-o1-o3-mini-and-r1-on-temporal-clue">https://openpipe.ai/blog/using-grpo-to-beat-o1-o3-mini-and-r1-on-temporal-clue</a>, See on <a href="https://news.ycombinator.com/item?id=43284420">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p><img alt="" data-framer-asset="data:framer/asset-reference,XbGE2sLW99jyy3eDz6naY1UFHE.png" data-framer-height="971" data-framer-width="1626" height="485" src="https://framerusercontent.com/images/XbGE2sLW99jyy3eDz6naY1UFHE.png" srcset="https://framerusercontent.com/images/XbGE2sLW99jyy3eDz6naY1UFHE.png?scale-down-to=512 512w,https://framerusercontent.com/images/XbGE2sLW99jyy3eDz6naY1UFHE.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/XbGE2sLW99jyy3eDz6naY1UFHE.png 1626w" width="813" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (max-width: 1199px) 100vw"></p><p><br>In this post we’ll discuss how we used <!--$--><a href="https://arxiv.org/abs/2402.03300" target="_blank" rel="noopener">Group Relative Policy Optimization (GRPO)</a><!--/$--> to surpass R1, o1, o3-mini, and come within a couple percentage points of Sonnet 3.7 on a reasoning-heavy game called “temporal clue”, while being over 100x cheaper to run at inference time. We’ll include specific lessons learned about task design and hyperparameters we’ve found to work well. And finally, we share the <!--$--><a href="https://github.com/openpipe/deductive-reasoning" rel="noopener">training recipe</a><!--/$--> we used to achieve these results, built on top of <!--$--><a href="https://github.com/pytorch/torchtune" rel="noopener">torchtune</a><!--/$-->.</p><h3>Background</h3><p>Since OpenAI launched its powerful&nbsp;<!--$--><a href="https://openai.com/index/learning-to-reason-with-llms/" rel="noopener">new o-series of reasoning models last year</a><!--/$-->, we've seen rapid progress in Large Language Models (LLMs) trained with Reinforcement Learning (RL). Leading organizations like&nbsp;<!--$--><a href="https://deepmind.google/technologies/gemini/flash-thinking/" rel="noopener">Google DeepMind</a><!--/$-->,&nbsp;<!--$--><a href="https://qwenlm.github.io/blog/qwq-32b-preview/" rel="noopener">Alibaba</a><!--/$-->,&nbsp;<!--$--><a href="https://arxiv.org/abs/2501.12948" rel="noopener">DeepSeek</a><!--/$-->, and&nbsp;<!--$--><a href="https://www.anthropic.com/research/visible-extended-thinking" rel="noopener">Anthropic</a><!--/$--> quickly followed suit, and have trained their own advanced models to reason with long “chains-of-thought” (CoT), taught with reinforcement learning on verifiable problems. Many previously challenging benchmarks—in areas like mathematics and coding—now approach saturation.</p><p>Yet despite these impressive strides, logical deduction remains stubbornly difficult for even today's best models. Typically, LLMs struggle to consistently attend to all relevant details, maintain logically sound reasoning chains, or reliably link multiple deduction steps. Even state-of-the-art models generating outputs 10–100 times longer frequently introduce elementary mistakes that a human solver would easily catch.</p><p>Intrigued by this unsolved mystery, we donned our deerstalker caps and set out to investigate: Could smaller, open-weight models reach frontier-level deduction performance with the latest reinforcement learning techniques? We began with substantially weaker models and iteratively trained them on a novel deduction task. Over time, we observed clear improvements in their detective prowess, eventually matching or even exceeding some of the strongest proprietary models.</p><p>Now we're happy to share our findings, including <!--$--><a href="https://github.com/openpipe/rl-experiments" rel="noopener">our experiments</a><!--/$-->, <!--$--><a href="https://github.com/openpipe/deductive-reasoning" rel="noopener">training recipe</a><!--/$-->, <!--$--><a href="https://github.com/bradhilton/temporal-clue" rel="noopener">dataset</a><!--/$-->, and <!--$--><a href="https://huggingface.co/OpenPipe/Deductive-Reasoning-Qwen-32B" rel="noopener">model weights</a><!--/$-->, all freely available under the MIT license, along with key practical insights (right here). Grab your magnifying glass, detective; the game is afoot!</p><h3>Benchmarking</h3><p>To begin our experiments, we first had to identify a challenging reasoning task with clearly verifiable solutions and scalable complexity. As it happened (not coincidentally), one of the authors previously created a puzzle set named <!--$--><a href="https://github.com/bradhilton/temporal-clue" rel="noopener">Temporal Clue</a><!--/$--> that matched these needs perfectly. Beyond meeting the criteria of ground truth clarity, new puzzles can be created as needed—a neat bonus.</p><p>Temporal Clue is inspired by the popular board game, <!--$--><a href="https://en.wikipedia.org/wiki/Cluedo" rel="noopener">Clue (Cluedo)</a><!--/$-->, where players race to uncover who killed Mr. Boddy in his palatial estate. Temporal Clue turns the game into a solitary logic puzzle that extends beyond the standard dimensions—<strong>who</strong>, (with)&nbsp;<strong>what</strong>, and&nbsp;<strong>where</strong>—and incorporates two additional dimensions:&nbsp;<strong>when</strong>&nbsp;(time) and&nbsp;<strong>why </strong>(motive). Puzzles are randomly generated, and minimal yet sufficient clues are selected with&nbsp;&nbsp;<!--$--><a href="https://developers.google.com/optimization" rel="noopener">OR-Tools'</a><!--/$-->&nbsp;<!--$--><a href="https://developers.google.com/optimization/cp/cp_solver" rel="noopener">CP-SAT solver</a><!--/$-->.</p><blockquote><p><!--$--><a href="https://gist.github.com/bradhilton/911e208183a389dc00e52de73f2c66bb" target="_blank" rel="noopener">On a dark winter night, wealthy and enigmatic Mr. John Q. Boddy hosted a small, but lavish, dinner party for some of his closest associates. However, the night ended in tragedy when Mr. Boddy was found dead in one of the rooms of Tudor Mansion in the early hours of the morning. The following persons of interest have been identified as suspects…</a><!--/$--></p></blockquote><p>To establish the current state-of-the-art for this deduction task, we benchmarked leading reasoning models—including DeepSeek R1, OpenAI’s o1 and o3-mini, and Anthropic’s Claude Sonnet 3.7. Additionally, we have benchmarked the 14B and 32B Qwen models, which we later improve using reinforcement learning, and include a preview of our final results:</p><figure><table><tbody><tr><td><p>Organization</p></td><td><p>Model</p></td><td><p>Reasoning Effort</p></td><td><p>Avg. Accuracy</p></td><td><p>Avg. Cost</p></td></tr><tr><td><p>DeepSeek</p></td><td><p>R1</p></td><td><p>Default</p></td><td><p>51.6%</p></td><td><p>$0.029</p></td></tr><tr><td><p>OpenAI</p></td><td><p>o1</p></td><td><p>Default</p></td><td><p>54.9%</p></td><td><p>$0.901</p></td></tr><tr><td><p>OpenAI</p></td><td><p>o3-mini</p></td><td><p>Medium</p></td><td><p>55.9%</p></td><td><p>$0.068</p></td></tr><tr><td><p>OpenAI</p></td><td><p>o3-mini</p></td><td><p>High</p></td><td><p>56.7%</p></td><td><p>$0.170</p></td></tr><tr><td><p>Anthropic</p></td><td><p>Sonnet 3.7</p></td><td><p>None</p></td><td><p>51.7%</p></td><td><p>$0.017</p></td></tr><tr><td><p>Anthropic</p></td><td><p>Sonnet 3.7</p></td><td><p>16k Token Budget</p></td><td><p>61.7%</p></td><td><p>$0.222</p></td></tr><tr><td><p>Anthropic</p></td><td><p>Sonnet 3.7</p></td><td><p>64k Token Budget</p></td><td><p><strong>69.5%</strong></p></td><td><p>$0.392</p></td></tr><tr><td><p>Alibaba</p></td><td><p>Qwen 2.5 14B Instruct</p></td><td><p>None</p></td><td><p>28.1% → 59.4%</p></td><td><p><strong>$0.001</strong></p></td></tr><tr><td><p>Alibaba</p></td><td><p>Qwen 2.5 32B Instruct</p></td><td><p>None</p></td><td><p>37.3% → 67.1%</p></td><td><p>$0.002</p></td></tr></tbody></table></figure><p>From these benchmarks, we saw that Claude Sonnet 3.7 with a 64k token thinking budget performed best on our task, but that all the leading models showed room for improvement. DeepSeek R1, a popular open-weight model, performed nearly as well as OpenAI's o1 and o3-mini. However, the untuned Qwen 2.5 Instruct models’ performance is unimpressive in comparison. The big question is: Can we train these smaller, open-weight models to frontier-level performance? Elementary, our dear reader—we just need the right approach.</p><h3>Training</h3><p>To train a frontier-level deduction model, we turned to reinforcement learning—an approach that allows agents to learn from their own experience inside a controlled environment. Here, the LLMs were our agents, and the puzzles were our environment. We guided the LLMs’ learning by having them generate multiple responses for each puzzle, exploring the problem landscape. We reinforced deductions leading to correct solutions and penalized reasoning that took the models astray.</p><p>Among various RL methods, we selected the popular <!--$--><a href="https://arxiv.org/abs/2402.03300" rel="noopener">Group Relative Policy Optimization (GRPO)</a><!--/$--> algorithm developed by DeepSeek. GRPO simplifies the training process compared to more traditional methods like <!--$--><a href="https://en.wikipedia.org/wiki/Proximal_policy_optimization" rel="noopener">Proximal Policy Optimization (PPO)</a><!--/$-->, while still providing robust performance. To speed up our experiments, we omitted the &nbsp;<!--$--><a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener">Kullback–Leibler (KL) divergence</a><!--/$--> penalty, although our training recipe supports it for interested readers.</p><p>At a high level, our training loop followed these basic steps:</p><ul><li data-preset-tag="p"><p>Generate model responses to puzzle tasks</p></li><li data-preset-tag="p"><p>Grade responses and estimate advantages for each group of chat completions (that’s the “Group Relative” part in GRPO)</p></li><li data-preset-tag="p"><p>Fine-tune the model using clipped policy gradients guided by these advantage estimates</p></li><li data-preset-tag="p"><p>Repeat these steps with new puzzles and the latest version of the model until we reach peak performance</p></li></ul><p>For generating responses, we used the popular&nbsp;<!--$--><a href="https://blog.vllm.ai/2023/06/20/vllm.html" rel="noopener">vLLM</a><!--/$-->&nbsp;inference engine. We tuned our parameter choices to maximize throughput and minimize startup time. Prefix caching was particularly important because we sampled many responses for each task, and caching prompts helps avoid redundant computation.</p><p>We observed that overwhelming vLLM with too many requests forces preemption or swapping out of in-progress requests. To address this, we limited requests using a semaphore tuned to maintain high key-value (KV) cache utilization while minimizing swaps. More advanced scheduling mechanisms could yield even higher utilization while still supporting flexible generation lengths.</p><p>After sampling, we processed completions using the standard&nbsp;<!--$--><a href="https://huggingface.co/docs/transformers" rel="noopener">HuggingFace Transformers</a><!--/$-->&nbsp;<!--$--><a href="https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoTokenizer" rel="noopener">AutoTokenizer</a><!--/$-->. Its chat template feature, which renders message objects as a prompt string, includes an assistant mask for determining which tokens the LLM generated. We found the models lacked the necessary&nbsp;<code>% generation %</code>&nbsp;tags in their default templates, so we modified them during the tokenization step. The resulting assistant mask was included in the dictionary of tensors used for tuning, identifying which positions required loss calculations.</p><p>After tokenizing responses and obtaining assistant masks, we packed the data for tuning. In addition to including multiple prompt/response pairs in each packed sequence, we identified shared prompt tokens and assigned each token a Parent ID alongside the standard Group ID. Particularly for tasks like Temporal Clue—averaging over 1,000 tokens per puzzle—generating numerous responses per task and efficiently packing tensors significantly reduced redundancy. Once packed with all necessary information, we could visualize our training dataset two-dimensionally, each row being a sequence of tokens potentially containing multiple prompts and completions:</p><p><img alt="" data-framer-asset="data:framer/asset-reference,yTOg965WW2AFP43ZerKodVHRROE.png" data-framer-height="1990" data-framer-width="1469" height="995" src="https://framerusercontent.com/images/yTOg965WW2AFP43ZerKodVHRROE.png" srcset="https://framerusercontent.com/images/yTOg965WW2AFP43ZerKodVHRROE.png?scale-down-to=1024 755w,https://framerusercontent.com/images/yTOg965WW2AFP43ZerKodVHRROE.png 1469w" width="734" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (max-width: 1199px) 100vw"></p><p>With tightly-packed data in hand, we could proceed to tuning. Our models were already pre-trained, instruction-tuned, fairly intelligent, and adept at following instructions. However, they could not yet reliably solve Temporal Clue puzzles. Still, they occasionally succeeded, and that was enough. By increasing the probability of good reasoning and decreasing the probability of “not good” reasoning, we incrementally steered the models toward Master Detective status. We achieved this using standard machine learning techniques, employing policy gradient methods to compute loss and shift the weights beneficially.</p><p>For training, we used the&nbsp;<!--$--><a href="https://pytorch.org/torchtune" rel="noopener">torchtune</a><!--/$-->&nbsp;library provided by the PyTorch team. Torchtune features efficient decoder-only transformer implementations for popular models including Llama, Gemma, Phi, and more. Although we primarily used the Qwen models for this project, we also ran experiments with 8B and 70B Llama models. Torchtune also provides memory-saving and performance-enhancing utilities, including:</p><ul><li data-preset-tag="p"><p>Activation Checkpointing</p></li><li data-preset-tag="p"><p>Activation Offloading</p></li><li data-preset-tag="p"><p>Quantization</p></li><li data-preset-tag="p"><p><!--$--><a href="https://arxiv.org/abs/2312.12148" rel="noopener">Parameter-Efficient Fine-Tuning (PEFT)</a><!--/$-->, e.g.,&nbsp;<!--$--><a href="https://arxiv.org/abs/2106.09685" rel="noopener">Low Rank Adaptation (LoRA)</a><!--/$--></p></li></ul><p>See the&nbsp;<!--$--><a href="https://github.com/pytorch/torchtune?tab=readme-ov-file#optimization-flags" rel="noopener">README here</a><!--/$-->&nbsp;for the full list of supported optimizations.</p><p>Additionally, Torchtune supports multi-device (<!--$--><a href="https://pytorch.org/torchtune/main/tutorials/multinode.html" rel="noopener">and now multi-node</a><!--/$-->) training, making it ideal for larger models. It supports both Fully Sharded Data Parallel (FSDP) and Tensor Parallel (TP) training, which can be combined. They also provide&nbsp;<!--$--><a href="https://github.com/pytorch/torchtune/tree/main/recipes" rel="noopener">over a dozen recipes</a><!--/$-->, encouraging users to copy and customize them for their use cases. We created a modified version of their full fine-tune recipes supporting:</p><ul><li data-preset-tag="p"><p>Both multi-device and single-device training</p></li><li data-preset-tag="p"><p>Reference model loading and weight swapping for calculating KL divergences</p></li><li data-preset-tag="p"><p>Advanced causal mask calculations using group and parent IDs</p></li><li data-preset-tag="p"><p>GRPO loss integration and component logging</p></li></ul><p>The recipe can be seen <!--$--><a href="https://github.com/openpipe/deductive-reasoning/blob/main/lib/recipe.py" rel="noopener">here</a><!--/$-->. In the future, we would like to add tensor parallelism support and explore PEFT and quantization.</p><p>The RL training process involves selecting a myriad of hyperparameters. While training our models, we tested various configurations and largely settled upon the following:</p><ul><li data-preset-tag="p"><p>Models: Qwen 2.5 Instruct 14B &amp; 32B</p></li><li data-preset-tag="p"><p>Tasks per Iteration: 32</p></li><li data-preset-tag="p"><p>Samples per Task per Iteration: 50</p></li><li data-preset-tag="p"><p>Total Samples per Iteration: 32 * 50 = 1600</p></li><li data-preset-tag="p"><p>Learning Rate: 6e-6</p></li><li data-preset-tag="p"><p>Micro-Batch Size: 4 sequences for 14B model, 8 for 32B model</p></li><li data-preset-tag="p"><p>Batch Size: Variable, depending on the number of sequences</p></li></ul><p>The batch size is variable because response lengths can vary during training, sequence packing efficiency fluctuates each iteration, and responses with zero advantage are discarded. For one run, we tried dynamically adjusting learning rates inversely proportional to batch size, but this resulted in excessively high learning rates for small batch sizes, requiring a cap. The capped version didn’t meaningfully differ from using a constant learning rate, but tuning batch size and learning rate remains an interesting area for future experimentation.</p><p>We also ran brief experiments increasing tasks per iteration while reducing samples per task—and vice versa—keeping total samples per iteration roughly equal. Over a short training horizon, these variations showed no meaningful differences, suggesting the recipe is robust to different balances between the number of tasks and samples per task.</p><h3>Results</h3><p>After training our models for over 100 iterations, we reached frontier-level deduction.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,TnI1xBpe3Xn46Ql65su2Ad7p46I.png" data-framer-height="1078" data-framer-width="1626" height="539" src="https://framerusercontent.com/images/TnI1xBpe3Xn46Ql65su2Ad7p46I.png" srcset="https://framerusercontent.com/images/TnI1xBpe3Xn46Ql65su2Ad7p46I.png?scale-down-to=512 512w,https://framerusercontent.com/images/TnI1xBpe3Xn46Ql65su2Ad7p46I.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/TnI1xBpe3Xn46Ql65su2Ad7p46I.png 1626w" width="813" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (max-width: 1199px) 100vw"></p><p>Our models quickly improved before accuracy gains started to taper off and eventually degrade, sometimes aggressively. At their best, the 14B model approached Claude Sonnet 3.7’s performance at 16k tokens and the 32B model nearly matched Sonnet's results with the larger 64k budget.</p><p>While training, performance gains followed a power law, forming a linear relationship on a log-log chart (before deteriorating).</p><p><img alt="" data-framer-asset="data:framer/asset-reference,En7QbGZgBPfIwsBtipfujcDH6gY.png" data-framer-height="1078" data-framer-width="1626" height="539" src="https://framerusercontent.com/images/En7QbGZgBPfIwsBtipfujcDH6gY.png" srcset="https://framerusercontent.com/images/En7QbGZgBPfIwsBtipfujcDH6gY.png?scale-down-to=512 512w,https://framerusercontent.com/images/En7QbGZgBPfIwsBtipfujcDH6gY.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/En7QbGZgBPfIwsBtipfujcDH6gY.png 1626w" width="813" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (max-width: 1199px) 100vw"></p><p>We suspect the models may have converged too early on greedy strategies that worked out of the gate, but potentially limited their long-term prospects. A logical next step would explore approaches that encourage diverse responses, or that build capabilities incrementally (like with curriculum learning), or that assign larger rewards to particularly outstanding solutions incentivizing thorough exploration.</p><p>Additionally, we noted interesting patterns in output length during training. Initially, responses grew longer, then stabilized, before diverging near the end of training, with the 14B model’s responses getting longer and the 32B model’s response lengths collapsing, especially after reaching peak performance.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,jwjoe4Q3D2GPN5qKiA53kLu1fQ.png" data-framer-height="590" data-framer-width="1590" height="295" src="https://framerusercontent.com/images/jwjoe4Q3D2GPN5qKiA53kLu1fQ.png" srcset="https://framerusercontent.com/images/jwjoe4Q3D2GPN5qKiA53kLu1fQ.png?scale-down-to=512 512w,https://framerusercontent.com/images/jwjoe4Q3D2GPN5qKiA53kLu1fQ.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/jwjoe4Q3D2GPN5qKiA53kLu1fQ.png 1590w" width="795" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (max-width: 1199px) 100vw"></p><p>To qualitatively assess improvements in logical reasoning, we asked the strongest frontier model, Claude Sonnet 3.7, to identify and evaluate the soundness of deductions made by the Qwen 32B model—before and after training for 100+ iterations—on similar puzzles.&nbsp;<!--$--><a href="https://gist.github.com/bradhilton/c012ce9ed7b8e7c993b14b9761574a3c#file-base-md" rel="noopener">Sonnet identified 6 deductions from the base model, with all but one judged erroneous;</a><!--/$-->&nbsp;<!--$--><a href="https://gist.github.com/bradhilton/c012ce9ed7b8e7c993b14b9761574a3c#file-trained-md" rel="noopener">conversely, it identified 7 deductions from the trained model, with all but one judged logically sound.</a><!--/$--></p><p><img alt="" data-framer-asset="data:framer/asset-reference,r9dGPn71HUynBKCOZO5Sr03jInQ.png" data-framer-height="433" data-framer-width="1626" height="216" src="https://framerusercontent.com/images/r9dGPn71HUynBKCOZO5Sr03jInQ.png" srcset="https://framerusercontent.com/images/r9dGPn71HUynBKCOZO5Sr03jInQ.png?scale-down-to=512 512w,https://framerusercontent.com/images/r9dGPn71HUynBKCOZO5Sr03jInQ.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/r9dGPn71HUynBKCOZO5Sr03jInQ.png 1626w" width="813" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (max-width: 1199px) 100vw"></p><p>Finally, assuming <!--$--><a href="https://fireworks.ai/blog/why-gpus-on-demand" rel="noopener">sufficient throughput</a><!--/$--> with <!--$--><a href="https://fireworks.ai/pricing#ondemand" rel="noopener">on-demand deployments</a><!--/$-->, we estimated Qwen model costs from <!--$--><a href="https://fireworks.ai/" rel="noopener">Fireworks AI’s</a><!--/$--> <!--$--><a href="https://fireworks.ai/pricing#text" rel="noopener">serverless pricing tiers.</a><!--/$--> We plotted accuracy against the natural logarithm of the average inference cost per response, and observed a clear linear Pareto frontier among untuned models. By successfully training open-weight models to frontier-level accuracy, we dramatically improved the cost–accuracy trade-off.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,9Zwetqv69wY3YceuOG3KMeYyzKg.png" data-framer-height="1078" data-framer-width="1626" height="539" src="https://framerusercontent.com/images/9Zwetqv69wY3YceuOG3KMeYyzKg.png" srcset="https://framerusercontent.com/images/9Zwetqv69wY3YceuOG3KMeYyzKg.png?scale-down-to=512 512w,https://framerusercontent.com/images/9Zwetqv69wY3YceuOG3KMeYyzKg.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/9Zwetqv69wY3YceuOG3KMeYyzKg.png 1626w" width="813" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (max-width: 1199px) 100vw"></p><p>Here, after sharing satisfied nods for a job well done, we hail a hansom cab and return to Baker Street—the perfect place to contemplate our findings.</p><h3>Conclusion</h3><p>In our investigation, we set out to explore whether smaller, open-weight language models could achieve frontier-level deductive reasoning through reinforcement learning. After training Qwen 14B and 32B models on challenging Temporal Clue puzzles—using carefully selected hyperparameters and the GRPO method—we achieved impressive performance gains. These improvements brought open-weight models to the cutting edge of reasoning performance, at significantly reduced costs. Our findings highlight the promising potential for reinforcement learning to efficiently train open models on complex deduction tasks.</p><p>As mentioned previously, the <!--$--><a href="https://github.com/bradhilton/temporal-clue" rel="noopener">dataset</a><!--/$-->, <!--$--><a href="https://github.com/openpipe/rl-experiments" rel="noopener">experiments</a><!--/$-->, <!--$--><a href="https://github.com/openpipe/deductive-reasoning" rel="noopener">training recipe</a><!--/$-->, and model weights (<!--$--><a href="https://huggingface.co/OpenPipe/Deductive-Reasoning-Qwen-14B" rel="noopener">14B</a><!--/$-->, <!--$--><a href="https://huggingface.co/OpenPipe/Deductive-Reasoning-Qwen-32B" rel="noopener">32B</a><!--/$-->) are freely available under the MIT license. We encourage you to try reproducing and improving on our results.</p><p>Additionally, we’ve held out one particularly exciting finding for the end. We discovered that meaningful performance improvements, as high as 10–15%, can be achieved with as <strong>few as 16 training examples</strong>. This means you don’t need a lot of data to get started; just some intuition about the problem you’d like to solve.</p><p>Are you interested in using reinforcement learning to train your own models, or would like some help getting started? Feel free to <!--$--><a href="https://openpipe.ai/contact" rel="noopener">reach out to us at OpenPipe</a><!--/$-->—we'd love to chat!</p><p>Now, dear reader, please keep your deerstalker cap and magnifying glass handy; there's much more to explore. The game remains very much afoot.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Shelgon: A Framework for Building Interactive REPL Shells in Rust (103 pts)]]></title>
            <link>https://github.com/NishantJoshi00/shelgon</link>
            <guid>43284227</guid>
            <pubDate>Thu, 06 Mar 2025 19:32:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/NishantJoshi00/shelgon">https://github.com/NishantJoshi00/shelgon</a>, See on <a href="https://news.ycombinator.com/item?id=43284227">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">Shelgon <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/08d57aea49b14624ab0bb3cb13284f4881eb98c3630676167591a818c54ff373/68747470733a2f2f696d672e706f6b656d6f6e64622e6e65742f617274776f726b2f766563746f722f7368656c676f6e2e706e67"><img src="https://camo.githubusercontent.com/08d57aea49b14624ab0bb3cb13284f4881eb98c3630676167591a818c54ff373/68747470733a2f2f696d672e706f6b656d6f6e64622e6e65742f617274776f726b2f766563746f722f7368656c676f6e2e706e67" width="128" data-canonical-src="https://img.pokemondb.net/artwork/vector/shelgon.png"></a></h2><a id="user-content-shelgon-" aria-label="Permalink: Shelgon " href="#shelgon-"></a></div>
<p dir="auto">Shelgon is a robust Rust framework for building interactive REPL (Read-Eval-Print Loop) applications and custom shells. It provides a flexible, type-safe foundation with built-in terminal UI capabilities using <code>ratatui</code>.</p>
<p dir="auto"><a href="https://crates.io/crates/shelgon" rel="nofollow"><img src="https://camo.githubusercontent.com/0d227aec5f941ba77f076328aecb56ca87d140bf2f7db97ba6b94e4ad817739e/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f7368656c676f6e2e737667" alt="Crates.io" data-canonical-src="https://img.shields.io/crates/v/shelgon.svg"></a>
<a href="https://docs.rs/shelgon" rel="nofollow"><img src="https://camo.githubusercontent.com/6d3a6fb333012817acaef07380d99eb970e72b9ccf18adea728938c2cd4c8c92/68747470733a2f2f646f63732e72732f7368656c676f6e2f62616467652e737667" alt="Documentation" data-canonical-src="https://docs.rs/shelgon/badge.svg"></a>
<a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>🛡️ <strong>Type-safe Command Execution</strong> - Like Shelgon's protective shell, your commands are wrapped in a type-safe interface</li>
<li>🔄 <strong>Async Runtime Integration</strong> - Built on tokio for high-performance async operations</li>
<li>🎨 <strong>Beautiful TUI</strong> - Powered by ratatui with support for styling and colors</li>
<li>⌨️ <strong>Rich Input Handling</strong> - Complete keyboard interaction support including:
<ul dir="auto">
<li>Command history</li>
<li>Cursor movement</li>
<li>Tab completion</li>
<li>Ctrl+C/Ctrl+D handling</li>
</ul>
</li>
<li>📝 <strong>Custom Context Support</strong> - Maintain state between commands with your own context type</li>
<li>📥 <strong>STDIN Support</strong> - Handle multi-line input for commands that need it</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Add Shelgon to your <code>Cargo.toml</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[dependencies]
shelgon = &quot;0.1.0&quot;
tokio = { version = &quot;1.43.0&quot;, features = [&quot;full&quot;] }
anyhow = &quot;1.0.95&quot;"><pre>[<span>dependencies</span>]
<span>shelgon</span> = <span><span>"</span>0.1.0<span>"</span></span>
<span>tokio</span> = { <span>version</span> = <span><span>"</span>1.43.0<span>"</span></span>, <span>features</span> = [<span><span>"</span>full<span>"</span></span>] }
<span>anyhow</span> = <span><span>"</span>1.0.95<span>"</span></span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto">Create a simple echo shell:</p>
<div dir="auto" data-snippet-clipboard-copy-content="use shelgon::{command, renderer};

struct EchoExecutor {}

impl command::New for EchoExecutor {
    fn new() -> anyhow::Result<(Self, ())> {
        Ok((Self {}, ()))
    }
}

impl command::Execute for EchoExecutor {
    type Context = ();

    fn prompt(&amp;self, _: &amp;Self::Context) -> String {
        &quot;$&quot;.to_string()
    }

    fn execute(
        &amp;self,
        _: &amp;mut Self::Context,
        cmd: command::CommandInput,
    ) -> anyhow::Result<command::OutputAction> {
        Ok(command::OutputAction::Command(command::CommandOutput {
            prompt: cmd.prompt,
            command: cmd.command.clone(),
            stdin: cmd.stdin.unwrap_or_default(),
            stdout: vec![cmd.command],
            stderr: Vec::new(),
        }))
    }
}

fn main() -> anyhow::Result<()> {
    let rt = tokio::runtime::Runtime::new()?;
    let app = renderer::App::<EchoExecutor>::new(rt)?;
    app.execute()
}"><pre><span>use</span> shelgon<span>::</span><span>{</span>command<span>,</span> renderer<span>}</span><span>;</span>

<span>struct</span> <span>EchoExecutor</span> <span>{</span><span>}</span>

<span>impl</span> command<span>::</span><span>New</span> <span>for</span> <span>EchoExecutor</span> <span>{</span>
    <span>fn</span> <span>new</span><span>(</span><span>)</span> -&gt; anyhow<span>::</span><span>Result</span><span>&lt;</span><span>(</span><span>Self</span><span>,</span> <span>(</span><span>)</span><span>)</span><span>&gt;</span> <span>{</span>
        <span>Ok</span><span>(</span><span>(</span><span>Self</span> <span>{</span><span>}</span><span>,</span> <span>(</span><span>)</span><span>)</span><span>)</span>
    <span>}</span>
<span>}</span>

<span>impl</span> command<span>::</span><span>Execute</span> <span>for</span> <span>EchoExecutor</span> <span>{</span>
    <span>type</span> <span>Context</span> = <span>(</span><span>)</span><span>;</span>

    <span>fn</span> <span>prompt</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> _<span>:</span> <span>&amp;</span><span>Self</span><span>::</span><span>Context</span><span>)</span> -&gt; <span>String</span> <span>{</span>
        <span>"$"</span><span>.</span><span>to_string</span><span>(</span><span>)</span>
    <span>}</span>

    <span>fn</span> <span>execute</span><span>(</span>
        <span>&amp;</span><span>self</span><span>,</span>
        _<span>:</span> <span>&amp;</span><span>mut</span> <span>Self</span><span>::</span><span>Context</span><span>,</span>
        <span>cmd</span><span>:</span> command<span>::</span><span>CommandInput</span><span>,</span>
    <span>)</span> -&gt; anyhow<span>::</span><span>Result</span><span>&lt;</span>command<span>::</span><span>OutputAction</span><span>&gt;</span> <span>{</span>
        <span>Ok</span><span>(</span>command<span>::</span><span>OutputAction</span><span>::</span><span>Command</span><span>(</span>command<span>::</span><span>CommandOutput</span> <span>{</span>
            <span>prompt</span><span>:</span> cmd<span>.</span><span>prompt</span><span>,</span>
            <span>command</span><span>:</span> cmd<span>.</span><span>command</span><span>.</span><span>clone</span><span>(</span><span>)</span><span>,</span>
            <span>stdin</span><span>:</span> cmd<span>.</span><span>stdin</span><span>.</span><span>unwrap_or_default</span><span>(</span><span>)</span><span>,</span>
            <span>stdout</span><span>:</span> <span>vec</span><span>!</span><span>[</span>cmd<span>.</span>command<span>]</span><span>,</span>
            <span>stderr</span><span>:</span> <span>Vec</span><span>::</span><span>new</span><span>(</span><span>)</span><span>,</span>
        <span>}</span><span>)</span><span>)</span>
    <span>}</span>
<span>}</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> -&gt; anyhow<span>::</span><span>Result</span><span>&lt;</span><span>(</span><span>)</span><span>&gt;</span> <span>{</span>
    <span>let</span> rt = tokio<span>::</span>runtime<span>::</span><span>Runtime</span><span>::</span><span>new</span><span>(</span><span>)</span>?<span>;</span>
    <span>let</span> app = renderer<span>::</span><span>App</span><span>::</span><span>&lt;</span><span>EchoExecutor</span><span>&gt;</span><span>::</span><span>new</span><span>(</span>rt<span>)</span>?<span>;</span>
    app<span>.</span><span>execute</span><span>(</span><span>)</span>
<span>}</span></pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">Evolution Guide: Building Your Own Shell <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4832259768fb24274598bbd53dfea3ba4ba0235358cfda481b8ff1cbbcaa84bc/68747470733a2f2f696d672e706f6b656d6f6e64622e6e65742f617274776f726b2f766563746f722f73616c616d656e63652e706e67"><img src="https://camo.githubusercontent.com/4832259768fb24274598bbd53dfea3ba4ba0235358cfda481b8ff1cbbcaa84bc/68747470733a2f2f696d672e706f6b656d6f6e64622e6e65742f617274776f726b2f766563746f722f73616c616d656e63652e706e67" width="128" data-canonical-src="https://img.pokemondb.net/artwork/vector/salamence.png"></a></h2><a id="user-content-evolution-guide-building-your-own-shell-" aria-label="Permalink: Evolution Guide: Building Your Own Shell " href="#evolution-guide-building-your-own-shell-"></a></div>
<p dir="auto">Here's how to build a dragon-like shell with <code>shelgon</code>:</p>
<ol dir="auto">
<li><strong>Define Your Executor</strong>: Create a type that implements <code>command::Execute</code></li>
<li><strong>Create Your Context</strong>: Design a context type to maintain state between commands</li>
<li><strong>Implement Command Logic</strong>: Add your command execution logic in the <code>execute</code> method</li>
<li><strong>Add Tab Completion</strong>: Implement the <code>completion</code> method for smart suggestions</li>
<li><strong>Handle STDIN</strong>: Use the <code>prepare</code> method to indicate which commands need input</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">Check out the <a href="https://github.com/NishantJoshi00/shelgon/blob/main/examples">examples</a> directory for more advanced usage patterns, including:</p>
<ul dir="auto">
<li><code>echosh.rs</code>: A basic echo shell demonstrating core functionality</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome! Please feel free to submit a Pull Request. Before contributing, please:</p>
<ol dir="auto">
<li>Check existing issues or create a new one</li>
<li>Fork the repository</li>
<li>Create your feature branch (<code>git checkout -b feature/amazing-feature</code>)</li>
<li>Commit your changes (<code>git commit -m 'Add some amazing feature'</code>)</li>
<li>Push to the branch (<code>git push origin feature/amazing-feature</code>)</li>
<li>Open a Pull Request</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the MIT License - see the <a href="https://github.com/NishantJoshi00/shelgon/blob/main/LICENSE">LICENSE</a> file for details.</p>
<hr>
<p dir="auto">Built by Human, Documented by LLM.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rayhunter – Rust tool to detect cell site simulators on an orbic mobile hotspot (144 pts)]]></title>
            <link>https://github.com/EFForg/rayhunter</link>
            <guid>43283917</guid>
            <pubDate>Thu, 06 Mar 2025 19:04:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/EFForg/rayhunter">https://github.com/EFForg/rayhunter</a>, See on <a href="https://news.ycombinator.com/item?id=43283917">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9f49b5fbd88a547902f070f4d2fd25a48d4f39023e2d059c487a1d8c0e998e8b/68747470733a2f2f7777772e6566662e6f72672f66696c65732f7374796c65732f6d656469615f62726f777365725f707265766965772f7075626c69632f62616e6e65725f6c6962726172792f72617968756e7465722d62616e6e65722e706e67"><img src="https://camo.githubusercontent.com/9f49b5fbd88a547902f070f4d2fd25a48d4f39023e2d059c487a1d8c0e998e8b/68747470733a2f2f7777772e6566662e6f72672f66696c65732f7374796c65732f6d656469615f62726f777365725f707265766965772f7075626c69632f62616e6e65725f6c6962726172792f72617968756e7465722d62616e6e65722e706e67" alt="Rayhunter Logo - An Orca taking a bite out of a cellular signal bar" data-canonical-src="https://www.eff.org/files/styles/media_browser_preview/public/banner_library/rayhunter-banner.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Rayhunter</h2><a id="user-content-rayhunter" aria-label="Permalink: Rayhunter" href="#rayhunter"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/EFForg/rayhunter/actions/workflows/check-and-test.yml/badge.svg"><img src="https://github.com/EFForg/rayhunter/actions/workflows/check-and-test.yml/badge.svg" alt="Tests"></a></p>
<p dir="auto">Rayhunter is an IMSI Catcher Catcher for the Orbic mobile hotspot.</p>
<p dir="auto"><strong>THIS CODE IS PROOF OF CONCEPT AND SHOULD NOT BE RELIED UPON IN HIGH RISK SITUATIONS</strong></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The Hardware</h2><a id="user-content-the-hardware" aria-label="Permalink: The Hardware" href="#the-hardware"></a></p>
<p dir="auto">Code is built and tested for the Orbic RC400L mobile hotspot, it may work on other orbics and other
linux/qualcom devices but this is the only one we have tested on.
Buy the orbic <a href="https://www.amazon.com/Orbic-Verizon-Hotspot-Connect-Enabled/dp/B08N3CHC4Y" rel="nofollow">using bezos bucks</a>
Or on <a href="https://www.ebay.com/sch/i.html?_nkw=orbic+rc400l" rel="nofollow">Ebay</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto"><em>NOTE: We don't currently support automated installs on windows, you will have to follow the manual install instructions below</em></p>
<ol dir="auto">
<li>Download the latest <a href="https://github.com/EFForg/rayhunter/releases">rayhunter release bundle</a> and extract it.
<strong>If you are installing from the cloned github repository please see the development instructions below, running <code>install-linux.sh</code> from the git tree will not work.</strong></li>
<li>Run the install script inside the bundle corresponding to your platform (<code>install-linux.sh</code>, <code>install-mac.sh</code>). The Linux installer has only been tested on the latest version of Ubuntu. If it fails you will need to follow the install steps outlined in <strong>Development</strong> below.</li>
<li>Once finished, rayhunter should be running! You can verify this by visiting the web UI as described below.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Once installed, rayhunter will run automatically whenever your Orbic device is running. It serves a web UI that provides some basic controls, such as being able to start/stop recordings, download captures, and view heuristic analyses of captures. You can access this UI in one of two ways:</p>
<ol dir="auto">
<li>Over wifi: Connect your phone/laptop to the Orbic's wifi network and visit <code>http://192.168.1.1:8080</code> (click past your browser warning you about the connection not being secure, rayhunter doesn't have HTTPS yet!)
<ul dir="auto">
<li>Note that you'll need the Orbic's wifi password for this, which can be retrieved by pressing the "MENU" button on the device and opening the 2.4 GHz menu.</li>
</ul>
</li>
<li>Over usb: Connect the Orbic device to your laptop via usb. Run <code>adb forward tcp:8080 tcp:8080</code>, then visit <code>http://localhost:8080</code>. For this you will need to install the Android Debug Bridge (ADB) on your computer, you can copy the version that was downloaded inside the releases/platform-tools/` folder to somewhere else in your path or you can install it manually.  You can find instructions for doing so on your platform <a href="https://www.xda-developers.com/install-adb-windows-macos-linux/#how-to-set-up-adb-on-your-computer" rel="nofollow">here</a>, (don't worry about instructions for installing it on a phone/device yet).</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<ul dir="auto">
<li>Install ADB on your computer using the instructions above, and make sure it's in your terminal's PATH
<ul dir="auto">
<li>You can verify if ADB is in your PATH by running <code>which adb</code> in a terminal. If it prints the filepath to where ADB is installed, you're set! Otherwise, try following one of these guides:
<ul dir="auto">
<li><a href="https://askubuntu.com/questions/652936/adding-android-sdk-platform-tools-to-path-downloaded-from-umake" rel="nofollow">linux</a></li>
<li><a href="https://www.repeato.app/setting-up-adb-on-macos-a-step-by-step-guide/" rel="nofollow">macOS</a></li>
<li><a href="https://medium.com/@yadav-ajay/a-step-by-step-guide-to-setting-up-adb-path-on-windows-0b833faebf18" rel="nofollow">Windows</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">If your are on x86 linux</h3><a id="user-content-if-your-are-on-x86-linux" aria-label="Permalink: If your are on x86 linux" href="#if-your-are-on-x86-linux"></a></p>
<ul dir="auto">
<li>
<p dir="auto">on your linux laptop install rust the usual way and then install cross compiling dependences.</p>
</li>
<li>
<p dir="auto">run <code>sudo apt install  build-essential libc6-armhf-cross libc6-dev-armhf-cross gcc-arm-linux-gnueabihf</code></p>
</li>
<li>
<p dir="auto">set up cross compliing for rust:</p>
</li>
</ul>
<div data-snippet-clipboard-copy-content="rustup target add x86_64-unknown-linux-gnu
rustup target add armv7-unknown-linux-gnueabihf"><pre><code>rustup target add x86_64-unknown-linux-gnu
rustup target add armv7-unknown-linux-gnueabihf
</code></pre></div>
<p dir="auto">Now you can root your device and install rayhunter by running <code>./tools/install-dev.sh</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">If you are on windows or can't run the install scripts</h3><a id="user-content-if-you-are-on-windows-or-cant-run-the-install-scripts" aria-label="Permalink: If you are on windows or can't run the install scripts" href="#if-you-are-on-windows-or-cant-run-the-install-scripts"></a></p>
<ul dir="auto">
<li>
<p dir="auto">Root your device on windows using the instructions here: <a href="https://xdaforums.com/t/resetting-verizon-orbic-speed-rc400l-firmware-flash-kajeet.4334899/#post-87855183" rel="nofollow">https://xdaforums.com/t/resetting-verizon-orbic-speed-rc400l-firmware-flash-kajeet.4334899/#post-87855183</a></p>
</li>
<li>
<p dir="auto">Build for arm using <code>cargo build</code></p>
</li>
<li>
<p dir="auto">Run tests using <code>cargo test_pc</code></p>
</li>
<li>
<p dir="auto">Push the scripts in <code>scripts/</code> to /etc/init.d  on device and make a directory called /data/rayhunter using <code>adb shell</code> (and sshell for your root shell if you followed the steps above)</p>
</li>
<li>
<p dir="auto">you also need to copy <code>config.toml.example</code> to /data/rayhunter/config.toml</p>
</li>
<li>
<p dir="auto">Then run <code>./make.sh</code> this will build the binary and push it over adb. Restart your device or run <code>/etc/init.d/rayhunter_daemon start</code> on the device and you are good to go.</p>
</li>
<li>
<p dir="auto">Write your code and write tests</p>
</li>
<li>
<p dir="auto">Build for arm using <code>cargo build</code></p>
</li>
<li>
<p dir="auto">Run tests using <code>cargo test_pc</code></p>
</li>
<li>
<p dir="auto">push to the device with <code>./make.sh</code></p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<ul dir="auto">
<li>Build docs locallly using <code>RUSTDOCFLAGS="--cfg docsrs" cargo doc --no-deps --all-features  --open</code></li>
</ul>
<p dir="auto"><strong>LEGAL DISCLAIMER:</strong> Use this program at your own risk. We beilieve running this program does not currently violate any laws or regulations in the United States. However, we are not responsible for civil or criminal liability resulting from the use of this software. If you are located outside of the US please consult with an attorney in your country to help you assess the legal risks of running this program.</p>
<p dir="auto"><em>Good Hunting!</em></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Exposing Russian EFF Impersonators: The Inside Story on Stealc and Pyramid C2 (106 pts)]]></title>
            <link>https://hunt.io/blog/russian-speaking-actors-impersonate-etf-distribute-stealc-pyramid-c2</link>
            <guid>43283884</guid>
            <pubDate>Thu, 06 Mar 2025 19:01:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hunt.io/blog/russian-speaking-actors-impersonate-etf-distribute-stealc-pyramid-c2">https://hunt.io/blog/russian-speaking-actors-impersonate-etf-distribute-stealc-pyramid-c2</a>, See on <a href="https://news.ycombinator.com/item?id=43283884">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Open directories often expose more than just files--they provide a window into how malicious campaigns operate. In this case, we identified a <strong>threat actor impersonating the Electronic Frontier Foundation (EFF) to target the online gaming community</strong>. The exposed directory contained decoy documents alongside the malware used in this operation: Steal and Pyramid C2.</p><p>Further analysis linked 11 additional servers to the campaign through shared SSH keys, indicating a broad network footprint. Code comments found within <a href="https://hunt.io/blog/unboxing-the-threat-how-malicious-python-scripts-use-the-boxedapp-sdk-to-evade-detection?page=4" target="_blank" classname="link-hover">malicious Python</a> and PowerShell scripts suggest the work of a Russian-speaking developer. The tactics and malware observed align with financially motivated cybercrime activity. Hunt had already identified both <a href="https://hunt.io/glossary/command-and-control-server-c2" target="_blank" classname="link-hover">C2 servers</a> weeks earlier as part of routine scanning, but the <a href="https://hunt.io/glossary/open-directories" target="_blank" classname="link-hover">open directory</a> provided the link between the malware and this operation.</p><p>This post examines the role of the decoy documents and phishing attempts in the activity and explores how code analysis revealed additional infrastructure.</p><h2 id="Campaign_Overview">Campaign Overview</h2><p>A threat group impersonating the Electronic Frontier Foundation (EFF) is targeting Albion Online players through decoy documents designed to lend credibility while malware executes in the background.</p><p>Albion Online is a multiplayer online role-playing game (MMORPG) with a player-driven economy. While real-money transactions are against the game's terms of service and can result in permanent bans, third-party markets exist where in-game assets are exchanged for money, making player accounts a lucrative target.</p><p>Players on the game's forum have reported receiving messages from other members directing them to phishing websites, with the EFF's name used as a pretext to discuss the security of in-game goods tied to their accounts.</p><h2 id="Technical_Details">Technical Details</h2><h3>Open Directory</h3><p>On February 27th, Hunt's <a href="https://hunt.io/blog/attack-capture-launch" target="_blank" classname="link-hover">AttackCapture™</a> identified an open directory at http[:]//83.217.208[.]90/documents.</p><p>AttackCapture™ scans and archives files from exposed servers while categorizing malicious samples based on sandbox analysis, enabling quick reference during <a href="https://hunt.io/glossary/ttp-threat-hunting" target="_blank" classname="link-hover">threat hunting</a>. This particular server contained detections for PowerShell usage and <a href="https://hunt.io/malware-families/stealc" target="_blank" classname="link-hover">Stealc</a>, which immediately grabbed our attention.</p><p><img src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_1_screenshot_of_the_directory_contents_hosted_at_83_217_208_90_in_hunt_2x.webp" alt="Screenshot of the directory contents hosted at 83.217.208[.]90 in Hunt" title="" data-src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_1_screenshot_of_the_directory_contents_hosted_at_83_217_208_90_in_hunt_2x.webp" data-alt="Screenshot of the directory contents hosted at 83.217.208[.]90 in Hunt"></p><p><span>Figure 1: Screenshot of the directory contents hosted at 83.217.208[.]90 in <a href="https://app.hunt.io/open-directory-crawler?host=http%3A%2F%2F83.217.208.90" target="_blank" classname="link-hover">Hunt</a>.</span></p><p>The server hosted a mix of PDFs, ZIP archives, PowerShell scripts, and filenames with double extensions---common indicators of malware staging. While this section provides a brief overview of notable files, a detailed analysis follows in the next section.</p><h3>Infrastructure &amp; SSH Key Overlaps</h3><p>Let's first look at the IP address hosting the malware. Clicking on 'Hunt IP Search' in the Host section brings us to the overview page, which quickly shows two areas of interest.</p><p><img src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_2_hunt_overview_for_the_suspicious_ip_2x.webp" alt="Hunt overview for the suspicious IP" title="" data-src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_2_hunt_overview_for_the_suspicious_ip_2x.webp" data-alt="Hunt overview for the suspicious IP"></p><p><span>Figure 2: <a href="https://app.hunt.io/search/detail/83.217.208.90" target="_blank" classname="link-hover">Hunt</a> overview for the suspicious IP.</span></p><p>First, the 'Associations' tab displayed a pivot point revealing additional infrastructure. This IP address shares SSH keys (fingerprint <code>
        b48b0e3657560b80ce5e8309e422aa1655e4df2642d4a955b83945bac096b3f</code>) with 11 other IPs, all hosted on the Partner Hosting LTD network.</p><p>While no significant indicators were found linking these servers to other known operations, all remained active between early to mid-January 2025 before ceasing activity around February 21.</p><p>More interestingly, the icon next to the magnifying glass for port 80 identifies that Hunt has already detected this IP as hosting a <a href="https://malpedia.caad.fkie.fraunhofer.de/details/win.stealc" target="_blank" classname="link-hover">Stealc C2</a>. We currently track <a href="https://app.hunt.io/active-malware/Stealc#listing" target="_blank" classname="link-hover">23 unique</a> command-and-control servers associated with the stealer, with detailed information fully accessible to users.</p><h3>Decoy Documents and Phishing Strategy</h3><p>Within the <code>
        /albion/files</code> folder is a document titled <code>
        'Albion.pdf.'</code> Upon opening the file, readers are presented with what appears to be a report from the Electronic Frontier Foundation titled:</p><p><em>"Electronic Report on Investigation of Virtual Asset Theft in Albion Online."</em></p><p>Hunt researchers have not been able to verify the document's authenticity as of this article's publication.</p><p>*The EFF is a nonprofit that advocates for digital privacy, free expression, and cybersecurity protections while challenging government surveillance and online censorship.</p><p><img src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_3_suspicious_pdf_targeting_users_of_the_albion_online_game_2x.webp" alt="Suspicious PDF targeting users of the Albion online game" title="" data-src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_3_suspicious_pdf_targeting_users_of_the_albion_online_game_2x.webp" data-alt="Suspicious PDF targeting users of the Albion online game"></p><p><span>Figure 3: Suspicious PDF targeting users of the Albion online game.</span></p><p>The document is three pages in length, and informs the reader that EFF received a request from the administrators of the online game to analyze transactions on the individuals account.</p><p>After listing seemingly random item IDs linked to the potential victim's account, the report leads directly into the investigation results, informing the reader that unauthorized login attempts were detected and that stolen items were transferred to their account.</p><p>The report concludes with recommended steps to further secure the user's account and leaves only the URL for the EFF contact webpage for questions.</p><h3>Document Analysis</h3><p>Extracting metadata from the PDF using pdfinfo revealed several notable details:</p><ul><li><p>Creation Date: Feb 18, 2025</p></li><li><p>PDF Library: Skia/PDF m132</p></li><li><p>Title: <code>
        localhost:36223/webpageToPdf_67b3548070585_14546073906135025492.html</code></p></li><li><p>Creator: <code>
        Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/132.0.0.0 Safari/537.36</code></p></li></ul>
<p>The above suggests the PDF was generated programmatically rather than manually created. The title field indicates it was converted from an HTML page hosted on localhost, reinforcing our belief that this was an automated process to mass-generate lures for phishing campaigns.</p><p><img src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_4_results_of_running_pdfinfo_on_albion_2x.webp" alt="Results of running pdfinfo on Albion.pdf" title="" data-src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_4_results_of_running_pdfinfo_on_albion_2x.webp" data-alt="Results of running pdfinfo on Albion.pdf"></p><p><span>Figure 4: Results of running pdfinfo on Albion.pdf.</span></p><p>We won't discuss in detail the other PDF on the server, '1710407310845,' as we could not find proof of its use in the wild. The document appears to target individuals in India with DCMA takedown notices.</p><p>Of note, this document contained the below details, which were a departure from Albion.pdf:</p><ul><li><p>Title: New Applications_April 2023.xlsx</p></li><li><p>Author: shitesh</p></li><li><p>Creator: Acrobat PDFMaker 22 для Word ("для" translates to "for" in English)</p></li></ul>
<h3>Phishing Attempts Against Forum Users</h3><p>On 28 February, a user on the Albion Online forum (forum[.]albiononline[.]com) created a thread detailing phishing messages they had received. The messages, impersonating the EFF, attempted to lure players into engagement under the pretense of an investigation. Notably, the user ended their post by expressing frustration at the increasing volume of these messages, suggesting the campaign is widespread.</p><p><img src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_5_screenshot_of_forum_posts_describing_the_phishing_attempts_against_users_2x.webp" alt="Screenshot of forum posts describing the phishing attempts against users" title="" data-src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_5_screenshot_of_forum_posts_describing_the_phishing_attempts_against_users_2x.webp" data-alt="Screenshot of forum posts describing the phishing attempts against users"></p><p><span>Figure 5: Screenshot of forum posts describing the phishing attempts against users.</span></p><p>The moderator acknowledged the attempts and provided general security recommendations as the discussion continued. An additional user reported receiving the same message and attached a screenshot showing the sender and contents.</p><p>The image revealed another piece of attacker-controlled infrastructure hosting a PDF at: <code>
        act-7wbq8j3peso0qc1.pages[.]dev/819768.pdf</code></p><p><img src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_6_user_provided_screenshot_of_the_phishing_message_they_received_2x.webp" alt="User-provided screenshot of the phishing message they received" title="" data-src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_6_user_provided_screenshot_of_the_phishing_message_they_received_2x.webp" data-alt="User-provided screenshot of the phishing message they received"></p><p><span>Figure 6: User-provided screenshot of the phishing message they received.</span></p><p>We were unable to retrieve the document as requests to the page resulted in a perpetual loading page. The sender of these messages, "reraveca1977," created their forum account the same day, February 28, before likely wiping their activity. There have been no further posts/activity from this account as of this writing.</p><p>The forum discussions confirm that phishing messages were actively circulating, aligning with findings from the open directory and leading to additional attacker-controlled infrastructure hosting decoy documents.</p><h2 id="Malware_Analysis">Malware Analysis</h2><p>Looking into the /albion directory revealed a Windows shortcut (LNK) file designed to execute a PowerShell script, which facilitates malware delivery. The directory contains:</p><ul><li><p>Report-Albion-Online.lnk</p></li><li><p>/files/Python.zip</p></li><li><p>/files/Albion.pdf</p></li></ul>
<p><img src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_7_screenshot_of_the_files_contained_within_the_albion_directory_2x.webp" alt="Screenshot of the files contained within the /albion directory" title="" data-src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_7_screenshot_of_the_files_contained_within_the_albion_directory_2x.webp" data-alt="Screenshot of the files contained within the /albion directory"></p><p><span>Figure 7: Screenshot of the files contained within the /albion directory.</span></p><p>The LNK file executes PowerShell using an Execution Policy Bypass, running <code>
        albion.ps1</code>-a script located in /documents/pwsh/ on the same server. Once executed, the code retrieves Albion.pdf and Python.zip from the directory depicted in Figure 7.</p><p>The PowerShell code contains multiple comments in Russian, further supporting earlier indicators that Russian-speaking developers were involved in this operation. The script performs the following actions:</p><ol><li><p>Opens Albion.pdf to distract the user while the malware executes in the background.</p></li><li><p>Extracts Python.zip and sleeps for 30 seconds.</p></li><li><p>Searches for <code>
        pythonw.lnk</code>, a secondary shortcut file.</p></li><li><p>If found, it executes pythonw.lnk and moves it to the Windows Startup folder, ensuring persistence on reboot.</p></li><li><p>Drops <code>
        albion.exe</code> and <code>
        12.py</code> from the zip archive into the TEMP folder.</p><ul><li><p><code>
        albion.exe</code> is a renamed legitimate Python 3.10.8 executable, likely used to execute the accompanying script.</p></li><li><p><code>
        12.py</code> will be discussed below.</p></li></ul></li></ol>
<p>Reviewing the contents of <code>
        12.py</code> in a text editor, we found that the developer commented (in Russian) on every line of roughly 130+ lines of code.</p><p>The script includes logging, error checking, and, more importantly, two strings defined, <code>
        encoded_script_1</code> and <code>
        encoded_script_2</code>, obfuscated using zlib compression and base64 encoding. Upon decoding, both scripts are executed in the background while the main program sleeps for 30 minutes and then terminates the two processes at the end of the time.</p><p><img src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_8_snippet_of_12_2x.webp" alt="Snippet of 12.py" title="" data-src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_8_snippet_of_12_2x.webp" data-alt="Snippet of 12.py"></p><p><span>Figure 8: Snippet of 12.py.</span></p><p>Decoding the strings in CyberChef reveals configuration files for <a href="https://hunt.io/blog/tracking-pyramid-c2-identifying-post-exploitation-servers" target="_blank" classname="link-hover">Pyramid C2, an open-source framework we previously wrote about</a>, including tips on hunting for these servers in the wild.</p><h3>Extracted C2 Infrastructure</h3><p>encoded_script_1 contains the C2: 104.245.240[.]19:443</p><p><img src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_9_cyberchef_decoding_results_for_encoded_script_1_2x.webp" alt="CyberChef decoding results for encoded_script_1" title="" data-src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_9_cyberchef_decoding_results_for_encoded_script_1_2x.webp" data-alt="CyberChef decoding results for encoded_script_1"></p><p><span>Figure 9: CyberChef decoding results for encoded_script_1.</span></p><p>C2 for encoded_script_2: 212.87.222[.]84:443</p><p><img src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_10_cyberchef_decoding_results_for_encoded_script_2_2x.webp" alt="CyberChef decoding results for encoded_script_2" title="" data-src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_10_cyberchef_decoding_results_for_encoded_script_2_2x.webp" data-alt="CyberChef decoding results for encoded_script_2"></p><p><span>Figure 10: CyberChef decoding results for encoded_script_2.</span></p><p>Checking these IPs in Hunt, we were surprised to find that our scanners detected the second C2 as Pyramid C2 infrastructure roughly two weeks ago.</p><p><img src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_11_ip_overview_of_212_87_222_8_showing_it_as_a_pyramid_c2_in_hunt_2x.webp" alt="IP overview of 212.87.222[.]8, showing it as a Pyramid C2 in Hunt" title="" data-src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_11_ip_overview_of_212_87_222_8_showing_it_as_a_pyramid_c2_in_hunt_2x.webp" data-alt="IP overview of 212.87.222[.]8, showing it as a Pyramid C2 in Hunt"></p><p><span>Figure 11: IP overview of 212.87.222[.]8, showing it as a Pyramid C2 in <a href="https://app.hunt.io/search/detail/212.87.222.84" target="_blank" classname="link-hover">Hunt</a>.</span></p><h3>Pyramid C2 Behavior</h3><p>Pyramid C2 is designed to deliver files encrypted, a technique that may allow it to bypass endpoint detection and response (EDR) solutions. Prior research identified its use of Basic Authentication and a distinct JSON response format, which appeared again in this case. Reviewing the network communications from a malware sandbox analysis displayed an HTTP GET request to <code>
        http[:]//104.245.240[.]19:443/login/3keXipGb5Rr+gpGO9CjsSfdz+of5</code></p><p>The response is consistent with previously observed Pyramid C2 research, reinforcing its role in this campaign.</p><p><img src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_12_json_response_from_the_pyramid_c2_server_2x.webp" alt="JSON response from the Pyramid C2 server" title="" data-src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_12_json_response_from_the_pyramid_c2_server_2x.webp" data-alt="JSON response from the Pyramid C2 server"></p><p><span>Figure 12: JSON response from the Pyramid C2 server.</span></p><h3>Steal Communications</h3><p>Following the Pyramid C2 check-in, the malware initiates multiple POST requests to <code>
        http[:]//104.245.240[.]18/d7f85cd3e24a4757.php</code>.</p><p>These requests, made over port 80, align with Stealc stealer's standard check-in process. The malware proceeds to interact with the Firefox and Chrome browsers, extracting stored credentials before sending them back to the C2 server.</p><p><img src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_13_snippet_of_the_c2_communications_as_found_by_2x.webp" alt="Snippet of the C2 communications as found by Triage" title="" data-src="https://public-hunt-static-blog-assets.s3.us-east-1.amazonaws.com/3-2025/figure_13_snippet_of_the_c2_communications_as_found_by_2x.webp" data-alt="Snippet of the C2 communications as found by Triage"></p><p><span>Figure 13: Snippet of the C2 communications as found by <a href="https://tria.ge/250227-rkndbsyzcy/behavioral2" target="_blank" classname="link-hover">Triage</a>.</span></p><p>The remaining PowerShell script, <code>
        osnova.ps1</code>, functions identically to the previously analyzed code and, unfortunately, does not introduce any new tactics.</p><h2 id="Conclusion">Conclusion</h2><p>The recent campaign targeting the Albion Online gaming community underscores the evolving tactics of cyber adversaries. By impersonating reputable organizations like the Electronic Frontier Foundation (EFF), attackers disseminated phishing messages that directed users to malicious infrastructures hosting both decoy documents and malware.</p><p>This strategy not only exploits the trust users place in well-known entities but also leverages the immersive nature of gaming environments to increase the likelihood of successful compromises.​</p><p>Our investigation revealed that the threat actors mistakenly or intentionally left directories exposed where their malicious payloads were stored and distributed, a misstep that can easily go unnoticed without proactive monitoring. By analyzing these open directories, we identified the deployment of tools such as the Stealc stealer and Pyramid C2, highlighting the sophistication and resourcefulness of the adversaries.​</p><p><strong>Mitigation Strategies</strong></p><p>To stay safe against such phishing campaigns, users are advised to:</p><ul><li><p><strong>Exercise caution with unsolicited communications</strong>: Be wary of unexpected messages, especially those requesting personal information or urging immediate action.​</p></li><li><p><strong>Verify the authenticity of sources</strong>: Cross-check the legitimacy of emails or messages purportedly from reputable organizations by contacting them through official channels.</p></li><li><p><strong>Utilize security tools for link and attachment analysis</strong>: Before interacting with links or downloading attachments, employ sandbox services like <strong>URLScan</strong> or <strong>VirusTotal</strong> to assess potential threats. *Ensure you aren't uploading sensitive information first.</p></li></ul>
<h2 id="Infrastructure_Observables_and_IOCs">Infrastructure Observables and IOCs</h2><p><strong>SSH Fingerprint</strong>:  <code>
        b48b0e3657560b80ce5e8309e422aa1655e4df2642d4a955b83945bac096b3fb</code></p><h2 id="Network_Observables_and_IOCs">Network Observables and IOCs</h2><div><table><thead><tr><th>IP Address</th><th>ASN</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>83.217.208[.]90</td><td>Partner Hosting LTD</td><td>Opendir/Stealc C2 -- Port 80</td></tr>
<tr><td>104.245.240[.]19</td><td>Railnet LLC</td><td>Pyramid C2</td></tr>
<tr><td>212.87.222[.]84</td><td>GLOBAL CONNECTIVITY SOLUTIONS LLP</td><td>Pyramid C2 -- Port 443</td></tr>
<tr><td>185.102.115[.]18</td><td>Partner Hosting LTD</td><td>Shared SSH keys w/ 83.217.208[.]90</td></tr>
<tr><td>185.102.115[.]16</td><td>Partner Hosting LTD</td><td>Shared SSH keys w/ 83.217.208[.]90</td></tr>
<tr><td>185.102.115[.]20</td><td>Partner Hosting LTD</td><td>Shared SSH keys w/ 83.217.208[.]90</td></tr>
<tr><td>185.102.115[.]22</td><td>Partner Hosting LTD</td><td>Shared SSH keys w/ 83.217.208[.]90</td></tr>
<tr><td>83.217.208[.]108</td><td>Partner Hosting LTD</td><td>Shared SSH keys w/ 83.217.208[.]90 <br> Domain: immediate-zenar[.]net</td></tr>
<tr><td>185.102.115[.]17</td><td>Partner Hosting LTD</td><td>Shared SSH keys w/ 83.217.208[.]90</td></tr>
<tr><td>185.102.115[.]11</td><td>Partner Hosting LTD</td><td>Shared SSH keys w/ 83.217.208[.]90</td></tr>
<tr><td>185.102.115[.]14</td><td>Partner Hosting LTD</td><td>Shared SSH keys w/ 83.217.208[.]90</td></tr>
<tr><td>185.102.115[.]19</td><td>Partner Hosting LTD</td><td>Shared SSH keys w/ 83.217.208[.]90</td></tr>
<tr><td>185.102.115[.]21</td><td>Partner Hosting LTD</td><td>Shared SSH keys w/ 83.217.208[.]90</td></tr>
<tr><td>185.102.115[.]15</td><td>Partner Hosting LTD</td><td>Shared SSH keys w/ 83.217.208[.]90</td></tr>
<tr><td>N/A</td><td>CloudFlare, Inc.</td><td>act-7wbq8j3peso0qc1.pages[.]dev</td></tr>
</tbody>
</table></div><h2 id="Host_Observables_and_IOCs">Host Observables and IOCs</h2><div><table><thead><tr><th>Filename</th><th>SHA-256</th></tr>
</thead>
<tbody>
<tr><td>albion.ps1</td><td>a524d1acb0692fc90e20548d4bea29b4996c4113420942e43addd8c5609e29a4</td></tr>
<tr><td>osnova.ps1</td><td>4dcca5d3269eb44f3cf7af62c0da3b6acab67eb758c9fb2f5cc5b1d13a7286f7</td></tr>
<tr><td>Report-Albion-Online.lnk</td><td>cf8065df8674c2a09b3cb94f308c48f04a8664b066dd5107b117e99062f5621e</td></tr>
<tr><td>terms-of-service.pdf.lnk</td><td>cf8065df8674c2a09b3cb94f308c48f04a8664b066dd5107b117e99062f5621e</td></tr>
<tr><td>1710407310845.pdf</td><td>a7e617783d7f1b0079c605126fba074ee7ee431077cd97d391e41f364a0afe1b</td></tr>
<tr><td>Albion.pdf</td><td>b7612517337a7a3678e7f138dab36cd8a42e843f0536c0ccb74a2b0aa2224505</td></tr>
<tr><td>Python.zip (/albion/files/)</td><td>f60c212190a69149480586c9c9e340605dfa4b16a571f34b2ce31db4d0f7659a</td></tr>
<tr><td>12.py</td><td>aa89169a746709de1fd18510fc6e8850a863ebcc419ba0ca21fa479e59730c6e</td></tr>
<tr><td>albion.exe</td><td>56f1a4d528fdee439b5b747c00d0b4a61b2c0bd8783e0abdb87c6d969a8f1e91</td></tr>
<tr><td>Python.zip (/files/zip/)</td><td>3d3559a29f94bb349b928518dcf0c3757813e32195d16880e94169ca9affdede</td></tr>
</tbody>
</table></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Warewulf is a stateless and diskless container OS provisioning system (103 pts)]]></title>
            <link>https://github.com/warewulf/warewulf</link>
            <guid>43283669</guid>
            <pubDate>Thu, 06 Mar 2025 18:45:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/warewulf/warewulf">https://github.com/warewulf/warewulf</a>, See on <a href="https://news.ycombinator.com/item?id=43283669">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-turbo-body="">
      


    <div>
      <p><a href="#start-of-content" data-skip-target-assigned="false">Skip to content</a>

      <span data-view-component="true">
    <span data-view-component="true"></span>
</span></p>

<react-partial partial-name="keyboard-shortcuts-dialog" data-ssr="false" data-attempted-ssr="false">
  
  
  
</react-partial>




      

          

              


<header role="banner" data-is-top="true" data-color-mode="light" data-light-theme="light" data-dark-theme="dark">
  <h2>Navigation Menu</h2>

  

  <div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>GitHub Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;security_link_product_navbar&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code Review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
        <p>Code Search</p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>

                  <li>
      
      
</li>

                    <li>
      
      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      Events &amp; Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      
      <div>
              <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      
      <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
        <p>Enterprise platform</p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:warewulf/warewulf" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="EUsTHmyM0SvByO6ai--anyty-f0wkIsquMJRJDKoZNjE6wuC85lZNdFVhx9qLHR6eDXp9Mi_fqtUxPGSIMeGnA" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="warewulf/warewulf" data-current-org="warewulf" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=warewulf%2Fwarewulf" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/warewulf/warewulf&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="bb44ea6a49e53b64b7ad8331366a2a1111a2e04789a041d1f07524c000d04176" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a>
          
        </p></div>
      </div>
</header>

      
    </div>

  








    


    






  

          



    <ghcc-consent id="ghcc" data-initial-cookie-consent-allowed="" data-cookie-consent-required="true"></ghcc-consent>



  

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[50 Years in Filesystems: 1984 (144 pts)]]></title>
            <link>https://blog.koehntopp.info/2023/05/06/50-years-in-filesystems-1984.html</link>
            <guid>43283498</guid>
            <pubDate>Thu, 06 Mar 2025 18:34:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.koehntopp.info/2023/05/06/50-years-in-filesystems-1984.html">https://blog.koehntopp.info/2023/05/06/50-years-in-filesystems-1984.html</a>, See on <a href="https://news.ycombinator.com/item?id=43283498">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p>This is part 2 of a series. The first part is “<a href="https://blog.koehntopp.info/2023/05/05/50-years-in-filesystems-1974.html">1974</a>

”.</p>
<p>Progress is sometimes hard to see, especially when you have been part of it or otherwise lived through it.
Often, it is easier to see if you compare modern educational material, and the problems discussed with older material.
And then look for the research papers and sources that fueled the change.</p>
<p>In Linux (and Unix in general), this is easy.</p>
<h2 id="1984--the-bsd-fast-filing-system">
    <a href="#1984--the-bsd-fast-filing-system">
	1984 — The BSD Fast Filing System
    </a>
</h2>
<p>The original Unix filesystem was doing well, but also had a large number of obvious problems.
BSD Unix undertook an effort to fix them, and this is documented in the book
“<a href="https://www.amazon.de/Design-Implementation-4-3Bsd-Operating-System/dp/0201061961" target="_blank" rel="noopener">The Design and Implementation of the 4.3BSD UNIX Operating System</a>

”
by Leffler, McKusick et. al<a href="http://libgen.rs/book/index.php?md5=61457A629D5DE3B8966141A9D51FE89B" target="_blank" rel="noopener">.</a>

</p>
<p>A more concise, but also more academic discussion can be found in the classic 1984 paper <a href="https://dsf.berkeley.edu/cs262/FFS.pdf" target="_blank" rel="noopener">A Fast File System for UNIX</a>

,
which lists Marshall McKusick, Bill Joy (then at Sun), Samuel Leffler (then at LucasFilm) and Robert Fabry as authors.
The paper promises a reimplementation of the Unix filesystem for higher throughput, better allocation and better locality of reference.</p>
<h2 id="the-hardware">
    <a href="#the-hardware">
	The hardware
    </a>
</h2>
<p>It is 1984.
The computers targeted by 4.3BSD are desktop and cabinet workstations.
These are machines with 32-bit data registers and 32-bit address registers.</p>
<p>External data and address bus sizes vary:
Earlier 68k CPUs had smaller sized buses, but in 1984 the Motorola 68020 debuted.
It was the first 68k to offer buses with the full width of 32 bits, at a budget of ca. 200k transistors on the die.
Later the 68030 integrated the MMU, previously a separate chip,
and the 68040 also integrated the FPU, again previously a separate chip.</p>
<p>Early Sun workstations, the Sun-3 series, feature these CPUs.
But Sun took the designs from the experimental Berkeley RISC systems and released the Sun-4 series in 1986 with SPARC architecture RISC chips.
SPARC architecture is not without compromises, but was very viable and saw continuous development until after the purchase of Sun by Oracle, which then killed both the SPARC, and later also the Itanium CPU architecture.</p>
<p>Curt Schimmel discusses the tradeoffs made by SPARC in the MMU, register and memory access design, and why they made sense. See <a href="https://www.amazon.de/UNIX-Systems-Modern-Architectures-Multiprocessing/dp/0201633388" target="_blank" rel="noopener">UNIX Systems for Modern Architectures</a>

<a href="http://libgen.rs/book/index.php?md5=0E4A02E80A6250838CB1D3C3A1405CAD" target="_blank" rel="noopener">.</a>

</p>
<p>In between, in 1985, the MIPS architecture debuted, which is another series of RISC CPU architectures. It also starts out as a fully 32-bit type of system, and found use in SGI workstations.</p>
<p>HP had another RISC-type of CPU, the PA-RISC, an outgrowth of their “Spectrum” research programme, coming to market in 1986 (and later replaced by Intel’s failed Itanium).</p>
<p>Systems pioneer DEC themselves had the VAX, a 32-bit cabinet computer with a CISC CPU, and that since 1977 already.
They would not go RISC until 1992, but then fully 64-bit with the Alpha AXP (“DEC Alpha”) architecture.
While interesting, this did not last long: with the sale to Compaq in 1998, the CPU was discontinued, and the IP was sold to Intel in 2001.</p>
<p>In general, workstation type systems in 1984 had main memory in the low two-digit MB range, and ran at clock speeds of two-digit MHz system clocks.</p>
<h2 id="43bsds-fast-filing-system">
    <a href="#43bsds-fast-filing-system">
	4.3BSD’s Fast Filing System
    </a>
</h2>
<h2 id="the-traditional-filesystems-shortcomings">
    <a href="#the-traditional-filesystems-shortcomings">
	The traditional filesystems shortcomings
    </a>
</h2>
<p>The 32-bit VAX systems were being used for typical 1980’s workstation work, which include things such as image processing or VLSI chip design.
On these systems, the original Unix filesystem showed structural problems in keeping up with file size, I/O speed, and simple number of files.
Also, the tiny 512-byte I/O size slowed disk subsystem performance considerably.</p>
<p>The paper mentions the strict segregation of filesystem metadata at the front of the file system from the actual data in the back part of the filesystem.</p>
<blockquote>
<p>A 150 MB traditional UNIX file system consists of 4 megabytes of inodes followed by 146
megabytes of data.
This organization segregates the inode information from the data; thus accessing a file
normally incurs a long seek from the file’s inode to its data.
Files in a single directory are not typically
allocated consecutive slots in the 4 megabytes of inodes, causing many non-consecutive blocks of inodes to
be accessed when executing operations on the inodes of several files in a directory.</p></blockquote>
<p>This defines one major goal for BSD FFS: Better filesystem layout, bringing metadata and data closer together,
storing files in a single directory closer together,
and preventing fragmentation of a file into small fragments that can be loaded only inefficiently.</p>
<p>
  <img src="https://blog.koehntopp.info/uploads/2023/05/filesystem-fragmentierung.png" alt="">
</p>


<p><em>Fragmentation: Initially, four files are being created, each using 2 blocks.
Then the files B and D are being deleted.
The free space is then being reclaimed by the three-block-sized file E, which is stored in non-adjacent blocks.
This causes small disk seeks, and slow I/O.</em></p>
<p>Another goal stated is to increase disk block size.
Larger disk blocks benefit throughput in two ways:</p>
<ul>
<li>Larger disk blocks provide larger units of I/O, so more data is transferred in a single I/O operation.</li>
<li>Larger disk blocks also allow the filesystem to store more file pointers in an indirect block, greatly reducing the number of indirect block accesses.
This is primarily a problem if indirect blocks are not cached in a file system buffer cache.</li>
</ul>
<p>The paper quotes the throughput of an already marginally optimized, traditional Unix filesystem at around 4% of the theoretical maximum,
which is abysmally bad.
This is mainly attributed to fragmentation, non-contiguous storage of adjacent blocks in a file.
Defragmentation, already suggested in 1976, was discarded as a non-viable idea.
The authors instead aim for a solution that places files sensibly in the first place.</p>
<h2 id="bsd-ffs-innovations">
    <a href="#bsd-ffs-innovations">
	BSD FFS innovations
    </a>
</h2>
<h3 id="cylinder-groups-and-understanding-chs">
    <a href="#cylinder-groups-and-understanding-chs">
	Cylinder Groups and understanding CHS
    </a>
</h3>
<p>The BSD FFS understands the physical layout of a harddisk, with <a href="https://en.wikipedia.org/wiki/Cylinder-head-sector" target="_blank" rel="noopener">cylinders, heads and sectors</a>

 (CHS).
It divides the disk into cylinder groups, adjacent tracks of all disk heads.</p>
<p>
  <img src="https://blog.koehntopp.info/uploads/2023/05/cylinder-groups.png" alt="">
</p>


<p><em>As the disk rotates, various disk heads reach inside the platter stack like a comb.
Each head marks a track on the disk, which is subdivided into physical disk blocks by the controller hardware.
Together, all tracks marked by all heads form a cylinder.
A cylinder group is a set of consecutive cylinders. (Image: <a href="https://pages.cs.wisc.edu/~remzi/OSTEP/file-ffs.pdf" target="_blank" rel="noopener">OSTEP</a>

, page 3)</em></p>
<p>Each cylinder group becomes a mini-version of a traditional Unix filesystem, with a copy of the superblock, its own local inode area, and local inode and block usage bitmaps.
The usage of bitmaps is also novel, as they replace the free lists used in the traditional filesystem.
As the filesystem has information about the CHS layout, it also makes sure that the superblock is not always placed on the same platter for each copy,
trying to make the filesystem better redundant against harddisk failure.</p>
<blockquote>
<h2 id="excursion-raid-and-other-efforts-at-berkeley">
    <a href="#excursion-raid-and-other-efforts-at-berkeley">
	Excursion: Raid and other Efforts at Berkeley
    </a>
</h2>
<p>The <a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/1987/CSD-87-391.pdf" target="_blank" rel="noopener">RAID paper</a>

 was published only several years later,
but <a href="http://web.eecs.umich.edu/~michjc/eecs584/Papers/katz-2010.pdf" target="_blank" rel="noopener">according to Katz</a>

 was developed also in Berkeley, during the same time frame, 1983/1984.</p>
<p>Katz also mentions that during that time Stonebraker was around, working on Ingres (a Postgres predecessor),
and refers to his demands for low-commit latency as driving the attempts on improving disk bandwidth with FFS and, later,  RAID.
Serious work on the RAID taxonomy we know today did not begin before 1987, though.</p>
<p>The RAID paper was used by many startups and storage companies as the foundation of their development,
among them NetApp, and EMC (via Data General’s Clariion Disk Array)</p></blockquote>
<p>BSD FFS not only understood CHS geometry of disks, but also processor speed and disk rotational speed.
This allowed it to configure and record in the superblock an <a href="https://en.wikipedia.org/wiki/Interleaving_%28disk_storage%29" target="_blank" rel="noopener">interleave factor</a>

 to optimize disk I/O throughput.</p>
<p>
  <img src="https://blog.koehntopp.info/uploads/2023/05/interleave.jpg" alt="">
</p>


<p><em>The harddisk rotates continuously, but the CPU needs time to set up the next transfer.
During this time the head may have moved already past the next block start boundary, and now the system would need to wait one full rotation to be able to write.
Using an appropriate interleave factor, blocks of adjacent numbers are not stored adjacently on disk, but instead other blocks are interleaved in-between.
This gives the CPU enough time to think and set up the next block transfer.</em></p>
<p><em>The faster the CPU, the lower the interleave factor required.</em></p>
<p>All of these optimizations became irrelevant relatively quickly the moment harddrives were sold with integrated controllers,
started to lie about their CHS geometry and ultimately as linear block addresses (LBA) took over.
But for ten to 15 years, this provided a nice performance advantage.</p>
<h3 id="large-blocks-smaller-fragments-and-tail-packing">
    <a href="#large-blocks-smaller-fragments-and-tail-packing">
	Large blocks, smaller fragments, and tail packing
    </a>
</h3>
<p>Internally, FFS uses logical blocks of at least 4 KB size.
Anything with at least 4 KB block size can create files of 4 GB size with at most two levels of indirection.</p>
<p>Large blocks make for faster I/O, but they also come with storage overhead, as files grow in sizes of blocks.
Since logical blocks in FFS are made up from multiple physical blocks, FFS introduces the concept of fragments to expose the smaller internal physical blocks.
Through tail packing, the ends of multiple files can be stored together in the same logical block, using only as many physical blocks as necessary.</p>
<p>Additional logic was necessary to prevent a slowly growing file from going through phases of fragment-by-fragment growth and constant re-layouting.
To overcome this, space is being pre-allocated to full logical blocks, and tail packing only happens on file close when the preallocation is canceled.</p>
<h3 id="long-seek-layout-policy">
    <a href="#long-seek-layout-policy">
	Long Seek Layout Policy
    </a>
</h3>
<p>BSD FFS introduces a number of layout policies that control the placement of new directories, new files and the handling of large files.
Global policies are mostly concerned with choosing a well-suited cylinder group to place data in,
while local policies then handle the placement inside a cylinder group.</p>
<p>The new filesystem layout has cylinder groups. Each has their own inode table, and free space bitmaps for inodes and blocks.
The filesystem aims to prevent fragmentation.</p>
<p>This is of course impossible in certain circumstances:
If, for example, a cylinder group is 512 MB in size, and a file larger than 512 MB is to be written, it will use up one inode in that cylinder group, but all available free blocks are gone.
If a second file is to be placed into this cylinder group, the inode can be used, but the data blocks for that file need to be placed somewhere else – which is undesirable.</p>
<p>It would be better to force a long seek, a switch from one cylinder group to the next, for large files.
The filesystem would profit from forcing such a long seek every megabyte of filesize or so.
This would use up free blocks from one cylinder group to the next, evenly, while at the same time leaving some number of free blocks for other files in each cylinder group.</p>
<p>This would, of course, fragment a file, on purpose, but also make sure the fragments are sufficiently large to allow large file I/O.
Fragmentation (non-adjacent placement of blocks in a file) is only really a performance problem if the fragments are too small to be read efficiently.</p>
<h3 id="directory-layout-policy">
    <a href="#directory-layout-policy">
	Directory Layout Policy
    </a>
</h3>
<p>Files in the same directory are often used together.
It is useful to place all files in the same directory together in the same cylinder group.</p>
<p>Of course, when this is done, it is also necessary to put different directories into different cylinder groups, to ensure even use of the filesystem space available.
That means a shell script such as</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span><span>#! /usr/bin/bash
</span></span></span><span><span><span></span>
</span></span><span><span><span>for</span> i in <span>$(</span>seq -w <span>1</span> 10<span>)</span>
</span></span><span><span><span>do</span>
</span></span><span><span>  touch file<span>$i</span>
</span></span><span><span>  mkdir dir<span>$i</span>
</span></span><span><span><span>done</span>
</span></span></code></pre></div><p>will create ten files named <code>fileXX</code>, which will all be placed in the same cylinder group as the current directory.</p>
<p>It will also create ten subdirectories of the current directory named <code>dirXX</code>.
Each of them will be placed in a different cylinder group, if possible.
FFS will choose the cylinder group that has a greater than average number of free inodes, and the smallest number of directories already in it.</p>
<p>The actual choice of the inode in a cylinder group is “next available”, so pretty simple.
But that is not a problem, because the whole cylinder group inode table fits into 8-16 blocks.</p>
<p>For placement of data blocks, a lot of effort is invested into finding rotationally optimal block, given the needed interleave factor for this machine.</p>
<p>BSD FFS requires some free space to be available in the filesystem at all times.
Many of its algorithms degenerate to the performance of the traditional file system if the filesystem fills up more than 90%.</p>
<h2 id="other-changes-and-improvements">
    <a href="#other-changes-and-improvements">
	Other changes and improvements
    </a>
</h2>
<p>BSD FFS also removes several limits that came with the traditional filesystem.</p>
<h3 id="long-inode-numbers-and-block-addresses">
    <a href="#long-inode-numbers-and-block-addresses">
	Long Inode Numbers and Block Addresses
    </a>
</h3>
<p>For example, <a href="https://github.com/dspinellis/unix-history-repo/blob/BSD-4_3_Tahoe-Snapshot-Development/.ref-BSD-4_3/usr/src/sys/h/dir.h#L42" target="_blank" rel="noopener">Inode numbers are now 32-bit numbers</a>

.
This increases the number of files possible per filesystem from 64 K to 4 G.</p>
<p>The size of an <a href="https://github.com/dspinellis/unix-history-repo/blob/BSD-4_3_Tahoe-Snapshot-Development/.ref-BSD-4_3/usr/src/sys/h/inode.h#L40-L59" target="_blank" rel="noopener">inode</a>

 has doubled:
It is now <a href="https://github.com/dspinellis/unix-history-repo/blob/BSD-4_3_Tahoe-Snapshot-Development/.ref-BSD-4_3/usr/src/sys/h/inode.h#L61-L65" target="_blank" rel="noopener">forced to be 128 bytes</a>

 in size (with 20 unused bytes)
Also, disk block addresses are now 4 bytes.
At 4 KB block size, this is sufficient to account for 4 G blocks, or a maximum of 16 TB filesystem size.<br>
File length is recorded in a <code>quad</code>, allowing for more than 4 G individual filesize.</p>
<p>Inodes now contain 12 direct blocks, and three types of indirect blocks.
At 4 KB block size, this is good for 1024 block addresses per indirect block, resulting in
<code>12 + 1024 + 1024^2 + 1024^3 = 1074791436</code> blocks per file, or a maximum filesize just north of 4 TB.</p>
<p>Unix User-ID and Group-ID are still limited to a short, limiting the number of users and groups per system to 64 K.</p>
<p>Space has been preallocated for 8-byte timestamps, even if the time types in the inode are still limited to 4 bytes.</p>
<h3 id="long-filenames">
    <a href="#long-filenames">
	Long filenames
    </a>
</h3>
<p>The traditional filesystem has directory slots of a fixed 16-byte length,
with 2 bytes for the inode number and 14 bytes for the filename.</p>
<p>BSD FFS defined a <a href="https://github.com/dspinellis/unix-history-repo/blob/BSD-4_3_Tahoe-Snapshot-Development/.ref-BSD-4_3/usr/src/sys/h/inode.h#L61-L65" target="_blank" rel="noopener">more complex directory entry structure</a>

.
A single entry contains a 4-byte inode number, a 2-byte record length and a 2-byte name length, and then the actual filename.
Filenames are limited to 255 bytes for each pathname component,
and directory entries are rounded up in length to the next 4-byte boundary.</p>
<p>Directories are still essentially a linked list, and searching for names in large directories is slow.</p>
<p>Searching for free space in directories is now more complicated:
To create a new directory entry, we now need to search through the directory from the start, trying to find a gap in the current structure that is large enough for the name we are being asked to create.
If none is found, the new name is appended at the end, growing the directory in size.</p>
<p>Free space in directories is never reclaimed through compaction, only eventually re-used if a new name happens to fit.</p>
<h3 id="symlinks">
    <a href="#symlinks">
	Symlinks
    </a>
</h3>
<p>The traditional filesystem allowed a file to have multiple names, using the <code>link()</code> system call and the hardlink mechanism.
Hardlinks are limited in number (a <code>short</code>, so 64 K names).</p>
<p>They can be lost accidentally, for example, by saving a hardlinked file with certain editors.
If the editor does write a file as <code>filename.new</code>, then unlinks the old <code>filename</code> and moves the new file into place, the hardlinked nature of the file will be modified.</p>
<p>Hardlinks also reference the original inode of the file multiple times, so they cannot span filesystem boundaries.</p>
<p>BSD introduces a new filetype (<code>l</code>, symlink), and places a “replacement filename” in the linked file, which determines the link target location.
It can be an absolute or relative name (relative to the location of the symlink file).</p>
<p>This creates a “soft” or “symbolic link.
Trying to access a symlink will kick off a reinterpretation of the filename in <code>namei()</code> using the replacement filename,
resulting in the attempted <code>open()</code> system call being deflected to the link target location.</p>
<p>Since the deflection happens in <code>namei()</code>, which can traverse filesystem boundaries, the new link type is not subject to the single filesystem limitation.
It is also not counting towards any link count limits.</p>
<h3 id="rename-system-call">
    <a href="#rename-system-call">
	Rename System Call
    </a>
</h3>
<p>BSD introduces the <code>rename()</code> system call, which previously needed to be implemented as a library function using calls to <code>unlink()</code> and <code>link()</code>.
Since this uses more than one system call, the operation is not atomic:
It is subject to partial execution, and it is subject to malicious interferences, because it is a multistep process.</p>
<h3 id="quotas">
    <a href="#quotas">
	Quotas
    </a>
</h3>
<p>BSD also introduces the idea of filesystem usage quotas:
These are soft and hard limits on the number of files and the amount of disk space that a user or a group can use.</p>
<p>In order to implement them in a useful way, the behavior of the filesystem had to be modified:</p>
<ul>
<li>It is now a privileged operation to change the owner of a file away from oneself.
Without that, it is possible to create a directory that is only accessible for oneself, and then gift all files in it to another user.
The files would then count against that user’s quota.</li>
<li>Similarly, it is now no longer possible to change the group membership of files to just any group.
Instead, only groups from the user’s group set can be used.</li>
<li>And finally, new directories and files inherit their group from their parent directory, not from a users primary group.
That way, project directories would contain files counting against a project’s quota, not a user’s primary group quota.</li>
</ul>
<h3 id="advisory-locking">
    <a href="#advisory-locking">
	Advisory Locking
    </a>
</h3>
<p>Advisory file locking is already introduced in 4.2BSD.
For this, the new <code>flock()</code> syscall has been implemented.</p>
<ul>
<li>Locks can be shared (read locks) or exclusive (write locks).</li>
<li>They always apply to the entire file, and not to byte ranges.</li>
<li>No deadlock detection is attempted.</li>
<li>They are tied to a file descriptor.
So when a process dies, its file-handles are automatically closed, which also automatically releases all locks held.
This is very robust, until <code>dup()</code> and <code>fork()</code> are coming into play.</li>
</ul>
<p>Posix later tried to improve on this, introducing a second, completely different system of locks, using <code>fcntl()</code>.
This is flawed in different ways, but can do byte-ranges, and it implements some rudimentary deadlock detection.</p>
<p>Kernels that implement both systems such as Linux now have two different,
incompatible file locking implementations that do not know of each other.</p>
<p><a href="https://loonytek.com/2015/01/15/advisory-file-locking-differences-between-posix-and-bsd-locks/" target="_blank" rel="noopener">This article</a>

 discusses all of this some more,
and has example programs.</p>
<h2 id="performance">
    <a href="#performance">
	Performance
    </a>
</h2>
<p>The authors note the following advantages in their paper:</p>
<ul>
<li><code>ls</code> and <code>ls -l</code> are fast, because the inodes of the files in a single directory are within the same cylinder group.
Hence, reading and listing a directory is very low on seeks, and on seek distance (except for subdirectories, which are guaranteed to be far away).
They measure a 8x speedup for directories without subdirectories.</li>
<li>Utilization of the theoretical maximal bandwidth increased from 3% in the traditional filesystem to 22% or even 47%, depending on the controller hardware used.
The authors are very proud of the results because they have been achieved on an actual production system with real user production data being layouted,
and not on a synthetic benchmark layout. Throughput is stable over the lifetime of the filesystem, as its file population changes.</li>
</ul>
<p>This solves the main drivers for the improvements: Better throughput and a stable layout that does not degrade performance over time.</p>
<p>Additionally, a number of quality-of-life enhancements have been made, enabling more comfortable working in groups, and unlocking new functionality.</p>
<p>While Linux contains no BSD code, the ext2 filesystem is pretty much an implementation-blind rewrite of the BSD FFS for Linux,
recreating the features as described in the literature without using any BSD code.</p>
<p>Both BSD FFS and Linux ext2 are still non-logging filesystems that require a filesystem check after a crash.
They also cannot deal well with directories with many entries, and deal only slightly better with deep directory hierarchies.
Additional changes are required to enable truly large filesystems in order to keep up with increasing storage sizes.</p>
<p>Also, other limitations of more hidden nature still apply:
Several places in the filesystem code are guarded by locks that make scaling certain operations hard on systems with high concurrency.</p>
<p>It would take another ten years, until 1994, for SGI’s XFS to tackle these things.</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Succinct Data Structures (391 pts)]]></title>
            <link>https://blog.startifact.com/posts/succinct/</link>
            <guid>43282995</guid>
            <pubDate>Thu, 06 Mar 2025 17:48:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.startifact.com/posts/succinct/">https://blog.startifact.com/posts/succinct/</a>, See on <a href="https://news.ycombinator.com/item?id=43282995">Hacker News</a></p>
Couldn't get https://blog.startifact.com/posts/succinct/: Error: connect EHOSTUNREACH 2a01:7c8:aab6:68:5054:ff:fe1a:c852:443]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral OCR (1394 pts)]]></title>
            <link>https://mistral.ai/fr/news/mistral-ocr</link>
            <guid>43282905</guid>
            <pubDate>Thu, 06 Mar 2025 17:39:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/fr/news/mistral-ocr">https://mistral.ai/fr/news/mistral-ocr</a>, See on <a href="https://news.ycombinator.com/item?id=43282905">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p dir="ltr">Throughout history, advancements in information abstraction and retrieval have driven human progress. From hieroglyphs to papyri, the printing press to digitization, each leap has made human knowledge more accessible and actionable, fueling further innovation.&nbsp;</p>
<div dir="ltr"><p>Today, we’re at the precipice of the next big leap—to unlock the collective intelligence of all digitized information. Approximately&nbsp;<a href="https://resources.data.gov/glossary/unstructured-data/">90%</a> of the world’s organizational data is stored as documents, and to harness this potential, we are introducing <a href="https://docs.mistral.ai/capabilities/document/">Mistral OCR</a>. </p><p>Mistral OCR is an Optical Character Recognition API that sets a new standard in document understanding. Unlike other models, Mistral OCR comprehends each element of documents—media, text, tables, equations—with unprecedented accuracy and cognition. It takes images and PDFs as input and extracts content in an ordered interleaved text and images.</p></div>
<p dir="ltr">As a result, Mistral OCR is an ideal model to use in combination with a RAG system taking multimodal documents (such as slides or complex PDFs) as input.</p>
<p dir="ltr">We have made Mistral OCR as the default model for document understanding across millions of users on Le Chat, and are releasing the API <em>mistral-ocr-latest</em> at 1000 pages / $ (and approximately double the pages per dollar with batch inference). The API is available today on our developer suite&nbsp;<a href="http://console.mistral.ai/">la Plateforme</a>, and coming soon to our cloud and inference partners, as well as on-premises.</p>
<h2 dir="ltr">Highlights</h2>
<ol>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">State of the art understanding of complex documents</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Natively multilingual and multimodal</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Top-tier benchmarks</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Fastest in its category</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Doc-as-prompt, structured output</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Selectively available to self-host for organizations dealing with highly sensitive or classified information</p>
</li>
</ol>
<p dir="ltr">Let’s dive into each.&nbsp;</p>
<h3 dir="ltr">State of the art understanding of complex documents</h3>
<p dir="ltr">Mistral OCR excels in understanding complex document elements, including interleaved imagery, mathematical expressions, tables, and advanced layouts such as LaTeX formatting. The model enables deeper understanding of rich documents such as scientific papers with charts, graphs, equations and figures.&nbsp;</p>
<p dir="ltr">Below is an example of the model extracting text as well as imagery from a given PDF into a markdown file. You can access the notebook <a href="https://colab.research.google.com/drive/11NdqWVwC_TtJyKT6cmuap4l9SryAeeVt?usp=sharing" target="_blank" rel="noopener">here</a>.&nbsp;</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/6lRBm0KnzBI?si=qLSblC2rsBdxg4qu" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen"></iframe></p>
<p dir="ltr">Below we have side-by-side comparisons of PDFs and their respective OCR's outputs. Hover the slider&nbsp; to switch between input and output.&nbsp;</p>





<h3 dir="ltr">Top-tier benchmarks</h3>
<p dir="ltr">Mistral OCR has consistently outperformed other leading OCR models in rigorous benchmark tests. Its superior accuracy across multiple aspects of document analysis is illustrated below. We extract embedded images from documents along with text. The other LLMs compared below, do not have that capability. For a fair comparison, we evaluate them on our internal “text-only” test-set containing various publication papers, and PDFs from the web; below:</p>
<div dir="ltr">
<table><colgroup> <col width="178"> <col width="105"> <col width="108"> <col width="106"> <col width="103"> <col width="101"> </colgroup>
<thead>
<tr>
<th>Model</th>
<th>Overall</th>
<th>Math</th>
<th>Multilingual</th>
<th>Scanned</th>
<th>Tables</th>
</tr>
</thead>
<tbody>
<tr>
<td>Google Document AI</td>
<td>83.42</td>
<td>80.29</td>
<td>86.42</td>
<td>92.77</td>
<td>78.16</td>
</tr>
<tr>
<td>Azure OCR</td>
<td>89.52</td>
<td>85.72</td>
<td>87.52</td>
<td>94.65</td>
<td>89.52</td>
</tr>
<tr>
<td>Gemini-1.5-Flash-002</td>
<td>90.23</td>
<td>89.11</td>
<td>86.76</td>
<td>94.87</td>
<td>90.48</td>
</tr>
<tr>
<td>Gemini-1.5-Pro-002</td>
<td>89.92</td>
<td>88.48</td>
<td>86.33</td>
<td>96.15</td>
<td>89.71</td>
</tr>
<tr>
<td>Gemini-2.0-Flash-001</td>
<td>88.69</td>
<td>84.18</td>
<td>85.80</td>
<td>95.11</td>
<td>91.46</td>
</tr>
<tr>
<td>GPT-4o-2024-11-20</td>
<td>89.77</td>
<td>87.55</td>
<td>86.00</td>
<td>94.58</td>
<td>91.70</td>
</tr>
<tr>
<td>Mistral OCR 2503</td>
<td>94.89</td>
<td>94.29</td>
<td>89.55</td>
<td>98.96</td>
<td>96.12</td>
</tr>
</tbody>
</table>
</div>
<h3 dir="ltr">Natively multilingual</h3>
<p dir="ltr">Since Mistral’s founding, we have aspired to serve the world with our models, and consequently strived for multilingual capabilities across our offerings. Mistral OCR takes this to a new level, being able to parse, understand, and transcribe thousands of scripts, fonts, and languages across all continents. This versatility is crucial for both global organizations that handle documents from diverse linguistic backgrounds, as well as hyperlocal businesses serving niche markets.</p>
<div dir="ltr">
<table><colgroup> <col width="236"> <col width="203"> </colgroup>
<thead>
<tr>
<th>Model</th>
<th>Fuzzy Match in Generation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Google-Document-AI</td>
<td>95.88</td>
</tr>
<tr>
<td>Gemini-2.0-Flash-001</td>
<td>96.53</td>
</tr>
<tr>
<td>Azure OCR</td>
<td>97.31</td>
</tr>
<tr>
<td>Mistral OCR</td>
<td>99.02</td>
</tr>
</tbody>
</table>
</div>


<div dir="ltr">
<table><colgroup> <col width="133"> <col width="123"> <col width="126"> <col width="170"> </colgroup>
<thead>
<tr>
<th>Language</th>
<th>Azure OCR</th>
<th>Google Doc AI</th>
<th>Mistral OCR</th>
</tr>
</thead>
<tbody>
<tr>
<td>ru</td>
<td>97.35</td>
<td>95.56</td>
<td>99.09</td>
</tr>
<tr>
<td>fr</td>
<td>97.50</td>
<td>96.36</td>
<td>99.20</td>
</tr>
<tr>
<td>hi</td>
<td>96.45</td>
<td>95.65</td>
<td>97.55</td>
</tr>
<tr>
<td>zh</td>
<td>91.40</td>
<td>90.89</td>
<td>97.11</td>
</tr>
<tr>
<td>pt</td>
<td>97.96</td>
<td>96.24</td>
<td>99.42</td>
</tr>
<tr>
<td>de</td>
<td>98.39</td>
<td>97.09</td>
<td>99.51</td>
</tr>
<tr>
<td>es</td>
<td>98.54</td>
<td>97.52</td>
<td>99.54</td>
</tr>
<tr>
<td>tr</td>
<td>95.91</td>
<td>93.85</td>
<td>97.00</td>
</tr>
<tr>
<td>uk</td>
<td>97.81</td>
<td>96.24</td>
<td>99.29</td>
</tr>
<tr>
<td>it</td>
<td>98.31</td>
<td>97.69</td>
<td>99.42</td>
</tr>
<tr>
<td>ro</td>
<td>96.45</td>
<td>95.14</td>
<td>98.79</td>
</tr>
</tbody>
</table>
</div>

<h2 dir="ltr">Fastest in its category</h2>
<p dir="ltr">Being lighter weight than most models in the category, Mistral OCR performs significantly faster than its peers, processing up to 2000 pages per minute on a single node. The ability to rapidly process documents ensures continuous learning and improvement even for high-throughput environments.</p>
<h3 dir="ltr">Doc-as-prompt, structured output</h3>
<p dir="ltr">Mistral OCR also introduces the use of documents as prompts, enabling more powerful and precise instructions. This capability allows users to extract specific information from documents and format it in structured outputs, such as JSON. Users can chain extracted outputs into downstream function calls and build agents.</p>
<h3 dir="ltr">Available to self-host on a selective basis</h3>
<p dir="ltr">For organizations with stringent data privacy requirements, Mistral OCR offers a self-hosting option. This ensures that sensitive or classified information remains secure within your own infrastructure, providing compliance with regulatory and security standards. If you would like to explore self-deployment with us, please <a href="https://mistral.ai/">let us know</a>.</p>
<h2 dir="ltr">Use cases</h2>
<p dir="ltr">We are empowering our beta customers to elevate their organizational knowledge by transforming their extensive document repositories into actions and solutions. Some of the key use cases where our technology is making a significant impact include:</p>
<p dir="ltr"><strong>Digitizing scientific research</strong>: Leading research institutions have been experimenting with Mistral OCR to convert scientific papers and journals into AI-ready formats, making them accessible to downstream intelligence engines. This has facilitated measurably faster collaboration and accelerated scientific workflows.</p>
<p dir="ltr"><strong>Preserving historical and cultural heritage</strong>: Organizations and nonprofits that are custodians of heritage have been using Mistral OCR to digitize historical documents and artifacts, ensuring their preservation and making them accessible to a broader audience.</p>
<p dir="ltr"><strong>Streamlining customer service</strong>: Customer service departments are exploring Mistral OCR to transform documentation and manuals into indexed knowledge, reducing response times and improving customer satisfaction.</p>
<p dir="ltr"><strong>Making literature across design, education, legal, etc. AI ready</strong>: Mistral OCR has also been helping companies convert technical literature, engineering drawings, lecture notes, presentations, regulatory filings and much more into indexed, answer-ready formats, unlocking intelligence and productivity across millions of documents.</p>
<h2 dir="ltr">Experience it today</h2>
<p dir="ltr">Mistral OCR capabilities are free to try on <a href="http://chat.mistral.ai/">le Chat</a>. To try the API, head over to <a href="http://console.mistral.ai/">la Plateforme</a>. We’d love to get your feedback; expect the model to continue to get even better in the weeks to come. As part of our strategic engagement programs, we will also offer on-premises deployment on a <a href="https://mistral.ai/contact">selective basis</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UK quietly scrubs encryption advice from government websites (116 pts)]]></title>
            <link>https://techcrunch.com/2025/03/06/uk-quietly-scrubs-encryption-advice-from-government-websites/</link>
            <guid>43282892</guid>
            <pubDate>Thu, 06 Mar 2025 17:38:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/03/06/uk-quietly-scrubs-encryption-advice-from-government-websites/">https://techcrunch.com/2025/03/06/uk-quietly-scrubs-encryption-advice-from-government-websites/</a>, See on <a href="https://news.ycombinator.com/item?id=43282892">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">The U.K. government appears to have quietly scrubbed encryption advice from government web pages, just weeks after demanding backdoor access to encrypted data stored on Apple’s cloud storage service, iCloud.&nbsp;</p>

<p>The change was spotted by security expert Alec Muffett, who wrote in <a href="https://alecmuffett.com/article/112522" target="_blank" rel="noreferrer noopener nofollow">a blog post on Wednesday</a> that the U.K.’s National Cyber Security Centre (NCSC) is no longer recommending that high-risk individuals use encryption to protect their sensitive information.</p>







<p>The NCSC in October published a document titled “Cybersecurity tips for barristers, solicitors &amp; legal professionals,” that advised the use of encryption tools such as Apple’s Advanced Data Protection (ADP).&nbsp;</p>

<p>ADP allows users to turn on end-to-end encryption for their iCloud backups, effectively making it impossible for anyone, including Apple and government authorities, to view data stored on iCloud.</p>

<p>The URL hosting the NCSC document now redirects to a <a href="https://www.ncsc.gov.uk/collection/defending-democracy/guidance-for-high-risk-individuals" target="_blank" rel="noreferrer noopener nofollow">different page</a> that makes no mention of encryption or ADP. Instead, it recommends that at-risk individuals use Apple’s Lockdown Mode, an “extreme” security tool that restricts access to certain functions and features.</p>

<p>Muffett reports that the original document, still accessible via <a href="https://web.archive.org/web/20241102140713/https://www.ncsc.gov.uk/files/Cyber-security-tips-for-barristers.pdf" target="_blank" rel="noreferrer noopener nofollow">the Wayback Machine</a>, has been “wholesale deleted from the internet.” TechCrunch wasn’t able to find any encryption advice on the U.K. government’s website.&nbsp;</p>

<p>The U.K. Home Office and NCSC did not respond to TechCrunch’s questions.</p>


<p>The removal of the encryption advice comes weeks after the U.K. government <a href="https://techcrunch.com/2025/02/07/uk-government-demands-apple-backdoor-to-encrypted-cloud-data-report/">secretly ordered Apple to build a backdoor</a> that would give authorities access to users’ encrypted iCloud data. </p>

<p>Following the order, first reported by <a href="https://www.washingtonpost.com/technology/2025/02/07/apple-encryption-backdoor-uk/" target="_blank" rel="noreferrer noopener nofollow">The Washington Post</a>, Apple pulled its ADP feature in the U.K., and <a href="https://techcrunch.com/2025/02/21/apple-pulls-icloud-end-to-end-encryption-feature-for-uk-users-after-government-demanded-backdoor/">confirmed</a> to TechCrunch that the feature will no longer be made available to new users in the U.K., and its current users would eventually need to disable it.</p>

<p>Apple is <a href="https://techcrunch.com/2025/02/07/uk-government-demands-apple-backdoor-to-encrypted-cloud-data-report/">challenging</a> the U.K.’s data access order in the Investigatory Powers Tribunal (IPT), <a href="https://www.ft.com/content/3d8fe709-f17a-44a6-97ae-f1bbe6d0dccd?shareType=nongift" target="_blank" rel="noreferrer noopener nofollow">The Financial Times</a> reported this week.&nbsp;</p>
</div><div>
	
	
	
	

	
<div>
		<p>Carly Page is a Senior Reporter at TechCrunch, where she covers the cybersecurity beat. She has spent more than a decade in the technology industry, writing for titles including Forbes, TechRadar and WIRED. </p>

<p>You can contact Carly securely on Signal at +441536 853956 or via email at <a href="mailto:carly.page@techcrunch.com">carly.page@techcrunch.com</a>.</p>	</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/carly-page/" data-event="button" href="https://techcrunch.com/author/carly-page/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NASA Shuts Off Voyager Science Instrument (190 pts)]]></title>
            <link>https://gizmodo.com/nasa-shuts-off-voyager-science-instrument-more-power-cuts-ahead-to-keep-both-probes-going-2000572202</link>
            <guid>43282594</guid>
            <pubDate>Thu, 06 Mar 2025 17:14:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/nasa-shuts-off-voyager-science-instrument-more-power-cuts-ahead-to-keep-both-probes-going-2000572202">https://gizmodo.com/nasa-shuts-off-voyager-science-instrument-more-power-cuts-ahead-to-keep-both-probes-going-2000572202</a>, See on <a href="https://news.ycombinator.com/item?id=43282594">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              
              
              <p>The Voyager spacecraft have been cruising through interstellar space for more than 47 years, collecting precious data on the vast cosmos. All that traveling has taken a toll on the farthest human-made objects, and the spacecraft’s days are numbered. NASA engineers are resorting to shutting off science instruments on both Voyager probes to keep&nbsp;the two iconic missions alive.</p> <p>Mission engineers at NASA’s Jet Propulsion Laboratory turned off Voyager 1’s cosmic ray subsystem experiment on February 25, and will shut off the low-energy charged particle instrument aboard Voyager 2 on March 24, NASA <a href="https://www.jpl.nasa.gov/news/nasa-turns-off-two-voyager-science-instruments-to-extend-mission/">announced</a> Wednesday. If it weren’t for these energy-conserving measures, the twin probes may have had a few more months left before running out of power. Both spacecraft now have enough power to operate for another year or so before engineers are forced to turn off two more instruments. It’s a grim reality for the popular interstellar travelers, who have suffered from a<a href="https://gizmodo.com/nasa-voyager-probe-making-sense-months-gibberish-1851427197"> fair share of glitches in the past couple of years</a>.</p> <p>“The Voyagers have been deep space rock stars since launch, and we want to keep it that way as long as possible,” Suzanne Dodd, Voyager project manager at JPL, said in a statement. “But electrical power is running low. If we don’t turn off an instrument on each Voyager now, they would probably have only a few more months of power before we would need to declare end of mission.”</p> <p>The Voyagers are powered by heat from decaying plutonium, which is converted into electricity. Each year, the aging spacecraft lose about 4 watts of power. In an effort to conserve power, the mission team has turned off any systems that were deemed unnecessary to keep the missions going, including a few of the science instruments. Each Voyager spacecraft started off with 10 science instruments when they launched in 1977, but are now left with just three each.</p> <p>Some of the instruments were necessary to collect data during planetary flybys. Those instruments, however, were turned off as soon as both spacecraft completed their exploration of the solar system’s planets. Voyager 1 reached the beginning of interstellar space in 2012, while Voyager 2 reached the boundary in 2018, traveling beyond the protective bubble surrounding the solar system that is known as the heliosphere.</p>

 <p>The Voyager spacecraft were then left with instruments designed to study the solar system’s heliosphere and interstellar space. In October 2024, the team decided to turn off Voyager 2’s plasma science instrument, which measured the amount of electrically charged atoms, in an effort to conserve power.</p> <p>Voyager 1’s cosmic ray subsystem, which was shut down last week, is a suite of three telescopes designed to study cosmic rays by measuring their energy and flux. The data collected by those telescopes helped the Voyager team determine when and where Voyager 1 exited the heliosphere, according to NASA. Voyager 2’s low-energy charged particle instrument, which is scheduled for deactivation later this month, measures the various ions, electrons, and cosmic rays originating from our solar system and galaxy.</p>

 <p>Voyager 1 still has its magnetometer and plasma wave subsystem, and its low-energy charged particle instrument will be shut off next year. Voyager 2 will continue to collect data through its magnetic field and plasma wave instruments, while its cosmic ray subsystem is scheduled to be shut off in 2026.</p> <p>“The Voyager spacecraft have far surpassed their original mission to study the outer planets,” Patrick Koehn, Voyager program scientist, said in a statement. “Every bit of additional data we have gathered since then is not only valuable bonus science for heliophysics, but also a testament to the exemplary engineering that has gone into the Voyagers — starting nearly 50 years ago and continuing to this day.”</p> <p>Voyager 1 launched on September 5, 1977, less than a month after its twin probe, Voyager 2, began its journey to space. The spacecraft took a faster route, exiting the asteroid belt earlier than its twin, and making close encounters with Jupiter and Saturn, where it discovered two Jovian moons, Thebe and Metis, and five new moons, and a new ring called the G-ring, around Saturn. Voyager 2 was launched on August 20, 1977, on a trajectory toward the solar system’s gas giants, Jupiter and Saturn, and explored the icy giants Uranus and Neptune.</p>

 <p>Voyager 1 is more than 15 billion miles (25 billion kilometers) away from Earth, while Voyager 2 is over 13 billion miles (21 billion kilometers) away.</p> <p>With the current energy-conserving plan, NASA engineers believe the twin spacecraft could continue operating into the 2030s with one instrument each. “Every minute of every day, the Voyagers explore a region where no spacecraft has gone before,” Linda Spilker, Voyager project scientist at JPL, said in a statement. “That also means every day could be our last.”</p>
                          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anime fans stumbled upon a mathematical proof (370 pts)]]></title>
            <link>https://www.scientificamerican.com/article/the-surprisingly-difficult-mathematical-proof-that-anime-fans-helped-solve/</link>
            <guid>43282133</guid>
            <pubDate>Thu, 06 Mar 2025 16:37:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scientificamerican.com/article/the-surprisingly-difficult-mathematical-proof-that-anime-fans-helped-solve/">https://www.scientificamerican.com/article/the-surprisingly-difficult-mathematical-proof-that-anime-fans-helped-solve/</a>, See on <a href="https://news.ycombinator.com/item?id=43282133">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p data-block="sciam/paragraph">Math solutions can be found in surprising places, including the dark realms of the Internet. In 2011 an anonymous poster on the now infamously controversial image board 4chan posed a mathematical puzzle about the cult classic anime series <i>The Melancholy of Haruhi Suzumiya</i>. Though the bulletin board has become littered with hateful, violent and extreme content, that original post led to a solution to the sophisticated math problem.</p><p data-block="sciam/paragraph">The first season of this anime series consists of 14 episodes that were designed so that you can watch them in any order you like. (For people who are as unfamiliar with the anime world as I am: an eight-part live-action thriller called <i>Kaleidoscope</i> on Netflix follows the same principle.) At some point in a 2011 discussion of the series on 4chan, someone asked the minimum number of episodes they would have to watch to have seen it in every possible order.</p><p data-block="sciam/paragraph">In fact, this question is related to so-called superpermutations. And as it turns out, this mathematical area holds many puzzles: to this day, mathematicians are still unable to fully answer the problem that the 4chan user had posed.</p><hr><h2>On supporting science journalism</h2><p>If you're enjoying this article, consider supporting our award-winning journalism by<!-- --> <a href="https://www.scientificamerican.com/getsciam/">subscribing</a>. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.</p><hr><p data-block="sciam/paragraph">But amazingly, in that discussion, one of the anonymous users made an estimate of the minimum amount of all episodes to watch with an approach that was previously unknown to mathematicians. “I’ll need to [elaborate on] this in multiple posts. Please look it over for any loopholes I might have missed,” wrote the user, who explained in several steps how they arrived at their estimate. Other users then took up the arguments and discussed them—but outside of 4chan, none of this made any waves. No one seemed to take any notice.</p><h2 id="extreme-binge-watching" data-block="sciam/heading">Extreme Binge-Watching</h2><p data-block="sciam/paragraph">In mathematics, two objects permutate when they are rearranged or recombined. For example, you can permutate AB to BA. If an anime series consisted of only two parts, you could either watch the first and then the second episode (1-2) or the second and then the first (2-1).</p><p data-block="sciam/paragraph">If you want to watch a series in multiple arrangements—perhaps to figure out which sequence of episodes makes the most sense—you need a superpermutation. This is a sequence of all possible permutations. Imagine a marathon showing where you watch the first episode, followed by the second, and then watch the second episode, followed by the first (1-2-2-1). To avoid watching the second episode twice in a row, a shorter superpermutation would be 1-2-1; you would only have to watch three episodes to still have every possible order covered.</p><p data-block="sciam/paragraph">If a series consists of three episodes, it becomes a little trickier to find the shortest superpermutation. In this case, there are 3! = 6 different sequences: 1-2-3, 1-3-2, 2-3-1, 2-1-3, 3-1-2, 3-2-1. Fortunately, you don’t have to watch 3 × 6 = 18 parts but can find a clever shortcut, in this case: 1-2-3-1-2-1-3-2-1. That order contains all possible permutations of the numbers 1, 2 and 3, but you only have to watch nine episodes!</p><p data-block="sciam/paragraph">Mathematicians have also calculated the shortest superpermutations for a series consisting of <i>n</i> = 4 and <i>n</i> = 5 episodes (33 and 153 episodes, respectively). Beyond that, however, they are in the dark. The shortest superpermutations for <i>n</i> &gt; 5 are not known.</p><p data-block="sciam/paragraph">In fact, the challenge relates to one of the most intractable problems in algorithmics: <a href="https://www.scientificamerican.com/article/the-math-mystery-that-connects-sudoku-flight-schedules-and-protein-folding/">the traveling salesperson problem</a>. In this problem, a person wants to visit different cities and end up back in their hometown. The task is to find the shortest route that connects all the cities. The shortest superpermutation is a variation of this problem in which the individual permutations represent different cities. In this case, you assign different distances between cities by determining the overlap of the permutations. For example, cities 1-2-3 and 2-3-1 have a large overlap: the last two digits of the first permutation match the first two digits of the second, so they can be combined to form 1-2-3-1. We can therefore assign a short distance between those two cities. On the other hand, 1-2-3 and 2-1-3 do not overlap. (To see both sequences, you have to look at a full six parts; no shortcut is possible.) Thus, these cities have a large distance between them.</p><p data-block="sciam/paragraph">To find the shortest route within permutations, you connect the permutations that overlap the most. There is only one difficulty: there is no known algorithm that solves the traveling salesperson problem quickly. If we are dealing with a few cities—or, in the case of an anime series, a few episodes—this is not a major drawback. But as soon as the <i>n</i> becomes large, computers fail at the task because the computing time grows exponentially with <i>n</i>.</p><p data-block="sciam/paragraph">Computers are able to calculate superpermutations for <i>n</i> = 4 and <i>n</i> = 5 but not for anything beyond that. And although it is possible to calculate elaborate superpermutations for larger numbers, finding the shortest superpermutation becomes more difficult.</p><p data-block="sciam/paragraph">Experts must therefore make do with estimates. For example, there is an algorithm that helps estimate the length of the shortest possible superpermutation for <i>n</i> objects: 1! + 2! + 3! + ... + <i>n</i>! Using that algorithm, if <i>n</i> = 2, you get a superpermutation of length 1 + 2 = 3. For <i>n</i> = 3, this results in a length of 1 + 2 + 6 = 9. For <i>n</i> = 4, you get 33. And for <i>n</i> = 5, you get 153, which corresponds to the shortest superpermutation in each case.</p><p data-block="sciam/paragraph">For larger <i>n,</i> however, this algorithm no longer applies: computers have been able to find shorter superpermutations than it would suggest exists. In fact, the formula 1! + 2! + 3! + ... + <i>n</i>! massively overestimates the length of the shortest superpermutation for large <i>n</i>. Although the algorithm offers only an approximate answer, mathematicians use it as a starting place, with the goal of narrowing down options to find more precise answers.</p><h2 id="coincidences-and-rediscoveries" data-block="sciam/heading">Coincidences and Rediscoveries</h2><p data-block="sciam/paragraph">In 2013 Nathaniel Johnston, now a mathematics professor at Mount Allison University in New Brunswick, landed on a <i>Melancholy of Haruhi Suzumiya</i> fandom page. Johnston himself was not an anime fan. He had arrived at the site after Googling some search terms related to superpermutations. There he came across the discussion that had been held on 4chan almost two years earlier, which a user had copied to the fandom site.</p><p data-block="sciam/paragraph">Johnston didn’t bother doing the math <a href="http://www.njohnston.ca/2013/04/the-minimal-superpermutation-problem/">but cited the fandom post on his blog</a>. This comment, too, went unnoticed for several years.</p><p data-block="sciam/paragraph">Then in October 2018 mathematician Robin Houston came across his colleague’s blog post through a curious coincidence. Houston had just learned that Australian science fiction author Greg Egan had found a new <i>maximum </i>length for the shortest superpermutations, expressed as:</p><p data-block="sciam/paragraph"><i>n</i>! +(<i>n</i> –1)! + (<i>n</i> – 2)! + (<i>n</i> – 3)! + <i>n </i>– 3</p><p data-block="sciam/paragraph">That in itself was bizarre. But when Houston started learning more about this result, he realized that the minimum length of a superpermutation had been given a new value by an anonymous anime fandom user (he didn’t know about the origins on 4chan at that time). The formula for the minimum length is:</p><p data-block="sciam/paragraph"><i>n</i>! +(<i>n</i> – 1)! + (<i>n</i> – 2)! + <i>n</i> – 3</p><p data-block="sciam/paragraph">Houston shared <a href="https://x.com/robinhouston/status/1054637891085918209?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1054637891085918209%7Ctwgr%5Eaa026a45fed15b42edf702c30c79bae8cc6e60c7%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fwww.spektrum.de%2Fkolumne%2Fsuperpermutation-und-anime-4chan-user-gelingt-mathematischer-beweis%2F2251925">his discovery on Twitter</a> (now X) on October 23 of that year. “A curious situation. The best known lower bound for the minimal length of superpermutations was proven by an anonymous user of a wiki mainly devoted to anime,” he wrote.</p><p data-block="sciam/paragraph">Along with his colleagues, mathematicians Jay Pantone and Vince Vatter, Houston decided to check the 4chan user’s proof and write it down in a mathematical way. <a href="https://oeis.org/A180632/a180632.pdf">The researchers posted their mathematical work</a> to the Online Encyclopedia of Integer Sequences that same month, and the first author is listed as “Anonymous 4chan Poster.”</p><p data-block="sciam/paragraph">So what do these formulas tell us? If you want to watch all episodes of an <i>n</i>-part series in all possible combinations, you must sit through at least <i>n</i>! +(<i>n </i>– 1)! + (<i>n</i> – 2)! + <i>n</i> – 3 episodes—that’s the 4chan user’s contribution—and at most <i>n</i>! +(<i>n</i> – 1)! + (<i>n</i> – 2)! + (<i>n</i> – 3)! + <i>n</i> – 3, which we know through Egan’s work.</p><p data-block="sciam/paragraph">In the case of the eight-episode series <i>Kaleidoscope,</i> you would have to watch at least 46,085 and, at most, 46,205 episodes. For <i>The Melancholy of Haruhi Suzumiya, </i>or <i>Haruhi,</i> with 14 episodes, the number increases drastically: a minimum of 93,884,313,611 episodes and a maximum of 93,924,230,411. Recall that this is not a complete solution—it’s just setting a range for the size of a superpermutation that would allow you to efficiently watch the series in every possible order.</p><p data-block="sciam/paragraph">Fortunately, Egan also provided an algorithm for constructing the corresponding superpermutation. This allows <i>Haruhi</i> fans to work out the best viewing order of episodes. But with an average episode length of around 24 minutes, it would take about 4 million years to sit through this superpermutation.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: CodeTracer – A new time-traveling debugger implemented in Nim and Rust (290 pts)]]></title>
            <link>https://github.com/metacraft-labs/codetracer</link>
            <guid>43280615</guid>
            <pubDate>Thu, 06 Mar 2025 14:30:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/metacraft-labs/codetracer">https://github.com/metacraft-labs/codetracer</a>, See on <a href="https://news.ycombinator.com/item?id=43280615">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a href="https://github.com/metacraft-labs/codetracer/actions/workflows/codetracer.yml"><img src="https://github.com/metacraft-labs/codetracer/actions/workflows/codetracer.yml/badge.svg?branch=main" alt="CI Status"></a>
<a href="https://discord.gg/aH5WTMnKHT" rel="nofollow"><img src="https://camo.githubusercontent.com/a518703f55e2d1bb914de5c1654d4bf2f321d5f812cfa54aeb7a063d8ce12e3b/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f313332363934393731343637393033383031343f6c6162656c3d446973636f7264266c6f676f3d646973636f7264267374796c653d666c6174" alt="Discord" data-canonical-src="https://img.shields.io/discord/1326949714679038014?label=Discord&amp;logo=discord&amp;style=flat"></a></p>
<p dir="auto"><a href="https://downloads.codetracer.com/CodeTracer-25.03.1-amd64.AppImage" rel="nofollow"><img src="https://camo.githubusercontent.com/193f9b8fefa98d37a72bd9dfe441c86416a0682ca6740b2500d07df52228aa3b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f776e6c6f61642d4c696e7578253230417070496d6167652d626c75653f7374796c653d666f722d7468652d6261646765" alt="Download AppImage" data-canonical-src="https://img.shields.io/badge/Download-Linux%20AppImage-blue?style=for-the-badge"></a>
<a href="https://downloads.codetracer.com/CodeTracer-25.03.1-arm64.dmg" rel="nofollow"><img src="https://camo.githubusercontent.com/0152015edf13040a520170813f0e48c2fb2d49ac7b92e316d00a4b7b21c1ae10/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f776e6c6f61642d6d61634f532d626c75653f7374796c653d666f722d7468652d6261646765" alt="Download macOS" data-canonical-src="https://img.shields.io/badge/Download-macOS-blue?style=for-the-badge"></a></p>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">You can place the downloaded app in a location of your choosing (e.g., the <code>Applications</code> folder on macOS or <code>~/.local/share/applications</code> on Linux).
When you launch CodeTracer for the first time, it will prompt you to complete the remaining installation steps, such as adding the command-line utilities to your PATH.</p>
</div>
<div dir="auto"><p dir="auto">Caution</p><p dir="auto">Upon the first launch, macOS users will see the error message "CodeTracer is damaged and can't be opened". To resolve this problem, please execute the command <code>xattr -c &lt;path/to/CodeTracer.app&gt;</code>. We expect this inconvenience will be remedied soon through our enrollment in the Apple Developer program that will ensure CodeTracer is properly signed and whitelisted by Apple. See <a href="https://discussions.apple.com/thread/253714860?sortBy=rank" rel="nofollow">https://discussions.apple.com/thread/253714860?sortBy=rank</a> for more details.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">CodeTracer is a user-friendly time-traveling debugger designed to support a wide range of programming languages.</p>
<p dir="auto">It records the execution of a program into a sharable self-contained trace file. You can load the produced trace files in a GUI environment that allows you to move forward and backward through the execution and to examine the history of all memory locations. They say a picture is worth a thousand words — well, a video is even better! Watch the demo below to see CodeTracer in action:</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=xZsJ55JVqmU" rel="nofollow"><img src="https://camo.githubusercontent.com/579ce66477a6c9253101d6ac952c56903dcf4b4aff5b14c83a3503d1f95eb61b/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f785a734a35354a56716d552f6d617872657364656661756c742e6a7067" alt="Watch the video" data-canonical-src="https://img.youtube.com/vi/xZsJ55JVqmU/maxresdefault.jpg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The Benefits of Time-Travel</h2><a id="user-content-the-benefits-of-time-travel" aria-label="Permalink: The Benefits of Time-Travel" href="#the-benefits-of-time-travel"></a></p>
<p dir="auto">Compared to traditional debuggers, CodeTracer gives you two major superpowers:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Once you capture a bug in a recording, consider it squashed!</strong></p>
<p dir="auto">Bugs that are hard to reproduce can be painful to fix — you’ve surely been there. Once such a bug is captured with CodeTracer, you'll rarely need more than 30 minutes to track it down! This is largely a consequence of the next superpower:</p>
</li>
<li>
<p dir="auto"><strong>Most bugs are easily revealed when you know the origin of any value in the program.</strong></p>
<p dir="auto">All programs produce output. Some examples are bytes generated as a response to a web request, pixels being drawn on your screen or perhaps a simple log line written to the terminal.</p>
<p dir="auto">When CodeTracer creates a recording, it captures a user-extensible set of output events relevant to the program. The GUI displays these events in a searchable chronological event log.</p>
<p dir="auto">Consider a misbehaving program that prints unexpected output to a log file midway through its execution. Clicking on the specific output event in CodeTracer will take you to the precise moment and code line where it was generated.</p>
<p dir="auto">The unexpected value must be originating from some variable that's being passed to the logging function. With CodeTracer, you can now ask the question "Where did this value come from?". CodeTracer will find another moment in the execution, potentially multiple seconds earlier, in a completely different part of the program where this particular memory location was last written to.</p>
<p dir="auto">This could be memory corruption or a genuine logical error. Either way, CodeTracer will report the origin. Let's say that you end up in the correct function that is responsible for computing the problematic value, but another input there leads to the issue. You can continue the search by repeating the question "Where did this input come from"? It usually takes just a few of these jumps to earlier moments in time to arrive at the root cause for the bug.</p>
<p dir="auto">Every time you jump to a new moment in the execution, you can fearlessly explore your surroundings by stepping forward or backwards, having access to a back button that can always get you to any previous point of interest. At every point of the journey, our novel UI shows you details about the past and future program states at a glance and you know your position in the precisely recorded call trace of the program.</p>
<p dir="auto">These features combined, make for a truly powerful debugging experience.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Current state of the project and 2025 roadmap</h2><a id="user-content-current-state-of-the-project-and-2025-roadmap" aria-label="Permalink: Current state of the project and 2025 roadmap" href="#current-state-of-the-project-and-2025-roadmap"></a></p>
<p dir="auto">The initial release of CodeTracer supports the Noir programming language. It has been developed in collaboration with the Blocksense team and currently requires the use of the <a href="https://github.com/blocksense-network/noir">Blocksense Noir Compiler</a>, which is included in the CodeTracer distribution.</p>
<p dir="auto">CodeTracer’s open-source development is made possible by the generous support of Aztec Network, Arbitrum Foundation, and Aptos Foundation. During 2025, CodeTracer will evolve into a comprehensive Web3 development toolkit by gaining support for many additional smart contract and zero-knowledge programming languages. Most of its functionality will be packaged into reusable libraries that will power the creation of block explorers with advanced transaction tracing capabilities and omniscient debugging plugins for Visual Studio Code and other IDEs.</p>
<p dir="auto">CodeTracer uses an <a href="https://github.com/metacraft-labs/runtime_tracing">open format</a> for its trace files and we've started several community-driven projects which aim to add support for other programming languages:</p>
<ul dir="auto">
<li><a href="https://github.com/metacraft-labs/codetracer-ruby-recorder">Ruby</a></li>
<li><a href="https://github.com/metacraft-labs/codetracer-python-recorder">Python</a></li>
</ul>
<p dir="auto">Metacraft Labs is also developing an alternative back-end, capable of working with <a href="https://rr-project.org/" rel="nofollow">RR</a> recordings, which will make CodeTracer suitable for debugging large-scale programs in a variety of system programming languages such as C/C++, Rust, Nim, D, Zig, Go, Fortran and FreePascal.</p>
<p dir="auto">To shape our priorities and to help us understand the demographics of our user base better, please fill out <a href="https://form.typeform.com/to/M2Z28VFj?utm_source=Github" rel="nofollow">CodeTracer Developer Preferences Survey</a>.</p>
<p dir="auto">To accelerate our development, please consider donating to our Open Collective campaign. Anyone who contributes more than €50 before June 2025 will get early access to our beta releases for system programming languages, forever.</p>
<p dir="auto"><a href="https://opencollective.com/codetracer" rel="nofollow"><img src="https://camo.githubusercontent.com/d170aaac869438e2d8a85057c801cfb6e30089f36f5a1f8a27ed742eadbb33ab/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652532304f6e2d4f70656e436f6c6c6563746976652d677265656e3f7374796c653d666f722d7468652d6261646765" alt="CodeTracer OpenCollective" data-canonical-src="https://img.shields.io/badge/Donate%20On-OpenCollective-green?style=for-the-badge"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The features of CodeTracer in more depth</h2><a id="user-content-the-features-of-codetracer-in-more-depth" aria-label="Permalink: The features of CodeTracer in more depth" href="#the-features-of-codetracer-in-more-depth"></a></p>
<p dir="auto">Once you have time travel, you can re-imagine how a debugger works from the ground up! Here are some of the features of CodeTracer that set it apart from traditional debuggers:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Omniscience</h3><a id="user-content-omniscience" aria-label="Permalink: Omniscience" href="#omniscience"></a></p>
<p dir="auto">When CodeTracer breaks at a certain line of code, it knows not only the past but also the future. You can immediately see the values of all variables in the current function right next to the source code. This includes variables in loops where you can easily scroll through the iterations. Taken code branches are highlighted, while the non-executed code is immediately grayed out.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4f8eb695cf1e248f1148590de20f1909515280ce4ead15404dce4f320f00d4de/68747470733a2f2f646f776e6c6f6164732e636f64657472616365722e636f6d2f666561747572652d686967686c69676874732f6f6d6e69736369656e63652e77656270"><img src="https://camo.githubusercontent.com/4f8eb695cf1e248f1148590de20f1909515280ce4ead15404dce4f320f00d4de/68747470733a2f2f646f776e6c6f6164732e636f64657472616365722e636f6d2f666561747572652d686967686c69676874732f6f6d6e69736369656e63652e77656270" alt="omniscience" title="omniscience" data-canonical-src="https://downloads.codetracer.com/feature-highlights/omniscience.webp"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Tracepoints</h3><a id="user-content-tracepoints" aria-label="Permalink: Tracepoints" href="#tracepoints"></a></p>
<p dir="auto">Many developers tend to transition from using interactive debugging early on in their careers, to mostly relying on carefully placed print statements as they start to face harder problems in distributed and real-time systems.</p>
<p dir="auto">CodeTracer gives you the best of both worlds. A tracepoint allows you to see the effects of adding additional code to your program, without recompiling and rerecording it.</p>
<p dir="auto">The added code can feature if statements, additional function calls and various ways to print or plot the captured data. The output is typically produced in seconds.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/47f097503a88912574ed29ae17fa4d741713776a7bf71366b250d411eff016f6/68747470733a2f2f646f776e6c6f6164732e636f64657472616365722e636f6d2f666561747572652d686967686c69676874732f7472616365706f696e742e77656270"><img src="https://camo.githubusercontent.com/47f097503a88912574ed29ae17fa4d741713776a7bf71366b250d411eff016f6/68747470733a2f2f646f776e6c6f6164732e636f64657472616365722e636f6d2f666561747572652d686967686c69676874732f7472616365706f696e742e77656270" alt="tracepoint" title="tracepoint" data-canonical-src="https://downloads.codetracer.com/feature-highlights/tracepoint.webp"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Call Trace</h3><a id="user-content-call-trace" aria-label="Permalink: Call Trace" href="#call-trace"></a></p>
<p dir="auto">Instead of a stack trace limited to a single moment in time, CodeTracer shows you the entire tree of function calls in the recorded execution. You can navigate and filter it in various ways and the clever collapsing and expanding algorithms still allow you to obtain a classic stack trace whenever you need it.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/aa3100b789a8703f94ec8a268ff10f44ad38a3aea058eacedaa197f9020c4c81/68747470733a2f2f646f776e6c6f6164732e636f64657472616365722e636f6d2f666561747572652d686967686c69676874732f63616c6c74726163652e77656270"><img src="https://camo.githubusercontent.com/aa3100b789a8703f94ec8a268ff10f44ad38a3aea058eacedaa197f9020c4c81/68747470733a2f2f646f776e6c6f6164732e636f64657472616365722e636f6d2f666561747572652d686967686c69676874732f63616c6c74726163652e77656270" alt="calltrace" title="calltrace" data-canonical-src="https://downloads.codetracer.com/feature-highlights/calltrace.webp"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">State and History Explorer</h3><a id="user-content-state-and-history-explorer" aria-label="Permalink: State and History Explorer" href="#state-and-history-explorer"></a></p>
<p dir="auto">Every variable in CodeTracer has a history. You can see all the values that the variable held during the entire execution of the program and for each of them you can jump to the originating expression that computed it. CodeTracer follows the path of trivial copies in assignments to save you extra time when tracking down more convoluted bugs.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b64f1cdaf70fb990f7a85e96e1ab0c80e046a5f5dcc2e8ff7757791a14335f4f/68747470733a2f2f646f776e6c6f6164732e636f64657472616365722e636f6d2f666561747572652d686967686c69676874732f73746174652d616e642d686973746f72792e77656270"><img src="https://camo.githubusercontent.com/b64f1cdaf70fb990f7a85e96e1ab0c80e046a5f5dcc2e8ff7757791a14335f4f/68747470733a2f2f646f776e6c6f6164732e636f64657472616365722e636f6d2f666561747572652d686967686c69676874732f73746174652d616e642d686973746f72792e77656270" alt="state-and-history" title="state-and-history" data-canonical-src="https://downloads.codetracer.com/feature-highlights/state-and-history.webp"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Event Log</h3><a id="user-content-event-log" aria-label="Permalink: Event Log" href="#event-log"></a></p>
<p dir="auto">The event log gives you a chronological view over anything of interest in your program, interleaved with the tracepoint outputs. Clicking on any event takes you to the precise moment in time when it was produced, which gives you excellent starting points for your investigations. Tracking down the origins of anomalous events and program states through the history of the involved variables is the essence of debugging with CodeTracer.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4e120a36e9623ec73ba08a9237a0fe7c89d893d3d9cca24558491b7e61efeda6/68747470733a2f2f646f776e6c6f6164732e636f64657472616365722e636f6d2f666561747572652d686967686c69676874732f6576656e746c6f672e77656270"><img src="https://camo.githubusercontent.com/4e120a36e9623ec73ba08a9237a0fe7c89d893d3d9cca24558491b7e61efeda6/68747470733a2f2f646f776e6c6f6164732e636f64657472616365722e636f6d2f666561747572652d686967686c69676874732f6576656e746c6f672e77656270" alt="eventlog" title="eventlog" data-canonical-src="https://downloads.codetracer.com/feature-highlights/eventlog.webp"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Terminal Output</h3><a id="user-content-terminal-output" aria-label="Permalink: Terminal Output" href="#terminal-output"></a></p>
<p dir="auto">The terminal output panel renders the recorded <code>stdout</code> and <code>stderr</code> events like a standard terminal. Clicking anywhere in the output takes you to the exact moment when the specific character was generated. This allows for a fallback to a more traditional print-style debugging when the expressivity of tracepoints is not sufficient. In the future, CodeTracer will allow you to slide through the states of the terminal through time which will help for debugging highly interactive TUI apps.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3bf7c7a06196837bcfa067c9360801ea8b32df027a31f123ce328c2d76cdcb63/68747470733a2f2f646f776e6c6f6164732e636f64657472616365722e636f6d2f666561747572652d686967686c69676874732f7465726d696e616c2e77656270"><img src="https://camo.githubusercontent.com/3bf7c7a06196837bcfa067c9360801ea8b32df027a31f123ce328c2d76cdcb63/68747470733a2f2f646f776e6c6f6164732e636f64657472616365722e636f6d2f666561747572652d686967686c69676874732f7465726d696e616c2e77656270" alt="terminal" title="terminal" data-canonical-src="https://downloads.codetracer.com/feature-highlights/terminal.webp"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mouse Stepping</h3><a id="user-content-mouse-stepping" aria-label="Permalink: Mouse Stepping" href="#mouse-stepping"></a></p>
<p dir="auto">Since CodeTracer is aware of all past and future control flow, it offers some convenient shortcuts for quickly navigating the program execution by interacting with its source code:</p>
<ul dir="auto">
<li>Jump to a line: Middle-click on any undimmed line in the current function to quickly step to it. If you use a mouse without a middle button, you can achieve the same with <code>Ctrl+Click</code> or by pressing <code>Ctrl+F8</code>(as a special kind of <code>continue</code>) after navigating to the respective line with your keyboard.</li>
<li>Jump into a call: Double middle-click on the function name in a call expression to jump into it. If you use a mouse without a middle button, you can achieve the same with <code>Ctrl+Alt+Click</code> or by pressing <code>Ctrl+F11</code>(as a special kind of <code>step-in</code>) after navigating to the respective call expression with your keyboard.</li>
</ul>
<p dir="auto">Explore the right-click context menu for additional operations.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Scratchpad</h3><a id="user-content-scratchpad" aria-label="Permalink: Scratchpad" href="#scratchpad"></a></p>
<p dir="auto">The scratchpad provides a play area where you can pin values from different locations and moments in time. You can explore their differences both manually and algorithmically to gain quick insights into the behavior of your program.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The CodeTracer CLI</h3><a id="user-content-the-codetracer-cli" aria-label="Permalink: The CodeTracer CLI" href="#the-codetracer-cli"></a></p>
<p dir="auto">When you launch the CodeTracer GUI, it will offer you the option to also install the CodeTracer CLI. It provides convenient ways to create and load trace files from the command-line or to integrate CodeTracer with CI processes.</p>
<p dir="auto">Run <code>ct --help</code> to see the full list of supported subcommands, but the most commonly used ones are the following:</p>
<p dir="auto"><code>&lt;application&gt;</code> can be a source file or a project folder (depending on the language):</p>
<ol dir="auto">
<li><code>ct run &lt;application&gt;</code> - Creates a recording and load it in CodeTracer with a single command.</li>
<li><code>ct record &lt;application&gt;</code> - Creates a trace file that can be loaded later or shared.</li>
<li><code>ct replay</code> - Launches the CodeTracer GUI with a previously recorded trace file. Common usages are:
<ul dir="auto">
<li><code>ct replay</code> - Opens a simple console-based dialog to choose what recording you want to replay.</li>
<li><code>ct replay &lt;program-name&gt;</code> - Opens the last trace of an application.</li>
<li><code>ct replay --id=&lt;trace-id&gt;</code> - Opens a trace by its trace id.</li>
<li><code>ct replay --trace-folde=&lt;trace-folder&gt;</code> - Opens a trace by its trace folder.</li>
</ul>
</li>
<li><code>ct</code> - Launches the startup screen of the CodeTracer GUI.</li>
<li><code>ct help / ct --help</code> - Gives you a help message.</li>
<li><code>ct version</code> - Returns the current version of CodeTracer.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Keyboard Shortcuts</h2><a id="user-content-keyboard-shortcuts" aria-label="Permalink: Keyboard Shortcuts" href="#keyboard-shortcuts"></a></p>
<p dir="auto">You can learn all CodeTracer keyboard shortcuts by examining the main menu and the tooltips over all buttons in the interface.
The default configuration should be familiar to users experienced with Microsoft Visual Studio™.</p>
<p dir="auto">Since CodeTracer provides a reverse counterpart to most traditional debugging operations, we typically use the <code>Shift</code> modifier to indicate this (e.g. <code>F10</code> is "Next Step" while <code>Shift+F10</code> is "Previous Step").</p>
<p dir="auto">The user config file located at <code>~/.config/codetracer/.config.yml</code> allows you to specify custom shortcuts for all operations.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building from source</h2><a id="user-content-building-from-source" aria-label="Permalink: Building from source" href="#building-from-source"></a></p>
<p dir="auto">The Metacraft Labs team is using <a href="https://nixos.org/" rel="nofollow">Nix</a> to define a reproducible development environment for working on CodeTracer. Linux is our primary development platform, while some of our team members use macOS. Building on Windows will be supported in the near future.</p>
<p dir="auto">To enter the Nix development environment, perform the following steps:</p>
<ol dir="auto">
<li><a href="https://zero-to-nix.com/start/install/" rel="nofollow">Install Nix</a>.</li>
<li>Clone this repository.</li>
<li>At the repository root, execute <code>git submodule update --init --recursive</code>.</li>
<li>At the repository root, execute <code>nix develop</code> (or <code>direnv allow</code> for users of <code>direnv</code>).</li>
<li>In the resulting shell, you can build all targets by running <code>just build-once</code> or <code>just build</code> if you intend to make continuous changes to the source code.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Check out our <a href="https://github.com/metacraft-labs/codetracer/blob/main/CONTRIBUTING.md">Contributors Guide</a> for more details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">CodeTracer is distributed under the GNU Affero General Public License (AGPLv3).</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Age and cognitive skills: Use it or lose it (661 pts)]]></title>
            <link>https://www.science.org/doi/full/10.1126/sciadv.ads1560?af=R</link>
            <guid>43279494</guid>
            <pubDate>Thu, 06 Mar 2025 12:33:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/doi/full/10.1126/sciadv.ads1560?af=R">https://www.science.org/doi/full/10.1126/sciadv.ads1560?af=R</a>, See on <a href="https://news.ycombinator.com/item?id=43279494">Hacker News</a></p>
Couldn't get https://www.science.org/doi/full/10.1126/sciadv.ads1560?af=R: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Finland ends homelessness and provides shelter for all in need (190 pts)]]></title>
            <link>https://thebetter.news/housing-first-finland-homelessness/</link>
            <guid>43279454</guid>
            <pubDate>Thu, 06 Mar 2025 12:29:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thebetter.news/housing-first-finland-homelessness/">https://thebetter.news/housing-first-finland-homelessness/</a>, See on <a href="https://news.ycombinator.com/item?id=43279454">Hacker News</a></p>
Couldn't get https://thebetter.news/housing-first-finland-homelessness/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists crack how aspirin might stop cancers from spreading (152 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c1d4n119xr7o</link>
            <guid>43279147</guid>
            <pubDate>Thu, 06 Mar 2025 11:53:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c1d4n119xr7o">https://www.bbc.com/news/articles/c1d4n119xr7o</a>, See on <a href="https://news.ycombinator.com/item?id=43279147">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/75b9/live/e6fd55d0-f9af-11ef-b257-2b725ccbdd87.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/75b9/live/e6fd55d0-f9af-11ef-b257-2b725ccbdd87.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/75b9/live/e6fd55d0-f9af-11ef-b257-2b725ccbdd87.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/75b9/live/e6fd55d0-f9af-11ef-b257-2b725ccbdd87.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/75b9/live/e6fd55d0-f9af-11ef-b257-2b725ccbdd87.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/75b9/live/e6fd55d0-f9af-11ef-b257-2b725ccbdd87.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/75b9/live/e6fd55d0-f9af-11ef-b257-2b725ccbdd87.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/75b9/live/e6fd55d0-f9af-11ef-b257-2b725ccbdd87.jpg.webp" loading="eager" alt="Getty Images Person tipping white pills out of a brown bottle and into the palm of their hand"><span>Getty Images</span></p></div></figure><div data-component="text-block"><p>Scientists believe they have discovered how the cheap painkiller aspirin can stop cancers spreading.<!-- --></p><p>In animal experiments they showed the drug enhanced the ability of the immune system to fight back.<!-- --></p><p>The team at the University of Cambridge said it was an exciting and surprise discovery that could eventually lead to cancer patients being prescribed the drug - but not yet and people are advised against just taking the pills themselves.<!-- --></p><p>Regular aspirin comes with risks and trials are still trying to figure out which patients are most likely to benefit.<!-- --></p></div><div data-component="text-block"><p>Tantalising data from <!-- --><a target="_blank" href="https://pubmed.ncbi.nlm.nih.gov/22440947/">more than a decade ago<!-- --></a> showed people who were already taking a daily aspirin were more likely to survive if they were diagnosed with cancer.<!-- --></p><p>But how?<!-- --></p><p>It appears to centre on a moment of vulnerability for a cancer - when a lone cell breaks off from the original tumour and tries, like a seed on the wind, to spread elsewhere in the body. <!-- --></p><p>This process is called metastasis and is the cause of <!-- --><a target="_blank" href="https://www.nature.com/articles/s41392-020-0134-x">the majority of deaths<!-- --></a> from cancer.<!-- --></p><p>Part of our immune defences - a white blood cell called a T-cell - can swoop in and destroy the spreading cancer as it tries to take root.  <!-- --></p><p>But the study showed that another part of our blood - the platelets that normally stop bleeding - were suppressing the T-cells and making it harder for them to take out the cancer. <!-- --></p><p>Aspirin disrupts the platelets and removes their influence over the T-cells so they can hunt out the cancer.   <!-- --></p><p>Prof Rahul Roychoudhuri, from the University of Cambridge, told me: "What we've discovered is that aspirin might work, surprisingly, by unleashing the power of the immune system to recognize and kill metastasizing cancer cells."<!-- --></p><p>He thinks the drug would work best in cancers that have been caught early and could be used after treatment such as surgery to help the immune system find any cancer that might already have spread. <!-- --></p></div><p data-component="subheadline-block"><h2>Should I take aspirin for cancer?<!-- --></h2></p><div data-component="text-block"><p>The most natural question for anybody with cancer to ask is should they be taking aspirin. <!-- --></p><p>"If you are a cancer patient, don't rush to your local pharmacy to buy aspirin just yet, but actively consider participation in ongoing or upcoming trials of aspirin," says Prof Mangesh Thorat, a surgeon and cancer researcher at Queen Mary University of London.<!-- --></p><p>He says the study provided "the missing piece of the jigsaw puzzle" in understanding how aspirin works, but there were still questions to answer.<!-- --></p><p>Aspirin can cause dangerous internal bleeding including strokes so the risks have to be balanced. It is also not clear whether the effect works for all cancer or just specific ones. And this is still animal research so while the scientists think this would apply in people that will still need to be confirmed.<!-- --></p><p>Some patients - with Lynch syndrome, which increases the risk of cancers - are already <!-- --><a target="_blank" href="https://www.nice.org.uk/guidance/ng151/chapter/Recommendations">recommended aspirin<!-- --></a>.<!-- --></p><p>But it will still take proper clinical trials to understand whether more patients would benefit too. <!-- --></p><p>These are already under way. Prof Ruth Langley, from the MRC Clinical Trials Unit at University College London, is leading the <!-- --><a target="_blank" href="https://www.addaspirintrial.org/">Add-Aspirin trial<!-- --></a> to see if aspirin can stop early stage cancers from coming back. <!-- --></p><p>She said the study's results were "an important discovery" as they would help to work out "who is most likely to benefit from aspirin after a cancer diagnosis".<!-- --></p><p>However, she again warned of the risks of taking aspirin and to "always talk to your doctor before starting".<!-- --></p><p>In the long-run, Prof Roychoudhuri suspects new drugs would be developed that take the benefits of aspirin, but with fewer of the risky side-effects.<!-- --></p></div><p data-component="subheadline-block"><h2>'Eureka moment'<!-- --></h2></p><div data-component="text-block"><p>The discovery, <!-- --><a target="_blank" href="https://www.nature.com/articles/s41586-025-08626-7">published in the journal Nature<!-- --></a>, happened by accident as the scientists were not researching aspirin.<!-- --></p><p>The team in Cambridge were investigating how the immune system responded to cancers when they spread.<!-- --></p><p>They were using genetically engineered mice and found those lacking a specific set of genetic instructions were less likely to get metastatic cancer that had spread.<!-- --></p><p>Further investigation revealed how those T-cells were being suppressed and this started to overlap with how aspirin was known to work in the body. <!-- --></p><p>Dr Jie Yang, who carried out the research, said: "It was a Eureka moment.<!-- --></p><p>"It was an entirely unexpected finding which sent us down quite a different path of inquiry than we had anticipated."<!-- --></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD Announces "Instella" Open-Source 3B Language Models (163 pts)]]></title>
            <link>https://www.phoronix.com/news/AMD-Intella-Open-Source-LM</link>
            <guid>43278845</guid>
            <pubDate>Thu, 06 Mar 2025 11:17:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phoronix.com/news/AMD-Intella-Open-Source-LM">https://www.phoronix.com/news/AMD-Intella-Open-Source-LM</a>, See on <a href="https://news.ycombinator.com/item?id=43278845">Hacker News</a></p>
Couldn't get https://www.phoronix.com/news/AMD-Intella-Open-Source-LM: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Buy European Made. Support European Values (444 pts)]]></title>
            <link>https://www.buy-european-made.eu/</link>
            <guid>43278705</guid>
            <pubDate>Thu, 06 Mar 2025 10:58:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.buy-european-made.eu/">https://www.buy-european-made.eu/</a>, See on <a href="https://news.ycombinator.com/item?id=43278705">Hacker News</a></p>
<div id="readability-page-1" class="page">




    <div>
        <p><a href="https://www.softr.io/?via=madewithbadge" target="_blank" title="Client Portal | Internal Tools | Web App Builder | Free Website Builder">
            <img src="https://assets.softr-files.com/assets/images/softr_logo/softr_logo_icon_only.svg" alt="Client Portal | Internal Tools | Web App Builder | Free Website Builder">
            <span>Made with</span>
            <span>Softr</span>
        </a>
    </p></div>



<!-- Scripts -->














<!-- Localhost -->


<!-- Modal HTML -->


<!--Scroll to section-->





</div>]]></description>
        </item>
    </channel>
</rss>