<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 05 Mar 2025 04:30:12 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Trump's 'Crypto Reserve' Is Such Brazen Corruption (163 pts)]]></title>
            <link>https://zeteo.com/p/trumps-crypto-reserve-is-such-brazen</link>
            <guid>43261899</guid>
            <pubDate>Wed, 05 Mar 2025 02:30:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zeteo.com/p/trumps-crypto-reserve-is-such-brazen">https://zeteo.com/p/trumps-crypto-reserve-is-such-brazen</a>, See on <a href="https://news.ycombinator.com/item?id=43261899">Hacker News</a></p>
Couldn't get https://zeteo.com/p/trumps-crypto-reserve-is-such-brazen: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Mox – modern, secure, all-in-one email server (112 pts)]]></title>
            <link>https://www.xmox.nl/</link>
            <guid>43261729</guid>
            <pubDate>Wed, 05 Mar 2025 01:58:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.xmox.nl/">https://www.xmox.nl/</a>, See on <a href="https://news.ycombinator.com/item?id=43261729">Hacker News</a></p>
Couldn't get https://www.xmox.nl/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Best Buy and Target CEOs say prices are about to go up because of tariffs (131 pts)]]></title>
            <link>https://www.theverge.com/news/624254/best-buy-target-raise-prices-tariffs</link>
            <guid>43261626</guid>
            <pubDate>Wed, 05 Mar 2025 01:36:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/news/624254/best-buy-target-raise-prices-tariffs">https://www.theverge.com/news/624254/best-buy-target-raise-prices-tariffs</a>, See on <a href="https://news.ycombinator.com/item?id=43261626">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><img alt="Emma Roth" data-chromatic="ignore" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195810/EMMA_ROTH.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=48 1x, https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195810/EMMA_ROTH.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96 2x" src="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195810/EMMA_ROTH.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96"></p><p><a href="https://www.theverge.com/authors/emma-roth">Emma Roth</a> <span>is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO.</span></p></div><div id="zephr-anchor"><p>Target and Best Buy say Trump’s tariffs on Mexico, Canada, and China could raise prices in their stores as soon as this week. <a href="https://www.cnbc.com/2025/03/04/trump-mexico-tariffs-will-raise-produce-prices-target-ceo-cornell-says.html">During an interview with CNBC</a>, Target CEO Brian Cornell said consumers will “likely see prices increase over the next couple of days,” while Best Buy CEO Corie Barry <a href="https://www.cnbc.com/2025/03/04/best-buy-bby-q4-2025-earnings.html">similarly told investors</a> that more expensive prices are “highly likely.”</p><p>Cornell told CNBC that half of Target’s goods come from the United States, but the company depends on Mexico for “a significant amount” of fruits and vegetables during winter, potentially leading to more expensive strawberries, bananas, and avocados. “Those are categories where we’ll try to protect pricing, but the consumer will likely see price increases over the next couple of days,” Cornell added.</p><p>Meanwhile, Best Buy’s Barry <a href="https://go.skimresources.com/?id=1025X1701640&amp;xs=1&amp;url=https%3A%2F%2Fwww.fool.com%2Fearnings%2Fcall-transcripts%2F2025%2F03%2F04%2Fbest-buy-bby-q4-2025-earnings-call-transcript%2F" rel="sponsored">said during an earnings</a> call that China and Mexico remain the top two countries where the company gets its products. “We expect our vendors across our entire assortment will pass along some level of tariff costs to retailers, making price increases for American consumers highly likely,” Barry said.</p><p>On Tuesday, <a href="https://www.theverge.com/news/623403/trump-imposes-tariffs-mexico-canada-china">Trump followed through on threats</a> to impose 25 percent tariffs on products imported from Canada and Mexico, while imports from China will face an additional 10 percent tax on top of the 10 percent tax previously enacted. However, Commerce Secretary Howard Lutnick told Fox Business that Trump <a href="https://www.bloomberg.com/news/articles/2025-03-04/lutnick-says-trump-considering-some-mexico-canada-tariff-relief?utm_source=website&amp;utm_medium=share&amp;utm_campaign=twitter">might “work something out” with Canada and Mexico</a>, adding that he could announce a potential compromise on Wednesday.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ARC-AGI without pretraining (231 pts)]]></title>
            <link>https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html</link>
            <guid>43259182</guid>
            <pubDate>Tue, 04 Mar 2025 19:52:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html">https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html</a>, See on <a href="https://news.ycombinator.com/item?id=43259182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
  <p><a name="topofpage"></a><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/teaser_figure_w_title.png" alt="image">
By <a href="https://iliao2345.github.io/">Isaac Liao</a> and <a href="https://goombalab.github.io/">Albert Gu</a></p>

  
  <p>In this blog post, we aim to answer a simple yet fundamental question:</p>

  <p><strong>Can lossless information compression by itself produce intelligent behavior?</strong></p>

  <p>The idea that efficient compression by itself lies at the heart of intelligence is not new (see, e.g., <a href="https://www.researchgate.net/publication/2472570_A_Formal_Definition_of_Intelligence_Based_on_an_Intensional_Variant_of_Algorithmic_Complexity">Hernández-Orallo &amp; Minaya-Collado, 1998</a>; <a href="https://gwern.net/doc/cs/algorithm/information/compression/1999-mahoney.pdf">Mahoney, 1999</a>; <a href="https://link.springer.com/book/10.1007/b138233">Hutter, 2005</a>; <a href="https://arxiv.org/abs/0712.3329">Legg &amp; Hutter, 2007</a>). Rather than revisiting those theoretical discussions, we make a practical demonstration instead.</p>

  <p>In this work, we give evidence that lossless compression during inference time is sufficient to produce intelligent behavior, by developing a method <strong>purely based on compression</strong> that performs well on the <a href="https://arcprize.org/">ARC-AGI challenge</a>, a dataset of IQ-test-like puzzles about inferring a procedure/rule from limited demonstrations. Crucially, our solution, which we name <em>CompressARC</em>, obeys the following three restrictions:</p>

  <ul>
    <li><strong>No pretraining</strong>; models are randomly initialized and trained during inference time.</li>
    <li><strong>No dataset</strong>; one model trains on just the target ARC-AGI puzzle and outputs one answer.</li>
    <li><strong>No search</strong>, in most senses of the word—just gradient descent.</li>
  </ul>

  <p>Despite these constraints, CompressARC achieves 34.75% on the training set and 20% on the evaluation set—processing each puzzle in roughly 20 minutes on an RTX 4070. To our knowledge, this is the first neural method for solving ARC-AGI where the training data is limited to just the target puzzle. CompressARC’s intelligence emerges not from pretraining, vast datasets, exhaustive search, or massive compute—but from compression. We challenge the conventional reliance on extensive pretraining and data, and propose a future where tailored compressive objectives and efficient inference-time computation work together to extract deep intelligence from minimal input.</p>

  

  <h2 id="what-is-arc-agi">What is ARC-AGI?</h2>

  <p><a href="https://arcprize.org/">ARC-AGI</a>, <a href="https://arxiv.org/abs/1911.01547">introduced in 2019</a>, is an artificial intelligence benchmark designed to test a system’s ability to infer and generalize abstract rules from minimal examples. The dataset consists of IQ-test-like puzzles, where each puzzle provides several example images that demonstrate an underlying rule, along with a test image that requires completing or applying that rule. While some have suggested that solving ARC-AGI might signal the advent of <a href="https://arxiv.org/abs/1911.01547">artificial general intelligence</a> (AGI), its true purpose is to spotlight the current challenges hindering progress toward AGI. Below are three of the 1000 puzzles:</p>

  <table>
    <thead>
      <tr>
        <th>Hidden rule: Shift every object to the right by one pixel, except the bottom/right edges of the object.</th>
        <th>Hidden rule: Shrink the big object and set its color to the scattered dots’ color.</th>
        <th>Hidden rule: Extend the green line to meet the red line by turning when hitting a wall.</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/025d127b_problem.png" alt="image"></td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/5ad4f10b_problem.png" alt="image"></td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/2dd70a9a_problem.png" alt="image"></td>
      </tr>
    </tbody>
  </table>

  <p>For every puzzle, there is a hidden rule that maps each input grid to each output grid. You are given some number of examples of input-to-output mappings, and you get <strong>two attempts</strong> to guess the output grid for a given input grid, without being told the hidden rule. If either guess is correct, then you score 1 for that puzzle, else you score 0. You are allowed to change the size of the output grid and pick the color of every pixel. The puzzles are designed so that <strong>humans can reasonably find the answer, but machines should have more difficulty</strong>. <a href="https://arcprize.org/guide">The average human can solve 76.2% of the training set</a>, and <a href="https://arxiv.org/abs/2409.01374">a human expert can solve 98.5%.</a></p>

  <p>The 400 training puzzles are easier than the rest, and are meant to help you learn the following patterns:</p>
  <ul>
    <li><strong>Objectness:</strong> Objects persist and cannot appear or disappear without reason. Objects can interact or not depending on the circumstances.</li>
    <li><strong>Goal-directedness:</strong> Objects can be animate or inanimate. Some objects are “agents” - they have intentions and they pursue goals.</li>
    <li><strong>Numbers &amp; counting:</strong> Objects can be counted or sorted by their shape, appearance, or movement using basic mathematics like addition, subtraction, and comparison.</li>
    <li><strong>Basic geometry &amp; topology:</strong> Objects can be shapes like rectangles, triangles, and circles which can be mirrored, rotated, translated, deformed, combined, repeated, etc. Differences in distances can be detected.</li>
  </ul>

  <p>The ARC Prize team has repeatedly launched competitions for solving ARC-AGI, with monetary rewards. <a href="https://www.kaggle.com/competitions/arc-prize-2024">The most recent competition</a> involved potential prizes and awards of upwards of <strong>$1,000,000</strong>, with the main prize reserved for methods which could achieve 85% on a private test set of 100 puzzles, using 12 hours of compute in a constrained environment.</p>

  

  <h2 id="our-solution-method">Our Solution Method</h2>

  <!--<img align="right" src="./resources/algorithm_environment.JPG" width="50%" style="margin: 20px 0 20px 10px;">-->

  <p><strong>We propose that lossless information compression can serve as an effective framework for solving ARC-AGI puzzles. A more efficient (i.e., lower-bit) compression of a puzzle correlates with a more accurate solution.</strong> To solve ARC-AGI puzzles, we design a system that transforms an incomplete puzzle into a completed one—filling in the answers—by finding a compact representation that, when decompressed, reproduces the puzzle with any solution. The key challenge is to obtain this compact representation without needing the answers as inputs.</p>

  <p>CompressARC uses a neural network as the decoder. However, the encoding algorithm is not another network—instead, encoding is realized by the gradient descent algorithm that performs inference-time training on the decoder while maintaining correct decoded output. In other words, running the encoder means optimizing the decoder’s parameters and input distribution to achieve the most compressed puzzle representation. The resulting optimized parameters (e.g., weights and input distribution settings) themselves serve as the compressed bit representation that encodes the puzzle along with its answer.</p>

  <p>In standard machine learning lingo: (without compression terminology, and with some simplifications)</p>

  <ol>
    <li>We start at inference time, and we are given an ARC-AGI puzzle to solve. (e.g., puzzle in the diagram below.)</li>
    <li>We construct a neural network $f$ (see <a href="#architecture">architecture</a>) designed for the puzzle’s specifics (e.g., number of examples, observed colors). The network takes random normal input $z \sim N(\mu, \Sigma)$, and per-pixel color logit predictions across all the grids, including an answer grid (3 input-output examples, for a total of 6 grids). Importantly, $f_\theta$ is equivariant to common augmentations—such as reordering input-output pairs (including the answer’s pair), color permutations, and spatial rotations/reflections.</li>
    <li>We initialize the network weights $\theta$ and set the parameters $\mu$ and $\Sigma$ for the $z$ distribution.</li>
    <li>We jointly optimize $\theta$, $\mu$, and $\Sigma$ to minimize the sum of cross-entropies over the known grids (5 of them,) ignoring the answer grid. A KL divergence penalty keeps $N(\mu, \Sigma)$ close to $N(0,1)$, as in a VAE.</li>
    <li>Since the generated answer grid is stochastic due to the randomness in $z$, we save the answer grids throughout training and choose the most frequently occuring one as our final prediction.</li>
  </ol>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Method_Overview.png"></p>

  <p>It isn’t obvious why such a method is performing compression. You’ll see later <a href="#how-to-derive-our-solution-method">how we derived it</a> from trying to compress ARC-AGI. First, let’s see it try to solve the puzzle above.</p>

  <h3 id="watching-the-network-learn-color-the-boxes">Watching the Network Learn: Color the Boxes</h3>

  <h4 id="human-solution">Human Solution:</h4>
  <p>We first realize that the input is divided into boxes, and the boxes are still there in the output, but now they’re colored. We then try to figure out which colors go in which boxes. First, we notice that the corners are always black. Then, we notice that the middle is always magenta. And after that, we notice that the color of the side boxes depends on which direction they are in: red for up, blue for down, green for right, and yellow for left. At this point, we copy the input over to the answer grid, then we color the middle box magenta, and then color the rest of the boxes according to their direction.</p>

  <h4 id="compressarc-solution">CompressARC Solution:</h4>
  <table>
  <tbody><tr>
  <td>
  <strong> 50 steps of learning:</strong>
  <p>
  CompressARC's network outputs an answer grid (sample) with light blue rows/columns wherever the input has the same. It has noticed that all the other input-output pairs in the puzzle exhibit this correspondence. It doesn't know how the other output pixels are assigned colors; an exponential moving average of the network output (sample average) shows the network assigning mostly the same average color to non-light-blue pixels.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_50_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 150 steps of learning:</strong>
  <p>
  The network outputs a grid where nearby pixels have similar colors. It has likely noticed that this is common among all the outputs, and is guessing that it applies to the answer too.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_150_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 200 steps of learning:</strong>
  <p>
  The network output now shows larger blobs of colors that are cut off by the light blue borders. It has noticed the common usage of borders to demarcate blobs of colors in other outputs, and applies the same idea here. It has also noticed black corner blobs in other given outputs, which the network imitates.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_200_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 350 steps of learning:</strong>
  <p>
  The network output now shows the correct colors assigned to boxes of the correct direction from the center. It has realized that a single color-to-direction mapping is used to pick the blob colors in the other given outputs, so it imitates this mapping. It is still not the best at coloring within the lines, and it's also confused about the center blob, probably because the middle does not correspond to a direction. Nevertheless, the averate network output does show a tinge of the correct magenta color in the middle, meaning the network is catching on.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_350_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 1500 steps of learning:</strong>
  <p>
  The network is as refined as it will ever be. Sometimes it will still make a mistake in the sample it outputs, but this uncommon and filtered out.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_1500_steps.png"></td>
  </tr>
</tbody></table>

  <p>After training, <a href="#solution-analysis-color-the-boxes">we can deconstruct the learned z distribution</a> to find that it codes for a color-direction correspondence table and row/column divider positions!</p>

  

  <h2 id="how-to-derive-our-solution-method">How to Derive Our Solution Method</h2>

  <p>Again, it isn’t obvious how we get from trying to perform compression to the method we ended up using. The derivation of our algorithm takes us on a detour through <a href="https://en.wikipedia.org/wiki/Information_theory">information theory</a>, <a href="https://arxiv.org/abs/0809.2754">algorithmic information theory</a>, and <a href="https://en.wikipedia.org/wiki/Coding_theory">coding theory</a>, with machine learning only making an appearance near the end.</p>

  <h3 id="a-primer-on-lossless-information-compression">A Primer on Lossless Information Compression</h3>

  <p>In <a href="https://en.wikipedia.org/wiki/Information_theory">information theory</a>, lossless information compression is about trying to represent some information in as few bits as possible, while still being able to reconstruct that information from the bit representation. This type of problem is abstracted as follows:</p>
  <ul>
    <li>A source produces some symbol $x$ from some process that generates symbols from a probability distribution $p(x)$.</li>
    <li>A compressor/encoder $E$ must map the symbol $x$ to a string of bits $s$.</li>
    <li>A decompressor/decoder $D$ must exactly map $s$ back to the original symbol $x$.</li>
  </ul>

  <p>The goal is to use $p$ to construct functions $(E, D)$ which are bit-efficient, (ie. that minimize the expected length of $s$,) without getting any symbols wrong. In our case, the symbol $x$ is the ARC-AGI dataset (many puzzle + answer pairs), and we want to figure out what answers the best compression system might decompress the answers to be. Except, we don’t have the answers (only the puzzles) to give as input to $E$, and we don’t know $p$, since it’s hard to model the intelligent process of puzzle ideation in humans.</p>

  <h3 id="one-size-fits-all-compression">One-Size-Fits-All Compression</h3>

  <p>To build our compression scheme, you might think we need to know what $p$ is, but we argue that it doesn’t really matter since we can make a one-size-fits-all compressor. It all hinges on the following assumption:</p>
  <blockquote>
    <p>There exists some practically implementable, bit efficient compression system $(E, D)$ for ARC-AGI datasets $x$ sampled from $p$.</p>
  </blockquote>

  <p>If this were false, our whole idea of solving ARC-AGI with compression is doomed even if we knew $p$ anyways, so we might as well make this assumption.</p>

  <p>Our one-size-fits-all compressor $(E’, D’)$ is built without knowing $p$, and it is almost just as bit-efficient as the original $(E, D)$:</p>
  <ul>
    <li>$E’$ observes symbol $x$, picks a program $f$ and input $s$ to minimize $\text{len}(f)+\text{len}(s)$ under the constraint that running the program makes $f(s)=x$, and then sends the pair $(f, s)$.</li>
    <li>$D’$ is just a program executor that executes $f$ on $s$, correctly producing $x$.</li>
  </ul>

  <p>It is possible to prove with <a href="https://arxiv.org/abs/0809.2754">algorithmic information theory</a> that $(E’, D’)$ achieves a bit efficiency at most $\text{len}(f)$ bits worse than the bit efficiency of $(E, D)$, where $f$ is the <em>code for implementing D</em>. But since compression is practically implementable, the code for $D$ should be simple enough for a human engineer to write, so $\text{len}(f)$ must be short, meaning our one-size-fits-all compressor will be close to the best possible bit efficiency.</p>

  <p>Ironically, the only problem with using this to solve ARC-AGI is that implementing $E’$ is not practical, since $E’$ needs to minimize the length of a program-input pair $(f, s)$ under partial fixed output constraint $f(s)_{answers}=x_{answers}$.</p>

  <h3 id="neural-networks-to-the-rescue">Neural Networks to the Rescue</h3>

  <p>To avoid searching through program space, we just pick a program $f$ for a small sacrifice in bit efficiency. We hope the diversity of program space can be delegated to diversity in input $s$ space instead. Specifically, we write a program $f$ that runs the forward pass of a neural network, where $s=(\theta, z, \epsilon)$ are the weights, inputs, and corrections to the outputs of the neural network. Then, we can use gradient descent to “search” over $s$.</p>

  <p>This restricted compression scheme uses <a href="https://arxiv.org/abs/2010.01185">Relative Entropy Coding</a> (REC)<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">1</a></sup> to encode noisy weights $\theta$ and neural network inputs $z$ into bits $s_\theta$ and $s_z$, and <a href="https://en.wikipedia.org/wiki/Arithmetic_coding">arithmetic coding</a> to encode output error corrections $\epsilon$ into bits $s_\epsilon$, to make a bit string $s$ consisting of three blocks $(s_\theta, s_z, s_\epsilon)$. The compression scheme runs as follows:</p>
  <ul>
    <li>The decoder runs $\theta = \text{REC-decode}(s_\theta)$, $z = \text{REC-decode}(s_z)$, $\text{logits} = \text{Neural-Net}(\theta, z)$, and $x=\text{Arithmetic-decode}(s_\epsilon, \text{logits})$.</li>
    <li>The encoder trains $\theta$ and $z$ to minimize the total code length $\mathbb{E}[\text{len}(s)]$. $s_\epsilon$ is fixed by arithmetic coding to guarantee correct decoding. To calculate the three components of the loss $\mathbb{E}[\text{len}(s)]$ in a differentiable way, we refer to the properties of REC and arithmetic coding:
      <ul>
        <li>It turns out that the $\epsilon$ code length $\mathbb{E}[\text{len}(s_\epsilon)]$ is equal to the total crossentropy error on all the given grids in the puzzle.</li>
        <li>REC requires us to fix some reference distribution $q_\theta$, and also add noise to $\theta$, turning it into a distribution $p_\theta$. Then, REC allows you to store noisy $\theta$ using a code length of $\mathbb{E}[\text{len}(s_\theta)] = KL(p_\theta|| q_\theta) = \mathbb{E}_{\theta \sim p_\theta} [\log (p_\theta(\theta) / q_\theta(\theta))]$ bits. We will choose to fix $q_\theta = N(0, I/2\lambda)$ for large $\lambda$, such that the loss component $\mathbb{E}[\text{len}(s_\theta)] \approx \lambda | \theta|^2 + \text{const}$ is equivalent to regularizing the decoder.</li>
        <li>We must also do for $z$ what we do for $\theta$, since it’s also represented using REC. We will choose to fix $q_z = N(0,I)$, so the code length of $z$ is $\mathbb{E}[\text{len}(s_z)] = KL(p_z|| q_z) = \mathbb{E}_{z \sim p_z} [\log (p_z(z) / q_z(z))]$.</li>
      </ul>

      <p>We can compute gradients of these code lengths via the <a href="https://arxiv.org/abs/1312.6114">reparameterization trick</a>.</p>
    </li>
  </ul>

  <p>At this point, we observe that the total code length for $s$ that we described is actually the VAE loss with decoder regularization (= KL for $z$ + reconstruction error + regularization). Likewise, if we port the rest of what we described above (plus modifications regarding equivariances and inter-puzzle independence, and ignoring regularization) into typical machine learning lingo, we get the <a href="#our-solution-method">above description of CompressARC</a>.</p>

  

  <h2 id="architecture">Architecture</h2>

  <p>We designed our own neural network architecture for decoding the latents $z$ into ARC-AGI puzzles. The most important feature of our architecture is it’s equivariances, which are symmetry rules dictating that whenever the input $z$ undergoes a transformation, the output ARC-AGI puzzle must also transform the same way. Some examples:</p>

  <ul>
    <li>reordering of input/output pairs</li>
    <li>shuffling colors</li>
    <li>flips, rotations, and reflections of grids</li>
  </ul>

  <p>There are too many equivariances for us to think about at once, so we decided to make a <strong>base architecture that’s fully symmetric</strong>, and break unwanted symmetries one by one by <strong>adding asymmetric layers</strong> to give it <a href="#what-puzzles-can-and-cant-we-solve">specific non-equivariant abilities</a>.</p>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Equivariant_Architecture.png"></p>

  <p>To illustrate what we mean, suppose that both $z$ and an ARC-AGI puzzle take the form of a tensor of shape $[n\_examples, n\_colors, height, width, 2 \text{ for input/output}]$ (This is not actually the format of the data (see <a href="#multitensors">multitensors</a>) but it gets the idea across best.) Then, our network starts out as equivariant to permutations of indices in the $example$, $color$, $height$, and $width$ dimensions. Some extra care must be taken with weight sharing, to force the network to also be equivariant to swapping the $width$ and $height$ dimensions. We may then add a layer involving a roll by one in the $width$ and $height$ dimensions, to let the network distinguish short range spatial interactions but not long-range ones.</p>

  <!-- When $z$ is fully symmetrical, then the outputted puzzle must also be fully symmetrical. But notice that [CompressARC](#our-solution-method) is allowed to learn asymmetrical $z$ in order to obtain asymmetrical outputs. Since the $z$ distribution is penalized for deviating from the fully symmetric $N(0,I)$, asymmetrical outputs are discouraged. So, the network must pay a penalty to use $z$ to distinguish the learned roles of two colors, two rows, two pixels, etc. in a puzzle. This makes $z$ naturally lean towards simpler representations of the puzzle. -->

  <p>The actual data ($z$, hidden activations, and puzzles) passing through our layers comes in a format that we call a “<strong>multitensor</strong>”, which is just a bucket of tensors of various shapes. All the equivariances can be described in terms of how they change a multitensor. <strong>In order to understand any of the layers we list, you must first read the below section on multitensors.</strong></p>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Multitensor.png" width="50%"></p>

  <h3 id="multitensors">Multitensors</h3>

  <p>Most common classes of machine learning architectures operate on a single type of tensor with constant rank. LLMs operate on rank 3 tensors of shape $[n\_batch, n\_tokens, n\_channels]$, and CNNs operate on a rank 4 tensors of shape $[n\_batch, n\_channels, height, width]$. Our multitensors are a set of varying-rank tensors of unique type, whose dimensions are a subset of a rank 6 tensor of shape $[n\_examples$, $n\_colors$, $n\_directions$, $height$, $width$, $n\_channels]$. We always keep the $channel$ dimension, so there are at most 32 tensors in every multitensor. We also maintain <a href="#rules-for-legal-multitensors">several rules</a> that determine whether a tensor shape is “legal” or not, which reduces the number of tensors in a multitensor to 18.</p>

  <table>
    <thead>
      <tr>
        <th>Dimension</th>
        <th>Size</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Example</td>
        <td>Number of examples in the ARC-AGI puzzle, including the one with held-out answer</td>
      </tr>
      <tr>
        <td>Color</td>
        <td>Number of unique colors in the ARC-AGI puzzle, <a href="#number-of-colors">not including black</a></td>
      </tr>
      <tr>
        <td>Direction</td>
        <td>8</td>
      </tr>
      <tr>
        <td>Height</td>
        <td><a href="#output-shape-determination">Determined when preprocessing the puzzle</a></td>
      </tr>
      <tr>
        <td>Width</td>
        <td><a href="#output-shape-determination">Determined when preprocessing the puzzle</a></td>
      </tr>
      <tr>
        <td>Channel</td>
        <td>In the residual connections, the size is 8 if the $direction$ dimension is included, else 16. Within layers it is layer-dependent.</td>
      </tr>
    </tbody>
  </table>

  <p>To give an idea of how a multitensor stores data, an ARC-AGI puzzle can be represented by using the $[examples, colors, height, width, channel]$ tensor, by using the $channel$ dimension to select either the input or output grid, and the $width$/$height$ dimensions for pixel location, a one hot vector in the $color$ dimension, specifying what color that pixel is. The $[examples, width, channel]$ and $[examples, height, channel]$ tensors can similarly be used to store masks representing grid shapes for every example for every input/output grid. All those tensors are included in a single multitensor that is computed by the network just before the final <a href="#linear-heads">linear heads</a> layer.</p>

  <p>When we apply an operation on a multitensor, we by default assume that all non-$channel$ dimensions are treated identically as batch dimensions by default. The operation is copied across the indices of dimensions unless specified. This ensures that we keep all our symmetries intact until we use a specific layer meant to break a specific symmetry.</p>

  <p>A final note on the $channel$ dimension: usually when talking about a tensor’s shape, we will not even mention the $channel$ dimension as it is included by default.</p>

  <p><strong>The full architecture consists of the following layers, which are each described in the Appendix:</strong></p>
  <ul>
    <li>Begin with parameters of the $z$ distribution,</li>
    <li><a href="#decoding-layer">Decoding Layer</a></li>
    <li>4x
      <ul>
        <li><a href="#multitensor-communication-layer">Multitensor Communication Layer, Upwards</a></li>
        <li><a href="#softmax-layer">Softmax Layer</a></li>
        <li><a href="#directional-cummaxshift-layer">Directional Cummax Layer</a></li>
        <li><a href="#directional-cummaxshift-layer">Directional Shift Layer</a></li>
        <li><a href="#directional-communication-layer">Directional Communication Layer</a></li>
        <li><a href="#nonlinear-layer">Nonlinear Layer</a></li>
        <li><a href="#multitensor-communication-layer">Multitensor Communication Layer, Downwards</a></li>
        <li><a href="#normalization-layer">Normalization Layer</a></li>
      </ul>
    </li>
    <li><a href="#linear-heads">Linear Heads</a></li>
  </ul>

  

  <h2 id="results">Results</h2>

  <h3 id="training-set-3475">Training set: 34.75%</h3>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/accuracy_curve_at_n_training.png">
</p>

  <div>
  <table>
    <tbody><tr>
      <th>Training Iteration</th>
      <th>Time</th>
      <th>Pass@1</th>
      <th>Pass@2</th>
      <th>Pass@5</th>
      <th>Pass@10</th>
      <th>Pass@100</th>
      <th>Pass@1000</th>
    </tr>
    <tr>
      <td>100</td>
      <td>6 h</td>
      <td>1%</td>
      <td>2.25%</td>
      <td>3.5%</td>
      <td>4.75%</td>
      <td>6.75%</td>
      <td>6.75%</td>
    </tr>
    <tr>
      <td>200</td>
      <td>13 h</td>
      <td>11.5%</td>
      <td>14.25%</td>
      <td>16.5%</td>
      <td>18.25%</td>
      <td>23.25%</td>
      <td>23.5%</td>
    </tr>
    <tr>
      <td>300</td>
      <td>19 h</td>
      <td>18.5%</td>
      <td>21.25%</td>
      <td>23.5%</td>
      <td>26.75%</td>
      <td>31.5%</td>
      <td>32.5%</td>
    </tr>
    <tr>
      <td>400</td>
      <td>26 h</td>
      <td>21%</td>
      <td>25%</td>
      <td>28.75%</td>
      <td>31%</td>
      <td>36%</td>
      <td>37.5%</td>
    </tr>
    <tr>
      <td>500</td>
      <td>32 h</td>
      <td>23%</td>
      <td>27.5%</td>
      <td>31.5%</td>
      <td>33.5%</td>
      <td>39.25%</td>
      <td>40.75%</td>
    </tr>
    <tr>
      <td>750</td>
      <td>49 h</td>
      <td>28%</td>
      <td>30.5%</td>
      <td>34%</td>
      <td>36.25%</td>
      <td>42.75%</td>
      <td>44.5%</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>65 h</td>
      <td>28%</td>
      <td>31.75%</td>
      <td>35.5%</td>
      <td>37.75%</td>
      <td>43.75%</td>
      <td>46.5%</td>
    </tr>
    <tr>
      <td>1250</td>
      <td>81 h</td>
      <td>29%</td>
      <td>32.25%</td>
      <td>37%</td>
      <td>39.25%</td>
      <td>45.5%</td>
      <td>49.25%</td>
    </tr>
    <tr>
      <td>1500</td>
      <td>97 h</td>
      <td>29.5%</td>
      <td>33%</td>
      <td>38.25%</td>
      <td>40.75%</td>
      <td>46.75%</td>
      <td>51.75%</td>
    </tr>
    <tr>
      <td>2000</td>
      <td>130 h</td>
      <td>30.25%</td>
      <td>34.75%</td>
      <td>38.25%</td>
      <td>41.5%</td>
      <td>48.5%</td>
      <td>52.75%</td>
    </tr>
  </tbody></table>
</div>

  <h3 id="evaluation-set-20">Evaluation set: 20%</h3>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/accuracy_curve_at_n_evaluation.png">
</p>

  <div>
  <table>
    <tbody><tr>
      <th>Training Iteration</th>
      <th>Time</th>
      <th>Pass@1</th>
      <th>Pass@2</th>
      <th>Pass@5</th>
      <th>Pass@10</th>
      <th>Pass@100</th>
      <th>Pass@1000</th>
    </tr>
    <tr>
      <td>100</td>
      <td>7 h</td>
      <td>0.75%</td>
      <td>1.25%</td>
      <td>2.25%</td>
      <td>2.5%</td>
      <td>3%</td>
      <td>3%</td>
    </tr>
    <tr>
      <td>200</td>
      <td>14 h</td>
      <td>5%</td>
      <td>6%</td>
      <td>7%</td>
      <td>7.75%</td>
      <td>12%</td>
      <td>12.25%</td>
    </tr>
    <tr>
      <td>300</td>
      <td>21 h</td>
      <td>10%</td>
      <td>10.75%</td>
      <td>12.25%</td>
      <td>13.25%</td>
      <td>15.5%</td>
      <td>16.25%</td>
    </tr>
    <tr>
      <td>400</td>
      <td>28 h</td>
      <td>11.75%</td>
      <td>13.75%</td>
      <td>16%</td>
      <td>17%</td>
      <td>19.75%</td>
      <td>20%</td>
    </tr>
    <tr>
      <td>500</td>
      <td>34 h</td>
      <td>13.5%</td>
      <td>15%</td>
      <td>17.75%</td>
      <td>19.25%</td>
      <td>20.5%</td>
      <td>21.5%</td>
    </tr>
    <tr>
      <td>750</td>
      <td>52 h</td>
      <td>15.5%</td>
      <td>17.75%</td>
      <td>19.75%</td>
      <td>21.5%</td>
      <td>22.75%</td>
      <td>25.5%</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>69 h</td>
      <td>16.75%</td>
      <td>19.25%</td>
      <td>21.75%</td>
      <td>23%</td>
      <td>26%</td>
      <td>28.75%</td>
    </tr>
    <tr>
      <td>1250</td>
      <td>86 h</td>
      <td>17%</td>
      <td>20.75%</td>
      <td>23%</td>
      <td>24.5%</td>
      <td>28.25%</td>
      <td>30.75%</td>
    </tr>
    <tr>
      <td>1500</td>
      <td>103 h</td>
      <td>18.25%</td>
      <td>21.5%</td>
      <td>24.25%</td>
      <td>25.5%</td>
      <td>29.5%</td>
      <td>31.75%</td>
    </tr>
    <tr>
      <td>2000</td>
      <td>138 h</td>
      <td>18.5%</td>
      <td>20%</td>
      <td>24.25%</td>
      <td>26%</td>
      <td>31.25%</td>
      <td>33.75%</td>
    </tr>
  </tbody></table>
</div>

  <h3 id="what-puzzles-can-and-cant-we-solve">What Puzzles Can and Can’t We Solve?</h3>

  <p><strong>CompressARC tries to use its abilities to figure out as much as it can, until it gets bottlenecked by one of it’s inabilities.</strong></p>

  <p>For example, puzzle 28e73c20 in the training set requires extension of a pattern from the edge towards the middle:</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/28e73c20_problem.png">
</p>

  <p>Given the layers in it’s network, CompressARC is generally able to extend patterns for short ranges but not long ranges. So, it does the best that it can, and correctly extends the pattern a short distance before guessing at what happens near the center:</p>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/28e73c20_solutions.png" alt="image"></p>

  <p>A short list of abilities that <strong>can</strong> be performed by CompressARC includes:</p>
  <ul>
    <li>Assigning individual colors to individual procedures (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">0ca9ddb6</a>)</li>
    <li>Infilling (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">0dfd9992</a>)</li>
    <li>Cropping (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">1c786137</a>)</li>
    <li>Connecting dots with lines, including 45 degree diagonal lines (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">1f876c06</a>)</li>
    <li>Same color detection (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">1f876c06</a>)</li>
    <li>Identifying pixel adjacencies (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">42a50994</a>)</li>
    <li>Assigning individual colors to individual examples (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">3bd67248</a>)</li>
    <li>Identifying parts of a shape (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">025d127b</a>)</li>
    <li>Translation by short distances (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">025d127b</a>)</li>
  </ul>

  <p>A short list of abilities that <strong>cannot</strong> be performed by CompressARC includes:</p>
  <ul>
    <li>Assigning two colors to each other (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">0d3d703e</a>)</li>
    <li>Repeating an operation in series many times (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">0a938d79</a>)</li>
    <li>Counting/numbers (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">ce9e57f2</a>)</li>
    <li>Translation, rotation, reflections, rescaling, image duplication (see puzzles <a href="#list-of-mentioned-arc-agi-puzzles">0e206a2e</a>, <a href="#list-of-mentioned-arc-agi-puzzles">5ad4f10b</a>, and <a href="#list-of-mentioned-arc-agi-puzzles">2bcee788</a>)</li>
    <li>Detecting topological properties such as connectivity (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">7b6016b9</a>)</li>
    <li>Planning, simulating the behavior of an agent (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">2dd70a9a</a>)</li>
    <li>Long range extensions of patterns (see puzzle 28e73c20 above)</li>
  </ul>

  

  <h2 id="case-study-color-the-boxes">Case Study: Color the Boxes</h2>

  <p>(Additional case studies can be found in the <a href="#additional-case-studies">Appendix</a>.)</p>

  <p>We show the puzzle again for convenience.</p>
  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_problem.png">
</p>

  <p>During training, the reconstruction error fell extremely quickly. It remained low on average, but would spike up every once in a while, causing the KL from $z$ to bump upwards at these moments.</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_KL_vs_reconstruction.png">
</p>

  <h3 id="solution-analysis-color-the-boxes">Solution Analysis: Color the Boxes</h3>

  <p>So how does CompressARC learn to solve the puzzle? Let’s look at the representations stored in $z$ to find out.</p>

  <p>Since $z$ is a <a href="#multitensors">multitensor</a>, each of the tensors it contains produces an additive contribution to the total KL for $z$. By looking at the per-tensor contributions, we can determine which tensors in $z$ code for information that is used to represent the puzzle. Below is a plot showing the quantity of information stored in each tensor of $z$, ie. the KL contribution used by the <a href="#decoding-layer">decoding layer</a>.</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_KL_components.png">
</p>

  <p>All the tensors fall to zero information content during training, except for four tensors. In some replications of this experiment, we saw one of these four necessary tensors fall to zero information content, and CompressARC typically does not recover the correct answer after that. Here we are showing a lucky run where the $(color, direction, channel)$ tensor almost falls but gets picked up 200 steps in, which is right around when the samples from the model begin to show the correct colors in the correct boxes.</p>

  <p>We can look at the average output of the <a href="#decoding-layer">decoding layer</a> corresponding to individual tensors of $z$, to see what information is stored there. Each tensor contains a vector of dimension $n\_channels$ for various indices of the tensor. Taking the PCA of these vectors reveals some number of activated components, telling us how many pieces of information are coded by the tensor.</p>

  <table>
  <tbody><tr>
  <td>
  <strong> (Examples, height, channel) tensor:</strong>
  <p>
  For every example and row, there is a vector of dimension $n\_channels$. This forms a dataset of vectors. Taking the PCA of these vectors, the top principal component vector reformatted back into an $(examples, height)$ matrix (shown on right) can tell us which examples/row combinations are uniquely identified by the stored information. The top principal component (shown on right) is 1485 times stronger than the second principal component, which indicates to us that basically all of the information is in the above tensor. <strong>For every example, the two brightest pixels give the rows where the light blue rows in the grids are.</strong></p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_example_height_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Examples, width, channel) tensor:</strong>
  <p>
  A very similar story here: in the top principal component of this tensor, <strong>the two darkest pixels for every example give the columns where the light blue columns in the grids are.</strong> The top principal component is 1253 times stronger than the next principal component.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_example_width_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Direction, color, channel) tensor:</strong>
  <p>
  In this tensor, we see that the four brightest pixels identify blue with up, green with left, red with down, and yellow with right. <strong>This tensor seems to tell each direction which color to use for the opposite direction's corresponding box.</strong> The top principal component is 829 times stronger than the next principal component.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_color_direction_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Color, channel) tensor:</strong>
  <p>
  Here, we look at the top three principal components, since the first and second principal components are 134 and 87 times stronger than the third component, indicating that they play a role while the third component does not. The <strong>magenta and light blue colors</strong> are uniquely identified, indicating their special usage amongst the rest of the colors as <strong>the center color and the color of the row/column divisions</strong>, respectively.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_color_component_0.png"><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_color_component_1.png"></td>
  </tr>
</tbody></table>

  

  <h2 id="how-to-improve-our-work">How to Improve Our Work</h2>

  <p>At the time of release of CompressARC, there were several ideas which we thought of trying or attempted at some point, but didn’t manage to get working for one reason or another. Some ideas we still believe in, but didn’t use, are listed below.</p>

  <h4 id="joint-compression-via-weight-sharing-between-puzzles">Joint Compression via Weight Sharing Between Puzzles</h4>

  <p>CompressARC tries to solve each puzzle serially by compressing each puzzle on its own. We believe that joint compression of all the entire ARC-AGI dataset at once should yield better learned inductive biases per-puzzle, since computations learned for one puzzle can be transferred to other puzzles. We do not account for the complexity of $f$ in our <a href="#how-to-derive-our-solution-method">method</a>, allowing for $f$ to be used for memorization/overfitting. By jointly compressing the whole dataset, we only need to have one $f$, whereas when compressing each puzzle individually, we need to have an $f$ for every puzzle, allowing for more memorization/overfitting.</p>

  <p>To implement this, we would most likely explore strategies like:</p>
  <ul>
    <li>Using the same network weights for all puzzles, and training for puzzles in parallel. Each puzzle gets assigned some perturbation to the weights, that is constrained in some way, e.g., <a href="https://arxiv.org/abs/2106.09685">LORA</a>.</li>
    <li>Learning a “puzzle embedding” for every puzzle that is a high dimensional vector (more than 16 dim, less than 256 dim), and learning a linear mapping from puzzle embeddings to weights for our network. This mapping serves as a basic <a href="https://arxiv.org/abs/2306.06955">hypernetwork</a>, ie. a neural network that outputs weights for another neural network.
In a successful case, we might want to also try adding in some form of positional encodings, with the hope that $f$ is now small/simple enough to be incapable of memorization/overfitting using positional encodings.</li>
  </ul>

  <p>The reason we didn’t try this is because it would slow down the research iteration process.</p>

  <h4 id="convolution-like-layers-for-shape-copying-tasks">Convolution-like Layers for Shape Copying Tasks</h4>

  <p>This improvement is more ARC-AGI-specific and may have less to do with AGI in our view. Many ARC-AGI puzzles can be seen to involve copying shapes from one place to another, and our network has no inductive biases for such an operation. An operation which is capable of copying shapes onto multiple locations is the <a href="https://en.wikipedia.org/wiki/Convolution">convolution</a>. With one grid storing the shape and another with pixels activated at locations to copy to, convolving the two grids will produce another grid with the shape copied to the designated locations.</p>

  <p>There are several issues with introducing a convolutional operation for the network to use. Ideally, we would read two grids via projection from the residual stream, convolve them, and write it back in via another projection, with norms in the right places and such. Ignoring the fact that the grid size changes during convolution (can be solved with two parallel networks using different grid sizes), the bigger problem is that convolutions tend to amplify noise in the grids much more than the sparse signals, so their inductive bias is not good for shape copying. We can try to apply a softmax to one or both of the grids to reduce the noise (and to draw an interesting connection to attention), but we didn’t find any success.</p>

  <p>The last idea that we were tried before discarding the idea was to modify the functional form of the convolution:</p><p>

\[(f * g)(x) = \sum_y f(x-y)g(y)\]

  </p><p>to <a href="https://arxiv.org/abs/2103.02096">a tropical convolution</a>, which we found to work well on toy puzzles, but not well enough for ARC-AGI training puzzles (which is why we discarded this idea):</p><p>

\[(f*g)(x) = \max_y f(x-y) + g(y)\]

  </p><p>Convolutions, when repeated with some grids flipped by 180 degrees, tend to create high activations at the center pixel, so sometimes it is important to zero out the center pixel to preserve the signal.</p>

  <h4 id="kl-floor-for-posterior-collapse">KL Floor for Posterior Collapse</h4>

  <p>We noticed during testing that crucial posterior tensors whose <a href="https://arxiv.org/abs/1711.00937">KL fell to zero during learning</a> would never make a recovery and play their role in the encoding. We believe that the KL divergence may upper bound the information content of the gradient training signal for parts of the network that process the encoded information. Thus, when a tensor falls to zero KL, the network stops learning to use its information, so the KL is no longer given encouragement to recover. If we can hold the KL above zero for a while, the network may then learn to use the information, giving the KL a reason to stay above zero when released again.</p>

  <p>We implemented a mechanism to keep the KL above a minimum threshold so that the network always learns to use that information, but we do not believe it learns fast enough for this to be useful, as we have never seen a tensor recover before. Therefore, it might be useful to explore different ways to schedule this KL floor to start high and decay to zero, to allow learning when the KL is forced to be high, and to leave the KL unaffected later on in learning. This might cause training results to be more consistent across runs.</p>

  <h4 id="regularization">Regularization</h4>

  <p>We don’t use it. Maybe it matters, but we don’t know. Regularization measures the complexity of $f$ in our <a href="#how-to-derive-our-solution-method">problem formulation</a>, and is native to our derivation of CompressARC. It is somewhat reckless for us to exclude it in our implementation.</p>

  

  

  <h4 id="equivalence-of-compression-and-intelligence">Equivalence of Compression and Intelligence</h4>

  <p>The original inspiration of this work came from the <a href="http://prize.hutter1.net/">Hutter Prize</a>, which awards a prize for those who can compress a file of Wikipedia text the most, as a motivation for researchers to build intelligent systems. It is premised upon the idea that the ability to compress information is equivalent to intelligence.</p>

  <p>This equivalence between inteeligence and compression has a long history. For example, when talking about intelligent solutions to prediction problems, the ideal predictor implements <a href="https://www.sciencedirect.com/science/article/pii/S0019995864902232">Solomonoff Induction</a>, a theoretically best possible but uncomputable prediction algorithm that works universally for all prediction tasks. This prediction algorithm is then equivalent to a best possible compression algorithm whose compressed code length is the <a href="https://www.sciencedirect.com/science/article/pii/S0304397598000759?via%3Dihub">Kolmogorov Complexity</a> of the data. In our work, we try to approximate this best possible compression algorithm with a neural network. A related measure of complexity is known as the <a href="https://www.sciencedirect.com/science/article/abs/pii/0005109878900055?via%3Dihub">Minimum Description Length</a>.</p>

  <h4 id="information-theory-and-coding-theory">Information Theory and Coding Theory</h4>

  <p>Since we build an information compression system, we make use of many results in information theory and coding theory. The main result required to motivate our model architecture is the existence of <a href="https://arxiv.org/abs/2010.01185">Relative Entropy Coding</a> (REC). The fact that REC exists means that as long as a KL divergence can be bounded, the construction of a compression algorithm is always possible and the issue of realizing the algorithm can be abstracted away. Thus, problems about coding theory and translating information from Gaussians into binary and back can be ignored, since we can figure out the binary code length directly from the Gaussians instead. In other words, we only need to do enough information theory using the Gaussians to get the job done, with no coding theory at all. While the existence of <a href="https://en.wikipedia.org/wiki/Arithmetic_coding">arithmetic coding</a> would suffice to abstract the problem away when distributions are discrete, neural networks operate in a continuous space so we need REC instead.</p>

  <p>Our architecture sends $z$ information through an additive white Gaussian noise (AWGN) channel, so the <a href="https://en.wikipedia.org/wiki/Shannon%E2%80%93Hartley_theorem">AWGN channel capacity formula</a> (Gaussian input Gaussian noise) plays a heavy role in the design of our <a href="#decoding-layer">decoding layer</a>.</p>

  <h4 id="variational-autoencoders">Variational Autoencoders</h4>

  <p>The decoder side of the <a href="https://arxiv.org/abs/1312.6114">variational autoencoder</a> serves as our decompression algorithm. While we would use something that has more general capabilities like a <a href="https://arxiv.org/abs/1410.5401">neural Turing machine instead</a>, neural Turing machines are not very amenable to gradient descent-based optimization so we stuck with the VAE.</p>

  <p>VAEs have a long history of developments that are relevant to our work. At one point, we tried using multiple <a href="#decoding-layer">decoding layers</a> to make a <a href="https://arxiv.org/abs/1602.02282">hierarchical VAE</a> decoder instead. This does not affect Relative Entropy Coding with the AWGN channel because <a href="https://ieeexplore.ieee.org/document/1056798">channel capacity with feedback is equal to channel capacity without feedback</a>. But, we found empirically that the first decoding layer would absorb all of the KL contribution, making the later decoding layers useless. Thus, we only used one decoding layer at the beginning.</p>

  <p>The <a href="https://openreview.net/forum?id=Sy2fzU9gl">beta-VAE</a> introduces a reweighting of the reconstruction loss to be stronger than the KL loss, and we found that to work well in our case. The <a href="https://arxiv.org/abs/2007.03898">NVAE</a> applies a non-constant weighting to loss components. A rudimentary form of scheduled loss recombination is used in CompressARC.</p>

  <h4 id="arc-agi-methods">ARC-AGI Methods</h4>

  <p>Current methods for solving ARC-AGI focus primarily on using large language models (LLMs). ARC-AGI puzzles are converted into textual representations which are fed into LLMs as input. The LLM may directly output a textual representation of an answer, or some <a href="https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt">code which tries to convert input grids into output grids</a>. Top methods rely heavily on data augmentation and larger <a href="https://arxiv.org/abs/2411.02272">alternative datasets</a>, and sometimes perform autoregressive training on the target puzzle during inference time. Top solutions (<a href="https://ironbar.github.io/arc24/05_Solution_Summary/">example</a>) in the 2024 Kaggle prize competition frequently used <a href="https://arxiv.org/abs/1909.13231">test-time training</a>. <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">Reasoning models</a> have managed to get up to 87.5% on the semi-private evaluation set, albeit with astronomical amounts of compute.</p>

  <p>An older class of methods consists of hard-coded searches through program spaces in <a href="https://github.com/michaelhodel/arc-dsl">hand-written domain-specific languages designed specifically for ARC</a>. Another example <a href="https://github.com/victorvikram/ARC-icecuber">here</a>.</p>

  <p><a href="https://arxiv.org/html/2411.08706v1">Bonnet and Macfarlane introduced a VAE-based method</a> for searching through a latent space of programs.</p>

  <p>We believe CompressARC is the only method so far that uses deep learning without external pretraining nor any large-scale search.</p>

  <h4 id="deep-learning-architectures">Deep Learning Architectures</h4>

  <p>We designed our own neural network architecture from scratch, but not without borrowing crucial design principles from many others.</p>

  <p>Our architecture is fundamentally structured like a <a href="https://arxiv.org/abs/1706.03762">transformer</a>, consisting of a <a href="https://arxiv.org/abs/1512.03385">residual stream</a> where representations are stored and operated upon, followed by a linear head. <a href="https://arxiv.org/abs/2002.04745">Pre-and post-norms</a> with linear up- and down-projections allow layers to read and write to the residual stream. The <a href="https://arxiv.org/abs/1606.08415">SiLU</a>-based <a href="#nonlinear-layer">nonlinear layer</a> is especially similar to a transformer’s.</p>

  <p>Our equivariance structures are inspired by <a href="https://arxiv.org/abs/1703.06114">permutation-invariant neural networks</a>, which are a type of <a href="https://arxiv.org/abs/1602.07576">equivariant neural network</a>. Equivariance transformations are taken from common augmentations to ARC-AGI puzzles.</p>

  

  <hr>
  

  <h2 id="appendix">Appendix</h2>

  <h3 id="layers-in-the-architecture">Layers in the Architecture</h3>

  <h4 id="decoding-layer">Decoding Layer</h4>

  <p>This layer’s job is to sample a multitensor $z$ and bound its information content, before it is passed to the next layer. This layer and outputs the KL divergence between the learned $z$ distribution and $N(0,I)$. Penalizing the KL prevents CompressARC from learning a distribution for $z$ that memorizes the ARC-AGI puzzle in an uncompressed fashion, and forces it to represent the puzzle more succinctly. Specifically, it forces CompressARC to spend more bits on the KL whenever it uses $z$ to break a symmetry, and the larger the symmetry group broken, the more bits it spends.</p>

  <p>This layer takes as input:</p>
  <ul>
    <li>A learned target multiscalar, called the “target capacity”.<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" rel="footnote">2</a></sup> The decoding layer will output $z$ whose information content per tensor is close to the target capacity,<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" rel="footnote">3</a></sup></li>
    <li>learned per-element means for $z$,<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" rel="footnote">4</a></sup></li>
    <li>learned per-element capacity adjustments for $z$.</li>
  </ul>

  <p>We begin by normalizing the learned per-element means for $z$.<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" rel="footnote">5</a></sup> Then, we figure out how much Gaussian noise we must add into every tensor to make the <a href="https://en.wikipedia.org/wiki/Shannon%E2%80%93Hartley_theorem">AWGN channel capacity</a> equal to the target capacity for every tensor (including per-element capacity adjustments). We apply the noise to sample $z$, keeping unit variance of $z$ by rescaling.<sup id="fnref:13" role="doc-noteref"><a href="#fn:13" rel="footnote">6</a></sup></p>

  <p>We compute the information content of $z$ as the KL divergence between the distribution of this sample and $N(0,1)$.</p>

  <p>Finally, we postprocess the noisy $z$ by scaling it by the sigmoid of the signal-to-noise ratio.<sup id="fnref:14" role="doc-noteref"><a href="#fn:14" rel="footnote">7</a></sup> This ensures that $z$ is kept as-is when its variance consists mostly of useful information and it is nearly zero when its variance consists mostly of noise. All this is done 4 times to make a $channel$ dimension of 4. Then we apply a projection (with different weights per tensor in the multitensor, ie. per-tensor projections) mapping the $channel$ dimension up to the dimension of the residual stream.</p>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Multitensor_Sharing.png" width="50%"></p>

  <h4 id="multitensor-communication-layer">Multitensor Communication Layer</h4>

  <p>This layer allows different tensors in a multitensor to interact with each other.</p>

  <p>First, the input from the residual stream passes through per-tensor projections to a fixed size (8 for downwards communication and 16 for upwards communication). Then a message is sent to every other tensor that has at least the same dimensions for upwards communication, or at most the same dimensions for downwards communication. This message is created by either taking means along dimensions to remove them, or unsqueezing+broadcasting dimensions to add them. All the messages received by every tensor are summed together and normalization is applied. This result gets up-projected back and then added to the residual stream.</p>

  <h4 id="softmax-layer">Softmax Layer</h4>

  <p>This layer allows the network to work with internal one-hot representations, by giving it the tools to denoise and sharpen noisy one-hot vectors. For every tensor in the input multitensor, this layer lists out all the possible subsets of dimensions of the tensor to take a softmax over<sup id="fnref:15" role="doc-noteref"><a href="#fn:15" rel="footnote">8</a></sup>, takes the softmax over these subsets of dimensions, and concatenates all the softmaxxed results together in the $channel$ dimension. The output dimension varies across different tensors in the multitensor, depending on their tensor rank. A pre-norm is applied, and per-tensor projections map to and from the residual stream. The layer has input $channel$ dimension of 2.</p>

  <h4 id="directional-cummaxshift-layer">Directional Cummax/Shift Layer</h4>

  <p>The directional cummax and shift layers allow the network to perform the non-equivariant cummax and shift operations in an equivariant way, namely by applying the operations once per direction, and only letting the output be influenced by the results once the directions are aggregated back together (by the <a href="#multitensor-communication-layer">multitensor communication layer</a>). These layers are the sole reason we included the $direction$ dimension when defining a multitensor: to store the results of directional layers and operate on each individually. Of course, this means when we apply a spatial equivariance transformation, we must also permute the indices of the $direction$ dimension accordingly, which can get complicated sometimes.</p>

  <p>The directional cummax layer takes the eight indices of the $direction$ dimension, treats each slice as corresponding to one direction (4 cardinal, 4 diagonal), performs a cumulative max in the respective direction for each slice, does it in the opposite direction for half the channels, and stacks the slices back together in the $direction$ dimension. The slices are rescaled to have min $-1$ and max $1$ before applying the cumulative max.</p>

  <p>The directional shift layer does the same thing, but for shifting the grid by one pixel instead of applying the cumulative max, and without the rescaling.</p>

  <p>Some details:</p>
  <ul>
    <li>Per-tensor projections map to and from the residual stream, with pre-norm.</li>
    <li>Input $channel$ dimension is 4</li>
    <li>These layers are only applied to the $[example, color, direction, height, width, channel]$ and $[example, direction, height, width, channel]$ tensors in the input multitensor.</li>
  </ul>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Directional_Shift_Cummax.png" alt="image"></p>

  <h4 id="directional-communication-layer">Directional Communication Layer</h4>

  <p>By default, the network is equivariant to permutations of the eight directions, but we only want symmetry up to rotations and flips. So, this layer provides a way to send information between two slices in the $direction$ dimension, depending on the angular difference in the two directions. This layer defines a separate linear map to be used for each of the 64 possible combinations of angles, but the weights of the linear maps are minimally tied such that the directional communication layer is equivariant to reflections and rotations. This gets complicated really fast, since the $direction$ dimension’s indices also permute when equivariance transformations are applied. Every direction slice in a tensor accumulates it’s 8 messages, and adds the results together.<sup id="fnref:16" role="doc-noteref"><a href="#fn:16" rel="footnote">9</a></sup></p>

  <p>For this layer, there are per-tensor projections to and from the residual stream with pre-norm. The input $channel$ dimension is 2.</p>

  <h4 id="nonlinear-layer">Nonlinear Layer</h4>

  <p>We use a SiLU nonlinearity with $channel$ dimension 16, surrounded by per-tensor projections with pre-norm.</p>

  <h4 id="normalization-layer">Normalization Layer</h4>

  <p>We normalize all the tensors in the multitensor, using means and variances computed across all dimensions except the $channel$ dimension. Normalization as used within other layers also generally operates this way.</p>

  <h4 id="linear-heads">Linear Heads</h4>

  <p>We must take the final multitensor, and convert it to the format of an ARC-AGI puzzle. More specifically, we must convert the multitensor into a distribution over ARC-AGI puzzles, so that we can compute the log-likelihood of the observed grids in the puzzle.</p>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Linear_heads.png" width="50%"></p>

  <p>The colors of every pixel for every example for both input and output, have logits defined by the $[examples, colors, height, width, channel]$ tensor, with the $channel$ dimension linearly mapped down to a size of 2, representing the input and output grids.<sup id="fnref:17" role="doc-noteref"><a href="#fn:17" rel="footnote">10</a></sup> The log-likelihood is given by the crossentropy, with sum reduction across all the grids.</p>

  <p>For grids of non-constant shape, the $[examples, width, channel]$ and $[examples, height, channel]$ tensors are used to create distributions over possible contiguous rectangular slices of each grid of colors. Again, the $channel$ dimension is mapped down to a size of 2 for input and output grids. For every grid, we have a vector of size $[width]$ and a vector of size $[height]$. The log likelihood of every slice of the vector is taken to be the sum of the values within the slice, minus the values outside the slice. The log likelihoods for all the possible slices are then normalized to have total probability one, and the colors for every slice are given by the color logits defined in the previous paragraph.</p>

  <p>With the puzzle distribution now defined, we can now evaluate the log-likelihood of the observed target puzzle, to use as the reconstruction error.<sup id="fnref:18" role="doc-noteref"><a href="#fn:18" rel="footnote">11</a></sup></p>

  <h3 id="other-architectural-details">Other Architectural Details</h3>

  <h4 id="rules-for-legal-multitensors">Rules for legal multitensors</h4>

  <ol>
    <li>At least one non-$example$ dimension must be included. Examples are not special for any reason not having to do with colors, directions, rows, and columns.</li>
    <li>If the $width$ or $height$ dimension is included, the $example$ dimension should also be included. Positions are intrinsic to grids, which are indexed by the $example$ dimension. Without a grid it doesn’t make as much sense to talk about positions.</li>
  </ol>

  <h4 id="weight-tying-for-reflectionrotation-symmetry">Weight Tying for Reflection/Rotation Symmetry</h4>

  <p>When applying a different linear layer to every tensor in a multitensor, we have a linear layer for tensors having a $width$ but not $height$ dimension, and another linear layer for tensors having a $height$ but not $width$ dimension. Whenever this is the case, we tie the weights together in order to preserve the whole network’s equivariance to diagonal reflections and 90 degree rotations, which swap the $width$ and $height$ dimensions.</p>

  <p>The softmax layer is not completely symmetrized because different indices of the output correspond to different combinations of dimension to softmax over. Tying the weights properly would be a bit complicated and time consuming for the performance improvement we expect, so we did not do this.</p>

  <h4 id="training">Training</h4>

  <p>We train for 2000 iterations using Adam, with learning rate 0.01, $\beta_1$ of 0.5, and $\beta_2$ of 0.9.</p>

  <h3 id="preprocessing">Preprocessing</h3>

  <h4 id="output-shape-determination">Output Shape Determination</h4>

  <p>The raw data consists of grids of various shapes, while the neural network operates on grids of constant shape. Most of the preprocessing that we do is aimed towards this shape inconsistency problem.</p>

  <p>Before doing any training, we determine whether the given ARC-AGI puzzle follows three possible shape consistency rules:</p>
  <ol>
    <li>The outputs in a given ARC-AGI puzzle are always the same shape as corresponding inputs.</li>
    <li>All the inputs in the given ARC-AGI puzzle are the same shape.</li>
    <li>All the outputs in the given ARC-AGI puzzle are the same shape.</li>
  </ol>

  <p>Based on rules 1 and 3, we try to predict the shape of held-out outputs, prioritizing rule 1 over rule 3. If either rule holds, we force the postprocessing step to only consider the predicted shape by overwriting the masks produced by the <a href="#linear-heads">linear heads</a>. If neither rule holds, we make a temporary prediction of the largest width and height out of the grids in the given ARC-AGI puzzle, and we allow the masks to predict shapes that are smaller than that.</p>

  <p>The largest width and height that is given or predicted, are used as the size of the <a href="#multitensors">multitensor</a>’s $width$ and $height$ dimensions.</p>

  <p>The predicted shapes are also used as masks when performing the <a href="#multitensor-communication-layer">multitensor communication</a>, <a href="#directional-communication-layer">directional communication</a> and <a href="#directional-cummaxshift-layer">directional cummax/shift</a> layers<sup id="fnref:19" role="doc-noteref"><a href="#fn:19" rel="footnote">12</a></sup>. We did not apply masks for the other layers because of time constraints and because we do not believe it will provide for much of a performance improvement.</p>

  <h4 id="number-of-colors">Number of Colors</h4>

  <p>We notice that in almost all ARC-AGI puzzles, colors that are not present in the puzzle are not present in the true answers. Hence, any colors that do not appear in the puzzle are not given an index in the $color$ dimension of the <a href="#multitensors">multitensor</a>.</p>

  <p>In addition, black is treated as a special color that is never included in the multitensor, since it normally represents the background in many puzzles. When performing color classification, a tensor of zeros is appended to the $color$ dimension after applying the <a href="#linear-heads">linear head</a>, to represent logits for the black color.</p>

  <h3 id="postprocessing">Postprocessing</h3>

  <p>Postprocessing primarily deals with denoising the answers sampled from the network. There are also <a href="#linear-heads">some operations</a> performed to convert the constant-shape grids outputted by the network to the variable shape grids present in some puzzles.</p>

  <p>Generally, when we sample answers from the network by taking the logits of the $[examples, colors, height, width, channels]$ tensor and argmaxxing over the $color$ dimension, we find that the grids are noisy and will often have the wrong colors for several random pixels. We developed several methods for removing this noise:</p>
  <ol>
    <li>Find the most commonly sampled answer.</li>
    <li>Construct an exponential moving average of the output color logits before taking the softmax to produce probabilities. Also construct an exponential moving average of the masks.</li>
    <li>Construct an exponential moving average of the output color probabilities after taking the softmax. Also construct an exponential moving average of the masks.</li>
  </ol>

  <p>When applying these techniques, we always take the slice of highest probability given the mask, and then we take the colors of highest probability afterwards.</p>

  <p>We explored several different rules for when to select which method, and arrived at a combination of 1 and 2 with a few modifications:</p>
  <ul>
    <li>At every iteration, count up the sampled answer, as well as the exponential moving average answer (decay $=0.97$).</li>
    <li>If before 150 iterations of training, then downweight the answer by a factor of $e^{-10}$. (Effectively, don’t count the answer.)</li>
    <li>If the answer is from the exponential moving average as opposed to the sample, then downweight the answer by a factor of $e^{-4}$.</li>
    <li>Downweight the answer by a factor of $e^{-10*uncertainty}$, where $uncertainty$ is the average (across pixels) negative log probability assigned to the top color of every pixel.</li>
  </ul>

  <h3 id="what-happens-to-the-representations-during-learning">What Happens to the Representations during Learning</h3>

  <p>During training, the gradient descent tries to find representations of the puzzle that require less and less information to encode. This information is measured by the KL term for $z$, plus the a heavily penalized reconstruction error.</p>

  <p>Due to the 10x penalization on reconstruction error, and the initial high capacity for $z$, the $z$ distribution (which we call the “posterior”) quickly learns the information that is required to perfectly reconstruct the given input/output pairs in the puzzle, within the first 20 or so steps. The remainder of the training steps are about compressing $z$ information under the constraint of perfect reconstruction, by tuning the representations to be more concise.</p>

  <p>Our mental model of how gradient descent compresses the $z$ information consists of several steps which we list below:</p>
  <ol>
    <li>Suppose the posterior $p$ originally codes for some number $n$ pieces of information $z_1, \dots, z_n$ using thin Gaussians.</li>
    <li>The posterior widens and becomes more noisy to try to get closer to the wide Gaussian “prior” $q=N(0,1)$, but since all $n$ pieces of information are needed to ensure good reconstruction, the noise is limited by the reconstruction loss incurred.</li>
    <li>The ever-widening posteriors push the neurons to become more and more resilient to noise, until some limit is reached.</li>
    <li>Learning remains stagnant for a while, as a stalemate between compression and reconstruction.</li>
    <li>If it turns out that $z_1$ is not reconstructible using $z_2, \dots, z_n$, then stop. Else, proceed to step 6.</li>
    <li>The neurons, pushed by the widening posterior of $z_1$, figure out a procedure to denoise $z_1$ using information from $z_2, \dots, z_n$, in the event that the noise sample for $z_1$ is too extreme.</li>
    <li>The posterior for the last piece keeps pushing wider, producing more extreme values for $z_1$, and the denoising procedure is improved, until the $z_1$ representation consists completely of noise, and its usage in the network is replaced by the output of the denoising procedure.</li>
    <li>The posterior for $z_1$ is now identical to the prior, so nothing is coded in $z_1$ and it no longer contributes to the KL loss.</li>
    <li>The posterior now codes for $n-1$ pieces of information $z_2, \dots, z_n$, and compression has occurred.</li>
  </ol>

  <p>These steps happen repeatedly for different unnecessarily coded pieces of information, until there are no more. More than one piece of information can be compressed away at once, and there is no need for the steps to proceed serially. The process stops when all information coded by the posterior is unique, and no piece is reconstructable using the others.</p>

  

  <h2 id="additional-case-studies">Additional Case Studies</h2>

  <p>Below, we show two additional puzzles and a dissection of CompressARC’s solution to them.</p>

  <h3 id="case-study-bounding-box">Case Study: Bounding Box</h3>

  <p>Puzzle 6d75e8bb is part of the training split.</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_problem.png" width="20%">
</p>

  <h4 id="watching-the-network-learn-bounding-box">Watching the Network Learn: Bounding Box</h4>

  <h5 id="human-solution-1">Human Solution:</h5>
  <p>We first realize that the input is red and black, and the output is also red and black, but some of the black pixels are replaced by light blue pixels. We see that the red shape remains unaffected. We notice that the light blue box surrounds the red shape, and finally that it is the smallest possible surrounding box that contains the red shape. At this point, we copy the input over to the answer grid, then we figure out the horizontal and vertical extent of the red shape, and color all of the non-red pixels within that extent as light blue.</p>

  <h5 id="compressarc-solution-1">CompressARC Solution:</h5>
  <table>
  <tbody><tr>
  <td>
  <strong> 50 steps of learning:</strong>
  <p>
  The average of sampled outputs shows that light blue pixels in the input are generally preserved in the output. However, black pixels in the input are haphazardly and randomly colored light blue and red. CompressARC does not seem to know that the colored input/output pixels lie within some kind of bounding box, or that the bounding box is the same for the input and output grids.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_at_50_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 100 steps of learning:</strong>
  <p>
  The average of sampled outputs shows red pixels confined to an imaginary rectangle surrounding the light blue pixels. CompressARC seems to have perceived that other examples use a common bounding box for the input and output pixels, but is not completely sure about where the boundary lies and what colors go inside the box in the output. Nevertheless, guess 2 (the second most frequently sampled output) shows that the correct answer is already being sampled quite often now.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_at_100_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 150 steps of learning:</strong>
  <p>
  The average of sampled outputs shows almost all of the pixels in the imaginary bounding box to be colored red. CompressARC has figured out the answer, and further training only refines the answer.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_at_150_steps.png"></td>
  </tr>
</tbody></table>

  <h4 id="solution-analysis-bounding-box">Solution Analysis: Bounding Box</h4>

  <p>Below is a plot of the amount of contained information in every tensor composing the latent $z$:</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_KL_components.png">
</p>

  <p>All the tensors in $z$ fall to zero information content during training, except for three tensors. From 600-1000 steps, we see the $(example, height, width, channel)$ tensor suffer a massive drop in information content, with no change in the outputted answer. We believe it was being used to identify the light blue pixels in the input, but this information then got memorized by the nonlinear portions of the network, using the $(example, height, channel)$ and $(example, width, channel)$ as positional encodings.</p>

  <p>We can look at the average output of the <a href="#decoding-layer">decoding layer</a> for these tensors to see what information is stored there.</p>

  <table>
  <tbody><tr>
  <td>
  <strong> (Examples, height, channel) tensor:</strong>
  <p>
  The first principal component is 771 times stronger than the second principal component. <strong>A brighter pixel indicates a row with more light blue pixels.</strong> It is unclear how CompressARC knows where the borders of the bounding box are.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_example_height_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Examples, width, channel) tensor:</strong>
  <p>
  The first principal component is 550 times stronger than the second principal component. <strong>A darker pixel indicates a column with more light blue pixels.</strong> It is unclear how CompressARC knows where the borders of the bounding box are.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_example_width_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Color, channel) tensor:</strong>
  <p>
  This tensor serves to distinguish the roles of the two colors apart.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_color_component_0.png" width="50%"></td>
  </tr>
</tbody></table>

  

  <h3 id="case-study-center-cross">Case Study: Center Cross</h3>

  <table>
    <thead>
      <tr>
        <th>Puzzle 41e4d17e from training split</th>
        <th>Our Network’s Answer</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_problem.png" alt="image"></td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_at_1500_steps.png" alt="image"></td>
      </tr>
    </tbody>
  </table>

  <h5 id="human-solution-2">Human Solution:</h5>
  <p>We first notice that the input consists of blue “bubble” shapes (really they are just squares, but the fact that they’re blue reminds us of bubbles) on a light blue background and the output has the same. But in the output, there are now magenta rays emanating from the center of each bubble. We copy the input over to the answer grid, and then draw magenta rays starting from the center of each bubble out to the edge in every cardinal direction. At this point, we submit our answer and find that it is wrong, and we notice that in the given demonstrations, the blue bubble color is drawn on top of the magenta rays, and we have drawn the rays on top of the bubbles instead. So, we pick up the blue color and correct each point where a ray pierces a bubble, back to blue.</p>

  <h5 id="compressarc-solution-2">CompressARC Solution:</h5>
  <p>We don’t show CompressARC’s solution evolving over time because we think it is uninteresting; instead will describe. We don’t see much change in CompressARC’s answer over time during learning. It starts by copying over the input grid, and at some point, magenta rows and columns start to appear, and they slowly settle on the correct positions. At no point does CompressARC mistakenly draw the rays on top of the bubbles; it has always had the order correct.</p>

  <h4 id="solution-analysis-center-cross">Solution Analysis: Center Cross</h4>

  <p>Another plot of the amount of information in every tensor in $z$:</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_KL_components.png">
</p>

  <p>The only surviving tensors are the $(color, channel)$ and $(example, height, width, channel)$ tensors.</p>

  <table>
  <tbody><tr>
  <td>
  <strong> (Examples, height, width, channel) tensor:</strong>
  <p>
  The top principal component is 2496 times stronger than the second principal component. <strong>The (examples, height, width, channel) tensor codes for the centers of the bubbles.</strong> In the KL contribution plot, we can see that the information content of this tensor is decreasing over time. Likely, CompressARC is in the process of eliminating the plus shaped representation, and replacing it with a pixel instead, which takes fewer bits.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_example_height_width_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Color, channel) tensor:</strong>
  <p>
  The $(color, channel)$ tensor just serves to distinguish the individual roles of the colors in the puzzle.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_color_component_0.png"><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_color_component_1.png"></td>
  </tr>
</tbody></table>

  

  <h2 id="list-of-mentioned-arc-agi-puzzles">List of Mentioned ARC-AGI Puzzles</h2>

  <p>All the puzzles we mentioned are part of the training split.</p>

  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Puzzle</th>
        <th>Name</th>
        <th>Puzzle</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>025d127b</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/025d127b_problem.png" alt="image"></td>
        <td>0a938d79</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0a938d79_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>0ca9ddb6</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0ca9ddb6_problem.png" alt="image"></td>
        <td>0d3d703e</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0d3d703e_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>0dfd9992</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0dfd9992_problem.png" alt="image"></td>
        <td>0e206a2e</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0e206a2e_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>1c786137</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/1c786137_problem.png" alt="image"></td>
        <td>1f876c06</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/1f876c06_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>28e73c20</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/28e73c20_problem.png" alt="image"></td>
        <td>272f95fa</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>2bcee788</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/2bcee788_problem.png" alt="image"></td>
        <td>2dd70a9a</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/2dd70a9a_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>3bd67248</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/3bd67248_problem.png" alt="image"></td>
        <td>41e4d17e</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>42a50994</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/42a50994_problem.png" alt="image"></td>
        <td>5ad4f10b</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/5ad4f10b_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>6d75e8bb</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_problem.png" alt="image"></td>
        <td>7b6016b9</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/7b6016b9_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>ce9e57f2</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/ce9e57f2_problem.png" alt="image"></td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
      </tr>
    </tbody>
  </table>

  

  <h2 id="code">Code</h2>

  <p>Code for this project is available <a href="https://github.com/iliao2345/CompressARC">here</a>.</p>

  <p>If you’d like to cite this blog post, use the following entry:</p>
  <div><pre><code>@online{liao2025arcagiwithoutpretraining,
	author = {Isaac Liao and Albert Gu},
	title = {ARC-AGI Without Pretraining},
	year = {2025},
	url = {https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html},
}
</code></pre></div>

  

  

  

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why FastDoom Is Fast (403 pts)]]></title>
            <link>https://fabiensanglard.net/fastdoom/index.html</link>
            <guid>43258709</guid>
            <pubDate>Tue, 04 Mar 2025 19:05:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fabiensanglard.net/fastdoom/index.html">https://fabiensanglard.net/fastdoom/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=43258709">Hacker News</a></p>
<div id="readability-page-1" class="page"><br><center>
    

</center><p>
Mar 04, 2025</p>
<p>Why fastDOOM is fast</p><hr>
 



<p>During the winter of 2024, I restored an IBM PS/1 486-DX2 66Mhz, "Mini-Tower", model 2168. It was the computer I always wanted as a teenager but could never afford. Words cannot do justice
to the joy I felt while working on this machine.</p>

<img loading="lazy" src="https://fabiensanglard.net/fastdoom/2168.png" width="1000" height="598">
 <p>As soon as I got something able to boot, I benchmarked the one software I wanted to run.
</p>


<pre>C:\DOOM&gt;doom.exe -timedemo demo1
timed 1710 gametics in 2783 realtics  
</pre>

<p>Doom doesn't give the fps right away. You have to do a bit of math to get the framerate. In this instance, that's 1710/2783*35 = <span><b>21.5</b></span> fps. An honorable performance for the best machine money could (<a href="https://fabiensanglard.net/fastdoom/pentium_ad_pcmag_nov_1993.jpg">reasonably</a>) buy in Dec 1993 (<a href="https://fabiensanglard.net/fastdoom/ASTRA01.png">specs</a>, <a href="https://fabiensanglard.net/fastdoom/ASTRA02.png">chipset</a>, <a href="https://fabiensanglard.net/fastdoom/ASTRA06.png">video</a>, <a href="https://fabiensanglard.net/fastdoom/ASTRA03.png">disk1</a>, <a href="https://fabiensanglard.net/fastdoom/ASTRA04.png">disk2</a>, <a href="https://fabiensanglard.net/fastdoom/speedsys.png">speedsys</a>).</p>

<p>I was resigned to playing under Ibuprofen until I heard of fastDOOM. I am usually not a fan of ports
  because they tend to add features without cohesion (except for the dreamy Chocolate DOOM) but I gave it a try out of curiosity.
</p>

<pre>C:\DOOM&gt;fdoom.exe -timedemo demo1
Timed 1710 gametics in 1988 realtics. FPS: <span><b>30.1</b></span>
</pre>

<p>30% faster without cutting any features<a name="back_1" href="#footnote_1"><sup>[1]</sup></a>! On a demanding map like doom2's demo1, the gain is even higher, from <span><b>16.8</b></span> fps to <span><b>24.9</b></span>&nbsp;fps. That is 48% faster!</p>

<p>I did not suspect that DOOM had left that much on the table. Obviously shipping within one year left little time to optimize. I had to understand how this magic trick happened.</p>


<p>A byte of history </p><hr><p>Before digging into fastDOOM, let's understand where the code comes from. DOOM was originally developed on NeXT Workstation. The game was structured to be easy to port with most of the code in a core surrounded by small sub-systems performing I/O.</p>


<img loading="lazy" src="https://fabiensanglard.net/fastdoom/doom_arch.svg" width="153.81099mm" height="159.21635mm">
 <center><small>Source: Game Engine Black Book: DOOM</small></center>

<p>During development, DOS I/Os were written by id Software. This became the commercial release of DOOM.
But that version could not be open sourced in 1997 because it relied on a proprietary sound library called DMX.</p>

<p>What ended up being open sourced was the linux version, cleaned up by Bernd Kreimeier when he was working on a book project to explain the engine.</p>

<p>A DOS version of DOOM was reconstructed by using linux's core, Heretic I/O, and APODMX (Apogee Sound wrapper) to emulate DMX. 
  Because Heretic used video mode 13h while DOOM used video mode Y, the graphic I/O (<code>i_ibm.c</code>)
was reverse-engineered from <code>DOOM.EXE</code> disassembly. That is how the community got PCDOOM v2<a name="back_2" href="#footnote_2"><sup>[2]</sup></a>.</p>

<p>fastDOOM starting point was PCDOOM v2.</p>

<pre>                    ┌───────────────┐                              
                    │ NeXTStep DOOM │                              
                    └─────┬────┬────┘                              
                          │    │                                 
                          │    │                                 
                          │    │                                 
          ┌────────────┐  │    │  ┌──────┐      ┌─────────┐      
          │ Linux DOOM │◄─┘    └─►│ DOOM ├─────►│ Heretic │      
          └──────┬─────┘          └──────┘      └────┬────┘      
                 │                    ⁞              │           
                 │                    ▼              │           
                 │              ┌──────────┐         │           
                 └─────────────►│ PCDOOMv2 │◄────────┘           
                                └─────┬────┘                     
                                      ▼                          
                                ┌──────────┐                     
                                │ fastDOOM │                     
fastDoom genealogy              └──────────┘
──────────────────</pre>


<p>The big performance picture</p><hr><p>Victor "Viti95" Nieto, wrote release notes to describe the performance improvement of each version but he seemed more interested in making <code>FDOOM.EXE</code> awesome than detailing how he did it.</p>


<p>To get the big picture of performance evolution over time, I downloaded all 52 releases of fastDOOM, PCDOOMv2, and the original <code>DOOM.EXE</code>, wrote a go program to generate a <code>RUN.BAT</code> running <code>-timedemo demo1</code> on all of them, and mounted it all with mTCP's <code>NETDRIVE</code>.</p>


<p>I chose to timedemo <code>DOOM.WAD</code> with sound on and screen size = 10 (fullscreen with status bar). After several hours of shotguns and imps agony, I had run the whole suite five times and graphed the average fps with <code>chart.js</code>.</p>


<canvas id="bpChart"></canvas>


<p>The first thing this graph allows to rule out is that fastDOOM improvements were mostly due to using a modern compiler. <code>PCDOOMv2</code> is built with OpenWatcom 2 but only gets a marginal improvement over <code>DOOM.EXE</code>.</p> 





<p>git archeology</p><hr><p>On top of releasing often, Viti95 displayed outstanding git discipline where one commit does one thing and each release was tagged. fastDOOM git history is made of 3,042 commits which allows to benchmark each feature.</p>

<p>I wrote another go program to build every single commit. I will pass on the gory details of handling the many build system changes (especially from DOS to Linux). After an hour I had the
  most ugly program I ever wrote and 3,042 <code>DOOM.EXE</code>. I was pleased to see the build was almost never broken.</p>

<canvas id="sizesChart"></canvas>



<p>Graphing the files size shows that the early effort was to be lean by cleaning and deleting code. There are major drops with <a href="https://github.com/viti95/FastDoom/commit/bf0e983ed00f038b65a34f10fa626abb99c87fe6">bf0e983</a> 
  (build 239 where sound recording was removed), <a href="https://github.com/viti95/FastDoom/commit/5f3832310b32c32895688b3112301f98e77119b8" target="_blank">5f38323</a>
  (build 0340 where error code strings were deleted), 
  and <a href="https://github.com/viti95/FastDoom/commit/8b9cac591945fa93af84c0e85845b6a55bc76fe3" target="_blank">8b9cac5</a>
   (build 1105 where TASM was replaced with NASM).
 </p>


<p>Going deep</p><hr><p>Timedemoing all builds would have taken a very long time (3042x1.5/60/24 * 3 passes = 9 days) so I focused
on the release where most of the speed was gained. I wrote yet another go program to generate a <code>.BAT</code> file running timedemo for all commits in <code>v0.1</code>, <code>v0.6</code>, <code>v0.8</code>, <code>v0.9.2</code>, and <code>v0.9.7</code>.
I mounted 1.4 GiB of <code>FDOOM.EXE</code> with mTCP and ran it. It took a while because versions with 200+ commit runtime was 8h/pass.</p>



<p>fastDOOM v0.1</p><hr><p>This release featured <a href="">220 commits</a>.</p>

<pre>$ git log --reverse --oneline "0.1" | wc -l
     220
</pre>



<canvas id="v0Chart"></canvas>
<p>Chart is click-able and mouseover-able</p>



<!-- <p>The very first improvement (in a long series) is build 34 (<a href="https://github.com/viti95/FastDoom/commit/d9940eddddd9dd9debe5809adb1b5bb7e93f4dc6">93f4dc6</a>) 
to remove unnecessary vsync.</p>
 -->

<p>The MPV patch of v0.1 is without a doubt build 36 (<a href="https://github.com/viti95/FastDoom/commit/e16bab87df86baaf2bdec5e17e4844fac9b110be">e16bab8</a>). The "Cripy optimization" turns status bar percentage rendering into a noop if they have not changed. This prevents rendering to a scrap buffer and blitting to the screen for a total
of 2 fps boost. At first I could not believe it. I assume my toolchain had a bug. But cherry-picking this patch on PCDOOMv2 confirmed the tremendous speed gain.</p>

<p>Next is build 167 (<a href="https://github.com/viti95/FastDoom/commit/a9359d599e339d7e8bcd730ea2bed7cc22fa2947">a9359d5</a>)
which inlines FixedDiv via macro.</p>

<div><p>Near the end, we see a series of optimizations granting 0.5 fps.</p><!-- Build 128 (<a href="https://github.com/viti95/FastDoom/commit/cd47382c800e3614a0b454235f0757cb51d0313c">1d0313c</a>) Remove visplane limit!<br/> --><p>
  Build 207 (<a href="https://github.com/viti95/FastDoom/commit/9bd3f207df1837f2b9cb87ca2e67fdc01fcc1a95">9bd3f20</a>): A PSX Doom optimization which optimizes the way the BSP is traversed.<br>

Build 212 (<a href="https://github.com/viti95/FastDoom/commit/dc0f48e22d2804bf025d1b8efe4436311ad1d56d">dc0f48e</a>) "Inlined R_MakeSpans" which renders horizontal surfaces.<br>
</p></div>

<p>Overall this version saw a lot of code being deleted (50% of commits were deletions) which probably helped to cuddle the 486 cachelines of my machine.</p>
<pre>git log --reverse --oneline "0.1" | grep -i -E "remove|delete" | wc -l
     100
</pre>

<p>Somehow, <a href="https://github.com/viti95/FastDoom/commit/609c42df">one of my patches</a> made it to fastDOOM. Probably when I was
writing the Black Book? I have zero recollection of writing this!</p>

<p>fastDOOM v0.6</p><hr><p>This release featured 33 commits.</p>

<pre>$ git log --reverse --oneline "0.5"^.."0.6" | wc -l 
      33
</pre>




<canvas id="v6Chart"></canvas>
<p>Chart is click-able and mouseover-able</p>


<div><p>Among many small optimizations (hello GbaDOOM <a href="https://github.com/viti95/FastDoom/commit/e745a5dc62c21de8926e5845b41fea89cd7f03ad">341</a>)) are MVP ones.</p><p>
  
Build 342 (<a href="https://github.com/viti95/FastDoom/commit/b6aea6ab6c966d1d78edb5da915871e6a22819fd">22819fd</a>) Skips rendering unneeded visplanes.<br>
Build 359 (<a href="https://github.com/viti95/FastDoom/commit/3329d704e081d8a18e7fd3a9c4a239d8840e0d4b">40e0d4b</a>) Removes a level of player pointer indirection.<br>
Build 360 (<a href="https://github.com/viti95/FastDoom/commit/8e5e6355d232d3f4dfd9203dc6aa963adccd296f">ccd296f</a>) Double down on indirection removal.<br>
Build 369 (<a href="https://github.com/viti95/FastDoom/commit/dff38f9fa903f15b24fb13da712feeb09f29e665">f29e665</a>) Inlines the screenspace line splitter.</p></div>


<p>fastDOOM v0.8</p><hr><p>This release featured <a href="">282 commits</a>.</p>

<pre>$ git log --reverse --oneline "0.7"^.."0.8" | wc -l
     282
</pre>

<p>The sound system was a bit unstable so I had to timedemo without sound and then normalize the fps. 
  Moreover the focus of v0.8 seems to have been text-mode renderer so two regressions happened at Build 670 
  (<a href="https://github.com/viti95/FastDoom/commit/50848560577f8d008b5c7f5bb69d7595fa92c67f">a92c67f</a>)
   and Build 730 (<a href="https://github.com/viti95/FastDoom/commit/17095d801f6603f9ab8091fa60453d5b6c3f5f50">c3f5f50</a>) where the Cripy optimization went away.</p>

<canvas id="v8Chart"></canvas>
<p>Chart is click-able and mouseover-able</p>



<p><u><b>MVPs:</b></u></p>
<p>
Build 792 (<a href="https://github.com/viti95/FastDoom/commit/b253754a80045e9809ccfb7bf0f0cf680f279b7d">f279b7d</a>): One executable per renderer (<code>FDOOM.EXE</code>, <code>FDOOM13H.EXE</code>, and so on).<br> 
Build 793 (<a href="https://github.com/viti95/FastDoom/commit/5ddc4e1c967a737c271327cbd4290137a1874ee8">1874ee8</a>): Disable debugging for compiler.<br>
Build 796 (<a href="https://github.com/viti95/FastDoom/commit/85a895802e750360e4251e7fd836906db6aae724">6aae724</a>): Bring back Crispy opt.<br> 
Build 794 (<a href="https://github.com/viti95/FastDoom/commit/eefc599d6c740e890fffa96b5d68b03281366ebf">1366ebf</a>): Compile less code whenever possible.<br>
</p>


<p>fastDOOM v0.9.2</p><hr><p>This release featured <a href="">110 commits</a>.</p>
  
<pre>$ git log --reverse --oneline "0.9.1"^.."0.9.2" | wc -l
     110
</pre>

<canvas id="v92Chart"></canvas>
<p>Chart is click-able and mouseover-able</p>


<p><u><b>MVPs:</b></u></p>
<p>
Build 1639 (<a href="https://github.com/viti95/FastDoom/commit/7d992d86be7e3b3492e7c032d76ea90a2ae2a951">ae2a951</a>): Optimize skyflatnum comparison.<br>
Build 1645 (<a href="https://github.com/viti95/FastDoom/commit/9fb58c2ec8072c5de6e5f4bf8094b51b30730cdc">0730cdc</a>): Optimize R_DrawColumn for Mode Y.<br>
Build 1646 (<a href="https://github.com/viti95/FastDoom/commit/c906b0cba8aac6e44698c7a942b77641217c9e83">17c9e83</a>): Cleanup R_DrawSpan code.<br>
</p>




<p>fastDOOM v0.9.7</p><hr><p>This release featured <a href="">293 commits</a>.</p>

<pre>$ git log --reverse --oneline "0.9.6"^.."0.9.7" | wc -l
     294
</pre>

<p>Despite running the benchmark several times, I was unable to reduce the noise of this release.</p>

<canvas id="v97Chart"></canvas>
<p>Chart is click-able and mouseover-able</p>



<p><u><b>MVPs:</b></u></p>
<p>
Build 1941 (<a href="https://github.com/viti95/FastDoom/commit/c0657d1eb1ca2f6caebbda58c5fb20b150688235">0688235</a>): Testing x86 ASM changes.<br>
Build 1943 (<a href="https://github.com/viti95/FastDoom/commit/cd506d432d63d0e0813b97c8d7c915c02f326e73">f326e73</a>): Add CPU selection + CR2 optimization for 386SX.<br>
Build 1944 (<a href="https://github.com/viti95/FastDoom/commit/7682c40f69bd7db825670758b9a0fb881a836abb">a836abb</a>): Add ESP optimization for R_DrawSpan386SX.<br>
Build 2000 (<a href="https://github.com/viti95/FastDoom/commit/38fbcdbd9ccc4c077daa8bd65138f2b403432590">3432590</a>): Add basecode for rendering fuzz columns in ASM.<br>
Build 2031 (<a href="https://github.com/viti95/FastDoom/commit/95232f3d748978a27d142f1cbf7ef40710edab46">0edab46</a>): Remove a CMP comparison each loop (<a href="https://github.com/viti95/FastDoom/issues/143">ken silverman's optimization</a>?).<br>
</p>

<p>Mode 13h vs Mode Y</p><hr><p>fastDOOM explored many ways to make things faster, for a broad range of CPUs (386, 486, Pentium, Cyrix) and video buses (ISA, VLB, PCI). One
optimization that did not work on my machine was to use video mode 13h instead of mode Y.</p>

<p>In mode 13h dispatch of data toward the four VRAM banks of the VGA is done in hardware. To the CPU, the VRAM appears like a single linear 320x200 framebuffer.
The inconvenience is that you can't double buffer in VRAM so you have to do it in RAM which means bytes are written twice. First into the framebuffer in RAM. And then
a second time when sent to VRAM. Also, the engine must block on VSYNC.</p>

<pre>Mode 13h                                                                                               
────────             RAM                   VRAM (VGA card)                 SCREEN              
             ┌───────────────────┐      ┌───────────────────┐       ┌───────────────────┐      
             │ ┌───────────────┐ │      │                   │       │                   │      
             │ │ framebuffer 1 │ │      │                   │       │                   │      
             │ └───────────────┘ │      │                   │       │                   │      
             │ ┌───────────────┐ │      │ ┌───────────────┐ │       │                   │      
    CPU ────►│ │ framebuffer 2 │ ├────► │ │framebuffer(fb)│ ├──────►│                   │      
             │ └───────────────┘ │      │ └───────────────┘ │       │                   │      
             │ ┌───────────────┐ │      │                   │       │                   │      
             │ │ framebuffer 3 │ │      │                   │       │                   │      
             │ └───────────────┘ │      │                   │       │                   │      
             └───────────────────┘      └───────────────────┘       └───────────────────┘      
</pre>


<p>The mode-Y lets programmers access the VGA banks individually. This allows triple-buffering in VRAM. Moreover, it has the advantage of writing bytes once, directly into
 VRAM. The target bank must be manually selected by the developer via very slow <code>OUT</code> instructions
 but that allows to duplicating pixels horizontally (which gives low-detail mode for free) by writing to two VGA banks at once via latches<a name="back_3" href="#footnote_3"><sup>[3]</sup></a>. Another inconvenience is that it makes drawing invisible Specter much slower since it requires reading back from the VRAM.</p>

<pre>Mode Y                                                                                               
───────                                    VRAM (VGA card)                 SCREEN         
                                        ┌───────────────────┐       ┌───────────────────┐ 
                                        │ ┌───────────────┐ │       │                   │ 
                                        │ │fb1 | fb2 | fb3│ │       │                   │ 
                                        │ └───────────────┘ │       │                   │ 
                                        │ ┌───────────────┐ │       │                   │ 
                                        │ │fb1 | fb2 | fb3│ │       │                   │ 
    CPU ──────────────────────────────► │ └───────────────┘ ├──────►│                   │ 
                                        │ ┌───────────────┐ │       │                   │ 
                                        │ │fb1 | fb2 | fb3│ │       │                   │ 
                                        │ └───────────────┘ │       │                   │ 
                                        │ ┌───────────────┐ │       │                   │ 
                                        │ │fb1 | fb2 | fb3│ │       │                   │ 
                                        │ └───────────────┘ │       │                   │ 
                                        └───────────────────┘       └───────────────────┘    
</pre>

<p>For machines with fast CPUs and bus (100+ Mhz/ Pentium and VLB/PCI) where video cards are less likely to handle <code>OUT</code> instruction well, mode 13h is better. 
For "slow CPUs", it is faster to write data once to VRAM via mode Y.</p>

<p>Anyway, Doom used mode Y.</p>



<blockquote>
       DOOM uses 320*200*256 VGA mode, which is slightly different from MCGA
mode (it would NOT run on an MCGA equiped machine). I access the
frame buffer in an interleaved planar mode similar to Michael
Abrash's "Mode X", but still at 200 scan lines instead of 240 (less
pixels == faster update rate).<p>

DOOM cycles between three display pages. If only two were used, it
would have to sync to the VBL to avoid possible display flicker. If
you look carefully at a HOM effect, you should see three distinct
images being cycled between.</p>
    </blockquote>

<p>Another reason John game me for using Mode-Y back in the days is that the tools used by the graphic team (Deluxe Paint) only supported 320x200 (whereas Mode-X is 320x240).</p>



<blockquote><span>
e...@agora.rdrop.com (Ed Hurtley) wrote:
&gt;Check, please... In case you haven't hit ESC ever, the Options menu
&gt;has a Low/High resolution toggle... Low is 320x200, High is
&gt;640x400, with the border graphics (the score bar, menu, etc...) are
&gt;still 320x200... (Just the same graphics files)</span>
<p>
Low detail is 160*200 in the view screen. This is done by setting
two bits in the mapmask register whenever the texturing functions are
writing to video memory, causing two pixels to be set for each byte
written.
</p><p><span>
ui...@freenet.Victoria.BC.CA (Ben Morris) wrote:
<p>
&gt;John,
</p><p>
&gt;You're using a planar graphics system for a bitmapped game that
&gt;updates the entire screen at a respectable framrate on a 486/66?</p></span></p><p>
Its planar, but not bit planar (THAT would stink). Pixels 0,4,8 are
in plane 0, pixels 1,5,9 are in plane 1, etc.
</p><p><span>
&gt;That's pretty incredible. I would have thought all the over-
&gt;head for programming the VGA registers would kill that
&gt;possibility.</span></p><p>
The registers don't need to be programed all that much. The map mask
register only needs to be set once for each vertical column, and four
times for each horizontal row (I step by four pixels in the inner
loop to stay on the same plane, then increment the start pixel and
move to the next plane).
</p><p>
It is still a lot of grief, and it polutes the program quite a bit,
but texture mapping directly to the video memory gives you a fair
amount of extra speed (10% - 15%) on most video cards because the
video writes are interleaved with main memory accesses and texture
calculations, giving the write time to complete without stalling.
</p><p>
Going to that trouble also gets a perfect page flip, rather than the
tearing you get with main memory buffering.</p>
    </blockquote>



  <p>Heretic was released in 1994. Hardware had evolved to make mode 13h<a name="back_6" href="#footnote_6"><sup>[6]</sup></a><a name="back_7" href="#footnote_7"><sup>[7]</sup></a>  more appealing so Raven modified the DOOM engine to this effect. 
  PCDoom v2 used Heretic I/O but re-implemented the video I/O with mode Y. 
  Finally fastDOOM gives users the choice by providing several executable <code>FDOOM.EXE</code>, <code>FDOOM13H.EXE</code>, and <code>FDOOMVBD.EXE</code>.
  </p>

  <blockquote>
The DOOM press release beta (October '93) used Mode 13h, so I assume they switched to Mode Y to improve performance on slower machines (low-detail). I wonder why they didn’t also implement the so-called "potato mode", which writes four pixels with a single 8-bit write to VRAM.
<p>
In FastDoom, I reintroduced Mode 13h because Heretic/Hexen had better-optimized ASM rendering code for this mode. Later, I was able to partially port this approach to column rendering in Mode Y, which resulted in a 5% to 7% performance improvement.
</p><p>
Based on my testing, the best mode for 486 CPUs is the VESA direct mode (FDOOMVBD.EXE for 320x200). This mode combines the advantages of Mode Y with the optimized rendering code from Heretic while avoiding any OUT instructions—except for one to switch buffers, which executes only once per rendered frame. The only downside is that it requires a VLB or PCI graphics card with LFB enabled and has slower performance in low-detail and potato-detail modes.</p><p>- Conversation with Viti95</p>
    </blockquote>

<p>Viti95 elaborated further on fastDOOM mode 13h during proof-reading.</p>
  <blockquote>
In FastDoom, Mode 13h uses a single framebuffer in RAM, which is copied to VRAM after the entire scene is rendered. Vsync is not enforced, which may result in flickering. There are two methods for copying the backbuffer to VRAM, optimized for different bus speeds. For slow buses (8-bit ISA), a differential copy method is used, transferring only modified pixels.
<p>
This approach involves many branches but is faster overall because branching is less expensive than excessive bus transfers. For faster buses (16-bit ISA, VLB, PCI, etc.), a full backbuffer copy is performed using REP MOVS instructions, which is efficient when the bus bandwidth is sufficient.
</p><p>- Conversation with Viti95</p>
    </blockquote>

<p>More optimization which did not work</p><hr><p>Another venue I appreciated seeing explored is OpenWatcom's processor-specific flags (<code>4r</code>/<code>4s</code> vs <code>3r</code>/<code>3s</code>)<a name="back_8" href="#footnote_8"><sup>[8]</sup></a>. Both wcc386's 386 and 486 flags were attempted but ultimately discontinued
  because the 386 version always seemed faster.</p>


<blockquote>
One of my goals for FastDoom is to switch the compiler from OpenWatcom v2 to DJGPP (GCC), which has been shown to produce faster code with the same source. Alternatively, it would be great if someone could improve OpenWatcom v2 to close the performance gap.
 <p>- Conversation with Viti95</p>
    </blockquote>


<p>Overall impression</p><hr><div><p>What splendid work by Victor Nieto!
 If software can die from a thousand cuts, Viti95 made fastDOOM awesome with three thousand optimizations! 
  Not only he leveraged existing improvements (crispy, psx, gba, Lee Killough), he also came up with many news one
and generated so much hype that even Ken Silverman (author of Duke3D build engine) came <a href="https://github.com/viti95/FastDoom/issues/143">to participate</a><a name="back_9" href="#footnote_9"><sup>[9]</sup></a>.
</p><p> I tip my beret to you Victor!</p></div><p>References</p><hr><p id="paperbox"><table><tbody><tr><td><a name="footnote_1"></a><a href="#back_1">^</a></td><td> [1]</td><td><u><b>Note from Viti95:</b></u> Joystick and network gameplay support have been removed, so it's not a completely feature-intact port ^^ (People are still trying to convince me to bring network gameplay back).</td></tr><tr><td><a name="footnote_2"></a><a href="#back_2">^</a></td><td> [2]</td><td><a href="https://doomwiki.org/wiki/Gamesrc-ver-recreation">DOOM engine: gamesrc-ver-recreation</a></td></tr><tr><td><a name="footnote_3"></a><a href="#back_3">^</a></td><td> [3]</td><td><a href="https://fabiensanglard.net/">Game Engine 
 Black Book: Wolfenstein 3D</a></td></tr><tr><td><a name="footnote_4"></a><a href="#back_4">^</a></td><td> [4]</td><td><a href="https://groups.google.com/g/alt.games.doom/c/3tMB2UmEBK0/m/m1VR6LiJRQMJ">Doom graphics modes usenet</a></td></tr><tr><td><a name="footnote_5"></a><a href="#back_5">^</a></td><td> [5]</td><td><a href="https://groups.google.com/g/alt.games.doom/c/3tMB2UmEBK0/m/m1VR6LiJRQMJ">Doom graphics modes usenet</a></td></tr><tr><td><a name="footnote_6"></a><a href="#back_6">^</a></td><td> [6]</td><td><a href="https://www.vogons.org/viewtopic.php?t=61839">Doom vs Heretic VGA performance difference</a></td></tr><tr><td><a name="footnote_7"></a><a href="#back_7">^</a></td><td> [7]</td><td><a href="https://www.vogons.org/viewtopic.php?f=7&amp;t=40699">Doom in DOS: Original vs Source Ports </a></td></tr><tr><td><a name="footnote_8"></a><a href="#back_8">^</a></td><td> [8]</td><td><a href="https://fabiensanglard.net/fastdoom/openwatcom_doc.pdf">OpenWatcom documentation</a></td></tr><tr><td><a name="footnote_9"></a><a href="#back_9">^</a></td><td> [9]</td><td><u><b>Note from Viti95:</b></u> Some of Ken Silverman’s ideas and code made their way into the rendering functions for UMC Green CPUs, resulting in a significant speed boost on that hardware..</td></tr></tbody></table></p> <hr>
 <center>*</center></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Translating Natural Language to First-Order Logic for Logical Fallacy Detection (190 pts)]]></title>
            <link>https://arxiv.org/abs/2405.02318</link>
            <guid>43257719</guid>
            <pubDate>Tue, 04 Mar 2025 17:36:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2405.02318">https://arxiv.org/abs/2405.02318</a>, See on <a href="https://news.ycombinator.com/item?id=43257719">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2405.02318">View PDF</a></p><blockquote>
            <span>Abstract:</span>Translating natural language into formal language such as First-Order Logic (FOL) is a foundational challenge in NLP with wide-ranging applications in automated reasoning, misinformation tracking, and knowledge validation. In this paper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework to autoformalize natural language to FOL step by step using Large Language Models (LLMs). Our approach addresses key challenges in this translation process, including the integration of implicit background knowledge. By leveraging structured representations generated by NL2FOL, we use Satisfiability Modulo Theory (SMT) solvers to reason about the logical validity of natural language statements. We present logical fallacy detection as a case study to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach also provides interpretable insights into the reasoning process and demonstrates robustness without requiring model fine-tuning or labeled training data. Our framework achieves strong performance on multiple datasets. On the LOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing effectively to the LOGICCLIMATE dataset with an F1-score of 80%.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Abhinav Lalwani [<a href="https://arxiv.org/show-email/a716fc53/2405.02318" rel="nofollow">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2405.02318v1" rel="nofollow">[v1]</a></strong>
        Thu, 18 Apr 2024 00:20:48 UTC (106 KB)<br>
    <strong>[v2]</strong>
        Mon, 3 Mar 2025 00:38:48 UTC (1,297 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Learn How to Break AES (155 pts)]]></title>
            <link>https://davidwong.fr/blockbreakers/</link>
            <guid>43257583</guid>
            <pubDate>Tue, 04 Mar 2025 17:25:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://davidwong.fr/blockbreakers/">https://davidwong.fr/blockbreakers/</a>, See on <a href="https://news.ycombinator.com/item?id=43257583">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
					<p>This page is here to teach you about <strong>block cipher cryptanalysis</strong>. It does not pretend to be an exhaustive list of attacks, but will give you enough to get started.</p>
					<p>The course is split in several sets, the first being on <strong>AES</strong>. We could have made up a simple block cipher, but it's more fun to attack the real thing don't you think?</p>
					<p>So head up to <a href="https://davidwong.fr/blockbreakers/aes.html"><img src="https://davidwong.fr/blockbreakers/images/AES/aes_icons-11.jpg">Set 1</a> and start by implementing <strong>AES</strong>.</p>

					<p>Don't worry, we're holding your hand ;)</p>

					<p><a href="https://davidwong.fr/blockbreakers/aes.html">Get Started <i></i></a></p>

					

					<p>This page was started by <a href="https://www.cryptologie.net/">David Wong</a>.</p>

					

					<p>Check our friends <a href="http://cryptopals.com/">Cryptopals</a> and <a href="https://microcorruption.com/">Microcorruption</a>.</p>

					

					<p>Check this <a href="https://www.reddit.com/r/crypto/wiki/index">wiki page</a>.</p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DARPA exploring growing bio structures of "unprecedented size" in microgravity (124 pts)]]></title>
            <link>https://sam.gov/opp/426e5868fcf74dd4ada3768b00b09234/view</link>
            <guid>43257473</guid>
            <pubDate>Tue, 04 Mar 2025 17:16:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sam.gov/opp/426e5868fcf74dd4ada3768b00b09234/view">https://sam.gov/opp/426e5868fcf74dd4ada3768b00b09234/view</a>, See on <a href="https://news.ycombinator.com/item?id=43257473">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Enhanced Radar (YC W25) – A safety net for air traffic control (121 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43257323</link>
            <guid>43257323</guid>
            <pubDate>Tue, 04 Mar 2025 17:04:17 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43257323">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="43257323">
      <td><span></span></td>      <td><center><a id="up_43257323" href="https://news.ycombinator.com/vote?id=43257323&amp;how=up&amp;goto=item%3Fid%3D43257323"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=43257323">Launch HN: Enhanced Radar (YC W25) – A safety net for air traffic control</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_43257323">106 points</span> by <a href="https://news.ycombinator.com/user?id=kristian1109">kristian1109</a> <span title="2025-03-04T17:04:17 1741107857"><a href="https://news.ycombinator.com/item?id=43257323">8 hours ago</a></span> <span id="unv_43257323"></span> | <a href="https://news.ycombinator.com/hide?id=43257323&amp;goto=item%3Fid%3D43257323">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Enhanced%20Radar%20%28YC%20W25%29%20%E2%80%93%20A%20safety%20net%20for%20air%20traffic%20control&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=43257323&amp;auth=f3af032166c0094530e6a205fe90cdddea346c7c">favorite</a> | <a href="https://news.ycombinator.com/item?id=43257323">75&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Hey HN, we’re Eric and Kristian of Enhanced Radar. We’re working on making air travel safer by augmenting control services in our stressed airspace system.</p><p>Recent weeks have put aviation safety on everyone’s mind, but we’ve been thinking about this problem for years. Both of us are pilots — we have 2,500 hours of flight time between us. Eric flew professionally and holds a Gulfstream 280 type rating and both FAA and EASA certificates. Kristian flies recreationally, and before this worked on edge computer vision for satellites.</p><p>We know from our flying experience that air traffic management is imperfect (every pilot can tell stories of that one time…), so this felt like an obvious problem to work on.</p><p>Most accidents are the result of an overdetermined “accident chain” (<a href="https://code7700.com/accident_investigation.htm" rel="nofollow">https://code7700.com/accident_investigation.htm</a>). The popular analogy here is the swiss cheese model, where holes in every slice line up perfectly to cause an accident. Often, at least one link in that chain is human error.</p><p>We’ll avoid dissecting this year’s tragedies and take a close call from last April at DCA as an example:</p><p>The tower cleared JetBlue 1554 to depart on Runway 04, but simultaneously a ground controller on a different frequency cleared a Southwest jet to cross that same runway, putting them on a collision course. Controllers noticed the conflict unfolding and jumped in to yell at both aircraft to stop, avoiding a collision with about 8 seconds to spare (<a href="https://www.youtube.com/watch?v=yooJmu30DxY" rel="nofollow">https://www.youtube.com/watch?v=yooJmu30DxY</a>).</p><p>Importantly, the error that caused this incident occurred approximately 23 seconds before the conflict became obvious. In this scenario, a good solution would be a system that understands when an aircraft has been cleared to depart from a runway, and then makes sure no aircraft are cleared to cross (or are in fact crossing) that runway until the departing aircraft is wheels-up. And so on.</p><p>To do this, we’ve developed Yeager, an ensemble of models including state of the art speech-to-text that can understand ATC audio. It’s trained on a large amount of our own labeled ATC audio collected from our VHF receivers located at airports around the US. We improve performance by injecting context such as airport layout details, nearby/relevant navaids, and information on all relevant aircraft captured via ADS-B.</p><p>Our product piggy-backs on the raw signal in the air (VHF radio from towers to pilots) by having our own antennas, radios, and software installed at the airport. This system is completely parallel to existing infrastructure, requires zero permission, and zero integration. It’s an extra safety net over existing systems (no replacement required). All the data we need is open-source and unencrypted.</p><p>Building models for processing ATC speech is our first step toward building a safety net that detects human error (by both pilots and ATC). The latest system transcribes the VHF control audio at about ~1.1% WER (Word Error Rate), down from a previous record of ~9%. We’re using these transcripts with NLP and ADS-B (the system that tracks aircraft positions in real time) for readback detection (ensuring pilots correctly repeat ATC instructions) and command compliance.</p><p>There are different views about the future of ATC. Our product is naturally based on our own convictions and experience in the field. For example, it’s sometimes said that voice comms are going away — we think they aren’t (<a href="https://www.ericbutton.co/p/speech" rel="nofollow">https://www.ericbutton.co/p/speech</a>). People also point out that airplanes are going to fly themselves — in fact they already do. But passenger airlines, for example, will keep a pilot onboard (or on the ground) with ultimate control, for a long time from now; the economics and politics and mind-boggling safety and legal standards for aviation make this inevitable. Also, while next-gen ATC systems like ASDE-X are already in place, they don’t eliminate the problem. The April 2024 scenario mentioned above occurred at DCA, an ASDE-X-equipped airport.</p><p>America has more than 5,000 public-use airports, but only 540 of these have control towers (due to cost). As a result, there are over 100 commercial airline routes that fly into uncontrolled airports, and 4.4M landings at these fields. Air traffic control from first principles looks significantly more automated, more remote-controlled, and much cheaper — and as a result, much more widespread.</p><p>We’ve known each other for 3 years, and decided independently that we needed to work on air traffic. Having started on this, we feel like it’s our mission for the next decade or two.</p><p>If you’re a pilot or an engineer who’s thought about this stuff, we’d love to get your input. We look forward to hearing everyone’s thoughts, questions, ideas!</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Satellogic's Open Satellite Feed (197 pts)]]></title>
            <link>https://tech.marksblogg.com/satellogic-open-data-feed.html</link>
            <guid>43256349</guid>
            <pubDate>Tue, 04 Mar 2025 15:56:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tech.marksblogg.com/satellogic-open-data-feed.html">https://tech.marksblogg.com/satellogic-open-data-feed.html</a>, See on <a href="https://news.ycombinator.com/item?id=43256349">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article_text">
            <p>Satellogic is a satellite designer, manufacturer and constellation operator. They were founded in 2010 and have offices in Argentina, Uruguay, Spain and the US.</p>
<p>They launched three prototype Cube satellites between 2013 and 2014, using China and Russia as their launch partners.</p>
<p>In 2016, they began launching ~38 KG, 10K-component, 51 x 57 x 82-cm NewSat microsatellites. These take around three months to build and are meant to last for three years in orbit. They've named the constellation of these satellites "Aleph-1".</p>
<p>Below is a rendering of their NewSat satellite.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/Aleph-1_internal_architecture.png"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/Aleph-1_internal_architecture.png"></a></p><p>They're aiming to have 300 satellites in orbit at some point which would provide revisit times up to every 5 minutes.</p>
<p>Last week, Satellogic announced an open satellite feed programme and named their dataset "Satellogic EarthView".</p>
<p>In this post, I'll examine Satellogic's constellation and open data feed.</p>
<div id="my-workstation">
<h2>My Workstation</h2>
<p>I'm using a 6 GHz Intel Core <a href="https://www.intel.com/content/www/us/en/products/sku/236773/intel-core-i9-processor-14900k-36m-cache-up-to-6-00-ghz/specifications.html">i9-14900K</a> CPU. It has 8 performance cores and 16 efficiency cores with a total of 32 threads and 32 MB of L2 cache. It has a liquid cooler attached and is housed in a spacious, full-sized Cooler Master HAF 700 computer case. I've come across videos on YouTube where people have managed to overclock the i9-14900KF to <a href="https://www.youtube.com/watch?v=fb7pl7PZOYo">9.1 GHz</a>.</p>
<p>The system has 96 GB of DDR5 RAM clocked at 6,000 MT/s and a 5th-generation, Crucial T700 4 TB NVMe M.2 SSD which can read at speeds up to 12,400 MB/s. There is a heatsink on the SSD to help keep its temperature down. This is my system's C drive.</p>
<p>The system is powered by a 1,200-watt, fully modular Corsair Power Supply and is sat on an ASRock Z790 Pro RS Motherboard.</p>
<p>I'm running Ubuntu 22 LTS via Microsoft's Ubuntu for Windows on Windows 11 Pro. In case you're wondering why I don't run a Linux-based desktop as my primary work environment, I'm still using an Nvidia GTX 1080 GPU which has better driver support on Windows and I use ArcGIS Pro from time to time which only supports Windows natively.</p>
</div>
<div id="installing-prerequisites">
<h2>Installing Prerequisites</h2>
<p>I'll use GDAL 3.4.1, Python 3.8 and a few other tools to help analyse the data in this post.</p>
<div><pre><span></span>$<span> </span>sudo<span> </span>apt<span> </span>update
$<span> </span>sudo<span> </span>apt<span> </span>install<span> </span><span>\</span>
<span>    </span>gdal-bin<span> </span><span>\</span>
<span>    </span>jq<span> </span><span>\</span>
<span>    </span>libimage-exiftool-perl<span> </span><span>\</span>
<span>    </span>python3-pip<span> </span><span>\</span>
<span>    </span>python3-virtualenv
</pre></div>
<p>I'll set up a Python Virtual Environment and install some dependencies.</p>
<div><pre><span></span>$<span> </span>python3<span> </span>-m<span> </span>venv<span> </span>~/.satl
$<span> </span><span>source</span><span> </span>~/.satl/bin/activate
$<span> </span>python3<span> </span>-m<span> </span>pip<span> </span>install<span> </span><span>\</span>
<span>    </span>astropy<span> </span><span>\</span>
<span>    </span>awscli<span> </span><span>\</span>
<span>    </span>html2text<span> </span><span>\</span>
<span>    </span>requests<span> </span><span>\</span>
<span>    </span>rich<span> </span><span>\</span>
<span>    </span>sgp4<span> </span><span>\</span>
<span>    </span>utm
</pre></div>
<p>I'll use DuckDB, along with its <a href="https://github.com/isaacbrodsky/h3-duckdb">H3</a>, <a href="https://duckdb.org/docs/extensions/json">JSON</a>, <a href="https://community-extensions.duckdb.org/extensions/lindel.html">Lindel</a>, <a href="https://duckdb.org/docs/data/parquet/overview">Parquet</a> and <a href="https://duckdb.org/docs/extensions/spatial.html">Spatial</a> extensions, in this post.</p>
<div><pre><span></span>$<span> </span><span>cd</span><span> </span>~
$<span> </span>wget<span> </span>-c<span> </span>https://github.com/duckdb/duckdb/releases/download/v1.1.3/duckdb_cli-linux-amd64.zip
$<span> </span>unzip<span> </span>-j<span> </span>duckdb_cli-linux-amd64.zip
$<span> </span>chmod<span> </span>+x<span> </span>duckdb
$<span> </span>~/duckdb
</pre></div>
<div><pre><span></span><span>INSTALL</span><span> </span><span>h3</span><span> </span><span>FROM</span><span> </span><span>community</span><span>;</span>
<span>INSTALL</span><span> </span><span>lindel</span><span> </span><span>FROM</span><span> </span><span>community</span><span>;</span>
<span>INSTALL</span><span> </span><span>json</span><span>;</span>
<span>INSTALL</span><span> </span><span>parquet</span><span>;</span>
<span>INSTALL</span><span> </span><span>spatial</span><span>;</span>
</pre></div>
<p>I'll set up DuckDB to load every installed extension each time it launches.</p>

<div><pre><span></span>.timer on
.width 180
LOAD h3;
LOAD lindel;
LOAD json;
LOAD parquet;
LOAD spatial;
</pre></div>
<p>The maps in this post were rendered with <a href="https://www.qgis.org/en/site/forusers/download.html">QGIS</a> version 3.42. QGIS is a desktop application that runs on Windows, macOS and Linux. The application has grown in popularity in recent years and has ~15M application launches from users all around the world each month.</p>
<p>I used QGIS' <a href="https://github.com/marklit/tile_plus">Tile+ plugin</a> to add geospatial context with Bing's Virtual Earth Basemap to the maps. The dark, non-satellite imagery maps are mostly made up of vector data from Natural Earth and Overture.</p>
</div>
<div id="aleph-1-s-launch-history">
<h2>Aleph-1's Launch History</h2>
<p>Satellogic has launched 41 satellites for its Aleph-1 constellation as of January 14, 2025.</p>
<p>Their first seven satellites were launched by China between May 2016 and September 2020.</p>
<p>Their eighth satellite was launched by ESA from the Spaceport in Kourou, French Guiana in September 2020.</p>
<p>China then launched 10 satellites of theirs in one mission in November 2020.</p>
<p>Since June 2021, SpaceX have launched 23 of their satellites with the most recent  being on January 14th, 2025.</p>
</div>
<div id="active-satellites">
<h2>Active Satellites</h2>
<p>Below I've tried to find all of their active satellites' locations. The positions of their satellites should change each time the following script is executed.</p>
<p>I couldn't find an official listing of their satellites <a href="https://en.wikipedia.org/wiki/Two-line_element_set">Two-line elements</a> (TLEs) but I found a listing of NORAD IDs. I scraped the n2yo listings for each satellite and parsed the name and TLE for each of those results.</p>

<div><pre><span></span><span>import</span> <span>json</span>
<span>from</span>   <span>glob</span>            <span>import</span> <span>glob</span>
<span>from</span>   <span>random</span>          <span>import</span> <span>randint</span>
<span>from</span>   <span>time</span>            <span>import</span> <span>sleep</span>

<span>from</span>   <span>html2text</span>       <span>import</span> <span>html2text</span>
<span>import</span> <span>requests</span>
<span>from</span>   <span>rich.progress</span>   <span>import</span> <span>track</span>


<span>for</span> <span>norad_id</span> <span>in</span> <span>track</span><span>((</span><span>46832</span><span>,</span> <span>46829</span><span>,</span> <span>46827</span><span>,</span> <span>46833</span><span>,</span> <span>46831</span><span>,</span> <span>46830</span><span>,</span>
                       <span>46840</span><span>,</span> <span>46835</span><span>,</span> <span>46836</span><span>,</span> <span>48905</span><span>,</span> <span>48921</span><span>,</span> <span>48920</span><span>,</span>
                       <span>48919</span><span>,</span> <span>52168</span><span>,</span> <span>52178</span><span>,</span> <span>52171</span><span>,</span> <span>52184</span><span>,</span> <span>52172</span><span>,</span>
                       <span>52747</span><span>,</span> <span>52764</span><span>,</span> <span>42760</span><span>,</span> <span>52748</span><span>,</span> <span>52752</span><span>,</span> <span>55064</span><span>,</span>
                       <span>55047</span><span>,</span> <span>55045</span><span>,</span> <span>55048</span><span>,</span> <span>56190</span><span>,</span> <span>56203</span><span>,</span> <span>56202</span><span>,</span>
                       <span>56201</span><span>,</span> <span>56943</span><span>,</span> <span>56944</span><span>,</span> <span>56966</span><span>,</span> <span>56968</span><span>,</span> <span>59122</span><span>,</span>
                       <span>60498</span><span>,</span> <span>60500</span><span>,</span> <span>60493</span><span>,</span> <span>46272</span><span>,</span> <span>45017</span><span>,</span> <span>45018</span><span>,</span>
                       <span>46828</span><span>)):</span>
    <span>resp</span> <span>=</span> <span>requests</span><span>.</span><span>get</span><span>(</span><span>'https://www.n2yo.com/satellite/?s=</span><span>%d</span><span>'</span> <span>%</span> <span>norad_id</span><span>)</span>
    <span>assert</span> <span>resp</span><span>.</span><span>status_code</span> <span>==</span> <span>200</span><span>,</span> <span>resp</span><span>.</span><span>status_code</span>

    <span>with</span> <span>open</span><span>(</span><span>'</span><span>%d</span><span>.md'</span> <span>%</span> <span>norad_id</span><span>,</span> <span>'w'</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
        <span>f</span><span>.</span><span>write</span><span>(</span><span>html2text</span><span>(</span><span>resp</span><span>.</span><span>content</span><span>.</span><span>decode</span><span>(</span><span>'utf-8'</span><span>)))</span>

    <span>sleep</span><span>(</span><span>randint</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>))</span>

<span>tles</span> <span>=</span> <span>{}</span>

<span>for</span> <span>filename</span> <span>in</span> <span>glob</span><span>(</span><span>'*.md'</span><span>):</span>
    <span>lines</span> <span>=</span> <span>open</span><span>(</span><span>filename</span><span>)</span><span>.</span><span>readlines</span><span>()</span>
    <span>norad_id</span> <span>=</span> <span>int</span><span>(</span><span>filename</span><span>.</span><span>split</span><span>(</span><span>'.'</span><span>)[</span><span>0</span><span>])</span>
    <span>sat_name</span> <span>=</span> <span>None</span>

    <span>for</span> <span>num</span><span>,</span> <span>line</span> <span>in</span> <span>enumerate</span><span>(</span><span>lines</span><span>):</span>
        <span>if</span> <span>line</span><span>.</span><span>startswith</span><span>(</span><span>'# NUSAT-'</span><span>):</span>
            <span>sat_name</span> <span>=</span> <span>line</span><span>.</span><span>lstrip</span><span>(</span><span>'# '</span><span>)</span><span>.</span><span>strip</span><span>()</span>

        <span>if</span> <span>line</span><span>.</span><span>startswith</span><span>(</span><span>'    1'</span><span>):</span>
            <span>tles</span><span>[</span><span>norad_id</span><span>]</span> <span>=</span> <span>{</span>
                <span>'tle'</span><span>:</span> <span>[</span><span>line</span><span>.</span><span>strip</span><span>(),</span> <span>lines</span><span>[</span><span>num</span> <span>+</span> <span>1</span><span>]</span><span>.</span><span>strip</span><span>()],</span>
                <span>'name'</span><span>:</span> <span>sat_name</span><span>}</span>

            <span>break</span> <span># The TLE always comes after the sat name</span>

<span>open</span><span>(</span><span>'tles.json'</span><span>,</span> <span>'w'</span><span>)</span><span>.</span><span>write</span><span>(</span><span>json</span><span>.</span><span>dumps</span><span>(</span><span>tles</span><span>))</span>
</pre></div>
<p>The above produced names and TLEs for 22 of their satellites. I'm not sure if the above is an accurate reflection of their constellation's status or not so please don't treat this as gospel.</p>
<p>I then ran the following on March 4th, 2025. It produced a CSV file with names and estimated locations of those 22 satellites.</p>

<div><pre><span></span><span>from</span>   <span>datetime</span> <span>import</span> <span>datetime</span>
<span>import</span> <span>json</span>

<span>from</span>   <span>astropy</span> <span>import</span> <span>units</span> <span>as</span> <span>u</span>
<span>from</span>   <span>astropy.time</span> <span>import</span> <span>Time</span>
<span>from</span>   <span>astropy.coordinates</span> <span>import</span> <span>ITRS</span><span>,</span> \
                                  <span>TEME</span><span>,</span> \
                                  <span>CartesianDifferential</span><span>,</span> \
                                  <span>CartesianRepresentation</span>
<span>from</span>   <span>sgp4.api</span> <span>import</span> <span>Satrec</span>
<span>from</span>   <span>sgp4.api</span> <span>import</span> <span>SGP4_ERRORS</span>


<span>tles</span> <span>=</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>open</span><span>(</span><span>'tles.json'</span><span>)</span><span>.</span><span>read</span><span>())</span>

<span>with</span> <span>open</span><span>(</span><span>'locations.csv'</span><span>,</span> <span>'w'</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
    <span>for</span> <span>norad_id</span> <span>in</span> <span>tles</span><span>:</span>

        <span>name</span>  <span>=</span> <span>tles</span><span>[</span><span>norad_id</span><span>][</span><span>'name'</span><span>]</span>
        <span>line1</span> <span>=</span> <span>tles</span><span>[</span><span>norad_id</span><span>][</span><span>'tle'</span><span>][</span><span>0</span><span>]</span>
        <span>line2</span> <span>=</span> <span>tles</span><span>[</span><span>norad_id</span><span>][</span><span>'tle'</span><span>][</span><span>1</span><span>]</span>

        <span>satellite</span> <span>=</span> <span>Satrec</span><span>.</span><span>twoline2rv</span><span>(</span><span>line1</span><span>,</span> <span>line2</span><span>)</span>

        <span>t</span> <span>=</span> <span>Time</span><span>(</span><span>datetime</span><span>.</span><span>utcnow</span><span>()</span><span>.</span><span>isoformat</span><span>(),</span> <span>format</span><span>=</span><span>'isot'</span><span>,</span> <span>scale</span><span>=</span><span>'utc'</span><span>)</span>

        <span>error_code</span><span>,</span> <span>teme_p</span><span>,</span> <span>teme_v</span> <span>=</span> <span>satellite</span><span>.</span><span>sgp4</span><span>(</span><span>t</span><span>.</span><span>jd1</span><span>,</span> <span>t</span><span>.</span><span>jd2</span><span>)</span> <span># in km and km/s</span>

        <span>if</span> <span>error_code</span> <span>!=</span> <span>0</span><span>:</span>
            <span>raise</span> <span>RuntimeError</span><span>(</span><span>SGP4_ERRORS</span><span>[</span><span>error_code</span><span>])</span>

        <span>teme_p</span> <span>=</span> <span>CartesianRepresentation</span><span>(</span><span>teme_p</span> <span>*</span> <span>u</span><span>.</span><span>km</span><span>)</span>
        <span>teme_v</span> <span>=</span> <span>CartesianDifferential</span><span>(</span><span>teme_v</span> <span>*</span> <span>u</span><span>.</span><span>km</span> <span>/</span> <span>u</span><span>.</span><span>s</span><span>)</span>
        <span>teme</span> <span>=</span> <span>TEME</span><span>(</span><span>teme_p</span><span>.</span><span>with_differentials</span><span>(</span><span>teme_v</span><span>),</span> <span>obstime</span><span>=</span><span>t</span><span>)</span>

        <span>itrs_geo</span> <span>=</span> <span>teme</span><span>.</span><span>transform_to</span><span>(</span><span>ITRS</span><span>(</span><span>obstime</span><span>=</span><span>t</span><span>))</span>
        <span>location</span> <span>=</span> <span>itrs_geo</span><span>.</span><span>earth_location</span>
        <span>loc</span> <span>=</span> <span>location</span><span>.</span><span>geodetic</span>

        <span>f</span><span>.</span><span>write</span><span>(</span><span>'"</span><span>%s</span><span>", "POINT (</span><span>%f</span><span> </span><span>%f</span><span>)"</span><span>\n</span><span>'</span> <span>%</span> <span>(</span><span>name</span><span>.</span><span>lstrip</span><span>(</span><span>'0'</span><span>)</span><span>.</span><span>lstrip</span><span>(),</span>
                                             <span>loc</span><span>.</span><span>lon</span><span>.</span><span>deg</span><span>,</span>
                                             <span>loc</span><span>.</span><span>lat</span><span>.</span><span>deg</span><span>))</span>
</pre></div>
<p>Below is a rendering of the above CSV data in QGIS.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_gfkvrYHvtc.jpg"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_gfkvrYHvtc.jpg"></a>
</p></div>
<div id="open-data-feed">
<h2>Open Data Feed</h2>
<p>Their S3 bucket appears to contain ~7M unique images from more than 3M locations. There is a metadata file for each image they've collected as well as several versions, such as thumbnails, of each image available. There are both RGB and near-infrared imagery available with a resolution of one meter.</p>
<p>Below, I'll list out the contents of their S3 bucket.</p>
<div><pre><span></span>$<span> </span>aws<span> </span>--no-sign-request<span> </span><span>\</span>
<span>      </span>--output<span> </span>json<span> </span><span>\</span>
<span>      </span>s3api<span> </span><span>\</span>
<span>      </span>list-objects<span> </span><span>\</span>
<span>      </span>--bucket<span> </span>satellogic-earthview<span> </span><span>\</span>
<span>      </span>--max-items<span>=</span><span>100000000</span><span> </span><span>\</span>
<span>    </span><span>|</span><span> </span>jq<span> </span>-c<span> </span><span>'.Contents[]'</span><span> </span><span>\</span>
<span>    </span>&gt;<span> </span>satellogic.s3.json
</pre></div>
<p>The resulting JSON file contains 35,480,124 lines and is 13 GB uncompressed. There are almost 10 TBs of content in this bucket.</p>
<div><pre><span></span>$<span> </span><span>echo</span><span> </span><span>"Total objects: "</span><span> </span><span>`</span>wc<span> </span>-l<span> </span>satellogic.s3.json<span> </span><span>|</span><span> </span>cut<span> </span>-d<span>' '</span><span> </span>-f1<span>`</span>,<span> </span><span>\</span>
<span>        </span><span>" TB: "</span><span> </span><span>`</span>jq<span> </span>.Size<span> </span>satellogic.s3.json<span> </span><span>|</span><span> </span>awk<span> </span><span>'{s+=$1}END{print s/1024/1024/1024/1024}'</span><span>`</span>
</pre></div>
<div><pre><span></span>Total objects:  35480124,  TB:  9.79704
</pre></div>
<p>Each metadata filename contains its corresponding image's location as well as other useful attributes. I'll parse these filenames and produce a metadata database in DuckDB.</p>

<div><pre><span></span><span>import</span> <span>json</span>

<span>from</span>   <span>rich.progress</span> <span>import</span> <span>track</span>
<span>import</span> <span>utm</span>


<span>with</span> <span>open</span><span>(</span><span>'enriched.s3.json'</span><span>,</span> <span>'w'</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
    <span>for</span> <span>line</span> <span>in</span> <span>track</span><span>(</span><span>open</span><span>(</span><span>'satellogic.s3.json'</span><span>),</span>
                      <span>total</span><span>=</span><span>35_480_124</span><span>):</span>
        <span>rec</span> <span>=</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>line</span><span>)</span>
        <span>if</span> <span>not</span> <span>rec</span><span>[</span><span>'Key'</span><span>]</span><span>.</span><span>startswith</span><span>(</span><span>'data/json/'</span><span>):</span>
            <span>continue</span>

        <span>date_</span><span>,</span> \
        <span>time_</span><span>,</span> \
        <span>rec</span><span>[</span><span>'sat'</span><span>],</span> \
        <span>rec</span><span>[</span><span>'zone'</span><span>],</span> \
        <span>rec</span><span>[</span><span>'region'</span><span>],</span> \
        <span>rec</span><span>[</span><span>'region2'</span><span>],</span> \
        <span>_</span> <span>=</span> <span>rec</span><span>[</span><span>'Key'</span><span>]</span><span>.</span><span>split</span><span>(</span><span>'/'</span><span>)[</span><span>5</span><span>]</span><span>.</span><span>split</span><span>(</span><span>'_'</span><span>)</span>

        <span>rec</span><span>[</span><span>'captured_at'</span><span>]</span> <span>=</span> \
            <span>date_</span><span>[</span><span>0</span><span>:</span><span>4</span><span>]</span> <span>+</span> <span>'-'</span> <span>+</span> <span>date_</span><span>[</span><span>4</span><span>:</span><span>6</span><span>]</span> <span>+</span> <span>'-'</span> <span>+</span> <span>date_</span><span>[</span><span>6</span><span>:</span><span>8</span><span>]</span> <span>+</span> <span>'T'</span> <span>+</span> \
            <span>time_</span><span>[</span><span>0</span><span>:</span><span>2</span><span>]</span> <span>+</span> <span>':'</span> <span>+</span> <span>time_</span><span>[</span><span>2</span><span>:</span><span>4</span><span>]</span> <span>+</span> <span>':'</span> <span>+</span> <span>time_</span><span>[</span><span>4</span><span>:</span><span>6</span><span>]</span> <span>+</span> <span>'Z'</span>

        <span>try</span><span>:</span>
            <span>lat</span><span>,</span> <span>lon</span> <span>=</span> <span>utm</span><span>.</span><span>to_latlon</span><span>(</span><span>float</span><span>(</span><span>rec</span><span>[</span><span>'region'</span><span>]),</span>
                                     <span>float</span><span>(</span><span>rec</span><span>[</span><span>'region2'</span><span>]),</span>
                                     <span>int</span><span>(</span><span>rec</span><span>[</span><span>'zone'</span><span>][</span><span>0</span><span>:</span><span>2</span><span>]),</span>
                                     <span>northern</span><span>=</span><span>rec</span><span>[</span><span>'zone'</span><span>][</span><span>2</span><span>]</span> <span>==</span> <span>'N'</span><span>)</span>
        <span># WIP:</span>
        <span># OutOfRangeError: northing out of range</span>
        <span># (must be between 0 m and 10,000,000 m)</span>
        <span>except</span> <span>Exception</span> <span>as</span> <span>exc</span><span>:</span>
            <span>continue</span>

        <span>rec</span><span>[</span><span>'lat'</span><span>],</span> <span>rec</span><span>[</span><span>'lon'</span><span>]</span> <span>=</span> <span>float</span><span>(</span><span>lat</span><span>),</span> <span>float</span><span>(</span><span>lon</span><span>)</span>

        <span>f</span><span>.</span><span>write</span><span>(</span><span>json</span><span>.</span><span>dumps</span><span>(</span><span>rec</span><span>,</span> <span>sort_keys</span><span>=</span><span>True</span><span>)</span> <span>+</span> <span>'</span><span>\n</span><span>'</span><span>)</span>
</pre></div>
<p>The resulting JSON produced by the above script is 3.3 GB uncompressed and contains  7,095,985 lines. Below, I'll import it into DuckDB.</p>
<div><pre><span></span>$<span> </span>~/duckdb<span> </span>~/satellogic.duckdb
</pre></div>
<div><pre><span></span><span>CREATE</span><span> </span><span>OR</span><span> </span><span>REPLACE</span><span> </span><span>TABLE</span><span> </span><span>s3</span><span> </span><span>AS</span>
<span>    </span><span>SELECT</span><span> </span><span>*</span><span> </span><span>EXCLUDE</span><span>(</span><span>lat</span><span>,</span><span> </span><span>lon</span><span>),</span>
<span>           </span><span>ST_POINT</span><span>(</span><span>lon</span><span>,</span><span> </span><span>lat</span><span>)</span><span> </span><span>AS</span><span> </span><span>geom</span>
<span>    </span><span>FROM</span><span>   </span><span>READ_JSON</span><span>(</span><span>'enriched.s3.json'</span><span>);</span>
</pre></div>
</div>
<div id="data-fluency">
<h2>Data Fluency</h2>
<p>The imagery in this feed was captured between July and December 2022. Below is the breakdown by month and satellite of the number of images captured. I've rounded the amounts to the nearest thousand.</p>
<div><pre><span></span><span>WITH</span><span> </span><span>a</span><span> </span><span>AS</span><span> </span><span>(</span>
<span>    </span><span>SELECT</span><span>   </span><span>sat</span><span>,</span>
<span>             </span><span>STRFTIME</span><span>(</span><span>captured_at</span><span>,</span><span>  </span><span>'%m'</span><span>)</span><span> </span><span>month_</span><span>,</span>
<span>             </span><span>ROUND</span><span>(</span><span>COUNT</span><span>(</span><span>*</span><span>)</span><span> </span><span>/</span><span> </span><span>1000</span><span>)::</span><span>INT</span><span> </span><span>num_rec</span>
<span>    </span><span>FROM</span><span>     </span><span>s3</span>
<span>    </span><span>GROUP</span><span> </span><span>BY</span><span> </span><span>1</span><span>,</span><span> </span><span>2</span>
<span>    </span><span>ORDER</span><span> </span><span>BY</span><span> </span><span>1</span><span>,</span><span> </span><span>2</span>
<span>)</span>
<span>PIVOT</span><span>    </span><span>a</span>
<span>ON</span><span>       </span><span>month_</span>
<span>USING</span><span>    </span><span>SUM</span><span>(</span><span>num_rec</span><span>)</span>
<span>GROUP</span><span> </span><span>BY</span><span> </span><span>sat</span>
<span>ORDER</span><span> </span><span>BY</span><span> </span><span>sat</span><span>[</span><span>3</span><span>:]::</span><span>INT</span><span>;</span>
</pre></div>
<div><pre><span></span>┌─────────┬────────┬────────┬────────┬────────┬────────┬────────┐
│   sat   │   07   │   08   │   09   │   10   │   11   │   12   │
│ varchar │ int128 │ int128 │ int128 │ int128 │ int128 │ int128 │
├─────────┼────────┼────────┼────────┼────────┼────────┼────────┤
│ SN8     │      3 │        │        │        │        │        │
│ SN9     │     54 │     66 │     68 │     69 │    100 │     58 │
│ SN11    │     55 │     52 │     82 │     51 │     80 │     78 │
│ SN12    │        │        │      1 │        │        │        │
│ SN13    │     66 │     15 │        │        │     13 │     41 │
│ SN14    │        │        │      7 │      4 │        │      1 │
│ SN15    │     86 │     52 │     95 │     86 │     49 │        │
│ SN16    │     71 │     53 │     37 │     72 │     67 │     96 │
│ SN18    │     80 │     75 │     60 │     63 │    118 │    153 │
│ SN20    │     29 │     71 │     56 │     94 │    130 │     73 │
│ SN21    │     70 │     51 │     60 │    117 │    101 │     82 │
│ SN22    │     61 │     33 │     63 │     95 │    119 │    111 │
│ SN23    │     66 │     72 │     70 │     16 │        │        │
│ SN24    │     68 │     75 │     66 │     95 │     78 │    131 │
│ SN25    │     93 │     39 │     88 │      9 │        │        │
│ SN27    │     71 │     77 │    106 │     99 │     86 │    100 │
│ SN28    │        │     52 │     94 │    107 │    129 │    110 │
│ SN29    │     18 │     44 │     71 │    104 │     75 │    159 │
│ SN30    │     39 │     45 │     70 │     79 │     95 │     60 │
│ SN31    │     51 │    100 │     64 │     81 │    146 │    107 │
├─────────┴────────┴────────┴────────┴────────┴────────┴────────┤
│ 20 rows                                             7 columns │
└───────────────────────────────────────────────────────────────┘
</pre></div>
<p>I'll produce a heatmap of where the imagery was captured.</p>
<div><pre><span></span>$<span> </span>~/duckdb<span> </span>~/satellogic.duckdb
</pre></div>
<div><pre><span></span><span>COPY</span><span> </span><span>(</span>
<span>    </span><span>SELECT</span><span>   </span><span>h3_cell_to_boundary_wkt</span><span>(</span>
<span>                  </span><span>h3_latlng_to_cell</span><span>(</span><span>ST_Y</span><span>(</span><span>geom</span><span>),</span>
<span>                                    </span><span>ST_X</span><span>(</span><span>geom</span><span>),</span>
<span>                                    </span><span>5</span><span>))::</span><span>GEOMETRY</span><span> </span><span>geom</span><span>,</span>
<span>             </span><span>COUNT</span><span>(</span><span>*</span><span>)</span><span> </span><span>num_recs</span>
<span>    </span><span>FROM</span><span>     </span><span>s3</span>
<span>    </span><span>WHERE</span><span>    </span><span>ST_X</span><span>(</span><span>geom</span><span>)</span><span> </span><span>BETWEEN</span><span> </span><span>-</span><span>175</span><span> </span><span>AND</span><span> </span><span>175</span>
<span>    </span><span>AND</span><span>      </span><span>ST_Y</span><span>(</span><span>geom</span><span>)</span><span> </span><span>BETWEEN</span><span> </span><span>-</span><span>90</span><span>  </span><span>AND</span><span>  </span><span>90</span>
<span>    </span><span>GROUP</span><span> </span><span>BY</span><span> </span><span>1</span>
<span>)</span><span> </span><span>TO</span><span> </span><span>'h3_5.gpkg'</span>
<span>    </span><span>WITH</span><span> </span><span>(</span><span>FORMAT</span><span> </span><span>GDAL</span><span>,</span>
<span>          </span><span>DRIVER</span><span> </span><span>'GPKG'</span><span>,</span>
<span>          </span><span>LAYER_CREATION_OPTIONS</span><span> </span><span>'WRITE_BBOX=YES'</span><span>);</span>
</pre></div>
<p>This is a global view of their imagery footprints.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_7L3wLAfGAc.png"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_7L3wLAfGAc.png"></a></p><p>Below is a zoomed-in view of North America.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_gfxsBb5stx.png"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_gfxsBb5stx.png"></a></p><p>Below is a zoomed-in view of Europe, North Africa and the Middle East. Hotspots in Albania, Saudi Arabia and between India and China really stand out.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_QDvTFxYjIN.png"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_QDvTFxYjIN.png"></a></p><p>The image below might be a bit easier to see on a mobile device.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_ISlxCDKTTQ.jpg"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_ISlxCDKTTQ.jpg"></a>
</p></div>
<div id="imagery-by-country">
<h2>Imagery by Country</h2>
<p>I'll use the Database of Global Administrative Areas (<a href="https://gadm.org/">GADM</a>) to get the number of images per country in this dataset.</p>
<div><pre><span></span>$<span> </span>wget<span> </span>-c<span> </span>https://geodata.ucdavis.edu/gadm/gadm4.1/gadm_410-gpkg.zip
$<span> </span>unzip<span> </span>gadm_410-gpkg.zip
</pre></div>
<div><pre><span></span>$<span> </span>~/duckdb<span> </span>~/satellogic.duckdb
</pre></div>
<div><pre><span></span><span>CREATE</span><span> </span><span>OR</span><span> </span><span>REPLACE</span><span> </span><span>TABLE</span><span> </span><span>gadm</span><span> </span><span>AS</span>
<span>    </span><span>FROM</span><span>   </span><span>ST_READ</span><span>(</span><span>'/mnt/f/gis/Global/gadm/gadm_410.gpkg'</span><span>);</span>
</pre></div>
<p>The heatmap GeoPackage (GPKG) file I produced earlier has a record count for each hexagon across the globe containing at least one image. It contains 6,122 records in total. I'll use the centroid of each hexagon to find its matching country.</p>
<div><pre><span></span><span>CREATE</span><span> </span><span>OR</span><span> </span><span>REPLACE</span><span> </span><span>TABLE</span><span> </span><span>countries</span><span> </span><span>AS</span>
<span>    </span><span>SELECT</span><span>   </span><span>b</span><span>.</span><span>COUNTRY</span><span> </span><span>AS</span><span> </span><span>country</span><span>,</span>
<span>             </span><span>SUM</span><span>(</span><span>a</span><span>.</span><span>num_recs</span><span>)</span><span> </span><span>AS</span><span> </span><span>num_recs</span>
<span>    </span><span>FROM</span><span>     </span><span>ST_READ</span><span>(</span><span>'h3_5.gpkg'</span><span>)</span><span> </span><span>a</span>
<span>    </span><span>JOIN</span><span>     </span><span>gadm</span><span> </span><span>b</span><span> </span><span>ON</span><span> </span><span>ST_CONTAINS</span><span>(</span><span>b</span><span>.</span><span>geom</span><span>,</span>
<span>                                   </span><span>ST_CENTROID</span><span>(</span><span>a</span><span>.</span><span>geom</span><span>))</span>
<span>    </span><span>GROUP</span><span> </span><span>BY</span><span> </span><span>1</span><span>;</span>
</pre></div>
<p>Not every centroid landed on a polygon for any of the countries listed in GADM. It could be that the missing ~1.5M image locations are just off of the coast or out in territorial waters.</p>
<div><pre><span></span><span>SELECT</span><span> </span><span>SUM</span><span>(</span><span>num_recs</span><span>)</span>
<span>FROM</span><span>   </span><span>countries</span><span>;</span>
</pre></div>

<p>Nonetheless, the following should be representative of how often any one country features in this dataset.</p>
<div><pre><span></span><span>FROM</span><span>     </span><span>countries</span>
<span>ORDER</span><span> </span><span>BY</span><span> </span><span>num_recs</span><span> </span><span>DESC</span><span>;</span>
</pre></div>
<div><pre><span></span>┌────────────────────────┬──────────┐
│        country         │ num_recs │
│        varchar         │  int128  │
├────────────────────────┼──────────┤
│ United States          │   886871 │
│ Saudi Arabia           │   428122 │
│ Australia              │   423079 │
│ Spain                  │   326465 │
│ China                  │   324572 │
│ México                 │   304916 │
│ Pakistan               │   293720 │
│ Russia                 │   250520 │
│ Ukraine                │   184756 │
│ Brazil                 │   176929 │
│ Argentina              │   156790 │
│ Albania                │   115472 │
│ United Arab Emirates   │   114530 │
│ Oman                   │   102500 │
│ Bangladesh             │   101888 │
│ Iran                   │    98160 │
│ India                  │    81026 │
│ Iraq                   │    72990 │
│ Canada                 │    65153 │
│ North Korea            │    64570 │
│   ·                    │       ·  │
│   ·                    │       ·  │
│   ·                    │       ·  │
│ Haiti                  │     1519 │
│ Denmark                │     1428 │
│ Moldova                │     1384 │
│ Romania                │     1291 │
│ Mozambique             │     1217 │
│ South Korea            │     1188 │
│ Colombia               │     1139 │
│ Northern Cyprus        │     1088 │
│ Slovakia               │     1075 │
│ Indonesia              │     1070 │
│ Dominican Republic     │      717 │
│ Trinidad and Tobago    │      592 │
│ Somalia                │      563 │
│ Switzerland            │      511 │
│ Serbia                 │      472 │
│ Guam                   │      431 │
│ Cuba                   │      370 │
│ Bosnia and Herzegovina │      289 │
│ Philippines            │      263 │
│ Barbados               │      244 │
├────────────────────────┴──────────┤
│ 127 rows (40 shown)     2 columns │
└───────────────────────────────────┘
</pre></div>
</div>

<div id="qatari-imagery">
<h2>Qatari Imagery</h2>
<p>I drew a bounding box around Qatar to see how many images in this dataset feature the country. Due to the country's shape and proximity to Bahrain and Saudi Arabia, a few of their images were also included in this count.</p>
<div><pre><span></span>$<span> </span>~/duckdb<span> </span>~/satellogic.duckdb
</pre></div>
<div><pre><span></span><span>SELECT</span><span> </span><span>COUNT</span><span>(</span><span>*</span><span>)</span>
<span>FROM</span><span>   </span><span>s3</span>
<span>WHERE</span><span>  </span><span>ST_X</span><span>(</span><span>geom</span><span>)</span><span> </span><span>BETWEEN</span><span> </span><span>50</span><span>.</span><span>6515</span><span> </span><span>AND</span><span> </span><span>51</span><span>.</span><span>8086</span>
<span>AND</span><span>    </span><span>ST_Y</span><span>(</span><span>geom</span><span>)</span><span> </span><span>BETWEEN</span><span> </span><span>24</span><span>.</span><span>3595</span><span> </span><span>AND</span><span> </span><span>26</span><span>.</span><span>3612</span><span>;</span>
</pre></div>
<div><pre><span></span>┌──────────────┐
│ count_star() │
│    int64     │
├──────────────┤
│        64375 │
└──────────────┘
</pre></div>
<p>I'll get the URLs of the metadata for those 64,375 images and download them using eight threads.</p>
<div><pre><span></span>$<span> </span><span>echo</span><span> </span><span>"SELECT Key</span>
<span>        FROM   s3</span>
<span>        WHERE  ST_X(geom) BETWEEN 50.6515 AND 51.8086</span>
<span>        AND    ST_Y(geom) BETWEEN 24.3595 AND 26.3612"</span><span> </span><span>\</span>
<span>        </span><span>|</span><span> </span>~/duckdb<span> </span>-csv<span> </span>-noheader<span> </span>~/satellogic.duckdb<span> </span><span>\</span>
<span>        </span>&gt;<span> </span>manifest.txt
</pre></div>
<div><pre><span></span>$<span> </span>mkdir<span> </span>qatar
$<span> </span><span>cd</span><span> </span>qatar

$<span> </span>cat<span> </span>../manifest.txt<span> </span><span>\</span>
<span>    </span><span>|</span><span> </span>xargs<span> </span><span>\</span>
<span>        </span>-P8<span> </span><span>\</span>
<span>        </span>-I%<span> </span><span>\</span>
<span>        </span>wget<span> </span>-c<span> </span><span>"https://satellogic-earthview.s3.us-west-2.amazonaws.com/%"</span>
</pre></div>
<p>The above downloaded 252 MB of JSON and took almost an hour to complete. I'll concatenate those files into a single line-delimited JSON file. I'll then use DuckDB to convert the JSON into a GPKG file with a few fields cleaned up for the sake of ergonomics.</p>
<div><pre><span></span>$<span> </span><span>cd</span><span> </span>../
$<span> </span>find<span> </span>qatar/<span> </span><span>\</span>
<span>    </span>-type<span> </span>f<span> </span><span>\</span>
<span>    </span>-exec<span> </span>cat<span> </span><span>{}</span><span> </span>+<span> </span><span>\</span>
<span>    </span>&gt;<span> </span>qatar.json
$<span> </span>~/duckdb
</pre></div>
<div><pre><span></span><span>COPY</span><span> </span><span>(</span>
<span>    </span><span>SELECT</span><span> </span><span>properties</span><span>.</span><span>*</span><span> </span><span>EXCLUDE</span><span>(</span><span>providers</span><span>,</span>
<span>                                </span><span>"proj:shape"</span><span>,</span>
<span>                                </span><span>"proj:transform"</span><span>),</span>
<span>           </span><span>assets</span><span>.</span><span>preview</span><span>.</span><span>href</span><span>          </span><span>AS</span><span> </span><span>url_preview</span><span>,</span>
<span>           </span><span>assets</span><span>.</span><span>visual</span><span>.</span><span>href</span><span>           </span><span>AS</span><span> </span><span>url_visual</span><span>,</span>
<span>           </span><span>assets</span><span>.</span><span>analytic</span><span>.</span><span>href</span><span>         </span><span>AS</span><span> </span><span>url_analytic</span><span>,</span>
<span>           </span><span>assets</span><span>.</span><span>thumbnail</span><span>.</span><span>href</span><span>        </span><span>AS</span><span> </span><span>url_thumbnail</span><span>,</span>
<span>           </span><span>ST_GEOMFROMGEOJSON</span><span>(</span><span>geometry</span><span>)</span><span> </span><span>AS</span><span> </span><span>geom</span>
<span>    </span><span>FROM</span><span>   </span><span>'qatar.json'</span>
<span>)</span><span> </span><span>TO</span><span> </span><span>'qatar.footprints.gpkg'</span>
<span>    </span><span>WITH</span><span> </span><span>(</span><span>FORMAT</span><span> </span><span>GDAL</span><span>,</span>
<span>          </span><span>DRIVER</span><span> </span><span>'GPKG'</span><span>,</span>
<span>          </span><span>LAYER_CREATION_OPTIONS</span><span> </span><span>'WRITE_BBOX=YES'</span><span>);</span>
</pre></div>
<p>Below is an example record of the above GPKG file.</p>
<div><pre><span></span>$<span> </span><span>echo</span><span> </span><span>"FROM  ST_READ('qatar.footprints.gpkg')</span>
<span>        LIMIT 1"</span><span> </span><span>\</span>
<span>    </span><span>|</span><span> </span>~/duckdb<span> </span>-json<span> </span><span>\</span>
<span>    </span><span>|</span><span> </span>jq<span> </span>-S<span> </span>.
</pre></div>
<div><pre><span></span><span>[</span>
<span>  </span><span>{</span>
<span>    </span><span>"datetime"</span><span>:</span><span> </span><span>"2022-07-14T11:06:22.192899+00:00"</span><span>,</span>
<span>    </span><span>"geom"</span><span>:</span><span> </span><span>"POLYGON ((50.65626187904185 26.24828637962653, 50.656251675908884 26.25175366324302, 50.65240649343577 26.25174441056527, 50.652416810695826 26.24827712835427, 50.65626187904185 26.24828637962653))"</span><span>,</span>
<span>    </span><span>"grid:code"</span><span>:</span><span> </span><span>"39N-465288_2903610"</span><span>,</span>
<span>    </span><span>"gsd"</span><span>:</span><span> </span><span>1</span><span>,</span>
<span>    </span><span>"license"</span><span>:</span><span> </span><span>"CC-BY-4.0"</span><span>,</span>
<span>    </span><span>"platform"</span><span>:</span><span> </span><span>"newsat21"</span><span>,</span>
<span>    </span><span>"proj:epsg"</span><span>:</span><span> </span><span>"32639"</span><span>,</span>
<span>    </span><span>"satl:altitude"</span><span>:</span><span> </span><span>461.4741909855675</span><span>,</span>
<span>    </span><span>"satl:altitude_units"</span><span>:</span><span> </span><span>"km"</span><span>,</span>
<span>    </span><span>"satl:product_name"</span><span>:</span><span> </span><span>"L1"</span><span>,</span>
<span>    </span><span>"url_analytic"</span><span>:</span><span> </span><span>"https://satellogic-earthview.s3.us-west-2.amazonaws.com/data/tif/zone=39N/region=465288_2903610/date=2022-07-14/20220714_110622_SN21_39N_465288_2903610_TOA.tif"</span><span>,</span>
<span>    </span><span>"url_preview"</span><span>:</span><span> </span><span>"https://satellogic-earthview.s3.us-west-2.amazonaws.com/data/png/zone=39N/region=465288_2903610/date=2022-07-14/20220714_110622_SN21_39N_465288_2903610_preview.png"</span><span>,</span>
<span>    </span><span>"url_thumbnail"</span><span>:</span><span> </span><span>"https://satellogic-earthview.s3.us-west-2.amazonaws.com/data/png/zone=39N/region=465288_2903610/date=2022-07-14/20220714_110622_SN21_39N_465288_2903610_thumbnail.png"</span><span>,</span>
<span>    </span><span>"url_visual"</span><span>:</span><span> </span><span>"https://satellogic-earthview.s3.us-west-2.amazonaws.com/data/tif/zone=39N/region=465288_2903610/date=2022-07-14/20220714_110622_SN21_39N_465288_2903610_VISUAL.tif"</span><span>,</span>
<span>    </span><span>"view:azimuth"</span><span>:</span><span> </span><span>97.95684658243894</span><span>,</span>
<span>    </span><span>"view:off_nadir"</span><span>:</span><span> </span><span>9.250787005630361</span><span>,</span>
<span>    </span><span>"view:sun_azimuth"</span><span>:</span><span> </span><span>269.67692021214305</span><span>,</span>
<span>    </span><span>"view:sun_elevation"</span><span>:</span><span> </span><span>57.10934720889825</span>
<span>  </span><span>}</span>
<span>]</span>
</pre></div>
<p>Below are the footprints of those 64,375 images. I've colour-coded them by the date of their capture. I've tinted the basemap blue to help the footprints stand out. The red bounding box was the search area.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_m7ijcDVW5m.jpg"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_m7ijcDVW5m.jpg"></a></p><p>When I zoom in on any one of the large stripes of footprints, the individual footprints of each image can be seen.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_7rISpM7bAw.jpg"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_7rISpM7bAw.jpg"></a></p><p>I'll pick a small part of Qatar along its coastline and download that area's images.</p>
<div><pre><span></span>$<span> </span><span>echo</span><span> </span><span>"SELECT url_visual</span>
<span>        FROM   ST_READ('qatar.footprints.gpkg')</span>
<span>        WHERE  ST_X(ST_CENTROID(geom)) BETWEEN 50.811115 AND 50.829002</span>
<span>        AND    ST_Y(ST_CENTROID(geom)) BETWEEN 25.522661 AND 25.542736"</span><span> </span><span>\</span>
<span>        </span><span>|</span><span> </span>~/duckdb<span> </span>-csv<span> </span>-noheader<span> </span><span>\</span>
<span>        </span>&gt;<span> </span>url_visual.txt

$<span> </span>mkdir<span> </span>imagery
$<span> </span><span>cd</span><span> </span>imagery

$<span> </span>cat<span> </span>../url_visual.txt<span> </span><span>\</span>
<span>    </span><span>|</span><span> </span>xargs<span> </span><span>\</span>
<span>        </span>-P8<span> </span><span>\</span>
<span>        </span>-I%<span> </span><span>\</span>
<span>        </span>wget<span> </span>-c<span> </span><span>"%"</span>
</pre></div>
<p>The above downloaded 24 GeoTIFFs totalling 9.1 MB. Each image is 384x384-pixels.</p>
<p>Each image has geospatial information embedded within it, so I can group them together into a single image that will retain the spatial data. I'll re-compress the image using WebP, so the resulting image is much smaller than the original source imagery.</p>
<div><pre><span></span>$<span> </span>gdalbuildvrt<span> </span>mosaic.vrt<span> </span>*VISUAL.tif

$<span> </span>gdal_translate<span> </span><span>\</span>
<span>    </span>-co<span> </span><span>NUM_THREADS</span><span>=</span>ALL_CPUS<span> </span><span>\</span>
<span>    </span>-of<span> </span>COG<span> </span><span>\</span>
<span>    </span>-co<span> </span><span>COMPRESS</span><span>=</span>WEBP<span> </span><span>\</span>
<span>    </span>-co<span> </span><span>PREDICTOR</span><span>=</span><span>2</span><span> </span><span>\</span>
<span>    </span>mosaic.vrt<span> </span><span>\</span>
<span>    </span>mosaic.tif
</pre></div>
<p>The resulting mosaic GeoTIFF is 225 KB.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_ZBoY0S4gxb.jpg"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_ZBoY0S4gxb.jpg"></a></p><p>The 24 images look very different from one another and don't blend seamlessly. They share the same processing level and capture statistics, as far as I can tell.</p>
<div><pre><span></span><span>.</span><span>mode</span><span> </span><span>line</span>

<span>SELECT</span><span>   </span><span>datetime</span><span>,</span>
<span>         </span><span>platform</span><span>,</span>
<span>         </span><span>"satl:product_name"</span><span>,</span>
<span>         </span><span>"view:azimuth"</span><span>,</span>
<span>         </span><span>"view:off_nadir"</span><span>,</span>
<span>         </span><span>"view:sun_azimuth"</span><span>,</span>
<span>         </span><span>"view:sun_elevation"</span><span>,</span>
<span>         </span><span>COUNT</span><span>(</span><span>*</span><span>)</span>
<span>FROM</span><span>     </span><span>ST_READ</span><span>(</span><span>'qatar.footprints.gpkg'</span><span>)</span>
<span>WHERE</span><span>    </span><span>ST_X</span><span>(</span><span>ST_CENTROID</span><span>(</span><span>geom</span><span>))</span><span> </span><span>BETWEEN</span><span> </span><span>50</span><span>.</span><span>811115</span><span> </span><span>AND</span><span> </span><span>50</span><span>.</span><span>829002</span>
<span>AND</span><span>      </span><span>ST_Y</span><span>(</span><span>ST_CENTROID</span><span>(</span><span>geom</span><span>))</span><span> </span><span>BETWEEN</span><span> </span><span>25</span><span>.</span><span>522661</span><span> </span><span>AND</span><span> </span><span>25</span><span>.</span><span>542736</span>
<span>GROUP</span><span> </span><span>BY</span><span> </span><span>1</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>4</span><span>,</span><span> </span><span>5</span><span>,</span><span> </span><span>6</span><span>,</span><span> </span><span>7</span><span>;</span>
</pre></div>
<div><pre><span></span>          datetime = 2022-10-10T07:11:56.894566+00:00
          platform = newsat16
 satl:product_name = L1
      view:azimuth = 283.5615135582265
    view:off_nadir = 23.243003077262802
  view:sun_azimuth = 149.1609429350105
view:sun_elevation = 53.39441361193675
      count_star() = 24
</pre></div>
<p>For clarity, below are the grid codes for each image in the mosaic.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_QmnW2jl4ox.png"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_QmnW2jl4ox.png"></a></p><p>The following are types of imagery deliverables that Satellogic offers:</p>
<ul>
<li>L0: Raw</li>
<li>L1A: Raw corrected</li>
<li>L1B: L1 Basic (Cloud, shadow and usable data mask)</li>
<li>L1C: Ortho ready</li>
<li>L1D/L1D_SR: Ortho</li>
<li>L1: TOA Reflectance</li>
<li>L1_SR: TOA Reflectance SuperResolution</li>
</ul>
<p>The product listed in the metadata for these images was L1 which I'm assuming means "Radiometric-corrected imagery (TOA reflectance imagery)". Their product documentation says the imagery is corrected for sensor, optical and terrain distortions (orthorectified).</p>
<p>My best guess is that the images have been processed independently of one another. Basemap producers often strive to blend images next to one another seamlessly but that might not be a requirement for this imagery's original deliverable.</p>
</div>

        </div><p>
            Thank you for taking the time to read this post. I offer both consulting and hands-on development services to clients in North America and Europe. If you'd like to discuss how my offerings can help your business please contact me via <a href="https://uk.linkedin.com/in/marklitwintschik/">LinkedIn</a>.
        </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The owner of ip4.me/ip6.me, Kevin Loch, passed away (271 pts)]]></title>
            <link>https://ip4only.me/</link>
            <guid>43256298</guid>
            <pubDate>Tue, 04 Mar 2025 15:54:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ip4only.me/">https://ip4only.me/</a>, See on <a href="https://news.ycombinator.com/item?id=43256298">Hacker News</a></p>
<div id="readability-page-1" class="page">
<h2>!! NOTICE !!</h2>
<p>
Notice: The owner of ip4.me/ip6.me, Kevin Loch, passed away.<br>
The Kevin M Loch Estate will be shutting down Kevin's websites in the near future (4/1/2025).<br>
Inquiries for purchasing Kevin's domains may be sent to ipadmin@dulles-ix.net.
</p>
<p>
Click this link to continue to your ip4/ip6 address reporting <a href="https://ip4only.me/home.cgi">Website</a>
</p>
<p>
List of Websites impacted:
ip4.me, ip4only.me<br>
ip6addr.com, ip6addr.net, ip6addr.org<br>
ip6.me, ip6only.me<br>
ipv6addr.com, ipv6addr.net, ipv6addr.org<br>
onlyip4.me, onlyip6.me<br>
whatismyipv6address.com, whatismyipv6address.net, whatismyipv6address.org<br>
whatismyv6.com, whatismyv6.net, whatismyv6.org<br>
</p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Should managers still code? (145 pts)]]></title>
            <link>https://theengineeringmanager.substack.com/p/should-managers-still-code</link>
            <guid>43256113</guid>
            <pubDate>Tue, 04 Mar 2025 15:41:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theengineeringmanager.substack.com/p/should-managers-still-code">https://theengineeringmanager.substack.com/p/should-managers-still-code</a>, See on <a href="https://news.ycombinator.com/item?id=43256113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>This month we have a mailbag question from a reader who asks:</p><blockquote><p>Hi James,</p><p><span>Your newsletter </span><a href="https://www.theengineeringmanager.com/managing-managers/being-in-the-details/" rel="">"being in the details"</a><span> resonated with me.</span></p><p>I would like to ask your opinion about managers writing code. I skim PRs but don’t critically review them and don’t write code. Should EMs write code in their day job?</p><p>Thanks!</p></blockquote><p>It's a great question, and given the higher scrutiny we've seen on the role of engineering managers in the past few years, it's worth spending some time on it.</p><p><span>Before we go any deeper, the short answer is that it depends exactly on what you mean by coding. I think that there is a big difference between being </span><em>in</em><span> the code and </span><em>writing</em><span> code. All managers should be in the code, but not all managers should be writing code.</span></p><p><span>But spending some time digging into the nuances of the question can, I think, highlight both the practical aspects of ensuring that managers are sufficiently </span><em>in</em><span> the code, and, more importantly, identify the existential worry that many engineering managers may have about their role, and what they can do about it.</span></p><p>Let's get going.</p><p><span>We'll begin by revisiting Andy Grove's equation for measuring a manager's impact, which states that </span><em>the output of a manager is the output of their team, plus the output of the neighboring teams under their influence</em><span>. This is always useful to refer to when thinking about how to spend your time. I contemplate it a lot.</span></p><p>There are, of course, a multitude of ways to increase your output as a manager. These range from, and are not limited to:</p><ul><li><p>Hiring and retaining great people.</p></li><li><p>Owning the team's strategy and roadmap, and ensuring efficient execution.</p></li><li><p>Making decisions to ensure that the team is working on the right things and saying no to the things that don't matter.</p></li><li><p>Dealing with fires, escalations, and other crises that pop up all of the time.</p></li><li><p>Building a strong culture within the team so that people are engaged, challenged, and motivated.</p></li><li><p>Mentoring and coaching your reports so they get better and can have more work delegated to them, thus increasing output further.</p></li><li><p>Managing the team's stakeholders so they can offer their steer to the team early and often.</p></li><li><p>Actively performance managing the team so that superstars can continue to shine and underperformers can be coached or exited.</p></li><li><p>Building close working relationships with other teams so that smooth collaboration happens across the organization, leading to a better and more cohesive product.</p></li></ul><p>And so on.</p><p><span>Now, it isn't a stretch to say that engineering managers, who typically get to a senior individual contributor level before transitioning to management, are </span><em>also</em><span> often very good at writing code.</span></p><p><span>However, there's a whole </span><em>team</em><span> of people working for them who are </span><em>also</em><span> very good at writing code, and typically they won't be spending as much time on the items listed above, which means they can be more productive at writing code than the manager can.</span></p><p><span>So, therefore, surely it makes sense for managers to focus on the things that </span><em>only they can do</em><span>, and leave the coding to the people who have more time for it and are better at it as a result?</span></p><p>Right?</p><p>Or is there something else at play here?</p><p>The last few years have created somewhat of an existential crisis for engineering managers.</p><p><span>Since the tail end of the Covid-19 pandemic, where the </span><a href="https://www.theengineeringmanager.com/growth/2024-year-in-review/" rel="">zero-percent interest rate environment ended</a><span>, leading to high inflation and interest rates, the end of cheap debt and plentiful investment, and the consequential slowdown in growth and corrective layoffs at many technology companies, managers have </span><a href="https://www.businessinsider.com/middle-manager-hiring-white-collar-recession-layoffs-jobs-efficiency-2024-12" rel="">felt the brunt of the "great flattening"</a><span>, where the number of managers has been reduced in favor of more individual contributors.</span></p><p>This has typically meant that the average number of direct reports per manager has increased, the amount of scope per manager has increased too, and the number of total layers in the org chart has shrunk.</p><p>This, therefore, feeds into a narrative that managers worth their salt need to be more productive, more impactful, and more efficient than before.</p><p>Depending on where you read your opinions on the internet, It can also build a more toxic picture: that managers are just unnecessary overhead for having some number of individual contributors; that it is a non-technical job for those that aren't as good as others at actually building the product.</p><p>Hmm.</p><p><span>For those that made a conscious decision to move into management and learn this new role, and especially for those who take their craft </span><em>seriously</em><span> as managers, it can feel like the tide has turned against them.</span></p><p>As such, engineering managers, and the organizations that they work for, are pushing for ways for managers to differentiate themselves, typically by being more technical, more hands-on, and more in the details than they may have been before.</p><p><span>This isn't a bad thing. When done right, it can be extremely beneficial. I wrote extensively about </span><a href="https://www.theengineeringmanager.com/managing-managers/being-in-the-details/" rel="">being in the details</a><span> from a senior leadership perspective.</span></p><p><span>I would argue that being in the details is the key tenet of not just being a great manager in the climate that we find ourselves in, but also being a great manager </span><em>full stop</em><span>.</span></p><p>For senior leaders such as Directors and VPs, being in the details covers ideas such as having ICs report to you, as well as managers; doing regular, hands-on deep dives into the architecture and codebases your team owns; mixing up 1:1s with pair programming sessions, code reviews, and other technical activities, and more.</p><p>But what does this mean for frontline engineering managers? Is the new normal just about writing more code and doing less of the other things that peacetime managers would normally do?</p><p>It's more nuanced than that.</p><p>Here is a list of statements that represent how I would want my engineering managers to be in terms of their relationship with the codebase:</p><ul><li><p><span>Should they be </span><em>able</em><span> to write code? Yes.</span></p></li><li><p>Should they understand how the codebase and their features and services are built? Yes.</p></li><li><p>Should they be able to do code reviews? Yes.</p></li><li><p>Should they review all design documents and architecture proposals from their team? Yes.</p></li><li><p>Should they be able to debug and triage production issues? Yes.</p></li><li><p>Should they be able to pair program with their reports? Yes.</p></li><li><p>Should they be accountable for the quality of the code that their team produces? Yes.</p></li><li><p><span>Should they write code themselves? </span><em>Maybe.</em></p></li></ul><p>Why maybe?</p><p><span>It depends on the manager, the team, and the organization. As a senior leader, I would rather my managers be </span><em>in</em><span> the code as per the above list, but not necessarily putting themselves in the critical path by </span><em>writing</em><span> code, given that they are likely to be interrupted more often, have more meetings, and be pulled in more directions than their reports.</span></p><p>I expect you to know how everything works, insofar that if I asked you to show me how a feature works by tracing through the code, you could do it. However, I would also know that there are other people on the team that are better placed to be the primary implementers of features in the area that you understand really well.</p><p><span>But, if you are a manager that is absolutely </span><em>itching</em><span> to write code and stay close to the details, then so be it and more power to you.</span></p><p>Here's some approaches that might work for you:</p><ul><li><p><strong>Explicitly set aside uninterrupted time for coding.</strong><span> This could be a day a week, or a few hours a day, or whatever works for your team. Make sure that your team knows that you are doing this, to keep interruptions to a minimum. Block it out in your calendar, and set your statuses accordingly.</span></p></li><li><p><strong>Pair program with your reports.</strong><span> This is a great way to get into the code, and also to mentor your reports at the same time. It's a win-win. Sure, you're not taking the lead on the coding yourself, but you're in the details by proxy of collaborating with others. I love doing this myself.</span></p></li><li><p><strong>Do code reviews.</strong><span> Don't just skim PRs (sorry, reader!), but really dig into them: run the branch locally, test it, think critically about the design and the implementation, and provide feedback. Record a video of your review to highlight things that could be better.</span></p></li><li><p><strong>Increase your coding involvement during specific occasions.</strong><span> Depending on how you and your team work, you may find that prototyping phases are where you can get your hands dirty effectively. That code isn't going into production, so go to town! Alternatively, you might be great at digging in during incidents or times of high stress. Find the times that work for you and your team, and lean into them. This approach isn't about spending a consistent amount of time coding every week; it's about finding the </span><em>right</em><span> times to be in the code where you can be most effective.</span></p></li><li><p><strong>Find time for exploratory coding that expands the knowledge of your team.</strong><span> Block out some regular time to stay on top of the latest technologies in a way that is fun and engaging for you, sharpens your skills, but also brings back learnings to your team. For example, if your team uses Apache Flink and the latest version has new aggregation functionality that could make your pipelines simpler, spend a couple of hours doing a prototype with it for a team-specific use case and then share it back.</span></p></li></ul><p>Going back to the original question: should managers write code? I think it's best to rephrase that.</p><p><span>Should managers be </span><em>in</em><span> the code? Yes, absolutely.</span></p><p><span>Should managers </span><em>write</em><span> code? Maybe, but it also depends on what you mean by writing code.</span></p><p>If you mean being the primary implementer of features, then probably not. If you mean being an integral part of how your team produces code, then yes, absolutely. I recommend it highly.</p><p>So there we are.</p><p>I enjoyed doing this month's article as a mailbag question. If you ever have a question that you think would make a good article, please send it in! I'd love to hear from you, and I'll try my best to answer it.</p><p>Happy coding (in some form), managers.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bayleaf · Building a low-profile wireless split keyboard (551 pts)]]></title>
            <link>https://www.graz.io/articles/bayleaf-wireless-keyboard</link>
            <guid>43255529</guid>
            <pubDate>Tue, 04 Mar 2025 15:00:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.graz.io/articles/bayleaf-wireless-keyboard">https://www.graz.io/articles/bayleaf-wireless-keyboard</a>, See on <a href="https://news.ycombinator.com/item?id=43255529">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[DIY "infinity contrast" TV – with 100% recycled parts [video] (107 pts)]]></title>
            <link>https://www.youtube.com/watch?v=qXrn4MqY1Wo</link>
            <guid>43255446</guid>
            <pubDate>Tue, 04 Mar 2025 14:55:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=qXrn4MqY1Wo">https://www.youtube.com/watch?v=qXrn4MqY1Wo</a>, See on <a href="https://news.ycombinator.com/item?id=43255446">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[What a crab sees before it gets eaten by a cuttlefish (132 pts)]]></title>
            <link>https://www.nytimes.com/2025/03/03/science/cuttlefish-camouflage-huting-crabs.html</link>
            <guid>43254995</guid>
            <pubDate>Tue, 04 Mar 2025 14:24:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/03/03/science/cuttlefish-camouflage-huting-crabs.html">https://www.nytimes.com/2025/03/03/science/cuttlefish-camouflage-huting-crabs.html</a>, See on <a href="https://news.ycombinator.com/item?id=43254995">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/03/03/science/cuttlefish-camouflage-huting-crabs.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla electric car sales plunge again in Australia – Model 3 down more than 81 p (101 pts)]]></title>
            <link>https://thedriven.io/2025/03/04/tesla-electric-car-sales-plunge-again-in-australia-model-3-down-more-than-81-per-cent/</link>
            <guid>43254637</guid>
            <pubDate>Tue, 04 Mar 2025 13:57:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thedriven.io/2025/03/04/tesla-electric-car-sales-plunge-again-in-australia-model-3-down-more-than-81-per-cent/">https://thedriven.io/2025/03/04/tesla-electric-car-sales-plunge-again-in-australia-model-3-down-more-than-81-per-cent/</a>, See on <a href="https://news.ycombinator.com/item?id=43254637">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

										
			
							<section>

								<p>The plunge in Tesla electric vehicle sales has continued into February, according to the latest official data, with combined sales of the Model Y and Model 3 EVs plunging 71.9 per cent in the month of February, compared to the same month a year earlier.</p>
<p>The data from the Electric Vehicle Council shows that Tesla recorded just 1,592 EV sales in February, down from &nbsp;5,665 for &nbsp;February last year. For the first two months of the year, sales have slumped 66 per cent to 2,331 from 6,772 in 2024.</p>
<p>Tesla supporters insist the sales plunge – which <a href="https://thedriven.io/2025/03/04/tesla-sales-continue-to-plummet-across-europe-despite-overall-ev-market-growth/">is also intense in European countries</a> – is only the result of inventory levels and customers waiting for refreshed Model Y, and some increased competition.</p>
<p>But most analysts and observers also point to the influence that CEO Elon Musk is having on the market because of his partnership with US president Donald Trump and his open support for far right political causes.</p>
<p>In Australia, sales of the Model Y fell to 924 in February from 2,072 in February last year. If that were to be the result only of inventory issues and customers waiting for the refreshed Model Y, it does not explain the 81.4 per cent fall in Model 3 sales to just 668 units in February, from 3,593 in the same month of 2024, and 2,671 in February, 2023.</p>
<p>It could also be that the Model 3 is simply past its use-by date, particularly with the arrival of competition from China – such as the BYD Seal.</p>

<figure id="attachment_185703" aria-describedby="caption-attachment-185703"><a href="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales.jpg?lossy=1&amp;strip=0&amp;webp=1"><img loading="lazy" decoding="async" src="https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-1160x375.jpg" alt="" width="1160" height="375" srcset="https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-1160x375.jpg 1160w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-388x126.jpg 388w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-800x259.jpg 800w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-1536x497.jpg 1536w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-120x39.jpg 120w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-90x29.jpg 90w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-320x104.jpg 320w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-560x181.jpg 560w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-150x49.jpg 150w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-240x78.jpg 240w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-180x58.jpg 180w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-640x207.jpg 640w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-1120x362.jpg 1120w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-1600x518.jpg 1600w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-300x97.jpg 300w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales.jpg 1854w" sizes="(max-width: 1160px) 100vw, 1160px" data-old-src="https://thedriven.io/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-1160x375.jpg?lossy=1&amp;strip=0&amp;webp=1" data-srcset="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-1160x375.jpg?lossy=1&amp;strip=0&amp;webp=1 1160w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-388x126.jpg?lossy=1&amp;strip=0&amp;webp=1 388w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-800x259.jpg?lossy=1&amp;strip=0&amp;webp=1 800w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-1536x497.jpg?lossy=1&amp;strip=0&amp;webp=1 1536w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-120x39.jpg?lossy=1&amp;strip=0&amp;webp=1 120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-90x29.jpg?lossy=1&amp;strip=0&amp;webp=1 90w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-320x104.jpg?lossy=1&amp;strip=0&amp;webp=1 320w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-560x181.jpg?lossy=1&amp;strip=0&amp;webp=1 560w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-150x49.jpg?lossy=1&amp;strip=0&amp;webp=1 150w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-240x78.jpg?lossy=1&amp;strip=0&amp;webp=1 240w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-180x58.jpg?lossy=1&amp;strip=0&amp;webp=1 180w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-640x207.jpg?lossy=1&amp;strip=0&amp;webp=1 640w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-1120x362.jpg?lossy=1&amp;strip=0&amp;webp=1 1120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-1600x518.jpg?lossy=1&amp;strip=0&amp;webp=1 1600w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-300x97.jpg?lossy=1&amp;strip=0&amp;webp=1 300w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales.jpg?lossy=1&amp;strip=0&amp;webp=1 1854w"></a><figcaption id="caption-attachment-185703">Source: Electric Vehicle Council.</figcaption></figure>
<p>Polestar is also seeing sales of its Polestar 2 electric fasts-back – once seen as a rival to the M3 – slump sharply, down to just 36 from 112 a year earlier.</p>
<p>But Polestar is also rolling out a number of new models, and the EVC data reveals that its overall sales are up 11.6 per cent over the same month a year earlier. This has been driven by the newly launched Polestar 4 has achieved 83 sales, taking its year to date total to 144, while the more expensive Polestar 3 achieved just 6 sales in February, taking its year to date sales to 8.</p>
<p>The health of the overall EV market in Australia will be revealed in more detail on Wednesday with the release of data from other car makers via the main car lobby, the FCAI. Tesla and Polestar left the FCAI last year because of differences about policies, including the federal government’s new vehicle emissions standards.</p>
<p>Tesla, however, has an outsize influence on the Australian market, having accounted for well over half of all EV sales in recent years, although the emergence of lower cost Chinese EVs ate away some of that market dominance in 2024.</p>
<p>In Europe, where the EV market is resurgent – Tesla EVs account for between 5 and 15 per cent of individual country markets. Its sales, however,<a href="https://thedriven.io/2025/03/04/tesla-sales-continue-to-plummet-across-europe-despite-overall-ev-market-growth/"> are also plunging badly</a>, amid community push back against his support of the far right AfD party in Germany, and his intervention on behalf of far right politicians and individuals in the UK and elsewhere.</p>
<p>In the US, demonstrations were held at more than 50 Tesla dealerships over the weekend, according to CNN, while in Canada more than 300,000 people have signed a petition calling on the government to rescind Musk’s citizenship, offended by his statements that include a declaration that Canada is “not a real country.”</p>
<p>Tesla stock has also fallen sharply, reversing almost all of the gains it made in the post Trump election victory euphoria. That are now growing calls for the Tesla board to consider replacing Musk, although there is no sign they intend to do that and are more interested in selling their shares.</p>
<p>According to Electrek, chairperson Robyn Denholm, an Australia, has sold more Tesla stock, <a href="https://electrek.co/2025/03/03/tesla-chairwoman-sells-33-million-worth-of-tsla-as-she-lets-elon-musk-destroy-the-brand/">taking her total sales over the last</a> three months (since the Trump election) to more than $US100 million.</p>
<p>Kimball Musk, Elon’s brother, and Tesla’s Chief Financial Officer Taneja Vaibhav also recently sold ahead of a recent drop in the company’s stock price. See: <a href="https://electrek.co/2025/03/03/tesla-chairwoman-sells-33-million-worth-of-tsla-as-she-lets-elon-musk-destroy-the-brand/">Tesla chairwoman sells $33 million worth of Tesla stock as she lets Elon Musk destroy the brand</a>.</p>

<p>The FCAI data is due to be released on Wednesday.</p>
<p>See also:&nbsp;<a href="https://thedriven.io/2025/03/04/tesla-sales-continue-to-plummet-across-europe-despite-overall-ev-market-growth/">Tesla sales continue to plummet across Europe despite overall EV market growth</a></p>
<p>And:&nbsp;<a href="https://thedriven.io/2025/03/04/australian-electric-vehicle-sales-by-month-and-by-model-in-2025/">Australian electric vehicle sales by month and by model in 2025</a></p>
<div itemtype="http://schema.org/Person" itemscope="" itemprop="author"><p><img loading="lazy" decoding="async" src="https://thedriven.io/wp-content/uploads/2021/11/giles_100x100.jpg" width="100" height="100" alt="giles parkinson" itemprop="image" data-old-src="https://thedriven.io/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2021/11/giles_100x100.jpg?lossy=1&amp;strip=0&amp;webp=1"></p><div><p>Giles Parkinson is founder and editor of <a href="https://thedriven.io/">The Driven</a>, and also edits and founded the <a href="https://reneweconomy.com.au/">Renew Economy</a> and <a href="http://onestepoffthegrid.com.au/">One Step Off The Grid</a>&nbsp;web sites. He has been a journalist for nearly 40 years, is a former business and deputy editor of the Australian Financial Review, and owns a Tesla Model 3.</p></div></div><!-- AI CONTENT END 1 -->

								
							</section>

										
			
						</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Fork of Claude-code working with local and other LLM providers (115 pts)]]></title>
            <link>https://github.com/dnakov/anon-kode</link>
            <guid>43254351</guid>
            <pubDate>Tue, 04 Mar 2025 13:35:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/dnakov/anon-kode">https://github.com/dnakov/anon-kode</a>, See on <a href="https://news.ycombinator.com/item?id=43254351">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-turbo-body="">
      


    <div>
      <p><a href="#start-of-content" data-skip-target-assigned="false">Skip to content</a>

      <span data-view-component="true">
    <span data-view-component="true"></span>
</span></p>

<react-partial partial-name="keyboard-shortcuts-dialog" data-ssr="false" data-attempted-ssr="false">
  
  
  
</react-partial>




      

          

              


<header role="banner" data-is-top="true" data-color-mode="light" data-light-theme="light" data-dark-theme="dark">
  <h2>Navigation Menu</h2>

  

  <div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>GitHub Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;security_link_product_navbar&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code Review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
        <p>Code Search</p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>

                  <li>
      
      
</li>

                    <li>
      
      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      Events &amp; Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      
      <div>
              <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      
      <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
        <p>Enterprise platform</p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:dnakov/anon-kode" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="dhVu3lFe87tcSfb0aFuQZy5s6OlGMzM36mCAR1Ha-5PW5WxTpmescQov-E7t1gv8ZvslpEJf-rZrk7DwfkfWvw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="dnakov/anon-kode" data-current-org="" data-current-owner="dnakov" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=dnakov%2Fanon-kode" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/dnakov/anon-kode&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="4dbd6159b652241c5c897196051dc18c8414e63dd8f77e4148009093278bef06" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a>
          
        </p></div>
      </div>
</header>

      
    </div>

  








    


    






  <div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="" data-project-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  





    






  
  

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div data-view-component="true" id="repo-content-pjax-container">      











<react-partial partial-name="repos-overview" data-ssr="true" data-attempted-ssr="true">
  
  
  <div data-hpc="true" data-target="react-partial.reactRoot"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">ANON KODE</h2><a id="user-content-anon-kode" aria-label="Permalink: ANON KODE" href="#anon-kode"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description anon-kode.mov">anon-kode.mov</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/3777433/419028277-7a9253a7-8bb0-40d5-a3f3-5e6096d7c789.mov?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDExMzg1MDgsIm5iZiI6MTc0MTEzODIwOCwicGF0aCI6Ii8zNzc3NDMzLzQxOTAyODI3Ny03YTkyNTNhNy04YmIwLTQwZDUtYTNmMy01ZTYwOTZkN2M3ODkubW92P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDMwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTAzMDVUMDEzMDA4WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OWJjMjZjNjE2MzZhMTdjYTRmMTdiZTczY2NkMDM4M2NkNzc0ZjNlNzZmMTlhMzUwY2Y0ZGM1ZWZiZTE5ODk3MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.akFo9ApY6zqPoggOt2O4iAJOCyZQr35f4DUDyt3PHjg" data-canonical-src="https://private-user-images.githubusercontent.com/3777433/419028277-7a9253a7-8bb0-40d5-a3f3-5e6096d7c789.mov?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDExMzg1MDgsIm5iZiI6MTc0MTEzODIwOCwicGF0aCI6Ii8zNzc3NDMzLzQxOTAyODI3Ny03YTkyNTNhNy04YmIwLTQwZDUtYTNmMy01ZTYwOTZkN2M3ODkubW92P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDMwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTAzMDVUMDEzMDA4WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OWJjMjZjNjE2MzZhMTdjYTRmMTdiZTczY2NkMDM4M2NkNzc0ZjNlNzZmMTlhMzUwY2Y0ZGM1ZWZiZTE5ODk3MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.akFo9ApY6zqPoggOt2O4iAJOCyZQr35f4DUDyt3PHjg" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Terminal-based AI coding tool that can use any model that supports the OpenAI-style API.</p>
<ul dir="auto">
<li>Fixes your spaghetti code</li>
<li>Explains wtf that function does</li>
<li>Runs tests, shell commands and stuff</li>
<li>Whatever else claude-code can do, depending on the model you use</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">HOW TO USE</h2><a id="user-content-how-to-use" aria-label="Permalink: HOW TO USE" href="#how-to-use"></a></p>
<div data-snippet-clipboard-copy-content="npm install -g anon-kode
cd your-project
kode"><pre><code>npm install -g anon-kode
cd your-project
kode
</code></pre></div>
<p dir="auto">You can use the onboarding to set up the model, or <code>/model</code>.
If you don't see the models you want on the list, you can manually set them in <code>/config</code>
As long as you have an openai-like endpoint, it should work.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Warning</h2><a id="user-content-warning" aria-label="Permalink: Warning" href="#warning"></a></p>
<p dir="auto">Use at own risk.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">YOUR DATA</h2><a id="user-content-your-data" aria-label="Permalink: YOUR DATA" href="#your-data"></a></p>
<ul dir="auto">
<li>There's no telemetry or backend servers other than the AI providers you choose</li>
</ul>
</article></div>
</react-partial>

      </div>

</turbo-frame>


    </main>
  </div>

          



    <ghcc-consent id="ghcc" data-initial-cookie-consent-allowed="" data-cookie-consent-required="true"></ghcc-consent>



  

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Federal workers ordered to return to offices without desks, Wi-Fi and lights (279 pts)]]></title>
            <link>https://www.cnn.com/2025/03/04/politics/federal-employees-return-to-office-problems/index.html</link>
            <guid>43253562</guid>
            <pubDate>Tue, 04 Mar 2025 12:14:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2025/03/04/politics/federal-employees-return-to-office-problems/index.html">https://www.cnn.com/2025/03/04/politics/federal-employees-return-to-office-problems/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=43253562">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-editable="content" itemprop="articleBody" data-reorderable="content">
                <p><cite>
      <span data-editable="location"></span>
      <span data-editable="source">CNN</span>
        &nbsp;—&nbsp;
    </cite>
</p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thliwo000v2cozgh2xbdev@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Millions of federal workers were <a href="https://www.cnn.com/2025/01/23/business/trump-federal-workers-rto-mandate/index.html">ordered to return to offices</a> across the country in recent weeks, marking an end to Covid-era rules allowing more flexibility to work from home.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thlk6200003b6m4ysl92ra@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Many have come back to workplaces that weren’t ready for them.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thv5ao00083b6mw2qej22u@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            In one Department of Health and Human Services office, there was no Wi-Fi or full electricity in the first hours when people returned last week.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok3000e3b6mmdfnpnek@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Department of Education employees at an office in Dallas returned to ethernet cords in piles around the floor, random wires sticking out of walls, and motion-sensor lights that weren’t working correctly, leading to dark workspaces. One employee tripped over a pile of cords on her first day back, resulting in a large gash on her foot. She’s submitted a workers’ compensation complaint.
    </p>

  
<div data-image-variation="image_inline-small" data-breakpoints="{&quot;image_inline-small--eq-extra-small&quot;: 115, &quot;image_inline-small--eq-small&quot;: 300, &quot;image_inline-small--eq-large&quot;: 660}" data-uri="cms.cnn.com/_components/image/instances/cm7ti30t500013b6mvbtdzglx@published" data-name="c-IMG_2545.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.666875" data-original-height="1067" data-original-width="1600" data-url="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?c=original" data-editable="settings">
       <picture><source height="1067" width="1600" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?q=w_256,c_fill/f_webp" type="image/webp"><source height="1067" width="1600" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?q=w_256,c_fill/f_webp" type="image/webp"><source height="1067" width="1600" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?q=w_256,c_fill/f_webp" type="image/webp"><source height="1067" width="1600" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?q=w_680,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?q=w_256,c_fill" alt="Ethernet cords lay on the floor at a Department of Education office in Dallas. " onload="this.classList.remove('image_inline-small__dam-img--loading')" onerror="imageLoadError(this)" height="1067" width="1600" loading="lazy"></picture>
    </div>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000f3b6mlbe4g8bi@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            And a Department of Defense employee who returned to in-office work and handles sensitive information was stuck in a conference room with people on different teams, forcing them to leave the room to make calls. The employee was eventually moved to an office — but one without Wi-Fi, so they had to use their phone’s spotty hot spot.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000g3b6mvpdz4pmj@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “The only thing a return to the office has given me is an hour of traffic while driving and a loss in efficiency,” said the worker, who requested anonymity for fear of job reprisals.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000h3b6my9a5k5lc@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The problems, confusion and slipups that federal employees told CNN they’ve encountered returning to the office have only added to the chaos inside the workforce six weeks into a Trump administration determined to <a href="https://www.cnn.com/politics/tracking-federal-workforce-firings-dg/index.html">slash the size and scope</a> of the federal government.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000i3b6m1ztsyzip@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Some federal workers being told to return to the office have no space to return to. At least two office buildings used by the Interior Department in the Western US were told last week their leases had been canceled, according to a source familiar with the matter — while a third office housing hundreds of people was notified its lease will be up in June.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000j3b6maute1e0f@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            One source said the General Services Administration, which manages federal buildings, did not appear to coordinate the lease terminations with Interior officials, leaving employees unclear of what to do. An Interior spokesperson said the department was “working with GSA to ensure facilities or alternative options will be available” for employees.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000k3b6myjlfpb4f@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            President <a href="https://www.cnn.com/politics/president-donald-trump-47">Donald Trump</a> has repeatedly demanded all federal workers return to the office as his administration has undertaken sweeping actions seeking to drastically reduce the size of the federal workforce, including mass firings of probationary workers and employees in government diversity departments.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000l3b6mpk1596b3@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Trump’s in-office mandate has been coupled with a push to slash government real estate, setting up a dilemma of too little space for too many people. Even before Trump took office in January, the federal government was downsizing office space due in part to the shift toward telework during the pandemic.
    </p>

  





    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000m3b6m9c6j2qe2@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Nearly half of the more than 2.3 million civilian federal workers were eligible for telework, and 10% were in remote positions with no expectation of in-person work, according to a 2024 Office of Management and Budget report.<strong> </strong>Many workers stopped working full time in their offices during the Covid-19 pandemic, but others had long-held arrangements for working from home<strong> </strong>— and now could face the prospect of choosing between fundamentally changing their job or leaving it altogether.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000n3b6m0hjimos2@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            More than 80% of federal workers reside outside the Washington, DC, metro area, meaning the return-to-office mandate will cause ripple effects across the country.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000o3b6m93ver5jt@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Some government employees are now making the post-pandemic transition that millions of private-sector workers already have — upending their lives and schedules in the process. For many federal employees, however, those worries have been subsumed by their larger fear of losing jobs entirely in Trump’s multipronged effort to shrink the federal workforce.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000p3b6m4evksl91@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Federal workers and union officials told CNN they see this as part of an attempt by the Trump administration to <a href="https://www.propublica.org/article/video-donald-trump-russ-vought-center-renewing-america-maga" target="_blank">make life uncomfortable</a> for federal workers in hopes that some will quit.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000q3b6mquhadaqy@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Trump and Elon Musk, de facto chief of the Department of Government Efficiency, have both threatened to fire workers who do not come back to the office, even those represented by unions who have signed long-term telework agreements. Many federal agencies set February 24 as the first date for some of their employees to comply with Trump’s return-to-office directive.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000r3b6mvsxcej23@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “If they don’t report for work, we’re firing them. In other words, you have to go to office,” Trump said at a conservative political conference last month, while claiming that his <a href="https://www.cnn.com/2025/02/18/politics/mar-a-lago-trump-remote-work-golf/index.html">golf game would improve dramatically</a> if he were to work remotely.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000s3b6mjad0bg9i@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Federal employee unions have argued Trump’s demand to break long-term telework agreements <a href="https://www.cnn.com/2025/02/07/politics/trump-musk-federal-workforce/index.html">is unlawful</a>, though the return-to-office mandate isn’t central to the major legal challenges unions have brought against the president. But there have been pockets of pushback, such as the employee union at the Environmental Protection Agency, which is demanding to bargain around the return-to-office mandate. Additionally, some individual EPA employees represented by the union are filing grievances around returning to the office.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000t3b6moq891utg@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            One of their biggest concerns, an EPA union official told CNN, is “local fire and safety regulations — how many people can you jam into a room?”
    </p>

  

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000v3b6m57ha4i02@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            DOGE has kept a running list on its website touting more than 200 building leases the Musk-run agency says it’s canceled. The canceled leases, which include Social Security Administration and US attorneys’ offices, have <a href="https://www.kristv.com/news/local-news/d-o-g-e-terminates-lease-for-local-u-s-attorneys-office" target="_blank">raised questions locally</a> about where those employees are supposed to go.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000w3b6mla7qqoax@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Trump also signed an executive order last week instructing each government agency to identify all leases that can be terminated and submit a plan to dispose of “government-owned real property which has been deemed by the agency as no longer needed.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000x3b6mj4swzprz@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The drive to shrink the federal government’s real estate portfolio predates the Trump administration. The Office of Management and Budget issued a “Reduce the Footprint” directive in 2015 that requires agencies to make more efficient use of federal property and dispose of surplus assets.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000y3b6mlikxf8fh@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Last year’s OMB report included a list of agencies’ efforts to downsize their holdings. For instance, the Department of Veterans Affairs reduced office space in its headquarters locations in the Washington, DC, metro area by 16% between 2020 and 2022, saving $15 million annually.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000z3b6mrrjwv4ij@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The GSA cut its own footprint by more than 2 million square feet over 10 years, avoiding $300 million in costs. And the Department of Energy moved out of 193,000 square feet of leased space in 2023, saving $9 million annually.
    </p>

<div data-image-variation="image_large" data-breakpoints="{&quot;image_large--eq-extra-small&quot;: 115, &quot;image_large--eq-small&quot;: 300, &quot;image_large--eq-large&quot;: 660}" data-uri="cms.cnn.com/_components/image/instances/cm7ti3k1r00033b6m5wj2k31k@published" data-name="c-GettyImages-2201176517.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.6666666666666666" data-original-height="1600" data-original-width="2400" data-url="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?c=original" data-editable="settings">
       <picture><source height="1600" width="2400" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?q=w_1110,c_fill/f_webp" type="image/webp"><source height="1600" width="2400" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?q=w_1015,c_fill/f_webp" type="image/webp"><source height="1600" width="2400" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?q=w_1160,c_fill/f_webp" type="image/webp"><source height="1600" width="2400" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?q=w_680,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?q=w_1110,c_fill" alt="A pedestrian near a General Services Administration building in Washington, DC, on Monday, February 24. " onload="this.classList.remove('image_large__dam-img--loading')" onerror="imageLoadError(this)" height="1600" width="2400" loading="lazy"></picture>
    </div>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400103b6mb5d7s02n@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Those downsizings have had an impact. One federal employee still waiting for their date to return to the office told CNN they suspect it’s been delayed because of a lack of room. Their agency, which the employee asked not to name because of concerns for their job, has been shedding office space for several years, so teleworking staffers have had to reserve desks in the remaining locations for the days they come in.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400113b6moszzk9vr@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The space constraint, coupled with the Trump administration’s <a href="https://www.cnn.com/2025/02/26/politics/federal-mass-layoffs-trump-memo/index.html">reduction-in-force order</a>, has the worker fearing their agency will suffer heavy layoffs. “The only way RTO (return-to-office) works in these types of situations is if you now reduce the number of people,” the employee said.
    </p>

  

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400133b6myk6mc303@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Multiple federal agencies brought the bulk of their employees back last week, a return that was met in some cases with a lack of desks, basic supplies and working equipment.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400143b6mo8c8mzg8@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “There was very little prep and planning and it was messy with equipment,” an HHS staffer told CNN. The employee, who asked for their specific location not to be named, said there were reports of Wi-Fi and electricity not working.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7tzbkg60000356mwlpg5w7y@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Asked about the issue, Andrew Nixon, HHS’ communications director, said the agency is complying with Trump’s return to the office executive order. “We look forward to seeing and collaborating with our colleagues in person to Make America Healthy Again,” he continued.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400153b6mwhuve4yt@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The Department of Education sent an email last week to staff in regional offices acknowledging the shortcomings in some facilities on Day 1 of their return.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400163b6mbx9ev3n5@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “At the present time we are unable to deploy full peripheral setups in the regional/remote offices,” the department’s Office of the Chief Information Officer told employees in an email, which was obtained by CNN.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400173b6mmbs9f9rw@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Sheria Smith, president of the American Federation of Government Employees Local 252 in Dallas and a Department of Education employee, said her office was “chaos” when employees returned. She filed the workers’ comp complaint after injuring her foot tripping over a pile of cords on the floor.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400183b6mgniynv3n@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “The facilities were not actually ready for us to return,” Smith said.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400193b6m4wt9l8ci@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “No one is on-site to try to fix the issues,” she added, saying the mess leads her to believe “they would be hoping that we would quit, I guess — that they didn’t expect us to come.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001a3b6mit76tbc8@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Another Department of Education employee in Washington, DC, said the first week back lacked basic office needs: computers, pens and headsets — as well as private space and conference rooms necessary for confidential job requirements.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001b3b6m0l2xr20t@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            At the National Oceanic and Atmospheric Administration headquarters in Silver Spring, Maryland, a telework policy has been in place for more than 20 years, officials said. Many employees had been teleworking three to four days a week and are now adapting to the rigidity of returning to the office.
    </p>

<div data-image-variation="image_large" data-breakpoints="{&quot;image_large--eq-extra-small&quot;: 115, &quot;image_large--eq-small&quot;: 300, &quot;image_large--eq-large&quot;: 660}" data-uri="cms.cnn.com/_components/image/instances/cm7ti3w9800053b6m4thchr42@published" data-name="c-GettyImages-2203083573.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.6670833333333334" data-original-height="1601" data-original-width="2400" data-url="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?c=original" data-editable="settings">
       <picture><source height="1601" width="2400" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?q=w_1110,c_fill/f_webp" type="image/webp"><source height="1601" width="2400" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?q=w_1015,c_fill/f_webp" type="image/webp"><source height="1601" width="2400" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?q=w_1160,c_fill/f_webp" type="image/webp"><source height="1601" width="2400" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?q=w_680,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?q=w_1110,c_fill" alt="Hundreds of demonstrators gather to protest against Department of Government Efficiency (DOGE) cuts outside the headquarters of the National Oceanic and Atmospheric Administration on March 3, in Silver Spring, Maryland." onload="this.classList.remove('image_large__dam-img--loading')" onerror="imageLoadError(this)" height="1601" width="2400" loading="lazy"></picture>
    </div>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001c3b6mt5ajyxux@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “Morale is pretty low,” a staffer in the agency said. “If you have a dentist appointment at 3 p.m. near where you live, and it ends at 4:15 p.m., you cannot work that last hour from home. You have to go back to the office or take sick leave.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001d3b6m8sdzmo6f@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The return-to-office mandate comes as budget cuts at EPA led to reduced cleaning and facility services at key offices, even before the Trump administration took over.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001e3b6mokfltdj8@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            In the three major EPA offices around the country that make up the agency’s headquarters — Washington, Cincinnati and Research Triangle Park, North Carolina — health units were closed and in-office mail delivery was cut back, according to a memo obtained by CNN. In Cincinnati, drinking fountain cleaning schedules were reduced to once per week, carpets and hallways were swept once a month, and bathroom cleanings were pared back to once a day.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001f3b6m81n56i52@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “With these people coming back to the offices, they’re going to have fewer facility services,” the EPA union official said.
    </p>

  

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001h3b6mxf5m3uln@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The 2024 OMB report found those eligible for telework spend about 60% of their working hours in the office, on average, though that figure varies widely by agency. About 10% of staffers have remote jobs, where they are not expected to report to an office at all, according to the report, which noted the Biden administration also directed agencies to increase the amount of time federal workers spent at the office.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001i3b6m9ga6xl82@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The Biden administration’s goal was for teleworkers to spend at least 50% of the time in the office. Now Trump turned that into a full-time mandate.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001j3b6mw8ryoj8k@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            But not all federal workers are returning at the same pace, as Trump’s executive order, issued hours after he took office in January, directed agencies to terminate remote work arrangements “as soon as practicable.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001k3b6movkqhmp4@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Several agencies, including the departments of Veterans Affairs and Health and Human Services, have said political appointees, senior executives and other senior staffers could no longer work remotely or telework as of February 24, and neither could supervisors who live within 50 miles of an agency facility.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001l3b6mqhwzi9zp@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Others have more time. For instance, at the VA, lower-level non-union employees who live within 50 miles of a facility will have their remote and telework arrangements terminated by April 28, except for ad hoc or situational circumstances, the agency said. But these arrangements for supervisors and other non-union employees who reside farther away were not ended — those staffers will receive additional guidance.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001m3b6mcycmncdy@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            As for union workers at the VA, their return-to-office date will be announced at a later time, the agency said.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7tpdftm00053b6mfxan4xsn@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            At the Department of Education, more than 70% of its workforce started reporting to the Washington and regional offices full time last week, the agency said in a news release. The rest are expected back by June 1 after building renovations and relocation arrangements are complete.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001n3b6maetty1jv@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            However, many union members at the Department of Housing and Urban Development who were previously able to telework on certain days had to return to the office full time in late February, said Antonio Gaines, president of the AFGE Council 222, which represents 5,300 employees at the agency. Workers at some regional and field offices are exempt for now because of lack of space and safety concerns, including building renovations and inadequate HVAC systems.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001o3b6mk4fzp47r@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The mandate violates the union’s collective bargaining agreement, he argued.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001p3b6mwn7j8d8n@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “They made a unilateral decision to bypass the negotiating process,” Gaines said of the agency, adding that the union plans to file a complaint on this matter as part of a bigger grievance package.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001q3b6meyhnp4y8@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The department did not return a request for comment.
    </p>

              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Italy moves to reverse anti-nuclear stance (217 pts)]]></title>
            <link>https://www.world-nuclear-news.org/articles/cabinet-moves-to-reverse-italys-anti-nuclear-stance</link>
            <guid>43253407</guid>
            <pubDate>Tue, 04 Mar 2025 11:51:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.world-nuclear-news.org/articles/cabinet-moves-to-reverse-italys-anti-nuclear-stance">https://www.world-nuclear-news.org/articles/cabinet-moves-to-reverse-italys-anti-nuclear-stance</a>, See on <a href="https://news.ycombinator.com/item?id=43253407">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
                    Monday, 3 March 2025

                    </p>

                        <p>Italy's Council of Ministers has approved a draft law calling for the government to adopt a series of legislative decrees to create the legal framework for the reintroduction of nuclear power, which was phased out following a referendum in 1987.</p>
                    
                            <figure>
                                <img src="https://www.world-nuclear-news.org/images/articles/GilbertoPichettoFratin_67730.jpg" alt="Cabinet moves to reverse Italy's anti-nuclear stance">
                                    <figcaption>Pichetto Fratin speaking at a press conference following the cabinet meeting (Image: Ministry of Environment and Energy Security)</figcaption>

                            </figure>
                    
                    


               

                <p>On 28 February, on the proposal of President Giorgia Meloni and the Minister of the Environment and Energy Security Gilberto Pichetto Fratin, the Council of Ministers (the Italian cabinet) approved the draft delegation law on 'sustainable nuclear energy'.</p>

<p>The government said the text is aimed at "the inclusion of sustainable nuclear and fusion in the so-called 'Italian energy mix' and intervenes organically from an economic, social and environmental perspective, within the framework of European decarbonisation policies with a time horizon of 2050, consistently with the objectives of carbon neutrality and security of supply".</p>

<p>It added that the intervention aims to: ensure continuity of energy supply in the presence of a constant increase in demand and promote the achievement of energy independence; contribute to the decarbonisation objectives necessary to tackle climate change; and ensure the sustainability of costs borne by end users and the competitiveness of the national industrial system.</p>

<p>The draft law says that Italy should make "a clear break ... with respect to the nuclear plants of the past" and "use of the best available technologies, including modular and advanced technologies". It calls for the government to establish an independent authority for the regulation, supervision and control of nuclear infrastructures.</p>

<p>"Promoters of nuclear projects must provide adequate financial and legal guarantees to cover the costs of construction, operation and decommissioning of the plants and for risks, even those not directly attributable to them, arising from nuclear activity," it adds.</p>

<p>The draft law requires the government to adopt a series of legislative decrees, within 12 months of entry into force, to "organically regulate the entire life cycle of the new sustainable energy, through the drafting of a national programme: from the testing, localisation, construction and operation of the new reactors, to the issue of manufacturing and reprocessing of the fuel will be addressed in a circular economy vision". It will also "serve to provide training and information tools, train new technicians and professionals in the sector, and identify benefits for the territories involved".</p>

<p>The draft law will now be submitted to parliament for final approval.</p>

<p>"With the latest generation nuclear, together with renewables, we will be able to achieve the objectives of decarbonisation, guaranteeing the full energy security of the country," Minister Pichetto Fratin said. "In this way, Italy is ready to face the challenges of the future."</p>

<h4><span>The background</span><br>
&nbsp;</h4>

<p>Italy operated a total of four nuclear power plants starting in the early 1960s but decided to phase out nuclear power in a referendum that followed the 1986 Chernobyl accident. It closed its last two operating plants, Caorso and Trino Vercellese, in 1990.</p>

<p>In late March 2011, following the Fukushima Daiichi accident, the Italian government approved a moratorium of at least one year on construction of nuclear power plants in the country, which had been looking to restart its long-abandoned nuclear programme.</p>

<p>The public mood has changed since then, and in May 2023, the Italian Parliament approved a motion to urge the government to consider incorporating nuclear power into the country's energy mix. In September last year, the first meeting was held of the National Platform for a Sustainable Nuclear, set up by the government to define a time frame for the possible resumption of nuclear energy in Italy and identify opportunities for the country's industrial chain already operating in the sector.</p>

<p>Italy's government included potential nuclear capacity - up to 16 GW/20-22% of capacity by 2050 - in its National Integrated Energy and Climate Plan, which was submitted to the European Commission on 1 July 2024.</p>

<p>Speaking the following day at the <em>Global Energy Transition Congress in Milan</em>, Pichetto Fratin, said: "We expect to be able to reach about 8 GW from nuclear power by 2050, covering more than 10% of the nation's electricity demand. This percentage may increase to over 20-22% by fully exploiting the potential of nuclear power in our country."</p>


                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The End of Weather Forecasting (118 pts)]]></title>
            <link>https://thinc.blog/2025/03/03/the-end-of-weather-forecasting/</link>
            <guid>43251150</guid>
            <pubDate>Tue, 04 Mar 2025 06:49:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thinc.blog/2025/03/03/the-end-of-weather-forecasting/">https://thinc.blog/2025/03/03/the-end-of-weather-forecasting/</a>, See on <a href="https://news.ycombinator.com/item?id=43251150">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
		<p><a href="#content">
			Skip to content		</a></p><!-- .site-header -->

		<div id="content">
	<main id="main">
		
<article id="post-106095">
	<!-- .entry-header -->

	
	
	<div>
		<div>
<figure><a href="https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-8.png?ssl=1"><img data-recalc-dims="1" decoding="async" width="840" height="420" data-attachment-id="106099" data-permalink="https://thinc.blog/2025/03/03/the-end-of-weather-forecasting/image-767/" data-orig-file="https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-8.png?fit=1200%2C600&amp;ssl=1" data-orig-size="1200,600" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-8.png?fit=300%2C150&amp;ssl=1" data-large-file="https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-8.png?fit=840%2C420&amp;ssl=1" src="https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-8.png?resize=840%2C420&amp;ssl=1" alt="" srcset="https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-8.png?resize=1024%2C512&amp;ssl=1 1024w, https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-8.png?resize=300%2C150&amp;ssl=1 300w, https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-8.png?resize=768%2C384&amp;ssl=1 768w, https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-8.png?resize=700%2C350&amp;ssl=1 700w, https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-8.png?w=1200&amp;ssl=1 1200w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px"></a><figcaption>Triumph of the War on Science</figcaption></figure></div>


<p>Got a heads up on this from a senior scientist who emailed today:</p>



<p><em>“If this happens, weather forecasting as we know it will end.”</em></p>



<p>Further underlined by<a href="https://bsky.app/profile/weatherwest.bsky.social/post/3ljj22cqucq2n" data-type="link" data-id="https://bsky.app/profile/weatherwest.bsky.social/post/3ljj22cqucq2n"> Daniel Swain</a>, well known to readers here.</p>


<div>
<figure><a href="https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-7.png?ssl=1"><img data-recalc-dims="1" decoding="async" width="840" height="189" data-attachment-id="106096" data-permalink="https://thinc.blog/2025/03/03/the-end-of-weather-forecasting/image-766/" data-orig-file="https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-7.png?fit=1524%2C344&amp;ssl=1" data-orig-size="1524,344" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-7.png?fit=300%2C68&amp;ssl=1" data-large-file="https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-7.png?fit=840%2C189&amp;ssl=1" src="https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-7.png?resize=840%2C189&amp;ssl=1" alt="" srcset="https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-7.png?resize=1024%2C231&amp;ssl=1 1024w, https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-7.png?resize=300%2C68&amp;ssl=1 300w, https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-7.png?resize=768%2C173&amp;ssl=1 768w, https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-7.png?resize=1200%2C271&amp;ssl=1 1200w, https://i0.wp.com/thinc.blog/wp-content/uploads/2025/03/image-7.png?w=1524&amp;ssl=1 1524w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px"></a></figure></div>


<p><a href="https://www.axios.com/2025/03/03/doge-noaa-weather-building-leases-trump" data-type="link" data-id="https://www.axios.com/2025/03/03/doge-noaa-weather-building-leases-trump">Axios:</a></p>



<p>The Trump administration has informed NOAA that two pivotal centers for&nbsp;<a href="https://www.axios.com/2025/03/02/major-storm-severe-outbreak-weather-service-layoffs">weather forecasting</a>will soon have their leases canceled, sources told Axios.&nbsp;</p>



<p><strong>Why it matters:&nbsp;</strong>One of the buildings is the nerve center for generating national weather forecasts.&nbsp;</p>



<ul>
<li>It was designed to integrate multiple forecasting centers in one building to improve operating efficiency. It houses telecommunications equipment to send weather data and forecasts across the U.S. and abroad.</li>
</ul>



<p><strong>Driving the news:&nbsp;</strong>The NOAA Center for Weather and Climate Prediction is on the lease cancellation list, according to a NOAA employee who spoke on condition of anonymity for fear of retribution.&nbsp;</p>



<ul>
<li>Two ex-National Oceanic and Atmospheric Administration officials also confirmed the list.</li>



<li>The building houses the National Weather Service’s National Centers for Environmental Prediction, or NCEP, which includes the Environmental Modeling Center. It&nbsp;<a href="https://www.weather.gov/media/wrn/1pgr_NCWCP.pdf" target="_blank" rel="noreferrer noopener">opened in 2012</a>and has about 268,000 square feet of space.</li>



<li>The modeling center runs the computer models used in day-to-day weather forecasting, and ensures that weather data correctly goes into these models and that they are operating correctly.&nbsp;</li>
</ul>



<p><strong>The lease cancellation</strong>&nbsp;was&nbsp;<a href="https://www.theverge.com/news/622990/trump-doge-government-layoffs-doge-weather-forecasts-noaa" target="_blank" rel="noreferrer noopener">first reported by The Verge</a>. The National Weather Service didn’t immediately respond to a request for comment.</p>



<ul>
<li>The NOAA employee told Axios the cancellations — along with recent layoffs, early retirements, and travel and hiring limitations — point to an effort to dismantle the agency.</li>
</ul>



<p><strong>Between the lines:&nbsp;</strong>Elon Musk’s&nbsp;<a href="https://www.axios.com/politics-policy/doge-department-of-government-efficiency">Department of Government Efficiency</a>&nbsp;(DOGE) has been working through the General Services Administration to cancel government leases of office space.</p>



<ul>
<li>The NOAA employee told Axios a nightmare scenario could unfold if the College Park building was shuttered, but the agency still was tasked with the same missions as at present.</li>



<li>In that case, NOAA would have to somehow replicate its functionality somewhere else in a process that could take a year or more and leave critical forecasting gaps.</li>



<li>It would also require new congressional appropriations to get that done.</li>
</ul>



<p><strong>The intrigue:&nbsp;</strong>The cancellation notice for the College Park facility isn’t final, as a spreadsheet detailing all the properties on the cancellation list has an end date of “TBD” for that building, according to the NOAA staff member.</p>




	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article><!-- #post-106095 -->

<!-- .comments-area -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
	</main><!-- .site-main -->

	
</div><!-- .site-content -->

		<!-- .site-footer -->
	</div></div>]]></description>
        </item>
    </channel>
</rss>