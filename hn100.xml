<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 20 Apr 2025 23:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[U.S. citizen in Arizona detained by immigration officials for 10 days (174 pts)]]></title>
            <link>https://news.azpm.org/p/news-articles/2025/4/18/224512-us-citizen-in-arizona-detained-by-immigration-officials-for-10-days/</link>
            <guid>43745469</guid>
            <pubDate>Sun, 20 Apr 2025 18:20:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.azpm.org/p/news-articles/2025/4/18/224512-us-citizen-in-arizona-detained-by-immigration-officials-for-10-days/">https://news.azpm.org/p/news-articles/2025/4/18/224512-us-citizen-in-arizona-detained-by-immigration-officials-for-10-days/</a>, See on <a href="https://news.ycombinator.com/item?id=43745469">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img src="https://media.azpm.org/master/image/2019/10/23/hero/ice.jpg" alt="ICE arrests">
<span>A March 2018 photo of U.S. Immigration and Customs Enforcement (ICE) officers.</span></p><p>ICE/Flickr</p>
</div><div>




<p>19-year-old Jose Hermosillo, who is visiting Tucson from Albuquerque, says he was lost and walking near the Border Patrol headquarters when an agent arrested him for illegally entering the country. Hermosillo was not carrying identification.</p>
<p>Court documents say a Border Patrol agent arrested Hermosillo “at or near Nogales, Arizona, without proper immigration documents” and that Hermosillo admitted to illegally entering the U.S.</p>
<p>Hermosillo and his girlfriend, who have a 9-month-old child together, live in Albuquerque, New Mexico, and are visiting family in Tucson. He says he has never been to Nogales. </p>
<p>His girlfriend’s aunt Grace Layva says she and her family made numerous calls looking for him before they found out he was being detained in the Florence Correctional Center, which Immigration and Customs Enforcement uses to detain people. </p>
<p>Another family member drove to the detention center, about 70 miles northwest of Tucson, but said officials wouldn’t provide any information or release him. </p>
<p>ICE did not respond to a request for comment about the wrongful detention. </p>
<p>The family later provided officials with his birth certificate and social security card.</p>
<p>“He did say he was a U.S. citizen, but they didn't believe him,” Layva said. “I think they would have kept him. I think they would have if they would have not got that information yesterday in the court and gave that to ICE and the Border Patrol. He probably would have been deported already to Mexico.”</p>
<p>A magistrate judge in Tucson dismissed his case on Thursday, and family says he was released much later that night. </p>
<p>There have been other recent cases of U.S. citizens being wrongly detained by immigration officials, including a man wrongly held in Florida after being pulled over during his commute to work. </p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First hormone-free male birth control pill enters human trials (130 pts)]]></title>
            <link>https://scitechdaily.com/99-effective-first-hormone-free-male-birth-control-pill-enters-human-trials/</link>
            <guid>43745296</guid>
            <pubDate>Sun, 20 Apr 2025 17:54:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scitechdaily.com/99-effective-first-hormone-free-male-birth-control-pill-enters-human-trials/">https://scitechdaily.com/99-effective-first-hormone-free-male-birth-control-pill-enters-human-trials/</a>, See on <a href="https://news.ycombinator.com/item?id=43745296">Hacker News</a></p>
Couldn't get https://scitechdaily.com/99-effective-first-hormone-free-male-birth-control-pill-enters-human-trials/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The Movie Mistake Mystery from "Revenge of the Sith" (285 pts)]]></title>
            <link>https://fxrant.blogspot.com/2025/04/the-movie-mistake-mystery-from-revenge.html</link>
            <guid>43745141</guid>
            <pubDate>Sun, 20 Apr 2025 17:29:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fxrant.blogspot.com/2025/04/the-movie-mistake-mystery-from-revenge.html">https://fxrant.blogspot.com/2025/04/the-movie-mistake-mystery-from-revenge.html</a>, See on <a href="https://news.ycombinator.com/item?id=43745141">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Movies are handmade, and just like any other art form, sometimes the seams that hold movies together become visible to the audience. For movie fans, these moments are very exciting. Catching a glimpse behind the scenes is an exhilarating experience. My favorite kind of “movie mistake” is the kind that is hiding in plain sight... but the casual viewer missed it upon first viewing. Or perhaps even the second viewing, or even the third.&nbsp;</p><p>I’m particularly obsessed with moments that reveal the craft and artistry of the magic trick of a shot that slightly shatters the illusion of cinema. These revealing moments have been in movies since the dawn of cinema, and are everywhere (if you know exactly where to look).</p><p>One of my favorite films of all time also has one of the funniest revealing mistakes I've seen. Edward Zwick's "Glory" (1989) takes place during the American Civil War, and this scene has a blink-and-you'll-miss-it reminder of the film's very modern production:</p><p>Because the audiences' eyes are firmly fixed on Morgan Freeman's character in the center of frame, very few will ever pick up the little kid with the extremely modern wristwatch that enters frame on far screen right. Sometimes the on-set teams that work with featured extras—as well as the costume department that dress the extras—will occasionally miss a modern piece of jewelry on an actor.</p><p>Here's a fun one from Martin Scorsese's masterpiece "Goodfellas" (1990), in one of the closing shots of a nail-bitingly tense scene where Karen nearly walks into an ambush:</p><p>The period-appropriate "movie" license plate dramatically dangles then completely falls off the car in the middle of the take, revealing the actual 1990-era license plate of the car used for the scene. This is an accidental and hilarious glimpse into the detailed hard work that goes into making a Hollywood period piece (this portion of the film takes place in 1980), where every license plate of every car in the movie needed special, detailed work to make them period-appropriate.&nbsp;</p><div><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMcj2VC4Ju_5PCmfKdkGkSC6r2UzvK2qzh0d6NcxdtSdPSk24l8m5oTt3Yeq5GLp0wPZqjUlq1pHDiutRKWpMZucWukSa3qBfHw8J8KOu-dGExAWf97ym5Ikpif9BbZnKqLOwef_z1I2FXFFradn3C8BOr2GkMU8SfJzmBafEDtnv_4d8FeV4/s1908/illustration.png"><img data-original-height="1080" data-original-width="1908" height="226" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMcj2VC4Ju_5PCmfKdkGkSC6r2UzvK2qzh0d6NcxdtSdPSk24l8m5oTt3Yeq5GLp0wPZqjUlq1pHDiutRKWpMZucWukSa3qBfHw8J8KOu-dGExAWf97ym5Ikpif9BbZnKqLOwef_z1I2FXFFradn3C8BOr2GkMU8SfJzmBafEDtnv_4d8FeV4/w400-h226/illustration.png" width="400"></a></p><p>The finale of James Cameron's epic "Aliens" (1986) features the android Bishop (Lance Henriksen) getting severed in half, but still functioning enough to save Newt (Carrie Henn) from getting sucked into the vacuum of space. The action-packed scene features an absolutely wonderful accidental reveal of how the cut-in-half android was accomplished on the set:</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHvAdErohL2BD-qytmJ7mGVvxdRGRuzMOiwXiI8kR6y0Pw707rqoLC58rp3AWp3gHhAt-O5YeR-thdq5AbnQrlhTtoc44TQsDctIyA2Ykga8UQVB7gGQphV1-wPNOChutR7MIFC2dzrBfxkRyeWXr0NjwW9kwU8N28sIgHlotDkprZNKZP9EY/s1920/bishop.png"><img data-original-height="1036" data-original-width="1920" height="216" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHvAdErohL2BD-qytmJ7mGVvxdRGRuzMOiwXiI8kR6y0Pw707rqoLC58rp3AWp3gHhAt-O5YeR-thdq5AbnQrlhTtoc44TQsDctIyA2Ykga8UQVB7gGQphV1-wPNOChutR7MIFC2dzrBfxkRyeWXr0NjwW9kwU8N28sIgHlotDkprZNKZP9EY/w400-h216/bishop.png" width="400"></a></p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrvAm_i6yGBzc5507CmHJY6q_LcLnMS-eTcXVQhyphenhyphenA8uYrX4dXL80CVd4EwJOAEbJZPgeGawDEUn2pdc5KqlBuZJLuaw7fWJdzoG-r2a1vy_uQU_neb-5N5Yz8AaiDR0UtxciF4GhUhY3Tc6-0VamANpf-FePIL0YbeFnucGGA4QEKyVhMau7E/s433/alienshole.2025-04-17%2012_43_03.gif"><img data-original-height="227" data-original-width="433" height="210" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrvAm_i6yGBzc5507CmHJY6q_LcLnMS-eTcXVQhyphenhyphenA8uYrX4dXL80CVd4EwJOAEbJZPgeGawDEUn2pdc5KqlBuZJLuaw7fWJdzoG-r2a1vy_uQU_neb-5N5Yz8AaiDR0UtxciF4GhUhY3Tc6-0VamANpf-FePIL0YbeFnucGGA4QEKyVhMau7E/w400-h210/alienshole.2025-04-17%2012_43_03.gif" width="400"></a></p><p><i><span>"Aliens" (1986)</span></i></p><p>The amazing makeup effects applied to Henriksen's body covers the bottom half of his body which is hidden through a hole in the set. But in order to get that little bit of extra athletic stretch to grab Newt, Henriksen popped his body out of the hole a little too far, revealing the classic stage trick. However, I'd gather that 99% of the audience has never noticed this little reveal of stagecraft since our eyes are fixed on Newt on screen right, sliding toward the airlock, and not on the ground contact of Bishop's half-body, which had already been firmly established in the scene.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU9LeqQEqiQZjf4bniZIvtLsXvLwZa6loSJneVo01NSsgsU60jw0nlF-YB-lQmdd3o-Uq1GOWb19_ctP1J5GMakIBHgCWrFCIlg2OQ1QS49k1ZIFyPaqOCOhpnfLCJCnWn3rWQ-QmSqboHlgpw6WxJPAy-OnoK0rnsMiPOX5SqBjT9fLJOQRc/s1045/circled.png"><img data-original-height="568" data-original-width="1045" height="217" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU9LeqQEqiQZjf4bniZIvtLsXvLwZa6loSJneVo01NSsgsU60jw0nlF-YB-lQmdd3o-Uq1GOWb19_ctP1J5GMakIBHgCWrFCIlg2OQ1QS49k1ZIFyPaqOCOhpnfLCJCnWn3rWQ-QmSqboHlgpw6WxJPAy-OnoK0rnsMiPOX5SqBjT9fLJOQRc/w400-h217/circled.png" width="400"></a></p><p>Avoiding reflections of the crew appearing to camera is a constant struggle for filmmakers. In Steven Spielberg's first masterpiece "Duel" (1971), David (Dennis Weaver) gets into a phone booth to make a call, with the front glass face of the booth aimed directly at the camera, and if the audience's gaze drifted off of Weaver's face, they could catch a glimpse of the crew:</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiF2nMOIJJcigCUwmwYiNCUFK3LSPyqR98CGn7XdriPNtxl1cJ-wHIb11RxPjm3u9LVV8LmypcMzbLLQl8DTTNwrp5eGMx6rs-PPazN7hPp2PihyphenhyphenZrkldyEB49amd3N0gr4tlT-KCokNOIhysaS60VIYxoVWxvTWUoNJR6QUAzH99MOtfshAHY/s442/duel_short.gif"><img data-original-height="249" data-original-width="442" height="225" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiF2nMOIJJcigCUwmwYiNCUFK3LSPyqR98CGn7XdriPNtxl1cJ-wHIb11RxPjm3u9LVV8LmypcMzbLLQl8DTTNwrp5eGMx6rs-PPazN7hPp2PihyphenhyphenZrkldyEB49amd3N0gr4tlT-KCokNOIhysaS60VIYxoVWxvTWUoNJR6QUAzH99MOtfshAHY/w400-h225/duel_short.gif" width="400"></a></p><p><i><span>"Duel" (1971)</span></i></p><p>In the reflection, we see a few crew members on screen left, the camera itself, and director Spielberg on the right (he's the one shuffling left and right, who lowers his head in the middle of the take). Again, like all the examples I'm providing in this article, hardly anyone would ever notice these moments. When a viewer catches these brief moments, the illusion of the movie is briefly broken, but for fans of the filmmaking process, it's a joyful reminder of the overall magic trick. The most intimate movie scene with only two characters in a desolate, isolated environment actually was created by dozens and dozens of crew members standing slightly out of frame.</p><p>Look for another accidental 'crew caught on camera' moment in the reflection in a car window in the 'leave the gun, take the cannoli' scene from "The Godfather" (1972), one that very few people ever notice.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgASHi1vEZJq9xexlwe6N6XnaKUFAH4aOLXmHKCv9BVNxVMiPwJ_Pj_nEabuKrD-nJqZDhITBBmRoNdyYmO3jKQEgLzkHPChAr50lpMRd5yJYt6whAJlaIVY7wOBLK18qVEOOpCbj4b7YeAIAjvItvDNC0tKFlLCk2P8s3NtQRy7Ahvy9kMCe8/s1920/duel_circle.png"><img data-original-height="1080" data-original-width="1920" height="225" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgASHi1vEZJq9xexlwe6N6XnaKUFAH4aOLXmHKCv9BVNxVMiPwJ_Pj_nEabuKrD-nJqZDhITBBmRoNdyYmO3jKQEgLzkHPChAr50lpMRd5yJYt6whAJlaIVY7wOBLK18qVEOOpCbj4b7YeAIAjvItvDNC0tKFlLCk2P8s3NtQRy7Ahvy9kMCe8/w400-h225/duel_circle.png" width="400"></a></p><p>Here's a super quick revealing mistake from "The Dark Knight" (2008) that is a true "you'll never see this in real time" moment:</p><div><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg914mIcnatARLUxXQIIv3YV-sMpkCbW5HIt6fqjlMW70OT5z6_Rn39zeenC42Sq_UYQk-eFX6DwW4aAUj9td-cqk0pzWhx0utnQOq8yuzxDn2hSJ2ptXYxhfydZaTz-F8Dho6lDeOgrdNuegiP2B0cHQe8UuDjRSX4EhWDSoU_HilsML5NXIA/s526/darkknight_crew_BRIGHTfull.2025-04-17%2012_56_27.gif"><img data-original-height="219" data-original-width="526" height="166" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg914mIcnatARLUxXQIIv3YV-sMpkCbW5HIt6fqjlMW70OT5z6_Rn39zeenC42Sq_UYQk-eFX6DwW4aAUj9td-cqk0pzWhx0utnQOq8yuzxDn2hSJ2ptXYxhfydZaTz-F8Dho6lDeOgrdNuegiP2B0cHQe8UuDjRSX4EhWDSoU_HilsML5NXIA/w400-h166/darkknight_crew_BRIGHTfull.2025-04-17%2012_56_27.gif" width="400"></a></p><p><i><span>"The Dark Knight" (2008)</span></i></p></div><p>Although "The Dark Knight" example gives the audience a much clearer look at the camera operator, the focus puller(?) and the camera itself reflected in the interrogation room’s mirrors, the shot is a lot harder to see the crew members and equipment in real time due to the chaotic and energetic camera movement, as opposed to the locked off nature of the "Duel" example.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0YNfNS0gG-xlaqmH-9KJDvwwpsHkIMycqul7SW8X2hE-LNZf0RHBM3LIXuQRlqQo0xD4sj5xzUW_Cu-A2gMXeBUmN55gZwHMlDuBL3nuxqCt0rJDDSGM2hTUzrkuWXxuxIg73mSaclQudBt75y_w_X8YsuWQRFC7iPc5NDSb6dQnQ2D3JxAQ/s1918/darkknight_compare.png"><img data-original-height="800" data-original-width="1918" height="166" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0YNfNS0gG-xlaqmH-9KJDvwwpsHkIMycqul7SW8X2hE-LNZf0RHBM3LIXuQRlqQo0xD4sj5xzUW_Cu-A2gMXeBUmN55gZwHMlDuBL3nuxqCt0rJDDSGM2hTUzrkuWXxuxIg73mSaclQudBt75y_w_X8YsuWQRFC7iPc5NDSb6dQnQ2D3JxAQ/w400-h166/darkknight_compare.png" width="400"></a></p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibaiynJTHf2-1u0HH7_do8IU4-CU7kzWOzAUP4syOn9_7YRAVLSowGvKwEPI52YA3rk4xAUpd4BzXJbBpfnADRmYZoijN4Vkim_OHVJjFrp1S1e8p2AokM1GJDAPSojCoRP4DgcbKlGzmjGpRLrPrZoADLBab4haCQzNsrzxSHo7ThaySW63w/s500/abyss_wipe.gif"><img data-original-height="281" data-original-width="500" height="225" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibaiynJTHf2-1u0HH7_do8IU4-CU7kzWOzAUP4syOn9_7YRAVLSowGvKwEPI52YA3rk4xAUpd4BzXJbBpfnADRmYZoijN4Vkim_OHVJjFrp1S1e8p2AokM1GJDAPSojCoRP4DgcbKlGzmjGpRLrPrZoADLBab4haCQzNsrzxSHo7ThaySW63w/w400-h225/abyss_wipe.gif" width="400"></a></p><p><i><span>"The Abyss" (1989)</span></i></p><p>Amazingly, many folks who watch that clip from the dramatic drowning sequence cannot consciously see the bit of filmmaking that literally blocks the actors in an intimate moment. This is my favorite example of a movie's incredible emotional power — the scene is so dramatic and intense that most viewers cannot consciously see a giant cloth wiping away water from the lens of the camera in the middle of a shot.</p><p>Incidentally, <b>some of these revealing mistakes are being erased from cinema history</b> due to overzealous restoration projects — the process of “cleaning up” a film for newer formats like Blu-ray and 4K — which is deeply wrong. This is a much bigger topic on which I have very strong thoughts and the hottest of takes. Just look at what modern restorations have done to two of these revealing mistakes from "Goodfellas" and "Aliens":</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHamPBMDtOZiH_6g10yv7e4Pp9vRyA26atkaG2cW-uzswK3zQqOUHwgGiH_mvWTXKWIvddFqc4RKdsaanqjgxHOtGHlUZa79k81xaf_i-YV1AeIpbNzfCx1DBMMSeHkBYMQWsyuKrevfCYObg-z5MoMRfn6k5uVnxSqPtvZFIqBlwmClkwU80/s640/LicensePlate%20_restoration.gif"><img data-original-height="362" data-original-width="640" height="226" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHamPBMDtOZiH_6g10yv7e4Pp9vRyA26atkaG2cW-uzswK3zQqOUHwgGiH_mvWTXKWIvddFqc4RKdsaanqjgxHOtGHlUZa79k81xaf_i-YV1AeIpbNzfCx1DBMMSeHkBYMQWsyuKrevfCYObg-z5MoMRfn6k5uVnxSqPtvZFIqBlwmClkwU80/w400-h226/LicensePlate%20_restoration.gif" width="400"></a></p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjczOajV2AciOuOHRI9NXzBIfGyIKJbjRdfinr7TKVb_TTBP-uQQtDCWqGuDeoaY1FJElsr6HXU_I8sqhXHx7gAdNKKbpk8WHL6ByirFITWb_Vhqsq_TL042SKHjqftXYhZSbYXDGxA73zmjHaF3vBPtHDp_POCUDligVeWAi5N5G8XYRKL_A/s500/alienshole_fullmoviewithtextandcompare.gif"><img data-original-height="270" data-original-width="500" height="216" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjczOajV2AciOuOHRI9NXzBIfGyIKJbjRdfinr7TKVb_TTBP-uQQtDCWqGuDeoaY1FJElsr6HXU_I8sqhXHx7gAdNKKbpk8WHL6ByirFITWb_Vhqsq_TL042SKHjqftXYhZSbYXDGxA73zmjHaF3vBPtHDp_POCUDligVeWAi5N5G8XYRKL_A/w400-h216/alienshole_fullmoviewithtextandcompare.gif" width="400"></a></p><p>Painting out these movie mistakes as part of a restoration is wrong. <i>What's in the movie is in the movie, </i>and altering the movie to this extent is a form of revisionist history. Cinema is worse off when over-aggressive restorations alter the action within the frame. To me, this is equivalent to swapping out an actor's performance with a different take, or changing the music score during an action sequence, or replacing a puppet creature with a computer graphics version of the same creature decades after release. But I digress.</p><p>• &nbsp;• &nbsp;• &nbsp;•</p><p>Like I said at the start, movies are handmade, and that's true even in today's landscape where digital visual effects are a prominent part of filmmaking. In the same way that physical crews use physical tools to build sets, construct costumes and craft props, visual effects artists use digital tools to craft an image. And with the hand-made nature of any art form, the lack of clinical accuracy lends to its charm and sometimes offers an accidental peek behind the scenes of how the art was constructed.</p><p>Every few years, a "Star Wars" revealing mistake bubbles up on the internet, one from the Mustafar sequence from Episode III, "Revenge of the Sith" (2005). But the bizarre moment in the single shot was not as easily explainable as the examples I've shown above.</p><p>Being in the privileged position of currently working at Industrial Light &amp; Magic, the visual effects company that made the visual effects for the movie (and having worked on that movie [and that sequence!]), I took it upon myself to try and solve the mystery.</p><p>Please enjoy the story, written by Ian Kintzle, of how I investigated the mystery of the "Force Ghost" in "Revenge of the Sith", as it originally appeared in the Star Wars Celebration Program for Japan 2025.</p><p>• &nbsp;• &nbsp;• &nbsp;• &nbsp;• &nbsp;• &nbsp;• &nbsp;•</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEb2mtF1FNE5KEKPJjSs9hyphenhyphend5cAatilKzhr5B1E8cS-zRT1QI3XEXcX4SBRgLql4-C9ilaCiSSETbvWEl6SlxcbW495GsA6SbcjvulKOl6ZhyynisOsIH22CMDP-JmdMXb2opBnT-iOGoHglz_E6rgqXc8HSvqgbgm4Ibihz-P1RVnOKewPmQ/s1918/finalshot_singleframeA.jpg"><img data-original-height="816" data-original-width="1918" height="170" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEb2mtF1FNE5KEKPJjSs9hyphenhyphend5cAatilKzhr5B1E8cS-zRT1QI3XEXcX4SBRgLql4-C9ilaCiSSETbvWEl6SlxcbW495GsA6SbcjvulKOl6ZhyynisOsIH22CMDP-JmdMXb2opBnT-iOGoHglz_E6rgqXc8HSvqgbgm4Ibihz-P1RVnOKewPmQ/w400-h170/finalshot_singleframeA.jpg" width="400"></a></p><p><i><b><span>THE FORCE GHOST IN THE MACHINE</span></b></i></p><p><i><span>By Ian Kintzle</span></i></p><p><i><span>April 2025, for Star Wars Celebration Japan</span></i></p><p><span>It was spring 2005, and Industrial Light &amp; Magic (ILM)— George Lucas’ dream factory—had just completed two years of work on one of its most ambitious projects yet: "Star Wars: Revenge of the Sith". A massive undertaking, "Sith" required a herculean effort from hundreds of artists and technicians at ILM, crafting 367 computer- generated models, hundreds of 3D and 2D environments, 47 practical miniature setups, and 13,000,000 renders and composites across 2,151 effects shots.</span></p><p><span>Out of all of the effects sequences in the picture, perhaps none was more challenging than the operatic duel between Darth Vader (Hayden Christensen) and Obi-Wan Kenobi (Ewan McGregor) on the volcanic planet of Mustafar. The battle starts within the Klegger Corp Mining Facility situated high on the rocky banks of a vast lava river, and progresses through the facility onto a heat-collection arm stretching over a fast- moving river of boiling magma, and then onto a pair of lava skiffs and panning droids. The battle finally ends on a bank with Vader severely burned and maimed.</span></p><p><span>For the Mustafar sequence, ILM’s team of compositors, led by Compositing Supervisor Pat Tubach and Sequence Supervisor Michael Conte, were faced with the daunting challenge of seamlessly blending all of these live-action plates, computer-generated imagery, and miniature effects, into one cohesive sequence. But with so many individual elements, mistakes happen, and in the case of Revenge of the Sith, a peculiar anomaly slipped through the cracks at precisely 1 hour, 59 minutes, and 2 seconds into the film.</span></p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9Y3af6L5NqwRyByVmK5V-h8fEwV8KjiN0r1UbtqwggsMCVMTciUmSBPrJO9YHUushdOESEJ2UsYi4113wq3UGQo2_Q1LRMccUwyI3_lR7dpLbGn3D0EQoVvELQYDjg8yHCrD9b6mN-evcA8cmvodXq1CS13glDu4Yw_rgxKZMR53P3gDJHGc/s500/finalsequence.gif"><span><img data-original-height="213" data-original-width="500" height="170" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9Y3af6L5NqwRyByVmK5V-h8fEwV8KjiN0r1UbtqwggsMCVMTciUmSBPrJO9YHUushdOESEJ2UsYi4113wq3UGQo2_Q1LRMccUwyI3_lR7dpLbGn3D0EQoVvELQYDjg8yHCrD9b6mN-evcA8cmvodXq1CS13glDu4Yw_rgxKZMR53P3gDJHGc/w400-h170/finalsequence.gif" width="400"></span></a></p><p><span>The internet, ever vigilant, began to take notice of this curious artifact around 2015 – a blink-and-you’ll-miss- it moment of a ghostly-robed figure with dark hair that appears behind Anakin Skywalker for only a frame or two just as he leaps from the panning droid to meet Obi-Wan on the lava skiff. The strange figure sparked countless theories and speculation. Was it a “Force ghost”? An easter egg from a mischievous ILM artist?</span></p><p><span>Todd Vaziri, a seasoned veteran at ILM who also worked on the film as a compositor, was intrigued by the mystery. “Just before the release of The Force Awakens, I started to see this ‘easter egg’ bubble up on social media from time to time of what appears to be a Force ghost on Mustafar,” Vaziri says. “The discourse would really get going. Somebody would spot the artifact and go, ‘What the heck was this?’ And another would say, ‘What do you mean? I don’t see anything.’ And only when you step through the scene, frame-by-frame, do you see what looks like a ghostly face behind Anakin in the shot where he jumps up from the panning droid to continue the lightsaber duel on the lava skiff. And honestly, in-motion, nobody can spot this.”</span></p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYvOBxrHhHq0CiZlSavot6K1cDeaDHKK-OxHOqGpiSBTKMahcyR_4sUwab0y8EJscd2r8mJQ75Diy1xT2gfg4jxRGa6TTmg6dmtG7WUF2u08Se3LSXIRzPUQBE9ItefL8dKzv8yAISsj9fjyVb3VtciyuWcKQDtQpefp2SocuSAFUSMAxeL_M/s500/animation.gif"><span><img data-original-height="213" data-original-width="500" height="170" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYvOBxrHhHq0CiZlSavot6K1cDeaDHKK-OxHOqGpiSBTKMahcyR_4sUwab0y8EJscd2r8mJQ75Diy1xT2gfg4jxRGa6TTmg6dmtG7WUF2u08Se3LSXIRzPUQBE9ItefL8dKzv8yAISsj9fjyVb3VtciyuWcKQDtQpefp2SocuSAFUSMAxeL_M/w400-h170/animation.gif" width="400"></span></a></p><p><span>Getting to the bottom of the mystery would prove difficult. ILM works on dozens of motion pictures and television shows per year, and as older projects are moved offline into their archives, the steps to bring them back to the servers are involved. Revenge of the Sith was no exception. It would require scavenging through terabytes of unaltered greenscreen photography that hasn’t been touched in years. So Vaziri put it behind him – for a time. But in 2024 when the discourse regarding the “Force ghost” roared alive again on social media, Vaziri decided that enough was enough. But in order to locate the anomaly, he would need to spelunk into the film’s digital archives at ILM which had since gone dark.</span></p><p><span>“I think it took 24 hours to unearth the footage and put it back on our servers. I was so excited, my heart was pounding out of my chest. No one had seen the original greenscreen footage for nearly twenty years,” Vaziri says. “The problem was I didn’t remember exactly what these plates looked like, both because it wasn’t my shot, and it was two decades prior. So I dug, and I dug, and finally I found the plate photography. I couldn’t believe it. There on set was a man—likely a stunt rigger—wearing not a robe, but a peculiar shirt that resembled one, standing behind Hayden, manually puppeteering the greenscreen lava skiff that he and Ewan were fighting on. His face and the “Force ghost” matched up frame-for-frame.” During this excavation process, Vaziri was also able to uncover a variety of in-progress versions of the shot composited with very basic layering. In those early takes, the robed man was not present. This meant one thing: the compositor had done some articulates to remove the mystery man, but the green screen extraction wasn’t quite done yet.</span></p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxmX6_02BHHum6AiA3Hz9X3NDxDsyzBCfeyj6Rs-rmTDZoZEw78OEj1hvTlxbHn1-G1R-Ur0Qdy7G2vhbuy_AP1tza0uAlHr216oz-MXNbrp0WWbLEGNjVDALO9UZJG8WYd1jKz2yv5HAic4dNDS4SpXNTxr1dWCdI4rmtyb8VLFBnqDvRw74/s1175/greenscreen_compare.png"><span><img data-original-height="1175" data-original-width="1024" height="400" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxmX6_02BHHum6AiA3Hz9X3NDxDsyzBCfeyj6Rs-rmTDZoZEw78OEj1hvTlxbHn1-G1R-Ur0Qdy7G2vhbuy_AP1tza0uAlHr216oz-MXNbrp0WWbLEGNjVDALO9UZJG8WYd1jKz2yv5HAic4dNDS4SpXNTxr1dWCdI4rmtyb8VLFBnqDvRw74/w349-h400/greenscreen_compare.png" width="349"></span></a></p><p><span>“We have to do frame-by-frame tweaking by hand, which means creating new garbage mattes in order to paint details into the motion- blurred edges,” Vaziri explains. “At some point during the process of refining the edges of the green screen extraction—which required new garbage mattes—the stunt rigger’s head was inadvertently revealed again in that paint process—but because you can’t see it unless you are stepping through it frame-by-frame— it was deemed finished by the artist, by the compositing supervisor, by the visual effects supervisor, by the editors, and by George Lucas himself. Nobody that was part of this process ever caught that and that’s how it made it in the movie. But in a way, I think it’s really wonderful. Plenty of my shots have mistakes in them, and as the saying goes: perfect is the enemy of good. We want our shots to be as perfect as they can be, but we can’t hit everything. In the last 20 years, we have evolved what we call the “Final Check” process, which is our way of scrutinizing shots before they leave ILM. An extra step of quality control, if you will. The bottom line is that we put human hands on every single one of the thousands of shots that you see in Star Wars. This world is handmade, and little things like this become part of ILM history.”</span></p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTXZ4c7qoZ-d8aejJQ29QoqgvIpqihCiRAzFobTxRC5Wd_fJxQThJNkviLOd6L1Af4jhMZrRBRq4ZqoqelEiwSrtaeucjjLlMbdIxhRj_JY9uZccfDtynD7uPPTj-leY98QlCpz2uyy8cFBtkyQiBRHMF2a7EsFY6sfBlL6gDr9XFN2VCBI7E/s1593/greenscreen_singleframeBCROP.jpeg"><span><img data-original-height="783" data-original-width="1593" height="196" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTXZ4c7qoZ-d8aejJQ29QoqgvIpqihCiRAzFobTxRC5Wd_fJxQThJNkviLOd6L1Af4jhMZrRBRq4ZqoqelEiwSrtaeucjjLlMbdIxhRj_JY9uZccfDtynD7uPPTj-leY98QlCpz2uyy8cFBtkyQiBRHMF2a7EsFY6sfBlL6gDr9XFN2VCBI7E/w400-h196/greenscreen_singleframeBCROP.jpeg" width="400"></span></a></p><p><span><span><i>detail of the original greenscreen footage</i></span></span></p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMNJa-NicVw7dMWKVk8mXjvd0wilCDs1xhqFR8-_L8uo2WD7_q9AcNWKUmY6eO8KNuHB1RflBg_z2UnTAJV-EB4BUJJrkEqzWcr4SrauZGkSAK2TqA9LHHvNJ1-6oiNdIYZVlsaVrE4AV4l4uIqArxzA8HfgyNJkzXgHnPHUvWw2eBUa0SJzg/s1918/finalshot_singleframeB.jpg"><span><img data-original-height="816" data-original-width="1918" height="170" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMNJa-NicVw7dMWKVk8mXjvd0wilCDs1xhqFR8-_L8uo2WD7_q9AcNWKUmY6eO8KNuHB1RflBg_z2UnTAJV-EB4BUJJrkEqzWcr4SrauZGkSAK2TqA9LHHvNJ1-6oiNdIYZVlsaVrE4AV4l4uIqArxzA8HfgyNJkzXgHnPHUvWw2eBUa0SJzg/w400-h170/finalshot_singleframeB.jpg" width="400"></span></a></p><p><span><span><i>the final shot as it appears in the film</i></span></span></p><p><span>So there you have it, readers. Another Star Wars mystery solved. In this case it wasn’t a Force ghost, but a stunt rigger who slipped into the shot during the compositing process, providing a wonderful look at the technical seams and handmade nature of the world of visual effects. Star Wars: Revenge of the Sith is streaming now on Disney+.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Things Zig comptime won't do (258 pts)]]></title>
            <link>https://matklad.github.io/2025/04/19/things-zig-comptime-wont-do.html</link>
            <guid>43744591</guid>
            <pubDate>Sun, 20 Apr 2025 15:57:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matklad.github.io/2025/04/19/things-zig-comptime-wont-do.html">https://matklad.github.io/2025/04/19/things-zig-comptime-wont-do.html</a>, See on <a href="https://news.ycombinator.com/item?id=43744591">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <article>
        <h2>
          Things Zig comptime Won’t Do <time datetime="2025-04-19">Apr 19, 2025</time>
        </h2>

        <figure>
          <blockquote>
            <p>
              Es el disco de Odín. Tiene un solo lado. En la tierra no hay otra
              cosa que tenga un solo lado.
            </p>
          </blockquote>
        </figure>
        <p>
          Zig’s comptime feature is most famous for what it can do: generics!,
          <a href="https://mitchellh.com/writing/zig-comptime-conditional-disable">conditional compilation</a>!,
          <a href="https://mitchellh.com/writing/zig-comptime-tagged-union-subset">subtyping</a>!, serialization!,
          <a href="https://matklad.github.io/2025/03/19/comptime-zig-orm.html">ORM</a>! That’s fascinating, but, to be fair, there’s a bunch of
          languages with quite powerful compile time evaluation capabilities
          that can do equivalent things. What I find more interesting is that
          Zig comptime is actually quite restrictive, by design, and won’t do
          many things! It manages to be very expressive <em>despite</em> being
          pretty limited. Let’s see!
        </p>
        <section id="No-Host-Leakage">
          <h2>
            <a href="#No-Host-Leakage">No Host Leakage </a>
          </h2>
          <p>
            When you execute code at compile time, on which machine does it
            execute? The natural answer is “on your machine”, but it is wrong!
            The code might not run on your machine, it can be cross compiled!
            For overall development sanity, it is important that <code>comptime</code> code observes the same behavior as the runtime
            code, and doesn’t leak details about the host on which the code is
            compiled. Zig doesn’t give comptime code access to host architecture
            (host — machine on which you compile code). Consider this Zig
            program:
          </p>

          <figure>
            <pre><code><span><span>const</span> std = <span>@import</span>(<span>"std"</span>);</span>
<span></span>
<span><span>comptime</span> {</span>
<span>    <span>const</span> x: <span>usize</span> = <span>0xbeef</span>;</span>
<span>    <span>const</span> xs: []<span>const</span> <span>u8</span> = std.mem.asBytes(<span>&amp;</span>x);</span>
<span>    <span>for</span> (xs) <span>|</span>byte<span>|</span> {</span>
<span>        <span>@compileLog</span>(byte);</span>
<span>    }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            I get the following output when compiling normally, on my computer
            for my computer:
          </p>

          <figure>
            <pre><code><span>λ ~/zig-0.14/zig build-lib main.zig</span>
<span>@as(u8, 239)</span>
<span>@as(u8, 190)</span>
<span>@as(u8, 0)</span>
<span>@as(u8, 0)</span>
<span>@as(u8, 0)</span>
<span>@as(u8, 0)</span>
<span>@as(u8, 0)</span>
<span>@as(u8, 0)</span></code></pre>
          </figure>
          <p>
            But if I cross compile to a 32 bit big-endian architecture, comptime
            observes correct <code>usize</code>:
          </p>

          <figure>
            <pre><code><span>λ ~/zig-0.14/zig build-lib -target thumbeb-freestanding-none main.zig</span>
<span>@as(u8, 0)</span>
<span>@as(u8, 0)</span>
<span>@as(u8, 190)</span>
<span>@as(u8, 239)</span></code></pre>
          </figure>
          <p>
            My understanding is that Jai, for example, doesn’t do this, and runs
            comptime code on the host.
          </p>
          <p>
            Rust’s declarative macros and const-fn don’t observe host
            architecture, but procedural macros do.
          </p>
        </section>
        <section id="No-eval">
          <h2>
            <a href="#No-eval">No #eval </a>
          </h2>
          <p>
            Many powerful compile-time meta programming systems work by allowing
            you to inject arbitrary strings into compilation, sort of like <code>#include</code> whose argument is a shell-script that generates the
            text to include dynamically. For example, D mixins work that way:
            <a href="https://dlang.org/articles/mixin.html">https://dlang.org/articles/mixin.html</a>
          </p>
          <p>
            And Rust macros, while technically producing a token-tree rather
            than a string, are more or less the same. In contrast, there’s
            absolutely no facility for dynamic source code generation in Zig.
            You just can’t do that, the feature isn’t!
          </p>
          <p>
            Zig has a completely different feature, partial
            evaluation/specialization, which, none the less, is enough to cover
            most of use-cases for dynamic code generation. Let’s see an
            artificial example:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> f</span>(x: <span>u32</span>, y: <span>u32</span>) <span>u32</span> {</span>
<span>    <span>if</span> (x <span>==</span> <span>0</span>) <span>return</span> y <span>+</span> <span>1</span>;</span>
<span>    <span>if</span> (x <span>==</span> <span>1</span>) <span>return</span> y <span>*</span> <span>2</span>;</span>
<span>    <span>return</span> y;</span>
<span>}</span></code></pre>
          </figure>
          <p>
            This is a normal function that dispatches on the first argument to
            select an operation to apply to the second argument. Nothing fancy!
            Now, the single feature that Zig has is marking the first argument
            with <code>comptime</code>
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> f</span>(<span>comptime</span> x: <span>u32</span>, y: <span>u32</span>) <span>u32</span> {</span>
<span>    <span>if</span> (x <span>==</span> <span>0</span>) <span>return</span> y <span>+</span> <span>1</span>;</span>
<span>    <span>if</span> (x <span>==</span> <span>1</span>) <span>return</span> y <span>*</span> <span>2</span>;</span>
<span>    <span>return</span> y;</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The restriction here is that now, of course, when you call <code>f</code>, the first argument must be comptime-known. You can <span><code>f(92, user_input())</code>,</span> but you can’t
            <span><code>f(user_input(), 92)</code>.</span>
          </p>
          <p>
            The carrot you’ll get in exchange is a guarantee that, for each
            specific call with a particular value of <code>x</code>, the
            compiler will partially evaluate <code>f</code>, so only one branch
            will be left.
          </p>
          <p>
            Zig is an imperative language. Not everything is a function, there’s
            also control flow expressions, and they include partially-evaluated
            variations. For example, <code>for(xs)</code> is a normal runtime
            for loop over a slice, <code>comptime for(xs)</code> evaluates the
            entire loop at compile time, requiring that <code>xs</code> is
            comptime-known, and <code>inline for(xs)</code> requires that just
            the length of <code>xs</code> is known at comptime.
          </p>
          <p>
            Let’s apply specialization to the classic problem solved by
            code-generation — printing. You can imagine a proc-macro style
            solution that prints a struct by reflecting on which fields it has
            and emitting the code to print each field.
          </p>
          <p>
            In Zig, the same is achieved by specializing a recursive <code>print</code> function on the value of type:
          </p>

          <figure>
            <pre><code><span><span>const</span> S = <span>struct</span> {</span>
<span>    int: <span>u32</span>,</span>
<span>    string: []<span>const</span> <span>u8</span>,</span>
<span>    nested: <span>struct</span> {</span>
<span>        int: <span>u32</span>,</span>
<span>    },</span>
<span>};</span>
<span></span>
<span><span>pub</span> <span>fn</span><span> main</span>() <span>void</span> {</span>
<span>    <span>const</span> s: S = .{</span>
<span>        .int = <span>1</span>,</span>
<span>        .string = <span>"hello"</span>,</span>
<span>        .nested = .{ .int = <span>2</span> },</span>
<span>    };</span>
<span>    print(S, s);</span>
<span>}</span>
<span></span>
<span><span>fn</span><span> print</span>(<span>comptime</span> T: <span>type</span>, value: T) <span>void</span> {</span>
<span>    <span>if</span> (T <span>==</span> <span>u32</span>) <span>return</span> print_u32(value);</span>
<span>    <span>if</span> (T <span>==</span> []<span>const</span> <span>u8</span>) <span>return</span> print_string(value);</span>
<span>    <span>switch</span> (<span>@typeInfo</span>(T)) {</span>
<span>        .@<span>"struct"</span> =&gt; <span>|</span>info<span>|</span> {</span>
<span>            print_literal(<span>"{"</span>);</span>
<span>            <span>var</span> space: []<span>const</span> <span>u8</span> = <span>""</span>;</span>
<span>            <span>inline</span> <span>for</span> (info.fields) <span>|</span>field<span>|</span> {</span>
<span>                print_literal(space);</span>
<span>                space = <span>", "</span>;</span>
<span></span>
<span>                print_literal(field.name);</span>
<span>                print_literal(<span>" = "</span>);</span>
<span>                <span>const</span> field_value = <span>@field</span>(value, field.name);</span>
<span>                print(field.<span>type</span>, field_value);</span>
<span>            }</span>
<span>            print_literal(<span>"}"</span>);</span>
<span>        },</span>
<span>        <span>else</span> =&gt; <span>comptime</span> <span>unreachable</span>,</span>
<span>    }</span>
<span>}</span>
<span></span>
<span><span>fn</span><span> print_u32</span>(value: <span>u32</span>) <span>void</span> {</span>
<span>    std.debug.print(<span>"{d}"</span>, .{value});</span>
<span>}</span>
<span></span>
<span><span>fn</span><span> print_string</span>(value: []<span>const</span> <span>u8</span>) <span>void</span> {</span>
<span>    std.debug.print(<span>"<span>\"</span>{s}<span>\"</span>"</span>, .{value});</span>
<span>}</span>
<span></span>
<span><span>fn</span><span> print_literal</span>(literal: []<span>const</span> <span>u8</span>) <span>void</span> {</span>
<span>    std.debug.print(<span>"{s}"</span>, .{literal});</span>
<span>}</span></code></pre>
          </figure>
          <p>
            Our <code>print</code> is set up exactly as our <code>f</code>
            before — the first argument is a comptime-known dispatch parameter.
            If <code>T</code> is an int or a string, the compiler calls <code>print_u32</code> or <code>print_string</code> directly.
          </p>
          <p>
            The third case is more complicated. First, we use <code>@typeInfo</code> to get a comptime value describing our type, and,
            in particular, the list of fields it has. Then, we iterate this list
            and recursively print each field. Note that although the list of
            fields is known in its entirety, we can’t <code>comptime for</code>
            it, we need <code>inline for</code>. This is because the <em>body</em> of our loop depends on the runtime
            <code>value</code>, and can’t be fully evaluated at compile time.
            This might be easier to see if you think in terms of functions. The
            <code>for</code> loop is essentially a map:
          </p>

          <figure>
            <pre><code><span>map :: [a] -&gt; (a -&gt; b) -&gt; [b]</span>
<span>map xs f = ...</span></code></pre>
          </figure>
          <p>
            If both <code>xs</code> and <code>f</code> are comptime-known, you
            can evaluate the entire loop at compile time. But in our case <code>f</code> actually closes over a runtime value, so we can’t evaluate
            everything. Still, we can specialize on the first argument, which
            <em>is</em> known at compile time. This is precisely the difference
            between <code>comptime</code> and <code>inline</code>
            <code>for</code>.
          </p>
        </section>
        <section id="No-DSLs">
          <h2>
            <a href="#No-DSLs">No DSLs </a>
          </h2>
          <p>
            Many meta programming systems, like macros in Lisp or Rust, not only
            <em>produce</em> arbitrary code, but also take arbitrary custom
            syntax as input, as long as parentheses are matched:
          </p>

          <figure>
            <pre><code><span><span>use</span> inline_python::python;</span>
<span></span>
<span><span>let</span> <span>who</span> = <span>"world"</span>;</span>
<span><span>let</span> <span>n</span> = <span>5</span>;</span>
<span>python! {</span>
<span>    <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>'n</span>):</span>
<span>        <span>print</span>(i, <span>"Hello"</span>, <span>'who</span>)</span>
<span>    <span>print</span>(<span>"Goodbye"</span>)</span>
<span>}</span></code></pre>
          </figure>
          <p>
            Zig doesn’t have any extension points for custom syntax. Indeed, you
            can’t pass Zig <em>syntax</em> (code) to comptime functions at all!
            Everything operates on Zig values. That being said, Zig is very
            lightweight when it comes to describing free-form data, so this
            isn’t much of a hindrance. <em>And</em>
            in any case, you can always pass your custom syntax as a comptime
            string. This is exactly how “printf” works:
          </p>

          <figure>
            <pre><code><span><span>pub</span> <span>fn</span><span> print</span>(<span>comptime</span> fmt: []<span>const</span> <span>u8</span>, args: <span>anytype</span>) <span>void</span></span></code></pre>
          </figure>
          <p>
            Here, <code>fmt</code> is an embedded DSL, which is checked at
            compile time to match the arguments.
          </p>
        </section>
        <section id="No-RTTI">
          <h2>
            <a href="#No-RTTI">No RTTI </a>
          </h2>
          <p>
            Zig printing code looks suspiciously close to how you’d do this sort
            of thing in a dynamic language like Python. In fact, it is <em>precisely</em> that same code, except that it is specialized over
            runtime type information Python has to enable this sort of thing.
            Furthermore, Zig actually <em>requires</em> that all type meta
            programming is specialized away. Types as values <em>only</em> exist
            at compile time. Still, looking at our print, we might be concerned
            over code size — we are effectively generating a fresh copy of <code>print</code> for any data structure. Our code will be smaller, and
            will compile faster if there’s just a single <code>print</code> that
            takes an opaque pointer and runtime parameter describing the type of
            the value (its fields and offsets). So let’s roll our own runtime
            type information. For our example, we support ints, strings, and
            structs with fields. For fields, our RTTI should include their names
            and offsets:
          </p>

          <figure>
            <pre><code><span><span>const</span> RTTI = <span>union</span>(<span>enum</span>) {</span>
<span>    <span>u32</span>,</span>
<span>    string,</span>
<span>    @<span>"struct"</span>: []<span>const</span> Field,</span>
<span></span>
<span>    <span>const</span> Field = <span>struct</span> {</span>
<span>        name: []<span>const</span> <span>u8</span>,</span>
<span>        offset: <span>u32</span>,</span>
<span>        rtti: RTTI,</span>
<span>    };</span>
<span>};</span></code></pre>
          </figure>
          <p>
            The printing itself is not particularly illuminating, we just need
            to cast an opaque pointer according to RTTI:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> print_dyn</span>(T: RTTI, value: <span>*</span><span>const</span> anyopaque) <span>void</span> {</span>
<span>    <span>switch</span> (T) {</span>
<span>        .<span>u32</span> =&gt; {</span>
<span>            <span>const</span> value_u32: <span>*</span><span>const</span> <span>u32</span> =</span>
<span>                <span>@alignCast</span>(<span>@ptrCast</span>(value));</span>
<span>            print_u32(value_u32.<span>*</span>);</span>
<span>        },</span>
<span>        .string =&gt; {</span>
<span>            <span>const</span> value_string: <span>*</span><span>const</span> []<span>const</span> <span>u8</span> =</span>
<span>                <span>@alignCast</span>(<span>@ptrCast</span>(value));</span>
<span>            print_string(value_string.<span>*</span>);</span>
<span>        },</span>
<span>        .@<span>"struct"</span> =&gt; <span>|</span>info<span>|</span> {</span>
<span>            print_literal(<span>"{"</span>);</span>
<span>            <span>var</span> space: []<span>const</span> <span>u8</span> = <span>""</span>;</span>
<span>            <span>for</span> (info) <span>|</span>field<span>|</span> {</span>
<span>                print_literal(space);</span>
<span>                space = <span>", "</span>;</span>
<span></span>
<span>                print_literal(field.name);</span>
<span>                print_literal(<span>" = "</span>);</span>
<span>                <span>const</span> field_ptr: <span>*</span><span>const</span> anyopaque =</span>
<span>                    <span>@as</span>([<span>*</span>]<span>const</span> <span>u8</span>, <span>@ptrCast</span>(value)) <span>+</span> field.offset;</span>
<span>            }</span>
<span>            print_literal(<span>"}"</span>);</span>
<span>        },</span>
<span>    }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            Finally, we need to compute <code>RTTI</code>, which amounts to
            taking comptime-only Zig type info and extracting important bits
            into an <code>RTTI</code> struct which is computed at compile time,
            but can exist at runtime as well:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> reflect</span>(<span>comptime</span> T: <span>type</span>) RTTI {</span>
<span>    <span>comptime</span> {</span>
<span>        <span>if</span> (T <span>==</span> <span>u32</span>) <span>return</span> .<span>u32</span>;</span>
<span>        <span>if</span> (T <span>==</span> []<span>const</span> <span>u8</span>) <span>return</span> .string;</span>
<span>        <span>switch</span> (<span>@typeInfo</span>(T)) {</span>
<span>            .@<span>"struct"</span> =&gt; <span>|</span>info<span>|</span> {</span>
<span>                <span>var</span> fields: [info.fields.len]Field = <span>undefined</span>;</span>
<span>                <span>for</span> (<span>&amp;</span>fields, info.fields) <span>|</span><span>*</span>slot, field<span>|</span> {</span>
<span>                    slot.<span>*</span> = .{</span>
<span>                        .name = field.name,</span>
<span>                        .offset = <span>@offsetOf</span>(T, field.name),</span>
<span>                        .rtti = reflect(field.<span>type</span>),</span>
<span>                    };</span>
<span>                }</span>
<span>                <span>const</span> fields_frozen = fields;</span>
<span>                <span>return</span> .{ .@<span>"struct"</span> = <span>&amp;</span>fields_frozen };</span>
<span>            },</span>
<span>            <span>else</span> =&gt; <span>unreachable</span>,</span>
<span>        }</span>
<span>    }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The call site is illustrative: we need <code>comptime</code> to <em>compute</em> the type information, but then we reify it as some
            real bytes in the binary, and use it as runtime value when calling
            <code>print_dyn</code>.
          </p>

          <figure>
            <pre><code><span><span>pub</span> <span>fn</span><span> main</span>() <span>void</span> {</span>
<span>    <span>const</span> s: S = .{</span>
<span>        .int = <span>1</span>,</span>
<span>        .string = <span>"hello"</span>,</span>
<span>        .nested = .{ .int = <span>2</span> },</span>
<span>    };</span>
<span>    print_dyn(<span>comptime</span> RTTI.reflect(S), <span>&amp;</span>s);</span>
<span>}</span></code></pre>
          </figure>
        </section>
        <section id="No-New-API">
          <h2>
            <a href="#No-New-API">No New API </a>
          </h2>
          <p>
            You can use Zig comptime to create new types. That’s how a <a href="https://matklad.github.io/2025/03/19/comptime-zig-orm.html">Zig ORM</a> can work. However, it is impossible to add methods to
            generated types, they must be inert bundles of fields. In Rust, when
            you use a derive macro, it can arbitrarily extend type’s public API,
            and you need to read proc macro docs (or look at the generated code)
            to figure out what’s available. In Zig, types’s API is always hand
            written, but it can use comptime reflection internally.
          </p>
          <p>
            So, if you are building a JSON serialization library in Zig, you
            can’t add <code>.to_json</code> method to user-types. You’ll
            necessarily have to supply a normal top-level function like
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> to_json</span>(<span>comptime</span> T: <span>type</span>, value: T, writer: Writer) <span>!</span><span>void</span> {</span>
<span>    ...</span>
<span>}</span></code></pre>
          </figure>
          <p>
            If you want to make sure that types explicitly opt-in JSON
            serialization, you need to ask the user to mark types specially:
          </p>

          <figure>
            <pre><code><span><span>const</span> Person = <span>struct</span> {</span>
<span>    first_name: []<span>const</span> <span>u8</span>,</span>
<span>    last_name: []<span>const</span> <span>u8</span>,</span>
<span></span>
<span>    <span>pub</span> <span>const</span> JSONOptions = .{</span>
<span>        .style = .camelCase,</span>
<span>    };</span>
<span>}</span></code></pre>
          </figure>
          <p>
            With this setup, <code>to_json</code> can only allow primitives and
            types with <code>JSONOptions</code>.
          </p>
        </section>
        <section id="No-IO">
          <h2>
            <a href="#No-IO">No IO </a>
          </h2>
          <p>
            Last but not least, Zig comptime does not allow any kind of input
            output. There isn’t even any kind of sandbox, as there are no IO
            facilities in the first place. So, while compiling the code, you
            can’t talk to your database to generate the schema. In exchange,
            compile time evaluation is hermetic, reproducible, safe, and
            cacheable.
          </p>
          <p>
            If you do need to talk to the database at build time, you can still
            do that, just through the build system! Zig’s <code>build.zig</code>
            is a general purpose build system, which easily supports the
            use-case of running an arbitrary Zig program to generate arbitrary
            Zig code which can then be normally
            <code>@import</code>ed.
          </p>
        </section>
        <section id="El-Disco">
          <h2>
            <a href="#El-Disco">El Disco </a>
          </h2>
          <p>
            <a href="https://www.tedinski.com/2018/01/30/the-one-ring-problem-abstraction-and-power.html">Any abstraction has two sides</a>. Powerful abstractions are useful
            because they are more expressive. But the flip-side is that
            abstraction-using code becomes harder to reason about, because the
            space of what it can possibly do is so vast. This dependency is not
            zero sum. A good abstraction can be simultaneously more powerful and
            easier to reason about than a bad one.
          </p>
          <p>
            Meta programming is one of the more powerful abstractions. It is
            very capable in Zig, and comes at a significant cost — Zig doesn’t
            have declaration-site type checking of comptime code. That being
            said, I personally find Zig’s approach to be uniquely tidy, elegant,
            and neat! It does much less than alternative systems, but ends
            extremely ergonomic in practice, and <em>relatively</em> easy to
            wrap ones head around.
          </p>
        </section>
      </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The skill of the future is not 'AI', but 'Focus' (150 pts)]]></title>
            <link>https://www.carette.xyz/posts/focus_will_be_the_skill_of_the_future/</link>
            <guid>43744394</guid>
            <pubDate>Sun, 20 Apr 2025 15:28:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.carette.xyz/posts/focus_will_be_the_skill_of_the_future/">https://www.carette.xyz/posts/focus_will_be_the_skill_of_the_future/</a>, See on <a href="https://news.ycombinator.com/item?id=43744394">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>If you frequent Hacker News regurlarly, you have likely noticed the buzz around engineers using AI
(specifically Large Language Models, or LLMs) to tackle Computer Science problems.</p>
<p>I want to be clear: <strong>I’m not against LLMs</strong>.<br>
LLMs are incredibly powerful tools, and <strong>can</strong> be a huge boon to engineers.
They can automate repetitive tasks, generate code snippets, help with brainstorming, assist in debugging, …
and this can frees up engineers’ time and mental energy, which could be channeled into more complex, creative problem-solving.<br>
But, like any tool, LLMs should be used <strong>wisely</strong>.<br>
LLMs can hallucinate, exhibit inconsistencies (especially with self-reflection models), and harbor biases. These limitations mean that LLM outputs require careful review before they can be trusted.</p>
<p>A key concern with LLMs lies in their training data.<br>
The data can be biased, contradictory sometimes, but those data contain solutions to known problems.<br>
If an engineer wants to “reinvent the wheel,” an LLM might offer a solution (good or bad, depending on the prompt). But when faced with truly <em>novel</em> problems, LLMs often provide unreliable responses, placing the burden of error detection squarely on the engineer.</p>
<p>This reliance on readily available solutions, particularly for familiar problems, creates a real risk: engineers may inadvertently atrophy their own problem-solving skills, hindering their ability to tackle truly novel challenges.<br>
The solution lies is <strong>balance</strong>, and a focus on the “why”, not just the “what”.<br>
Engineers should strive to understand the <em>reasoning</em> behind LLM-generated solutions, not simply accept them blindly.  Blind acceptance shifts the focus from <em>solving</em> problems to merely <em>obtaining</em> a solution.  Crucially, solving complex problems often depends on mastering simpler and foundational skills, which the engineer might lose quickly.</p>
<p>This idea summarizes why I disagree with those who equate the LLM revolution to the rise of search engines, like Google in the 90s.
Search enginers offer a good choice between <em>Exploration</em> (crawl through the list and pages of results) and
<em>Exploitation</em> (click on the top result).<br>
LLMs, however, do not give this choice, and tend to encourage immediate exploitation instead.
Users may explore if the first solution <strong>does not work</strong>, but the first
choice is <strong>always</strong> to exploit.<br>
<em>Exploitation</em> and <em>exploration</em> are complementary. Remove the exploration and you will introduce more and more instability into the exploitation process.</p>
<p>Computer Science emerged because Humans needed <strong>tools</strong> to solve problems faster and wanted to <strong>focus</strong> on the real problems, not repetitive tasks.
Humans built machines to accelerate problem-solving, but engineers remained the masters of the algorithms.<br>
I fear we’re losing our grip on this mastery.
Not because engineers are becoming less and less intelligent, but because the pressure to deliver solutions quickly is paramount.<br>
In embracing these “fast-paced solutions”, we risk losing a fundamental skill: <em>focus</em>. Because focus, like any skill, requires practice.</p>
<p>This is a worrying trend. If engineers become less adept at solving complex problems, what does the future hold?
Will our ability to tackle complex challenges rest solely on self-reflecting AIs, rather than human ingenuity?</p>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jagged AGI: o3, Gemini 2.5, and everything after (143 pts)]]></title>
            <link>https://www.oneusefulthing.org/p/on-jagged-agi-o3-gemini-25-and-everything</link>
            <guid>43744173</guid>
            <pubDate>Sun, 20 Apr 2025 14:55:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.oneusefulthing.org/p/on-jagged-agi-o3-gemini-25-and-everything">https://www.oneusefulthing.org/p/on-jagged-agi-o3-gemini-25-and-everything</a>, See on <a href="https://news.ycombinator.com/item?id=43744173">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>Amid today’s AI boom, it’s disconcerting that we still don’t know how to measure how smart, creative, or empathetic these systems are. Our tests for these traits, never great in the first place, were made for humans, not AI. </span><a href="https://ai-analytics.wharton.upenn.edu/generative-ai-labs/research-and-technical-reports/tech-report-prompt-engineering-is-complicated-and-contingent/" rel="">Plus, our recent paper testing prompting techniques</a><span> finds that AI test scores can change dramatically based simply on how questions are phrased. Even famous challenges like the Turing Test, where humans try to differentiate between an AI and another person in a text conversation, were designed as thought experiments at a time when such tasks seemed impossible. But now that </span><a href="https://arxiv.org/pdf/2503.23674" rel="">a new paper shows that AI passes the Turing Test</a><span>, we need to admit that we really don’t know what that actually means. </span></p><p><span>So, it should come as little surprise that one of the most important milestones in AI development, Artificial General Intelligence, or AGI, is badly defined and much debated. Everyone agrees that it has something to do with the ability of AIs to perform human-level tasks, though no one agrees whether this means expert or average human performance, or how many tasks and which kinds an AI would need to master to qualify. Given the definitional morass surrounding AGI, illustrating its nuances and history from its precursors to its initial coining by Shane Legg, Ben Goertzel and Peter Voss to today is challenging. As an experiment in both substance and form (and speaking of potentially intelligent machines) I delegated the work entirely to AI. I had Google Deep Research put together a </span><a href="https://docs.google.com/document/d/1VJ-OzBRJUChgUB0L0--dbdNjU2e-14PXnoTqNFYgXNQ/edit?tab=t.0" rel="">really solid 26 page summary on the topic</a><span>. I then had HeyGen turn it into a video podcast discussion between a twitchy AI-generated version of me and an AI-generated host. It’s not actually a bad discussion (though I don’t fully agree with AI-me), but every part of it, from the research to the video to the voices is 100% AI generated.</span></p><p><span>Given all this, it was interesting to see </span><a href="https://marginalrevolution.com/marginalrevolution/2025/04/o3-and-agi-is-april-16th-agi-day.html" rel="">this post </a><span>by influential economist and close AI observer Tyler Cowen declaring that o3 is AGI. Why might he think that?</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg" width="370" height="368.8787878787879" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1316,&quot;width&quot;:1320,&quot;resizeWidth&quot;:370,&quot;bytes&quot;:243143,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.oneusefulthing.org/i/161512556?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65e30c6c-abb0-46e9-9699-cbe7c4211185_1320x1507.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47409a3c-004a-47f4-903a-adb6807684ae_1320x1316.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>First, a little context. Over the past couple of weeks, two new AI models, Gemini 2.5 Pro from Google and o3 from OpenAI were released. These models, along with a set of slightly less capable but faster and cheaper models (Gemini 2.5 Flash, o4-mini, and Grok-3-mini), represent a pretty</span><a href="https://epoch.ai/data/ai-benchmarking-dashboard" rel=""> large leap in benchmarks</a><span>. But benchmarks aren’t everything, as Tyler pointed out. For a real-world example of how much better these models have gotten, we can turn to </span><a href="https://a.co/d/2qRbAxA" rel="">my book</a><span>. To illustrate a chapter on how AIs can generate ideas, a little over a year ago I asked ChatGPT-4 to come up with marketing slogans for a new cheese shop:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png" width="389" height="304.7465437788018" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:510,&quot;width&quot;:651,&quot;resizeWidth&quot;:389,&quot;bytes&quot;:159134,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.oneusefulthing.org/i/161510512?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b89b3d-85f2-426d-89be-424e88aebabf_651x510.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>Today I gave the latest successor to GPT-4, o3, an </span><em>ever so slightly</em><span> more involved version of the same prompt: “</span><strong>Come up with 20 clever ideas for marketing slogans for a new mail-order cheese shop. Develop criteria and select the best one. Then build a financial and marketing plan for the shop, revising as needed and analyzing competition. Then generate an appropriate logo using image generator and build a website for the shop as a mockup, making sure to carry 5-10 cheeses that fit the marketing plan.” </strong><span>With that single prompt, in less than two minutes, the AI not only provided a list of slogans, but ranked and selected an option, did web research, developed a logo, built marketing and financial plans, and launched a demo website for me to react to. The fact that my instructions were vague, and that common sense was required to make decisions about how to address them, was not a barrier.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png" width="1456" height="723" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:723,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2019800,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.oneusefulthing.org/i/161510512?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b4aa8b2-65fb-479e-8776-5e552195f88b_4390x2179.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>In addition to being, presumably, a larger model than GPT-4, o3 also works as a </span><a href="https://www.oneusefulthing.org/p/a-new-generation-of-ais-claude-37" rel="">Reasoner </a><span>- you can see its “thinking” in the initial response. It also is an agentic model, one that can use tools and decide how to accomplish complex goals. You can see how it took multiple actions with multiple tools, including web searches and coding, to come up with the extensive results that it did.</span></p><p><span>And this isn’t the only extraordinary examples, o3 can also do an impressive job guessing locations from photos if you just give it an image and prompt </span><strong>“be a geo-guesser”</strong><span> (with some quite profound privacy implications). Again, you can see the agentic nature of this model at work, as it zooms into parts of the picture, adds web searches, and does multi-step processes to get the right answer.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png" width="525" height="434.4951923076923" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1205,&quot;width&quot;:1456,&quot;resizeWidth&quot;:525,&quot;bytes&quot;:3215925,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.oneusefulthing.org/i/161510512?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe91fa6e3-cf1d-4d85-bc99-564d5fc18fea_2062x1707.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Or I gave o3 a large dataset of historical machine learning systems as a spreadsheet and asked </span><strong>“figure out what this is and generate a report examining the implications statistically and give me a well-formatted PDF with graphs and details”</strong><span> and got a full analysis with a single prompt. (I did give it some feedback to make the PDF better, though, as you can see).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png" width="1456" height="669" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:669,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:694441,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.oneusefulthing.org/i/161512556?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e01ecd-26b6-4569-b5d2-86798bdc1102_3529x1622.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>This is all pretty impressive stuff and you should experiment with these models on your own. </span><a href="https://gemini.google.com/u/1/app" rel="">Gemini 2.5 Pro</a><span> is free to use and as “smart” as o3, though it lacks the same full agentic ability. If you haven’t tried it or o3, take a few minutes to do it now. Try </span><a href="https://x.com/emollick/status/1910534521998487709" rel="">giving Gemini an academic paper and asking it to turn the paper into a game</a><span> or have it brainstorm with you for startup ideas, or just ask for the AI</span><a href="https://g.co/gemini/share/b2ce16d04017" rel=""> to impress you</a><span> (and then keep saying “more impressive”). Ask the Deep Research option to do a research report on your industry, or to research a purchase you are considering, or to </span><a href="https://bsky.app/profile/emollick.bsky.social/post/3lmdminw4m22o" rel="">develop a marketing plan for a new product</a><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png" width="374" height="253.2753623188406" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:514,&quot;width&quot;:759,&quot;resizeWidth&quot;:374,&quot;bytes&quot;:169048,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.oneusefulthing.org/i/161512556?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43be9304-5824-4995-ae0d-1ea1514090ff_759x514.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>You might find yourself “feeling the AGI” as well. Or maybe not. Maybe the AI failed you, even when you gave it the exact same prompt I used. If so, you just encountered the jagged frontier.</p><p><a href="https://www.oneusefulthing.org/p/centaurs-and-cyborgs-on-the-jagged" rel="">My co-authors and I coined the term “Jagged Frontier”</a><span> to describe the fact that AI has surprisingly uneven abilities. An AI may succeed at a task that would challenge a human expert but fail at something incredibly mundane. For example, consider this puzzle, a variation on a classic old brainteaser (a concept first explored by </span><a href="https://x.com/colin_fraser" rel="">Colin Fraser</a><span> and expanded by </span><a href="https://x.com/goodside/status/1790912819442974900" rel="">Riley Goodside</a><span>): </span><em>"A young boy who has been in a car accident is rushed to the emergency room. Upon seeing him, the surgeon says, "I can operate on this boy!" How is this possible?"</em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png" width="443" height="461.75132275132273" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a0b6c099-fc6c-42c2-b78b-380972895420_945x985.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:985,&quot;width&quot;:945,&quot;resizeWidth&quot;:443,&quot;bytes&quot;:66935,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.oneusefulthing.org/i/161512556?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0b6c099-fc6c-42c2-b78b-380972895420_945x985.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span> o3 insists the answer is “the surgeon is the boy’s mother,” which is wrong, as a careful reading of the brainteaser will show. Why does the AI come up with this incorrect answer? Because that is the answer to the classic version of the riddle, meant to expose unconscious bias: </span><em>“A father and son are in a car crash, the father dies, and the son is rushed to the hospital. The surgeon says, 'I can't operate, that boy is my son,' who is the surgeon?”</em><span> The AI has “seen” this riddle in its training data so much that even the smart o3 model fails to generalize to the new problem, at least initially. And this is just one example of the kinds of issues and hallucinations that even advanced AIs can fall prey to, showing how jagged the frontier can be.</span></p><p><span>But the fact that the AI often messes up on this particular brainteaser does not take away from the fact that it can </span><a href="https://scale.com/leaderboard/enigma_eval" rel="">solve much harder brainteasers</a><span>, or that it can do the other impressive feats I have demonstrated above. That is the nature of the Jagged Frontier. In some tasks, AI is unreliable. In others, it is superhuman. You could, of course, say the same thing about calculators, but it is also clear that AI is different. It is already demonstrating general capabilities and performing a wide range of intellectual tasks, including those that it is not specifically trained on. Does that mean that o3 and Gemini 2.5 are AGI? Given the definitional problems, I really don’t know, but I do think they can be credibly seen as a form of “Jagged AGI” - superhuman in enough areas to result in real changes to how we work and live, but also unreliable enough that human expertise is often needed to figure out where AI works and where it doesn’t. Of course, models are likely to become smarter, and a good enough Jagged AGI may still beat humans at every task, including in ones the AI is weak in.</span></p><p><span>Returning to Tyler’s post, you will notice that, despite thinking we have achieved AGI, he doesn’t think that threshold </span><a href="https://marginalrevolution.com/marginalrevolution/2025/02/why-i-think-ai-take-off-is-relatively-slow.html" rel="">matters much to our lives in the near term</a><span>. That is because, as many people have pointed out, technologies do not instantly change the world, no matter how compelling or powerful they are. Social and organizational structures change much more slowly than technology, and technology itself takes time to diffuse. Even if we have AGI today, we have years of trying to figure out how to integrate it into our existing human world.</span></p><p><span>Of course, that assumes that AI acts like </span><a href="https://knightcolumbia.org/content/ai-as-normal-technology" rel="">a normal technology</a><span>, and one whose jaggedness will never be completely solved. There is the possibility that this may not be true. The agentic capabilities we're seeing in models like o3, like the ability to decompose complex goals, use tools, and execute multi-step plans independently, might actually accelerate diffusion dramatically compared to previous technologies. If and when AI can effectively navigate human systems on its own, rather than requiring integration, we might hit adoption thresholds much faster than historical precedent would suggest.</span></p><p>And there's a deeper uncertainty here: are there capability thresholds that, once crossed, fundamentally change how these systems integrate into society? Or is it all just gradual improvement? Or will models stop improving in the future as LLMs hit a wall? The honest answer is we don't know.</p><p><span>What's clear is that we continue to be in uncharted territory. The latest models represent something qualitatively different from what came before, whether or not we call it AGI. Their agentic properties, combined with their jagged capabilities, create a genuinely novel situation with few clear analogues. It may be that history continues to be the best guide, and that figuring out how to successfully apply AI in a way that shows up in the economic statistics may be a process measured in decades. Or it might be that we are on the edge of some sort of </span><a href="https://www.nytimes.com/2025/04/03/technology/ai-futures-project-ai-2027.html" rel="">faster take-off</a><span>, where AI-driven change sweeps our world suddenly. Either way, those who learn to navigate this jagged landscape now will be best positioned for what comes next… whatever that is.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://www.oneusefulthing.org/p/on-jagged-agi-o3-gemini-25-and-everything?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.oneusefulthing.org/p/on-jagged-agi-o3-gemini-25-and-everything?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png" width="349" height="219.13953488372093" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/afdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:864,&quot;width&quot;:1376,&quot;resizeWidth&quot;:349,&quot;bytes&quot;:2622025,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.oneusefulthing.org/i/161512556?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafdef6bb-e268-4abe-a2ed-53ae3a68d4f1_1376x864.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why on Earth is OpenAI buying Windsurf? (184 pts)]]></title>
            <link>https://theahura.substack.com/p/tech-things-openai-buys-windsurf</link>
            <guid>43743993</guid>
            <pubDate>Sun, 20 Apr 2025 14:28:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf">https://theahura.substack.com/p/tech-things-openai-buys-windsurf</a>, See on <a href="https://news.ycombinator.com/item?id=43743993">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>The quiet news of the last few days was the leak/announcement of a $3 billion OpenAI acquisition of Windsurf. That's not the largest private acquisition ever made — that honor goes to Google's $30 billion acquisition of Wiz a few months prior</span><strong> </strong><span>— but man, it's up there! $3B is the kind of exit startup founders dream about. Especially for a startup that's been around for 2 years, </span><a href="https://news.ycombinator.com/item?id=42127882" rel="">with its current branding for about 5 months</a><span>.</span></p><p>I assume most people don't know what Windsurf is, which is fair because it has so few users that when you try to Google for that information you get data about the sport.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png" width="1040" height="409" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:409,&quot;width&quot;:1040,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:117182,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://theahura.substack.com/i/161689970?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff572743-5141-4eaf-918f-90c7f25aca6e_1040x409.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Thanks Gemini!</figcaption></figure></div><p><span>Maybe that’s unfair, supposedly the company </span><a href="https://www.latent.space/p/windsurf?open=false" rel="">has over a million users</a><span>. But I'm always a bit skeptical of numbers like that. A person who uses Windsurf every day is obviously in a different category than one that installed the tool to play around for five minutes and quickly discarded it. This slipperiness has always been one of the benefits of working in the world of privately held companies. Anecdotally I know only one person who uses Windsurf, and I only kinda sorta know that person because he's just a guy that I met at an </span><a href="https://theahura.substack.com/p/notes-from-the-sf-party-scene" rel="">SF house party</a><span>.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-1-161689970" target="_self" rel="">1</a></span></p><p>If you aren't familiar with Windsurf, you may know it by its previous name, Codeium. And if you aren't familiar with Codeium, you may know its primary competition, a company called Cursor. And if you don't know what Cursor is, a) you might know what GitHub Copilot is, and b) how did you find my blog?</p><p>All of these products are roughly in the same category of "AI tools for software engineers". They all have basically the same form factor too — they integrate AI models directly into your coding workflow. And traditionally they operate on three levels of granularity:</p><ul><li><p>Auto-complete. As you type the AI will suggest the rest of the line or function, which you can generally accept with a single button press.</p></li><li><p>Sidebar Q&amp;A. The code window itself will have an integrated sidebar where you can ask models to modify a few files. You'll get a diff, which you can then choose to apply or modify.</p></li><li><p>Agentic flows. The term "agent" is wildly underspecified, but in the AI coding space the term has generally come to mean "an AI model operates in a loop over an entire code base, often with a very vague or high level prompt as a starting point, and is equipped with tools to write, run, and otherwise analyze code and the computer system".</p></li></ul><p>Different companies aim to be best in class along different verticals. Some are better at the auto complete (Copilot), others at the agent flow (Claude Code). Some aim to be the best for non-technical people (Bolt or Replit), others for large enterprises (again, Copilot). Still, all of this "differentiation" ends up making a 1-2% difference in product. In fact, I can't stress enough how much the UX and core functionality of these tools is essentially identical. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png" width="1159" height="692" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:692,&quot;width&quot;:1159,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Cursor Download Free (Windows) - 0.48.9 | Softpedia&quot;,&quot;title&quot;:&quot;Cursor Download Free (Windows) - 0.48.9 | Softpedia&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="Cursor Download Free (Windows) - 0.48.9 | Softpedia" title="Cursor Download Free (Windows) - 0.48.9 | Softpedia" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fd1fe08-4081-4353-8b9e-278709d03c08_1159x692.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>A screencap of cursor. This has become the ‘canonical’ UX for vibe coding — a sidebar with integrated chat that is able to call out to tools and directly change code using diffs.</figcaption></figure></div><p>These all would have pejoratively been known as "GPT wrappers" just two years ago, because they do not actually compete on the model layer but rather allow users to choose and switch between any of the big LLM providers. To really emphasize how interchangeable these AI code assistants all are, I use an even lesser known tool called Avante, an entirely free and open source neovim plugin. It does the same things as all the other tools. I like it because I don't have to leave vim.</p><p><span>But the similarities of these tools does not take away from how game changing they are. Once you get used to a form of AI powered coding, you cannot go back. The real issue with all of these products is that they are</span><em> </em><span>too</span><em> </em><span>easily verticalized. Anyone who wants to spin up a version of Windsurf with one slight change that targets a tiny market segment can do so fairly easily — again, I'm on Avante entirely because it supports vim. That means the addressable market for any of these companies may actually go </span><em>down</em><span> over time as more competitors and free alternatives enter the market, even as the number of "programmers" goes up.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-2-161689970" target="_self" rel="">2</a></span><span> In point of fact, even though I love the ingenuity behind Cursor (which really spearheaded the current AI coding paradigm) I have openly said that their long term opportunities are slim. Even though Cursor had significant first mover advantage, they have no moat or stickiness. As with the rest of the AI market, switching cost remains extremely low, and there is simply no reason to use Cursor when you can use a free version or one with better enterprise support.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-3-161689970" target="_self" rel="">3</a></span><span> Cursor isn't even living on its own platform — it's a fork of VSCode. I am personally convinced that their only long term exit opportunity is an acquisition by Microsoft, and even that seems less and less likely as Satya puts more resources into the already-VSCode-native Copilot as a real competitor.</span></p><p><span>All of which makes the $3B price tag for Windsurf seem eye wateringly high. Compared to Cursor, Windsurf has fewer users, has been around for less time, has less brand recognition, and has diminishing prospects for future growth. It’s not as tied to VSCode, which is a plus, I guess. But it all begs the question: why on </span><em>earth</em><span> is OpenAI paying so much?</span></p><p><span>This is especially strange in the context of OpenAI's financial situation. </span><a href="https://www.thealgorithmicbridge.com/p/google-is-winning-on-every-ai-front" rel="">Smart</a><span> </span><a href="https://www.wired.com/story/google-openai-gemini-chatgpt-artificial-intelligence/" rel="">observers</a><span> </span><a href="https://www.reddit.com/r/singularity/comments/1hh03ri/google_is_winning_the_ai_race/" rel="">have</a><span> </span><a href="https://www.pymnts.com/news/artificial-intelligence/2025/google-debuts-touted-gemini-winner-take-all-ai-model-race" rel="">caught</a><span> </span><a href="https://medium.com/artificial-corner/how-google-quietly-took-the-lead-in-the-ai-race-with-gemini-2-5-c98dfb58b6a1" rel="">on</a><span> </span><a href="https://venturebeat.com/ai/from-catch-up-to-catch-us-how-google-quietly-took-the-lead-in-enterprise-ai/" rel="">to</a><span> Google's inherent advantages in the space, something that I first publicly called out as early as </span><a href="https://theahura.substack.com/p/tech-things-gpt-pro-and-the-state?" rel="">December</a><span> and more fully </span><a href="https://theahura.substack.com/p/tech-things-gemini-25-and-the-bull?utm_source=publication-search" rel="">two weeks ago</a><span>. OpenAI needs to shore up both its access to compute and its access to data in order to compete. But it's once-sterling relationship with its previous patron, Microsoft, has frayed significantly. This has essentially forced the company to go to SoftBank (yes, that SoftBank) for additional capital.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-4-161689970" target="_self" rel="">4</a></span><span> </span></p><p><span>It's true that OpenAI managed to get $40bn committed, and it's also true that this is the largest amount of capital ever raised by a privately held company. But they're going against </span><em>Google</em><span>, one of the most valuable companies in the entire world, and extremely profitable to boot. That's a hell of a war chest to compete with.</span></p><p><span>In that light, the decision to spend 3 out of 40 of those billions is even harder to rationalize.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-5-161689970" target="_self" rel="">5</a></span><span> Even worse, it's not yet clear OpenAI actually </span><em>has </em><span>$40bn to spend — so far they've only got $10bn actually lined up, with the rest being held by SoftBank contingent on OpenAI actually becoming a for-profit company by the end of the year.</span></p><p>It seems pretty obvious that Windsurf will not help OpenAI get more compute. Maybe Windsurf is providing OpenAI access to data? There's certainly some possibility that this is the case — though it makes me wonder just how bad OpenAI's relationship with Microsoft has gotten if they no longer have access to GitHub, which surely dwarfs any amount of code that Windsurf could provide.</p><p><span>The other possibility is that this is entirely a long term distribution play, akin to Facebook buying WhatsApp or Instagram. People criticized those deals for being overpriced too. OpenAI may think that Windsurf will be a crown jewel in how people access GPT models. There's some sense in this — OpenAI has also announced a social media project, likely also an attempt at maintaining lines to unique data sources while providing more native ways to improve distribution and "</span><a href="https://gwern.net/complement" rel="">commoditize their complement</a><span>".</span></p><p><span>But the issue that they will inevitably run into with Windsurf is that GPT just isn't the best in class for programming. Everyone who's using Windsurf is almost definitely using Claude or Gemini. Even though the "GPT wrapper" term was always meant as an insult, it is in practice a huge table stakes feature to be able to wrap around many different LLM providers. That flexibility is what allows a company like Windsurf to ride the machine learning wave, buoyed along by everyone else's investments. Cursor really only took off when Claude suddenly got really good at programming, after all. If Windsurf ends up being tied exclusively to GPT, many of its users may leave the platform simply because it is now a worse platform. But if there isn't any vendor lock in, we're back to square one — what is the </span><em>point</em><span>?</span></p><p>Personally, I don't get it. Maybe someone smarter than I am (or more connected than I am) can help me figure it out. But for now, I'm chalking this particular check size as a symptom of the AI market being way too hot right now.</p><p>The other quiet news of the last few days is the dawning realization of just how quiet it has been. The last two weeks saw the release of 3 new OpenAI models — o3, o4-mini, and GPT 4.1 — as well as the new Llama 4 model family from Meta and Grok-3 from Grok. And…nothing. It's just crickets. In past months, a release calendar like this would have had headlines blazing. The hype train should be chugging at ridiculous speeds. But compared to what I'd expect, there's nothing.</p><p>The reason is obvious: Google is still in the lead. Take a look at these two charts.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png" width="1456" height="529" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7e34249c-afab-4751-87ae-0932f234e111_1584x575.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:529,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:137731,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://theahura.substack.com/i/161689970?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e34249c-afab-4751-87ae-0932f234e111_1584x575.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg" width="1456" height="1067" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1067,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Imagen&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="Imagen" title="Imagen" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41c4c230-9fa1-4333-8332-4e98419ba870_3080x2258.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Even though this chart is only two weeks old, it is already out of date. </figcaption></figure></div><p>The former is the current state of the LMSYS chatbot arena; the latter maps chatbot arena performance against price. The over under? There's no headlines because there's nothing to write about. "OpenAI takes second place" no one cares!</p><p><span>It’s still too early to write about ‘general consensus’ since the OpenAI models were released only a few days ago. And to their credit, those models </span><em>do </em><span>top many of the LLM benchmarks.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-6-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-6-161689970" target="_self" rel="">6</a></span><span> But so far, the </span><a href="https://www.reddit.com/r/ChatGPTCoding/comments/1k10ehv/openais_o3_and_o4mini_just_dethroned_gemini_25_pro/" rel="">reception has been extremely muted</a><span>, with many saying something like:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png" width="867" height="195" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:195,&quot;width&quot;:867,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:37315,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://theahura.substack.com/i/161689970?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3874ceec-fd23-44dc-99cf-34c24c89851e_867x195.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Even though some of the new OpenAI models are quite powerful, they are simply too expensive and too slow for not enough extra juice. </p><p><span>I've already </span><a href="https://theahura.substack.com/p/tech-things-gemini-25-and-the-bull" rel="">written extensively about Google's Gemini 2.5 release</a><span>, which quickly became the go-to model for just about everything. What I didn't originally clock was just how much Google had shored up its model offerings all over the price/performance curve. Put bluntly: at every price point, the best model is a Google model.</span></p><p><span>That's not all. I mentioned rumors that Google has disallowed new publications; that is now confirmed as of </span><a href="https://arstechnica.com/ai/2025/04/deepmind-is-holding-back-release-of-ai-research-to-give-google-an-edge/" rel="">earlier this month</a><span>:</span></p><blockquote><p>Among the changes in the company’s publication policies is a six-month embargo before “strategic” papers related to generative AI are released. Researchers also often need to convince several staff members of the merits of publication, said two people with knowledge of the matter.</p></blockquote><p><span>Google has also apparently started offering </span><a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/google-accused-of-paying-employees-to-do-nothing-for-up-to-a-year-to-stifle-ai-talent-migration" rel="">extremely generous non-compete deals</a><span> to researchers, preferring to keep them on payroll doing nothing than have them go to competitors and leak secrets:</span></p><blockquote><p><a href="https://www.tomshardware.com/tag/google" rel="">Google</a><span> is making use of aggressive noncompete clauses and extended notice periods, contends former GoogDeepMinder Nando de Freitas in a recent post on X. In some cases, Google DeepMind’s employment contracts may lock an AI developer into doing nothing for as long as a year, notes </span><a href="https://www.businessinsider.com/google-deepmind-ai-talent-war-aggressive-noncompetes-2025-4" rel="">Business Insider</a><span>, to prevent its AI talent from moving to competing firms. That’s a long time away from working on the cutting edge in the rapidly developing world of AI.</span></p></blockquote><p>And finally, Google has continued to release and improve its TPU offerings on GCP, giving them yet another method to profit off the back of the AI boom — even if they don't win on the model, they can win by providing the underlying hardware.</p><p><span>With almost no fanfare, we all just woke up one day to a Google-dominated AI landscape. I have been </span><a href="https://theahura.substack.com/p/tech-things-eu-ai-act-biden-admin?utm_source=publication-search" rel="">critical of Sundar in the past</a><span>, but I have to hand it to him — sometimes the showmanship really is just a distraction from executing a slow but precise strategy.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-7-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-7-161689970" target="_self" rel="">7</a></span><span> Google is clearly now on a war footing. They are relentlessly poaching employees while trying to close up their shop as much as possible. The AI industry as a whole owes more to Google than any other organization. It's unclear how many other players in that industry will survive when cut off from Google's research. It's also unclear how much this will last in the face of a continuing DOJ antitrust suit. More on that in a different article, though.</span></p><p><span>One last thought. I've always been a staunch defender of capitalism and free markets, even though that's historically been an unpopular opinion in my particular social circle. Watching the LLM market, I can't help but feel extremely vindicated. Over the last 5 years, the cost per token has been driven down relentlessly even as model quality has skyrocketed. The brutal and bruising competition between the tech giants has left nothing but riches for the average consumer. There's an alternative world where all of this is priced so high that only the wealthiest businesses can justify a "GPT license", or where the government ends up keeping all the best AI technology for themselves. That world would objectively suck — not only would most people not be able to access the technology, there would also be significantly less interest in or ability to innovate. Just look at Google, which has finally risen like a beast from slumber to show the world what it means to innovate once more.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-8-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-8-161689970" target="_self" rel="">8</a></span><span> </span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-9-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-9-161689970" target="_self" rel="">9</a></span></p><p>Since we've been talking about things that didn't happen, I want to talk about one last notable absence: where the hell is Apple?</p><p><a href="https://theahura.substack.com/p/tech-things-gpt-pro-and-the-state" rel="">Something</a><span> </span><a href="https://theahura.substack.com/p/tech-things-deepseek-r1-and-the-arrival?utm_source=publication-search" rel="">I've</a><span> </span><a href="https://theahura.substack.com/p/tech-things-deepseek-but-make-it?utm_source=publication-search" rel="">said</a><span> </span><a href="https://theahura.substack.com/p/tech-things-gemini-25-and-the-bull?utm_source=publication-search" rel="">repeatedly</a><span> is that the LLM market has strong winner-take-all effects, and players in the market are heavily dependent on access to scientists, compute, and data. Apple is an extraordinarily wealthy company, so they have no problem getting access to scientists. But it seems like they have had a ton of issues on both of the latter two categories.</span></p><p><span>On the compute side, it seems like Apple sorta own goaled themselves? From the </span><a href="https://www.nytimes.com/2025/04/11/technology/apple-issues-trump-tariffs.html" rel="">NYT</a><span>:</span></p><blockquote><p>The A.I. stumble was set in motion in early 2023. Mr. Giannandrea, who was overseeing the effort, sought approval from the company’s chief executive, Tim Cook, to buy more A.I. chips, known as graphics processing units, or GPUs, five people with knowledge of the request said. The chips, which can perform hundreds of computations at the same time, are critical to building the neural networks of A.I. systems, like chatbots, that can answer questions or write software code.</p><p>At the time, Apple’s data centers had about 50,000 GPUs that were more than five years old — far fewer than the hundreds of thousands of chips being bought at the time by A.I. leaders like Microsoft, Amazon, Google and Meta, these people said.</p><p>Mr. Cook approved a plan to double the team’s chip budget, but Apple’s finance chief, Luca Maestri, reduced the increase to less than half that, the people said. Mr. Maestri encouraged the team to make the chips they had more efficient.</p><p>The lack of GPUs meant the team developing A.I. systems had to negotiate for data center computing power from its providers like Google and Amazon, two of the people said. The leading chips made by Nvidia were in such demand that Apple used alternative chips made by Google for some of its A.I. development.</p></blockquote><p><span>Well, that at least explains why they haven’t been putting out any decent models. Anecdotally, Apple obviously has data centers, but they aren't a cloud provider like Google/Microsoft/Amazon, which at various points have powered DeepMind/OpenAI/Anthropic directly. So Apple is starting way behind on the whole chip thing. Maybe it makes some kind of strategic sense to try and double down on their own unique chip capacity — maybe try to do what Google has done with TPUs — but that's really being extremely generous. The more obvious answer is the simple one: Apple cheaped out, and was penny wise pound foolish. As a result, the company that dominated the mobile wave is all but absent from the AI wave.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-10-161689970" href="https://theahura.substack.com/p/tech-things-openai-buys-windsurf#footnote-10-161689970" target="_self" rel="">10</a></span></p><p>On the data side, Apple definitely own goaled themselves. In an environment of data hoarders and open disregard for information safety, Apple struck out as an ardent defender of user privacy. They made a brand out of it! They ran ads on it!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6e29cc1-c7dd-45d1-b4ec-385969d0ba8c_1600x900.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>That all made sense a few years ago, when the data itself was more questionably useful and Apple had full control of the hardware stack. Apple was able to poke fun at Google while </span><a href="https://www.businessinsider.com/facebook-blames-apple-10-billion-loss-ad-privacy-warning-2022-2" rel="">giving Meta a pretty serious black eye</a><span> over their ads policy.</span></p><p>But now, data is almost literally fuel for deep learning models. Worse, there's basically no way to avoid leaking data through the model! People have consistently been able to get models to directly reproduce training data! Google has more or less avoided using any training data because they can, they have the whole Internet already indexed. Meta, xAI, OpenAI, and Anthropic all train on public data — the former two from public posts on their social media platforms, and all four from extremely questionable flouting of copyright law. </p><p><span>Meanwhile, Apple is stuck with the same problem Google had </span><a href="https://theahura.substack.com/p/tech-things-openai-is-an-unaligned-590?utm_source=publication-search" rel="">when they got rid of their "Don't be evil" motto</a><span>. </span></p><blockquote><p><span>"Don't be evil" was a lot of things, and there were a lot of disagreeing interpretations about what it meant. One thing that no one disagreed about: it was hard to get rid of. Execs at Google ended up regretting the "Don't be evil" motto, because no matter what Google did they would get raked over the coals for doing it. "I thought you said you </span><em>wouldn't be evil</em><span>", internet commenters would snidely say. They even </span><a href="https://en.wikipedia.org/wiki/Don%27t_be_evil#Lawsuit" rel="">got sued over it</a><span>!</span></p></blockquote><p>Apple is in a similar boat. Either they use the user data they have and risk serious brand damage that the rest of FAANG is sure to capitalize on, or they handicap themselves in the AI race. Which, really, is less of a race and more of an all out brawl, one in which Apple is fighting with both hands behind its back.</p><p><span>So far, they've taken the "handicap" approach. They've tried to pay their way out of the data access problem by straight up </span><a href="https://www.inc.com/kit-eaton/apple-signs-deal-for-ai-training-data-from-image-service-shutterstock.html" rel="">buying the copyright licenses for data</a><span> they want to train on, but, come on, it's just not anywhere near enough training data.</span></p><p><span>The worst case scenario for Apple is they decide to use user data </span><em>late</em><span>. In that setting, Apple incurs the brand risk while also being miles behind everyone else. That increasingly seems like what will happen, though, because I just can't imagine Apple actually sitting the entire AI race out.</span></p><p>So yeah. All in all, a pretty quiet few weeks for AI.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemma 3 QAT Models: Bringing AI to Consumer GPUs (392 pts)]]></title>
            <link>https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/</link>
            <guid>43743337</guid>
            <pubDate>Sun, 20 Apr 2025 12:22:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/">https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/</a>, See on <a href="https://news.ycombinator.com/item?id=43743337">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    
      
    

    

    

    

    
    <div>
          

<div>
    <p data-block-key="sisye">Last month, we launched Gemma 3, our latest generation of open models. Delivering state-of-the-art performance, Gemma 3 quickly established itself as a leading model capable of running on a single high-end GPU like the NVIDIA H100 using its native BFloat16 (BF16) precision.</p><p data-block-key="4impe">To make Gemma 3 even more accessible, we are announcing new versions optimized with Quantization-Aware Training (QAT) that dramatically reduces memory requirements while maintaining high quality. This enables you to run powerful models like Gemma 3 27B locally on consumer-grade GPUs like the NVIDIA RTX 3090.</p>
</div>   

<div>
        
            <p><img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma3QuantizedChart_RD1_01_1_qlGnyVc.original.png" alt="Chatbot Arena Elo Score - Gemma 3 QAT"></p><p>
                    This chart ranks AI models by Chatbot Arena Elo scores; higher scores (top numbers) indicate greater user preference. Dots show estimated NVIDIA H100 GPU requirements.
                </p>
            
        
    </div>
  <div>
    <h2 data-block-key="sisye">Understanding performance, precision, and quantization</h2><p data-block-key="2a2jr">The chart above shows the performance (Elo score) of recently released large language models. Higher bars mean better performance in comparisons as rated by humans viewing side-by-side responses from two anonymous models. Below each bar, we indicate the estimated number of NVIDIA H100 GPUs needed to run that model using the BF16 data type.</p><p data-block-key="ae0co"><b><br>Why BFloat16 for this comparison?</b> BF16 is a common numerical format used during inference of many large models. It means that the model parameters are represented with 16 bits of precision. Using BF16 for all models helps us to make an apples-to-apples comparison of models in a common inference setup. This allows us to compare the inherent capabilities of the models themselves, removing variables like different hardware or optimization techniques like quantization, which we'll discuss next.</p><p data-block-key="3q45i">It's important to note that while this chart uses BF16 for a fair comparison, deploying the very largest models often involves using lower-precision formats like FP8 as a practical necessity to reduce immense hardware requirements (like the number of GPUs), potentially accepting a performance trade-off for feasibility.</p><h2 data-block-key="8r8jc"><b><br></b>The Need for Accessibility</h2><p data-block-key="dkjr0">While top performance on high-end hardware is great for cloud deployments and research, we heard you loud and clear: you want the power of Gemma 3 on the hardware you already own. We're committed to making powerful AI accessible, and that means enabling efficient performance on the consumer-grade GPUs found in desktops, laptops, and even phones.</p><h2 data-block-key="950c"><b><br></b>Performance Meets Accessibility with Quantization-Aware Training in Gemma 3</h2><p data-block-key="158md">This is where quantization comes in. In AI models, quantization reduces the precision of the numbers (the model's parameters) it stores and uses to calculate responses. Think of quantization like compressing an image by reducing the number of colors it uses. Instead of using 16 bits per number (BFloat16), we can use fewer bits, like 8 (int8) or even 4 (int4).</p><p data-block-key="3a7o0">Using int4 means each number is represented using only 4 bits – a 4x reduction in data size compared to BF16. Quantization can often lead to performance degradation, so we’re excited to release Gemma 3 models that are robust to quantization. We released several quantized variants for each Gemma 3 model to enable inference with your favorite inference engine, such as Q4_0 (a common quantization format) for Ollama, llama.cpp, and MLX.</p><p data-block-key="epsbi"><b><br>How do we maintain quality?</b> We use QAT. Instead of just quantizing the model after it's fully trained, QAT incorporates the quantization process during training. QAT simulates low-precision operations during training to allow quantization with less degradation afterwards for smaller, faster models while maintaining accuracy. Diving deeper, we applied QAT on ~5,000 steps using probabilities from the non-quantized checkpoint as targets. We reduce the perplexity drop by 54% (using llama.cpp perplexity evaluation) when quantizing down to Q4_0.</p><h2 data-block-key="eatdo"><b><br></b>See the Difference: Massive VRAM Savings</h2><p data-block-key="9f9so">The impact of int4 quantization is dramatic. Look at the VRAM (GPU memory) required just to load the model weights:</p><ul><li data-block-key="41cf1"><b>Gemma 3 27B:</b> Drops from 54 GB (BF16) to just <b>14.1 GB</b> (int4)</li></ul><ul><li data-block-key="5uebr"><b>Gemma 3 12B:</b> Shrinks from 24 GB (BF16) to only <b>6.6 GB</b> (int4)</li></ul><ul><li data-block-key="9d985"><b>Gemma 3 4B:</b> Reduces from 8 GB (BF16) to a lean <b>2.6 GB</b> (int4)</li></ul><ul><li data-block-key="bf7p"><b>Gemma 3 1B:</b> Goes from 2 GB (BF16) down to a tiny <b>0.5 GB</b> (int4)</li></ul>
</div>   

<div>
    <p><img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma3QuantizedChart_RD1_02.original.png" alt="Comparison chart of model weights showing VRAM required to load">
        
        
    </p>
</div>
  <div>
    <blockquote data-block-key="7ui9z"><b><sup>Note:</sup></b> <i><sup>This figure only represents the VRAM required to load the model weights. Running the model also requires additional VRAM for the KV cache, which stores information about the ongoing conversation and depends on the context length</sup></i></blockquote><h2 data-block-key="e1hmg"><b><br></b>Run Gemma 3 on Your Device</h2><p data-block-key="4jdc3">These dramatic reductions unlock the ability to run larger, powerful models on widely available consumer hardware:</p><ul><li data-block-key="bqsl5"><b>Gemma 3 27B (int4):</b> Now fits comfortably on a single desktop NVIDIA RTX 3090 (24GB VRAM) or similar card, allowing you to run our largest Gemma 3 variant locally.</li></ul><ul><li data-block-key="c4adc"><b>Gemma 3 12B (int4):</b> Runs efficiently on laptop GPUs like the NVIDIA RTX 4060 Laptop GPU (8GB VRAM), bringing powerful AI capabilities to portable machines.</li></ul><ul><li data-block-key="be3pr"><b>Smaller Models (4B, 1B):</b> Offer even greater accessibility for systems with more constrained resources, including phones and <a href="https://youtu.be/lgsD_wSZ0hI?si=pyQj23bOxNPLrxtL&amp;t=102">toasters</a> (if you have a good one).</li></ul><h2 data-block-key="7c8ds"><b><br></b>Easy Integration with Popular Tools</h2><p data-block-key="2324e">We want you to be able to use these models easily within your preferred workflow. Our official int4 and Q4_0 unquantized QAT models are available on Hugging Face and Kaggle. We’ve partnered with popular developer tools that enable seamlessly trying out the QAT-based quantized checkpoints:</p><ul><li data-block-key="avkt"><a href="https://ollama.com/library/gemma3"><b>Ollama</b></a><b>:</b> Get running quickly – all our Gemma 3 QAT models are natively supported starting today with a simple command.</li></ul><ul><li data-block-key="6sp3o"><a href="https://lmstudio.ai/model/gemma-3-12b-it-qat"><b>LM Studio</b></a><b>:</b> Easily download and run Gemma 3 QAT models on your desktop via its user-friendly interface.</li></ul><ul><li data-block-key="69oug"><a href="https://huggingface.co/collections/mlx-community/gemma-3-qat-68002674cd5afc6f9022a0ae"><b>MLX</b></a><b>:</b> Leverage MLX for efficient, optimized inference of Gemma 3 QAT models on Apple Silicon.</li></ul><ul><li data-block-key="19s6q"><a href="https://www.kaggle.com/models/google/gemma-3/gemmaCpp"><b>Gemma.cpp</b></a><b>:</b> Use our dedicated C++ implementation for highly efficient inference directly on the CPU.</li></ul><ul><li data-block-key="f05gl"><a href="https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b"><b>llama.cpp</b></a><b>:</b> Integrate easily into existing workflows thanks to native support for our GGUF-formatted QAT models.</li></ul><h2 data-block-key="c5073"><b><br></b>More Quantizations in the Gemmaverse</h2><p data-block-key="74vnv">Our official Quantization Aware Trained (QAT) models provide a high-quality baseline, but the vibrant <a href="https://ai.google.dev/gemma/gemmaverse">Gemmaverse</a> offers many alternatives. These often use Post-Training Quantization (PTQ), with significant contributions from members such as <a href="https://huggingface.co/bartowski/google_gemma-3-27b-it-GGUF">Bartowski</a>, <a href="https://huggingface.co/collections/unsloth/gemma-3-67d12b7e8816ec6efa7e4e5b">Unsloth</a>, and <a href="https://huggingface.co/collections/ggml-org/gemma-3-67d126315ac810df1ad9e913">GGML</a> readily available on Hugging Face. Exploring these community options provides a wider spectrum of size, speed, and quality trade-offs to fit specific needs.</p><h2 data-block-key="fnsl0"><b><br></b>Get Started Today</h2><p data-block-key="fdu54">Bringing state-of-the-art AI performance to accessible hardware is a key step in democratizing AI development. With Gemma 3 models, optimized through QAT, you can now leverage cutting-edge capabilities on your own desktop or laptop.</p><p data-block-key="179dk">Explore the quantized models and start building:</p><ul><li data-block-key="6uj07">Use on your PC with <a href="https://ollama.com/library/gemma3">Ollama</a></li></ul><ul><li data-block-key="741fu">Find the Models on <a href="https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b">Hugging Face</a> &amp; <a href="https://www.kaggle.com/models/google/gemma-3/transformers">Kaggle</a></li></ul><ul><li data-block-key="6069i">Run on your phone with <a href="https://developers.googleblog.com/en/gemma-3-on-mobile-and-web-with-google-ai-edge/">Google AI Edge</a></li></ul><p data-block-key="4j3l0">We can't wait to see what you build with Gemma 3 running locally!</p>
</div> 
      </div>
    

    

    
    
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[100 Years to Solve an Integral (2020) (200 pts)]]></title>
            <link>https://liorsinai.github.io/mathematics/2020/08/27/secant-mercator.html</link>
            <guid>43741273</guid>
            <pubDate>Sun, 20 Apr 2025 03:16:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://liorsinai.github.io/mathematics/2020/08/27/secant-mercator.html">https://liorsinai.github.io/mathematics/2020/08/27/secant-mercator.html</a>, See on <a href="https://news.ycombinator.com/item?id=43741273">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content">
        <p><em>The integral of sec(x) is well known to any beginners calculus student. Yet this integral was once a major outstanding maths problem. It was first introduced by Geradus Mercator who needed it to make his famous map in 1569. He couldn’t find it and used an approximation instead. The exact solution was found accidentally 86 years later without calculus in 1645. It then took another two decades until a formal proof was given in 1668, 99 years after Mercator first proposed the problem.</em></p>

<p><em>Update 13 March 2021: added a note on how Napier calculated logarithm trigonometry tables. This was prompted by a correction raised in a discussion of this post on <a href="https://news.ycombinator.com/item?id=24304311">HackerNews</a>.</em></p>

<p><em>Update 10 October 2021: the great circle and rhumb line images are now made with a script that uses <a href="https://scitools.org.uk/cartopy/docs/latest/">Cartopy</a>. Previously they were made with a Matlab application. You can see the new script at my <a href="https://github.com/LiorSinai/Navigation">Github repository</a> and enter your co-ordinates to generate your own lines.</em></p>

<p>As this <a href="https://www.smbc-comics.com/comic/how-math-works">comic</a> by SMBC rightly teases, the history of mathematics is often not so straightforward. 
Theorems, formulas and notation that are routinely discussed in class, were once insights or accidents themselves.
This is the story of one such formula, the integral of the secant. 
I first read about it almost a decade ago when I got interested in cartography: the science and art of map making.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup> 
This integral is of vital importance to the Mercator map and therefore many online maps that use it like <a href="https://www.apple.com/ios/maps/">Apple Maps</a> and <a href="https://www.google.com/maps/">Google Maps</a>.</p>

<p>This story has been told several times before: see <a href="https://www.jstor.org/stable/3603395">1</a>, <a href="https://doi.org/10.1080/0025570X.1980.11976846">2</a> or <a href="https://scholarworks.umt.edu/tme/vol7/iss2/12/">3</a>.
But these are all journal articles, consigned mostly to academics.
I want to present it here in a less formal and more colourful setting to make it more accessible.</p>

<p>This is an article about mathematics so familiarity with the following is helpful: algebra, trigonometry, radians and basic calculus. 
These are usually covered in advanced high school maths classes or first year maths courses.</p>

<h2 id="first-year-maths">First year maths</h2>

<p>In first year maths at university after a month of differentiation we were starting the inverse problem: integration. 
Differentiation  is the mathematics of finding gradient functions for curves.
Integration is the mathematics of inverting this - given a gradient function, what is the curve? 
My lecturer was introducing the integration of trigonometric functions.
He started off with:</p><p>

\[\int \sin(x) dx = -\cos(x) + c \:\text{  and } \int \cos(x) dx = \sin(x) + c\]

</p><p>This relationship made sense because sine and cosine derivatives were opposites. Just had to be careful of minus signs. Next he derived the integral for the tangent:</p><p>

\[\int \tan(x) dx = \int \frac{\sin(x)}{\cos(x)}dx = -\ln|\cos(x)| + c\]

</p><p>Ok, that was tricky. It was not immediately obvious that the inverse of the chain rule could be used here, because the function $\cos(x)$ was present with its derivative $\sin(x)$.
But given enough thought it made sense. Then he said, this is the integral of the secant and learn it off by heart:</p><p>

\[\int \sec(x) dx = \ln|\sec(x) + \tan(x)| + c\]

</p><p>OK, where did that come from? My lecturer offered no explanation. It was easy to verify that it worked by finding the derivative.<sup id="fnref:derivative" role="doc-noteref"><a href="#fn:derivative" rel="footnote">2</a></sup>
(Paper <a href="https://doi.org/10.1080/0025570X.1980.11976846">2</a> has a more complex proof using only integration.)
But how had he come up with that?</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/integral_secant_graph.png" alt="Secant integral graph">
	<figcaption>The curve $\ln|\sec(x) + \tan(x)|$ has tangents with a gradient of $\sec(x)$. Discovering this fact took 100 years.</figcaption>
</figure>

<p>I think at this point most first year calculus students like me have the following fleeting thoughts:</p>
<ol>
  <li>Integration is much harder than differentiation.</li>
  <li>Some mathematician must have stumbled on this through differentiation first.</li>
  <li>It doesn’t matter anyway because where will I ever use this?</li>
</ol>

<p>Actually number 1 is true as many a student can testify after writing a test. 
Number 2 is false - in fact it was found by a teacher while looking at raw numbers. 
Such a method for finding an integral is so unusual that one might conjecture it is the <em>only</em> integral that has been found like this.
Surely in calculus class where raw numbers are so rare you would be laughed at if you attempted to solve an integral like that.
Lastly, number 3 remains true for me. But this doesn’t mean this integral isn’t useful - it is used to construct the Mercator map. 
That is why a teacher was crunching numbers when he serendipitously realised what the formula was.</p>

<h2 id="quick-revision-trigonometry">Quick revision: trigonometry</h2>

<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/secant_def.png" alt="Definition of the secant">
</figure>

<p>The secant is a standard trigonometric function. It is defined as the ratio of the hypotenuse $c$ to the adjacent side $a$ for an angle $\varphi$ in a right angled triangle.
In mathematical notation the definition is:</p><p>

\[\sec(\varphi) = \frac{c}{a}\]

</p><p>It is the reciprocal of the more widely used cosine function:</p><p>

\[\sec(\varphi) = \frac{1}{\cos(\varphi)}\]

</p><p>Here are the graphs of the secant and cosine for the angles from $-2\pi$ (-360°) to $2\pi$ (360°):</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/secant_graph.png" alt="Secant graph">
</figure>

<p>The integral of the secant can be interpreted as the area under the graph.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">3</a></sup> This is illustrated by the shaded region</p>

<h2 id="an-introduction-to-cartography">An introduction to cartography</h2>

<p>The earth cannot be projected onto a flat map without distortion. 
Over the years cartographers have devised many different map projections which try to balance minimising distortion with other properties. 
They come in all shapes and sizes.
Lists of these projections can be found <a href="https://map-projections.net/singleview.php">here</a> or <a href="https://en.wikipedia.org/wiki/List_of_map_projections">here</a>.
I will explain two of the simplest here, which will help with understanding the Mercator map in the next section.</p>

<p>All map projections can be represented as equations that transform spherical co-ordinates to flat map co-ordinates.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">4</a></sup>
The co-ordinates on the sphere are the angles $\varphi$ and $\lambda$. These correspond to lines of latitude (parallels) and longitude (meridans) respectively. 
The co-ordinates on the flat map are $x$ and $y$. A map projection is therefore a transformation from $\varphi$ and $\lambda$ to $x$ and $y$.</p>

<p>One of the simplest and oldest known projections is the equirectangular projection:</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/equirectangular.png" alt="Equirectangular map">
	<figcaption>Equirectangular map. From <a href="https://map-projections.net/single-view/rectang-0">map-projections.net/singleviewp/rectang-0</a>  </figcaption>
</figure>

<p>It is made by mapping meridians and parallels to vertical and horizontal straight lines of constant spacing.
This has the affect of stretching out objects along the parallels.
The equations for this projection are:</p><p>

\[\begin{align} y &amp;= R\varphi\\ x &amp;= R\lambda \end{align}\]

</p><p>While the equations are simple the construction process can be hard to visualise. Here is my attempt:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/equirectangular_construction.png" alt="Equirectangular map construction">
	<figcaption>A segment of the sphere is peeled off and flattened.
The right most image shows a top view of the flattening. The arcs are pulled flat in the longitudinal direction so that they become straight lines.
They do not change length in this process but that requires stretching out the sides along the $x$-axis. 
Thus a single point where all the arcs meet is stretched into a line.   </figcaption>
</figure>

<p>The equirectangular map has a total area of $(2\pi R)(\pi R)=2\pi^2 R^2$ while the surface area of the sphere is $4\pi R^2$. 
Therefore this projection distorts the area by a factor of $\frac{\pi}{2}\approx1.57$.</p>

<p>A different kind of projection can be obtained by projecting lines from the sphere onto the mapping plane. 
An example is the Lambert cylindrical projection. 
It is made by wrapping a cylinder around the sphere and projecting points onto it via lines parallel to the $x$-axis. 
Here is a visualisation of this construction:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/Cilinderprojectie-constructie.jpg" alt="Lambert cylindrical map construction">
	<figcaption>From <a href="https://en.wikipedia.org/wiki/File:Cilinderprojectie-constructie.jpg">Wikipedia</a>  </figcaption>
</figure>

<p>And here is a cross section of the sphere along side the final map:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/Lambert_construction.png" alt="Lambert cylindrical  map">
	<figcaption>Lambert cylindrical  map. Edited from <a href="https://map-projections.net/single-view/lambert">map-projections.net/singleviewp/lambert</a>  </figcaption>
</figure>

<p>The equations are:</p><p>

\[\begin{align} y &amp;= R\sin(\varphi)\\ x &amp;= R\lambda \end{align}\]

</p><p>For objects near the equator this results in very little distortion such as for Africa. But objects near the poles are compressed because of the sphere’s curvature. 
This can be seen clearly with Greenland.</p>

<p>This map has the useful property that its area is equal to the surface area of the sphere. 
The area of the flat map is $(2\pi R)(2R) = 4\pi R^2$ which is the same as that of the sphere.
Thus, while objects are distorted, their areas are still correct.
The relative scale of Greenland to Africa is therefore accurately represented in this map.</p>

<h2 id="the-mercator-map">The Mercator map</h2>

<p>In 1569mGerardus Mercator wanted to make a global world map that would be useful for navigation. 
He lived in a time when sailing across vast ocean distances was the norm. (In 1492 Christopher Columbus had discovered America by sailing all the way from Spain.)
The maps shown above are fine for artistic impressions and applications but not for navigation.
The distortions prevent doing any accurate distance and bearing measurements on the map. 
At a local level lines (eg. roads) which in reality intersect perpendicularly to each other would be appear to be slanted with respect to each other.</p>

<p>In particular Mercator wanted to make a map where rhumb lines would be straight. Rhumb lines are curves of constant bearing relative to meridians. 
To follow a rhumb line a navigator only needs to maintain the same bearing on their compass for the whole journey.
For example, here is the rhumb line through modern day New York and Cape Town:</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/NY_to_CPT.png" alt="Arcs from NY to CPT">
	<figcaption>Orthographic map with great circle and rhumb line. Source code: <a href="https://github.com/LiorSinai/Navigation">link</a></figcaption>
</figure>

<p>At each point along the rhumb line the angle $\theta$ with the meridian is 48.56° which corresponds to a bearing of 311°26’18’’ from Cape Town to New York. 
I’ve also shown the great circle which is a circle whose centre lies on the centre of the sphere. 
Traveling along the great circle is always shorter. In this case the distance is 12550 km instead of 12600 km along the rhumb line.
With modern technology it is easy for ships and aeroplanes to stick to the great circles. But back in Mercator’s day this was rather difficult. 
So sailors preferred to stick to rhumb lines. They would rather get to their destination by traveling a little longer then get lost and travel a lot more.</p>

<p>Mercator’s idea was to stretch out a cylindrical projection map in the North-South direction to preserve shapes and angles.
Looking at the Lambert projection,<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">5</a></sup> it can be seen that a different stretch factor is required for each latitude. At the equator no stretch is required.
At the 45° parallel only a small amount of upward stretching is required. The objects close to the poles have to be stretched a lot to uncompress them.</p>

<p>This stretch factor can be calculated as follows:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/Mercator_construction.png" alt="Mercator construction">
	<figcaption>Construction process for the Mercator map. Edited from <a href="https://en.wikipedia.org/wiki/Mercator_projection"> Wikipedia</a> images </figcaption>
</figure>

<p>First Mercator divided the globe into graticules of equal spacing $\delta\varphi$ and $\delta\lambda$. 
Along the meridians the arc length of each graticule is $R\delta\varphi$.
Along the parallels the radius of the circle is $R\cos(\varphi)$ so that the arc length is $(R\cos(\varphi))\delta\lambda$.
The tangent can be then approximated as:</p><p>

\[\tan(\alpha) \approx \frac{R\cos(\varphi)\delta\lambda}{R\delta\varphi}\]

</p><p>This graticule is then flattened into the rectangle with the following two requirements:</p>

<ol>
  <li>The angles are kept constant by setting $\alpha =\beta$.</li>
  <li>The parallels are projected on to the $x$-axis like in the Lambert projection. This means $\delta x = R\delta \lambda$.</li>
</ol>

<p>Therefore the transformation is:</p><p>

\[\begin{align} \tan(\alpha) &amp;= \tan(\beta) \\ 
\frac{R\cos(\varphi)\delta\lambda}{R\delta\varphi} &amp;= \frac{\delta x}{\delta y} \\
\delta y &amp;=  \frac{\delta x}{\delta \lambda} \frac{1}{\cos(\varphi)} \delta \varphi = R \sec(\varphi) \delta \varphi
 \end{align}\]

</p><p>From here it is a small step to turn this into an integral. However, calculus was only properly invented a century later after Mercator published his map. 
Instead what Mercator did was realise that he could add up the stretch factors at each point. 
The stretch at graticule <em>n</em> is approximately the stretch of the graticule below it plus $R \sec(\varphi) \delta \varphi$. This can then be turned into a sum:</p><p>

\[\begin{align} y_n &amp;\approx R \sec(n \cdot \delta \varphi) \delta \varphi + y_{n-1} \\
                     &amp;=\sum^{n}_{k=0} R \sec(k \cdot \delta \varphi) \delta \varphi \end{align}\]

</p><p>Using a constant value for $\delta \varphi $ Mercator was able to calculate the spacings for his map. Then he drew a world map over it.
This is a modern rendering of the final result:</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/mercator_NY_to_CPT.png" alt="Mercator map">
	<figcaption>Mercator map with great circle and rhumb line. Source code: <a href="https://github.com/LiorSinai/Navigation">link</a></figcaption>
</figure>

<p>This map is very heavily distorted. Greenland now looks larger than Africa. The great circles lie along strange curves. But the rhumb lines are straight!
Calculating the bearing can be done simply with a ruler and a projector. 
No thinking or maths required! (And no fancy instruments on globes either.) In modern day terms we would say, it was a big hit with sailors.</p>

<p>For online maps the local projection is very important. If you are looking for directions in a city what matters most to you is that the roads look correct.
This is why the Mercator map is used - to preserve angles between grids in roads. Other map projections don’t meet this simple requirement.
A minor problem is that the scale changes at each latitude but online maps can easily calculate this at each point. 
Try this with <a href="https://www.google.com/maps/">Google Maps</a>. For the same zoom factor, the scale bar is not the same length at each latitude. 
Also, along the equator, the scale bar has a minimum of 5m. Up near the poles, which is way more stretched out, the scale bar goes down to 1m.
But you don’t notice this when zooming in on a specific point in the map.</p>

<p>For the record, if you are going to look at long distances on Google Maps it’s best to turn the “Globe view” option on.</p>

<h2 id="tables-for-trig-mercator-and-logs">Tables for trig, Mercator and logs</h2>

<p>This is where things take an unexpected turn. 
But in order to understand how, I want to explain another part of history: mathematical  tables. 
These days pocket calculators are so common that we have forgotten that they were once ubiquitous with maths.
This was true even into the ’90s.</p>

<p>In the past if you wanted to calculate sec(36°) you could draw a big triangle and physically measure the angle and distances with the ruler. 
Then you could write out the long division calculation.
More likely, however, you would read up the value in a trigonometry table. 
The numbers in these tables were painstakingly calculated using approximation formulas and trigonometric identities. 
But for the user they were very simple. You just had to look up the numbers and occasionally interpolate between numbers if you wanted higher accuracy. 
These tables had the added benefit of making inversion easy. 
For example if you looked for the number 1.23606 in the secant table, you would see it was next to 36°.</p>

<p>In 1599 Edward Wright published tables for the equator Mercator map equation. He used $\delta \varphi = 1’ = \frac{1}{60} 1^{\circ}$. 
He also gave the first mathematical description of the Mercator map which Mercator himself did not explain fully. 
This made it easier for others to make their own Mercator maps.</p>

<p>In 1614 John Napier introduced logarithms. This is the inverse of the exponential operation. In modern terms, the logarithm $y$ of $x$ to base $b$ is written as:</p><p>

\[y = \log_b x  \quad ; \quad b^y = x\]

</p><p>Napier’s main motivation was to find an easier way to do multiplication and division. 
For example from the laws of exponents:</p><p>

\[2^a \div 2^b = 2^{a-b}\]

</p><p>Therefore a division can be done as follows:</p><p>

\[3764 \div 873 = 2^{11.878} \div 2^{9.770} = 2^{2.108} = 4.311\]

</p><p>Where $\log_{2}(3764) = 11.878 $ and $\log_{2}(873) = 9.770 $</p>

<p>The logarithms again had to be painstakingly calculated through approximation calculations. 
Napier did this using a kinetic framework. While this idea may be unusual today, it has to do with how Napier originally visualised logarithms.<sup id="fnref:Napier" role="doc-noteref"><a href="#fn:Napier" rel="footnote">6</a></sup>
His final table related numbers to logarithms and their sines.<sup id="fnref:correction" role="doc-noteref"><a href="#fn:correction" rel="footnote">7</a></sup> Here is an example:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/Napiertable.png" alt="Napier trigonometric table">
	<figcaption>Napier's original trigonometric table. From <a href="https://jscholarship.library.jhu.edu/bitstream/handle/1774.2/34187/31151005337641.pdf">John Napier and the Invention of Logarithms</a></figcaption>
</figure>

<p>A  user could use such a table to look up a logarithm with the added benefit that inversion was easy.
These proved to be very popular - people clearly did not like doing multiplication and division in the past.
Using addition and subtraction in their place also made calculations less error prone, especially with successive calculations.</p>

<p>Next, mathematicians extended these tables to other trigonometric function:</p>
<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/APN2002-table3-30deg.png" alt="Logarithmic trigonometric table">
	<figcaption>A page from the 2002 American Practical Navigator. Values are calculated as $log_{10}(f(n^\circ + \frac{k}{60})) + 10$. From <a href="https://en.wikipedia.org/wiki/File:APN2002-table3-30deg.tiff"> Wikipedia</a></figcaption>
</figure>

<p>In 1645, according to legend, a teacher named Henry Bond noticed something strange. 
The numbers in Wright’s Mercator table were similar to the numbers in a $\log_e(\tan(\varphi))$ table.
They just were offset by a factor of 2 and 45° in the tables. So he essentially conjectured that:</p><p>

\[\int_0^{\varphi_1} \sec(\varphi) d\varphi = \ln \left| \tan \left( \frac{\varphi_1}{2} + 45^\circ \right) \right |\]

</p><p>Mathematicians caught on to the claim but could not prove it. Calculus was still in its infancy. 
In 1668, 99 years after Mercator first made his map and 23 years after Bond gave the solution, it was finally proven by James Gregory.
This proof however was considered long-winded and “wearisome”.
In 1670 Isaac Barrow offered a more succinct proof through integration with partial fractions which can be found in <a href="https://doi.org/10.1080/0025570X.1980.11976846">2</a>.</p>

<p>Lastly, through trigonometric identities, it can be proven that the following three formulas are all equivalent:</p><p>

\[\int \sec(\varphi) d\varphi = 
\begin{cases} 
\ln |\sec(\varphi) + \tan(\varphi) | + c \\
\ln \left| \tan \left( \frac{\varphi}{2} + 45^\circ \right) \right | + c\\ 
\frac{1}{2}\ln \left| \frac{1+\sin(\varphi)}{1-\sin(\varphi)}  \right| + c
\end{cases}\]

</p><h2 id="conclusion">Conclusion</h2>

<p>This has been a long post. I hope you found this history as fascinating as I did.
It truly is remarkable to me that this little formula on my first year exam had such a colourful and varied history.
I really think it should be taught more in class. This was already tried and tested by <a href="https://scholarworks.umt.edu/tme/vol7/iss2/12/">3</a>. 
At a small scale they found it worked.</p>

<p>If you are more interested in how Google makes its map I highly suggest reading this blog post by a Google engineer:
<a href="https://medium.com/google-design/google-maps-cb0326d165f5">medium.com/google-design/google-maps-cb0326d165f5</a>.
Can you spot the integral of the secant in the Google code?</p>

<p>There is one last comment I would like to add. 
There is a lot of controversy surrounding the Mercator map.
It is an extremely common projection. When I was younger I had a map of the world on my wall in the Mercator projection.
However I hope you now fully appreciate its main purpose is navigation.
Outside of that, it unnecessarily distorts shapes and in particular makes the Americas and Europe look much larger than they actually are.
This has been linked, not without rational, to colonialism and racism. 
For decades cartographers have bemoaned its use in applications where it really has no right to be.<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" rel="footnote">8</a></sup>
Here is even an amusing clip from a 90’s TV show: <a href="https://www.youtube.com/watch?v=vVX-PrBRtTY">www.youtube.com/watch?v=vVX-PrBRtTY</a>.</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/secant-mercator/Robinson&amp;Winkel.png" alt="Robinson and Winkel Triple projections">
	<figcaption>Robinson projection (left) and Winkel Triple projection (right). Don't they look so much more natural? Source: <a href="https://en.wikipedia.org/wiki/List_of_map_projections">Wikipedia</a></figcaption>
</figure>

<p>There are many different projections out there, all with their own purpose. 
My personal favourite is the Winkel Triple. 
It is the official map of the National Geographic Society.
It is an elegant compromise between form and scale, in both the final representation and in the mathematics. 
A more general favourite is the Robinson Projection.
It was designed with an “artistic approach”. Unlike the other projections, instead of using equations, Arthur H. Robinson manually fixed the scale factors at 5° intervals.</p>

<hr>



        <hr>
        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pretty State Machine Patterns in Rust (2016) (122 pts)]]></title>
            <link>https://hoverbear.org/blog/rust-state-machine-pattern/</link>
            <guid>43741051</guid>
            <pubDate>Sun, 20 Apr 2025 02:14:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hoverbear.org/blog/rust-state-machine-pattern/">https://hoverbear.org/blog/rust-state-machine-pattern/</a>, See on <a href="https://news.ycombinator.com/item?id=43741051">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <p>Lately I've been thinking a lot about the <em>patterns</em> and <em>structures</em> which we program with. It's really wonderful to start exploring a project and see familiar patterns and styles which you've already used before. It makes it easier to understand the project, and empowers you to start working on the project faster.</p>
<p>Sometimes you're working on a new project and realize that you need to do something in the same way as you did in another project. This <em>thing</em> might not be a functionality or a library, it might not be something which you can encode into some clever macro or small crate. Instead, it may be simply a pattern, or a structural concept which addresses a problem nicely.</p>
<p>One interesting pattern that is commonly applied to problems is that of the 'State Machine'. Let's take some time to consider what exactly we mean when we say that, and why they're interesting.</p>
<span id="continue-reading"></span>
<blockquote>
<p>Throughout this post you can run all examples in <a rel="noopener" target="_blank" href="https://play.rust-lang.org/">the playground</a>, I typically use 'Nightly' out of habit.</p>
</blockquote>
<h2 id="founding-our-concepts"><a href="#founding-our-concepts" aria-label="Anchor link for: founding-our-concepts">Founding Our Concepts</a></h2>
<p>There are a <strong>lot</strong> of resources and topical articles about state machines out there on the internet. Even more so, there are a lot of <strong>implementations</strong> of state machines.</p>
<p>Just to get to this web page you used one. You can model TCP as a state machine. You can model HTTP requests with one too. You can model any <em>regular</em> language, such as a regex, as a state machine. They're everywhere, hiding inside things we use every day.</p>
<p>So, a State Machine is any <strong>'machine'</strong> which has a set of <strong>'states'</strong> and <strong>'transitions'</strong> defined between them.</p>
<p>When we talk about a machine we're referring to the abstract concept of something which <em>does something</em>. For example, your 'Hello World!' function is a machine. It is started and eventually outputs what we expect it to. Some model which you use to interact with your database is just the same. We'll regard our most basic machine simply as a <code>struct</code> that can be created and destroyed.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>struct</span> </span><span><span>Machine</span></span><span>;</span>
</span><span>
</span><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>  <span>let</span> my_machine <span>=</span> Machine<span>;</span> <span> Create.
</span></span></span></span><span><span><span>  <span> `my_machine` is destroyed when it falls out of scope below.
</span></span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>States are a way to reason about <em>where</em> a machine is in its process. For example, we can think about a bottle filling machine as an example. The machine is in a 'waiting' state when it is waiting for a new bottle. Once it detects a bottle it moves to the 'filling' state. Upon detecting the bottle is filled it enters the 'done' state. After the bottle is left the machine we return to the 'waiting' state.</p>
<p>A key takeaway here is that none of the states have any information relevant for the other states. The 'filling' state doesn't care how long the 'waiting' state waited. The 'done' state doesn't care about what rate the bottle was filled at. Each state has <em>discrete responsibilities and concerns</em>. The natural way to consider these <em>variants</em> is as an <code>enum</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>enum</span> <span>BottleFillerState</span> <span><span>{</span>
</span></span></span><span><span><span>  Waiting <span><span>{</span> waiting_time<span>:</span> <span>std<span>::</span></span><span>time<span>::</span></span>Duration </span><span><span>}</span></span><span>,</span>
</span></span></span><span><span><span>  Filling <span><span>{</span> rate<span>:</span> <span>usize</span> </span><span><span>}</span></span><span>,</span>
</span></span></span><span><span><span>  Done<span>,</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>struct</span> </span><span><span>BottleFiller</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>  <span>state</span><span>:</span> BottleFillerState,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>Using an <code>enum</code> in this way means all the states are mutually exclusive, you can only be in one at a time. Rust's 'fat enums' allow us to have each of these states to carry data with them as well. As far as our current definition is concerned, everything is totally okay.</p>
<p>But there is a bit of a problem here. When we described our bottle filling machine above we described three transitions: <code>Waiting -&gt; Filling</code>, <code>Filling -&gt; Done</code>, and <code>Done -&gt; Waiting</code>. We never described <code>Waiting -&gt; Done</code> or <code>Done -&gt; Filling</code>, those don't make sense!</p>
<p>This brings us to the idea of transitions. One of the nicest things about a true state machine is we never have to worry about our bottle machine going from <code>Done -&gt; Filling</code>, for example. The state machine pattern should <strong>enforce</strong> that this can never happen. Ideally this would be done before we even start running our machine, at compile time.</p>
<p>Let's look again at the transitions we described for our bottle filler in a diagram:</p>
<pre><code><span>  +++++++++++   +++++++++++   ++++++++
</span><span>  |         |   |         |   |      |
</span><span>  | Waiting +--&gt;+ Filling +--&gt;+ Done |
</span><span>  |         |   |         |   |      |
</span><span>  ++++-++++-+   +++++++++++   +--+++++
</span><span>       ^                         |
</span><span>       +++++++++++++++++++++++++-+
</span></code></pre>
<p>As we can see here there are a finite number of states, and a finite number of transitions between these states. Now, it is possible to have a valid transition between each state and every other state, but in most cases this is not true.</p>
<p>This means moving between a state such as 'Waiting' to a state such as 'Filling' should have defined semantics. In our example this can be defined as "There is a bottle in place." In the case of a TCP stream it might be "We have received a FIN packet" which means we need to finish closing out the stream.</p>
<h2 id="determining-what-we-want"><a href="#determining-what-we-want" aria-label="Anchor link for: determining-what-we-want">Determining What We Want</a></h2>
<p>Now that we know what a state machine is, how do we represent them in Rust? First, let's think about what we <strong>want</strong> from some pattern.</p>
<p>Ideally, we'd like to see the following characteristics:</p>
<ul>
<li>Can only be in one state at a time.</li>
<li>Each state should have its own associated values if required.</li>
<li>Transitioning between states should have well defined semantics.</li>
<li>It should be possible to have some level of shared state.</li>
<li>Only explicitly defined transitions should be permitted.</li>
<li>Changing from one state to another should <strong>consume</strong> the state so it can no longer be used.</li>
<li>We shouldn't need to allocate memory for <strong>all</strong> states. No more than largest sized state certainly</li>
<li>Any error messages should be easy to understand.</li>
<li>We shouldn't need to resort to heap allocations to do this. Everything should be possible on the stack.</li>
<li>The type system should be harnessed to our greatest ability.</li>
<li>As many errors as possible should be at <strong>compile-time</strong>.</li>
</ul>
<p>So if we could have a design pattern which allowed for all these things it'd be truly fantastic. Having a pattern which allowed for most would be pretty good too.</p>
<h2 id="exploring-possible-implementation-options"><a href="#exploring-possible-implementation-options" aria-label="Anchor link for: exploring-possible-implementation-options">Exploring Possible Implementation Options</a></h2>
<p>With a type system as powerful and flexible as Rusts we should be able to represent this. The truth is: there are a number of ways to try, each has valuable characteristics, and each teaches us lessons.</p>
<h3 id="a-second-shot-with-enums"><a href="#a-second-shot-with-enums" aria-label="Anchor link for: a-second-shot-with-enums">A Second Shot with Enums</a></h3>
<p>As we saw above the most natural way to attempt this is an <code>enum</code>, but we noted already that you can't control which transitions are actually permitted in this case. So can we just wrap it? We sure can! Let's take a look:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>enum</span> <span>State</span> <span><span>{</span>
</span></span></span><span><span><span>    Waiting <span><span>{</span> waiting_time<span>:</span> <span>std<span>::</span></span><span>time<span>::</span></span>Duration </span><span><span>}</span></span><span>,</span>
</span></span></span><span><span><span>    Filling <span><span>{</span> rate<span>:</span> <span>usize</span> </span><span><span>}</span></span><span>,</span>
</span></span></span><span><span><span>    Done
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>struct</span> </span><span><span>StateMachine</span> </span><span><span><span>{</span> <span>state</span><span>:</span> State </span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>impl</span> </span><span><span>StateMachine</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>new</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Self</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        StateMachine <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> <span>State<span>::</span></span>Waiting <span><span>{</span> waiting_time<span>:</span> <span>std<span>::</span></span><span>time<span>::</span></span><span>Duration<span>::</span></span>new<span><span>(</span><span>0</span><span>,</span> <span>0</span></span><span><span>)</span></span> </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>to_filling</span></span><span><span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        <span>self</span><span>.</span>state <span>=</span> <span>match</span> <span>self</span><span>.</span>state <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            </span></span></span></span></span></span><span><span><span><span><span><span>            <span>State<span>::</span></span>Waiting <span><span>{</span> <span>..</span> </span><span><span>}</span></span> <span>=&gt;</span> <span>State<span>::</span></span>Filling <span><span>{</span> rate<span>:</span> <span>1</span> </span><span><span>}</span></span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            </span></span></span></span></span></span><span><span><span><span><span><span>            <span>_</span> <span>=&gt;</span> <span>panic!</span><span><span>(</span><span><span>"</span>Invalid state transition!<span>"</span></span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span>    <span> ...
</span></span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>let</span> <span>mut</span> state_machine <span>=</span> <span>StateMachine<span>::</span></span>new<span><span>(</span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>    state_machine<span>.</span><span>to_filling</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>At first glance it seems okay. But notice some problems?</p>
<ul>
<li>Invalid transition errors happen at runtime, which is awful!</li>
<li>This only prevents invalid transitions <em>outside</em> of the module, since the private fields can be manipulated freely inside the module. For example, <code>state_machine.state = State::Done</code> is perfectly valid inside the module.</li>
<li>Every function we implement that works with the state has to include a match statement!</li>
</ul>
<p>However this does have some good characteristics:</p>
<ul>
<li>The memory required to represent the state machine is only the size of the largest state. This is because a fat enum is only as big as its biggest variant.</li>
<li>Everything happens on the stack.</li>
<li>Transitioning between states has well defined semantics... It either works or it crashes!</li>
</ul>
<p>Now you might be thinking "Hoverbear you could totally wrap the <code>to_filling()</code> output with a <code>Result&lt;T,E&gt;</code> or have an <code>InvalidState</code> variant!" But let's face it: That doesn't make things that much better, if at all. Even if we get rid of the runtime failures we still have to deal with a lot of clumsiness with the match statements and our errors would still only be found at runtime! Ugh! We can do better, I promise.</p>
<p>So let's keep looking!</p>
<h3 id="structures-with-transitions"><a href="#structures-with-transitions" aria-label="Anchor link for: structures-with-transitions">Structures With Transitions</a></h3>
<p>So what if we just used a set of structs? We could have them all implement traits which all states should share. We could use special functions that transitioned the type into the new type! How would it look?</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>//</span> This is some functionality shared by all of the states.
</span></span><span><span><span>trait</span> <span>SharedFunctionality</span> <span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>get_shared_value</span></span><span><span><span>(</span><span>&amp;</span><span>self</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>usize</span></span></span><span>;</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>struct</span> </span><span><span>Waiting</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>waiting_time</span><span>:</span> <span>std<span>::</span></span><span>time<span>::</span></span>Duration,
</span></span></span><span><span><span>    <span> Value shared by all states.
</span></span></span></span><span><span><span>    <span>shared_value</span><span>:</span> <span>usize</span>,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>impl</span> </span><span><span>Waiting</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>new</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Self</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        Waiting <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            waiting_time<span>:</span> <span>std<span>::</span></span><span>time<span>::</span></span><span>Duration<span>::</span></span>new<span><span>(</span><span>0</span><span>,</span><span>0</span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            shared_value<span>:</span> <span>0</span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span>    <span> Consumes the value!
</span></span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>to_filling</span></span><span><span><span>(</span><span>self</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> Filling</span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        Filling <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            rate<span>:</span> <span>1</span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            shared_value<span>:</span> <span>0</span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>impl</span> </span><span>SharedFunctionality <span>for</span></span><span> <span>Waiting</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>get_shared_value</span></span><span><span><span>(</span><span>&amp;</span><span>self</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>usize</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        <span>self</span><span>.</span>shared_value
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>struct</span> </span><span><span>Filling</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>rate</span><span>:</span> <span>usize</span>,
</span></span></span><span><span><span>    <span> Value shared by all states.
</span></span></span></span><span><span><span>    <span>shared_value</span><span>:</span> <span>usize</span>,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>impl</span> </span><span>SharedFunctionality <span>for</span></span><span> <span>Filling</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>get_shared_value</span></span><span><span><span>(</span><span>&amp;</span><span>self</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>usize</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        <span>self</span><span>.</span>shared_value
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> ...
</span></span><span>
</span><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>let</span> in_waiting_state <span>=</span> <span>Waiting<span>::</span></span>new<span><span>(</span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>    <span>let</span> in_filling_state <span>=</span> in_waiting_state<span>.</span><span>to_filling</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>Gosh that's a buncha code! So the idea here was that all states have some common shared values along with their own specialized values. As you can see from the <code>to_filling()</code> function we can consume a given 'Waiting' state and transition it into a 'Filling' state. Let's do a little rundown:</p>
<ul>
<li>Transition errors are caught at compile time! For example you can't even create a <code>Filling</code> state accidentally without first starting with a <code>Waiting</code> state. (You could on purpose, but this is beside the matter.)</li>
<li>Transition enforcement happens everywhere.</li>
<li>When a transition between states is made the old value is <strong>consumed</strong> instead of just modified. We could have done this with the enum example above as well though.</li>
<li>We don't have to <code>match</code> all the time.</li>
<li>Memory consumption is still lean, at any given time the size is that of the state.</li>
</ul>
<p>There are some downsides though:</p>
<ul>
<li>There is a bunch of code repetition. You have to implement the same functions and traits for multiple structures.</li>
<li>It's not always clear what values are shared between all states and just one. Updating code later could be a pain due to this.</li>
<li>Since the size of the state is variable we end up needing to wrap this in an <code>enum</code> as above for it to be usable where the state machine is simply one component of a more complex system. Here's what this could look like:</li>
</ul>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>enum</span> <span>State</span> <span><span>{</span>
</span></span></span><span><span><span>    Waiting<span><span>(</span>Waiting</span><span><span>)</span></span><span>,</span>
</span></span></span><span><span><span>    Filling<span><span>(</span>Filling</span><span><span>)</span></span><span>,</span>
</span></span></span><span><span><span>    Done<span><span>(</span>Done</span><span><span>)</span></span><span>,</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>let</span> in_waiting_state <span>=</span> <span>State<span>::</span></span>Waiting<span><span>(</span><span>Waiting<span>::</span></span>new<span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>    <span> This doesn't work since the `Waiting` struct is wrapped! We need to `match` to get it out.
</span></span></span></span><span><span><span>    <span>let</span> in_filling_state <span>=</span> <span>State<span>::</span></span>Filling<span><span>(</span>in_waiting_state<span>.</span><span>to_filling</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>As you can see, this isn't very ergonomic. We're getting closer to what we want though. The idea of moving between distinct types seems to be a good way forward! Before we go try something entirely different though, let's talk about a simple way to change our example that could enlighten further thinking.</p>
<p>The Rust standard library defines two highly related traits: <a rel="noopener" target="_blank" href="https://doc.rust-lang.org/std/convert/trait.From.html"><code>From</code></a> and <a rel="noopener" target="_blank" href="https://doc.rust-lang.org/std/convert/trait.Into.html"><code>Into</code></a> that are extremely useful and worth checking out. An important thing to note is that implementing one of these automatically implements the other. In general implementing <code>From</code> is preferable as it's a bit more flexible. We can implement them very easily for our above example like so:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>//</span> ...
</span></span><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span>Waiting<span>&gt;</span></span> <span>for</span></span><span> <span>Filling</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> Waiting</span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> Filling</span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        Filling <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            rate<span>:</span> <span>1</span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            shared_value<span>:</span> val<span>.</span>shared_value<span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>//</span> ...
</span></span></code></pre>
<p>Not only does this give us a common function for transitioning, but it also is nice to read about in the source code! This reduces mental burden on us and makes it easier for readers to comprehend. <em>Instead of implementing custom functions we're just using a pattern already existing.</em> Building our pattern on top of already existing patterns is a great way forward.</p>
<p>So this is cool, but how do we deal with all this nasty code repetition and the repeating <code>shared_value</code> stuff? Let's explore a bit more!</p>
<h3 id="generically-sophistication"><a href="#generically-sophistication" aria-label="Anchor link for: generically-sophistication">Generically Sophistication</a></h3>
<p>In this adventure we'll combine lessons and ideas from the first two, along with a few new ideas, to get something more satisfying. The core of this is to harness the power of generics. Let's take a look at a fairly bare structure representing this:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>struct</span> </span><span><span><span>BottleFillingMachine</span><span><span>&lt;</span>S<span>&gt;</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>shared_value</span><span>:</span> <span>usize</span>,
</span></span></span><span><span><span>    <span>state</span><span>:</span> S
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> The following states can be the 'S' in StateMachine&lt;S&gt;
</span></span><span>
</span><span><span><span>struct</span> </span><span><span>Waiting</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>waiting_time</span><span>:</span> <span>std<span>::</span></span><span>time<span>::</span></span>Duration,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>struct</span> </span><span><span>Filling</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>rate</span><span>:</span> <span>usize</span>,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>struct</span> </span><span><span>Done</span></span><span>;</span>
</span></code></pre>
<p>So here we're actually building the state into the type signature of the <code>BottleFillingMachine</code> itself. A state machine in the 'Filling' state is <code>BottleFillingMachine&lt;Filling&gt;</code> which is just <strong>awesome</strong> since it means when we see it as part of an error message or something we know immediately what state the machine is in.</p>
<p>From there we can go ahead and implement <code>From&lt;T&gt;</code> for some of these specific generic variants like so:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span><span>BottleFillingMachine<span>&lt;</span>Waiting<span>&gt;</span></span><span>&gt;</span></span> <span>for</span></span><span> <span>BottleFillingMachine</span><span><span>&lt;</span>Filling<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>BottleFillingMachine<span>&lt;</span>Waiting<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>BottleFillingMachine<span>&lt;</span>Filling<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        BottleFillingMachine <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            shared_value<span>:</span> val<span>.</span>shared_value<span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> Filling <span><span>{</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>                rate<span>:</span> <span>1</span><span>,</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>            </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span><span>BottleFillingMachine<span>&lt;</span>Filling<span>&gt;</span></span><span>&gt;</span></span> <span>for</span></span><span> <span>BottleFillingMachine</span><span><span>&lt;</span>Done<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>BottleFillingMachine<span>&lt;</span>Filling<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>BottleFillingMachine<span>&lt;</span>Done<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        BottleFillingMachine <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            shared_value<span>:</span> val<span>.</span>shared_value<span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> Done<span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>Defining a starting state for the machine looks like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>impl</span> </span><span><span>BottleFillingMachine</span><span><span>&lt;</span>Waiting<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>new</span></span><span><span><span>(</span><span>shared_value</span><span>:</span> <span>usize</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Self</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        BottleFillingMachine <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            shared_value<span>:</span> shared_value<span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> Waiting <span><span>{</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>                waiting_time<span>:</span> <span>std<span>::</span></span><span>time<span>::</span></span><span>Duration<span>::</span></span>new<span><span>(</span><span>0</span><span>,</span> <span>0</span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>            </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>So how does it look to change between two states? Like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>let</span> in_waiting <span>=</span> <span>BottleFillingMachine<span>::</span></span><span><span>&lt;</span>Waiting<span>&gt;</span></span><span><span>::</span></span>new<span><span>(</span><span>0</span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>    <span>let</span> in_filling <span>=</span> <span>BottleFillingMachine<span>::</span></span><span><span>&lt;</span>Filling<span>&gt;</span></span><span><span>::</span></span>from<span><span>(</span>in_waiting</span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>Alternatively if you're doing this inside of a function whose type signature restricts the possible outputs it might look like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>transition_the_states</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>BottleFillingMachine<span>&lt;</span>Waiting<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>BottleFillingMachine<span>&lt;</span>Filling<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    val<span>.</span><span>into</span><span><span>(</span></span><span><span>)</span></span> <span> Nice right?
</span></span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>What do the <strong>compile time</strong> error messages look like?</p>
<pre><code><span>error[E0277]: the trait bound `BottleFillingMachine&lt;Done&gt;: std::convert::From&lt;BottleFillingMachine&lt;Waiting&gt;&gt;` is not satisfied
</span><span>  --&gt; &lt;anon&gt;:50:22
</span><span>   |
</span><span>50 |     let in_filling = BottleFillingMachine::&lt;Done&gt;::from(in_waiting);
</span><span>   |                      ^^^^^^^^^^^^^^^^^^^^^^^^^^
</span><span>   |
</span><span>   = help: the following implementations were found:
</span><span>   = help:   &lt;BottleFillingMachine&lt;Filling&gt; as std::convert::From&lt;BottleFillingMachine&lt;Waiting&gt;&gt;&gt;
</span><span>   = help:   &lt;BottleFillingMachine&lt;Done&gt; as std::convert::From&lt;BottleFillingMachine&lt;Filling&gt;&gt;&gt;
</span><span>   = note: required by `std::convert::From::from`
</span></code></pre>
<p>It's pretty clear what's wrong from that. The error message even hints to us some valid transitions!</p>
<p>So what does this scheme give us?</p>
<ul>
<li>Transitions are ensured to be valid at compile time.</li>
<li>The error messages about invalid transitions are very understandable and even list valid options.</li>
<li>We have a 'parent' structure which can have traits and values associated with it that aren't repeated.</li>
<li>Once a transition is made the old state no longer exists, it is consumed. Indeed, the entire structure is consumed so if there are side effects of the transition on the parent (for example altering the average waiting time) we can't access stale values.</li>
<li>Memory consumption is lean and everything is on the stack.</li>
</ul>
<p>There are some downsides still:</p>
<ul>
<li>Our <code>From&lt;T&gt;</code> implementations suffer from a fair bit of "type noise". This is a highly minor concern though.</li>
<li>Each <code>BottleFillingMachine&lt;S&gt;</code> has a different size, with our previous example, so we'll need to use an enum. Because of our structure though we can do this in a way that doesn't completely suck.</li>
</ul>
<blockquote>
<p>You can play with this example <a rel="noopener" target="_blank" href="https://is.gd/CyuJlH"><strong>here</strong></a></p>
</blockquote>
<h3 id="getting-messy-with-the-parents"><a href="#getting-messy-with-the-parents" aria-label="Anchor link for: getting-messy-with-the-parents">Getting Messy With the Parents</a></h3>
<p>So how can we have some parent structure hold our state machine without it being a gigantic pain to interact with? Well, this circles us back around to the <code>enum</code> idea we had at first.</p>
<p>If you recall the primary problem with the <code>enum</code> example above was that we had to deal with no ability to enforce transitions, and the only errors we got were at runtime when we did try.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>enum</span> <span>BottleFillingMachineWrapper</span> <span><span>{</span>
</span></span></span><span><span><span>    Waiting<span><span>(</span><span>BottleFillingMachine<span>&lt;</span>Waiting<span>&gt;</span></span></span><span><span>)</span></span><span>,</span>
</span></span></span><span><span><span>    Filling<span><span>(</span><span>BottleFillingMachine<span>&lt;</span>Filling<span>&gt;</span></span></span><span><span>)</span></span><span>,</span>
</span></span></span><span><span><span>    Done<span><span>(</span><span>BottleFillingMachine<span>&lt;</span>Done<span>&gt;</span></span></span><span><span>)</span></span><span>,</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>struct</span> </span><span><span>Factory</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>bottle_filling_machine</span><span>:</span> BottleFillingMachineWrapper,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>impl</span> </span><span><span>Factory</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>new</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Self</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        Factory <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            bottle_filling_machine<span>:</span> <span>BottleFillingMachineWrapper<span>::</span></span>Waiting<span><span>(</span><span>BottleFillingMachine<span>::</span></span>new<span><span>(</span><span>0</span></span><span><span>)</span></span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>At this point your first reaction is likely "Gosh, Hoverbear, look at that awful and long type signature!" You're quite right! Frankly it's rather long, but I picked long, explanatory type names! You'll be able to use all your favorite arcane abbreviations and type aliases in your own code. Have at!</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>impl</span> </span><span><span>BottleFillingMachineWrapper</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>step</span></span><span><span><span>(</span><span>mut</span> <span>self</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Self</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        <span>match</span> <span>self</span> <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            <span>BottleFillingMachineWrapper<span>::</span></span>Waiting<span><span>(</span>val</span><span><span>)</span></span> <span>=&gt;</span> <span>BottleFillingMachineWrapper<span>::</span></span>Filling<span><span>(</span>val<span>.</span><span>into</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            <span>BottleFillingMachineWrapper<span>::</span></span>Filling<span><span>(</span>val</span><span><span>)</span></span> <span>=&gt;</span> <span>BottleFillingMachineWrapper<span>::</span></span>Done<span><span>(</span>val<span>.</span><span>into</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            <span>BottleFillingMachineWrapper<span>::</span></span>Done<span><span>(</span>val</span><span><span>)</span></span> <span>=&gt;</span> <span>BottleFillingMachineWrapper<span>::</span></span>Waiting<span><span>(</span>val<span>.</span><span>into</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>let</span> <span>mut</span> the_factory <span>=</span> <span>Factory<span>::</span></span>new<span><span>(</span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>    the_factory<span>.</span>bottle_filling_machine <span>=</span> the_factory<span>.</span>bottle_filling_machine<span>.</span><span>step</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>Again you may notice that this works by <strong>consumption</strong> not mutation. Using <code>match</code> the way we are above <em>moves</em> <code>val</code> so that it can be used with <code>.into()</code> which we've already determined should consume the state. If you'd really like to use mutation you can consider having your states <code>#[derive(Clone)]</code> or even <code>Copy</code>, but that's your call.</p>
<p>Despite this being a bit less ergonomic and pleasant to work with than we might want we still get strongly enforced state transitions and all the guarantees that come with them.</p>
<p>One thing you will notice is this scheme <strong>does</strong> force you to handle all potential states when manipulating the machine, and that makes sense. You are reaching into a structure with a state machine and manipulating it, you need to have defined actions for each state that it is in.</p>
<p>Or you can just <code>panic!()</code> if that's what you really want. But if you just wanted to <code>panic!()</code> then why didn't you just use the first attempt?</p>
<blockquote>
<p>You can see a fully worked example of this Factory example <a rel="noopener" target="_blank" href="https://is.gd/s03IaQ"><strong>here</strong></a></p>
</blockquote>
<h2 id="worked-examples"><a href="#worked-examples" aria-label="Anchor link for: worked-examples">Worked Examples</a></h2>
<p>This is the kind of thing it's always nice to have some examples for. So below I've put together a couple worked examples with comments for you to explore.</p>
<h3 id="three-state-two-transitions"><a href="#three-state-two-transitions" aria-label="Anchor link for: three-state-two-transitions">Three State, Two Transitions</a></h3>
<p>This example is very similar to the Bottle Filling Machine above, but instead it <strong>actually</strong> does work, albeit trivial work. It takes a string and returns the number of words in it.</p>
<blockquote>
<p><a rel="noopener" target="_blank" href="https://is.gd/4ITDyV">Playground link</a></p>
</blockquote>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span> The `&lt;StateA&gt;` is implied here. We don't need to add type annotations!
</span></span></span></span><span><span><span>    <span>let</span> in_state_a <span>=</span> <span>StateMachine<span>::</span></span>new<span><span>(</span><span><span>"</span>Blah blah blah<span>"</span></span><span>.</span><span>into</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> This is okay here. But later once we've changed state it won't work anymore.
</span></span></span></span><span><span><span>    in_state_a<span>.</span>some_unrelated_value<span>;</span>
</span></span></span><span><span><span>    <span>println!</span><span><span>(</span></span><span><span><span>"</span>Starting Value: <span>{}</span><span>"</span></span></span><span><span>,</span> in_state_a<span>.</span>state<span>.</span>start_value<span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> Transition to the new state. This consumes the old state.
</span></span></span></span><span><span><span>    <span> Here we need type annotations (since not all StateMachines are linear in their state).
</span></span></span></span><span><span><span>    <span>let</span> in_state_b <span>=</span> <span>StateMachine<span>::</span></span><span><span>&lt;</span>StateB<span>&gt;</span></span><span><span>::</span></span>from<span><span>(</span>in_state_a</span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> This doesn't work! The value is moved when we transition!
</span></span></span></span><span><span><span>    <span> in_state_a.some_unrelated_value;
</span></span></span></span><span><span><span>    <span> Instead, we can use the existing value.
</span></span></span></span><span><span><span>    in_state_b<span>.</span>some_unrelated_value<span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span>println!</span><span><span>(</span></span><span><span><span>"</span>Interm Value: <span>{:?}</span><span>"</span></span></span><span><span>,</span> in_state_b<span>.</span>state<span>.</span>interm_value<span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> And our final state.
</span></span></span></span><span><span><span>    <span>let</span> in_state_c <span>=</span> <span>StateMachine<span>::</span></span><span><span>&lt;</span>StateC<span>&gt;</span></span><span><span>::</span></span>from<span><span>(</span>in_state_b</span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> This doesn't work either! The state doesn't even contain this value.
</span></span></span></span><span><span><span>    <span> in_state_c.state.start_value;
</span></span></span></span><span><span><span>
</span></span></span><span><span><span>    <span>println!</span><span><span>(</span></span><span><span><span>"</span>Final state: <span>{}</span><span>"</span></span></span><span><span>,</span> in_state_c<span>.</span>state<span>.</span>final_value<span>)</span></span><span>;</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> Here is our pretty state machine.
</span></span><span><span><span>struct</span> </span><span><span><span>StateMachine</span><span><span>&lt;</span>S<span>&gt;</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>some_unrelated_value</span><span>:</span> <span>usize</span>,
</span></span></span><span><span><span>    <span>state</span><span>:</span> S,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> It starts, predictably, in `StateA`
</span></span><span><span><span>impl</span> </span><span><span>StateMachine</span><span><span>&lt;</span>StateA<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>new</span></span><span><span><span>(</span><span>val</span><span>:</span> String</span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Self</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        StateMachine <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            some_unrelated_value<span>:</span> <span>0</span><span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> <span>StateA<span>::</span></span>new<span><span>(</span>val</span><span><span>)</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> State A starts the machine with a string.
</span></span><span><span><span>struct</span> </span><span><span>StateA</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>start_value</span><span>:</span> String,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>impl</span> </span><span><span>StateA</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>new</span></span><span><span><span>(</span><span>start_value</span><span>:</span> String</span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Self</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        StateA <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            start_value<span>:</span> start_value<span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> State B goes and breaks up that String into words.
</span></span><span><span><span>struct</span> </span><span><span>StateB</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>interm_value</span><span>:</span> <span><span>Vec</span><span>&lt;</span><span>String</span><span>&gt;</span></span>,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span><span>StateMachine<span>&lt;</span>StateA<span>&gt;</span></span><span>&gt;</span></span> <span>for</span></span><span> <span>StateMachine</span><span><span>&lt;</span>StateB<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>StateMachine<span>&lt;</span>StateA<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>StateMachine<span>&lt;</span>StateB<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        StateMachine <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            some_unrelated_value<span>:</span> val<span>.</span>some_unrelated_value<span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> StateB <span><span>{</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>                interm_value<span>:</span> val<span>.</span>state<span>.</span>start_value<span>.</span><span>split</span><span><span>(</span><span><span>"</span> <span>"</span></span></span><span><span>)</span></span><span>.</span><span>map</span><span><span>(</span><span><span><span>|</span></span></span><span><span><span>x</span><span>|</span></span> </span><span>x<span>.</span><span>into</span><span><span>(</span></span><span><span>)</span></span></span></span><span><span>)</span></span><span>.</span><span>collect</span><span><span>(</span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>            </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> Finally, StateC gives us the length of the vector, or the word count.
</span></span><span><span><span>struct</span> </span><span><span>StateC</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>final_value</span><span>:</span> <span>usize</span>,
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span><span>StateMachine<span>&lt;</span>StateB<span>&gt;</span></span><span>&gt;</span></span> <span>for</span></span><span> <span>StateMachine</span><span><span>&lt;</span>StateC<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>StateMachine<span>&lt;</span>StateB<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>StateMachine<span>&lt;</span>StateC<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        StateMachine <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            some_unrelated_value<span>:</span> val<span>.</span>some_unrelated_value<span>,</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> StateC <span><span>{</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>                final_value<span>:</span> val<span>.</span>state<span>.</span>interm_value<span>.</span><span>len</span><span><span>(</span></span><span><span>)</span></span><span>,</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>            </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<h3 id="a-raft-example"><a href="#a-raft-example" aria-label="Anchor link for: a-raft-example">A Raft Example</a></h3>
<p>If you've followed my posts for awhile you may know I rather enjoy thinking about Raft. Raft, and a discussion with <a rel="noopener" target="_blank" href="https://twitter.com/Argorak"><strong>@argorak</strong></a> were the primary motivators behind all of this research.</p>
<p>Raft is a bit more complex than the above examples as it does not just have linear states where <code>A-&gt;B-&gt;C</code>. Here is the transition diagram:</p>
<pre><code><span>++++++++++-+    ++++++++++--+    +++++++--+
</span><span>|          ++++-&gt;           |    |        |
</span><span>| Follower |    | Candidate ++++-&gt; Leader |
</span><span>|          &lt;+++-+           |    |        |
</span><span>+++++++--^-+    ++++++++++--+    +-++++++++
</span><span>         |                         |
</span><span>         +++++++++++++++++++++++++-+
</span></code></pre>
<blockquote>
<p><a rel="noopener" target="_blank" href="https://is.gd/HDZeGR">Playground link</a></p>
</blockquote>
<pre data-lang="rust"><code data-lang="rust"><span><span><span>//</span> You can play around in this function.
</span></span><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>let</span> is_follower <span>=</span> <span>Raft<span>::</span></span>new<span><span>(</span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>    <span> Raft typically comes in groups of 3, 5, or 7. Just 1 for us. :)
</span></span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> Simulate this node timing out first.
</span></span></span></span><span><span><span>    <span>let</span> is_candidate <span>=</span> <span>Raft<span>::</span></span><span><span>&lt;</span>Candidate<span>&gt;</span></span><span><span>::</span></span>from<span><span>(</span>is_follower</span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> It wins! How unexpected.
</span></span></span></span><span><span><span>    <span>let</span> is_leader <span>=</span> <span>Raft<span>::</span></span><span><span>&lt;</span>Leader<span>&gt;</span></span><span><span>::</span></span>from<span><span>(</span>is_candidate</span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> Then it fails and rejoins later, becoming a Follower again.
</span></span></span></span><span><span><span>    <span>let</span> is_follower_again <span>=</span> <span>Raft<span>::</span></span><span><span>&lt;</span>Follower<span>&gt;</span></span><span><span>::</span></span>from<span><span>(</span>is_leader</span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> And goes up for election...
</span></span></span></span><span><span><span>    <span>let</span> is_candidate_again <span>=</span> <span>Raft<span>::</span></span><span><span>&lt;</span>Candidate<span>&gt;</span></span><span><span>::</span></span>from<span><span>(</span>is_follower_again</span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span> But this time it fails!
</span></span></span></span><span><span><span>    <span>let</span> is_follower_another_time <span>=</span> <span>Raft<span>::</span></span><span><span>&lt;</span>Follower<span>&gt;</span></span><span><span>::</span></span>from<span><span>(</span>is_candidate_again</span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span>
</span><span><span><span>//</span> This is our state machine.
</span></span><span><span><span>struct</span> </span><span><span><span>Raft</span><span><span>&lt;</span>S<span>&gt;</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span> ... Shared Values
</span></span></span></span><span><span><span>    <span>state</span><span>:</span> S
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> The three cluster states a Raft node can be in
</span></span><span>
</span><span><span><span>//</span> If the node is the Leader of the cluster services requests and replicates its state.
</span></span><span><span><span>struct</span> </span><span><span>Leader</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span> ... Specific State Values
</span></span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> If it is a Candidate it is attempting to become a leader due to timeout or initialization.
</span></span><span><span><span>struct</span> </span><span><span>Candidate</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span> ... Specific State Values
</span></span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> Otherwise the node is a follower and is replicating state it receives.
</span></span><span><span><span>struct</span> </span><span><span>Follower</span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span> ... Specific State Values
</span></span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> Raft starts in the Follower state
</span></span><span><span><span>impl</span> </span><span><span>Raft</span><span><span>&lt;</span>Follower<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>new</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Self</span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        </span></span></span></span></span><span><span><span><span><span>        Raft <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            </span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> Follower <span><span>{</span>  </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> The following are the defined transitions between states.
</span></span><span>
</span><span><span><span>//</span> When a follower timeout triggers it begins to campaign
</span></span><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span><span>Raft<span>&lt;</span>Follower<span>&gt;</span></span><span>&gt;</span></span> <span>for</span></span><span> <span>Raft</span><span><span>&lt;</span>Candidate<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>Raft<span>&lt;</span>Follower<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Raft<span>&lt;</span>Candidate<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        </span></span></span></span></span><span><span><span><span><span>        Raft <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            </span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> Candidate <span><span>{</span>  </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> If it doesn't receive a majority of votes it loses and becomes a follower again.
</span></span><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span><span>Raft<span>&lt;</span>Candidate<span>&gt;</span></span><span>&gt;</span></span> <span>for</span></span><span> <span>Raft</span><span><span>&lt;</span>Follower<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>Raft<span>&lt;</span>Candidate<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Raft<span>&lt;</span>Follower<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        </span></span></span></span></span><span><span><span><span><span>        Raft <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            </span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> Follower <span><span>{</span>  </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> If it wins it becomes the leader.
</span></span><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span><span>Raft<span>&lt;</span>Candidate<span>&gt;</span></span><span>&gt;</span></span> <span>for</span></span><span> <span>Raft</span><span><span>&lt;</span>Leader<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>Raft<span>&lt;</span>Candidate<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Raft<span>&lt;</span>Leader<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        </span></span></span></span></span><span><span><span><span><span>        Raft <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            </span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> Leader <span><span>{</span>  </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span>//</span> If the leader becomes disconnected it may rejoin to discover it is no longer leader
</span></span><span><span><span>impl</span> </span><span><span><span>From</span><span>&lt;</span><span>Raft<span>&lt;</span>Leader<span>&gt;</span></span><span>&gt;</span></span> <span>for</span></span><span> <span>Raft</span><span><span>&lt;</span>Follower<span>&gt;</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span><span>fn</span> </span><span>from</span></span><span><span><span>(</span><span>val</span><span>:</span> <span>Raft<span>&lt;</span>Leader<span>&gt;</span></span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Raft<span>&lt;</span>Follower<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        </span></span></span></span></span><span><span><span><span><span>        Raft <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>            </span></span></span></span></span></span><span><span><span><span><span><span>            state<span>:</span> Follower <span><span>{</span>  </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>    </span><span><span>}</span></span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<h2 id="alternatives-from-feedback"><a href="#alternatives-from-feedback" aria-label="Anchor link for: alternatives-from-feedback">Alternatives From Feedback</a></h2>
<p>I saw an interesting comment by <a rel="noopener" target="_blank" href="https://www.reddit.com/r/rust/comments/57ccds/pretty_state_machine_patterns_in_rust/d8rhwq4">I-impv on Reddit</a> showing off <a rel="noopener" target="_blank" href="https://play.rust-lang.org/?gist=ee3e4df093c136ced7b394dc7ffb78e1&amp;version=stable&amp;backtrace=0">this approach based on our examples above</a>. Here's what they had to say about it:</p>
<blockquote>
<p>I like the way you did it. I am working on a fairly complex FSM myself currently and did it slightly different.</p>
<p>Some things I did different:</p>
<ul>
<li>I also modeled the input for the state machine. That way you can model your transitions as a match over (State, Event) every invalid combination is handled by the 'default' pattern</li>
<li>Instead of using panic for invalid transitions I used a Failure state, So every invalid combination transitions to that Failure state</li>
</ul>
</blockquote>
<p>I really like the idea of modeling the input in the transitions!</p>
<h2 id="closing-thoughts"><a href="#closing-thoughts" aria-label="Anchor link for: closing-thoughts">Closing Thoughts</a></h2>
<p>Rust lets us represent State Machines in a fairly good way. In an ideal situation we'd be able to make <code>enum</code>s with restricted transitions between variants, but that's not the case. Instead, we can harness the power of generics and the ownership system to create something expressive, safe, and understandable.</p>
<p>If you have any feedback or suggestions on this article I'd suggest checking out the footer of this page for contact details. I also hang out on Mozilla's IRC as Hoverbear.</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Novel color via stimulation of individual photoreceptors at population scale (166 pts)]]></title>
            <link>https://www.science.org/doi/10.1126/sciadv.adu1052</link>
            <guid>43741013</guid>
            <pubDate>Sun, 20 Apr 2025 02:03:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/doi/10.1126/sciadv.adu1052">https://www.science.org/doi/10.1126/sciadv.adu1052</a>, See on <a href="https://news.ycombinator.com/item?id=43741013">Hacker News</a></p>
Couldn't get https://www.science.org/doi/10.1126/sciadv.adu1052: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Layered Design in Go (117 pts)]]></title>
            <link>https://jerf.org/iri/post/2025/go_layered_design/</link>
            <guid>43740992</guid>
            <pubDate>Sun, 20 Apr 2025 01:58:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jerf.org/iri/post/2025/go_layered_design/">https://jerf.org/iri/post/2025/go_layered_design/</a>, See on <a href="https://news.ycombinator.com/item?id=43740992">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>This post will describe how I design my programs in Go. I needed this
for work, and while I searched for a link, nothing <em>quite</em> fits my
coding practices out there. The word “Layered” can pull up some fairly
close descriptions, but I want to lay out what I do.</p>
<h2 id="deriving-some-requirements">Deriving Some Requirements</h2>
<p>Go has a rule that I believe is underappreciated in its utility and
whose implications are often not fully grasped, which is: Packages may
not circularly reference each other. It is strictly forbidden. A
compile error.</p>
<p>Packages are also the primary way of hiding information within Go,
through the mechanism of <a href="https://go.dev/tour/basics/3">exported and unexported fields and
identifiers in the package</a>. Some people
will pile everything into a single package, and while I’m not quite
ready to call this unconditionally a bad idea, it does involve
sacrificing all ability to use information hiding to maintain
invariants, and that is a heck of a tool to put down. At any sort of
scale, you’d better have some concept of the discipline you’re going
to replace that with.</p>
<p>So for the purposes of this discussion, I’m going to discard the
“one large package approach”.</p>
<p>We also know that Go uses a package named <code>main</code> that contains a
function named <code>main</code> to define the entry point for a given
executable. The resulting package import structure is a <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic
graph</a>, where
the packages are the nodes and the imports are the directed edges, and
there is a distinguished “top node” for each executable.</p>
<p>So how do I deal with this requirement in Go?</p>

<p>This is a portion of a package hierarchy extracted from a real
project, with many of the names substituted and rubbed off, but the
relationships are approximately correct:</p>



<figure>

    <img src="https://jerf.org/iri/post/2025/go_layered_design/initial_mess.png" alt="a mess of boxes and lines that, with my apologies, really can't be captured in an alt text. sorry. hope the rest of the text makes sense to you.">
    
        <figcaption>A sample set of packages</figcaption>
</figure>



<p>All imports of external modules are automatically not part of a loop
in the base package, so we can just look at the import patterns of the
application currently being written. Since loops are forbidden that
implies there must be packages that do not import any other package in
the application. Pull those out and put them on the bottom:</p>



<figure>

    <img src="https://jerf.org/iri/post/2025/go_layered_design/layer_1.png" alt="">
    
        <figcaption>The packages that reference nothing separated.</figcaption>
</figure>



<p>There is now a set of packages in the remaining set that only
reference the packages we just pulled out and put on the bottom. Put
them in their own layer:</p>



<figure>

    <img src="https://jerf.org/iri/post/2025/go_layered_design/layer_2.png" alt="">
    
        <figcaption>The packages that reference only the lower layer sorted out.</figcaption>
</figure>



<p>You can repeat this process until you’ve layered everything by the
depth of the import stack:</p>



<figure>

    <img src="https://jerf.org/iri/post/2025/go_layered_design/layer_full.png" alt="">
    
        <figcaption>The layers.</figcaption>
</figure>



<p>All package imports now point downwards, though it can be hard to
tell.</p>
<p>If you look at the bottom, you see very basic things, like
“the package that provides metrics to everything else”,
“the package that refines the logging for our system”, and
“a set data structure”.</p>
<p>These are composed up into slightly higher level functionality that
uses logging, metrics, etc. and puts together things like header
functionality, or information about users (imagine permissions or
metadata are stored here).</p>
<p>Throughout this post I will refer “higher level packages”; in this
case, it literally refers to the way packages will appear “higher” on
this graph than any package it imports. It is not the definition of
“higher” we often use to mean “higher level of abstraction”; yes,
if a package offers a “higher level of abstraction” it will generally
of necessity also be a “higher level package”, but a
“higher level package” is often just a package that is using the lower
level package, not an abstraction, e.g., a “crawler” may have
<code>net/http</code> show up as a “lower level package” but a crawler is not any
sort of abstraction around <code>net/http</code> so much as it is just an
application that is using <code>net/http</code>.</p>
<p>These things are then composed into higher layer objects, and so
forth, and so forth, until you finally get to the desired application
functionality.</p>
<h3 id="this-is-descriptive-not-prescriptive">This Is Descriptive, Not Prescriptive</h3>
<p>If you are the sort of person who reads things about software
architecture, you are used to people making prescriptive statements
about design. For instance, one such statement I found in another
article about layered design in Go has the prescriptive statement that
layers should be organized into very formal layers, and that no layer
should ever reach down more than one level to import a package. That
is not a requirement imposed by Go, it is a <em>prescriptive</em> statement
by the author. You can see in my graph above I don’t agree with that
particular prescription, as I have imports that reach down multiple
levels.</p>
<p>However, the previous section is not prescriptive. It may sound like
prescriptions you’ve heard before, but it’s actually <em>required</em>. You
can graph <em>all</em> Go modules in the layers I described. It’s a
mathematical consequence of the rules for how packages are allowed to
import each other. It is descriptive of the reality in Go.</p>
<p>Which means that any other prescriptive design for a Go program <em>must</em>
sit on top of this structure. It has no choice. It may not be
necessary, but it is certainly <em>convenient</em> in MVC design for the
Model, View, and Controller to be able to mutually reference each
other, at least a little. In Go, that’s not an option, if you want
them separate from each other. You can do MVC, but you must do “MVC on
top of Go layering”. If you slam all of the MVC stuff into one package
for convenience, or for it to work at all, you’re increasing the odds
that you’ll still end up with circular loops between the packages
implementing other related Models or Views or Controllers.</p>
<p>You can “hexagonal architecture”, but you <em>must</em> do it on top of Go
layering.</p>
<p>You can do any design methodology you want, but it must always rigidly
fit the layered design described in the previous section, because it
is not an option.</p>
<p>They do not all go on top of this design constraint equally well.</p>
<h3 id="so-whats-the-best-prescription">So What’s The Best Prescription?</h3>
<p>Naturally, this raises the question of what’s the best methodology to
sit on top of this.</p>
<p>My personal contention is that the answer is “none”. This is a
perfectly sensible and adequate design methodology on its own.</p>
<p>It harmonizes well with a lot of the points I make in <a href="https://jerf.org/iri/blogbooks/functional-programming-lessons-in-imperative-code/">Functional
Programming Lessons in Imperative
Code</a>. Particularly
the section about <a href="https://jerf.org/iri/post/2025/fp_lessons_purity/">purifiable
subcomponents</a> and
the ability to compose multiple component’s purifications together. If
you design a shim for the metrics for testing the metrics in a pure
manner, you can then use that to design the purification for the
bodypart2 above. Then you can use that to design the purification for
the body package, which can then be fed into the classify package, and
through all that, you can get classification tests that don’t have to
depend on any external state, despite the fact it is sitting on top of
a lot of other packages.</p>
<p>Sometimes a purification component will “absorb” the tree under it, so
for instance, the contentmodel module may absorb all metrics and
other packages into itself so the contentmodel can ignore all that and
just use the test shim provided by the contentmodel package. Other
times it may be necessary to have multiple shims related to the
packages being imported. They’re already coupled by the import, so
this is not additional coupling. Generally it’s a bit of both in my
experience.</p>
<p>Any codebase can be decomposed this way with enough work. The work can
be nontrivial, but it gets a lot easier as you practice.</p>
<p>My favorite advantage of this methodology is that for any package you point
at, there is a well-defined and limited number of packages you need to
understand in order to understand the package you are looking at, even
considering the transitive closure of imports. It is effectively
impossible to write code that requires you to understand the entire
rest of the code base to understand it, because you can’t circularly
loop in all the important code in the entire code base
accidentally.</p>
<p>As code scales up, this strongly affords a style in which packages
generally use only and exactly what they need, because otherwise they
become too likely to participate in a circular import loop at some
point.</p>
<h3 id="local-prescriptions">Local Prescriptions</h3>
<p>That’s not to say that there aren’t high level designs that can be
helpful. Web handlers can use some higher level design
patterns. Database-heavy applications may bring in some higher level
patterns. The code base I extracted the diagram I used earlier from
has a plugin-based architecture in which a particular data structure
is flowed through a uniform plugin interface which has a few dozen
implementations on it. However, these can be used in just the parts of
the code base that make sense.</p>
<p>I don’t think it’s a particularly good idea to try to force any other
architecture on Go programs at the top level. Isolate them to parts of
the codebase where they make sense. At the very least, if one is going
to impose an architecture on to a code base, it needs to be one that
came from a world that has a similar circular import restriction,
whether imposed by the language or self-imposed, as I think
methodologies that implicitly incorporate the ability to have circular
imports in them tend to scale up poorly in Go.</p>
<h2 id="avoiding-circular-dependencies">Avoiding Circular Dependencies</h2>
<p>If circular dependencies are impossible in Go, how do we deal with them?</p>
<p>There are multiple solutions, depending on exactly what your code base
is trying to express. I’ll go over some of them below, but first,
there’s a step to do first: When a circular dependency arises in your
code base, you must <em>deeply analyze</em> it and figure out <em>exactly</em> where
the circular dependency arises from.</p>
<p>I will not be so dogmatic as to say that every language should be
programmed without circular import loops, but I do think they are
something that should be <em>minimized</em> in every language. Pervasive
circular loops in a code base eventually make it impossible to
understand the code base without understanding the code base. This
poses… logical difficulties. Fortunately, the real situation is
never so stark as that, and one can eventually work one’s way up to
understanding even such a code base, but it is going to be a much more
difficult task for the pervasive circularity. So the techniques
discussed below are generally useful, even if you aren’t being
<em>forced</em> into using them. For non-Go languages, for the word
“package”, read
“whatever your primary information hiding mechanism is”, be it
classes, modules, or some other word.</p>
<p>It is not enough to say
“this package is circularly depending on that package”. It isn’t even
enough to say that this data structure is depending on that one. It
isn’t even enough to say that this method depends on that method. You
need to trace it down to what bits and pieces of a structure, specific
functionality the code is embodying. Break it down as finely as
possible, because as you ponder how to fix it, you may well be
breaking the code up along the lines you discover.</p>
<p>In general, when you come across a circular import error in the
compiler, it will be because you added some new code that created some
new dependency that turned out to be circular. In the discussions
below, I will refer to this as the “new circular code”. Upon analysis,
you should also be able to identify some particular set of variables
and bits of code that are the most modifiable aspect of the circular
dependency, which I will call the “breakable link” in the discussions
below.</p>
<p>Packages aren’t allowed to circularly reference each other, but
generally what is causing the circularity is much smaller than the
entire package.</p>
<p>Generally, if you can do one of these refactorings, you should,
because the resulting code will increase in conceptual clarity and
have a stronger design. It often as a side effect ends up reducing the
size of an interface of a package as it moves bits out of the exported
public interface that turn out not to belong there. Often if you
listen to the code, it tells you that it is wanting these things.</p>
<p>These are listed in roughly the order you should prefer them:</p>
<h3 id="move-the-functionality">Move The Functionality</h3>
<p>Unfortunately, this is not the most common in my experience, but it is
also the most important when it can be done. Sometimes after you
analyze the situation, you will find that the bit that is causing
circularity is simply in the wrong place. It probably belongs in the
same location as the new circular code.</p>
<p>This may involve slicing apart an existing conglomeration of
functionality. It could be three fields in a larger struct that turn
out to still belong together, but not to belong to the larger struct
at all. You may be slicing out bits of code and a field here and a
field there. It won’t just be about moving entire types around.</p>
<p>This will hold true for all the techniques below, so I’m not going to
mention this again and again. I just can’t emphasize enough that this
does not simply involve moving entire types around. You can even have
cases where you need to split something that looks like an atomic
field in half, though that’s rare. You need to get very granular.</p>
<p>In the rough and tumble of greenfield implementation, I find this
happens a lot initially as I am feeling out the correct
structure. Once past that phase though, this becomes rare. Still, if
you can do it, this is the best move, not just because it breaks the
circularity, but because it has the strongest outcome on the package
conceptual clarity I was talking about. A wholesale removing of the
concept that didn’t belong in the breakable link into the place it
belongs is, in the long term, a huge win for the code base.</p>
<h3 id="create-a-third-package-that-can-be-imported-for-the-shared-bit">Create A Third Package That Can Be Imported For The Shared Bit</h3>
<p>If a package is reaching for something that lives in another package
that results in a circular dependency, consider moving that thing into
a new third package that both can import.</p>
<p>This happens for me most often when I was banging away, implementing
some particular functionality, and I needed some particular type,
let’s say <code>Username</code>, and since nothing else yet needed it, I just
dropped it into the package that needed it. As the program grows,
eventually something else wants to reference the <code>Username</code> in such a
way that it causes circularity. But <code>Username</code> should generally be
just a validated string of some sort. It can almost certainly be moved
into its own package.</p>
<p>The most likely reason you’re reluctant to do this… ok, well,
honestly, the <em>most</em> likely reason is that fundamental laziness we all
share and just not wanting to make the change at all… but the
<em>second</em> most likely reason you’re reluctant to do this is that having
an entire package for a single type like this feels like bad
design.</p>
<p>However, I suggest that you learn to just do it anyhow, because in my
experience, the <em>vast</em> majority of the time, if you could see how that
new package is going to evolve, it will turn out that this new type is
not going to live there alone forever, and indeed, probably not for
very long at all. Of all the times I’ve done this, I think only once
or twice have I ended up with a package that ended up with just one
type in it by the time the program finally “settled”. Try to think
about packages not just as snapshots in time but in terms of their
evolution through time. It is almost always the case that rather than
an isolated type or value, what you have is the first exemplar of some
new non-trivial concept your package will shortly start to embody in a
more complicated and complete sense.</p>
<h3 id="a-new-third-package-that-composes-the-circular-packages">A New Third Package That Composes The Circular Packages</h3>
<p>This is much like the previous case, but moves in the other
direction. If you have two packages circularly depending on each other
for some purpose, you may be able to extract out the dependency and
turn it into something that uses the two packages to achieve the task
that requires the circularity.</p>
<p>I use this less often after I got used to designing architectures
natively in Go. However, before then, I naturally brought in my old
inheritance-based object oriented architecture, and that makes it easy
to expect architectures that deeply depend on circularity to come into
play.</p>
<p>Consider this perfectly sensible example from the ORM world. You have
a <code>Category</code> in a package, representing a category table in the DB, and
a <code>BlogPost</code> in a package. Each has a many-to-many relationship with
the other, and as such the <code>.Save()</code> operation for each ends up
depending on other, creating a circularity.</p>
<p>What I do in these cases is usually make <code>Category</code> and <code>BlogPost</code>
effectively dumber; I break away from them the idea that they know how
to “save” themselves. I create a <code>Category</code> and a <code>BlogPost</code> that are
just the data structures they represent. A higher package can tie them
together through a many-to-many relationship, and a yet higher package
will be the thing that “knows” how to load them from the DB and save
any changes. That top-level package may get assistance from the
lower-level values through various custom interface methods like
<code>UnmarshalFromDB</code> or something.</p>
<p>(This doesn’t work terribly well with ORMs, but, well, this is
technically one of the many many reasons I tend to avoid them. I don’t
like the way they make every object have to “know” about the DB in
order to have one in hand. This is one of the more common
manifestations of the whole <a href="https://www.johndcook.com/blog/2011/07/19/you-wanted-banana/">you wanted a banana but what you
got was a gorilla holding the banana and the entire
jungle</a>
problem. The layered design in Go is hostile to this approach because
the more of the jungle you end up with the more likely you have a
circular dependency, with odds rapidly approaching one. Go almost
forces you to have <code>Banana</code>s and <code>Gorilla</code>s that can be in isolation,
and expressing relationships in higher level packages… but it
doesn’t fully force it, and you can still fight it. You won’t have a
good time of it, though.)</p>
<h3 id="interface-to-break-the-dependency">Interface To Break The Dependency</h3>
<p>If a circular dependency results from taking some type that one of the
circular bits of code is going to call methods on, and the circularity
comes from the reference to the concrete type itself, you can break
the circularity by having one of the places involved in the circular
reference take an interface instead. That is, if you have something
like</p>
<div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> (<span>ur</span> <span>UserRef</span>) <span>MyUserIsAdmin</span>(<span>adminDB</span> <span>users</span>.<span>DBList</span>) <span>bool</span> {
</span></span><span><span>     <span>return</span> <span>adminDB</span>.<span>Exists</span>(<span>ur</span>.<span>User</span>)
</span></span><span><span>}</span></span></code></pre></div>
<p>and that is somehow a circular reference, you could consider:</p>
<div><pre tabindex="0"><code data-lang="go"><span><span><span>type</span> <span>UserList</span> <span>interface</span> {
</span></span><span><span>    <span>Exists</span>(<span>username</span> <span>string</span>)
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>func</span> (<span>ur</span> <span>UserRef</span>) <span>MyUserIsAdmin</span>(<span>adminDB</span> <span>UserList</span>) <span>bool</span> {
</span></span><span><span>     <span>return</span> <span>adminDB</span>.<span>Exists</span>(<span>ur</span>.<span>User</span>)
</span></span><span><span>}</span></span></code></pre></div>
<p>This is not always a full solution. If the interface needs values from
that package as arguments or returns them as parameters this may still
leave a circular reference behind. However, even in these cases,
interfaces can still be a part of the solution.</p>
<p>You may need to create a new method that an interface can
implement. A common instance of this in Go is if the circular
reference is trying to get an exported field of some other struct. It
is fine to take that field, unexport it, and wrap it behind a method,
just so you can then use an interface to break the circular reference
chain.</p>
<p>It is also worth pointing out that this is further down the list for a
reason. While interfaces may loosen the relationship between the two
packages enough to avoid being a circular dependency, it still creates
a relationship of a sort between them. While my example above
intrinsically lacks context, as is the way of little example snippets,
it would still suggest to me that there’s probably an improper mixing
of the concept of “user” and “admin” potentially going on here that is
the root cause of the circularity. Taking the time to slice packages
apart into clean pieces that do not have intermixed concepts will
still produce superior results.</p>
<p>In many cases I start pulling this out as a project matures, and it
has nearly attained the correct degree of separation. Sometimes you
get that last minute
“oh wait, these things need to be connected after all”, and this can
be a good tool to go ahead and spend some of that design capital you
have accrued through being more careful earlier in the design process.</p>
<h3 id="copy-the-dependency">Copy The Dependency</h3>
<p>One of the <a href="https://go-proverbs.github.io/">proverbs of the Go
community</a> is
“A little copying is better than a little dependency.” This is usually
referenced in the context of not bringing in a large library just to
use a few line’s worth of code out of it, but it can apply to your own
code base too. If you’re importing an entire separate package for a
particular tiny snippet of code, and that code really does belong
there, maybe it would just be enough to have a copy of the lines in
the circular package as well.</p>
<p>This is also lower on the list for a reason. Do this every time you
have a problem and you will have the joy of rediscovering for yourself
the concept of “Don’t Repeat Yourself”. However, in my experience,
about the half the time I find myself backed into this particular
solution to circular dependency, as the code evolves it turns out this
was <a href="https://www.jerf.org/iri/post/2024/dry_strong/">false sharing and false
DRY</a> and the two
copies end up non-trivially, and correctly, diverging anyhow,
indicating they weren’t really the same thing after all.</p>
<h3 id="maybe-they-shouldnt-be-two-separate-packages">Maybe They Shouldn’t Be Two Separate Packages</h3>
<p>Finally, if none of the preceding solutions seem viable, even if you
put the effort in, perhaps because the circularity is just too
substantial, the answer may be that the code is telling you that this
is one package after all.</p>
<p>I rather like breaking things into lots of packages, for lots of
reasons, but every once in a while it’s true that I do get a bit too
zealous and I try to break something apart that just doesn’t work
separately. If this is happening to you <em>all the time</em>, you may still
need more practice and not be doing the work necessary to have a good
design, but this should happen at least sometimes in my opinion, or
you’re not trying hard enough to break things up.</p>
<p>The larger the resulting combined package would be, the more you
should fight for some other solution for breaking circular
dependencies, but as with all things in engineering, it’s ultimately a
cost/benefits decision.</p>
<h2 id="so-whats-the-difference-between-this-and-anything-else">So What’s The Difference Between This And Anything Else?</h2>
<p>I admit I struggle to describe how this differs from other design
methodologies, mostly because I’ve been doing this for too long so I’m
too close to the problem, and after a while honestly all the
methodologies sort of blur together into one vague blob of good design
practices and the Quality Without A Name. Should this get to some link
aggregator site, let me issue my own criticism in advance of anyone
else that I am well aware there is still a certain fogginess to this
entire post, in that I have not succeeded in providing a simple recipe
that you can follow every time and I am myself dissatisfied with my
own description of the benefits, despite the fact I believe I have
experienced substantial benefits from taking this approach over
years. Consider this an attempt to write my way to that level of
clarity, and one that is not done yet.</p>
<p>However, this approach does produce at least one thing distinctly
different from many other designs, which is that every package ends up
being <em>something useful</em> on its own terms. For instance, consider a
standard web framework with a standard ORM design. ORMs make it very
easy to introduce the want-banana-get-jungle problem described
above. Even architectures heavy on dependency injection can still end
up making it so every service requires effectively every possible
dependency and thus even though in principle everything is
“purifiable”, nothing is actually usable in isolation because
you still have to provide every service that exists in the system, because
nothing stops this from happening if you can have circular
references. In principle you can use things if you only provide their
dependencies, but if “providing their dependencies” still involves
providing every service in the system, it still is not truly
severable.</p>
<p>This architecture tends to force you to narrow things down to just
what they need and nothing else. If you have a system for classifying
emails, it will generally just need an email and the relevant
classification services you may need for an email; it won’t need the users
and whether those users are admins and what the users are admins over
and the forums that the users are admins over and the top posts of the
forums the users are admins over. If the email classification needs to
reach back up to something about those details, it’ll be isolated into
interfaces that can be mocked or stubbed or whathaveyou. If you just
need a user’s name, you will generally find yourself being forced to
do that over an interface for “yielding a name” rather than pulling in
the entire package for a user, and all of its dependencies.</p>
<p>Many methodologies might metaphorically object that their entire point
is to produce useful things that stand on their own, but it’s really
easy for them to <em>in practice</em> still require jungles in order to have
bananas. This methodology tends to produce <em>practically</em> useful things
in isolation.</p>
<p>On such occasions as I have had to split bits and pieces off into
microservices from what was previously a monolith, it has been an
almost mechanical process, simply following the dependencies,
providing them, and ending up with a functioning service. Every once
in a while I have to trim something I lazily bundled together a bit,
but the system has already been pushed far in that direction so it’s
not a shock to the code base. I find this is a fantastic way to design
“monolithic microservice” codebases, where the microservices can
indeed be pulled out later in very practical amounts of work if that
is desirable.</p>
<p>One of the things I would not call part of this design methodology in
general, but something that is a good idea for any Go package
regardless of what is is doing, is to try to minimize the amount of
exported stuff coming from the package. Use godoc to see what is being
exported… after all, you write documentation on your code anyhow,
right?… and when a package is sort of wrapping up, audit each
exported symbol to see if you <em>really</em> need to export it. The thinner
the public interface, the better this works. It is better to
overzealously keep things unexported, because it is really easy to
re-export something it turns out you should have simply by renaming (a
rename operation your IDE should support and which is guaranteed by
its nature to be isolated within your one package even if you do it
manually) then to unexport something previously exported. (Your IDE
should tell you if a symbol you are trying to unexport is still being
used if you use its rename functionality.)</p>
<p>Still, I don’t deny that if you read this and either don’t feel like
you have a good grasp of what I’m really talking about, or how it
differs from other methodologies, or indeed whether or not it is
really possible and practical, there is an irreducible degree to which
you just need to try it out for yourself, even in a language other
than Go. (Although if you try it in something other than Go, you
<em>need</em> circular imports to be some sort of compile or build failure
for this to work. They get sneaky!) I would recommend a greenfield
project; it is possible but really quite tedious and at times
difficult to tear apart an existing system written against another
methodology and pull it into this one, though I think that’s just a
general rearchitecting truth and not a particular problem with this.</p>
</div></div>]]></description>
        </item>
    </channel>
</rss>