<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 06 Sep 2023 16:00:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[‚ÄòModern cars are a privacy nightmare,‚Äô the worst Mozilla‚Äôs seen (131 pts)]]></title>
            <link>https://www.theverge.com/2023/9/6/23861047/car-user-privacy-report-mozilla-foundation-data-collection</link>
            <guid>37405519</guid>
            <pubDate>Wed, 06 Sep 2023 14:17:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2023/9/6/23861047/car-user-privacy-report-mozilla-foundation-data-collection">https://www.theverge.com/2023/9/6/23861047/car-user-privacy-report-mozilla-foundation-data-collection</a>, See on <a href="https://news.ycombinator.com/item?id=37405519">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>If you‚Äôre wondering which gadgets have the worst user privacy practices, it turns out the answer may be parked outside. According to a <a href="https://foundation.mozilla.org/en/privacynotincluded/articles/its-official-cars-are-the-worst-product-category-we-have-ever-reviewed-for-privacy/">report published by the Mozilla Foundation</a> on Wednesday, cars are ‚Äúthe official worst category of products for privacy‚Äù that it‚Äôs ever reviewed. The global nonprofit found that 92 percent of the reviewed automakers provide drivers with little (if any) control over their personal data, with 84 percent sharing user data with outside parties.</p><p>Best known for its open-source Firefox web browser, the Mozilla Foundation claims to ‚Äústand up for the health of the internet.‚Äù It‚Äôs produced several reports and guides under its ‚ÄúPrivacy Not Included‚Äù series over the years that detail how products and services like <a href="https://www.theverge.com/2022/5/2/23045250/mozilla-mental-health-app-privacy-analysis">mental health apps</a> and <a href="https://www.theverge.com/2023/2/23/23612009/google-play-android-apps-privacy-false-misleading-mozilla-study">app stores</a> handle user data, with advice on how to better protect ourselves.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="A graphic showing a car and a shocked emoji that says all 25 cars tested by Mozilla failed the organization‚Äôs privacy checks." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1350x706/376x197/filters:focal(675x353:676x354):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24900103/car_PNI_Final_Graphics_OG_Image_Privacy_Nighm.width_1350.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1350x706/384x201/filters:focal(675x353:676x354):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24900103/car_PNI_Final_Graphics_OG_Image_Privacy_Nighm.width_1350.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1350x706/415x217/filters:focal(675x353:676x354):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24900103/car_PNI_Final_Graphics_OG_Image_Privacy_Nighm.width_1350.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1350x706/480x251/filters:focal(675x353:676x354):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24900103/car_PNI_Final_Graphics_OG_Image_Privacy_Nighm.width_1350.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1350x706/540x282/filters:focal(675x353:676x354):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24900103/car_PNI_Final_Graphics_OG_Image_Privacy_Nighm.width_1350.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1350x706/640x335/filters:focal(675x353:676x354):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24900103/car_PNI_Final_Graphics_OG_Image_Privacy_Nighm.width_1350.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1350x706/750x392/filters:focal(675x353:676x354):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24900103/car_PNI_Final_Graphics_OG_Image_Privacy_Nighm.width_1350.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1350x706/828x433/filters:focal(675x353:676x354):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24900103/car_PNI_Final_Graphics_OG_Image_Privacy_Nighm.width_1350.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1350x706/1080x565/filters:focal(675x353:676x354):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24900103/car_PNI_Final_Graphics_OG_Image_Privacy_Nighm.width_1350.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1350x706/1200x628/filters:focal(675x353:676x354):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24900103/car_PNI_Final_Graphics_OG_Image_Privacy_Nighm.width_1350.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1350x706/1440x753/filters:focal(675x353:676x354):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24900103/car_PNI_Final_Graphics_OG_Image_Privacy_Nighm.width_1350.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1350x706/1920x1004/filters:focal(675x353:676x354):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24900103/car_PNI_Final_Graphics_OG_Image_Privacy_Nighm.width_1350.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1350x706/2048x1071/filters:focal(675x353:676x354):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24900103/car_PNI_Final_Graphics_OG_Image_Privacy_Nighm.width_1350.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1350x706/2400x1255/filters:focal(675x353:676x354):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24900103/car_PNI_Final_Graphics_OG_Image_Privacy_Nighm.width_1350.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1350x706/2400x1255/filters:focal(675x353:676x354):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24900103/car_PNI_Final_Graphics_OG_Image_Privacy_Nighm.width_1350.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>Kia and Nissan were notably highlighted for including sexual activity in their data collection practices.</em></figcaption> <p><cite>Image: Mozilla</cite></p></div></div><p>All 25 of the car brands that were researched for the report ‚Äî including <a href="https://foundation.mozilla.org/en/privacynotincluded/ford/">Ford</a>, <a href="https://foundation.mozilla.org/en/privacynotincluded/toyota/">Toyota</a>, <a href="https://foundation.mozilla.org/en/privacynotincluded/volkswagen/">Volkswagen</a>, <a href="https://foundation.mozilla.org/en/privacynotincluded/bmw/">BMW</a>, and <a href="https://foundation.mozilla.org/en/privacynotincluded/tesla/">Tesla</a> ‚Äî failed to meet the nonprofit organization‚Äôs <a href="https://foundation.mozilla.org/en/privacynotincluded/about/methodology/">minimum privacy standards</a> and were found to collect more personal data from customers than necessary. The kind of information collected varies from personal information like medical data to how drivers are using the vehicle itself ‚Äî such as how fast they drive, where they drive, and even the music they listen to. Both <a href="https://foundation.mozilla.org/en/privacynotincluded/nissan/">Nissan</a> and <a href="https://foundation.mozilla.org/en/privacynotincluded/kia/">Kia</a> are noted to allow the collection of information regarding a user‚Äôs sex life. By contrast, Mozilla claims that 37 percent of mental health apps (<a href="https://www.theverge.com/2023/5/4/23710840/mental-health-therapy-apps-mozilla-report-privacy-data-security">which also have a poor reputation for data privacy</a>) had better practices for collecting and using personal data.</p><p>Eighty-four percent of the reviewed car brands share personal user data with service providers, data brokers, and <a href="https://foundation.mozilla.org/privacynotincluded/articles/what-data-does-my-car-collect-about-me-and-where-does-it-go/">potentially sketchy businesses</a>, according to the report, with 76 percent claiming the right to <em>sell</em> that personal data. Fifty-six percent are willing to share user information with the government and / or law enforcement if requested.&nbsp;</p><p>Tesla was the worst-ranked brand in the study, getting flagged in every privacy category ‚Äî only the second time this happened. Tesla‚Äôs AI-powered autopilot was highlighted as ‚Äúuntrustworthy‚Äù following its <a href="https://www.theverge.com/2023/7/26/23809183/tesla-autopilot-investigation-false-advertising-california-attorney-general">involvement</a> in <a href="https://www.theverge.com/2022/6/9/23161365/tesla-autopilot-nhtsa-crash-investigation-emergency-vehicle">numerous</a> <a href="https://www.theverge.com/2022/10/26/23425335/tesla-autopilot-justice-department-criminal-investigation">crashes</a> and <a href="https://www.theverge.com/2022/7/27/23280461/tesla-autopilot-crash-motorcyclist-fatal-utah-nhtsa">fatalities</a>. </p><div><p>Mozilla found that numerous car companies collect sensitive user information like photos, immigration status, and even sexual activity</p></div><p>Alongside the report, Mozilla also <a href="https://foundation.mozilla.org/en/privacynotincluded/articles/what-data-does-my-car-collect-about-me-and-where-does-it-go/">published a breakdown</a> explaining how car companies collect and share user data. This can include anything from the user‚Äôs name, address, phone number, and email address to more intimate data like photos, calendar information, and even details on the driver‚Äôs race, genetic information, and immigration status.</p><p>Mozilla says it also couldn‚Äôt confirm that any of the automakers could meet the organization‚Äôs minimum security standards regarding data encryption and protection against theft. In fact, it claims dating apps and even sex toys typically provide more detailed security information about their products than cars. </p><p>‚ÄúWhile we worried that our doorbells and watches that connect to the internet might be spying on us, car brands quietly entered the data business by turning their vehicles into powerful data-gobbling machines,‚Äù says Mozilla in the report.</p><p>Mozilla claims it spent over 600 hours researching the privacy practices of car brands ‚Äî three times longer per product than it usually spends on these <a href="https://www.theverge.com/2023/2/23/23612009/google-play-android-apps-privacy-false-misleading-mozilla-study">privacy reviews</a>. The report was so scathing that the organization said the advice it typically provides to help customers protect their personal data feels like ‚Äútiny drops in a massive bucket.‚Äù Instead, the Mozilla Foundation has <a href="https://foundation.mozilla.org/en/privacynotincluded/articles/car-companies-stop-your-huge-data-collection-programs-en/?utm_source=PNI&amp;utm_campaign=23-PNI-Cars&amp;utm_medium=FMO&amp;utm_term=en&amp;utm_content=blog_1">started a petition</a> urging car companies to stop the data collection programs they‚Äôre unfairly benefitting from, expressing that ‚Äúour hope is that increasing awareness will encourage others to hold car companies accountable for their terrible privacy practices.‚Äù</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Now it's PostgreSQL's turn to have a bogus CVE (101 pts)]]></title>
            <link>https://opensourcewatch.beehiiv.com/p/now-postgresqls-turn-bogus-cve</link>
            <guid>37404936</guid>
            <pubDate>Wed, 06 Sep 2023 13:37:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://opensourcewatch.beehiiv.com/p/now-postgresqls-turn-bogus-cve">https://opensourcewatch.beehiiv.com/p/now-postgresqls-turn-bogus-cve</a>, See on <a href="https://news.ycombinator.com/item?id=37404936">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-blocks"><p><img alt="" src="https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/7e93734a-d5e4-4467-a5ef-7a9574c5aa64/PostgresSQL_logo.jpg"></p><p> This time, according to the <a href="https://docs.google.com/document/u/0/d/1zb78aBeMwS8wxJvdFbpHcC_tKqo9a1XmqMltQUy3oUY/edit?utm_source=opensourcewatch.beehiiv.com&amp;utm_medium=referral&amp;utm_campaign=now-it-s-postgresql-s-turn-to-have-a-bogus-cve" target="_blank" rel="noopener noreferrer nofollow">PostgreSQL</a> Security Team reports, just like with cURL, whoever the unknown reporter was didn't bother to tell them that was a security problem. Had they done so, the security team would have told them the same thing they told the NVD crew: There was no problem. </p><p> The "issue," <a href="https://www.cve.org/CVERecord?id=CVE-2020-21469&amp;utm_source=opensourcewatch.beehiiv.com&amp;utm_medium=referral&amp;utm_campaign=now-it-s-postgresql-s-turn-to-have-a-bogus-cve" target="_blank" rel="noopener noreferrer nofollow">CVE-2020-21469</a>, claimed that PostgreSQL 12.2 allowed attackers to cause a denial of service by repeatedly sending SIGHUP signals. SIGHUP, which dates back to the days when we connected terminals over serial port connections, is a kill-the-process signal. </p><p> That would be bad news. It would well deserve the 9.8 score it was branded with. </p><p> However, there's one little, itty-bitty problem. Ordinary users can't send SIGHUP signals. In fact, they can't kill PostgreSQL processes, period. SIGHUP can only be sent by a PostgreSQL superuser; a user with pg_reload_conf instructs PostgreSQL to reload its configuration, access, or the root user. In short, anyone who could use this "flaw" to kill PostgreSQL could use any ordinary method to terminate it without this nonsense. </p><p> Or, to quote the PostgreSQL Security Team, "THIS IS NOT A SECURITY VULNERABILITY." </p><p> They've got that right! </p><p> Mind you, as they'll tell you should update from PostgreSQL 12.2 because of <a href="https://www.postgresql.org/support/security/12/?utm_source=opensourcewatch.beehiiv.com&amp;utm_medium=referral&amp;utm_campaign=now-it-s-postgresql-s-turn-to-have-a-bogus-cve" target="_blank" rel="noopener noreferrer nofollow">these actual CVEs</a> and other bug fixes. After all, PostgreSQL 12.2 is over three years old. That makes it a dinosaur of a release. </p><p> But, there's a bigger problem going on. Why are these crap security releases appearing in the first place? And, why are they being blindly passed on to the public without any attempt to see if there is anything to them? In both cURL and PostgreSQL cases, simply asking the developers would have revealed there's "no there, there." </p><p> These aren't technically sophisticated bug reports. They are "Are you kidding me?" bug reports. </p><p> This brings up another point. It's 2023. Usually, a CVE is given the year it's reported as part of its designation. That, combined with over a hundred of them all appearing on the same day, should have raised red flags. </p><p> As to where they all came from, Dan Lorenc, CEO and co-founder of <a href="https://www.chainguard.dev/?utm_source=opensourcewatch.beehiiv.com&amp;utm_medium=referral&amp;utm_campaign=now-it-s-postgresql-s-turn-to-have-a-bogus-cve" target="_blank" rel="noopener noreferrer nofollow">Chainguard</a>, a software supply chain security company, speculated, "The curl commit had 'integer overflow' in the commit message. This Postgres one says 'buffer overflow' in it.<a href="https://twitter.com/lorenc_dan/status/1696669900985827738?utm_source=opensourcewatch.beehiiv.com&amp;utm_medium=referral&amp;utm_campaign=now-it-s-postgresql-s-turn-to-have-a-bogus-cve" target="_blank" rel="noopener noreferrer nofollow"> I'd bet $1,000 this is someone running a script</a> on grepping old commit messages for things like this and auto-filing CVEs." </p><p> No bet. </p><p> In a LinkedIn post, Lorenc expanded on his theme. "All of these CVEs link out to bug or patch entries on repos or mailing lists, <a href="https://www.linkedin.com/feed/update/urn:li:activity:7102609622657548288/?utm_source=opensourcewatch.beehiiv.com&amp;utm_medium=referral&amp;utm_campaign=now-it-s-postgresql-s-turn-to-have-a-bogus-cve" target="_blank" rel="noopener noreferrer nofollow">all were filed without maintainer interaction,</a> and all contain an obvious "vulnerable sounding" string in the commit message or issue. Something like 'use after free,' 'denial of service,' or "buffer overflow.'" </p><p> In short, these are garbage. </p><p> Lorenic said this flood of junk CVE " is literally going to DOS triage teams, the NVD itself, and open source maintainers stuck dealing with the fallout. Stop doing this!" He's correct. </p><p> The CVSS process itself is broken. Of course, we must take security seriously, but taking anonymous, poor complaints as Gospel security truth about critical projects only gets in the way of improving security. What we see with these nonsense reports doesn't help security, it only makes the process harder than ever. </p><p> Stop it! Stop it now! </p><p><span><b>Noteworthy Linux and open-source stories:</b></span></p><div><ul><li><p><a href="https://www.zdnet.com/article/amd-and-intel-cpu-security-bugs-bring-linux-patches/?utm_source=opensourcewatch.beehiiv.com&amp;utm_medium=referral&amp;utm_campaign=now-it-s-postgresql-s-turn-to-have-a-bogus-cve" target="_blank" rel="noopener noreferrer nofollow">AMD and Intel CPU security bugs bring Linux patches</a></p></li><li><p><a href="https://www.techradar.com/news/best-linux-distro-privacy-security?utm_source=opensourcewatch.beehiiv.com&amp;utm_medium=referral&amp;utm_campaign=now-it-s-postgresql-s-turn-to-have-a-bogus-cve" target="_blank" rel="noopener noreferrer nofollow">Best Linux distro for privacy and security of 2023</a></p></li><li><p><a href="https://www.theregister.com/2023/08/25/bodhi_linux_7/?utm_source=opensourcewatch.beehiiv.com&amp;utm_medium=referral&amp;utm_campaign=now-it-s-postgresql-s-turn-to-have-a-bogus-cve" target="_blank" rel="noopener noreferrer nofollow">Bodhi Linux 7 brings Enlightenment to Ubuntu</a></p></li></ul></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is this Duplo train track under too much tension? (382 pts)]]></title>
            <link>https://puzzling.stackexchange.com/questions/122232/is-this-duplo-train-track-under-too-much-tension</link>
            <guid>37404740</guid>
            <pubDate>Wed, 06 Sep 2023 13:21:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://puzzling.stackexchange.com/questions/122232/is-this-duplo-train-track-under-too-much-tension">https://puzzling.stackexchange.com/questions/122232/is-this-duplo-train-track-under-too-much-tension</a>, See on <a href="https://news.ycombinator.com/item?id=37404740">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>First, we can check that there is no angular misalignment. Since 12 curved pieces are needed to make a full circle, the number of left pieces minus the number of right pieces must be a multiple of 12. Keeping track of the angle, starting at the crossing and going counterclockwise, I think that is satisfied in this setup:</p>
<p><a href="https://i.stack.imgur.com/mrYNQ.jpg" rel="noreferrer"><img src="https://i.stack.imgur.com/mrYNQ.jpg" alt="enter image description here"></a></p>
<p>Second, we look for positional misalignment. What we want to do is add up the vectors from the start to the end of each piece and check that the vector sum is zero. I haven't found a way to do this that ends up neater than just doing the trigonometry. However, things work out nicely in that all the coordinates end up being the sum of a rational number and a rational multiple of <span>$\sqrt{3}$</span> (in math lingo, they are members of <span>$\mathbb{Q}[\sqrt{3}]$</span>). This means that the coordinates sum to zero only if their rational parts sum to zero and the coefficients of <span>$\sqrt{3}$</span> sum to zero independently. This means that we can assign each piece type a list of 4 numbers (2 for the x-coordinate and 2 for the y-coordinate) and the pieces form a loop only if all 4 numbers sum to zero. Those numbers are:</p>
<pre><code>Curves:
0-1:  1    0    2   -1
1-2: -1    1   -1    1
2-3:  2   -1    1    0
3-4: -2    1    1    0
4-5:  1   -1   -1    1
5-6: -1    0    2   -1
Straights:
0-0:  1    0    0    0
1-1:  0    0.5  0.5  0
2-2:  0.5  0    0    0.5
3-3:  0    0    1    0
4-4: -0.5  0    0    0.5
5-5:  0   -0.5  0.5  0
</code></pre>
<p>Note that a 0-to-1 curve and a 1-to-0 curve have the same displacement so we only list the left curve. Also, a 180-degree rotation just negates the coordinates, so we can get away with only calculating the first half of the table.</p>
<p>Adding up the pieces in the layout gives a result of <code>4.5 -3 -2 0.5</code>, i.e. a displacement of <span>$\left(4.5-3\sqrt{3},-2+0.5\sqrt{3}\right)$</span>. This is not zero, so the layout is not free of stress. In fact, if we draw out the path that the shown pieces would ideally take, we get this shape:</p>
<p><a href="https://i.stack.imgur.com/Zetlz.png" rel="noreferrer"><img src="https://i.stack.imgur.com/Zetlz.png" alt="enter image description here"></a></p>
<p>No combination of fewer than 5 pieces will complete the loop, however the following combination of 5 will: <code>3-4 3-4 1-1 8-8 11-11</code>. (There are 3 more combinations of 5 that give the correct displacement, but would add an odd number of curves and therefore cause angular misalignment.)</p>
<p>Note that instead of adding a piece we can remove its 180-degree rotation. Therefore, I would suggest removing <code>10-9-10</code> and <code>5-5</code> and adding a <code>1-1</code> and <code>8-8</code>:</p>
<p><a href="https://i.stack.imgur.com/YsqAA.jpg" rel="noreferrer"><img src="https://i.stack.imgur.com/YsqAA.jpg" alt="enter image description here"></a></p>
<p>That's only if you want an 'exact' solution, for only changing a single piece you'd want to add a <code>2-2</code>, e.g. after the first gray curve, which would yield the following:</p>
<p><a href="https://i.stack.imgur.com/4u6qg.png" rel="noreferrer"><img src="https://i.stack.imgur.com/4u6qg.png" alt="enter image description here"></a></p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Falcon 180B (107 pts)]]></title>
            <link>https://huggingface.co/blog/falcon-180b</link>
            <guid>37404424</guid>
            <pubDate>Wed, 06 Sep 2023 12:55:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://huggingface.co/blog/falcon-180b">https://huggingface.co/blog/falcon-180b</a>, See on <a href="https://news.ycombinator.com/item?id=37404424">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<p><a href="https://huggingface.co/blog">
				Back to blog</a></p>
		<!-- HTML_TAG_START -->



<h2>
	<a id="introduction" href="#introduction">
		
	</a>
	<span>
		Introduction
	</span>
</h2>
<p><strong>Today, we're excited to welcome <a href="https://falconllm.tii.ae/">TII's</a> Falcon 180B to HuggingFace!</strong> Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII's <a href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb">RefinedWeb</a> dataset. This represents the longest single-epoch pretraining for an open model. </p>
<p>You can find the model on the Hugging Face Hub (<a href="https://huggingface.co/tiiuae/falcon-180B">base</a> and <a href="https://huggingface.co/tiiuae/falcon-180B-chat">chat</a> model) and interact with the model on the <a href="https://huggingface.co/spaces/tiiuae/falcon-180b-chat">Falcon Chat Demo Space</a>.</p>
<p>In terms of capabilities, Falcon 180B achieves state-of-the-art results across natural language tasks. It tops the leaderboard for (pre-trained) open-access models and rivals proprietary models like PaLM-2. While difficult to rank definitively yet, it is considered on par with PaLM-2 Large, making Falcon 180B one of the most capable LLMs publicly known.</p>
<p>In this blog post, we explore what makes Falcon 180B so good by looking at some evaluation results and show how you can use the model.</p>
<ul>
<li><a href="#what-is-falcon-180b">What is Falcon-180B?</a></li>
<li><a href="#how-good-is-falcon-180b">How good is Falcon 180B?</a></li>
<li><a href="#how-to-use-falcon-180b">How to use Falcon 180B?</a><ul>
<li><a href="#demo">Demo</a></li>
<li><a href="#hardware-requirements">Hardware requirements</a></li>
<li><a href="#prompt-format">Prompt format</a></li>
<li><a href="#transformers">Transformers</a></li>
</ul>
</li>
<li><a href="#additional-resources">Additional Resources</a></li>
</ul>
<h2>
	<a id="what-is-falcon-180b" href="#what-is-falcon-180b">
		
	</a>
	<span>
		What is Falcon-180B?
	</span>
</h2>
<p>Falcon 180B is a model released by <a href="https://falconllm.tii.ae/">TII</a> that follows previous releases in the Falcon family.</p>
<p>Architecture-wise, Falcon 180B is a scaled-up version of <a href="https://huggingface.co/tiiuae/falcon-40b">Falcon 40B</a> and builds on its innovations such as multiquery attention for improved scalability. We recommend reviewing the <a href="https://huggingface.co/blog/falcon">initial blog post</a> introducing Falcon to dive into the architecture. Falcon 180B was trained on 3.5 trillion tokens on up to 4096 GPUs simultaneously, using Amazon SageMaker for a total of ~7,000,000 GPU hours. This means Falcon 180B is 2.5 times larger than Llama 2 and was trained with 4x more compute. </p>
<p>The dataset for Falcon 180B consists predominantly of web data from <a href="https://arxiv.org/abs/2306.01116">RefinedWeb</a> (~85%). In addition, it has been trained on a mix of curated data such as conversations, technical papers, and a small fraction of code (~3%). This pretraining dataset is big enough that even 3.5 trillion tokens constitute less than an epoch.</p>
<p>The released <a href="https://huggingface.co/tiiuae/falcon-180B-chat">chat model</a> is fine-tuned on chat and instruction datasets with a mix of several large-scale conversational datasets.</p>
<p>‚ÄºÔ∏è Commercial use: 
Falcon 180b can be commercially used but under very restrictive conditions, excluding any "hosting use". We recommend to check the <a href="https://huggingface.co/spaces/tiiuae/falcon-180b-license/blob/main/LICENSE.txt">license</a> and consult your legal team if you are interested in using it for commercial purposes.</p>
<h2>
	<a id="how-good-is-falcon-180b" href="#how-good-is-falcon-180b">
		
	</a>
	<span>
		How good is Falcon 180B?
	</span>
</h2>
<p>Falcon 180B is the best openly released LLM today, outperforming Llama 2 70B and OpenAI‚Äôs GPT-3.5 on MMLU, and is on par with Google's PaLM 2-Large on HellaSwag, LAMBADA, WebQuestions, Winogrande, PIQA, ARC, BoolQ, CB, COPA, RTE, WiC, WSC, ReCoRD. Falcon 180B typically sits somewhere between GPT 3.5 and GPT4 depending on the evaluation benchmark and further finetuning from the community will be very interesting to follow now that it's openly released.</p>
<p><a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/162_falcon_180b/palm2_480.jpg" rel="noopener nofollow"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/162_falcon_180b/palm2_480.jpg" alt="Palm 2 comparison"></a></p>
<p>With 68.74 on the Hugging Face Leaderboard, Falcon 180B is the highest-scoring openly released pre-trained LLM, surpassing Meta‚Äôs LLaMA 2 (67.35).</p>
<div>
	<table>
		<thead><tr>
<th>Model</th>
<th>Size</th>
<th>Leaderboard score</th>
<th>Commercial use or license</th>
<th>Pretraining length</th>
</tr>

		</thead><tbody><tr>
<td>Falcon</td>
<td>180B</td>
<td>68.74</td>
<td>üü†</td>
<td>3,500B</td>
</tr>
<tr>
<td>Llama 2</td>
<td>70B</td>
<td>67.35</td>
<td>üü†</td>
<td>2,000B</td>
</tr>
<tr>
<td>LLaMA</td>
<td>65B</td>
<td>64.23</td>
<td>üî¥</td>
<td>1,400B</td>
</tr>
<tr>
<td>Falcon</td>
<td>40B</td>
<td>61.48</td>
<td>üü¢</td>
<td>1,000B</td>
</tr>
<tr>
<td>MPT</td>
<td>30B</td>
<td>56.15</td>
<td>üü¢</td>
<td>1,000B</td>
</tr>
</tbody>
	</table>
</div>
<p><a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/162_falcon_180b/open_llm_leaderboard.jpg" rel="noopener nofollow"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/162_falcon_180b/open_llm_leaderboard.jpg" alt="open_llm_leaderboard.png"></a></p>
<p>The quantized Falcon models preserve similar metrics across benchmarks. The results were similar when evaluating <code>torch.float16</code>, <code>8bit</code>, and <code>4bit</code>. See results in the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Open LLM Leaderboard</a>.</p>
<h2>
	<a id="how-to-use-falcon-180b" href="#how-to-use-falcon-180b">
		
	</a>
	<span>
		How to use Falcon 180B?
	</span>
</h2>
<p>Falcon 180B is available in the Hugging Face ecosystem, starting with Transformers version 4.33.</p>
<h3>
	<a id="demo" href="#demo">
		
	</a>
	<span>
		Demo
	</span>
</h3>
<p>You can easily try the Big Falcon Model (180 billion parameters!) in <a href="https://huggingface.co/spaces/tiiuae/falcon-180b-demo">this Space</a> or in the playground embedded below:</p>


<h3>
	<a id="hardware-requirements" href="#hardware-requirements">
		
	</a>
	<span>
		Hardware requirements
	</span>
</h3>
<p>We ran several tests on the hardware needed to run the model for different use cases. Those are not the minimum numbers, but the minimum numbers for the configurations we had access to.</p>
<div>
	<table>
		<thead><tr>
<th></th>
<th>Type</th>
<th>Kind</th>
<th>Memory</th>
<th>Example</th>
</tr>

		</thead><tbody><tr>
<td>Falcon 180B</td>
<td>Training</td>
<td>Full fine-tuning</td>
<td>5120GB</td>
<td>8x 8x A100 80GB</td>
</tr>
<tr>
<td>Falcon 180B</td>
<td>Training</td>
<td>LoRA with ZeRO-3</td>
<td>1280GB</td>
<td>2x 8x A100 80GB</td>
</tr>
<tr>
<td>Falcon 180B</td>
<td>Training</td>
<td>QLoRA</td>
<td>160GB</td>
<td>2x A100 80GB</td>
</tr>
<tr>
<td>Falcon 180B</td>
<td>Inference</td>
<td>BF16/FP16</td>
<td>640GB</td>
<td>8x A100 80GB</td>
</tr>
<tr>
<td>Falcon 180B</td>
<td>Inference</td>
<td>GPTQ/int4</td>
<td>320GB</td>
<td>8x A100 40GB</td>
</tr>
</tbody>
	</table>
</div>
<h3>
	<a id="prompt-format" href="#prompt-format">
		
	</a>
	<span>
		Prompt format
	</span>
</h3>
<p>The base model has no prompt format. Remember that it‚Äôs not a conversational model or trained with instructions, so don‚Äôt expect it to generate conversational responses‚Äîthe pretrained model is a great platform for further finetuning, but you probably shouldn‚Äôt driectly use it out of the box. The Chat model has a very simple conversation structure.</p>
<pre><code>System: Add an optional system prompt here
User: This is the user input
Falcon: This is what the model generates
User: This might be a second turn input
Falcon: and so on
</code></pre>
<h3>
	<a id="transformers" href="#transformers">
		
	</a>
	<span>
		Transformers
	</span>
</h3>
<p>With the release of Transformers 4.33, you can use Falcon 180B and leverage all the tools in the HF ecosystem, such as:</p>
<ul>
<li>training and inference scripts and examples</li>
<li>safe file format (safetensors)</li>
<li>integrations with tools such as bitsandbytes (4-bit quantization), PEFT (parameter efficient fine-tuning) and GPTQ</li>
<li>assisted generation (also known as ‚Äúspeculative decoding‚Äù)</li>
<li>RoPE scaling support for larger context lengths</li>
<li>rich and powerful generation parameters</li>
</ul>
<p>Use of the model requires you to accept its license and terms of use. Please, make sure you are logged into your Hugging Face account and ensure you have the latest version of <code>transformers</code>:</p>
<pre><code>pip install --upgrade transformers
huggingface-cli login
</code></pre>
<h4>
	<a id="bfloat16" href="#bfloat16">
		
	</a>
	<span>
		bfloat16
	</span>
</h4>
<p>This is how you‚Äôd use the base model in <code>bfloat16</code>. Falcon 180B is a big model, so please take into account the hardware requirements summarized in the table above.</p>
<pre><code><span>from</span> transformers <span>import</span> AutoTokenizer, AutoModelForCausalLM
<span>import</span> transformers
<span>import</span> torch

model_id = <span>"tiiuae/falcon-180B"</span>

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map=<span>"auto"</span>,
)

prompt = <span>"My name is Pedro, I live in"</span>
inputs = tokenizer(prompt, return_tensors=<span>"pt"</span>).to(<span>"cuda"</span>)

output = model.generate(
    input_ids=inputs[<span>"input_ids"</span>],
    attention_mask=inputs[<span>"attention_mask"</span>],
    do_sample=<span>True</span>,
    temperature=<span>0.6</span>,
    top_p=<span>0.9</span>,
    max_new_tokens=<span>50</span>,
)
output = output[<span>0</span>].to(<span>"cpu"</span>)
<span>print</span>(tokenizer.decode(output)
</code></pre>
<p>This could produce an output such as:</p>
<pre><code>My name is Pedro, I live in Portugal and I am 25 years old. I am a graphic designer, but I am also passionate about photography and video.
I love to travel and I am always looking for new adventures. I love to meet new people and explore new places.
</code></pre>
<h4>
	<a id="8-bit-and-4-bit-with-bitsandbytes" href="#8-bit-and-4-bit-with-bitsandbytes">
		
	</a>
	<span>
		8-bit and 4-bit with <code>bitsandbytes</code>
	</span>
</h4>
<p>The 8-bit and 4-bit quantized versions of Falcon 180B show almost no difference in evaluation with respect to the <code>bfloat16</code> reference! This is very good news for inference, as you can confidently use a quantized version to reduce hardware requirements. Keep in mind, though, that 8-bit inference is <em>much faster</em> than running the model in <code>4-bit</code>.</p>
<p>To use quantization, you need to install the <code>bitsandbytes</code> library and simply enable the corresponding flag when loading the model:</p>
<pre><code>model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    load_in_8bit=<span>True</span>,
    device_map=<span>"auto"</span>,
)
</code></pre>
<h4>
	<a id="chat-model" href="#chat-model">
		
	</a>
	<span>
		Chat Model
	</span>
</h4>
<p>As mentioned above, the version of the model fine-tuned to follow conversations used a very straightforward training template. We have to follow the same pattern in order to run chat-style inference. For reference, you can take a look at the <a href="https://huggingface.co/spaces/tiiuae/falcon-180b-demo/blob/main/app.py#L28">format_prompt</a> function in the Chat demo, which looks like this:</p>
<pre><code><span>def</span> <span>format_prompt</span>(<span>message, history, system_prompt</span>):
    prompt = <span>""</span>
    <span>if</span> system_prompt:
        prompt += <span>f"System: <span>{system_prompt}</span>\n"</span>
    <span>for</span> user_prompt, bot_response <span>in</span> history:
        prompt += <span>f"User: <span>{user_prompt}</span>\n"</span>
        prompt += <span>f"Falcon: <span>{bot_response}</span>\n"</span>
        prompt += <span>f"User: <span>{message}</span>\nFalcon:"</span>
    <span>return</span> prompt
</code></pre>
<p>As you can see, interactions from the user and responses by the model are preceded by <code>User: </code> and <code>Falcon: </code> separators. We concatenate them together to form a prompt containing the conversation's whole history. We can provide a system prompt to tweak the generation style.</p>
<h2>
	<a id="additional-resources" href="#additional-resources">
		
	</a>
	<span>
		Additional Resources
	</span>
</h2>
<ul>
<li><a href="https://huggingface.co/models?other=falcon&amp;sort=trending&amp;search=180">Models</a></li>
<li><a href="https://huggingface.co/spaces/tiiuae/falcon-180b-chat">Demo</a></li>
<li><a href="https://huggingface.co/blog/falcon">The Falcon has landed in the Hugging Face ecosystem</a></li>
<li><a href="https://falconllm.tii.ae/">Official Announcement</a></li>
</ul>
<h2>
	<a id="acknowledgments" href="#acknowledgments">
		
	</a>
	<span>
		Acknowledgments
	</span>
</h2>
<p>Releasing such a model with support and evaluations in the ecosystem would not be possible without the contributions of many community members, including <a href="https://huggingface.co/clefourrier">Cl√©mentine</a> and <a href="https://github.com/EleutherAI/lm-evaluation-harness">Eleuther Evaluation Harness</a> for LLM evaluations; <a href="https://huggingface.co/loubnabnl">Loubna</a> and <a href="https://huggingface.co/bigcode">BigCode</a> for code evaluations; <a href="https://hf.co/narsil">Nicolas</a> for Inference support; <a href="https://huggingface.co/lysandre">Lysandre</a>, <a href="https://huggingface.co/Rocketknight1">Matt</a>, <a href="https://huggingface.co/DanielHesslow">Daniel</a>, <a href="https://huggingface.co/amyeroberts">Amy</a>, <a href="https://huggingface.co/joaogante">Joao</a>, and <a href="https://huggingface.co/ArthurZ">Arthur</a> for integrating Falcon into transformers. Thanks to <a href="https://huggingface.co/BapBap">Baptiste</a> and <a href="https://huggingface.co/patrickvonplaten">Patrick</a> for the open-source demo. Thanks to <a href="https://huggingface.co/thomwolf">Thom</a>, <a href="https://huggingface.co/lewtun">Lewis</a>, <a href="https://huggingface.co/thebloke">TheBloke</a>, <a href="https://huggingface.co/nouamanetazi">Nouamane</a>, <a href="https://huggingface.co/timdettmers">Tim Dettmers</a> for multiple contributions enabling this to get out. Finally, thanks to the HF Cluster for enabling running LLM evaluations as well as providing inference for a free, open-source demo of the model.</p>
<!-- HTML_TAG_END --></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[If You‚Äôve Got a New Car, It‚Äôs a Data Privacy Nightmare (376 pts)]]></title>
            <link>https://gizmodo.com/mozilla-new-cars-data-privacy-report-1850805416</link>
            <guid>37404413</guid>
            <pubDate>Wed, 06 Sep 2023 12:54:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/mozilla-new-cars-data-privacy-report-1850805416">https://gizmodo.com/mozilla-new-cars-data-privacy-report-1850805416</a>, See on <a href="https://news.ycombinator.com/item?id=37404413">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Bad news: your car is a spy. If your vehicle was made in the last few years, you‚Äôre probably driving around in a data-harvesting machine that may collect personal information as sensitive as your race, weight, and sexual activity. Volkswagen‚Äôs cars reportedly know if you‚Äôre fastening your seatbelt and how hard you hit the brakes.</p><div data-video-id="193651" data-monetizable="true" data-position="sidebar" data-video-title="The FTC Just Prescribed a Can of Whoop Ass on Health Data" data-video-blog-id="4" data-video-network="gizmodo" data-video-duration="136" data-playlist="193651,195613,195603" data-current="193651"><div><p>The FTC Just Prescribed a Can of Whoop Ass on Health Data</p></div><video disablepictureinpicture="" muted="" playsinline="" width="100%" height="100%" crossorigin="anonymous" preload="none"><source data-src="https://vid.kinja.com/prod/193651/193651_240p.mp4" label="240p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/193651/193651_480p.mp4" label="480p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/193651/193651_720p.mp4" label="720p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/193651/193651_1080p.mp4" label="1080p" type="video/mp4"><track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/19103.vtt" srclang="en"></video><div><ul><li data-label="">Off</li><li data-label="English">English</li></ul></div></div><p>That‚Äôs according to new findings from Mozilla‚Äôs <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://foundation.mozilla.org/privacynotincluded/categories/cars/&quot;,{&quot;metric25&quot;:1}]]" href="https://foundation.mozilla.org/privacynotincluded/categories/cars/" target="_blank" rel="noopener noreferrer">*Privacy Not Included</a></span> project. The nonprofit found that every major car brand fails to adhere to the most basic privacy and security standards in new internet-connected models, and all 25 of the brands Mozilla examined flunked the organization‚Äôs test. Mozilla found brands including BMW, Ford, Toyota, Tesla, and Subaru collect data about drivers including race, facial expressions, weight, health information, and where you drive. Some of the cars tested collected data you wouldn‚Äôt expect your car to know about, including details about sexual activity, race, and immigration status, according to Mozilla.</p><p>‚ÄúMany people think of their car as a private space ‚Äî somewhere to call your doctor, have a personal conversation with your kid on the way to school, cry your eyes out over a break-up, or drive places you might not want the world to know about,‚Äù said Jen Caltrider, program direction of the *Privacy Not Included project, in a press release. ‚ÄúBut that perception no longer matches reality. All new cars today are privacy nightmares on wheels that collect huge amounts of personal information.‚Äù </p><p>Modern cars use a variety of data harvesting tools including microphones, cameras, and the phones drivers connect to their cars. Manufacturers also collect data through their apps and websites, and can then sell or share that data with third parties. </p><p>The worst offender was Nissan, Mozilla said. The carmaker‚Äôs privacy policy suggests the manufacturer collects information including sexual activity, health diagnosis data, and genetic data, though there‚Äôs no details about how exactly that data is gathered. Nissan reserves the right to share and sell ‚Äúpreferences, characteristics, psychological trends, predispositions, behavior, attitudes, intelligence, abilities, and aptitudes‚Äù to data brokers, law enforcement, and other third parties.</p><p>Other brands didn‚Äôt fare much better. Volkswagen, for example, collects your driving behaviors such as your seatbelt and braking habits and pairs that with details such as age and gender for targeted advertising. Kia‚Äôs privacy policy reserves the right to monitor your ‚Äúsex life,‚Äù and Mercedes-Benz ships cars with TikTok pre-installed on the infotainment system, an app that has its own <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/tiktok-ban-joe-biden-28000-apps-sdk-data-china-1850174019&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/tiktok-ban-joe-biden-28000-apps-sdk-data-china-1850174019">thicket of privacy problems</a></span>.</p><p>‚ÄúBMW NA provides our customers with comprehensive data privacy notices regarding the collection of their personal information. For individual control, BMW NA allows vehicle drivers to make granular choices regarding the collection and processing of their personal information,‚Äù said Phil DiIanni, a BMW spokesperson. DiIanni said BMW hasn‚Äôt reviewed the study, but said ‚ÄúBMW NA does not sell our customer‚Äôs in-vehicle personal information,‚Äù and the company takes ‚Äúcomprehensive measures to protect our customers‚Äô data.‚Äù</p><p>Mercedes-Benz spokesperson  Andrea Berg declined to comment, as the company hasn‚Äôt reviewed the study, but Berg said the MercedesMe Connect app gives users privacy settings and the ability to opt-out of certain services. Gizmodo contacted the other manufacturers named in this story, but none immediately provided comments.<br></p><p>The privacy and security problems extend beyond the nature of the data car companies siphon off about you. Mozilla said it was unable to determine whether the brands encrypt any of the data they collect, and only Mercedes-Benz responded to the organization‚Äôs questions. </p><p>Mozilla also found that many car brands engage in ‚Äúprivacy washing,‚Äù or presenting consumers with information that suggests they don‚Äôt have to worry about privacy issues when the exact opposite is true. Many leading<!-- --> manufacturers are signatories to the Alliance for Automotive Innovation‚Äôs ‚Äú<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.autosinnovate.org/innovation/Automotive%20Privacy/Consumer_Privacy_Principlesfor_VehicleTechnologies_Services-03-21-19.pdf&quot;,{&quot;metric25&quot;:1}]]" href="https://www.autosinnovate.org/innovation/Automotive%20Privacy/Consumer_Privacy_Principlesfor_VehicleTechnologies_Services-03-21-19.pdf" target="_blank" rel="noopener noreferrer">Consumer Privacy Protection Principles</a></span>.‚Äù According to Mozilla, these are a non-binding set of vague promises organized by the car manufacturers themselves. <br></p><p>Brian Weiss, a spokesperson for t<!-- -->he Alliance for Automotive<!-- --> Innovation, shared a link to <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.autosinnovate.org/association-update/1-Alliance%20for%20Automotive%20Innovation%20Letter%20on%20Federal%20Privacy%20Legislation.pdf&quot;,{&quot;metric25&quot;:1}]]" href="https://www.autosinnovate.org/association-update/1-Alliance%20for%20Automotive%20Innovation%20Letter%20on%20Federal%20Privacy%20Legislation.pdf" target="_blank" rel="noopener noreferrer">a letter the organization wrote</a></span> to congress about it<!-- -->s Privacy Principles. <!-- -->These principles ‚Äú<!-- -->are in effect today and enforceable by the Federal Trade Commission,‚Äù Weiss said. <br></p><p>Questions around consent are essentially a joke as well. Subaru, for example, says that by being a passenger in the car, you are considered a ‚Äúuser‚Äù who has given the company consent to harvest information about you. Mozilla said a number of car brands say it‚Äôs the drivers responsibility to let passengers know about their car‚Äôs privacy policies‚Äîas if the privacy policies are comprehensible to drivers in the first place. Toyota, for example, has a constellation of 12 different privacy policies for your reading pleasure. </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linux Network Performance Parameters Explained (177 pts)]]></title>
            <link>https://github.com/leandromoreira/linux-network-performance-parameters</link>
            <guid>37403799</guid>
            <pubDate>Wed, 06 Sep 2023 11:56:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/leandromoreira/linux-network-performance-parameters">https://github.com/leandromoreira/linux-network-performance-parameters</a>, See on <a href="https://news.ycombinator.com/item?id=37403799">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a href="https://github.com/leandromoreira/linux-network-performance-parameters/blob/master/README_RU.md" title="Russian">üá∑üá∫</a></p>
<h2 tabindex="-1" dir="auto">TOC</h2>
<ul dir="auto">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#linux-network-queues-overview">Linux network queues overview</a></li>
<li><a href="#fitting-the-sysctl-variables-into-the-linux-network-flow">Fitting the sysctl variables into the Linux network flow</a>
<ul dir="auto">
<li>Ingress - they're coming</li>
<li>Egress - they're leaving</li>
<li>How to check - perf</li>
</ul>
</li>
<li><a href="#what-why-and-how---network-and-sysctl-parameters">What, Why and How - network and sysctl parameters</a>
<ul dir="auto">
<li>Ring Buffer - rx,tx</li>
<li>Interrupt Coalescence (IC) - rx-usecs, tx-usecs, rx-frames, tx-frames (hardware IRQ)</li>
<li>Interrupt Coalescing (soft IRQ) and Ingress QDisc</li>
<li>Egress QDisc - txqueuelen and default_qdisc</li>
<li>TCP Read and Write Buffers/Queues</li>
<li>Honorable mentions - TCP FSM and congestion algorithm</li>
</ul>
</li>
<li><a href="#network-tools-for-testing-and-monitoring">Network tools</a></li>
<li><a href="#references">References</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Introduction</h2>
<p dir="auto">Sometimes people are looking for <a href="https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt" rel="nofollow">sysctl</a> cargo cult values that bring high throughput and low latency with no trade-off and that works on every occasion. That's not realistic, although we can say that the <strong>newer kernel versions are very well tuned by default</strong>. In fact, you might <a href="https://medium.com/@duhroach/the-bandwidth-delay-problem-c6a2a578b211" rel="nofollow">hurt performance if you mess with the defaults</a>.</p>
<p dir="auto">This brief tutorial shows <strong>where some of the most used and quoted sysctl/network parameters are located into the Linux network flow</strong>, it was heavily inspired by <a href="https://blog.packagecloud.io/eng/2016/10/11/monitoring-tuning-linux-networking-stack-receiving-data-illustrated/" rel="nofollow">the illustrated guide to Linux networking stack</a> and many of <a href="https://blog.cloudflare.com/how-to-achieve-low-latency/" rel="nofollow">Marek Majkowski's posts</a>.</p>
<blockquote>
<h4 tabindex="-1" dir="auto">Feel free to send corrections and suggestions! :)</h4>
</blockquote>
<h2 tabindex="-1" dir="auto">Linux network queues overview</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/linux-network-performance-parameters/blob/master/img/linux_network_flow.png"><img src="https://github.com/leandromoreira/linux-network-performance-parameters/raw/master/img/linux_network_flow.png" alt="linux network queues" title="A graphic representation of linux/kernel network main buffer / queues"></a></p>
<h2 tabindex="-1" dir="auto">Fitting the sysctl variables into the Linux network flow</h2>
<h2 tabindex="-1" dir="auto">Ingress - they're coming</h2>
<ol dir="auto">
<li>Packets arrive at the NIC</li>
<li>NIC will verify <code>MAC</code> (if not on promiscuous mode) and <code>FCS</code> and decide to drop or to continue</li>
<li>NIC will <a href="https://en.wikipedia.org/wiki/Direct_memory_access" rel="nofollow">DMA packets at RAM</a>, in a region previously prepared (mapped) by the driver</li>
<li>NIC will enqueue references to the packets at receive <a href="https://en.wikipedia.org/wiki/Circular_buffer" rel="nofollow">ring buffer</a> queue <code>rx</code> until <code>rx-usecs</code> timeout or <code>rx-frames</code></li>
<li>NIC will raise a <code>hard IRQ</code></li>
<li>CPU will run the <code>IRQ handler</code> that runs the driver's code</li>
<li>Driver will <code>schedule a NAPI</code>, clear the <code>hard IRQ</code> and return</li>
<li>Driver raise a <code>soft IRQ (NET_RX_SOFTIRQ)</code></li>
<li>NAPI will poll data from the receive ring buffer until <code>netdev_budget_usecs</code> timeout or <code>netdev_budget</code> and <code>dev_weight</code> packets</li>
<li>Linux will also allocate memory to <code>sk_buff</code></li>
<li>Linux fills the metadata: protocol, interface, setmacheader, removes ethernet</li>
<li>Linux will pass the skb to the kernel stack (<code>netif_receive_skb</code>)</li>
<li>It will set the network header, clone <code>skb</code> to taps (i.e. tcpdump) and pass it to tc ingress</li>
<li>Packets are handled to a qdisc sized <code>netdev_max_backlog</code> with its algorithm defined by <code>default_qdisc</code></li>
<li>It calls <code>ip_rcv</code> and packets are handed to IP</li>
<li>It calls netfilter (<code>PREROUTING</code>)</li>
<li>It looks at the routing table, if forwarding or local</li>
<li>If it's local it calls netfilter (<code>LOCAL_IN</code>)</li>
<li>It calls the L4 protocol (for instance <code>tcp_v4_rcv</code>)</li>
<li>It finds the right socket</li>
<li>It goes to the tcp finite state machine</li>
<li>Enqueue the packet to  the receive buffer and sized as <code>tcp_rmem</code> rules
<ol dir="auto">
<li>If <code>tcp_moderate_rcvbuf</code> is enabled kernel will auto-tune the receive buffer</li>
</ol>
</li>
<li>Kernel will signalize that there is data available to apps (epoll or any polling system)</li>
<li>Application wakes up and reads the data</li>
</ol>
<h2 tabindex="-1" dir="auto">Egress - they're leaving</h2>
<ol dir="auto">
<li>Application sends message (<code>sendmsg</code> or other)</li>
<li>TCP send message allocates skb_buff</li>
<li>It enqueues skb to the socket write buffer of <code>tcp_wmem</code> size</li>
<li>Builds the TCP header (src and dst port, checksum)</li>
<li>Calls L3 handler (in this case <code>ipv4</code> on <code>tcp_write_xmit</code> and <code>tcp_transmit_skb</code>)</li>
<li>L3 (<code>ip_queue_xmit</code>) does its work: build ip header and call netfilter (<code>LOCAL_OUT</code>)</li>
<li>Calls output route action</li>
<li>Calls netfilter (<code>POST_ROUTING</code>)</li>
<li>Fragment the packet (<code>ip_output</code>)</li>
<li>Calls L2 send function (<code>dev_queue_xmit</code>)</li>
<li>Feeds the output (QDisc) queue of <code>txqueuelen</code> length with its algorithm <code>default_qdisc</code></li>
<li>The driver code enqueue the packets at the <code>ring buffer tx</code></li>
<li>The driver will do a <code>soft IRQ (NET_TX_SOFTIRQ)</code> after <code>tx-usecs</code> timeout or <code>tx-frames</code></li>
<li>Re-enable hard IRQ to NIC</li>
<li>Driver will map all the packets (to be sent) to some DMA'ed region</li>
<li>NIC fetches the packets (via DMA) from RAM to transmit</li>
<li>After the transmission NIC will raise a <code>hard IRQ</code> to signal its completion</li>
<li>The driver will handle this IRQ (turn it off)</li>
<li>And schedule (<code>soft IRQ</code>) the NAPI poll system</li>
<li>NAPI will handle the receive packets signaling and free the RAM</li>
</ol>
<h2 tabindex="-1" dir="auto">How to check - perf</h2>
<p dir="auto">If you want to see the network tracing within Linux you can use <a href="https://man7.org/linux/man-pages/man1/perf-trace.1.html" rel="nofollow">perf</a>.</p>
<div data-snippet-clipboard-copy-content="docker run -it --rm --cap-add SYS_ADMIN --entrypoint bash ljishen/perf
apt-get update
apt-get install iputils-ping

# this is going to trace all events (not syscalls) to the subsystem net:* while performing the ping
perf trace --no-syscalls --event 'net:*' ping globo.com -c1 > /dev/null"><pre><code>docker run -it --rm --cap-add SYS_ADMIN --entrypoint bash ljishen/perf
apt-get update
apt-get install iputils-ping

# this is going to trace all events (not syscalls) to the subsystem net:* while performing the ping
perf trace --no-syscalls --event 'net:*' ping globo.com -c1 &gt; /dev/null
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/55913/147019725-69624e67-b3ca-48b4-a823-10521d2bed83.png"><img src="https://user-images.githubusercontent.com/55913/147019725-69624e67-b3ca-48b4-a823-10521d2bed83.png" alt="perf trace network"></a></p>
<h2 tabindex="-1" dir="auto">What, Why and How - network and sysctl parameters</h2>
<h2 tabindex="-1" dir="auto">Ring Buffer - rx,tx</h2>
<ul dir="auto">
<li><strong>What</strong> - the driver receive/send queue a single or multiple queues with a fixed size, usually implemented as FIFO, it is located at RAM</li>
<li><strong>Why</strong> - buffer to smoothly accept bursts of connections without dropping them, you might need to increase these queues when you see drops or overrun, aka there are more packets coming than the kernel is able to consume them, the side effect might be increased latency.</li>
<li><strong>How:</strong>
<ul dir="auto">
<li><strong>Check command:</strong> <code>ethtool -g ethX</code></li>
<li><strong>Change command:</strong> <code>ethtool -G ethX rx value tx value</code></li>
<li><strong>How to monitor:</strong> <code>ethtool -S ethX | grep -e "err" -e "drop" -e "over" -e "miss" -e "timeout" -e "reset" -e "restar" -e "collis" -e "over" | grep -v "\: 0"</code></li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto">Interrupt Coalescence (IC) - rx-usecs, tx-usecs, rx-frames, tx-frames (hardware IRQ)</h2>
<ul dir="auto">
<li><strong>What</strong> - number of microseconds/frames to wait before raising a hardIRQ, from the NIC perspective it'll DMA data packets until this timeout/number of frames</li>
<li><strong>Why</strong> - reduce CPUs usage, hard IRQ, might increase throughput at cost of latency.</li>
<li><strong>How:</strong>
<ul dir="auto">
<li><strong>Check command:</strong> <code>ethtool -c ethX</code></li>
<li><strong>Change command:</strong> <code>ethtool -C ethX rx-usecs value tx-usecs value</code></li>
<li><strong>How to monitor:</strong> <code>cat /proc/interrupts</code></li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto">Interrupt Coalescing (soft IRQ) and Ingress QDisc</h2>
<ul dir="auto">
<li><strong>What</strong> - maximum number of microseconds in one <a href="https://en.wikipedia.org/wiki/New_API" rel="nofollow">NAPI</a> polling cycle. Polling will exit when either <code>netdev_budget_usecs</code> have elapsed during the poll cycle or the number of packets processed reaches  <code>netdev_budget</code>.</li>
<li><strong>Why</strong> - instead of reacting to tons of softIRQ, the driver keeps polling data; keep an eye on <code>dropped</code> (# of packets that were dropped because <code>netdev_max_backlog</code> was exceeded) and <code>squeezed</code> (# of times ksoftirq ran out of <code>netdev_budget</code> or time slice with work remaining).</li>
<li><strong>How:</strong>
<ul dir="auto">
<li><strong>Check command:</strong> <code>sysctl net.core.netdev_budget_usecs</code></li>
<li><strong>Change command:</strong> <code>sysctl -w net.core.netdev_budget_usecs value</code></li>
<li><strong>How to monitor:</strong> <code>cat /proc/net/softnet_stat</code>; or a <a href="https://raw.githubusercontent.com/majek/dump/master/how-to-receive-a-packet/softnet.sh" rel="nofollow">better tool</a></li>
</ul>
</li>
<li><strong>What</strong> - <code>netdev_budget</code> is the maximum number of packets taken from all interfaces in one polling cycle (NAPI poll). In one polling cycle interfaces which are registered to polling are probed in a round-robin manner. Also, a polling cycle may not exceed <code>netdev_budget_usecs</code> microseconds, even if <code>netdev_budget</code> has not been exhausted.</li>
<li><strong>How:</strong>
<ul dir="auto">
<li><strong>Check command:</strong> <code>sysctl net.core.netdev_budget</code></li>
<li><strong>Change command:</strong> <code>sysctl -w net.core.netdev_budget value</code></li>
<li><strong>How to monitor:</strong> <code>cat /proc/net/softnet_stat</code>; or a <a href="https://raw.githubusercontent.com/majek/dump/master/how-to-receive-a-packet/softnet.sh" rel="nofollow">better tool</a></li>
</ul>
</li>
<li><strong>What</strong> - <code>dev_weight</code> is the maximum number of packets that kernel can handle on a NAPI interrupt, it's a Per-CPU variable. For drivers that support LRO or GRO_HW, a hardware aggregated packet is counted as one packet in this.</li>
<li><strong>How:</strong>
<ul dir="auto">
<li><strong>Check command:</strong> <code>sysctl net.core.dev_weight</code></li>
<li><strong>Change command:</strong> <code>sysctl -w net.core.dev_weight value</code></li>
<li><strong>How to monitor:</strong> <code>cat /proc/net/softnet_stat</code>; or a <a href="https://raw.githubusercontent.com/majek/dump/master/how-to-receive-a-packet/softnet.sh" rel="nofollow">better tool</a></li>
</ul>
</li>
<li><strong>What</strong> - <code>netdev_max_backlog</code> is the maximum number  of  packets,  queued  on  the  INPUT side (<em>the ingress qdisc</em>), when the interface receives packets faster than kernel can process them.</li>
<li><strong>How:</strong>
<ul dir="auto">
<li><strong>Check command:</strong> <code>sysctl net.core.netdev_max_backlog</code></li>
<li><strong>Change command:</strong> <code>sysctl -w net.core.netdev_max_backlog value</code></li>
<li><strong>How to monitor:</strong> <code>cat /proc/net/softnet_stat</code>; or a <a href="https://raw.githubusercontent.com/majek/dump/master/how-to-receive-a-packet/softnet.sh" rel="nofollow">better tool</a></li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto">Egress QDisc - txqueuelen and default_qdisc</h2>
<ul dir="auto">
<li><strong>What</strong> - <code>txqueuelen</code> is the maximum number of packets, queued on the OUTPUT side.</li>
<li><strong>Why</strong> - a buffer/queue to face connection burst and also to apply <a href="http://tldp.org/HOWTO/Traffic-Control-HOWTO/intro.html" rel="nofollow">tc (traffic control).</a></li>
<li><strong>How:</strong>
<ul dir="auto">
<li><strong>Check command:</strong> <code>ip link show dev ethX</code></li>
<li><strong>Change command:</strong> <code>ip link set dev ethX txqueuelen N</code></li>
<li><strong>How to monitor:</strong> <code>ip -s link</code></li>
</ul>
</li>
<li><strong>What</strong> - <code>default_qdisc</code> is the default queuing discipline to use for network devices.</li>
<li><strong>Why</strong> - each application has different load and need to traffic control and it is used also to fight against <a href="https://www.bufferbloat.net/projects/codel/wiki/" rel="nofollow">bufferbloat</a></li>
<li><strong>How:</strong>
<ul dir="auto">
<li><strong>Check command:</strong> <code>sysctl net.core.default_qdisc</code></li>
<li><strong>Change command:</strong> <code>sysctl -w net.core.default_qdisc value</code></li>
<li><strong>How to monitor:</strong>   <code>tc -s qdisc ls dev ethX</code></li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto">TCP Read and Write Buffers/Queues</h2>
<blockquote>
<p dir="auto">The policy that defines what is <a href="https://wwwx.cs.unc.edu/~sparkst/howto/network_tuning.php" rel="nofollow">memory pressure</a> is specified at tcp_mem and tcp_moderate_rcvbuf.</p>
</blockquote>
<ul dir="auto">
<li><strong>What</strong> - <code>tcp_rmem</code> - min (size used under memory pressure), default (initial size), max (maximum size) - size of receive buffer used by TCP sockets.</li>
<li><strong>Why</strong> - the application buffer/queue to the write/send data, <a href="https://blog.cloudflare.com/the-story-of-one-latency-spike/" rel="nofollow">understand its consequences can help a lot</a>.</li>
<li><strong>How:</strong>
<ul dir="auto">
<li><strong>Check command:</strong> <code>sysctl net.ipv4.tcp_rmem</code></li>
<li><strong>Change command:</strong> <code>sysctl -w net.ipv4.tcp_rmem="min default max"</code>; when changing default value, remember to restart your user space app (i.e. your web server, nginx, etc)</li>
<li><strong>How to monitor:</strong> <code>cat /proc/net/sockstat</code></li>
</ul>
</li>
<li><strong>What</strong> - <code>tcp_wmem</code> - min (size used under memory pressure), default (initial size), max (maximum size) - size of send buffer used by TCP sockets.</li>
<li><strong>How:</strong>
<ul dir="auto">
<li><strong>Check command:</strong> <code>sysctl net.ipv4.tcp_wmem</code></li>
<li><strong>Change command:</strong> <code>sysctl -w net.ipv4.tcp_wmem="min default max"</code>; when changing default value, remember to restart your user space app (i.e. your web server, nginx, etc)</li>
<li><strong>How to monitor:</strong> <code>cat /proc/net/sockstat</code></li>
</ul>
</li>
<li><strong>What</strong> <code>tcp_moderate_rcvbuf</code> - If set, TCP performs receive buffer auto-tuning, attempting to automatically size the buffer.</li>
<li><strong>How:</strong>
<ul dir="auto">
<li><strong>Check command:</strong> <code>sysctl net.ipv4.tcp_moderate_rcvbuf</code></li>
<li><strong>Change command:</strong> <code>sysctl -w net.ipv4.tcp_moderate_rcvbuf value</code></li>
<li><strong>How to monitor:</strong> <code>cat /proc/net/sockstat</code></li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto">Honorable mentions - TCP FSM and congestion algorithm</h2>
<blockquote>
<p dir="auto">Accept and SYN Queues are governed by net.core.somaxconn and net.ipv4.tcp_max_syn_backlog. <a href="https://blog.cloudflare.com/syn-packet-handling-in-the-wild/#queuesizelimits" rel="nofollow">Nowadays net.core.somaxconn caps both queue sizes.</a></p>
</blockquote>
<ul dir="auto">
<li><code>sysctl net.core.somaxconn</code> - provides an upper limit on the value of the backlog parameter passed to the <a href="https://eklitzke.org/how-tcp-sockets-work" rel="nofollow"><code>listen()</code> function</a>, known in userspace as <code>SOMAXCONN</code>. If you change this value, you should also change your application to a compatible value (i.e. <a href="http://nginx.org/en/docs/http/ngx_http_core_module.html#listen" rel="nofollow">nginx backlog</a>).</li>
<li><code>cat /proc/sys/net/ipv4/tcp_fin_timeout</code> - this specifies the number of seconds to wait for a final FIN packet before the socket is forcibly closed.  This is strictly a violation of the TCP specification but required to prevent denial-of-service attacks.</li>
<li><code>cat /proc/sys/net/ipv4/tcp_available_congestion_control</code> - shows the available congestion control choices that are registered.</li>
<li><code>cat /proc/sys/net/ipv4/tcp_congestion_control</code> - sets the congestion control algorithm to be used for new connections.</li>
<li><code>cat /proc/sys/net/ipv4/tcp_max_syn_backlog</code> - sets the maximum number of queued connection requests which have still not received an acknowledgment from the connecting client; if this number is exceeded, the kernel will begin dropping requests.</li>
<li><code>cat /proc/sys/net/ipv4/tcp_syncookies</code> - enables/disables <a href="https://en.wikipedia.org/wiki/SYN_cookies" rel="nofollow">syn cookies</a>, useful for protecting against <a href="https://www.cloudflare.com/learning/ddos/syn-flood-ddos-attack/" rel="nofollow">syn flood attacks</a>.</li>
<li><code>cat /proc/sys/net/ipv4/tcp_slow_start_after_idle</code> - enables/disables tcp slow start.</li>
</ul>
<p dir="auto"><strong>How to monitor:</strong></p>
<ul dir="auto">
<li><code>netstat -atn | awk '/tcp/ {print $6}' | sort | uniq -c</code> - summary by state</li>
<li><code>ss -neopt state time-wait | wc -l</code> - counters by a specific state: <code>established</code>, <code>syn-sent</code>, <code>syn-recv</code>, <code>fin-wait-1</code>, <code>fin-wait-2</code>, <code>time-wait</code>, <code>closed</code>, <code>close-wait</code>, <code>last-ack</code>, <code>listening</code>, <code>closing</code></li>
<li><code>netstat -st</code> - tcp stats summary</li>
<li><code>nstat -a</code> - human-friendly tcp stats summary</li>
<li><code>cat /proc/net/sockstat</code> - summarized socket stats</li>
<li><code>cat /proc/net/tcp</code> - detailed stats, see each field meaning at the <a href="https://www.kernel.org/doc/Documentation/networking/proc_net_tcp.txt" rel="nofollow">kernel docs</a></li>
<li><code>cat /proc/net/netstat</code> - <code>ListenOverflows</code> and <code>ListenDrops</code> are important fields to keep an eye on
<ul dir="auto">
<li><code>cat /proc/net/netstat | awk '(f==0) { i=1; while ( i&lt;=NF) {n[i] = $i; i++ }; f=1; next} \ (f==1){ i=2; while ( i&lt;=NF){ printf "%s = %d\n", n[i], $i; i++}; f=0} ' | grep -v "= 0</code>; a <a href="https://sa-chernomor.livejournal.com/9858.html" rel="nofollow">human readable <code>/proc/net/netstat</code></a></li>
</ul>
</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/91bdb6f4cc46fa8db6b6e41c712eb1e37c3c598e05c553a889a3d270e7e6a60e/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f612f61322f5463705f73746174655f6469616772616d5f66697865642e737667"><img src="https://camo.githubusercontent.com/91bdb6f4cc46fa8db6b6e41c712eb1e37c3c598e05c553a889a3d270e7e6a60e/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f612f61322f5463705f73746174655f6469616772616d5f66697865642e737667" alt="tcp finite state machine" title="A graphic representation of tcp tcp finite state machine" data-canonical-src="https://upload.wikimedia.org/wikipedia/commons/a/a2/Tcp_state_diagram_fixed.svg"></a>
Source: <a href="https://commons.wikimedia.org/wiki/File:Tcp_state_diagram_fixed_new.svg" rel="nofollow">https://commons.wikimedia.org/wiki/File:Tcp_state_diagram_fixed_new.svg</a></p>
<h2 tabindex="-1" dir="auto">Network tools for testing and monitoring</h2>
<ul dir="auto">
<li><a href="https://iperf.fr/" rel="nofollow">iperf3</a> - network throughput</li>
<li><a href="https://github.com/tsenart/vegeta">vegeta</a> - HTTP load testing tool</li>
<li><a href="https://github.com/firehol/netdata">netdata</a> - system for distributed real-time performance and health monitoring</li>
</ul>
<h2 tabindex="-1" dir="auto">References</h2>
<ul dir="auto">
<li><a href="https://www.kernel.org/doc/Documentation/sysctl/net.txt" rel="nofollow">https://www.kernel.org/doc/Documentation/sysctl/net.txt</a></li>
<li><a href="https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt" rel="nofollow">https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt</a></li>
<li><a href="https://www.kernel.org/doc/Documentation/networking/scaling.txt" rel="nofollow">https://www.kernel.org/doc/Documentation/networking/scaling.txt</a></li>
<li><a href="https://www.kernel.org/doc/Documentation/networking/proc_net_tcp.txt" rel="nofollow">https://www.kernel.org/doc/Documentation/networking/proc_net_tcp.txt</a></li>
<li><a href="https://www.kernel.org/doc/Documentation/networking/multiqueue.txt" rel="nofollow">https://www.kernel.org/doc/Documentation/networking/multiqueue.txt</a></li>
<li><a href="http://man7.org/linux/man-pages/man7/tcp.7.html" rel="nofollow">http://man7.org/linux/man-pages/man7/tcp.7.html</a></li>
<li><a href="http://man7.org/linux/man-pages/man8/tc.8.html" rel="nofollow">http://man7.org/linux/man-pages/man8/tc.8.html</a></li>
<li><a href="http://cseweb.ucsd.edu/classes/fa09/cse124/presentations/TCPlinux_implementation.pdf" rel="nofollow">http://cseweb.ucsd.edu/classes/fa09/cse124/presentations/TCPlinux_implementation.pdf</a></li>
<li><a href="https://netdevconf.org/1.2/papers/bbr-netdev-1.2.new.new.pdf" rel="nofollow">https://netdevconf.org/1.2/papers/bbr-netdev-1.2.new.new.pdf</a></li>
<li><a href="https://blog.cloudflare.com/how-to-receive-a-million-packets/" rel="nofollow">https://blog.cloudflare.com/how-to-receive-a-million-packets/</a></li>
<li><a href="https://blog.cloudflare.com/how-to-achieve-low-latency/" rel="nofollow">https://blog.cloudflare.com/how-to-achieve-low-latency/</a></li>
<li><a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/" rel="nofollow">https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/</a></li>
<li><a href="https://www.youtube.com/watch?v=6Fl1rsxk4JQ" rel="nofollow">https://www.youtube.com/watch?v=6Fl1rsxk4JQ</a></li>
<li><a href="https://oxnz.github.io/2016/05/03/performance-tuning-networking/" rel="nofollow">https://oxnz.github.io/2016/05/03/performance-tuning-networking/</a></li>
<li><a href="https://www.intel.com/content/dam/www/public/us/en/documents/reference-guides/xl710-x710-performance-tuning-linux-guide.pdf" rel="nofollow">https://www.intel.com/content/dam/www/public/us/en/documents/reference-guides/xl710-x710-performance-tuning-linux-guide.pdf</a></li>
<li><a href="https://access.redhat.com/sites/default/files/attachments/20150325_network_performance_tuning.pdf" rel="nofollow">https://access.redhat.com/sites/default/files/attachments/20150325_network_performance_tuning.pdf</a></li>
<li><a href="https://medium.com/@matteocroce/linux-and-freebsd-networking-cbadcdb15ddd" rel="nofollow">https://medium.com/@matteocroce/linux-and-freebsd-networking-cbadcdb15ddd</a></li>
<li><a href="https://blogs.technet.microsoft.com/networking/2009/08/12/where-do-resets-come-from-no-the-stork-does-not-bring-them/" rel="nofollow">https://blogs.technet.microsoft.com/networking/2009/08/12/where-do-resets-come-from-no-the-stork-does-not-bring-them/</a></li>
<li><a href="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/multi-core-processor-based-linux-paper.pdf" rel="nofollow">https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/multi-core-processor-based-linux-paper.pdf</a></li>
<li><a href="http://syuu.dokukino.com/2013/05/linux-kernel-features-for-high-speed.html" rel="nofollow">http://syuu.dokukino.com/2013/05/linux-kernel-features-for-high-speed.html</a></li>
<li><a href="https://www.bufferbloat.net/projects/codel/wiki/Best_practices_for_benchmarking_Codel_and_FQ_Codel/" rel="nofollow">https://www.bufferbloat.net/projects/codel/wiki/Best_practices_for_benchmarking_Codel_and_FQ_Codel/</a></li>
<li><a href="https://software.intel.com/en-us/articles/setting-up-intel-ethernet-flow-director" rel="nofollow">https://software.intel.com/en-us/articles/setting-up-intel-ethernet-flow-director</a></li>
<li><a href="https://courses.engr.illinois.edu/cs423/sp2014/Lectures/LinuxDriver.pdf" rel="nofollow">https://courses.engr.illinois.edu/cs423/sp2014/Lectures/LinuxDriver.pdf</a></li>
<li><a href="https://www.coverfire.com/articles/queueing-in-the-linux-network-stack/" rel="nofollow">https://www.coverfire.com/articles/queueing-in-the-linux-network-stack/</a></li>
<li><a href="http://vger.kernel.org/~davem/skb.html" rel="nofollow">http://vger.kernel.org/~davem/skb.html</a></li>
<li><a href="https://www.missoulapubliclibrary.org/ftp/LinuxJournal/LJ13-07.pdf" rel="nofollow">https://www.missoulapubliclibrary.org/ftp/LinuxJournal/LJ13-07.pdf</a></li>
<li><a href="https://opensourceforu.com/2016/10/network-performance-monitoring/" rel="nofollow">https://opensourceforu.com/2016/10/network-performance-monitoring/</a></li>
<li><a href="https://www.yumpu.com/en/document/view/55400902/an-adventure-of-analysis-and-optimisation-of-the-linux-networking-stack" rel="nofollow">https://www.yumpu.com/en/document/view/55400902/an-adventure-of-analysis-and-optimisation-of-the-linux-networking-stack</a></li>
<li><a href="https://lwn.net/Articles/616241/" rel="nofollow">https://lwn.net/Articles/616241/</a></li>
<li><a href="https://medium.com/@duhroach/tools-to-profile-networking-performance-3141870d5233" rel="nofollow">https://medium.com/@duhroach/tools-to-profile-networking-performance-3141870d5233</a></li>
<li><a href="https://www.lmax.com/blog/staff-blogs/2016/05/06/navigating-linux-kernel-network-stack-receive-path/" rel="nofollow">https://www.lmax.com/blog/staff-blogs/2016/05/06/navigating-linux-kernel-network-stack-receive-path/</a></li>
<li><a href="https://fasterdata.es.net/host-tuning/linux/100g-tuning/" rel="nofollow">https://fasterdata.es.net/host-tuning/linux/100g-tuning/</a></li>
<li><a href="http://tcpipguide.com/free/t_TCPOperationalOverviewandtheTCPFiniteStateMachineF-2.htm" rel="nofollow">http://tcpipguide.com/free/t_TCPOperationalOverviewandtheTCPFiniteStateMachineF-2.htm</a></li>
<li><a href="http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html" rel="nofollow">http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html</a></li>
<li><a href="https://people.cs.clemson.edu/~westall/853/tcpperf.pdf" rel="nofollow">https://people.cs.clemson.edu/~westall/853/tcpperf.pdf</a></li>
<li><a href="http://tldp.org/HOWTO/Traffic-Control-HOWTO/classless-qdiscs.html" rel="nofollow">http://tldp.org/HOWTO/Traffic-Control-HOWTO/classless-qdiscs.html</a></li>
<li><a href="https://fasterdata.es.net/assets/Papers-and-Publications/100G-Tuning-TechEx2016.tierney.pdf" rel="nofollow">https://fasterdata.es.net/assets/Papers-and-Publications/100G-Tuning-TechEx2016.tierney.pdf</a></li>
<li><a href="https://www.kernel.org/doc/ols/2009/ols2009-pages-169-184.pdf" rel="nofollow">https://www.kernel.org/doc/ols/2009/ols2009-pages-169-184.pdf</a></li>
<li><a href="https://devcentral.f5.com/articles/the-send-buffer-in-depth-21845" rel="nofollow">https://devcentral.f5.com/articles/the-send-buffer-in-depth-21845</a></li>
<li><a href="http://packetbomb.com/understanding-throughput-and-tcp-windows/" rel="nofollow">http://packetbomb.com/understanding-throughput-and-tcp-windows/</a></li>
<li><a href="https://www.speedguide.net/bdp.php" rel="nofollow">https://www.speedguide.net/bdp.php</a></li>
<li><a href="https://www.switch.ch/network/tools/tcp_throughput/" rel="nofollow">https://www.switch.ch/network/tools/tcp_throughput/</a></li>
<li><a href="https://www.ibm.com/support/knowledgecenter/en/SSQPD3_2.6.0/com.ibm.wllm.doc/usingethtoolrates.html" rel="nofollow">https://www.ibm.com/support/knowledgecenter/en/SSQPD3_2.6.0/com.ibm.wllm.doc/usingethtoolrates.html</a></li>
<li><a href="https://blog.tsunanet.net/2011/03/out-of-socket-memory.html" rel="nofollow">https://blog.tsunanet.net/2011/03/out-of-socket-memory.html</a></li>
<li><a href="https://unix.stackexchange.com/questions/12985/how-to-check-rx-ring-max-backlog-and-max-syn-backlog-size" rel="nofollow">https://unix.stackexchange.com/questions/12985/how-to-check-rx-ring-max-backlog-and-max-syn-backlog-size</a></li>
<li><a href="https://serverfault.com/questions/498245/how-to-reduce-number-of-time-wait-processes" rel="nofollow">https://serverfault.com/questions/498245/how-to-reduce-number-of-time-wait-processes</a></li>
<li><a href="https://unix.stackexchange.com/questions/419518/how-to-tell-how-much-memory-tcp-buffers-are-actually-using" rel="nofollow">https://unix.stackexchange.com/questions/419518/how-to-tell-how-much-memory-tcp-buffers-are-actually-using</a></li>
<li><a href="https://eklitzke.org/how-tcp-sockets-work" rel="nofollow">https://eklitzke.org/how-tcp-sockets-work</a></li>
<li><a href="https://www.linux.com/learn/intro-to-linux/2017/7/introduction-ss-command" rel="nofollow">https://www.linux.com/learn/intro-to-linux/2017/7/introduction-ss-command</a></li>
<li><a href="https://staaldraad.github.io/2017/12/20/netstat-without-netstat/" rel="nofollow">https://staaldraad.github.io/2017/12/20/netstat-without-netstat/</a></li>
<li><a href="https://loicpefferkorn.net/2016/03/linux-network-metrics-why-you-should-use-nstat-instead-of-netstat/" rel="nofollow">https://loicpefferkorn.net/2016/03/linux-network-metrics-why-you-should-use-nstat-instead-of-netstat/</a></li>
<li><a href="http://assimilationsystems.com/2015/12/29/bufferbloat-network-best-practice/" rel="nofollow">http://assimilationsystems.com/2015/12/29/bufferbloat-network-best-practice/</a></li>
<li><a href="https://wwwx.cs.unc.edu/~sparkst/howto/network_tuning.php" rel="nofollow">https://wwwx.cs.unc.edu/~sparkst/howto/network_tuning.php</a></li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Norway court rules against Facebook owner Meta in privacy case (131 pts)]]></title>
            <link>https://www.reuters.com/technology/norway-data-regulator-fine-meta-over-privacy-breaches-2023-08-07/</link>
            <guid>37403583</guid>
            <pubDate>Wed, 06 Sep 2023 11:23:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/norway-data-regulator-fine-meta-over-privacy-breaches-2023-08-07/">https://www.reuters.com/technology/norway-data-regulator-fine-meta-over-privacy-breaches-2023-08-07/</a>, See on <a href="https://news.ycombinator.com/item?id=37403583">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="primary-gallery"><p data-testid="Body"><b data-testid="Body">[1/2]</b><span>A smartphone with Meta logo is seen in front of displayed Facebook's new rebrand logo Meta in this illustration taken, October 28, 2021. REUTERS/Dado Ruvic/Illustration/File Photo <a data-testid="Link" href="https://www.reutersagency.com/en/licensereuterscontent/?utm_medium=rcom-article-media&amp;utm_campaign=rcom-rcp-lead" target="_blank"> Acquire Licensing Rights</a></span></p></div><div><div><div><ul role="tablist"><li data-testid="Text" role="tab" aria-selected="true" tabindex="0">Summary</li><li data-testid="Text" role="tab" aria-selected="false" tabindex="-1">Companies</li></ul></div><div><ul><li data-testid="Body">Norwegian data regulator hails 'big victory for privacy'</li><li data-testid="Body">Meta says it is 'disappointed'</li><li data-testid="Body">Fine could be widened to rest of Europe</li></ul></div></div><p data-testid="paragraph-0">OSLO, Sept 6 (Reuters) - Meta Platforms <a data-testid="Link" href="https://www.reuters.com/markets/companies/META.O" target="_blank">(META.O)</a> can be fined for breaching users' privacy, a Norwegian court ruled on Wednesday, stopping an attempt by the owner of Facebook and Instagram to halt a fine imposed by the country's data regulator.</p><p data-testid="paragraph-1">Meta has been fined one million crowns ($93,200) per day since <a data-testid="Link" href="https://www.reuters.com/technology/norway-data-regulator-fine-meta-over-privacy-breaches-2023-08-07/">Aug. 14</a> for harvesting user data and using it to target advertising at them. So-called behavioural advertising is a business model common to Big Tech.</p><p data-testid="paragraph-2">The owner of Facebook and Instagram had sought a temporary injunction against the order from the Norwegian data regulator, Datatilsynet, which imposed a daily fine for three months.</p><p data-testid="paragraph-3">"This is a big victory for privacy," Datatilsynet said in a statement.</p><p data-testid="paragraph-4">The case could have wider European implications as Datatilsynet is considering referring the decision to the European data regulator.</p><p data-testid="paragraph-5">If the European Data Protection Board agrees with Datatilsynet, it could widen the decision's territorial scope to the rest of Europe and make the fine permanent.</p><p data-testid="paragraph-6">The Norwegian agency said on Wednesday it had not yet made a decision on a referral.</p><p data-testid="paragraph-7">Norway is not a member of the European Union but it is a member of the European single market.</p><h2 data-testid="Heading">VERDICT</h2><p data-testid="paragraph-8">Meta argued, among other things, that the authority's decision had been disproportionate, impossible to meet and in violation of other laws, but the court rejected the claims.</p><p data-testid="paragraph-9">"None of these arguments will affect the outcome," Judge Henning Kristiansen said in his ruling, which also orders Meta to pay Datatilsynet's case costs.</p><p data-testid="paragraph-10">Meta declined to say whether it would appeal the verdict.</p><p data-testid="paragraph-11">"We are disappointed by today's decision and will now consider our next steps," a Meta spokesperson said in an emailed statement to Reuters.</p><p data-testid="paragraph-12">Meta told a two-day <a data-testid="Link" href="https://www.reuters.com/technology/facebook-owner-meta-platforms-seeks-stop-privacy-breach-fine-norway-2023-08-22/">court hearing</a> in August it had already committed to ask for consent from users and that Datatilsynet used an "expedited process" that was unnecessary and did not give the company enough time to answer.</p><p data-testid="paragraph-13">The regulator has said it was unclear when, and how, Meta would seek consent from users and that, in the meantime, users' rights were being violated.</p><p data-testid="paragraph-14">($1 = 10.7271 Norwegian crowns)</p><p data-testid="Body">Reporting by Gwladys Fouche, editing by Terje Solsvik and Christina Fincher</p><p data-testid="Body">Our Standards: <a data-testid="Link" href="https://www.thomsonreuters.com/en/about-us/trust-principles.html" target="_blank">The Thomson Reuters Trust Principles.</a></p><div><address><p data-testid="Body">Oversees news coverage from Norway for Reuters and loves flying to Svalbard in the Arctic, oil platforms in the North Sea, and guessing who is going to win the Nobel Peace Prize. Born in France and with Reuters since 2010, she has worked for The Guardian, Agence France-Presse and Al Jazeera English, among others, and speaks four languages.</p></address></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Toyota blames factory shutdown in Japan on ‚Äòinsufficient disk space‚Äô (116 pts)]]></title>
            <link>https://www.theguardian.com/business/2023/sep/06/toyota-blames-factory-shutdown-in-japan-on-insufficient-disk-space</link>
            <guid>37403566</guid>
            <pubDate>Wed, 06 Sep 2023 11:19:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/business/2023/sep/06/toyota-blames-factory-shutdown-in-japan-on-insufficient-disk-space">https://www.theguardian.com/business/2023/sep/06/toyota-blames-factory-shutdown-in-japan-on-insufficient-disk-space</a>, See on <a href="https://news.ycombinator.com/item?id=37403566">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p><a href="https://www.theguardian.com/business/toyota" data-link-name="in body link">Toyota</a> has blamed a recent shutdown of all of its factories in <a href="https://www.theguardian.com/world/japan" data-link-name="in body link">Japan</a> on a system malfunction caused by ‚Äúinsufficient disk space‚Äù.</p><p>The Japanese carmaker said the stoppage on 29 August at all 14 of its domestic plants occurred after servers that process orders for vehicle parts broke down following a maintenance procedure carried out the previous day.</p><p>During this operation, ‚Äúdata that had accumulated in the database was deleted and organised, and an error occurred due to insufficient disk space, causing the system to stop‚Äù, <a href="https://www.theguardian.com/business/toyota" data-link-name="in body link" data-component="auto-linked-tag">Toyota</a> said on Wednesday.</p><p>The world‚Äôs top-selling automaker reiterated that the incident had not been caused by a cyber-attack. ‚ÄúWe would like to apologise once again to our customers, suppliers, and related parties for any inconvenience caused by the suspension of our domestic plants,‚Äù it said.</p><p>Toyota said the system had been restored after the data was transferred to a server with a bigger capacity, enabling it to restart production at the plants ‚Äì which together account for about a third of the automaker‚Äôs global production ‚Äì the following day.</p><p>‚ÄúWe will review our maintenance procedures and strengthen our efforts to prevent a recurrence, so that we can deliver as many vehicles to our customers as soon as possible,‚Äù it added.</p><p>Toyota is known for its ‚Äújust-in-time‚Äù production system of providing only small deliveries of necessary parts and other items at various stages of the assembly process.</p><p>This minimises costs and improves efficiency, and is studied by other manufacturers and at business schools around the world. However, as last month‚Äôs technical glitch proves, it comes with risks.</p><p>Toyota had to shut down the same 14 factories for a day in February last year when one of its suppliers said one of its file servers had been infected with a virus, raising questions about the cybersecurity of Japan‚Äôs supply chains.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How would you say ‚ÄúShe said goodbye too many times before.‚Äù in Latin? (309 pts)]]></title>
            <link>https://latin.stackexchange.com/a/21492</link>
            <guid>37403136</guid>
            <pubDate>Wed, 06 Sep 2023 09:57:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://latin.stackexchange.com/a/21492">https://latin.stackexchange.com/a/21492</a>, See on <a href="https://news.ycombinator.com/item?id=37403136">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>Cicero provides a possibility for "too many times" with <em>nimium saepe</em>:</p>
<blockquote>
<p>Quare "bene et praeclare" quamvis nobis saepe dicatur; "belle et festive" <strong>nimium saepe</strong> nolo (Cicero, <em>De Oratore</em> 3.101.2)</p>
</blockquote>
<p>I think the meaning is even clearer with Seneca's <em>Medea</em>:</p>
<blockquote>
<p>Quodsi <strong>nimium saepe</strong> vocari quereris votis, ignosce, precor:<br>
But if you protest at <strong>too frequent</strong> a summons from my entreaties, forgive me, I pray:</p>
</blockquote>
<p>Cicero uses <em>nimium</em> joined with <em>saepe</em> often, perhaps too often, since it's rarely found in other others (once in Seneca, the above passage, and I think twice in Ovid, but I haven't translated the passages in full to see if they belong together or if <em>nimium</em> goes with another word).</p>
<p>You could also easily just use the comparative or superlative forms of <em>saepe</em>, which would get the point across. In searching the Loeb library, I found several translators who have done that, so it's not just my intuition.</p>
<p>While Draconis is right that you don't need to translate "she", you might consider it if you want to single the subject out. For this, you could use <em>illa</em>, "that woman." For this particular sense of <em>illa</em>, see Lewis &amp; Short:</p>
<blockquote>
<p>C. Opp. to hic, <strong>to indicate that object which is the more remote</strong>, either as regards the position of the word denoting it, or as it is conceived of by the writer; v. hic, I. D.‚Äî</p>
</blockquote>
<p>So saying <em>illa valedixit</em> would have the same sense as the English "that woman."</p>
<p>I really only bring this up because the previous line has (and the song is called) "this love," so it provides a nice contrast: <em>hic amor, illa mulier.</em></p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Chrome pushes browser history-based ad targeting (227 pts)]]></title>
            <link>https://www.theregister.com/2023/09/06/google_privacy_popup_chrome/</link>
            <guid>37401909</guid>
            <pubDate>Wed, 06 Sep 2023 06:31:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2023/09/06/google_privacy_popup_chrome/">https://www.theregister.com/2023/09/06/google_privacy_popup_chrome/</a>, See on <a href="https://news.ycombinator.com/item?id=37401909">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Google has been gradually rolling out Chrome's "Enhanced Ad Privacy." That's the technology that, unless switched off, allows websites to target the user with adverts tuned to their online activities and interests based on their browser histories.</p>
<p>A popup announcing this functionality has been appearing for some folks since the July release of Chrome 115, which <a target="_blank" href="https://www.theregister.com/2023/06/27/google_tweaks_topics_api_ahead/">included support</a> for Google's Topics API, which is part of the tech titan's <a target="_blank" href="https://www.theregister.com/2023/05/20/google_privacy_sandbox_july/">Privacy Sandbox project</a>.</p>
<p>It would appear more and more people are now seeing this popup as those not keen on Chrome mining their browsing histories to support Google's advertising profits have been speaking up. We understand a small percentage of Chrome's users are being pulled into the Topics API regime at a time, so you may not have noticed or been offered or alerted to anything. And how the Chocolate Factory asks you to agree to or accept the ad targeting depends on where you live, or rather, the laws of where you live.</p>

    

<p>Google next year aims to drop support for third-party cookies, which store browser data that ad companies use for tracking and analytics ‚Äì to the frequent detriment of user privacy. The US mega-corp has developed a variety of replacement technologies, such as the <a target="_blank" rel="nofollow" href="https://developer.chrome.com/docs/privacy-sandbox/topics/">Topics API</a> that will allow ad targeting to continue without cookie-based tracking and ‚Äì it's claimed ‚Äì no privacy consequences.</p>

        


        

<p>Topics essentially works like this: rather than using cookies to track people around the web and figure out their interests from the sites they visit and the apps they use, websites can ask Chrome directly, via its Topics JavaScript API, what sort of things the user is interested in, and then display ads based on that. Chrome picks these topics of interest from studying the user's browser history.</p>
<p>So if you visit lots of financial websites, one of your Chrome-selected topics may be "investing." If a site you visit queries the Topics API, it may learn of this interest from Chrome and decide to serve you an advert about bonds or retirement funds. It also means websites can fetch your online interests straight from your browser.</p>

        

<p>Some people presented with the notification of the new regime complain it's a dark pattern ‚Äì&nbsp;a term Googlers consider unfairly provocative ‚Äì as Chrome users may think they're accepting or enabling "enhanced" privacy from ads when in actual fact the Topics API is already enabled, and will remain enabled, and has to be disabled in the browser's settings. That is to say: the popup is a notice that you've been opted in with a little link to your settings to disable the tech if you so wish.</p>
<div><p><a href="https://regmedia.co.uk/2023/09/06/screenshot_chrome_ad_privacy.jpg" target="_blank"><img src="https://regmedia.co.uk/2023/09/06/screenshot_chrome_ad_privacy.jpg?x=648&amp;y=698&amp;infer_y=1" alt="Screenshot of Chrome's ad privacy popup" title="Screenshot of Chrome's ad privacy popup" height="698" width="648"></a></p><p>Screenshot of a 'Got It' variant of Chrome's 'enhanced' ad privacy popup ... Click to enlarge</p>
</div>
<p>Will Dormann, a software vulnerability analyst with the Carnegie Mellon Software Engineering Institute's CERT Coordination Center, <a target="_blank" rel="nofollow" href="https://twitter.com/wdormann/status/1695843146842664997">noted</a> last week that Google's popup provides a default "Got It" button that dismisses the popup pane and does "the exact opposite of what the title text describes" ‚Äì it leaves Chrome's ad targeting based on browsing history active.</p>
<p>It's worth noting that this popup does explicitly say, "you can make changes in Chrome settings," and that you can switch off the Topics API support using those linked controls. It otherwise doesn't change the status quo. Where third-party cookies were previously used to deliver targeted ads, Chrome users also had to take steps to disable them.</p>
<p>Nonetheless, there's more push back now against the norms preferred by Google and other ad industry firms.</p>
<p>Matthew Green, a cryptography professor at Johns Hopkins University in the US, just encountered the popup and <a target="_blank" rel="nofollow" href="https://twitter.com/matthew_d_green/status/1698813531062185996">expressed his dismay</a>.</p>
<blockquote>

<p>I definitely don‚Äôt want my browser sharing any function of my browsing history with every random website I visit</p>
</blockquote>
<p>"I don‚Äôt want my browser keeping track of my browsing history to help serve me ads, and I definitely don‚Äôt want my browser sharing any function of my browsing history with every random website I visit," he said <a target="_blank" rel="nofollow" href="https://twitter.com/matthew_d_green/status/1699020653196661144?s=20">via Twitter</a>.</p>
<p>And VC Paul Graham has derided ad targeting tech <a target="_blank" rel="nofollow" href="https://twitter.com/paulg/status/1699021936573940154?s=20">as spyware</a>.</p>

        

<p>Google has offered repeated reassurances that its <a target="_blank" rel="nofollow" href="https://github.com/patcg-individual-drafts/topics">Topics API</a> does not allow companies to identify those whose interests inform its ad API. But some developers <a target="_blank" rel="nofollow" href="https://github.com/patcg-individual-drafts/topics/issues/74">claim</a> Topics may be useful for browser fingerprinting and both <a target="_blank" rel="nofollow" href="https://github.com/WebKit/standards-positions/issues/111#issuecomment-1359609317">Apple</a> and <a target="_blank" rel="nofollow" href="https://github.com/mozilla/standards-positions/issues/622">Mozilla</a> have said they won't adopt Topics due to privacy concerns.</p>
<p>Google's popup appears to have regional variations that make the call to action and the button labels clearer and more consistent. One version that's been <a target="_blank" rel="nofollow" href="https://twitter.com/supersat/status/1698826450567323754">reported</a> is titled "Turn on an ad privacy feature" and there's a button that says, "Turn it on."</p>
<ul>

<li><a href="https://www.theregister.com/2023/05/20/google_privacy_sandbox_july/">Privacy Sandbox, Google's answer to third-party cookies, promised within months</a></li>

<li><a href="https://www.theregister.com/2023/02/01/google_cookie_sandbox/">Google ready to kick the cookie habit by Q3 2024, for real this time</a></li>

<li><a href="https://www.theregister.com/2023/06/27/google_tweaks_topics_api_ahead/">Google asks websites to kindly not break its shiny new targeted-advertising API</a></li>

<li><a href="https://www.theregister.com/2023/08/11/chrome_extension_developer_pressure/">Maker of Chrome extension with 300,000+ users tells of constant pressure to sell out</a></li>
</ul>
<p>Unlike the highlighted "Got It" button cited by Dormann and its unadorned "Settings" companion that defers any decision until the linked menu is loaded, "Turn it on" in this variant menu is the same color as the "No thanks" alternative and performs the action suggested by the popup title.</p>
<p>This variation reflects different legal regimes. Unlike America, where opt-out is acceptable and opt-in requirements are broadly opposed by marketers, EU data privacy rules are more demanding in the way data choices are presented.</p>
<p>So if you see a pop-up with "Got It," you've probably been opted-in, based on where you are, and you need to turn off the Topics API support in your Chrome settings if you don't like it; and if you have the option to "Turn it on," you're being asked to opt in or out as you're in a region that requires it.</p>
<p>Depending on what Chrome version you're using, and whether you've been selected to start using Topics API, you can switch this functionality off and on by visiting <code>chrome://settings/adPrivacy</code> and/or <code>chrome://settings/privacySandbox</code> ‚Äì cut'n'paste these URLs into your address bar to jump straight to the controls.</p>
<div><p><a href="https://regmedia.co.uk/2023/09/06/screenshot_ad_topics_controls.jpg" target="_blank"><img src="https://regmedia.co.uk/2023/09/06/screenshot_ad_topics_controls.jpg?x=648&amp;y=293&amp;infer_y=1" alt="Screenshot of Google Chrome's Topics API settings" title="Screenshot of Google Chrome's Topics API settings" height="293" width="648"></a></p><p>Screenshot of Google Chrome's Topics API settings, via <code>chrome://settings/adPrivacy</code> though yours may be at <code>chrome://settings/privacySandbox</code> ... Click to enlarge</p>
</div>
<p>"Users in the UK, EEA, and Switzerland who have not already opted out of the Chrome trials will be presented with an invitation to participate in Topics, and manage their participation in Measurement and Protected Audience (formerly FLEDGE)," Google explained to <em>The Register</em>.</p>
<p>"All users will have robust controls, and can make individual choices, per API, at any point. Chrome will continue to evolve the user controls carefully and in consultation with regulators, and will have more to share once they've evaluated this initial rollout to a small percentage of users. All users will have robust controls, and can opt out of eligibility for the trials at any point." ¬Æ</p>
<div>
<h3>Droidnote</h3>

<p>Meanwhile, Android 14, which is set to be released later this month, is separating <a target="_blank" rel="nofollow" href="https://wiki.mozilla.org/CA">CA certificates</a> from the operating system image so they can be updated remotely without an OS update.</p>

<p>As noted by Tim Perry, creator of the open source HTTP Toolkit, in a <a target="_blank" rel="nofollow" href="https://httptoolkit.com/blog/android-14-breaks-system-certificate-installation/">blog post</a>, while this is a worthwhile defense against untrustworthy Certificate Authorities, its design will make life more difficult for developers and security researchers.</p>

<p>"Unfortunately though, despite those sensible goals, the reality of the implementation has serious consequences: system CA certificates are no longer loaded from /system, and when using root access to either directly modify or mount over the new location on disk, all changes are ignored by all apps on the device," wrote Perry. "Uh oh."</p>

<p><em>The Register</em> asked Perry to elaborate and he explained that this doesn't mean much for alternative Android distributions like LineageOS and GrapheneOS because they can disable this feature if necessary.</p>

<p>"This will most seriously affect security &amp; privacy researchers and reverse engineers, who all need to be able to inspect traffic from third-party apps to fully understand the apps' behavior," he said. "[It] will also cause daily practical problems for the many Android developers &amp; testers who use HTTP debugging tools like HTTP Toolkit and others with their own applications. In the development case, it adds significant friction, but it's possible to work around this for your own single app with more complex setup work."</p>

<p>Perry said the change will be a huge problem for security researchers who will have to rely on alternative versions of Android that don't have this change and which may not behave in the same way. And many apps won't run in these alternative Android builds due to protections like Google's <a target="_blank" rel="nofollow" href="https://developer.android.com/google/play/integrity">Play Integrity API</a>.</p>

<p>Perry said that mobile devices have become increasingly locked down, and even on Linux, restrictions to tools like Flatpak and Snap are moving toward the sandbox model inspired by phones.</p>

<p>"The underlying reasons for locking down like this aren't bad ‚Äì both desktop computers and mobile phones are huge targets for attackers, and this restriction and others like it will help to protect day to day users from serious risks," he said. "The issue though is that the needs of security and privacy researchers and developers are completely ignored. While it's important to protect devices by default, there need to be practical and officially supported mechanisms for advanced users who know what they're doing to override these protections."</p>
</div>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It‚Äôs Official: Cars Are the Worst Category We Have Ever Reviewed for Privacy (112 pts)]]></title>
            <link>https://foundation.mozilla.org/en/privacynotincluded/articles/its-official-cars-are-the-worst-product-category-we-have-ever-reviewed-for-privacy/</link>
            <guid>37401563</guid>
            <pubDate>Wed, 06 Sep 2023 05:16:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://foundation.mozilla.org/en/privacynotincluded/articles/its-official-cars-are-the-worst-product-category-we-have-ever-reviewed-for-privacy/">https://foundation.mozilla.org/en/privacynotincluded/articles/its-official-cars-are-the-worst-product-category-we-have-ever-reviewed-for-privacy/</a>, See on <a href="https://news.ycombinator.com/item?id=37401563">Hacker News</a></p>
Couldn't get https://foundation.mozilla.org/en/privacynotincluded/articles/its-official-cars-are-the-worst-product-category-we-have-ever-reviewed-for-privacy/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Molly Holzschlag has died (240 pts)]]></title>
            <link>https://www.tucsonsentinel.com/local/report/090523_molly_holzschlag/tucsons-molly-holzschlag-known-as-the-fairy-godmother-web-dead-60/</link>
            <guid>37401348</guid>
            <pubDate>Wed, 06 Sep 2023 04:29:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tucsonsentinel.com/local/report/090523_molly_holzschlag/tucsons-molly-holzschlag-known-as-the-fairy-godmother-web-dead-60/">https://www.tucsonsentinel.com/local/report/090523_molly_holzschlag/tucsons-molly-holzschlag-known-as-the-fairy-godmother-web-dead-60/</a>, See on <a href="https://news.ycombinator.com/item?id=37401348">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body-text">
<p>Molly Holzschlag, whose pioneering work in online design standards led to her being dubbed "the fairy godmother of the web," has died at age 60.</p><p>Holzschlag, a longtime Tucson resident, dealt with a series of illnesses over the past decade, including being diagnosed with aplastic anemia. She was found dead Tuesday at her home, family said.</p><p>She was a prolific author and regular speaker about the "open web," advocating for accessible and inclusive online design standards. Also known as "mollydotcom" after her eponymous site that was one of the first blogs, she wrote or co-wrote more than 30 books, and before falling ill she was frequently appearing on Internet conference stages around the world.</p><p>"Molly has changed the world several times over," said the organizers of a 2013 GoFundMe effort that raised more than $70,000 to support Holzschlag while she underwent chemotherapy. </p><p>Holzschlag, who reported on music for the Tucson Weekly in the 1990s, founded Open Web Camp, a Silicon Valley event that ran from 2009-2013, and was a leader of the Web Standards Project in the years before that. That group successfully pushed browser developers, including Microsoft, Opera and Netscape, to adopt web standards. More than once, she challenged Bill Gates face-to-face to fix problems with Internet Explorer.</p><p>She was an "invited expert" on the CSS Working Group of the World Wide Web Consortium, the body that determines the standards that run the Internet. She also served on the W3C HTML and GEO working groups.</p><p>She was steadfast in her insistence that the World Wide Web be usable by people who are differently abled, including sites being able to be parsed by screenreader technology for people with impaired vision.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[On the 10th anniversary of the Snowden revelations (242 pts)]]></title>
            <link>https://www.electrospaces.net/2023/06/on-10th-anniversary-of-snowden.html</link>
            <guid>37400526</guid>
            <pubDate>Wed, 06 Sep 2023 01:57:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.electrospaces.net/2023/06/on-10th-anniversary-of-snowden.html">https://www.electrospaces.net/2023/06/on-10th-anniversary-of-snowden.html</a>, See on <a href="https://news.ycombinator.com/item?id=37400526">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-4344820364338635502" itemprop="description articleBody">
<p><span size="2" color="gray">(Updated: July 4, 2023)</span></p>
<p>
To mark the 10-year anniversary of the start of the Snowden revelations I will look back at some of the most notable disclosures and how they developed, based upon the most recent books and the numerous blog posts I have written here. Still, it should be noted that this overview is not a complete coverage of this wide-ranging topic.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyhZ8oMTZBwed-5n7dMj4FYGxNBxrR8NfDPbzx9Te6eeni5ePQDuNKSVPKfGm1TYMJTToHO19ZFzFVvLPJrgJ_yXPPbT06cJrAE7Nr2F5B_NoWy7aHGgdxlD6xqDPf4yy19JIbBk2TfBVZPPoiojdk039bRHMtNCmkzUuznUDeE24c_tEwqEhFaFH7/s800/10yearsnowden-header.jpg" target="_blank"><img alt="" width="620" data-original-height="420" data-original-width="800" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyhZ8oMTZBwed-5n7dMj4FYGxNBxrR8NfDPbzx9Te6eeni5ePQDuNKSVPKfGm1TYMJTToHO19ZFzFVvLPJrgJ_yXPPbT06cJrAE7Nr2F5B_NoWy7aHGgdxlD6xqDPf4yy19JIbBk2TfBVZPPoiojdk039bRHMtNCmkzUuznUDeE24c_tEwqEhFaFH7/s600/10yearsnowden-header.jpg"></a></p>








<p><span size="+2"><b>Books and archives</b></span></p><p>

Between June 2013 and May 2019, the Snowden revelations resulted in over 200 press reports and more than 1200 classified documents published in full or in part. Additionally, The Intercept published 2148 editions of the NSA's internal newsletter <a href="https://theintercept.com/snowden-sidtoday/" target="_blank">SIDtoday</a>. In total, that may be well over 5000 pages.</p><p>

A collection that allows a useful visual recognition of the documents was found on the private website <a href="https://web.archive.org/web/20230220194734/https://nsa.gov1.info/dni/2020/index.html" target="_blank">IC Off the Record</a>, while text searches are possible at the <a href="https://grid.glendon.yorku.ca/exhibits/show/welcome-to-the-snowden-digital" target="_blank">Snowden Archive</a> which is a collaboration between Canadian Journalists for Free Expression (CJFE) and the University of Toronto. A private collection of the documents is also available at <a href="https://github.com/iamcryptoki/snowden-archive" target="_blank">GitHub</a>.</p><p>

There are also at least 12 <a href="https://www.electrospaces.net/p/books.html#snowden">books about the Snowden revelations</a>. Glenn Greenwald's <i>No Place To Hide</i> from 2014 reads like a pamphlet against perceived mass surveillance. A much more factual overview can be found in <i>Der NSA Komplex</i>, which is also published in 2014 and written by two journalists from Der Spiegel, but unfortunately only available in German.</p><p>

Detailed insights into the political and legal background of the NSA's collection programs are provided in Timothy Edgar's <i>Beyond Snowden</i> from 2017, which is in contrast to Snowden's own memoir <i>Permanent Record</i> from 2019, which leaves more questions than answers.</p><p>

Finally, there's also the long-awaited book <i>Dark Mirror</i> by Washington Post journalist Barton Gellman, which was published in 2020 and offers some important new angles to the initial stories told by Snowden and Greenwald.</p>
<p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhhtqJPTdbbQy4xiU_ry0G6brToakH7JMt1iDp1_smof7c6BOkXQ17WfJKTCTbT8H9FiCny4qPwqaior9yBUbj0RuI_QiBH0fqdy8cHuKD7IocRbpLR9tpstHvsNxtThbA5kyEhJUJjMeOpqxFQP7OqMLkf2MndGaJFBVEuKdRcCUisXuIJEe47c0Eb=s800" imageanchor="1" target="_blank"><img src="https://blogger.googleusercontent.com/img/a/AVvXsEhhtqJPTdbbQy4xiU_ry0G6brToakH7JMt1iDp1_smof7c6BOkXQ17WfJKTCTbT8H9FiCny4qPwqaior9yBUbj0RuI_QiBH0fqdy8cHuKD7IocRbpLR9tpstHvsNxtThbA5kyEhJUJjMeOpqxFQP7OqMLkf2MndGaJFBVEuKdRcCUisXuIJEe47c0Eb=s800" width="500"></a></p>


<p><span size="+2"><b>Incentives</b></span></p><p>

Some people assume that Snowden is a spy who worked for Russian intelligence, but nowadays, requests for information come from transparency activists as well. Wikileaks' wiki-page titled <a href="https://web.archive.org/web/20090619014040/https://wikileaks.org/wiki/Draft:The_Most_Wanted_Leaks_of_2009" target="_blank">The Most Wanted Leaks of 2009</a> may have inspired Manning to search for information on SIPRNet and to download hundreds of thousands of military and diplomatic reports.</p><p>

Likewise, the incentive for Snowden may have come from the news program <a href="https://en.wikipedia.org/wiki/Democracy_Now!" target="_blank">Democracy Now!</a>, in which on April 20, 2012, former NSA crypto-mathematician Bill Binney, documentary filmmaker Laura Poitras and hacktivist Jacob Appelbaum were interviewed by Amy Goodman (a full transcript can be found <a href="https://www.bibliotecapleyades.net/sociopolitica/sociopol_nsa05.htm" target="_blank">here</a>).</p><p>

In the program, Binney claimed that after 9/11 "all the wraps came off for NSA, and they decided to eliminate the protections on U.S. citizens and collect on domestically". </p><p>

Appelbaum <a href="https://www.democracynow.org/2012/4/20/we_do_not_live_in_a?autostart=true" target="_blank">repeated</a> what he said at the <a href="https://en.wikipedia.org/wiki/Hackers_on_Planet_Earth" target="_blank">HOPE</a> conference in 2010: "I feel that people like Bill need to come forward to talk about what the U.S. government is doing, so that we can make informed choices as a democracy" - which is exactly what Snowden would do: leaking documents because "<i>the public</i> needs to <a href="https://youtu.be/0hLjuVyIIrs?t=300" target="_blank">decide</a> whether these programs and policies are right or wrong."</p><p>

Later that day, Binney and Appelbaum spoke at a "Surveillance Teach-In" in the Whitney Museum, where Appelbaum emphasized that disclosing secret information is also important for privacy and civil liberties organizations: because of a lack of hard evidence and concrete harm it was almost impossible for them to fight NSA surveillance in court.</p><div>
<iframe width="500" height="300" src="https://www.youtube.com/embed/s976iyaO39A" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
  <p><span size="2">
    Binney and Appelbaum at the Surveillance Teach-In on April 20, 2012<br>
  </span>
</p></div>
<p>

Just a month earlier, Snowden had started a <a href="https://www.electrospaces.net/2019/12/review-of-snowdens-book-permanent.html#sysadmin">new job</a> as a SharePoint systems administrator at the NSA's <a href="https://www.electrospaces.net/2019/06/the-nsas-regional-cryptologic-centers.html">regional cryptologic center</a> in the Kunia Tunnel complex in Hawaii. There, he began automating his tasks to free up time for something more interesting, which he describes in <i>Permanent Record</i>:</p><p>
"I want to emphasize this: my active searching out of NSA abuses began not with the copying of documents, but with the reading of them. My initial intention was just to confirm the suspicions that I'd first had back in 2009 in Tokyo. Three years later I was determined to find out if an American system of mass surveillance existed and, if it did, how it functioned." <a nohref="" title="Edward Snowden, Permanent Record, p. 214-215">*</a><br>
</p>
<p>
With this, Snowden basically admits that he isn't a whistleblower: he wasn't confronted with illegal activities or significant abuses and subsequently secured evidence of that, but acted the other way around, by first gathering as much information he could get and then look whether there was something incriminating in it.</p><p>

In his memoir, Snowden doesn't come up with concrete misconducts or other things that could have triggered his decision to hand the files over to journalists. He even omits almost all the disclosures made by the press, which makes that<i> Permanent Record</i> contains hardly anything that justifies his unprecedented data theft.</p><p><a href="https://1.bp.blogspot.com/-WJHs31cb8nw/Xcu8fWiZdJI/AAAAAAAAENY/DVGBGU0tU94MYT-y5TJtdWekkmJmbgc9wCLcBGAsYHQ/s1600/nsa-kunia-pict86.jpg" imageanchor="1" target="_blank"><img src="https://1.bp.blogspot.com/-WJHs31cb8nw/Xcu8fWiZdJI/AAAAAAAAENY/DVGBGU0tU94MYT-y5TJtdWekkmJmbgc9wCLcBGAsYHQ/s1600/nsa-kunia-pict86.jpg" width="500"></a><br>
<span size="2">
The tunnel entrance to the former Kunia Regional Security Operations Center<br>
in Hawaii, where Snowden worked from March 2012 to March 2013<br>
<span color="gray">(photo: NSA - click to enlarge)</span><br>
</span>
</p>

<p><span size="+2"><b>The documents</b></span></p><p>

The actual number of documents which Snowden eventually exfiltrated from the NSA has never been clarified. According to the 2016 <a href="https://fas.org/irp/congress/2016_rpt/hpsci-snowden.pdf" target="_blank">report</a> from the US House Intelligence Committee, Snowden removed more than 1.5 million documents from NSANet and the JWICS intelligence network.</p><p>

Glenn Greenwald repeatedly <a href="https://theintercept.com/2014/05/08/keith-alexander-unplugged-bushobama-matters/" target="_blank">said</a> that number was "pure fabrication" and he could probably agree with former NSA director Keith Alexander who in November 2013 estimated that Snowden had <a href="https://technical.ly/baltimore/2013/11/01/surveillance-necessary-hornets-nest-nsa-director-keith-alexander/" target="_blank">exposed</a> only between 50,000 and 200,000 documents.<a href="https://rumble.com/v2sgyx2-snowden-revelations-10-year-anniversary-glenn-greenwald-speaks-with-snowden.html" target="_blank" title="As of 10:55 Greenwald speaks about hundreds of thousands of documents">*</a></p><p>

According to Barton Gellman, Snowden provided him and Laura Poitras with an encrypted archive of documents called "Pandora" on May 21, 2013. This archive was 8 gigabytes and contained over 50,000 separate documents, all neatly organized in folders.<a nohref="" title="Barton Gellman, Dark Mirror, p. 22-27">*</a></p><p>

Poitras gave Greenwald a copy of the Pandora archive just before they boarded their flight to Hong Kong on June 1. There, Snowden <a href="https://archive.is/Evmoi" target="_blank">gave</a> Ewen MacAskill from The Guardian some 50,000 documents about GCHQ and handed over all the remaining files to Greenwald and Poitras, who are the <a href="https://www.rollingstone.com/culture/culture-news/snowden-and-greenwald-the-men-who-leaked-the-secrets-104970/" target="_blank">only ones</a> with a complete set. Other media outlets only got partial sets of documents.</p><p>

Greenwald's cache eventually ended up at <a href="https://en.wikipedia.org/wiki/The_Intercept" target="_blank">The Intercept</a>, the online news outlet he co-founded with Jeremy Scahill and Laura Poitras in 2014 to report about the Snowden documents. In March 2019, however, The Intercept closed its Snowden archive and <a href="https://mmm.verdi.de/beruf/snowden-und-die-grosse-datenmisshandlung-89797" target="_blank">reportedly</a> destroyed it. </p>
<p><a href="https://2.bp.blogspot.com/-2Mtv0SIBJdk/XJw8vqzmvJI/AAAAAAAAD_k/D-V9GxJYjxUuQf9L-21ecbcgCdh7INBjACLcBGAs/s1600/snowdenfiles-treucrypt.png" imageanchor="1" target="_blank"><img src="https://2.bp.blogspot.com/-2Mtv0SIBJdk/XJw8vqzmvJI/AAAAAAAAD_k/D-V9GxJYjxUuQf9L-21ecbcgCdh7INBjACLcBGAs/s1600/snowdenfiles-treucrypt.png" width="550"></a><br>
<span size="2">
Screenshot from a Brazilian television report, showing some of the Snowden files<br>
opened in a TrueCrypt window on the laptop of Glenn Greenwald.<br>
<span color="gray">(screenshot by koenrh - click to enlarge)</span>
</span>
</p>


<p><span size="+2"><b>Non-Snowden leaks</b></span></p><p>

In a message to Gellman, Snowden said that "he was not resigned to life in prison or worse. He wanted to show other whistleblowers that there could be a happy ending".<a nohref="" title="Barton Gellman, Dark Mirror, p. 129">*</a> Later, whistleblower attorney Jesselyn Radack <a href="https://abcnews.go.com/blogs/headlines/2013/10/more-nsa-leakers-followed-snowdens-footsteps-whistleblower-lawyer-says" target="_blank">hoped</a> that "courage is contagious, and we see more and more people from the NSA coming through our door after Snowden made these revelations."</p><p>

Indeed, other sources started to leak documents to the press. The first one was a so-called tasking record showing that the NSA had targeted the non-secure cell phone of German chancellor Angela Merkel. This was <a href="https://www.spiegel.de/politik/deutschland/nsa-merkel-beschwert-sich-bei-obama-a-929636.html" target="_blank">revealed</a> by Der Spiegel on October 23, 2013, which is less than five months after the start of Snowden's revelations. </p>
<p>
The second leaked document that wasn't attributed to Snowden was just as spectacular: the <a href="https://nsa.gov1.info/dni/nsa-ant-catalog/index.html" target="_blank">ANT product catalog</a> with a range of sophisticated spying gadgets from the NSA's hacking division TAO. This catalog was also published by Der Spiegel and <a href="https://www.youtube.com/watch?v=dy3-QZLTpbQ" target="_blank">discussed</a> by Jacob Appelbaum during the <a href="https://en.wikipedia.org/wiki/Chaos_Communication_Congress" target="_blank">CCC</a> on December 30, 2013.</p><p> 

Initially, hardly anyone noticed that these documents didn't come from Snowden, and so a mysterious "second source" was able to publish files that were sometimes even more embarrassing and damaging than those from the Snowden trove, like intercepted conversations from foreign government leaders.</p><p>

Later, other piggybackers who called themselves <a href="https://en.wikipedia.org/wiki/The_Shadow_Brokers" target="_blank">The Shadow Brokers</a> leaked highly sensitive information about NSA hacking tools. The sources of these leaks have never been identified, although it's often <a href="https://www.schneier.com/blog/archives/2023/06/snowden-ten-years-later.html" target="_blank">assume</a>d that Russian intelligence was behind it. Snowden never addressed these other leaks, nor distanced himself from them.</p>
<p><a href="https://2.bp.blogspot.com/-NE70qy84JJQ/VYoeWkx6HSI/AAAAAAAACiY/uG6vAMItssY/s1600/wikileaks-france-nsa-comint-gamma.jpg" imageanchor="1" target="_blank"><img src="https://2.bp.blogspot.com/-NE70qy84JJQ/VYoeWkx6HSI/AAAAAAAACiY/uG6vAMItssY/s1600/wikileaks-france-nsa-comint-gamma.jpg" title="NSA intelligence report about an intercepted conversation between Fran√ßois Hollande and Jean-Marc Ayrault" width="500"></a><br>
<span size="2">
NSA report about an intercepted conversation of French president Hollande.<br>
Leaked by an unknown source and published by Wikileaks in 2015<br>
<span color="gray">(click to enlarge)</span><br>
</span></p>


<p><span size="+2"><b>The Section 215 program</b></span></p><p>

The very first disclosure of a document that did come from Snowden was the <a href="https://www.theguardian.com/world/interactive/2013/jun/06/verizon-telephone-data-court-order" target="_blank">Verizon order</a> of the Foreign Intelligence Surveillance Court (FISC). This court convenes behind closed doors and is often, but <a href="https://www.emptywheel.net/2017/06/28/confirmed-the-fisa-court-is-less-of-a-rubber-stamp-than-title-iii-courts/" target="_blank">injustly</a> referred to as a "rubber stamp". The order was <a href="https://www.theguardian.com/world/2013/jun/06/nsa-phone-records-verizon-court-order" target="_blank">published</a> by The Guardian on June 6, 2013.</p><p>

The Verizon order showed that the NSA was collecting domestic telephone metadata under the so-called <a href="https://www.electrospaces.net/2015/09/nsas-legal-authorities.html#215">Section 215</a> program. In the US, this became the most controversial issue and initially it seemed to confirm cryptic public warnings by US senators Ron Wyden and Mark Udall, as well as the aforementioned claims by Bill Binney about domestic mass surveillance.</p><p>

In reaction, Director of National Intelligence (DNI) James Clapper started an unprecedented declassification effort and released numerous FISC and NSA documents about the Section 215 program on a newly created Tumblr site called <a href="https://icontherecord.tumblr.com/" target="_blank">IC On the Record</a>. </p><p>

This was meant to clarify a central misunderstanding: the fact that the NSA collects data inside the US doesn't mean they are spying on Americans. The NSA is still focused on foreign targets, but because they are using American internet services, it proved to be fruitful to intercept their data not only abroad, but at telecoms and internet companies inside the US as well (the "home field advantage").</p><p>

Accordingly, the purpose of the Section 215 program was to find out whether foreign terrorists were in contact with unknown conspirators inside the US, which was one of the failures that could have prevented the attacks of 9/11.</p><p>

Therefore, the only thing the domestic telephone records were used for was simple contact chaining: NSA started with a phone number of a foreign terrorist and then the MAINWAY system presented the (foreign and domestic) phone numbers with which that initial number had been in contact with, as well as the numbers they, in their turn had been in contact with, the so-called "second hop":</p><p><img src="https://1.bp.blogspot.com/-FK7jUtlV9IM/VrrfHYPnSjI/AAAAAAAADCg/ji4VHF0HNVw/s1600/contact-chaining-federated2.jpg" width="500" title="Federated contact chaining queries including domestic and foreign phone call records"></p>

<p>

In 2012, the NSA used 288 phone numbers as a "seed" for such a contact chaining query, <a href="https://www.npr.org/templates/transcript/transcript.php?storyId=261079074?storyId=261079074" target="_blank">resulting</a> in 6000 phone numbers that analysts actually looked at. When this led to a suspicious American phone number, the NSA passed it on to the FBI for further investigation.</p><p>

This true purpose of the domestic metadata collection was clearly laid out in a <a href="https://documents.pclob.gov/prod/Documents/OversightReport/cf0ce183-7935-4b06-bb41-007d1f437412/215-Report_on_the_Telephone_Records_Program%20-%20Completed%20508%20-%2011292022.pdf" target="_blank">public report</a> which the independent Privacy and Civil Liberties Oversight Board (PCLOB) published in January 2014. The PCLOB found "no instance in which the program directly contributed to the discovery of a previously unknown terrorist plot", but the program was of some value as it offered additional leads and could show that foreign terrorist plots had <i>no</i> US nexus.</p><p> 

Although these domestic telephone records were not used to spy on Americans, and the FISC limited their retention to 5 years and prohibited the collection of location data, many people would not like to have them in an NSA database because of what Binney and Snowden called the possibility of a "turnkey tyranny".<a nohref="" title="Barton Gellman, Dark Mirror, p. 143">*</a></p><p>

The publication of the Verizon order did not only make the general public aware of the Section 215 program, but also gave civil liberty organizations standing in court, which fulfilled Jacob Appelbaum's wish from the 2012 Surveillance Teach-In.</p><p>

Meanwhile there have been two cases in which a Circuit Court of Appeals ruled about the Section 215 program. They both found that the bulk collection of metadata exceeded the scope of Section 215 of the <a href="https://en.wikipedia.org/wiki/Patriot_Act" target="_blank">Patriot Act</a> (because the actual practice hadn't been foreseen by lawmakers, although they had been briefed about it later). The courts didn't decide on whether the program was constitutional or not.</p>
<p><a href="https://www.theguardian.com/world/interactive/2013/jun/06/verizon-telephone-data-court-order" imageanchor="1" target="_blank"><img src="https://4.bp.blogspot.com/-tHoasJd3Mx8/XjEoogD6LyI/AAAAAAAAEZc/DSNnc_lkC-40lAAs9yCvz0IfrgFJe_YAQCLcBGAsYHQ/s1600/verizon-order.JPG" width="500"></a><br>
<span size="2">
The first page of the Verizon order from April 25, 2013<br>
<span color="gray">(click for the full document)</span><br>
</span>
</p>


<p><span size="+2"><b>The PRISM program</b></span></p><p>

One day after the publication of the Verizon order, The Guardian and The Washington Post revealed the PRISM program, which became 
synonymous for an all encompassing NSA spying system, just like <a href="https://en.wikipedia.org/wiki/ECHELON" target="_blank">ECHELON</a> was before.</p><p>

In his book <i>Dark Mirror</i>, Barton Gellman tells a different story than Greenwald did in <i>No Place to Hide</i>. Greenwald presented himself as the one who was chosen by Snowden to lead the revelations and claimed that he and Laura Poitras were working with Snowden since February 2013, while Gellman only got "some documents" and that Snowden was angry about the fear-driven approach of The Washington Post.<a nohref="" title="Glenn Greenwald, No Place to Hide, p. 54-57">*</a></p><p>

According to Gellman, the opposite was the case: on January 31, 2013, Laura Poitras already asked him for advice and on May 7, they agreed to work together. She introduced Gellman to her source, who still called himself Verax, and they started encrypted chat conversations. On May 20, Snowden sent them the full PRISM presentation, after which they signed a contract with The Washington Post on May 24.<a nohref="" title="Barton Gellman, Dark Mirror, p. 8-11 &amp; 138-139">*</a></p><p>

But Snowden was under severe time pressure and urged Gellman to rapidly publish the full PRISM presentation, which he had signed with a <a href="https://en.wikipedia.org/wiki/Digital_signature" target="_blank">digital signature</a> associated with his Verax alter ego. Only gradually did Gellman realize the implications of this. Snowden's plan was to ask political asylum at a foreign <a href="https://en.wikipedia.org/wiki/Consular_missions_in_Hong_Kong" target="_blank">diplomatic mission</a> in Hong Kong, where he wanted to use the cryptographic signature to identify himself as the source of the PRISM document (and didn't rule out to "provide raw source material to a foreign government").<a nohref="" title="Barton Gellman, Dark Mirror, p. 129">*</a></p><p>

As a journalist, Gellman protected the identity of his source, but publishing the digitally signed PRISM presentation would make him and The Washington Post complicit in Snowden's flight from American law. After consulting Poitras, Gellman decided not to do so. On May 27, Snowden withdrew the exclusive right for the Washington Post and turned to Greenwald, who until that moment didn't know who Snowden was, nor had seen any of the documents.<a nohref="" title="Barton Gellman, Dark Mirror, p. 128-139">*</a></p><p><a href="https://3.bp.blogspot.com/-KVB97E3pnWc/U1SNOMrZqSI/AAAAAAAABj4/hXu1tKvOJm8/s1600/prism-01-combined.jpg" imageanchor="1" target="_blank"><img src="https://3.bp.blogspot.com/-KVB97E3pnWc/U1SNOMrZqSI/AAAAAAAABj4/hXu1tKvOJm8/s1600/prism-01-combined.jpg" width="450"></a></p>
<p>

When Greenwald finally managed to get <a href="https://en.wikipedia.org/wiki/Pretty_Good_Privacy" target="_blank">PGP</a> working, Snowden sent him a zip-file with some 25 documents, including the 41-slide PRISM presentation. Greenwald started writing his own story about PRISM, which was <a href="https://www.theguardian.com/world/2013/jun/06/us-tech-giants-nsa-data" target="_blank">published</a> by The Guardian on June 6, 2013.<a nohref="" title="Glenn Greenwald, No Place To Hide, p. 18-20 &amp; 75-76">*</a> Just an hour earlier, The Washington Post had <a href="http://www.washingtonpost.com/investigations/us-intelligence-mining-data-from-nine-us-internet-companies-in-broad-secret-program/2013/06/06/3a0c0da8-cebf-11e2-8845-d970ccb04497_story.html" target="_blank">released</a> its own PRISM story.</p><p>

The most controversial part of these stories was the claim that "the National Security Agency has obtained direct access to the systems of Google, Facebook, Apple and other US internet giants", which those companies vigorously denied.</p><p>

That "direct access" was taken from one of the slides, but it's unclear why both Gellman and Greenwald stuck to the most simple interpretation of it. Fact is that they had access to the extensive accompanying speaker's notes, which said: "PRISM access is 100% dependent on ISP provisioning".<a nohref="" title="Barton Gellman, Dark Mirror, p. 119 &amp; 124">*</a></p><p>

They also had all the other PRISM slides, including two that were published later on, which clearly show that the FBI is in between the NSA and the internet companies:
</p><p><a href="https://3.bp.blogspot.com/-Ejp2lgqoEok/U3ksoG-pZ1I/AAAAAAAABnA/P2RTtvxQvKA/s1600/prism-14a.jpg" imageanchor="1" target="_blank"><img src="https://3.bp.blogspot.com/-Ejp2lgqoEok/U3ksoG-pZ1I/AAAAAAAABnA/P2RTtvxQvKA/s1600/prism-14a.jpg" width="450"></a></p>
<p><span size="2">
PRISM-slide published by <a href="http://www.lemonde.fr/technologies/article/2013/10/21/france-in-the-nsa-s-crosshair-wanadoo-and-alcatel-targeted_3499739_651865.html" target="_blank">Le Monde</a> on October 22, 2013<br>
  </span>
</p>
<p>

In July 2014, the Privacy and Civil Liberties Oversight Board report (PCLOB) published an extensive <a href="https://documents.pclob.gov/prod/Documents/OversightReport/ba65702c-3541-4125-a67d-92a7f974fc4c/702-Report-2%20-%20Complete%20-%20Nov%2014%202022%201548.pdf" target="_blank">public report</a> about PRISM as well, which confirms that individual selectors (like a target's e-mail address) are sent to internet companies, which are "compelled to give the communications sent to or from that selector to the government." According to the PCLOB, PRISM "has proven valuable in the government‚Äôs efforts to combat terrorism as well as in other areas of foreign intelligence."</p>
<p>
In <i>Dark Mirror</i>, Gellman admits: "In retrospect, I do not love the way I wrote the [PRISM] story. I knew a lot less then than I learned later, with more time in the documents and many more interviews". A well-informed source told him that the systems of a company like Facebook are too complex to let the NSA plug in a cable. Only Facebook knows how to pull things out, which they can hand over upon a valid request.<a nohref="" title="Barton Gellman, Dark Mirror, p. 124 &amp; 148">*</a> Google <a href="https://eu.usatoday.com/story/money/business/2013/06/12/google-nsa-servers-secure-ftp/2416181/" target="_blank">did</a> that through secure <a href="https://en.wikipedia.org/wiki/File_Transfer_Protocol" target="_blank">FTP</a> transfers and in person.</p><p>

Another interesting addition provided by Gellman is about the date of the PRISM presentation, April 2013, which is less than one and a half months before Snowden left the NSA:</p><p>
  "Nothing Snowden had seen until now better suited his plan. He had been talking to Poitras for three months, but he still did not feel confident that his disclosures would seize attention from a public that had seldom responded strongly to privacy warnings. Most of the NSA programs that worried him were legally and technically intricate, not easy to explain. He needed examples that ordinary people would recognize. Along came [the PRISM] presentation, festooned at the top of every slide with iconic logos from the best-known Internet companies in the world. "PRISM hits close to people's hearts", he told me."<a nohref="" title="Barton Gellman, Dark Mirror, p. 120">*</a><br>
  </p>


<p><span size="+2"><b>Overcollection</b></span></p><p>

While PRISM is no mass surveillance, but targeted collection against individual foreign targets, it still has a problematic aspect: overcollection. Snowden was eager to draw public attention to this issue and, according to Greenwald, took his last job at NSA Hawaii only in order to get access to the NSA's raw data repositories.<a nohref="" title="Glenn Greenwald, No Place To Hide, p. 48">*</a> Snowden declined to repeat or explain that to Gellman though.<a nohref="" title="Barton Gellman, Dark Mirror, p. 84">*</a></p><p>

He succeeded and was able to exfiltrate a cache of ca. 22,000 collection reports, <a href="https://www.washingtonpost.com/world/national-security/your-questions-answered-about-the-posts-recent-investigation-of-nsa-surveillance/2014/07/11/43d743e6-0908-11e4-8a6a-19355c7e870a_story.html?utm_term=.e6244eb277df" target="_blank">containing</a> 160,000 individual conversations (75% of which instant messages), which the NSA collected via the PRISM program between 2009 and 2012.<a nohref="" title="Barton Gellman, Dark Mirror, p. 340">*</a></p><p>

Snowden handed them over to Barton Gellman who <a href="https://www.washingtonpost.com/world/national-security/in-nsa-intercepted-data-those-not-targeted-far-outnumber-the-foreigners-who-are/2014/07/05/8139adf8-045a-11e4-8572-4b1b969b6322_story.html?utm_term=.676dfdc9ca3a" target="_blank">reported</a> about these files in July 2014. Researchers at The Washington Post found that the intercepted communications contained valuable foreign intelligence information, but also that over 9 out of 10 accountholders were not the intended surveillance targets and that nearly half of the files contained US person identifiers.</p><p>

It's probably technically impossible to prevent such overcollection, but instead of deleting irrelevant personal content, the NSA only "minimizes" it, which means that names of Americans are redacted before they are distributed. Gellman saw that NSA personnel takes these procedures seriously, but when he confronted former NSA deputy director Rick Ledgett with his unease, Ledgett's only reply was that the NSA really doesn't care about ordinary people.<a nohref="" title="Barton Gellman, Dark Mirror, p. 341-345">*</a></p>


<p><span size="+2"><b>The Mission List</b></span></p><p>

Ledgett's answer is confirmed by a comprehensive listing of the tasks of the NSA in the <a href="http://cryptome.org/2014/09/nsa-strategic-mission-list.pdf" target="_blank">Strategic Mission List</a> from January 2007. It was published by The New York Times in November 2013, but got hardly any attention, despite the fact that it clearly contradicts the claims by Snowden and Greenwald that the NSA has just one single goal: collect all digital communications from all over the world.</p><p>

Equally less traction <a href="https://archive.is/KnUS7" target="_blank">gained</a> reports by Ewen MacAskill from The Guardian and Scott Shane from The New York Times, who tried to provide balance and nuance by showing that NSA and GCHQ also did many good things, like monitoring terrorists, the Taliban, hostage takers, human traffickers, and drug cartels.</p><p>

The Mission List says that China, North-Korea, Iraq, Iran, Russia and Venezuela were "Enduring Targets", which means they are of long-term strategic importance and therefore require a holistic approach. Next there were 16 "Topical Missions", which are subject to some change, but can be considered legitimate targets for any large intelligence agency:</p><p>
- Winning the Global War on Terrorism (GWOT)<br>
- Protecting the US homeland<br>
- Combating proliferation of Weapons of Mass Destruction (WMD)<br>
- Protecting US military forces deployed overseas<br>
- Providing warning of impending state instability<br>
- Providing warning of a strategic nuclear missile attack<br>
- Monitoring regional tensions that could escalate<br>
- Preventing an attack on US critical information systems<br>
- Early detection of critical foreign military developments<br>
- Preventing technological surprise<br>
- Ensuring diplomatic advantage for the US<br>
- Ensuring a steady and reliable energy supply for the US<br>
- Countering foreign intelligence threats<br>
- Countering narcotics and transnational criminal networks<br>
- Mapping foreign military and civil communications infrastructure<br>
</p>
<p>
In 2013, terrorism was <a href="https://odnigov.tumblr.com/post/138558113835/dni-clapper-visits-us-naval-academy" target="_blank">replaced</a> by cyber attacks as top threat to American national security. Since then, cyber threats are increasing in frequency, scale, sophistication and severity of impact.</p>

<p><a href="https://1.bp.blogspot.com/-m-T9d8mqNMA/UpJoG-qhJAI/AAAAAAAABBY/qtZemadSakk/s1600/boundless-worldwide-aggregate.jpg" imageanchor="1" target="_blank"><img src="https://1.bp.blogspot.com/-m-T9d8mqNMA/UpJoG-qhJAI/AAAAAAAABBY/qtZemadSakk/s1600/boundless-worldwide-aggregate.jpg" width="550" title="Screenshot of BOUNDLESSINFORMANT"></a><br>
<span size="2">
Screenshot of the BOUNDLESSINFORMANT tool showing where the NSA collected most data<br>
</span>
</p>

<p><span size="+2"><b>Spying among friends</b></span></p><p>

For its mission of "Ensuring Diplomatic Advantage for the U.S.", the NSA intercepts the communications of numerous foreign governments and government leaders. Based upon documents from the Snowden trove, media reported about eavesdropping operations against the Mexican candidate for the presidency, Enrique Pe√±a Nieto, Brazilian president Dilma Rousseff, the <a href="https://www.electrospaces.net/2015/12/how-nsa-targeted-venezuelan-oil-company.html" target="_blank">Venezuelan oil company PdVSA</a> and many others.</p>
<p>
The NSA's interest in Germany's chancellor Angela Merkel had the most far-reaching consequences. Merkel herself made clear to president Obama that "spying on friends is not acceptable" (<i>Aussp√§hen unter Freunden, das geht gar nicht</i>) and the German parliament started an official investigation into the spying activities of the NSA (<a href="https://de.wikipedia.org/wiki/1._Untersuchungsausschuss_der_18._Wahlperiode_des_Deutschen_Bundestages" target="_blank"><i>NSA-Untersuchungsausschuss</i></a> or <a href="https://twitter.com/hashtag/NSAUA?src=hashtag_click" target="_blank">#NSAUA</a>). This inquiry lasted from March 2014 to June 2017, but soon shifted its focus to Germany's own foreign intelligence agency BND.</p><p>

Extensive hearings of BND employees resulted in unprecedented insights into the details of the cable tapping and satellite interception operations which the BND conducted in cooperation with the NSA. Eventually it became clear that the NSA wasn't spying on German citizens, but did try to collect communications from European governments and companies of interest - just like the BND itself, which was also <a href="http://www.dw.com/en/germany-spies-among-friends-controversy-grows-over-espionage-activities/a-18844401" target="_blank">targeting</a> American and French foreign ministers, the interior departments of EU member states, and many others.</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh67yFyvaSd6MOhzMXsYnQacbFQUvqoTNL9pv-h9DVsc2Keb1WTLEG1E7Avx8Ig3w4C8i5FJL-VCmk4JIuqUooYzYVbKKoVdRwB7VHeOpenv1L5HHTd4iqwScO05Mvq42hPDS66D0Rq9_Y1yClrit3TX2Z5yzChxkeRvbKS0hk6SGJueKo1ZPpdQ9Z0/s800/merkel-cellphone-header.jpg" target="_blank"><img alt="" width="500" data-original-height="420" data-original-width="800" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh67yFyvaSd6MOhzMXsYnQacbFQUvqoTNL9pv-h9DVsc2Keb1WTLEG1E7Avx8Ig3w4C8i5FJL-VCmk4JIuqUooYzYVbKKoVdRwB7VHeOpenv1L5HHTd4iqwScO05Mvq42hPDS66D0Rq9_Y1yClrit3TX2Z5yzChxkeRvbKS0hk6SGJueKo1ZPpdQ9Z0/s600/merkel-cellphone-header.jpg"></a></p>
<p><span size="2">
German chancellor Angela Merkel holding a secure BlackBerry Z10 in 2013<br>
    <span color="gray">(photo: Nicki Demarco/The Fold/The Washington Post)</span><br>
  </span>
</p>


<p><span size="+2"><b>Backdoor tapping Google</b></span></p><p>

A disclosure that caused outrage in Silicon Valley was about MUSCULAR, a collection program in which the NSA cooperates with its British counterpart GCHQ. In October 2013, The Washington Post <a href="https://www.washingtonpost.com/world/national-security/nsa-infiltrates-links-to-yahoo-google-data-centers-worldwide-snowden-documents-say/2013/10/30/e51d661e-4166-11e3-8b74-d89d714ca4dd_story.html" target="_blank">reported</a> that under this program, the NSA had secretly broken into the main communications links between Yahoo and Google data centers around the world.</p><p>

A big question was: why would the NSA do that, given that they already had "front door" access to Google and Yahoo via the PRISM program? Gellman asked Snowden, but his only answer was: "Because it could" and: "I'm speculating, but NSA doesn't ignore low-hanging fruit". Eventually Gellman realized that inside the US, the NSA had to specify individual targets, but abroad it was possible to acquire such data in bulk and to search and analyse it with <a href="https://en.wikipedia.org/wiki/XKeyscore" target="_blank">XKEYSCORE</a>.<a nohref="" title="Barton Gellman, Dark Mirror, p. 285-286">*</a></p><p>

The Post didn't mention the XKEYSCORE system by name and it's also not explained in Gellman's book <i>Dark Mirror</i>. That's unfortunate, because while Greenwald and Snowden presented XKEYSCORE as a global mass surveillance tool, it's actually a smart system to find targets who are communicating anonymously and therefore cannot be traced in the traditional way, via identifiers like phone numbers and e-mail addresses.</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglLBeqt_uSQyOyu2IBstqYg-dq_ODbFpvEab0YTi2EVdQVKw3LU4fohs12nxnMoeIXRmx2YsIV-koifbS1gf5a-4cUivqSDXj8vJtuJVnggBfjCuCVEzpJncsCR3n3jKy1J4_km3zcQe7hDVAPQtvqgyxgm7lUqqOiF_6iIrm1N-AJ1qJBrIHq21pN/s1484/muscular%20google%20cloud.jpg" target="_blank"><img alt="" width="450" data-original-height="1113" data-original-width="1484" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglLBeqt_uSQyOyu2IBstqYg-dq_ODbFpvEab0YTi2EVdQVKw3LU4fohs12nxnMoeIXRmx2YsIV-koifbS1gf5a-4cUivqSDXj8vJtuJVnggBfjCuCVEzpJncsCR3n3jKy1J4_km3zcQe7hDVAPQtvqgyxgm7lUqqOiF_6iIrm1N-AJ1qJBrIHq21pN/s600/muscular%20google%20cloud.jpg"></a></p>
<p><span size="2">
  NSA slide showing where to intercept data from the Google cloud<br>
  </span>
</p>


<p><span size="+2"><b>BOUNDLESSINFORMANT</b></span></p><p>

Where Section 215 was most controversial in the United States, but lesser-known in Europe, the opposite was the case with <a href="https://en.wikipedia.org/wiki/Boundless_Informant" target="_blank">BOUNDLESSINFORMANT</a>, which caused fury in Europe, but is hardly known across the ocean. BOUNDLESSINFORMANT isn't a system to collect data, but an internal visualization tool that counts metadata records to provide insights into the NSA's worldwide data collection.</p><p>

The results are shown in heat maps and charts, for example for countries and collection programs. Such charts for Germany and a few other countries were published on July 29, 2013 by Der Spiegel, but on August 5, the German foreign intelligence agency BND said that they collected these data during military operations abroad and subsequently <a href="https://www.spiegel.de/international/world/german-intelligence-sends-massive-amounts-of-data-to-the-nsa-a-914821.html" target="_blank">shared</a> them with the NSA. </p><p>

Despite this statement, Glenn Greenwald interpreted these charts as evidence of American mass surveillance on European citizens and started publishing them in major European newspapers.</p>
<p><a href="https://1.bp.blogspot.com/-W66ZWknVuLw/VLnJbvaL6QI/AAAAAAAACJ0/PjkJ6dm-4NY/s1600/boundless-germany.jpg" imageanchor="1" target="_blank"><img src="https://1.bp.blogspot.com/-W66ZWknVuLw/VLnJbvaL6QI/AAAAAAAACJ0/PjkJ6dm-4NY/s1600/boundless-germany.jpg" width="550" title="BOUNDLESSINFORMANT screenshot showing metadata related to Germany"></a></p>
<p><span size="2">
  BOUNDLESSINFORMANT chart showing the numbers of<br>
    metadata which German BND shared with the NSA<br>
  </span>
</p>
<p>

On October 21, for example, the French paper Le Monde published a <a href="https://www.lemonde.fr/technologies/article/2013/10/21/france-in-the-nsa-s-crosshair-phone-networks-under-surveillance_3499741_651865.html" target="_blank">story</a> saying that "telephone communications of French citizens are intercepted on a massive scale." After a similar story appeared in Spain, NSA director Keith Alexander came with a remarkable clarification, <a href="https://www.reuters.com/article/us-usa-security-nsa-idUSBRE99S03N20131029" target="_blank">saying</a>: "This is not information that we collected on European citizens. It represents information that we and our NATO allies have collected in defense of our countries and in support of military operations."</p><p>

Greenwald continued his framing in Norwegian and Italian papers. Only in The Netherlands it was <a href="https://tweakers.net/nieuws/92067/nsa-onderschepte-in-maand-metadata-1-komma-8-miljoen-telefoontjes-in-nederland.html" target="_blank">found out</a> that the BOUNDLESSINFORMANT charts were not about content, but about metadata. Dutch interior minister Ronald Plasterk, however, still followed Greenwald's interpretation and assumed the Americans were spying on Dutch citizens. A court case forced the government to admit that Dutch military intelligence had collected the data during operations abroad.</p>
<p>
It was only in May 2019 that <a href="https://theintercept.com/2019/05/29/nsa-data-afghanistan-iraq-mexico-border/" target="_blank">The Intercept</a> put the pieces together and set the record straight: the various BOUNDLESSINFORMANT charts showed cellphone metadata that had been collected by members of the Afghanistan SIGINT Coalition (AFSC, also known as the 9 Eyes) and fed them into the NSA's Real-Time Regional Gateway (RT-RG) big data analysis platform.</p><p>

When The Intercept confronted Greenwald with this new research, he still <a href="https://theintercept.com/2019/05/29/nsa-data-afghanistan-iraq-mexico-border/" target="_blank">tried</a> to blame the NSA: "At the time, Der Spiegel had already reported this interpretation, the NSA wouldn‚Äôt answer our questions, and they wouldn‚Äôt give us any additional information. I am totally in favor of correcting the record if the reporting was inaccurate."</p><p>

While Greenwald ignored the declaration by general Alexander, he was right when he said that the NSA's internal <a href="https://www.theguardian.com/world/interactive/2013/jun/08/boundless-informant-nsa-full-text" target="_blank">documentation</a> about BOUNDLESSINFORMANT was somewhat confusing. Apparently, Greenwald had to rely on that documentation because Snowden was of little help, just like he was for various other programs that journalists did not fully understand.</p>
<p><a href="https://4.bp.blogspot.com/-4AOOwxorACQ/XX8cgvOthgI/AAAAAAAAEGA/m-OrX8PbqI0YuDbe7hOyftyFroLnlCqNgCLcBGAsYHQ/s1600/afsc-rtrg-datasources.PNG" imageanchor="1" target="_blank"><img src="https://4.bp.blogspot.com/-4AOOwxorACQ/XX8cgvOthgI/AAAAAAAAEGA/m-OrX8PbqI0YuDbe7hOyftyFroLnlCqNgCLcBGAsYHQ/s1600/afsc-rtrg-datasources.PNG" width="500"></a><br>
<span size="2">
Slide showing all the collection systems that fed the RT-RG platform<br> 
<span color="gray">(click to enlarge)</span><br>
</span>
</p>



<p><span size="+2"><b>Truth</b></span></p><p>

Many of the documents that Snowden provided to the press have been misinterpreted or exaggerated, sometimes unintentional, but in other cases maybe deliberately. In <i>Dark Mirror</i>, Barton Gellman writes:</p><div><p>
"There were signs that Snowden was capable of an instrumental approach to truth. In conversations about my work, when I got stuck on a hard reporting problem, he sometimes suggested that I provoke fresh disclosures from government officials by pretending to know more than I did."</p><p>

"Another time he went further, proposing that I actually publish informed speculation as fact. If my story outran the evidence, he said, the government would be forced to respond and thereby reveal more. There would be a net gain for public information either way." </p><p>
  
  "He said misinformation from people like Mike Hayden, supporters of the intelligence establishment, pushed the terms of debate so far off center that only rhetorical counterforce could set the record straight."<a nohref="" title="Barton Gellman, Dark Mirror, p. 324-326">*</a></p></div>
  <p>
Gellman declined this approach because it would make his reporting unreliable and it undermines confidence in the press if it would turn out that certain things weren't true. However, claims made by Greenwald and Snowden himself showed that his "counterforce" method sometimes did work: the government came up with new facts - but those never got the same attention as the original story, which was already stuck in people's minds.</p><p><span size="+1"><b>Conclusion</b></span></p><p>

There's no doubt that the Snowden revelations provided unprecedented insight into modern-day signals intelligence as conducted by the NSA and its Five Eyes partners. </p><p>

In part this was much needed to understand how the legal framework is implemented and where safeguards need improvement. That, however, requires a close examination of the documents, which shows the problems are smaller and more complex than the mythical "global mass surveillance" which Snowden and Greenwald tried to proof.</p><p>

On the other hand, many things have been published that were merely sensational and weakened the US and its signals intelligence system. By revealing its workings and capacity, the Snowden revelations unintentionally set a new standard which other countries <a href="https://blog.erratasec.com/2014/12/snowden-made-things-worse.html" target="_blank">hurried</a> to catch up with.</p><p>



<b>Links </b></p><p><span size="2">
  <br>
- Der Spiegel: <a href="https://archive.is/QZwpB" target="_blank">Das Internet ist heute anders unsicher</a> (June 9, 2023)<br>
- The Atlantic: <a href="https://archive.is/KnUS7" target="_blank">Did the Snowden Revelations Change Anything?</a> (June 7, 2023)<br>
- The Guardian: <a href="https://www.theguardian.com/us-news/2023/jun/07/edward-snowden-mi5-nsa-prism-ghcq" target="_blank">Snowden, MI5 and me: how the leak of the century came to be published</a> (June 7, 2023)<br>
- The Guardian: <a href="https://www.theguardian.com/us-news/2023/jun/07/edward-snowden-10-years-surveillance-revelations" target="_blank">What‚Äôs really changed 10 years after the Snowden revelations?</a> (June 7, 2023)<br>
- Schneier on Security: <a href="https://www.schneier.com/blog/archives/2023/06/snowden-ten-years-later.html" target="_blank">Snowden Ten Years Later</a> (June 6, 2023)<br>
- System Update: <a href="https://rumble.com/v2sgyx2-snowden-revelations-10-year-anniversary-glenn-greenwald-speaks-with-snowden.html" target="_blank">SNOWDEN REVELATIONS 10-Year Anniversary: Glenn Greenwald Speaks with Snowden &amp; Laura Poitras on the Past, Present, &amp; Future of Their Historic Reporting</a> (June 6, 2023)<br>
- neues deutschland: <a href="https://www.nd-aktuell.de/artikel/1173743.jahre-snowden-leaks-jahre-snowden-leaks-enthuellungen-nicht-mehr-erwuenscht.html" target="_blank">10 Jahre Snowden-Leaks: Enth√ºllungen nicht mehr erw√ºnscht</a> (June 6, 2023)<br>
- neues deutschland: <a href="https://www.nd-aktuell.de/artikel/1173746.zehn-jahre-snowden-leaks-snowden-leaks-geheimdokumente-belegen-globale-massenueberwachung.html" target="_blank">Snowden-Leaks: Geheimdokumente belegen globale Massen√ºberwachung</a> (June 6, 2023)<br>
- Heise: <a href="https://www.heise.de/hintergrund/10-Jahre-Snowden-Enthuellungen-Was-hat-der-NSA-Whistleblower-bewirkt-9060879.html" target="_blank">Edward Snowden: Die Enth√ºllungen des NSA-Whistleblowers 10 Jahre sp√§ter</a> (June 5, 2023)<br>
- Der Tagesspiegel: <i>Edward Snowden und die Whistleblower-Frage Feiert die Verr√§ter!</i> (June 2023)<br>
- Netkwesties: <a href="https://www.netkwesties.nl/1472/barton-gellman-herziet-nsa-onthullingen.htm" target="_blank">Barton Gellman herziet NSA-onthullingen</a> (Dec. 7, 2020)<br>
    - See also: <a href="https://web.archive.org/web/20200206094116/https://signpostfilmproductions.com/timeline/" target="_blank">Timeline of Edward Snowden</a><br>
</span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing Small CLI Programs in Common Lisp (2021) (121 pts)]]></title>
            <link>https://stevelosh.com/blog/2021/03/small-common-lisp-cli-programs/</link>
            <guid>37400398</guid>
            <pubDate>Wed, 06 Sep 2023 01:40:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stevelosh.com/blog/2021/03/small-common-lisp-cli-programs/">https://stevelosh.com/blog/2021/03/small-common-lisp-cli-programs/</a>, See on <a href="https://news.ycombinator.com/item?id=37400398">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-blog-entry"><article><p>Posted on March 17th, 2021.</p><p>I write a lot of command-line programs.  For tiny programs I usually go with the
typical UNIX approach: throw together a half-assed shell script and move on.
For large programs I make a full Common Lisp project, with an ASDF system
definition and such.  But there's a middle ground of small<em>ish</em> programs that
don't warrant a full repository on their own, but for which I still want a real
interface with proper <code>--help</code> and error handling.</p>

<p>I've found Common Lisp to be a good language for writing these small command
line programs.  But it can be a little intimidating to get started (especially
for beginners) because Common Lisp is a very flexible language and doesn't lock
you into one way of working.</p>

<p>In this post I'll describe how I write small, stand-alone command line programs
in Common Lisp.  It might work for you, or you might want to modify things to
fit your own needs.</p>

<ol><li><a href="#s1-requirements">Requirements</a></li><li><a href="#s2-solution-skeleton">Solution Skeleton</a><ol><li><a href="#s3-directory-structure">Directory Structure</a></li><li><a href="#s4-lisp-files">Lisp Files</a></li><li><a href="#s5-building-binaries">Building Binaries</a></li><li><a href="#s6-building-man-pages">Building Man Pages</a></li><li><a href="#s7-makefile">Makefile</a></li></ol></li><li><a href="#s8-case-study-a-batch-coloring-utility">Case Study: A Batch Coloring Utility</a><ol><li><a href="#s9-libraries">Libraries</a></li><li><a href="#s10-package">Package</a></li><li><a href="#s11-configuration">Configuration</a></li><li><a href="#s12-errors">Errors</a></li><li><a href="#s13-colorization">Colorization</a></li><li><a href="#s14-not-quite-top-level-interface">Not-Quite-Top-Level Interface</a></li><li><a href="#s15-user-interface">User Interface</a></li><li><a href="#s16-top-level-interface">Top-Level Interface</a></li></ol></li><li><a href="#s17-more-information">More Information</a></li></ol>

<h2 id="s1-requirements"><a href="#s1-requirements">Requirements</a></h2>

<p>When you're writing programs in Common Lisp, you've got a lot of options.
Laying out the requirements I have helped me decide on an approach.</p>

<p>First: each new program should be one single file.  A few other files for the
collection as a whole (e.g. a <code>Makefile</code>) are okay, but once everything is set
up creating a new program should mean adding one single file.  For larger
programs a full project directory and ASDF system are great, but for small
programs having one file per program reduces the mental overhead quite a bit.</p>

<p>The programs need to be able to be developed in the typical Common Lisp
interactive style (in my case: with Swank and VLIME).  Interactive development
is one of the best parts of working in Common Lisp, and I'm not willing to give
it up.  In particular this means that a shell-script style approach, with
<code>#!/path/to/sbcl --script</code> and the top and directly running code at the top
level in the file, doesn't work for two main reasons:</p>

<ul>
<li><code>load</code>ing that file will fail due to the shebang unless you have some ugly
  reader macros in your startup file.</li>
<li>The program will need to do things like parsing command-line arguments and
  exiting with an error code, and calling <code>exit</code> would kill the Swank process.</li>
</ul>

<p>The programs need to be able to use libraries, so Quicklisp will need to be
involved.  Common Lisp has a lot of nice things built-in, but there are some
libraries that are just too useful to pass up.</p>

<p>The programs will need to have proper user interfaces.  Command line arguments
must be robustly parsed (e.g. collapsing <code>-a -b -c foo -d</code> into <code>-abcfoo -d</code>
should work as expected), malformed or unknown options must be caught instead of
dropping them on the floor, error messages should be meaningful, and the
<code>--help</code> should be thoroughly and thoughtfully written so I can remember how to
use the program months later.  A <code>man</code> page is a nice bonus, but not required.</p>

<p>Relying on some basic conventions (e.g. a command <code>foo</code> is always in <code>foo.lisp</code>
and defines a package <code>foo</code> with a function called <code>toplevel</code>) is okay if it
makes my life easier.  These programs are just for me, so I don't have to worry
about people wanting to create executables with spaces in the name or something.</p>

<p>Portability between Common Lisp implementations is nice to have, but not
required.  If using a bit of SBCL-specific grease will let me avoid a bunch of
extra dependencies, that's fine for these small personal programs.</p>

<h2 id="s2-solution-skeleton"><a href="#s2-solution-skeleton">Solution Skeleton</a></h2>

<p>After trying a number of different approaches I've settled on a solution that
I'm pretty happy with.  First I'll describe the general approach, then we'll
look at one actual example program in its entirety.</p>

<h3 id="s3-directory-structure"><a href="#s3-directory-structure">Directory Structure</a></h3>

<p>I keep all my small single-file Common Lisp programs in a <code>lisp</code> directory
inside my dotfiles repository.  Its contents look like this:</p>

<pre><code>‚Ä¶/dotfiles/lisp/
    bin/
        foo
        bar
    man/
        man1/
            foo.1
            bar.1
    build-binary.sh
    build-manual.sh
    Makefile
    foo.lisp
    bar.lisp</code></pre>

<p>The <code>bin</code> directory is where the executable files end up.  I've added it to my
<code>$PATH</code> so I don't have to symlink or copy the binaries anywhere.</p>

<p><code>man</code> contains the generated <code>man</code> pages.  Because it's adjacent to <code>bin</code> (which
is on my path) the <code>man</code> program automatically finds the <code>man</code> pages as
expected.</p>

<p><code>build-binary.sh</code>, <code>build-manual.sh</code>, and <code>Makefile</code> are some glue to make
building programs easier.</p>

<p>The <code>.lisp</code> files are the programs.  Each new program I want to add only
requires adding the <code>&lt;programname&gt;.lisp</code> file in this directory and running
<code>make</code>.</p>

<h3 id="s4-lisp-files"><a href="#s4-lisp-files">Lisp Files</a></h3>

<p>My small Common Lisp programs follow a few conventions that make building them
easier.  Let's look at the skeleton of a <code>foo.lisp</code> file as an example.  I'll
show the entire file here, and then step through it piece by piece.</p>

<pre><code><span><span>(<span><i><span>eval-when</span></i> <span>(<span><span>:compile-toplevel</span> <span>:load-toplevel</span> <span>:execute</span></span>)</span>
  <span>(<span>ql:quickload '<span>(<span><span>:with-user-abort</span> ‚Ä¶</span>)</span> <span>:silent</span> t</span>)</span></span>)</span>

<span>(<span><i><span>defpackage</span></i> <span>:foo</span>
  <span>(<span><span>:use</span> <span>:cl</span></span>)</span>
  <span>(<span><span>:export</span> <span>:toplevel</span> <span>*ui*</span></span>)</span></span>)</span>

<span>(<span>in-package <span>:foo</span></span>)</span>

<span>;;;; Configuration -----------------------------------------------
</span><span>(<span><i><span>defparameter</span></i> <span>*whatever*</span> 123</span>)</span>

<span>;;;; Errors ------------------------------------------------------
</span><span>(<span><i><span>define-condition</span></i> user-error <span>(<span>error</span>)</span> <span>(<span></span>)</span></span>)</span>

<span>(<span><i><span>define-condition</span></i> missing-foo <span>(<span>user-error</span>)</span> <span>(<span></span>)</span>
  <span>(<span><span>:report</span> <span>"A foo is required, but none was supplied."</span></span>)</span></span>)</span>

<span>;;;; Functionality -----------------------------------------------
</span><span>(<span><i><span>defun</span></i> foo <span>(<span>string</span>)</span>
  ‚Ä¶</span>)</span>

<span>;;;; Run ---------------------------------------------------------
</span><span>(<span><i><span>defun</span></i> run <span>(<span>arguments</span>)</span>
  <span>(<span>map nil #'foo arguments</span>)</span></span>)</span>

<span>;;;; User Interface ----------------------------------------------
</span><span>(<span><i><span>defmacro</span></i> exit-on-ctrl-c <span>(<span>&amp;body body</span>)</span>
  `<span>(<span>handler-case <span>(<span><i><span>with-user-abort:with-user-abort</span></i> <span>(<span><i><span>progn</span></i> ,@body</span>)</span></span>)</span>
     <span>(<span>with-user-abort:user-abort <span>(<span></span>)</span> <span>(<span>sb-ext:exit <span>:code</span> 130</span>)</span></span>)</span></span>)</span></span>)</span>

<span>(<span><i><span>defparameter</span></i> <span>*ui*</span>
  <span>(<span>adopt:make-interface
    <span>:name</span> <span>"foo"</span>
    ‚Ä¶</span>)</span></span>)</span>

<span>(<span><i><span>defun</span></i> toplevel <span>(<span></span>)</span>
  <span>(<span>sb-ext:disable-debugger</span>)</span>
  <span>(<span>exit-on-ctrl-c
    <span>(<span>multiple-value-bind <span>(<span>arguments options</span>)</span> <span>(<span>adopt:parse-options-or-exit <span>*ui*</span></span>)</span>
      ‚Ä¶       <span>(<span>handler-case <span>(<span>run arguments</span>)</span>
        <span>(<span>user-error <span>(<span>e</span>)</span> <span>(<span>adopt:print-error-and-exit e</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>

<p>Let's go through each chunk of this.</p>

<pre><code><span><span>(<span><i><span>eval-when</span></i> <span>(<span><span>:compile-toplevel</span> <span>:load-toplevel</span> <span>:execute</span></span>)</span>
  <span>(<span>ql:quickload '<span>(<span><span>:with-user-abort</span> ‚Ä¶</span>)</span> <span>:silent</span> t</span>)</span></span>)</span></span></code></pre>

<p>First we <code>quickload</code> any necessary libraries.  We always want to do this, even
when compiling the file, because we need the appropriate packages to exist when
we try to use their symbols later in the file.</p>

<p><a href="https://github.com/compufox/with-user-abort">with-user-abort</a> is a library for easily handling <code>control-c</code>, which all of
these small programs will use.</p>

<pre><code><span><span>(<span><i><span>defpackage</span></i> <span>:foo</span>
  <span>(<span><span>:use</span> <span>:cl</span></span>)</span>
  <span>(<span><span>:export</span> <span>:toplevel</span> <span>*ui*</span></span>)</span></span>)</span>

<span>(<span>in-package <span>:foo</span></span>)</span></span></code></pre>

<p>Next we define a package <code>foo</code> and switch to it.  The package is always named
the same as the resulting binary and the basename of the file, and always
exports the symbols <code>toplevel</code> and <code>*ui*</code>.  These conventions make it easy to
build everything automatically with <code>make</code> later.</p>

<pre><code><span><span>;;;; Configuration -----------------------------------------------
</span><span>(<span><i><span>defparameter</span></i> <span>*whatever*</span> 123</span>)</span></span></code></pre>

<p>Next we define any configuration variables.  These will be set later after
parsing the command line arguments (when we run the command line program) or
at the REPL (when developing interactively).</p>

<pre><code><span><span>;;;; Errors ------------------------------------------------------
</span><span>(<span><i><span>define-condition</span></i> user-error <span>(<span>error</span>)</span> <span>(<span></span>)</span></span>)</span>

<span>(<span><i><span>define-condition</span></i> missing-foo <span>(<span>user-error</span>)</span> <span>(<span></span>)</span>
  <span>(<span><span>:report</span> <span>"A foo is required, but none was supplied."</span></span>)</span></span>)</span></span></code></pre>

<p>We define a <code>user-error</code> condition, and any errors the user might make will
inherit from it.  This will make it easy to treat user errors (e.g. passing
a mangled regular expression like <code>(foo+</code> as an argument) differently from
programming errors (i.e. bugs).  This makes it easier to treat those errors
differently:</p>

<ul>
<li>Bugs should print a backtrace or enter the debugger.</li>
<li>Expected user errors should print a helpful error message with no backtrace or debugger.</li>
</ul>

<pre><code><span><span>;;;; Functionality -----------------------------------------------
</span><span>(<span><i><span>defun</span></i> foo <span>(<span>string</span>)</span>
  ‚Ä¶</span>)</span></span></code></pre>

<p>Next we have the actual functionality of the program.</p>

<pre><code><span><span>;;;; Run ---------------------------------------------------------
</span><span>(<span><i><span>defun</span></i> run <span>(<span>arguments</span>)</span>
  <span>(<span>map nil #'foo arguments</span>)</span></span>)</span></span></code></pre>

<p>We define a function <code>run</code> that takes some arguments (as strings) and performs
the main work of the program.</p>

<p>Importantly, <code>run</code> does <em>not</em> handle command line argument parsing, and it does
<em>not</em> exit the program with an error code, which means we can safely call it to
say "run the whole program" when we're developing interactively without worrying
about it killing our Lisp process.</p>

<p>Now we need to define the command line interface.</p>

<pre><code><span><span>;;;; User Interface ----------------------------------------------
</span><span>(<span><i><span>defmacro</span></i> exit-on-ctrl-c <span>(<span>&amp;body body</span>)</span>
  `<span>(<span>handler-case <span>(<span><i><span>with-user-abort:with-user-abort</span></i> <span>(<span><i><span>progn</span></i> ,@body</span>)</span></span>)</span>
     <span>(<span>with-user-abort:user-abort <span>(<span></span>)</span> <span>(<span>adopt:exit 130</span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>

<p>We'll make a little macro around <code>with-user-abort</code> to make it less wordy.  We'll
<a href="https://tldp.org/LDP/abs/html/exitcodes.html">exit with a status of 130</a> if the
user presses <code>ctrl-c</code>.  Maybe some day I'll pull this into Adopt so I don't have
to copy these three lines everywhere.</p>

<pre><code><span><span>(<span><i><span>defparameter</span></i> <span>*ui*</span>
  <span>(<span>adopt:make-interface
    <span>:name</span> <span>"foo"</span>
    ‚Ä¶</span>)</span></span>)</span></span></code></pre>

<p>Here we define the <code>*ui*</code> variable whose symbol we exported above.  <a href="https://docs.stevelosh.com/adopt">Adopt</a> is
a command line argument parsing library I wrote.  If you want to use a different
library, feel free.</p>

<pre><code><span><span>(<span><i><span>defun</span></i> toplevel <span>(<span></span>)</span>
  <span>(<span>sb-ext:disable-debugger</span>)</span>
  <span>(<span>exit-on-ctrl-c
    <span>(<span>multiple-value-bind <span>(<span>arguments options</span>)</span> <span>(<span>adopt:parse-options-or-exit <span>*ui*</span></span>)</span>
      ‚Ä¶       <span>(<span>handler-case <span>(<span>run arguments</span>)</span>
        <span>(<span>user-error <span>(<span>e</span>)</span> <span>(<span>adopt:print-error-and-exit e</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>

<p>And finally we define the <code>toplevel</code> function.  This will only ever be called
when the program is run as a standalone program, never interactively.  It
handles all the work beyond the main guts of the program (which are handled by
the <code>run</code> function), including:</p>

<ul>
<li>Disabling or enabling the debugger.</li>
<li>Exiting the process with an appropriate status code on errors.</li>
<li>Parsing command line arguments.</li>
<li>Setting the values of the configuration parameters.</li>
<li>Calling <code>run</code>.</li>
</ul>

<p>That's it for the structure of the <code>.lisp</code> files.</p>

<h3 id="s5-building-binaries"><a href="#s5-building-binaries">Building Binaries</a></h3>

<p><code>build-binary.sh</code> is a small script to build the executable binaries from the
<code>.lisp</code> files.  <code>./build-binary.sh foo.lisp</code> will build <code>foo</code>:</p>

<pre><code>#!/usr/bin/env bash

set -euo pipefail

LISP=$1
NAME=$(basename "$1" .lisp)
shift

sbcl --load "$LISP" \
     --eval "(sb-ext:save-lisp-and-die \"$NAME\"
               :executable t
               :save-runtime-options t
               :toplevel '$NAME:toplevel)"</code></pre>

<p>Here we see where the naming conventions have become important ‚Äî we know that
the package is named the same as the binary and that it will have the symbol
<code>toplevel</code> exported, which always names the entry point for the binary.</p>

<h3 id="s6-building-man-pages"><a href="#s6-building-man-pages">Building Man Pages</a></h3>

<p><code>build-manual.sh</code> is similar and builds the <code>man</code> pages using <a href="https://docs.stevelosh.com/adopt">Adopt</a>'s
built-in <code>man</code> page generation.  If you don't care about building <code>man</code> pages
for your personal programs you can ignore this.  I admit that generating <code>man</code>
pages for these programs is a little bit silly because they're only for my own
personal use, but I get it for free with Adopt, so why not?</p>

<pre><code>#!/usr/bin/env bash

set -euo pipefail

LISP=$1
NAME=$(basename "$LISP" .lisp)
OUT="$NAME.1"
shift

sbcl --load "$LISP" \
     --eval "(with-open-file (f \"$OUT\" :direction :output :if-exists :supersede)
               (adopt:print-manual $NAME:*ui* :stream f))" \
     --quit</code></pre>

<p>This is why we always name the Adopt interface variable <code>*ui*</code> and export it
from the package.</p>

<h3 id="s7-makefile"><a href="#s7-makefile">Makefile</a></h3>

<p>Finally we have a simple <code>Makefile</code> so we can run <code>make</code> to regenerate any
out of date binaries and <code>man</code> pages:</p>

<pre><code>files := $(wildcard *.lisp)
names := $(files:.lisp=)

.PHONY: all clean $(names)

all: $(names)

$(names): %: bin/% man/man1/%.1

bin/%: %.lisp build-binary.sh Makefile
    mkdir -p bin
    ./build-binary.sh $&lt;
    mv $(@F) bin/

man/man1/%.1: %.lisp build-manual.sh Makefile
    mkdir -p man/man1
    ./build-manual.sh $&lt;
    mv $(@F) man/man1/

clean:
    rm -rf bin man</code></pre>

<p>We use a <code>wildcard</code> to automatically find the <code>.lisp</code> files so we don't have to
do anything extra after adding a new file when we want to make a new program.</p>

<p>The most notable line here is <code>$(names): %: bin/% man/man1/%.1</code> which uses
a <a href="https://www.gnu.org/software/make/manual/html_node/Static-Pattern.html#Static-Pattern">static pattern rule</a>
to automatically define the phony rules for building each program.  If
<code>$(names)</code> is <code>foo bar</code> this line effectively defines two phony rules:</p>

<pre><code>foo: bin/foo man/man1/foo.1
bar: bin/bar man/man1/bar.1</code></pre>

<p>This lets us run <code>make foo</code> to make both the binary and <code>man</code> page for
<code>foo.lisp</code>.</p>

<h2 id="s8-case-study-a-batch-coloring-utility"><a href="#s8-case-study-a-batch-coloring-utility">Case Study: A Batch Coloring Utility</a></h2>

<p>Now that we've seen the skeleton, let's look at one of my actual programs that
I use all the time.  It's called <code>batchcolor</code> and it's used to highlight regular
expression matches in text (usually log files) with a twist: each unique match
is highlighted in a separate color, which makes it easier to visually parse the
result.</p>

<p>For example: suppose we have some log files with lines of the form <code>&lt;timestamp&gt;
[&lt;request ID&gt;] &lt;level&gt; &lt;message&gt;</code> where request ID is a UUID, and messages might
contain other UUIDs for various things.  Such a log file might look something
like this:</p>

<pre><code>2021-01-02 14:01:45 [f788a624-8dcd-4c5e-b1e8-681d0a68a8d3] INFO Incoming request GET /users/28b2d548-eff1-471c-b807-cc2bcee76b7d/things/7ca6d8d2-5038-42bd-a559-b3ee0c8b7543/
2021-01-02 14:01:45 [f788a624-8dcd-4c5e-b1e8-681d0a68a8d3] INFO Thing 7ca6d8d2-5038-42bd-a559-b3ee0c8b7543 is not cached, retrieving...
2021-01-02 14:01:45 [f788a624-8dcd-4c5e-b1e8-681d0a68a8d3] WARN User 28b2d548-eff1-471c-b807-cc2bcee76b7d does not have access to thing 7ca6d8d2-5038-42bd-a559-b3ee0c8b7543, denying request.
2021-01-02 14:01:46 [f788a624-8dcd-4c5e-b1e8-681d0a68a8d3] INFO Returning HTTP 404.
2021-01-02 14:01:46 [bea6ae06-bd06-4d2a-ae35-3e83fea2edc7] INFO Incoming request GET /users/28b2d548-eff1-471c-b807-cc2bcee76b7d/things/7ca6d8d2-5038-42bd-a559-b3ee0c8d7543/
2021-01-02 14:01:46 [bea6ae06-bd06-4d2a-ae35-3e83fea2edc7] INFO Thing 7ca6d8d2-5038-42bd-a559-b3ee0c8d7543 is not cached, retrieving...
2021-01-02 14:01:46 [b04ced1d-1cfa-4315-aaa9-0e245ff9a8e1] INFO Incoming request POST /users/sign-up/
2021-01-02 14:01:46 [bea6ae06-bd06-4d2a-ae35-3e83fea2edc7] INFO Returning HTTP 200.
2021-01-02 14:01:46 [b04ced1d-1cfa-4315-aaa9-0e245ff9a8e1] ERR Error running SQL query: connection refused.
2021-01-02 14:01:47 [b04ced1d-1cfa-4315-aaa9-0e245ff9a8e1] ERR Returning HTTP 500.</code></pre>

<p>If I try to just read this directly, it's easy for my eyes to glaze over unless
I laboriously walk line-by-line.</p>

<p><a href="https://stevelosh.com/static/images/blog/2021/03/uncolored.png"><img src="https://stevelosh.com/static/images/blog/2021/03/uncolored.png" alt="Screenshot of uncolored log output"></a></p>

<p>I could use <code>grep</code> to highlight the UUIDs:</p>

<pre><code>grep -P \
    '\b[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}\b' \
    example.log
</code></pre>

<p>Unfortunately that doesn't really help too much because all the UUIDs are
highlighted the same color:</p>

<p><a href="https://stevelosh.com/static/images/blog/2021/03/grepcolored.png"><img src="https://stevelosh.com/static/images/blog/2021/03/grepcolored.png" alt="Screenshot of grep-colored log output"></a></p>

<p>To get a more readable version of the log, I use <code>batchcolor</code>:</p>

<pre><code>batchcolor \
    '\b[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}\b' \
    example.log
</code></pre>

<p><code>batchcolor</code> also highlights matches, but it highlights each unique match in its
own color:</p>

<p><a href="https://stevelosh.com/static/images/blog/2021/03/batchcolored.png"><img src="https://stevelosh.com/static/images/blog/2021/03/batchcolored.png" alt="Screenshot of batchcolored log output"></a></p>

<p>This is <em>much</em> easier for me to visually parse.  The interleaving of separate
request logs is now obvious from the colors of the IDs, and it's easy to match
up various user IDs and thing IDs at a glance.  Did you even notice that the two
thing IDs were different before?</p>

<p><code>batchcolor</code> has a few other quality of life features, like picking explicit
colors for specific strings (e.g. red for <code>ERR</code>):</p>

<p><a href="https://stevelosh.com/static/images/blog/2021/03/batchcoloredfull.png"><img src="https://stevelosh.com/static/images/blog/2021/03/batchcoloredfull.png" alt="Screenshot of fully batchcolored log output"></a></p>

<p>I use this particular <code>batchcolor</code> invocation so often I've put it in its own
tiny shell script.  I use it to <code>tail</code> log files when developing locally almost
every day, and it makes visually scanning the log output <em>much</em> easier.  It can
come in handy for other kinds of text too, like highlighting nicknames in an IRC
log.</p>

<p>Let's step through its code piece by piece.</p>

<h3 id="s9-libraries"><a href="#s9-libraries">Libraries</a></h3>

<pre><code><span><span>(<span><i><span>eval-when</span></i> <span>(<span><span>:compile-toplevel</span> <span>:load-toplevel</span> <span>:execute</span></span>)</span>
  <span>(<span>ql:quickload '<span>(<span><span>:adopt</span> <span>:cl-ppcre</span> <span>:with-user-abort</span></span>)</span> <span>:silent</span> t</span>)</span></span>)</span></span></code></pre>

<p>First we <code>quickload</code> libraries.  We'll use <a href="https://docs.stevelosh.com/adopt">Adopt</a> for command line argument
processing, <a href="http://edicl.github.io/cl-ppcre/">cl-ppcre</a> for regular expressions, and the previously-mentioned
<a href="https://github.com/compufox/with-user-abort">with-user-abort</a> to handle <code>control-c</code>.</p>

<h3 id="s10-package"><a href="#s10-package">Package</a></h3>

<pre><code><span><span>(<span><i><span>defpackage</span></i> <span>:batchcolor</span>
  <span>(<span><span>:use</span> <span>:cl</span></span>)</span>
  <span>(<span><span>:export</span> <span>:toplevel</span> <span>:*ui*</span></span>)</span></span>)</span>

<span>(<span>in-package <span>:batchcolor</span></span>)</span></span></code></pre>

<p>We define and switch to the appropriately-named package.  Nothing special here.</p>

<h3 id="s11-configuration"><a href="#s11-configuration">Configuration</a></h3>

<pre><code><span><span>;;;; Configuration ------------------------------------------------------------
</span><span>(<span><i><span>defparameter</span></i> <span>*start*</span> 0</span>)</span>
<span>(<span><i><span>defparameter</span></i> <span>*dark*</span> t</span>)</span></span></code></pre>

<p>Next we <code>defparameter</code> some variables to hold some settings.  <code>*start*</code> will be
used later when randomizing colors, don't worry about it for now.</p>

<h3 id="s12-errors"><a href="#s12-errors">Errors</a></h3>

<pre><code><span><span>;;;; Errors -------------------------------------------------------------------
</span><span>(<span><i><span>define-condition</span></i> user-error <span>(<span>error</span>)</span> <span>(<span></span>)</span></span>)</span>

<span>(<span><i><span>define-condition</span></i> missing-regex <span>(<span>user-error</span>)</span> <span>(<span></span>)</span>
  <span>(<span><span>:report</span> <span>"A regular expression is required."</span></span>)</span></span>)</span>

<span>(<span><i><span>define-condition</span></i> malformed-regex <span>(<span>user-error</span>)</span>
  <span>(<span><span>(<span>underlying-error <span>:initarg</span> <span>:underlying-error</span></span>)</span></span>)</span>
  <span>(<span><span>:report</span> <span>(<span><i><span>lambda</span></i> <span>(<span>c s</span>)</span>
             <span>(<span>format s <span>"Invalid regex: ~A"</span> <span>(<span>slot-value c 'underlying-error</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span>

<span>(<span><i><span>define-condition</span></i> overlapping-groups <span>(<span>user-error</span>)</span> <span>(<span></span>)</span>
  <span>(<span><span>:report</span> <span>"Invalid regex: seems to contain overlapping capturing groups."</span></span>)</span></span>)</span>

<span>(<span><i><span>define-condition</span></i> malformed-explicit <span>(<span>user-error</span>)</span>
  <span>(<span><span>(<span>spec <span>:initarg</span> <span>:spec</span></span>)</span></span>)</span>
  <span>(<span><span>:report</span>
    <span>(<span><i><span>lambda</span></i> <span>(<span>c s</span>)</span>
      <span>(<span>format s <span>"Invalid explicit spec ~S, must be of the form </span><span>\"</span><span>R,G,B:string</span><span>\"</span><span> with colors being 0-5."</span>
              <span>(<span>slot-value c 'spec</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>

<p>Here we define the user errors.  Some of these are self-explanatory, while
others will make more sense later once we see them in action.  The specific
details aren't as important as the overall idea: for user errors we know might
happen, display a helpful error message instead of just spewing a backtrace at
the user.</p>

<h3 id="s13-colorization"><a href="#s13-colorization">Colorization</a></h3>

<p>Next we have the actual meat of the program.  Obviously this is going to be
completely different for every program, so feel free to skip this if you don't
care about this specific problem.</p>

<pre><code><span><span>;;;; Functionality ------------------------------------------------------------
</span><span>(<span><i><span>defun</span></i> rgb-code <span>(<span>r g b</span>)</span>
  <span>;; The 256 color mode color values are essentially r/g/b in base 6, but
</span>  <span>;; shifted 16 higher to account for the intiial 8+8 colors.
</span>  <span>(<span>+ <span>(<span>* r 36</span>)</span>
     <span>(<span>* g 6</span>)</span>
     <span>(<span>* b 1</span>)</span>
     16</span>)</span></span>)</span></span></code></pre>

<p>We're going to highlight different matches with different colors.  We'll need
a reasonable amount of colors to make this useful, so using the basic 8/16 ANSI
colors isn't enough.  Full 24-bit truecolor is overkill, but the 8-bit ANSI
colors will work nicely.  If we ignore the base colors, we essentially have
6 x 6 x 6 = 216 colors to work with.  <code>rgb-code</code> will take the red, green, and
blue values from <code>0</code> to <code>5</code> and return the color code.  See <a href="https://en.wikipedia.org/wiki/ANSI_escape_code#8-bit">Wikipedia</a>
for more information.</p>

<pre><code><span><span>(<span><i><span>defun</span></i> make-colors <span>(<span>excludep</span>)</span>
  <span>(<span><i><span>let</span></i> <span>(<span><span>(<span>result <span>(<span>make-array 256 <span>:fill-pointer</span> 0</span>)</span></span>)</span></span>)</span>
    <span>(<span>dotimes <span>(<span>r 6</span>)</span>
      <span>(<span>dotimes <span>(<span>g 6</span>)</span>
        <span>(<span>dotimes <span>(<span>b 6</span>)</span>
          <span>(<span>unless <span>(<span>funcall excludep <span>(<span>+ r g b</span>)</span></span>)</span>
            <span>(<span>vector-push-extend <span>(<span>rgb-code r g b</span>)</span> result</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span>
    result</span>)</span></span>)</span>

<span>(<span><i><span>defparameter</span></i> <span>*dark-colors*</span>  <span>(<span>make-colors <span>(<span><i><span>lambda</span></i> <span>(<span>v</span>)</span> <span>(<span>&lt; v 3</span>)</span></span>)</span></span>)</span></span>)</span>
<span>(<span><i><span>defparameter</span></i> <span>*light-colors*</span> <span>(<span>make-colors <span>(<span><i><span>lambda</span></i> <span>(<span>v</span>)</span> <span>(<span>&gt; v 11</span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>

<p>Now we can build some arrays of colors.  We <em>could</em> use any of the 216 available
colors, but in practice we probably don't want to, because the darkest colors
will be too dark to read on a dark terminal, and vice versa for light terminals.
In a concession to practicality we'll generate two separate arrays of colors,
one that excludes colors whose total value is too dark and one excluding those
that are too light.</p>

<p>(Notice that <code>*dark-colors*</code> is "the array of colors which are suitable for use
on dark terminals" and not "the array of colors which are <em>themselves</em> dark".
Naming things is hard.)</p>

<p>Note that these arrays will be generated when the <code>batchcolor.lisp</code> file is
<code>load</code>ed, which is <em>when we build the binary</em>.  They <em>won't</em> be recomputed every
time you run the resulting binary.  In this case it doesn't really matter (the
arrays are small) but it's worth remembering in case you ever have some data you
want (or don't want) to compute at build time instead of run time.</p>

<pre><code><span><span>(<span><i><span>defparameter</span></i> <span>*explicits*</span> <span>(<span>make-hash-table <span>:test</span> #'equal</span>)</span></span>)</span></span></code></pre>

<p>Here we make a hash table to store the strings and colors for strings we want to
explicitly color (e.g. <code>ERR</code> should be red, <code>INFO</code> cyan).  The keys will be the
strings and values the RGB codes.</p>

<pre><code><span><span>(<span><i><span>defun</span></i> djb2 <span>(<span>string</span>)</span>
  <span>;; http://www.cse.yorku.ca/~oz/hash.html
</span>  <span>(<span>reduce <span>(<span><i><span>lambda</span></i> <span>(<span>hash c</span>)</span>
            <span>(<span>mod <span>(<span>+ <span>(<span>* 33 hash</span>)</span> c</span>)</span> <span>(<span>expt 2 64</span>)</span></span>)</span></span>)</span>
          string
          <span>:initial-value</span> 5381
          <span>:key</span> #'char-code</span>)</span></span>)</span>

<span>(<span><i><span>defun</span></i> find-color <span>(<span>string</span>)</span>
  <span>(<span>gethash string <span>*explicits*</span>
           <span>(<span><i><span>let</span></i> <span>(<span><span>(<span>colors <span>(<span><i><span>if</span></i> <span>*dark*</span> <span>*dark-colors*</span> <span>*light-colors*</span></span>)</span></span>)</span></span>)</span>
             <span>(<span>aref colors
                   <span>(<span>mod <span>(<span>+ <span>(<span>djb2 string</span>)</span> <span>*start*</span></span>)</span>
                        <span>(<span>length colors</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>

<p>For strings that we want to explicitly color, we just look up the appropriate
code in <code>*explicits*</code> and return it.</p>

<p>Otherwise, we want to highlight unique matches in different colors.  There are
a number of different ways we could do this, for example: we could randomly pick
a color the first time we see a string and store it in a hash table for
subsequent encounters.  But this would mean we'd grow that hash table over time,
and one of the things I often use this utility for is <code>tail -f</code>ing long-running
processes when developing locally, so the memory usage would grow and grow until
the <code>batchcolor</code> process was restarted, which isn't ideal.</p>

<p>Instead, we'll hash each string with a simple <a href="http://www.cse.yorku.ca/~oz/hash.html">DJB hash</a> and use it to
index into the appropriate array of colors.  This ensures that identical matches
get identical colors, and avoids having to store every match we've ever seen.</p>

<p>There will be some collisions, but there's not much we can do about that with
only ~200 colors to work with.  We could have used 16-bit colors like
I mentioned before, but then we'd have to worry about picking colors different
enough for humans to easily tell apart, and for this simple utility I didn't
want to bother.</p>

<p>We'll talk about <code>*start*</code> later, ignore it for now (it's <code>0</code> by default).</p>

<pre><code><span><span>(<span><i><span>defun</span></i> ansi-color-start <span>(<span>color</span>)</span>
  <span>(<span>format nil <span>"~C[38;5;~Dm"</span> <span>#\Escape</span> color</span>)</span></span>)</span>

<span>(<span><i><span>defun</span></i> ansi-color-end <span>(<span></span>)</span>
  <span>(<span>format nil <span>"~C[0m"</span> <span>#\Escape</span></span>)</span></span>)</span>

<span>(<span><i><span>defun</span></i> print-colorized <span>(<span>string</span>)</span>
  <span>(<span>format <span>*standard-output*</span> <span>"~A~A~A"</span>
          <span>(<span>ansi-color-start <span>(<span>find-color string</span>)</span></span>)</span>
          string
          <span>(<span>ansi-color-end</span>)</span></span>)</span></span>)</span></span></code></pre>

<p>Next we have some functions to output the appropriate ANSI escapes to highlight
our matches.  We could use a library for this but it's only two lines.  <a href="http://xn--rpa.cc/irl/term.html">It's
not worth it</a>.</p>

<p>And now we have the beating heart of the program:</p>

<pre><code><span><span>(<span><i><span>defun</span></i> colorize-line <span>(<span>scanner line &amp;aux <span>(<span>start 0</span>)</span></span>)</span>
  <span>(<span>ppcre:do-scans <span>(<span>ms me rs re scanner line</span>)</span>
            <span>(<span><i><span>let*</span></i> <span>(<span><span>(<span>regs? <span>(<span>plusp <span>(<span>length rs</span>)</span></span>)</span></span>)</span>
           <span>(<span>starts <span>(<span><i><span>if</span></i> regs? <span>(<span>remove nil rs</span>)</span> <span>(<span>list ms</span>)</span></span>)</span></span>)</span>
           <span>(<span>ends   <span>(<span><i><span>if</span></i> regs? <span>(<span>remove nil re</span>)</span> <span>(<span>list me</span>)</span></span>)</span></span>)</span></span>)</span>
      <span>(<span>map nil <span>(<span><i><span>lambda</span></i> <span>(<span>word-start word-end</span>)</span>
                 <span>(<span>unless <span>(<span>&lt;= start word-start</span>)</span>
                   <span>(<span>error 'overlapping-groups</span>)</span></span>)</span>
                 <span>(<span>write-string line <span>*standard-output*</span> <span>:start</span> start <span>:end</span> word-start</span>)</span>
                 <span>(<span>print-colorized <span>(<span>subseq line word-start word-end</span>)</span></span>)</span>
                 <span>(<span>setf start word-end</span>)</span></span>)</span>
           starts ends</span>)</span></span>)</span></span>)</span>
  <span>(<span>write-line line <span>*standard-output*</span> <span>:start</span> start</span>)</span></span>)</span></span></code></pre>

<p><code>colorize-line</code> takes a CL-PPCRE scanner and a line, and outputs the line with
any of the desired matches colorized appropriately.  There are a few things to
note here.</p>

<p>First: if the regular expression contains any capturing groups, we'll only
colorize those parts of the match.  For example: if you run <code>batchcolor
'^&lt;(\\w+)&gt; '</code> to colorize the nicks in an IRC log, only the nicknames themselves
will be highlighted, not the surrounding angle brackets.  Otherwise, if there
are no capturing groups in the regular expression, we'll highlight the entire
match (as if there were one big capturing group around the whole thing).</p>

<p>Second: overlapping capturing groups are explicitly disallowed and
a <code>user-error</code> signaled if we notice any.  It's not clear what do to in this
case ‚Äî if we match <code>((f)oo|(b)oo)</code> against <code>foo</code>, what should the output be?
Highlight <code>f</code> and <code>oo</code> in the same color?  In different colors?  Should the <code>oo</code>
be a different color than the <code>oo</code> in <code>boo</code>?  There's too many options with no
clear winner, so we'll just tell the user to be more clear.</p>

<p>To do the actual work we iterate over each match and print the non-highlighted
text before the match, then print the highlighted match.  Finally we print any
remaining text after the last match.</p>

<h3 id="s14-not-quite-top-level-interface"><a href="#s14-not-quite-top-level-interface">Not-Quite-Top-Level Interface</a></h3>

<pre><code><span><span>;;;; Run ----------------------------------------------------------------------
</span><span>(<span><i><span>defun</span></i> run% <span>(<span>scanner stream</span>)</span>
  <span>(<span><i><span>loop</span></i> <span>:for</span> line = <span>(<span>read-line stream nil</span>)</span>
        <span>:while</span> line
        <span>:do</span> <span>(<span>colorize-line scanner line</span>)</span></span>)</span></span>)</span>

<span>(<span><i><span>defun</span></i> run <span>(<span>pattern paths</span>)</span>
  <span>(<span><i><span>let</span></i> <span>(<span><span>(<span>scanner <span>(<span>handler-case <span>(<span>ppcre:create-scanner pattern</span>)</span>
                   <span>(<span>ppcre:ppcre-syntax-error <span>(<span>c</span>)</span>
                     <span>(<span>error 'malformed-regex <span>:underlying-error</span> c</span>)</span></span>)</span></span>)</span></span>)</span>
        <span>(<span>paths <span>(<span>or paths '<span>(<span><span>"-"</span></span>)</span></span>)</span></span>)</span></span>)</span>
    <span>(<span>dolist <span>(<span>path paths</span>)</span>
      <span>(<span><i><span>if</span></i> <span>(<span>string= <span>"-"</span> path</span>)</span>
        <span>(<span>run% scanner <span>*standard-input*</span></span>)</span>
        <span>(<span><i><span>with-open-file</span></i> <span>(<span>stream path <span>:direction</span> <span>:input</span></span>)</span>
          <span>(<span>run% scanner stream</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>

<p>Here we have the not-quite-top-level interface to the program.  <code>run</code> takes
a pattern string and a list of paths and runs the colorization on each path.
This is safe to call interactively from the REPL, e.g. <code>(run "&lt;(\\w+)&gt;"
"foo.txt")</code>, so we can test without worrying about killing the Lisp process.</p>

<h3 id="s15-user-interface"><a href="#s15-user-interface">User Interface</a></h3>

<p>In the last chunk of the file we have the user interface.  There are a couple of
things to note here.</p>

<p>I'm using a command line argument parsing library I wrote myself: <a href="https://docs.stevelosh.com/adopt">Adopt</a>.
I won't go over exactly what all the various Adopt functions do.  Most of them
should be fairly easy to understand, but <a href="https://docs.stevelosh.com/adopt/usage/">check out the Adopt
documentation</a> for the full story if you're curious.</p>

<p>If you prefer another library (and there are quite a few around) feel free
to use it ‚Äî it should be pretty easy to adapt this setup to a different library.
The only things you'd need to change would be the <code>toplevel</code> function and the
<code>build-manual.sh</code> script (if you even care about building <code>man</code> pages at all).</p>

<p>You might also notice that the user interface for the program is almost as much
code as the entire rest of the program.  This may seem strange, but I think it
makes a certain kind of sense.  When you're writing code to interface with an
external system, a messier and more complicated external system will usually
require more code than a cleaner and simpler external system.  A human brain is
probably the messiest and most complicated external system you'll ever have to
deal with, so it's worth taking the extra time and code to be especially careful
when writing an interface to it.</p>

<p>First we'll define a typical <code>-h</code>/<code>--help</code> option:</p>

<pre><code><span><span>(<span><i><span>defparameter</span></i> <span>*option-help*</span>
  <span>(<span>adopt:make-option 'help
    <span>:help</span> <span>"Display help and exit."</span>
    <span>:long</span> <span>"help"</span>
    <span>:short</span> <span>#\h</span>
    <span>:reduce</span> <span>(<span>constantly t</span>)</span></span>)</span></span>)</span></span></code></pre>

<p>Next we'll define a pair of options for enabling/disabling the Lisp debugger:</p>

<pre><code><span><span>(<span><i><span>adopt:defparameters</span></i> <span>(<span><span>*option-debug*</span> <span>*option-no-debug*</span></span>)</span>
  <span>(<span>adopt:make-boolean-options 'debug
    <span>:long</span> <span>"debug"</span>
    <span>:short</span> <span>#\d</span>
    <span>:help</span> <span>"Enable the Lisp debugger."</span>
    <span>:help-no</span> <span>"Disable the Lisp debugger (the default)."</span></span>)</span></span>)</span></span></code></pre>

<p>By default the debugger will be off, so any unexpected error will print
a backtrace to standard error and exit with a nonzero exit code.  This is the
default because if I add a <code>batchcolor</code> somewhere in a shell script, I probably
don't want to suddenly hang the entire script if something breaks.  But we still
want to be <em>able</em> to get into the debugger manually if something goes wrong.
This is Common Lisp ‚Äî we don't have to settle for a stack trace or core dump, we
can have a real interactive debugger in the final binary.</p>

<p>Note how Adopt's <code>make-boolean-options</code> function creates <em>two</em> options here:</p>

<ul>
<li><code>-d</code>/<code>--debug</code> will enable the debugger.</li>
<li><code>-D</code>/<code>--no-debug</code> will disable the debugger.</li>
</ul>

<p>Even though <em>disabled</em> is the default, it's still important to have both
switches for boolean options like this.  If someone wants the debugger to be
<em>enabled</em> by default instead (along with some other configuration options), they
might have a shell alias like this:</p>

<pre><code>alias bcolor='batchcolor --debug --foo --bar'
</code></pre>

<p>But sometimes they might want to temporarily <em>disable</em> the debugger for a single
run.  Without a <code>--no-debug</code> option, they would have to run the vanilla
<code>batchcolor</code> and retype all the <em>other</em> options.  But having the <code>--no-debug</code>
option allows them to just say:</p>

<pre><code>bcolor --no-debug
</code></pre>

<p>This would expand to:</p>

<pre><code>batchcolor --debug --foo --bar --no-debug
</code></pre>

<p>The later option wins, and the user gets the behavior they expect.</p>

<p>Next we'll define some color-related options.  First an option to randomize the
colors each run, instead of always picking the same color for a particular
string, and then a toggle for choosing colors that work for dark or light
terminals:</p>

<pre><code><span><span>(<span><i><span>adopt:defparameters</span></i> <span>(<span><span>*option-randomize*</span> <span>*option-no-randomize*</span></span>)</span>
  <span>(<span>adopt:make-boolean-options 'randomize
    <span>:help</span> <span>"Randomize the choice of color each run."</span>
    <span>:help-no</span> <span>"Do not randomize the choice of color each run (the default)."</span>
    <span>:long</span> <span>"randomize"</span>
    <span>:short</span> <span>#\r</span></span>)</span></span>)</span>

<span>(<span><i><span>adopt:defparameters</span></i> <span>(<span><span>*option-dark*</span> <span>*option-light*</span></span>)</span>
  <span>(<span>adopt:make-boolean-options 'dark
    <span>:name-no</span> 'light
    <span>:long</span> <span>"dark"</span>
    <span>:long-no</span> <span>"light"</span>
    <span>:help</span> <span>"Optimize for dark terminals (the default)."</span>
    <span>:help-no</span> <span>"Optimize for light terminals."</span>
    <span>:initial-value</span> t</span>)</span></span>)</span></span></code></pre>

<p>The last option we'll define is <code>-e</code>/<code>--explicit</code>, to allow the user to select
an explicit color for a particular string:</p>

<pre><code><span><span>(<span><i><span>defun</span></i> parse-explicit <span>(<span>spec</span>)</span>
  <span>(<span>ppcre:register-groups-bind
      <span>(<span><span>(<span>#'parse-integer r g b</span>)</span> string</span>)</span>
      <span>(<span><span>"^([0-5]),([0-5]),([0-5]):(.+)$"</span> spec</span>)</span>
    <span>(<span><i><span>return-from</span></i> parse-explicit <span>(<span>cons string <span>(<span>rgb-code r g b</span>)</span></span>)</span></span>)</span></span>)</span>
  <span>(<span>error 'malformed-explicit <span>:spec</span> spec</span>)</span></span>)</span>

<span>(<span><i><span>defparameter</span></i> <span>*option-explicit*</span>
  <span>(<span>adopt:make-option 'explicit
    <span>:parameter</span> <span>"R,G,B:STRING"</span>
    <span>:help</span> <span>"Highlight STRING in an explicit color.  May be given multiple times."</span>
    <span>:manual</span> <span>(<span>format nil <span>"~
      Highlight STRING in an explicit color instead of randomly choosing one.  ~
      R, G, and B must be 0-5.  STRING is treated as literal string, not a regex.  ~
      Note that this doesn't automatically add STRING to the overall regex, you ~
      must do that yourself!  This is a known bug that may be fixed in the future."</span></span>)</span>
    <span>:long</span> <span>"explicit"</span>
    <span>:short</span> <span>#\e</span>
    <span>:key</span> #'parse-explicit
    <span>:reduce</span> #'adopt:collect</span>)</span></span>)</span></span></code></pre>

<p>Notice how we signal a <code>malformed-explicit</code> condition if the user gives us
mangled text.  This is a subtype of <code>user-error</code>, so the program will print the
error and exit even if the debugger is enabled.  We also include a slightly more
verbose description in the <code>man</code> page than the terse one in the <code>--help</code> text.</p>

<p>Next we write the main help and manual text, as well as some real-world
examples:</p>

<pre><code><span><span>(<span><i><span>adopt:define-string</span></i> <span>*help-text*</span>
  <span>"batchcolor takes a regular expression and matches it against standard ~
   input one line at a time.  Each unique match is highlighted in its own color.~@
   ~@
   If the regular expression contains any capturing groups, only those parts of ~
   the matches will be highlighted.  Otherwise the entire match will be ~
   highlighted.  Overlapping capturing groups are not supported."</span></span>)</span>

<span>(<span><i><span>adopt:define-string</span></i> <span>*extra-manual-text*</span>
  <span>"If no FILEs are given, standard input will be used.  A file of - stands for ~
   standard input as well.~@
   ~@
   Overlapping capturing groups are not supported because it's not clear what ~
   the result should be.  For example: what should ((f)oo|(b)oo) highlight when ~
   matched against 'foo'?  Should it highlight 'foo' in one color?  The 'f' in ~
   one color and 'oo' in another color?  Should that 'oo' be the same color as ~
   the 'oo' in 'boo' even though the overall match was different?  There are too ~
   many possible behaviors and no clear winner, so batchcolor disallows ~
   overlapping capturing groups entirely."</span></span>)</span>

<span>(<span><i><span>defparameter</span></i> <span>*examples*</span>
  '<span>(<span><span>(<span><span>"Colorize IRC nicknames in a chat log:"</span>
     . <span>"cat channel.log | batchcolor '&lt;(</span><span>\\</span><span>\\</span><span>w+)&gt;'"</span></span>)</span>
    <span>(<span><span>"Colorize UUIDs in a request log:"</span>
     . <span>"tail -f /var/log/foo | batchcolor '[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}'"</span></span>)</span>
    <span>(<span><span>"Colorize some keywords explicitly and IPv4 addresses randomly (note that the keywords have to be in the main regex too, not just in the -e options):"</span>
     . <span>"batchcolor 'WARN|INFO|ERR|(?:[0-9]{1,3}</span><span>\\</span><span>\\</span><span>.){3}[0-9]{1,3}' -e '5,0,0:ERR' -e '5,4,0:WARN' -e '2,2,5:INFO' foo.log"</span></span>)</span>
    <span>(<span><span>"Colorize earmuffed symbols in a Lisp file:"</span>
     . <span>"batchcolor '(?:^|[^*])([*][-a-zA-Z0-9]+[*])(?:$|[^*])' tests/test.lisp"</span></span>)</span></span>)</span></span>)</span></span></code></pre>

<p>Finally we can wire everything together in the main Adopt interface:</p>

<pre><code><span><span>(<span><i><span>defparameter</span></i> <span>*ui*</span>
  <span>(<span>adopt:make-interface
    <span>:name</span> <span>"batchcolor"</span>
    <span>:usage</span> <span>"[OPTIONS] REGEX [FILE...]"</span>
    <span>:summary</span> <span>"colorize regex matches in batches"</span>
    <span>:help</span> <span>*help-text*</span>
    <span>:manual</span> <span>(<span>format nil <span>"~A~2%~A"</span> <span>*help-text*</span> <span>*extra-manual-text*</span></span>)</span>
    <span>:examples</span> <span>*examples*</span>
    <span>:contents</span> <span>(<span>list
                <span>*option-help*</span>
                <span>*option-debug*</span>
                <span>*option-no-debug*</span>
                <span>(<span>adopt:make-group 'color-options
                                  <span>:title</span> <span>"Color Options"</span>
                                  <span>:options</span> <span>(<span>list <span>*option-randomize*</span>
                                                 <span>*option-no-randomize*</span>
                                                 <span>*option-dark*</span>
                                                 <span>*option-light*</span>
                                                 <span>*option-explicit*</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>

<p>All that's left to do is the top-level function that will be called when the
binary is executed.</p>

<h3 id="s16-top-level-interface"><a href="#s16-top-level-interface">Top-Level Interface</a></h3>

<p>Before we write <code>toplevel</code> we've got a couple of helpers:</p>

<pre><code><span><span>(<span><i><span>defmacro</span></i> exit-on-ctrl-c <span>(<span>&amp;body body</span>)</span>
  `<span>(<span>handler-case <span>(<span><i><span>with-user-abort:with-user-abort</span></i> <span>(<span><i><span>progn</span></i> ,@body</span>)</span></span>)</span>
     <span>(<span>with-user-abort:user-abort <span>(<span></span>)</span> <span>(<span>adopt:exit 130</span>)</span></span>)</span></span>)</span></span>)</span>

<span>(<span><i><span>defun</span></i> configure <span>(<span>options</span>)</span>
  <span>(<span><i><span>loop</span></i> <span>:for</span> <span>(<span>string . rgb</span>)</span> <span>:in</span> <span>(<span>gethash 'explicit options</span>)</span>
        <span>:do</span> <span>(<span>setf <span>(<span>gethash string <span>*explicits*</span></span>)</span> rgb</span>)</span></span>)</span>
  <span>(<span>setf <span>*start*</span> <span>(<span><i><span>if</span></i> <span>(<span>gethash 'randomize options</span>)</span>
                  <span>(<span>random 256 <span>(<span>make-random-state t</span>)</span></span>)</span>
                  0</span>)</span>
        <span>*dark*</span> <span>(<span>gethash 'dark options</span>)</span></span>)</span></span>)</span></span></code></pre>

<p>Our <code>toplevel</code> function looks much like the one in the skeleton, but fleshed out
a bit more:</p>

<pre><code><span><span>(<span><i><span>defun</span></i> toplevel <span>(<span></span>)</span>
  <span>(<span>sb-ext:disable-debugger</span>)</span>
  <span>(<span>exit-on-ctrl-c
    <span>(<span>multiple-value-bind <span>(<span>arguments options</span>)</span> <span>(<span>adopt:parse-options-or-exit <span>*ui*</span></span>)</span>
      <span>(<span>when <span>(<span>gethash 'debug options</span>)</span>
        <span>(<span>sb-ext:enable-debugger</span>)</span></span>)</span>
      <span>(<span>handler-case
          <span>(<span><i><span>cond</span></i>
            <span>(<span><span>(<span>gethash 'help options</span>)</span> <span>(<span>adopt:print-help-and-exit <span>*ui*</span></span>)</span></span>)</span>
            <span>(<span><span>(<span>null arguments</span>)</span> <span>(<span>error 'missing-regex</span>)</span></span>)</span>
            <span>(<span>t <span>(<span>destructuring-bind <span>(<span>pattern . files</span>)</span> arguments
                 <span>(<span>configure options</span>)</span>
                 <span>(<span>run pattern files</span>)</span></span>)</span></span>)</span></span>)</span>
        <span>(<span>user-error <span>(<span>e</span>)</span> <span>(<span>adopt:print-error-and-exit e</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>

<p>This <code>toplevel</code> has a few extra bits beyond the skeletal example.</p>

<p>First, we disable the debugger immediately, and then re-enable it later if the
user asks us to.  We want to keep it disabled until <em>after</em> argument parsing
because we can't know whether the user wants it or not until we parse the
arguments.</p>

<p>Instead of just blindly running <code>run</code>, we check for <code>--help</code> and print it if
desired.  We also validate that the user passes the correct amount of arguments,
signaling a subtype of <code>user-error</code> if they don't.  Assuming everything looks
good we handle the configuration, call <code>run</code>, and that's it!</p>

<p>Running <code>make</code> generates <code>bin/batchcolor</code> and <code>man/man1/batchcolor.1</code>, and we
can view our log files in beautiful color.</p>

<h2 id="s17-more-information"><a href="#s17-more-information">More Information</a></h2>

<p>I hope this overview was helpful.  This has worked for me, but Common Lisp is
a flexible language, so if you want to use this layout as a starting point and
modify it for your own needs, go for it!</p>

<p>If you want to see some more examples you can find them in <a href="https://hg.stevelosh.com/dotfiles/file/tip/lisp">my dotfiles
repository</a>.  Some of the more
fun ones include:</p>

<ul>
<li><code>weather</code> for displaying the weather over the next few hours so I can tell if
  I need a jacket or umbrella before I go out for a walk.</li>
<li><code>retry</code> to retry shell commands if they fail, with options for how many times
  to retry, strategies for waiting/backing off on failure, etc.</li>
<li><code>pick</code> to interactively filter the output of one command into another
  (inspired by the <code>pick</code> program in "The UNIX Programming Environment" but with
  more options).</li>
</ul>

<p>The approach I laid out in this post works well for small, single-file programs.
If you're creating a larger program you'll probably want to move to a full ASDF
system in its own directory/repository.  My friend Ian <a href="http://atomized.org/blog/2020/07/06/common-lisp-in-practice/">wrote a post about
that</a> which you
might find interesting.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fighting API bots with Cloudflare's invisible turnstile (125 pts)]]></title>
            <link>https://www.troyhunt.com/fighting-api-bots-with-cloudflares-invisible-turnstile/</link>
            <guid>37400018</guid>
            <pubDate>Wed, 06 Sep 2023 00:58:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.troyhunt.com/fighting-api-bots-with-cloudflares-invisible-turnstile/">https://www.troyhunt.com/fighting-api-bots-with-cloudflares-invisible-turnstile/</a>, See on <a href="https://news.ycombinator.com/item?id=37400018">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
<p>There's a "hidden" API on HIBP. Well, it's not "hidden" insofar as it's easily discoverable if you watch the network traffic from the client, but it's not meant to be called directly, rather only via the web app. It's called "unified search" and it looks just like this:</p><figure><img src="https://www.troyhunt.com/content/images/2023/03/image-20.png" alt="" loading="lazy" width="1441" height="1031" srcset="https://www.troyhunt.com/content/images/size/w600/2023/03/image-20.png 600w, https://www.troyhunt.com/content/images/size/w1000/2023/03/image-20.png 1000w, https://www.troyhunt.com/content/images/2023/03/image-20.png 1441w" sizes="(min-width: 720px) 720px"></figure><p>It's been there in one form or another since day 1 (so almost a decade now), and it serves a sole purpose: to perform searches from the home page. That is all - <em>only from the home page</em>. It's called asynchronously from the client without needing to post back the entire page and by design, it's super fast and super easy to use. Which is bad. Sometimes.</p><p>To understand why it's bad we need to go back in time all the way to when <a href="https://www.troyhunt.com/have-i-been-pwned-you-can-now-ask-api/">I first launched the API that was intended to be consumed programmatically by other people's services</a>. That was easy, because it was basically just documenting the API that sat behind the home page of the website already, the predecessor to the one you see above. And then, unsurprisingly in retrospect, <a href="https://www.troyhunt.com/the-have-i-been-pwned-api-rate-limiting-and-commercial-use/">it started to be abused so I had to put a rate limit on it</a>. Problem is, that was a very rudimentary IP-based rate limit and it could be circumvented by someone with enough IPs, so fast forward a bit further and <a href="https://www.troyhunt.com/authentication-and-the-have-i-been-pwned-api/">I put auth on the API which required a nominal payment to access it</a>. At the same time, that unified search endpoint was created and home page searches updated to use that rather than the publicly documented API. So, 2 APIs with 2 different purposes.</p><p>The primary objective for putting a price on the public API was to tackle abuse. And it did - it stopped it <em>dead</em>. By attaching a rate limit to a key that required a credit card to purchase it, abusive practices (namely enumerating large numbers of email addresses) disappeared. This wasn't just about putting a <em>financial </em>cost to queries, it was about putting an <em>identity</em> cost to them; people are reluctant to start doing nasty things with a key traceable back to their own payment card! Which is why they turned their attention to the non-authenticated, non-documented unified search API.</p><p>Let's look at a 3 day period of requests to that API earlier this year, keeping in mind this should only ever be requested organically by humans performing searches from the home page:</p><figure><img src="https://www.troyhunt.com/content/images/2023/03/image-12.png" alt="" loading="lazy" width="1200" height="645" srcset="https://www.troyhunt.com/content/images/size/w600/2023/03/image-12.png 600w, https://www.troyhunt.com/content/images/size/w1000/2023/03/image-12.png 1000w, https://www.troyhunt.com/content/images/2023/03/image-12.png 1200w" sizes="(min-width: 720px) 720px"></figure><p>This is far from organic usage with requests peaking at 121.3k in just 5 minutes. Which poses an interesting question: <strong>how do you create an API that should only be consumed asynchronously from a web page and never programmatically via a script?</strong> You could chuck a CAPTCHA on the front page and require that be solved first but let's face it, that's not a pleasant user experience. Rate limit requests by IP? See the earlier problem with that. Block UA strings? Pointless, because they're easily randomised. Rate limit an <a href="https://www.cloudflare.com/learning/network-layer/what-is-an-autonomous-system/?ref=troyhunt.com">ASN</a>? It gets you part way there, but what happens when you get a genuine flood of traffic because the site has hit the mainstream news? <a href="https://www.troyhunt.com/brief-lessons-on-handling-huge-traffic-spikes/">It happens</a>.</p><p>Over the years, I've played with all sorts of combinations of firewall rules based on parameters such as geolocations with incommensurate numbers of requests to their populations, <a href="https://developers.cloudflare.com/bots/concepts/ja3-fingerprint/?ref=troyhunt.com">JA3 fingerprints</a> and, of course, the parameters mentioned above. Based on the chart above these obviously didn't catch all the abusive traffic, but they did catch a significant portion of it:</p><figure><img src="https://www.troyhunt.com/content/images/2023/03/image-13.png" alt="" loading="lazy" width="1075" height="756" srcset="https://www.troyhunt.com/content/images/size/w600/2023/03/image-13.png 600w, https://www.troyhunt.com/content/images/size/w1000/2023/03/image-13.png 1000w, https://www.troyhunt.com/content/images/2023/03/image-13.png 1075w" sizes="(min-width: 720px) 720px"></figure><p>If you combine it with the previous graph, that's about a third of all the bad traffic in that period or in other words, two thirds of the bad traffic was still getting through. There had to be a better way, which brings us to <a href="https://developers.cloudflare.com/turnstile/?ref=troyhunt.com">Cloudflare's Turnstile</a>:</p><blockquote>With Turnstile, we adapt the actual challenge outcome to the individual visitor or browser. First, we run a series of small non-interactive JavaScript challenges gathering more signals about the visitor/browser environment. Those challenges include, proof-of-work, proof-of-space, probing for web APIs, and various other challenges for detecting browser-quirks and human behavior. As a result, we can fine-tune the difficulty of the challenge to the specific request and avoid ever showing a visual puzzle to a user.</blockquote><p>"Avoid ever showing a visual puzzle to a user" is a polite way of saying they avoid the sucky UX of CAPTCHA. Instead, Turnstile offers the ability to issue a "non-interactive challenge" which implements the sorts of clever techniques mentioned above and as it relates to this blog post, that can be <strong>an invisible non-interactive challenge</strong>. This is one of <a href="https://developers.cloudflare.com/turnstile/reference/widget-types/?ref=troyhunt.com">3 different widget types</a> with the others being <strong>a visible non-interactive challenge</strong> and a <strong>non-<em>intrusive</em> interactive challenge</strong>. For my purposes on HIBP, I wanted a zero-friction implementation nobody saw, hence the invisible approach. Here's how it works:</p><figure><img src="https://www.troyhunt.com/content/images/2023/08/turnstile-overview_hu857217e6cfe3055a024af7c1505ed0dc_210985_3757x2700_resize_q75_box_3-3bb896c3.png" alt="" loading="lazy" width="2000" height="1437" srcset="https://www.troyhunt.com/content/images/size/w600/2023/08/turnstile-overview_hu857217e6cfe3055a024af7c1505ed0dc_210985_3757x2700_resize_q75_box_3-3bb896c3.png 600w, https://www.troyhunt.com/content/images/size/w1000/2023/08/turnstile-overview_hu857217e6cfe3055a024af7c1505ed0dc_210985_3757x2700_resize_q75_box_3-3bb896c3.png 1000w, https://www.troyhunt.com/content/images/size/w1600/2023/08/turnstile-overview_hu857217e6cfe3055a024af7c1505ed0dc_210985_3757x2700_resize_q75_box_3-3bb896c3.png 1600w, https://www.troyhunt.com/content/images/size/w2400/2023/08/turnstile-overview_hu857217e6cfe3055a024af7c1505ed0dc_210985_3757x2700_resize_q75_box_3-3bb896c3.png 2400w"></figure><p>Get it? Ok, let's break it down further as it relates to HIBP, starting with when the front page first loads and it embeds the Turnstile widget from Cloudflare:</p><pre><code>&lt;script src="https://challenges.cloudflare.com/turnstile/v0/api.js" async defer&gt;&lt;/script&gt;</code></pre><p>The widget takes responsibility for running the non-interactive challenge and returning a token. This needs to be persisted somewhere on the client side which brings us to <a href="https://developers.cloudflare.com/turnstile/get-started/client-side-rendering/?ref=troyhunt.com">embedding the widget</a>:</p><pre><code>&lt;div&nbsp;ID="turnstileWidget"&nbsp;class="cf-turnstile"&nbsp;data-sitekey="0x4AAAAAAADY3UwkmqCvH8VR"&nbsp;data-callback="turnstileCompleted"&gt;&lt;/div&gt;</code></pre><p>Per the docs in that link, the main thing here is to have an element with the "cf-turnstile" class set on it. If you happen to go take a look at the HIBP HTML source right now, you'll see that element precisely as it appears in the code block above. However, check it out in your browser's dev tools so you can see how it renders in the DOM and it will look more like this:</p><figure><img src="https://www.troyhunt.com/content/images/2023/08/image-4.png" alt="" loading="lazy" width="968" height="204" srcset="https://www.troyhunt.com/content/images/size/w600/2023/08/image-4.png 600w, https://www.troyhunt.com/content/images/2023/08/image-4.png 968w"></figure><p>Expand that DIV tag and you'll find a whole bunch more content set as a result of loading the widget, but that's not relevant right now. What's important is the data-token attribute because that's what's going to prove you're not a bot when you run the search. How you implement this from here is up to you, but what HIBP does is picks up the token and sets it in the "cf-turnstile-response" header then sends it along with the request when that unified search endpoint is called:</p><figure><img src="https://www.troyhunt.com/content/images/2023/08/image-6.png" alt="" loading="lazy" width="948" height="582" srcset="https://www.troyhunt.com/content/images/size/w600/2023/08/image-6.png 600w, https://www.troyhunt.com/content/images/2023/08/image-6.png 948w"></figure><p>So, at this point we've issued a challenge, the browser has solved the challenge and received a token back, now that token has been sent along with the request for the actual resource the user wanted, in this case the unified search endpoint. The final step is to validate the token and for this I'm using a Cloudflare worker. <a href="https://www.troyhunt.com/tag/cloudflare/">I've written a lot about workers in the past</a> so here's the short pitch: it's code that runs in each one of Cloudflare's 300+ edge nodes around the world and can inspect and modify requests and responses on the fly. I already had a worker to do some other processing on unified search requests, so I just added the following:</p><pre><code>const token = request.headers.get('cf-turnstile-response');

if (token === null) {
    return new Response('Missing Turnstile token', { status: 401 });
}

const ip = request.headers.get('CF-Connecting-IP');

let formData = new FormData();
formData.append('secret', '[secret key goes here]');
formData.append('response', token);
formData.append('remoteip', ip);

const turnstileUrl = 'https://challenges.cloudflare.com/turnstile/v0/siteverify';
const result = await fetch(turnstileUrl, {
    body: formData,
    method: 'POST',
});
const outcome = await result.json();

if (!outcome.success) {
    return new Response('Invalid Turnstile token', { status: 401 });
}</code></pre><p>That should be pretty self-explanatory and you can find the docs for this on <a href="https://developers.cloudflare.com/turnstile/get-started/server-side-validation/?ref=troyhunt.com">Cloudflare's server-side validation page</a> which goes into more detail, but in essence, it does the following:</p><ol><li>Gets the token from the request header and rejects the request if it doesn't exist</li><li>Sends the token, your secret key and the user's IP along to Turnstile's "siteverify" endpoint</li><li>If the token is not successfully verified then return 401 "Unauthorised", otherwise continue with the request</li></ol><p>And because this is all done in a Cloudflare worker, any of those 401 responses never even touch the origin. Not only do I not need to process the request in Azure, the person attempting to abuse my API gets a nice speedy response directly from an edge node near them üôÇ</p><p>So, what does this mean for bots? If there's no token then they get booted out right away. If there's a token but it's not valid then they get booted out at the end. But can't they just take a previously generated token and use that? Well, yes, but only once:</p><blockquote>If the same response is presented twice, the second and each subsequent request will generate an error stating that the response has already been consumed.</blockquote><p>And remember, a real browser had to generate that token in the first place so it's not like you can just automate the process of token generation then throw it at the API above. (Sidenote: that server-side validation link includes how to handle idempotency, for example when retrying failed requests.) But what if a <em>real human</em> fails the verification? That's entirely up to you but in HIBP's case, that 401 response causes a fallback to a full page post back which then implements other controls, for example an interactive challenge.</p><p>Time for graphs and stats, starting with the one in the hero image of this page where we can see the number of times Turnstile was issued and how many times it was solved over the week prior to publishing this post:</p><figure><img src="https://www.troyhunt.com/content/images/2023/08/2023-08-21_16-38-03.png"></figure><p>That's a 91% hit rate of solved challenges which is great. That remaining 9% is either humans with a false positive or... bots getting rejected üòé </p><p>More graphs, this time how many requests to the unified search page were rejected by Turnstile:</p><figure><img src="https://www.troyhunt.com/content/images/2023/08/image-9.png" alt="" loading="lazy" width="1110" height="834" srcset="https://www.troyhunt.com/content/images/size/w600/2023/08/image-9.png 600w, https://www.troyhunt.com/content/images/size/w1000/2023/08/image-9.png 1000w, https://www.troyhunt.com/content/images/2023/08/image-9.png 1110w"></figure><p>That 990k number doesn't marry up with the 476k unsolved ones from before because they're 2 different things: the unsolved challenges are when the Turnstile widget is loaded but not solved (hopefully due to it being a bot rather than a false positive), whereas the 401 responses to the API is when a successful (and previously unused) Turnstile token isn't in the header. This could be because the token wasn't present, wasn't solved or had already been used. You get more of a sense of how many of these rejected requests were legit humans when you drill down into attributes like <a href="https://developers.cloudflare.com/bots/concepts/ja3-fingerprint/?ref=troyhunt.com">the JA3 fingerprints</a>:</p><figure><img src="https://www.troyhunt.com/content/images/2023/08/image-10.png" alt="" loading="lazy" width="540" height="238"></figure><p>In other words, of those 990k failed requests, almost 40% of them were from the same 5 clients. Seems legit ü§î</p><p>And about a third were from clients with an identical UA string:</p><figure><img src="https://www.troyhunt.com/content/images/2023/08/image-11.png" alt="" loading="lazy" width="538" height="238"></figure><p>And so on and so forth. The point being that the number of actual legitimate requests from end users that were inconvenienced by Turnstile would be exceptionally small, almost certainly a very low single-digit percentage. I'll never know exactly because bots obviously attempt to emulate legit clients and sometimes legit clients look like bots and if we could easily solve this problem then we wouldn't need Turnstile in the first place! Anecdotally, that very small false positive number stacks up as people tend to complain pretty quickly when something isn't optimal, and I implemented this all the way back in March. Yep, 5 months ago, and I've waited this long to write about it just to be confident it's actually working. Over 100M Turnstile challenges later, I'm confident it is - I've not seen a single instance of abnormal traffic spikes to the unified search endpoint since rolling this out. What I did see initially though is a lot of this sort of thing:</p><figure><img src="https://www.troyhunt.com/content/images/2023/03/image-16.png" alt="" loading="lazy" width="865" height="838" srcset="https://www.troyhunt.com/content/images/size/w600/2023/03/image-16.png 600w, https://www.troyhunt.com/content/images/2023/03/image-16.png 865w" sizes="(min-width: 720px) 720px"></figure><p>By now it should be pretty obvious what's going on here, and it should be equally obvious that it didn't work out real well for them üòä</p><p>The bot problem is a hard one for those of us building services because we're continually torn in different directions. We want to build a slick UX for humans but an obtrusive one for bots. We want services to be easily consumable, but only in the way we intend them to... which might be by the good bots playing by the rules!</p><p>I don't know exactly what Cloudflare is doing in that challenge and I'll be honest, I don't even know what a "proof-of-space" is. But the point of using a service like this is that <em>I don't need to know!</em> What I do know is that Cloudflare sees about 20% of the internet's traffic and because of that, they're in an unrivalled position to look at a request and make a determination on its legitimacy.</p><p>If you're in my shoes, <a href="https://developers.cloudflare.com/turnstile/?ref=troyhunt.com">go and give Turnstile a go</a>. And if you want to consume data from HIBP, go and check out <a href="https://haveibeenpwned.com/API/v3?ref=troyhunt.com">the official API docs</a>, the uh, unified search doesn't work real well for you any more üòé</p>
<section>
<a href="https://www.troyhunt.com/tag/cloudflare/">Cloudflare</a>
<a href="https://www.troyhunt.com/tag/have-i-been-pwned-3f/">Have I Been Pwned</a>
</section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Can LLMs learn from a single example? (373 pts)]]></title>
            <link>https://www.fast.ai/posts/2023-09-04-learning-jumps/</link>
            <guid>37399873</guid>
            <pubDate>Wed, 06 Sep 2023 00:40:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fast.ai/posts/2023-09-04-learning-jumps/">https://www.fast.ai/posts/2023-09-04-learning-jumps/</a>, See on <a href="https://news.ycombinator.com/item?id=37399873">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-content">




<main id="quarto-document-content">
<blockquote>
<p>Summary: recently while fine-tuning a large language model (LLM) on multiple-choice science exam questions, we observed some highly unusual training loss curves. In particular, it appeared the model was able to rapidly memorize examples from the dataset after seeing them just once. This astonishing feat contradicts most prior wisdom about neural network sample efficiency. Intrigued by this result, we conducted a series of experiments to validate and better understand this phenomenon. It‚Äôs early days, but the experiments support the hypothesis that the models are able to rapidly remember inputs. This might mean we have to re-think how we train and use LLMs.</p>
</blockquote>
<section id="how-neural-networks-learn">
<h2 data-anchor-id="how-neural-networks-learn">How neural networks learn</h2>
<p>We train neural network classifiers by showing them examples of inputs and outputs, and they learn to predict outputs based on inputs. For example, we show examples of pictures of dogs and cats, along with the breed of each, and they learn to guess the breed from the image. To be more precise, for a list of possible breeds, they output their guess as to the probability of each breed. If it‚Äôs unsure, it will guess a roughly equal probability of each possible breed, and if it‚Äôs highly confident, it will guess a nearly 1.0 probability of its predicted breed.</p>
<p>The training process consists of every image in a <em>training set</em> being shown to the network, along with the correct label. A pass through all the input data is called an ‚Äúepoch‚Äù. We have to provide many examples of the training data for the model to learn effectively.</p>
<p>During training the neural network attempts to reduce the <em>loss</em>, which is (roughly speaking) a measure of how often the model is wrong, with highly confident wrong predictions penalised the most, and vise versa. We calculate the loss after each batch for the training set, and from time to time (often at the end of each epoch) we also calculated the loss for a bunch of inputs the model does <em>not</em> get to learn from ‚Äì this is the ‚Äúvalidation set‚Äù. Here‚Äôs what that looks like in practice when we train for 11 epochs:</p>
<div>
<figure>
<p><img src="https://www.fast.ai/posts/2023-09-04-learning-jumps/2023-09-04-09-00-35.png" width="350"></p>
<figcaption>Loss chart from training on pet breeds</figcaption>
</figure>
</div>
<p>As you see, the training loss gradually (and bumpily) improves relatively quickly, slowing down over time, and the validation loss improves more slowly (and would eventually flatten out entirely, and then eventually get worse, if trained for longer).</p>
<p>You can‚Äôt see from the chart where epochs start and stop, because it takes many epochs before a model learns what any particular image looks like. This has been a fundamental constraint of neural networks throughout the decades they‚Äôve been developed ‚Äì they take an awfully long time to learn anything! It‚Äôs actually an <a href="https://www.sciencedirect.com/science/article/pii/S1364661323002036">area of active research</a> about why neural nets are so ‚Äúsample inefficient‚Äù, especially compared to how children learn.</p>
</section>
<section id="a-very-odd-loss-curve">
<h2 data-anchor-id="a-very-odd-loss-curve">A very odd loss curve</h2>
<p>We have recently been working on the <a href="https://www.kaggle.com/competitions/kaggle-llm-science-exam">Kaggle LLM Science Exam</a> competition, which ‚Äúchallenges participants to answer difficult science-based questions written by a Large Language Model‚Äù. For instance, here‚Äôs the first question:</p>
<div>
<p><em>Which of the following statements accurately describes the impact of Modified Newtonian Dynamics (MOND) on the observed ‚Äúmissing baryonic mass‚Äù discrepancy in galaxy clusters?</em></p>
<ol type="A">
<li>MOND is a theory that reduces the observed missing baryonic mass in galaxy clusters by postulating the existence of a new form of matter called ‚Äúfuzzy dark matter.‚Äù</li>
<li>MOND is a theory that increases the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 20.</li>
<li>MOND is a theory that explains the missing baryonic mass in galaxy clusters that was previously considered dark matter by demonstrating that the mass is in the form of neutrinos and axions.</li>
<li>MOND is a theory that reduces the discrepancy between the observed missing baryonic mass in galaxy clusters and the measured velocity dispersions from a factor of around 10 to a factor of about 2.</li>
<li>MOND is a theory that eliminates the observed missing baryonic mass in galaxy clusters by imposing a new mathematical formulation of gravity that does not require the existence of dark matter.</li>
</ol>
</div>
<p>For those playing along at home, the correct answer, apparently, is D.</p>
<p>Thankfully, we don‚Äôt have to rely on our knowledge of Modified Newtonian Dynamics to answer these questions ‚Äì instead, we are tasked to train a model to answer these questions. When we submit our model to Kaggle, it will be tested against thousands of ‚Äúheld out‚Äù questions that we don‚Äôt get to see.</p>
<p>We trained our model for 3 epochs on a <a href="https://www.kaggle.com/code/radek1/new-dataset-deberta-v3-large-training">big dataset of questions</a> created by our friend Radek Osmulski, and saw the following most unexpected training loss curve:</p>
<div>
<figure>
<p><img src="https://www.fast.ai/posts/2023-09-04-learning-jumps/2023-09-04-09-33-39.png" width="500"></p>
<figcaption>Loss chart from 3 epoch training on Kaggle comp</figcaption>
</figure>
</div>
<p>The problem here is that you can clearly see the end of each epoch - there‚Äôs a sudden downwards jump in loss. We‚Äôve seen similar loss curves before, and they‚Äôve always been due to a bug. For instance, it‚Äôs easy to accidentally have the model continue to learn when evaluating the validation set ‚Äì such that after validation the model suddenly appears much better. So we set out to look for the bug in our training process. We were using Hugging Face‚Äôs <code>Trainer</code>, so we guessed there must be a bug in that.</p>
<p>Whilst we began stepping through the code, we also asked fellow open source developers on the <a href="https://discord.gg/k36qjUxyJC">Alignment Lab AI Discord</a> if they‚Äôve seen similar odd training curves, and pretty much everyone said ‚Äúyes‚Äù. But everyone who responded was using Trainer as well, which seemed to support our theory of a bug in that library.</p>
<p>But then <span data-cites="anton">@anton</span> on Discord told us he was seeing this curve with his own simple custom training loop:</p>
<div>
<figure>
<p><img src="https://www.fast.ai/posts/2023-09-04-learning-jumps/2023-09-04-09-39-06.png" width="500"></p>
<figcaption>Anton‚Äôs custom loop training loss chart</figcaption>
</figure>
</div>
<p>‚Ä¶and he also showed us this accompanying extremely surprising validation loss curve:</p>
<div>
<figure>
<p><img src="https://www.fast.ai/posts/2023-09-04-learning-jumps/2023-09-04-09-41-02.png" width="500"></p>
<figcaption>Anton‚Äôs custom loop validation loss chart</figcaption>
</figure>
</div>
<p>Then we started hearing from more and more Discord friends that they had seen similar strange behavior, including when not using Trainer. We wondered if it was some oddity specific to the <a href="https://arxiv.org/abs/2106.09685v2">LoRA</a> approach we were using, but we heard from folks seeing the same pattern when doing full fine-tuning too. In fact, it was basically common knowledge in the LLM fine-tuning community that this is just how things go when you‚Äôre doing this kind of work!‚Ä¶</p>
</section>
<section id="digging-deeper">
<h2 data-anchor-id="digging-deeper">Digging deeper</h2>
<p>The hypothesis that we kept hearing from open source colleagues is that that these training curves were actually showing overfitting. This seemed, at first, quite impossible. It would imply that the model was learning to recognise inputs from just one or two examples. If you look back at that first curve we showed, you can see the loss diving from 0.8 to 0.5 after the first epoch, and then from 0.5 to under 0.2 after the second. Furthermore, <em>during</em> each of the second and third epochs it wasn‚Äôt really learning anything new at all. So, other than its initial learning during the beginning of the first epoch, nearly all the apparent learning was (according to this theory) memorization of the training set occurring with only 3 examples per row! Furthermore, for each question, it only gets a tiny amount of signal: how its guess as to the answer compared to the true label.</p>
<p>We tried out an experiment ‚Äì we trained our Kaggle model for two epochs, using the following learning rate schedule:</p>
<div>
<figure>
<p><img src="https://www.fast.ai/posts/2023-09-04-learning-jumps/2023-09-04-10-05-56.png" width="500"></p>
<figcaption>Learning rate schedule</figcaption>
</figure>
</div>
<p>Nowadays this kind of schedule is not that common, but it‚Äôs an approach that saw a lot of success after it was created by Leslie Smith, who discussed it in his 2015 paper <a href="https://arxiv.org/abs/1506.01186">Cyclical Learning Rates for Training Neural Networks</a>.</p>
<p>And here‚Äôs the crazy-looking training and validation loss curves we saw as a result:</p>
<div>
<figure>
<p><img src="https://www.fast.ai/posts/2023-09-04-learning-jumps/2023-09-04-10-09-29.png" width="500"></p>
<figcaption>Result of 2-epoch CLR experiment</figcaption>
</figure>
</div>
<p>The only thing that we have come up with (so far!) that fully explains this picture is that the hypothesis is correct: the model is rapidly learning to recognise examples even just seeing them once. Let‚Äôs work through each part of the loss curve in turn‚Ä¶</p>
<p>Looking at the first epoch, this looks like a very standard loss curve. We have the learning rate warming up over the first 10% of the epoch, and then gradually decreasing following a cosine schedule. Once the LR comes up to temperature, the training and validation loss rapidly decrease, and then they both slow down as the LR decreases and the ‚Äúquick wins‚Äù are captured.</p>
<p>The second epoch is where it gets interested. We‚Äôre not re-shuffling the dataset at the start of the epoch, so those first batches of the second epoch are when the learning rate was still warming up. That‚Äôs why we don‚Äôt see an immediate step-change like we did from epoch 2 to 3 in the very first loss curve we showed ‚Äì these batches were only seen when the LR was low, so it couldn‚Äôt learn much.</p>
<p>Towards the end of that first 10% of the epoch, the training loss plummets, because the LR was high when these batches were seen during the first epoch, and the model has learned what they look like. The model quickly learns that it can very confidentally guess the correct answer.</p>
<p>But during this time, validation loss suffers. That‚Äôs because although the model is getting very confident, it‚Äôs not actually getting any better at making predictions. It has simply memorised the dataset, but isn‚Äôt improving at generalizing. Over-confident predictions cause validation loss to get worse, because the loss function penalizes more confident errors higher.</p>
<p>The end of the curve is where things get particularly interesting. The training loss starts getting worse ‚Äì and that really never ought to happen! In fact, neither of us remember ever seeing such a thing before when using a reasonable LR.</p>
<p>But actually, this makes perfect sense under the memorization hypothesis: these are the batches that the model saw at a time when the LR had come back down again, so it wasn‚Äôt able to memorize them as effectively. But the model is still over-confident, because it has just got a whole bunch of batches nearly perfectly correct, and hasn‚Äôt yet adjusted to the fact that it‚Äôs now seeing batches that it didn‚Äôt have a chance to learn so well.</p>
<p>It gradually recalibrates to a more reasonable level of confidence, but it takes a while, because the LR is getting lower and lower. As it recalibrates, the validation loss comes back down again.</p>
<p>For our next experiment, we tried <a href="https://arxiv.org/abs/1803.09820">1cycle training</a> over 3 epochs, instead of CLR ‚Äì that is, we did a single LR warmup for 10% of batches at the start of training, and then decayed the LR over the remaining batches following a cosine schedule. Previously, we did a separate warmup and decay cycle for each epoch. Also, we increased the LoRA rank, resulting in slower learning. Here‚Äôs the resulting loss curve:</p>
<div>
<figure>
<p><img src="https://www.fast.ai/posts/2023-09-04-learning-jumps/2023-09-06-06-32-18.png" width="500"></p>
<figcaption>1cycle training over 3 epochs</figcaption>
</figure>
</div>
<p>The shape largely follows what we‚Äôd expect, based on the previous discussion, except for one thing: the validation loss does not jump up at epoch 2 ‚Äì it‚Äôs not until epoch 3 that we see that jump. However previously the training loss was around 0.2 by the 2nd epoch, which is only possible when it‚Äôs making highly confident predictions. In the 1cycle example it doesn‚Äôt make such confident predictions until the third epoch, and we don‚Äôt see the jump in validation loss until that happens.</p>
<p>It‚Äôs important to note that the validation <em>loss</em> getting worse doesn‚Äôt mean that we‚Äôre over-fitting in practice. What we generally care about is <em>accuracy</em>, and it‚Äôs fine if the model is over-confident. In the Kaggle competition the metric used for the leaderboard is <a href="https://www.fast.ai/posts/2023-09-04-learning-jumps/www.kaggle.com/competitions/kaggle-llm-science-exam/overview/evaluation">Mean Average Precision @ 3 (MAP@3)</a>, which is the accuracy of the ranked top-3 multiple-choice predictions made my the model. Here‚Äôs the validation <em>accuracy</em> per batch of the 1cycle training run shown in the previous chart ‚Äì as you see, it keeps improving, even although the validation <em>loss</em> got worse in the last epoch:</p>
<div>
<figure>
<p><img src="https://www.fast.ai/posts/2023-09-04-learning-jumps/2023-09-06-06-46-17.png" width="500"></p>
<figcaption>MAP@3 for 1cycle training</figcaption>
</figure>
</div>
<p>If you‚Äôre interested in diving deeper, take a look at <a href="https://wandb.ai/johnowhitaker/llmemo/reports/Can-LLMs-learn-from-a-single-example---Vmlldzo1MjQ2MDYy">this report</a> where Johno shares logs from some additional examples, along with a notebook for those who‚Äôd like to see this effect in action for themselves.</p>
</section>
<section id="how-could-the-memorization-hypothesis-be-true">
<h2 data-anchor-id="how-could-the-memorization-hypothesis-be-true">How could the memorization hypothesis be true?</h2>
<p>There is no fundamental law that says that neural networks can‚Äôt learn to recognise inputs from a single example. It‚Äôs just what researchers and practitioners have generally found to be the case in practice. It takes a lot of examples because the loss surfaces that we‚Äôre trying to navigate using stochastic gradient descent (SGD) are too bumpy to be able to jump far at once. We do know, however, that some things can make loss surfaces smoother, such as using residual connections, as shown in the classic <a href="https://arxiv.org/abs/1712.09913">Visualizing the Loss Landscape of Neural Nets</a> paper (Li et al, 2018).</p>
<div>
<figure>
<p><img src="https://www.fast.ai/posts/2023-09-04-learning-jumps/2023-09-04-12-55-45.png" width="500"></p>
<figcaption>Loss surfaces of a ResNet-56 (Li et al, 2018)</figcaption>
</figure>
</div>
<p>It could well be the case that pre-trained large language models have extremely smooth loss surfaces in areas close to the minimal loss, and that a lot of the fine-tuning work done in the open source community is in this area. This is based on the underlying premise surrounding the original development of fine-tuned universal language models. These models were first documented in the <a href="https://aclanthology.org/P18-1031/">ULMFiT paper</a> back in 2018 by one of us (Jeremy) and Sebastian Ruder. The reason Jeremy originally built the ULMFiT algorithm is because it seemed necessary that any model that could do a good job of language modeling (that is, predicting the next word of a sentence) would have to build a rich hierarchy of abstractions and capabilities internally. Furthermore, Jeremy believed that this hierarchy could then be easily adapted to solve other tasks requiring similar capabilities using a small amount of fine-tuning. The ULMFiT paper demonstrated for the first time that this is indeed exactly what happens.</p>
<p>Large language models, which today are orders of magnitude bigger than those studied in ULMFiT, must have an even richer hierarchy of abstractions. So fine-tuning one of these models to, for instance, answer multiple-choice questions about science, can largely harness capabilities and knowledge that is already available in the model. It‚Äôs just a case of surfacing the right pieces in the right way. These should not require many weights to be adjusted very much.</p>
<p>Based on this, it‚Äôs perhaps not surprising to think that a pre-trained language model with a small random classification head could be in a part of the weight space where the loss surface smoothly and clearly points exactly in the direction of a good weight configuration. And when using the Adam optimiser (as we did), having a consistent and smooth gradient results in effective dynamic learning rate going up and up, such that steps can get very big.</p>
</section>
<section id="what-now">
<h2 data-anchor-id="what-now">What now?</h2>
<p>Having a model that learns really fast sounds great ‚Äì but actually it means that a lot of basic ideas around how to train models may be turned on their head! When models train very slowly, we can train them for a long time, using a wide variety of data, for multiple epochs, and we can expect that our model will gradually pull out generalisable information from the data we give it.</p>
<p>But when models learn this fast, the <a href="https://arxiv.org/abs/1312.6211">catastrophic forgetting</a> problem may suddenly become far more pronounced. For instance, if a model sees ten examples of a very common relationship, and then one example of a less common counter-example, it may well remember the counter-example instead of just slightly downweighting its memory of the original ten examples.</p>
<p>It may also be the case now that <a href="https://arxiv.org/abs/1904.12848">data augmentation</a> is now less useful for avoiding over-fitting. Since LLMs are so effective at pulling out representations of the information they‚Äôre given, mixing things up by paraphrasing and back-translation may now not make much of a difference. The model would be effectively getting the same information either way.</p>
<p>Perhaps we can mitigate these challenges by greatly increasing our use of techniques such as <a href="https://dl.acm.org/doi/10.5555/2627435.2670313">dropout</a> (which is already used a little in fine-tuning techniques such as LoRA) or <a href="https://arxiv.org/abs/1603.09382">stochastic depth</a> (which does not seem to have been used in NLP to any significant extent yet).</p>
<p>Alternatively, maybe we just need to be careful to use rich mixtures of datasets throughout training, so that our models never have a chance to forget. Although Llama Code, for instance, did suffer from catastrophic forgetting (as it got better at code, it got much worse at everything else), it was fine-tuned with only 10% of non-code data. Perhaps with something closer to a 50/50 mix it would have been possible to get just as good at coding, without losing its existing capabilities.</p>
<p>If you come up with any alternative hypotheses, and are able to test them, or if you find any empirical evidence that the memorization hypothesis is wrong, please do let us know! We‚Äôre also keen to hear about other work in this space (and apologies if we failed to reference any prior work here), and any ideas about how (if at all) we should adjust how we train and use these models based on these observations. We‚Äôll be keeping an eye on replies to <a href="https://twitter.com/jeremyphoward/status/1699216721473880425">this twitter thread</a>, so please respond there if you have any thoughts or questions.</p>
</section>
</main> 

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Interoperability can save the open web (190 pts)]]></title>
            <link>https://spectrum.ieee.org/doctorow-interoperability</link>
            <guid>37399799</guid>
            <pubDate>Wed, 06 Sep 2023 00:32:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/doctorow-interoperability">https://spectrum.ieee.org/doctorow-interoperability</a>, See on <a href="https://news.ycombinator.com/item?id=37399799">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-elid="2664563919" data-post-url="https://spectrum.ieee.org/doctorow-interoperability" data-authors="Michael Nolan" data-headline="Cory Doctorow: Interoperability Can Save the Open Web" data-page-title="Cory Doctorow: Interoperability Can Save the Open Web - IEEE Spectrum"><p>In his new book <a href="https://www.versobooks.com/products/3035-the-internet-con" rel="noopener noreferrer" target="_blank"><u><em>The Internet Con: How to Seize the Means of Computation</em></u></a>, author <a href="https://www.eff.org/about/staff/cory-doctorow" rel="noopener noreferrer" target="_blank"><u>Cory Doctorow</u></a> presents a strong case for disrupting Big Tech. While the dominance of Internet platforms like <a href="https://spectrum.ieee.org/tag/twitter">Twitter</a>, <a href="https://spectrum.ieee.org/tag/facebook">Facebook</a>, Instagram, or <a href="https://spectrum.ieee.org/tag/amazon">Amazon</a> is often taken for granted, Doctorow argues that these walled gardens are fenced in by legal structures, not feats of engineering. Doctorow proposes forcing interoperability‚Äîany given platform‚Äôs ability to interact with another‚Äîas a way to break down those walls and to make the Internet freer and more democratic.</p><p><em><a href="https://spectrum.ieee.org/">IEEE Spectrum</a></em> contributor Michael Nolan spoke with Doctorow about his new book and how interoperability could break up monopolies both in tech and beyond.</p><p><strong>Your new book, <em>The Internet Con</em>, as you write in its acknowledgements, ‚Äúcrystallizes two decades‚Äô worth of advocacy writing about and working on issues in digital human rights.‚Äù How did that come to take the form of an argument for interoperability as a way to break up Big Tech monopolies?</strong></p><p data-rm-resized-container="25%"><img alt="Black and white portrait of a smiling bespectacled man." data-rm-shortcode-id="083c69fef209bca45afec51763f356bb" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/black-and-white-portrait-of-a-smiling-bespectacled-man.jpg?id=36341066&amp;width=980" height="1240" id="9f28a" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/black-and-white-portrait-of-a-smiling-bespectacled-man.jpg?id=36341066&amp;width=980" width="1240"><small placeholder="Add Photo Caption...">Cory Doctorow</small><small placeholder="Add Photo Credit..."><a href="http://jonathanworth.com/" target="_blank">Jonathan Worth</a></small></p><p><strong>Cory Doctorow:</strong> Over the decades that I‚Äôve been involved in technology, the entities that are on the user‚Äôs side have really changed. Sometimes it was tech platforms or companies and sometimes it wasn‚Äôt. Sometimes it was governments and sometimes it wasn‚Äôt. Having started off defending tech companies that really did have their users‚Äô backs from entertainment companies, I realized that the distinction between them was not that one industry was made up of entertainment executives whose commitment to human rights was very thin and the other was made up of tech executives who had a more good-faith commitment. When a sector is extremely concentrated, the people who are willing to trade the public good and foundational democratic values for incremental increases in their employer‚Äôs profitability get a hearing within the company and take over the company‚Äôs decision making. When a business doesn‚Äôt have to worry about losing its customers due to abusing them, then the people arguing, ‚ÄúWe shouldn‚Äôt do this because it‚Äôs wrong and <em>also</em> it‚Äôs bad for business‚Äù can only argue, ‚ÄúWe shouldn‚Äôt do this because it‚Äôs wrong.‚Äù They have to grudgingly admit that it might be good for business. Any firm in that state eventually becomes a serious hazard to human rights. It‚Äôs this ‚Äúcurse of bigness,‚Äù as <a href="https://en.wikipedia.org/wiki/Louis_Brandeis" target="_blank">Brandeis</a> called it, that we should really be attuned to and is really pernicious.</p><p><strong>That leads to the main proposal of the book, which is that reaffirming interoperability between systems and platforms can break apart these very large companies. Can you define interoperability?</strong></p><p><strong>Doctorow:</strong> At its root, it‚Äôs just the ability to use one thing with something else. Use any ink in your printer with any paper, use any socks with your shoes, anyone‚Äôs gasoline in your car, put any lightbulb in your light socket. There‚Äôs voluntary, mandatory interoperability, where a group of stakeholders get together and they say, ‚ÄúThis is the goal we want all of our products to achieve, and we are going to design a framework so that we can make sure that every lightbulb lights up when you stick it in a light socket.‚Äù Then there‚Äôs the stuff where they‚Äôre indifferent: Car companies don‚Äôt stop you from putting a little cigarette-lighter-to-USB adapter into your car. </p><p>Companies can grow very quickly because tech has got these great network effects, but they also have, because of interoperability, really low switching costs.<strong>‚ÄîCory Doctorow</strong></p><p>Then there‚Äôs the third kind of interop, the kind of chewy, interesting, lots-of-rich-Internet-history interop, which is adversarial interoperability, which in the book we call ‚Äúcomcom,‚Äù short for competitive compatibility. It‚Äôs the interop that‚Äôs done against the wishes of the original equipment manufacturer: scraping, reverse engineering, bots, all of that gnarly stuff done in the face of active hostility. This would be like Apple reverse-engineering <a href="https://spectrum.ieee.org/tag/microsoft">Microsoft</a> Office and making the iWork suite‚ÄîPages, Numbers, and Keynote‚Äîso that anyone with a Mac could read any Windows-based office file without having to buy any software from Microsoft. </p><p>There are so many examples of this from technology‚Äôs history. It‚Äôs really the engine of technology. It‚Äôs the reason we‚Äôve had this incredible boom-and-bust cycle within tech companies. Companies can grow very quickly because tech has got these great network effects, but they also have, because of interoperability, really low switching costs. Anytime a platform isn‚Äôt suited to you or someone has a better idea for it, they can just make a tool [that] kind of greases the skids for you to leave the last one and to jump to the new one. Because of that interoperability, the companies that grow really big become irresistible targets for other companies that want to come in, tempt their users away, and offer them a very easy path to get from the old place to the new one. I think interoperability being interrupted explains ‚ÄúWhy are tech companies bad when they used to be good?‚Äù It‚Äôs because the users can‚Äôt leave. If you can‚Äôt leave, they don‚Äôt have to treat you well.</p><p><strong>So how has tech interoperability become interrupted?</strong></p><p><strong>Doctorow:</strong> There are lots of platforms that have become, effectively, walled gardens. The argument is that the reason that these businesses are so sturdy, that no one has disrupted them or displaced them is that they have such good engineers. I get this from people about iOS all the time, saying ‚ÄúiOS is an impregnable vault, they‚Äôve figured out how to stop people from doing it.‚Äù and I say, ‚Äúwait a second‚Äîit‚Äôs a felony to try.‚Äù If I say I‚Äôm the world champion boxer, and no one has ever defeated me, but I can also send you to prison for five years for trying to take my title, how do we know how good a boxer I am? The question of why they use the lawyers and not the engineers, why their go-to is the lawyers and not the engineers, to me the answer is because the lawyers are much more effective at preventing a capitalized, motivated rival from figuring out how to bypass their technical countermeasures.</p><p><strong>So you‚Äôre saying it‚Äôs not technical dominance or business superiority or network effects that keep these platforms dominant; it‚Äôs instead the switching costs imposed by law. <strong>How could those laws be changed to make existing platforms more interoperable?</strong></strong></p><p><strong>Doctorow:</strong> One of the things I‚Äôm interested in is how to make a law you can‚Äôt cheat on. Facebook often proposes rules only they can follow. Rules for interoperability have two constraints: they have to be administrable and cannot constitute a capital barrier. For example, we can propose two different rules for Twitter [now X]. One would be an end-to-end principle, which is a rule that says: If I follow someone, and they post something, I can see it. That rule makes it really hard for Twitter to overweight content from its preferred suppliers. On top of that we add the Right to Exit. This is the right to leave Twitter without losing your followers and followees. This would be a mandate to stand up an API, and it would be, probably, an <a href="https://www.w3.org/TR/activitypub/" target="_blank">ActivityPub API</a>. It‚Äôs a pretty good standard and its deficiencies can be remedied more easily than we can design a new standard. You put those two together, Twitter can‚Äôt keep anyone prisoner, and Twitter will have to treat the people who choose to stay well.</p><p><strong>You also argue that requiring interoperability will lead to something of a virtuous antitrust cycle. Is it that breaking down tech monopolies lessens their ability to reinforce the policies that have prevented interoperability in the first place?</strong></p><p><strong>Doctorow: </strong>That‚Äôs half of it. To understand the success of an industry in achieving its regulatory goals, you have to understand it needs two things. The first thing is power in the form of excess profits it can maneuver to lobbying. The second is unity.</p><p>Think back to the Napster wars. That was a sector of hundreds of small and medium-sized tech companies that in aggregate had more profit than the whole music industry and the whole movie industry. But the movie industry and the music industry had a smaller number of participants: music was five, movies were seven back then, now it‚Äôs three and four. As a result, they were able to agree on what to spend the money on. The tech industry was all over the place. They were knifing each other in the back. The reason the tech industry treats people like the music industry treated them 20 years ago is that unity.</p><p>The two benefits of competition are that it breaks the cash reserves that are used to enact public policy and it introduces the collective action problem that makes the remaining reserves harder to spend.</p><p><strong>How does that virtuous cycle then extend from tech into other sectors?</strong></p><p><strong>Doctorow:</strong> Think about what happened with the breakup of Standard Oil in the first part of the 20th century. Standard Oil was not the only trust. There were trusts for everything: whiskey, railroads, iron, aluminum, cars. Standard Oil‚Äôs dominance made people so hopeless about whether or not they could have an accountable government that the toppling of Standard Oil opened up a floodgate of political will that saw all of those other trusts shattered.</p><p>I want to go after tech because it has this characteristic interoperability that makes it a soft target. We start with tech, and that gives us the momentum, the credibility, and the political will to go after everybody else.</p><p><em>Cory Doctorow is an author, activist, journalist, and blogger. His new book, </em><a href="https://www.versobooks.com/products/3035-the-internet-con" rel="noopener noreferrer" target="_blank">The Internet Con: How to Seize the Means of Computation</a>,<em> from Verso Books, is available 5 September 2023. He is special advisor to the </em><a href="http://eff.org/" target="_blank"><em>Electronic Frontier Foundation</em></a><em>.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Experts fear crooks are cracking keys stolen in LastPass breach (155 pts)]]></title>
            <link>https://krebsonsecurity.com/2023/09/experts-fear-crooks-are-cracking-keys-stolen-in-lastpass-breach/</link>
            <guid>37399745</guid>
            <pubDate>Wed, 06 Sep 2023 00:25:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://krebsonsecurity.com/2023/09/experts-fear-crooks-are-cracking-keys-stolen-in-lastpass-breach/">https://krebsonsecurity.com/2023/09/experts-fear-crooks-are-cracking-keys-stolen-in-lastpass-breach/</a>, See on <a href="https://news.ycombinator.com/item?id=37399745">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
												<p>In November 2022, the password manager service <strong>LastPass</strong> disclosed a breach in which hackers stole password vaults containing both encrypted and plaintext data for more than 25 million users. Since then, a steady trickle of six-figure cryptocurrency heists targeting security-conscious people throughout the tech industry has led some security experts to conclude that crooks likely have succeeded at cracking open some of the stolen LastPass vaults.</p>
<p><strong>Taylor Monahan</strong> is founder and CEO of <strong>MetaMask</strong>, a popular software cryptocurrency wallet used to interact with the Ethereum blockchain. Since late December 2022, Monahan and other researchers have identified a highly reliable set of clues that they say connect recent thefts targeting more than 150 people, Collectively, these individuals have been robbed of more than $35 million worth of crypto.</p>
<p>Monahan said virtually all of the victims she has assisted were longtime cryptocurrency investors, and security-minded individuals. Importantly, none appeared to have suffered the sorts of attacks that typically preface a high-dollar crypto heist, such as the compromise of one‚Äôs email and/or mobile phone accounts.</p>
<p>‚ÄúThe victim profile remains the most striking thing,‚Äù Monahan wrote. ‚ÄúThey truly all are reasonably secure. They are also deeply integrated into this ecosystem, [including] employees of reputable crypto orgs, VCs [venture capitalists], people who built DeFi protocols, deploy contracts, run full nodes.‚Äù</p>
<p>Monahan has been <a href="https://twitter.com/tayvano_/status/1648187031468781568" target="_blank" rel="noopener">documenting the crypto thefts via Twitter/X</a> since March 2023, frequently expressing frustration in the search for a common cause among the victims. Then on Aug. 28, Monahan said she‚Äôd concluded that the common thread among nearly every victim was that they‚Äôd previously used LastPass to store their ‚Äúseed phrase,‚Äù the private key needed to unlock access to their cryptocurrency investments.</p>
<div id="attachment_64831"><p><img aria-describedby="caption-attachment-64831" decoding="async" src="https://krebsonsecurity.com/wp-content/uploads/2023/09/tayaug28.png" alt="" width="582" height="844"></p><p id="caption-attachment-64831">MetaMask owner Taylor Monahan on Twitter. Image: twitter.com/tayvano</p></div>
<p>Armed with your secret seed phrase, anyone can instantly access all of the cryptocurrency holdings tied to that cryptographic key, and move the funds to anywhere they like.</p>
<p>Which is why the best practice for many cybersecurity enthusiasts has long been to store their seed phrases either in some type of encrypted container ‚Äî such as a password manager ‚Äî or else inside an offline, special-purpose hardware encryption device, such as a Trezor or Ledger wallet.</p>
<p>‚ÄúThe seed phrase is literally the money,‚Äù said <strong>Nick Bax</strong>, director of analytics at <a href="https://www.unciphered.com/" target="_blank" rel="noopener">Unciphered</a>, a cryptocurrency wallet recovery company. ‚ÄúIf you have my seed phrase, you can copy and paste that into your wallet, and then you can see all my accounts. And you can transfer my funds.‚Äù</p>
<p>Bax said he closely reviewed the massive trove of cryptocurrency theft data that Taylor Monahan and others have collected and linked together.</p>
<p>‚ÄúIt‚Äôs one of the broadest and most complex cryptocurrency investigations I‚Äôve ever seen,‚Äù Bax said. ‚ÄúI ran my own analysis on top of their data and reached the same conclusion that Taylor reported. The threat actor moved stolen funds from multiple victims to the same blockchain addresses, making it possible to strongly link those victims.‚Äù</p>
<p>Bax, Monahan and others interviewed for this story say they‚Äôve identified a unique signature that links the theft of more than $35 million in crypto from more than 150 confirmed victims, with roughly two to five high-dollar heists happening each month since December 2022.</p>
<p>KrebsOnSecurity has reviewed this signature but is not publishing it at the request of Monahan and other researchers, who say doing so could cause the attackers to alter their operations in ways that make their criminal activity more difficult to track.</p>
<p>But the researchers have published findings about the dramatic similarities in the ways that victim funds were stolen and laundered through specific cryptocurrency exchanges. They also learned the attackers frequently grouped together victims by sending their cryptocurrencies to the same destination crypto wallet.</p>
<div id="attachment_64829"><p><img aria-describedby="caption-attachment-64829" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2023/09/taygraph.png" alt="" width="250" height="653"></p><p id="caption-attachment-64829">A graphic published by @tayvano on Twitter depicting the movement of stolen cryptocurrencies from victims who used LastPass to store their crypto seed phrases.</p></div>
<p>By identifying points of overlap in these destination addresses, the researchers were then able to track down and interview new victims. For example, the researchers said their methodology identified a recent multi-million dollar crypto heist victim as an employee at <strong>Chainalysis</strong>, a blockchain analysis firm that works closely with law enforcement agencies to help track down cybercriminals and money launderers.</p>
<p>Chainalysis confirmed that the employee had suffered a high-dollar cryptocurrency heist late last month, but otherwise declined to comment for this story.</p>
<p>Bax said the only obvious commonality between the victims who agreed to be interviewed was that they had stored the seed phrases for their cryptocurrency wallets in LastPass.</p>
<p>‚ÄúOn top of the overlapping indicators of compromise, there are more circumstantial behavioral patterns and tradecraft which are also consistent between different thefts and support the conclusion,‚Äù Bax told KrebsOnSecuirty. ‚ÄúI‚Äôm confident enough that this is a real problem that I‚Äôve been urging my friends and family who use LastPass to change all of their passwords and migrate any crypto that may have been exposed, despite knowing full well how tedious that is.‚Äù</p>
<p>LastPass declined to answer questions about the research highlighted in this story, citing an ongoing law enforcement investigation and pending litigation against the company in response to its 2022 data breach.</p>
<p>‚ÄúLast year‚Äôs incident remains the subject of an ongoing investigation by law enforcement and is also the subject of pending litigation,‚Äù LastPass said in a written statement provided to KrebsOnSecurity. ‚ÄúSince last year‚Äôs attack on LastPass, we have remained in contact with law enforcement and continue to do so.‚Äù</p>
<p>Their statement continues:</p>
<p>‚ÄúWe have shared various technical information, Indicators of Compromise (IOCs), and threat actor tactics, techniques, and procedures (TTPs) with our law enforcement contacts as well as our internal and external threat intelligence and forensic partners in an effort to try and help identify the parties responsible. In the meantime, we encourage any security researchers to share any useful information they believe they may have with our Threat Intelligence team by contacting securitydisclosure@lastpass.com.‚Äù<span id="more-64785"></span></p>
<h2>THE LASTPASS BREACH(ES)</h2>
<p>On August 25, 2022, <strong>LastPass CEO Karim Toubba</strong> wrote to users that the company had detected unusual activity in its software development environment, and that the intruders stole some source code and proprietary LastPass technical information. On Sept. 15, 2022, LastPass said an investigation into the August breach determined the attacker did not access any customer data or password vaults.</p>
<p>But on Nov. 30, 2022, LastPass notified customers about another, far more serious security incident that the company said leveraged data stolen in the August breach. LastPass disclosed that criminal hackers had compromised encrypted copies of some password vaults, as well as other personal information.</p>
<p>In February 2023, LastPass disclosed that the intrusion involved a highly complex, targeted attack against a DevOps engineer who was one of only four LastPass employees with access to the corporate vault.</p>
<p>‚ÄúThis was accomplished by targeting the DevOps engineer‚Äôs home computer and exploiting a vulnerable third-party media software package, which enabled remote code execution capability and allowed the threat actor to implant keylogger malware,‚Äù LastPass officials wrote. ‚ÄúThe threat actor was able to capture the employee‚Äôs master password as it was entered, after the employee authenticated with MFA, and gain access to the DevOps engineer‚Äôs LastPass corporate vault.‚Äù</p>
<p><strong>Dan Goodin </strong>at Ars Technica&nbsp;<a href="https://arstechnica.com/information-technology/2023/02/lastpass-hackers-infected-employees-home-computer-and-stole-corporate-vault/" target="_blank" rel="noopener">reported</a> and then <a href="https://infosec.exchange/@dangoodin/109950447675626971" target="_blank" rel="noopener">confirmed</a> that the attackers exploited a known vulnerability in a <strong>Plex</strong> media server that the employee was running on his home network, and succeeded in installing malicious software that stole passwords and other authentication credentials. The vulnerability exploited by the intruders was patched back in 2020, but the employee never updated his Plex software.</p>
<p>As it happens, Plex announced its own data breach one day before LastPass disclosed its initial August intrusion. On August 24, 2022, Plex‚Äôs security team urged users to reset their passwords, saying an intruder had accessed customer emails, usernames and encrypted passwords.</p>
<h2>OFFLINE ATTACKS</h2>
<p>A basic functionality of LastPass is that it will pick and remember lengthy, complex passwords for each of your websites or online services. To automatically populate the appropriate credentials at any website going forward, you simply authenticate to LastPass using your master password.</p>
<p>LastPass has always emphasized that if you lose this master password, that‚Äôs too bad because they don‚Äôt store it and their encryption is so strong that even they can‚Äôt help you recover it.</p>
<p>But experts say all bets are off when cybercrooks can get their hands on the encrypted vault data itself ‚Äî as opposed to having to interact with LastPass via its website. These so-called ‚Äúoffline‚Äù attacks allow the bad guys to conduct unlimited and unfettered ‚Äúbrute force‚Äù password cracking attempts against the encrypted data using powerful computers that can each try millions of password guesses per second.</p>
<p>‚ÄúIt does leave things vulnerable to brute force when the vaults are stolen en masse, especially if info about the vault HOLDER is available,‚Äù said <strong>Nicholas Weaver</strong>, a researcher at University of California, Berkeley‚Äôs&nbsp;<a href="https://www.icsi.berkeley.edu/icsi/" target="_blank" rel="noopener">International Computer Science Institute</a> (ICSI) and lecturer at UC Davis. ‚ÄúSo you just crunch and crunch and crunch with GPUs, with a priority list of vaults you target.‚Äù</p>
<p>How hard would it be for well-resourced criminals to crack the master passwords securing LastPass user vaults? Perhaps the best answer to this question comes from <strong>Wladimir Palant</strong>, a security researcher and the original developer behind the <strong>Adblock Plus</strong> browser plugin.</p>
<p>In <a href="https://palant.info/2022/12/28/lastpass-breach-the-significance-of-these-password-iterations/" target="_blank" rel="noopener">a December 2022 blog post</a>, Palant explained that the crackability of the LastPass master passwords depends largely on two things: The complexity of the master password, and the default settings for LastPass users, which appear to have varied quite a bit based on when those users began patronizing the service.</p>
<p>LastPass says that since 2018 it has required a twelve-character minimum for master passwords, which the company said ‚Äúgreatly minimizes the ability for successful brute force password guessing.‚Äù</p>
<p>But Palant said while LastPass indeed improved its master password defaults in 2018, it did not force all existing customers who had master passwords of lesser lengths to pick new credentials that would satisfy the 12-character minimum.</p>
<p>‚ÄúIf you are a LastPass customer, chances are that you are completely unaware of this requirement,‚Äù Palant wrote. ‚ÄúThat‚Äôs because LastPass didn‚Äôt ask existing customers to change their master password. I had my test account since 2018, and even today I can log in with my eight-character password without any warnings or prompts to change it.‚Äù</p>
<p>Palant believes LastPass also failed to upgrade many older, original customers to more secure encryption protections that were offered to newer customers over the years. One important setting in LastPass is the number of ‚Äúiterations,‚Äù or how many times your master password is run through the company‚Äôs encryption routines. The more iterations, the longer it takes an offline attacker to crack your master password.</p>
<p>Palant noted last year that for many older LastPass users, the initial default setting for iterations was anywhere from ‚Äú1‚Äù to ‚Äú500.‚Äù By 2013, new LastPass customers were given 5,000 iterations by default. In February 2018, LastPass changed the default to 100,100 iterations. And very recently, it upped that again to 600,000.</p>
<p>Palant said the 2018 change was in response to a security bug report he filed about some users having dangerously low iterations in their LastPass settings.</p>
<p>‚ÄúWorse yet, for reasons that are beyond me, LastPass didn‚Äôt complete this migration,‚Äù Palant wrote. ‚ÄúMy test account is still at 5,000 iterations, as are the accounts of many other users who checked their LastPass settings. LastPass would know how many users are affected, but they aren‚Äôt telling that. In fact, it‚Äôs painfully obvious that LastPass never bothered updating users‚Äô security settings. Not when they changed the default from 1 to 500 iterations. Not when they changed it from 500 to 5,000. Only my persistence made them consider it for their latest change. And they still failed implementing it consistently.‚Äù</p>
<p>A chart on Palant‚Äôs blog post offers an idea of how increasing password iterations dramatically increases the costs and time needed by the attackers to crack someone‚Äôs master password. Palant said it would take a single GPU about a year to crack a password of average complexity with 500 iterations, and about 10 years to crack the same password run through 5,000 iterations.</p>
<div id="attachment_64836"><p><img aria-describedby="caption-attachment-64836" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2023/09/gpuguesstime.png" alt="" width="754" height="219" srcset="https://krebsonsecurity.com/wp-content/uploads/2023/09/gpuguesstime.png 850w, https://krebsonsecurity.com/wp-content/uploads/2023/09/gpuguesstime-768x223.png 768w, https://krebsonsecurity.com/wp-content/uploads/2023/09/gpuguesstime-782x227.png 782w" sizes="(max-width: 754px) 100vw, 754px"></p><p id="caption-attachment-64836">Image: palant.info</p></div>
<p>However, these numbers radically come down when a determined adversary also has other large-scale computational assets at their disposal, such as a bitcoin mining operation that can coordinate the password-cracking activity across multiple powerful systems simultaneously.</p>
<p>Weaver said a password or passphrase with average complexity ‚Äî such as ‚ÄúCorrect Horse Battery Staple‚Äù is only secure against online attacks, and that its roughly 40 bits of randomness or ‚Äú<a href="https://www.omnicalculator.com/other/password-entropy" target="_blank" rel="noopener">entropy</a>‚Äù means a graphics card can blow through it in no time.</p>
<p>‚ÄúAn Nvidia 3090 can do roughly 4 million [password guesses] per second with 1000 iterations, but that would go down to 8 thousand per second with 500,000 iterations, which is why iteration count matters so much,‚Äù Weaver said. ‚ÄúSo a combination of ‚Äònot THAT strong of a password‚Äô and ‚Äòold vault‚Äô and ‚Äòlow iteration count‚Äô would make it theoretically crackable but real work, but the work is worth it given the targets.‚Äù</p>
<p>Reached by KrebsOnSecurity, Palant said he never received a response from LastPass about why the company apparently failed to migrate some number of customers to more secure account settings.</p>
<p>‚ÄúI know exactly as much as everyone else,‚Äù Palant wrote in reply. ‚ÄúLastPass published <a href="https://palant.info/2023/02/28/lastpass-breach-update-the-few-additional-bits-of-information/" target="_blank" rel="noopener">some additional information in March</a>. This finally answered the questions about the timeline of their breach ‚Äì meaning which users are affected. It also made obvious that business customers are very much at risk here, Federated Login Services being highly compromised in this breach (LastPass downplaying as usual of course).‚Äù</p>
<p>Palant said upon logging into his LastPass account a few days ago, he found his master password was still set at 5,000 iterations.</p>
<h2>INTERVIEW WITH A VICTIM</h2>
<p>KrebsOnSecurity interviewed one of the victims tracked down by Monahan, a software engineer and startup founder who recently was robbed of approximately $3.4 million worth of different cryptocurrencies. The victim agreed to tell his story in exchange for anonymity because he is still trying to claw back his losses. We‚Äôll refer to him here as ‚ÄúConnor‚Äù (not his real name).</p>
<p>Connor said he began using LastPass roughly a decade ago, and that he also stored the seed phrase for his primary cryptocurrency wallet inside of LastPass. Connor chose to protect his LastPass password vault with an eight character master password that included numbers and symbols (~50 bits of entropy).</p>
<p>‚ÄúI thought at the time that the bigger risk was losing a piece of paper with my seed phrase on it,‚Äù Connor said. ‚ÄúI had it in a bank security deposit box before that, but then I started thinking, ‚ÄòHey, the bank might close or burn down and I could lose my seed phrase.'‚Äù</p>
<p>Those seed phrases sat in his LastPass vault for years. Then, early on the morning of Sunday, Aug. 27, 2023, Connor was awoken by a service he‚Äôd set up to monitor his cryptocurrency addresses for any unusual activity: Someone was draining funds from his accounts, and fast.</p>
<p>Like other victims interviewed for this story, Connor didn‚Äôt suffer the usual indignities that typically presage a cryptocurrency robbery, such as account takeovers of his email inbox or mobile phone number.</p>
<p>Connor said he doesn‚Äôt know the number of iterations his master password was given originally, or what it was set at when the LastPass user vault data was stolen last year. But he said he recently logged into his LastPass account and the system forced him to upgrade to the new 600,000 iterations setting.</p>
<p>‚ÄúBecause I set up my LastPass account so early, I‚Äôm pretty sure I had whatever weak settings or iterations it originally had,‚Äù he said.</p>
<p>Connor said he‚Äôs kicking himself because he recently started the process of migrating his cryptocurrency to a new wallet protected by a new seed phrase. But he never finished that migration process. And then he got hacked.</p>
<p>‚ÄúI‚Äôd set up a brand new wallet with new keys,‚Äù he said. ‚ÄúI had that ready to go two months ago, but have been procrastinating moving things to the new wallet.‚Äù</p>
<p>Connor has been exceedingly lucky in regaining access to some of his stolen millions in cryptocurrency. The Internet is swimming with con artists masquerading as legitimate cryptocurrency recovery experts. To make matters worse, because time is so critical in these crypto heists, many victims turn to the first quasi-believable expert who offers help.</p>
<p>Instead, several friends steered Connor to <a href="https://noteforms.com/forms/flashbots-whitehat-intake-form?notionforms=1&amp;utm_source=notionforms" target="_blank" rel="noopener">Flashbots.net</a>, a cryptocurrency recovery firm that employs several custom techniques to help clients claw back stolen funds ‚Äî particularly those on the Ethereum blockchain.</p>
<p>According to Connor, Flashbots helped rescue approximately $1.5 million worth of the $3.4 million in cryptocurrency value that was suddenly swept out of his account roughly a week ago. Lucky for him, Connor had some of his assets tied up in a type of digital loan that allowed him to borrow against his various cryptocurrency assets.</p>
<p>Without giving away too many details about how they clawed back the funds, here‚Äôs a high level summary: When the crooks who stole Connor‚Äôs seed phrase sought to extract value from these loans, they were borrowing the maximum amount of credit that he hadn‚Äôt already used. But Connor said that left open an avenue for some of that value to be recaptured, basically by repaying the loan in many small, rapid chunks.</p>
<h2>WHAT SHOULD LASTPASS USERS DO?</h2>
<p>According to MetaMask‚Äôs Monahan, users who stored any important passwords with LastPass ‚Äî particularly those related to cryptocurrency accounts ‚Äî should change those credentials immediately, and migrate any crypto holdings to new offline hardware wallets.</p>
<p>‚ÄúReally the ONLY thing you need to read is this,‚Äù Monahan pleaded to her 70,000 followers on Twitter/X: ‚ÄúPLEASE DON‚ÄôT KEEP ALL YOUR ASSETS IN A SINGLE KEY OR SECRET PHRASE FOR YEARS. THE END. Split up your assets. Get a hw [hardware] wallet. Migrate. Now.‚Äù</p>
<p>If you also had passwords tied to banking or retirement accounts, or even just important email accounts ‚Äî now would be a good time to change those credentials as well.</p>
<p>I‚Äôve never been comfortable recommending password managers, because I‚Äôve never seriously used them myself. Something about putting all your eggs in one basket. Heck, I‚Äôm so old-fashioned that most of my important passwords are written down and tucked away in safe places.</p>
<p>But I recognize this antiquated approach to password management is not for everyone.&nbsp;Connor says he now uses 1Password, a competing password manager that recently earned the best overall marks from <a href="https://www.wired.com/story/best-password-managers/" target="_blank" rel="noopener">Wired</a> and <a href="https://www.nytimes.com/wirecutter/reviews/1password-review/" target="_blank" rel="noopener">The New York Times</a>.</p>
<p>1Password says that three things are needed to decrypt your information: The encrypted data itself, your account password, and your Secret Key. Only you know your account password, and your Secret Key is generated locally during setup.</p>
<p>‚ÄúThe two are combined on-device to encrypt your vault data and are never sent to 1Password,‚Äù explains a 1Password blog post ‚Äò<a href="https://blog.1password.com/what-if-1password-gets-hacked/" target="_blank" rel="noopener">What If 1Password Gets Hacked?</a>‚Äò ‚ÄúOnly the encrypted vault data lives on our servers, so neither 1Password nor an attacker who somehow manages to guess or steal your account password would be able to access your vaults ‚Äì or what‚Äôs inside them.</p>
<p>Weaver said that Secret Key adds an extra level of randomness to all user master passwords that LastPass didn‚Äôt have.</p>
<p>‚ÄúWith LastPass, the idea is the user‚Äôs password vault is encrypted with a cryptographic hash (H) of the user‚Äôs passphrase,‚Äù Weaver said. ‚ÄúThe problem is a hash of the user‚Äôs passphrase is remarkably weak on older LastPass vaults with master passwords that do not have many iterations. 1Password uses H(random-key||password) to generate the password, and it is why you have the QR code business when adding a new device.‚Äù</p>
<p>Weaver said LastPass deserves blame for not having upgraded iteration counts for all users a long time ago, and called the latest forced upgrades ‚Äúa stunning indictment of the negligence on the part of LastPass.‚Äù</p>
<p>‚ÄúThat they never even notified all those with iteration counts of less than 100,000 ‚Äî who are really vulnerable to brute force even with 8-character random passwords or ‚Äòcorrect horse battery staple‚Äô type passphrases ‚Äî is outright negligence,‚Äù Weaver said. ‚ÄúI would personally advocate that nobody ever uses LastPass again: Not because they were hacked. Not because they had an architecture (unlike 1Password) that makes such hacking a problem. But because of their consistent refusal to address how they screwed up and take proactive efforts to protect their customers.‚Äù</p>
<p>Bax and Monahan both acknowledged that their research alone can probably never conclusively tie dozens of high-dollar crypto heists over the past year to the LastPass breach. But Bax says at this point he doesn‚Äôt see any other possible explanation.</p>
<p>‚ÄúSome might say it‚Äôs dangerous to assert a strong connection here, but I‚Äôd say it‚Äôs dangerous to assert there isn‚Äôt one,‚Äù he said. ‚ÄúI was arguing with my fiance about this last night. She‚Äôs waiting for LastPass to tell her to change everything. Meanwhile, I‚Äôm telling her to do it now.‚Äù</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Engineer‚Äôs guide to career growth: Advice from my time at Stripe and Facebook (174 pts)]]></title>
            <link>https://review.firstround.com/the-engineers-guide-to-career-growth-advice-from-my-time-at-stripe-and-facebook</link>
            <guid>37398921</guid>
            <pubDate>Tue, 05 Sep 2023 22:53:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://review.firstround.com/the-engineers-guide-to-career-growth-advice-from-my-time-at-stripe-and-facebook">https://review.firstround.com/the-engineers-guide-to-career-growth-advice-from-my-time-at-stripe-and-facebook</a>, See on <a href="https://news.ycombinator.com/item?id=37398921">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body-content"><p data-label="Paragraph"><strong><a href="https://www.linkedin.com/in/rayleney/" title="null" target="_blank">Raylene Yung</a></strong> likes to say that her path into engineering management was in some ways gradual, and in others, a wild ride full of challenges at every twist and turn. Her first taste of life as an engineering manager came at Facebook in 2011, on what she describes as a tiny team. ‚ÄúIt was three people ‚Äî including myself,‚Äù says Yung. ‚ÄúThe day-to-day wasn‚Äôt all that different. I was still diving into code, but balancing that with learning how to recruit and hold 1:1s.‚Äù</p><p data-label="Paragraph">Then the wild ride began. Yung had to strap in as her team took on more in scope, growing from three to thirty, and beyond. ‚ÄúI moved on from reviewing code to owning company metrics and building teams in multiple offices,‚Äù she says. By the time Yung left Facebook in 2015, she was the youngest engineering director at a public company with over 10,000 employees ‚Äî several rungs up from where she had started, as a new grad at the then 700-person startup.</p><p data-label="Paragraph">Yung then took on a challenge of a different sort when she joined Stripe. ‚ÄúI wanted to see if I had what it takes to be a great leader in a completely different environment,‚Äù she says. (Spoiler alert: she did.) In her nearly four years at Stripe as it scaled up from 200 to over 1500 people, Yung built the product management team, defined career growth and recruiting frameworks, ran the core Payments business, and spun up the company‚Äôs global engineering hub in Singapore.</p><p data-label="Paragraph">Taking a front-row seat on two very different startup rides left her with a clear takeaway. ‚Äú<strong>Personal growth is compounded by company growth</strong>. At Facebook, I focused on learning as much as I could from the experienced leaders who were joining as we grew. At Stripe, <em>I</em> was one of those leaders, tasked with figuring it out and teaching others,‚Äù Yung says. ‚ÄúI had to learn from piloting new internal programs, reaching out to peers and former managers for advice, and seeking out coaching and training.‚Äù</p><p data-label="Paragraph">She translated this crash course into a lot of heads down writing time, producing <a href="https://stripe.com/atlas/guides/scaling-eng" title="null" target="_blank">Stripe‚Äôs Atlas guide to scaling engineering teams</a>, as well as her own <a href="https://github.com/raylene/eng-handbook" title="null" target="_blank">handbook for eng teams</a> (which covers everything from 1:1s to performance reviews). After a decade of working at explosive growth startups, Yung left Stripe to take some much needed time off and carve out white space to continue sharing what she‚Äôs learned.</p><p data-label="Paragraph">And she‚Äôd like to train her focus on one subject in particular: <strong>career growth for engineers</strong>. ‚ÄúMost of the advice you hear is about advancing quickly, not stopping to soak up all you can,‚Äù she says. In this exclusive interview, Yung shares a collection of counterintuitive career lessons, the ones she learned the hard way. She also dives into why the IC and management tracks aren‚Äôt parallel ladders, but rather intertwined steps. Finally, she covers more specific roadblocks that can appear at every stage of your engineering career, as well as guiding questions that can serve as a gut-check as you seek to push them aside.</p><p data-label="Paragraph">With both broad lessons and targeted advice for different career phases, there‚Äôs plenty of wisdom to mine here. Whether it‚Äôs the young engineer peering around the corner to get a sense of what lies ahead or a seasoned leader trying to wrap their arms around increasingly thorny technical and managerial obstacles, Yung challenges <strong>every engineer to ask themselves the </strong><strong><em>right</em></strong><strong> questions as they plot out their careers</strong>.</p><h2 id="looking-for-advice-specific-to-your-situation-jump-ahead-to-yungs-career-tips-for-new-engineers-technical-leads-domain-experts-early-engineering-managers-more-seasoned-managers-and-engineering-org-leaders-and-get-a-helpful-rundown-of-her-advice-here"><em>Looking for advice specific to your situation? Jump ahead to Yung‚Äôs career tips for new engineers, technical leads, domain experts, early engineering managers, more seasoned managers, and engineering org leaders. And get a helpful rundown of her advice here.</em></h2><h2 id="three-critical-lessons-for-building-a-career-as-an-engineer">THREE CRITICAL LESSONS FOR BUILDING A CAREER AS AN ENGINEER:</h2><p data-label="Paragraph">There are some underlying themes that can carry you throughout your career ‚Äî and these three lessons have been the foundation of Yung‚Äôs at every stage:</p><h2 id="1-strive-to-be-the-most-valuable-but-least-critical">1. Strive to be the most valuable, but least critical.</h2><p data-label="Paragraph">Most of us would probably be thrilled to be described as ‚Äúcritical‚Äù to a project‚Äôs success. But in Yung‚Äôs experience, that‚Äôs dangerous territory for an engineer to be in. Take this example from her time at Facebook. Yung had cemented herself as a company expert on posting permissions, but she was quickly about to discover the drawbacks of being the lynchpin.</p><p data-label="Paragraph">‚ÄúI was training for my first marathon and had just finished a 20-mile run ‚Äî only to discover that I was being paged for a live incident. I remember sitting in my bathtub fully clothed, somewhat delirious, with giant bottles of Gatorade in each hand and my laptop open, trying to figure out what had happened,‚Äù she says. ‚ÄúIt was a high and a low point at the same time. I felt so important and needed, but all I wanted to do was rest and get out of the way. I vowed to immediately start transferring as much knowledge to my teammates as I could so they would have the tools to solve problems without me.‚Äù</p><p data-label="Paragraph"><strong>While you‚Äôve probably heard the common working adage to ‚Äúmanage yourself out of a job,‚Äù Yung finds that phrasing too simplistic</strong>. For her, it‚Äôs less about making yourself obsolete, and more about making yourself less critical.</p><p data-label="Pullquote">When you‚Äôre critical to a project, you‚Äôre playing a decisive or crucial role in the success, failure or existence of something. As leaders and engineers, we‚Äôre all in this position at some point. But you can‚Äôt stay there. </p><p data-label="Paragraph">‚ÄúWhen you‚Äôre valuable, you‚Äôre extremely useful or important ‚Äî but not a failure point," says Yung. "The best engineers and managers are great at adding value, getting the most out of the people around them, and helping the team see around corners even when they‚Äôre not there.‚Äù</p><h2 id="2-want-to-succeed-as-a-manager-establish-an-emotional-equilibrium">2. Want to succeed as a manager? Establish an emotional equilibrium.</h2><p data-label="Paragraph">By now, it‚Äôs clear to most that <a href="https://firstround.com/review/this-is-what-impactful-engineering-leadership-looks-like/" title="null" target="_blank">forcing engineers to move into management is a mistake</a>. The challenge for most engineers is deciding if it‚Äôs a mantle they want to take on. ‚ÄúAlthough transitions into management sometimes happen quickly, think about it deeply and prepare as best as you can. Too many engineers plunge in, without exploring lightweight ways to try the manager‚Äôs hat on first,‚Äù says Yung.</p><p data-label="Paragraph">‚ÄúI started by taking on more team responsibilities and onboarding new hires as a Facebook bootcamp mentor. I led a large technical project and got a feel for balancing engineering work against people leadership. By the time the project launched, I knew I wanted to invest time in becoming a good manager.‚Äù</p><p data-label="Pullquote">Management is not for the faint of heart. While being a great manager is really hard, it‚Äôs incredibly rewarding ‚Äî and something I‚Äôve chosen to spend many of my waking hours trying to get better at.</p><p data-label="Paragraph">Still, all the prep work in the world won‚Äôt ready you for the emotional challenge that lies ahead. ‚ÄúMost of the time, this is what I end up coaching new managers through,‚Äù says Yung. ‚ÄúAs an IC, you don‚Äôt see the full picture of all the ups and downs managers have to deal with, so it‚Äôs easy to fall into the trap of thinking you‚Äôre ready ‚Äî and then freaking out once you realize you‚Äôre not.‚Äù </p><p data-label="Paragraph">To help would-be managers get a better sense of what‚Äôs to come, Yung sketches out this chart on a whiteboard to give them a visual metaphor (<a href="https://medium.com/@emdashry/modern-portfolio-theory-for-management-variance-maximization-in-engineering-7dde7269e8a4" title="null" target="_blank">which she explains in more detail here</a>):</p><figure><img src="https://assets.proof.pub/2056/firstround/Z2caX5TlTY9P2TWpuMKg_The%20productivity%20journey%20of%20engineering%20life,%20IC%20vs.%20EM.jpeg"><figcaption>The productivity journey of engineering life, contrasting individual contributors and engineering managers</figcaption></figure><p data-label="Paragraph">‚ÄúAs an individual contributor, your job often looks like a local hill-climbing exercise. For most projects, you steadily make progress until you don‚Äôt. You‚Äôre either done and can move on, or maybe something went awry and your project was canceled or simply failed. Either way you get to start over from zero the next time,‚Äù says Yung.</p><p data-label="Paragraph">But engineering managers <strong>unlock the negative zone of this chart</strong>. ‚ÄúAs an IC, I was a bit of a perfectionist. I thought in black and white, in designing systems, in exhaustively listing out all the edge cases,‚Äù she says. ‚ÄúAs a manager, it took me a lot of time to realize you can't solve problems that way. People are not optimization problems ‚Äî you have to accept there's a range of solutions and there isn't one true great answer.‚Äù</p><p data-label="Paragraph">Another observation: <strong>progress and feelings are no longer tightly correlated</strong>. ‚ÄúAs an individual contributor, generally when you were doing well on a project you were making visible progress. As an engineering manager, you can work hard on something for over a year, only to see no progress in the short-term and <em>maybe</em> experience a big payoff in the much longer-term,‚Äù says Yung.</p><p data-label="Paragraph">All of that combined can bring your emotions down in ways you can‚Äôt predict or control. ‚ÄúYour mood will swing wildly. The larger your teams get, the greater the probability something is going off the rails at any given time. The crests and dips grow bigger and bigger,‚Äù she says. ‚ÄúThe first step is being aware that <a href="https://firstround.com/review/give-away-your-legos-and-other-commandments-for-scaling-startups/" title="null" target="_blank">this is completely normal</a>. The second step is trying to establish more of an equilibrium so you don‚Äôt get knocked off course so easily.‚Äù</p><p data-label="Pullquote">Part of what makes experienced managers great is their ability to weather storms and normalize their own emotions to somewhere closer to zero.</p><p data-label="Paragraph">Yung cautions that engineering managers are also liable to develop what she calls split-brain syndrome. ‚ÄúYou put on a mask. To your teammates or coworkers you feel this need to maintain an even, steady zero, but on the inside you‚Äôre going through these wild swings,‚Äù she says. ‚ÄúDon‚Äôt make the mistake of trying to handle this on your own. Some of my closest friends and mentors from work are the people I‚Äôve been the most vulnerable with, and who have helped me become a much better leader.‚Äù</p><h2 id="3-focus-on-growth-and-learning-at-every-step-not-on-climbing-the-ladder-as-quickly-as-possible">3. Focus on growth and learning at every step ‚Äî not on climbing the ladder as quickly as possible.</h2><p data-label="Paragraph">Even though there‚Äôs a lot of industry emphasis on ladders and levels, Yung‚Äôs no longer sure that‚Äôs the best way for engineers to think about their careers. ‚ÄúWhile companies have developed ‚Äòparallel‚Äô ladders between IC and engineering management tracks ‚Äî and I helped with this very task at Stripe ‚Äî I‚Äôve since come to think of these engineering roles more as <strong>progressing along a pair of joint and often intertwined steps</strong>,‚Äù she says.</p><p data-label="Paragraph">Here are those steps as Yung sees them:</p><figure><img src="https://assets.proof.pub/2056/firstround/GXfYljTbTb69aOtPbUIM_Eng%20career%20guide%20-%20Raylene.jpg"></figure><p data-label="Paragraph">‚ÄúWe‚Äôre incredibly lucky that in engineering, unlike many industries, management is not the only path to growth. <strong>Despite that,</strong> <strong>too many believe that it‚Äôs a binary choice, that you‚Äôre locked into a path once you‚Äôve started on it. In reality, the skills required to succeed as a technical lead or manager are much more connected than you think.</strong> I‚Äôve seen first-hand that many people switch between these roles throughout their careers or even at the same company,‚Äù she says.</p><p data-label="Paragraph">‚ÄúAt Stripe and Facebook, I hired several ICs who were formerly managers and looking for a break from that role. The thing they all had in common was the ability to break down large problems and deliver on technical projects ‚Äî everyone saw that in them, no matter what ladder they were on at the moment. That career flexibility is only possible if you‚Äôve nailed the skills of being a great team <em>and</em> technical leader.‚Äù</p><p data-label="Pullquote">Management and IC careers aren‚Äôt strictly parallel paths ‚Äî in practice, they criss-cross all over the place. But it‚Äôs only possible to toggle back and forth later on in your career if you put in the work early on. Push for the manager track prematurely and your technical lead chops may not be sharp enough to pull it off.</p><p data-label="Paragraph">In Yung‚Äôs experience, many engineers are focused on moving up the ranks as quickly as possible, asking questions such as, ‚ÄúHow can I become a people manager?‚Äô or ‚ÄúI‚Äôve been here for a year, what‚Äôs next?‚Äù or ‚ÄúHow can I get more headcount on my team?‚Äù to skip ahead.</p><p data-label="Paragraph">‚ÄúThose are all questions centered around a desire to climb that ladder even faster. But that can be a rigid path to set yourself on, one that limits your options and flexibility in the future,‚Äù she says. Instead, Yung recommends engineers engage in serious introspection to deeply understand what motivates and excites them before moving on to the next step. </p><p data-label="Paragraph">‚Äú<strong>It‚Äôs about asking the </strong><strong><em>right</em></strong><strong> questions, the ones that keep you focused on growth and learning, not on moving up</strong>. My best teammates have been the ones who constantly pushed themselves to identify their weaknesses, systematically learn from their mistakes and get better,‚Äù she says.</p><p data-label="Pullquote">The best career advice I have for young engineers is to focus on learning instead of worrying about tracks and career ladders. That way, no matter what path you take as an IC, domain expert, engineering manager or even PM, you‚Äôll only get better over time.</p><p data-label="Paragraph">In the sections that follow, Yung walks us through each of these career steps to provide tailored advice, outlining common mistakes that are all too easy to make, as well as the more productive questions engineers should be asking instead. Drawing on examples from her own journey, she guides us along the path from early engineer to the transition into management to the most complex and senior roles an engineer can hold.</p><figure><img src="https://assets.proof.pub/2056/firstround/SaOhxGjNQTqlcZ1Kh2sf_Raylene-Yung_189RT.jpg" alt="Raylene Yung, former engineering and product leader at Stripe and Facebook"><figcaption>Raylene Yung, former engineering and product leader at Stripe and Facebook</figcaption></figure><h2 id="advice-for-early-engineers-and-ics-how-to-set-yourself-on-the-right-path">ADVICE FOR EARLY ENGINEERS AND ICs: HOW TO SET YOURSELF ON THE RIGHT PATH</h2><p data-label="Paragraph">Many new grads (or those who came to software engineering later on) have eagerness, drive, hunger ‚Äî and impatience ‚Äî in spades. But that impatience can have unintentional consequences if it‚Äôs not channeled in the right direction, says Yung.</p><p data-label="Paragraph">In her experience, these three moves can knock early engineers off-course:</p><p data-label="Paragraph"><strong>Skipping engineering fundamentals to pivot into other roles prematurely:</strong> ‚ÄúI‚Äôve seen people lose focus on their engineering work too soon, in hopes of becoming a manager or switching into another role like product management. Building a great foundation early on is critical in your career, no matter what function you ultimately want to end up in,‚Äù she says. ‚ÄúDon‚Äôt rush through the fundamentals at the beginning. There‚Äôs no real substitute for time spent with your hands on the keyboard.‚Äù</p><p data-label="Paragraph"><strong>Moving on from a team or company too quickly:</strong> ‚ÄúAs a hiring manager, I‚Äôm skeptical of candidates who say some variation of ‚ÄòI‚Äôve worked at X company for over a year, and I‚Äôve learned everything I can so it‚Äôs time to move on.‚Äô‚Äù she says. ‚Äú<strong>Sometimes the most interesting challenges only become tractable to you after a few years ‚Äî you might be overlooking massive learning opportunities that come with staying in your current role</strong>. The best engineers are those who‚Äôve taken the time to deeply understand a system or product and can apply their experience to a new problem.‚Äù</p><p data-label="Paragraph"><strong>Chasing the latest trend</strong>: ‚ÄúI get a lot of ‚ÄòHow can I learn more about [insert cool technology here]?‚Äô questions from early engineers,‚Äù says Yung. ‚ÄúTechnology continuously evolves, and while it‚Äôs important to be informed about the latest frameworks and best practices, learning this at the cost of developing fundamental skills is a bad tradeoff. At both Facebook and Stripe, you can interview in the language you feel most comfortable in, on the belief that if you‚Äôre a good engineer, your skills will help you pick things up as needed,‚Äù she says. ‚ÄúYou also don‚Äôt need to over-index on the new and shiny because what‚Äôs old can become new again. For example, when the Android platform first came out, no one really knew how to be a ‚Äògood Android developer.‚Äô Product teams were actually bootstrapped from infrastructure Java developers who were recruited over,‚Äù she says. ‚ÄúAs it turns out, experience with parallelism and multi-threaded applications was far more applicable to Android than the single-threaded, asynchronous request patterns being used in the main Facebook web app.‚Äù</p><p data-label="Pullquote">Frameworks come and go, but core programming fundamentals ‚Äî thinking through edge cases, debugging, the ability to learn new languages ‚Äî are what stand the test of time.</p><p data-label="Paragraph"><em>Ask these questions instead:</em></p><p data-label="Paragraph">Instead of plotting future functional pivots, jumping ship after a year, or spending too much time on the framework du jour, Yung recommends leaning on these two tactical, guiding questions:</p><p data-label="Paragraph"> <strong><em>How can I make every code change a great one? </em></strong>‚ÄúThis might seem super tactical, but every commit is a building block in software engineering, and if you build confidence with every code review, you‚Äôll move more quickly and take on larger challenges,‚Äù she says. ‚ÄúConversely, if every pull request is a chore and you continue to make common mistakes, it‚Äôs hard to build credibility with your team. One of the most common pieces of practical advice I give to new grad engineers is to always review your own code before you submit it to others. Catch your own mistakes and quickly learn to not make them again.‚Äù</p><p data-label="Paragraph"> <strong><em>How can I be as good at [X] as [this person] on my team? </em></strong>‚Äú<a href="https://firstround.com/review/how-to-be-a-career-changing-mentor-25-tips-from-the-best-mentors-we-know/" title="null" target="_blank">Mentors come in all shapes and sizes</a>, and are always around you ‚Äî identify what they‚Äôre best at and try to absorb their superpowers,‚Äù says Yung. Take this example from her first year as a new grad engineer: ‚ÄúI had two teammates with a lot more experience than me. <a href="https://hupp.org/adam/" title="null" target="_blank">Adam</a> came from a graphics background and was quiet in meetings, but wrote amazing code and reviews. He always spotted the flaw in my designs. Mark came from a product and business focused background. He spent less time on design but was an incredibly fast coder and thought through every edge case. I wanted to be as good at design and code review as Adam, and as good at product as Mark, so I vowed to get as much code reviewed as I possibly could by the former and get all my user-facing changes reviewed by the latter.‚Äù</p><h2 id="technical-leads-soak-up-all-you-can-to-build-a-strong-foundation-regardless-of-your-future-path">Technical leads, soak up all you can to build a strong foundation regardless of your future path. </h2><p data-label="Paragraph">After a few years of mastering the fundamentals, engineers are ready to move on up, earning titles such as Senior Engineer or Technical Lead. But as she alluded to earlier, in Yung‚Äôs experience, often engineers rush to move to the next level or switch into management.</p><p data-label="Paragraph"><em>Instead, lean on these four growth-focused questions to make sure the technical lead well you‚Äôve drilled is deep enough to support future moves:</em></p><p data-label="Paragraph"><strong><em>Do I deeply understand the systems I work on, and how they can break or be improved? </em></strong>‚ÄúThis goes beyond knowing the ins and outs of the systems or products you own,‚Äù says Yung. ‚ÄúLet‚Äôs say you're building some new infrastructure used to launch your product in one country today. Not only do you need to think about maintainability now, but you need to think about what‚Äôs next. What if we need to launch in ten new countries? A hundred countries? What would break? How would your current design change? Pushing yourself to think through a wide range of scenarios helps you not only deepen your understanding of the here and now, but also helps you strategically steer into the future.‚Äù</p><p data-label="Paragraph"><strong><em>How do I know we are working on the right things? </em></strong>‚ÄúBeing a technical lead is not just about seeing around corners and solving problems with well-designed, scalable and well-tested solutions. Some of the best technical leads I know are the ones who can also tell you <em>why</em> what they‚Äôre working on matters, and why it‚Äôs important to do <em>now</em>,‚Äù says Yung. ‚ÄúThis doesn‚Äôt mean you‚Äôll always get it right, and some of the best ones will stop themselves when they go down the wrong path.‚Äù</p><p data-label="Paragraph"><strong><em>How can I build better people leadership skills? </em></strong>Regardless of whether you want to become a manager in the future, there are many ways to build team and people skills that will help you in your engineering career. ‚ÄúStart to own projects more holistically, taking into account not only the technical challenges but also the people side ‚Äî understand who‚Äôs working on what and why, and how people work together,‚Äù says Yung. ‚ÄúHelp less experienced engineers grow by mentoring interns and new team members. Work cross-functionally and think of your PMs and partners as your teammates. If you do want to become a manager later, this will help give you a much stronger sense of whether you‚Äôll enjoy making the switch.‚Äù</p><h2 id="domain-experts-double-check-to-make-sure-you-arent-getting-trapped">Domain experts, double check to make sure you aren‚Äôt getting trapped.</h2><p data-label="Paragraph">Some engineers relish the opportunity to become deep-domain experts, particularly by staying in one part of the company or technical system. But especially as the years go by, check in to make sure you‚Äôre still growing and learning. ‚ÄúIf you stop learning, you can become complacent and rely on your deep background to solve problems the way you ‚Äòjust know‚Äô is right because you‚Äôve seen it before. <strong>Not only have you stopped growing, but even worse, you may have stopped building the best solutions</strong>,‚Äù says Yung.</p><p data-label="Pullquote">As you advance in your engineering career, you‚Äôll become an expert in a particular domain and feel as though you‚Äôre the ‚Äúonly one‚Äù who can handle it. That may be true, but if you‚Äôre burning out, it‚Äôs really not worth it ‚Äî and you‚Äôre probably selling your teammates short.</p><p data-label="Paragraph"><em>If you find yourself at this point in your engineering career, ask these questions to make sure your deep expertise isn‚Äôt holding you ‚Äî or your teammates ‚Äî back:</em></p><p data-label="Paragraph"><strong><em>How do I keep learning and stay challenged? </em></strong>Look out for signs that your growth is stalling and push yourself to find new sources of inspiration from other people, teams or systems. ‚ÄúAfter years of working on privacy at Facebook, I knew how to do precise surgery on the existing infrastructure, but didn‚Äôt have any big new ideas on how to make things better,‚Äù says Yung. ‚ÄúLooking back, that was a red flag. The feeling of being essential, of being needed for my expertise had masked that gap. It wasn‚Äôt until we merged with another team that was working on an entirely different approach that I felt like I was learning again. This led to a great renaissance and a complete re-invention of our entire system.‚Äù</p><p data-label="Paragraph"><strong><em>How do I continue to show value when I‚Äôm starting over in a new area? </em></strong>Another barrier to growth is the fear that you‚Äôll never reach the same level of impact by doing something different. ‚ÄúIt‚Äôs true that it will take time to transition, but if you‚Äôve become an expert in one area you‚Äôre likely capable of doing it again <em>and</em> you‚Äôll be able to go deeper faster next time using the skills you bring with you,‚Äù says Yung. One rule of thumb for onboarding very experienced hires is that for every year they spent in their last role, they need one month of slow initial onboarding time; once ramped up, their impact accelerates quickly.</p><p data-label="Paragraph"><strong><em>How can I transfer knowledge to the people around me and lay out a strategy for future generations? </em></strong>Think about how you can transfer knowledge to even more people, at a deeper level of complexity and a higher level of efficiency. ‚ÄúIf you find yourself explaining something to one person at a time, find a better way to do it. Develop documentation, training programs, or reusable components and infrastructure to pass knowledge on at scale,‚Äù says Yung. ‚ÄúStripe has a strong culture of information sharing and discovery, where anyone can search across shared emails and documents. But, even in such an information-rich environment, engineers who were deeply knowledgeable about a system were often less impactful than those who could also communicate about it in a usable and digestible way.‚Äù</p><figure><img src="https://assets.proof.pub/2056/firstround/7aBEVklHR9uvnvSFwXtG_Raylene-Yung_230RT.jpg"></figure><h2 id="advice-for-engineers-moving-along-the-management-track-how-to-better-support-your-team">ADVICE FOR ENGINEERS MOVING ALONG THE MANAGEMENT TRACK: HOW TO BETTER SUPPORT YOUR TEAM</h2><p data-label="Paragraph">First-time engineering <a href="https://books.firstround.com/management/" title="null" target="_blank">managers face a steep learning curve</a>, with added pressure to still set a good technical example for the team. To top it off, you can‚Äôt measure your new people responsibilities work in a way that‚Äôs as satisfyingly concrete or objective as writing code was. <strong>An easy mistake to make is to try and work around this by clinging on to your coding time, even if it means doing it at night after your ‚Äúday job‚Äù as a manager is done.</strong></p><p data-label="Paragraph">Instead, Yung advises making peace with this shift. ‚ÄúNot feeling productive and overcompensating with technical work can be unbelievably stressful. It can make you wonder if transitioning to management was a mistake. These emotions can last for a while ‚Äî but <a href="https://firstround.com/review/make-friends-with-the-monster-chewing-on-your-leg-and-other-tips-for-surviving-startups/" title="null" target="_blank">it‚Äôs a mistake to act on them</a>. Ride it out for at least six months to a year if you can, and then look back to evaluate how you feel,‚Äù she says.</p><p data-label="Paragraph"><em>Instead of worrying about your own contributions, ask these questions as a new engineering manager:</em></p><p data-label="Paragraph"><strong><em>How healthy is my team? How high-quality and high-impact is our work?</em></strong> As a new manager, it‚Äôs easy to get derailed by specific challenges or buoyed by big wins. Don‚Äôt lose sight of the overall output and health of your team. ‚ÄúEven if you write the best performance reviews, diligently hold your 1:1s every week and go to every new manager training session, if your team isn‚Äôt showing results your impact will be lower than it should be,‚Äù says Yung. But there‚Äôs also the risk of pushing <em>too hard</em> for results and compromising on quality and team health. ‚ÄúYou could be working on all the right things and driving up the metrics, but your on-call rotation is on fire and causes engineers to burn out and switch teams. <strong>The key is to invest in all aspects of your team and keep them in balance</strong> ‚Äî even if some changes may seem counterproductive in the short-term, like slowing down your roadmap to invest in developer efficiency and team happiness, they‚Äôll increase your impact over time.‚Äù</p><p data-label="Paragraph"><strong><em>How well can my team operate without me?</em></strong> ‚ÄúAn old manager once told me that he could roughly gauge his level of experience by how long he could be away without his team getting too far off course,‚Äù says Yung. ‚ÄúAt first it was a few days, then maybe a week. Eventually he could stretch it to a month, and then several for paternity leave. Now he aspires to be able to go away for a full year. I suspect most companies won‚Äôt let you actually be away that long, but it‚Äôs a helpful litmus test to try for yourself. <strong>Growing as a manager is not just about transferring knowledge, it‚Äôs also about building the right team and filling in the gaps.</strong> Maybe you have a team of folks who, given the right tasks, can plow through them faster than you expect and ask for more. But if they‚Äôre still relying on you to tell them what‚Äôs next and break down the next project, you‚Äôll need someone to step in if you try to go on vacation.‚Äù</p><h2 id="experienced-managers-dont-default-to-adding-more-engineers">Experienced managers, don‚Äôt default to adding more engineers. </h2><p data-label="Paragraph">After gaining experience as a manager, concerns start to shift. You‚Äôve mastered the art of shipping projects, but at some point you might hit diminishing returns. In Yung‚Äôs experience, one trap many managers fall into is relying too much on team expansion. ‚ÄúIt‚Äôs easy to get on the treadmill and only think about growth and headcount,‚Äù she says. ‚ÄúBut while adding more people might give you the illusion of progress, it could be papering over real issues and making them harder to solve.‚Äù</p><p data-label="Pullquote">Adding more headcount isn‚Äôt always the cure. It often creates more problems than it solves.</p><p data-label="Paragraph"><em>To move beyond increasing headcount, ask these questions to get more out of your team:</em></p><p data-label="Paragraph"><strong><em>Why does my team exist, and why does what we work on matter? </em></strong>‚ÄúTalk about your long-term goals and objectives, rather than only the projects on your plate today. Help your team think further ahead, and invite them to suggest different paths to your goals, or debate whether they are the right ones‚Äù says Yung. If you can‚Äôt explain why your goals and projects matter, maybe you‚Äôre working on the wrong things. Solicit feedback and don‚Äôt be afraid to change them.‚Äù</p><p data-label="Paragraph"><strong><em>How can I help my teammates grow, across all experience levels and scenarios?</em></strong><em> </em>Standout engineering managers can capably manage engineers of different experience levels. The telltale sign of a less experienced manager is when they‚Äôre great for new grads, but rarely have the most senior engineers on their team ‚Äî often due to a mutual unwillingness to manage or be managed by them. Yung‚Äôs advice? Instead of trying to work around the problem, tackle it head on. ‚ÄúOne of my first team members had been working for twelve more years than I had ‚Äî I was intimidated, but decided to try and learn from the experience,‚Äù she says. ‚ÄúIn our first 1:1, I shared areas I thought I could help him with (e.g. product domain knowledge, company context), and asked for his help in others (e.g. managing senior engineers, scaling complex systems). Over the next few years, we developed a strong working relationship and learned a lot from one another.‚Äù</p><h2 id="advice-for-leading-organizations-how-to-manage-in-uncharted-territory">ADVICE FOR LEADING ORGANIZATIONS: HOW TO MANAGE IN UNCHARTED TERRITORY</h2><p data-label="Paragraph">‚ÄúSimilar to the initial transition from individual contributor to engineering manager, moving to managing multiple teams or organizations can feel like an entirely new job,‚Äù says Yung. ‚ÄúIt becomes even easier to fall into the headcount trap, as the need for growth compounds with the number of managers on your team. On top of that, <strong>once you start managing managers it can be tempting to spot new managers everywhere</strong>. And sometimes, you‚Äôll be most tempted to select the reluctant, those incredible technical leaders who don‚Äôt really want to manage,‚Äù she says.</p><p data-label="Paragraph">Of course, there are ‚Äúwar times‚Äù when it‚Äôs all hands on deck and battlefield transitions might be necessary. But do it with care, Yung cautions. ‚ÄúEarly at Stripe, I transitioned three engineers into management because we needed immediate support and were still spinning up our recruiting pipeline. Two of them grew to really enjoy the role; the third realized they were happiest when coding and providing technical leadership and switched back,‚Äù she says. ‚ÄúThe key is to work closely to develop new managers ‚Äî make sure they go into it with eyes wide open and have an opportunity to learn, but also give them the room to switch back. By keeping people in ill-suited management roles, you risk not only burning out some of your most productive team members, but are also preventing yourself from finding someone better.‚Äù</p><p data-label="Pullquote">Too many leaders convince the reluctant to become or stay managers, drafting them into duty. This might be necessary in crunchtime, but keeping unwilling managers in roles for too long is a costly mistake.</p><p data-label="Paragraph"><em>Organizational leaders</em> <em>should ask these questions to increase their effectiveness:</em></p><p data-label="Paragraph"><strong><em>What unites your teams and why are you supporting them together?</em></strong><em> </em>In high-growth companies, reorgs are constant and expected. There‚Äôs a perennial switch between horizontal, vertical, domain-specific, or user/product-specific matrixed team structures ‚Äî which means org leads can often end up with a disparate set of groups under their wing. Done wrong, this can feel like whiplash, where new organizations feel arbitrarily grouped together and lack a broader purpose. ‚ÄúAt one point at Stripe, I was leading teams that built applications using specific eng and product skills, such as frontend development, ML and understanding the SaaS business model. At first, it did feel almost like a grab bag of apps that were far from Stripe‚Äôs core business,‚Äù says Yung. ‚ÄúBut after work and iteration, we tied the mission to two things: One, enhancing the core payments platform by building powerful features that might not be needed by every user, but were critical to those who did, and two, being the stewards of a new business model that could compound over time and be a meaningful company driver. This helped paint a more coherent vision and enabled people to explain what they were working on, both to themselves and to other teams.‚Äù</p><p data-label="Paragraph"><strong><em>Are your teams as healthy as they can be? Are they even the right teams?</em></strong> Don‚Äôt be afraid to take action if you can‚Äôt confidently answer why your teams fit together or if you‚Äôre working on the right things. ‚ÄúThink about bigger cross-team improvements to make. Maybe one team shouldn‚Äôt exist at all. Maybe other teams don‚Äôt have enough people for their most important projects. Enlist help from your manager, peers and other cross-functional partners to assess your teams and make changes,‚Äù says Yung. ‚ÄúI like to assess along the following axes: <strong><em>people</em></strong> (Do I have the right skill sets and leaders on the team? Are teams engaged and are people getting the development and growth opportunities they need?), <strong><em>projects and goals</em></strong> (Are we working on the highest impact things right now? Do people understand what they are and why we are working on them?) and <strong><em>progress</em></strong> (Are we moving at the right pace and investing time in a high-leverage way? What would speed us up the most, both for the short-term and long-term?)‚Äù</p><p data-label="Paragraph"><strong><em>Am I spending time with my teams in the right way?</em></strong> ‚ÄúIn the same way that knowing the stack and technical work can help you be a good first-line manager, knowing how your managers and their teams operate day-to-day can help you grow as an org leader. Do what you can to support each team and don‚Äôt shy away from gathering first-hand knowledge. Attend team meetings as a guest observer, do skip-level 1:1s, and communicate transparently along the way,‚Äù says Yung. <strong>But don‚Äôt let gathering information take you too deeply into the weeds.</strong> ‚ÄúIt‚Äôs easy to get caught in that awkward spot where you‚Äôre helping debug local team issues in lieu of making changes that help your org overall. You also might feel the impulse to step in and direct the groups that report to you. But now is when the ‚Äòask don‚Äôt tell‚Äô approach is more important than ever. Look to your group leads to tell you what they need. Maximize your value by providing the external context they don‚Äôt have and contributing at a broader and higher level.‚Äù</p><p data-label="Pullquote">Org leadership is about supporting, listening and coaching, not actively directing. Your teammates at each layer below you know strictly more about their day-to-day challenges than you will, so your meddling is low ROI.</p><h2 id="tying-it-all-together">TYING IT ALL TOGETHER</h2><p data-label="Paragraph">If Yung had to distill all of this advice into a simple checklist for engineers pushing for career growth, here‚Äôs what she‚Äôd say:</p><p data-label="Paragraph">Strive to be the most valuable, but least critical. Think about how long you could be away from your team and how far off course they would get. Do everything you can to close that gap.</p><p data-label="Paragraph">Focus on growth and learning at every step ‚Äî not on climbing the ladder as quickly as possible. Don‚Äôt move on from a role too quickly, and try not to get caught up in chasing the latest trends.</p><p data-label="Paragraph">The best engineers can toggle back and forth between manager and IC roles. Make sure your technical foundation and fundamental skills are solid enough to pull that off.</p><p data-label="Paragraph">Do your homework before signing on to become a manager, and try your best to prepare for the emotional ups and downs coming your way.</p><p data-label="Paragraph">If you‚Äôre a new manager, focus on the health and impact of your team, not on how much code you‚Äôre personally shipping.</p><p data-label="Paragraph">Before you request additional headcount, drill down into why your team exists and how you can help every engineer level up.</p><p data-label="Paragraph">Don‚Äôt lose the forest for the trees when you‚Äôre leading a large org. Dig in enough to understand what‚Äôs going on, but focus your contributions on where they‚Äôll matter most.</p><p data-label="Paragraph"><em>Photography by </em><em><a href="http://www.bonnieraemillsphoto.com/" title="null" target="_blank">Bonnie Rae Mills</a></em><em>. Charts courtesy of Raylene Yung.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta's Segment Anything written with C++ / GGML (211 pts)]]></title>
            <link>https://github.com/YavorGIvanov/sam.cpp</link>
            <guid>37398891</guid>
            <pubDate>Tue, 05 Sep 2023 22:49:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/YavorGIvanov/sam.cpp">https://github.com/YavorGIvanov/sam.cpp</a>, See on <a href="https://news.ycombinator.com/item?id=37398891">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">SAM.cpp</h2>
<p dir="auto">Inference of Meta's <a href="https://github.com/facebookresearch/segment-anything/">Segment Anything Model</a> in pure C/C++</p>
<details open="">
  <summary>
    
    <span aria-label="Video description demo-0.mp4">demo-0.mp4</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/1991296/265760997-a69be66f-8e27-43a0-8a4d-6cfe3b1d9335.mp4" data-canonical-src="https://user-images.githubusercontent.com/1991296/265760997-a69be66f-8e27-43a0-8a4d-6cfe3b1d9335.mp4" controls="controls" muted="muted">

  </video>
</details>

<h2 tabindex="-1" dir="auto">Quick start</h2>
<div dir="auto" data-snippet-clipboard-copy-content="git clone --recursive https://github.com/YavorGIvanov/sam.cpp
cd sam.cpp"><pre>git clone --recursive https://github.com/YavorGIvanov/sam.cpp
<span>cd</span> sam.cpp</pre></div>
<p dir="auto">Note: you need to download the model checkpoint below (<code>sam_vit_b_01ec64.pth</code>) first from <a href="https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth" rel="nofollow">here</a> and place it in the <code>checkpoints</code> folder</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Convert PTH model to ggml. Requires python3, torch and numpy
python convert-pth-to-ggml.py checkpoints/sam_vit_b_01ec64.pth . 1

# Build sam.cpp. Might require cmake and SDL2 to be installed
mkdir build &amp;&amp; cd build
cmake .. &amp;&amp; make -j4

# run inference
./bin/sam -t 16 -i ../img.jpg -m ../checkpoints/ggml-model-f16.bin"><pre><span><span>#</span> Convert PTH model to ggml. Requires python3, torch and numpy</span>
python convert-pth-to-ggml.py checkpoints/sam_vit_b_01ec64.pth <span>.</span> 1

<span><span>#</span> Build sam.cpp. Might require cmake and SDL2 to be installed</span>
mkdir build <span>&amp;&amp;</span> <span>cd</span> build
cmake .. <span>&amp;&amp;</span> make -j4

<span><span>#</span> run inference</span>
./bin/sam -t 16 -i ../img.jpg -m ../checkpoints/ggml-model-f16.bin</pre></div>
<h2 tabindex="-1" dir="auto">Downloading and converting the model checkpoints</h2>
<p dir="auto">You can download a <a href="https://github.com/facebookresearch/segment-anything/tree/main#model-checkpoints">model checkpoint</a> and convert it to <code>ggml</code> format using the script <code>convert-pth-to-ggml.py</code>:</p>
<div data-snippet-clipboard-copy-content="# Convert PTH model to ggml
python convert-pth-to-ggml.py sam_vit_b_01ec64.pth . 1"><pre><code># Convert PTH model to ggml
python convert-pth-to-ggml.py sam_vit_b_01ec64.pth . 1
</code></pre></div>
<h2 tabindex="-1" dir="auto">Example output on M2 Ultra</h2>
<div data-snippet-clipboard-copy-content=" $ ‚ñ∂ make -j sam &amp;&amp; time ./bin/sam -t 8 -i img.jpg
[ 28%] Built target common
[ 71%] Built target ggml
[100%] Built target sam
main: seed = 1693224265
main: loaded image 'img.jpg' (680 x 453)
sam_image_preprocess: scale = 0.664062
main: preprocessed image (1024 x 1024)
sam_model_load: loading model from 'models/sam-vit-b/ggml-model-f16.bin' - please wait ...
sam_model_load: n_enc_state      = 768
sam_model_load: n_enc_layer      = 12
sam_model_load: n_enc_head       = 12
sam_model_load: n_enc_out_chans  = 256
sam_model_load: n_pt_embd        = 4
sam_model_load: ftype            = 1
sam_model_load: qntvr            = 0
operator(): ggml ctx size = 202.32 MB
sam_model_load: ...................................... done
sam_model_load: model size =   185.05 MB / num tensors = 304
embd_img
dims: 64 64 256 1 f32
First &amp; Last 10 elements:
-0.05117 -0.06408 -0.07154 -0.06991 -0.07212 -0.07690 -0.07508 -0.07281 -0.07383 -0.06779
0.01589 0.01775 0.02250 0.01675 0.01766 0.01661 0.01811 0.02051 0.02103 0.03382
sum:  12736.272313

Skipping mask 0 with iou 0.705935 below threshold 0.880000
Skipping mask 1 with iou 0.762136 below threshold 0.880000
Mask 2: iou = 0.947081, stability_score = 0.955437, bbox (371, 436), (144, 168)


main:     load time =    51.28 ms
main:    total time =  2047.49 ms

real	0m2.068s
user	0m16.343s
sys	0m0.214s"><pre><code> $ ‚ñ∂ make -j sam &amp;&amp; time ./bin/sam -t 8 -i img.jpg
[ 28%] Built target common
[ 71%] Built target ggml
[100%] Built target sam
main: seed = 1693224265
main: loaded image 'img.jpg' (680 x 453)
sam_image_preprocess: scale = 0.664062
main: preprocessed image (1024 x 1024)
sam_model_load: loading model from 'models/sam-vit-b/ggml-model-f16.bin' - please wait ...
sam_model_load: n_enc_state      = 768
sam_model_load: n_enc_layer      = 12
sam_model_load: n_enc_head       = 12
sam_model_load: n_enc_out_chans  = 256
sam_model_load: n_pt_embd        = 4
sam_model_load: ftype            = 1
sam_model_load: qntvr            = 0
operator(): ggml ctx size = 202.32 MB
sam_model_load: ...................................... done
sam_model_load: model size =   185.05 MB / num tensors = 304
embd_img
dims: 64 64 256 1 f32
First &amp; Last 10 elements:
-0.05117 -0.06408 -0.07154 -0.06991 -0.07212 -0.07690 -0.07508 -0.07281 -0.07383 -0.06779
0.01589 0.01775 0.02250 0.01675 0.01766 0.01661 0.01811 0.02051 0.02103 0.03382
sum:  12736.272313

Skipping mask 0 with iou 0.705935 below threshold 0.880000
Skipping mask 1 with iou 0.762136 below threshold 0.880000
Mask 2: iou = 0.947081, stability_score = 0.955437, bbox (371, 436), (144, 168)


main:     load time =    51.28 ms
main:    total time =  2047.49 ms

real	0m2.068s
user	0m16.343s
sys	0m0.214s
</code></pre></div>
<p dir="auto">Input point is (414.375, 162.796875) (currently hardcoded)</p>
<p dir="auto">Input image:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/8558655/261301565-37b7bf4b-bf91-40cf-8ec1-1532316e1612.jpg"><img src="https://user-images.githubusercontent.com/8558655/261301565-37b7bf4b-bf91-40cf-8ec1-1532316e1612.jpg" alt="llamas"></a></p>
<p dir="auto">Output mask (mask_out_2.png in build folder):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/8558655/265732931-e7e31285-7efc-4009-98c8-57fd819bdfc1.png"><img src="https://user-images.githubusercontent.com/8558655/265732931-e7e31285-7efc-4009-98c8-57fd819bdfc1.png" alt="mask_glasses"></a></p>
<h2 tabindex="-1" dir="auto">References</h2>
<ul dir="auto">
<li><a href="https://github.com/ggerganov/ggml">ggml</a></li>
<li><a href="https://github.com/ggerganov/ggml/tree/master/examples/sam">ggml SAM example</a></li>
<li><a href="https://segment-anything.com/" rel="nofollow">SAM</a></li>
<li><a href="https://segment-anything.com/demo" rel="nofollow">SAM demo</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Next steps</h2>
<ul>
<li> Reduce memory usage by utilizing the new ggml-alloc</li>
<li> Remove redundant graph nodes</li>
<li> Make inference faster</li>
<li> Fix the difference in output masks compared to the PyTorch implementation</li>
<li> Filter masks based on stability score</li>
<li> Add support for point user input</li>
<li> Support F16 for heavy F32 ops</li>
<li> Test quantization</li>
<li> Support bigger model checkpoints</li>
<li> GPU support</li>
<li> Add support for mask and box input</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Fully client-side GPT2 prediction visualizer (135 pts)]]></title>
            <link>https://perplexity.vercel.app/</link>
            <guid>37398812</guid>
            <pubDate>Tue, 05 Sep 2023 22:42:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://perplexity.vercel.app/">https://perplexity.vercel.app/</a>, See on <a href="https://news.ycombinator.com/item?id=37398812">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[HashiCorp did it backwards (111 pts)]]></title>
            <link>https://galenmarchetti.substack.com/p/hashicorp-did-it-backwards</link>
            <guid>37398254</guid>
            <pubDate>Tue, 05 Sep 2023 21:50:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://galenmarchetti.substack.com/p/hashicorp-did-it-backwards">https://galenmarchetti.substack.com/p/hashicorp-did-it-backwards</a>, See on <a href="https://news.ycombinator.com/item?id=37398254">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c1dcfc-96b7-4654-9ffc-73f01c347051_994x300.jpeg" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c1dcfc-96b7-4654-9ffc-73f01c347051_994x300.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c1dcfc-96b7-4654-9ffc-73f01c347051_994x300.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c1dcfc-96b7-4654-9ffc-73f01c347051_994x300.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c1dcfc-96b7-4654-9ffc-73f01c347051_994x300.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c1dcfc-96b7-4654-9ffc-73f01c347051_994x300.jpeg" width="994" height="300" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/88c1dcfc-96b7-4654-9ffc-73f01c347051_994x300.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:300,&quot;width&quot;:994,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:33325,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c1dcfc-96b7-4654-9ffc-73f01c347051_994x300.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c1dcfc-96b7-4654-9ffc-73f01c347051_994x300.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c1dcfc-96b7-4654-9ffc-73f01c347051_994x300.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c1dcfc-96b7-4654-9ffc-73f01c347051_994x300.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>Hashicorp recently announced they‚Äôre changing the license of Terraform to a source-available license, BSL1.1. Source-available as in </span><strong>not</strong><span> </span><a href="https://opensource.org/licenses/" rel="nofollow ugc noopener">OSI-certified</a><span> ‚Äúopen-source‚Äù, and very much not </span><a href="https://www.gnu.org/philosophy/shouldbefree.en.html" rel="nofollow ugc noopener">Stallman‚Äôs definition</a><span> of free software.</span></p><p><span>Terraform was open source for a long time. Meaning, every version of Terraform prior to the license shift was released with a little </span><code>LICENSE</code><span> file specifying that the code was open source under the OSI-approved Mozilla Public License. You can see where they changed that </span><a href="https://github.com/hashicorp/terraform/commit/b145fbcaadf0fa7d0e7040eac641d9aef2a26433" rel="nofollow ugc noopener">here</a><span>.&nbsp;</span></p><p><span>All previous versions of Terraform, before they changed the license, are still open-source. If you want you can even still get open-source Terraform from their Github organization‚Ää‚Äî‚Ää</span><a href="https://github.com/hashicorp/terraform/tree/v1.5.6" rel="nofollow ugc noopener">have fun</a><span>.</span></p><p><span>The problem is that a lot of people contributed to Terraform, and adopted Terraform, on the understanding that the </span><em>project</em><span> was open-source. A lot of people built businesses using Terraform on the understanding that the </span><em>project</em><span> was open-source. Unfortunately the license does not apply to the project, but to the version of the code released at a given time.</span></p><p><span>However, the Terraform </span><em>brand, </em><span>and Terraform‚Äôs </span><em>mindshare</em><span> as a developer tool, is not tied to any particular version. Those things grow as usage of the project grows, and don‚Äôt go away as versions change. Unlike brand and mindshare, which Hashicorp is fortunate to hold in abundance, people‚Äôs rights as users of Terraform do change when they upgrade from </span><code>v1.5.6</code><span> to </span><code>v1.6.0</code><span>&nbsp;.</span></p><p>I think this is the heart of the Terraform license changing issue. The non-Hashicorp contributors who worked on the Terraform project helped Hashicorp gain developer mindshare and improve their brand, and they had an implicit and entirely reasonable assumption that they were doing so because the code was open-source.&nbsp;</p><p>Hashicorp is well within their legal rights to change the license from version to version, but they‚Äôre the ones that get to retain the Terraform mindshare and brand that was built under the old license. Those don‚Äôt go away.</p><p><span>At the heart of this, I think changing from OSI-approved licenses to BSL is just not the best way for a project to evolve. Generally speaking licenses should probably become </span><em>more </em><span>permissive over time‚Ä¶start BSL and then choose to go OSI-approved later. That way you‚Äôre never breaking the implicit assumptions of trust that run in open-source communities.&nbsp;</span></p><p>The backlash makes sense. Hashicorp‚Äôs side makes sense, too. Who is taking advantage of who, anyway? Hashicorp funded the majority of the development of Terraform. Shouldn‚Äôt they be able to profit from their risk taking, and their effort? Isn‚Äôt that what drives American innovation?</p><p>On the other hand, Terraform‚Äôs contributors worked on Terraform for free. They just gave to the project with no expectation of reward, under the ethos implied by the OSI-license tagged to the software. Now, Hashicorp is changing the rules of the project for future versions for their own profit.</p><p><span>The beautiful thing about this whole situation is that these contributors are well within their rights to self-organize, </span><a href="https://github.com/opentffoundation/opentf" rel="nofollow ugc noopener">fork Terraform</a><span> into their own foundation, and </span><a href="https://opentf.org/" rel="nofollow ugc noopener">keep the dream alive</a><span>. And they‚Äôre doing so, and we get to see how this evolves.</span></p><p>My main takeaway is‚Ä¶if you want to start a business building developer tooling and you‚Äôre concerned about competitors using your work, you should probably start source-available (BSL) and then go for a license that is OSI-approved. That way, expectations are clear and understood from the beginning.</p><p><em><span>Full disclaimer: I‚Äôm building a developer tool business and we‚Äôve been source-avalailable, BSL1.1 licensed from our first public launch: </span><a href="https://github.com/kurtosis-tech/kurtosis" rel="nofollow ugc noopener">https://github.com/kurtosis-tech/kurtosis</a></em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google was incorporated 25 years ago on September 4 1998 (175 pts)]]></title>
            <link>https://about.google/intl/ALL_us/our-story/</link>
            <guid>37398202</guid>
            <pubDate>Tue, 05 Sep 2023 21:46:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://about.google/intl/ALL_us/our-story/">https://about.google/intl/ALL_us/our-story/</a>, See on <a href="https://news.ycombinator.com/item?id=37398202">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-page-title="How we started and where we are today - Google" data-page-description="Find" out="" where="" it="" all="" began.="" read="" the="" history="" of="" how="" google="" has="" grown="" since="" larry="" page="" and="" sergey="" brin="" founded="" company="" in="" 1998.="" data-track-page-load="" data-ui-view="main">
    

    <div>
        <dl>
            <dt>Founded</dt>
            <dd>1998</dd>
            <dt>Founders</dt>
            <dd>Larry Page and Sergey Brin</dd>
            <dt>Incorporation</dt>
            <dd>September 4, 1998</dd>
            <dt>Initial public offering (NASDAQ)</dt>
            <dd>August 19, 2004</dd>
        </dl>
      <h2 data-unite-orphan="">From the garage to the Googleplex</h2>
    </div>

    <div>
      <article>
        <p>The Google story begins in 1995 at Stanford University. Larry Page was considering Stanford for grad school and Sergey Brin, a student there, was assigned to show him around. </p>
<p>By some accounts, they disagreed about nearly everything during that first meeting, but by the following year they struck a partnership. Working from their dorm rooms, they built a search engine that used links to determine the importance of individual pages on the World Wide Web. They called this search engine Backrub.</p>
<p>Soon after, Backrub was renamed Google (phew). The name was a play on the mathematical expression for the number 1 followed by 100 zeros and aptly reflected Larry and Sergey's mission ‚Äúto organize the world‚Äôs information and make it universally accessible and useful.‚Äù</p>
<p>Over the next few years, Google caught the attention of not only the academic community, but Silicon Valley investors as well. In August 1998, Sun co-founder Andy Bechtolsheim wrote Larry and Sergey a check for $100,000, and Google Inc. was officially born. With this investment, the newly incorporated team made the upgrade from the dorms to their first office: a garage in suburban Menlo Park, California, owned by Susan Wojcicki (employee #16 and former CEO of YouTube). Clunky desktop computers, a ping pong table, and bright blue carpet set the scene for those early days and late nights. (The tradition of keeping things <a href="https://www.google.com/search?q=google+office+locations&amp;espv=2&amp;biw=2560&amp;bih=1253&amp;site=webhp&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0CAYQ_AUoAWoVChMIv-ih3J7exwIVx0qICh3-DQ-Q" target="_blank">colorful</a> continues to this day.)</p>
<p>Even in the beginning, things were unconventional: from Google‚Äôs initial server (made of Lego) to the <a href="https://www.google.com/doodles/burning-man-festival" target="_blank">first ‚ÄúDoodle‚Äù</a> in 1998: a stick figure in the logo announcing to site visitors that the entire staff was playing hooky at the Burning Man Festival. ‚ÄúDon't be evil‚Äù captured the spirit of our intentionally unconventional methods. In the years that followed, the company expanded rapidly ‚Äî hiring engineers, building a sales team, and introducing the first company dog, <a href="http://googleblog.blogspot.com/2004/06/yoshkas-weekend-amble.html" target="_blank">Yoshka</a>. Google outgrew the garage and eventually moved to its current headquarters (a.k.a.‚ÄúThe Googleplex‚Äù) in Mountain View, California. The spirit of doing things differently made the move. So did Yoshka.</p>
<p>The relentless search for better answers continues to be at the core of everything we do. Today, Google makes hundreds of products used by billions of people across the globe, from YouTube and Android to Gmail and, of course, Google Search. Although we‚Äôve ditched the Lego servers and added just a few more company dogs, our passion for building technology for everyone has stayed with us ‚Äî from the dorm room, to the garage, and to this very day.</p>



      </article>
    </div>
  </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Poor people ‚Äòsurviving not living‚Äô as UK social contract collapses, says report (148 pts)]]></title>
            <link>https://www.theguardian.com/society/2023/sep/04/poor-people-surviving-not-living-as-uk-social-contract-collapses-says-report</link>
            <guid>37398026</guid>
            <pubDate>Tue, 05 Sep 2023 21:30:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/society/2023/sep/04/poor-people-surviving-not-living-as-uk-social-contract-collapses-says-report">https://www.theguardian.com/society/2023/sep/04/poor-people-surviving-not-living-as-uk-social-contract-collapses-says-report</a>, See on <a href="https://news.ycombinator.com/item?id=37398026">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>The collapse of the UK‚Äôs social contract is leaving millions of low-income families ‚Äúsurviving not living‚Äù and forced to endure unacceptable levels of poverty, according to an independent cross-party report.</p><p>The <a href="https://povertystrategycommission.org.uk/" data-link-name="in body link">Poverty Strategy Commission</a>, which seeks to forge a new national political consensus on reducing poverty, and which includes former ministers from the three main parties, says poverty levels are too high and <a href="https://www.theguardian.com/business/2023/aug/18/uk-poorest-families-fall-in-living-standards" data-link-name="in body link">hardship is becoming more extreme</a>. It warns a ‚Äúmore of the same‚Äù approach to poverty in the future will fail.</p><p>It estimates the broad annual cost of significantly reducing poverty in the UK at ¬£36bn ‚Äì equivalent to ¬£6,000 a year for 6 million families in poverty ‚Äì a figure reached through a combination of benefits and wage increases, and investment to lower housing and energy costs, and improved health services.</p><p>The interim report of the commission, published on Tuesday, seeks to put poverty reduction back on the party political agenda before the next general election. It is concerned at the lack of urgency from the two main parties over the scale and nature of poverty, and society‚Äôs failure to offer adequate protection to its poorest.</p><p>The report says not only have overall relative poverty rates barely budged at between 21% and 24% over the past two decades, but even in areas where some progress was made ‚Äì such as reducing pensioner and single-parent poverty ‚Äì they have started to go into reverse.</p><p>It also highlights the rise in the proportion of people in poverty experiencing ‚Äúdeep poverty‚Äù ‚Äì measured as at least 50% below the poverty line. It says nearly a third (31%) of those officially in hardship are in deep poverty and struggling to buy sufficient food, energy and clothes, pay rent and meet other everyday bills.</p><p>While the government often implies there is a ‚Äúsocial contract‚Äù between state and citizen, the commission says in practice this does not exist, noting that the social security system routinely fails to protect people from poverty even when they are working, are unable to work through disability, or are pensioners.</p><p>Its interim recommendations include a 5% uplift in benefits and wages for people in poverty, outside any inflation increases, although it says poverty cannot be tackled by raising incomes alone, and highlights housing and childcare costs, the additional costs of disability, and energy and travel costs as areas to be addressed.</p><p>The chair of the commission, Lady Philippa Stroud, is a Tory peer and former political adviser to Iain Duncan Smith when he was welfare secretary between 2010 and 2016. She put the commission together to try to forge a new political momentum for tackling poverty, and attempt to engineer broad agreement over how it should be done.</p><p>She said: ‚ÄúPoverty in the UK is too high and the experiences of many people in poverty are now getting worse. Governments of all colours have worked hard to change that picture, but as a society, we have failed to make significant progress.‚Äù</p><figure id="cab65f60-5437-4c52-b253-5498a53c9ef9" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" deferuntil="idle" props="{&quot;richLinkIndex&quot;:10,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/society/2023/sep/04/more-children-expected-to-arrive-at-uk-schools-with-dirty-clothes-and-hair&quot;,&quot;text&quot;:&quot;More children expected to arrive at UK schools with dirty clothes and hair&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;elementId&quot;:&quot;cab65f60-5437-4c52-b253-5498a53c9ef9&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}"></gu-island></figure><p>The commission, which has been meeting for 18 months, includes the Conservative MP and former work and pensions secretary Stephen Crabb, the former Liberal Democrat MP and coalition minister David Laws, and the Labour MP and former Treasury minister Stephen Timms. Thinktanks from left and right, charities including the Trussell Trust food bank network, and academic experts are also members.</p><div><p>The commission member Miatta Fahnbulleh, the head of the New Economics Foundation, said: ‚ÄúEveryone in 21st-century Britain should be able to afford the basics they need to avoid poverty. It‚Äôs shameful that instead millions of families are on the edge of destitution.‚Äù</p><p>She added: ‚ÄúIn recent years, we‚Äôve made little progress in tackling the causes of poverty: low wages, an inadequate social security system and sky-rocketing housing costs. As the commission shows, the solutions are there to tackle these issues. What we need now is the collective will.‚Äù</p></div><p>The final report of the commission is expected to be published at the end of the year.</p><p>Mel Stride, the secretary of state for work and pensions said:<strong> </strong>‚ÄúWe have helped nearly two million people out of absolute poverty after housing costs since 2010, including 400,000 children, and it‚Äôs absolutely right we have a joined-up approach to ensure this downward trend continues.</p><p>‚ÄúOur record financial support - benefits and pensions up by 10.1%, ¬£900 in direct Cost of Living payments to some of the most financially vulnerable, and our boost to the National Living Wage ‚Äì has cushioned those most in need from the worst of the cost of living pressures and we will continue to support them.</p><p>‚ÄúI look forward to reading the commission‚Äôs full report next year, and continuing with a collaborative approach across government and society to reduce poverty and support the most vulnerable.‚Äù</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Amazon uses chaos engineering to handle 80k requests per second (136 pts)]]></title>
            <link>https://community.aws/posts/how-search-uses-chaos-engineering</link>
            <guid>37397405</guid>
            <pubDate>Tue, 05 Sep 2023 20:39:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://community.aws/posts/how-search-uses-chaos-engineering">https://community.aws/posts/how-search-uses-chaos-engineering</a>, See on <a href="https://news.ycombinator.com/item?id=37397405">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="middle-panel"><article aria-labelledby="question-title-Any Day Can Be Prime Day: How Amazon.com Search Uses Chaos Engineering to Handle Over 84K Requests Per Second"><div><h2>Read time: ~<!-- -->21 minutes</h2><h2>Discover how Amazon Search combines technology and culture to empower its builder teams, ensuring platform resilience through Chaos Engineering.</h2><div><a href="https://github.com/setheliot"><div><p><img src="https://github.com/setheliot.png" alt="Seth Eliot"></p></div></a></div></div><div><p><h2>Table of Contents</h2></p></div><div>
<p>This is a story about Chaos Engineering, and how the high scale distributed services that power Search on <a href="http://amazon.com/?sc_channel=el&amp;sc_campaign=resiliencewave&amp;sc_geo=mult&amp;sc_country=mult&amp;sc_outcome=acq&amp;sc_content=how-search-uses-chaos-engineering" rel="noopener noreferrer">Amazon.com</a> use it to ensure that all customers can search Amazon‚Äôs expansive catalog whenever they need to. Chaos Engineering enables teams to experiment with faults or load to better understand how their applications will react, and therefore improve resilience. And this is also a story about DevOps, and how a single team dedicated to resilience was able to create technologies and drive changes that made it easier for the multiple teams that are part of Search to run Chaos experiments on the many services powering Search.</p>
<p>If you are looking to implement Chaos Engineering to improve resilience, looking for how to create an effective model to empower builders, or both, then read on.</p>
<h2 tabindex="-1"></h2>
<p>DevOps is a big topic and I cannot cover it all here. I do like the definition that my colleague Jacquie Grindrod wrote in her explanation of <a href="https://community.aws/concepts/what-is-devops?sc_channel=el&amp;sc_campaign=resiliencewave&amp;sc_geo=mult&amp;sc_country=mult&amp;sc_outcome=acq&amp;sc_content=how-search-uses-chaos-engineering" rel="noopener noreferrer">What is DevOps?</a></p>
<blockquote><p>DevOps is an approach to solving problems collaboratively. It values teamwork and communication, fast feedback and iteration, and removing friction or waste through automation.</p></blockquote>
<p>You may have heard about all the tools used in DevOps like Kubernetes, Terraform, and GitLab, but tools are not the right place to start. DevOps is about <em>culture.</em> See Jacquie‚Äôs definition above? <em>Collaboration</em>, <em>teamwork</em>, <em>communication</em> ‚Äî these are all part of culture. The automation tools then enable the culture to get stuff done.</p>
<p>As I said, DevOps is big, so here I will only focus on the primary DevOps concepts that are illustrated in the story of how Search adopted Chaos Engineering:</p>
<ul><li><strong>Empowering teams</strong>: DevOps fosters a culture where teams are empowered to try new ideas and learn from both successes and failures.</li><li><strong>Ownership and responsibility</strong>: In DevOps, teams own the services they build and are responsible for ensuring the right outcomes. Empowerment is a pre-requisite to this, as the team needs to be able to understand how their services are used and be able to implement changes they see fit.</li><li><strong>Breaking down walls:</strong> The impulse behind "DevOps" is to <a href="https://community.aws/posts/devops-wrong-answers-only#walls" rel="noopener noreferrer">break down the traditional barriers that often exist between development and operations teams</a>. By promoting collaboration and shared goals, DevOps aims to eliminate silos and create a more streamlined and efficient workflow.</li><li><strong>Enabling teams to do more</strong>: This is where automation and tools can play a major role. But also organizational structure and responsibilities are crucial here. A team that takes a hand-off from the development team and operates the service <em>for</em> them is not ideal (see ‚Äúbreaking down walls‚Äù above). But a specialized team that works <em>with</em> development and reduces the <a href="https://community.aws/posts/devops-wrong-answers-only#undifferentiated-heavy-lifting" rel="noopener noreferrer">undifferentiated heavy lifting</a> of operating the service is better. <em>Undifferentiated heavy lifting</em> is all the hard work (‚Äúheavy lifting‚Äù) that is necessary to accomplish a task (say, deploy and operate a service) but does vary appreciably from service to service (‚Äúundifferentiated‚Äù). If every service team has to do this work themselves, then it is wasteful. Having one team to create tools and processes that do much of this heavy lifting removes the burden from the service teams is liberating!</li></ul>
<h2 tabindex="-1"></h2>
<p>If you have shopped on Amazon.com or any of the Amazon sites worldwide you have probably used Search to find what you were looking for. Searching for a topic like Chaos Engineering returns over 1,000 results (Figure 1), and Amazon Search then lets you refine that search by many different parameters like language, book format, or release date.</p>
<figure><div data-rmiz-content="not-found" aria-owns="rmiz-modal-" data-rmiz=""><p><img alt="Amazon search results page showing results for the query: chaos engineering" title="Figure 1. Amazon Search returns over 1,000 results for &quot;chaos engineering&quot;" sizes="25vw" srcset="https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure1.webp&amp;w=256&amp;q=75 256w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure1.webp&amp;w=384&amp;q=75 384w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure1.webp&amp;w=640&amp;q=75 640w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure1.webp&amp;w=750&amp;q=75 750w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure1.webp&amp;w=828&amp;q=75 828w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure1.webp&amp;w=1080&amp;q=75 1080w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure1.webp&amp;w=1200&amp;q=75 1200w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure1.webp&amp;w=1920&amp;q=75 1920w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure1.webp&amp;w=2048&amp;q=75 2048w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure1.webp&amp;w=3840&amp;q=75 3840w" src="https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure1.webp&amp;w=3840&amp;q=75" decoding="async" data-nimg="fill" loading="lazy"></p></div><figcaption>Figure 1. Amazon Search returns over 1,000 results for "chaos engineering"</figcaption></figure>
<p>Over 1,000 results is a lot, and Search is responsible for quickly serving results from a catalog of many millions of products to over 300 million active customers. On Prime day 2022, Amazon Search served 84,000 requests per second. That is massive scale. The principles I will share with you here work to enable resilience at that scale, but they also work at whatever scale your systems run too.</p>
<p>Amazon Search consists of over 40 backend services, owned by different teams of builders (also known as <a href="https://aws.amazon.com/executive-insights/content/amazon-two-pizza-team?sc_channel=el&amp;sc_campaign=resiliencewave&amp;sc_geo=mult&amp;sc_country=mult&amp;sc_outcome=acq&amp;sc_content=how-search-uses-chaos-engineering" rel="noopener noreferrer">two-pizza teams</a>).  Each team has ownership of their service (or services), from design and implementation, to deployment and operation. So already we can see DevOps practices emerging in our story. When the team owns both development and operations, that is one way to adopt a culture of <strong>ownership and responsibility</strong>, and <strong>breaking down walls</strong> between development and operations. Amazon builder teams are able to own deployment and operation because there is an Amazon-wide builder tools team that creates tooling and processes, removing undifferentiated heavy lifting, and <strong>enabling teams to do more.</strong> A specialized team (Builder Tools) enables two-pizza teams to do more. We will see an echo of this approach later when we talk about how Search adopted Chaos Engineering.</p>
<h2 tabindex="-1"></h2>
<blockquote><p>Chaos Engineering is the discipline of experimenting on a system in order to build confidence in the system‚Äôs capability to withstand turbulent conditions in production. ‚Äì <a href="https://principlesofchaos.org/" rel="noopener noreferrer">Principles of Chaos Engineering</a></p></blockquote>
<p>Some folks are put off by the term ‚Äúchaos‚Äù, but it is important to know that Chaos Engineering is <em>not</em> about creating chaos. Instead, it is about protecting your applications from the chaos that is already in production by exposing them to chaos in a controlled manner. You apply the scientific method, creating a hypothesis. The hypothesis is based on how you have designed your application to stay resilient to specific events such as faults or load scenarios. Then you run an experiment by simulating those events, and observing how your application performs, testing the hypothesis. This will show you were your application is doing well against those events, or where you can improve it.</p>
<p>Watch <a href="https://community.aws/posts/chaos-engineering-2-minutes?sc_channel=el&amp;sc_campaign=resiliencewave&amp;sc_geo=mult&amp;sc_country=mult&amp;sc_outcome=acq&amp;sc_content=how-search-uses-chaos-engineering" rel="noopener noreferrer">Chaos Engineering in under 2 minutes</a> to learn more, and there is also a list of resources to learn more there.</p>
<p><strong>Empowering teams</strong> includes giving them the autonomy to create and run chaos experiments on their services.</p>
<h2 tabindex="-1"></h2>
<p>The Search Resilience Team is a two-pizza team within the Search organization, on a mission to improve and drive the resilience of the Amazon Search service. They bring everything I have discussed above together: DevOps + Amazon Search + Chaos Engineering. That said, they would not necessarily describe themselves a DevOps team, preferring to call themselves an operational excellence and site reliability engineering organization. But just like the Amazon builder tools team operates as a specialized team <strong>enabling teams to do more</strong> across all of Amazon, the Search Resilience Team operates as a specialized team <em>within</em> Search <strong>enabling teams to do more</strong> across the 40+ two-pizza teams that own services in the Search org.</p>
<p>Remember that the DevOps model does not have Search Resilience team creating nor owning the chaos experiments for their service teams. Instead they needed to create a scalable process, and the tech behind it, to make it easier for those service teams to create, own, and run chaos experiments, even in production. To do this the Search Resilience team created the Amazon Search Chaos Orchestrator.</p>
<h2 tabindex="-1"></h2>
<p>The Search Resilience team had many specific goals in creating this chaos orchestrator, which I will discuss a little later. But the overall goal was to create a system to make it easier for Search two-pizza teams to create and run chaos experiments with the services they own. Figure 2. shows an overview of the orchestrator.</p>
<figure><div data-rmiz-content="not-found" aria-owns="rmiz-modal-" data-rmiz=""><p><img alt="Architecture diagram showing a system for chaos experimentation orchestration across Search" title="Figure 2. A system for chaos experimentation orchestration across Search" sizes="25vw" srcset="https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure2.png&amp;w=256&amp;q=75 256w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure2.png&amp;w=384&amp;q=75 384w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure2.png&amp;w=640&amp;q=75 640w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure2.png&amp;w=750&amp;q=75 750w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure2.png&amp;w=828&amp;q=75 828w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure2.png&amp;w=1080&amp;q=75 1080w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure2.png&amp;w=1200&amp;q=75 1200w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure2.png&amp;w=1920&amp;q=75 1920w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure2.png&amp;w=2048&amp;q=75 2048w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure2.png&amp;w=3840&amp;q=75 3840w" src="https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure2.png&amp;w=3840&amp;q=75" decoding="async" data-nimg="fill" loading="lazy"></p></div><figcaption>Figure 2. A system for chaos experimentation orchestration across Search</figcaption></figure>
<p>Note I have made an annotation so you cannot miss <a href="https://docs.aws.amazon.com/fis/latest/userguide/what-is.html?sc_channel=el&amp;sc_campaign=resiliencewave&amp;sc_geo=mult&amp;sc_country=mult&amp;sc_outcome=acq&amp;sc_content=how-search-uses-chaos-engineering" rel="noopener noreferrer">AWS Fault Injection Simulator (AWS FIS)</a>. AWS FIS is a managed service that enables you to perform fault injection experiments on your AWS-hosted applications, and therefore is a natural choice when it came to creating and running chaos experiments on the Search services, which are all hosted on AWS using AWS services like Amazon S3, Amazon API Gateway, Amazon DynamoDB, Amazon EC2, Amazon ECS, and more. FIS is something anyone running on AWS can use today. The Search Resilience team wanted to make it as easy as possible for Search teams to use FIS, without each of them having to build the same integrations and overhead.</p>
<p>There were Search-specific requirements they wanted to implement, so each team did not have to do it themselves. For example see in Figure 2 where it says ‚ÄúChaos experiment execution workflow.‚Äù  By making the chaos experiment part of a workflow, they can add Search-specific steps before and after the experiment. For example the pre-experiment steps checks if tests have passed in pre-production environments before running them in production, and they also check that no customer-impacting events are in progress before running an experiment. After the experiment, the workflow checks if metrics were adversely impacted (see <a href="#consistent-guardrails-using-slos" rel="noopener noreferrer">Consistent Guardrails Using SLOs</a> below for more details on these metrics).</p>
<p>There were several other Search-specific requirements that are made easier for teams by using the chaos orchestrator. These goals are shown in Figure 3, and I will give you an explanation of each.</p>
<figure><div data-rmiz-content="not-found" aria-owns="rmiz-modal-" data-rmiz=""><p><img alt="Graphical diagram showing Search organization goals for enabling Chaos Engineering" title="Figure 3. Search organization goals for enabling Chaos Engineering" sizes="25vw" srcset="https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure3.png&amp;w=256&amp;q=75 256w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure3.png&amp;w=384&amp;q=75 384w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure3.png&amp;w=640&amp;q=75 640w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure3.png&amp;w=750&amp;q=75 750w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure3.png&amp;w=828&amp;q=75 828w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure3.png&amp;w=1080&amp;q=75 1080w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure3.png&amp;w=1200&amp;q=75 1200w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure3.png&amp;w=1920&amp;q=75 1920w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure3.png&amp;w=2048&amp;q=75 2048w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure3.png&amp;w=3840&amp;q=75 3840w" src="https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure3.png&amp;w=3840&amp;q=75" decoding="async" data-nimg="fill" loading="lazy"></p></div><figcaption>Figure 3. Search organization goals for enabling Chaos Engineering</figcaption></figure>
<h3 tabindex="-1"></h3>
<p>Just by removing the undifferentiated heavy lifting, the Search Orchestrator helps to achieve this goal. Also, the Search resilience team wanted to make it easy to use so they created graphical UX front-end. You can see API Gateway in Figure 2 above that presents two APIs. Teams are free to programmatically call these, or they can use the graphical UX that calls these APIs. You can see in Figure 4 it is not the ‚Äúprettiest‚Äù UX ever, but it gives Search teams all the functionality they need to define and run chaos experiments. This is also in line with agile and DevOps where we spend effort only on things that matter.</p>
<figure><div data-rmiz-content="not-found" aria-owns="rmiz-modal-" data-rmiz=""><p><img alt="Screenshot showing UX front-end for Amazon Search Chaos Orchestrator" title="Figure 4. UX front-end for Amazon Search Chaos Orchestrator" sizes="25vw" srcset="https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure4.png&amp;w=256&amp;q=75 256w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure4.png&amp;w=384&amp;q=75 384w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure4.png&amp;w=640&amp;q=75 640w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure4.png&amp;w=750&amp;q=75 750w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure4.png&amp;w=828&amp;q=75 828w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure4.png&amp;w=1080&amp;q=75 1080w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure4.png&amp;w=1200&amp;q=75 1200w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure4.png&amp;w=1920&amp;q=75 1920w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure4.png&amp;w=2048&amp;q=75 2048w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure4.png&amp;w=3840&amp;q=75 3840w" src="https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure4.png&amp;w=3840&amp;q=75" decoding="async" data-nimg="fill" loading="lazy"></p></div><figcaption>Figure 4. UX front-end for Amazon Search Chaos Orchestrator</figcaption></figure>
<h3 tabindex="-1"></h3>
<p>AWS FIS documents <a href="https://docs.aws.amazon.com/fis/latest/userguide/fis-tutorial-recurring-experiment.html?sc_channel=el&amp;sc_campaign=resiliencewave&amp;sc_geo=mult&amp;sc_country=mult&amp;sc_outcome=acq&amp;sc_content=how-search-uses-chaos-engineering" rel="noopener noreferrer">instructions on how to schedule experiments using Amazon EventBridge Scheduler</a>. But remember, we want to eliminate the undifferentiated work. So this is just built in to the orchestrator, and Search teams can use it. Note in Figure 2 that the Search Resilience team went a somewhat different route, using Amazon DynamoDb to store schedules and EventBridge actually invokes a Lambda function to read the schedules and kick of the experiment runs.</p>
<h3 tabindex="-1"></h3>
<p>Similar to scheduling, anyone using FIS can run it with their deployment pipelines. For example <a href="https://aws.amazon.com/blogs/architecture/chaos-testing-with-aws-fault-injection-simulator-and-aws-codepipeline?sc_channel=el&amp;sc_campaign=resiliencewave&amp;sc_geo=mult&amp;sc_country=mult&amp;sc_outcome=acq&amp;sc_content=how-search-uses-chaos-engineering" rel="noopener noreferrer">here is how to integrate it from AWS CodePipeline</a>. But again, to save work for the two-pizza teams, why not just make it work with the teams‚Äô pipelines. Also, Search uses an internal Amazon tool for pipelines and deployment, so the orchestrator takes on the work of integrating with this.</p>
<h3 tabindex="-1"></h3>
<p>OK, a lot to unpack here.  First guardrails ‚Äî these are a MUST for chaos experiments. Guardrails are conditions you define that indicate the experiment will cause unwanted impact, so when these conditions happen, the experiment must be stopped and rolled back. Of course AWS FIS lets your define stop conditions as guardrails ‚Äî you can see those on the right side of Figure 2. So what added benefit does the Search chaos Orchestrator provide here? That is where SLOs come in.</p>
<p>Service Level Objective, or SLOs, are simply goals that look something like this (this is just a fictional example): In a 28 day trailing window, we will serve 99.9% of requests with a latency of less than 1000 milliseconds. The Search Resilience team did not just build the chaos experiment orchestrator, but they also build an SLO definition tracking system too. This system lets teams define the SLOs for their service, and then it monitors those SLOs, tracking when services are out of compliance. The Search Chaos Orchestrator integrates with this, and uses these SLOs as guardrails for the experiments run by the service teams. In addition to stopping the experiment, the orchestrator notifies service owners and cuts a tracking ticket when a guardrail is breached.</p>
<h3 tabindex="-1"></h3>
<p>See that big red button in Figure 4 that says ‚ÄúHalt All Search Chaos Experiments‚Äù?  That is the Andon cord. The <a href="https://mag.toyota.co.uk/andon-toyota-production-system/" rel="noopener noreferrer">Andon was created by Toyota manufacturing</a> in their factories where ‚Äúeach and every one is permitted to stop the production line if they spot something they perceive to be a threat to vehicle quality‚Äù. Here it allows anyone to stop all the experiments if there is any risk to customer experience. They can use the big red button or a CLI command. You can see in Figure 2 how the Andon functionality is implemented making use of the stop condition functionality built into FIS. In addition to stopping all running experiments, the Andon will cause the Search Resilience engineer who is on-call to be paged. Most two-pizza teams across Amazon have at least one member on-call to handle incidents 24/7, which is part of the DevOps practice of owning service operation.</p>
<h3 tabindex="-1"></h3>
<p><a href="https://docs.aws.amazon.com/fis/latest/userguide/monitoring-experiments.html?sc_channel=el&amp;sc_campaign=resiliencewave&amp;sc_geo=mult&amp;sc_country=mult&amp;sc_outcome=acq&amp;sc_content=how-search-uses-chaos-engineering" rel="noopener noreferrer">AWS FIS supports metrics and logging.</a> The chaos engineering orchestrator uses that functionality, and aggregates the results from all Search teams to present as a single report.</p>
<h2 tabindex="-1"></h2>
<p>Amazon Search, like many Amazon services, makes use of emergency levers as part of their resilience strategy. An <a href="https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_mitigate_interaction_failure_emergency_levers.html?sc_channel=el&amp;sc_campaign=resiliencewave&amp;sc_geo=mult&amp;sc_country=mult&amp;sc_outcome=acq&amp;sc_content=how-search-uses-chaos-engineering" rel="noopener noreferrer">emergency lever</a> is a rapid process that allows systems to recover from stress or impact. So naturally Search wants to experiment to understand that the emergency levers work as intended. In this case, a simplified hypothesis might be as follows:</p>
<blockquote><p>When the search load exceeds [some value] and errors and latency start to climb (specify which metrics and by how much), then activating the emergency lever to disable non-critical services will keep errors and latency within acceptable limits (define these), up to loads of [specified amount].</p></blockquote>
<p>This particular emergency lever disables all non-critical services, conserving resources when the system is under duress, so that critical services remain available. You can see this in Figure 5. On the left is the normal Search experience. On the right is after the emergency lever has been pulled. Critical functionality such as title, image, and price is shown, but nothing else. Search would rather show the experience on the right, than to fail to return any results at all. This is a resilience best practice called <a href="https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_mitigate_interaction_failure_graceful_degradation.html?sc_channel=el&amp;sc_campaign=resiliencewave&amp;sc_geo=mult&amp;sc_country=mult&amp;sc_outcome=acq&amp;sc_content=how-search-uses-chaos-engineering" rel="noopener noreferrer">graceful degradation</a>.</p>
<figure><div data-rmiz-content="not-found" aria-owns="rmiz-modal-" data-rmiz=""><p><img alt="Side by side screenshots of search results, illustrating an emergency lever that disables non-critical services in Search, and presents a gracefully degraded experience to customers" title="Figure 5. An emergency lever that disables non-critical services in Search, and presents a gracefully degraded experience to customers" sizes="25vw" srcset="https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure5.webp&amp;w=256&amp;q=75 256w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure5.webp&amp;w=384&amp;q=75 384w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure5.webp&amp;w=640&amp;q=75 640w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure5.webp&amp;w=750&amp;q=75 750w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure5.webp&amp;w=828&amp;q=75 828w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure5.webp&amp;w=1080&amp;q=75 1080w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure5.webp&amp;w=1200&amp;q=75 1200w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure5.webp&amp;w=1920&amp;q=75 1920w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure5.webp&amp;w=2048&amp;q=75 2048w, https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure5.webp&amp;w=3840&amp;q=75 3840w" src="https://community.aws/_next/image?url=https%3A%2F%2Fcommunity.aws%2Fraw-post-images%2Fposts%2Fhow-search-uses-chaos-engineering%2Fimages%2Ffigure5.webp&amp;w=3840&amp;q=75" decoding="async" data-nimg="fill" loading="lazy"></p></div><figcaption>Figure 5. An emergency lever that disables non-critical services in Search, and presents a gracefully degraded experience to customers</figcaption></figure>
<p>For the chaos experiment, the events include a combination of adding synthetic load to the system and then pulling the emergency lever. This way Search can build confidence that in the case of a real high load event, the lever will enable Search to remain available.</p>
<h2 tabindex="-1"></h2>
<p>Chaos Engineering is a great way to better understand the resilience of your services. And AWS FIS is a great service for creating and running chaos experiments on AWS. Two-pizza teams in Search could have each independently began using FIS and running experiments. But by adopting a DevOps culture that focused on <strong>enabling teams to do more</strong>, the Search Resilience team was able to make the process even easier for Search two-pizza teams, and add many valuable features that make chaos engineering more effective across all of Search.</p>
<h2 tabindex="-1"></h2>
<h4 tabindex="-1"></h4>
<ul><li>Read about three more examples of Amazon teams using DevOps to drive resilience</li></ul>
<h4 tabindex="-1"></h4>
<ul><li>Learn how Amazon Prime Video followed their journey to enable teams to use DevOps practices and Chaos Engineering.</li></ul>
<h4 tabindex="-1"></h4>
<ul><li>With real-world examples of massive-scale production workloads from IMDb, Amazon Search, Amazon Selection and Catalog Systems, Amazon Warehouse Operations, and Amazon Transportation, this presentation shows how Amazon builds and runs cloud workloads at scale and how they reliably process millions of transactions per day</li></ul>
<h4 tabindex="-1"></h4>
<ul><li>This blog introduces you to Chaos Engineering for cloud-based applications</li></ul>
</div><p>Any opinions in this post are those of the individual author and may not reflect the opinions of AWS.</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[deVStudio ‚Äì Runs VS Code on Android (156 pts)]]></title>
            <link>https://play.google.com/store/apps/details?id=tech.ula.devstudio&amp;hl=en_US</link>
            <guid>37397359</guid>
            <pubDate>Tue, 05 Sep 2023 20:36:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://play.google.com/store/apps/details?id=tech.ula.devstudio&#x26;hl=en_US">https://play.google.com/store/apps/details?id=tech.ula.devstudio&#x26;hl=en_US</a>, See on <a href="https://news.ycombinator.com/item?id=37397359">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-g-id="description"><p>This is really runs Visual Studio Code ( vscode ) on your device.  It is full featured and professionally supported.  This runs the Linux desktop edition of Visual Studio Code specifically.</p><p>Note:  This is currently a very minimal install.  So you have to add some packages and extensions based on your need.  <br>For example if you wanted to do C++ development you would do the following:<br>1) in the termal: sudo apt install build-essential gdb<br>2) in vscode: install the C++ extension<br>3) have fun<br>Versions already setup for certain development flows will be added in the future.</p><p>About Visual Studio Code:<br>  Visual Studio Code, also commonly referred to as VS Code, is a source-code editor made by Microsoft. Features include support for debugging, syntax highlighting, intelligent code completion, snippets, code refactoring, and embedded Git. Users can change the theme, keyboard shortcuts, preferences, and install extensions that add functionality.</p><p>You can read more about it here: https://code.visualstudio.com/</p><p>How to use this deVStudio Android app:<br>When using the graphical interface, use vscode just like normal.  But here are some specifics to the Android interface.<br>* Tap with one figure to left click.<br>* Move mouse by sliding around one finger.<br>* Pinch to zoom.<br>* Press and hold and then slide one finger to pan (useful when zoomed in).<br>* Slide two fingers up and down to scroll.<br>* If you want to bring up a keyboard, tap on the screen to get a set of icons to appear and then click the keyboard icon.<br>* If you want to do the equivalent to a right click, tap with two fingers.<br>* If you want to change the desktop scaling, find the service android notification and click the settings.  You have to stop and restart the app after changing this settings for it to take effect.<br>This is all easier to do on a tablet and with a stylus, but it can be done on a phone or using your finger as well.</p><p>To access files from the rest of Android, there are many useful links in your home directory (/home/userland) to places like your Documents, Pictures, etc.  No need to import or export files.</p><p>If you don't want to, or cannot pay the cost of this app, you can run vscode via the UserLAnd app by installing all the necessary packages etc.</p><p>Licensing:<br>This app is released under the GPLv3.  The source code can be found here:<br>https://github.com/CypherpunkArmory/deVStudio<br>The icon is provided via the Creative Commons Attribution Share-Alike 3.0 Unported (CC-by-sa) from the Document Foundation.</p><p>This app is not created by the vscode development team.  Instead it is a application that allows the Linux version to run on Android.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When deployments are easy, code becomes simpler (161 pts)]]></title>
            <link>https://bitbytebit.substack.com/p/when-deployments-are-easy-code-becomes</link>
            <guid>37397323</guid>
            <pubDate>Tue, 05 Sep 2023 20:33:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bitbytebit.substack.com/p/when-deployments-are-easy-code-becomes">https://bitbytebit.substack.com/p/when-deployments-are-easy-code-becomes</a>, See on <a href="https://news.ycombinator.com/item?id=37397323">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Feature flagging tools like Launch Darkly, PostHog, Flagsmith come with costs. They buy you a ton of capabilities like A/B testing, phased rollouts, and reducing the need for deployments. If you sense a but coming, there is. Well, sort of.</p><p>Much of the time I‚Äôm using flags so I can commit unfinished code because I don‚Äôt want to create a separate branch and have one source of truth (low WIP baby - can you get more agile than that?). Basically, I want to reap the benefits of trunk-based development. For example, I‚Äôm implementing a Gift Card feature for JumpComedy.com and have the customer side working but not the admin side, so I want to hold back.</p><p><span>I could have created another Flagsmith toggle around the admin code, but that seemed to be overkill. Instead, I chose to have a simple hard-coded </span><code>const isGiftCardFeatureEnabled = false</code><span> variable and use that instead. Is this bad because I now have to do a deployment to enable the feature?</span></p><p>I would argue as long as your deployments are easy, this is the better way to do things because it reduces the complexity of integrating with a third-party tool. I did an inventory of my recent feature flags and realized that about 80% of them aren‚Äôt there to roll things out to specific populations, or do any sort of A/B testing, but to hide unfinished code.</p><p><span>Yet more evidence that </span><a href="https://services.google.com/fh/files/misc/state-of-devops-2021.pdf" rel="nofollow ugc noopener">DORA</a><span> (PDF) isn‚Äôt crazy for prioritizing deployment frequency so high when labelling organizations as elite.</span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Klack ‚Äì Satisfying sound with every keystroke (197 pts)]]></title>
            <link>https://tryklack.com/</link>
            <guid>37395370</guid>
            <pubDate>Tue, 05 Sep 2023 17:55:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tryklack.com/">https://tryklack.com/</a>, See on <a href="https://news.ycombinator.com/item?id=37395370">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Mentra aims to match neurodivergent jobseekers with jobs (104 pts)]]></title>
            <link>https://techcrunch.com/2023/09/05/sam-altman-backed-startup-aims-to-match-neurodivergent-jobseekers-with-ideal-jobs/</link>
            <guid>37395168</guid>
            <pubDate>Tue, 05 Sep 2023 17:39:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2023/09/05/sam-altman-backed-startup-aims-to-match-neurodivergent-jobseekers-with-ideal-jobs/">https://techcrunch.com/2023/09/05/sam-altman-backed-startup-aims-to-match-neurodivergent-jobseekers-with-ideal-jobs/</a>, See on <a href="https://news.ycombinator.com/item?id=37395168">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary"><span>Neurodivergent individuals often have </span><a href="https://werth.institute.uconn.edu/neurodiversitycenter-2/"><span>a harder time</span></a><span> finding jobs than their non-neurodivergent counterparts. </span><span>Unemployment for neurodivergent adults runs at least as high as 30-40% ‚Äì three times the rate for people with disability, and eight times the rate for people without disability, according to UConn‚Äôs Center for Neurodiversity and Employment Innovation.</span></p>
<p><span>Some neurodivergent individuals may lack the social skills necessary to go through a grueling interview process, and others may simply not have the confidence to apply.&nbsp;</span></p>
<p><span>But in fact, this population may have specialized skill sets that not only make them good candidates but ‚Äì some may argue ‚Äì even </span><i><span>better</span></i><span> suited for certain roles than non-neurodivergent people. Some research shows that neurodivergent people can </span><span>make teams up to </span><a href="https://hbr.org/2017/05/neurodiversity-as-a-competitive-advantage"><span>30% more productive</span></a><span> when placed in the right environments.&nbsp;</span></p>
<p><span>Enter </span><a href="http://mentra.com/"><span>Mentra</span></a><span>. The Charlotte, N.C.-based startup, whose three co-founders are all autistic is building what it describes as an AI-powered ‚Äúneuroinclusive employment network.‚Äù Specifically, its tech platform leverages artificial intelligence to help large enterprises hire employees with cognitive differences such as autism, </span><span>attention-deficit/hyperactivity disorder (</span><span>ADHD), dyslexia, obsessive-compulsive disorder (OCD), traumatic brain injury (TBI) and post-traumatic stress disorder (PTSD).</span></p>
<p><span>The startup‚Äôs unique premise caught the early attention of OpenAI co-founder and CEO Sam Altman, who first invested in the company with a $1 million pre-seed investment in February 2022 through his venture firm, Hydrazine Capital. Mentra also </span><span>won an AI for accessibility grant from Microsoft. Shine Capital led its $3.5 million seed round this year, which also included participation from Altman‚Äôs fund, Verissimo,&nbsp;Full Circle,&nbsp;Charlotte Fund, as well as angel investors including&nbsp;David Apple&nbsp;and&nbsp;Dawn Dobras.</span></p>
<p><span>‚ÄúDiversity of thought is the key to tackling humanity‚Äôs most complex challenges. The most innovative companies of our time have embraced neurodivergent thinkers,‚Äù said Altman in a written statement. ‚ÄúMentra is the bridge companies have long-needed to access this untapped talent pool.‚Äù</span></p>
<p><span>Since its launch, Mentra ‚Äì whose name </span><span>comes from a combination of the two words, ‚Äúmentor‚Äù and ‚Äúmantra‚Äù ‚Äì </span><span>&nbsp;has signed on over a dozen companies including Harvard Business Publishing, Trellix and Auticon. The platform has also partnered with over 30 universities and more than 200 service providers across the United States. Its talent pool has grown from 300 neurodivergent job seekers in March of 2022 to over 33,000 today.</span></p>
<p><span>What sets Mentra apart is its approach to job fit, maintains Mentra co-founder and CEO Jhillika Kumar. The startup goes beyond keywords in resumes to match employers with talent, she said,&nbsp; considering factors around a person‚Äôs neurotype, aptitude, environmental sensitivities. To date, its one-year retention rate has remained at an impressive 97.5%.</span></p>
<h2><strong>Using AI to parse through job descriptions</strong></h2>
<p><span>Kumar came up with the idea for the startup while studying at Georgia Tech. She was doing research on the non speaking community, looking for ways to help her non-speaking autistic brother, Vikram, to communicate. Existing tools were sub par.</span></p>
<p><span>After 27 years of being unable to communicate his thoughts, Vikram‚Äôs ability to learn how to type through an accessible letterboard both surprised and inspired Kumar.</span></p>
<p><span>‚ÄúI didn‚Äôt actually know for a while whether or not he was intelligent because he‚Äôs very disconnected ‚Äì in his own world,‚Äù she recalls. ‚ÄùBut over time, I realized that even though he couldn‚Äôt speak, he could use the iPad, and he was very proficient at going to YouTube and doing things. So we were like, ‚ÄòOkay, there‚Äôs clearly intelligence here.‚Äô ‚Äù&nbsp;</span></p>
<p><span>One way Mentra uses AI is </span><span>to parse through job descriptions to make sure they are cognitively accessible and broken down in a consistent format that is not exclusionary.</span></p>
<p><span>‚ÄúThen we are able to use an algorithm to go through the jobseekers on our platform to identify who‚Äôs the best fit based on mostly neuro type,‚Äù Kumar told TechCrunch. ‚ÄúOne person might be extremely good at hyper focusing, very detail-oriented, very process-oriented or very strategic, and you have specific skills that map to their strengths in the role.‚Äù</span></p>
<p><span>Over 70% of the data Mentra collects is not collected by an Indeed or a traditional job-finding platform. It uses that holistic data to make the match between the job and the individual.&nbsp;</span></p>
<h2><strong>A ‚Äòscaleable‚Äô SaaS model</strong></h2>
<p><span>One in every seven humans are neurodivergent, and many of them are highly under or unemployed ‚Äì still living with their parents and/or financially dependent on them.</span></p>
<p><span>‚ÄúI began to recognize the importance of employment,‚Äù she said. ‚ÄúI realized what makes us cognitively unique is what makes us talented. The moment someone has to mask a dimension of their neurodiversity, they suppress dimensions of their talent.‚Äù</span></p>
<p><span>So she teamed up with </span><a href="https://www.linkedin.com/in/connerreinhardt/"><span>Conner Reinhardt</span></a><span> and </span><a href="https://linkedin.com/in/sheabelsky"><span>Shea Belsky</span></a><span> to start Mentra with the goal of helping companies approach neuroinclusion as more than just a DEI initiative. The team‚Äôs argument is that neuroinclusion should be integrated into the company‚Äôs infrastructure and DNA. In many cases, that may require a culture shift across teams.</span></p>
<p><span>‚ÄúWe‚Äôre firm believers that all companies will be more productive if you have diversity of thought in every organization,‚Äù Kumar said. </span><span>‚ÄúBy embracing our divergence, companies can unlock the full potential of their employees.‚Äù</span></p>
<p><span>It‚Äôs also proving to be a viable business model.</span></p>
<p><span>When Mentra first launched in 2022, it operated largely as a services organization, becoming profitable with a traditional per-hire pricing model. Despite being profitable, the team felt that that model would be limited in its ability to scale. It has since transitioned to a ‚Äúscalable‚Äù SaaS model where employers subscribe to access Mentra‚Äôs talent pool and recruiting product, according to Kumar. Since that transition earlier this year, the company has brought in an additional stream of SaaS revenue, 67% of which is annual recurring revenue.&nbsp;</span></p>
<p><span>‚ÄúWhile the enterprise focus and more economical SaaS offering could mean a longer road to profitability, we have seen strong market adoption and are actively in conversations with 40+ enterprise customers,‚Äù Kumar told TechCrunch. ‚ÄúOur goal is to hit $3m in SaaS ARR by the end of 2024.‚Äù</span></p>
<h2><strong>Not just another DEI play</strong></h2>
<p><span>While Mentra has seen ‚Äústrong‚Äù global demand, centered primarily in the United Kingdom and Asia-Pacific region, Kumar said the company is currently focused ‚Äúon nailing the U.S. market for enterprise companies.‚Äù&nbsp;</span></p>
<p><span>The startup‚Äôs current revenue model is free for neurodivergent jobseekers, and it charges an annual subscription for enterprise companies to access the platform. It is also building out a neuroinclusion marketplace for service providers such as consultancies and training firms to provide hands-on services to companies that accompany Mentra‚Äôs core platform.&nbsp;</span></p>
<p><span>‚ÄúIn the future, we plan to have a similar marketplace available for neurodivergents to access tailored services as well throughout the life of their career such as bootcamps and job coaches,‚Äù Kumar added.</span></p>
<p><span>Shine Capital founder and general partner Mo Koyfman said Mentra is unlike any other startup he‚Äôs come across. Koyfman ‚Äì who has backed the likes of Plaid and Harry‚Äôs ‚Äì supports Mentra‚Äôs premise that </span><span>people who might have different learning styles ‚Äúmay spike on certain things versus other things.‚Äù</span></p>
<p><span>‚ÄúAnd so actually, they may even be better for certain jobs than others. For example, in the AI world, we know that there are folks with Asperger‚Äôs, or some form of autism, that tend to be way better at data labeling tasks,‚Äù he told TechCrunch in an interview.</span></p>
<p><span>It‚Äôs easy to look at Mentra and label it as ‚Äúanother DEI play.‚Äù But to do so, he said, would be undermining its uniqueness.</span></p>
<p><span>‚ÄúThere are a lot</span><span> of DEI (diversity, equity and inclusion) initiatives where they try and force the wrong people on the wrong jobs in the name of DEI and that to me is is not a good solution, because those people will end up not doing as well at those jobs and either fail or foster resentment amongst others,‚Äù said Koyfman, who has a dyslexic daughter. ‚ÄúBut Mentra is about getting people in the jobs that they‚Äôre best suited for, and where they are the right fit for those jobs and will actually outperform their otherwise traditional peers‚Ä¶So it‚Äôs a pretty big addressable market that the traditional recruiting platforms simply don‚Äôt cater to specifically and I fell in love with that.‚Äù</span></p>
<p>He added: ‚ÄúNot <span>only are they doing something that‚Äôs good, they‚Äôre doing something that is economically positive and productive for society, and finding opportunities to do both, is very, very, very, very rare.‚Äù</span></p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[If you can use open source, you can build hardware (287 pts)]]></title>
            <link>https://redeem-tomorrow.com/if-you-can-use-open-source-you-can-build-hardware</link>
            <guid>37395096</guid>
            <pubDate>Tue, 05 Sep 2023 17:33:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://redeem-tomorrow.com/if-you-can-use-open-source-you-can-build-hardware">https://redeem-tomorrow.com/if-you-can-use-open-source-you-can-build-hardware</a>, See on <a href="https://news.ycombinator.com/item?id=37395096">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>As a technologist, hardware has always been my final frontier.</p>
<p>Things you can touch, things that create physical outcomes, these can have so much more power than software alone. But the cost of that power is complexity. Electrons don‚Äôt care about our ambitions. Circuits can be harder to debug than code. And even if everything works perfectly on the level of logic and voltage, you‚Äôre still managing the complexity of physical objects, their wiring, their position in space, and even their heat dissipation.</p>
<p>Building any product is hard, but building a hardware product is a superset of the basics of any product challenge, adding in the iron constraints of the physical world. It‚Äôs not enough to imagine how something looks: you have to also find a way to build it in a way that matches your imagination, while simultaneously accommodating all those physical constraints.</p>
<p>It‚Äôs work.</p>
<p>But also? It‚Äôs never been <em>easier</em> than it is today. I just built <a href="https://github.com/daniloc/ThermTerm" rel="nofollow">complete replacements for my heat pump controllers</a>. I hated those dinky remotes. You couldn‚Äôt read them in the dark at all, and programming them was about as bad as anything you remember from the bad old days of VCRs.</p>
<p>I imagined something that would solve their UI problems, <em>and</em> integrate my heat pumps into my home automation system. I‚Äôm not an electronics engineer, but my dreams are now real. I‚Äôve got five of these across the house.</p>
<p><img src="https://redeem-tomorrow.com/images/uploads/dsc00023.jpg" alt="A thermostat-like thing, fully-custom made by me"></p>
<p><strong>Using open source code is a skill:</strong> knowing how to navigate repos and someone else‚Äôs code, understanding how to troubleshoot and navigate communities to get help, discerning between quality projects and junk‚Ä¶ this experience is a hard-won component of being a modern software explorer. It can take you further than you might realize, past mere bits and into the land of electrons and atoms.</p>
<h2 id="of-course-microcontrollers">Of course: microcontrollers</h2>
<p>Arduino was a revolution in developer experience.</p>
<p>Beginners could write simple C code and and have a <em>physical computing experience</em> within the space of five minutes.</p>
<p>But since the advent of Arduino, the landscape of microcontroller boards‚Äîcomponents that can be programmed to emit and receive complex electronic signals‚Äîhas exploded.</p>
<p>Boards of every scale and configuration are available today, from the size of a peanut butter and jelly sandwich all the way down to a postage stamp. Microcontrollers are the foundation of the hardware adventure, allowing infinite iteration of custom logic on inexpensive components. Different designs sport different connectors and accessories. There are also different chip architectures out there, and boards built around <a href="https://en.wikipedia.org/wiki/ESP32" rel="nofollow">ESP32</a>, or the new <a href="https://www.adafruit.com/product/5526" rel="nofollow">Pico W</a>, even include WiFi and Bluetooth capabilities.</p>
<p>Unifying all of this are software ecosystems. Open source Arduino code exists to solve so many kinds of problems, from networking to button handling. Regardless of your board‚Äôs architecture, there‚Äôs usually a port of the Arduino environment you can use, opening up all that code for your project. If you prefer Python, the <a href="https://micropython.org/" rel="nofollow">MicroPython</a> and <a href="https://circuitpython.org/" rel="nofollow">CircuitPython</a> projects even offer an alternative to C/C++.</p>
<p>But the ecosystem doesn‚Äôt stop at software.</p>
<h2 id="stemmaqt-and-qwiic-an-old-standard-with-new-strategy">StemmaQT and Qwiic: an old standard with new strategy</h2>
<p><a href="https://en.wikipedia.org/wiki/I%C2%B2C" rel="nofollow">I2C</a> is a two wire serial data standard that dates to 1982. In practice, it looks and works a lot like USB: two more wires add power and ground, and you can chain dozens of devices together on a single bus.</p>
<p>For the typical hobbyist hardware tinkerer, the hardest challenge comes down to <em>circuit design</em>. Electronics is arcane, governed by both the laws of physics and decades of component lore. Building circuits means understanding how to use all of these to channel electrons consistently, without letting the magic smoke out. It means understanding how to adjust voltages, manage resistance, <em>and do it all in a way that‚Äôs physically sturdy and manageable</em>.</p>
<p>This is a tall order. I‚Äôve never had the time or motivation to learn enough circuit design to build anything more complex than arrays of blinking lights.</p>
<p>But now, I don‚Äôt need to.</p>
<p>If you build modern software, you‚Äôre well-versed in <em>composition</em>: grab a handful of existing projects‚Äîa database here, a UI framework there, an HTTP library to round it all out‚Äîand arrange them together. You write your custom logic‚Äîthe stuff unique to your project‚Äîand let other people‚Äôs code do work that‚Äôs common across all projects.</p>
<p>Thanks to I2C and a convenient cable standard‚Äîbranded <strong><a href="https://learn.adafruit.com/introducing-adafruit-stemma-qt/what-is-stemma-qt" rel="nofollow">StemmaQT</a></strong> by Adafruit, and <strong><a href="https://www.sparkfun.com/qwiic" rel="nofollow">Qwiic</a></strong> by Sparkfun, two leading hobbyist component vendors‚Äîthe same approach to composition is possible in an electronics project. Decide your requirements, and you can probably find a few boards you can quickly wire together‚Äîwithout soldering!‚Äîthat will address your problem.</p>
<p>In the case of my heat pump controller:</p>
<ul><li><a href="https://www.adafruit.com/product/5691" rel="nofollow">ESP32-based microcontroller with a built-in screen</a></li>
<li><a href="https://www.adafruit.com/product/4681" rel="nofollow">Light sensor</a>, so I can automatically dim the bright lights</li>
<li><a href="https://www.adafruit.com/product/4636" rel="nofollow">Temperature and humidity sensor</a>, so I can have real data about the actual temperature in the room</li>
<li><a href="https://www.adafruit.com/product/4991" rel="nofollow">Rotary encoder board</a>, to make a dial for setting temperature and toggling power</li></ul>
<p><em>All of that</em> just plugs in via I2C. Each board, like a well-made library, abstracts its gnarly implementation details under the hood. You don‚Äôt worry about power management, or the details of how to interpret signals as input from a rotary encoder.</p>
<p><img src="https://redeem-tomorrow.com/images/uploads/264531564-6f04c1c9-9900-47f6-958d-a4b7f66801d4.jpeg" alt="Prototype with various components affixed to cardboard"></p>
<p>Instead, after wiring things together, you look up <a href="https://learn.adafruit.com/adafruit-i2c-qt-rotary-encoder" rel="nofollow">the example code</a> for each component, and adapt it to your project. In the past, actually using these components could be something of a mystery. But now vendors go to great lengths to maintain supporting libraries and docs‚Äîdeveloper experience sells!</p>
<p>Adafruit deserves special mention here: they have hundreds of <a href="https://www.adafruit.com/category/1018" rel="nofollow">useful, unique boards</a> that support this compositional approach, and their supporting materials are just exhaustive.</p>
<p>Again, if you know how to apply open source code, you can do this.</p>
<h2 id="from-imagination-to-physical-objects">From imagination to physical objects</h2>
<p>Once you create a working circuit, you can take it all a step further: design and manufacture your own custom enclosures.</p>
<p>It‚Äôs really kind of nuts: for $500, you can buy an <a href="https://www.prusa3d.com/category/original-prusa-mini/" rel="nofollow">incredible, reliable 3D printer from Prusa</a>. You can‚Äôt beat Prusa: the printers come out of the box working perfectly, they‚Äôre vertically integrated with Prusa‚Äôs excellent cross-platform <a href="https://www.prusa3d.com/page/prusaslicer_424/" rel="nofollow">slicer software</a>, and the user community is among the most active and helpful of anything on the internet. At that price, its build volume isn‚Äôt mammoth, but it doesn‚Äôt have to be for electronics projects.</p>
<p>Open hardware vendors often provide 3D models of their products you can bring into a CAD program. With these models, it‚Äôs possible to design plastic cases with perfect accuracy, allowing you to mount and arrange the various boards in your project.</p>
<p><img src="https://redeem-tomorrow.com/images/uploads/264529970-ef934ef7-89c7-4ac9-8dc7-f238c8ad32e6.jpeg" alt="CAD model of the device's internals"></p>
<p>Best of all, iteration is fast and cheap. Trying a new version of a design takes only a few cents of material and time to print it. You can figure out quickly if your approach is working. Remember heat dissipation? Waste heat from the microcontroller was biasing my temperature sensor. It only took a couple evenings to find an iteration of the case that solved the problem (I moved both sensors to the top side of the enclosure).</p>
<p>There is a learning curve to 3D printing. It might be the steepest factor here, in fact. Like electronics, 3D printing also has constraints to manage: you have to work with both heat and gravity, and design your models with that collaboration in mind. The physics of how layers are deposited impacts the strength and durability of your output, so the orientation at which you print can have consequences. Choice of materials also matters. I found <a href="https://all3dp.com/2/petg-filament-all-you-need-to-know/" rel="nofollow">PETG</a> to be the ideal material for this kind of work: it‚Äôs easy to work with and super durable. By contrast, the more common <a href="https://all3dp.com/2/what-is-pla-plastic-material-properties/" rel="nofollow">PLA</a> was too brittle.</p>
<p>You‚Äôll also want to pick up some CAD skills, but this is less painful and more fun than you might imagine. CAD seems to be the fine art of sketching simple flat shapes, pulling and pushing them into 3D objects, and carving away at small details. I couldn‚Äôt stand most of the affordable desktop CAD software‚Äîit feels icky, like using a poorly-aged Flash app in IE6. Then I found <a href="https://www.shapr3d.com/" rel="nofollow">Shapr3D</a>, for iPad.</p>
<p>This is some futuristic shit. You tap away with your Apple Pencil, on a tablet computer, drawing things out of your imagination. You nudge your sketches into models, which turn into prints. There‚Äôs something far more intuitive about this process than using keyboard and mouse. The positive future shock makes me delirious if I dwell on it.</p>
<p>Community is everything to learning emerging skills. Hobbyists are deeply engaged with 3D printing, and you‚Äôll find help for everything from planning your models to debugging thermal drama with your prints.</p>
<h2 id="i-think-you-should-try-it">I think you should try it</h2>
<p>I‚Äôve been dreaming of building my own electronics since I was a kid. I spent so many afternoons at Radio Shack, and even tried my hand at the occasional kit, with limited success. Every few years in adulthood, I‚Äôve given it another try, observing a steady downward trend in difficulty.</p>
<p>I‚Äôm telling you: we‚Äôre at a special moment here. The labor savings of open source, the composability, the fun: all of it has come to hardware. You can build things that solve real problems for yourself. I first imagined my heat pump devices over a year ago, and I have been frustrated they didn‚Äôt exist every day since.</p>
<p>Now my dreams are real, and the largest energy consumer in the house can be automated and remotely controlled.</p>
<p>That‚Äôs amazing.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rockstar is selling cracked game copies on Steam (446 pts)]]></title>
            <link>https://twitter.com/__silent_/status/1698345924840296801</link>
            <guid>37394665</guid>
            <pubDate>Tue, 05 Sep 2023 17:01:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/__silent_/status/1698345924840296801">https://twitter.com/__silent_/status/1698345924840296801</a>, See on <a href="https://news.ycombinator.com/item?id=37394665">Hacker News</a></p>
Couldn't get https://twitter.com/__silent_/status/1698345924840296801: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[GitHub was down (139 pts)]]></title>
            <link>https://www.githubstatus.com/incidents/smdz34v7j8q0</link>
            <guid>37394275</guid>
            <pubDate>Tue, 05 Sep 2023 16:37:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.githubstatus.com/incidents/smdz34v7j8q0">https://www.githubstatus.com/incidents/smdz34v7j8q0</a>, See on <a href="https://news.ycombinator.com/item?id=37394275">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <div>
      <p>Incident with Actions, Pages, Pull Requests, Codespaces, API Requests, Issues, Webhooks and Packages</p>
      
    </div>

    <div>
      <!-- postmortem if it's published -->

      <!-- incident updates in reverse order -->
        <div>
          <p>
            Resolved
          </p>
          <div>
            <p><span>From 16:24-16:43 UTC, multiple GitHub services were down or degraded due to an outage in one of our primary databases. <p>The primary host for a shared datastore for GitHub experienced an underlying file system write error which affected availability for the majority of public-facing GitHub services. SAML login was affected, as was access to Actions, Issues, Pull Requests, Pages, API, Webhooks, Codespaces, and Packages. In this case, our automatic failover was unable to handle the partial file system failure mode. We mitigated by manually performing a forced failover, initiated 17 minutes after our first alert and completed 2 minutes later.</p><p>With the incident mitigated, we are working to assess more detailed impact and resilience improvements to each impacted service while also improving the automated failover mechanism to support this scenario.</p></span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693933309000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">17:01</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>Pages is operating normally.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693933290000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">17:01</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>Actions is operating normally.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693933227000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">17:00</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>Issues is operating normally.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693932678000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">16:51</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>We have performed a mitigation affecting write traffic across services and are seeing recovery for affected customers.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693932671000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">16:51</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>Packages is operating normally.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693932638000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">16:50</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>Webhooks is operating normally.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693932594000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">16:49</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>API Requests is operating normally.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693932557000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">16:49</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>Codespaces is operating normally.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693932501000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">16:48</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>Packages is experiencing degraded performance. We are continuing to investigate.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693932310000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">16:45</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>Webhooks is experiencing degraded performance. We are continuing to investigate.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693932254000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">16:44</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>We are investigating an issue that is impacting a small percentage of requests across all services including authentication and are working on mitigating impact.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693932008000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">16:40</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>Issues is experiencing degraded performance. We are continuing to investigate.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693931957000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">16:39</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>API Requests is experiencing degraded performance. We are continuing to investigate.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693931733000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">16:35</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>Codespaces is experiencing degraded availability. We are continuing to investigate.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693931608000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">16:33</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>Pull Requests is experiencing degraded performance. We are continuing to investigate.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693931501000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">16:31</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>Pages is experiencing degraded performance. We are continuing to investigate.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693931478000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">16:31</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Investigating
          </p>
          <div>
            <p><span>We are investigating reports of degraded performance for Actions</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1693931439000"></span>Sep <var data-var="date">05</var>, <var data-var="year">2023</var> - <var data-var="time">16:30</var> UTC
            </p>
          </div>
        </div>

      <!-- affected components -->
        <p>
          This incident affected: API Requests, Webhooks, Issues, Pull Requests, Actions, Packages, Pages, and Codespaces.
        </p>
    </div>

    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deno KV Is in Open Beta (120 pts)]]></title>
            <link>https://deno.com/blog/kv-open-beta</link>
            <guid>37394219</guid>
            <pubDate>Tue, 05 Sep 2023 16:34:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deno.com/blog/kv-open-beta">https://deno.com/blog/kv-open-beta</a>, See on <a href="https://news.ycombinator.com/item?id=37394219">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://deno.com/kv" rel="noopener noreferrer">Deno KV</a> is the fastest, easiest way to add a strongly consistent database
to your application. It‚Äôs built right into the runtime so you can skip
configuration and dive right into coding:</p>
<figure>

<div><pre><span>const</span> kv <span>=</span> <span>await</span> <span>Deno</span><span>.</span><span>openKv</span><span>(</span><span>)</span><span>;</span></pre></div><figcaption>Add state with a single line of code ‚Äî no need to provision a database or
copy/paste environmental variables.</figcaption>

</figure>

<p>Today, we‚Äôre excited to announce that <strong>Deno KV on Deno Deploy is now in open
beta</strong>. There‚Äôs no longer a waitlist so you can get access to Deno KV by simply
signing up for <a href="https://deno.com/deploy" rel="noopener noreferrer">Deno Deploy</a>. For those already using Deno Deploy, your
projects are automatically updated with KV access.</p>
<iframe width="100%" src="https://www.youtube.com/embed/GVtNU6MOa2c?si=nDneprHU-uct7Gxe" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>

<p>You can also watch our 2 minute video announcement summarizing this post.</p>

<p>Alongside our announcement, here are some other exciting updates about Deno KV
on Deno Deploy:</p>
<ul>
<li><a href="#connect-to-kv-from-outside-of-deno-deploy">Connect to KV from outside of Deno Deploy</a></li>
<li><a href="#pricing">Pricing</a></li>
<li><a href="#more-read-regions-for-lower-latency">More read regions for lower latency</a></li>
<li><a href="#more-powerful-atomic-operations">More powerful atomic operations</a></li>
<li><a href="#oauth-made-easy">OAuth made easy</a></li>
<li><a href="#key-ttl-with-expirein">Coming Soon: key TTL</a></li>
<li><a href="#s3-backups">Coming Soon: S3 backups</a></li>
<li><a href="#primary-region-selection">Coming Soon: primary region selection</a></li>
<li><a href="#whats-next">What‚Äôs next</a></li>
</ul>
<h2 id="connect-to-kv-from-outside-of-deno-deploy">Connect to KV from outside of Deno Deploy</h2><p>To make it easier to explore, import, migrate, and update your KV data on Deno
Deploy, you can now connect directly to your KV instance from the command line.
Simply set
<a href="https://dash.deno.com/account#access-tokens" rel="noopener noreferrer">a Deno Deploy personal access token</a>
as the local environmental variable <code>DENO_KV_ACCESS_TOKEN</code> and pass your
database URL to <code>Deno.openKv()</code>:</p>
<div><pre><span>const</span> kv <span>=</span> <span>await</span> <span>Deno</span><span>.</span><span>openKv</span><span>(</span>
  <span>"https://api.deno.com/databases/&lt;database-id&gt;/connect"</span><span>,</span>
<span>)</span><span>;</span></pre></div><p>You can find your database ID, URL, and more information on how to connect to
your managed KV database from your Deno Deploy project page:</p>
<p><img src="https://deno.com/blog/kv-open-beta/connecting-to-kv-locally.png" alt="Connecting to your Deno KV locally" title=""></p>
<p>Now it‚Äôll be easier to import data into production, handle data migrations,
inspecting/modifying/exploring data manually, and hosting Deno apps on a
different platform with global Deno KV for storage:</p>
<div><pre>

<span>import</span> <span><span>{</span> <span>Semaphore</span> <span>}</span></span> <span>from</span> <span>"https://deno.land/x/semaphore@v1.1.2/semaphore.ts"</span><span>;</span>

<span>const</span> kv <span>=</span> <span>await</span> <span>Deno</span><span>.</span><span>openKv</span><span>(</span>
  <span>"https://api.deno.com/databases/&lt;database-id&gt;/connect"</span><span>,</span>
<span>)</span><span>;</span>
<span>const</span> concurrencyLimit <span>=</span> <span>20</span><span>;</span>
<span>const</span> sem <span>=</span> <span>new</span> <span><span>Semaphore</span></span><span>(</span>concurrencyLimit<span>)</span><span>;</span>

<span>for</span> <span>await</span> <span>(</span>
  <span>const</span> <span>{</span> key<span>,</span> value<span>,</span> versionstamp <span>}</span> <span>of</span> kv<span>.</span><span><span>list</span><span><span>&lt;</span><span>any</span><span>&gt;</span></span></span><span>(</span><span>{</span> prefix<span>:</span> <span>[</span><span>"items"</span><span>]</span> <span>}</span><span>)</span>
<span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>typeof</span> value<span>?.</span>createdAt <span>===</span> <span>"number"</span><span>)</span> <span>{</span>
    value<span>.</span><span>createdAt</span> <span>=</span> <span>new</span> <span><span>Date</span></span><span>(</span>value<span>.</span><span>createdAt</span><span>)</span><span>.</span><span>toISOString</span><span>(</span><span>)</span><span>;</span>
    <span>const</span> release <span>=</span> <span>await</span> sem<span>.</span><span>acquire</span><span>(</span><span>)</span><span>;</span>
    <span>(</span><span>async</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
      <span>await</span> kv<span>.</span><span>atomic</span><span>(</span><span>)</span><span>.</span><span>check</span><span>(</span><span>{</span> key<span>,</span> versionstamp <span>}</span><span>)</span><span>.</span><span>set</span><span>(</span>key<span>,</span> value<span>)</span><span>.</span><span>commit</span><span>(</span><span>)</span><span>;</span>
      <span>release</span><span>(</span><span>)</span><span>;</span>
    <span>}</span><span>)</span><span>(</span><span>)</span><span>;</span>
  <span>}</span>
<span>}</span>

<span>for</span> <span>(</span><span>let</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> concurrencyLimit<span>;</span> i<span>++</span><span>)</span> <span>{</span>
  <span>await</span> sem<span>.</span><span>acquire</span><span>(</span><span>)</span><span>;</span>
<span>}</span>

<span>console</span><span>.</span><span>log</span><span>(</span><span>"Migration completed!"</span><span>)</span><span>;</span></pre></div><p><a href="https://deno.com/deploy/docs/kv#connect-to-managed-databases-from-outside-of-deno-deploy" rel="noopener noreferrer">Learn more about connecting to your global Deno KV from the command line ‚Üí</a></p>
<h2 id="pricing">Pricing</h2><p>We‚Äôre excited to announce transparent Deno KV pricing that scales with your
needs. Deno KV is billed along three dimensions: storage, read units, and write
units.</p>
<p>Storage is the amount of data you store in your Deno KV database over a month,
measured in gigabytes. Read and write units are the number of times you read and
write data from your Deno KV database, measured in 4 KiB read units and 1 KiB
write units respectively. If you perform 3 read transactions of 1 KiB each, you
will be billed for 3 read units. If you perform 1 write transaction of 4 KiB,
you will be billed for 4 write units.</p>
<p>Write unit usage is additionally multiplied by the number of regions you have
enabled for your database. If you have the primary region, and two read replicas
enabled, you will be billed for 3 write units for every 1 KiB of writes.</p>
<p>For every user, Deno Deploy‚Äôs free tier includes 1 GiB of storage, 15k read
units per day and 10k write units per day. Daily usage resets at UTC 00:00.</p>
<p>For those scaling with Deno KV and Deno Deploy, our Pro tier offers:</p>
<ul>
<li>5 GiB of monthly storage, with $0.75 per additional GiB</li>
<li>45k read units per day, with $1 per additional million reads</li>
<li>30k write units per day, with $2.5 per additional million writes</li>
</ul>
<p>Here is the complete pricing table for more details:</p>
<figure>

<table>
<thead>
<tr>
<th></th>
<th>Free</th>
<th>Pro</th>
<th>Custom</th>
</tr>
</thead>
<tbody><tr>
<td><strong>KV storage</strong></td>
<td>1 GB</td>
<td>5GB (then $0.50/GB)</td>
<td>Custom</td>
</tr>
<tr>
<td><strong>KV read units/day</strong> (4kb)</td>
<td>15,000</td>
<td>45,000 (then $1/M)</td>
<td>Custom</td>
</tr>
<tr>
<td><strong>KV write units/day</strong> (1kb)</td>
<td>10,000</td>
<td>30,000 (then $2.50/M)</td>
<td>Custom</td>
</tr>
<tr>
<td><strong>Number of DB regions</strong></td>
<td>1</td>
<td>Custom</td>
<td>Custom</td>
</tr>
<tr>
<td><strong>Select your write region</strong></td>
<td>No</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td><strong>Max monthly usage</strong></td>
<td>Free</td>
<td>$250</td>
<td>N/A</td>
</tr>
</tbody></table>
<figcaption>Does your application or business have unique scaling or global data storage
needs? <a href="mailto:deploy@deno.com">We‚Äôd love to learn more</a>.</figcaption>

</figure>

<p>As a reminder, using Deno KV locally without Deno Deploy will be free.</p>
<p><a href="https://deno.com/deploy/pricing" rel="noopener noreferrer">Learn more about our Deno Deploy and Deno KV pricing plans ‚Üí</a></p>
<h2 id="more-read-regions-for-lower-latency">More read regions for lower latency</h2><p>By default, every Deno KV database comes with a single write region and a single
read region. If your users are all over the world, some may experience longer
latencies due to the single read region.</p>
<p>Now, you can enable more read regions to expand your database globally for even
lower latency:</p>
<figure>

<p><img src="https://deno.com/blog/kv-open-beta/selecting-read-regions.png" alt="Screenshot of selecting read regions" title=""></p>
<figcaption>Expand your database globally by enabling multiple read regions with a click.</figcaption>

</figure>

<h2 id="more-powerful-atomic-operations">More powerful atomic operations</h2><p>Atomic operations are a powerful feature of Deno KV that allow you to perform
multiple operations in a single transaction. If any of the operations fail, the
entire transaction is rolled back, ensuring that your data is always consistent.</p>
<p>Previously, you could only perform up to 10 mutation operations in a single
transaction. Many of you told us that this limit was too low, especially when
dealing with complex data that require many secondary indexes.</p>
<p>We‚Äôve heard you! <strong>The limit has been increased to 1000 mutations per atomic
operation</strong>, or a total atomic operation size of 800 KiB, whichever is reached
first.</p>
<h2 id="oauth-made-easy">OAuth made easy</h2><p>User authentication is table stakes for any modern web app. To make it as simple
as possible to implement auth into your Deno Deploy projects, we‚Äôve built
<a href="https://github.com/denoland/deno_kv_oauth" rel="noopener noreferrer">Deno KV OAuth</a>, a high-level OAuth
2.0 wrapper with Deno KV as a backend with a ton of pre-configured common OAuth
2.0 providers, such as GitHub, Google, Facebook, Slack, Discord, and more.</p>
<p>You can add OAuth functionality to your Fresh project using
<a href="https://github.com/denoland/deno_kv_oauth#getting-started-with-fresh" rel="noopener noreferrer">the new plugin</a>:</p>
<div><pre>
<span>import</span> <span><span>{</span> start <span>}</span></span> <span>from</span> <span>"$fresh/server.ts"</span><span>;</span>
<span>import</span> <span><span>{</span> createGitHubOAuth2Client <span>}</span></span> <span>from</span> <span>"https://deno.land/x/deno_kv_oauth/mod.ts"</span><span>;</span>
<span>import</span> <span><span>{</span> kvOAuthPlugin <span>}</span></span> <span>from</span> <span>"https://deno.land/x/deno_kv_oauth/fresh.ts"</span><span>;</span>
<span>import</span> <span>manifest</span> <span>from</span> <span>"./fresh.gen.ts"</span><span>;</span>

<span>await</span> <span>start</span><span>(</span>manifest<span>,</span> <span>{</span>
  plugins<span>:</span> <span>[</span>
    <span>kvOAuthPlugin</span><span>(</span><span>createGitHubOAuth2Client</span><span>(</span><span>)</span><span>)</span><span>,</span>
  <span>]</span><span>,</span>
<span>}</span><span>)</span><span>;</span></pre></div><p>Or, if using another framework using
<a href="https://github.com/denoland/deno_kv_oauth#getting-started-with-other-frameworks" rel="noopener noreferrer">the Web API</a>:</p>
<figure>

<div><pre>
<span>import</span> <span><span>{</span>
  createGitHubOAuth2Client<span>,</span>
  handleCallback<span>,</span>
  signIn<span>,</span>
  signOut<span>,</span>
<span>}</span></span> <span>from</span> <span>"https://deno.land/x/deno_kv_oauth/mod.ts"</span><span>;</span>

<span>const</span> oauth2Client <span>=</span> <span>createGitHubOAuth2Client</span><span>(</span><span>)</span><span>;</span>

<span>async</span> <span>function</span> <span>handleSignIn</span><span>(</span>request<span>:</span> <span>Request</span><span>)</span> <span>{</span>
  <span>return</span> <span>await</span> <span>signIn</span><span>(</span>request<span>,</span> oauth2Client<span>)</span><span>;</span>
<span>}</span>

<span>async</span> <span>function</span> <span>handleOAuth2Callback</span><span>(</span>request<span>:</span> <span>Request</span><span>)</span> <span>{</span>
  <span>return</span> <span>await</span> <span>handleCallback</span><span>(</span>request<span>,</span> oauth2Client<span>)</span><span>;</span>
<span>}</span>

<span>async</span> <span>function</span> <span>handleSignOut</span><span>(</span>request<span>:</span> <span>Request</span><span>)</span> <span>{</span>
  <span>return</span> <span>await</span> <span>signOut</span><span>(</span>request<span>)</span><span>;</span>
<span>}</span></pre></div><figcaption>This high level wrapper provides many helper functions to make it easy to add auth, login, and more to your app.</figcaption>

</figure>

<p>Projects are already using Deno KV OAuth for authentication, such as
<a href="https://deno.com/saaskit" rel="noopener noreferrer">SaaSKit</a>, <a href="https://hashrock-kv-sketchbook.deno.dev/" rel="noopener noreferrer">KV Sketchbook</a>,
and <a href="https://github.com/denoland/deno_kv_oauth#in-the-wild" rel="noopener noreferrer">more</a>.</p>
<h2 id="coming-soon">Coming soon</h2><p>We‚Äôve received your feedback and are working hard to add useful features to make
Deno KV on Deno Deploy the easiest, fastest way to add state to your globally
hosted project.</p>
<p>Here‚Äôs an incomplete list of previews of what‚Äôs to come.</p>
<h3 id="key-ttl-with-expirein">Key TTL with <code>expireIn</code></h3><p>Key time-to-live expiration is an important feature in Redis, which allows you
to easily manage user sessions or create your own caching system. Soon, you can
do the same in Deno KV with the <code>expireIn</code> option, which accepts a number in
milliseconds:</p>
<div><pre><span>const</span> db <span>=</span> <span>await</span> <span>Deno</span><span>.</span><span>openKv</span><span>(</span><span>)</span><span>;</span>

<span>await</span> db<span>.</span><span>set</span><span>(</span><span>[</span><span>"a"</span><span>]</span><span>,</span> <span>1</span><span>,</span> <span>{</span> expireIn<span>:</span> <span>1000</span> <span>}</span><span>)</span><span>;</span></pre></div><p>The key will be deleted from the database after the specified number of
milliseconds in <code>expireIn</code> have elapsed. If the <code>expireIn</code> is not specified, the
key will not expire.</p>
<p>Note this feature is
<a href="https://deno.land/api@v1.36.4?s=Deno.Kv&amp;unstable=&amp;p=prototype.set" rel="noopener noreferrer">already available in the Deno runtime</a>.</p>
<h3 id="s3-backups">S3 backups</h3><p>Every production application demands data backups in case things go awry. Soon,
you‚Äôll be able to enable backups of your Deno KV data to your own S3 bucket:</p>
<p><img src="https://deno.com/blog/kv-open-beta/enabling-s3-backups.png" alt="Enabling S3 backups for your Deno KV data" title=""></p>
<p>The backup is real-time, supporting point-in-time recovery for your data.
Initially you‚Äôll be able to restore backups to a local Deno KV database, with
support for restoring to a hosted Deno KV database coming soon after.</p>
<h3 id="primary-region-selection">Primary region selection</h3><p>Deno KV uses a single primary region that processes writes and multiple read
regions handling reads. When a <code>write</code> transaction is sent, it‚Äôs first processed
at the primary region, then replicated to all read regions, providing eventual
data consistency across all regions. Thusly, your <code>write</code> latencies are impacted
by how far your users are from your primary region, while <code>read</code> latencies when
using when using <code>consistency: "eventual"</code> are impacted by how far your users
are from any of your read regions.</p>
<p>Soon, you‚Äôll be able to optimize latency and performance for your users by
<strong>selecting your primary region</strong>:</p>
<p><img src="https://deno.com/blog/kv-open-beta/selecting-primary-region.png" alt="Selecting your primary region in Deno KV" title=""></p>
<p>Since there‚Äôs no one-size-fits-all configuration for every app or business, its
important to have full control over how and where your data is being stored and
retrieved to optimize performance for your users.</p>
<p>A primary region switch takes less than 5 seconds to take effect and causes no
downtime for your application. We‚Äôll be rolling out this feature to Deno Deploy
users in the coming weeks.</p>
<h2 id="whats-next">What‚Äôs next?</h2><p>Our vision for Deno Deploy is to make it the easiest place to host JavaScript
through built-in cloud primitives such as <a href="https://deno.com/kv" rel="noopener noreferrer">Deno KV</a> (and more ‚Äî stay
tuned!). As we work towards a generally available release of Deno KV and Deno
Deploy, we‚Äôll continue to add features to make it as simple as possible to build
and host production-ready JavaScript apps.</p>
<p>We‚Äôre always open to feedback and feature requests! Feel free to
<a href="https://discord.gg/deno" rel="noopener noreferrer">join our growing Discord</a> or
<a href="https://github.com/denoland/deploy_feedback/issues" rel="noopener noreferrer">create an issue here</a>.</p>
<p><em><a href="https://deno.com/deploy" rel="noopener noreferrer">Deploy a project to to Deno Deploy today ‚Äî it‚Äôll take less than 5 minutes.</a></em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Bedframe ‚Äì open-source Browser Extension Development framework (128 pts)]]></title>
            <link>https://github.com/nyaggah/bedframe</link>
            <guid>37394194</guid>
            <pubDate>Tue, 05 Sep 2023 16:33:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/nyaggah/bedframe">https://github.com/nyaggah/bedframe</a>, See on <a href="https://news.ycombinator.com/item?id=37394194">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><div dir="auto"><p>
  &gt;_</p><p>
  
  <span>B R O W S E R</span><br>
  <span>E X T E N S I O N</span><br>
  <span>D E V E L O P M E N T</span><br>
  <span>F R A M E W O R K</span></p></div>

<p dir="auto">
  <a aria-label="Bedframe logo" href="https://bedframe.dev/" rel="nofollow">
    <img src="https://camo.githubusercontent.com/5f7bedc6cc4b78144e91dba7f8e8c91253a62188ad18f91ac5b99393e34b44e8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4245444652414d452d3761343666632e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d4265646672616d65266c6162656c436f6c6f723d434343" data-canonical-src="https://img.shields.io/badge/BEDFRAME-7a46fc.svg?style=for-the-badge&amp;logo=Bedframe&amp;labelColor=CCC">
  </a>
  <a aria-label="@bedframe/cli - NPM version" href="https://www.npmjs.com/package/@bedframe/cli" rel="nofollow">
    <img alt="@bedframe/cli - NPM version" src="https://camo.githubusercontent.com/1b33ee1f3b211374c2e907edf08b406b65a9f4c4bbc81a185a844a0935050233/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f406265646672616d652f636c692e7376673f7374796c653d666f722d7468652d6261646765266c6162656c436f6c6f723d303030303030266c6162656c3d636c69" data-canonical-src="https://img.shields.io/npm/v/@bedframe/cli.svg?style=for-the-badge&amp;labelColor=000000&amp;label=cli">
  </a>
  <a aria-label="@bedframe/core - NPM version" href="https://www.npmjs.com/package/@bedframe/core" rel="nofollow">
    <img alt="@bedframe/core - NPM version" src="https://camo.githubusercontent.com/03707b37c68850d258edf595517c788fa07f03ce6cd9738a704dd26238b188dc/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f406265646672616d652f636f72652e7376673f7374796c653d666f722d7468652d6261646765266c6162656c436f6c6f723d303030303030266c6162656c3d636f7265" data-canonical-src="https://img.shields.io/npm/v/@bedframe/core.svg?style=for-the-badge&amp;labelColor=000000&amp;label=core">
  </a>
  <a aria-label="License" href="https://github.com/nyaggah/bedframe/blob/main/LICENSE">
    <img alt="License" src="https://camo.githubusercontent.com/c3da3bc6c9a5232ffe83c60fc948252cb7f2c038da303ef7f3729f063bcb292e/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f6c2f6e6578742e7376673f7374796c653d666f722d7468652d6261646765266c6162656c436f6c6f723d303030303030" data-canonical-src="https://img.shields.io/npm/l/next.svg?style=for-the-badge&amp;labelColor=000000">
  </a>
</p>
<h2 tabindex="-1" dir="auto"><a href="https://bedframe.dev/" rel="nofollow">B E D F R A M E</a></h2>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/284415/265583920-d545dea4-129e-42f0-82fd-5e856c655e61.png"><img src="https://user-images.githubusercontent.com/284415/265583920-d545dea4-129e-42f0-82fd-5e856c655e61.png" alt="Bedframe - Make, Version &amp; Publish cross-browser extensions continously with ease"></a>
<a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/284415/265586669-512540dd-18b7-4fbe-9ebe-861722b83a97.png"><img src="https://user-images.githubusercontent.com/284415/265586669-512540dd-18b7-4fbe-9ebe-861722b83a97.png" alt="bedframe-cli"></a></p>
<p dir="auto">Your <strong>B</strong>rowser <strong>E</strong>xtension <strong>D</strong>evelopment Framework</p>
<p dir="auto">Make, Version &amp; Publish cross-browser extensions (continously) with ease.</p>
<h2 tabindex="-1" dir="auto">Getting Started</h2>
<div dir="auto" data-snippet-clipboard-copy-content="pnpm add @bedframe/cli -g

bedframe make # ...follow prompts üëç"><pre>pnpm add @bedframe/cli -g

bedframe make <span><span>#</span> ...follow prompts üëç</span></pre></div>
<p dir="auto">Visit <a href="https://bedframe.dev/" rel="nofollow">https://bedframe.dev</a> to join early access.</p>
<h2 tabindex="-1" dir="auto">Documentation</h2>
<p dir="auto">Visit <a href="https://bedframe.dev/docs" rel="nofollow">https://bedframe.dev/docs</a> (coming soon) to view the full documentation.</p>
<h2 tabindex="-1" dir="auto">Packages</h2>
<table>
<thead>
<tr>
<th>Package</th>
<th>Version</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/nyaggah/bedframe/blob/main/packages/cli">@bedframe/cli</a></td>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/34a90448aa69208882daf471981f374162425a18c3e8190f9946433c47a9a41b/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f406265646672616d652f636c692e7376673f6c6162656c3d253230"><img src="https://camo.githubusercontent.com/34a90448aa69208882daf471981f374162425a18c3e8190f9946433c47a9a41b/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f406265646672616d652f636c692e7376673f6c6162656c3d253230" alt="@bedframe/cli version" data-canonical-src="https://img.shields.io/npm/v/@bedframe/cli.svg?label=%20"></a></td>
<td>Bedframe CLI. Make, Version and Publish your BED from the command line</td>
</tr>
<tr>
<td><a href="https://github.com/nyaggah/bedframe/blob/main/packages/core">@bedframe/core</a></td>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/50392735affa5c8846f8c4a9b0890fc43adcb4c8eb346467f6ba6f80443abaf1/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f406265646672616d652f636f72652e7376673f6c6162656c3d253230"><img src="https://camo.githubusercontent.com/50392735affa5c8846f8c4a9b0890fc43adcb4c8eb346467f6ba6f80443abaf1/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f406265646672616d652f636f72652e7376673f6c6162656c3d253230" alt="@bedframe/core version" data-canonical-src="https://img.shields.io/npm/v/@bedframe/core.svg?label=%20"></a></td>
<td>Bedframe core types and functions</td>
</tr>
<tr>
<td><a href="https://github.com/nyaggah/bedframe/blob/main/packages/create-bedframe">create-bedframe</a></td>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/50392735affa5c8846f8c4a9b0890fc43adcb4c8eb346467f6ba6f80443abaf1/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f406265646672616d652f636f72652e7376673f6c6162656c3d253230"><img src="https://camo.githubusercontent.com/50392735affa5c8846f8c4a9b0890fc43adcb4c8eb346467f6ba6f80443abaf1/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f406265646672616d652f636f72652e7376673f6c6162656c3d253230" alt="create-bedframe version" data-canonical-src="https://img.shields.io/npm/v/@bedframe/core.svg?label=%20"></a></td>
<td>Standalone <a href="https://github.com/nyaggah/bedframe/blob/main/packages/cli">@bedframe/cli</a> <code>make</code> command utility</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto">Monorepo Utility Packages</h2>
<table>
<thead>
<tr>
<th>Package</th>
<th>Version</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/nyaggah/bedframe/blob/main/packages/eslint-config-bedframe">eslint-config-bedframe</a></td>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1769b9f8674da1119e8f831b41beb5c4a4dafe0671aace701a52bad2730e87ac/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f65736c696e742d636f6e6669672d6265646672616d652e7376673f6c6162656c3d253230"><img src="https://camo.githubusercontent.com/1769b9f8674da1119e8f831b41beb5c4a4dafe0671aace701a52bad2730e87ac/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f65736c696e742d636f6e6669672d6265646672616d652e7376673f6c6162656c3d253230" alt="eslint-config-bedframe version" data-canonical-src="https://img.shields.io/npm/v/eslint-config-bedframe.svg?label=%20"></a></td>
<td>Shared ESLint configs used in this Turborepo</td>
</tr>
<tr>
<td><a href="https://github.com/nyaggah/bedframe/blob/main/packages/tsconfig">@bedframe/tsconfig</a></td>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/185cfb3e42e41abcd278e0fc7f02d4b0998796b651c82d34fa437d9417d084ab/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f406265646672616d652f7473636f6e6669672e7376673f6c6162656c3d253230"><img src="https://camo.githubusercontent.com/185cfb3e42e41abcd278e0fc7f02d4b0998796b651c82d34fa437d9417d084ab/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f406265646672616d652f7473636f6e6669672e7376673f6c6162656c3d253230" alt="@bedframe/tsconfig version" data-canonical-src="https://img.shields.io/npm/v/@bedframe/tsconfig.svg?label=%20"></a></td>
<td>Shared TypeScript configs used in this Turborepo</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto">Authors</h2>
<ul dir="auto">
<li>Joe Nyaggah (<a href="https://twitter.com/nyaggah" rel="nofollow">@nyaggah</a>)</li>
</ul>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto"><a href="https://github.com/nyaggah/bedframe/blob/main/LICENSE">MIT</a>.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hackers selling hacked police emails to request user data from TikTok, Facebook (135 pts)]]></title>
            <link>https://www.404media.co/buying-and-selling-hacked-government-emails-edrs-discord-snapchat-facebook-tiktok/</link>
            <guid>37394044</guid>
            <pubDate>Tue, 05 Sep 2023 16:26:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/buying-and-selling-hacked-government-emails-edrs-discord-snapchat-facebook-tiktok/">https://www.404media.co/buying-and-selling-hacked-government-emails-edrs-discord-snapchat-facebook-tiktok/</a>, See on <a href="https://news.ycombinator.com/item?id=37394044">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
          <div>
              <!--kg-card-begin: html--><!--kg-card-end: html--><p>‚ÄúHowdy Joseph,‚Äù the July email I got from Zdravko Krivokapiƒá, who was the Prime Minister of Montenegro until last year, read.</p><p>Obviously, this wasn‚Äôt actually Krivokapiƒá emailing me. Instead, it was a hacker who had gained access to what seemed to be Krivokapiƒá‚Äôs personal Gmail account. The hackers proceeded to send me a mass of alleged documents from the government of Montenegro, including some related to the country‚Äôs Ministry of Finance. Alongside those, the hacker also sent photos of cash, flashy watches, and weapons, which appear to be from the hacker‚Äôs own collection and not the former Prime Minister‚Äôs.</p><p>Beyond wanting to flex their access to Krivokapiƒá‚Äôs account, the hacker said they might use the compromised email to then target other services, using the former Prime Minster‚Äôs identity as a cover. It‚Äôs unclear how successful that attempt may have been, but the brazenness of emailing a journalist from an official‚Äôs email account did highlight something gaining popularity in the digital underground. Hackers are compromising the email accounts of government and law enforcement officials, selling them on the open market, and in some cases using that access to trick social media giants and other legitimate companies to hand over their customers' data. Desired targets include TikTok, Discord, Snapchat, Facebook, and Instagram. The groups where these email accounts are often advertised include criminals who use personal information to target people for harassment, extortion, or physical violence.</p><p>The hacker‚Äôs initial email to me ended with ‚ÄúLOL.‚Äù</p><div><p>üí°</p><p><strong>Do you know anything else about fraudulent use of EDRs? I would love to hear from you. Using a non-work device, you can message me securely on Signal at +44 20 8133 5190. Otherwise, send me an email at joseph@404media.co.</strong></p></div><p>Cybercriminals sell access to these compromised government accounts across a variety of forums and groups chats, especially on the messaging app Telegram. One person who is a reputable seller of personal information on Telegram also claims to be selling such email accounts. One screenshot they shared on Telegram shows an inbox allegedly belonging to a Brazilian municipality; the seller said they are offering accounts for $400 each. In another post and accompanying screenshot, they claimed to have access to an FBI email account.</p><p>A second apparent seller wrote in one popular Telegram group they are ‚ÄúSELLING INDIAN GOV MAILS, $100 A PIECE, CAN ACCESS FB LAW PANEL/EDR IG/FB ACCS.‚Äù The post adds they are selling ‚Äúother third world gov mails‚Äù for $50 each.</p><p>Other messages viewed by 404 Media advertise emails belonging to the governments of Thailand, the UK, Germany, Bangladesh, and Nepal.</p><!--kg-card-begin: html-->  <div>
    <h5>Subscribe</h5>
    <div>
      <p>Join the newsletter to get the latest updates.</p>
      <form data-members-form="subscribe">
        
        
        <div>
          
          <p>
            Great! Check your inbox and click the link.
          </p>
        </div>
        <div>
          
          <p>
            Please enter a valid email address.
          </p>
        </div>
      </form>
    </div>
  </div>
<!--kg-card-end: html--><p>Many of the adverts explicitly say that buyers can use these email accounts to then make Emergency Data Requests, or EDRs. EDRs are a common mechanism across social media or tech companies designed to provide user data to law enforcement in high stakes situations. This, for example, might include a child kidnapping, where authorities may need data quickly in an attempt to apprehend a suspect or locate a victim.</p><p>One Telegram group where government emails are being explicitly advertised as a way to gain access to sensitive user data <a href="https://www.404media.co/the-secret-weapon-hackers-can-use-to-dox-nearly-anyone-in-america-for-15-tlo-usinfosearch-transunion/">is focused on physical violence against targets</a>. Here, members can hire one another to perform shootings, stabbings, robberies, and more. </p><p>Companies each have their own way for handling EDRs, be that a locked-off web portal or a dedicated department to contact. But they typically require anyone requesting data to contact the company from an official government or law enforcement agency email address.</p><p>That‚Äôs why these compromised accounts are so valuable to criminals. They allow hackers to tap into a stream of data that is usually off limits, simply by pretending to be a law enforcement officer. In March last year <a href="https://krebsonsecurity.com/2022/03/hackers-gaining-power-of-subpoena-via-fake-emergency-data-requests/?ref=404media.co">cybersecurity reporter Brian Krebs reported</a> on the rise of fraudulent EDR requests among cybercriminals and pointed to a specific case involving Discord. A day later, <a href="https://www.bloomberg.com/news/articles/2022-03-30/apple-meta-gave-user-data-to-hackers-who-forged-legal-requests?ref=404media.co#xj4y7vzkg">Bloomberg reported</a> that Apple and Meta had given up user data in response to such demands.</p><p>In more recent Telegram messages, 404 Media has seen criminals specifically discuss the ability to make fraudulent EDRs with TikTok, Instagram, Facebook, and GoDaddy. Others have shown interest in targeting Discord and Snapchat.</p><p>Meta told 404 Media it blocks known compromised accounts from making requests to its dedicated Law Enforcement Response Team (LERT).</p><p>TikTok confirmed to 404 Media it more commonly sees fraudulent requests from people impersonating law enforcement agencies in foreign countries. TikTok said it has successfully blocked some fraudulent requests, but declined to say whether any have managed to get through. TikTok added it has additional safeguards in place to vet EDRs and tools to protect those requests.</p><p>A Discord spokesperson told 404 Media in a statement that ‚ÄúLike any company, we are obligated to comply with law enforcement requests. To ensure the legitimacy of requests from law enforcement, we follow thorough guidelines to carefully evaluate them and ensure they come from a genuine source and that they are not overly broad or vague.‚Äù</p><p>Snapchat and GoDaddy did not respond to a request for comment.</p><p>Krivokapiƒá, the former Prime Minister of Montenegro who a hacker appeared to have targeted, did not respond to multiple requests for comment.</p><!--kg-card-begin: html--><!--kg-card-end: html--><p><em>Update: this piece has been updated with a statement from Discord.</em></p>
          </div>
        </article>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Learn WebAssembly by writing small programs (381 pts)]]></title>
            <link>https://github.com/EmNudge/watlings</link>
            <guid>37393820</guid>
            <pubDate>Tue, 05 Sep 2023 16:13:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/EmNudge/watlings">https://github.com/EmNudge/watlings</a>, See on <a href="https://news.ycombinator.com/item?id=37393820">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><div dir="auto">
<h2 tabindex="-1" dir="auto">Watlings</h2>
<p dir="auto">Learn the WebAssembly Text Format<br>
by fixing a bunch of small programs!</p>

<p dir="auto"><a href="https://github.com/users/EmNudge/projects/1"><img src="https://camo.githubusercontent.com/15dc6d9fc7651c306a598f40f23808815bb4bcb8adab052fdeeb340fc30e6f28/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f526f61646d61702d3139413937343f7374796c653d666f722d7468652d6261646765266c6f676f436f6c6f723d7768697465266c6f676f3d6f70656e7374726565746d6170" alt="Button Roadmap" data-canonical-src="https://img.shields.io/badge/Roadmap-19A974?style=for-the-badge&amp;logoColor=white&amp;logo=openstreetmap"></a></p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/24513691/261171971-a777c665-fd13-4422-a570-2d3669b0ee94.png"><img src="https://user-images.githubusercontent.com/24513691/261171971-a777c665-fd13-4422-a570-2d3669b0ee94.png" alt="Example Exercise"></a></p>
</div>

<p dir="auto"><span>Warning</span><br>
This project is incomplete and in active development.<br>
Feel free to help out by filing issues and creating PRs!</p>
<h2 tabindex="-1" dir="auto">Usage</h2>
<p dir="auto">This project uses <strong><a href="https://nodejs.org/en" rel="nofollow">Node 16+</a></strong> &amp; <strong><a href="https://www.npmjs.com/" rel="nofollow">NPM</a></strong> for compilation and testing.</p>

<p dir="auto">Clone the repository and install dependencies with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:EmNudge/watlings.git
cd watlings
npm install"><pre>git clone git@github.com:EmNudge/watlings.git
<span>cd</span> watlings
npm install</pre></div>

<p dir="auto">Test your answer to an exercise with the <code>start</code> command:</p>


<p dir="auto">If you'd like to view the solution to an exercise, use the <code>solve</code> command:</p>

<br>
<h2 tabindex="-1" dir="auto">Using Wat2Wasm Directly ( Recommended )</h2>
<p dir="auto">For syntax highlighting and up-to-date builds, you can <strong>optionally</strong> use the official <strong><a href="https://github.com/WebAssembly/wabt">WebAssembly Binary Toolkit</a></strong> which will provide you with a <code>wat2wasm</code> CLI tool.</p>
<p dir="auto">If it is found on your path as <code>wat2wasm</code>, it will be used instead of <strong><a href="https://www.npmjs.com/package/wabt" rel="nofollow">NPM WABT</a></strong>.</p>
<p dir="auto"><em>While it is strictly optional, it can help with debugging.</em></p>
<br>
<h2 tabindex="-1" dir="auto">Recommended Editor</h2>
<p dir="auto">We recommend using <strong><a href="https://code.visualstudio.com/" rel="nofollow">VSCode</a></strong> with the <strong><a href="https://github.com/NateLevin1/wati">WATI</a></strong> extension.</p>
<p dir="auto">This should provide syntax highlighting, intellisense, and other helpful features as you work through the exercises.</p>
<br>
<h2 tabindex="-1" dir="auto">Motivations</h2>
<p dir="auto">I've found just diving in to be the best way to build experience with programming.</p>
<p dir="auto"><strong><a href="https://github.com/rust-lang/rustlings">Rustlings</a></strong> &amp; <strong><a href="https://github.com/ratfactor/ziglings">Ziglings</a></strong> have both had tremendous returns to my journeys with both languages.</p>
<p dir="auto">WebAssembly (and by extension WAT) has a more sparse educational landscape than most and I was hoping to fill some of the gaps by building a project with the same sort of structure.</p>
<br>
<h2 tabindex="-1" dir="auto">Pedagogical Philosophy</h2>
<p dir="auto">Outlined here are some thoughts on what makes a good teaching experience.</p>
<h3 tabindex="-1" dir="auto">Typing Over Reading</h3>
<p dir="auto">The goal is to learn by doing. Comments on each file outline a task and some background. However, a lot about a language can be gleaned by its syntax alone. We should be adding <strong>as little</strong> explanation as possible.</p>
<p dir="auto">Occasional gaps in knowledge can be filled by consistent exposure to the syntax within different contexts. Certain things can therefore be learned without any mention.</p>
<p dir="auto">Introduction text is superfluous. Words add visual noise, so we should be careful with our count. Coding itself should supplement ambiguities in the text.</p>
<p dir="auto">If you find a text confusing or too verbose, <strong>please create a discussion post</strong>!</p>
<h3 tabindex="-1" dir="auto">Create Struggle</h3>
<p dir="auto">Studies have shown that one cannot learn effectively without effort. This applies to practically every domain of knowledge. These projects should be educational, not easy.</p>
<p dir="auto">This does not mean we should make the education itself elusive. We should not make learning more difficult, but instead more intentional.</p>
<p dir="auto">When introducing a lot of new syntax, keep the problem scope small, but force the user to read a bit. If the syntax is not new, increase the problem scope. Maybe many variations of the same task.</p>
<br>
<h2 tabindex="-1" dir="auto">Credits</h2>
<ul dir="auto">
<li><a href="https://github.com/rust-lang/rustlings">rustlings</a></li>
<li><a href="https://github.com/ratfactor/ziglings">Ziglings</a></li>
</ul>

</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Czech scientists confirm the existence of the œÄ-hole in molecules (112 pts)]]></title>
            <link>https://www.avcr.cz/en/news-archive/Czech-scientists-confirm-the-existence-of-the-hole-in-molecules-proving-a-decades-old-theory/</link>
            <guid>37393652</guid>
            <pubDate>Tue, 05 Sep 2023 16:03:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.avcr.cz/en/news-archive/Czech-scientists-confirm-the-existence-of-the-hole-in-molecules-proving-a-decades-old-theory/">https://www.avcr.cz/en/news-archive/Czech-scientists-confirm-the-existence-of-the-hole-in-molecules-proving-a-decades-old-theory/</a>, See on <a href="https://news.ycombinator.com/item?id=37393652">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>30. 08. 2023</p><p><strong>Researchers from the <a href="https://www.uochb.cz/en" target="_blank" rel="noopener">IOCB of the CAS</a>, the <a href="https://www.fzu.cz/en/home" target="_blank" rel="noopener">Institute of Physics of the CAS</a> and <a href="https://www.upol.cz/en/" target="_blank" rel="noopener">Palack√Ω University Olomouc</a> have made a groundbreaking discovery ‚Äì using an advanced method of scanning electron microscopy, they‚Äôve managed to image not only the inside of a molecule, but also the structure of the electron shell of the atom. Their experiment was the first in the world to confirm the non-uniform distribution of electron density in aromatic molecules and the existence of the so-called œÄ-hole. </strong><strong>The significance of the results of this research at the submolecular level is comparable to that of the discovery of cosmic black holes. </strong><strong>The study was published in <em><a href="https://www.nature.com/articles/s41467-023-40593-3" target="_blank" rel="noopener">Nature Communications</a></em>.</strong></p><div>
			<p>The scientists focused their experiments on aromatic hydrocarbons. Anthracene, benzene, naphthalene, and other aromatic molecules have so-called pi electrons located above and below the carbon skeleton. ‚ÄúHowever, if we replace the peripheral hydrogens with more electronegative atoms or groups of atoms that pull electrons away, the originally negatively charged clouds turn into positively charged electron holes ‚Äì œÄ-holes [pi-holes],‚Äù explains Pavel Hobza from the Institute of Organic Chemistry and Biochemistry of the CAS (IOCB Prague), who contributed significantly to the research in question.</p>
<p>The researcher likens the aromatic molecule to a volcano. When the volcano erupts, a huge crater appears underneath, and we can imagine this as the œÄ-hole. Pavel Hobza explains more in the following video (English subtitles available):</p>
<p><iframe title="YouTube video player" src="/system/modules/cz.osw.opencms.types/elements/cookies/content_is_blocked.html" data-src="https://www.youtube.com/embed/-Tgiob7vePk?si=F9Od0r45OJS6myIN" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p>The significance of the results of this research at the submolecular level is comparable to that of the discovery of cosmic black holes. In fact, black holes were theorized for decades as well before experiments proved and confirmed their existence. The same research team involved in the discovery of the œÄ-hole had already achieved great success with their previous study, which was published in <em><a href="https://www.science.org/doi/10.1126/science.abk1479" target="_blank" rel="noopener">Science</a></em> in 2021, in which they proved the existence of so-called sigma-holes (œÉ-holes).</p>
<p>‚ÄúThe confirmation of the existence of the œÄ-hole, as well as the œÉ-hole before it, fully demonstrates the quality of the theoretical predictions of quantum chemistry, which have accounted for both phenomena for decades. It shows that they can be relied upon even in the absence of available experiments,‚Äù Hobza explains.</p>
<blockquote>

‚Äî IOCB Prague (@IOCBPrague) <a href="https://twitter.com/IOCBPrague/status/1696424519936643407?ref_src=twsrc%5Etfw">August 29, 2023</a></blockquote>
<p><strong>Unique microscopes<br></strong>The above-mentioned discoveries have been made thanks to the unique scanning electron microscopy available at the Czech Advanced Technology and Research Institute (CATRIN). ‚ÄúThanks to our previous experience with the Kelvin Probe Force Microscopy (KPFM) technique, we have been able to refine our measurements and acquire very complete data sets that have helped us to deepen our understanding not only of how the charge is distributed in the molecules but also of what observables are obtained with the technique,‚Äù says Bruno de la Torre, head of the research group at CATRIN.</p>
<p>A better knowledge of the distribution of the electron charge will help scientists understand a number of chemical and biological processes. In practice, this will translate into the ability to build new supramolecules and subsequently develop advanced nanomaterials with improved properties.</p>
<hr>
<p><em>Prepared by: Leona Matu≈°kov√°, External Relations Division, CAO of the CAS, drawing on the <a href="https://www.uochb.cz/en/news/535/scientists-continue-to-push-the-boundaries-of-imaging-techniques-and-reveal-the-mysterious-world-of-molecules" target="_blank" rel="noopener">IOCB Prague press release</a><br></em><em>Translated by: Tereza Novick</em><em>√°, </em><em>External Relations Division, CAO of the CAS<br></em><em>Photo: Shutterstock (aromatic hydrocarbons)</em></p></div></div>]]></description>
        </item>
    </channel>
</rss>