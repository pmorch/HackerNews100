<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 07 Jan 2024 22:00:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Windows XP 2024 Edition is everything I want from a new OS (170 pts)]]></title>
            <link>https://overclock3d.net/news/software/windows-xp-2024-edition-is-everything-i-want-from-a-new-os/</link>
            <guid>38903314</guid>
            <pubDate>Sun, 07 Jan 2024 17:48:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://overclock3d.net/news/software/windows-xp-2024-edition-is-everything-i-want-from-a-new-os/">https://overclock3d.net/news/software/windows-xp-2024-edition-is-everything-i-want-from-a-new-os/</a>, See on <a href="https://news.ycombinator.com/item?id=38903314">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>




 
<p><a href="https://media.overclock3d.net/2024/01/Windows-XP-2024-Edition.jpg" target="_blank"><img fetchpriority="high" decoding="async" src="https://media.overclock3d.net/2024/01/Windows-XP-2024-Edition.jpg" alt="" width="1200" height="675"></a></p>
<h2>Windows XP 2024 is the OS I wish was real</h2>
<p>2024 is here, and rumour has it that Microsoft plans to announce/release Windows 12 this year. Yes, another new OS from Microsoft. Windows XP may be a dead OS, but it still holds a place in our hearts, and with Windows XP 2024 Edition, we can see what a revived Windows XP could look like.</p><p><a data-no-instant="1" href="https://www.scan.co.uk/products/deepcool-assassin-iv-cooler" rel="noopener" target="_blank"><img decoding="async" src="https://media.overclock3d.net/2023/09/728x90.jpg" alt="" width="728" height="90"></a></p>
<p><a href="https://www.youtube.com/watch?v=YLFUl9MW_Ks"><strong>AR 4789</strong></a> has designed and showcased a modern interpretation of Windows XP 2024 Edition. The OS’ design is familiar, yet new, and feels remarkably simple. There is no Cortana, no AI Co-Pilot, or an extreme amount of unwanted, pre-installed software. XP 2024 Edition is a call-back to simpler times, before your OS could show you ads, and when the Windows search function was a useful feature (No, I don’t want you to search using Bing!).</p>
<p>Honestly, now that I have seen it, I am a little sad that Windows XP 2024 Edition doesn’t exist. It is a concept, nothing more. It is a shame that Microsoft cannot create a new version of Windows that runs as smoothly as AR 4789’s concepts. Watch the video below to see it for yourself.</p>
<p><iframe title="Windows XP 2024" width="500" height="281" src="https://www.youtube.com/embed/YLFUl9MW_Ks?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<p>With a simple UI, a nice looking dark mode and its iconic wallpaper, Windows XP looks great in its modern form. Like many PC users, I remember Windows XP and Windows 7 more fondly than their successors. While nostalgia plays a role here, so do other factors. Many aspects of Windows 11 are unwanted. Login requirements, changes to Windows Search, and taskbar changes, are all bugbears. Windows 12 is unlikely to address these issues, and moving to Linux is a step too far for most PC users.</p><p><a data-no-instant="1" href="https://msi.gm/S96E4205" rel="noopener" target="_blank"><img loading="lazy" decoding="async" src="https://media.overclock3d.net/2023/12/image_2023_12_06T15_16_39_674Z.jpg" alt="" width="728" height="90"></a></p>
<p>Today, Windows XP is practically extinct. Even newer OS’ like Windows 7 and 8 are becoming an increasingly rare sight. This week <a href="https://overclock3d.net/news/software/steam-has-dropped-support-for-windows-7-8-and-8-1/"><strong>Steam dropped support for pre-Windows 10 OS’</strong></a>. Windows 7 will be 15 years old this year, which means that it is getting close to “Retro” status. By comparison, Windows XP is practically ancient. Even so, I still like it, and I love the idea of its 2024 reimagining.</p>
<p>You can join the discussion on <a href="https://forum.overclock3d.net/showthread.php?t=101213"><strong>Windows XP 2024 Edition on the OC3D Forums</strong></a>.</p>

<div>
<p><img src="https://media.overclock3d.net/2023/09/l9qmlVQ4_400x400.png" alt="Mark Campbell">
</p>

</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The immediate victims of the con would rather act as if the con never happened (110 pts)]]></title>
            <link>https://statmodeling.stat.columbia.edu/2024/01/07/french-bio-lab-research-scandal/</link>
            <guid>38903145</guid>
            <pubDate>Sun, 07 Jan 2024 17:32:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://statmodeling.stat.columbia.edu/2024/01/07/french-bio-lab-research-scandal/">https://statmodeling.stat.columbia.edu/2024/01/07/french-bio-lab-research-scandal/</a>, See on <a href="https://news.ycombinator.com/item?id=38903145">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<p>Dorothy Bishop <a rel="nofollow" href="http://deevybee.blogspot.com/2023/02/open-letter-to-cnrs.html">has the story</a> about “a chemistry lab in CNRS-Université Sorbonne Paris Nord”:</p>
<blockquote><p>More than 20 scientific articles from the lab of one principal investigator have been shown to contain recycled and doctored graphs and electron microscopy images. That is, results from different experiments that should have distinctive results are illustrated by identical figures, with changes made to the axis legends by copying and pasting numbers on top of previous numbers. . . . the problematic data are well-documented in a number of PubPeer comments on the articles (see links in Appendix 1 of <a rel="nofollow" href="https://docs.google.com/document/d/1di9gvmdMDi81PPgX-7jK8QDD_nREHDVKpuNvOi3rjbM/edit">this document</a>).</p>
<p>The response by CNRS [Centre National de la Recherche Scientifique] to this case . . . was to request correction rather than retraction of what were described as “shortcomings and errors”, to accept the scientist’s account that there was no intentionality, despite clear evidence of a remarkable amount of manipulation and reuse of figures; a disciplinary sanction of exclusion from duties was imposed for just one month.</p></blockquote>
<p>I’m not surprised.  The sorts of people who will cheat on their research are likely to be the same sorts of people who will instigate lawsuits, start media campaigns, and attack in other ways.  These are researchers who’ve already shown a lack of scruple and a willingness to risk their careers; in short, they’re loose cannons, scary people, so it can seem like the safest strategy to not try to upset them too much, not trap them into a corner where they’ll fight like trapped rats.  I’m not speaking specifically of this CNRS researcher—I know nothing of the facts of this case beyond what’s reported in Bishop’s post—I’m just speaking to the mindset of the academic administrators who would just like the problem to go away so they can get on with their regular jobs.</p>
<p>But Bishop and her colleagues were annoyed.  If even blatant examples of scientific misconduct cannot be handled straightforwardly, what does this say about the academic and scientific process more generally?  Is science just a form of social media, where people can make any sort of claim and evidence doesn’t matter?</p>
<p>They write:</p>
<blockquote><p>So what should happen when fraud is suspected?  We propose that there should be a prompt investigation, with all results transparently reported. Where there are serious errors in the scientific record, then the research articles should immediately be retracted, any research funding used for fraudulent research should be returned to the funder, and the person responsible for the fraud should not be allowed to run a research lab or supervise students. The whistleblower should be protected from repercussions.</p>
<p>In practice, this seldom happens. Instead, we typically see, as in this case, prolonged and secret investigations by institutions, journals and/or funders. There is a strong bias to minimize the severity of malpractice, and to recommend that published work be “corrected” rather than retracted.</p></blockquote>
<p>Bishop and her colleagues continue:</p>
<blockquote><p>One can see why this happens. First, all of those concerned are reluctant to believe that researchers are dishonest, and are more willing to assume that the concerns have been exaggerated. It is easy to dismiss whistleblowers as deluded, overzealous or jealous of another’s success. Second, there are concerns about reputational risk to an institution if accounts of fraudulent research are publicised. And third, there is a genuine risk of litigation from those who are accused of data manipulation. So in practice, research misconduct tends to be played down.</p></blockquote>
<p>But:</p>
<blockquote><p>This failure to act effectively has serious consequences:</p>
<p>1.   It gives credibility to fictitious results, slowing down the progress of science by encouraging others to pursue false leads. . . . [and] erroneous data pollutes the databases on which we depend.</p>
<p>2.   Where the research has potential for clinical or commercial application, there can be direct damage to patients or businesses.</p>
<p>3.   It allows those who are prepared to cheat to compete with other scientists to gain positions of influence, and so perpetuate further misconduct, while damaging the prospects of honest scientists who obtain less striking results.  </p>
<p>4.   It is particularly destructive when data manipulation involves the Principal Investigator of a lab. . . . CNRS has a mission to support research training: it is hard to see how this can be achieved if trainees are placed in a lab where misconduct occurs.</p>
<p>5.   It wastes public money from research grants.</p>
<p>6.   It damages public trust in science and trust between scientists.</p>
<p>7.   It damages the reputation of the institutions, funders, journals and publishers associated with the fraudulent work.</p>
<p>8.   Whistleblowers, who should be praised by their institution for doing the right thing, are often made to feel that they are somehow letting the side down by drawing attention to something unpleasant. . . .</p></blockquote>
<p><strong>What happened next?</strong></p>
<p>It’s the usual bad stuff.  They receive a series of stuffy bureaucratic responses, none of which address any of items 1 through 8 above, let alone the problem of the data which apparently have obviously been faked.  Just disgusting.</p>
<p>But I’m not surprised.  We’ve seen it many times before:</p>
<p>– The University of California’s unresponsive response when informed of research misconduct by their star sleep expert.</p>
<p>– The American Political Science Association refusing to retract an award given to an author for a book with plagiarized material, or even to retroactively have the award shared with the people whose material was copied without acknowledgment.</p>
<p>– The London Times never acknowledging the blatant and repeated plagiarism by its celebrity chess columnist.</p>
<p>– The American Statistical Association refusing to retract an award given to a professor who plagiarized multiple times, including from wikipedia (in an amusing case where he created negative value by introducing an error into the material he’d copied, so damn lazy that he couldn’t even be bothered to proofread his pasted material).</p>
<p>– Cornell University . . . ok they finally canned the pizzagate dude, but only after emitting some platitudes.  Kind of amazing that they actually moved on that one.</p>
<p>– The Association for Psychological Science:  this one’s personal for me, as they ran an article that flat-out lied about me and then refused to correct it just because, hey, they didn’t want to.</p>
<p>– Lots and lots of examples of people finding errors or fraud in published papers and journals refusing to run retractions or corrections or even to publish letters pointing out what went wrong.</p>
<p>Anyway, this is one more story.</p>
<p><strong>What gets my goat</strong></p>
<p>What really annoys me in these situations is how the institutions show loyalty to the people who did research misconduct.  When researcher X works at or publishes with institution Y, and it turns out that X did something wrong, why does Y so often try to bury the problem and attack the messenger?  Y should be mad at X; after all, it’s X who has leveraged the reputation of Y for his personal gain.  I’d think that the leaders of Y would be <em>really</em> angry at X, even angrier than people from the outside.  But it doesn’t happen that way.  The immediate victims of the con would rather act as if the con never happened.  Instead, they’re mad at the outsiders who showed them that they were being fooled.  I’m sure that <a href="https://statmodeling.stat.columbia.edu/2023/07/07/cheating-in-science-sports-journalism-business-and-art-how-do-they-differ/">Dan Davies would have something to say</a> about all this.</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NHS to investigate Palantir influencer campaign as possible contract breach (205 pts)]]></title>
            <link>https://goodlawproject.org/nhs-to-investigate-palantir-influencer-campaign-as-possible-contract-breach/</link>
            <guid>38902983</guid>
            <pubDate>Sun, 07 Jan 2024 17:12:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://goodlawproject.org/nhs-to-investigate-palantir-influencer-campaign-as-possible-contract-breach/">https://goodlawproject.org/nhs-to-investigate-palantir-influencer-campaign-as-possible-contract-breach/</a>, See on <a href="https://news.ycombinator.com/item?id=38902983">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">


		
			<div>
						<p><span>News</span></p>
												<p><span>6th January 2024</span></p><p><span>NHS England has confirmed that it will investigate whether Palantir violated the terms of its contract to run the Federated Data Platform, after the tech giant covertly launched an influencer campaign which targeted Good Law Project.</span></p>
					</div>

			<div>
						      
<p><span>We can reveal </span><span>that Palantir was required to but failed to seek prior approval from NHS England (NHSE) for its campaign to promote its contract to run the Federated Data Platform and brief against Good Law Project. Now NHSE has confirmed to <a href="https://www.bloomberg.com/news/articles/2024-01-05/nhs-probes-whether-palantir-influencer-campaign-breached-contract-terms?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTcwNDUyMzA4MSwiZXhwIjoxNzA1MTI3ODgxLCJhcnRpY2xlSWQiOiJTNlFYVVBUMVVNMFcwMCIsImJjb25uZWN0SWQiOiIzMkI3NjREMjRGNDQ0OTEyQjE0Mzc1OTA4ODY4N0FFNiJ9.EzD2aOa0WmaF2BaPJBRRmxt5p5vebXePUNh0ZRHNS-g" target="_blank" rel="noopener">Bloomberg UK</a> that it will be investigating whether Palantir violated the contract terms just weeks after signing it.</span><span><br>
</span></p>
<p><span>A leaked briefing and emails from the campaign shows that Palantir used Tory-linked PR agency, Topham Guerin and marketing agency, Disrupt, to <a href="https://goodlawproject.org/how-palantir-and-topham-guerins-plan-to-discredit-us-unravelled/" target="_blank" rel="noopener">approach social media influencers</a> to ask them what they would like to be paid to take part. This was done at arms-length, with the briefing asking influencers not to mention Palantir in their content.<br>
</span><span><br>
</span><span><img fetchpriority="high" decoding="async" src="https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755-1024x254.png" alt="Quoted text from the 'Things to avoid' section of the Topham Guerin Briefing. Quoted text starts: &quot;Things to avoid ● When the content is posted, please keep the brand confidential and not tag Palantir. ● If responding to comments, please refrain from mentioning the brand. Timings DEADLINE: 22nd December &quot; Quoted text ends." width="640" height="159" srcset="https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755-1024x254.png 1024w, https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755-300x74.png 300w, https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755-768x190.png 768w, https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755.png 1183w" sizes="(max-width: 640px) 100vw, 640px" data-srcset="https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755-1024x254.png 1024w, https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755-300x74.png 300w, https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755-768x190.png 768w, https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755.png 1183w" data-src="https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755-1024x254.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></span><span>The Federated Data Platform <a href="https://www.contractsfinder.service.gov.uk/notice/2e8c61c0-faab-4f99-ae69-b00df6bae165?origin=SearchResults&amp;p=1" target="_blank" rel="noopener">contract</a> was finally published on the last working day before Christmas.</span> <span>Page after page of this three-part contract has been redacted – including a section on the protection of personal data.</span><span><br>
</span><span><br>
</span><span>But one of the sections we can actually see – which covers ‘Publicity and Branding’ – states that Palantir is not permitted to use the Authority’s name or brand in any marketing or publicise the contract without the prior written consent of NHS England.&nbsp;</span></p>
<p><span>An NHS spokesperson told Bloomberg that the NHS takes any potential breach of contract by a supplier seriously and is investigating what happened.</span></p>
<p><img decoding="async" src="https://goodlawproject.org/wp-content/uploads/2024/01/FDPcontract01-Copy.png" alt="An excerpt from the Federated Data Contract signed between the NHS and Palantir which reads: &quot;24 PUBLICITY AND BRANDING 24.1 The Supplier shall not: (a) make any press announcements or publicise this Agreement or its contents in any way; or (b) use the Authority's or any Authority Service Recipients name or brand in any promotion or marketing or announcement of orders; without the prior written consent of the Authority or relevant Authority Service Recipient, which shall not be unreasonably withheld or delayed.&quot;" width="541" height="174" srcset="https://goodlawproject.org/wp-content/uploads/2024/01/FDPcontract01-Copy.png 990w, https://goodlawproject.org/wp-content/uploads/2024/01/FDPcontract01-Copy-300x96.png 300w, https://goodlawproject.org/wp-content/uploads/2024/01/FDPcontract01-Copy-768x247.png 768w" sizes="(max-width: 541px) 100vw, 541px" data-srcset="https://goodlawproject.org/wp-content/uploads/2024/01/FDPcontract01-Copy.png 990w, https://goodlawproject.org/wp-content/uploads/2024/01/FDPcontract01-Copy-300x96.png 300w, https://goodlawproject.org/wp-content/uploads/2024/01/FDPcontract01-Copy-768x247.png 768w" data-src="https://goodlawproject.org/wp-content/uploads/2024/01/FDPcontract01-Copy.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></p>
<p><span>“Palantir is not – and frankly never has been – a company that can be trusted with this nationally important contract with our NHS” says Good Law Project’s Executive Director, Jo Maugham.</span></p>
<p><span>“By its own behaviour it is telling us exactly that.”</span><span><br>
</span><span><br>
</span><span>“Within weeks, it commissioned a covert smear campaign against a prominent critic and appears to have broken the terms of that contract. If this Government won’t act to protect the national interest, the next one must.”</span></p>
<p><span>Responding to Bloomberg, Palantir has claimed that its campaign was an “exploratory project” so it did not need to consult NHSE. Palantir’s Executive Vice President for UK and Europe, Louis Mosely said, “We decided not to pursue the project — as such, the campaign was not discussed with NHS England”.</span></p>
<p><span>But it is clear that the campaign did seek to use the NHS name and brand in marketing and publicized its agreement with the NHS.</span></p>
<p><span>The campaign materials were dated 28 November 2023, gave as a deadline 22 December and stated “We are on a tight timeline for this one and would require a response as soon as possible with content going live before the new year”, so it is clear that there was no intention to seek the NHS England’s consent.</span></p>
<p><span>Palantir’s conduct in commissioning a covert, paid-for smear campaign against a prominent critic; its decision to breach its contract with the NHS within weeks of signing it; and subsequent dissembling about its intention to breach, will serve to substantiate the concerns of those who believe Palantir was an entity that ought never to have been given the contract in the first place.<br>
</span><span><br>
Good Law Project is attacked because of the work it does – and the threat it poses to the likes of Palantir and the Government. We can only do this because of donations from people like you, right across the UK. If you can, please </span><a href="https://goodlawproject.org/donate/"><span>support our work.</span></a><span><br>
</span></p>

    
  					</div>

		


	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Irish State announce plan to build a porn preference register for most of the EU (193 pts)]]></title>
            <link>https://www.thegist.ie/the-gist-wtf-commission/</link>
            <guid>38902407</guid>
            <pubDate>Sun, 07 Jan 2024 16:16:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thegist.ie/the-gist-wtf-commission/">https://www.thegist.ie/the-gist-wtf-commission/</a>, See on <a href="https://news.ycombinator.com/item?id=38902407">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <article>

        <header>

                <a href="https://www.thegist.ie/tag/ireland/">Ireland</a>
            
                <p>The Irish State announced its new plan to build a porn preference register for most of the EU, amongst other mad things. This is the Gist.</p>

            

                <figure>
        <img srcset="https://www.thegist.ie/content/images/size/w320/2024/01/photo-1599663252656-f7054ba72a44.jpeg 320w,
                    https://www.thegist.ie/content/images/size/w600/2024/01/photo-1599663252656-f7054ba72a44.jpeg 600w,
                    https://www.thegist.ie/content/images/size/w960/2024/01/photo-1599663252656-f7054ba72a44.jpeg 960w,
                    https://www.thegist.ie/content/images/size/w1200/2024/01/photo-1599663252656-f7054ba72a44.jpeg 1200w,
                    https://www.thegist.ie/content/images/size/w2000/2024/01/photo-1599663252656-f7054ba72a44.jpeg 2000w" src="https://www.thegist.ie/content/images/size/w1200/2024/01/photo-1599663252656-f7054ba72a44.jpeg" alt="The Gist: Coimisiún na Meán go off the rails">
            <figcaption><span>Photo by </span><a href="https://unsplash.com/@karsten116?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span>Karsten Winegeart</span></a><span> / </span><a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span>Unsplash</span></a></figcaption>
    </figure>

        </header>

        <section>
            <p>Let's start with where things stand right now and then go from there. </p><p>The first thing I should say: <strong>this is not a joke. </strong></p><p>I say that now, because later on, as you're reading the rest of this piece, you're going to keep asking yourself "Is this some kind of a joke?" </p><p>Hence, this helpful reference statement for you to return to repeatedly throughout. </p><p>This week the Executive Chairman of the new Irish regulator for internet content, Jeremy Godfrey, made himself available for comment to the press to discuss the <a href="https://www.cnam.ie/" rel="noreferrer">Coimisiún na Meán</a> (Media Commission) <a href="https://www.cnam.ie/coimisiun-na-mean-opens-public-consultation-on-irelands-first-online-safety-code/" rel="noreferrer">plan</a> to introduce a code of conduct to control how adults would access websites.</p><p>He outlined <a href="https://www.irishexaminer.com/news/arid-41300860.html" rel="noreferrer">the plan set out in page 17 of their Consultation and proposal document to the Irish Examiner.</a> </p><p>This body regulates content (and in particular video content) for the largest internet companies in the world due to the fact so many of them are based in Ireland.</p><p>Mr Godfrey suggested that his Commission would require adults and minors (children under 18) to send a copy of their passport to websites- including porn sites- and then, also, send them a live selfie so the porn sites could see what they looked like right now. And then the porn sites would run biometric data processing on those images (details unspecified) to confirm they were over 18. Call this the Nightclub Bouncer plan. </p><p>This is the national internet regulator proposing that it would require that everyone, adult and children alike, would upload their state ID and live selfies, to porn sites to have biometric processing of their facial images performed. Resulting, amongst other things, in an effective register of porn preferences for adults and a collection of selfies of children kept by the porn sites for six years (required to prove they have complied with the regulation, you see). </p><p>I refer you now to the top of the newsletter. I am still not joking.  </p><div><p>💡</p><p>Hey! Good news! This crackpot scheme isn't law yet. And you can do something to stop it. The Commission has extended its time for feedback on this plan to the 31st January 2024. You can just email what you think of it, giving as many reasons why you think it is a bad idea to <a href="mailto:VSPSregulation@cnam.ie" target="_blank" rel="noreferrer noopener">VSPSregulation@cnam.ie</a> Please do drop them a line, and tell your friends too.</p></div><div><p>But wait! That's not all! The CnaM Executive Chairman wanted to talk about porn sites because that's the least popular class of entities covered by this regulation. But the age-verification requirement actually can cover any <a href="https://www.cnam.ie/designation-notices/" rel="noreferrer">video-sharing platform</a> under the jurisdiction of the Irish State (link to the designation notice under section 139E and section 139G of the Broadcasting 2009 Act). That's a list that includes Facebook, WhatsApp, XTwitter and YouTube, just to pick four household names (because of <a href="https://www.irishstatutebook.ie/eli/2022/act/41/section/5/enacted/en/html?q=video-sharing+platform&amp;search_type=all" rel="noreferrer">Section 5 of the Online Safety and Media Regulation Act 2022</a>). It might also mean homegrown platforms such as <a href="https://mastodon.ie/" rel="noreferrer">Mastodon.ie</a>, the most prominent Irish part of the <a href="https://www.theverge.com/23990974/social-media-2023-fediverse-mastodon-threads-activitypub" rel="noreferrer">Fediverse,</a> who also allow videos to be shared. </p><p>In fact, Section 2 of the Broadcasting Act 2009 (as amended) casts the net wide enough to cover almost anywhere that lets you post a video. </p></div><blockquote>In this Act, ‘video-sharing platform service’ means, subject to subsection (3), a service, within the meaning of Articles 56 and 57 of the Treaty on the Functioning of the European Union, where—<br>(a) the principal purpose of the service is devoted to,<br>(b) the principal purpose of a dissociable section of the service is devoted to, or<br>(c) an essential functionality of the service is devoted to,<br>providing audiovisual programmes or user-generated videos, or both, by electronic communications networks, to the general public, in order to inform, entertain or educate.</blockquote><p>Also, these restrictions won't just limit and record access to porn sites. They can be applied to any sites which contains material the Commission decides may be legal, but on the other hand, oughtn't be seen by children. In other countries, this has been the kind of legal provision which has seen libraries restricting access to <a href="https://www.nytimes.com/2023/09/21/books/book-ban-rise-libraries.html" rel="noreferrer">books relating to LGBTQ+ themes</a>, racial justice themes and anything else you could imagine the Burke family objecting to. </p><p>The exact description of what is to be restricted behind the Commission's Nightclub Bouncer plan is simply 'age-inappropriate content', defined broadly in <a href="https://www.irishstatutebook.ie/eli/2022/act/41/section/45/enacted/en/html#sec45" rel="noreferrer">Section 45 of the Online Safety Act 2023</a> as "online content that is likely to be unsuitable for children (either generally or below a particular age), having regard to their capabilities, their development, and their rights and interests". It goes on to give examples of pornography and acts of violence, but the controls aren't limited to them. </p><p>This plan goes far beyond the requirement of the EU Audio-visual directive, which the EU Commission suggests "could be done by the use of PIN codes". It therefore must be measured against the twin EU law requirements of "necessity and proportionality" under the Charter of Fundamental Rights and the GDPR. It fails both tests, of course. </p><p>If there is an alternative method of meeting the requirement of age restriction (as the European Commission's suggestion of the use of PIN numbers demonstrates) then it fails the test of necessity and cannot be in compliance with EU law. </p><p>And even if there were no other method, it has to be considered whether creating a distributed database of internet use, including porn preferences, for all EU adults is proportionate to the aim being pursued. Then consider it also requires the additional security risk of sending copies of sensitive personal documents such as passports to platforms such as Elon Musk's X AND also requiring they perform biometric processing of <a href="https://gdpr-info.eu/art-9-gdpr/" rel="noreferrer">article 9 GDPR</a> facial data of both adults and minors. I think the question of proportionality answers itself by the time you reach the end of that sentence. </p><p>It is at this point that I feel justified in pointing to my <a href="https://www.rte.ie/radio/radio1/clips/21574302/" rel="noreferrer">interview on Morning Ireland</a> from 2019, where I warned that the Broadcasting Authority of Ireland was the <a href="https://www.rte.ie/news/ireland/2019/0624/1057148-online-safety/" rel="noreferrer">wrong body</a> to put in charge regulating the internet, because it had neither the experience nor understanding of the medium necessary. </p><p>In creating Coimisiún na Meán the Government did just that. The fresh new regulator is now demonstrating exactly how unfit for this role it is. </p><p>I take it back. In a way, this is a joke. It just isn't funny.</p><hr><p>Hello, dear reader. Many thanks for your interest in our first Gist of 2024. A reminder that you can <a href="https://www.thegist.ie/#/portal/signup/free" rel="noreferrer">subscribe for free</a> or, if you would like to support the production of more Gists into the future on a self-hosted <a href="https://www.nytimes.com/2023/12/22/business/substack-nazis-content-moderation.html" rel="noreferrer">non-nazi </a> system you can opt to contribute (from €2 a month) at <a href="https://www.thegist.ie/#/portal/signup" rel="noreferrer">this link</a>. But please know, I will always love you equally, regardless of which subscription option that's right for you. </p>
        </section>

    </article>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Behind the scenes: the struggle for each paper (2021) (110 pts)]]></title>
            <link>https://jeffhuang.com/struggle_for_each_paper/</link>
            <guid>38902258</guid>
            <pubDate>Sun, 07 Jan 2024 15:58:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jeffhuang.com/struggle_for_each_paper/">https://jeffhuang.com/struggle_for_each_paper/</a>, See on <a href="https://news.ycombinator.com/item?id=38902258">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
	The start of my sabbatical has given me a moment to reflect on my publications. But <a href="https://jeffhuang.com/CurriculumVitae.pdf">my CV</a> only shows a list of neatly cataloged papers: title, authors, conference. Each one appearing no different from another. But how each paper ended up published is its own story, a story about people and opportunity.
</p><p>
	Using notes from <a href="https://jeffhuang.com/productivity_text_file/">my research journal</a> and conference records, I reassembled the "Behind the scenes" story for each of my full research papers: 15 papers as a student, and 15 papers after becoming a professor (excluding papers not led by my research group, as I feel it's not my place to tell those stories). This is as much a <a href="#introspection">reflective exercise for myself</a> than it is for an audience.
</p><p>
	If you read this page in its entirety, it will take about 30 minutes. But you can skip any story, and it should still make sense. The first half are my <a href="#student_papers">student stories</a>, and the second half are my <a href="#professor_papers">professor stories</a>, so you can even just read the half that's more interesting to you. This would probably be more enticing as a series of Tweets or a Substack newsletter, but I'd rather post it all at once.
</p><p>
	I'd love to read the backstories of other peoples' publications too. So if you feel comfortable sharing, post yours and email me if you want it linked at the bottom of this page so we can start a collection. Anyways, here goes.
</p><div><li>
I first stumbled upon research in September 2003 while I was a second-year undergraduate student. My inbox popped up the message, <em>"call me or track me down in class, 'cause a full explanation will take a lot of typing. -Bo"</em>.<p>
Bo was a friend who needed some programming help for an automated help generator as part of a research project, and off I went. What I thought would be a one week task turned into three rewrites of an application, two studies, and one long faculty mentorship. Neither of us could predict that Bo would graduate before the main study began, so he did not even become a co-author of a project he started (though in hindsight, I could have just put him as second author).</p><p>
Because I was learning as I went, I made all the novice mistakes from study design to paper structure. It was a long 3.5 years between when I got that first message and when it was published, after my graduation. And the acceptance decision came only after eeking out a lucky coin-flip, as the initial metareview score was "3-Borderline".</p><p>
But this paper was critical to getting me accepted to a Ph.D. program. Why do I think that? Well I was rejected by every Ph.D. program I applied to before this publication (but that's another story). So with this paper published, I left my job, squeezed my belongings into my car, and drove up the I-5 highway from California to Seattle.
</p></li><hr><li>
My next paper was sadly my first and only full paper with my original advisor, <a href="https://efthi.ischool.uw.edu/">Efthi, before he passed</a>. I don't remember coming up with this idea to study query reformulations, so it must have been he who led me to it. It was intended to be something easy but original—a starter project analyzing the sequences in existing data released by AOL.<p>
It was rejected from SIGIR on my first try, and I was starting to worry the topic would become stale, especially with the controversy about the original dataset that led to AOL closing their research department. So I was relieved this wrapped up quickly, and it felt good to have a paper under my belt in my first year.</p><p>
It puzzles me that this is my most cited full paper, but I think it's due to its topic rather than because of its contribution; though when I checked recently, a few citations are from people using the source code even 12 years later. I guess there's something to be said about the compound interest for citations.
</p></li><hr><li>
I met Ryen from Microsoft Research when he was a visiting speaker in Efthi's class, and he must have found my internship application that mentioned my paper about query reformulations. Well it turned out he was interested in search trails, which are essentially a series of reformulations, so I was able to continue along that line of work.<p>
The gold standard for these internships was to do a paper from start to finish in the twelve weeks, and I was desperately trying to do paper-worthy work to prove myself. I carried out the analysis as best I could, however I struggled to do enough for a full paper by the end of the internship. Fortunately, Ryen finished up and expanded on it substantially after I left, so I'm grateful he didn't just give it up as unfinished.</p><p>
To my complete surprise, this paper won the best paper award that year at SIGIR. Even after being nominated for the award, I felt it was so unlikely to win that I didn't attend the conference. In fact, at that time I thought my other related paper at that conference (the one below) was a better paper overall, but now I see that the best paper award committee probably felt that evaluation was a messier topic so wanted to reward that effort. This award boosted my confidence during my Ph.D. and opened some doors later, so I'm both lucky and thankful it happened.
</p></li><hr><li>
This related paper came out of the same summer internship with Ryen. I was lucky to be included in the author list because I had a small role and never even met the first author. Besides my initial work over the summer that was moreso for the paper above this, I contributed only a paragraph of original writing. After reading this paper, I felt like it was better than the one I had contributed more, but it has received fewer than half the citations.
</li><hr><li>
I met a brilliant intern during that same summer, Anna, who was doing her <em>first of two</em> Math PhDs at that time and often seemed idle. She explained, "the people who are always busy never seem to get much done." So sensing her free time, I asked her to help solve a made-up hypothetical problem that simplified assumptions from the information retrieval community.<p>
We had a good time working out the math, mostly me asking questions while watching her think aloud on the whiteboards. I learned about different strategies for deriving proofs in a real setting, which were unlike problem sets, where you knew a proof existed and were roughly the right difficulty for you to do. During this summer, I was mathematically in my best shape, as I could follow along enough to write out the solution and check for errors, whereas today it would take a while to even familiarize myself with the equations in the paper.
</p></li><hr><li>
After the summer, I was looking for new ideas to pursue outside my usual information retrieval topics. A friend in the Ph.D. program Gifford came to me with the suggestion that watching people playing video games was an overlooked phenomenon. But no one was paying attention. At that time, you could really only watch low-resolution videos of Korean players with amateur commentary dubbed in English.<p>
That idea clicked with me, so I helped out with a qualitative study to understand why. Gifford taught me most of what I know about grounded theory (and gave me a second lesson a few years ago while collaborating on a more recent 2018 paper).</p><p>
At that time, I had in my mind that I was helping him rescue a rejected idea, but now in retrospect, it's clear that it was he who gave me the opportunity to help out on work he already had a vision for. I'm quite proud of this work, which is my third most-cited paper, but mainly because what we thought would become a phenomenon did indeed come to pass.
</p></li><hr><li>
This became the most important paper during my Ph.D., and it was minutes from not happening. The idea was one of three that I tossed around during my interview to be a Research Intern at Bing. But to apply this idea to the search engine required a complex series of software integration steps that I was not too familiar with; yet it had to be committed by deadlines that occurred every two weeks.<p>
I aimed for a commit deadline during the second half of my internship, and the day it had to ship by 5pm, my code simply wouldn't pass the unit tests. I tried to force it by (naively) changing the unit tests, but that just broke other parts of the build process. It was 4pm and I even had to be somewhere else across the bridge in 30 minutes; I was desperate. I ran in the hallways panicking and found a software developer, Sarvesh, who took a look and gave me some tips. But by the time I had to leave, I still could not push the code. Sarvesh again came to my rescue and assured me, "you take off, I've got this" and sat down at my desk to fix the problem and ship my code while I drove out of the parking lot. Without his help, I would have had to ship it during the next cycle, and would not have left enough time to analyze the data before my internship was over. Not only that, but because it was during a corporate internship, I would have no rights to the intellectual property of any of the work.</p><p>
But my 20 lines of JavaScript did ship and luckily my was bug-free (after I pored over every line a hundred times), so this paper became the first paper I published at the main conference in my field; it was nominated for a best paper award, and became the foundational chapter in my dissertation. Not only that, but it was this work that led to a Google Research Grant, Facebook Ph.D. Fellowship, and Microsoft patent.
</p></li><hr><li>
Back at school, and with momentum from our previous paper about gaming, Gifford and I looked at one of his rejected papers about Texas Hold'em, and expanded it with data from another game, Halo 2. We repeated our formula of qualitative analysis, and submitted to CSCW. It received borderline ratings from our reviewers, but CSCW was experimenting with a "revise and resubmit" process that we were lucky to go through.
</li><hr><li>
At this point, one of my friends from the summer internship, Abdi, felt like I had "the magic touch" so I offered to help out with a rejected internship paper. I only helped brainstorm and edit, but it did end up getting accepted without much fanfare. While this one worked out, a year later I tried to rescue another paper with him, but it only ended up in the bin. Same effort, opposite outcomes.
</li><hr><li>
Now it was the end of my third year, and I felt guilty that I only had one dissertation chapter ready. I had co-founded a startup as part of Techstars Seattle while teaching a class in the evening, so those things had occupied all of my time. Our startup eventually ran out of money and my co-founders left for other opportunities, so in the summer I returned to Microsoft Research for my third internship. Again I joined a different group, this one managed by Sue who a year later became my Ph.D. advisor.<p>
Her group was probably the closest to my core research interests, and it just happened that Ryen had moved to this group. So with this combination of good circumstances, I aimed to write three papers with Ryen and others to get enough material for my dissertation.</p><p>
This first one was difficult for me, using a technique I was unfamiliar with, and required a lot of compute. My compute jobs would compete with higher priority jobs during the day so often timed out. So I had to stay many late evenings to kick off the 8-hour distributed processes that could only finish at night when the cluster was not in heavy use. My efforts paid off; reviewers were generally favorable and it was a clean accept.</p><p>
Now I have to confess, this is the paper I am most skeptical about. I don't fully trust the model, even though I kept checking it over. The results showed a modest improvement, but I can't seem to shake the feeling that they were due to a secondary factor like a collinearity, or even worse that there was a calculation error somewhere in there. But now my code is probably gone, and it seems like others were able to show practical improvements using similar models.
</p></li><hr><li>
A Bing employee, Georg, had collected a nice dataset from a study for one purpose, but it seemed like it could be used to study something closer to what I had done before. He graciously lent me the data, and I did enough analysis to produce this paper. However, one reviewer felt that the descriptive results were not that novel (just a bigger study), nor were the predictive analysis that successful, summarizing "with their paper they now try to add more to this field, but I don't see the important contribution that would justify the publication at CHI." The opinions were ultimately divided.<p>
I sweated over the rebuttal and promised changes, and luckily convinced the metareviewers to let this one slip through. But it actually turned out to be a fairly influential paper, with 223 citations as of today, and served as a baseline for some of my students' work later. So this was paper number two from that summer. I think it hit a trend of papers about user attention that came out the next few years, but this trend has dwindled since then.
</p></li><hr><li>
The third paper of that summer was an easier analysis with a novel idea—that sophisticated use of browser tabs was a central part to finding information. I recruited another student from my university, Tom, who happened to be there interning the same summer. The metareviewer remarked quite correctly, "The paper is not rocket science but [...] to my knowledge has not really been looked at, at least not at this large scale."<p>
While browser tabs did continue to be a phenomenon, this paper got fewer citations than a lighter short paper I wrote earlier on this topic. I think partly because the title was too clever to be easily recognizable.
</p></li><hr><li>
As a bonus for the summer, Georg was writing his own paper, and I had a minor contribution that he deemed enough for a co-authorship. It got a decent number of citations and filled a nice gap in the research field, as well as in my dissertation. So it became a lucky 4-paper summer. I was exhausted by the end of it and took a nice long break.
</li><hr><li>
Besides the two papers with Gifford about gaming, I wasn't doing any research during the school year, as all my other papers were from internships. Without an advisor, I served as a teaching assistant every quarter (sometimes for two or even three classes at once), and got a bit distracted with some other activities.<p>
But one day my opportunity to be a TA vanished (itself another story), and I begged over to Oren, a professor from the computer science department for an office and funding. To my surprise, he immediately agreed within hours of my email. So I started to learn about natural language processing and got to see how he ran his lab.</p><p>
This resulting paper was a combination of his interests and mine. A couple of undergraduate students joined in under my supervision, which was also my first time mentoring students. The study almost did not happen, because I had trouble fitting the procedure under the rules of our human subjects guidance. But after last-minute discussions with Oren and an HCI professor, we found a way to thread the needle.</p><p>
Its publication helped launch one of the undergraduate students into the Ph.D. program at MIT, and led to my interest in involving undergraduate students in research for many years to come.
</p></li><hr><li>
I wasn't originally planning to do another research internship at Microsoft in my last summer, but Tom and Nachi reached out to me about an opportunity I couldn't say no to—a summer to study gaming with large-scale Xbox data. So in my last summer, I joined their gaming research initiative. It was a collaboration between Microsoft Research and folks from Xbox, and along the way I learned a few techniques for time-series analysis.<p>I barely finished the final analysis in the paper the day that I had my farewell lunch. Reviewers loved the work more than I expected, and this paper led to a few opportunities later so I'm glad it worked out. It also brought closure to my Ph.D., as I ended up with no leftover working papers in the pipeline, hence the 2-year gap until my next paper.
</p></li>
<hr id="professor_papers">
<h3>Part 2: Papers as a faculty author</h3>
<hr><li>
Up to this point, I was feeling comfortable leading a paper from start to finish, but the job changed when I became faculty. While I could come up with the idea and advise the process, the initial drafts would be written by students.<p>
So fast forward past my move to Providence and a few false starts at unfinished projects. My first published paper as a faculty member came from a student referred by a professor at another university. The student was Eddie, an undergraduate student from UCLA who reached out to the professor about conducting research analyzing patterns in StarCraft replays. That professor thought I fit the topic better but cautioned, "he [I] probably doesn't have the bandwidth to supervise external students at this time". While that would probably be true now, back then I took the chance and steered him towards an adjacent investigation.</p><p>
I invited Gifford (yes, my classmate from before) to help out, and the work from start to finish was about 8 months of intense analysis and figure-making. The paper ended up with two strong ratings (4.5/5, 5/5) and two unenthusiastic ratings (2.5/5, 3/5), so the compromise was that it ended up shepherded (a paper deemed borderline but asked to make specific changes to be acceptable) to guide us to "accept". This made us nervous for longer, but after this paper got in, I wrote to Eddie, "You've earned your golden ticket to grad school :-) congrats!" and he chose to do a Ph.D. at the University of Washington, my own alma mater. 
</p></li><hr><li>
While teaching my graduate seminar, I was overwhelmed by the enthusiasm of the students so I started assigning projects that students could recycle into research. This became the first of a few papers that were born from class projects in my HCI seminars. Each student worked on their own mini-study, and we combined it into a meta analysis, which is a formula that worked for a couple more papers in later years as well.<p>
The timing for this particular paper was a bit lucky because the reviewers nominated it for a best paper award, but our follow-up work was not as successful; we still had more to say on this topic, but met a lot of resistance in writing the sequels after years of trying to publish newer findings with only rejections.
</p></li><hr><li>
I led my first Ph.D. student Alexandra in a few unfruitful directions trying to continue ideas from my Ph.D. that ended up with two unpublished papers worth of work (so I'm grateful for her patience). But one day while laying in bed I realized we could flip the story from estimating attention with the cursor, to using the webcam while the cursor auto-calibrates the webcam model during regular web browsing. We could deploy this as a library, basically shipping a product.<p>
This paper was the first of many product-style papers that have become the norm in our research group. The work was initially rejected at multiple conferences because while the overall system was effective and the functionality was novel, the technique was not innovative and the results were numerically worse than some of our competitors. I was frustrated about being compared against competitors who only reported data from the users for whom they get good results from (even when they are upfront about omitting results from most of their users), while we were reporting full results from every user.</p><p>
Anyways, it took over two years to build and publish, but I'm proud that the system in our paper is <a href="https://github.com/brownhci/webgazer">used by a sizable community</a>. It has become <a href="https://www.jspsych.org/overview/eye-tracking/">part of a popular psychology library</a> used for many research studies, and adopted by a few startups including one which bought a non-exclusive commercial license. We knew this work would have impact later, as Alexandra sent me one of my favorite acceptance notifications, "It got in!!!! I am going back to sleep, I'll email the rest of the authors tomorrow! :D Very excited, Alexandra" This paper was the foundation for her dissertation, and we are both still working on the project now seven years since it began.
</p></li><hr><li>
I admitted a second student, Nedi, to our Ph.D. program who had prior work on sleep diary research during her undergrad. My brainstorming notes in the months before she arrived was, "we will use data-driven techniques across a large populations sleep data to make (personalized) prescriptive sleep recommendations". By coincidence, I met some new collaborators who had clinical research expertise for this, so Nedi and her team of research assistants set out with these collaborators to develop the software and the study.<p>
We had a tough start and ate a few rejections at both UIST and CHI before we published the paper at the following UIST, 2 years after the initial idea. Even then, the paper almost didn't happen because the reviewers were skeptical (borderline ratings) but Nedi wrote a convincing rebuttal, as a metareviewer summarized, "I re-read the paper in light of the rebuttal. The proposed changes [...] pushed me into the slightly positive end of the spectrum. The submission was discussed at length at the PC meeting and received additional input from another PC member who reviewed the submission at a prior venue. The overall feeling is that this isn't a perfect paper, but it is a difficult area in which to do research and we do learn something from the submission."</p><p>
Close call, but this became the foundation for the rest of Nedi's Ph.D. work. We were lucky to publish it sooner than later because I later found out there were other research groups working on similar ideas.
</p></li><hr><li>
This paper followed from Alexandra's previous one, so was a bit more straightforward as a follow-on application to our previous work. Reviewers like it, and it received an honorable mention. I wish I had more to say, but it was one of the rare times where the idea was universally agreeable and the results were as expected. Later I learned that during my tenure application, an external letter writer remarked that I didn't have many follow-up papers (sequels) at that time, in fact just this one.
</li><hr><li>
Work on this paper started mid-2015, led by Shaun, a Masters student who became a Ph.D. student later. It was a fairly complex product so it took the team substantial time to build it out, and the paper was not published until 2 years later. What's nice is the paper had some broad impacts: two of the undergraduate students working on it are in Ph.D. programs now, and there are still active users of the online web application 6 years since the work started.<p>This paper set the standard that we would try to include undergraduate students in every paper, and so far that still holds true—100% of the papers from our group have included undergraduate authors.
</p></li><hr><li>
We followed a similar formula as before, having students in the HCI seminar run mini-studies, which became a meta study for this paper. But things were not so easy this time around, as the first version of the paper with one cohort struggled to reveal enough compelling findings. So we had to develop a new procedure for students in the seminar in another year, and combined the results from both cohorts for the submission.<p>
We submitted to CHI 2017 and while two reviewers rated it highly (4.5/5 and 4/5), the third wrote a scathing review; the two metareviewers examined the paper closely, and ultimately decided to reject. It was a little frustrating to be so close, as this paper had the highest average rating of all the rejected papers that year. However, we revised it and ultimately published it at IMWUT the following year after a cycle of major revisions.
</p></li><hr><li>
This paper was an exhausting amount of work for Alexandra, collecting a high-quality dataset with the hopes to release it as a contribution. The setup, lengthy procedure, and large number of participants were meant to provide stronger validity for the general topic of eye tracking during interactions.<p>
However, even after the data collection, there were a few snags. We learned that video frames did not inherently have timestamps associated with them, and it was nearly impossible to retroactively infer them to millisecond-level accuracy. While the dataset itself still felt like a strong contribution in the end, it was harder for other researchers to apply immediately so hasn't been as broadly used as I hoped. I still feel like this paper is a bit underrated today, and someone could write one or two other papers from the dataset we collected.
</p></li><hr><li>
My most recent Ph.D. admit, Jing, came with a unique design and technical background, so started working on an ambitious virtual reality project. However, that idea had trouble producing consistent results in practice, but I noted in my research journal on October 2016 that the "motion movement physical device that Jing built that can be used for replay too". So we pivoted to building the product for a different use case which became this paper.<p>
The paper was accepted on its second submission and one of the rare times I've encountered an "accept" decision without having to do a major revisions beforehand. But as a product it has been disappointing; we attempted to deploy it to usability professionals, the target audience, but it turns out that very few people were willing or capable of 3D-printing their own components. We learned from this so the project has not ended here, and we are nearly finished with a sequel, 5 years after the initial idea to reach our original vision.
</p></li><hr><li>
After meeting with Eda, a Masters student who wanted to work on a research project with me, the first note in my research journal was "discussed rewind: cool idea but low chance of publishing". I had no idea how true that would be, as this became the most challenging paper my group would publish. What started in January 2015 was published December 2018, 4 years later, and was passed from student to student after each one graduated. I lost count of the number of rejections.<p>
Many of the authors had never met each other, and it was a bittersweet moment for me when by pure coincidence, the original author, Eda, was standing in the hallway outside my office with the final author, Neilly, without knowing one other (which I immediately corrected by introducing them).</p><p>
What was challenging about this paper was the engineering work used existing known techniques, so we had to emphasize how the experience was a contribution on its own. This was hard to do in a study, as it wasn't about directly improving any specific aspect of life, but being able to experience it differently. I begged my old colleague Gifford to help in early 2018, and what put it over the finish line was a careful mixed methods descriptive writing based on the detailed analysis Gifford directed.
</p></li><hr><li>
This was the first time I had been involved in a project with both hardware and software components, so the system itself took longer than expected, and we were writing this paper down to the deadline. The paper had to be carefully crafted to describe a complex configuration, with 3D-printed parts, augmented reality, cameras and sensors, computer vision, heating and energy issues, and both wired and wireless networking.<p>
Reviews were mixed, but we thankfully had support from our metareviewer, "I am looking forward to hopefully a strong rebuttal so I can be your advocate at the UIST 2019 PC meeting." This encouragement was exactly what we needed in that moment.</p><p>
The effort was worth it, because this system led to a few other projects, and serves as a foundational paper for Jing's Ph.D. What we are still trying to figure out today is how to deploy this as a product to regular people, as the hardware requirements again posed a barrier for adoption.
</p></li><hr><li>
Nedi turned her earlier SleepCoacher paper into a fully automated process, completing our vision from the seed of an idea 6 years ago. The product described in this paper was shipped to the app stores and used by whoever would come across and download it, basically real usage by people we did not recruit. We maintained Android and iOS apps separately, and a server to do the calculations. It was a costly mistake to build out three separate systems; we should have started with something cross-platform and performed the calculations in the app itself to reduce the software maintenance from three systems to one.<p>
The paper was hard to publish, because unlike recruited and paid participants, our 5,000 app store installs (now 7,700) led to messy data—a lot of people never opened the app, or did so only once. Reviewers were unimpressed that thousands of installs only led to a couple hundred active participants, of which only about a fifth of the users tracked for enough nights to get useful information.</p><p>
In the end, it was a close decision but the CHI program committee decided that it could be acceptable if shepherded, "I am still leaning positive given the difficulty of the method, the importance of the topic, and complexity of the project as a whole." Being on the program committee myself that year, I wondered to another faculty member why our papers always seem to only barely get in, and they responded matter-of-factly, "all accepted papers barely get in," referring to the declining average scores at CHI over the years.
</p></li><hr><li>
This idea grew out of my NSF CAREER Award as the required educational component, where I proposed a classroom tool for large-class simultaneous design activities. But the code was written and rewritten several times by different teams even after that, because running a real-time collaborative system with over 100 simultaneous users introduced its own share of problems.<p>
There were many nervous moments leading up to each attempt. The tool failed the first semester or two that we tried it; the server would crash or some of the data would be lost or corrupted, and we would lose our chance to get data that semester.</p><p>
Submission-wise, reviewers always wanted more, so the paper itself went through multiple different narratives before being accepted in a tough revise and resubmit cycle. The mood around this revise and resubmit is best portrayed by a reviewer, "I find this to be a mostly solidly executed project, but I don't see a substantial contribution to CSCW / creativity support tools here. I am not sure if this can be fixed in a revision cycle, but if the authors are keen, ..."</p><p>
Well, we were keen.
</p></li><hr><li>
In 2014, I met with a faculty member in Psychiatry and Human Behavior to discuss a collaboration, where we concluded that the area of mental health lacked innovative computational techniques. I did a rough prototype of our initial idea in 2015, then students picked it up the following year. The work was passed between teams of students, mainly supporting the application development and real usage from a growing list of collaborators. It turns out that many clinical researchers felt the same, and this became our most funded project.<p>
However, the papers took a while and the one led by my group was rejected throughout 2018 and 2019. The hard part was because while we were using a fairly unique approach, I didn't have much experience writing about this topic. Reviewers would have varying opinions of what needed to change, so the text would waffle back and forth; we finally arrived at a version of the paper that was satisfying enough, and it was accepted for publication in 2020.</p><p>
About 20 people were involved at different points in time, but the study didn't start until 2017 so the paper itself was about 3 years in preparation. I feel like the overall goal of computational interventions for mental health is a good one, but it feels like we've only taken a small step.
</p><p><a href="https://jeffhuang.com/papers/Sochiatrist_CSCW20.pdf">Sochiatrist: Signals of Affect in Messaging Data</a>.
Talie Massachi, Grant Fong, Varun Mathur, Sachin Pendse, Gabriela Hoefer, Jessica Fu, Chong Wang, Nikita Ramoji, Nicole Nugent, Megan Ranney, Daniel Dickstein, Michael Armey, Ellie Pavlick, Jeff Huang.
CSCW 2020.
</p></li><hr><li>
We had been wanting to expand Nedi's SleepBandits app beyond sleep to all sorts of self-experimentation, and finally got a chance when NSF extended one of my expiring grants to fund this paper.<p>
Our first attempt to publish at CHI 2020 was rejected partly due to weak findings, but Nedi had already been preparing a second study to complement what we had. While it still required major edits, CHI 2021 accepted the paper after a straightforward rebuttal. Did I finally end a long streak of struggling to publish? We considered this our easiest paper, but it still took an 8-person team nearly 30 months.</p><p>
But maybe publishing faster or publishing more is not what it's about. I care more about this project as an app that people can use, so I rebuilt it in the cross-platform Flutter framework, with hopes to use it as a foundation for later work. Hopefully the papers are just a milestone towards people making their lives better through self-experimentation.
</p></li></div><p>After reviewing these notes, I'm a bit ambivalent. When I was a clueless student, I got lucky with my collaborators and acceptance decisions, which made all the difference. After becoming a professor, I had the experience yet the papers take even longer to publish.</p><p>Part of <em>why</em> is the focus on systems papers, which are <a href="http://dubfuture.blogspot.com/2009/11/i-give-up-on-chiuist.html">known to take 3-4 years</a>, but shipping them as products has been an even longer 4+ year agenda. Worth it, sure, but we'll never be like most groups that publish 5+ papers a year.</p><p>But I also think about how my group has encountered a lot of rejection, and keeping up morale was sometimes difficult. Bad news injects doubt and discouragement into students' minds, who then have to rally the team to continue the work in hope that acceptance is just around the corner. This dissonance is hard to manage.</p><p>The other thing I noticed is that papers that get the most citations later often got poor reviews or multiple rejections. They're usually about a new phenomenon, but the novelty can always be recast as "old thing, but just on the web" or "mostly engineering work, with so-so results". With this in mind, I should probably be generous in my own interpretation of what's novel when I review papers.</p><p>In retrospect, I am grateful for many key collaborators for their extra help in those times, and that some conferences like UIST accepted papers despite some obvious flaw because those papers ended up defining long-term research programs for multiple young researchers.</p><p>Thanks to Alexandra Papoutsaki, Bo Lu, Gifford Cheung, Tongyu Zhou, and Zainab Iftikhar for their comments on earlier drafts.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made an app that consolidated 18 apps (doc, sheet, form, site, chat…) (448 pts)]]></title>
            <link>https://nino.app</link>
            <guid>38901504</guid>
            <pubDate>Sun, 07 Jan 2024 14:34:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nino.app">https://nino.app</a>, See on <a href="https://news.ycombinator.com/item?id=38901504">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[First new vax in 30 years? (2021) (177 pts)]]></title>
            <link>https://mail-index.netbsd.org/port-vax/2021/07/03/msg003899.html</link>
            <guid>38901012</guid>
            <pubDate>Sun, 07 Jan 2024 13:27:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mail-index.netbsd.org/port-vax/2021/07/03/msg003899.html">https://mail-index.netbsd.org/port-vax/2021/07/03/msg003899.html</a>, See on <a href="https://news.ycombinator.com/item?id=38901012">Hacker News</a></p>
<div id="readability-page-1" class="page">
<!--X-Body-Begin-->
<!--X-User-Header-->
<address>
Port-vax archive
</address>
<!--X-User-Header-End-->
<!--X-TopPNI-->
<hr>
[<a href="https://mail-index.netbsd.org/port-vax/2021/05/03/msg003898.html">Date Prev</a>][<a href="https://mail-index.netbsd.org/port-vax/2021/07/03/msg003900.html">Date Next</a>][<a href="https://mail-index.netbsd.org/port-vax/2021/04/17/msg003867.html">Thread Prev</a>][<a href="https://mail-index.netbsd.org/port-vax/2021/07/03/msg003906.html">Thread Next</a>][<a href="https://mail-index.netbsd.org/port-vax/2021/07/date1.html#003899">Date Index</a>][<a href="https://mail-index.netbsd.org/port-vax/2021/07/thread1.html#003899">Thread Index</a>][<a href="https://mail-index.netbsd.org/port-vax/2021/07/oindex.html">Old Index</a>]

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->

<hr>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul>
<li><strong>To</strong>: <strong>port-vax List &lt;<a href="mailto:port-vax%NetBSD.org@localhost">port-vax%NetBSD.org@localhost</a>&gt;</strong></li>
<li><strong>Subject</strong>: <strong>First new vax in ...30 years? :-)</strong></li>
<li><strong>From</strong>: <strong>Anders Magnusson &lt;<a href="mailto:ragge%tethuvudet.se@localhost">ragge%tethuvudet.se@localhost</a>&gt;</strong></li>
<li>Date: Sat, 3 Jul 2021 12:14:02 +0200</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
<hr>
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<pre>Hi,

</pre><tt>some time ago I ended up in an architectural discussion (risc vs cisc 
</tt><tt>etc...) and started to think about vax.
</tt><tt>Even though the vax is considered the "ultimate cisc" I wondered if its 
</tt><tt>cleanliness and nice instruction set still could be implemented 
</tt><tt>efficient enough.
</tt><tt>Well, the only way to know would be to try to implement it :-)&nbsp; I had an 
</tt><tt>15-year-old demo board with a small low-end FPGA (Xilinx XC3S400), so I 
</tt><tt>just had to learn Verilog and try to implement something.&nbsp; And it just 
</tt><tt>passed EVKAA.EXE:
</tt><pre>
&gt;FR00000000 200
&gt;G
EVKAA V10.4 Hardcore Instruction Test

Hit any key to continue
EVKAA&nbsp;&nbsp; V10.4 pass # 1(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # 19(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # 32(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # 4B(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # 64(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # 7D(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # 96(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # AF(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # C8(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # E1(X) done!^C
Input breakpoint: 000000001
&gt;

</pre><tt>(the microcode console works like the Nova4 microcode console - simpler 
</tt><tt>to implement than the VAX style...)
</tt><tt>It runs at 50MHz, but could easily be increased to about 80, just to 
</tt><tt>program DCM.
</tt><pre>
Photo of the "vax": <a rel="nofollow" href="https://www.ludd.ltu.se/~ragge/pics/IMG_0837.jpg">https://www.ludd.ltu.se/~ragge/pics/IMG_0837.jpg</a>
</pre><tt>I had to get a new FPGA board, since I started to get bit errors on the 
</tt><tt>old one, so I bought a chinese board with essentially the same FPGA 
</tt><tt>(XC3S500E):
</tt><pre>
</pre><tt>I have implemented all addressing modes, the interrupt hierarchy, 
</tt><tt>timers, and 151 instructions.
</tt><pre>No memory management yet though, but that should be quite straight-forward.

</pre><tt>Currently it uses about 70% of available FPGA resources, which is around 
</tt><tt>6000 LUTs (which is quite inefficient implemented, since I have learned 
</tt><tt>how to do verilog programming while writing the code...)
</tt><pre>
</pre><tt>I'll follow up this mail with two more, one about the implementation and 
</tt><tt>one about how to go forward with the vax architecture :-)
</tt><pre>
-- R
</pre>
<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
<hr>
<ul><li><strong>Follow-Ups</strong>:
<ul>
<li><strong><a name="003992" href="https://mail-index.netbsd.org/port-vax/2021/07/07/msg003992.html">Re: First new vax in ...30 years? :-)</a></strong>
<ul><li><em>From:</em> Michael Parson</li></ul></li>
<li><strong><a name="003941" href="https://mail-index.netbsd.org/port-vax/2021/07/05/msg003941.html">RE: [EXTERNAL] First new vax in ...30 years? :-)</a></strong>
<ul><li><em>From:</em> Luke Brennan</li></ul></li>
<li><strong><a name="003912" href="https://mail-index.netbsd.org/port-vax/2021/07/03/msg003912.html">Re: First new vax in ...30 years? :-)</a></strong>
<ul><li><em>From:</em> Paul Koning</li></ul></li>
<li><strong><a name="003906" href="https://mail-index.netbsd.org/port-vax/2021/07/03/msg003906.html">Re: First new vax in ...30 years? :-)</a></strong>
<ul><li><em>From:</em> Dave McGuire</li></ul></li>
</ul></li></ul>
<!--X-Follow-Ups-End-->
<!--X-References-->
<!--X-References-End-->
<!--X-BotPNI-->
<hr>
<ul>
<li>Prev by Date:
<strong><a href="https://mail-index.netbsd.org/port-vax/2021/05/03/msg003898.html">Re: Bountysource campaign for gcc-vax</a></strong>
</li>
<li>Next by Date:
<strong><a href="https://mail-index.netbsd.org/port-vax/2021/07/03/msg003900.html">New vax - implementation :-)</a></strong>
</li>

<li>Previous by Thread:
<strong><a href="https://mail-index.netbsd.org/port-vax/2021/04/17/msg003867.html">HEADS UP: GCC 10 now default on several ports</a></strong>
</li>
<li>Next by Thread:
<strong><a href="https://mail-index.netbsd.org/port-vax/2021/07/03/msg003906.html">Re: First new vax in ...30 years? :-)</a></strong>
</li>

<li>Indexes:
<ul>
<li><a href="https://mail-index.netbsd.org/port-vax/2021/07/date1.html#003899">
<strong>reverse Date</strong></a></li>
<li><a href="https://mail-index.netbsd.org/port-vax/2021/07/thread1.html#003899">
<strong>reverse Thread</strong></a></li>
<li><a href="https://mail-index.netbsd.org/port-vax/2021/07/oindex.html">
<strong>Old Index</strong></a></li>
</ul>
</li>
</ul>

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<strong>
<a href="https://mail-index.netbsd.org/index.html">Home</a> |
<a href="https://mail-index.netbsd.org/port-vax/index.html">Main Index</a> |
<a href="https://mail-index.netbsd.org/port-vax/tindex.html">Thread Index</a> |
<a href="https://mail-index.netbsd.org/port-vax/oindex.html">Old Index</a>
</strong>
<!--X-User-Footer-End-->


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Things engineers believe about Web development (140 pts)]]></title>
            <link>https://birtles.blog/2024/01/06/weird-things-engineers-believe-about-development/</link>
            <guid>38900577</guid>
            <pubDate>Sun, 07 Jan 2024 12:11:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://birtles.blog/2024/01/06/weird-things-engineers-believe-about-development/">https://birtles.blog/2024/01/06/weird-things-engineers-believe-about-development/</a>, See on <a href="https://news.ycombinator.com/item?id=38900577">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>  <p><em>I wrote most of this post sometime in 2022 but I think it holds up alright
in 2024 so I decided to publish it for posterity.
I don’t really like doing posts like this—I’d much rather share some
innocuous learnings or tips but it turns out I have opinions too</em> 😓
<em>Sorry!</em></p>
<p>Since I quit Mozilla and went back to full-time Web development, I’ve discovered
a few surprises.
It turns out Web development is actually pretty hard, Web developers are
actually very smart, and some of these frameworks and techniques we mocked as
browser engineers aren’t so bad.
Oops.</p>
<p>At the same time, it turns out some Web developers have ideas about browsers and
the Web that, as a former browser engineer and standards editor, I’m a bit
dubious of.</p>
<p>Here are a few of the things that surprised me.</p>
<!-- end summary -->
<ol>
<li><a href="#web-browser-engineers-know-web-development-really-well">“Web browser engineers know Web development really well”</a></li>
<li><a href="#the-people-who-make-web-specifications-know-web-development-really-well">“The people who make Web specifications know Web development really well”</a></li>
<li><a href="#web-developers-know-web-development-really-well">“Web developers know Web development really well”</a></li>
<li><a href="#browsers-arent-made-to-run-spas">“Browsers aren’t made to run SPAs”</a></li>
<li><a href="#mpas-will-replace-spas">”MPAs will replace SPAs”</a></li>
<li><a href="#all-sites-should-work-without-javascript">“All sites should work without JavaScript”</a></li>
<li><a href="#web-development-shouldnt-need-a-build-step">“Web development shouldn’t need a build step”</a></li>
<li><a href="#my-blog-is-representative-of-web-development-at-large">“My blog is representative of Web development at large”</a></li>
</ol>
<h2 id="web-browser-engineers-know-web-development-really-well"><a href="#web-browser-engineers-know-web-development-really-well">“Web browser engineers know Web development really well”</a></h2>
<p>It’s easy to imagine that Web browser engineers, who write the code that makes
up the Web platform, must know the Web inside-out like no-one else.</p>
<p>The trouble is, writing Web browsers is hard.</p>
<p>Most browser engineers focus on a particular area and know it really well with
only a superficial understanding of other areas.
Furthermore, browser platform engineers are writing C++ and Rust code all
day long with a smattering of very simple JavaScript test cases.
On top of that, they contribute to one massive respository where someone else
takes care of all the tooling.</p>
<p>As a result, in their day jobs browser engineers are
not fighting webpack,
not trying to understand full-page TypeScript errors
(they’ve got C++ template errors to fill <em>that</em> hole in their lives),
not trying to get Safari on iOS to behave like other browsers,
not struggling with CSS at scale,
not trying to evaluate if the latest SSR/SSG/island framework is worth
investing in,
not refactoring massive JS codebases for more optimal chunking,
not filing exasperated issues on GitHub because the latest version of one of
their dependencies broke their app,
not trying to get all their tools to agree on ESM vs CJS,
and not losing sleep over whether or not they chose the right state
management approach or if they should rewrite the entire thing for the tenth
time over.</p>
<p>In short, they’re not doing Web development day in and day out so they know
much less about real-world Web development than Web developers probably
expect.</p>
<p>Now the engineers working on browser <em>developer tools</em> and the browser
<em>front-end</em> often <em>are</em> using JS day-to-day and certainly have more awareness of
the issues, but it’s still a few degrees removed from regular Web development.
For example, they only need to target a single browser engine’s platform
features, and often only a single browser version (being able to use the new
and shiny features without worrying about compatibility is <em>amazing</em>), don’t
need to worry about bundle size, or servers, or being offline, or so many
other issues that make Web development hard.</p>
<p>Obviously some browser engineers have hobby projects, but the constraints on
a hobby project are a world apart from being in a startup where you live or
die by the success of your Web app.</p>
<p>I started my career in graphic design and Web development and even after
I started contributing to Firefox I worked at a Web development company in
Osaka for a time and produced some Web apps at Mozilla Japan too.
However, after I quit Mozilla and got back into Web development full-time
I was shocked at how much it had changed and how little I knew about it.</p>
<p>My experience as a browser engineer has been incredibly useful in Web
development not least because I know where to look, who to ask, and where to file
bugs, but I’d be kidding myself if I said that being a browser engineer
automatically qualified me as a Web developer.</p>
<p>During my time at Mozilla, the Firefox OS days were by far the most enjoyable.
We had internal teams building real-world mobile Web apps that we,
as the platform team, had to debug and make successful as our highest priority.
I saw how <a href="https://developer.mozilla.org/docs/Web/API/Element/transitionend_event" rel="noopener nofollow"><code>transitionend</code>
events</a>
could be unreliable and cause Web apps to be buggy and overly complicated and so
I proposed and implemented the
<a href="https://developer.mozilla.org/docs/Web/API/Element/transitioncancel_event" rel="noopener nofollow"><code>transitioncancel</code>
event</a>
and Web Animations’ <a href="https://developer.mozilla.org/docs/Web/API/Animation/finished" rel="noopener nofollow"><code>Animation.finished</code>
promise</a>.</p>
<p>But even working side-by-side with Web developers could not prepare me for
actually <em>being</em> a full-time Web developer again.
For the most part browser engineers just operate in a different world from Web
developers and might not be the Web developer superheroes we imagine them to be.</p>
<h2 id="the-people-who-make-web-specifications-know-web-development-really-well"><a href="#the-people-who-make-web-specifications-know-web-development-really-well">“The people who make Web specifications know Web development really well”</a></h2>
<p>Ok, but surely the people working on Web <em>standards</em> and <em>specifications</em>
(who, it turns out, are mostly browser engineers) must be well-versed in Web
development, right?</p>
<p>Back in 2012, <a href="https://brendaneich.com/2012/02/community-prioritized-web-standards/" rel="noopener nofollow">Brendan Eich pointed
out</a>
that, “Standards-making like law-making is definitely
<a href="https://en.wikiquote.org/wiki/John_Godfrey_Saxe" rel="noopener nofollow">sausage-making</a>” referring
to the following quote from John Godfrey Saxe:</p>
<blockquote>
<p>Laws, like sausages, cease to inspire respect in proportion as we know how
they are made.</p>
</blockquote>
<p>As a Web developer, it’s easy to imagine a group of people, infinitely
wise about all things related to Web technology, making calm, rational
decisions based on the technical merits of each proposal balanced against
a thorough understanding of industry needs.</p>
<p>That illusion typically won’t last your first working group meeting.
Despite the best of intentions, sometimes decisions get made based, at least in
part,
on one person’s charisma or forceful personality,
on who happens to be in the room at the time,
on how tired everyone is,
or, dare I suggest, maybe even
someone’s need to ship a feature in order to fill out their promotion packet.</p>
<p>That sounds very cynical so let me make two clarifications.</p>
<p>Firstly, the folks on these groups are well-meaning, wonderful people.
Furthermore, they are often aware of their limitations and try their best to
elicit Web developer feedback.
Unfortunately, I’ve yet to see a group do this very successfully.
There are Twitter/X polls, for example, but they tend to only be answered by the
Web developers on the bleeding edge and are easily skewed by <em>who</em> spreads the
word about the poll.</p>
<p>Secondly, I haven’t had much experience with WHATWG specs like HTML and
DOM where decisions appear to be made asynchronously (“any decisions made
during in-person meetings are to be considered non-binding”—<a href="https://whatwg.org/working-mode#meetings" rel="noopener nofollow">WHATWG
meetings</a>),
but my impression is that they seem to make better decisions.
Folks like Anne van Kesteren, Simon Pieters, and Domenic Denicola probably
know the Web better than anyone else on the planet.
But even that is not the same as knowing Web development.</p>
<h2 id="web-developers-know-web-development-really-well"><a href="#web-developers-know-web-development-really-well">“Web developers know Web development really well”</a></h2>
<p>As a browser engineer it’s really satisfying to ship a new platform feature.
There are articles about it on Smashing Magazine and CSS tricks and Twitter/X
goes abuzz with the news.
It’s easy to think that now the whole world now knows about this great new
advance in the Web’s capabilities.</p>
<p>At one point a few years ago, a few of us in Mozilla’s Japan team decided to
interview local Web developers in Tokyo to learn what developer tools
they would benefit from.</p>
<p>The results were surprising.</p>
<p>Many didn’t know about new CSS features that had shipped 10 years ago.
What’s more, even when we told them about them, they didn’t seem too excited.
They were doing just fine with jQuery and WordPress, thank you.</p>
<p>Instead, they were having trouble with things like, “When I show clients
a site in responsive design mode, if the window doesn’t have a mockup of
an iPhone around it, clients can’t grasp that they are looking at a preview of
how the site will look on a smartphone. I really need that iPhone mockup.”</p>
<p>As a browser engineer involved in developing new Web standards I was a little
disappointed but my lasting impression was of the constraints on these folks
who are making their living from shipping Web sites and Web apps.</p>
<p>Unlike people working on browsers or well-funded Silicon Valley startups, many
of these people are working for little shops with considerable pressure to
deliver something quickly and then move onto the next project in order to pay
the bills.
They don’t have the luxury of spending a morning tinkering with upcoming
technologies and instead reach for tried and trusted solutions they have
experience with.</p>
<h2 id="browsers-arent-made-to-run-spas"><a href="#browsers-arent-made-to-run-spas">“Browsers aren’t made to run <abbr title="single page application">SPA</abbr>s”</a></h2>
<p>Another surprise from moving from browser development back to Web development
was some of the assertions about how browsers work.</p>
<p>When I worked on animations, I was surprised at how many people believed that
some animations “run on the GPU” (the browser can offload some animations to
a separate process or thread that updates animations using the GPU to
composite or even paint each frame but it doesn’t offload them to the GPU
wholesale) but that was a fairly minor misunderstanding compared to some of
the other ones that get thrown around like, “browsers aren’t made to run
SPAs”.</p>
<p>I believe the argument here is that originally browsers would load content off
the network, progressively laying it out and rendering it and have been
heavily optimized for this. Dynamic content came later and so has been less
heavily optimized.</p>
<p>Having worked on a browser on and off for nearly two decades I’m not convinced
this is the case, or at least not anymore.</p>
<p>After all, the Firefox front-end and developer tools are, in effect, SPAs.
The developer tools in particular are written using React and Redux and are in
every sense an SPA.</p>
<p>To the argument that browsers are deficient at handling complex and long-lived
DOM trees with dynamic changes made via JavaScript, the browser itself stands
in contradiction to that claim.</p>
<p>An argument could be made that <em>on mobile</em> browsers aren’t optimized to run
SPAs.
After all, on Android, Firefox switched from an HTML rendered browser chrome
to a native browser chrome in order to improve performance.
I’m not in a position to comment on what the particular performance
constraints were that lead to that change, but <a href="https://lucasr.org/2011/11/15/native-ui-for-firefox-on-android/" rel="noopener nofollow">a blog from that
time</a>
suggests it was related to improving app startup time and panning and
scrolling performance, neither of which point to browsers being deficient at
handling SPAs compared to other architectures.</p>
<p>“Ok, so maybe browsers can handle complex long-lived DOM trees with frequent
dynamic changes, but SPAs tend to have large JS bundles that are slow to
download and parse, blocking the initial render.”
That’s a fair argument, but it’s an argument for smaller render-blocking
initial bundles, which applies to equally to your average WordPress site,
not an argument that browsers are somehow unsuited to running SPAs.</p>
<h2 id="mpas-will-replace-spas"><a href="#mpas-will-replace-spas">”<abbr title="multi-page application">MPA</abbr>s will replace SPAs”</a></h2>
<p>While we’re talking about SPAs and “the one true way to write Web apps”,
a more recent variation on the “browsers can’t handle SPAs” take is, “MPAs
will replace SPAs”.</p>
<p>I’m pretty excited about MPAs.
More specifically, as someone who is involved with a lot of animation specs,
I’m excited by the <a href="https://drafts.csswg.org/css-view-transitions-1/" rel="noopener nofollow">view transitions
spec</a> spec.
It’s something we wanted to do at Mozilla for a while, particularly during
the Firefox OS days, and made
a <a href="https://www.chrislord.net/2015/04/24/web-navigation-transitions/" rel="noopener nofollow">proposal</a>
to that end.
Kudos to Jake and others for finally making it happen.</p>
<p>View transitions were originally implemented for SPAs but have been adapted
to work for MPAs and to the extent that they make multi-page sites more
enjoyable they are a very welcome addition.</p>
<p>However, building on the “SPAs are bad” thinking, there seems to be a tendency
to assume that MPAs are the future and SPAs are on the way out.</p>
<p>Unlike the earlier points in this post, my surprise at this take is not
based on my experience with working on browsers, but on my more recent
experience with working on Web apps.</p>
<p>First though, what do we even mean by MPAs?</p>
<p>My understanding is that whereas SPAs are characterized by having a long-lived
DOM tree or two that are frequently updated by script, MPAs primarily update
content by navigating to different HTML resources served from the network.
These don’t have to be topmost navigations—it could be by navigating an
<code>&lt;iframe&gt;</code>, for example.
Similarly, SPAs might use <code>&lt;iframe&gt;</code>s as a means of chunking but there’s a
difference in how content is typically updated.</p>
<p>By this definition, Google Docs is an SPA since although each document is
served as a separate resource, most of the time you’re interacting with the one
document that is continually updated via JavaScript.
YouTube would probably be considered an MPA but it might actually be implemented
as an SPA in order to smooth out changes in content, intercepting navigations
and replacing the content via script—that is, until view transitions can help
with that.</p>
<p>In either case, my surprise at the idea that MPAs will replace <em>all</em> SPAs is
simple:
How would Figma or Photoshop for Web work as an MPA?
Or Slack, or Discord, or Google Maps?</p>
<p>I’m currently working on an offline-first mobile Web app that stores data
locally and syncs to the server.
Wanting to be on the forefront of Web tech I investigated how we could
embrace MPAs.</p>
<p>To keep a long story short, if we’re to retain our desired UX which has
independently navigable panels and our offline-first requirement, we <em>could</em>
introduce some <code>&lt;iframe&gt;</code>s to partition out some of the functionality into
separate HTML requests (as opposed to separate script chunk requests)
and we could probably even pre-render some chunks sometimes.
The trouble is it would increase the complexity ten-fold
(two-way sync becomes three-way sync for a start) while providing no benefit to
customers whatsoever—instead it would almost certainly lead to more bugs,
more latency, and shipping features more slowly.</p>
<p>Given that our app is not currently released I realise that’s a fairly weak
argument since no-one can look at the app and suggest a better approach so I’m
just asking you to trust me on this one.
I tried.
I really did.
It’s just not the right architecture for this particular app and I’m surprised
by the suggestion that <em>everything</em> should be an MPA.</p>
<h2 id="all-sites-should-work-without-javascript"><a href="#all-sites-should-work-without-javascript">“All sites should work without JavaScript”</a></h2>
<p>Continuing on our theme of Web development best practices, building a site
that works without JavaScript is an admirable goal.
Doing so probaby means it degrades gracefully, has no JS blocking the initial
render, and works on a wide range of browsers.
But I’ve been surprised to see how often this becomes dogmatic:
”<em>all</em> sites should work without JavaScript”.</p>
<p>I guess it’s easy to come to this conclusion if <em>your</em> site is able to work
without JavaScript (see <a href="#my-blog-is-representative-of-web-development-at-large">“My blog is representative of Web development at
large”</a>)
but it feels a little myopic to me.</p>
<p>I’ve mentioned Figma and Photoshop for Web before; it’s hard to imagine how
they could work without JavaScript. Likewise for a browser’s developer tools.
Or even <a href="https://birtles.blog/2012/01/27/parapara-animation/">Parapara Animation</a>!</p>
<p>Furthermore, although many advocates of the “no JS” camp are also concerned
about accessibility, JavaScript is often necessary in order to make an app
accessible.</p>
<p>One thing the accessibility folks at Mozilla taught me about keyboard
navigation was that <kbd>Tab</kbd> navigation should be fairly coarse.
That is, you use <kbd>Tab</kbd> to navigate to the control group (e.g.
toolbar) and then use the arrow keys to move within that group.
That allows you to move around an app more quickly without having to
<kbd>Tab</kbd> through every single control first.
WAI calls this a <a href="https://www.w3.org/WAI/ARIA/apg/practices/keyboard-interface/#kbd_roving_tabindex" rel="noopener nofollow">“roving
tabindex”</a>.</p>
<p>However, in order to implement this kind of coarse-grained keyboard navigation
you’re going to need JavaScript.
If you want to make the arrow-key based navigation two-dimensional, you’re
going to need even more JavaScript.
Maybe one day we’ll fill that gap in the platform (looking at you,
<a href="https://open-ui.org/components/focusgroup.explainer/" rel="noopener nofollow">focusgroup</a>) but for now
you should feel no shame about using client-side JavaScript to make your app
accessible.</p>
<p>Honestly, I think some sites should use <em>more</em> JavaScript.</p>
<p>As an example, the <a href="https://www.11ty.dev/docs/" rel="noopener nofollow">Eleventy
documentation</a> seems to avoid using client-side
JavaScript for the most part.
As Eleventy supports various templating languages it provides code samples in
each of the different languages.
Unfortunately, it doesn’t record which language you’ve selected so if your chosen
language is not the default one, you are forced to change tabs on <em>every
single</em> code sample.
A little client-side JavaScript here would make the experience so much more
pleasant for users.</p>
<h2 id="web-development-shouldnt-need-a-build-step"><a href="#web-development-shouldnt-need-a-build-step">“Web development shouldn’t need a build step”</a></h2>
<p><em>While just about everything in this post is from 2022, while tidying it up
I couldn’t resist adding just one more recent idea that surprised me.</em></p>
<p>Our final stop on the Web development dogma train is a view that’s come up a few
times but still surprises me.
The most recent rendition I saw went something like, “we’re so blind and
stubborn that we’ve ended up with a hugely complex toolchain but we really
should be able to ship Web apps without a build step at all”.</p>
<p>As someone who spent most of his career working with compiled languages the
desire to go without a build step is surprising.
The things compiler engineers do amaze me.
They are geniuses who layer clever optimization on top of clever optimization
transforming my very average code into something unrecognizable and insanely
fast.
If anything, I want more of that compiler magic in my Web development.</p>
<p>Obviously JavaScript presents its own challenges since it can be very hard to
statically determine the side effects of a certain operation but I’m sure
there’s still more room to explore in optimizing JavaScript at compile time.</p>
<p>Web developers seem to agree on optimizing image assets and pre-generating
static HTML pages where it makes sense, why is there resistance to optimizing
code assets too?
Why defer computation and I/O to runtime that can be done once at build time?
If nothing else, I have no desire to ship my megabyte-long comments cursing
iOS Safari to every user on every request.</p>
<p>Maybe 2024 will be the year where client-side Rust/WASM frontend frameworks
start to get traction and if that’s the case, we’d better get used to
having a build step!</p>
<h2 id="my-blog-is-representative-of-web-development-at-large"><a href="#my-blog-is-representative-of-web-development-at-large">“My blog is representative of Web development at large”</a></h2>
<p>A number of the points above could possibly be summarised as
“My blog is representative of Web development at large”.
That is, coming from browser engineer to Web developer, most of the notions
about Web development that have surprised me are the result of people
extrapolating their experience of the Web in a way that doesn’t overlap with
mine.</p>
<p>Since I left Mozilla over four years ago, I’ve spent most of my time
working on a Web app.
I’ve also spent <em>way</em> too long setting up this blog.
Surprisingly, with regards to tooling, architecture, or Web platform features
used, I’ve found almost no overlap between the two.
It’s almost as if blogs and apps exist in entirely disparate corners
of the Web development landscape.</p>
<p>My app is a mass of TypeScript code, my blog uses almost no client-side
JavaScript.
My app is hugely complicated by two-directional data sync, my blog is
read-only.
My app uses webpack, Playwright E2E tests, a component framework, and a state
management library, my blog uses none of those.</p>
<p>If you’re mostly engaged with one or the other, it’s easy to think that’s what
Web development looks like.
In truth, Web development is probably more diverse than any of us imagines.</p>
<h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2>
<p>There are other notions I’ve found surprising but the common theme in the
above is it’s easy to assume our experience of the Web is representative of
Web development in general.</p>
<p>Moving from browser development back to Web development has been humbling
because it’s so much broader and deeper than I knew.
I have a much greater respect for Web developers, especially those at little Web
shops and startups that live or die by the success of their Web apps.</p>
<p>If that’s you, I hope reading this post gives you confidence that even if
browser engineers, standards editors, and other Web developers insist on a
particular way of doing things, you know your constraints better than anyone
else and you’re probably doing things just fine.</p>  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The New York Times Launches a Strong Case Against Microsoft and OpenAI (178 pts)]]></title>
            <link>https://katedowninglaw.com/2024/01/06/the-new-york-times-launches-a-very-strong-case-against-microsoft-and-openai/</link>
            <guid>38900197</guid>
            <pubDate>Sun, 07 Jan 2024 11:03:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://katedowninglaw.com/2024/01/06/the-new-york-times-launches-a-very-strong-case-against-microsoft-and-openai/">https://katedowninglaw.com/2024/01/06/the-new-york-times-launches-a-very-strong-case-against-microsoft-and-openai/</a>, See on <a href="https://news.ycombinator.com/item?id=38900197">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-556">
	
	
	<div>
		
<p>It seems that The New York Times Company (“The Times”) got fed up with the pace of its negotiations with Microsoft and OpenAI over their use of The Times’ content for training and running their LLMs. So much so that The Times filed a post-Christmas<a href="https://nytco-assets.nytimes.com/2023/12/NYT_Complaint_Dec2023.pdf"> complaint</a> against the two, likely knowing full well they’d lay waste to the winter vacations of hundreds of people working for OpenAI and Microsoft. It might be the most well-known AI-related case to date because the case isn’t a class action and the plaintiff is globally recognized.</p>



<p>The complaint alleges:</p>



<ul>
<li>Copyright infringement against all defendants (related to handling of the datasets containing content from The Times, handling of models allegedly derivative of the datasets, and the ultimate output)</li>



<li>Vicarious copyright infringement (the idea that Microsoft and various OpenAI affiliates directed, controlled and profited from infringement committed by OpenAI OpCo LLC and OpenAI, LLC)</li>



<li>Contributory copyright infringement by all defendants (the idea that the defendants contribute to any infringement perpetrated by end users of the models)</li>



<li>DMCA Section 1202 violations by all defendants regarding removal of copyright management information from items in the datasets</li>



<li>Common law unfair competition by misappropriation by all defendants (related to training AI models on The Times’ content and offering AI services that reproduce The Times’ content in identical or substantially similar form (and without citing The Times or linking to the underlying content))</li>



<li>Trademark dilution by all defendants (arguing the the AIs dilute the quality associated with The Times’ trademarks by falsely claiming certain content originates from The Times)</li>
</ul>



<p>Unlike other complaints, this one doesn’t spend too much time explaining how AI models work or teeing up the analogies they plan to use in court. Instead, the complaint includes multiple extremely clear-cut examples of the LLMs spitting out The Times’ content nearly verbatim or stating bald-faced lies about The Times’ content. Many of the other complaints admitted they weren’t able to find clear-cut examples of infringing output, nebulously resting their claims on the idea that all output is, by definition, infringing. Here, Microsoft and OpenAI haven’t just used The Times’ content to teach the AI how to communicate, they’ve launched news-specific services and features that ingest both archived content and brand new articles from The Times. The other plaintiffs also weren’t able to argue that their specific content, out of the trillions of pieces of training data in the datasets, was particularly important for creating quality AIs. Here, The Times convincingly argues that its content was extremely valuable for training the AIs, both because of the quantity involved as well as the fact that the training process involved instructing the AI to prioritize The Times’ content.</p>



<p>This is probably the strongest AI-related complaint out there. I think it’s quite possible that a jury or judge angry at Microsoft and OpenAI for offering services that compete with and undercut The Times is more likely to also find that the training activities also constituted copyright infringement and that the model itself is a derivative work of the training data, without thinking too hard about a scenario where the ultimate model doesn’t supplant the business or livelihood of the copyright holders in the training data. It’s definitely a case where “bad facts invite bad law.”</p>



<p>This case is also notable for the fact that it explicitly goes after the defendants for their AIs’ hallucinations. An AI summarizing a news event based on one or more news articles opens a Pandora’s box worth of debate about the line between uncopyrightable facts and copyrightable expression, as well as how/if those same standards should be applied to a computer “reading” the news. But the hallucinations aren’t facts; they’re lies. And even if the defendants prevail in arguing that the AIs are mostly just providing people with unprotectable facts, there’s very little to shield them from liability for the lies, both with respect to trademark dilution claims, but also with respect to potential libel or privacy-related claims that might be brought by other individuals. Copyright law can forgive a certain amount of infringement under certain circumstances but these other areas of law are far less flexible.</p>



<p>The other really interesting thing about this complaint is the extent to which it describes the business of The Times – how much work the journalists put in to create the articles, the physical risks they take during reporting, the value of good journalism in general, and The Times’ struggle with adjusting to an online world. The complaint paints a picture of an honorable industry repeatedly pants-ed by the tech industry, which historically has only come to heel under enormous public pressure and the Herculean efforts of The Times to continue to survive. It’s interesting because US copyright law decisively rejects the idea that copyright protection is due for what is commonly referred to as “sweat of the brow.” In other words, the fact that it takes great effort or resources to compile certain information (like a phonebook), doesn’t entitle that work to any copyright protection – others may use it freely. And where there is copyrightable expression, the difficulty in creating it is irrelevant. So, is all this background aimed solely at supporting the unfair competition claim? Is it a quiet way of asking the court to ignore the “sweat of the brow” precedent, to the extent that it’s ultimately argued by the defendants, in favor of protecting the more sympathetic party? Maybe they’re truly concerned that the courts no longer recognize the value of journalism and need a history lesson? No other AI-related complaint has worked so hard to justify the very existence, needs, and frustrations of its plaintiffs.</p>



<p>Unless Microsoft and OpenAI hustle to strike a deal with the New York Times, this is definitely going to be the case to watch in the next year or two. Not only does it embody some of the strongest legal arguments related to copyright, it is likely to become a lightning rod for many interests who will use it to wage a proxy war on their behalf. The case, and especially the media coverage of the case, will likely embitter the public and politicians even further against big tech, treating its success as a zero sum game vis a vis journalists and creators more broadly. It’s the kind of case that ultimately results in federal legislation, either codifying a judgment or statutorily reversing it.&nbsp;</p>
	</div>
	<!-- .entry-footer -->

		<!-- .entry-auhtor -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Someone bought 26.9 BTC on Binance and sent it to Satoshi's dead wallet (183 pts)]]></title>
            <link>https://www.blockchain.com/explorer/transactions/btc/d7db4f96a4059c8906b953677ce533493d7b9da0f854a21b99f5772910dd0a31</link>
            <guid>38900049</guid>
            <pubDate>Sun, 07 Jan 2024 10:28:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.blockchain.com/explorer/transactions/btc/d7db4f96a4059c8906b953677ce533493d7b9da0f854a21b99f5772910dd0a31">https://www.blockchain.com/explorer/transactions/btc/d7db4f96a4059c8906b953677ce533493d7b9da0f854a21b99f5772910dd0a31</a>, See on <a href="https://news.ycombinator.com/item?id=38900049">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next" data-reactroot=""><div><a href="https://blockchain.com/" target="_self" rel=""><div><p><span>Blockchain<span>.com</span></span></p></div></a><div><a href="https://www.blockchain.com/explorer"><div><p><span>Home</span></p></div></a><a href="https://www.blockchain.com/explorer/prices"><div><p><span>Prices</span></p></div></a><a href="https://www.blockchain.com/explorer/charts"><div><p><span>Charts</span></p></div></a><a href="https://www.blockchain.com/explorer/nfts"><div><p><span>NFTs</span></p></div></a><a href="https://www.blockchain.com/wallet?utm_campaign=expmarketing_createwallet" target="_self" rel=""><div><p><span>Buy</span></p></div></a><div><p><span>More</span></p></div></div><div><a href="https://www.blockchain.com/explorer"><div><p><span>Home</span></p></div></a><a href="https://www.blockchain.com/explorer/prices"><div><p><span>Prices</span></p></div></a><a href="https://www.blockchain.com/explorer/charts"><div><p><span>Charts</span></p></div></a><a href="https://www.blockchain.com/explorer/nfts"><div><p><span>NFTs</span></p></div></a><a href="https://www.blockchain.com/explorer/defi"><div><p><span>DeFi</span></p></div></a><a href="https://www.blockchain.com/learning-portal/" target="_self" rel=""><div><p><span>Academy</span></p></div></a><a href="https://www.blockchain.com/explorer/news"><div><p><span>News</span></p></div></a><a href="https://www.blockchain.com/explorer/api"><div><p><span>Developers</span></p></div></a><a href="https://www.blockchain.com/wallet?utm_campaign=expmarketing_createwallet" target="_self" rel=""><div><p><span>Wallet</span></p></div></a><a href="https://exchange.blockchain.com/?utm_campaign=expmarketing_getstarted" target="_self" rel=""><div><p><span>Exchange</span></p></div></a><a href="https://www.blockchain.com/explorer/assets/btc"><div><p><span>Bitcoin</span></p></div></a><a href="https://www.blockchain.com/explorer/assets/eth"><div><p><span>Ethereum</span></p></div></a><a href="https://www.blockchain.com/explorer/assets/bch"><div><p><span>Bitcoin Cash</span></p></div></a></div></div><div><div><div><a href="https://blockchain.com/" target="_self" rel=""><div><p><span>Blockchain<span>.com</span></span></p></div></a><div><div><a href="http://wallet.blockchain.com/" target="_self" rel=""></a></div></div></div><div><div><form></form></div><div><a href="http://wallet.blockchain.com/" target="_self" rel=""></a></div></div></div><main><div><section><div><div><p>TX</p></div><div><p>Amount</p><p>Fee</p><p>From</p><p>To</p></div></div><section><section><div><p>Advanced Details</p></div><div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div></div></section><div><div></div><div><div><p>To</p></div><div><p>From</p></div></div></div></section></section><div><p>Explore top crypto assets.</p><div></div></div></div></main></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DNS Toys (2022) (416 pts)]]></title>
            <link>https://www.dns.toys/</link>
            <guid>38899290</guid>
            <pubDate>Sun, 07 Jan 2024 07:29:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dns.toys/">https://www.dns.toys/</a>, See on <a href="https://news.ycombinator.com/item?id=38899290">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<section>
		

		<p>
			dns.toys is a DNS server that takes creative liberties with the DNS
			protocol to offer handy utilities and services
			that are easily accessible via the command line.
		</p>

		<p>
			Copy and run the below commands to try it out.
		</p>
	</section>

	<section>
		<h2>World time</h2>
		<code>
			<p>dig mumbai.time @dns.toys</p>
			<p>dig newyork.time @dns.toys</p>
			<p>dig paris/fr.time @dns.toys</p>
		</code>
		<p>Pass city names without spaces suffixed with <code>.time</code>. Pass two letter country codes separated by slash optionally.</p>

		<br>
		<h3>Timezone conversion</h3>
		<p>Pass YYYY-MM-DD<strong>T</strong>HH:MM-$fromCity-$toCity (two letter country codes separated by slash optionally).</p>

		<code>
			<p>dig 2023-05-28T14:00-mumbai-paris/fr.time @dns.toys</p>
		</code>
	</section>

	<section>
		<h2>Weather</h2>
		<code>
			<p>dig mumbai.weather @dns.toys</p>
			<p>dig newyork.weather @dns.toys</p>
			<p>dig amsterdam/nl.weather @dns.toys</p>
		</code>
		<p>
			Pass city names without spaces suffixed with <code>.weather</code>.
			Pass two letter country codes optionally.
			This service is powered by <a href="https://www.yr.no/en">yr.no</a>
		</p>
	</section>

	<section>
		<h2>Unit conversion</h2>
		<code>
			<p>dig 42km-mi.unit @dns.toys</p>
			<p>dig 32GB-MB.unit @dns.toys</p>
		</code>
		<p>$Value$FromUnit-$ToUnit. To see all 70 available units,
			<code>dig unit @dns.toys</code>
		</p>
	</section>

	<section>
		<h2>Currency conversion (forex)</h2>
		<code>
			<p>dig 100USD-INR.fx @dns.toys</p>
			<p>dig 50CAD-AUD.fx @dns.toys</p>
		</code>
		<p>$Value$FromCurrency-$ToCurrency. Daily rates are from <a href="https://exchangerate.host/">exchangerate.host</a>.</p>
	</section>

	<section>
		<h2>IP echo</h2>
		<code>
			<p>dig -4 ip @dns.toys</p>
		</code>
		<p>Echo your IPv4 address.</p>
		<code>
			<p>dig -6 ip @dns.toys</p>
		</code>
		<p>Echo your IPv6 address.</p>
	</section>

	<section>
		<h2>Number to words</h2>
		<code>
			<p>dig 987654321.words @dns.toys</p>
		</code>
		<p>Convert numbers to English words.</p>
	</section>

	<section>
		<h2>Usable CIDR Range</h2>
		<code>
			<p>dig 10.0.0.0/24.cidr @dns.toys</p>
			<p>dig 2001:db8::/108.cidr @dns.toys</p>
		</code>
		<p>Parse CIDR notation to find out first and last usable IP address in the subnet.</p>
	</section>

	<section>
		<h2>Number base conversion</h2>
		<code>
			<p>dig 100dec-hex.base @dns.toys</p>
			<p>dig 755oct-bin.base @dns.toys</p>
		</code>
		<p>Converts a number from one base to another. Supported bases are hex, dec, oct and bin.</p>
	</section>

	<section>
		<h2>Pi</h2>
		<code>
			<p>dig pi @dns.toys</p>
			<p>dig pi -t txt @dns.toys</p>
			<p>dig pi -t aaaa @dns.toys</p>
		</code>
		<p>Print digits of Pi. Yep.</p>
	</section>

	<section>

		<h2>English dictionary</h2>
		<code>
			<p>dig fun.dict @dns.toys</p>
			<p>dig big-time.dict @dns.toys</p>
		</code>
		<p>Get dictionary definitions for English words. Powered by <a href="https://wordnet.princeton.edu/">WordNet®</a>. Replace spaces with dashes.</p>
	</section>

	<section>
		<h2>Rolling dice</h2>
		<code>
			<p>dig 1d6.dice @dns.toys</p>
			<p>dig 3d20/2.dice @dns.toys</p>
		</code>
		<p>The number of dice to roll, followed by <code>d</code>, followed by the number of sides for each dice, like in tabletop RPG games.</p>
		<p>Optionally suffix by <code>/$number</code> to add it to the total.
			For example, a DnD roll like 2d20+3 is written as 2d20/3.</p>
	</section>

	<section>
		<h2>Tossing coin</h2>
		<code>
			<p>dig coin @dns.toys</p>
			<p>dig 2.coin @dns.toys</p>
		</code>
		<p>Number of coins to toss.</p>
	</section>

	<section>
		<h2>Random number generation</h2>
		<code>
			<p>dig 1-100.rand @dns.toys</p>
			<p>dig 30-200.rand @dns.toys</p>
		</code>
		<p>Generate a random number in a specified range (inclusive of the range values).</p>
	</section>

	<section>
		<h2>Epoch/Unix timestamp conversion</h2>
		<code>
			<p>dig 784783800.epoch @dns.toys</p>
		</code>
		<p>Convert an epoch/unix timestamp into a human readable date. Supports Unix timestamps in s, ms, µs and ns. </p>
	</section>

	<section>
		<h2>Calculate aerial distance</h2>
		<code>
			<p>dig A12.9352,77.6245/12.9698,77.7500.aerial @dns.toys</p>
		</code>
		<p>Calculate aerial distance between a lat-long pair</p>
	</section>

	<section>
		<h2>Generate UUIDs</h2>
		<code>
			<p>dig 5.uuid @dns.toys</p>
		</code>
		<p>Generate N UUIDs (v4).</p>
	</section>

	<section>
		<h2>Help</h2>
		<code>
			<p>dig help @dns.toys</p>
		</code>
		<p>Lists available services.</p>
	</section>

	<section>
		<h2>Shortcut function</h2>
		<div>
			<h3>Bash</h3>
			<p>
				Add this bash alias to your <code>~/.bashrc</code> file.
				The <code>+</code> args show cleaner output from dig.
			</p>
			<p><code>
				<p>alias dy="dig +short @dns.toys"</p>
			</code></p><h3>Fish</h3>
			<p>
				Add this to your fish config file.
			</p>
			<p><code>
				<p>alias dy="dig +noall +answer +additional $argv @dns.toys"</p>
			</code></p><h3>Zsh</h3>
			<p>
				Add this zsh alias to your <code>~/.zshrc</code> file.
				The <code>+</code> args show cleaner output from dig.
			</p>
			<p><code>
				<p>alias dy="dig +short @dns.toys"</p>
			</code></p><p>Then, use the dy command as a shortcut.</p>
			<p><code>
				<p>dy berlin.time</p>
				<p>dy mumbai.weather</p>
				<p>dy 100USD-INR.fx</p>
			</code>
		</p></div>
	</section>

	<section>
		<h2>Why?</h2>
		Why not? For fun. I spend a lot of time on the terminal and doing quick unit
		conversions, weather checks etc. without having to open a clunky search
		page is useful. 
	</section>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linear CEO alleges Carta mishandled sensitive cap table data (145 pts)]]></title>
            <link>https://twitter.com/karrisaarinen/status/1743824345334714587</link>
            <guid>38899001</guid>
            <pubDate>Sun, 07 Jan 2024 06:22:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/karrisaarinen/status/1743824345334714587">https://twitter.com/karrisaarinen/status/1743824345334714587</a>, See on <a href="https://news.ycombinator.com/item?id=38899001">Hacker News</a></p>
Couldn't get https://twitter.com/karrisaarinen/status/1743824345334714587: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Pocketbase: Open-source back end in one file (512 pts)]]></title>
            <link>https://pocketbase.io/</link>
            <guid>38898934</guid>
            <pubDate>Sun, 07 Jan 2024 06:08:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pocketbase.io/">https://pocketbase.io/</a>, See on <a href="https://news.ycombinator.com/item?id=38898934">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>






<div data-wave-pattern-credits="https://www.freepik.com/author/garrykillian"><div><p>Open Source backend</p>
            <p>for your next <strong>SaaS</strong> and <strong>Mobile app</strong></p>
            <p><strong>in 1 file</strong></p></div>

        <div><p><i></i>
                <span>Realtime database</span></p>
            <p><i></i>
                <span>Authentication</span></p>
            <p><i></i>
                <span>File storage</span></p>
            <p><i></i>
                <span>Admin dashboard</span></p></div>

        <figure>

            <div><p><img data-gopher-credits="https://github.com/marcusolsson/gophers" src="data:image/png;base64,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" alt="Gopher" width="69" height="90"></p>
                </div>

            <img src="data:image/png;base64,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" alt="PocketBase dashboard preview" width="1106" height="626">

            </figure></div>



<div><section><h2>Ready to use out of the box</h2>

        <div><nav>
                
                
                
                <a href="https://pocketbase.io/docs"><span>Explore all features</span>
                    <i></i></a></nav>

            <div>
                <p><code><!-- HTML_TAG_START --><span>// JavaScript SDK</span>
<span>import</span> PocketBase <span>from</span> <span>'pocketbase'</span><span>;</span>

<span>const</span> pb <span>=</span> <span>new</span> <span>PocketBase</span><span>(</span><span>'http://127.0.0.1:8090'</span><span>)</span><span>;</span>

<span>...</span>

<span>// list and search for 'example' collection records</span>
<span>const</span> list <span>=</span> <span>await</span> pb<span>.</span><span>collection</span><span>(</span><span>'example'</span><span>)</span><span>.</span><span>getList</span><span>(</span><span>1</span><span>,</span> <span>100</span><span>,</span> <span>{</span>
    <span>filter</span><span>:</span> <span>'title != "" &amp;&amp; created &gt; "2022-08-01"'</span><span>,</span>
    <span>sort</span><span>:</span> <span>'-created,title'</span><span>,</span>
<span>}</span><span>)</span><span>;</span>

<span>// or fetch a single 'example' collection record</span>
<span>const</span> record <span>=</span> <span>await</span> pb<span>.</span><span>collection</span><span>(</span><span>'example'</span><span>)</span><span>.</span><span>getOne</span><span>(</span><span>'RECORD_ID'</span><span>)</span><span>;</span>

<span>// delete a single 'example' collection record</span>
<span>await</span> pb<span>.</span><span>collection</span><span>(</span><span>'example'</span><span>)</span><span>.</span><span>delete</span><span>(</span><span>'RECORD_ID'</span><span>)</span><span>;</span>

<span>// create a new 'example' collection record</span>
<span>const</span> newRecord <span>=</span> <span>await</span> pb<span>.</span><span>collection</span><span>(</span><span>'example'</span><span>)</span><span>.</span><span>create</span><span>(</span><span>{</span>
    <span>title</span><span>:</span> <span>'Lorem ipsum dolor sit amet'</span><span>,</span>
<span>}</span><span>)</span><span>;</span>

<span>// subscribe to changes in any record from the 'example' collection</span>
pb<span>.</span><span>collection</span><span>(</span><span>'example'</span><span>)</span><span>.</span><span>subscribe</span><span>(</span><span>'*'</span><span>,</span> <span>function</span> <span>(</span><span>e</span><span>)</span> <span>{</span>
    console<span>.</span><span>log</span><span>(</span>e<span>.</span>record<span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span>

<span>// stop listening for changes in the 'example' collection</span>
pb<span>.</span><span>collection</span><span>(</span><span>'example'</span><span>)</span><span>.</span><span>unsubscribe</span><span>(</span><span>)</span><span>;</span><!-- HTML_TAG_END --></code>
</p></div></div></section>

    <section><h2>Integrate nicely with your favorite frontend stack</h2>
        <p><a href="https://github.com/pocketbase/dart-sdk" target="_blank" rel="noreferrer noopener"><img src="https://pocketbase.io/images/flutter_logo.svg?v2" alt="Flutter logo" width="40" height="50"></a>
            <a href="https://github.com/pocketbase/js-sdk" target="_blank" rel="noreferrer noopener"><img src="https://pocketbase.io/images/svelte_logo.svg?v2" alt="Svelte logo" width="41" height="50"></a>
            <a href="https://github.com/pocketbase/js-sdk" target="_blank" rel="noreferrer noopener"><img src="https://pocketbase.io/images/vue_logo.svg?v2" alt="Vue logo" width="53" height="46"></a>
            <a href="https://github.com/pocketbase/js-sdk" target="_blank" rel="noreferrer noopener"><img src="https://pocketbase.io/images/react_logo.svg?v2" alt="React logo" width="57" height="51"></a>
            <a href="https://github.com/pocketbase/js-sdk" target="_blank" rel="noreferrer noopener"><img src="https://pocketbase.io/images/angular_logo.svg?v2" alt="Angular logo" width="47" height="50"></a></p></section></div>




			
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia Unveils RTX 5880 Graphics Card with 14,080 CUDA Cores and 48GB VRAM (110 pts)]]></title>
            <link>https://www.nvidia.com/en-us/design-visualization/rtx-5880/</link>
            <guid>38898388</guid>
            <pubDate>Sun, 07 Jan 2024 04:19:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nvidia.com/en-us/design-visualization/rtx-5880/">https://www.nvidia.com/en-us/design-visualization/rtx-5880/</a>, See on <a href="https://news.ycombinator.com/item?id=38898388">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <div id="container-403ed265bb" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">

         
           <div>
            
            <picture data-nv-lazyload="" data-srcset-mobile="/content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-uf1080-p.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-uf1080-p@2x.jpg 2x" data-srcset-tablet="/content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-l440-t.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-l440-t@2x.jpg 2x" data-srcset-laptop="/content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-l580-l.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-l580-l@2x.jpg 2x" data-srcset-desktop="/content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-l580-d.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-l580-d@2x.jpg 2x" srcset="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-l580-d.jpg, https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5880/nvidia-rtx5880-bm-l580-d@2x.jpg 2x">
                <source data-source-mobile="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(max-width: 639px)">
                <source data-source-tablet="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:640px) and (max-width:1023px)">
                <source data-source-laptop="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:1024px) and (max-width:1349px)">
                <source data-source-desktop="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:1350px)">
                <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" alt="NVIDIA RTX 5880 Ada Generation Graphics Card" title="NVIDIA RTX 5880 Ada Generation Graphics Card" id="image-container-403ed265bb" onload="window.initLazyLoadingImages('container-403ed265bb');">
            </picture>
            

            
            </div>
              

         


    	

        <div id="container-6432dbecc1" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    
<div id="nv-text-a145a9d24d">
				<p><span><span>Performance for endless possibilities.</span></span></p>
			</div>

<div id="container-3c5cf013ac" data-title-style="manual" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
        
    

        
    <h2 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Where to Buy
    </h2>

        
    <p>Find an NVIDIA design and visualization partner.</p>

        
	
    
    
    
    

    </div>

    
</div>
        
    </div>
<div id="l80-subnav">
    <ul>
      
      <li data-in-page-nav-item-index="0">
        <a href="#introduction">Introduction</a>
      </li>
    
      
      <li data-in-page-nav-item-index="1">
        <a href="#highlights">Highlights</a>
      </li>
    
      
      <li data-in-page-nav-item-index="2">
        <a href="#features">Features</a>
      </li>
    
      
      <li data-in-page-nav-item-index="3">
        <a href="#workloads">Workloads</a>
      </li>
    
      
      <li data-in-page-nav-item-index="4">
        <a href="#specifications">Specs</a>
      </li>
    
      
      <li data-in-page-nav-item-index="5">
        <a href="#get-started">Get Started</a>
      </li>
    </ul>
    <div>
      
      <ul>
        <li data-in-page-nav-item-index="0">
          <a href="#introduction">Introduction</a>
        </li>
      
        <li data-in-page-nav-item-index="1">
          <a href="#highlights">Highlights</a>
        </li>
      
        <li data-in-page-nav-item-index="2">
          <a href="#features">Features</a>
        </li>
      
        <li data-in-page-nav-item-index="3">
          <a href="#workloads">Workloads</a>
        </li>
      
        <li data-in-page-nav-item-index="4">
          <a href="#specifications">Specs</a>
        </li>
      
        <li data-in-page-nav-item-index="5">
          <a href="#get-started">Get Started</a>
        </li>
      </ul>
    </div>
    <div>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 21.4 5" height="24" width="24" fill="#FFFFFF">
        <path d="M12613.6,1800.5a2.654,2.654,0,1,0-2.6,2.5A2.575,2.575,0,0,0,12613.6,1800.5Zm2.7,0a2.708,2.708,0,1,0,2.7-2.5A2.6,2.6,0,0,0,12616.3,1800.5Zm8.1,0a2.654,2.654,0,1,0,2.6-2.5A2.575,2.575,0,0,0,12624.4,1800.5Z" transform="translate(-12608.3 -1798)"></path>
      </svg>
      <ul>
        <li data-in-page-nav-item-index="0">
          <a href="#introduction">Introduction</a>
        </li>
      
        <li data-in-page-nav-item-index="1">
          <a href="#highlights">Highlights</a>
        </li>
      
        <li data-in-page-nav-item-index="2">
          <a href="#features">Features</a>
        </li>
      
        <li data-in-page-nav-item-index="3">
          <a href="#workloads">Workloads</a>
        </li>
      
        <li data-in-page-nav-item-index="4">
          <a href="#specifications">Specs</a>
        </li>
      
        <li data-in-page-nav-item-index="5">
          <a href="#get-started">Get Started</a>
        </li>
      </ul>
    </div>
    
  </div>
<div id="introduction" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-131023db86">
    	<p>
	    	<h2>
		    	The Ultimate Performance for Professionals
	    	</h2>
    	</p>
     </div>
<div id="nv-text-bdf62596b8">
				<p><span>The NVIDIA RTX™ 5880 Ada Generation delivers the features, capabilities, and performance to meet the challenges of today’s professional workflows. Built on the NVIDIA Ada Lovelace GPU architecture, the RTX 5880 combines third-generation RT Cores, fourth-generation Tensor Cores, and next-gen CUDA® cores with 48GB of graphics memory for unprecedented rendering, graphics, and compute performance. NVIDIA RTX 5880-powered workstations provide what you need to succeed in today’s ultra-challenging business environment.</span></p>
			</div>

    
</div>
<div id="highlights" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-e64d0726d9">
    	<p>
	    	<h2>
		    	Highlights
	    	</h2>
    	</p>
     </div>
<div id="nv-title-ead035950e">
    	<p>
	    	<h2>
		    	Industry-Leading Performance
	    	</h2>
    	</p>
     </div>
<div id="container-fb9e6599d2" data-cmp-is="nv-container">
	        
	        <div id="container-59bc9b08f2" data-cmp-is="nv-container">
    	<p>
	    	<h3>
		    	Single-Precision Performance
	    	</h3>
    	</p>
     </div>
<div id="container-864620bb7f" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    	<p>
	    	<h3>
		    	RT Core Performance
	    	</h3>
    	</p>
     </div>
<div id="container-744d177b07" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    	<p>
	    	<h3>
		    	Tensor Performance
	    	</h3>
    	</p>
     </div>

	        
        </div>
<div id="nv-text-642d773f5b">
				<p><span><span><sup>1</sup> Peak rates based on GPU Boost Clock.</span></span></p>
			</div>


    
</div>
<div id="features" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-1eebf163f7">
    	<p>
	    	<h2>
		    	Features
	    	</h2>
    	</p>
     </div>
<div id="nv-title-4734ca1be9">
    	<p>
	    	<h2>
		    	Powered by the NVIDIA Ada Lovelace Architecture
	    	</h2>
    	</p>
     </div>

<div id="teaser-img" data-cmp-is="nv-container">
	        
	        <div id="nv-teaser-397597a4c4" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/icons/m48-gpu-chip.svg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/icons/m48-gpu-chip.svg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/icons/m48-gpu-chip.svg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/icons/m48-gpu-chip.svg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/icons/m48-gpu-chip.svg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/icons/m48-gpu-chip.svg" data-rendition-web.1280.1280="/content/dam/icons/m48-gpu-chip.svg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/icons/m48-gpu-chip.svg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        NVIDIA Ada Lovelace Architecture-Based CUDA Cores
    </h3>

        
    <p>2X the speed of the previous generation for single-precision floating-point (FP32) operations provides significant performance improvements for graphics and simulation workflows on the desktop, such as complex 3D computer-aided design (CAD) and computer-aided engineering (CAE).</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-573c25ce97" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/icons/m48-render.svg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/icons/m48-render.svg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/icons/m48-render.svg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/icons/m48-render.svg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/icons/m48-render.svg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/icons/m48-render.svg" data-rendition-web.1280.1280="/content/dam/icons/m48-render.svg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/icons/m48-render.svg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="Two" data-titlerowlaptop="Three" data-titlerowtablet="One">
        Third-Generation RT Cores
    </h3>

        
    <p>With up to 2X the throughput over the previous generation, third-generation RT Cores deliver massive speedups for workloads like photorealistic rendering of movie content, architectural design evaluations, and virtual prototyping of product designs. This technology also accelerates the rendering of ray-traced motion blur with greater visual accuracy.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-22374c70b9" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/icons/m48-digital-deep-learning-institute-talks-training.svg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/icons/m48-digital-deep-learning-institute-talks-training.svg/jcr:content/renditions/cq5dam.thumbnail.48.48.jpeg" data-rendition-thumbnail.600.338="/content/dam/icons/m48-digital-deep-learning-institute-talks-training.svg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/icons/m48-digital-deep-learning-institute-talks-training.svg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/icons/m48-digital-deep-learning-institute-talks-training.svg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/icons/m48-digital-deep-learning-institute-talks-training.svg" data-rendition-web.1280.1280="/content/dam/icons/m48-digital-deep-learning-institute-talks-training.svg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/icons/m48-digital-deep-learning-institute-talks-training.svg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="Two" data-titlerowlaptop="Two" data-titlerowtablet="One">
        Fourth-Generation Tensor Cores
    </h3>

        
    <p>Fourth-generation Tensor Cores support acceleration of the FP8 precision data type and provide independent floating-point and integer data paths to speed up execution of mixed floating-point and integer calculations.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-dc60e692ca" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/icons/m48-ram-memory.svg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/icons/m48-ram-memory.svg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/icons/m48-ram-memory.svg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/icons/m48-ram-memory.svg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/icons/m48-ram-memory.svg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/icons/m48-ram-memory.svg" data-rendition-web.1280.1280="/content/dam/icons/m48-ram-memory.svg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/icons/m48-ram-memory.svg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        48GB of GPU Memory
    </h3>

        
    <p>With 48GB GDDR6 memory, RTX 5880 gives data scientists, engineers, and creative professionals the large memory needed to work with large datasets and workloads like rendering, data science, and simulation.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-58f79ceec4" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/icons/m48-gpu-encoding.svg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/icons/m48-gpu-encoding.svg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/icons/m48-gpu-encoding.svg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/icons/m48-gpu-encoding.svg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/icons/m48-gpu-encoding.svg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/icons/m48-gpu-encoding.svg" data-rendition-web.1280.1280="/content/dam/icons/m48-gpu-encoding.svg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/icons/m48-gpu-encoding.svg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        AV1 Encoders
    </h3>

        
    <p>Eighth-generation dedicated hardware encoder (NVENC) with AV1 encoding unlocks new opportunities for streamers, broadcasters, and video conferencing. It’s 40% more efficient than H.264, allowing users streaming at 1080p to increase their resolution to 1440p while running at the same bit rate and quality.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-aba6273ef9" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/icons/m48-virtual-pc-cloud-computer.svg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/icons/m48-virtual-pc-cloud-computer.svg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/icons/m48-virtual-pc-cloud-computer.svg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/icons/m48-virtual-pc-cloud-computer.svg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/icons/m48-virtual-pc-cloud-computer.svg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/icons/m48-virtual-pc-cloud-computer.svg" data-rendition-web.1280.1280="/content/dam/icons/m48-virtual-pc-cloud-computer.svg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/icons/m48-virtual-pc-cloud-computer.svg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Virtualization-Ready
    </h3>

        
    <p>Support for <a href="https://www.nvidia.com/en-us/design-visualization/virtual-workstation/">NVIDIA RTX Virtual Workstation (vWS) software</a> allows a personal workstation to be repurposed into multiple high-performance virtual workstation instances, letting remote users share resources to drive high-end design, and compute workloads.</p>

        
	
    
    
    
    

    </div>

	        
        </div>


    
</div>
<div id="workloads" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-2b96a72d51">
    	<p>
	    	<h2>
		    	Workloads
	    	</h2>
    	</p>
     </div>
<div id="nv-title-717ac37f39">
    	<p>
	    	<h2>
		    	Unlock the Power of NVIDIA RTX for Your Workloads
	    	</h2>
    	</p>
     </div>

<div data-numcards="4" id="rtx5880-workloads">
    
      <div id="card-2" data-gradient-start="rgba(17,17,17,0.7)" data-gradient-end="rgba(17,17,17,0)" data-background-text-color="#FFFFFF" data-index="0" data-imgcredit-text-color="#CCCCCC">
        <div>
          <h4>3D Modeling and Rendering</h4>
          
          <div>
              <p>Create awe-inspiring 3D visuals with RTX 5880. Harness the power of the latest CUDA cores and RT Cores to drive real-time design, intricate geometry, and lifelike textures. Empower your artistic genius and immerse yourself in crafting photorealistic 3D masterpieces.</p>
            </div>
        </div>
        <figure>
          <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i1of4-d.jpg" alt="3D visuals with NVIDIA RTX 5880">
		      <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i1of4-l.jpg" alt="3D visuals with NVIDIA RTX 5880">
          <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i1of4-t.jpg" alt="3D visuals with NVIDIA RTX 5880">
		      <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i1of4-p.jpg" alt="3D visuals with NVIDIA RTX 5880">
		      <figcaption></figcaption>
        </figure>
        </div>
    
      <div id="card-4" data-gradient-start="rgba(17,17,17,0.7)" data-gradient-end="rgba(17,17,17,0)" data-background-text-color="#FFFFFF" data-index="1" data-imgcredit-text-color="#CCCCCC">
        <div>
          <h4>NVIDIA Omniverse</h4>
          
          <div>
              <p>For the most complex Omniverse workloads, the RTX 5880 enables accelerated ray-traced and path-traced rendering of materials, physically accurate simulations, and generating photorealistic 3D synthetic data.</p>
            </div>
        </div>
        <figure>
          <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/workloads-omniverse-fg-i4of6-d.jpg" alt="NVIDIA Omniverse">
		      <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/workloads-omniverse-fg-i4of6-l.jpg" alt="NVIDIA Omniverse">
          <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/workloads-omniverse-fg-i4of6-t.jpg" alt="NVIDIA Omniverse">
		      <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/workloads-omniverse-fg-i4of6-p.jpg" alt="NVIDIA Omniverse">
		      <figcaption></figcaption>
        </figure>
        </div>
    
      <div id="card-5" data-gradient-start="rgba(17,17,17,0.7)" data-gradient-end="rgba(17,17,17,0)" data-background-text-color="#FFFFFF" data-index="2" data-imgcredit-text-color="#CCCCCC">
        <div>
          <h4>Video Content and Streaming</h4>
          
          <div>
              <p>Experience a new era of GPU-accelerated video content creation and streaming. With RTX 5880, harness the power of three encode and three decode engines combined with fourth-gen Tensor Cores to unlock limitless possibilities in video production and&nbsp;<a href="https://www.nvidia.com/en-us/industries/media-and-entertainment/professional-broadcast/">broadcast</a>.</p>
            </div>
        </div>
        <figure>
          <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i3of4-d.jpg" alt="AI-powered video content creation and streaming">
		      <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i3of4-l.jpg" alt="AI-powered video content creation and streaming">
          <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i3of4-t.jpg" alt="AI-powered video content creation and streaming">
		      <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i3of4-p.jpg" alt="AI-powered video content creation and streaming">
		      <figcaption></figcaption>
        </figure>
        </div>
    
      <div id="card-6" data-gradient-start="rgba(17,17,17,0.7)" data-gradient-end="rgba(17,17,17,0)" data-background-text-color="#FFFFFF" data-index="3" data-imgcredit-text-color="#CCCCCC">
        <div>
          <h4>Data Visualization and Simulation</h4>
          
          <div>
              <p>Accomplish compute-intensive data science tasks faster. With 48GB of GPU memory, the RTX 5880 makes it possible to examine large datasets interactively without cutting down the size of the data or reducing fidelity.</p>
            </div>
        </div>
        <figure>
          <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i4of4-d.jpg" alt="Compute-intensive data science tasks faster.">
		      <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i4of4-l.jpg" alt="Compute-intensive data science tasks faster.">
          <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i4of4-t.jpg" alt="Compute-intensive data science tasks faster.">
		      <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-5000/workload-fg-i4of4-p.jpg" alt="Compute-intensive data science tasks faster.">
		      <figcaption></figcaption>
        </figure>
        </div>
    
  </div>

    
</div>
<div id="performance" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-f125866496">
    	<p>
	    	<h2>
		    	Performance
	    	</h2>
    	</p>
     </div>
<div id="nv-title-311fdb2599">
    	<p>
	    	<h2>
		    	Performance for Endless Possibilities
	    	</h2>
    	</p>
     </div>
<div id="nv-text-ce9aacb821">
				<p><span>Designed to meet the challenges of today’s professional workflows, the RTX 6000 provides unprecedented performance for rendering, AI, graphics, and compute workloads. Delivering up to 10X higher performance than the previous generation, the RTX 6000 helps you take your work to new heights.</span></p>
			</div>
<div id="container-0fde2f61c9" data-cmp-is="nv-container">
	        
	        <div id="container-94de67496f" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-d3e08d5752">
    	<p>
	    	<h3>
		    	Graphics
	    	</h3>
    	</p>
     </div>

<div id="nv-text-6fd52df93e">
				<p><span><span>3840x2160 resolution, SPECviewperf 2020 geomean.</span></span></p>
			</div>

    
</div>
<div id="container-e1160389ff" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-099372c4be">
    	<p>
	    	<h3>
		    	Rendering 
	    	</h3>
    	</p>
     </div>

<div id="nv-text-53ebe9a256">
				<p><span><span>1920x1080 resolution, Chaos V-Ray v5.0.</span></span></p>
			</div>

    
</div>
<div id="container-69b81098e8" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-1259499045">
    	<p>
	    	<h3>
		    	Generative AI
	    	</h3>
    	</p>
     </div>

<div id="nv-text-acb2477100">
				<p><span><span>RTX 6000 Ada Generation vs RTX A6000 image generation, 512x512 Stable Diffusion webUI v1.3.1.</span></span></p>
			</div>

    
</div>
<div id="container-321c29de5a" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-abb07af2f7">
    	<p>
	    	<h3>
		    	Omniverse
	    	</h3>
    	</p>
     </div>

<div id="nv-text-de1d3ede79">
				<p><span><span>NVIDIA Omniverse performance for real-time rendering at 4K with NVIDIA Deep Learning Super Sampling (DLSS) 3.</span></span></p>
			</div>

    
</div>
<div id="container-2c697d00b2" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-749ff90553">
    	<p>
	    	<h3>
		    	AI Inference
	    	</h3>
    	</p>
     </div>

<div id="nv-text-943434eb9c">
				<p><span><span>TensorRT, ResNet-50 V1.5 Inference, precision: mixed.</span></span></p>
			</div>

    
</div>
<div id="container-c5559848a4" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-d1e648e5bc">
    	<p>
	    	<h3>
		    	AI Training
	    	</h3>
    	</p>
     </div>

<div id="nv-text-77f7fe24ef">
				<p><span><span>PyTorch, BERT Large Pre-Training, precision: mixed.</span></span></p>
			</div>

    
</div>

	        
        </div>
<div id="nv-text-3f803d4e88">
				<p><span><span>Performance testing with RTX 6000 Ada Generation and RTX A6000 GPUs and Intel Core i9-12900K. Performance subject to change.</span></span></p>
			</div>

    
</div>
<div id="specifications" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-d012c15180">
    	<p>
	    	<h2>
		    	Specifications
	    	</h2>
    	</p>
     </div>
<div id="nv-title-ae7a3b9671">
    	<p>
	    	<h2>
		    	NVIDIA RTX 5880
	    	</h2>
    	</p>
     </div>




    
</div>
<div id="news-events" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-4f3671cf67">
    	<p>
	    	<h2>
		    	News and Events
	    	</h2>
    	</p>
     </div>

<div aria-atomic="false" aria-live="polite" id="success-stories-carousel" role="group" aria-roledescription="carousel" data-cmp-is="carousel" data-cmp-autoplay="" data-cmp-delay="5000" data-cmp-autoscroll="false" data-cmp-scroll-direction="left" data-cmp-scroll-delay="5000" data-cmp-scroll-disabled="false">
         <div>
            <div id="success-stories-carousel-item-e865de9f76-tabpanel" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none" role="tabpanel" aria-roledescription="slide" aria-label="Slide 1 of 2" data-cmp-slide-no="1" data-cmp-hook-carousel="item">

         
           <div>
            

            
            <picture>
                <source srcset="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-p.jpg, https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-p@2x.jpg 2x" media="(max-width: 639px)">
                <source srcset="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-t.jpg, https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-t@2x.jpg 2x" media="(min-width:640px) and (max-width:1023px)">
                <source srcset="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-l.jpg, https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-l@2x.jpg 2x" media="(min-width:1024px) and (max-width:1349px)">
                <source srcset="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-d.jpg, https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-d@2x.jpg 2x" media="(min-width:1350px)">
                <img src="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-d.jpg" srcset="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-proviz-d@2x.jpg 2x" id="image-container-e865de9f76" onload="setContainerHeight('container-e865de9f76')">
            </picture>
            
            </div>
              

         


    	

        <div id="container-7c52602917" data-cmp-is="nv-container">
    
    <div id="nv-title-3844c4db24">
    	<p>
	    	<h3>
		    	The Next Generation of Workstations Is Here
	    	</h3>
    	</p>
     </div>
<div id="nv-text-30c6aee7b5">
				<p><span>Intel and AMD CPUs, along with NVIDIA GPUs, usher in the next generation of OEM workstation platforms. These new workstations, powered by the latest Intel® Xeon® W and AMD Threadripper processors, <a href="https://www.nvidia.com/en-us/design-visualization/rtx-5880/">NVIDIA RTX 6000 Ada Generation</a> GPUs, and <a href="https://www.nvidia.com/en-us/networking/ethernet-adapters/">NVIDIA ConnectX®</a> smart network interface cards, bring unprecedented performance for creative and technical professionals.</span></p>
			</div>


    
</div>
        
    </div>
<div id="success-stories-carousel-item-514cb18dad-tabpanel" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none" role="tabpanel" aria-roledescription="slide" aria-label="Slide 2 of 2" data-cmp-slide-no="2" data-cmp-hook-carousel="item">

         
           <div>
            
            <picture data-nv-lazyload="" data-srcset-mobile="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-p.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-p@2x.jpg 2x" data-srcset-tablet="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-t.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-t@2x.jpg 2x" data-srcset-laptop="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-l.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-l@2x.jpg 2x" data-srcset-desktop="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-d.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-d@2x.jpg 2x" srcset="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-d.jpg, https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/success-ove-d@2x.jpg 2x">
                <source data-source-mobile="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(max-width: 639px)">
                <source data-source-tablet="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:640px) and (max-width:1023px)">
                <source data-source-laptop="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:1024px) and (max-width:1349px)">
                <source data-source-desktop="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:1350px)">
                <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" id="image-container-514cb18dad" onload="window.initLazyLoadingImages('container-514cb18dad');">
            </picture>
            

            
            </div>
              

         


    	

        <div id="container-5e8acd74f1" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    
<div id="nv-title-2927340625">
    	<p>
	    	<h3>
		    	NVIDIA RTX 6000 Ada Generation With Omniverse Enterprise
	    	</h3>
    	</p>
     </div>
<div id="nv-text-f5e192fe87">
				<p>NVIDIA RTX 6000 Ada Generation GPUs bundled with NVIDIA Omniverse™ Enterprise are now available, providing a turn-key, real-time collaboration solution for advanced design, visualization, and simulation projects.</p>
			</div>


    
</div>
        
    </div>

         </div>

    

        <!--
        <div class="cmp-carousel__actions">
            <button data-sly-test="true"
                    class="cmp-carousel__action cmp-carousel__action--pause"
                    type="button"
                    aria-label="Pause"
                    data-cmp-hook-carousel="pause">
                <span class="cmp-carousel__action-icon"></span>
                <span class="cmp-carousel__action-text">Pause</span>
            </button>
            <button data-sly-test="true"
                    class="cmp-carousel__action cmp-carousel__action--play cmp-carousel__action--disabled"
                    type="button"
                    aria-label="Play"
                    data-cmp-hook-carousel="play">
                <span class="cmp-carousel__action-icon"></span>
                <span class="cmp-carousel__action-text">Play</span>
            </button>
        </div>
        -->
      <ol role="tablist" aria-label="Choose a slide to display" data-cmp-hook-carousel="indicators">
            <li id="success-stories-carousel-item-e865de9f76-tab" role="tab" aria-controls="success-stories-carousel-item-e865de9f76-tabpanel" aria-label="Slide 1" data-cmp-hook-carousel="indicator">slide-1</li>
<li id="success-stories-carousel-item-514cb18dad-tab" role="tab" aria-controls="success-stories-carousel-item-514cb18dad-tabpanel" aria-label="Slide 2" data-cmp-hook-carousel="indicator">slide-2</li>

        </ol>
    </div>


    
</div>
<div id="resources" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="resources-title-1">
    	<p>
	    	<h2>
		    	Resources
	    	</h2>
    	</p>
     </div>

<div id="resources-title-2">
    	<p>
	    	<h2>
		    	Take a Deeper Dive Into NVIDIA RTX 6000
	    	</h2>
    	</p>
     </div>
<div id="resources-tabs" data-cmp-is="tabs" data-cmp-id="resources-tabs">
     <div>
          
          
    <ol role="tablist" aria-multiselectable="false">
        
        
        <li role="tab" id="resources-tabs-item-077913460b-tab" aria-controls="resources-tabs-item-077913460b-tabpanel" tabindex="0" data-cmp-hook-tabs="tab">
            <p>Videos</p>
        </li>
    
        
        
        <li role="tab" id="resources-tabs-item-edc9c5ea8b-tab" aria-controls="resources-tabs-item-edc9c5ea8b-tabpanel" tabindex="-1" data-cmp-hook-tabs="tab">
            <p>Blogs</p>
        </li>
    
        
        
        <li role="tab" id="resources-tabs-item-00331ef0f5-tab" aria-controls="resources-tabs-item-00331ef0f5-tabpanel" tabindex="-1" data-cmp-hook-tabs="tab">
            <p>Community</p>
        </li>
    </ol>

          
    </div>
    <div id="resources-tabs-item-077913460b-tabpanel" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none" role="tabpanel" aria-labelledby="resources-tabs-item-077913460b-tab" tabindex="0" data-cmp-hook-tabs="tabpanel">

         
           <div>
            
            <picture data-nv-lazyload="" data-srcset-mobile="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb770_550-p.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb580_440-p@2x.jpg 2x" data-srcset-tablet="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb770_550-t.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb580_440-t@2x.jpg 2x" data-srcset-laptop="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb770_550-l.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb580_440-l@2x.jpg 2x" data-srcset-desktop="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb770_550-d.jpg, /content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb580_440-d@2x.jpg 2x" srcset="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb770_550-d.jpg, https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/rtx6000-resources-videos-hero-bb580_440-d@2x.jpg 2x">
                <source data-source-mobile="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(max-width: 639px)">
                <source data-source-tablet="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:640px) and (max-width:1023px)">
                <source data-source-laptop="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:1024px) and (max-width:1349px)">
                <source data-source-desktop="" srcset="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" media="(min-width:1350px)">
                <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" id="image-tab-slide-1" onload="window.initLazyLoadingImages('tab-slide-1');">
            </picture>
            

            
            </div>
              

         


    	

        <div id="videos-slide-container" data-cmp-is="nv-container">
    
    <div id="container-1dc4debc02" data-cmp-is="nv-container">
    
    

<div id="nv-title-b4a71013c1">
    	<p>
	    	<h3>
		    	Accurately Reflect Real-World Conditions With Ease
	    	</h3>
    	</p>
     </div>
<div id="nv-text-f6de73eef0">
				<p><span>GPU-accelerated aerodynamic simulation enhanced with NVIDIA RTX 6000 takes product development to the next level. The RTX 6000 enables you to design, simulate, and optimize products–using the most powerful 3D design and simulation tools–faster and more interactively than ever before.</span></p>
			</div>



    
</div>

<div id="container-be8a1d3536" data-cmp-is="nv-container">
	        
	        <div id="nv-teaser-5b71d9b0bc" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-rtx-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-rtx-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-rtx-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-rtx-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-rtx-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-rtx-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-rtx-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-rtx-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Envision Endless Possibilities with NVIDIA RTX
    </h3>

        
    <p>Unlock endless possibilities with NVIDIA RTX to create stunning visuals that can revolutionize workflows for professionals. See how AI breakthroughs in design, engineering, and simulation bring your vision to life faster with photorealistic detail. Experience the power of NVIDIA RTX and redefine what comes next.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-21b16a8db4" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-forager-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-forager-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-forager-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-forager-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-forager-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-forager-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-forager-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-forager-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        3D Volumetric Time-Lapse with Forager
    </h3>

        
    <p>To bring the forest to life, Forager developed MycoMachine, the first time-lapse photogrammetry rig to capture a library of mushrooms growing over time and play them back as hyperrealistic real-time animations. The NVIDIA RTX 6000 Ada helps Forager tackle the computationally intense challenges presented during reconstruction, clean up, and real-time playback.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-15161af4d7" data-title-style="dynamic" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-vizrt-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-vizrt-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-vizrt-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-vizrt-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-vizrt-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-vizrt-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-vizrt-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/video-vizrt-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Live Graphics Production with Vizrt
    </h3>

        
    <p>Gerhard Lang, CTO of Vizrt Group talks about how Viz Engine 5 uses NVIDIA RTX 6000 Ada’s latest generation RT Cores, Tensor Cores, and DLSS to track talent, upscale footage, and create the necessary reflections and shadows to ground a subject inside a virtual environment.</p>

        
	
    
    
    
    

    </div>

	        
        </div>

    
</div>
        
    </div>
<div id="resources-tabs-item-edc9c5ea8b-tabpanel" data-cmp-is="nv-container" role="tabpanel" aria-labelledby="resources-tabs-item-edc9c5ea8b-tab" tabindex="0" data-cmp-hook-tabs="tabpanel">
	        
	        <div id="nv-teaser-39b150a72c" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-adidas-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-adidas-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-adidas-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-adidas-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-adidas-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-adidas-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-adidas-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-adidas-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        A Perfect Pair: Adidas and Covision Media Use AI, NVIDIA RTX to Create Photorealistic 3D Content
    </h3>

        
    <p>Covision’s AI-based 3D technology helps businesses scan thousands of products, creating photorealistic 3D images, videos, and AR experiences for websites and mobile apps.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-25c754bc8a" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-trek-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-trek-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-trek-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-trek-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-trek-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-trek-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-trek-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-trek-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Design Speed Takes the Lead: Trek Bicycle Competes in Tour de France With Bikes Developed Using NVIDIA GPUs
    </h3>

        
    <p>The team uses RTX technology to accelerate product design, iterate more quickly, and run realistic computational fluid dynamics simulations to build world-class bicycles.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-e01797e367" data-title-style="dynamic" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-ride-joy-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-ride-joy-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-ride-joy-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-ride-joy-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-ride-joy-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-ride-joy-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-ride-joy-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/blog-ride-joy-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        3D Artist Brings Ride and Joy to Automotive Designs With Real-Time Renders Using NVIDIA RTX
    </h3>

        
    <p>David Baylis uses the powerful ray-tracing features and large GPU memory of the NVIDIA RTX A6000 to create stunning visuals with sharp details, realistic lighting, and bouncing reflections.</p>

        
	
    
    
    
    

    </div>

	        
        </div>
<div id="resources-tabs-item-00331ef0f5-tabpanel" data-cmp-is="nv-container" role="tabpanel" aria-labelledby="resources-tabs-item-00331ef0f5-tab" tabindex="0" data-cmp-hook-tabs="tabpanel">
	        
	        <div id="nv-teaser-727cf96136" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-construction-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-construction-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-construction-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-construction-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-construction-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-construction-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-construction-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-construction-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Building Realistic Renders and 4D Sequences of Construction Projects With NVIDIA RTX
    </h3>

        
    <p>Explore how Layton Construction uses NVIDIA RTX 6000 Ada Generation GPUs to enhance visualization workflows, creating photo-realistic renders and animations that can capture every stage of a construction project.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-943186efba" data-title-style="manual" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-david-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-david-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-david-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-david-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-david-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-david-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-david-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-david-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        NVIDIA RTX Ambassador David Baylis - Podcast With Allan McKay
    </h3>

        
    <p>Listen to RTX Ambassador David Baylis share his incredible insights on innovations in ray tracing and AI, the future of VFX careers in the era of machine learning, how to get started in Unreal, and why learning 3D is a lifelong occupation.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-6b40d02045" data-title-style="dynamic" data-rendition-thumbnail.1200.676="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-sony-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.1200.676.png" data-rendition-thumbnail.48.48="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-sony-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.48.48.png" data-rendition-thumbnail.600.338="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-sony-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.600.338.png" data-rendition-thumbnail.140.100="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-sony-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.140.100.png" data-rendition-thumbnail.300.169="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-sony-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.300.169.png" data-rendition-original="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-sony-2560x1440.jpg" data-rendition-web.1280.1280="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-sony-2560x1440.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg" data-rendition-thumbnail.319.319="/content/dam/en-zz/Solutions/design-visualization/rtx-6000/community-sony-2560x1440.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Sony Pictures Animation Bringing 2D and 3D Worlds Together With NVIDIA Omniverse Enterprise
    </h3>

        
    <p>Built on <a href="https://www.nvidia.com/en-us/omniverse/enterprise/">NVIDIA Omniverse Enterprise</a> and powered by NVIDIA RTX, Sony Pictures Entertainment developed FlixiVerse, allowing 2D artists to effortlessly step into the world of 3D. Learn how they are accelerating creative workflows with more iterations and easier transitions into 3D environments.</p>

        
	
    
    
    
    

    </div>

	        
        </div>

    
</div>


    
</div>
<div id="get-started" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-title-33e67b86ec">
    	<p>
	    	<h2>
		    	Get Started
	    	</h2>
    	</p>
     </div>
<div id="container-afe5a0d17e" data-cmp-is="nv-container" data-cmp-breadcrumbcolor="none">
    
    <div id="nv-teaser-16d96ca65a" data-title-style="manual">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Ready to Purchase?
    </h3>

        
    <p><span>Talk with an NVIDIA design and visualization partner.</span></p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-b7f6a809cd" data-title-style="manual">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Need Help Selecting the Right Product or Partner?
    </h3>

        
    <p>Talk to an NVIDIA product specialist about your professional needs.</p>

        
	
    
    
    
    

    </div>
<div id="nv-teaser-91aee16bfa" data-title-style="manual">
        
    

        
    <h3 data-titlerow="One" data-titlerowlaptop="One" data-titlerowtablet="One">
        Get the Latest on NVIDIA RTX
    </h3>

        
    <p>Sign up for the latest news, updates, and more from NVIDIA.</p>

        
	
    
    
    
    

    </div>

    
</div>

    
</div>
<div data-form-page-path="/content/forms/af/nvidia-forms/industries/power-and-utilities-email-subscribe-form" id="notify-me">
		<p id="aem-form-title"> <h2><span>Sign Up To Be Notified On Availability</span></h2>

		</p>
		
		
		
		
		
    




		
		
		







    
        
        
    
    



































    








    
    
        
        
        
        
            
            
            
                




            
            


        
            
            
            
                







    
        
        
    
    







    
    
        

    


            
            
    










    




		
    




	</div>
<div id="specs">
    	<p>
	    	<h2>
		    	NVIDIA RTX 5880 Quick Specs
	    	</h2>
    	</p>
     </div>

    
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Teachable Machine (237 pts)]]></title>
            <link>https://teachablemachine.withgoogle.com/</link>
            <guid>38898104</guid>
            <pubDate>Sun, 07 Jan 2024 03:20:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://teachablemachine.withgoogle.com/">https://teachablemachine.withgoogle.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38898104">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[US unemployment has been under 4% for the longest streak since the Vietnam War (106 pts)]]></title>
            <link>https://www.npr.org/2024/01/05/1222714145/jobs-report-december-labor-wages</link>
            <guid>38897475</guid>
            <pubDate>Sun, 07 Jan 2024 01:33:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npr.org/2024/01/05/1222714145/jobs-report-december-labor-wages">https://www.npr.org/2024/01/05/1222714145/jobs-report-december-labor-wages</a>, See on <a href="https://news.ycombinator.com/item?id=38897475">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storytext">
      <div id="res1222714859">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s400-c85.webp 400w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s600-c85.webp 600w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s800-c85.webp 800w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s900-c85.webp 900w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1200-c85.webp 1200w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1800-c85.webp 1800w" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s600-c85.jpg 600w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s900-c85.jpg 900w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1200-c85.jpg 1200w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1800-c85.jpg 1800w" sizes="(min-width: 1300px) 763px, (min-width: 1025px) calc(100vw - 496px), (min-width: 768px) calc(100vw - 171px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                The U.S. job market held up well in 2023, despite rising interest rates. Employers added 2.7 million jobs last year and unemployment remained under 4% throughout the year.
                <b aria-label="Image credit">
                    
                    Mario Tama/Getty Images
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Mario Tama/Getty Images
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2024/01/03/gettyimages-1461883637-24aba91a9127c79346e2bcaf6a637d881f63984a-s1200.jpg">
        </picture>
    </div>
<div>
        <p>The U.S. job market held up well in 2023, despite rising interest rates. Employers added 2.7 million jobs last year and unemployment remained under 4% throughout the year.</p>
        <p><span aria-label="Image credit">
            
            Mario Tama/Getty Images
            
        </span>
    </p></div>
   </div>
   <p>The U.S. job market capped off a strong year in December, as employers continued hiring at a solid pace.</p>   <p>Employers added 216,000 jobs last month, according to the Labor Department. The unemployment rate held steady at 3.7%.</p>   <p>Unemployment has now been under 4% for almost two years — the longest streak of rock-bottom jobless rates since the Vietnam War. </p>   <p>"The labor market ended 2023 on a solid footing," said Nela Richardson, chief economist for the payroll processing company ADP. "We'll see what 2024 will bring."  </p>   <p>December's job gains were concentrated in government and health care. Retailers added 17,000 jobs, suggesting a solid finish to the holiday shopping season. </p>   
   
   
<!-- END ID="RES1223052238" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <h3>Job growth has been resilient despite Fed's brutal interest rate increases</h3>   <p>For all of 2023, employers added 2.7 million jobs. That's a slowdown from the two previous years, when the economy was red-hot, rapidly rebounding from pandemic layoffs. But last year's job growth was still stronger than every other year since 2015.</p>   <p>The job market has proven to be resilient despite the Federal Reserve's aggressive push to combat inflation with higher interest rates. Even sensitive industries where the cost of borrowing is elevated continued to add jobs last year. Construction companies added 17,000 jobs in December. </p>   
   
<!-- END ID="RES1223052384" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>Nancy McNamara completed a building trades internship in October and quickly secured a job with a busy weatherization contractor in Rutland, Vt.</p>   <p>"I feel like every time we're at a job site, he's getting a call from someone else," McNamara said. "He's booked right up through — I don't even know when."</p>   <p>McNamara is eager to learn new construction skills and has gotten training offers from a carpenter and a drywall contractor. </p>   <p>"I like being tired at the end of the day and feeling like I accomplished something," she said. "With work like this, that's exactly how I feel."</p>   
   
<!-- END ID="RES1223051787" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <h3>Hotels, restaurants still hasn't recovered to pre-pandemic levels</h3>   <p>The leisure and hospitality sector — which includes restaurants and hotels — added 40,000 jobs last month but overall employment in the sector still hasn't quite recovered to pre-pandemic levels.</p>   <p>Government employment was also slow to bounce back from the pandemic, but strong government hiring in 2023 finally closed that gap. </p>   <p>Wages are rising, but not as fast as they were earlier in the year. Average wages in December were up 4.1% from a year ago. Slower wage growth puts less upward pressure on prices, which should be reassuring to inflation watchdogs at the Fed. </p>   
   <p>"There's very little risk of a wage-price spiral that will push up inflation in 2024," Richardson said. </p>   <p>The good news for workers is that wages have been climbing faster than prices in recent months, so the average paycheck stretches further. </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Carta CEO's response to the unsolicited outreach to their customers' investors (195 pts)]]></title>
            <link>https://twitter.com/henrysward/status/1743794996732735679</link>
            <guid>38897363</guid>
            <pubDate>Sun, 07 Jan 2024 01:15:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/henrysward/status/1743794996732735679">https://twitter.com/henrysward/status/1743794996732735679</a>, See on <a href="https://news.ycombinator.com/item?id=38897363">Hacker News</a></p>
Couldn't get https://twitter.com/henrysward/status/1743794996732735679: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Original Age of Empires 2 dev talks about its usage of assembly code (364 pts)]]></title>
            <link>https://old.reddit.com/r/aoe2/comments/18ysttu/aoe_is_written_in_assembly_is_this_actually_true_o/</link>
            <guid>38896896</guid>
            <pubDate>Sat, 06 Jan 2024 23:59:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/aoe2/comments/18ysttu/aoe_is_written_in_assembly_is_this_actually_true_o/">https://old.reddit.com/r/aoe2/comments/18ysttu/aoe_is_written_in_assembly_is_this_actually_true_o/</a>, See on <a href="https://news.ycombinator.com/item?id=38896896">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>[–]<a href="https://old.reddit.com/user/ES_MattP">ES_MattP</a><span title="ES/FE">ES/FE</span><span></span> <span title="127">127 points</span><span title="128">128 points</span><span title="129">129 points</span> <time title="Sat Jan 6 17:16:52 2024 UTC" datetime="2024-01-06T17:16:52+00:00">13 hours ago</time><time title="last edited 6 hours ago" datetime="2024-01-07T00:56:45+00:00">*</time>&nbsp;(6 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_kgltqrg21n"><div><p>I guess I can clarify this, since I wrote all the assembly code used in AoE / Aok, along with many other parts of those games.</p>

<p>There were about ~13,000 lines of x86 32-bit assembly code written in total.</p>

<p>The vast majority, about ~11,500 lines worth, was in the 'drawing core', which drew SLP sprites in a variety of ways (mirrored, stippled, clipped).  The code for this was in a separate .asm file, which was "compiled" by Microsoft Macro Assembler 6.1 into a .obj file.</p>

<p>There were maybe a dozen other uses of assembly in the C++ code, as the compilers at the time supported 'inline assembly' in the middle of functions.  These blocks of asm code were added using the <strong>__asm {  }</strong> keyword.  Microsoft Visual C++ 4.2 to 6.0 compilers were the ones used to compile the games as originally shipped.    The inline assembly in other functions was generally used to speed up operations like scanning areas of the map for targeting and accessing compressed look up tables, which had bit-level operations that could be reduced in assembly, but c++ couldn't express as well.</p>

<p>The use of assembly in the drawing core resulting in a ~10x sprite drawing speed improvement over the C++ reference implementations, and AoE's drawing core was notably faster than competitors like StarCraft, which is why the default resolution 'out of the box' for AoE was 800 by 600, when nearly all our competition was 640 x 480 resolution - we could scroll the screen and fill it with sprites as fast or faster even though we had twice as many pixels-ish in the game world area. </p>

<p>The design of the drawing core used a number of techniques that could not easily (or at all) be expressed in C++ and took into account things like the cache architecture and UV pipes of the first Pentium processors.  A key speedup technique AoE used was realized during discussions I had with iD software programmer and optimization guru Michael Abrash over lunch at Tia's Mexican Restaurant in Mesquite, TX.   Other speedups came from being able eliminating function call overhead and hand-managing register usage.  "register starvation" was a real issue for CPUs in the pre-Pentium Pro era and languages like C++ were hurt by it.</p>

<p>Additional assembly code was added in Age of Kings to add the clipped outlines of obscured units.</p>

<p>The assembly code remained in use in AoK:HD edition (a 32-bit game)</p>

<p>I re-wrote the assembly functions into C++ for both Definitive Editions, as they are 64-bit programs, and inline assembly was never supported by the 64-bit C++ compiler, and the vastly improved register sets and compiler optimizations made it un-necessary.   Additionally, sprite drawing in the definitive editions is multi-threaded, and will use up to 4 cores for that task alone.</p>

<p>I hope that clears things up.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/aoe2/comments/18ysttu/aoe_is_written_in_assembly_is_this_actually_true_o/kgltqrg/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li>report</li><li>reply</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[On building a semantic search engine (165 pts)]]></title>
            <link>https://vickiboykis.com/2024/01/05/retro-on-viberary/</link>
            <guid>38896879</guid>
            <pubDate>Sat, 06 Jan 2024 23:57:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vickiboykis.com/2024/01/05/retro-on-viberary/">https://vickiboykis.com/2024/01/05/retro-on-viberary/</a>, See on <a href="https://news.ycombinator.com/item?id=38896879">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Viberary is a side project that I worked on in 2023, which does semantic search for books by vibe. It was hosted at [viberary.pizza.]</p><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/0c8fddcb-3bb1-4b67-8d0e-7b2462c6d561" width="600"></figure><p>I’m shutting down the running app and putting the codebase in maintenance mode because:</p><ul><li>A lot of what I want to continue to do there (i.e. changing embedding models, modifying training data) involves building out more complex infra: a model store, a feature store, data management, evaluation infra, and all of that’s going to take longer than I have</li><li>There’s a lot of maintenance that needs to happen for a running app (Python dependencies, etc. ) I.e. <a href="https://blog.professorbeekums.com/all-code-is-debt/">all code is technical debt.</a></li><li>Cost! I don’t want to maintain an app that is currently losing $100+ a month to maintenance costs unless I’m also planning to make money from it. I’m not planning to, but I have learned a LOT from this project and I have loved building and sharing it.</li><li>I have a new project idea I’d like to work on, so I need to make space for it.</li></ul><p>There were SO many, SO many things I learned from this project. Most of them are outlined in the post below, so read on. But, if you want a list of high-level bullets:</p><ul><li>The project HAS to be something you’re interested in. You will not work on it otherwise. I love books and I want to be recommended books, and I had a keen understanding of this problem space before I started.</li><li>Start as simple as you can, but no simpler. You should be able to test anything you deploy locally without external dependencies. You need to be able to go fast at the beginning, otherwise you’ll lose interest.</li><li>At the same time, you will not know what’s simple unless you try something, anything.</li><li>Simple means most of the code you write should be your library logic, <a href="https://vickiboykis.com/2022/12/05/the-cloudy-layers-of-modern-day-programming/">not glue code between cloud components.</a></li><li>Docker on new Mac M1+ architectures that have to be ported to Linux is really annoying but fixable.</li><li>Knowing <a href="https://newsletter.vickiboykis.com/archive/when-you-write-a-web-server-but-you-get-served/">nginx</a> well can save you a ton of time</li><li>Sometimes you don’t need large language models, BERT works just fine</li><li>Evaluating the results of unsupervised ranking and retrieval is really hard and no one has solved this problem yet</li><li>Digital Ocean has an amazing product suite that just works for small and medium-size projects</li><li>The satisfaction of shipping products that you’ve build is unparalleled</li></ul><p>For much, much more, read on!</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/learningtired.png" width="600"></figure><h3 id="august-5-2023">August 5, 2023</h3><p><em>TL;DR</em>: Viberary is a side project that I created to find books by <strong>vibe</strong>. I built it to satisfy an itch to do <a href="https://vickiboykis.com/2020/06/09/getting-machine-learning-to-production/">ML side projects</a> and navigate the current boundary between search and recommendations. It’s a production-grade complement to <a href="http://vickiboykis.com/what_are_embeddings/">my recent deep dive into embeddings.</a></p><p>This project is a lot of fun, but conclusively proves to me what I’ve known all along about myself: reaching MLE (machine learning enlightenment) is the cyclical process of working through modeling, engineering,and UI concerns, and connecting everything together - <a href="https://vickiboykis.com/2021/09/23/reaching-mle-machine-learning-enlightenment/">the system in production is the reward.</a>
And, like any production-grade system, machine learning is not magic. Even if the data outputs are not deterministic, it takes thoughtful engineering and design
choices to build any system like this, something that I think gets overlooked these days in the ML community.</p><p>I hope with this write-up to not only remind myself of what I did, but outline what it takes to build a production Transformer-based machine learning application, even a small one with a pre-trained model, and hope it serves as a resource and reference point.</p><hr><p>Viberary’s machine learning architecture is a <a href="https://blog.reachsumit.com/posts/2023/03/two-tower-model/">two-tower</a> semantic retrieval model that encodes the user search query and the Goodreads book corpus using the
<a href="https://www.sbert.net/docs/pretrained-models/msmarco-v3.html">Sentence Transformers pretrained asymmetric MSMarco Model</a>.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/viberary_arch.png" width="600"></figure><p>The training data is generated locally by <a href="https://github.com/veekaybee/viberary/blob/main/src/model/generate_training_data.py">proessing JSON in DuckDB</a> and the model is converted to ONNX for performant inference, with <a href="https://github.com/veekaybee/viberary/blob/main/src/model/generate_embeddings.ipynb">corpus embeddings learned on AWS P3 instances</a> against the same model and stored in Redis. Retrieval happens using the <a href="https://redis.io/docs/interact/search-and-query/">Redis Search</a> set with the <a href="https://arxiv.org/abs/1603.09320">HNSW algorithm</a> to search on cosine similarity. Results are served through a Flask API running four <a href="https://gunicorn.org/">Gunicorn</a> workers and served to a <a href="https://getbootstrap.com/">Bootstrap front-end.</a> using Flask’s ability to statically reder <a href="https://jinja.palletsprojects.com/en/3.1.x/">Jinja templates</a>. There is no Javascript dependencies internal to the project.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/tactical_app.png" width="600"></figure><p>It’s served from two <a href="https://www.digitalocean.com/products/droplets">Digital Ocean droplets</a> behind a <a href="https://www.digitalocean.com/products/load-balancer">Digital Ocean load balancer</a> and <a href="https://vicki.substack.com/p/when-you-write-a-web-server-but-you">Nginx</a>, as a Dockerized application with networking spun up through Docker compose between the web server and Redis Docker image, with data persisted to <a href="https://docs.digitalocean.com/products/volumes/">external volumes in DigitalOcean</a>, with [Digital Ocean] serving as the domain registrar and load balancer router.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/physical_arch_2.png" width="600"></figure><p>The deployable code artifact is generated through <a href="https://github.com/veekaybee/viberary/tree/main/.github/workflows">GitHub actions</a> on the main branch of the repo and then I manually refresh the docker image on the droplets through a set of Makefile commands. This all works fairly well at this scale for now.</p><h2 id="what-is-semantic-search">What is semantic search?</h2><hr><p>Viberary is a semantic search engine for books. It finds books based on ✨vibe✨. This is in contrast to traditional search engines, which work by performing lexical keyword
matching on terms like exact
keyword matches by genre, author, and title - as an example, if you type in “Nutella” into the search engine, it will try to find all documents that specifically have the word “Nutella” in the document.</p><p>Traditional search engines, including Elasticsearch/OpenSearch do this lookup efficiently by building <a href="https://en.wikipedia.org/wiki/Inverted_index">an inverted
index</a>, a data structure that creates a
key/value pair where the key is the term and the value is a collection of all the documents that match the term and performing retrieval from the inverted index. Retrieval performance from an inverted index can vary depending on how it’s implemented, but it is <code>O(1)</code> in the best case, making it an efficient data structure.</p><p>A commonc classic retrieval method from an inverted index is <a href="https://en.wikipedia.org/wiki/Okapi_BM25">BM25</a>, which is based on <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF</a> and calculates a relevance score for each element in the inverted index. The retrieval mechanism first selects all the documents with the keyword from the index, the calculates a relevance score, then ranks the documents based on the relevance score.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/inverted_index.png" width="400"></figure><p>Semantic search, in contrast, looks for near-meanings based on, as <a href="https://www.manning.com/books/ai-powered-search">“AI-Powered Search”</a> calls it, “things, not strings.” <a href="https://www.manning.com/books/relevant-search">In other words,</a></p><p>“Wouldn’t it be nice if you could search for a term like “dog” and pull back documents that contain terms like “poodle, terrier, and beagle,” even if those document happen to not use the word “dog?”</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/semantic_search.png" width="400"></figure><p>Semantic search is a vibe. A vibe can be hard to define, but generally it’s more of a feeling of association
than something concrete: a mood, a color, or a phrase. Viberary will not give you exact matches for “Nutella”, but if you type in “chocolately hazlenut goodness”, the expectation is that you’d get back Nutella, and probably also “cake” and “Ferrerro Rocher”.</p><p>Typically today, search engines will implement a number of both keyword-based and semantic approaches in a solution known as hybrid search. Semantic search includes methods like learning to rank, blending several retrieval models, query expansion which looks to enhance search results by adding synonyms to the original query, contextual search based on the user’s history and location, and vector similarity search, which looks to use NLP to help project the user’s query in a vector space.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/search_tree.png" width="600"></figure><p>The problem of semantic search is one researchers and companies have been grappling with for decades in the field known as information retrieval, which started with roots in library science. <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/334.pdf">The paper introducing Google in 1998</a> even discusses the problems with keyword-only search,</p><p>Netflix was one of the first companies that started doing vibe-based content exploration when it <a href="https://www.netflix.com/tudum/articles/netflix-secret-codes-guide">came up with a list of over 36,000
genres</a> like “Gentle British
Reality TV” and “WitchCraft and the Dark Arts” in the 2010s. They <a href="https://www.theatlantic.com/technology/archive/2014/01/how-netflix-reverse-engineered-hollywood/282679/">used large teams of people</a> to watch
movies and tag them with metadata. The process was so detailed that taggers received a 36-page document that “taught them how to rate movies on their sexually suggestive content, goriness, romance levels, and even narrative elements like plot conclusiveness.”</p><p>These labels were then incorporated into Netflix’s <a href="https://netflixtechblog.com/system-architectures-for-personalization-and-recommendation-e081aa94b5d8">recommendation architectures</a> as features for training data.</p><p>It can be easier to incorporate these kinds of features into recommendations than search because the process of recommendation is the process of implicitly learning user preferences through data about the user and offering them suggestions of content or items to purchase based on their past history, as well as the history of users across the site, or based on the properties of the content itself. As such, <a href="https://www.nngroup.com/articles/recommendation-guidelines/">recommender interfaces often include lists of suggestions</a> like <code>"you might like.."</code> or <code>"recommended for you"</code>, or <code>"because you interacted with X.."</code></p><p>Search, on the other hand, is an activity where the user expects their query to match results exactly, so users have specific expectations of modern search interfaces:</p><ol><li>They are <a href="http://glinden.blogspot.com/2006/11/marissa-mayer-at-web-20.html">extremely responsive and low-latency</a></li><li>Results are accurate and we get what we need in the first page</li><li>We use text boxes the same way <a href="https://arxiv.org/pdf/2301.08613.pdf">we have been conditioned</a> to use Google Search over the past 30 years in the SERP (search engine results page)</li></ol><p>As a result, in some ways, there is a tension between what makes traditional search interface and semantic search successful respectively, because semantic search is in that gray area between search and recommendations and traditional search expects exact results for exact queries. These are important aspects to keep in mind when designing conversational or semantic search interfaces. For more on this, <a href="https://www.theverge.com/2023/5/20/23731397/neeva-search-engine-google-shutdown">check out this recent article on Neeva.</a></p><p>Many search engines today, Google included, use a blend of traditional keyword search and semantic search to offer both direct results and related content, and with the explosion of generative AI and chat-based search and recommendation interfaces, this <a href="https://docs.google.com/presentation/d/12aoYVaqus600NEuWASw_eF9xSDXGUMzGedAftfqBCCE/edit">division is becoming even blurrier.</a></p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/searchandrec.png" width="600"></figure><h2 id="why-semantically-search-books">Why semantically search books?</h2><hr><p>I love reading, particularly fiction. I am always reading something. Check out my past reviews
<a href="https://vickiboykis.com/essays/2022-01-02-favorite-books/">2021</a>,
<a href="https://vickiboykis.com/essays/2021-04-16-favorite-books/">2020</a>, <a href="https://vickiboykis.com/essays/2020-01-01-books/">2019</a>,
and you get the idea. As a reader, I am always looking for something good to read. Often, I’ll get
recommendations by browsing sites like <a href="https://lithub.com/">LitHub</a>, but sometimes I’m in the mood for a particular
genre, or, more specifically a feeling that a book can capture. For example, after finishing <a href="https://www.richardpowers.net/the-overstory/">“The Overstory” by Richard Powers</a>, I was in the mood for more sprawling multi-generational epics
on arcane topics (I know so much about trees now!)</p><p>But you can’t find curated, quality collections of recommendations like this unless a human who reads a lot puts a list like this together. One of my favorite formats of book recommendations <a href="https://themorningnews.org/article/greetings-from-the-biblioracle">is
Biblioracle</a>, where readers
send John Warner, an extremely well-read novelist, a list of the last five books they’ve read and he recommends their next read
based on their reading preferences.</p><p>Given the recent rise in interest of semantic search and vector databases, as well as <a href="http://vickiboykis.com/what_are_embeddings/">the paper I just finished on embeddings</a>, I thought it would interesting if I could create a book search engine that gets at least somewhat close to what book nerd recommending humans can provide out of the box.</p><p>I started out by formulating the machine learning task as a recommendation problem: given that you know something about either a user or the item, can you generate a list of similar items that other users like the user has liked? We can either do this through collaborative filtering, which looks at previous user-item interactions, or content filtering, which looks purely at metadata of the items and returns similar items. Given that I have no desire to get deep into user data collection, with the exception of search queries and search query result lists, which I currently do log to see if I can fine-tune the model or offer suggestions at query time, collaborative filtering was off the table from the start.</p><p>Content-based filtering, i.e. looking at a book’s metadata rather than particular actions around a piece of content, would work well here for books. However, for content-based filtering, we also need information about the user’s preferences, which, again, I’m not storing.</p><p>What I realized is that the user would have to provide the query context to seed the recommendations, and that we don’t know anything about the user. At this point, based <a href="https://md.ekstrandom.net/blog/2015/10/search-and-recsys">on this heuristic,</a> it starts to become a search problem.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/searchrecscontext.png" width="600"></figure><p>An additional consideration was that recommendation surfaces are also traditionally rows of cards or lists that are loaded when the user is logged in, something that I don’t also don’t have and don’t want to implement from the front-end perspective. I’d like the user to be able to enter their own search query.</p><p>This idea eventually evolved into the thinking that, given my project constraints and preferences, what I had was really a semantic search problem aimed specifically at a non-personalized way of surfacing books.</p><p>After a <a href="https://vickiboykis.com/2022/11/10/how-i-learn-machine-learning/">literature search,</a>, what I found was <a href="https://arxiv.org/pdf/2006.02282.pdf">a great paper</a> that formulates the exact problem I wanted to solve, only in an ecommerce setting.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/dpsr.png" width="600"></figure><p>Their problem was more complicated in that, in addition to semantic search they also had to personalize it, and they also had to learn a model from scratch based on the data that they had, but the architecture was one that I could follow in my project, and the simplified online serving half was what I would be implementing.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/dpsr2.png" width="600"></figure><h2 id="architecting-semantic-search">Architecting Semantic Search</h2><hr><p>There are several stages to building semantic search that are related to some of the stages in <a href="https://medium.com/nvidia-merlin/recommender-systems-not-just-recommender-models-485c161c755e">a traditional four-stage recommender system</a>:</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/model_steps.png" width="600"></figure><ol><li>Data Collection</li><li>Modeling and generating embeddings</li><li>Indexing the embeddings</li><li>Model Inference, inclduing filtering</li></ol><p>and a fifth stage that’s often not included in search/recsys architectures but that’s just as important, Search/Conversational UX design.</p><p>Most <a href="https://eugeneyan.com/writing/system-design-for-discovery/">search and recommendation architectures</a> share a foundational set of commonalities that we’ve been developing for years. It’s interesting to note that <a href="https://dl.acm.org/doi/pdf/10.1145/138859.138867">Tapestry</a>, one of the first industrial recommender systems created in the 1990s to collaboratively filter emails, has an extremely similar structure to any search and recommendation system today, including components for indexing and filtering.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/tapestry.png" width="600"></figure><p>We start by collecting and processing a large set of documents. Our goal in information retrieval is to find the documents that are relevant to us, for any given definition of relevant. We update these collections of documents to be searchable at scale via an indexing function. We select a candidate set of relevant documents through either heuristics or machine learning. In our case, we do it by finding compressed numerical representations of text that are similar to the ones that we type into the query box. We generate these representations using an embedding space that’s created with deep learning models in the transformer family.</p><p>Then, once we find a candidate list of ~50 items that are potentially relevant to the query, we filter them and finally rank them, presenting them to the user through a front-end.</p><p>There are a number of related concerns that are not at all in this list but which make up the heart of machine learning projects: iteration on clean data, evaluation metrics both for online and offline testing, monitoring model performance in production over time, keeping track of model artifacts in model stores, exploratory data analysis, creating business logic for filtering rules, user testing, and much, much more. In the interest of time, I decided to forgo some of these steps as long as they made sense for the project.</p><h2 id="project-architecture-decisions">Project Architecture Decisions</h2><hr><p>Given this architecture and my time constraints, I constrained myself in several ways on this project. First, I wanted to a project that was well-scoped and had a UI component so that I was incentivized to ship it, because the worst ML project is the one that remains unshipped. <a href="https://mitchellh.com/writing/building-large-technical-projects">As Mitch writes</a>, you have an incentive to move forward if you have something tangible to show to yourself and others.</p><p>Second, I wanted to explore new technologies while also being careful of not wasting <a href="https://mcfunley.com/choose-boring-technology">my innovation tokens</a>. In other words, I wanted to <a href="https://normconf.com/">build something normcore</a>, i.e. using the right tool for the right job, and <a href="https://vicki.substack.com/p/you-dont-need-kafka">not going overboard.</a>. I wasn’t going to start with LLMs or Kubernetes or Flink or MLOps. I was going to start by writing simple Python classes and adding where I needed to as pain points became evident.</p><p>The third factor was to try to ignore <a href="https://vickiboykis.com/2022/11/10/how-i-learn-machine-learning/">the hype blast of the current ML ecsystem</a>, which comes out with a new model and a new product and a new wrapper for the model for the product every day. It wasn’t easy. It is extremely hard to ignore the noise and just build, particularly given all the discourse around LLMs and now in society at large.</p><p>Finally, I wanted to build everything as a traditional self-contained app with various components that were <a href="https://vickiboykis.com/2023/06/29/naming-things/">easy to understand</a>, and reusable components across the app. The architecture as it stands looks like this:</p><p>I wish I could say that I planned all of this out in advance, and the project that I eventually shipped was exactly what I had envisioned. But, like with any engineering effort, I had a bunch of false starts and dead ends. I started out <a href="https://vickiboykis.com/2022/12/05/the-cloudy-layers-of-modern-day-programming/">using Big Cloud</a>, a strategic mistake that cost me a lot of time and frustration because I couldn’t easily introspect the cloud components. This slowed down development cycles. I eventually moved to local data processing using DuckDB, but <a href="https://vickiboykis.com/2023/01/17/welcome-to-the-jungle-we-got-fun-and-frames/">it still look a long time to make this change and get to data understanding</a>, as is typically the case in any data-centric project.</p><p>Then, I spent a long time <a href="https://github.com/veekaybee/viberary/releases/tag/v0.0.1">working through creating baseline models in Word2Vec</a> so I could get some context for baseline text retrieval methods in the pre-Transformer era. Finally, in going from local development to production, I hit <a href="https://vickiboykis.com/2023/07/18/what-we-dont-talk-about-when-we-talk-about-building-ai-apps/">a bunch of different snags</a>, most of them related to making Docker images smaller, thinking about the size of the machine I’d need for infrence, Docker networking, load testing traffic, and, a long time on correctly routing Nginx behind a load balancer.</p><p>Generally, though, I’m really happy with this project, <a href="https://normconf.com/">guided by the spirit of Normconf</a> and all the great normcore ML engineering ideas <a href="https://vickiboykis.com/2022/12/22/everything-i-learned-about-accidentally-running-a-successful-tech-conference/">I both put in and took away from</a> people in the field looking to build practical solutions.</p><h2 id="tech-stack">Tech Stack</h2><hr><p>My project tech stack, as it now stands is primarily Python developed in <a href="https://gifted-bohr-74bf66.netlify.app/">virtual environments</a> with <code>requirements.txt</code> with:</p><ul><li>Original data in <strong>gzipped JSON</strong> files hosted locally not under version control</li><li>These files are rrocessed using the Python client for <strong>DuckDB</strong></li><li>Encoding of documents into model embeddings with <strong>SBERT</strong>, <a href="https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search">specifically the MS-Marco Asymmetric model</a></li><li>A <strong>Redis</strong> instance that indexes the embeddings into a special search index for retrieval</li><li>A <strong>Flask</strong> API that has a search query route that encodes the query with the same MSMarco model and then runs <strong>HNSW</strong> lookup in realtime against the Redis search index</li><li>A <strong>Bootstrap UI</strong> that returns the top 10 ranked results</li><li>Redis and Flask encapsulated in a networked <strong>docker compose</strong> configuration via <strong>Dockerfile</strong>, depending on the architecture (arm or AMD)</li><li>a <strong>Makefile</strong> that does a bunch of routine things around the app like reindexing the embeddigns and bringing up the app</li><li><strong>Nginx</strong> on the hosting server to reverse-proxy requests from the load balancer</li><li><strong>pre-commit</strong> for formatting and linting</li><li><strong>Locust</strong> for load testing</li><li>a logging module for capturing queries and outputs</li><li>and tests in <strong>pytest</strong></li></ul><hr><ul><li>PyCharm for development, <a href="https://www.jetbrains.com/help/pycharm/docker.html">including in Docker via bind mounts</a></li><li>iterm2</li><li>VSCode for specifically writing the documentation, it’s nicer than PyCharm for this</li><li><a href="https://whimsical.com/">Whimsical for charts</a></li><li>Docker Desktop for Mac (considered briefly switching to Podman but haven’t yet)</li></ul><h2 id="training-data">Training Data</h2><hr><p>The original book data comes from <a href="https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/books">UCSD Book Graph</a>, which scraped it from Goodreads for research papers in 2017-2019.</p><p>The data is stored in several gzipped-JSON files:</p><ul><li><a href="https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/books">books</a> - detailed meta-data about 2.36M books</li><li><a href="https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/reviews?authuser=0">reviews</a> - Complete 15.7m reviews (~5g):15M records with detailed review text</li></ul><p>Sample row: Note these are all encoded as strings!</p><p>There is a lot of good stuff in this data! So, like any good data scientist, I initially <a href="https://github.com/veekaybee/viberary/blob/main/src/notebooks/03_duckdb_eda.ipynb">did some data exploration</a> to get a feel for the data I had at hand. I wanted to know how full the dataset was, how many missing data I had, what language most of the reviews are in, and other things that will help understand what the model’s embedding space looks like.</p><p>The data input generally looks like this:</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/input_data.png" width="600"></figure><p>Then, I constructed several tables that I’d need to send to the embeddings model to generate embeddings for the text. I did this all in DuckDB. The final relationships between the tables look like this:</p><p>The <code>sentence</code> column which concatenates <code>review_text || goodreads_auth_ids.title || goodreads_auth_ids.description</code> is the most important because it’s this one that is used as a representation of the document to the embedding model and the one we use to generate numerical representations and look up similarity between the input vector.</p><p>There are a couple of things to note about the data. First, it’s from 2019 so the recency on the recommendations from the data won’t be great, but it should do fairly well on classical books. Second, since <a href="https://debugger.medium.com/goodreads-is-retiring-its-current-api-and-book-loving-developers-arent-happy-11ed764dd95">Goodreads no longer has an API</a>, it’s impossible to get this updated in any kind of reasonable way. It’s possible that future iterations of Viberary will use something like <a href="https://openlibrary.org/">Open Library</a>, but this will involve a lot of foundational data work. Third, there is a strong English-language bias in this data, which means we might not be able to get good results in other languages at query time if we want to make Viberary international.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/eng.png" width="600"></figure><p>Finally, in looking at the data available per column, it looks like we have a pretty full set of data available for author, title, ratings, and description (lower percent means less null values per column) which means we’ll be able to use most of our data for representing the corpus as embeddings.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/percent_data.png" width="600"></figure><h2 id="the-model">The Model</h2><hr><p><strong>If you want to understand more of the context behind this section, read <a href="https://raw.githubusercontent.com/veekaybee/what_are_embeddings/main/embeddings.pdf">my embeddings paper.</a></strong></p><p>Viberary uses <a href="https://www.sbert.net/">Sentence Transformers</a>, a modified version of <a href="https://jalammar.github.io/illustrated-bert/">the BERT architecture</a> that reduces computational overhead for deriving embeddings for sentence pairs <a href="https://arxiv.org/pdf/1908.10084.pdf">in a much more operationally efficient way</a> than the original BERT model, making it easy to generate sentence-level embeddings that can be compared relatively quickly using cosine similarity.</p><p>This fits our use case because our input documents are several sentences long, and our query will be a keyword like search of at most 10 or 11 words, much like a short sentence.</p><p>BERT stands for Bi-Directional Encoder and was released 2018, based on a paper written by Google as a way to solve common natural language tasks like sentiment analysis, question-answering, and text summarization. BERT is a transformer model, also based on the attention mechanism, but its architecture is such that it only includes the encoder piece. Its most prominent usage is in Google Search, where it’s the algorithm powering surfacing relevant search results. In the blog post they released on including BERT in search ranking in 2019, Google specifically discussed adding context to queries as a replacement for keyword-based methods as a reason they did this. BERT works as a masked language model, which means it works by removing words in the middle of sentences and guessing the probability that a given word fills in the gap. The B in Bert is for bi- directional, which means it pays attention to words in both ways through scaled dot-product attention. BERT has 12 transformer layers. It uses WordPiece, an algorithm that segments words into subwords, into tokens. To train BERT, the goal is to predict a token given its context, or the tokens surrounding it.</p><p>The output of BERT is latent representations of words and their context — a set of embeddings. BERT is, essentially, an enormous parallelized Word2Vec that remembers longer context windows. Given how flexible BERT is, it can be used for a number of tasks, from translation, to summarization, to autocomplete. Because it doesn’t have a decoder component, it can’t generate text, which paved the way for GPT models to pick up where BERT left off.</p><p>However, this architecture doesn’t work well for parallelizing sentence similarity, which is where sentence-transformers comes in.</p><p>Given a sentence, <code>a</code>, and a second sentence, <code>b</code>, from an input, upstream model with BERT or similar variations as its source data and model weights, we’d like to learn a model whose output is a similarity score for two sentences. In the process of generating that score, the intermediate layers of that model give us embeddings for subsentences and words that we can then use to encode our query and corpus and do semantic similarity matching.</p><p>Given two input sentences, we pass them through the sentence transformer network and uses mean-pooling (aka averaging) all the embeddings of words/subwords in the sentence, then compares the final <a href="https://github.com/UKPLab/sentence-transformers/blob/master/docs/training/overview.md">embedding using cosine similarity, a common distance measure that performs well for multidimensional vector spaces</a></p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/pooling.png" width="600"></figure><p>Sentence Transformers has a number of pre-trained models that are on this architecutre, the most common of which is <code>sentence-transformers/all-MiniLM-L6-v2</code>, which <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">maps sentences and paragraphs</a> into a 384-dimension vector space. This means that each sentence is encoded in a vector of 384 values.</p><p>The initial results of this model were <a href="https://github.com/veekaybee/viberary/releases/tag/v.0.0.4">just so-so</a>, so I had to decide whether to use a different model or tune this one. The different model I considered was the series of <a href="https://www.sbert.net/docs/pretrained-models/msmarco-v3.html">MSMarco models</a> , which were trained based on sample Bing searches. This was closer to what I wanted. Additionally, <a href="https://www.sbert.net/examples/applications/semantic-search/README.html">the search task was asymmetric</a>, which meant that the model accounted for the fact that the corpus vector would be longer than the query vector.</p><p>I chose <code>msmarco-distilbert-base-v3</code>, which is middle of the pack in terms of performance, and critically, is also tuned for cosine similarity lookups, instead of dot product, another similarity measure that takes into account both magnitude and direction. Cosine similarity only considers direction rather than size, making cosine similarity more suited for information retrieval with text because it’s not as affected by text length, and additionally, it’s more efficent at handling sparse representations of data.</p><p>There was a problem, however, because the vectors for this series of models was twice as long, at <code>768</code> dimensions per embedding vector. The longer a vector is, the more computationally intensive it is to work with, increasing, with the runtime and the memory requirement grows quadratic with the input length. However, the longer it is, the more information about the original input it compresses, so there is always a fine-lined tradeoff between being able to encode more information and faster inference, which is critical in search applications.</p><p>Learning embeddings was tricky not only in selecting the correct model, but also because everyone in the entire universe is using GPUs right now.</p><p>I first tried Colab, but soon found that, even at the paid tier, my instances would mysteriously get shut down or downgraded, particularly on Friday nights, when everyone is doing side projects.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/colab.png" width="600"></figure><p>I then tried Paperspace but found its UI hard to navigate, although, ironically, recently it’s been purchased by Digital Ocean which I always loved and have become even more a fan of over the course of this project. I settled on doing the training on AWS since I already have an account and, in doing PRs for PyTorch, <a href="https://vickiboykis.com/2022/07/26/how-to-prepare-an-aws-test-image-for-pytorch/">had already configured EC2 instances for deep learning.</a></p><p>The process turned out to be much less painless than I anticipated, with the exception that P3 instances run out very quickly due to everyone training on them. <a href="https://github.com/veekaybee/viberary/blob/main/src/model/generate_embeddings.ipynb">But it only took about 20 minutes to generate embeddings for my model</a>, which is a really fast feedback loop as far as ML is concerned. I then wrote that data out to a snappy-compressed parquet file that I then load manually to the server where inference is performed.</p><h2 id="redis-and-indexing">Redis and Indexing</h2><hr><p>Once I learned embeddings for the model, I needed to store them somewhere for use at inference time. Once the user inputs a query, that query is transformed also into an embedding representation using the same model, and then the KNN lookup happens. There are about <a href="https://thenewstack.io/vector-databases-long-term-memory-for-artificial-intelligence/">five million options now for storing embeddings</a> for all kinds of operations.</p><p>Some are better, some are worse, it all depends on your criteria. Here were my criteria:</p><ul><li>an existing technology I’d worked with before</li><li>something I could host on my own and introspect</li><li>something that provided blazing-fast inference</li><li>a software package where the documentation tells you <code>O(n)</code> performance time of <a href="https://redis.io/docs/data-types/hashes/">all its constitutent data structures</a></li></ul><p>I’m kidding about the last one but it’s one of the things I love about the Redis documentation. Since I’d previously worked with Redis as a cache, already knew it to be highly reliable and relatively simple to use, as well as plays well with high-traffic web apps and available packaged in Docker, which I would need for my next step to production, I went with <a href="https://redis.io/docs/interact/search-and-query/">Redis Search</a>, which offers storage and inference out of the box, as well as frequently updated Python modules.</p><p>Redis Search is an add-on to Redis that you can load as part of the <a href="https://github.com/RediSearch/RediSearch">redis-stack-server Docker image</a>.</p><p>It offers vector similarity search by indexing vectors stored as fields in Redis hash data structures, which are just field-value pairs like you might see in a dictionary or associative array. The common Redis commands for working with hashes are <code>HSET</code> and <code>HGET</code>, and <a href="https://github.com/veekaybee/viberary/blob/main/src/index/indexer.py">we can first HSET our embeddings</a> and then create an index with a schema on top of them. An important point is that we only want to <a href="https://github.com/veekaybee/viberary/blob/main/src/index/index_embeddings.py">create the index schema after we <code>HSET</code> the embeddings</a>, otherwise performance degrades significantly.</p><p>For our learned embeddings which encompass ~800k documents, this process takes about ~1 minute.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/indexing.png" width="600"></figure><h2 id="lookups-and-requestresponse">Lookups and Request/Response</h2><hr><p>Now that we have the data in Redis, we can perform lookups within the request-response cycle. The process looks like this:</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/request_response.png" width="600"></figure><p>Since we’ll be doing this in the context of a web app, we write a small <a href="https://github.com/veekaybee/viberary/tree/main/src/api">Flask application</a> that has several routes and captures the associated static files of the home page, the search box, and images, and takes a user query, runs it through the created search index object after cleaning the query, and returns a result:</p><p>that data gets passed into the model through a KNN Search object which takes a Redis connection and a config helper object:</p><p>The <a href="https://github.com/veekaybee/viberary/blob/main/src/search/knn_search.py#L13">search class</a> is where most of the real work happens. First, the user query string is parsed and sanitized, although in theory, in BERT models, you should be able to send the text as-is, since BERT was originally trained on data that does not do text clean-up and parsing, like traditional NLP does.</p><p>Then, that data is rewritten into the Python dialect for the Redis query syntax. The search syntax is can be a little hard to work with originally, both in the Python API and on the Redis CLI, so I spent a lot of time playing around with this and figuring out what works best, as well as tuning the hyperparameters passed in <a href="https://github.com/veekaybee/viberary/blob/9f55493e0c8f77c0727df9c0e9191033469e468a/config.yml#L24">from the config file</a>, such as the number of results, the vector size, and the float type (very important to make sure all these hyperparameters are correct given the model and vector inputs, or none of this works correctly.)</p><p><a href="https://github.com/RediSearch/RediSearch">HNSW is the algorithm, initially written at Twitter, implemented in Redis</a> that actually peforms the query to find <a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods">approximate nearest neighbors</a> based on cosine similarity. <a href="https://arxiv.org/abs/1603.09320">It looks for an approximate solution</a> to the k-nearest neighbors problem by formulating nearest neighbors as a graph search problem to be able to find nearest neighbors at scale. Naive solutions here would mean comparing each element to each other element, a process which computationally scales linearly with the number of elements we have. HNSW bypasses this problem by using skip list data structures to create multi-level linked lists to keep track of nearest neighbors. During the navigation process, HNSW traverses through the layers of the graph to find the shortest connections, leading to finding the nearest neighbors of a given point.</p><p>It then returns the closest elements, ranked by cosine similarity. In our case, it returns the document whose 768-dimension vector most closely matches the 768-dimension vector generated by our model at query time.</p><p>The final piece of this is filtering and ranking. We sort by cosine similarity descending, but then also by the number of reviews - we want to return not only books that are relevant to the query, but books that are high-quality, where number of reviews is (questionably) a proxy for the fact that people have read them. If we wanted to experiment with this, we could return by cosine similarity and then by nubmer of stars, etc. There are numerous ways to fine-tune.</p><h2 id="getting-the-ui-right">Getting the UI Right</h2><p>Once we get the results from the API, we get back is a list of elements that include the title, author, cosine similarity, and link to the book. It’s now our job to present this to the user, and to give them confidence that these are good results. Additionally, the results should be able to prompt them to build a query.</p><p>Research has found, and perhaps your personal experience has proven, that it’s hard to stare into a text box and know what to search for, particularly if
the dataset is new to you. Additionally, <a href="https://arxiv.org/ftp/arxiv/papers/2307/2307.01135.pdf">the UX of the SERP page matters greatly.</a> That’s why generative AI products, such as Bard and OpenAI often have prompts or ideas of how to use that open-ended search box.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/gpt.png" width="600"></figure><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/bard.png" width="600"></figure><p>The hard part for me was in getting users to understand how to write a successful vibe query that focused on semantic rather than direct search. I started out with a fairly simple results page that had the title and the rank of the results.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/viberary/main/assets/cats.png" width="600"></figure><p>It became clear that this was not satisfactory: there was no way to reference the author or to look up the book, and the ranking was confusing, particularly to non-developers who were not used to zero indexing. I then iterated to including the links to the books so that people could introspect the results.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/vibe2.jpeg" width="600"></figure><p>I removed the ranking because it felt more confusing and took up more computational power to include it, and additionally people generally understand that best search results are at the top. Finally, I added button suggestions for types of queries to write. I did this by looking at the list of Netflix original categories to see if I could create some of my own, and also by asking friends who had tested the app.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/netflix_cats.png" width="600"></figure><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/vibe3.jpeg" width="600"></figure><p>On top of all of this, I worked to make the site load quickly both on web and mobile, since most people are mobile-first when accessing sites in 2023. And finally, I changed the color to a lighter pink to be more legible. This concludes the graphic design is my passion section of this piece.</p><h2 id="digitalocean-docker-and-production">DigitalOcean, Docker, and Production</h2><hr><p>Now that this all worked in a development environment, it was time to scale it for production. My top requirements included being able to develop locally quickly and reproduce that environment almost exactly on my production instances, a fast build time for CI/CD and for Docker images, the ability to horizontally add more nodes if I needed to but <a href="https://www.youtube.com/watch?app=desktop&amp;v=9BXMWDXiugg">not mess with autoscaling or complicated AWS solutions</a>, and <a href="https://www.youtube.com/watch?v=kx-SeGbkNPU&amp;list=PLYXaKIsOZBsu3h2SSKEovRn7rGy7wkUAV&amp;index=5">smaller Docker images than is typical</a> for
AI apps, <a href="https://vickiboykis.com/2023/07/18/what-we-dont-talk-about-when-we-talk-about-building-ai-apps/">which can easily balloon to 10 GB with Cuda GPU-based layers.</a>. Since my dataset is fairly small and the app itself worked fairly well locally, I decided to stick with CPU-based operations for the time being, at least until I get to a volume of traffic where it’s a problem.</p><p>Another concern I had was that, halfway through the project (never do this), I got a new Macbook M2 machine, which meant <a href="https://www.youtube.com/watch?v=I4wkCSd7iMM">a whole new world of pain</a> in shipping code consistently between <code>arm</code> and <code>intel</code> architectures.</p><p>My deployment story works like this. The web app is developed in a Docker container that I have symlinked via bind mounts to my local directory so that I write code in PyCharm and changes are reflected in the Docker container. The web docker container is networked to Redis via Docker’s internal network. The web app is available at 8000 on the host machine, and, in production in Nginx, proxies port 80 so we can reach the main domain without typing in ports and hit Viberary. In the app dockerfile, I want to make sure to have the fastest load time possible, so I follow <a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/">Docker best practices</a> of having the layers that change the most last, caching, and mounting files into the Docker image so I’m not constantly copying data.</p><p>The docker image base for the web is <code>bitnami:pytorch</code> and it installs requirements via <code>requirements.txt</code></p><p>I have two Dockerfiles, one local and one for production. The production is linked from the <code>docker-compose</code> file and correctly builds on the Digital Ocean server. The local one is linked from the <code>docker-compose.override</code> file, which is excluded from version control, but which works only locally, so that each environment gets the proper build directives.</p><p>The Docker compose takes this Dockerfile and networks it to the Redis container.</p><p>All of this is run through a Makefile that has commands to build, serve, spin down, and run onnx model creation from the root of the directory. Once I’m happy with my code, I push a branch to GitHub where github actions runs basic tests and linting on code that should, in theory, already be checked since I have <code>precommit</code> set up. <a href="https://github.com/veekaybee/viberary/blob/main/.pre-commit-config.yaml">The pre-commit hook</a> lints it and cleans everything up, including black, ruff, and isort, before I even push to a branch.</p><p>Then, once the branch passes, I merge into main. The main branch does tests and pushes the latest git commit to the Digital Ocean server. I then manually go to the server, bring down the old docker image and spin up the new one, and the code changes are live.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/deploy.png" width="600"></figure><p>Finally, on the server, I have a very scientific shell script that helps me configure each additional machine. Since I only needed to do two, it’s fine that it’s fairly manual at the moment.</p><p>Finally everything is routed to port 80 via nginx, which I configured on each DigitalOcean droplet that I created. I load balanced two droplets behind a load balancer, pointing to the same web address, a domain I bought from Amazon’s Route 53. I eventually had to transfer the domain to Digital Ocean, because it’s easier to manage SSL and HTTPS on the load balancer when all the machines are on the same provider.</p><p>{% gist fc6a1b345c82ec4967e9dc3c4d8bba4f %}</p><p>Now, we have a working app. The final part of this was load testing, which I did with <a href="https://locust.io/">Python’s Locust library</a>, which provides a nice interface for running any type of code against any endpoint that you specify. One thing that I realized as I was load testing was that my model was slow, and search expects instant results, so I converted it to an <a href="https://blog.vespa.ai/stateful-model-serving-how-we-accelerate-inference-using-onnx-runtime/">ONNX artifact</a> and had to change the related code, as well.</p><figure><img src="https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/main/static/images/locust.jpeg" width="600"></figure><p>Finally, I wrote a small logging module that propogates across the app and keeps track of everything in the docker compose logs.</p><h2 id="key-takeaways">Key Takeaways</h2><ul><li><p><strong>Getting to a testable prototype is key</strong>. I did all my initial exploratory work locally in Jupyter notebooks, <a href="https://github.com/veekaybee/viberary/blob/main/src/notebooks/05_duckdb_0.7.1.ipynb">including working with Redis</a>, so I could see the data output of each cell. I <a href="https://vickiboykis.com/2021/11/07/the-programmers-brain-in-the-lands-of-exploration-and-production/">strongly believe</a> working with a REPL will get you the fastest results immediately. Then, when I had a strong enough grasp of all my datatypes and data flow, I immediately moved the code into object-oriented, testable modules. Once you know you need structure, you need it immediately because it will allow you to develop more quickly with reusable, modular components.</p></li><li><p><strong>Vector sizes and models are important</strong>. If you don’t watch your hyperparameters, if you pick the wrong model for your given machine learning task, the results are going to be bad and it won’t work at all.</p></li><li><p><strong>Don’t use the cloud if you don’t have to</strong>. I’m using DigitalOcean, which is really, really, really nice for medium-sized companies and projects and is often overlooked over AWS and GCP. I’m very versant in cloud, but it’s nice to not have to use BigCloud if you don’t have to and to be able to do a lot more with your server directly. DigitalOcean has reasonable pricing, reasonable servers, and a few extra features like monitoring, load balancing, and block storage that are nice coming from BigCloud land, but don’t overwhlem you with choices. They also recently acquired <a href="https://www.paperspace.com/">Paperspace</a>, which I’ve used before to train models, so should have GPU integration.</p></li><li><p><strong>DuckDB</strong> is becoming a stable tool for work up to 100GB locally. There are a lot of issues that still need to be worked out because it’s a growing project. For example, for two months I couldn’t use it for my JSON parsing because it didn’t have regex features that I was looking for, which were added in 0.7.1, so use with caution. Also, since it’s embedded, you can only run one process at a time which means you can’t run both command line queries and notebooks. But it’s a really neat tool for quickly munging data.</p></li><li><p><strong>Docker still takes time</strong> I spent a great amount of time on Docker. Why is Docker different than my local environment? How do I get the image to build quickly and why is my image now 3 GB? What do people do with CUDA libraries (exclude them if you don’t think you need them initially, it turns out). I spent a lot of time making sure this process worked well enough for me to not get frustrated rebuilding hundreds of times. Relatedly, <strong>Do not switch laptop architectures in the middle of a project</strong> .</p></li><li><p><strong>Deploying to production is magic</strong>, even when you’re a very lonely team of one, and as such <a href="https://vickiboykis.com/2021/06/20/the-ritual-of-the-deploy/">is filled with a lot of unknown variables</a>, so make your environments as absolutely reproducible as possible.</p></li></ul><p>And finally,</p><ul><li>True semantic search is very hard and involves a lot of algorithmic fine-tuning, both in the machine learning, and in the UI, and in deployment processes. People have been fine-tuning Google for years and years. Netflix had thousands of labelers. <a href="https://vicki.substack.com/p/what-we-talk-about-when-we-talk-about">Each company has teams of engineers working on search and recommendations</a> to steer the algorithms in the right direction. Just take a look at the company formerly known as Twitter’s algo stack. It’s fine if the initial results are not that great.</li></ul><p>The important thing is to keep benchmarking the current model against previous models and to keep iterating and keep on building.</p><h2 id="citations">Citations</h2><h2 id="resources">Resources</h2><ul><li><a href="https://www.manning.com/books/relevant-search">Relevant Search by Turnbull and Berryman</a></li><li><a href="https://corise.com/course/search-fundamentals">Corise Search Course</a> and Search with Machine Learning - I’ve taken these, and have nothing to sell except the fact that Grant and Daniel are aweosme. <a href="https://github.com/gsingers/search_fundamentals_course">Code is here.</a></li><li><a href="https://vickiboykis.com/what_are_embeddings/">What Are Embeddings</a> - during the process of writing this I came up with a lot of sources included in the site and bibliography</li><li><a href="https://arxiv.org/abs/2006.02282">Towards Personalized and Semantic Retrieval: An End-to-End Solution for E-commerce Search via Embedding Learning</a></li><li><a href="https://arxiv.org/pdf/2010.06467.pdf">Pretrained Transformers for Text Ranking: BERT and Beyond</a></li><li><a href="https://www.youtube.com/playlist?list=PLSg1mducmHTPZPDoal4m59pPxxsceXF-y">Advanced IR Youtube Series</a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Libwebsockets: pure C library for http, websockets, MQTT (165 pts)]]></title>
            <link>https://github.com/warmcat/libwebsockets</link>
            <guid>38896096</guid>
            <pubDate>Sat, 06 Jan 2024 22:17:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/warmcat/libwebsockets">https://github.com/warmcat/libwebsockets</a>, See on <a href="https://news.ycombinator.com/item?id=38896096">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a href="https://libwebsockets.org/git/libwebsockets" rel="nofollow"><img src="https://camo.githubusercontent.com/cc104d8b14cf92dc75824b09daacec37af1b696e2c63ccc80f7ede1bfec5b9a6/68747470733a2f2f6c6962776562736f636b6574732e6f72672f7361692f7374617475732f6c6962776562736f636b657473" alt="CI status" data-canonical-src="https://libwebsockets.org/sai/status/libwebsockets"></a> <a href="https://scan.coverity.com/projects/3576" rel="nofollow"><img src="https://camo.githubusercontent.com/27f2ec5ff7550588149f273c9ebe15356186ad5da1403efd8a68cb6c43280b2d/68747470733a2f2f7363616e2e636f7665726974792e636f6d2f70726f6a656374732f333537362f62616467652e737667" alt="Coverity Scan Build Status" data-canonical-src="https://scan.coverity.com/projects/3576/badge.svg"></a> <a href="https://bestpractices.coreinfrastructure.org/projects/2266" rel="nofollow"><img src="https://camo.githubusercontent.com/baa021ad8613d12b1efcee900fdabeb3b308742ebe6513e5963b42e52d378858/68747470733a2f2f626573747072616374696365732e636f7265696e6672617374727563747572652e6f72672f70726f6a656374732f323236362f6261646765" alt="CII Best Practices" data-canonical-src="https://bestpractices.coreinfrastructure.org/projects/2266/badge"></a> <a href="https://www.codacy.com/app/lws-team/libwebsockets?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=warmcat/libwebsockets&amp;utm_campaign=Badge_Grade" rel="nofollow"><img src="https://camo.githubusercontent.com/e8674ede74f29075a080b65e4870e98ea2875230ed56fdd8946f14824ac413c9/68747470733a2f2f6170692e636f646163792e636f6d2f70726f6a6563742f62616467652f47726164652f3134346662313935613833303436653438346137356338623463366366633939" alt="Codacy Badge" data-canonical-src="https://api.codacy.com/project/badge/Grade/144fb195a83046e484a75c8b4c6cfc99"></a> <a href="https://lgtm.com/projects/g/warmcat/libwebsockets/alerts/" rel="nofollow"><img src="https://camo.githubusercontent.com/2166be031424fb4ccffcb441f56d15c9f7fbc041cd6908d95abb6a057e84a6c3/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f616c657274732f672f7761726d6361742f6c6962776562736f636b6574732e7376673f6c6f676f3d6c67746d266c6f676f57696474683d3138" alt="Total alerts" data-canonical-src="https://img.shields.io/lgtm/alerts/g/warmcat/libwebsockets.svg?logo=lgtm&amp;logoWidth=18"></a> <a href="https://lgtm.com/projects/g/warmcat/libwebsockets/context:cpp" rel="nofollow"><img src="https://camo.githubusercontent.com/3926ba0b7d53bcf6803d58e0b015d571ac226aa900b65df49df1444a079006c9/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f6370702f672f7761726d6361742f6c6962776562736f636b6574732e7376673f6c6f676f3d6c67746d266c6f676f57696474683d3138" alt="Language grade: C/C++" data-canonical-src="https://img.shields.io/lgtm/grade/cpp/g/warmcat/libwebsockets.svg?logo=lgtm&amp;logoWidth=18"></a> <a href="https://lgtm.com/projects/g/warmcat/libwebsockets/context:javascript" rel="nofollow"><img src="https://camo.githubusercontent.com/a09fcd112f9f0510b98371a40fdbe8522cfc34503789421d78db50e9fd305d9f/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f6a6176617363726970742f672f7761726d6361742f6c6962776562736f636b6574732e7376673f6c6f676f3d6c67746d266c6f676f57696474683d3138" alt="Language grade: JavaScript" data-canonical-src="https://img.shields.io/lgtm/grade/javascript/g/warmcat/libwebsockets.svg?logo=lgtm&amp;logoWidth=18"></a></p>
<h2 tabindex="-1" dir="auto">Libwebsockets</h2>
<p dir="auto">Libwebsockets is a simple-to-use, MIT-license, pure C library providing client and server
for <strong>http/1</strong>, <strong>http/2</strong>, <strong>websockets</strong>, <strong>MQTT</strong> and other protocols in a security-minded,
lightweight, configurable, scalable and flexible way.  It's easy to build and
cross-build via cmake and is suitable for tasks from embedded RTOS through mass
cloud serving.</p>
<p dir="auto">It supports a lot of lightweight ancilliary implementations for things like JSON,
CBOR, JOSE, COSE, and supports OpenSSL and MbedTLS v2 and v3 out of the box for everything.
It's very gregarious when it comes to event loop sharing, supporting libuv, libevent, libev,
sdevent, glib and uloop, as well as custom event libs.</p>
<p dir="auto"><a href="https://libwebsockets.org/git/libwebsockets/tree/minimal-examples" rel="nofollow">100+ independent minimal examples</a> for various scenarios, CC0-licensed
(public domain) for cut-and-paste, allow you to get started quickly.</p>
<p dir="auto"><a href="https://libwebsockets.org/git/libwebsockets/tree/READMEs" rel="nofollow">There are a lot of READMEs</a> on a variety of topics.</p>
<p dir="auto"><a href="https://libwebsockets.org/sai/" rel="nofollow">We do a huge amount of CI testing per push</a>, currently 582 builds on 30 platforms.
<a href="https://warmcat.com/2021/08/21/Sai-CI.html" rel="nofollow">You can see the lws CI rack and read about how lws-based Sai is used to coordinate all the testing</a>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/warmcat/libwebsockets/blob/main/doc-assets/lws-overview.png"><img src="https://github.com/warmcat/libwebsockets/raw/main/doc-assets/lws-overview.png" alt="overview"></a></p>
<h2 tabindex="-1" dir="auto">News</h2>
<h2 tabindex="-1" dir="auto">HTML + CSS + JPEG + PNG display stack in lws</h2>
<p dir="auto">Want to drive your EPD or TFT / OLED display using HTML + CSS?  Only got an ESP32?</p>
<p dir="auto">Want remote JPEGs, PNGs, HTML, RGBA composition, gamma, error diffusion if needed?</p>
<p dir="auto">Realtime render into a line buffer because you don't have enough heap for a framebuffer?</p>
<p dir="auto"><a href="https://libwebsockets.org/git/libwebsockets/tree/READMEs/README.html-parser.md" rel="nofollow">Take a look here...</a></p>
<h2 tabindex="-1" dir="auto">Perl binding for lws available</h2>
<p dir="auto">Thanks to Felipe Gasper, there's now a <a href="https://metacpan.org/pod/Net::Libwebsockets" rel="nofollow">perl binding for lws available at metacpan</a>,
this uses the recent generic event loop support in lws to have lws as a guest on an existing perl event loop.</p>
<h2 tabindex="-1" dir="auto">Lws examples switching to Secure Streams</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/warmcat/libwebsockets/blob/main/doc-assets/ss-api1.png"><img src="https://github.com/warmcat/libwebsockets/raw/main/doc-assets/ss-api1.png" alt="Secure Streams direct"></a></p>
<p dir="auto"><strong>Secure Streams</strong> support in lws was introduced a couple of years ago, it's a
higher-level interface to lws <code>wsi</code>-level apis that simplifies connectivity by
segregating connection policy like protocol and endpoint information into a
separate <a href="https://github.com/warmcat/libwebsockets/blob/main/minimal-examples/client/hello_world/example-policy.json">JSON policy file</a>, and just having the <a href="https://github.com/warmcat/libwebsockets/blob/main/minimal-examples/clients/hello_world/hello_world-ss.c">code deal with payloads</a>; as many
details of the wire protocol as possible are hidden or moved to the policy, so
user code is almost identical even if the wire protocol changes.</p>
<p dir="auto">The user code just asks to create a SS by "streamtype name", it is created
according to the details (protocol, endpoint, etc) under the same name in the
policy.</p>
<p dir="auto">Key policy entries like endpoint can contain <code>${metadata-name}</code> string
substitutions to handle runtime adaptations via metadata.  h1, h2, ws and mqtt
are supported.</p>
<p dir="auto">As a layer on top of the <code>wsi</code> apis, SS provides a higher-level way to access
the existing wsi-level capabilities, both kinds of API will remain supported.
Secure Streams are longer-lived than a single wsi, so an SS can coordinate
retries by itself.  SS-based user code is typically significantly smaller and
more maintainable than wsi layer.</p>
<p dir="auto">In main branch I have moved the older examples into <code>./minimal-examples-lowlevel</code>
and am starting to port more cases from there into SS-based examples.</p>
<h3 tabindex="-1" dir="auto">Comparison between wsi and SS level lws usage</h3>
<table>
<thead>
<tr>
<th>Feature</th>
<th>"low-level" wsi way</th>
<th>Secure Streams way</th>
</tr>
</thead>
<tbody>
<tr>
<td>Create context</td>
<td>code</td>
<td>same</td>
</tr>
<tr>
<td>Loop support, sul scheduler</td>
<td>default, event libs</td>
<td>same</td>
</tr>
<tr>
<td>Supports comms mode</td>
<td>Client, Server, Raw</td>
<td>same</td>
</tr>
<tr>
<td>Supports protocols</td>
<td>h1, h2, ws, mqtt (client)</td>
<td>same</td>
</tr>
<tr>
<td>TLS support</td>
<td>mbedtls (including v3), openssl (including v3), wolfssl, boringssl, libressl</td>
<td>same</td>
</tr>
<tr>
<td>Serializable, proxiable, muxable, transportable</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Auto-allocated per-connection user object</td>
<td>pss specified in lws_protocols</td>
<td>Specified in ss info struct</td>
</tr>
<tr>
<td>Connection User API</td>
<td>Protocol-specific lws_protocols cbs (&gt; 100)</td>
<td>SS API (rx, tx, state callbacks only)</td>
</tr>
<tr>
<td>Sending adaptation</td>
<td>lws_callback_on_writeable()  + WRITEABLE</td>
<td>lws_ss_request_write() + tx() cb</td>
</tr>
<tr>
<td>Sending buffer</td>
<td>User-chosen + malloc'd partial handling</td>
<td>SS-provided, no partials</td>
</tr>
<tr>
<td>Create vhosts</td>
<td>code</td>
<td><strong>JSON policy</strong></td>
</tr>
<tr>
<td>TLS validation</td>
<td>cert bundle or code</td>
<td><strong>JSON policy</strong>, or cert bundle</td>
</tr>
<tr>
<td>Connection retry / backoff</td>
<td>code</td>
<td><strong>JSON policy</strong>, Auto</td>
</tr>
<tr>
<td>Nailing up</td>
<td>code</td>
<td><strong>JSON policy</strong>, Auto</td>
</tr>
<tr>
<td>Endpoint and protocol details</td>
<td>spread around the code</td>
<td><strong>JSON policy</strong></td>
</tr>
<tr>
<td>Protocol selection, pipeline / stream sharing</td>
<td>code</td>
<td><strong>JSON policy</strong></td>
</tr>
<tr>
<td>ws subprotocol selection</td>
<td>code</td>
<td><strong>JSON policy</strong></td>
</tr>
<tr>
<td>ws binary / text</td>
<td>code</td>
<td><strong>JSON policy</strong></td>
</tr>
<tr>
<td>Protocol-specific metadata</td>
<td>Protocol-specific apis in code (eg, lws_hdr)</td>
<td><strong>JSON policy</strong>, generic metadata apis in code</td>
</tr>
<tr>
<td>Connection validity rules</td>
<td>struct</td>
<td><strong>JSON policy</strong>, Auto</td>
</tr>
<tr>
<td>Stream as Long Poll</td>
<td>code</td>
<td><strong>JSON policy</strong></td>
</tr>
<tr>
<td>Auth</td>
<td>code</td>
<td><strong>JSON policy</strong> + automatic rotation if provider supported, else code</td>
</tr>
</tbody>
</table>
<h3 tabindex="-1" dir="auto">Serialized Secure Streams</h3>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/warmcat/libwebsockets/blob/main/doc-assets/ss-api2.png"><img src="https://github.com/warmcat/libwebsockets/raw/main/doc-assets/ss-api2.png" alt="Secure Streams direct"></a></p>
<p dir="auto">Secure Streams APIs are also <strong>serializable</strong>, the exact same client code can
fulfil the connection directly in the same process as you would expect, or
forward the actions, metadata and payloads to an <a href="https://github.com/warmcat/libwebsockets/blob/main/minimal-examples/ssproxy/ssproxy-socket">SS Proxy</a> that owns the policy
over a Unix Domain or TCP socket connection to be fulfilled centrally.  This
allows, eg, h2 streams from different processes sharing a single connection.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/warmcat/libwebsockets/blob/main/doc-assets/ss-api3.png"><img src="https://github.com/warmcat/libwebsockets/raw/main/doc-assets/ss-api3.png" alt="Secure Streams direct"></a></p>
<p dir="auto">The serialized SS can also travel over generic transports like UART, an <a href="https://github.com/warmcat/libwebsockets/blob/main/minimal-examples/embedded/pico/pico-sspc-binance">example
is provided implementing the Binance example on an RPi Pico</a> with a UART transport
to a <a href="https://github.com/warmcat/libwebsockets/blob/main/minimal-examples/ssproxy/ssproxy-custom-transport-uart">UART transport SS proxy</a>, where the pico itself has no network stack, tls, compression or
wss stack, but can send and receive to and from the endpoint as if it did.</p>
<p dir="auto">The optional <code>lws_trasport_mux</code> is used to interpose between the UART transport
and the SSPC layer, allowing a single pipe to carry many separate SS connections.</p>
<p dir="auto">The user SS code is identical however it is transported, muxed and fulfilled.</p>
<h2 tabindex="-1" dir="auto">v4.3 is released</h2>
<p dir="auto">See the <a href="https://libwebsockets.org/git/libwebsockets/tree/changelog" rel="nofollow">changelog</a></p>
<h2 tabindex="-1" dir="auto">Lws work retrospective</h2>
<p dir="auto">The initial commit for lws will have been 11 years ago come Oct 28 2021, it's been a lot of work.
There are a total of 4.3K patches, touching 800KLOC cumulatively (this is not the size in the
repo, but over the years, how many source lines were changed by patches).</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/warmcat/libwebsockets/blob/main/doc-assets/work.png"><img src="https://github.com/warmcat/libwebsockets/raw/main/doc-assets/work.png" alt="overview"></a></p>
<p dir="auto">Gratifyingly, it turns out over the years, ~15% of that was contributed by 404 contributors: that's not so bad.
Thanks a lot to everyone who has provided patches.</p>
<p dir="auto">Today at least tens of millions of devices and product features rely on lws to
handle their communications including several from FAANG; Google now include lws
as part of Android sources.</p>
<h2 tabindex="-1" dir="auto">Support</h2>
<p dir="auto">This is the libwebsockets C library for lightweight websocket clients and
servers.  For support, visit</p>
<p dir="auto"><a href="https://libwebsockets.org/" rel="nofollow">https://libwebsockets.org</a></p>
<p dir="auto">and consider joining the project mailing list at</p>
<p dir="auto"><a href="https://libwebsockets.org/mailman/listinfo/libwebsockets" rel="nofollow">https://libwebsockets.org/mailman/listinfo/libwebsockets</a></p>
<p dir="auto">You can get the latest version of the library from git:</p>
<ul dir="auto">
<li><a href="https://libwebsockets.org/git" rel="nofollow">https://libwebsockets.org/git</a></li>
</ul>
<p dir="auto">Doxygen API docs for development: <a href="https://libwebsockets.org/lws-api-doc-main/html/index.html" rel="nofollow">https://libwebsockets.org/lws-api-doc-main/html/index.html</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ten Noteworthy AI Research Papers of 2023 (124 pts)]]></title>
            <link>https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023</link>
            <guid>38896027</guid>
            <pubDate>Sat, 06 Jan 2024 22:11:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023">https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023</a>, See on <a href="https://news.ycombinator.com/item?id=38896027">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>This year has felt distinctly different. I've been working in, on, and with machine learning and AI for over a decade, yet I can't recall a time when these fields were as popular and rapidly evolving as they have been this year.</p><p>To conclude an eventful 2023 in machine learning and AI research, I'm excited to share 10 noteworthy papers I've read this year. My personal focus has been more on large language models, so you'll find a heavier emphasis on large language model (LLM) papers than computer vision papers this year.</p><p>I resisted labeling this article "Top AI Research Papers of 2023" because determining the "best" paper is subjective. The selection criteria were based on a mix of papers I either particularly enjoyed or found impactful and worth noting. (The sorting order is a recommended reading order, not an ordering by perceived quality or impact.)</p><p><strong>By the way, if you scroll down to the end of this article, you'll find a little surprise. Thanks for all your support, and I wish you a great start to the new year!</strong></p><p><span>With </span><em><strong><a href="https://arxiv.org/abs/2304.01373" rel="">Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling</a></strong></em><span>, the researchers originally released 8 LLMs ranging from 70M to 12B parameters (with both weights and data publicly released, which is rare).</span></p><p>But in my opinion, the standout feature of this paper is that they also released the training details, analyses, and insights (some of them shown in the annotated figure below).&nbsp;</p><p>Here are some questions that the Pythia paper addresses:</p><ol><li><p>Does pretraining on duplicated data (i.e., training for &gt;1 epoch) make a difference? It turns out that deduplication does not benefit or hurt performance.</p></li><li><p>Does training order influence memorization? Unfortunately, it turns out that it does not. "Unfortunately," because if this was true, we could mitigate undesirable verbatim memorization issues by reordering the training data.</p></li><li><p>Does pretrained term frequency influence task performance? Yes, few-shot accuracy tends to be higher for terms that occur more frequently.</p></li><li><p>Does increasing the batch size affect training efficiency and model convergence? Doubling the batch size halves the training time but doesn't hurt convergence.</p></li></ol><p><span>Today, only six months later, the LLMs are by no means groundbreaking. However, I am including this paper because it not only tries to answer interesting questions about training settings but is also a positive example regarding details and transparency. Moreover, the small LLMs in the &lt;1B range are nice templates for small studies and tinkering, or starters for pretraining experiments (here's a link to their </span><a href="https://github.com/EleutherAI/pythia" rel="">GitHub repository</a><span>).&nbsp;</span></p><p>My wish for 2024 is that we see more studies like this and well-written papers in the coming year!</p><p><em><strong><a href="https://arxiv.org/abs/2307.09288" rel="">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></strong></em><span> is the follow-up paper to Meta's popular first Llama paper.&nbsp;</span></p><p><span>Llama 2 models, which range from 7B to 70B parameters, are one of the reasons this paper made it onto this list: these are still among the most capable and widely used openly available models. Worth noting is that the </span><a href="https://github.com/facebookresearch/llama/blob/main/LICENSE" rel="">Llama 2 license</a><span> also permits use in commercial applications (see the </span><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" rel="">Request to Access page</a><span> for details).</span></p><p>On the model side, what differentiates the Llama 2 suite from many other LLMs is that the models come as standard pretrained models and chat models that have been finetuned via reinforcement learning with human feedback (RLHF, the method used to create ChatGPT) to follow human instructions similar to ChatGPT — RLHF-finetuned models are still rare.</p><p>For more details on RLHF and how it's used in Llama 2, see my more comprehensive standalone article below.</p><div data-component-name="DigestPostEmbed"><a href="https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives" target="_blank" rel="noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe06a33c8-cdbd-4f5e-8380-86fb71a075c8_2216x1232.png"><img src="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe06a33c8-cdbd-4f5e-8380-86fb71a075c8_2216x1232.png" sizes="100vw" alt="LLM Training: RLHF and Its Alternatives" width="140" height="140"></picture></div></a></div><p>Next to the fact that Llama 2 models are widely used and come with RLHF instruction-finetuned variants, the other reason I decided to include the paper on this list is the accompanying in-depth 77-page research report.</p><p>Here, the authors also nicely illustrated the evolution of the Llama 2 70B Chat models, tracing their journey from the initial supervised finetuning (SFT-v1) to the final RLHF finetuning stage with PPO (RLHF-v5). The chart reflects consistent improvements in both the harmlessness and helpfulness axes, as shown in the annotated plots below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png" width="1456" height="712" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:712,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5628bf36-ac20-43fe-96aa-670cb8a5cac0_1600x782.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em><span>Annotated figure from Llama 2 paper (</span><a href="https://arxiv.org/abs/2307.09288" rel="">https://arxiv.org/abs/2307.09288</a><span>) showing the performance progression from the first iteration of the supervised finetuned model (SFT-1) to the final RLHF-finetuned chat model (RLHF-v5).</span></em></figcaption></figure></div><p><br><span>Even though models such as Mistral-8x7B (more later), DeepSeek-67B, and YI-34B top the larger Llama-2-70B models in public benchmarks, Llama 2 still remains a common and popular choice when it comes to openly available LLMs and developing methods on top of it.&nbsp;</span></p><p>Furthermore, even though some benchmarks indicate that there may be better models, one of the bigger challenges this year has been the trustworthiness of benchmarks. For instance, how do we know that the models haven't been trained on said benchmarks and the scores aren't inflated? In classic machine learning, when someone proposed a new gradient boosting model, it was relatively easy to reproduce the results and check. Nowadays, given how expensive and complex it is to train LLMs (and the fact that most researchers either don't disclose the architecture or the training data details), it is impossible to tell.&nbsp;</p><p>To conclude, it's refreshing to see Meta doubling down on open source even though every other major company is now rolling out its own proprietary large language models (Google's Bard and Gemini, Amazon's Q, and Twitter/X's Grok, and OpenAI's ChatGPT).&nbsp;</p><p><em><strong><a href="https://arxiv.org/abs/2305.14314" rel="">QLoRA: Efficient Finetuning of Quantized LLMs</a></strong></em><span> has been one of the favorite techniques in the LLM research and finetuning community this year because it makes the already popular LoRA (low-rank adaptation) technique more memory efficient. In short, this means that you can fit larger models onto smaller GPUs.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png" width="616" height="261.46153846153845" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:618,&quot;width&quot;:1456,&quot;resizeWidth&quot;:616,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57930b6-afd0-48e6-90a6-97b5dcacb89f_1560x662.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>A short visual summary of regular LoRA</em></figcaption></figure></div><p>QLoRA stands for quantized LoRA (low-rank adaptation). The standard LoRA method modifies a pretrained LLM by adding low-rank matrices to the weights of the model's layers. These matrices are smaller and, therefore, require fewer resources to update during finetuning.</p><p>In QLoRA, these low-rank matrices are quantized, meaning their numerical precision is reduced. This is done by mapping the continuous range of values in these matrices to a limited set of discrete levels. This process reduces the model's memory footprint and computational demands, as operations on lower-precision numbers are less memory-intensive</p><p><span>According to the </span><a href="https://arxiv.org/abs/2305.14314" rel="">QLoRA paper</a><span>, QLoRA reduces the memory requirements of a 65B Llama model to fit onto a single 48 GB GPU (like an A100). The 65B Guanaco model, obtained from quantized 4-bit training of 65B Llama, maintains full 16-bit finetuning task performance, reaching 99.3% of the ChatGPT performance after only 24 hours of finetuning.</span></p><p>I've also run many QLoRA experiments this year and found QLoRA a handy tool for reducing GPU memory requirements during finetuning. There's a trade-off, though: the extra quantization step results in an additional computation overhead, meaning the training will be a bit slower than regular LoRA.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png" width="1456" height="340" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:340,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01911980-8546-4eaa-8a78-afe3582ba79c_1600x374.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em><span>Excerpt from my LoRA &amp; QLoRA experiments that I wrote about previously </span><a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms" rel="">here</a></em></figcaption></figure></div><p>LLM finetuning remains as relevant as ever as researchers and practitioners aim to create custom LLMs. And I appreciate techniques like QLoRA that help make this process more accessible by lowering the GPU memory-requirement barrier.</p><p><span>Looking at all the papers published this year, </span><em><strong><a href="https://arxiv.org/abs/2303.17564" rel="">BloombergGPT: A Large Language Model for Finance</a></strong></em><span> may look like an odd choice for a top-10 list because it didn't result in a groundbreaking new insight, methodology, or open-source model.&nbsp;</span></p><p>I include it because it's an interesting case study where someone pretrained a relatively large LLM on a domain-specific dataset. Moreover, the description was pretty thorough, which is becoming increasingly rare. This is especially true when it comes to papers with authors employed at companies -- one of the trends this year was that major companies are becoming increasingly secretive about architecture or dataset details to preserve trade secrets in this competitive landscape (PS: I don't fault them for that).</p><p>Also, BloombergGPT made me think of all the different ways we can pretrain and finetune models on domain-specific data, as summarized in the figure below (note that this was not explored in the BloombergGPT paper, but it would be interesting to see future studies on that).</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg" width="1456" height="1126" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1126,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40063902-cdd3-4dd9-9993-bb9b6f7e7663_1456x1126.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>The different ways of pretraining and finetuning LLMs.</em></figcaption></figure></div><p>In short, BloombergGPT is a 50-billion parameter language model for finance, trained on 363 billion tokens from finance data and 345 billion tokens from a general, publicly available dataset. For comparison, GPT-3 is 3.5x larger (175 billion parameters) but was trained on 1.4x fewer tokens (499 billion).</p><p>Why did the authors use an architecture with "only" 50 billion parameters since GPT-3 is 3.5x larger? That's easier to answer. They adopted the Chinchilla scaling laws and found this to be a good size given the available size of the finance data.</p><p>Is it worth (pre)training the LLM on the combined dataset from scratch? Based on the paper, the model performs really well in the target domain. However, we don't know whether it's better than a) further pretraining a pretrained model on domain-specific data or b) finetuning a pretrained model on domain-specific data.</p><p>Despite the little criticism above, overall, this is an interesting paper that serves as an interesting case study and example for domain-specific LLMs; plus, it leaves room for further research on pretraining versus finetuning to instill knowledge into an LLM.</p><p><span>(PS: For those curious about a comparison to finetuning, as </span><a href="https://x.com/rohanpaul_ai/status/1738474868214235163?s=20" rel="">Rohan Paul shared</a><span> with me, the "small" </span><a href="https://arxiv.org/abs/2309.09530" rel="">AdaptLLM-7B</a><span> model outperforms BloombergGPT on one dataset and nearly matches its performance on three other finance datasets. Although BloombergGPT appears to be slightly better overall, it's worth noting that training AdaptLLM-7B cost about $100, in contrast to BloombergGPT's multi-million dollar investment.)</span></p><p><span>Before discussing the </span><em><strong><a href="https://arxiv.org/abs/2305.18290" rel="">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></strong></em><span> paper, let's take a short step back and discuss the method it aims to replace, Reinforcement Learning from Human Feedback (RLHF).</span></p><p><span>RLHF is the main technique behind ChatGPT and Llama 2 Chat models. In RLHF, which I described in more detail in a </span><a href="https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives" rel="">separate article</a><span>, we use a multi-step procedure:</span></p><ol><li><p>Supervised finetuning: The model is initially trained on a dataset containing instructions and the desired responses.</p></li><li><p>Reward modeling: Human raters provide feedback on the model's outputs. This feedback is used to create a reward model, which learns to predict what kinds of outputs are to be preferred.</p></li><li><p>Proximal policy optimization (PPO): The model generates outputs, and the reward model scores each output. The PPO algorithm uses these scores to adjust the model's policy toward&nbsp;</p></li></ol><p>generating higher-quality outputs. (This is a reinforcement learning algorithm used to finetune the model's policy.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png" width="348" height="250.34418604651162" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:928,&quot;width&quot;:1290,&quot;resizeWidth&quot;:348,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09755a6f-ceb5-4011-8651-45ffffd260e5_1290x928.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Example of two training examples from a dataset for the supervised instruction finetuning step. Note that the "input" is optional.</em></figcaption></figure></div><p>While RLHF is popular and effective, as we've seen with ChatGPT and Llama 2, it's also pretty complex to implement and finicky.&nbsp;</p><p><a href="https://arxiv.org/abs/2305.18290" rel="">The Direct Preference Optimization (DPO) paper</a><span> introduces an algorithm that optimizes language models to align with human preferences </span><strong>without</strong><span> explicit reward modeling or reinforcement learning. Instead, DPO uses a simple classification objective.</span></p><p>In DPO, we still keep the supervised finetuning step (step 1 above), but we replace steps 2 and 3 with a single step to further finetune the model on the preference data. In other words, DPO skips the reward model creation required by RLHF entirely, which significantly simplifies the finetuning process.</p><p><span>How well does it work? There haven't been many models trained with DPO until very recently. (This makes sense because DPO is also a relatively recent method.) However, one recent example is the Zephyr 7B model described in </span><em><a href="https://arxiv.org/abs/2310.16944" rel="">Zephyr: Direct Distillation of LM Alignment</a></em><span>. Zephyr-7B is based on a Mistral-7B base LLM that has been finetuned using DPO. (There will be more on Mistral later.)</span></p><p><span>As the performance tables below reveal, the 7B-parameter Zephyr model outperformed all other models in its size class at the time of its release. Even more impressively, Zephyr-7B even surpassed the 10 times larger 70B-parameter Llama 2 chat model on the conversational </span><a href="https://arxiv.org/abs/2306.05685" rel="">MT-Bench</a><span> benchmark as well.</span><br></p><p>In summary, the appeal of the DPO paper lies in the simplicity of its method. The scarcity of chat models trained using RLHF, with Llama 2 as a notable exception, can likely be attributed to the complexity of the RLHF approach. Given this, I think it's reasonable to anticipate an increase in the adoption of DPO models in the coming year.</p><p><span>I must admit that the </span><em><strong><a href="https://arxiv.org/abs/2310.06825" rel="">Mistral 7B paper</a></strong></em><span> wasn't among my favorites due to its brevity. However, the model it proposed was quite impactful.</span></p><p>I decided to include the paper on this list because the Mistral 7B model was not only very popular upon release, but also served as the base model, leading to the development of two other notable models: Zephyr 7B and the latest Mistral Mixture of Experts (MoE) approach. These models are good examples of the trend I foresee for small LLMs in (at least) the early half of 2024.</p><p>Before we discuss the Zephyr 7B and Mistral MoE models, let's briefly talk about Mistral 7B itself.</p><p><span>In short, The Mistral 7B paper introduces a compact yet powerful language model that, despite its relatively modest size of 7 billion tokens, outperforms its larger counterparts, such as the 13B Llama 2 model, in various benchmarks. (Next to the two-times larger </span><a href="https://github.com/QwenLM/Qwen" rel="">Qwen 14B</a><span>, Mistral 7B was also the base model used in the winning solutions of this year's </span><a href="https://llm-efficiency-challenge.github.io/leaderboard" rel="">NeurIPS LLM Finetuning &amp; Efficiency challenge</a><span>.)</span></p><p>Why exactly it is so good is unclear, but it might likely be due to its training data. Neither Llama 2 nor Mistral discloses the training data, so we can only speculate.</p><p><span>Architecture-wise, the model shares group-query attention with Llama 2. While being very similar to Llama 2, one interesting addition to the Mistral architecture is sliding window attention to save memory and improve computational throughput for faster training. (Sliding window attention was previously proposed in </span><a href="https://arxiv.org/abs/1904.10509" rel="">Child et al. 2019</a><span> and </span><a href="https://arxiv.org/abs/2004.05150" rel="">Beltagy et al. 2020</a><span>.)</span></p><p>The sliding window attention mechanism used in Mistral is essentially a fixed-sized attention block that allows a current token to attend only a specific number of previous tokens (instead of all previous tokens), which is illustrated in the figure below.</p><p>In the specific case of 7B Mistral, the attention block size is 4096 tokens, and the researchers were training the model with up to 100k token context sizes. To provide a&nbsp; concrete example, in regular self-attention, a model at the 50,000th token can attend all previous 49,999 tokens. In sliding window self-attention, the Mistral model can only attend tokens 45,904 to 50,000 (since 50,000 - 4,096 = 45,904).&nbsp;</p><p>However, sliding window attention is mainly used to improve computational performance. The fact that Mistral outperforms larger Llama 2 models is likely not because of sliding window attention but rather despite sliding window attention.</p><p>One reason Mistral 7B is an influential model is that it served as the base model for Zephyr 7B, as mentioned earlier in the DPO section. Zephyr 7B, the first popular model trained with DPO to outperform other alternatives, has potentially set the stage for DPO to become the preferred method for finetuning chat models in the coming months.</p><p><span>Another noteworthy model derived from Mistral 7B is the recently released </span><a href="https://mistral.ai/news/mixtral-of-experts/" rel="">Mistral Mixture of Experts (MoE) model</a><span>, also known as Mixtral-8x7B. This model matches or exceeds the performance of the larger Llama-2-70B on several public benchmarks.</span></p><p><span>For more benchmarks, also see the official </span><a href="https://mistral.ai/news/mixtral-of-experts/" rel="">Mixtral blog post announcement</a><span>. The team also released a Mixtral-8x7B-Instruct model that has been finetuned with DPO (but as of this writing there are no benchmarks comparing it to Llama-2-70-Chat, the RLHF-finetuned model).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png" width="286" height="246.01421800947867" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d5e379db-3f34-4733-854a-f90987181118_844x726.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:726,&quot;width&quot;:844,&quot;resizeWidth&quot;:286,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e379db-3f34-4733-854a-f90987181118_844x726.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Mixtral architecture overview based on the param.json file that the Mistral team originally shared via a magnet link on social media</em></figcaption></figure></div><p><span>GPT-4 is also rumored to be an MoE consisting of 16 submodules. Each of these 16 submodules is rumored to have 111 billion parameters (for reference, GPT-3 has 175 billion parameters). If you read my </span><a href="https://magazine.sebastianraschka.com/p/ai-and-open-source-in-2023" rel="">AI and Open Source in 2023 article</a><span> approximately two months ago, I mentioned that "It will be interesting to see if MoE approaches can lift open-source models to new heights in 2024". It looks like Mixtral started this trend early, and I am sure that this is just the beginning.</span></p><p>If you are new to MoE models, here's a short explanation.</p><p>The figure above shows the architecture behind the Switch Transformer, which uses 1 expert per token with 4 experts in total. Mixtral-8x-7B, on the other hand, consists of 8 experts and uses 2 experts per token.</p><p>Why MoEs? Combined, the 8 experts in a 7B model like Mixtral are still ~56B parameters. Actually, it's less than 56B, because the MoE approach is only applied to the FFN (feed forward network, aka fully-connected) layers, not the self-attention weight matrices. So, it's likely closer to 40-50B parameters.</p><p>Note that the router reroutes the tokens such that only &lt;14B parameters (2x &lt;7B, instead of all &lt;56B) are used at a time for the forward pass, so the training (and especially inference) will be faster compared to the traditional non-MoE approach.</p><p><span>If you want to learn more about MoEs, here's a reading list recommended by </span><a href="https://twitter.com/sophiamyang" rel="">Sophia Yang</a><span>:&nbsp;</span></p><ul><li><p><a href="https://arxiv.org/abs/1701.06538" rel="">The Sparsely-Gated Mixture-of-Experts Layer (2017)</a></p></li><li><p><a href="https://arxiv.org/abs/2006.16668" rel="">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (2020)</a><span>&nbsp;</span></p></li><li><p><a href="https://arxiv.org/abs/2211.15841" rel="">MegaBlocks: Efficient Sparse Training with Mixture-of-Experts (2022)</a><span>&nbsp;</span></p></li><li><p><a href="https://arxiv.org/abs/2305.14705" rel="">Mixture-of-Experts Meets Instruction Tuning (2023)</a></p></li></ul><p><span>Furthermore, if you are interested in trying MoE LLMs, also check out the </span><a href="https://github.com/XueFuzhao/OpenMoE" rel="">OpenMoE</a><span> repository, which implemented and shared MoE LLMs earlier this year.</span></p><p>Mistral 7B, Zephyr 7B, and Mixtral-8x7B are excellent examples of the progress made in 2023 with small yet capable models featuring openly available weights. Another notable model, a runner-up on my favorite papers list, is Microsoft's phi series. </p><p>The secret sauce of phi is training on high-quality data (referred to as “textbook quality data”) obtained by filtering web data.</p><p>Released in stages throughout 2023, the phi models include phi-1 (1.3B parameters), phi-1.5 (1.3B parameters), and phi-2 (2.7B parameters). The latter, released just two weeks ago, is already said to match or outperform Mistral 7B, despite being only half its size.</p><p>For more information about the phi models, I recommend the following resources:</p><ul><li><p><a href="https://arxiv.org/abs/2306.11644" rel="">Textbooks Are All You Need</a><span> -- the phi-1 paper</span></p></li><li><p><a href="https://arxiv.org/abs/2309.05463" rel="">Textbooks Are All You Need II: phi-1.5 Technical Report</a></p></li><li><p><a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/" rel="">The Phi-2: The Surprising Power of Small Language Models</a><span> announcement</span></p></li></ul><p><em><strong><a href="https://arxiv.org/abs/2311.11045" rel="">Orca 2: Teaching Small Language Models How to Reason</a></strong></em><span> is a relatively new paper, and time will tell whether it has a lasting impact on how we train LLMs in the upcoming months or years.&nbsp;</span></p><p>I decided to include it because it combines several concepts and ideas.&nbsp;</p><p>One is the idea of distilling data from large, capable models such as GPT-4 to create a synthetic dataset to train small but capable LLMs. This idea was described in the Self-Instruct paper, which came out last year. Earlier this year, Alpaca (a Llama model finetuned on ChatGPT outputs) really popularized this approach.</p><p>How does this work? In a nutshell, it's a 4-step process:</p><ol><li><p>Seed task pool with a set of human-written instructions (175 in this case) and sample instructions;</p></li><li><p>Use a pretrained LLM (like GPT-3) to determine the task category;</p></li><li><p>Given the new instruction, let a pretrained LLM generate the response;</p></li><li><p>Collect, prune, and filter the responses before adding them to the task pool.</p></li></ol><p><span>The other idea may not be surprising but worth highlighting: high-quality data is important for finetuning. For instance, the </span><a href="https://arxiv.org/abs/2305.11206" rel="">LIMA paper</a><span> proposed a human-generated high-quality dataset consisting of only 1k training examples that can be used to finetuning to outperform the same model finetuned on 50k ChatGPT-generated responses.</span></p><p>Unlike previous research that heavily relied on imitation learning to replicate outputs from larger models, Orca 2 aims to teach "small" (i.e., 7B and 13B) LLMs various reasoning techniques (like step-by-step reasoning, recall-then-generate, etc.) and to help them determine the most effective strategy for each task. This approach has led Orca 2 to outperform similar-sized models noticeably and even achieve results comparable to models 5-10 times larger.</p><p><span>While we haven't seen any extensive studies on this, the Orca 2 approach might also be able to address the issue of using synthetic data that was highlighted in the </span><a href="https://arxiv.org/abs/2305.15717" rel="">The False Promise of Imitating Proprietary LLMs</a><span> paper. Here, the researchers investigated the finetuning weaker language models to imitate stronger proprietary models like ChatGPT, using examples such as Alpaca and Self-Instruct. Initially, the imitation models showed promising results, performing well in following instructions and receiving competitive ratings from crowd workers compared to ChatGPT. However, more follow-up evaluations revealed that these imitation models only seemed to perform well to a human observer but often generated factually incorrect responses.</span></p><p>In recent years, I've almost exclusively worked with large language transformers or vision transformers (ViTs) due to their good performance.&nbsp;</p><p>Switching gears from language to computer vision papers for the last three entries, what I find particularly appealing about transformers for computer vision is that pretrained ViTs are even easier to finetune than convolutional neural networks. (I summarized a short hands-on talk at CVPR earlier this year here: https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training).&nbsp;</p><p><span>To my surprise, I stumbled upon the </span><em><strong><a href="https://arxiv.org/abs/2310.16764" rel="">ConvNets Match Vision Transformers at Scale</a></strong></em><span> paper showing that convolutional neural networks (CNNs) are in fact, competitive with ViTs when given access to large enough datasets.</span></p><p>Here, researchers invested compute budgets of up to 110k TPU hours to do a fair comparison between ViTs and CNNs. The outcome was that when CNNs are pretrained with a compute budget similar to what is typically used for ViTs, they can match the performance of ViTs. For this, they pretrained on 4 billion labeled images from JFT and subsequently finetuned the models on ImageNet.</p><p>Object recognition and segmentation in images and videos, along with classification and generative modeling, are the main research fields in computer vision.&nbsp;</p><p>To briefly highlight the difference between these two tasks: object detection about predicting bounding boxes and the associated labels; segmentation classifies each pixel to distinguish between foreground and background objects.&nbsp;</p><p><span>Meta's </span><em><strong><a href="https://arxiv.org/abs/2304.02643" rel="">Segment Anything</a></strong></em><span> paper is a notable milestone for open source and image segmentation research. The paper introduces a new task, model, and dataset for image segmentation. The accompanying image datasets the largest segmentation dataset to date with over 1 billion masks on 11 million images.&nbsp;</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg" width="592" height="304.1717791411043" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:670,&quot;width&quot;:1304,&quot;resizeWidth&quot;:592,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfa09c5e-0b4f-4984-95c2-13ce9cf8de1d_1304x670.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em><span>The Segment Anything Model (SAM) is designed for efficient, prompt-based image segmentation. Annotated screenshot from the Segment Anything paper, </span><a href="https://arxiv.org/abs/2304.02643" rel="">https://arxiv.org/abs/2304.02643</a></em></figcaption></figure></div><p>However, what's rare and especially laudable is that the researchers used licensed and privacy-respecting images, so the model can be open-sourced without major copyright concerns.</p><p>The Segment Anything Model (SAM) consists of three main components, as summarized in the annotated figure above.</p><p>In slightly more details, the three components can be summarized as follows:</p><ol><li><p>An image encoder utilizing a masked autoencoder based on a pretrained vision transformer (ViT) that can handle high-resolution inputs. This encoder is run once per image and can be applied before prompting the model.</p></li><li><p>A prompt encoder that handles two types of prompts: sparse (points, boxes, text) and dense (masks). Points and boxes are represented by positional encodings combined with learned embeddings for each prompt type. And free-form text uses an off-the-shelf text encoder from CLIP. Dense prompts, i.e., masks, are embedded using convolutions and summed element-wise with the image embedding.</p></li><li><p>A mask decoder maps the image embedding, prompt embeddings, and an output token to a mask. This is a decoder-style transformer architecture that computes the mask foreground probability at each image location.</p></li></ol><p><span>Image segmentation is important for applications like self-driving cars, medical imaging, and many others. In the short amount of 6 months, the paper has already been </span><a href="https://scholar.google.com/scholar?cites=15741444728855576863&amp;as_sdt=5,39&amp;sciodt=0,39&amp;hl=en" rel="">cited more than 1500 times</a><span>, and there have already been many projects that have been built on top of this paper.</span></p><p><em><strong><a href="https://arxiv.org/abs/2311.10709" rel="">Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning</a></strong></em><span> is another notable computer vision project from Meta's research division.&nbsp;</span></p><p>Emu is a text-to-video model that can generate entire videos from text prompts.&nbsp;</p><p>While it's not the first model for impressive text-to-video generation, it compares very favorably to previous works.</p><p>As the authors note, the Emu architecture setup is relatively simple compared to previous approaches. One of the main ideas here is that Emu factorizes the generation process into two steps: first, generating an image based on text (using a diffusion model), then creating a video conditioned on both the text and the generated image (using another diffusion model).&nbsp;</p><p>2022 has been a big year for text-to-image models like DALL-E 2, Stable Diffusion, and Midjourney. While text-to-image models remain very popular in 2023 (even though LLMs got most of the attention throughout the year), I think that text-to-video models are just about to become more prevalent in online communities in the upcoming year.&nbsp;</p><p>Since I am not an image or video designer, I don't have use cases for these tools at the moment; however, text-to-image and text-to-video models are nonetheless interesting to watch as a general measure of progress regarding computer vision.</p><p><span>I've been coding and writing a new book since last summer, and I am excited to share that the </span><a href="http://mng.bz/amjo" rel="">first chapters are now available via Manning's early access program</a><span>.</span></p><p><span>In </span><em><strong><a href="http://mng.bz/amjo" rel="">Build a Large Language Model (from Scratch)</a></strong></em><span>, you will code an LLM step-by-step using PyTorch to gain a thorough understanding of its inner workings.&nbsp;</span></p><p>The book covers everything from coding the data input pipeline to implementing attention mechanisms from scratch and pretraining and finetuning the LLM. Each stage is accompanied by clear text, diagrams, and examples.</p><p><em><strong>I hope you have a good start to the new year!</strong></em></p></div></article></div></div>]]></description>
        </item>
    </channel>
</rss>