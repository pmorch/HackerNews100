<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 08 Oct 2024 20:30:10 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[End of the Road for Google Drive in Transmit (164 pts)]]></title>
            <link>https://blog.panic.com/end-of-the-road-for-google-drive-and-transmit/</link>
            <guid>41780395</guid>
            <pubDate>Tue, 08 Oct 2024 18:38:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.panic.com/end-of-the-road-for-google-drive-and-transmit/">https://blog.panic.com/end-of-the-road-for-google-drive-and-transmit/</a>, See on <a href="https://news.ycombinator.com/item?id=41780395">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
				
<figure><img fetchpriority="high" decoding="async" width="1024" height="323" src="https://blog-cdn.panic.com/wp-content/uploads/2024/10/street-ends-1024x323.jpg" alt="" srcset="https://blog-cdn.panic.com/wp-content/uploads/2024/10/street-ends-1024x323.jpg 1024w, https://blog-cdn.panic.com/wp-content/uploads/2024/10/street-ends-300x95.jpg 300w, https://blog-cdn.panic.com/wp-content/uploads/2024/10/street-ends-768x242.jpg 768w, https://blog-cdn.panic.com/wp-content/uploads/2024/10/street-ends-1536x485.jpg 1536w, https://blog-cdn.panic.com/wp-content/uploads/2024/10/street-ends-2048x646.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>We never like removing functionality from our apps. We especially don’t like doing it when it’s due to circumstances beyond our control. But, sometimes — rarely? — it can happen, and so, please take note:</p>



<p><strong>At some unknown point in the future, Google will revoke Transmit’s access to Google Drive. Sometime after that, we’ll be releasing updates to Transmit and Nova that remove the ability to create Google Drive connections.</strong></p>



<p><strong>Transmit itself is of course still in active development, and no other connection types are affected.</strong></p>



<p>(Note that existing connections should continue to work for as long as they remain authenticated!)</p>



<h2>Why Though</h2>



<p>Well, Google has a new set of policies that require apps that connect to Google Drive to go through expensive, time-consuming annual reviews, and this has made it extremely difficult for us to reasonably maintain Google Drive access. You may have seen <a href="https://ia.net/topics/our-android-app-is-frozen-in-carbonite">iA Writer’s announcement</a> that they are stopping development of their Android version for similar reasons. Our experience was different, but our circumstances are similar. While Google Drive may not be the most popular connection option in Transmit, we know many users rely on it, and we often use it here at Panic to send and receive files from the game developers we work with.&nbsp;</p>



<p>This is not a decision we took lightly, and was the result of much debate and anguish in the office. But rest assured we looked at every angle. Hopefully that explains everything.</p>



<h2>Actually, I Want to Know More</h2>



<p>Okay, okay, here’s a more background for the deeply curious. In 2019, Google <a href="https://workspace.google.com/blog/product-announcements/enhancing-security-controls-for-google-drive-third-party-apps">announced they were adding additional security checks</a> to apps with full access to users’ files on Drive. Shortly after, they prevented Transmit from authorizing new Drive users. We submitted Transmit to Google for review. And waited for months without hearing anything back.</p>



<p>Eventually, by reaching out through friends of friends of friends to find someone inside Google who could help, we got in contact with a Google employee who was very helpful in getting the process started. We went through review, and our access was restored in early 2020. Unfortunately, we were never able to get Google to approve Nova.</p>



<p>For the next couple years, the annual re-review was pretty straightforward. However, in December 2023, Google again disabled Transmit and emailed us, explaining that we would need to complete a “Cloud Application Security Assessment (CASA)” security review. The review found no security issues with Transmit, but it was an incredibly lengthy process. It involved registering with a security lab, running a vulnerability scanner on Transmit’s source code, and filling out a long form. Between each step, we had to wait for days before we’d hear back from the lab, causing the process to take nearly a month.</p>



<p>In March, Transmit was re-approved for Google Drive access — but we were told we would now need to pass this check annually. At this point, we began to question whether this yearly process was worth it.</p>



<p>Between the weeks of waiting, submitting the required documentation and the process of scanning the code, it took a significant amount of time from our engineers. For example, Google provided a Docker image for running the scanner, but it didn’t work. We had to spend more than a week debugging and fixing it. And because the scanner found no problems, it didn’t result in any improvements to Transmit. No one benefitted from this process. Not Google, not Panic, and not our users.</p>



<p>As a small, independent developer, losing this time for no benefit is a huge cost. That week could have been better spent improving our products. But even so, at the time, we resigned ourselves to the yearly checks. We didn’t want to let our users down, and hopefully, now that we had experience with it, the scanner would be easier to run next year.</p>



<p>But then… a couple of months later, <a href="https://old.reddit.com/r/androiddev/comments/1b483a6/end_of_google_drive_integration/"><strong>Google completely removed the option for us to scan our own code</strong></a>. Instead, to keep access to Google Drive, <strong>we would now have to pay one of Google’s business partners to conduct the review.</strong> They promised a discounted minimum price, but no maximum price. We realized that either we’d most likely be paying someone else a chunk of cash to run the same scanner we were running, or our bill would end up much higher.</p>



<p>These ever-shifting requirements and expenses are finally catching up to third parties. Other products have discontinued Google Drive support or come up with <a href="https://www.ghisler.com/googledrivehelp.htm">interesting workarounds</a> with various limitations that don’t work for all users. Ultimately, we think any workaround strategy is too risky and may result in banned accounts, and we definitely don’t want to be responsible for anyone getting banned.</p>



<h2>We’re Very Sorry!</h2>



<p>In short, with all these factors in play, we have decided we will not attempt to renew Google Drive access for Transmit once it expires. We’ll miss it too. We will instead focus our efforts on other features and products. We know that this situation, to put it in simplified terms, kinda sucks. If Google ever revises their security policies to be more in reach for a small software company like Panic, we will definitely take a second look.</p>



<p>Thanks for using Transmit and thanks for supporting Panic for all of these years. Onward.</p>
			
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lead drinking-water pipes must be replaced nationwide, EPA says (129 pts)]]></title>
            <link>https://www.nytimes.com/2024/10/08/climate/biden-epa-lead-pipes.html</link>
            <guid>41780347</guid>
            <pubDate>Tue, 08 Oct 2024 18:32:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/10/08/climate/biden-epa-lead-pipes.html">https://www.nytimes.com/2024/10/08/climate/biden-epa-lead-pipes.html</a>, See on <a href="https://news.ycombinator.com/item?id=41780347">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/10/08/climate/biden-epa-lead-pipes.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Kotlin Money (285 pts)]]></title>
            <link>https://blog.eriksen.com.br/en/introducing-kotlin-money</link>
            <guid>41776878</guid>
            <pubDate>Tue, 08 Oct 2024 12:59:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.eriksen.com.br/en/introducing-kotlin-money">https://blog.eriksen.com.br/en/introducing-kotlin-money</a>, See on <a href="https://news.ycombinator.com/item?id=41776878">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <p>Manipulating monetary amounts is a common computing chore. However, no mainstream language has a first-class data type for representing money, it’s up to programmers to code abstractions for it. This isn’t an issue per se until dealing with rounding issues from operations like installment payments (e.g., buy now, pay later), foreign exchange, or even simple things like fee processing and tax collection.</p>

<p>Inspired by my days at <a href="https://blog.eriksen.com.br/en/platform-engineering-n26">N26</a> dealing with these challenges, I introduce <a href="https://github.com/eriksencosta/money/">Money</a>: a Kotlin library that makes monetary calculations and allocations easy:</p>

<pre><code>val price = 100 money "USD"                     // USD 100.00
val shipping = 5 money "USD"                    // USD 5.00
val subtotal = price + shipping                 // USD 105.00
val discount = 10.percent()                     // 10%
val total = subtotal decreaseBy discount        // USD 94.50

val ratios = listOf(60.percent(), 40.percent()) // [60%, 40%]

total allocate 2                                // [USD 47.25, USD 47.25]
total allocate ratios                           // [USD 56.70, USD 37.80]
</code></pre>

<p>The library supports mathematical operations with monetary amounts, calculations with percentages, and allocation, making it simple to model use cases like those mentioned. Cryptocurrencies are also fully supported out of the box:</p>

<pre><code>val price = 0.01607580 money "BTC"           // BTC 0.01607580
val transactionFee = 1.25.percent()          // 1.25%
val total = price increaseBy transactionFee  // BTC 0.01627675
val installments = total allocate 3          // [BTC 0.00542559, BTC 0.00542558, BTC 0.00542558]

val rate = 62_555.60 money "USD"             // USD 62555.60
val totalInUsd = total exchange rate         // USD 1005.63
</code></pre>

<h2 id="allocation">Allocation</h2>

<p>One of the nicest features of the library is its allocation capability. Allocation allows the distribution of a monetary amount into parts while guaranteeing that the sum of the parts equals the original value. For example, a retailer may accept purchases by credit card installments or by buy now, pay later (BNPL). What happens when a customer makes a purchase totaling USD 100.00 to be paid in three installments?</p>

<pre><code>val price = 100 money "USD"
val number = 3
val installment = price / number
val installments = List(number) { installment } // [USD 33.33, USD 33.33, USD 33.33]
val total = installments.sum()                  // USD 99.99
</code></pre>

<p>As you noticed, there is a loss of USD 0.01. A penny here and there may seem a slight loss, but it may be <a href="https://slate.com/technology/2019/10/round-floor-software-errors-stock-market-battlefield.html">costly over time</a>. But there are other complications as well, such as overcharging a customer (which can be an infringement of consumer rights in several countries) due to rounding issues. The library provides a handy <code>allocate()</code> method that guarantees the result won’t differ from the original amount:</p>

<pre><code>val price = 100 money "USD"
val installments = price allocate 3          // [USD 33.34, USD 33.33, USD 33.33]
val total = installments.allocations().sum() // USD 100.00
</code></pre>

<p>To allocate in proportional parts, pass a list of <code>Percentage</code> values to the method:</p>

<pre><code>val amount = 2345.89 money "USD"
val result = dueAmount allocate listOf(50.percent(), 30.percent(), 20.percent())
val allocations = result.allocations() // [USD 1172.94, USD 703.77, USD 469.18]
val total = allocations.sum()          // USD 2345.89
</code></pre>

<p>As you can see in the previous examples, both results totaled up to the original monetary amount. No cent was lost or gained. By default, the library automatically allocates the difference. But you can tweak how the difference is allocated in the allocations list. For example, suppose your company requires the difference to be always allocated to the last item. You can do it by creating the allocator object directly with the desired allocation strategy:</p>

<pre><code>val price = 100 money "USD"
val allocator = EvenAllocator(OnLast)
val installments = allocator.allocate(price, 3) // [USD 33.33, USD 33.33, USD 33.34]
val total = installments.allocations().sum()    // USD 100.00
</code></pre>

<h2 id="wrapping-up">Wrapping up</h2>

<p>This post is just a glimpse of the library’s capabilities. I intend to keep the library’s API concise and to expand its capabilities gradually, including supporting Android development and out of the box persistence and serialization. Nevertheless, I hope it’s useful in its current version for people manipulating monetary amounts in Kotlin projects.</p>

<p>Refer to the <a href="https://github.com/eriksencosta/money/tree/trunk/docs/usage">usage guide</a> on how to work with Money. The library has built-in support for <a href="https://github.com/eriksencosta/money/blob/trunk/docs/appendixes/circulating-currencies.md">306 circulating currencies</a> and <a href="https://github.com/eriksencosta/money/blob/trunk/docs/appendixes/cryptocurrencies.md">2283 cryptocurrencies</a>. The <a href="https://github.com/eriksencosta/money/#installation">installation procedures</a> are explained in the project’s README. Give it a shot!</p>

<!-- Links. -->



    
      <h2>References</h2>
      
<ul>

  <li>
    <a id=""></a>
    

    <a href="https://blog.eriksen.com.br/en/platform-engineering-n26">Platform engineering at N26: how we planned and launched it</a>

    
  </li>

  <li>
    <a id=""></a>
    

    <a href="https://github.com/eriksencosta/money">Money: Project repository</a>

    
  </li>

  <li>
    <a id=""></a>
    Slate. Lav Varshney, 2019.

    <a href="https://slate.com/technology/2019/10/round-floor-software-errors-stock-market-battlefield.html">The Deadly Consequences of Rounding Errors</a>

    
  </li>

  <li>
    <a id=""></a>
    

    <a href="https://github.com/eriksencosta/money/tree/trunk/docs/usage">Money: Usage guide</a>

    
  </li>

  <li>
    <a id=""></a>
    

    <a href="https://github.com/eriksencosta/money/blob/trunk/docs/appendixes/circulating-currencies.md">Money: Circulating currencies</a>

    
  </li>

  <li>
    <a id=""></a>
    

    <a href="https://github.com/eriksencosta/money/blob/trunk/docs/appendixes/cryptocurrencies.md">Money: Cryptocurrencies</a>

    
  </li>

  <li>
    <a id=""></a>
    

    <a href="https://github.com/eriksencosta/money/#installation">Money: Installation procedures</a>

    
  </li>

</ul>


    
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Do U.S. ports need more automation? (124 pts)]]></title>
            <link>https://www.construction-physics.com/p/do-us-ports-need-more-automation</link>
            <guid>41776861</guid>
            <pubDate>Tue, 08 Oct 2024 12:57:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.construction-physics.com/p/do-us-ports-need-more-automation">https://www.construction-physics.com/p/do-us-ports-need-more-automation</a>, See on <a href="https://news.ycombinator.com/item?id=41776861">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>On October 1st, 47,000 members of the International Longshoremen's Association (ILA), primarily dockworkers on East and Gulf Coast ports, went on strike after failing to agree contract terms with </span><a href="https://www.usmx.com/" rel="">USMX</a><span>, an alliance of port operators and employers. (West Coast ports, which are worked by a different union, the International Longshore and Warehouse Union, remained open.) Along with higher pay, the main point of disagreement was automation; the ILA demanded a </span><a href="https://en.wikipedia.org/wiki/2024_United_States_port_strike" rel="">complete ban</a><span> on introducing new port automation. The strike ended on October 3rd with a tentative agreement on wage increases, but </span><a href="https://abcnews.go.com/US/dockworkers-strike-suspended-sources/story?id=114445386" rel="">negotiations over automation</a><span> will continue until January 15th.</span></p><p><span>The strike sparked a discussion about the quality of U.S. ports, and the degree they need to be automated to improve their performance. Most folks have (reasonably) assumed that the union is holding back port automation that would improve productivity. In the 1960s, dockworkers unions vehemently protested against introducing the productivity-enhancing shipping container, and current ILA president Harold Daggett has complained about </span><a href="https://nypost.com/2024/10/02/business/union-boss-harold-daggett-rages-against-e-zpass-for-costing-union-jobs-in-video-weeks-before-strike-shut-down-ports/" rel="">EZ passes for highway tolls</a><span> eliminating union jobs. The ILA’s case has not been helped by the extremely high wages of longshoremen (and of Daggett), the </span><a href="https://www.instagram.com/reel/DAj5A3-SfYn/" rel="">video of Daggett</a><span> threatening to “cripple” the entire economy, or the fact that Daggett is alleged to have </span><a href="https://www.telegraph.co.uk/us/news/2024/10/01/harold-daggett-union-leader-fought-allegations-mob-ties/" rel="">connections to organized crime</a><span>.</span></p><p>But despite Daggett and the ILA doing everything in their power to turn people against their cause, the case for port automation isn’t straightforward. It is true that U.S. ports perform poorly compared to ports around the world, but it’s hard to blame this entirely on a lack of automation. Few ports around the world are automated, and the most heavily automated ports (including in the U.S.) don’t necessarily perform the best. Lack of automation may be partly behind poor American port performance, but there are many other factors as well.</p><p><span>Since 2020, the World Bank has released a </span><a href="https://openknowledge.worldbank.org/entities/publication/87d77e6d-6b7b-4bbe-b292-ae0f3b4827e8" rel="">Container Port Performance Index</a><span>, which ranks ports around the world based on how long vessels stay in port. Since a ship waiting in port isn’t making money by transporting cargo, shipowners want them in and out of ports as quickly as possible. Major American ports routinely rank near the bottom of this list. Los Angeles, the largest port in the U.S. by container volume, ranked #375 in 2023, and Long Beach (the second-largest) ranked #373. Savannah ranked #395, and Seattle ranked #360. Of the five largest American container ports, only New York-New Jersey cracked the top 100 (at #92). On average, per this index the U.S. has the worst-performing ports of any country in the world.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ecd19f8-4dfa-437b-82b0-8f25f0fc699b_590x287.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ecd19f8-4dfa-437b-82b0-8f25f0fc699b_590x287.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ecd19f8-4dfa-437b-82b0-8f25f0fc699b_590x287.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ecd19f8-4dfa-437b-82b0-8f25f0fc699b_590x287.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ecd19f8-4dfa-437b-82b0-8f25f0fc699b_590x287.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ecd19f8-4dfa-437b-82b0-8f25f0fc699b_590x287.png" width="590" height="287" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3ecd19f8-4dfa-437b-82b0-8f25f0fc699b_590x287.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:287,&quot;width&quot;:590,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ecd19f8-4dfa-437b-82b0-8f25f0fc699b_590x287.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ecd19f8-4dfa-437b-82b0-8f25f0fc699b_590x287.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ecd19f8-4dfa-437b-82b0-8f25f0fc699b_590x287.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ecd19f8-4dfa-437b-82b0-8f25f0fc699b_590x287.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Via UNCTAD.</figcaption></figure></div><p>Other measures of port performance show the same thing. It takes roughly two to three times as long to move a container in a U.S. port as a Chinese port, and U.S. ports are slower than ports in most other major shipping countries.</p><p>The U.S. does better on bulk cargo and tanker handling, but only manages to be average or somewhat below average in cargo loading/unloading rates and ship waiting times.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac1b84cc-4146-4722-b1d3-d936e03ed4f9_858x705.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac1b84cc-4146-4722-b1d3-d936e03ed4f9_858x705.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac1b84cc-4146-4722-b1d3-d936e03ed4f9_858x705.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac1b84cc-4146-4722-b1d3-d936e03ed4f9_858x705.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac1b84cc-4146-4722-b1d3-d936e03ed4f9_858x705.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac1b84cc-4146-4722-b1d3-d936e03ed4f9_858x705.png" width="858" height="705" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ac1b84cc-4146-4722-b1d3-d936e03ed4f9_858x705.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:705,&quot;width&quot;:858,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac1b84cc-4146-4722-b1d3-d936e03ed4f9_858x705.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac1b84cc-4146-4722-b1d3-d936e03ed4f9_858x705.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac1b84cc-4146-4722-b1d3-d936e03ed4f9_858x705.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac1b84cc-4146-4722-b1d3-d936e03ed4f9_858x705.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Via UNCTAD.</figcaption></figure></div><p>This situation is not new. Since at least the 2010s, American container ports have been substantially less productive than ports in Southeast Asia.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5644fe12-95bf-42bb-aeb3-5c724ea0b944_555x344.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5644fe12-95bf-42bb-aeb3-5c724ea0b944_555x344.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5644fe12-95bf-42bb-aeb3-5c724ea0b944_555x344.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5644fe12-95bf-42bb-aeb3-5c724ea0b944_555x344.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5644fe12-95bf-42bb-aeb3-5c724ea0b944_555x344.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5644fe12-95bf-42bb-aeb3-5c724ea0b944_555x344.png" width="555" height="344" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5644fe12-95bf-42bb-aeb3-5c724ea0b944_555x344.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:344,&quot;width&quot;:555,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5644fe12-95bf-42bb-aeb3-5c724ea0b944_555x344.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5644fe12-95bf-42bb-aeb3-5c724ea0b944_555x344.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5644fe12-95bf-42bb-aeb3-5c724ea0b944_555x344.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5644fe12-95bf-42bb-aeb3-5c724ea0b944_555x344.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Via JOC.</figcaption></figure></div><p>However, it’s not clear how much this can be blamed on automation.</p><p>When we talk about port automation, we’re really talking about a collection of different tasks, which can be automated to different degrees. We can broadly break container handling automation into three different groups: ship- or quay-side tasks, yard tasks, and land-side tasks. The first are tasks required to move containers from the ship onto the shore, the second are tasks required to move containers around the storage yard, and the third are tasks required to move containers out of the port via land-based transport.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8149d67-edf1-4d93-a8ea-dc668339b1ba_1407x880.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8149d67-edf1-4d93-a8ea-dc668339b1ba_1407x880.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8149d67-edf1-4d93-a8ea-dc668339b1ba_1407x880.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8149d67-edf1-4d93-a8ea-dc668339b1ba_1407x880.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8149d67-edf1-4d93-a8ea-dc668339b1ba_1407x880.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8149d67-edf1-4d93-a8ea-dc668339b1ba_1407x880.png" width="1407" height="880" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c8149d67-edf1-4d93-a8ea-dc668339b1ba_1407x880.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:880,&quot;width&quot;:1407,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8149d67-edf1-4d93-a8ea-dc668339b1ba_1407x880.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8149d67-edf1-4d93-a8ea-dc668339b1ba_1407x880.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8149d67-edf1-4d93-a8ea-dc668339b1ba_1407x880.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8149d67-edf1-4d93-a8ea-dc668339b1ba_1407x880.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Examples of port automation, via GAO.</figcaption></figure></div><p><span>When containers are placed on a ship, they’re attached together with </span><a href="https://www.tandemloc.com/ad54000a-1ga" rel="">twistlocks</a><span> to keep them from moving around. These locks must be connected after the containers are loaded and disconnected before they can be unloaded, part of a process known as </span><a href="https://www.marineinsight.com/marine-safety/important-points-for-safe-container-lashing/" rel="">lashing</a><span> (which also includes installing tie-rods and turnbuckles). Lashing is a difficult process to automate (though automated twistlocks </span><a href="https://www.youtube.com/watch?v=euYwMviOuYo" rel="">do exist</a><span>), and it is </span><a href="https://portlogistics.akquinet.com/port-logistics-blog/blogpost-details/automatic-twistlock-handling-what-are-the-possibilities" rel="">mostly done manually</a><span>, even in highly automated ports.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8451d681-6337-45ff-958f-6911acbd889b_1140x500.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8451d681-6337-45ff-958f-6911acbd889b_1140x500.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8451d681-6337-45ff-958f-6911acbd889b_1140x500.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8451d681-6337-45ff-958f-6911acbd889b_1140x500.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8451d681-6337-45ff-958f-6911acbd889b_1140x500.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8451d681-6337-45ff-958f-6911acbd889b_1140x500.png" width="1140" height="500" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8451d681-6337-45ff-958f-6911acbd889b_1140x500.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:500,&quot;width&quot;:1140,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8451d681-6337-45ff-958f-6911acbd889b_1140x500.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8451d681-6337-45ff-958f-6911acbd889b_1140x500.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8451d681-6337-45ff-958f-6911acbd889b_1140x500.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8451d681-6337-45ff-958f-6911acbd889b_1140x500.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Container twistlock.</figcaption></figure></div><p><span>Containers are moved on and off ships using large </span><a href="https://www.liebherr.com/en/usa/products/maritime-cranes/port-equipment/container-bridges/ship-to-shore-container-cranes.html#:~:text=Ship%20to%20shore%20container%20cranes%20are%20custom%20designed%20with%20a,twin%20and%20tandem%20lift%20configuration." rel="">ship-to-shore cranes</a><span>. While some parts of this process </span><a href="https://www.porttechnology.org/wp-content/uploads/2019/05/PT50-21.pdf" rel="">can be automated</a><span>, skilled workers are still required to align the cranes with the container, in part because the ship is moving slightly as containers are being picked. Traditionally these cranes were operated from cabs attached to the crane itself, though some ports are installing systems that allow for remote operation.</span></p><p><span>Unloaded containers will typically be moved to a storage yard, using a range of types of vehicles. Some, like regular trucks, move horizontally, while others, like </span><a href="https://www.konecranes.com/en-us/port-equipment-services/container-handling-equipment/straddle-carriers" rel="">straddle carriers</a><span>, can both move it horizontally and lift the container to stack it. These operations can be automated using </span><a href="https://www.konecranes.com/en-us/port-equipment-services/container-handling-equipment/automated-guided-vehicles" rel="">automated guided vehicles</a><span> (AGVs) and </span><a href="https://www.kalmarusa.com/equipment/straddle-carriers/autostrad-automated-straddle-carrier/" rel="">automated lifting vehicles</a><span> (ALVs).</span></p><p><span>Containers in the storage yard will be moved around either via lifting vehicles or by large gantry cranes. Gantry cranes come in two types: either </span><a href="https://aicraneliftingsolution.com/gantry-cranes/rail-mounted/amp/" rel="">rail mounted</a><span> (called RMGs) or equipped with </span><a href="https://www.konecranes.com/en-us/port-equipment-services/container-handling-equipment/rubber-tired-gantry-cranes" rel="">large rubber tires</a><span> that allow them to move without needing rails (called RTGs). Both types of cranes can also be automated.</span></p><p>From the yard, containers are moved out of the port onto land via trucks or rail. Containers must be checked at the port gate, and parts of these gate-checking operations can also be automated using things like radiofrequency ID (RFID) and optical character recognition (OCR) systems to automatically track and update container movements.</p><p>When a container port is referred to as “automated,” it typically means it has adopted container-moving technologies like automated guided vehicles and straddle carriers, automated gantry cranes, and remotely operated ship-to-shore cranes. Lashing is still mostly done manually even in automated ports, and even “un-automated” ports (including most large ports in the U.S.) have adopted gate automation systems.</p><p><span>Does automating container handling improve port productivity? It’s not clear that it does, or that the lack of it is why U.S. ports perform so poorly. For one thing, few ports around the world have adopted this sort of automation. As of 2021 there were around 63 automated or semi-automated container terminals in the world out of more than 1,300 container terminals total, accounting for only about 4% of global container capacity. Where automation has been adopted, it’s often limited to a single container terminal out of </span><a href="https://www.portoflosangeles.org/business/terminals/container" rel="">many in a given port</a><span>, each operated by different companies.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86125dea-ff3f-4d3a-a501-6bc0de9c2dd4_1392x680.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86125dea-ff3f-4d3a-a501-6bc0de9c2dd4_1392x680.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86125dea-ff3f-4d3a-a501-6bc0de9c2dd4_1392x680.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86125dea-ff3f-4d3a-a501-6bc0de9c2dd4_1392x680.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86125dea-ff3f-4d3a-a501-6bc0de9c2dd4_1392x680.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86125dea-ff3f-4d3a-a501-6bc0de9c2dd4_1392x680.png" width="1392" height="680" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/86125dea-ff3f-4d3a-a501-6bc0de9c2dd4_1392x680.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:680,&quot;width&quot;:1392,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86125dea-ff3f-4d3a-a501-6bc0de9c2dd4_1392x680.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86125dea-ff3f-4d3a-a501-6bc0de9c2dd4_1392x680.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86125dea-ff3f-4d3a-a501-6bc0de9c2dd4_1392x680.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86125dea-ff3f-4d3a-a501-6bc0de9c2dd4_1392x680.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Automated container terminals circa 2021. Each dot represents an existing or planned automated terminal. Via </span><a href="https://www.pmanet.org/terminal-automation-in-southern-california-implications-for-growth-jobs-and-the-future-competitiveness-of-west-coast-ports/" rel="">PMA</a><span>.</span></figcaption></figure></div><p>Several of these automated terminals are in the U.S. Los Angeles, for instance, has two automated terminals, and its sister port of Long Beach has another. On a country-by-country basis, in fact, the U.S. is towards the higher end in the number of automated or semi-automated container terminals it operates.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d856e-185a-41b9-a633-647ab92ded28_912x440.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d856e-185a-41b9-a633-647ab92ded28_912x440.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d856e-185a-41b9-a633-647ab92ded28_912x440.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d856e-185a-41b9-a633-647ab92ded28_912x440.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d856e-185a-41b9-a633-647ab92ded28_912x440.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d856e-185a-41b9-a633-647ab92ded28_912x440.png" width="912" height="440" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a86d856e-185a-41b9-a633-647ab92ded28_912x440.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:440,&quot;width&quot;:912,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d856e-185a-41b9-a633-647ab92ded28_912x440.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d856e-185a-41b9-a633-647ab92ded28_912x440.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d856e-185a-41b9-a633-647ab92ded28_912x440.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d856e-185a-41b9-a633-647ab92ded28_912x440.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Automated container terminals by country circa 2022, via </span><a href="https://www.porteconomics.eu/container-terminal-automation-a-global-analysis-on-decision-making-drivers-benefits-realized-and-stakeholder-support/" rel="">Knatz et al</a><span>.</span></figcaption></figure></div><p>The ports that have adopted automation aren’t necessarily particularly efficient. Rotterdam was one of the first ports to automate (its first automated terminal opened in 1993), and today it’s one of the most heavily automated ports in the world, but its port performance ranking is just #91, one point above the comparatively less automated New York, and far below the un-automated ports of Charleston and Philadelphia (#53 and #55 respectively). Likewise, the U.S.’s most automated port, Los Angeles, comes in very close to the bottom of the worldwide rankings, while the top two ports on the list (Yangshan in China and Salalah in Oman) have just one and zero automated terminals, respectively.</p><p><span>Other analyses likewise point to a complex relationship between automation and efficient port operations. A </span><a href="https://www.mckinsey.com/industries/travel-logistics-and-infrastructure/our-insights/the-future-of-automated-ports#/download/%2F~%2Fmedia%2Fmckinsey%2Findustries%2Ftravel%20logistics%20and%20infrastructure%2Four%20insights%2Fthe%20future%20of%20automated%20ports%2Fthe-future-of-automated-ports-final.pdf%3FshouldIndex%3Dfalse" rel="">McKinsey survey</a><span> from 2017 found that while port automation reduced labor costs, it actually </span><em>reduced</em><span> port productivity between 7 and 15%, and the labor savings weren’t necessarily enough to justify the investment. A </span><a href="https://www.itf-oecd.org/container-port-automation-impacts-and-implications" rel="">2021 OECD report</a><span> similarly found that “automated ports are generally not more productive than their conventional counterparts,” and a </span><a href="https://www.gao.gov/products/gao-24-106498" rel="">2024 GAO report</a><span> also noted that both U.S. and international ports found “mixed effects” on performance when adopting automation. APM Terminals, a global container terminal operator, stated in 2021 that its fully automated terminal in Rotterdam (Maasvlakte 2) had lower performance than its older, non-automated terminal (Maasvlakte 1). A </span><a href="https://www.porteconomics.eu/container-terminal-automation-a-global-analysis-on-decision-making-drivers-benefits-realized-and-stakeholder-support/" rel="">2021 survey</a><span> of automated port operators found that while port operators did generally receive benefits they expected from automation, they were somewhat prone to overestimating the reduction in container handling costs (though they also underestimated the benefits to things like truck turn time).</span></p><p><span>Why is port automation not a more obvious win? A variety of factors are at work. For one, automated container handling equipment is not always as capable as manual handling. The GAO survey of automated port operators found that while automated equipment had benefits like being able to stack containers closer and sometimes needing fewer container moves, it was sometimes slower and less capable than conventional equipment, even after several years of operation. Automated cranes might make fewer crane moves per hour due to inability to operate in adverse weather conditions, like rain or fog, that a human could handle. When the Port of Auckland tried to adopt automated handling equipment, automated straddle carriers were found to operate at just one third the speed of manual ones, and often behaved unsafely and unpredictably. Auckland ultimately abandoned its automation project after spending $400 million (and losing hundreds of millions more due to shipping delays), and is now </span><a href="https://www.stuff.co.nz/nz-news/350189426/auckland-port-revives-machines-failed-automation-project" rel="">converting</a><span> its automated straddles to manual. The performance of remotely operated cranes has sometimes been disappointing for similar reasons. </span></p><p><span>Automated equipment is less flexible than a human operator, and it can’t respond to unusual or unexpected situations that a human could handle; AGVs, for instance, often follow fixed paths guided by </span><a href="https://www.goetting-agv.com/dateien/downloads/ptn_article.pdf" rel="">transponders</a><span> </span><a href="https://www.youtube.com/watch?v=NULoelb7PzA" rel="">embedded</a><span> in the concrete. And automation does best with a steady, predictable volume of containers. But as container ships have gotten larger, container volumes have often gotten </span><em>less</em><span> steady, with more peaks and troughs. Highly varying volumes might be more easily handled by a human labor force that can be scaled up and down as needed.</span></p><p>Relatedly, it’s typically easier to install automated equipment in a new, greenfield container terminal than to retrofit an existing terminal. Union work rules that limit labor flexibility might also make it hard to take maximum advantage of automated equipment. These last two factors are likely part of the reason why Chinese ports tend to see greater advantage from automation than U.S. ports.</p><p><span>Container unloading and transport is a long sequence of operations, of which only some can be addressed by automation. If your operations aren’t carefully synced, or they’re bottlenecked by some other factor unrelated to automation (such as the availability of truck chassis), then automation can’t do much to help. Conversely, carefully synchronizing operations to minimize waiting and waste doesn’t require automation to achieve. In 2013, the best-performing container terminal in the world was a non-automated APM terminal in the Japanese port of Yokohama. It achieved this via good old fashioned </span><a href="https://mag.toyota.co.uk/kaizen-toyota-production-system/" rel="">continuous improvement</a><span> methods to cut out as much waiting as possible.</span></p><p>Automated equipment reduces unit costs of container handling, but this reduction can be offset by the high upfront costs needed to install the equipment. A survey of automated port operators found that payback periods were most often 6 years or more, and some U.S. operators expected payback periods of 10 to 20 years. This makes automation risky if the expected future container volumes don’t materialize, and automation tends to be easiest to justify in very large ports with large container volumes.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde98017a-853a-4171-8dd3-8168f05e1153_1464x876.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde98017a-853a-4171-8dd3-8168f05e1153_1464x876.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde98017a-853a-4171-8dd3-8168f05e1153_1464x876.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde98017a-853a-4171-8dd3-8168f05e1153_1464x876.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde98017a-853a-4171-8dd3-8168f05e1153_1464x876.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde98017a-853a-4171-8dd3-8168f05e1153_1464x876.png" width="1456" height="871" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/de98017a-853a-4171-8dd3-8168f05e1153_1464x876.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:871,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde98017a-853a-4171-8dd3-8168f05e1153_1464x876.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde98017a-853a-4171-8dd3-8168f05e1153_1464x876.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde98017a-853a-4171-8dd3-8168f05e1153_1464x876.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde98017a-853a-4171-8dd3-8168f05e1153_1464x876.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Via GAO.</figcaption></figure></div><p>Large volumes might also be needed for the productivity benefits of automation to materialize. APM Terminals stated in 2021 that its Maasvlakte 2 Terminal was too small to fully achieve the benefits from automation:</p><blockquote><p><em>“The high degree of automation only comes into its own when large volumes can be rotated, and these are insufficient at this time. Sometimes, processes are still carried out manually, which should actually be automated. If the terminal is expanded, with the same staffing, more volume is processed, and productivity goes to the intended level.”</em></p></blockquote><p>In addition to the factors of scale, greenfield vs. brownfield sites, and work rules that make it harder for U.S. ports to take maximum advantage of automation (and to generally operate efficiently), there are other factors that drag down American port performance.</p><p><span>One factor is the type of work ports are doing. Most U.S. ports are “</span><a href="https://porteconomicsmanagement.org/pemp/contents/part1/ports-and-container-shipping/levels-transshipment-incidence/" rel="">gateway</a><span>” ports, where imports enter the country and exports leave via land-based travel like rail and trucks. By contrast, some ports that act as hubs for global trade do much more </span><a href="https://en.wikipedia.org/wiki/Transshipment" rel="">transhipment</a><span>, where containers arrive on one ship and depart on another. Singapore, for instance, has one of the largest ports in the world, but very little of the cargo is Singaporean imports and exports.</span></p><p><span>Transhipment cargo tends to take less time to move, and ports that have a large fraction of transhipment will naturally have higher performance as measured by vessel waiting time. On the Container Port Performance Index, many of the top-performing ports, like </span><a href="https://asyad.om/ports/salalah-port" rel="">Salalah</a><span>, </span><a href="https://www.seatrade-maritime.com/ports-logistics/cartagena-is-the-largest-transhipment-hub-in-the-caribbean" rel="">Cartagena</a><span>, and </span><a href="https://maritime-executive.com/article/maersk-expands-tangiers-med-terminal-as-port-s-transshipment-role-grows" rel="">Tanger-Med</a><span>, are primarily transhipment ports. (Of course, the Jones Act makes it hard for the U.S. to take advantage of transhipment for domestic cargoes.) High transhipment fractions may also make it easier to adopt automated equipment, though experts seem to disagree about this.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-149929827" href="https://www.construction-physics.com/p/do-us-ports-need-more-automation#footnote-1-149929827" target="_self" rel="">1</a></span></p><p>Another major performance factor is hours of operation. Top-performing ports tend to operate 24 hours a day, 7 days a week, whereas U.S. ports don’t operate around the clock.</p><p><span>The limited hours of operation relate to another potential issue, that of lack of coordination. As we noted, effective container handling requires all operations to be coordinated and synchronized, and there’s some evidence that American ports don’t coordinate their operations especially well. In Los Angeles and Long Beach, for instance, individual terminals have separate rail systems for moving cargo, and the systems don’t work together. Lack of coordination between truckers, shipyards, and companies that move containers also seems to be behind the failure of a </span><a href="https://ambrook.com/research/supply-chain/ag-exports-bottlenecked-at-oakland-port-despite-usda-efforts" rel="">program</a><span> designed to help farmers ship out crops through the Port of Oakland. Historically, U.S. ports have </span><a href="https://www.joc.com/article/us-ports-move-toward-truck-appointment-model-5238478" rel="">lacked appointment systems</a><span> for trucks to arrive and pick up cargo; instead trucks would simply arrive randomly, often creating large amounts of congestion as they waited to load and unload. And while U.S. ports seem to be adopting truck appointment systems, they don’t yet appear to be universal or completely coordinated (as of last year the ports of Los Angeles and Long Beach had </span><a href="https://www.truckingdive.com/news/california-harbor-trucking-association-port-of-long-beach-port-of-los-angeles-appointments-system/641644/" rel="">several different appointment systems</a><span> in place). During the post-pandemic cargo congestion at major ports, things like </span><a href="https://www.reuters.com/world/us/us-ports-supply-chain-fix-challenge-selling-247-shifts-2021-10-14/" rel="">lack of available warehouse space</a><span> made temporary 24/7 port operations less than successful. There are often shortages of </span><a href="https://www.foodlogistics.com/transportation/3pl-4pl/news/12090302/ports-and-truckers-face-chassis-shortage" rel="">truck chassis</a><span> or </span><a href="https://www.kanarysolutions.com/resources/what-is-causing-port-congestion" rel="">truck drivers</a><span> needed to move containers out of ports, and </span><a href="https://crsreports.congress.gov/product/pdf/IN/IN11800" rel="">customs processes are often slow</a><span> and not coordinated.</span></p><p><span>Another possible issue is infrastructure investment not keeping up with port container volumes, an issue highlighted in a 2015 Federal Maritime Commission Study. One example of this is </span><a href="https://www.heritage.org/trade/commentary/113-year-old-law-hurting-american-ports" rel="">insufficient dredging</a><span> to handle larger container ships (a problem made worse by the Foreign Dredge Act, which prevents the use of foreign-built dredges). Most American ports </span><a href="https://transportgeography.org/contents/chapter6/port-terminals/channel-depth-ports-north-america/" rel="">can’t handle</a><span> the very largest container ships. In 2021 a </span><a href="https://enotrans.org/article/port-of-long-beach-highlights-dredging-project-in-tis-wrda-hearing/" rel="">24,000-TEU container ship</a><span> needed to unload its cargo to a smaller ship at the Port of Long Beach, which then transferred its cargo ashore, a time-consuming operation required because the Bay was not deep enough to allow the larger ship to access the port.</span></p><p>Automation, at best, seems like a partial explanation of American port productivity problems. The U.S. is not necessarily all that far behind in automated equipment adoption, and it’s not as clear a win as some people seem to think. Plenty of automated ports (including American ones!) don’t operate particularly efficiently, and plenty of non-automated ports do.</p><p>Of course, this doesn’t mean that the union is correct to ban all automation going forward. Right now, automation is the worst that it will ever be. It will only get better and more capable, and banning it will hamstring port efficiencies in the future even if it doesn’t necessarily do it now. It’s hard to imagine how much poorer the U.S. (and the world) would be if unions had successfully fought off the introduction of the shipping container in the 1960s. We shouldn’t stop the introduction of new, productivity-enhancing technology, but we should also be realistic about what benefits it will actually be expected to yield.</p><p><em><strong><span>If you’re interested in learning more about port automation, a recommended reading list of the best and most useful sources I found is </span><a href="https://open.substack.com/pub/constructionphysics/p/port-automation-reading-list?r=23el8&amp;utm_campaign=post&amp;utm_medium=web" rel="">available here</a><span> for paid subscribers.</span></strong></em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Differential Transformer (365 pts)]]></title>
            <link>https://arxiv.org/abs/2410.05258</link>
            <guid>41776324</guid>
            <pubDate>Tue, 08 Oct 2024 11:54:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2410.05258">https://arxiv.org/abs/2410.05258</a>, See on <a href="https://news.ycombinator.com/item?id=41776324">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2410.05258">View PDF</a>
    <a href="https://arxiv.org/html/2410.05258v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Li Dong [<a href="https://arxiv.org/show-email/9d386852/2410.05258">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 7 Oct 2024 17:57:38 UTC (429 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nobel Prize in Physics Awarded for Machine Learning and Neural Networks (682 pts)]]></title>
            <link>https://www.nobelprize.org/prizes/physics/2024/summary/</link>
            <guid>41775463</guid>
            <pubDate>Tue, 08 Oct 2024 09:52:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nobelprize.org/prizes/physics/2024/summary/">https://www.nobelprize.org/prizes/physics/2024/summary/</a>, See on <a href="https://news.ycombinator.com/item?id=41775463">Hacker News</a></p>
Couldn't get https://www.nobelprize.org/prizes/physics/2024/summary/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[John Hopfield and Geoff Hinton Win Physics Nobel Prize [pdf] (148 pts)]]></title>
            <link>https://www.nobelprize.org/uploads/2024/10/press-physicsprize2024.pdf</link>
            <guid>41775449</guid>
            <pubDate>Tue, 08 Oct 2024 09:51:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nobelprize.org/uploads/2024/10/press-physicsprize2024.pdf">https://www.nobelprize.org/uploads/2024/10/press-physicsprize2024.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=41775449">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The Static Site Paradox (144 pts)]]></title>
            <link>https://kristoff.it/blog/static-site-paradox/</link>
            <guid>41775238</guid>
            <pubDate>Tue, 08 Oct 2024 09:08:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kristoff.it/blog/static-site-paradox/">https://kristoff.it/blog/static-site-paradox/</a>, See on <a href="https://news.ycombinator.com/item?id=41775238">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body"><p>In front of you are two personal websites, each used as a blog and to display basic contact info of the owner:</p><ol><li>One is a complex CMS written in PHP that requires a web server, multiple workers, a Redis cache, and a SQL database. The site also has a big frontend component that loads as a Single Page Application and then performs navigation by requesting the content in JSON form, which then gets "rehydrated" client-side.</li><li>The other is a collection of static HTML files and one or two CSS files. No JavaScript anywhere.</li></ol><p>If you didn't know any better, you would expect almost all normal users to have [2] and professional engineers to have something like [1], but it's actually the inverse: only few professional software engineers can "afford" to have the second option as their personal website, and almost all normal users are stuck with overcomplicated solutions.</p><p>Weird as it might be, it's not a great mystery why that is: it's easier to spin up a Wordpress blog than it is to figure out by yourself all the intermediate steps:</p><ol><li>Buy a domain</li><li>Find a hosting platform</li><li>Configure DNS</li><li>Find an SSG (or handcraft everything yourself)</li><li>Learn how to setup a deployment pipeline</li></ol><p>And so, while we software engineers enjoy free hosting &amp; custom domain support with GitHub Pages / Cloudflare Pages / etc, normal users are stuck with a bunch of <a href="https://techcrunch.com/2024/10/04/wordpress-vs-wp-engine-drama-explained/" target="_blank">greedy clowns</a> that make them pay for every little thing, all while wasting ungodly amounts of computational power to render what could have been a static website in 99% of cases.</p><p>Last week I spoke at SquiggleConf in Boston about my experience writing a language server for HTML. Most of the talk is tactical advice on what to do (or avoid) when implementing one, but I concluded the talk with a more high-level point, which I will now report here fully as conclusion to this blog post.</p><blockquote><p>When I published SuperHTML, I discovered that it was <a href="https://kristoff.it/blog/first-html-lsp/" target="_blank">the first ever language server for HTML</a> that reported diagnostics to the user. I wrote a blog post about it, it got <a href="https://news.ycombinator.com/item?id=41512213" target="_blank">on the frontpage of Hacker News</a> and nobody corrected me, so you know it's true.</p><p>I originally found it a funny thing, but thinking about it more, it's a bit sad that this is the case. Linters do exist, and people can get diagnostics in their editor, but that's usually tooling tied to a specific frontend framework and not vanilla HTML, which leads to people opting to use frameworks even if they don't really have a real need for all the complexity that those frameworks bring.</p><p>And that's bad in my opinion. Not because of an abstract appreciation for simplicity, but because <strong>the web doesn't belong just to software engineers</strong>. The more we make the web complex, the more we push normal users into the enclosures that we like to call <em>social networks</em>.</p><p>Don't you find it infuriating when lawyers and accountants fail to clarify how their respective domains work, making them unavoidable intermediaries of systems that in theory you should be able to navigate by yourself?</p><p><strong>Whenever we fail to make simple things easy in software engineering, and webdev especially, we are failing society in the exact same way.</strong></p><p>This is not something that startups or big tech can solve for us, their economic incentives are just too misaligned, so I invite you all to help make the web more accessible, partially as a matter of taking pride in our craft, and partially because the web used to be more interesting when more of it was made by people different from us.</p></blockquote></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The costs of the i386 to x86-64 upgrade (103 pts)]]></title>
            <link>https://blogsystem5.substack.com/p/x86-64-programming-models</link>
            <guid>41773559</guid>
            <pubDate>Tue, 08 Oct 2024 03:24:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blogsystem5.substack.com/p/x86-64-programming-models">https://blogsystem5.substack.com/p/x86-64-programming-models</a>, See on <a href="https://news.ycombinator.com/item?id=41773559">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>If you read my previous article on </span><a href="https://blogsystem5.substack.com/p/dos-memory-models" rel="">DOS memory models</a><span>, you may have dismissed everything I wrote as “legacy cruft from the 1990s that nobody cares about any longer”. After all, computers have evolved from sporting 8-bit processors to 64-bit processors and, on the way, the amount of memory that these computers can leverage has grown orders of magnitude: the 8086, a 16-bit machine with a 20-bit address space, could only use 1MB of memory while today’s 64-bit machines can theoretically access 16EB.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ea4b5bd-f26f-418d-96d3-0d3936356eed_2048x2048.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ea4b5bd-f26f-418d-96d3-0d3936356eed_2048x2048.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ea4b5bd-f26f-418d-96d3-0d3936356eed_2048x2048.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ea4b5bd-f26f-418d-96d3-0d3936356eed_2048x2048.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ea4b5bd-f26f-418d-96d3-0d3936356eed_2048x2048.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ea4b5bd-f26f-418d-96d3-0d3936356eed_2048x2048.jpeg" width="1456" height="1456" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7ea4b5bd-f26f-418d-96d3-0d3936356eed_2048x2048.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:992004,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ea4b5bd-f26f-418d-96d3-0d3936356eed_2048x2048.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ea4b5bd-f26f-418d-96d3-0d3936356eed_2048x2048.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ea4b5bd-f26f-418d-96d3-0d3936356eed_2048x2048.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ea4b5bd-f26f-418d-96d3-0d3936356eed_2048x2048.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>All of this growth has been in service of ever-growing programs. But… even if programs are now more sophisticated than they were before, do they all </span><em>really</em><span> require access to a 64-bit address space? Has the growth from 8 to 64 bits been a net positive in performance terms?</span></p><p>Let’s try to answer those questions to find some very surprising answers. But first, some theory.</p><p><span>An often overlooked property of machine code is </span><em>code density</em><span>, a loose metric that quantifies how many instructions are required to execute a given “action”. I double-quote action because an “action” in this context is a subjective operation that captures a programmer-defined outcome.</span></p><p>Suppose, for example, that you want to increment the value of a variable that resides in main memory, like so:</p><pre><code><code>static int a;
// ...
a = a + 1;</code></code></pre><p>On a processor with an ISA that provides complex instructions and addressing modes, you can potentially do so in just one machine instruction:</p><pre><code><code>; Take the 32-bit quantity at the address indicated by the
; 00001234h immediate, increment it by one, and store it in the
; same memory location.
add dword [1234h], 1</code></code></pre><p>In contrast, on a processor with an ISA that strictly separates memory accesses from other operations, you’d need more steps:</p><pre><code><code>li  r8, 1234h   ; Load the address of the variable into r8.
lm  r1, r8      ; Load the content of the address into r1.
li  r2, 1       ; Load the immediate value 1 into r2.
add r3, r1, r2  ; r3 = r1 + r2
sm  r8, r3      ; Store the result in r3 into the address in r8.</code></code></pre><p>Which ISA is better, you ask? Well, as usual, there are pros and cons to each approach:</p><ul><li><p><span>The former type of ISA offers a denser representation of the operation so it can fit more instructions in less memory. But, because of the semantic complexity of each individual instruction, the processor has to do more work to execute them (possibly requiring more transistors and drawing more power). This type of ISA is usually known as </span><a href="https://en.wikipedia.org/wiki/Complex_instruction_set_computer" rel="">CISC</a><span>.</span></p></li><li><p><span>The latter type of ISA offers a much more verbose representation of the operation but the instructions that the processor executes are simpler and can likely be executed faster. This type of ISA is usually known as </span><a href="https://en.wikipedia.org/wiki/Reduced_instruction_set_computer" rel="">RISC</a><span>.</span></p></li></ul><p><span>This is the eternal debate between CISC and RISC but note that the distinction between the two is not crystal clear: some RISC architectures support </span><em>more</em><span> instructions that CISC architectures, and contemporary Intel processors translate their CISC-style instructions into internal RISC-style </span><a href="https://en.wikipedia.org/wiki/Intel_microcode" rel="">micro-operations</a><span> immediately after decoding the former.</span></p><p><span>Code density is not about instruction </span><em>count</em><span> though. Code density is about the </span><em>aggregate size</em><span>, in bytes, of the instructions required to accomplish a specific outcome.</span></p><p><span>In the example I presented above, the single CISC-style </span><code>mov</code><span> x86 instruction is encoded using anywhere from 2 to 8 bytes depending on its arguments, whereas the fictitious RISC-style instructions typically take 4 bytes each. Therefore, the CISC-style snippet would take 8 bytes total whereas the RISC-style snippet would take 20 bytes total, while achieving the same conceptual result.</span></p><p>Is that increase in encoded bytes bad? Not necessarily because “bad” is subjective and we are talking about trade-offs here, but it’s definitely an important consideration due to its impact in various dimensions:</p><ul><li><p><strong>Memory usage</strong><span>: Larger programs take more memory. You might say that memory is plentiful these days, and that’s true, but it wasn’t always the case: when computers could only address 1 MB of RAM or less, the size of a program’s binary mattered a lot. Furthermore, even today, memory </span><em>bandwidth</em><span> is limited, and this is an often-overlooked property of a system.</span></p></li><li><p><strong>Disk space</strong><span>: Larger programs take more disk space. You might say that disk space is plentiful these days too and that program code takes a tiny proportion of the overall disk: most space goes towards storing media anyway. And that’s true, but I/O bandwidth is not as unlimited as disk space, and loading these programs from disk isn’t cheap. Have you noticed how utterly slow is it to open an Electron-based app?</span></p></li><li><p><strong>L1 thrashing</strong><span>: Larger or more instructions mean that you can fit fewer of them in the L1 instruction cache. Proper utilization of this cache is critical to achieve reasonable levels of computing performance, and even though main memory size can now be measured in terabytes, the L1 cache is still measured in kilobytes.</span></p></li></ul><p><span>We could beat the dead horse over CISC vs. RISC further but that’s not what I want to focus on. What I want to focus on is one very specific dimension of the instruction encoding that affects code density in all cases. And that’s the </span><em>size of the addresses</em><span> (pointers) required to reference program code or data.</span></p><p>Simply put, the larger the size of the addresses in the program’s code, the lower the code density. The lower the code density, the more memory and disk space we need. And the more memory code takes, the more L1 thrashing we will see.</p><p>So, for good performance, we probably want to optimize the way we represent addresses in the code and take advantage of surrounding context. For example: if we have a tight loop like this:</p><pre><code><code>static int a;
...
for (int i = 0; i &lt; 100; i++) {
    a = a + 1;
}</code></code></pre><p>The corresponding assembly code may be similar to this:</p><pre><code><code>        mov ecx, 0             ; i = 0.
repeat: cmp ecx, 100           ; i &lt; 100?
        je  out                ; If i == 100, jump to out.
        add dword [01234h], 1  ; Increment a.
        add ecx, 1             ; i++.
        jmp repeat             ; Jump back to repeat.
out:</code></code></pre><p>Now the questions are:</p><ul><li><p><span>Given that </span><code>repeat</code><span> and </span><code>out</code><span> are labels for memory addresses, how do we encode the </span><code>je</code><span> (jump if equal) and </span><code>jmp</code><span> (unconditional jump) instructions as bytes? On a 32-bit machine, do we encode the full 32-bit addresses (4 bytes each) in each instruction, or do we use 1-byte relative short pointers because the jump targets are within a 127-byte distance on either side of the jump instruction?</span></p></li><li><p><span>How do we represent the address of the </span><code>a</code><span> variable? Do we express it as an absolute address or as a relative one? Do we store </span><code>1234h</code><span> as a 32-bit quantity or do we use fewer bytes because this specific number is small?</span></p></li><li><p><span>How big should </span><code>int</code><span> be? The </span><code>dword</code><span> above implies 32 bits but… that’s just an assumption I made when writing the snippet. </span><code>int</code><span> could have been 16 bits, or 64 bits, or anything else you can imagine (which is a design mistake in C, if you ask me).</span></p></li></ul><p><span>The answers to the questions above are </span><em>choices</em><span>: we can decide whichever encoding we want for code and data addresses, and these choices have consequences on code density. This was true many years ago during the DOS days, which is why we had short, near, and far pointer types, and </span><a href="https://web.eece.maine.edu/~vweaver/papers/iccd09/iccd09_density.pdf" rel="">is still true today</a><span> as we shall see.</span></p><p>It is no secret that programming 16-bit machines was limiting, and we can identify at least two reasons to back this claim.</p><p><span>First, 16-bit machines usually had limited address spaces. It’s important to understand that a processor’s native register size has no direct connection to the size of the addresses the processor can reference. In theory, these machines could have leveraged larger chunks of memory and, in fact, the 16-bit 8086 proved this to be true with its 20-bit address space. But the point is that, </span><em>in the era</em><span> of 16-bit machines, 1 MB of RAM was considered “a lot” and so machines didn’t have much memory.</span></p><p><span>Second, 16-bit machines can only operate on 16-bit quantities at a time, and 16 bits only allow representing integers with limited ranges: a signed number between -32K and +32K isn’t… particularly large. Like before, it’s important to clarify that the processor’s native register size does not impose restrictions on the size of the numbers the processor can manipulate, but it does add limitations on how </span><em>fast</em><span> it can do so: operating 32-bit numbers on 16-bit machines requires at least twice the number of instructions for each operation.</span></p><p><span>So, when 32-bit processors entered the scene, it was pretty much a no-brainer that programs had to become 32-bit by default: all memory addresses and default integer types grew to 32 bits. This allowed programmers to not worry about memory limits for a while: 32 bits can address 4GB of memory, which was a huge amount back then and should </span><em>still</em><span> be huge if it wasn’t due to bloated software. Also, this upgrade allowed programmers to efficiently manipulate integers with large-enough ranges for most operations.</span></p><p>In technical mumbo-jumbo, what happened here was that C adopted the ILP32 programming model: integers (I), longs (L), and pointers (P) all became 32 bits, whereas chars and shorts remained 8-bit and 16-bit respectively.</p><p><span>It didn’t have to be this way though: if we look at the model for 16-bit programming, shorts and integers were 16-bit whereas longs were 32-bit, so why did integers change size but longs remained the same as integers? I do not have an answer for this, but if longs had become 64 bits back then, maybe we wouldn’t be in the situation today where </span><a href="https://en.wikipedia.org/wiki/Year_2038_problem" rel="">2038 will bring mayhem to Unix systems</a><span>.</span></p><p><span>4GB of RAM were a lot when 32-bit processors launched but slowly became insufficient as software kept growing. To support those needs, there were clutches like </span><a href="https://en.wikipedia.org/wiki/Physical_Address_Extension" rel="">Intel’s PAE</a><span>, which allowed manipulating up to 64GB of RAM on 32-bit machines without changing the programming model, but they were just that: hacks.</span></p><p><span>The thing is: it’s not only software that grew. It’s the </span><em>kinds of things</em><span> that people wanted to do with software that changed: people wanted to edit high-resolution photos and video as well as play more-realistic games, and writing code to achieve those goals on a limited 4GB address space was possible but not </span><em>convenient</em><span>. With 64-bit processors, </span><code>mmap</code><span>ing huge files made those programs easier to write, and using native 64-bit integers made them faster too. So 64-bit machines became mainstream sometime around the introduction of the Athlon 64 processor and the Power Mac G5, both in 2003.</span></p><p><span>So what happened to the programming model? ILP32 was a no-brainer for 32-bit machines, but were LP64 or ILP64 no-brainers for 64-bit machines? These 64-bit models were definitely tempting because they allowed the programmer to leverage the machine’s resources freely. Larger pointers allowed addressing “unlimited” memory transparently, and a larger long type naturally bumped file offsets (</span><code>ino_t</code><span>), timestamps (</span><code>time_t</code><span>), array lengths (</span><code>size_t</code><span>), and more to 64 bits as well. Without a lot of work from programmers, programs could “just do more” by simply recompiling them.</span></p><p><span>But there was a downside to that choice. According to the theory I presented earlier, LP64 would make programs bigger and would decrease code density when compared to ILP32. And lower code density could lead to L1 instruction cache thrashing, which is an important consideration to this day because a modern Intel Core i7-14700K from 2023 has just 80 KB of L1 cache and an Apple Silicon M3 from 2023 has less than 200KB. (These numbers are… </span><em>not big</em><span> when you stack them up against the multi-GB binaries that comprise modern programs, are they?)</span></p><p>We now know that LP64 was the preferred choice and that it became the default programming model for 64-bit operating systems, which means we can compare its impact against ILP32. So what are the consequences? Let’s take a look.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fda7a33-31e0-4cca-ba1c-dda557984d47_1363x844.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fda7a33-31e0-4cca-ba1c-dda557984d47_1363x844.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fda7a33-31e0-4cca-ba1c-dda557984d47_1363x844.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fda7a33-31e0-4cca-ba1c-dda557984d47_1363x844.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fda7a33-31e0-4cca-ba1c-dda557984d47_1363x844.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fda7a33-31e0-4cca-ba1c-dda557984d47_1363x844.png" width="1363" height="844" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6fda7a33-31e0-4cca-ba1c-dda557984d47_1363x844.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:844,&quot;width&quot;:1363,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:37898,&quot;alt&quot;:&quot;Comparison of ISO image sizes for various operating systems&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Comparison of ISO image sizes for various operating systems" title="Comparison of ISO image sizes for various operating systems" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fda7a33-31e0-4cca-ba1c-dda557984d47_1363x844.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fda7a33-31e0-4cca-ba1c-dda557984d47_1363x844.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fda7a33-31e0-4cca-ba1c-dda557984d47_1363x844.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fda7a33-31e0-4cca-ba1c-dda557984d47_1363x844.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Wait, what? The FreeBSD x86-64 installation image is definitely larger than the i386 one… but all other images are </span><em>smaller</em><span>? What’s going on here? This contradicts everything I said above!</span></p><p>I was genuinely surprised by this and I had to dig a bit. Cracking open the FreeBSD bootonly image revealed some differences in the kernel (slightly bigger binaries, but different sets of modules) which made it difficult to compare the two. But looking into the Debian netinst images, I did find that almost all i386 binaries were larger than their x86-64 counterparts.</p><p>To try to understand why that was, the first thing I did was compile a simple hello-world program on my x86-64 Debian VM, targeting both 64-bit and 32-bit binaries:</p><pre><code><code>$ gcc-14 -o hello64 hello.c
$ i686-linux-gnu-gcc-14 -o hello32 hello.c
$ file hello32 hello64
hello32: ELF 32-bit LSB pie executable, Intel 80386, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux.so.2, for GNU/Linux 3.2.0, not stripped
hello64: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=f1bf851d7f1d56ae5d50eb136793066f67607e06, for GNU/Linux 3.2.0, not stripped
$ ls -l hello32 hello64
-rwxrwxr-x 1 jmmv jmmv 15040 Oct  4 17:45 hello32
-rwxrwxr-x 1 jmmv jmmv 15952 Oct  4 17:45 hello64
$ █</code></code></pre><p><span>Based on this, it </span><em>looks</em><span> as if 32-bit binaries are indeed smaller than 64-bit binaries. But a “hello world” program is trivial and not worth 15kb of code: those 15kb shown above are definitely </span><em>not</em><span> code. They are probably mostly ELF overhead. Indeed, if we look at just the text portion of the binaries:</span></p><pre><code><code>$ objdump -h hello32 | grep text
 14 .text         00000169  00001060  00001060  00001060  2**4
$ objdump -h hello64 | grep text
 14 .text         00000103  0000000000001050  0000000000001050  00001050  2**4
$ █</code></code></pre><p><span>… we find that </span><code>hello32</code><span>’s text is 169h bytes whereas </span><code>hello64</code><span>’s text is 103h bytes. And if we disassemble the two:</span></p><pre><code><code>$ objdump --disassemble=main hello32
...
00001189 &lt;main&gt;:
    1189:       8d 4c 24 04             lea    0x4(%esp),%ecx
    118d:       83 e4 f0                and    $0xfffffff0,%esp
    1190:       ff 71 fc                push   -0x4(%ecx)
    1193:       55                      push   %ebp
    1194:       89 e5                   mov    %esp,%ebp
    1196:       53                      push   %ebx
    1197:       51                      push   %ecx
    1198:       e8 28 00 00 00          call   11c5 &lt;__x86.get_pc_thunk.ax&gt;
    119d:       05 57 2e 00 00          add    $0x2e57,%eax
    11a2:       83 ec 0c                sub    $0xc,%esp
    11a5:       8d 90 14 e0 ff ff       lea    -0x1fec(%eax),%edx
    11ab:       52                      push   %edx
    11ac:       89 c3                   mov    %eax,%ebx
    11ae:       e8 8d fe ff ff          call   1040 &lt;puts@plt&gt;
    11b3:       83 c4 10                add    $0x10,%esp
    11b6:       b8 00 00 00 00          mov    $0x0,%eax
    11bb:       8d 65 f8                lea    -0x8(%ebp),%esp
    11be:       59                      pop    %ecx
    11bf:       5b                      pop    %ebx
    11c0:       5d                      pop    %ebp
    11c1:       8d 61 fc                lea    -0x4(%ecx),%esp
    11c4:       c3                      ret
...
$ objdump --disassemble=main hello64
...
0000000000001139 &lt;main&gt;:
    1139:       55                      push   %rbp
    113a:       48 89 e5                mov    %rsp,%rbp
    113d:       48 8d 05 c0 0e 00 00    lea    0xec0(%rip),%rax        # 2004 &lt;_IO_stdin_used+0x4&gt;
    1144:       48 89 c7                mov    %rax,%rdi
    1147:       e8 e4 fe ff ff          call   1030 &lt;puts@plt&gt;
    114c:       b8 00 00 00 00          mov    $0x0,%eax
    1151:       5d                      pop    %rbp
    1152:       c3                      ret
...
$ █</code></code></pre><p><span>We observe massive differences in the machine code generated for the trivial </span><code>main</code><span> function. The 64-bit code is definitely </span><em>smaller</em><span> than the 32-bit code, contrary to my expectations. But the code is also </span><em>very</em><span> different; so different, in fact, that ILP32 vs. LP64 doesn’t explain it.</span></p><p>The first difference we can observe is around calling conventions. The i386 architecture has a limited number of registers, favors passing arguments via the stack, and only 3 registers can be clobbered within a function. x86-64, on the other hand, prefers passing arguments through registers as much as possible and defines 7 registers as volatile.</p><p>The second difference is that we don’t see 64-bit addresses anywhere in the code above. Jump addresses are encoded using near pointers, and data addresses are specified as offsets over a 64-bit base previously stored in a register. I found it smart that those addresses are relative to the program counter (the RIP register).</p><p><span>There may be more differences, but these two alone seem to be the reason why 64-bit binaries end up being more compact than 32-bit ones. </span><em>On Intel x86</em><span>, that is. You see: Intel x86’s instruction set is so versatile that the compiler and the ABI can play tricks to hide the cost of 64-bit pointers.</span></p><p><span>Is that true of more RISC-y 64-bit architectures though? I installed the PowerPC 32-bit and 64-bit toolchains and ran the same test. And guess what? The PowerPC 64-bit binary was indeed larger than the 32-bit one, so </span><em>maybe</em><span> it’s true. Unfortunately, running a broader comparison than this is difficult: there is no full operating system I can find that ships both builds any longer, and ARM images can’t easily be compared.</span></p><p><span>OK, fine, we’ve settled that 64-bit </span><em>code</em><span> isn’t necessarily larger than 32-bit code, at least on Intel, and thus any adverse impact on the L1 instruction cache is probably negligible. But… what about </span><em>data density</em><span>?</span></p><p><span>Pointers don’t only exist in instructions or as jump targets. They also exist within the most modest of data types: lists, trees, graphs… all contain pointers in them. And in those, unless the programmer explicitly plays complex </span><a href="https://v8.dev/blog/pointer-compression" rel="">tricks to compress pointers</a><span>, we’ll usually end up with larger data structures by simply jumping to LP64. The same applies to the innocent-looking </span><code>long</code><span> type by the way, which appears throughout codebases and also grows with this model.</span></p><p><span>And </span><em>this</em><span>—a decrease in data density—is where the real performance penalty comes from: it’s not so much about the code size but about the data size.</span></p><p><span>Let’s take a look. I wrote a simple program that creates a linked list of integers with 10 million nodes and then iterates over them in sequence. Each node is 8 bytes in 32-bit mode (4 bytes for the </span><code>int</code><span> and 4 bytes for the </span><code>next</code><span> pointer), whereas it is 16 bytes in 64-bit mode (4 bytes for the </span><code>int</code><span>, </span><em>4 bytes of padding</em><span>, and 8 bytes for the </span><code>next</code><span> pointer). I then compiled that program in 32-bit and 64-bit mode, measured it, and ran it:</span></p><pre><code><code>$ gcc -o list64 list.c
$ i686-linux-gnu-gcc-14 -o list32 list.c
$ objdump -h list32 | grep text
 13 .text         000001f6  00001070  00001070  00001070  2**4
$ objdump -h list64 | grep text
 14 .text         000001b0  0000000000001060  0000000000001060  00001060  2**4
$ hyperfine --warmup 1 ./list32 ./list64
Benchmark 1: ./list32
  Time (mean ± σ):     394.2 ms ±   2.1 ms    [User: 311.4 ms, System: 83.1 ms]
  Range (min … max):   392.1 ms … 398.2 ms    10 runs

Benchmark 2: ./list64
  Time (mean ± σ):     502.4 ms ±   4.5 ms    [User: 334.9 ms, System: 167.8 ms]
  Range (min … max):   494.9 ms … 509.5 ms    10 runs

Summary
  ./list32 ran
    1.27 ± 0.01 times faster than ./list64
$ █</code></code></pre><p><span>As before with the hello-world comparison, this simple microbenchmark’s 32-bit code continues to be slightly larger than its 64-bit counterpart (1F6h bytes vs 1B0h). However, its runtime is </span><em>27% faster</em><span>, and it is no wonder because the doubling of the linked list node size implies doubling the memory usage and thus the halving of cache hits. We can confirm the impact of this using the </span><code>perf</code><span> tool:</span></p><pre><code><code>$ perf stat -B -e cache-misses ./list32 2&gt;&amp;1 | grep cpu_core
         2,400,339      cpu_core/cache-misses:u/      (99.33%)
$ perf stat -B -e cache-misses ./list64 2&gt;&amp;1 | grep cpu_core
         4,687,156      cpu_core/cache-misses:u/      (99.34%)
$ █</code></code></pre><p>The 64-bit build of this microbenchmark incurs almost double the cache misses than the 32-bit build.</p><p>Of course, this is just a microbenchmark, and tweaking it slightly will make it show very different results and make it say whatever we want. I tried to add jitter to the memory allocations so that the nodes didn’t end up as consecutive in memory, and then the 64-bit version executed faster. I suspect this is due to the memory allocator having a harder time handling memory when the address space is limited.</p><p>The impact on real world applications is harder to quantify. It is difficult to find the same program built in 32-bit and 64-bit mode and to run it on the same kernel. It is even more difficult to find one such program where the difference matters. But at the end of the day, the differences exist, and I bet they are more meaningful than we might think in terms of bloat—but I did not intend to write a research paper here, so I’ll leave that investigation to someone else… or another day.</p><p>There is one last thing to discuss before we depart. While we find ourselves using the LP64 programming model on x86-64 processors, remember that this was a choice and there were and are other options on the table.</p><p>Consider this: we could have made the operating system kernel leverage 64 bits to gain access to a humongous address space, but we could have kept the user-space programming model as it was before—that is, we could have kept ILP32. And we could have gone even further and optimized the calling conventions to reduce the binary code size by leveraging the additional general-purpose registers that x86-64 provides.</p><p><span>And in fact, this exists and is known as </span><a href="https://en.wikipedia.org/wiki/X32_ABI" rel="">x32</a><span>.</span></p><p>We can install the x32 toolchain and see that it effectively works as we’d imagine:</p><pre><code><code>$ gcc -o hello64 hello.c
$ x86_64-linux-gnux32-gcc-14 -o hellox32 hello.c
$ objdump --disassemble=main hello64
...
0000000000001139 &lt;main&gt;:
    1139:       55                      push   %rbp
    113a:       48 89 e5                mov    %rsp,%rbp
    113d:       48 8d 05 c0 0e 00 00    lea    0xec0(%rip),%rax        # 2004 &lt;_IO_stdin_used+0x4&gt;
    1144:       48 89 c7                mov    %rax,%rdi
    1147:       e8 e4 fe ff ff          call   1030 &lt;puts@plt&gt;
    114c:       b8 00 00 00 00          mov    $0x0,%eax
    1151:       5d                      pop    %rbp
    1152:       c3                      ret
...
$ objdump --disassemble=main hellox32
...
00401126 &lt;main&gt;:
  401126:       55                      push   %rbp
  401127:       89 e5                   mov    %esp,%ebp
  401129:       b8 04 20 40 00          mov    $0x402004,%eax
  40112e:       89 c0                   mov    %eax,%eax
  401130:       48 89 c7                mov    %rax,%rdi
  401133:       e8 f8 fe ff ff          call   401030 &lt;puts@plt&gt;
  401138:       b8 00 00 00 00          mov    $0x0,%eax
  40113d:       5d                      pop    %rbp
  40113e:       c3                      ret
...
$ █</code></code></pre><p><span>Now, the </span><code>main</code><span> method of our hello-world program is really similar between the 64-bit and 32-bit builds, but pay close attention to the x32 version: it uses the same calling conventions as x86-64, but it contains a mixture of 32-bit and 64-bit registers, delivering a more compact binary size.</span></p><p>Unfortunately:</p><pre><code><code>$ ./hellox32
zsh: exec format error: ./hellox32
$ █</code></code></pre><p>We can’t run the resulting binary. x32 is an ABI that impacts the kernel interface too, so these binaries cannot be executed on a regular x86-64 kernel. Sadly, and as far as I can tell, x32 is pretty much abandonware today. Gentoo claims to support it but there are no official builds of any distribution I can find that are built in x32 mode.</p><p><span>In the end, even though LP64 </span><a href="https://unix.org/version2/whatsnew/lp64_wp.html" rel="">wasn’t the obvious choice</a><span> for x86-64, it’s the compilation mode that won and stuck.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An illustrated proof of the CAP theorem (2018) (245 pts)]]></title>
            <link>https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/</link>
            <guid>41772624</guid>
            <pubDate>Tue, 08 Oct 2024 00:32:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/">https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/</a>, See on <a href="https://news.ycombinator.com/item?id=41772624">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="container">
    
    <p>
    The <a href="http://en.wikipedia.org/wiki/CAP_theorem">CAP Theorem</a> is a
    fundamental theorem in distributed systems that states any distributed
    system can have at most two of the following three properties.
    </p>

    <ul>
      <li><strong>C</strong>onsistency</li>
      <li><strong>A</strong>vailability</li>
      <li><strong>P</strong>artition tolerance</li>
    </ul>

    <p>
    This guide will summarize
    <a href="http://lpd.epfl.ch/sgilbert/pubs/BrewersConjecture-SigAct.pdf">
      Gilbert and Lynch's specification and proof of the CAP Theorem</a>
    with pictures!
    </p>

    <h2>What is the CAP Theorem?</h2>
    <p>
    The CAP theorem states that a distributed system cannot simultaneously be
    consistent, available, and partition tolerant. Sounds simple enough, but
    what does it mean to be consistent? available? partition tolerant? Heck,
    what exactly do you even mean by a distributed system?
    </p>

    <p>
    In this section, we'll introduce a simple distributed system and explain
    what it means for that system to be available, consistent, and partition
    tolerant.  For a formal description of the system and the three properties,
    please refer to
    <a href="http://lpd.epfl.ch/sgilbert/pubs/BrewersConjecture-SigAct.pdf">
      Gilbert and Lynch's paper</a>.
    </p>

    <h2>A Distributed System</h2>
    <p>
    Let's consider a very simple distributed system. Our system is composed of
    two servers, $G_1$ and $G_2$. Both of these servers are keeping track of
    the same variable, $v$, whose value is initially $v_0$. $G_1$ and $G_2$
    can communicate with each other and can also communicate with external
    clients. Here's what our system looks like.
    </p>

    <center>
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap1.svg">
    </center>

    <p>
    A client can request to write and read from any server. When a server
    receives a request, it performs any computations it wants and then responds
    to the client. For example, here is what a write looks like.
    </p>

    <center>
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap2.svg">
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap3.svg">
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap4.svg">
    </center>

    <p>
    And here is what a read looks like.
    </p>

    <center>
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap5.svg">
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap6.svg">
    </center>

    <p>
    Now that we've gotten our system established, let's go over what it means
    for the system to be consistent, available, and partition tolerant.
    </p>

    <h2>Consistency</h2>
    <p>
    Here's how Gilbert and Lynch describe consistency.
    </p>

    <blockquote>
    any read operation that begins after a write operation completes must
    return that value, or the result of a later write operation
    </blockquote>

    <p>
    In a consistent system, once a client writes a value to any server and gets
    a response, it expects to get that value (or a fresher value) back from any
    server it reads from.
    </p>

    <p>
    Here is an example of an <strong>inconsistent</strong> system.
    </p>

    <center>
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap7.svg">
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap8.svg">
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap9.svg">
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap10.svg">
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap11.svg">
    </center>

    <p>
    Our client writes $v_1$ to $G_1$ and $G_1$ acknowledges, but when it
    reads from $G_2$, it gets stale data: $v_0$.
    </p>

    <p>
    On the other hand, here is an example of a <strong>consistent</strong>
    system.
    </p>

    <center>
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap12.svg">
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap13.svg">
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap14.svg">
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap15.svg">
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap16.svg">
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap17.svg">
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap18.svg">
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap19.svg">
    </center>

    <p>
    In this system, $G_1$ replicates its value to $G_2$ before sending an
    acknowledgement to the client. Thus, when the client reads from $G_2$, it
    gets the most up to date value of $v$: $v_1$.
    </p>

    <h2>Availability</h2>
    <p>
    Here's how Gilbert and Lynch describe availability.
    </p>

    <blockquote>
    every request received by a non-failing node in the system must result in a
    response
    </blockquote>

    <p>
    In an available system, if our client sends a request to a server and the
    server has not crashed, then the server must eventually respond to the
    client. The server is not allowed to ignore the client's requests.
    </p>

    <h2>Partition Tolerance</h2>
    <p>
    Here's how Gilbert and Lynch describe partitions.
    </p>

    <blockquote>
    the network will be allowed to lose arbitrarily many messages sent from one
    node to another
    </blockquote>

    <p>
    This means that any messages $G_1$ and $G_2$ send to one another can be
    dropped.  If all the messages were being dropped, then our system would
    look like this.
    </p>

    <center>
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap20.svg">
    </center>

    <p>
    Our system has to be able to function correctly despite arbitrary network
    partitions in order to be partition tolerant.
    </p>

    <h2>The Proof</h2>
    <p>
    Now that we've acquainted ourselves with the notion of consistency,
    availability, and partition tolerance, we can prove that a system cannot
    simultaneously have all three.
    </p>

    <p>
    Assume for contradiction that there does exist a system that is consistent,
    available, and partition tolerant. The first thing we do is partition our
    system. It looks like this.
    </p>

    <center>
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap21.svg">
    </center>

    <p>
    Next, we have our client request that $v_1$ be written to $G_1$. Since
    our system is available, $G_1$ must respond. Since the network is
    partitioned, however, $G_1$ cannot replicate its data to $G_2$. Gilbert
    and Lynch call this phase of execution $\alpha_1$.
    </p>

    <center>
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap22.svg">
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap23.svg">
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap24.svg">
    </center>

    <p>
    Next, we have our client issue a read request to $G_2$. Again, since our
    system is available, $G_2$ must respond. And since the network is
    partitioned, $G_2$ cannot update its value from $G_1$. It returns $v_0$.
    Gilbert and Lynch call this phase of execution $\alpha_2$.
    </p>

    <center>
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap25.svg">
      <img src="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/assets/cap26.svg">
    </center>

    <p>
    $G_2$ returns $v_0$ to our client after the client had already written
    $v_1$ to $G_1$. This is inconsistent.
    </p>

    <p>
    We assumed a consistent, available, partition tolerant system existed, but
    we just showed that there exists an execution for any such system in which
    the system acts inconsistently. Thus, no such system exists.
    </p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Video Surveillance with YOLO+llava (230 pts)]]></title>
            <link>https://github.com/PsyChip/machina</link>
            <guid>41772551</guid>
            <pubDate>Tue, 08 Oct 2024 00:21:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/PsyChip/machina">https://github.com/PsyChip/machina</a>, See on <a href="https://news.ycombinator.com/item?id=41772551">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">MACHINA</h2><a id="user-content-machina" aria-label="Permalink: MACHINA" href="#machina"></a></p>
<p dir="auto">CCTV viewer with realtime object tagger [WIP]</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/PsyChip/machina/blob/main/demo.png"><img src="https://github.com/PsyChip/machina/raw/main/demo.png" alt="partial screenshot"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Uses</h3><a id="user-content-uses" aria-label="Permalink: Uses" href="#uses"></a></p>
<ul dir="auto">
<li><a href="https://llava-vl.github.io/" rel="nofollow">LLAVA</a></li>
<li><a href="https://github.com/ultralytics/ultralytics">YOLO 11</a></li>
<li><a href="https://opencv.org/" rel="nofollow">OpenCV</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">How it works</h3><a id="user-content-how-it-works" aria-label="Permalink: How it works" href="#how-it-works"></a></p>
<p dir="auto">Simply it connects to a high-resolution RTSP stream in a separate thread,
queues the frames into memory as it is and resamples it for processing.</p>
<p dir="auto">YOLO takes this frame, application gives a specific id based on it's coordinates,
size and timestamp then tries to match the same object on every iteration.</p>
<p dir="auto">Another thread runs in background, iterates that object array continuously and
makes LLM requests to Ollama server for object tagging</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Object matching</h3><a id="user-content-object-matching" aria-label="Permalink: Object matching" href="#object-matching"></a></p>
<p dir="auto">It calculates the center of every detection box, pinpoint on screen and gives 16px
tolerance on all directions. Script tries to find closest object as fallback and
creates a new object in memory in last resort.
You can observe persistent objects in <code>/elements</code> folder</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Test Environment</h3><a id="user-content-test-environment" aria-label="Permalink: Test Environment" href="#test-environment"></a></p>
<p dir="auto">Every input frame resampled to 640x480 for processing, got avg 20ms interference time
with yolo 11 small model (yolo11s.pt) on Geforce GTX 1060 which is almost 7 years old
graphics card. Other models available in "models" directory</p>
<p dir="auto">Stream delays by 1-2 seconds on every 10~ minutes due to network conditions, script also
have a frame skip mechanism on 3 seconds of detection idle.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites</h3><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li>Clone the repository</li>
<li>Install <a href="https://ollama.com/" rel="nofollow">ollama</a> server</li>
<li>Pull the LLAVA model by running <code>ollama run llava</code></li>
<li>Open <code>app.py</code> and set your rtmp stream address at line 18</li>
<li>Install the dependencies by running <code>pip install -r requirements.txt</code></li>
<li>Run the script <code>py app.py</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Shortcuts</h3><a id="user-content-shortcuts" aria-label="Permalink: Shortcuts" href="#shortcuts"></a></p>
<ul dir="auto">
<li>S : snapshot, actual image from input stream</li>
<li>R : start/stop recording. it records what you see.</li>
<li>Q : quit app</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Project direction</h3><a id="user-content-project-direction" aria-label="Permalink: Project direction" href="#project-direction"></a></p>
<p dir="auto">This is a living project, trying to create a <em>complete</em> headless security system by
taking advantage of modern vision, object detection models on my spare time.</p>
<p dir="auto">Feel free to contribute with code, ideas or even maybe a little bit donation
via ko-fi or bitcoin</p>
<p dir="auto">-<a href="https://ko-fi.com/psychip" rel="nofollow">https://ko-fi.com/psychip</a>
-BTC: bc1qlq067vldngs37l5a4yjc4wvhyt89wv3u68dsuv</p>
<p dir="auto">Created by PsyChip</p>
<ul dir="auto">
<li><a href="mailto:root@psychip.net">root@psychip.net</a></li>
</ul>
<p dir="auto">.eof</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rust is rolling off the Volvo assembly line (113 pts)]]></title>
            <link>https://tweedegolf.nl/en/blog/137/rust-is-rolling-off-the-volvo-assembly-line</link>
            <guid>41771272</guid>
            <pubDate>Mon, 07 Oct 2024 21:30:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tweedegolf.nl/en/blog/137/rust-is-rolling-off-the-volvo-assembly-line">https://tweedegolf.nl/en/blog/137/rust-is-rolling-off-the-volvo-assembly-line</a>, See on <a href="https://news.ycombinator.com/item?id=41771272">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><strong>In my job I get to speak to lots of people about Rust. Some are just starting out, some have barely ever heard of it, and then some people are running Rust silently in production at a very large company in a very serious product.</strong></p><div><p>A while back, I had the fortune to talk with <a href="https://www.linkedin.com/in/julius-gustavsson/">Julius Gustavsson</a> from Sweden and he squarely falls in the last category of people. From 2019 onwards, he has been the main software architect for the low-power processor ECU (electronic control unit) at <a href="https://www.volvo.com/en/">Volvo</a>.</p><p>This ECU is responsible for the (low) power management of the car. Electric cars obviously have massive high-voltage batteries, but the classic 12 volt lines are still there. Those lines are always on and can drain the battery if there's any power usage, so you want all car systems to be turned off when you're away from your car. The ECU is responsible for waking up the electric system when needed, for example when you approach the car.</p><p>This ECU was not actively being worked on in 2019 and so Julius became part of a new dedicated team. But even back in 2017, when Julius joined Volvo, he already knew about Rust and saw its potential to replace existing C and C++ code.</p><p>It turned out the low-power processor was a perfect fit for using Rust! It was not classified as a safety-critical component and it was an Arm Cortex-M processor, so there was no technical or bureaucratic blocker for using Rust.</p><p>And so it has come to be that, at this moment, EX90s and Polestar 3s are rolling off the assembly line that would not work without their Rust components.</p><p>I think that's a great milestone for Rust!</p><p>I wanted to know more about this and thought the world would too, so Julius has kindly agreed to let me interview him. Here's everything we talked about:</p><h3 id="q-why-did-you-pick-rust">Q: Why did you pick Rust?</h3><p>Julius told me his first job was building air traffic control software, where a lot of Ada is used. A competitor was even required to use Ada by the US Air Force. "The language is amazing in its own way. However, the consensus at the company was that Ada was both too arcane and to proprietary for them, at that time."</p><p>And so he used a mixture of C and C++ for about 15 years. "At all different companies I worked at they all had different strategies and strictness, but memory-related bugs were always a problem".</p><p>"It always felt unsafe; Most codebases have a bunch of invariants and assumptions that are not written down but everyone must uphold. As the project grows in complexity and especially team size, this will inevitably fail at some point. After debugging the umpteenth bug, the thought came, 'Is this it? Isn't there a better way?'"</p><p>He had discovered Rust before its 1.0 release in 2015, and began paying more attention to it after the release. So when he joined Volvo Julius had a little bit of hobby experience. He had found it tricky to pick up the Rust concepts and had to read the O'Reilly book twice. However, "once it clicked, I got very enthusiastic."</p><p>Picking Rust for the ECU project didn't come out of the blue.</p><p>"When we were prototyping the precursor to the project, and doing interop with Android, I created a vehicle HAL in Rust using futures 0.1, back in the days before async, for Android that spoke grpc to the Rust system," he explained.</p><p>Describing the project, he said: "We'd have buttons on the screen to control the fans of the car. I had to write a lot of code before I could compile it all, a big jenga tower. But once it compiled, the fans started to work! Very impressed."</p><p>It was difficult to figure out how to build Rust for use with Android, which was the platform this prototype ran on. Today much more effort has been put in tooling around this, but Julius was there very early.</p><p>With this prototype he proved to himself and the people around him that Rust could be a serious option for production code; It delivered on its promises.</p><h3 id="q-how-did-it-go">Q: How did it go?</h3><p>Aside from the project being a good fit for Rust due to it not being safety-critical and running on common hardware, it was also quite straightforward due to its limited feature set.</p><p>In 2020 they had made a first proof-of-concept in C before continuing the project with Rust. What wasn't so straightforward was that the ECU had to communicate over CAN with the other systems in the car and they had to implement all diagnostics systems and port over the standard Volvo protocols.</p><p>So they needed to reimplement a lot of things. According to Julius though, they "got much higher quality". And they found they were writing far fewer bugs compared to C and C++.</p><p>Julius wasn't the only one really liking Rust. One of his colleagues had to leave and said: "It's hard to think about having to go back to something that's not Rust". Still, a healthy dose of scepticism remained. As Julius put it: "I always had the feeling, is Rust too good to be true? I'm always looking for the big pitfall. So far I have not found anything bad. Only some small things like const generics not being fully done."</p><p>As the project progressed, they got "a bigger and bigger pile of proof that Rust does actually work well".</p><p>There were regular cross-team meetings where team leads could discuss their problems. As time went on it became more and more noticeable that Julius didn't bring up many issues at all and when he showed his results, his colleagues were often left impressed.</p><h3 id="q-would-you-recommend-rust-to-others">Q: Would you recommend Rust to others?</h3><p>"Definitely", Julius answered promptly.</p><p>He continues: "For any project where you have very strict reliability and availability requirements, and you want to be confident that what you deploy is actually correct, then Rust is an excellent choice! Also <code>cargo</code> and all the other available tooling make the whole cycle of developing high quality software a really nice experience."</p><p>Rust also works great for teams with high turnover because there's a lot of confidence in the code. "Other people can just take over the code and fiddle around safely because when it compiles, it almost always works".</p><p>"For prototyping, then maybe it's not the best option because things are rigid. The compiler forces you to work more on the edge cases and minute details up front, something you are not always interested in doing at that particular point", he noted. This, of course, is an often expressed sentiment.</p><p>But to put that in perspective, Julius says: "Rust would work for most of the software I've worked on professionally. There are absolutely cases where Rust is not the best fit. But I think we're at that point where instead of asking 'Can we use Rust for this?', we should be asking 'Why can't we use Rust for this?' And then you have the discussion."</p><h3 id="q-whats-missing-that-got-in-the-way">Q: What's missing that got in the way?</h3><p>It wasn't easy to create the software so that it properly fit the requirements, according to Julius. This is mostly a tooling issue.</p><p>For example, it was hard to run the unit tests on the embedded target. Other challenges included code coverage, runtime profiling, software BOM and license tracking. This is my personal experience as well, although the state of these issues has much improved over the last couple of years.</p><p>Julius and I agreed that it would be very valuable to build more and better tooling. He said: "Most are halfway there, but you still need to do a lot yourself".</p><p>During the project, things got better. Julius explicitly mentioned the <a href="https://github.com/knurling-rs">Knurling</a> project, for example, and that those tools helped a lot.</p><h3 id="q-are-you-going-to-use-rust-in-the-future">Q: Are you going to use Rust in the future?</h3><p>"Yes."</p><p>In fact, Julius is actively cheerleading for other projects to pick up Rust. And there seems to be an overall enthusiasm for Rust at many  layers in the company.</p><p>A couple of days before my meeting with Julius, he had the final project presentation with the management. The result of that meeting was a common consensus among management to look into using Rust in more places.</p><h3 id="conclusion">Conclusion</h3><p>From our conversation it seems that using Rust has been a great success at Volvo thus far. People are happy, the quality of the product is high, and the company seems poised to use more Rust in the future.</p><p>It's obvious that there's more work to be done still, but with more safety-critical tooling like <a href="https://ferrocene.dev/en/">Ferrocene</a> becoming available, Rust is readier than ever for use in the automotive industry.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[uBlock Origin supports filtering CNAME cloaking sites on Firefox now (273 pts)]]></title>
            <link>https://github.com/gorhill/uBlock/commit/6acf97bf51</link>
            <guid>41770921</guid>
            <pubDate>Mon, 07 Oct 2024 20:52:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/gorhill/uBlock/commit/6acf97bf51">https://github.com/gorhill/uBlock/commit/6acf97bf51</a>, See on <a href="https://news.ycombinator.com/item?id=41770921">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  

    






  
  

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div id="repo-content-pjax-container">
  <p>
  <h2>Commit</h2>
</p>

<p><a href="https://github.com/gorhill/uBlock/commit/6acf97bf5143543c036c38a82160e5f8efe8b3f1" data-hotkey="y">Permalink</a></p>


<div>
  <div>
    <div>
        <p>
          Rewrite cname uncloaking code to account for new <code>ipaddress=</code> option
        </p>
    </div>

    <p><a id="browse-at-time-link" href="https://github.com/gorhill/uBlock/tree/6acf97bf5143543c036c38a82160e5f8efe8b3f1" rel="nofollow">Browse files</a></p><tool-tip id="tooltip-d69658f2-caa3-4a59-92f1-c9ca107e65a1" for="browse-at-time-link" popover="manual" data-direction="ne" data-type="description" data-view-component="true">Browse the repository at this point in the history</tool-tip>
  </div>

    <div><pre>This commit makes the DNS resolution code better suited for both
filtering on cname and ip address. The change allows early availability
of ip address so that `ipaddress=` option can be matched at
onBeforeRequest time.

As a result, it is now possible to block root document using
`ipaddress=` option -- so long as an ip address can be extracted
before first onBeforeRequest() call.

Related issue:
<a data-error-text="Failed to load title" data-id="1875533514" data-permission-text="Title is private" data-url="https://github.com/uBlockOrigin/uBlock-issues/issues/2792" data-hovercard-type="issue" data-hovercard-url="/uBlockOrigin/uBlock-issues/issues/2792/hovercard" href="https://github.com/uBlockOrigin/uBlock-issues/issues/2792">uBlockOrigin/uBlock-issues#2792</a>

Caveat
------

the ip address used is the first one among the list of ip
addresses returned by dns.resolve() method. There is no way for uBO
to know which exact ip address will be used by the browser when
sending the request, so this is at most a best guess. The exact IP
address used by the browser is available at onHeadersReceived time,
and uBO will also filter according to this value, but by then the
network request has already been sent to the remote server.

Possibly a future improvement would make available the whole list
of ip addresses to the filtering engine, but even then it's impossible
to know with certainty which ip address will ultimately be used by the
browser -- it is entirely possible that the ip address used by the
browser might not be in the list received through dns.resolve().</pre></div>

  <div>
  <include-fragment src="/gorhill/uBlock/branch_commits/6acf97bf5143543c036c38a82160e5f8efe8b3f1" id="async-branches-list">
    <div>
      
      <ul>
        <li>Loading branch information<span></span></li>
      </ul>
    </div>
</include-fragment></div>


  
</div>


  


  <diff-layout>
    
        </diff-layout>


</div>

</turbo-frame>


    </main>
  </div></div>]]></description>
        </item>
    </channel>
</rss>