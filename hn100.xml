<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 21 Oct 2025 13:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Tesla is heading into multi-billion-dollar iceberg of its own making (157 pts)]]></title>
            <link>https://electrek.co/2025/10/20/tesla-heading-into-multi-billion-dollar-iceberg-of-own-making/</link>
            <guid>45654635</guid>
            <pubDate>Tue, 21 Oct 2025 11:39:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2025/10/20/tesla-heading-into-multi-billion-dollar-iceberg-of-own-making/">https://electrek.co/2025/10/20/tesla-heading-into-multi-billion-dollar-iceberg-of-own-making/</a>, See on <a href="https://news.ycombinator.com/item?id=45654635">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>

	<img width="1600" height="909" src="https://electrek.co/wp-content/uploads/sites/3/2025/10/Elon-Musk-tesla-FSD-iceberg.png?w=1600" alt="" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/10/Elon-Musk-tesla-FSD-iceberg.png?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/10/Elon-Musk-tesla-FSD-iceberg.png?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/10/Elon-Musk-tesla-FSD-iceberg.png?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/10/Elon-Musk-tesla-FSD-iceberg.png?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high">
	</figure>

<p>Tesla’s ‘Full Self-Driving Supervised’ expansion is back firing as it exposes its shortcomings. Customers left without promised features are growing discontent and demanding to be compensated.</p>



<p>It’s turning into a multi-billion-dollar iceberg of Tesla’s own making.</p>



<p>In 2016, Tesla proudly announced that all its vehicles produced onward are equipped with “all the hardware for full self-driving,” which would be delivered through future software updates.</p>



<p>The automaker turned out to be significantly wrong about that.</p>	
	



<p>At the time, it was producing its electric vehicles with a hardware suite known as HW2, which it had to upgrade to HW3 because it couldn’t support self-driving (FSD) capability.</p>



<p>HW3 was produced in vehicles from 2019 to 2023 and Tesla switched to HW4 in 2024.</p>



<p>At first, CEO Elon Musk claimed that FSD software updates on newer HW4 cars would lag roughly 6 months behind updates to HW3 cars to make sure to deliver the promised self-driving capability to those who have been waiting and paid for the promised capabiltiy a long time ago.</p>



<p>That s<a href="https://electrek.co/2024/10/15/tesla-needs-to-come-clean-about-hw3-before-the-word-fraud-comes-out/">trategy barely lasted a few months</a>. Tesla quickly started releasing new FSD updates to HW4 cars first and it now hasn’t released a significant update to HW3 cars in close to a year.</p>



<p>Tesla only admitted in January 2025 that HW3 won’t be able to support unsupervised self-driving. Musk claimed that Tesla would retrofit the computers, but there has been no word about it for 10 months.</p>



<h2 id="h-tesla-customers-are-starting-to-be-fed-up">Tesla customers are starting to be fed up.</h2>



<p>The catalyst is Tesla’s current FSD expansion in international markets. Previously, Tesla’s FSD was limited to North America, but over the last year, the automaker has been expanding FSD to China and now Australia and New Zealand.</p>



<p>However, the expansion is back-firing as HW3 owners are starting to realize that they will never get what they paid for.</p>



<p>In Australia and NZ, Tesla only launched FSD on HW4 vehicles with no clear plan for HW3, which the automaker already admitted won’t support unsupervised self-driving. The automaker appears to have only adapted its latest version of FSD for HW4 to the Australian market.</p>



<p>To add to the insult, with the launch of FSD in Australia, Tesla started to offer FSD subcriptions for $149 AUD a month for both HW3 and HW3 cars despite the software not being available for HW3.</p>



<p>HW3 owners reached out to <em>Electrek</em> after seeing this in their app:</p>



<figure><img decoding="async" width="926" height="922" src="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png" alt="" srcset="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png 926w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=150,149 150w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=300,300 300w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=768,765 768w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=350,348 350w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=140,139 140w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=96,96 96w" sizes="(max-width: 926px) 100vw, 926px"></figure>



<p>It’s unclear why would Tesla sell a subcription to something that doesn’t even exist, but it is not helping build confidence with customers.</p>



<p>To try to appease owners, Tesla started sending emails to Australia HW3 owners offering $5,000 discounts on new inventory vehicles when transfering their FSD package:</p>



<figure><img loading="lazy" decoding="async" height="1024" width="546" src="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?w=546" alt="" srcset="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png 618w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=80,150 80w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=160,300 160w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=546,1024 546w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=187,350 187w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=140,262 140w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=534,1000 534w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=150,281 150w" sizes="auto, (max-width: 546px) 100vw, 546px"></figure>



<p>However, this offer is misleading in itself, as it is not actually specific to HW3 owners as the email leads people to believe.</p>



<p>A visit on Tesla’s Australia inventory website shows that Tesla is offering a $5,000 disounct on all inventory vehicles with FSD for any buyer:</p>



<figure><img loading="lazy" decoding="async" height="384" width="1024" src="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?w=1024" alt="" srcset="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png 2966w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=150,56 150w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=300,112 300w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=768,288 768w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=1024,384 1024w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=1536,576 1536w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=2048,768 2048w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=350,131 350w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=140,52 140w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=1600,600 1600w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>Therefore, it has nothing to do with “loyalty”.</p>



<p>As we recently reported, thousands of Tesla owners have now joined <a href="https://electrek.co/2025/10/13/thousands-of-tesla-owners-join-class-action-lawsuit-over-full-self-driving-in-australia/">a class action lawsuit in Australia</a> over Tesla misleading customers with its self-driving promises.</p>



<p>It adds to similar ongoing lawsuits in <a href="https://electrek.co/2025/08/19/tesla-loses-bid-to-kill-class-action-over-misleading-customers-on-self-driving-capabilities-for-years/">the US</a> and <a href="https://electrek.co/2025/09/22/tesla-being-sued-china-over-not-delivering-self-driving-hw3-cars/">China</a>.</p>



<p>With hundreds of thousands of FSD customers who paid up to $15,000 for package, Tesla is on the hook for billions of dollars in compensations or retrofits in the best-case scenario.</p>



<h2 id="h-electrek-s-take">Electrek’s Take</h2>



<p>We are seeing more people losing patience and it is only going to get worse.</p>



<p>There were a lot of interesting interactions on this post, which is pretty mild in my opinion. And yet, you see the usual Elon lemmings downplaying Tesla not delivering features it promised:</p>



<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Dear <a href="https://twitter.com/Tesla_AI?ref_src=twsrc%5Etfw">@Tesla_AI</a> Team,<br>I am writing on behalf of the community of Tesla owners equipped with Hardware 3 (HW3) who have purchased the Full Self-Driving (FSD) capability. As dedicated supporters of Tesla’s mission to accelerate the world’s transition to sustainable energy and advance…</p>— shawn.car◼️◼️◼️◼️◼️◼️ (@shawncarelli) <a href="https://twitter.com/shawncarelli/status/1980023896536391866?ref_src=twsrc%5Etfw">October 19, 2025</a></blockquote>
</div></figure>



<p>I don’t want to burst anyone’s bubble, but we need to be realistic here. If you are a HW3 owner and still think that Tesla is going to retrofit your up to 10-years-old car with a computer that is going to make self-driving, you are being delusional.</p>



<p>Tesla will have to end up compensating owners and at this point, I have serious doubts that it will do it by itself without being forced through courts.</p>



<p>Furthermore, it shouldn’t be just people who bought FSD. Tesla said that all cars had the hardware capable of self-driving whether people bought the software package or not. If that’s not true, it affects the resale value of the vehicle regardless of if someone purchased the package.</p>



<p>I have a fairly simple solution for Tesla to make it right.</p>



<p>Tesla needs to offer all HW3 owners a $5,000 loyalty discount, that goes on top of all other incentive program, when upgrading to a new car. </p>




	<p>As for HW3 owners who bought FSD, which basically turned out to be an interest free loan to Tesla for years, the automaker needs to offer free FSD transfer and a $10,000 discount on a car upgrade.</p>



<p>While this might sound like a lot, I think it’s in line with the incredible liability that Tesla is facing from all the on going lawsuits. </p>



<p>On top of it, it will go a long way to regain the trust of long-time customers, which Tesla swindled by selling them features it simply can’t deliver.</p>



<p>The main reason why I think Tesla doesn’t want to do that is that it will likely have to do the same thing to HW4 owners in the next few years and that would be the death of the company.</p>
	<p>
				<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
			</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US chess grandmaster Daniel Naroditsky dies aged 29 (125 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c15pz8vpjp9o</link>
            <guid>45654382</guid>
            <pubDate>Tue, 21 Oct 2025 10:44:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c15pz8vpjp9o">https://www.bbc.com/news/articles/c15pz8vpjp9o</a>, See on <a href="https://news.ycombinator.com/item?id=45654382">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div data-testid="byline-new" data-component="byline-block"><p><span data-testid="byline-new-contributors"><p><span>Harry Sekulich</span><span data-testid="undefined-role-location"></span><span> and</span></p><p><span>Gabriela Pomeroy</span><span data-testid="undefined-role-location"></span></p></span></p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251015-170100-8e96f025b0-web-2.31.4-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp" loading="eager" alt="Charlotte Chess Center Close-up shot of Daniel Naroditsky wearing a deep navy polo"><span>Charlotte Chess Center</span></p></div><p data-component="caption-block"><figcaption>Daniel Naroditsky, also known to his online fans as 'Danya', died two weeks out from his 30th birthday</figcaption></p></figure><div data-component="text-block"><p>US chess grandmaster and online commentator Daniel Naroditsky has died aged 29.</p><p>The popular chess player's family announced his "unexpected" death in a statement released by his club, the Charlotte Chess Center, on Monday. No cause of death was given.</p><p>"It is with great sadness that we share the unexpected passing of Daniel Naroditsky," the statement said. "Daniel was a talented chess player, commentator and educator, and a cherished member of the chess community, admired and respected by fans and players around the world."</p><p>The US and International chess federations have paid tribute to Naroditsky, along with other professional players.</p></div><div data-component="text-block"><p>American world number two Hikaru Nakamura said he was "devastated" at the news.</p><p>"This is a massive loss for the world of chess," Nakamura said in a social media post.</p><p>As well as competing in high-level events, Naroditsky ran a chess YouTube channel, with nearly 500,000 subscribers. </p><p>His Twitch stream drummed up 340,000 followers, with hundreds of thousands of viewers drawn to his regular video tutorials and livestreams against competitors. Fans praised his insight and passion, casually referring to him as 'Danya'.</p><p>He played a "pivotal role in popularising chess content online," the International Chess Federation said. </p><p>Naroditsky first took an interest in chess at the age of six, when his older brother Alan introduced him to the game to help entertain a group of children at a birthday party.</p><p>His father Vladimir and multiple coaches soon noticed his talents.</p><p>"As far as I was concerned, I was just playing games with my brother," Naroditsky told the New York Times in a 2022 interview.</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251015-170100-8e96f025b0-web-2.31.4-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp" loading="lazy" alt="Getty Images A young Daniel Naroditsky sitting behind a chessboard "><span>Getty Images</span></p></div><p data-component="caption-block"><figcaption>Naroditsky in 2008, following his World Youth Championship victory in Turkey</figcaption></p></figure><div data-component="text-block"><p>He gained international attention in 2007 when he won the under-12 boys world youth championship in Antalya, Turkey. In 2010, at the age of 14, he became one of the youngest ever published chess authors when he wrote a book titled Mastering Positional Chess, covering practical skills and technical manoeuvrings.</p><p>In 2013 Naroditsky won the US Junior Championship, helping him earn the title of grandmaster, the international chess federation's highest-ranked chess competitor, while he was still a teenager.</p><p>Naroditsky later graduated from Stanford University and worked as a chess coach in Charlotte, North Carolina.</p><p>In 2022 the New York Times named Naroditsky as its "new chess columnist" and invited him to contribute to a series of chess puzzles for the newspaper's games section.</p><p>In the publication's accompanying interview, the young grandmaster mused on chess's influence in his life.</p><p>"Even at my level, I can still discover beautiful things about the game every single time I train, teach, play or am a commentator at a tournament," he said.</p></div><div data-component="text-block"><p>Nemo Zhou –  a Toronto-based Woman Chess Grandmaster (WGM) and chess content creator – told the BBC Naroditsky was a friend and an "inspiration."</p><p>Zhou played chess with him, both in person and virtually at chess events across the US.</p><p>He was "everything that the combination of chess and content creation was supposed to be – he had this way to make chess fun", she said. </p><p>She added that he was known for being a "true historian of the game" who had a great memory for chess facts and historical games, and "did everything with kindness."</p><p>"Without people like him I probably would have quit chess at 17 and never touched it again," she said. </p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251015-170100-8e96f025b0-web-2.31.4-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp" loading="lazy" alt="International Chess Federation Naroditsky playing chess with spectators behind"><span>International Chess Federation</span></p></div></figure></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Language Support for Marginalia Search (125 pts)]]></title>
            <link>https://www.marginalia.nu/log/a_126_multilingual/</link>
            <guid>45653143</guid>
            <pubDate>Tue, 21 Oct 2025 06:48:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.marginalia.nu/log/a_126_multilingual/">https://www.marginalia.nu/log/a_126_multilingual/</a>, See on <a href="https://news.ycombinator.com/item?id=45653143">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><p>One of the big ambitions for the search engine this year has been to enable searching in more languages than English, and a pilot project for this has just been completed, allowing experimental support for German, French and Swedish.</p><p>These changes are now live for testing, but with an extremely small corpus of documents.</p><p>As the search engine has been up to this point built with English in mind, some anglo-centric assumptions made it into its code. A lot of the research on search engines generally seems to embed similar assumptions.</p><p>As this is a domain rife with unknown unknowns, the ambition for this pilot was to implement support for just a few additional languages in order to get a feel for how much work would be required to support more languages in general, as well as to assess how much the index grows when this is done.</p><p>Though it was fully understood upfront that supporting <em>all</em> languages in one go is unrealistic, as some languages are more different than others and require significant additional work. Human language is surprisingly disparate.</p><p>A language like Japanese, for example, has not only multiple alphabets, but <a href="https://en.wikipedia.org/wiki/Halfwidth_and_fullwidth_forms">embeds character width in unicode</a>; on top of that the language doesn’t put spaces between words. As such the language requires special normalization.</p><p>Latin, on the other hand, has <a href="https://dcc.dickinson.edu/grammar/latin/1st-and-2nd-declension-adjectives-%C4%81-o-stems">dozens</a> <a href="https://dcc.dickinson.edu/grammar/latin/2nd-declension-stem-paradigm-and-gender">of</a> <a href="https://dcc.dickinson.edu/grammar/latin/1st-conjugation">forms</a> for each word, and the words can often be reordered without significantly changing the meaning of a sentence. On the one hand this makes the grammatical analysis of the language somewhat easier since the words announce their function in the sentence fairly unambiguously, but on the other you probably need to store the text in a lemmatized form, and then strongly de-prioritize word order when matching.</p><p>Google’s bungled handling of Russian was supposedly why Yandex was able to eke out a foothold in that market.</p><h2 id="what-needs-changing">What needs changing</h2><p>The search engine’s language processing chain is fairly long, but the most salient parts go something like this:</p><ul><li>Text is extracted from the HTML</li><li>Language is identified using fasttext</li><li>Text is broken into sentences</li><li>Words are lowercased and Unicode is normalized</li><li>Sentences are stemmed and POS-tagged</li><li>Sentences, with stemming and POS-tag data is fed into keyword extraction algorithms<ul><li>Keywords are mapped to positions and HTML tags</li><li>Important keywords are identified using TF-IDF (using stemmed forms)</li><li>Important keywords are identified using grammar patterns (POS-tags)</li><li>Important keywords are identified using other heuristics</li></ul></li><li>Keywords are hashed</li></ul><p>Stemming is an imperfect way of getting a base form of a word, though generally such algorithms have a great number of flaws, so that e.g. universe and university seem to be the same word. This is only used in tf-idf calculations.</p><p>Part-of-Speech (POS) tagging is a grammatical annotation process where the role of each word is as best possible identified. This helps identify named entities, subjects, and so on.</p><p>Both of these processes needless to say require some awareness of the language being acted upon.</p><p>These “important keywords” are used to assign documents to a special index that helps with recall by ensuring these documents are included in the set that is ranked before the execution timer runs out. This is not strictly necessary, and in some cases such as where POS-tagging is not possible, can be disabled, partially or as a whole.</p><p>The normalization step is subject to cultural differences that do not translate. In English you’d probably expect to find the metal band Tröjan, typing “trojan”. In Swedish these are different letters entirely that should not match, the former means “the shirt”, the latter “trojan” in the Homeric or IT-security sense. Though a Swedish person would likely also say that they should be able to find mü(e)sli with the keyword “musli”, but a German-speaker would disagree and say that u and ü are clearly not the same.</p><p>There also exists a bootstrapping problem, as the statistical model used to calculate TF-IDF is based on documents in the index. Since almost all of the documents in the index up until this point have been in English, term frequencies for the newly added languages are missing. This breaks TF-IDF, as used in identifying important keywords, until a new model can be constructed. Thankfully the BM-25 model used in ranking is robust to this, as it relies on live data from the index itself.</p><p>The basic approach to parametrize language handling selected was to inject a language definition object, from which language appropriate logic is accessible.</p><p>This is configurable <a href="https://github.com/MarginaliaSearch/MarginaliaSearch/blob/master/code/functions/language-processing/resources/languages-experimental.xml">via XML</a>. Here XML was chosen because it arguably has the best built-in validation support, making it a fantastic use case for a self-contained configuration file like this one, where late validation would be very annoying to deal with.</p><p>Much of the configuration file consists of various grammatical patterns used to identify important keywords based on the role of a word in a sentence.</p><div><pre tabindex="0"><code data-lang="xml"><span><span><span>&lt;ngrams</span> <span>type=</span><span>"noun"</span><span>&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>VBG<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>RB VBG<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NNP* JJ)<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NN* JJ) NN*<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NN* JJ) (NN* JJ) NN*<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NN* JJ) (NN* JJ) (NN* JJ) NN*<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NNP* JJ) (NNP* IN TO CC) NNP*<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NNP* JJ) (NNP* IN TO CC) DT NNP*<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NNP* JJ) (NNP* IN TO CC) (NNP* IN TO CC) NNP*<span>&lt;/pospattern&gt;</span>
</span></span><span><span><span>&lt;/ngrams&gt;</span>
</span></span></code></pre></div><p>An expression like <code>(NN* JJ) (NN* JJ) NN*</code> is interpreted as</p><ol><li>Any tag starting with <code>NN</code>, or the tag <code>JJ</code></li><li>Any tag starting with <code>NN</code>, or the tag <code>JJ</code></li><li>Any tag starting with <code>NN</code></li></ol><p>Previously these patterns were hard coded, and finding a performant alternative implementation took some effort. A bit mask approach was selected, as it allows for some very basic bit-level concurrency that drastically reduces the number of branches needed.</p><p>As far as grammatical analysis goes, the approach used by the search engine is pretty medieval, but it does do a fairly good job at what it sets out to do, and as a result, one thing it is generally pretty good at is finding websites about some topic.</p><p>In some ways the imperfections introduced by the old-fashioned way of approaching language processing is almost helpful in bringing in more relevant results, as they tend to capture more variations of the words related to the topic of the document.</p><p>There are more places that need minor language dependent behavior changes that are glossed over here, both in the language processing pipeline discussed above, and in the query parser, though in the interest of keeping this update from becoming an overly verbose git diff, these will be glossed over.</p><h3 id="tooling">Tooling</h3><p>To help make sense of this, a test tool was built that runs the language processing pipeline in isolation, and outputs annotated intermediate results for human inspection.</p><figure><a href="https://www.marginalia.nu/log/a_126_multilingual/tool.png"><img src="https://www.marginalia.nu/log/a_126_multilingual/tool.png"></a><figcaption>Language Processing Tool illustrating some problems with keyword identification when run on a very short sample of text.</figcaption></figure><p>Work in this domain poses special problems that all but demand human testing. Machine testing can be good for catching regressions or getting access to some code for easier debugging, but natural language has so many nuances that any test suite is woefully inadequate compared to a pair of human eyeballs.</p><p>It has already helped refine the algorithms used to identify important keywords in English, which wasn’t the intent of building the tool, but its immediate consequence.</p><h2 id="integration">Integration</h2><p>Integrating the new multi-language search data into the system poses some design considerations.</p><p>One option would be to stick everything in one big index, and then filter results based on language during or after ranking. The strength of this is that it becomes possible to search in any language without specifying it upfront.</p><p>The drawbacks of the one-index approach is that it grows the index, which makes all queries slower; it also grows the number of keywords in the lexicon, which is something that we generally want to avoid.</p><p>The way the search engine handles mapping keywords to numeric ids is to use a hash algorithm. Not a hash table, but the output of the hash algorithm itself. This seems absolutely unhinged at first glance, but works remarkably well as long as the lexicon stays small enough.</p><p>Hash collisions do happen on rare occasions, but they need to happen between words where the words actually appear in the same documents to be a problem, generally leading to the ranking algorithm having to trudge through irrelevant documents and performing worse as a result of wasting its time budget.</p><p>Massively expanding the lexicon like we would if we were to mingle the documents increases the likelihood there will be an actual problem arising from these rare false positives.</p><p>If we stick every keyword from every language in the same index, a different problem arises, namely that homophones exist across different languages, meaning that the index lookup needs to wade through irrelevant documents that are trivially unrelated to the query.</p><p>The words <code>salt</code> and <code>lag</code>, if they appear in the same document in English likely selects documents relating to esports, whereas in Swedish they select for documents relating to food preservation.</p><p>The alternative option is to separate the indexes.</p><p>The drawback here is that you must specify the language upfront, and querying in all languages becomes very expensive, as it executing multiple queries, though the desired language of the search results are generally known beforehand so this is a relatively small concern that, at best, affects a small number of machine-access use cases.</p><p>Since it has far fewer problems, and promises to be faster and more accurate, this approach was selected.</p><p>In practice this was implemented as language-specific keyword-document mappings, that point into a common file containing document lists.</p><p>Initially the indexes were constructed from a common journal file, which was consumed repeatedly, but this turned out to be slow, and a partitioned approach was selected instead, with one journal per language. This almost completely removes any overhead.</p><h2 id="outcome">Outcome</h2><p>The changes discussed above have been implemented, and upon evaluation seems to work reasonably well, though evaluation has somewhat run into a dead end, as the index itself is <strong>extremely</strong> small for the newly added languages.</p><p>The experience of small index is devious as it may just mean poor recall, though looking at the documents database for one index partition, this is about 12% of the index, it really is quite small!</p><table><thead><tr><th>iso</th><th>document count</th></tr></thead><tbody><tr><td>en</td><td>112,846,397</td></tr><tr><td>de</td><td>7,623,983</td></tr><tr><td>fr</td><td>4,852,759</td></tr><tr><td>sv</td><td>1,020,962</td></tr></tbody></table><p>To verify this is not due some silent, catastrophic processing error, the proportions were compared against the number of documents found in the 50 GB document sample used in testing, using a simplified process that only does language identification.</p><table><thead><tr><th>iso</th><th>document count</th></tr></thead><tbody><tr><td>en</td><td>11,497,571</td></tr><tr><td>de</td><td>614,311</td></tr><tr><td>fr</td><td>409,877</td></tr><tr><td>es</td><td>267,408</td></tr><tr><td>ja</td><td>217,599</td></tr><tr><td>nl</td><td>196,130</td></tr><tr><td>…</td><td>…</td></tr><tr><td>sv</td><td>67,670</td></tr></tbody></table><p>The proportions aren’t identical, but in the same general ballpark. The small size of the sample, along with the uneven distribution and apparent rarity of these documents adequately explains the disparity.</p><p>The lack of documents in languages other than English is likely due to how the index has been grown, by following and adding links from English websites. These occasionally lead to bilingual websites, and on rare occasions to websites completely in a different language, though it seems reasonable most websites that are not at least partially in English sees few or no links from English-language websites.</p><p>Adding to the problem, up until fairly recently the index wasn’t really growing very much at all, only through manual submissions.</p><p>Beyond a certain point, meaningfully growing the index by just following links became difficult.</p><p>Most known domains are dead, so merely adding more domains to the list of websites to crawl only serves to pollute the database with junk data.</p><p>In order to get around this, and reach the goal of indexing a billion documents, a new process was built to visit candidate websites to verify that they are in fact real and on-line, before assigning them to an index partition.</p><p>The process has been running for almost a quarter, and has managed to identify about 800,000 viable new domains in that time window. (This has brought the document total up to 969M documents. So very nearly there now!)</p><p>Web search is unusual in how often you run into these extremely long running processes that need to cook for months, sometimes up to a year before they really begin to pay off.</p><p>We’ll have to see whether building this new process was so prescient it ends up being sufficient to identify and add new domains in more languages, as links from the newly processed Swedish, French and German websites have been added to the domain database, or if some sort of manual seeding or targeted selection process is needed.</p><p>It seems plausible it will at least begin to remedy the data starvation, as the rate of successful domain discovery has shot up significantly since processing links from the documents processed in the newly added languages, and many of the new domains are indeed from <code>.de</code>, <code>.se</code>, <code>.fr</code>, and <code>.ch</code> domains.</p><p>For now we’ll have to wait and see how the data-set evolves. It is difficult to further refine the multi-language aspect of the search data with a data-set this small.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[60k kids have avoided peanut allergies due to 2015 advice, study finds (169 pts)]]></title>
            <link>https://www.cbsnews.com/news/peanut-allergies-60000-kids-avoided-2015-advice/</link>
            <guid>45652307</guid>
            <pubDate>Tue, 21 Oct 2025 03:53:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cbsnews.com/news/peanut-allergies-60000-kids-avoided-2015-advice/">https://www.cbsnews.com/news/peanut-allergies-60000-kids-avoided-2015-advice/</a>, See on <a href="https://news.ycombinator.com/item?id=45652307">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    
                                                                                                                                
                                                                            
<article id="article-0" data-index="0" data-path="/news/peanut-allergies-60000-kids-avoided-2015-advice/">
  <!-- lil observer -->
      <span></span>
    
        



  <div id="article-header" data-sort-time="1761003592000" data-update-time="1761003592000">
    <header>
                                      
          

      

      
      
      <div>
                  
  



        
        <p>
            <time datetime="2025-10-20T19:39:52-0400">Updated on:  October 20, 2025 / 7:39 PM EDT</time>
            / CBS/AP
          </p>
      </div>

    </header>
  </div>


  <section>
    <p>A decade after a landmark study proved that feeding peanut products to young babies could prevent development of life-threatening allergies, new research finds the change has made a big difference in the real world. </p><p>About 60,000 children have avoided developing peanut allergies after guidance first issued in 2015 upended medical practice by recommending introducing the allergen to infants starting as early as 4 months. </p><p>"That's a remarkable thing, right?" said Dr. David Hill, an allergist and researcher at Children's Hospital of Philadelphia, and author of a study published Monday in the medical journal Pediatrics. Hill and colleagues analyzed electronic health records from dozens of pediatric practices to track diagnoses of food allergies in young children before, during and after the guidelines were issued. </p><p>"I can actually come to you today and say there are less kids with food allergy today than there would have been if we hadn't implemented this public health effort," he added.</p><p>"Our findings have relevance from those of us who treat patients to those caring for infants, and more awareness, education and advocacy could further increase the positive results we observed in this study," he continued. "Future studies could potentially explore specific feeding practices that help us better understand the timing, frequency and dose of foods that optimize protection against food allergies."</p>

    

<p>The researchers found that peanut allergies in children ages 0 to 3 declined by more than 27% after guidance for high-risk kids was first issued in 2015 and by more than 40% after the recommendations were expanded in 2017. </p><p>The effort hasn't yet reduced an overall increase in food allergies in the U.S. in recent years. About 8% of children are affected, including more than 2% with a peanut allergy. </p><p>Peanut allergy is caused when the body's immune system mistakenly identifies proteins in peanuts as harmful and releases chemicals that trigger allergic symptoms, including hives, respiratory symptoms and, sometimes, life-threatening anaphylaxis. </p><p>For decades, doctors had recommended delaying feeding children peanuts and other foods likely to trigger allergies until age 3. But in 2015, Gideon Lack at King's College London published the groundbreaking Learning Early About Peanut Allergy, or LEAP, trial.  </p>

    
    

<p>Lack and colleagues showed that introducing peanut products in infancy reduced the future risk of developing food allergies by more than 80%. Later analysis showed that the protection persisted in about 70% of kids into adolescence.  </p><p>The study immediately sparked new guidelines urging early introduction of peanuts — but putting them into practice has been slow. </p><p>Only about 29% of pediatricians and 65% of allergists reported following the expanded guidance issued in 2017, surveys found. </p><p>Confusion and uncertainty about the best way to introduce peanuts early in life led to the lag, according to a commentary that accompanied the study. Early on, medical experts and parents alike questioned whether the practice could be adopted outside of tightly controlled clinical settings.  </p><p>The data for the analysis came from a subset of participating practice sites and may not represent the entire U.S. pediatric population, noted the commentary, led by Dr. Ruchi Gupta, a child allergy expert at Northwestern University.  </p><p>However, the new research offers "promising evidence that early allergen introduction is not only being adopted but may be making a measurable impact," the authors concluded.  </p><p>Advocates for the 33 million people in the U.S. with food allergies welcomed signs that early introduction of peanut products is catching on. </p>

    
    

<p>"This research reinforces what we already know and underscores a meaningful opportunity to reduce the incidence and prevalence of peanut allergy nationwide," said Sung Poblete, chief executive of the nonprofit group Food Allergy Research &amp; Education, or FARE.  </p><p>The new study emphasizes the current guidance, updated in 2021, which calls for introducing peanuts and other major food allergens between four and six months, without prior screening or testing, Hill said. Parents should consult their pediatricians about any questions.  </p><p>"It doesn't have to be a lot of the food, but little tastes of peanut butter, milk-based yogurt, soy-based yogurts and tree butters," he said. "These are really good ways to allow the immune system exposure to these allergenic foods in a safe way." </p><p>Tiffany Leon, 36, a Maryland registered dietician and director at FARE, introduced peanuts and other allergens early to her own sons, James, 4, and Cameron, 2. </p><p>At first, Leon's own mother was shocked at the advice to feed babies such foods before the age of 3, she said. But Leon explained how the science had changed. </p><p>"As a dietician, I practice evidence-based recommendations," she said. "So when someone told me, 'This is how it's done now, these are the new guidelines,' I just thought, 'OK, well, this is what we're going to do.'"</p>
  </section>

  

                
        
      
                  
    <!-- data-recirc-source="queryly" -->
    



    
    
  <section>
  <h2>In:</h2>
  <ul>
          <li><a href="https://www.cbsnews.com/tag/allergies/">Allergies</a></li>
          <li><a href="https://www.cbsnews.com/tag/peanuts/">Peanuts</a></li>
      </ul>
</section>

  

  
  </article>
            

                                                                                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a message queue with only two UNIX signals (131 pts)]]></title>
            <link>https://leandronsp.com/articles/you-dont-need-kafka-building-a-message-queue-with-only-two-unix-signals</link>
            <guid>45650178</guid>
            <pubDate>Mon, 20 Oct 2025 22:22:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://leandronsp.com/articles/you-dont-need-kafka-building-a-message-queue-with-only-two-unix-signals">https://leandronsp.com/articles/you-dont-need-kafka-building-a-message-queue-with-only-two-unix-signals</a>, See on <a href="https://news.ycombinator.com/item?id=45650178">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Have you ever asked yourself what if we could replace any message broker with a very simple one using only two UNIX signals? Well, I’m not surprised if you didn’t. But I did. And I want to share my journey of how I achieved it.</p>
<p>If you want to learn about UNIX signals, binary operations the easy way, how a message broker works under the hood, and a bit of Ruby, this post is for you.</p>
<p>And if you came here just because of the clickbait title, I apologize and invite you to keep reading. It’ll be fun, I promise.</p>
<p><img src="https://leandronsp.com/uploads/3491.png" alt="image"></p>
<h2>It’s all about UNIX</h2>
<p>A few days ago, I saw some discussion on the internet about how we could send messages between processes. Many people think of sockets, which are the most common way to send messages, even allowing communication across different machines and networks. Some don’t even realize that pipes are another way to send messages between processes:</p>
<pre><code translate="no" tabindex="0"><p><span>$</span> <span>echo</span> <span>'hello'</span> <span>|</span> <span>base64</span>
</p><p><span>aGVsbG8K</span>
</p></code></pre>
<p>Here’s what’s happening:</p>
<ul>
<li>The process <code>echo</code> is started with the content “hello”</li>
<li><code>echo</code> is a program that prints the message to <em>STDOUT</em></li>
<li>Through the pipe, the content in <em>STDOUT</em> is <strong>sent</strong> directly to the <em>STDIN</em> of the <code>base64</code> process</li>
<li>The <code>base64</code> process encodes its input to Base64 and then puts the result in <em>STDOUT</em></li>
</ul>
<p>Note the word “send”. Yes, anonymous pipes are a form of <strong>IPC (Inter-process communication).</strong> Other forms of IPC in UNIX include:</p>
<ul>
<li>named pipes (mkfifo)</li>
<li>sockets</li>
<li>regular files</li>
<li>or even a simple <strong>signal</strong></li>
</ul>
<h2>UNIX signals</h2>
<p>According to <a href="https://leandronsp.com/articles/url">Wikipedia</a>:</p>
<blockquote>
<p>A UNIX signal is a standardized message sent to a program to trigger specific behaviour, such as quitting or error handling</p>
</blockquote>
<p>There are many signals we can send to a process, including:</p>
<ul>
<li>SIGTERM - sends a notification to the process to terminate. It can be “trapped,” which means the process can do some cleanup work before termination, like releasing OS resources and closing file descriptors</li>
<li>SIGKILL - sends a termination signal that cannot be trapped or ignored, forcing immediate termination</li>
<li>SIGINT - the interrupt signal, typically sent when you press <code>Ctrl+C</code> in the terminal. It can be trapped, allowing the process to perform cleanup before exiting gracefully</li>
<li>SIGHUP - the hangup signal, originally sent when a terminal connection was lost. Modern applications often use it to reload configuration files without restarting the process</li>
<li>SIGQUIT - similar to SIGINT but also generates a core dump for debugging</li>
<li>SIGSTOP - pauses (suspends) a process. Cannot be trapped or ignored</li>
<li>SIGCONT - resumes a process that was paused by <em>SIGSTOP</em></li>
<li>SIGCHLD - sent to a parent process when a child process terminates or stops</li>
<li><strong>SIGUSR1</strong> and <strong>SIGUSR2</strong> - user-defined signals that applications can use for custom purposes</li>
</ul>
<h2>Sending messages using  signals</h2>
<p>Okay, we know that signals are a primitive form of IPC. UNIX-like systems provide a syscall called <code>kill</code> that sends signals to processes. Historically, this syscall was created solely to terminate processes. But over time, they needed to accommodate other types of signals, so they reused the same syscall for different purposes.</p>
<p>For instance, let’s create a simple Ruby script <code>sleeper.rb</code> which sleeps for 60 seconds, nothing more:</p>
<pre><code translate="no" tabindex="0"><p><span>puts</span> <span>"</span><span>Process ID: </span><span>#{</span><span>Process</span><span>.</span><span>pid</span><span>}</span><span>"</span>
</p><p><span>puts</span> <span>"</span><span>Sleeping for 60 seconds...</span><span>"</span>
</p><p><span>sleep</span> <span>60</span>
</p></code></pre>
<p>After running we see:</p>
<pre><code translate="no" tabindex="0"><p>Process ID: 55402
</p><p>Sleeping for 60 seconds...
</p></code></pre>
<p>In another window, we can <strong>send</strong> the <code>SIGTERM</code> signal to the process <code>55402</code> via syscall <code>kill</code>:</p>
<pre><code translate="no" tabindex="0"><p><span>$</span> <span>kill</span> <span>-SIGTERM</span> <span>55402</span>
</p></code></pre>
<p>And then, in the script session:</p>
<pre><code translate="no" tabindex="0"><p>[1]    55402 terminated  ruby sleeper.rb
</p></code></pre>
<h3>Signal traps</h3>
<p>In Ruby, we can also <em>trap</em> a signal using the <code>trap</code> method in Ruby:</p>
<pre><code translate="no" tabindex="0"><p><span>puts</span> <span>"</span><span>Process ID: </span><span>#{</span><span>Process</span><span>.</span><span>pid</span><span>}</span><span>"</span>
</p><p><span>puts</span> <span>"</span><span>Sleeping for 60 seconds...</span><span>"</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGTERM</span><span>'</span><span>)</span> <span>do</span> 
</p><p><span>puts</span> <span>"</span><span>Received SIGTERM, exiting gracefully...</span><span>"</span>
</p><p><span>exit</span>
</p><p><span>end</span>
</p><p><span>sleep</span> <span>60</span>
</p></code></pre>
<p>Which in turn, after sending the signal, will gracefully:</p>
<pre><code translate="no" tabindex="0"><p>Process ID: 55536
</p><p>Sleeping for 60 seconds...
</p><p>Received SIGTERM, exiting gracefully...
</p></code></pre>
<p>After all, we <em>cannot send messages using signals</em>. They are a primitive way of sending <em>standardized messages</em> which will trigger specific behaviours. At most, we can trap some signals, but nothing more.</p>
<blockquote>
<p>Okay Leandro, but what’s the purpose of this article then?</p>
</blockquote>
<p><em>Hold on</em>. That’s exactly why I’m here. To prove points by doing useless stuff, like when I <a href="https://leandronsp.com/articles/simulating-oop-in-bash-3mop">simulated OOP in Bash</a> a couple of years ago (it was fun though).</p>
<p>To understand how we can “hack” UNIX signals and send messages between processes, let’s first talk a bit about <strong>binary operations</strong>. Yes, those “zeros” and “ones” you were scared of when you saw them for the first time. But they don’t bite (🥁 LOL), I promise.</p>
<h2>What is a message?</h2>
<p>If we model a message as a sequence of characters, we could say that at a high-level, messages are simply <em>strings</em>. But in memory, they are stored as <strong>bytes</strong>.</p>
<p>We know that bytes are made of bits. In computer terms, what’s a bit? It’s simply an abstraction representing <strong>only two states</strong>:</p>
<ul>
<li>zero</li>
<li>one</li>
</ul>
<p>That’s it. For instance, using <a href="https://leandronsp.com/articles/url">ASCII</a>, we know that the letter “h” has the following codes:</p>
<ul>
<li>104 in decimal</li>
<li><code>0x68</code> in hexadecimal</li>
<li><code>01101000</code> in binary</li>
</ul>
<p>Binary-wise, what if we represented each “0” with a specific signal and each “1” with another? We know that some signals such as SIGTERM, SIGINT, and SIGCONT can be trapped, but intercepting them would harm their original purpose.</p>
<p>But thankfully, UNIX provides two user-defined signals that are perfect for our hacking experiment.</p>
<h2>Sending SIGUSR1 and SIGUSR2</h2>
<p>First things first, let’s trap those signals in the code:</p>
<pre><code translate="no" tabindex="0"><p><span>puts</span> <span>"</span><span>Process ID: </span><span>#{</span><span>Process</span><span>.</span><span>pid</span><span>}</span><span>"</span>
</p><p><span>puts</span> <span>"</span><span>Sleeping forever. Send signals to this process to see how it responds.</span><span>"</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR1</span><span>'</span><span>)</span> <span>do</span> 
</p><p><span>puts</span> <span>"</span><span>Received SIGUSR1 signal</span><span>"</span>
</p><p><span>end</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR2</span><span>'</span><span>)</span> <span>do</span>
</p><p><span>puts</span> <span>"</span><span>Received SIGUSR2 signal</span><span>"</span>
</p><p><span>end</span>
</p><p><span>sleep</span>
</p></code></pre>
<pre><code translate="no" tabindex="0"><p>Process ID: 56172
</p><p>Sleeping forever. Send signals to this process to see how it responds.
</p></code></pre>
<p>After sending some <code>kill -SIGUSR1 56172</code> and <code>kill -SIGUSR2 56172</code>, we can see that the process prints the following content:</p>
<pre><code translate="no" tabindex="0"><p>Process ID: 56172
</p><p>Sleeping forever. Send signals to this process to see how it responds.
</p><p>Received SIGUSR1 signal
</p><p>Received SIGUSR2 signal
</p><p>Received SIGUSR2 signal
</p><p>Received SIGUSR1 signal
</p><p>Received SIGUSR1 signal
</p><p>Received SIGUSR2 signal
</p></code></pre>
<p><strong>Signals don’t carry data</strong>. But the example we have is perfect for changing to bits, uh?</p>
<pre><code translate="no" tabindex="0"><p>Received SIGUSR1 signal # 0
</p><p>Received SIGUSR2 signal # 1
</p><p>Received SIGUSR2 signal # 1
</p><p>Received SIGUSR1 signal # 0
</p><p>Received SIGUSR2 signal # 1
</p><p>Received SIGUSR1 signal # 0
</p><p>Received SIGUSR1 signal # 0
</p><p>Received SIGUSR1 signal # 0
</p></code></pre>
<p>That’s exactly <code>01101000</code>, the binary representation of the letter “h”. We’re simply <strong>encoding</strong> the letter as a binary representation and sending it via signals</p>
<p>Again, we’re <strong>encoding it as a binary</strong> and sending it <strong>via signals</strong>.</p>
<p><em>How cool is that</em>?</p>
<p><img src="https://leandronsp.com/uploads/3299.png" alt="image"></p>
<h3>Decoding the binary data</h3>
<p>On the other side, the receiver should be capable of decoding the message and converting it back to the letter “h”:</p>
<ul>
<li>sender <em>encodes</em> the message</li>
<li>receiver <em>decodes</em> the message</li>
</ul>
<p>So, how do we decode <code>01101000</code> (the letter “h” in ASCII)? Let’s break it down into a few steps:</p>
<ol>
<li>First, we need to see the 8 bits as individual digits in their respective positions</li>
<li>The rightmost bit is at position 0, whereas the leftmost bit is at position 7. This is how we define the most significant bit (<strong>MSB</strong>, the leftmost) and the least significant bit (<strong>LSB</strong>, the rightmost)</li>
<li>For this example, we perform a <strong>left shift</strong> operation on each bit and then sum all the values, in this case from MSB to LSB (the order doesn’t matter much for now): <code>(0 &lt;&lt; 7) + (1 &lt;&lt; 6) + (1 &lt;&lt; 5) + (0 &lt;&lt; 4) + ... + (0 &lt;&lt; 0)</code>:<br>
<em>left shift on <em>zeros</em> will always produce a <em>zero</em></em></li>
</ol>
<ul>
<li><code>0 &lt;&lt; 7</code> = <code>(2 ** 7) * 0</code> = <code>128 * 0</code> = 0</li>
<li><code>1 &lt;&lt; 6</code> = <code>(2 ** 6) * 1</code> = <code>64 * 1</code> = 64</li>
</ul>
<p>Similarly to the remaining bits:</p>
<ul>
<li><code>1 &lt;&lt; 5</code> = 32</li>
<li><code>0 &lt;&lt; 4</code> = 0</li>
<li><code>1 &lt;&lt; 3</code> = 8</li>
<li><code>0 &lt;&lt; 2</code> = 0</li>
<li><code>0 &lt;&lt; 1</code> = 0</li>
<li><code>0 &lt;&lt; 0</code> = 0</li>
</ul>
<p>So, our sum becomes, from MSB to LSB:</p>
<pre><code translate="no" tabindex="0"><p>MSB                          LSB
</p><p>0   1    1    0   1   0   0   0
</p><p>0 + 64 + 32 + 0 + 8 + 0 + 0 + 0 = 104
</p></code></pre>
<p>104 is exactly the <strong>decimal representation</strong> of the letter “h” in ASCII.</p>
<p><em>How wonderful is that?</em></p>
<h3>Sending the letter “h”</h3>
<p>Now let’s convert these operations to Ruby code. We’ll write a simple program <code>receiver.rb</code> that receives signals in order from LSB to MSB (positions 0 to 7) and then converts them back to ASCII characters, printing to <code>STDOUT</code>.</p>
<p>Basically, we’ll <strong>accumulate</strong> bits and whenever we form a complete byte, we’ll decode it to its ASCII representation. The very basic implementation of our <code>accumulate_bit(bit)</code> method would look like as follows:</p>
<pre><code translate="no" tabindex="0"><p><span>@position</span> <span>=</span> <span>0</span> <span># start with the LSB</span>
</p><p><span>@accumulator</span> <span>=</span> <span>0</span>
</p><p><span>def</span> <span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span># The left shift operator (&lt;&lt;) is used to </span>
</p><p><span># shift the bits of the number to the left.</span>
</p><p><span>#</span>
</p><p><span># This is equivalent of: (2 ** @position) * bit</span>
</p><p><span>@accumulator</span> <span>+=</span> <span>(</span><span>bit</span> <span>&lt;&lt;</span> <span>@position</span><span>)</span>
</p><p><span>return</span> <span>@accumulator</span> <span>if</span> <span>@position</span> <span>==</span> <span>7</span> <span># stop accumulating after 8 bits (byte)</span>
</p><p><span>@position</span> <span>+=</span> <span>1</span> <span># move to the next bit position: 0 becomes 1, 1 becomes 2, etc.</span>
</p><p><span>end</span>
</p><p><span># Letter "h" in binary is 01101000</span>
</p><p><span># But we'll send from the LSB to the MSB</span>
</p><p><span>#</span>
</p><p><span># 0110 1000 (MSB -&gt; LSB) becomes 0001 0110 (LSB -&gt; MSB)</span>
</p><p><span># The order doesn't matter that much, it'll depend on </span>
</p><p><span># the receiver's implementation.</span>
</p><p><span>accumulate_bit</span><span>(</span><span>0</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>0</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>0</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>1</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>0</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>1</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>1</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>0</span><span>)</span>
</p><p><span>puts</span> <span>@accumulator</span> <span># should print 104, which is the ASCII code for "h"</span>
</p></code></pre>
<p><em>Pay attention to this code. It’s very important and builds the foundation for the next steps. If you didn’t get it, go back and read it again. Try it yourself in the terminal or using your preferred programming language.</em></p>
<p>Now, how to convert the decimal <code>104</code> to the ASCII character representation? Luckily, Ruby provides a method called <code>chr</code> which does the job:</p>
<pre><code translate="no" tabindex="0"><p><span>irb</span><span>&gt;</span> <span>puts</span> <span>104</span><span>.</span><span>chr</span>
</p><p><span>=&gt;</span> <span>"</span><span>h</span><span>"</span>
</p></code></pre>
<p>We could do the same job for the rest of the word “hello”, for instance. According to the <a href="https://www.ascii-code.com/">ASCII table</a>, it should be the following:</p>
<ul>
<li><code>e</code> in decimal is <code>101</code></li>
<li><code>l</code> in decimal is <code>108</code></li>
<li><code>o</code> in decimal is <code>111</code></li>
</ul>
<p>Let’s check if Ruby knows that:</p>
<pre><code translate="no" tabindex="0"><p><span>104</span><span>.</span><span>chr</span>    <span># "h"</span>
</p><p><span>101</span><span>.</span><span>chr</span>    <span># "e"</span>
</p><p><span>108</span><span>.</span><span>chr</span>    <span># "l"</span>
</p><p><span>111</span><span>.</span><span>chr</span>    <span># "o"</span>
</p></code></pre>
<p>We can even “decode” the word to the decimal representation in ASCII:</p>
<pre><code translate="no" tabindex="0"><p><span>irb</span><span>&gt;</span> <span>"</span><span>hello</span><span>"</span><span>.</span><span>bytes</span>
</p><p><span>=&gt;</span> <span>[</span><span>104</span><span>,</span> <span>101</span><span>,</span> <span>108</span><span>,</span> <span>108</span><span>,</span> <span>111</span><span>]</span>
</p></code></pre>
<p>Now, time to finish our receiver implementation to properly print the letter “h”:</p>
<pre><code translate="no" tabindex="0"><p><span>@position</span> <span>=</span> <span>0</span> <span># start with the LSB</span>
</p><p><span>@accumulator</span> <span>=</span> <span>0</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR1</span><span>'</span><span>)</span> <span>{</span> <span>decode_signal</span><span>(</span><span>0</span><span>)</span> <span>}</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR2</span><span>'</span><span>)</span> <span>{</span> <span>decode_signal</span><span>(</span><span>1</span><span>)</span> <span>}</span>
</p><p><span>def</span> <span>decode_signal</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>return</span> <span>unless</span> <span>@position</span> <span>==</span> <span>8</span> <span># if not yet accumulated a byte, keep accumulating</span>
</p><p><span>print</span> <span>"</span><span>Received byte: </span><span>#{</span><span>@accumulator</span><span>}</span><span> (</span><span>#{</span><span>@accumulator</span><span>.</span><span>chr</span><span>}</span><span>)</span><span>\n</span><span>"</span>
</p><p><span>@accumulator</span> <span>=</span> <span>0</span> <span># reset the accumulator</span>
</p><p><span>@position</span> <span>=</span> <span>0</span> <span># reset position for the next byte</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span># The left shift operator (&lt;&lt;) is used to </span>
</p><p><span># shift the bits of the number to the left.</span>
</p><p><span>#</span>
</p><p><span># This is equivalent of: (2 ** @position) * bit</span>
</p><p><span>@accumulator</span> <span>+=</span> <span>(</span><span>bit</span> <span>&lt;&lt;</span> <span>@position</span><span>)</span>
</p><p><span>@position</span> <span>+=</span> <span>1</span> <span># move to the next bit position: 0 becomes 1, 1 becomes 2, etc.</span>
</p><p><span>end</span>
</p><p><span>puts</span> <span>"</span><span>Process ID: </span><span>#{</span><span>Process</span><span>.</span><span>pid</span><span>}</span><span>"</span>
</p><p><span>sleep</span>
</p></code></pre>
<p><em>Read that code and its comments. It’s very important. Do not continue reading until you really get what’s happening here.</em></p>
<ul>
<li>Whenever we get <code>SIGUSR1</code>, we accumulate the bit <code>0</code></li>
<li>When getting <code>SIGUSR2</code>, accumulate then the bit <code>1</code></li>
<li>When accumulator reaches  the position<code>8</code>, it means we have a byte. At this moment we should print the ASCII representation using the <code>.chr</code> we seen earlier. Then, reset bit position and accumulator</li>
</ul>
<p>Let’s see our receiver in action! Start the receiver in one terminal:</p>
<pre><code translate="no" tabindex="0"><p><span>$</span> <span>ruby</span> <span>receiver.rb</span>
</p><p><span>Process</span> <span>ID:</span> <span>58219</span>
</p></code></pre>
<p>Great! Now the receiver is listening for signals. In another terminal, let’s manually send signals<br>
to form the letter “h” (which is <code>01101000</code> in binary, remember?):</p>
<pre><code translate="no" tabindex="0"><p>  # Sending from LSB to MSB: 0, 0, 0, 1, 0, 1, 1, 0
</p><p>  $ kill -SIGUSR1 58219  # 0
</p><p>  $ kill -SIGUSR1 58219  # 0
</p><p>  $ kill -SIGUSR1 58219  # 0
</p><p>  $ kill -SIGUSR2 58219  # 1
</p><p>  $ kill -SIGUSR1 58219  # 0
</p><p>  $ kill -SIGUSR2 58219  # 1
</p><p>  $ kill -SIGUSR2 58219  # 1
</p><p>  $ kill -SIGUSR1 58219  # 0
</p></code></pre>
<p>And in the receiver terminal, we should see:</p>
<pre><code translate="no" tabindex="0"><p>Received byte: 104 (h)
</p></code></pre>
<p><em>How amazing is that?</em> We just sent the letter “h” using only two UNIX signals!</p>
<p>But wait. Manually sending 8 signals for each character? That’s tedious and error-prone. What if we wanted to send the word “hello”? That’s 5 characters × 8 bits = 40 signals to send manually. No way.</p>
<p><em>We need a sender.</em></p>
<h3>Building the sender</h3>
<p>The sender’s job is the opposite of the receiver: it should encode a message (string) into bits and send them as signals to the receiver process.</p>
<p>Let’s think about what we need:</p>
<ol>
<li>Take a message as input (like “hello”)</li>
<li>Convert each character to its byte representation</li>
<li>Extract the 8 bits from each byte</li>
<li>Send <code>SIGUSR1</code> for bit 0, <code>SIGUSR2</code> for bit 1</li>
<li>Repeat for all characters</li>
</ol>
<p>The tricky part here is the step 3: <strong>how do we extract individual bits from a byte?</strong> To extract the bit at position <code>i</code>, we can use the following formula:</p>
<pre><code translate="no" tabindex="0"><p>bit = (byte &gt;&gt; i) &amp; 1
</p></code></pre>
<p>Let me break this down:</p>
<ul>
<li><code>byte &gt;&gt; i</code> performs a <em>right shift</em> by <code>i</code> positions</li>
<li><code>&amp; 1</code> is a bitwise <code>AND</code> operation that extracts only the <em>rightmost</em> bit</li>
</ul>
<p>For the letter “h” (<code>01101000</code> in binary, <code>104</code> in decimal):</p>
<p><strong>Position 0 (LSB):</strong></p>
<ul>
<li><code>(104 &gt;&gt; 0)</code> = <code>104 / (2 ** 0)</code> = <code>104 / 1</code> = 104</li>
<li><code>01101000</code> &gt;&gt; 0 = <code>01101000</code></li>
<li><code>01101000</code> &amp; <code>00000001</code> = 0 (<em>one</em> AND <em>zero</em> is <em>zero</em>)</li>
</ul>
<p><strong>Position 1:</strong></p>
<ul>
<li><code>(104 &gt;&gt; 1)</code> = <code>104 / (2 ** 1)</code> = <code>104 / 2</code> = 52</li>
<li><code>01101000</code> &gt;&gt; 1 = <code>00110100</code></li>
<li><code>00110100</code> &amp; <code>00000001</code> = 0</li>
</ul>
<p><strong>Position 2:</strong></p>
<ul>
<li><code>(104 &gt;&gt; 2)</code> = <code>104 / (2 ** 2)</code> = <code>104 / 4</code> = 26</li>
<li><code>01101000</code> &gt;&gt; 2 = <code>00011010</code></li>
<li><code>00011010</code> &amp; <code>00000001</code> = 0</li>
</ul>
<p><strong>Position 3:</strong></p>
<ul>
<li><code>(104 &gt;&gt; 3)</code> = <code>104 / (2 ** 3)</code> = <code>104 / 8</code> = 13</li>
<li><code>01101000</code> &gt;&gt; 3 = <code>00001101</code></li>
<li><code>00001101</code> &amp; <code>00000001</code> = 1 (<em>one</em> AND <em>one</em> equals <em>one</em>)</li>
</ul>
<p>And so on for positions 4, 5, 6, and 7. This gives us: <code>0, 0, 0, 1, 0, 1, 1, 0</code> — exactly the bits we need from LSB to MSB!</p>
<ul>
<li><code>(104 &gt;&gt; 0) &amp; 1</code> = <code>104 &amp; 1</code> = 0</li>
<li><code>(104 &gt;&gt; 1) &amp; 1</code> = <code>52 &amp; 1</code> = 0</li>
<li><code>(104 &gt;&gt; 2) &amp; 1</code> = <code>26 &amp; 1</code> = 0</li>
<li><code>(104 &gt;&gt; 3) &amp; 1</code> = <code>13 &amp; 1</code> = 1</li>
<li><code>(104 &gt;&gt; 4) &amp; 1</code> = <code>6 &amp; 1</code> = 0</li>
<li><code>(104 &gt;&gt; 5) &amp; 1</code> = <code>3 &amp; 1</code> = 1</li>
<li><code>(104 &gt;&gt; 6) &amp; 1</code> = <code>1 &amp; 1</code> = 1</li>
<li><code>(104 &gt;&gt; 7) &amp; 1</code> = <code>0 &amp; 1</code> = 0</li>
</ul>
<blockquote>
<p>Pay close attention to this technique. It’s a fundamental operation in low-level programming.</p>
</blockquote>
<p>So now time to build the <code>sender.rb</code> which is pretty simple:</p>
<pre><code translate="no" tabindex="0"><p><span>receiver_pid</span> <span>=</span> <span>ARGV</span><span>[</span><span>0</span><span>]</span><span>.</span><span>to_i</span>
</p><p><span>message</span> <span>=</span> <span>ARGV</span><span>[</span><span>1</span><span>..</span><span>-</span><span>1</span><span>]</span><span>.</span><span>join</span><span>(</span><span>'</span><span> </span><span>'</span><span>)</span>
</p><p><span>def</span> <span>encode_byte</span><span>(</span><span>byte</span><span>)</span>
</p><p><span>8</span><span>.</span><span>times</span><span>.</span><span>map</span> <span>do</span> <span>|</span><span>i</span><span>|</span>
</p><p><span># Extract each bit from the byte, starting from the LSB</span>
</p><p><span>(</span><span>byte</span> <span>&gt;&gt;</span> <span>i</span><span>)</span> <span>&amp;</span> <span>1</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>message</span><span>.</span><span>bytes</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>byte</span><span>|</span>
</p><p><span>encode_byte</span><span>(</span><span>byte</span><span>)</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>bit</span><span>|</span>
</p><p><span>signal</span> <span>=</span> <span>bit</span> <span>==</span> <span>0</span> <span>?</span> <span>'</span><span>SIGUSR1</span><span>'</span> <span>:</span> <span>'</span><span>SIGUSR2</span><span>'</span>
</p><p><span>Process</span><span>.</span><span>kill</span><span>(</span><span>signal</span><span>,</span> <span>receiver_pid</span><span>)</span>
</p><p><span>sleep</span> <span>0.001</span> <span># Delay to allow the receiver to process the signal</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p></code></pre>
<p>For each byte (8-bit structure) we extract the bit performing the <em>right shift</em> + <em>AND</em> oprerations. The result is the extracted bit.</p>
<p>In the receiver window:</p>
<pre><code translate="no" tabindex="0"><p><span>$</span> <span>ruby</span> <span>receiver.rb</span>
</p><p><span>Process</span> <span>ID:</span> <span>68968</span>
</p></code></pre>
<p>And in the sender window:</p>
<pre><code translate="no" tabindex="0"><p><span>$</span> <span>ruby</span> <span>sender.rb</span> <span>68968</span> <span>h</span>
</p></code></pre>
<p>The receiver will print:</p>
<pre><code translate="no" tabindex="0"><p><span>$</span> <span>ruby</span> <span>receiver.rb</span>
</p><p><span>Process</span> <span>ID:</span> <span>68968</span>
</p><p><span>Received</span> <span>byte:</span> <span>104</span><span></span> <span>(</span><span>h</span><span>)</span>
</p></code></pre>
<p><em>Processes sending messages with only two signals!</em> How wonderful is that?</p>
<h3>Sending the “hello” message</h3>
<p>Now, sending the hello message is super easy. The sender is already able to send not only a letter but any message using signals:</p>
<pre><code translate="no" tabindex="0"><p><span>$</span> <span>ruby</span> <span>sender.rb</span> <span>68968</span> <span>hello</span>
</p><p><span># And the receiver:</span>
</p><p><span>Received</span> <span>byte:</span> <span>104</span><span></span> <span>(</span><span>h</span><span>)</span>
</p><p><span>Received</span> <span>byte:</span> <span>101</span><span></span> <span>(</span><span>e</span><span>)</span>
</p><p><span>Received</span> <span>byte:</span> <span>108</span><span></span> <span>(</span><span>l</span><span>)</span>
</p><p><span>Received</span> <span>byte:</span> <span>108</span><span></span> <span>(</span><span>l</span><span>)</span>
</p><p><span>Received</span> <span>byte:</span> <span>111</span><span></span> <span>(</span><span>o</span><span>)</span>
</p></code></pre>
<p>Just change the <code>receiver</code> implementation a little bit:</p>
<pre><code translate="no" tabindex="0"><p><span>def</span> <span>decode_signal</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>return</span> <span>unless</span> <span>@position</span> <span>==</span> <span>8</span> <span># if not yet accumulated a byte, keep accumulating</span>
</p><p><span>print</span> <span>@accumulator</span><span>.</span><span>chr</span> <span># print the byte as a character</span>
</p><p><span>@accumulator</span> <span>=</span> <span>0</span> <span># reset the accumulator</span>
</p><p><span>@position</span> <span>=</span> <span>0</span> <span># reset position for the next byte</span>
</p><p><span>end</span>
</p></code></pre>
<p>And then:</p>
<pre><code translate="no" tabindex="0"><p><span>$</span> <span>ruby</span> <span>sender.rb</span> <span>96875</span> <span>Hello</span>
</p><p><span># In the receiver's terminal</span>
</p><p><span>Process</span> <span>ID:</span> <span>96875</span>
</p><p><span>Hello</span>
</p></code></pre>
<p>However, if we send the message again, the receiver will print everything in the same line:</p>
<pre><code translate="no" tabindex="0"><p>$ <span>ruby</span> <span>sender</span><span>.</span><span>rb</span> <span>96875</span> <span>Hello</span>
</p><p>$ <span>ruby</span> <span>sender</span><span>.</span><span>rb</span> <span>96875</span> <span>Hello</span>
</p><p><span># In the receiver's terminal</span>
</p><p><span>Process</span> <span>ID</span><span>:</span> <span>96875</span>
</p><p><span>HelloHello</span>
</p></code></pre>
<p>It’s obvious: the receiver doesn’t know where the sender finished the message, so it’s impossible to know where we should stop one message and print the next one on a new line with <code>\n</code>.</p>
<p>We should then determine how the sender indicates the end of the message. How about being it all <em>zeroes</em> (<code>0000 0000</code>)?</p>
<ul>
<li>We send the message: first 5 bytes representing the “hello” message</li>
<li>Then we send a “NULL terminator”, just one byte <em>0</em> (<code>0000 0000</code>)</li>
</ul>
<pre><code translate="no" tabindex="0"><p>0110 1000 # h
</p><p>0110 0101 # e
</p><p>0110 1000 # l
</p><p>0110 1000 # l
</p><p>0110 1111 # o
</p><p>0000 0000 # NULL
</p></code></pre>
<p>Hence, when the <em>receiver</em> gets a NULL terminator, it will print a line feed <code>\n</code>. Let’s change the <code>sender.rb</code> first:</p>
<pre><code translate="no" tabindex="0"><p><span>receiver_pid</span> <span>=</span> <span>ARGV</span><span>[</span><span>0</span><span>]</span><span>.</span><span>to_i</span>
</p><p><span>message</span> <span>=</span> <span>ARGV</span><span>[</span><span>1</span><span>..</span><span>-</span><span>1</span><span>]</span><span>.</span><span>join</span><span>(</span><span>'</span><span> </span><span>'</span><span>)</span>
</p><p><span>def</span> <span>encode_byte</span><span>(</span><span>byte</span><span>)</span>
</p><p><span>8</span><span>.</span><span>times</span><span>.</span><span>map</span> <span>do</span> <span>|</span><span>i</span><span>|</span>
</p><p><span># Extract each bit from the byte, starting from the LSB</span>
</p><p><span>(</span><span>byte</span> <span>&gt;&gt;</span> <span>i</span><span>)</span> <span>&amp;</span> <span>1</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>message</span><span>.</span><span>bytes</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>byte</span><span>|</span>
</p><p><span>encode_byte</span><span>(</span><span>byte</span><span>)</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>bit</span><span>|</span>
</p><p><span>signal</span> <span>=</span> <span>bit</span> <span>==</span> <span>0</span> <span>?</span> <span>'</span><span>SIGUSR1</span><span>'</span> <span>:</span> <span>'</span><span>SIGUSR2</span><span>'</span>
</p><p><span>Process</span><span>.</span><span>kill</span><span>(</span><span>signal</span><span>,</span> <span>receiver_pid</span><span>)</span>
</p><p><span>sleep</span> <span>0.001</span> <span># Delay to allow the receiver to process the signal</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span># Send NULL terminator (0000 0000)</span>
</p><p><span>8</span><span>.</span><span>times</span> <span>do</span>
</p><p><span>Process</span><span>.</span><span>kill</span><span>(</span><span>'</span><span>SIGUSR1</span><span>'</span><span>,</span> <span>receiver_pid</span><span>)</span>
</p><p><span>sleep</span> <span>0.001</span> <span># Delay to allow the receiver to process the signal</span>
</p><p><span>end</span>
</p><p><span>puts</span> <span>"</span><span>Message sent to receiver (PID: </span><span>#{</span><span>receiver_pid</span><span>}</span><span>)</span><span>"</span>
</p></code></pre>
<p>Then, the <code>receiver.rb</code>:</p>
<pre><code translate="no" tabindex="0"><p><span>@position</span> <span>=</span> <span>0</span> <span># start with the LSB</span>
</p><p><span>@accumulator</span> <span>=</span> <span>0</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR1</span><span>'</span><span>)</span> <span>{</span> <span>decode_signal</span><span>(</span><span>0</span><span>)</span> <span>}</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR2</span><span>'</span><span>)</span> <span>{</span> <span>decode_signal</span><span>(</span><span>1</span><span>)</span> <span>}</span>
</p><p><span>def</span> <span>decode_signal</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>return</span> <span>unless</span> <span>@position</span> <span>==</span> <span>8</span> <span># if not yet accumulated a byte, keep accumulating</span>
</p><p><span>if</span> <span>@accumulator</span><span>.</span><span>zero?</span> <span># NULL terminator received</span>
</p><p><span>print</span> <span>"</span><span>\n</span><span>"</span>
</p><p><span>else</span>
</p><p><span>print</span> <span>@accumulator</span><span>.</span><span>chr</span> <span># print the byte as a character</span>
</p><p><span>end</span>
</p><p><span>@accumulator</span> <span>=</span> <span>0</span> <span># reset the accumulator</span>
</p><p><span>@position</span> <span>=</span> <span>0</span> <span># reset position for the next byte</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span># The left shift operator (&lt;&lt;) is used to </span>
</p><p><span># shift the bits of the number to the left.</span>
</p><p><span>#</span>
</p><p><span># This is equivalent of: (2 ** @position) * bit</span>
</p><p><span>@accumulator</span> <span>+=</span> <span>(</span><span>bit</span> <span>&lt;&lt;</span> <span>@position</span><span>)</span>
</p><p><span>@position</span> <span>+=</span> <span>1</span> <span># move to the next bit position: 0 becomes 1, 1 becomes 2, etc.</span>
</p><p><span>end</span>
</p><p><span>puts</span> <span>"</span><span>Process ID: </span><span>#{</span><span>Process</span><span>.</span><span>pid</span><span>}</span><span>"</span>
</p><p><span>sleep</span>
</p></code></pre>
<p>Output:</p>
<pre><code translate="no" tabindex="0"><p>$ ruby sender.rb 96875 Hello, World!
</p><p>$ ruby sender.rb 96875 You're welcome
</p><p>$ ruby sender.rb 96875 How are you?
</p><p># Receiver
</p><p>Process ID: 97176
</p><p>Hello, World!
</p><p>You're welcome
</p><p>How are you?
</p></code></pre>
<blockquote>
<p>OMG Leandro! That’s amazing!</p>
</blockquote>
<p><em>Amazing, right?</em> We just built an entire communication system between two processes using one of the most primitive methods available: <strong>UNIX signals.</strong></p>
<p>The sky’s the limit now! Why not build a <em>full-fledged message broker</em> using this crazy technique?</p>
<h2>A modest message broker using UNIX signals</h2>
<p>We’ll break down the development into three components:</p>
<ol>
<li><strong>Broker</strong>: the intermediary that routes messages</li>
<li><strong>Consumer</strong>: processes that receive messages</li>
<li><strong>Producer</strong>: processes that send messages</li>
</ol>
<p><img src="https://leandronsp.com/uploads/3395.png" alt="image"></p>
<ol>
<li>Let’s start with the Broker. It should register itself with the producer, then trap incoming signals, decode them, and enqueue the messages for delivery to consumers via outgoing signals:</li>
</ol>
<pre><code translate="no" tabindex="0"><p><span>#!/usr/bin/env ruby</span>
</p><p><span>require_relative</span> <span>'</span><span>signal_codec</span><span>'</span>
</p><p><span>require_relative</span> <span>'</span><span>consumer</span><span>'</span>
</p><p><span>class</span> <span>Broker</span> 
</p><p><span>PID</span> <span>=</span> <span>'</span><span>broker.pid</span><span>'</span><span>.</span><span>freeze</span>
</p><p><span>def</span> <span>initialize</span>
</p><p><span>@codec</span> <span>=</span> <span>SignalCodec</span><span>.</span><span>new</span>
</p><p><span>@queue</span> <span>=</span> <span>Queue</span><span>.</span><span>new</span>
</p><p><span>@consumer_index</span> <span>=</span> <span>0</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>start</span> 
</p><p><span>register_broker</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR1</span><span>'</span><span>)</span> <span>{</span> <span>process_bit</span><span>(</span><span>0</span><span>)</span> <span>}</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR2</span><span>'</span><span>)</span> <span>{</span> <span>process_bit</span><span>(</span><span>1</span><span>)</span> <span>}</span>
</p><p><span>puts</span> <span>"</span><span>Broker PID: </span><span>#{</span><span>Process</span><span>.</span><span>pid</span><span>}</span><span>"</span>
</p><p><span>puts</span> <span>"</span><span>Waiting for messages...</span><span>"</span>
</p><p><span>distribute_messages</span>
</p><p><span>sleep</span> <span># Keep alive</span>
</p><p><span>end</span> 
</p><p><span>private</span>
</p><p><span>def</span> <span>process_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>@codec</span><span>.</span><span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span> <span>do</span> <span>|</span><span>message</span><span>|</span>
</p><p><span>@queue</span><span>.</span><span>push</span><span>(</span><span>message</span><span>)</span> <span>unless</span> <span>message</span><span>.</span><span>empty?</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>register_broker</span> 
</p><p><span>File</span><span>.</span><span>write</span><span>(</span><span>PID</span><span>,</span> <span>Process</span><span>.</span><span>pid</span><span>)</span>
</p><p><span>at_exit</span> <span>{</span> <span>File</span><span>.</span><span>delete</span><span>(</span><span>PID</span><span>)</span> <span>if</span> <span>File</span><span>.</span><span>exist?</span><span>(</span><span>PID</span><span>)</span> <span>}</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>distribute_messages</span>
</p><p><span>Thread</span><span>.</span><span>new</span> <span>do</span>
</p><p><span>loop</span> <span>do</span>
</p><p><span>sleep</span> <span>0.1</span>
</p><p><span>next</span> <span>if</span> <span>@queue</span><span>.</span><span>empty?</span>
</p><p><span>consumers</span> <span>=</span> <span>File</span><span>.</span><span>exist?</span><span>(</span><span>Consumer</span><span>::</span><span>FILE</span><span>)</span> <span>?</span> <span>File</span><span>.</span><span>readlines</span><span>(</span><span>Consumer</span><span>::</span><span>FILE</span><span>)</span><span>.</span><span>map</span><span>(</span><span>&amp;</span><span>:to_i</span><span>)</span> <span>:</span> <span>[</span><span>]</span>
</p><p><span>next</span> <span>if</span> <span>consumers</span><span>.</span><span>empty?</span>
</p><p><span>message</span> <span>=</span> <span>@queue</span><span>.</span><span>pop</span><span>(</span><span>true</span><span>)</span> <span>rescue</span> <span>next</span>
</p><p><span>consumer_pid</span> <span>=</span> <span>consumers</span><span>[</span><span>@consumer_index</span> <span>%</span> <span>consumers</span><span>.</span><span>size</span><span>]</span>
</p><p><span>@consumer_index</span> <span>+=</span> <span>1</span>
</p><p><span>puts</span> <span>"</span><span>[SEND] </span><span>#{</span><span>message</span><span>}</span><span> → Consumer </span><span>#{</span><span>consumer_pid</span><span>}</span><span>"</span>
</p><p><span>@codec</span><span>.</span><span>send_message</span><span>(</span><span>message</span><span>,</span> <span>consumer_pid</span><span>)</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>if</span> <span>__FILE__</span> <span>==</span> <span>$0</span> 
</p><p><span>broker</span> <span>=</span> <span>Broker</span><span>.</span><span>new</span>
</p><p><span>broker</span><span>.</span><span>start</span>
</p><p><span>end</span>
</p></code></pre>
<ul>
<li>The broker registers itself</li>
<li>Traps incoming signals <code>USR1</code> (bit 0) and <code>USR2</code> (bit 1)</li>
<li>Enqueues the messages</li>
<li>Send messages to consumers using outgoing signals (<code>USR1</code> and <code>USR2</code> too)</li>
</ul>
<p><em>Note that we’re using a module called <code>SignalCodec</code> which will be explained soon. Basically this module contains all core components to encode/decode signals and perform bitwise operations.</em></p>
<ol start="2">
<li>Now the <code>Consumer</code> implementation:</li>
</ol>
<pre><code translate="no" tabindex="0"><p><span>#!/usr/bin/env ruby</span>
</p><p><span>require_relative</span> <span>'</span><span>signal_codec</span><span>'</span>
</p><p><span>class</span> <span>Consumer</span>
</p><p><span>FILE</span> <span>=</span> <span>'</span><span>consumers.txt</span><span>'</span><span>.</span><span>freeze</span>
</p><p><span>def</span> <span>initialize</span>
</p><p><span>@codec</span> <span>=</span> <span>SignalCodec</span><span>.</span><span>new</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>start</span>
</p><p><span>register_consumer</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR1</span><span>'</span><span>)</span> <span>{</span> <span>process_bit</span><span>(</span><span>0</span><span>)</span> <span>}</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR2</span><span>'</span><span>)</span> <span>{</span> <span>process_bit</span><span>(</span><span>1</span><span>)</span> <span>}</span>
</p><p><span>puts</span> <span>"</span><span>Consumer PID: </span><span>#{</span><span>Process</span><span>.</span><span>pid</span><span>}</span><span>"</span>
</p><p><span>puts</span> <span>"</span><span>Waiting for messages...</span><span>"</span>
</p><p><span>sleep</span> <span># Keep alive</span>
</p><p><span>end</span>
</p><p><span>private</span>
</p><p><span>def</span> <span>process_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>@codec</span><span>.</span><span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span> <span>do</span> <span>|</span><span>message</span><span>|</span>
</p><p><span>puts</span> <span>"</span><span>[RECEIVE] </span><span>#{</span><span>message</span><span>}</span><span>"</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>register_consumer</span>
</p><p><span>File</span><span>.</span><span>open</span><span>(</span><span>FILE</span><span>,</span> <span>'</span><span>a</span><span>'</span><span>)</span> <span>{</span> <span>|</span><span>f</span><span>|</span> <span>f</span><span>.</span><span>puts</span> <span>Process</span><span>.</span><span>pid</span> <span>}</span>
</p><p><span>at_exit</span> <span>{</span> <span>deregister_consumer</span> <span>}</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>deregister_consumer</span>
</p><p><span>if</span> <span>File</span><span>.</span><span>exist?</span><span>(</span><span>FILE</span><span>)</span>
</p><p><span>consumers</span> <span>=</span> <span>File</span><span>.</span><span>readlines</span><span>(</span><span>FILE</span><span>)</span><span>.</span><span>map</span><span>(</span><span>&amp;</span><span>:strip</span><span>)</span><span>.</span><span>reject</span> <span>{</span> <span>|</span><span>pid</span><span>|</span> <span>pid</span><span>.</span><span>to_i</span> <span>==</span> <span>Process</span><span>.</span><span>pid</span> <span>}</span>
</p><p><span>File</span><span>.</span><span>write</span><span>(</span><span>FILE</span><span>,</span> <span>consumers</span><span>.</span><span>join</span><span>(</span><span>"</span><span>\n</span><span>"</span><span>)</span><span>)</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>if</span> <span>__FILE__</span> <span>==</span> <span>$0</span>
</p><p><span>consumer</span> <span>=</span> <span>Consumer</span><span>.</span><span>new</span>
</p><p><span>consumer</span><span>.</span><span>start</span>
</p><p><span>end</span>
</p></code></pre>
<ul>
<li>The consumer starts and registers itself with the broker</li>
<li>Consumer then traps incoming signals (bit 0 and bit 1)</li>
<li>Decodes and prints messages</li>
</ul>
<ol start="3">
<li>Last but not least, the <code>Producer</code> implementation, which is pretty straightforward:</li>
</ol>
<pre><code translate="no" tabindex="0"><p><span>#!/usr/bin/env ruby</span>
</p><p><span>require_relative</span> <span>'</span><span>signal_codec</span><span>'</span>
</p><p><span>require_relative</span> <span>'</span><span>broker</span><span>'</span>
</p><p><span>unless</span> <span>File</span><span>.</span><span>exist?</span><span>(</span><span>Broker</span><span>::</span><span>PID</span><span>)</span>
</p><p><span>abort</span> <span>"</span><span>Error: Broker not running (</span><span>#{</span><span>Broker</span><span>::</span><span>PID</span><span>}</span><span> not found)</span><span>"</span>
</p><p><span>end</span>
</p><p><span>broker_pid</span> <span>=</span> <span>File</span><span>.</span><span>read</span><span>(</span><span>Broker</span><span>::</span><span>PID</span><span>)</span><span>.</span><span>strip</span><span>.</span><span>to_i</span>
</p><p><span>message</span> <span>=</span> <span>ARGV</span><span>.</span><span>join</span><span>(</span><span>'</span><span> </span><span>'</span><span>)</span>
</p><p><span>if</span> <span>message</span><span>.</span><span>empty?</span>
</p><p><span>puts</span> <span>"</span><span>Usage: ruby producer.rb &lt;message&gt;</span><span>"</span>
</p><p><span>exit</span> <span>1</span>
</p><p><span>end</span>
</p><p><span>codec</span> <span>=</span> <span>SignalCodec</span><span>.</span><span>new</span>
</p><p><span>puts</span> <span>"</span><span>Sending: </span><span>#{</span><span>message</span><span>}</span><span>"</span>
</p><p><span>codec</span><span>.</span><span>send_message</span><span>(</span><span>message</span><span>,</span> <span>broker_pid</span><span>)</span>
</p><p><span>puts</span> <span>"</span><span>Message sent to broker (PID: </span><span>#{</span><span>broker_pid</span><span>}</span><span>)</span><span>"</span>
</p></code></pre>
<ul>
<li>Producer receives a ASCII message from the <em>STDIN</em></li>
<li>Encode and sends the message to the broker via outgoing signals</li>
</ul>
<p>So far, this architecture should look familiar. Many broker implementations follow these basic foundations.</p>
<blockquote>
<p>Of course, production-ready implementations are far more robust than this one. Here, we’re just poking around with hacking and experimentation</p>
</blockquote>
<p>The coolest part is the <code>SignalCodec</code> though:</p>
<pre><code translate="no" tabindex="0"><p><span>class</span> <span>SignalCodec</span> 
</p><p><span>SIGNAL_DELAY</span> <span>=</span> <span>0.001</span> <span># Delay between signals to allow processing</span>
</p><p><span>def</span> <span>initialize</span>
</p><p><span>@accumulator</span> <span>=</span> <span>0</span>
</p><p><span>@position</span> <span>=</span> <span>0</span>
</p><p><span>@buffer</span> <span>=</span> <span>[</span><span>]</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>@accumulator</span> <span>+=</span> <span>(</span><span>bit</span> <span>&lt;&lt;</span> <span>@position</span><span>)</span>
</p><p><span>@position</span> <span>+=</span> <span>1</span>
</p><p><span>if</span> <span>@position</span> <span>==</span> <span>8</span> <span># Byte is complete</span>
</p><p><span>if</span> <span>@accumulator</span><span>.</span><span>zero?</span> <span># Message complete - NULL terminator</span>
</p><p><span>decoded</span> <span>=</span> <span>@buffer</span><span>.</span><span>pack</span><span>(</span><span>"</span><span>C*</span><span>"</span><span>)</span><span>.</span><span>force_encoding</span><span>(</span><span>'</span><span>UTF-8</span><span>'</span><span>)</span>
</p><p><span>yield</span><span>(</span><span>decoded</span><span>)</span> <span>if</span> <span>block_given?</span>
</p><p><span>@buffer</span><span>.</span><span>clear</span>
</p><p><span>else</span> 
</p><p><span>@buffer</span> <span>&lt;&lt;</span> <span>@accumulator</span>
</p><p><span>end</span>
</p><p><span>@position</span> <span>=</span> <span>0</span>
</p><p><span>@accumulator</span> <span>=</span> <span>0</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>send_message</span><span>(</span><span>message</span><span>,</span> <span>pid</span><span>)</span>
</p><p><span>message</span><span>.</span><span>each_byte</span> <span>do</span> <span>|</span><span>byte</span><span>|</span>
</p><p><span>8</span><span>.</span><span>times</span> <span>do</span> <span>|</span><span>i</span><span>|</span>
</p><p><span>bit</span> <span>=</span> <span>(</span><span>byte</span> <span>&gt;&gt;</span> <span>i</span><span>)</span> <span>&amp;</span> <span>1</span>
</p><p><span>signal</span> <span>=</span> <span>bit</span> <span>==</span> <span>0</span> <span>?</span> <span>'</span><span>SIGUSR1</span><span>'</span> <span>:</span> <span>'</span><span>SIGUSR2</span><span>'</span>
</p><p><span>Process</span><span>.</span><span>kill</span><span>(</span><span>signal</span><span>,</span> <span>pid</span><span>)</span>
</p><p><span>sleep</span> <span>SIGNAL_DELAY</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span># Send NULL terminator (0000 0000)</span>
</p><p><span>8</span><span>.</span><span>times</span> <span>do</span>
</p><p><span>Process</span><span>.</span><span>kill</span><span>(</span><span>'</span><span>SIGUSR1</span><span>'</span><span>,</span> <span>pid</span><span>)</span>
</p><p><span>sleep</span> <span>SIGNAL_DELAY</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p></code></pre>
<p>If you’ve been following along, this shouldn’t be hard to understand, but I’ll break down how this beautiful piece of code works:</p>
<ul>
<li>The codec is initialized with the bit position at zero, as well as the accumulator</li>
<li>A buffer is also initialized to store accumulated bits until a complete byte is formed</li>
<li>The <code>accumulate_bit</code> method should be familiar from our earlier implementation, but it now accepts a closure (block) that lets the caller decide what to do with each decoded byte</li>
<li><code>send_message</code> encodes a message into bits and sends them via UNIX signals</li>
</ul>
<p>Everything in action:</p>
<p><img src="https://leandronsp.com/uploads/3170.png" alt="image"></p>
<p><em>How cool, amazing, wonderful, impressive, astonishing is that?</em></p>
<h2>Conclusion</h2>
<p>Yes, we built a message broker using nothing but <strong>UNIX signals</strong> and a bit of Ruby magic. Sure, <strong>it’s not production-ready</strong>, and you definitely shouldn’t use this in your next startup (please don’t), but that was never the point.</p>
<p>The real takeaway here isn’t the broker itself: it’s understanding how the fundamentals work. We explored binary operations, UNIX signals, and IPC in a hands-on way that most people never bother with.</p>
<p>We took something “useless” and made it work, just for fun. So next time someone asks you about message brokers, you can casually mention that you once built (or saw) one using just two signals. And if they look at you weird, well, that’s their problem. Now go build something equally useless and amazing. The world needs more hackers who experiment just for the fun of it.</p>
<p><em>Happy hacking!</em></p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Today is when the Amazon brain drain sent AWS down the spout (794 pts)]]></title>
            <link>https://www.theregister.com/2025/10/20/aws_outage_amazon_brain_drain_corey_quinn/</link>
            <guid>45649178</guid>
            <pubDate>Mon, 20 Oct 2025 20:50:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/10/20/aws_outage_amazon_brain_drain_corey_quinn/">https://www.theregister.com/2025/10/20/aws_outage_amazon_brain_drain_corey_quinn/</a>, See on <a href="https://news.ycombinator.com/item?id=45649178">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p><span>column</span> "It's always DNS" is a long-standing sysadmin saw, and with good reason: a disproportionate number of outages are at their heart DNS issues. And so today, as AWS is still repairing its downed cloud as this article goes to press, it becomes clear that the culprit is once again DNS. But if you or I know this, AWS certainly does.</p>
<p>And so, a quiet suspicion starts to circulate: where have the senior AWS engineers who've been to this dance before gone? And the answer increasingly is that they've left the building — taking decades of hard-won institutional knowledge about how AWS's systems work at scale right along with them.</p>
<h3>What happened?</h3>
<p>AWS reports that on October 20, at 12:11 AM PDT, it began investigating “increased error rates and latencies for multiple AWS services in the US-EAST-1 Region.” About an hour later, at 1:26 AM, the company confirmed “significant error rates for requests made to the DynamoDB endpoint” in that region. By 2:01 AM, engineers had identified <a target="_blank" href="https://www.theregister.com/2025/10/20/aws_outage_chaos/">DNS resolution of the DynamoDB API endpoint</a> for US-EAST-1 as the likely root cause, which led to cascading failures for most other things in that region. DynamoDB is a "foundational service" upon which a whole mess of other AWS services rely, so the blast radius for an outage touching this thing can be huge.</p>
<p>As a result, <a target="_blank" href="https://www.theregister.com/2025/10/20/amazon_aws_outage/">much of the internet stopped working</a>: banking, gaming, social media, government services, buying things I don't need on Amazon.com itself, etc.</p>
<p>AWS has given increasing levels of detail, as is their tradition, when outages strike, and as new information comes to light. Reading through it, one really gets the sense that it took them 75 minutes to go from "things are breaking" to "we've narrowed it down to a single service endpoint, but are still researching," which is something of a bitter pill to swallow. To be clear: I've seen zero signs that this stems from a lack of transparency, and every indication that they legitimately did not know what was breaking for a patently absurd length of time.</p>

    

<p>Note that for those 75 minutes, visitors to the AWS status page (reasonably wondering why their websites and other workloads had just burned down and crashed into the sea) were met with an "all is well!" default response. Ah well, it's not as if AWS had <a target="_blank" href="https://aws.amazon.com/message/12721/" rel="nofollow">previously called out slow outage notification times</a> as an area for improvement. <a target="_blank" href="https://aws.amazon.com/message/11201/" rel="nofollow">Multiple times</a> even. We can <a target="_blank" href="https://aws.amazon.com/message/41926/" rel="nofollow">keep doing this</a> if you'd like.</p>
<h3>The prophecy</h3>
<p>AWS is very, very good at infrastructure. You can tell this is a true statement by the fact that a single one of their 38 regions going down (albeit a very important region!) causes this kind of attention, as opposed to it being "just another Monday outage." At AWS's scale, all of their issues are complex; this isn't going to be a simple issue that someone should have caught, just because they've already hit similar issues years ago and ironed out the kinks in their resilience story.</p>
<p>Once you reach a certain point of scale, there are no simple problems left. What's more concerning to me is the way it seems AWS has been flailing all day trying to run this one to ground. Suddenly, I'm reminded of something I had tried very hard to forget.</p>

        


        

<p>At the end of 2023, Justin Garrison left AWS and <a target="_blank" href="https://justingarrison.com/blog/2023-12-30-amazons-silent-sacking/" rel="nofollow">roasted them on his way out the door</a>. He stated that AWS had seen an increase in Large Scale Events (or LSEs), and predicted significant outages in 2024. It would seem that he discounted the power of inertia, but the pace of senior AWS departures certainly hasn't slowed — and now, with an outage like this, one is forced to wonder whether those departures are themselves a contributing factor.</p>
<p>You can hire a bunch of very smart people who will explain how DNS works at a deep technical level (or you can hire me, who will incorrect you by explaining that it's a database), but the one thing you can't hire for is the person who remembers that when DNS starts getting wonky, check that seemingly unrelated system in the corner, because it's historically played a contributing role to some outages of yesteryear.</p>

        

<p>When that tribal knowledge departs, you're left having to reinvent an awful lot of in-house expertise that didn't want to participate in your RTO games, or play Layoff Roulette yet again this cycle. This doesn't impact your service reliability — until one day it very much does, in spectacular fashion. I suspect that day is today.</p>
<ul>

<li><a href="https://www.theregister.com/2025/10/20/aws_outage_chaos/">AWS outage exposes Achilles heel: central control plane</a></li>

<li><a href="https://www.theregister.com/2025/10/20/amazon_aws_outage/">Major AWS outage across US-East region breaks half the internet</a></li>

<li><a href="https://www.theregister.com/2025/10/17/amazon_nuke_washington/">Amazon spills plan to nuke Washington...with X-Energy mini-reactors</a></li>

<li><a href="https://www.theregister.com/2025/10/06/amazon_007_without_golden_gun/">Amazon turns James Bond into the Man Without the Golden Gun</a></li>
</ul>
<h3>The talent drain evidence</h3>
<p>This is <em>The Register</em>, a respected journalistic outlet. As a result, I know that if I publish this piece as it stands now, an AWS PR flak will appear as if by magic, waving their hands, insisting that "there is no talent exodus at AWS," a la Baghdad Bob. Therefore, let me forestall that time-wasting enterprise with some data.</p>
<ul>
<li>It is a fact that there have been <a target="_blank" href="https://www.cnbc.com/2025/07/17/amazon-web-services-has-some-layoffs.html" rel="nofollow">27,000+ Amazonians impacted by layoffs</a> between 2022 and 2024, continuing into 2025. It's hard to know how many of these were AWS versus other parts of its Amazon parent, because the company is notoriously tight-lipped about staffing issues.</li>

<li>Internal documents reportedly say that Amazon <a target="_blank" href="https://www.engadget.com/amazon-attrition-leadership-ctsmd-201800110-201800100.html" rel="nofollow">suffers from 69 percent to 81 percent regretted attrition</a> across all employment levels. In other words, "people quitting who we wish didn't."</li>

<li>The internet is full of anecdata of senior Amazonians lamenting the hamfisted approach of their Return to Office initiative; <a target="_blank" href="https://finance.yahoo.com/news/amazon-back-office-crusade-could-090200105.html/" rel="nofollow">experts have weighed in</a> citing similar concerns.</li>
</ul>
<p>If you were one of the early employees who built these systems, the world is your oyster. There's little reason to remain at a company that increasingly demonstrates apparent disdain for your expertise.</p>
<h3>My take</h3>
<p>This is a tipping point moment. Increasingly, it seems that the talent who understood the deep failure modes is gone. The new, leaner, presumably less expensive teams lack the institutional knowledge needed to, if not prevent these outages in the first place, significantly reduce the time to detection and recovery. Remember, there was a time when Amazon's "Frugality" leadership principle meant doing more with less, not doing everything with basically nothing. AWS's operational strength was built on redundant, experienced people, and when you cut to the bone, basic things start breaking.</p>
<p>I want to be very clear on one last point. This isn't about the technology being old. It's about the people maintaining it being new. If I had to guess what happens next, the market will forgive AWS this time, but the pattern will continue.</p>
<p>AWS will almost certainly say this was an "isolated incident," but when you've hollowed out your engineering ranks, every incident becomes more likely. The next outage is already brewing. It's just a matter of which understaffed team trips over which edge case first, because the chickens are coming home to roost. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[iOS 26.1 lets users control Liquid Glass transparency (194 pts)]]></title>
            <link>https://www.macrumors.com/2025/10/20/ios-26-1-liquid-glass-toggle/</link>
            <guid>45648266</guid>
            <pubDate>Mon, 20 Oct 2025 19:39:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2025/10/20/ios-26-1-liquid-glass-toggle/">https://www.macrumors.com/2025/10/20/ios-26-1-liquid-glass-toggle/</a>, See on <a href="https://news.ycombinator.com/item?id=45648266">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2025/10/20/ios-26-1-liquid-glass-toggle/"><p>With the fourth betas of iOS 26.1, iPadOS 26.1, and macOS 26.1, Apple has introduced a new setting that's designed to allow users to customize the look of Liquid Glass.</p>
<p><img src="https://images.macrumors.com/t/OCRA_6Z8V4J8hTc_cWfWK-zkX9w=/400x0/article-new/2025/10/ios-26-1-liquid-glass-opaque.jpg?lossy" srcset="https://images.macrumors.com/t/OCRA_6Z8V4J8hTc_cWfWK-zkX9w=/400x0/article-new/2025/10/ios-26-1-liquid-glass-opaque.jpg?lossy 400w,https://images.macrumors.com/t/tj3V79n2YvpR0tslbBIv6K1qcvY=/800x0/article-new/2025/10/ios-26-1-liquid-glass-opaque.jpg?lossy 800w,https://images.macrumors.com/t/Nawho-r4OQp6pTe_7_FIPthUu2A=/1600x0/article-new/2025/10/ios-26-1-liquid-glass-opaque.jpg 1600w,https://images.macrumors.com/t/6PpVz-MHL6Gyx_oDr4Hqtac40xA=/2500x0/filters:no_upscale()/article-new/2025/10/ios-26-1-liquid-glass-opaque.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="ios 26 1 liquid glass opaque" width="2000" height="1125"><br>The toggle lets users select from a clear look for Liquid Glass, or a tinted look. Clear is the current Liquid Glass design, which is more transparent and shows the background underneath buttons, bars, and menus, while tinted increases the opacity of Liquid Glass and adds more contrast.</p>
<p>The new setting can be found on iOS and iPadOS by going to Settings &gt; Display and Brightness, or System Settings &gt; Appearance on the Mac.</p>
<p>Apple says that the new toggle was added because during the beta testing period over the summer, user feedback suggested that some people would prefer to have a more opaque option for Liquid Glass. The added setting provides additional customization in iOS 26.1, iPadOS 26.1, and macOS Tahoe 26.1. </p>
<p>Increasing opacity and adding contrast applies to Liquid Glass throughout the operating system, including in apps and Lock Screen notifications.</p>
<p>There are multiple other new features in iOS 26.1, including a new slide to stop feature for alarms and timers, new <a href="https://www.macrumors.com/guide/apple-intelligence/">Apple Intelligence</a> languages, a redesigned <a href="https://www.macrumors.com/roundup/apple-tv/">Apple TV</a> app icon, changes to the Settings app, and more, with a full list of features <a href="https://www.macrumors.com/guide/ios-26-1-beta-features/">available in our iOS 26.1 feature guide</a>.</p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2025/10/17/iphone-air-production-to-be-cut-amid-lower-sales/">Apple Said to Cut iPhone Air Production Amid Underwhelming Sales</a></h3><p>Apple plans to cut production of the iPhone Air amid underwhelming sales performance, Japan's Mizuho Securities believes (via The Elec).
The Japanese investment banking and securities firm claims that the iPhone 17 Pro and iPhone 17 Pro Max are seeing higher sales than their predecessors during the same period last year, while the standard iPhone 17 is a major success, performing...</p></div><div><h3><a href="https://www.macrumors.com/2025/10/18/ios-26-1-to-ios-26-4-expected-features/">iOS 26.1 to iOS 26.4 Will Add These New Features to Your iPhone</a></h3><p>Saturday October 18, 2025 11:00 am PDT by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>iOS 26 was released last month, but the software train never stops, and iOS 26.1 beta testing is already underway. So far, iOS 26.1 makes both Apple Intelligence and Live Translation on compatible AirPods available in additional languages, and it includes some other minor changes across the Apple Music, Calendar, Photos, Clock, and Safari apps.
More features and changes will follow in future ...</p></div><div><h3><a href="https://www.macrumors.com/2025/10/17/ios-26-0-2-coming-soon/">iOS 26.0.2 Update for iPhones Coming Soon</a></h3><p>Apple's software engineers continue to internally test iOS 26.0.2, according to MacRumors logs, which have been a reliable indicator of upcoming iOS versions.
iOS 26.0.2 will be a minor update that addresses bugs and/or security vulnerabilities, but we do not know any specific details yet.
The update will likely be released by the end of next week.
Last month, Apple released iOS 26.0.1,...</p></div><div><h3><a href="https://www.macrumors.com/2025/10/16/heres-whats-coming-next-from-apple/">Apple's Next Rumored Products: New HomePod Mini, Apple TV, and More</a></h3><p>Thursday October 16, 2025 9:13 am PDT by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Apple on Wednesday updated the 14-inch MacBook Pro, iPad Pro, and Vision Pro with its next-generation M5 chip, but previous rumors have indicated that the company still plans to announce at least a few additional products before the end of the year.
The following Apple products have at one point been rumored to be updated in 2025, although it is unclear if the timeframe for any of them has...</p></div><div><h3><a href="https://www.macrumors.com/2025/10/19/ios-26-4-revamped-siri-concerns/">Some Apple Employees Have 'Concerns' About iOS 26.4's Revamped Siri</a></h3><p>iOS 26.4 is expected to introduce a revamped version of Siri powered by Apple Intelligence, but not everyone is satisfied with how well it works.
In his Power On newsletter today, Bloomberg's Mark Gurman said some of Apple's software engineers have "concerns" about the overhauled Siri's performance. However, he did not provide any specific details about the shortcomings.
iOS 26.4 will...</p></div><div><h3><a href="https://www.macrumors.com/2025/10/18/new-ipad-pro-six-key-upgrades/">New iPad Pro Has Six Key Upgrades Beyond M5 Chip</a></h3><p>Saturday October 18, 2025 10:57 am PDT by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>While the new iPad Pro's headline feature is the M5 chip, the device has some other changes, including N1 and C1X chips, faster storage speeds, and more.
With the M5 chip, the new iPad Pro has up to a 20% faster CPU and up to a 40% faster GPU compared to the previous model with the M4 chip, according to Geekbench 6 results. Keep in mind that 256GB and 512GB configurations have a 9-core CPU,...</p></div><div><h3><a href="https://www.macrumors.com/2025/10/20/ios-26-1-liquid-glass-toggle/">iOS 26.1 Beta 4 Lets Users Control Liquid Glass Transparency with New Toggle</a></h3><p>Monday October 20, 2025 10:57 am PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>With the fourth betas of iOS 26.1, iPadOS 26.1, and macOS 26.1, Apple has introduced a new setting that's designed to allow users to customize the look of Liquid Glass.
The toggle lets users select from a clear look for Liquid Glass, or a tinted look. Clear is the current Liquid Glass design, which is more transparent and shows the background underneath buttons, bars, and menus, while tinted ...</p></div><div><h3><a href="https://www.macrumors.com/2025/10/16/new-14-inch-macbook-pro-two-key-upgrades/">New 14-Inch MacBook Pro Has Two Key Upgrades Beyond the M5 Chip</a></h3><p>Thursday October 16, 2025 8:31 am PDT by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Apple on Wednesday updated the 14-inch MacBook Pro base model with an M5 chip, and there are two key storage-related upgrades beyond that chip bump.
First, Apple says the new 14-inch MacBook Pro offers up to 2× faster SSD performance than the equivalent previous-generation model, so read and write speeds should get a significant boost. Apple says it is using "the latest storage technology," ...</p></div><div><h3><a href="https://www.macrumors.com/2025/10/16/m5-macbook-air-spring/">M5 MacBook Air Coming Spring 2026 With M5 Mac Studio and Mac Mini in Development</a></h3><p>Thursday October 16, 2025 3:57 pm PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>Apple plans to launch MacBook Air models equipped with the new M5 chip in spring 2026, according to Bloomberg's Mark Gurman. Apple is also working on M5 Pro and M5 Max MacBook Pro models that will come early in the year.
Neither the MacBook Pro models nor the MacBook Air models are expected to get design changes, with Apple focusing on simple chip upgrades. In the case of the MacBook Pro, a m...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[J.P. Morgan's OpenAI loan is strange (241 pts)]]></title>
            <link>https://marketunpack.com/j-p-morgans-openai-loan-is-strange/</link>
            <guid>45648258</guid>
            <pubDate>Mon, 20 Oct 2025 19:38:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://marketunpack.com/j-p-morgans-openai-loan-is-strange/">https://marketunpack.com/j-p-morgans-openai-loan-is-strange/</a>, See on <a href="https://news.ycombinator.com/item?id=45648258">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
             
            <p>In October, <a href="https://openai.com/index/new-credit-facility-enhances-financial-flexibility/" rel="noreferrer">OpenAI secured a 4 billion dollar revolving credit facility from J.P. Morgan and several other banks</a>.  I was surprised when I heard this because OpenAI is a young company with no earnings.  Shouldn't all their capital come from investors?  Let's run some numbers.</p><h2 id="from-first-principles">From first principles</h2><p>Let's do an <a href="https://www.investopedia.com/terms/e/expected-value.asp" rel="noreferrer">Expected Value (EV)</a> calculation, first from the perspective of an investor and then from the perspective of a lender.  We'll pick some arbitrary parameters first, then refine.</p><p>Putting our investor hat on, the possible returns for investing $1,000 into OpenAI look like this:</p><ul><li>Cost: $1,000</li><li>Case 1 (90%): OpenAI goes bankrupt. Return: $0</li><li>Case 2 (9%): OpenAI becomes a big successful company and goes 10x.  Return: $10,000</li><li>Case 3 (1%): OpenAI becomes the big new thing and goes 100x.  Return: $100,000</li></ul><p>Our expected value is:</p>
<!--kg-card-begin: html--><p>
\[\begin{align}
  EV &amp;= -1000 + 0.9 \times 0 + 0.09 \times 10000 + 0.01 \times 100000\\
  EV &amp;= -1000 + 0 + 900 + 1000\\
  EV &amp;= 900
\end{align}\]
</p><!--kg-card-end: html-->
<p>The EV is positive, so this is a good investment.  Obviously, there's a 90% chance of it going to zero, so if this were our only investment, it would be an insanely risky one.  But provided we can do many investments like this and provided their failure cases aren't correlated, this would be a profitable strategy.</p><p>What happens if we instead put our lender hat on?  Using the same probabilities as above, the possible returns for lending $1,000 to OpenAI at 5% interest look like this:</p><ul><li>Cost: $1,000</li><li>Case 1 (90%): OpenAI goes bankrupt. Return: $0</li><li>Case 2 (9%): OpenAI becomes a big successful company and goes 10x.  Return: $1,000 + 5% interest = $1,050</li><li>Case 3 (1%): OpenAI becomes the big new thing and goes 100x.  Return: $1,000 + 5% interest = $1,050</li></ul><p>Lenders don't benefit directly from the success of the company.  Whether it barely scrapes by but manages to repay the loan or becomes the greatest company ever and easily repays the loan, it's all the same to a lender.  So, we can merge cases 2 and 3 into:</p><ul><li>Case 2+3 (10%): OpenAI doesn't go bankrupt.  Return: $1,000 + 5% interest = $1,050</li></ul><p>This makes our EV in the lending case:</p>
<!--kg-card-begin: html--><p>
\[\begin{align}
  EV &amp;= -1000 + 0.9 \times 0 + 0.1 \times 1050\\
  EV &amp;= -1000 + 0 + 105\\
  EV &amp;= -895
\end{align}\]
</p><!--kg-card-end: html-->
<p>The EV is negative, so we'd end up losing most of our money on average.  Lending on these terms doesn't make sense.</p><p>There are two numbers we made up in the above calculation: the probability of bankruptcy and the interest rate.  Let's leave the interest rate fixed at 5% and see what the probability \(p\) would have to be for us to break even.</p>
<!--kg-card-begin: html--><p>
\[\begin{align}
  EV &amp;= -1000 + p \times 0 + (1 - p) \times 1050\\
  EV &amp;= -1000 + 1050 - p \times 1050 \\
  EV &amp;= 50 - p \times 1050 \\[0.5cm]
  &amp; \text{Set EV to 0} \\[0.5cm]
  0 &amp;= 50 - p \times 1050 \\
  p &amp;= \frac{50}{1050} \\
  p &amp;= 0.0476
\end{align}\]
</p><!--kg-card-end: html-->
<p>So, we'd break even if the probability of OpenAI going bankrupt was only about 5%.  In practice, we'd want it to be lower than that so that we made a profit and so that we had a margin of safety in case our assumptions were wrong.</p><p>This 5% failure rate seems very optimistic to me, but this scenario is basically the one the consortium of banks got into.  Concrete details on the deal are sparse, but this <a href="https://thecioleaders.com/openai-locks-down-4-billion-revolving-credit-putting-liquidity-over-10-billion/" rel="noreferrer">CIO Leaders article</a> claims the interest rate was "SOFR + 100 basis points".  The <a href="https://www.newyorkfed.org/markets/reference-rates/sofr" rel="noreferrer">overnight SOFR rate</a> is about 4.1% in October, so this puts OpenAI's interest at about 5%.</p><h2 id="from-market-data">From market data</h2><p>The problem with the above expected value calculation is that it's very idealized.  The shape of it is correct, but the real world is too messy to be accurately represented by just a couple of parameters.  I think it would be very difficult to build a model with enough predictive accuracy to be useful and I suspect there just isn't enough publicly available data to plug into it to make it work.</p><p>Luckily for us, banks exist!  We know the banks have the better model and the non-public data and we know they came up with about 5% interest.  So, let's work back from that and see what we can learn.</p><p>We're talking about a loan here and that's very similar to issuing bonds.  So, we should be able to look at the bond market and find companies in similar financial health (from the perspective of a creditor).  One problem is that we only know the overnight rate for OpenAI of about 5%, but bonds on the market will have longer maturities.  We need to calculate what what yield a longer maturity loan would require and we can do that by looking at US treasuries.</p><p>According to <a href="https://www.bloomberg.com/markets/rates-bonds/government-bonds/us" rel="noreferrer">Bloomberg</a>, the three month treasuries have a yield of 3.94%.  One year ones have a yield of 3.58%.</p><figure><img src="https://marketunpack.com/content/images/2025/10/image.png" alt="" loading="lazy" width="856" height="436" srcset="https://marketunpack.com/content/images/size/w600/2025/10/image.png 600w, https://marketunpack.com/content/images/2025/10/image.png 856w" sizes="(min-width: 720px) 720px"><figcaption><b><strong>Figure 1.</strong></b><span> Treasury Yields for US government bonds. The table shows the following yields: 3 months is 3.94%, 6 month is 3.81%, 12 month is 3.58%, 2 year is 3.50%, 5 year is 3.62%, 10 year is 4.03%, 30 year is 4.62%. This describes a smile that initially goes down, goes back up to starting level at 10 years, then continues upwards.</span></figcaption></figure><p>One way of thinking about corporate bonds is that they're basically treasury bonds plus some premium to account for the risk of default.  This <em>default spread</em> seems to be about \(5\% - 3.94\% \approx 1\%\) in OpenAI's case.  By this logic, OpenAI's one year debt would have a yield of about 4.6%.</p><p>Can we find some one year bonds with a yield of 4.6%?</p><figure><img src="https://marketunpack.com/content/images/2025/10/image-3.png" alt="" loading="lazy" width="1133" height="685" srcset="https://marketunpack.com/content/images/size/w600/2025/10/image-3.png 600w, https://marketunpack.com/content/images/size/w1000/2025/10/image-3.png 1000w, https://marketunpack.com/content/images/2025/10/image-3.png 1133w" sizes="(min-width: 720px) 720px"><figcaption><b><strong>Figure 2.</strong></b><span> A sample of corporate USD bonds expiring in one year or less, sorted by their mid-yield to maturity. (Source: Saxo Bank)</span></figcaption></figure><p>Some bonds in the vicinity of what we're looking for are:</p><ul><li>4.99%: HCA Inc. (US healthcare provider with credit rating BBB),</li><li>4.73%: Ziraat Katilim (Turkish bank with credit rating B+), and</li><li>4.24%: Citigroup (US bank with credit rating A).</li></ul><p>In fact, scanning the sample above, it's mostly banks with BBB and A ratings.  So, the consortium of big banks seems to have lent money to OpenAI at the kind of rates they themselves are borrowing at.</p><p>Looking at just a few bonds is interesting, but anecdotal.  It would be better if we had some statistics across the whole bond market.  Helpfully, Prof Damodaran goes through the exercise of calculating just <a href="https://pages.stern.nyu.edu/~adamodar/New_Home_Page/datafile/ratings.html" rel="noreferrer">such statistics</a> (<a href="https://web.archive.org/web/20250827084311/https://pages.stern.nyu.edu/~adamodar/New_Home_Page/datafile/ratings.html" rel="noreferrer">archive link</a>) every year, most recently this January.</p><figure><img src="https://marketunpack.com/content/images/2025/10/image-1.png" alt="" loading="lazy" width="417" height="458"><figcaption><b><strong>Figure 3.</strong></b><span> To quote the author: "This is a table that relates the interest coverage ratio of a firm to a 'synthetic' rating and a default spread that goes with that rating. The link between interest coverage ratios and ratings was developed by looking at all rated companies in the United States. The default spreads are obtained from traded bonds. Adding that number to a riskfree rate should yield the pre-tax cost of borrowing for a firm."</span></figcaption></figure><p>Looking up OpenAI's default spread of 1% in that table, we see it's at the level we'd expect for an A- or BBB firm (same as with the anecdotal search earlier).  This normally corresponds to an interest coverage ratio of 3.00-4.24.  However, OpenAI's actual interest coverage ratio is negative because their earnings before interest are negative.</p><p>This doesn't make sense: any way we look at it, OpenAI is getting the kind of interest rates only much more established and profitable firms would be getting.  So, my initial surprise at hearing about this is justified, but there must be an explanation because the big banks wouldn't make such an obvious mistake.</p><h2 id="making-this-make-sense">Making this make sense</h2><p>OpenAI is not a profitable company.  It's also a private company, so we don't get to see audited financials, but we still know some things.  This <a href="https://www.reuters.com/technology/artificial-intelligence/openai-establishes-4-bln-credit-facility-2024-10-03/" rel="noreferrer">Reuters article</a> claims OpenAI is going to generate $3.6 billion in revenue this year, but the costs will lead to a loss of more than $5 billion.</p><p>There's also speculation that their revenue next year will jump to $11.6 billion.  However, there's no speculation about what their earnings will be because they're currently selling their services below cost and there isn't really any story as to how they'll turn this profitable.</p><p>The banks are lenders in this scenario, so they don't really care about how many users OpenAI gets or how huge their revenue becomes.  As lenders, all they care about is getting paid back and it really doesn't seem like OpenAI will have the earnings to do that.  But maybe earnings aren't what matters here.</p><p>If OpenAI can't pay its debts, it goes bankrupt and the creditors seize the company.  Importantly, they seize it from the equity holders.  Who are these equity holders?  According to <a href="https://www.digitalinformationworld.com/2025/09/who-really-owns-openai-billion-dollar.html" rel="noreferrer">this Digital Information World article</a>, the owners are Microsoft (28%), OpenAI non-profit and employees (52%), and other investors (20%).</p><p>So, the hypothetical is OpenAI runs out of money.  They have revenue, but since their costs are higher, they don't actually have anything left over.  They can't make interest payments on their debt, so they go bankrupt, and the banks seize the company from Microsoft.  I don't think Microsoft will allow this to happen.  <a href="https://www.microsoft.com/en-us/investor/earnings/fy-2024-q4/press-release-webcast" rel="noreferrer">Microsoft's earnings for last year were $88 billion</a>, so I think Microsoft will just pay off OpenAI's $4 billion debt in this scenario.  And I think the banks know all this.</p><p>So, the banks loaning money to OpenAI at an A- interest rate doesn't make sense, but effectively loaning the same to <a href="https://www.spglobal.com/ratings/en/regulatory/article/-/view/sourceId/37291" rel="noreferrer">Microsoft with its AAA rating</a> does, and that's what's actually happening here.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When a stadium adds AI to everything, it's worse experience for everyone (152 pts)]]></title>
            <link>https://a.wholelottanothing.org/bmo-stadium-in-la-added-ai-to-everything-and-what-they-got-was-a-worse-experience-for-everyone/</link>
            <guid>45648249</guid>
            <pubDate>Mon, 20 Oct 2025 19:38:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://a.wholelottanothing.org/bmo-stadium-in-la-added-ai-to-everything-and-what-they-got-was-a-worse-experience-for-everyone/">https://a.wholelottanothing.org/bmo-stadium-in-la-added-ai-to-everything-and-what-they-got-was-a-worse-experience-for-everyone/</a>, See on <a href="https://news.ycombinator.com/item?id=45648249">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>I just got back from a 24hr trip to Los Angeles to catch my favorite Portland Thorns team, watching them clinch their playoff spot in a match at BMO stadium in downtown Los Angeles.</p><p>In May of 2024, I did the same trip to catch a match on Mother's Day, but I accidentally chose bad seats in the sun and it was hot and uncomfortable. Ultimately, it partially inspired <a href="https://unofficialnwsl.stadium.guide/the-book-is-out/?ref=a.wholelottanothing.org" rel="noreferrer">my wife and I's book reviewing every NWSL soccer stadium</a> so other fans wouldn't suffer the same fate when flying across the country to catch their favorite team.</p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/IMG_2952.JPG" alt="" loading="lazy" width="2000" height="1541" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/IMG_2952.JPG 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/IMG_2952.JPG 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/IMG_2952.JPG 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/IMG_2952.JPG 2400w" sizes="(min-width: 720px) 720px"><figcaption><span>Me and my pal Greg yesterday</span></figcaption></figure><p>This year, I got better seats in the shade and enjoyed the game. But overall? The experience of being in the stadium was worse a year later. After thinking about it on the flight home, I think the reason was the stadium's rush to automation and AI in several places. </p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/SF-20Giants-2004.webp" alt="" loading="lazy" width="2000" height="1500" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/SF-20Giants-2004.webp 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/SF-20Giants-2004.webp 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/SF-20Giants-2004.webp 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/SF-20Giants-2004.webp 2400w" sizes="(min-width: 1200px) 1200px"></figure><h2 id="spoiler-alert-deploying-cameraai-recognition-for-everything-isnt-great">Spoiler alert: deploying camera/AI recognition for everything isn't great</h2><p>Every concession stand, including the ones that didn't even serve hot food, used the apparatus in the photo above to control all checkouts. I assume these are expensive units, because most places that used to have several checkout lanes only had one of them, requiring everyone to checkout through a single location.</p><p>Here's how they worked in the stadium yesterday: You place all your items on the white shelf with some space between them. Although they were clearly designed to be a self-checkout experience, the stadium had a staff member rearrange your items, then for about 30 seconds the kiosk would be thinking. After, it would pop up all items on the menu, and the staff member would have to tap to confirm what each item was. Then another 30 seconds to calculate and move the purchase to a point of sale/tap on the side, then you'd pay. </p><p>Overall, this added at least one, if not two full minutes to every transaction that didn't normally have those delays. Lines were unbearably long, and it was a hot day in LA yesterday, at 87ºF/30ºC. I bought food and drinks several times over the the course of the day and had to endure the process multiple times.</p><h2 id="when-you-add-object-recognition-youre-incentivized-to-reduce-choices">When you add object recognition, you're incentivized to reduce choices</h2><p>Here's an unintended consequence of moving all your concession stand checkouts to computer vision: it's easier if you have less things on offer.</p><p>Case in point: Let's talk about my favorite concession stand at BMO last year, a place that served rotisserie chicken with waffle fries and chicken sandwiches. Here's our meal from 2024, it was well-seasoned, came with great sauces, and was one of the best meals I had at a stadium in my entire nationwide tour, which is why I remembered it.</p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/IMG_5563.JPG" alt="" loading="lazy" width="2000" height="1974" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/IMG_5563.JPG 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/IMG_5563.JPG 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/IMG_5563.JPG 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/IMG_5563.JPG 2400w" sizes="(min-width: 720px) 720px"></figure><p>I returned to the same concession stand yesterday and here's their new menu:</p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/IMG_2925.JPG" alt="" loading="lazy" width="2000" height="1380" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/IMG_2925.JPG 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/IMG_2925.JPG 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/IMG_2925.JPG 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/IMG_2925.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>When your checkout stand relies on computer vision, it's probably confusing to have half a dozen different menu items that fans can enjoy. But if you could condense it to just chicken tenders, fries, a hot dog, and boxes of candy, your computer vision-based checkout system will probably work faster since it has to do less work with the obvious shapes of each of those items.</p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/IMG_5571.JPG" alt="" loading="lazy" width="2000" height="1983" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/IMG_5571.JPG 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/IMG_5571.JPG 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/IMG_5571.JPG 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/IMG_5571.JPG 2400w" sizes="(min-width: 720px) 720px"></figure><p>Looking through my photos from my 2024 visit, I saw a variety of food options including smashburgers and a Korean BBQ rice bowl I also tried, pictured above. </p><p>If foods are difficult for computer vision to decipher, why not get rid of most options? Walking around the stadium yesterday, the menus were basically all hot dogs, pizza, nachos, and chicken tenders.</p><h2 id="even-quick-service-options-sucked">Even quick service options sucked</h2><p>As I said, it was a hot day, I was constantly parched, and I ended up drinking four bottles of water over the course of three hours. Each time, I had to go through the automated checkout gauntlet, and each time it required a long wait in a line, while I missed bits of the match.</p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/IMG_2984.JPG" alt="" loading="lazy" width="2000" height="2667" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/IMG_2984.JPG 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/IMG_2984.JPG 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/IMG_2984.JPG 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/IMG_2984.JPG 2400w" sizes="(min-width: 720px) 720px"></figure><p>Late in the game, I wanted to get water quickly and they had these "vending kiosks" that were fully automated. You'd tap your phone on the locked door, it would unlock, you'd grab items, then close the door. Next, you had to stand there for about 2 minutes while it said "calculating checkout" before showing you a receipt on the screen.</p><p>What was supposed to be fast was very slow. The person in front of me bought two items and saw she got charged for three. Since there were no paper receipts, she took a photo of the machine before going to the guest services to complain. I missed ten minutes of the game getting water.</p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/IMG_2983.JPG" alt="" loading="lazy" width="2000" height="1265" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/IMG_2983.JPG 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/IMG_2983.JPG 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/IMG_2983.JPG 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/IMG_2983.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>This was a quick service "market" style place and last year, you'd just grab stuff off a shelf, and checkout quickly from staff at multiple registers. This year, it had a long line snaking all over because of the slow AI/camera checkout kiosks.</p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/IMG_2980.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/IMG_2980.JPG 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/IMG_2980.JPG 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/IMG_2980.JPG 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/IMG_2980.JPG 2400w"></figure><p>It was a busy game, being the last home match for the fans and I would guess there were around 17,000-18,000 people in attendance. When it's nearly 90ºF/30ºC, heat exhaustion becomes a problem for crowds. When it takes people ten minutes to buy a bottle of water (I didn't see automated water fillers at the restrooms), the embrace of slow AI/Camera-based checkout systems starts to become a health and safety issue for the crowd.</p><h2 id="but-mrs-lincoln%E2%80%94besides-the-obvious%E2%80%94how-was-the-play">But Mrs. Lincoln—besides the obvious—how was the play?</h2><p>A year later visiting the same stadium, I got worse food, slower service, and a worse overall experience. On the bright side, the billionaire stadium owners probably got to reduce their staff in the process while maybe increasing profits.</p><p>The company behind the kiosks <a href="https://blog.mashgin.com/ai-retail/mashgin-expands-footprint-at-bmo-stadium?ref=a.wholelottanothing.org" rel="noreferrer">claims they are 400% faster than human checkers and result in a 25% increase in profits</a>. After experiencing it in person yesterday, I think those numbers are bullshit. Human checkers are clearly faster and smoother, and I bet they sold more food and drinks when people could get them quickly.</p><p>And the portions? They were so small!</p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/IMG_2977.JPG" alt="" loading="lazy" width="2000" height="2354" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/IMG_2977.JPG 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/IMG_2977.JPG 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/IMG_2977.JPG 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/IMG_2977.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure>
    </div><div>
                        <h3>
                                Subscribe to get new posts in your inbox
                        </h3>
                        

                            
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude Code on the Web (519 pts)]]></title>
            <link>https://www.anthropic.com/news/claude-code-on-the-web</link>
            <guid>45647166</guid>
            <pubDate>Mon, 20 Oct 2025 18:12:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/news/claude-code-on-the-web">https://www.anthropic.com/news/claude-code-on-the-web</a>, See on <a href="https://news.ycombinator.com/item?id=45647166">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><p>Today, we're introducing Claude Code on the web, a new way to delegate coding tasks directly from your browser.</p><p>Now in beta as a research preview, you can assign multiple coding tasks to Claude that run on Anthropic-managed cloud infrastructure, perfect for tackling bug backlogs, routine fixes, or parallel development work.</p><h2 id="run-coding-tasks-in-parallel">Run coding tasks in parallel</h2><p>Claude Code on the web lets you kick off coding sessions without opening your terminal. Connect your GitHub repositories, describe what you need, and Claude handles the implementation. </p><p>Each session runs in its own isolated environment with real-time progress tracking, and you can actively steer Claude to adjust course as it’s working through tasks.</p><p>With Claude Code running in the cloud, you can now <strong>run multiple tasks in parallel</strong> across different repositories from a single interface and <strong>ship faster</strong> with automatic PR creation and clear change summaries.</p><h2 id="flexible-for-every-workflow">Flexible for every workflow</h2><p>The web interface complements your existing Claude Code workflow. Running tasks in the cloud is especially effective for:</p><ul><li>Answering questions about how projects work and how repositories are mapped</li><li>Bugfixes and routine, well-defined tasks</li><li>Backend changes, where Claude Code can use test-driven development to verify changes</li></ul><p>You can also use Claude Code on mobile. As part of this research preview, we’re making Claude Code available on our iOS app so developers can explore coding with Claude on the go. It’s an early preview, and we hope to quickly refine the mobile experience based on your feedback.<br></p><h2 id="security-first-cloud-execution">Security-first cloud execution</h2><p>Every Claude Code task runs in an isolated sandbox environment with network and filesystem restrictions. Git interactions are handled through a secure proxy service that ensures Claude can only access authorized repositories—helping keep your code and credentials protected throughout the entire workflow.</p><p>You can also add custom network configuration to choose what domains Claude Code can connect to from its sandbox. For example, you can allow Claude to download npm packages over the internet so that it can run tests and validate changes.</p><p>Read our <a href="https://www.anthropic.com/engineering/claude-code-sandboxing">engineering blog</a> and <a href="https://docs.claude.com/en/docs/claude-code/sandboxing">documentation</a> for a deep dive on Claude Code’s sandboxing approach.</p><h2 id="getting-started">Getting started</h2><p>Claude Code on the web is available now in research preview for Pro and Max users. Visit <a href="http://claude.com/code">claude.com/code</a> to connect your first repository and start delegating tasks.</p><p>Cloud-based sessions share rate limits with all other Claude Code usage. <a href="https://docs.claude.com/en/docs/claude-code/claude-code-on-the-web">Explore our documentation</a> to learn more.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Peanut allergies have plummeted in children (137 pts)]]></title>
            <link>https://www.nytimes.com/2025/10/20/well/peanut-allergy-drop.html</link>
            <guid>45647133</guid>
            <pubDate>Mon, 20 Oct 2025 18:09:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/10/20/well/peanut-allergy-drop.html">https://www.nytimes.com/2025/10/20/well/peanut-allergy-drop.html</a>, See on <a href="https://news.ycombinator.com/item?id=45647133">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/10/20/well/peanut-allergy-drop.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[x86-64 Playground – An online assembly editor and GDB-like debugger (151 pts)]]></title>
            <link>https://x64.halb.it/</link>
            <guid>45646958</guid>
            <pubDate>Mon, 20 Oct 2025 17:55:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://x64.halb.it/">https://x64.halb.it/</a>, See on <a href="https://news.ycombinator.com/item?id=45646958">Hacker News</a></p>
<div id="readability-page-1" class="page"> <header>  </header> <section>   <h2>An online assembly editor and GDB-like debugger</h2>  </section> <section> <div> <picture> <source srcset="https://x64.halb.it/_astro/app_desktop_3.Bcfcbg3r.webp" type="image/webp"> <img src="https://x64.halb.it/_astro/app_desktop_3.CISWVIUM.jpg" alt="Screenshot of the Playground web app, in the desktop layout size." width="2880" height="1800" loading="lazy" decoding="async"> </picture> </div> <span></span> </section> <section> <h3 id="introduction">Features</h3> <p>x86-64 Playground is a web app for experimenting and learning 
          x86-64 assembly.
</p> <p>
The Playground web app provides an online code editor where you
        can write, compile, and share assembly code for a wide
        range of popular assemblers such as GNU As, Fasm and Nasm.
</p> <p>
Unlike traditional onlide editors, this playground
          allows you to follow the execution of your program step by step,
        inspecting memory and registers of the running process from a GDB-like interface.
</p> <p>
You can bring your own programs!
        Drag and drop into the app any x86-64-Linux static executable to run and 
        debug it in the same sandboxed environment, without having to install anything.
</p> </section> <section> <h3>Who is this for?</h3> <p>The app is for anyone that wants to run amd64 assembly snippets or
          inspect the inner workings of simple Linux ELF files.</p> <p>It has been designed with the academic world of binary exploitation in mind;
            The debugger interface offers visualizations similar to the GDB+PwnGDB debugger plugin,
            and all the controls are labelled with the respective GDB commands.
</p> <p>Combined with <a href="https://godbolt.org/">Compiler Explorer</a>, this app provides
            a noise-free environment to learn the basics behind the inner workings of a Linux process.
            When you are ready, it includes the guides and resources necessary to keep experimenting
            on your own linux environment, with the actual GDB debugger.
</p> </section> <div> <div> <h3>Designed for the web </h3> <p>Have you ever seen a responsive debugger?
        The app places the mobile experience at the center of its design,
        and can be embedded in any web page
        to add interactivity to technical tutorials or documentations.
</p> <p>
Follow the guide to <a href="https://x64.halb.it/"> embed in your website </a> both the asm editor and debugger.
</p> </div> <p><img src="https://x64.halb.it/_astro/app_mobile_art.nN-X8L8r_1Q1Cl8.webp" alt="Screenshot of the Playground web app, showing the layout on mobile devices." width="581" height="815" loading="lazy" decoding="async"> </p> </div> <section> <h3>Offline-first and open-source</h3> <p>
The app is open-source, and <a href="https://github.com/robalb/x86-64-playground">available on Github</a>.
  It's powered by the <a href="https://github.com/jart/blink/">Blink Emulator</a>,
  which emulates an x86-64-Linux environment entirely client side in your browser.
  This means that all the code you write, or the excutables you debug are never sent to the server.
</p> <p>
everything runs in your browser, and once the Web App loads it will work 
  without an internet connection.
</p> </section>     </div>]]></description>
        </item>
        <item>
            <title><![CDATA[TernFS – an exabyte scale, multi-region distributed filesystem (123 pts)]]></title>
            <link>https://www.xtxmarkets.com/tech/2025-ternfs/#posix-shaped</link>
            <guid>45646691</guid>
            <pubDate>Mon, 20 Oct 2025 17:36:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.xtxmarkets.com/tech/2025-ternfs/#posix-shaped">https://www.xtxmarkets.com/tech/2025-ternfs/#posix-shaped</a>, See on <a href="https://news.ycombinator.com/item?id=45646691">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent">
    
<p><strong>September 2025</strong></p>
<p>XTX is an algorithmic trading firm: it builds statistical models that produce price forecasts for over 50,000 financial instruments worldwide. We use those forecasts to make trades. As XTX's research efforts to build better models ramped up, the demand for resources kept increasing.</p>
<p>The firm started out with a couple of desktops and an NFS server, and 10 years later ended up with tens of thousands of high-end GPUs, hundreds of thousands of CPUs, and hundreds of petabytes of storage.</p>
<p>As compute grew, storage struggled to keep up. We rapidly outgrew NFS first and existing open-source and commercial filesystems later. After evaluating a variety of third-party solutions, we made the decision to implement our own filesystem, which we called TernFS<sup><a href="#f1">[1]</a></sup>.</p>

<p>We have decided to open source our efforts: TernFS is <a href="https://github.com/XTXMarkets/ternfs">available as free software on our public GitHub.</a> This post <a href="#another-filesystem">motivates TernFS</a>, explains its <a href="#high-level">high-level architecture</a>, and then explores some <a href="#important-details">key implementation details</a>. If you just want to spin up a local TernFS cluster, head to the <a href="https://github.com/XTXMarkets/ternfs?tab=readme-ov-file#playing-with-a-local-ternfs-instance">README</a>.</p>

<h2>Another filesystem?</h2>
<p>There's a reason why every major tech company has developed its own distributed filesystem — they're crucial to running large-scale compute efforts, and liable to cause intense disruption if they malfunction. <sup><a href="#f2">[2]</a></sup></p>

<p>XTX was in the same position, so we designed TernFS to be a one-stop solution for most of our storage needs, going from relatively 'cold' storage of raw market data to short-lived random-access data used to communicate between GPU jobs running on our cluster.</p>
<p>TernFS:</p>
<ul>
<li>Is designed to scale up to tens of exabytes, trillions of files, millions of concurrent clients.</li>
<li>Stores file contents redundantly to protect against drive failures.</li>
<li>Has no single point of failure in its metadata services.</li>
<li>Supports file snapshot to protect against accidental file deletion.</li>
<li>Can span across multiple regions.</li>
<li>Is hardware agnostic and uses TCP/IP to communicate.</li>
<li>Utilizes different types of storage (such as flash vs. hard disks) cost effectively.</li>
<li>Exposes read/write access through its own API over TCP and UDP, and a Linux kernel filesystem module.</li>
<li>Requires no external service and has a minimal set of build dependencies. <sup><a href="#f3">[3]</a></sup></li>
</ul>

<p>Naturally, there are some limitations, the main ones being:</p>
<ul>
<li>Files are immutable — once they're written they can't be modified.</li>
<li>TernFS should not be used for tiny files — our median file size is 2MB.</li>
<li>The throughput of directory creation and removal is significantly constrained compared to other operations.</li>
<li>TernFS is permissionless, deferring that responsibility to other services.</li>
</ul>
<p>We started designing TernFS in early 2022 and began putting it into production in summer 2023. By mid-2024 all of our machine learning efforts were driven out of TernFS, and we're migrating the rest of the firm's storage needs onto it as well.</p>
<p>As of September 2025, our TernFS deployment stores more than 500PB across 30,000 disks, 10,000 flash drives, and three data centres. At peak we serve multiple terabytes per second. To this day, we haven't lost a single byte.</p>

<h2>High-level overview</h2>
<p>Now that the stage is set, we're ready to explain the various components that make up TernFS. TernFS' core API is implemented by four services:</p>
<ul>
<li><em>Metadata shards</em> store the directory structure and file metadata.</li>
<li>The <em>cross-directory coordinator</em> (or CDC) executes cross-shard transactions.</li>
<li><em>Block services</em> store file contents.</li>
<li>The <em>registry</em> stores information about all the other services and monitors them.</li>
</ul>
<pre><code>
 A ──► B means "A sends requests to B" 
                                       
                                       
 ┌────────────────┐                    
 │ Metadata Shard ◄─────────┐          
 └─┬────▲─────────┘         │          
   │    │                   │          
   │    │                   │          
   │ ┌──┴──┐                │          
   │ │ CDC ◄──────────┐     │          
   │ └──┬──┘          │     │          
   │    │             │ ┌───┴────┐     
   │    │             └─┤        │     
 ┌─▼────▼────┐          │ Client │     
 │ Registry  ◄──────────┤        │     
 └──────▲────┘          └─┬──────┘     
        │                 │            
        │                 │            
 ┌──────┴────────┐        │            
 │ Block Service ◄────────┘            
 └───────────────┘

</code></pre>

<p>In the next few sections, we'll describe the high-level design of each service and then give more background on <a href="#important-details">other relevant implementation details</a>.<sup><a href="#f4">[4]</a></sup></p>

<h3>Metadata</h3>
<p>To talk about metadata, we first need to explain what metadata <em>is</em> in TernFS. The short answer is: 'everything that is not file contents.' The slightly longer answer is:</p>
<ul>
<li>Directory entries, including all files and directory names.</li>
<li>File metadata including creation/modification/access time, logical file size, and so on.</li>
<li>The mapping between files and the <a href="#block-services">blocks containing their contents</a>.</li>
<li>Other ancillary data structures to facilitate maintenance operations.</li>
</ul>
<p>TernFS' metadata is split into 256 logical <em>shards</em>. Shards never communicate with each other. This is a general principle in TernFS: each service is disaggregated from the others, deferring to the clients to communicate with each service directly.<sup><a href="#f5">[5]</a></sup></p>

<p>A logical shard is further split into five physical instances, one leader and four followers, in a typical distributed consensus setup. The distributed consensus engine is provided by a purpose-built Raft-like implementation, which we call LogsDB, while RocksDB is used to implement read/write capabilities within a shard instance.</p>
<p>Currently all reads and writes go through the leader, but it would be trivial to allow clients to read from followers, and with a bit more effort to switch to a write-write setup.</p>
<pre><code>    ┌─────────┐ ┌─────────┐       ┌───────────┐ 
    │ Shard 0 │ │ Shard 1 │  ...  │ Shard 255 │ 
    └─────────┘ │         │       └───────────┘ 
            ┌───┘         └───────────────────┐ 
            │                                 │ 
            │                  ┌────────────┐ │ 
            │ ┌───────────┐    │ Replica 0  │ │ 
            │ │           ◄────► (follower) │ │ 
 ┌────────┐ │ │ Replica 3 ◄──┐ └────────────┘ │ 
 │ Client ├─┼─► (leader)  ◄─┐│ ┌────────────┐ │ 
 └────────┘ │ │           ◄┐│└─► Replica 1  │ │ 
            │ └───────────┘││  │ (follower) │ │ 
            │              ││  └────────────┘ │ 
            │              ││  ┌────────────┐ │ 
            │              │└──► Replica 2  │ │ 
            │              │   │ (follower) │ │ 
            │              │   └────────────┘ │ 
            │              │   ┌────────────┐ │ 
            │              └───► Replica 4  │ │ 
            │                  │ (follower) │ │ 
            │                  └────────────┘ │ 
            └─────────────────────────────────┘ 
</code></pre>

<p>Splitting the metadata into 256 shards from the get-go simplifies the design, given that horizontal scaling of metadata requires no rebalancing, just the addition of more metadata servers.</p>
<p>For instance, our current deployment can serve hundreds of petabytes and more than 100,000 compute nodes with just 10 metadata servers per data centre, with each server housing roughly 25 shard leaders and 100 shard followers.</p>
<p>Given that the metadata servers are totally decoupled from one another, this means that we can scale metadata performance by 25× trivially, and by 100× if we were to start offloading metadata requests to followers.</p>
<p>TernFS shards metadata by assigning each directory to a single shard. This is done in a simple round-robin fashion by the <a href="#cdc">cross-directory coordinator</a>. Once a directory is created, all its directory entries and the files in it are housed in the same shard.</p>
<p>This design decision has downsides: TernFS assumes that the load will be spread across the 256 logical shards naturally. This is not a problem in large deployments, given that they will contain many directories, but it is something to keep in mind.<sup><a href="#f6">[6]</a></sup></p>


<h3>Cross-directory transactions</h3>
<p>Most of the metadata activity is contained within a single shard:</p>
<ul>
<li>File creation, same-directory renames, and deletion.</li>
<li>Listing directory contents.</li>
<li>Getting attributes of files or directories.</li>
</ul>
<p>However, some operations do require coordination between shards, namely directory creation, directory removal, and moving directory entries across different directories.</p>
<p>The <em>cross-directory coordinator</em> (CDC) performs these distributed transactions using a privileged metadata shard API. The CDC transactions are stateful, and therefore the CDC uses RocksDB and LogsDB much like the metadata shards themselves to persist its state safely.</p>
<pre><code> ┌────────┐    ┌──────────┐ ┌───────────┐ 
 │ Client ├─┐  │ Shard 32 │ │ Shard 103 │ 
 └────────┘ │  └────────▲─┘ └─▲─────────┘ 
 ┌─────┬────┼───────────┼─────┼─┐         
 │ CDC │  ┌─▼──────┐    │     │ │         
 ├─────┘  │ Leader ├────┴─────┘ │         
 │        └─────▲──┘            │         
 │              │               │         
 │       ┌──────┴───────┐       │         
 │       │              │       │         
 │ ┌─────▼────┐    ┌────▼─────┐ │         
 │ │ Follower │ .. │ Follower │ │         
 │ └──────────┘    └──────────┘ │         
 └──────────────────────────────┘   
</code></pre>

<p>The CDC executes transactions in parallel, which increases throughput considerably, but it is still a bottleneck when it comes to creating, removing, or moving directories. This means that TernFS has a relatively low throughput when it comes to CDC operations.<sup><a href="#f7">[7]</a></sup> <a name="block-services"></a></p>

<h3>Block services, or file contents</h3>
<p>In TernFS, files are split into chunks of data called <em>blocks</em>. Blocks are read and written to by <em>block services</em>. A block service is typically a single drive (be it a hard disk or a flash drive) storing blocks. At XTX a typical storage server will contain around 100 hard disks or 25 flash drives — or in TernFS parlance 100 or 25 block services.<sup><a href="#f8">[8]</a></sup></p>

<p>Read/write access to the block service is provided using a simple TCP API currently implemented by a Go process. This process is hardware agnostic and uses the Go standard library to read and write blocks to a conventional local file system. We originally planned to rewrite the Go process in C++, and possibly write to block devices directly, but the idiomatic Go implementation has proven performant enough for our needs so far. <a name="registry"></a></p>
<h3>The registry</h3>
<p>The final piece of the TernFS puzzle is the <em>registry</em>. The registry stores the location of each instance of service (be it a metadata shard, the CDC, or a block storage node). A client only needs to know the address of the registry to mount TernFS — it'll then gather the locations of the other services from it.</p>
<p>In TernFS all locations are IPv4 addresses. Working with IPv4 directly simplifies the kernel module considerably, since DNS lookups are quite awkward in the Linux kernel. The exception to this rule is addressing the registry itself, for which DNS is used.</p>
<p>The registry also stores additional information, such as the capacity and available size of each drive, who is a follower or a leader in LogsDB clusters, and so on.</p>
<p>Predictably, the registry itself is a RocksDB and LogsDB C++ process, given its statefulness. <a name="going-global"></a></p>
<h3>Going global</h3>
<p>TernFS tries very hard not to lose data, by storing both metadata and file contents on many different drives and servers. However, we also want to be resilient to the temporary or even permanent loss of one entire data centre. Therefore, TernFS can transparently scale across multiple <em>locations</em>.</p>
<p>The intended use for TernFS locations is for each location to converge to the same dataset. This means that each location will have to be provisioned with roughly equal resources.<sup><a href="#f9">[9]</a></sup> Both metadata and file contents replication are asynchronous. In general, we judge the event of losing an entire data centre rare enough to tolerate a time window where data is not fully replicated across locations.</p>

<p>Metadata replication is set up so that one location is the metadata primary. Write operations in non-primary locations pay a latency price since they are acknowledged only after they are written to the primary location, replicated, and applied in the originating location. In practice this hasn't been an issue since metadata write latencies are generally overshadowed by writing file contents.</p>
<p>There is no automated procedure to migrate off a metadata primary location — again, we deem it a rare enough occurrence to tolerate manual intervention. In the future we plan to move from the current protocol to a multi-master protocol where each location can commit writes independently, which would reduce write latencies on secondary locations and remove the privileged status of the primary location.</p>
<p>File contents, unlike metadata, are written locally to the location the client is writing from. Replication to other locations happens in two ways: proactively and on-demand. Proactive replication is performed by tailing the metadata log and replicating new file contents. On-demand replication happens when a client requests file content which has not been replicated yet. <a name="important-details"></a> <a name="speaking-ternfs"></a></p>
<h2>Important Details</h2>
<p>Now that we've laid down the high-level design of TernFS, we can talk about several key implementation details that make TernFS safer, more performant, and more flexible.</p>
<h3>Talking to TernFS</h3>
<h4>Speaking TernFS' language</h4>
<p>The most direct way to talk to TernFS is by using its own API. All TernFS messages are defined using a custom serialization format we call <em>bincode</em>. We chose to develop a custom serialization format since we needed it to work within the confines of the <a href="#posix-shaped">Linux kernel</a> and to be easily chopped into UDP packets.</p>
<p>We intentionally kept the TernFS API stateless, in the sense that each request executes without regard to previous requests made by the same client. This is in contrast to protocols like NFS, whereby each connection is very stateful, holding resources such as open files, locks, and so on.</p>
<p>A stateless API dramatically simplifies the state machines that make up the TernFS core services, therefore simplifying their testing. It also forces each request to be idempotent, or in any case have clear retry semantics, since they might have to be replayed, which facilitates testing further.</p>
<p>It also allows the metadata shards and CDC API to be based on UDP rather than TCP, which makes the server and clients (especially the kernel module) simpler, due to doing away with the need for keeping TCP connections. The block service API is TCP based, since it is used to stream large amounts of contiguous data, and any UDP implementation would have to re-implement a reliable stream protocol. The registry API is also TCP-based, given that it is rarely used by clients, and occasionally needs to return large amounts of data.</p>
<p>While the TernFS API is simple out-of-the-box, we provide a permissively licensed Go library implementing common tasks that clients might want to perform, such as caching directory policies and retrying requests. This library is used to implement many TernFS processes that are not part of the core TernFS services, such as <a href="#scrubbing">scrubbing</a>, <a href="#snapshots">garbage collection</a>, <a href="#migrations">migrations</a>, and the <a href="#web-ui">web UI</a>.</p>

<h4>Making TernFS POSIX-shaped</h4>
<p>While the Go library is used for most ancillary tasks, some with high performance requirements, the main way to access TernFS at XTX is through its Linux kernel module.</p>
<p>This is because, when migrating our machine learning workflows to TernFS, we needed to support a vast codebase working with files directly. This not only meant that we needed to expose TernFS as a normal filesystem, but also that said normal filesystem API needed to be robust and performant enough for our machine learning needs.<sup><a href="#f10">[10]</a></sup></p>

<p>For this reason, we opted to work with Linux directly, rather than using FUSE. Working directly with the Linux kernel not only gave us the confidence that we could achieve our performance requirements but also allowed us to bend the POSIX API to our needs, something that would have been more difficult if we had used FUSE.<sup><a href="#f11">[11]</a></sup></p>

<p>The main obstacle when exposing TernFS as a 'normal' filesystem is that TernFS files are immutable. More specifically, TernFS files are fully written before being 'linked' into the filesystem as a directory entry. This is intentional: it lets us cleanly separate the API for 'under construction' files and 'completed files', and it means that half-written files are not visible.</p>
<p>However this design is essentially incompatible with POSIX, which endows the user with near-absolute freedom when it comes to manipulating a file. Therefore, the TernFS kernel module is <em>not</em> POSIX-compliant, but rather exposes enough POSIX to allow many programs to work without modifications, but not all.</p>
<p>In practice this means that programs which write files left-to-right and never modify the files' contents will work out-of-the-box. While this might seem very restrictive, we found that a surprising number of programs worked just fine.<sup><a href="#f12">[12]</a></sup> Programs that did not follow this pattern were modified to first write to a temporary file and then copy the finished file to TernFS.</p>

<p>While we feel that writing our own kernel module was the right approach, it proved to be the trickiest part of TernFS, and we would not have been able to implement it without <a href="#block-proofs">some important safety checks</a> in the TernFS core services.<sup><a href="#f13">[13]</a></sup></p>

<h4>S3 gateway</h4>
<p>Almost all the storage-related activity at XTX is due to our machine-learning efforts, and for those purposes the TernFS' kernel module has served us well. However, as TernFS proved itself there, we started to look into offering TernFS to the broader firm.</p>
<p>Doing so through the kernel module presented multiple challenges. For starters installing a custom kernel module on every machine that needed to reach TernFS is operationally cumbersome. Moreover, while all machine-learning happens in clusters housed in the same data centre as TernFS itself, we wanted to expose TernFS in a way that's more amenable to less local networks, for instance by removing the need for UDP. Finally, TernFS does not have any built-in support for permissions or authentication, which is a requirement in multi-tenant scenarios.</p>
<p>To solve all these problems, we implemented a gateway for TernFS, which exposes a TernFS subtree using the S3 API. The gateway is a simple Go process turning S3 calls into TernFS API calls. The S3 gateway is not currently open sourced since it is coupled to authentication services internal to XTX, but we have open sourced a minimal S3 gateway to serve as a starting point for third-party contributors to build their own.</p>
<p>We've also planned an NFS gateway to TernFS, but we haven't had a pressing enough need yet to complete it.</p>

<h4>The web UI and the JSON interface</h4>
<p>Finally, a view of TernFS is provided by its web UI. The web UI is a stateless Go program which exposes most of the state of TernFS in an easy-to-use interface. This state includes the full filesystem contents (both metadata and file contents), the status of each service including information about decommissioned block services, and so on.</p>
<p>Moreover, the web UI also exposes the <a href="#speaking-ternfs">direct TernFS API</a> in JSON form, which is very useful for small scripts and curl-style automation that does not warrant a full-blown Go program.</p>

<h3>Directory Policies</h3>
<p>To implement some of the functionality we'll describe below, TernFS adopts a system of per-directory policies.</p>
<p>Policies are used for all sorts of decisions, including:</p>
<ul>
<li><a href="#reed-solomon">How to redundantly store files.</a></li>
<li><a href="#drive-type-picking">On which type of drive to store files.</a></li>
<li><a href="#snapshots">How long to keep files around after deletion.</a></li>
</ul>
<p>Each of the topics above (and a few more we haven't mentioned) correspond to a certain policy <em>tag</em>. The body of the policies are stored in the metadata together with the other directory attributes.</p>
<p>Policies are inherited: if a directory does not contain a certain policy tag, it transitively inherits from the parent directory. TernFS clients store a cache of policies to allow for traversal-free policy lookup for most directories.</p>
<h3>Keeping blocks in check</h3>
<p>A filesystem is no good if it loses, leaks, corrupts, or otherwise messes up its data. TernFS deploys a host of measures to minimize the chance of anything going wrong. So far, these have worked: we've never lost data in our production deployment of TernFS. This section focuses on the measures in place to specifically safeguard files' blocks.</p>
<h4>Against bitrot, or CRC32-C</h4>
<p>The first and possibly most obvious measure consists of aggressively checksumming all TernFS' data. The metadata is automatically checksummed by RocksDB, and every block is stored in a format interleaving 4KiB pages with 4byte CRC32-C checksums.</p>
<p>CRC32-C was picked since it is a high-quality checksum and implemented on most modern silicon.<sup><a href="#f14">[14]</a></sup> It also exhibits some desirable properties when used together with <a href="#block-proofs">Reed-Solomon coding</a>.</p>

<p>4KiB was picked since it is the read boundary used by Linux filesystems and is fine-grained while still being large enough to render the storage overhead of the 4byte checksums negligible.</p>
<p>Interleaving the CRCs with the block contents does not add any safety, but it does improve operations in two important ways. First, it allows for safe partial reads: clients can demand only a few pages from a block which is many megabytes in size and still check the reads against its checksum. Second, it allows <a href="#scrubbing">scrubbing</a> files locally on the server which hosts the blocks, without communicating with other services at all.</p>

<h4>Storing files redundantly, or Reed-Solomon codes</h4>
<p>We've been talking about files being split into blocks, but we haven't really explained <em>how</em> files become blocks.</p>
<p>The first thing we do to a file is split it into <em>spans</em>. Spans are at most 100MiB and are present just to divide files into sections of a manageable size.</p>
<p>Then each span is divided into D <em>data blocks</em>, and P <em>parity blocks</em>. D and P are determined by the corresponding <a href="#directory-policies">directory policy</a> in which the file is created. When D is 1, the entire contents of the span become a single block, and that block is stored D+P times. This scheme is equivalent to a simple mirroring scheme and allows it to lose up to P blocks before losing file data.</p>
<p>While wasteful, mirroring the entire contents of the file can be useful for very hot files, since TernFS clients will pick a block at random to read from, thereby sharing the read load across many block services. And naturally files which we do not care much for can be stored with D = 1 and P = 0, without any redundancy.</p>
<p>That said, most files will not be stored using mirroring but rather using Reed-Solomon coding. Other resources can be consulted to understand the <a href="https://mazzo.li/posts/reed-solomon.html">high-level idea</a> and the <a href="https://www.corsix.org/content/reed-solomon-for-software-raid">low-level details</a> of Reed-Solomon coding, but the gist is it allows us to split a span into D equally sized blocks (some padding might be necessary), and then generate P blocks of equal size such that up to any P blocks can be lost while retaining the ability to reconstruct all the other blocks.</p>
<p>As mentioned, D and P are fully configurable, but at XTX we tend to use D = 10 and P = 4, which allows us to lose up to any four drives for any file.</p>

<h4>Drive type picking</h4>
<p>We now know how to split files into a bunch of blocks. The next question is: which drives to pick to store the blocks on. The first decision is which kind of drive to use. At XTX we separate drives into two broad categories for this purpose — flash and spinning disks.</p>
<p>When picking between these two, we want to balance two needs: minimizing the cost of hardware by utilizing hard disks if we can <sup><a href="#f15">[15]</a></sup>, and maximizing hard disk productivity by having them reading data most of the time, rather than seeking.</p>

<p>To achieve that, directory policies offer a way to tune how large each block will be, and to tune which drives will be picked based on block size. This allows us to configure TernFS so that larger files that can be read sequentially are stored on hard disks, while random-access or small files are stored on flash. <sup><a href="#f16">[16]</a></sup></p>

<p>Currently this system is not adaptive, but we found that in practice it's easy to carve out sections of the filesystem which are not read sequentially. We have a default configuration which assumes sequential reads and then uses hard disks down to roughly 2.5MB blocks, below which hard disks stop being productive enough and blocks start needing to be written to flash. <a name="block-service-picking"></a></p>
<h4>Block service picking</h4>
<p>OK, we now know what type of drive to select for our files, but we still have tens of thousands of individual drives to pick from. Picking the 'right' individual drive requires some sophistication.</p>
<p>The first thing to note is that drive failures or unavailability are often correlated. For instance, at XTX a single server handles 102 spinning disks. If the server is down, faulty, or needs to be decommissioned, it'll render its 102 disks temporarily or permanently unavailable.</p>
<p>It's therefore wise to spread a file's blocks across many servers. To achieve this, each TernFS block service (which generally corresponds to a single drive) has a <em>failure domain</em>. When picking block services in which to store the blocks for a given file, TernFS will make sure that each block is in a separate failure domain. In our TernFS deployment a failure domain corresponds to a server, but other users might wish to tie it to some other factor as appropriate.</p>
<p>TernFS also tries hard to avoid write bottlenecks by spreading the current write load across many disks. Moreover, since new drives can be added at any time, it tries to converge to a situation where each drive is roughly equally filled by assigning writing more to drives with more available space.</p>
<p>Mechanically this is achieved by having each shard periodically request a set of block services to use for writing from the registry. When handing out block services to shards, the registry selects block services according to several constraints:</p>
<ul>
<li>It never gives block services from the same failure domain to the same shard</li>
<li>It minimizes the variance in how many shards each block service is currently assigned to</li>
<li>It prioritizes block services which have more available space.</li>
</ul>
<p>Then when a client wants to write a new span, requiring D+P blocks, the shard simply selects D+P block services randomly amongst the ones it last received from the registry.</p>
<p>One concept currently absent from TernFS is what is often known as 'copyset replication'. When assigning disks to files at random (even with the caveat of failure domains) the probability of rendering at least one file unreadable quickly becomes a certainty as more and more drives fail:</p>
<p><img src="https://www.xtxmarkets.com/assets/tech/2025-ternfs-faileddisks.png" alt="Probability of data loss vs Failed disks" title="Probability of data loss vs Failed disks"></p>

<p>Copysets reduce the likelihood of data loss occurring by choosing blocks out of a limited number of sets of drives, as opposed to picking the drives randomly. This dramatically reduces the probability of data loss<sup><a href="#f17">[17]</a></sup>.  They are generally a good idea, but we haven't found them to be worthwhile, for a few reasons.</p>

<p>First, evacuating a 20TB drive takes just a few minutes, and in the presence of multiple failed drives the migrator process evacuates first the files which are present in multiple failed drives to get ahead of possible data loss. This means that for TernFS to lose data within a single data centre tens of drives would have to fail within a matter of seconds.</p>
<p>More importantly, our TernFS deployment is replicated across three data centres. This replication eliminates the chance of losing data due to 'independent' drive failures — thousands of drives would need to fail at once. Obviously, data centre wide events <em>can</em> cause a large proportion of the drives within it to fail, but having such an event in three data centres at once is exceedingly unlikely.</p>
<p>Finally, copysets are not without drawbacks or complications. Assigning drives at random is an optimal strategy when it comes to evacuating drives quickly, since the files with blocks in the drives to be evacuated will be evenly spread over the rest of the filesystem, and since we only ever need to replace the failed blocks given that we're not constrained by fitting the new set of blocks in predetermined copysets. This means that the evacuation procedure will not be bottlenecked by drive throughput, which is what enables evacuation to finish in a matter of minutes. Moreover, the algorithm to distribute drives to shards is significantly simpler and more flexible than if it needed to care about copysets.</p>
<p>However, users that wish to deploy TernFS within a single data centre might wish to implement some form of copyset replication. Such a change would be entirely contained to the registry and would not change any other component.</p>

<h4>Block Proofs</h4>
<p>We now have a solid scheme to store files redundantly (thanks to Reed-Solomon codes) and protect against bitrot (thanks to the checksums). However, said schemes are only as good as their implementation.</p>
<p>As previously mentioned, TernFS clients communicate their intention to write a file to metadata servers, the metadata servers select block services that the blocks should be written to, and the clients then write the blocks to block services independently of the metadata services. The same happens when a client wants to erase blocks: the client first communicates its intentions to delete the blocks to the right metadata shard and then performs the erasing itself.</p>
<p>This poses a challenge. While verifying the correctness of the core TernFS services is feasible, verifying all clients is not, but we'd still like to prevent buggy clients from breaking key invariants of the filesystem.</p>
<p>Buggy clients can wreak havoc in several ways:</p>
<ul>
<li>They can <em>leak data</em> by writing blocks to block services that are not referenced anywhere in the metadata.</li>
<li>They can <em>lose data</em> by erasing blocks which are still referenced in metadata.</li>
<li>They can <em>corrupt data</em> by telling the metadata services they'll write something and then writing something else.</li>
</ul>
<p>We address all these points by using what we call <em>block proofs</em>. To illustrate how block proofs work, it's helpful to go through the steps required to write new data to a file.</p>
<ol>
<li>When a client is creating a file, it'll do so by adding its <a href="#reed-solomon">file spans</a> one-by-one. For each span the client wants to add it sends an 'initiate span creation' request to the right metadata shard. This request contains both the overall checksum of the span, and the checksum of each block in it (including parity blocks).</li>
<li>The metadata shard checks the consistency of the checksum of the span and of its blocks, something it can do thanks to <a href="https://mazzo.li/posts/rs-crc.html">some desirable mathematical properties</a> of CRCs.</li>
<li>The shard picks block services for the blocks to be written in and returns this information to the client together with a signature for each 'block write' instruction.</li>
<li>The client forwards this signature to the block services, which will refuse to write the block without it. Crucially, the cryptographic signature ranges over a unique identity for the block (ensuring we only write the block we mean to write), together with its checksum, ensuring we don't write the wrong data.<sup><a href="#f18">[18]</a></sup></li>
<li>After committing the block to disk, the block service returns a 'block written' signature to the client.</li>
<li>Finally, the client forwards the block written signature back to the shard, which certifies that the span has been written only when it has received the signatures for all the blocks that make up the span. <sup><a href="#f19">[19]</a></sup></li>
</ol>

<p>Similarly, when a client wants to delete a span, it first asks the metadata shard to start doing so. The metadata shard marks the span as 'in deletion' and returns a bunch of 'block erase' signatures to the client. The client then forwards the signatures to the block services that hold the blocks, which delete the blocks, and return a 'block erased' signature. The clients forward these signatures back to the metadata shards, which can then forget about the span entirely.</p>

<p>We use AES to generate the signatures for simplicity but note that the goal here is not protecting ourselves from malicious clients — just buggy ones. The keys used for the signature are not kept secret, and CRC32-C is not a secure checksum. That said, we've found this scheme enormously valuable in the presence of <a href="#posix-shaped">complex clients</a>. We spent considerable efforts making the core services very simple so we could then take more implementation risks in the clients, with the knowledge that we would have a very low chance of corrupting the filesystem itself.</p>

<h4>Scrubbing</h4>
<p>Finally, if things go wrong, we need to notice. The most common failure mode for a drive is for it to fail entirely, in which case our internal hardware monitoring system will pick it up and migrate from it automatically. The more insidious (and still very common) case is a single sector failing in a drive, which will only be noticed when we try to read the block involving that sector.</p>
<p>This is acceptable for files which are read frequently, but some files might be very 'cold' but still very important.</p>
<p>Consider the case of raw market data taps which are immediately converted to some processed, lossy format. While we generally will use the file containing the processed data, it's paramount to store the raw market data forever so that if we ever want to include more information from the original market data, we can. So important cold files might go months or even years without anyone reading them, and in the meantime, we might find that enough blocks have been corrupted to render them unreadable.<sup><a href="#f20">[20]</a></sup></p>

<p>To make sure this does not happen, a process called the <em>scrubber</em> continuously reads every block that TernFS stores, and replaces blocks with bad sectors before they can cause too much damage.</p>

<h3>Snapshots and garbage collection</h3>
<p>We've talked at length about what TernFS does to try to prevent data loss due to hardware failure or bugs in clients. However, the most common type of data loss is due to human error — the <code>rm —rf / home/alice/notes.txt</code> scenario.</p>
<p>To protect against these scenarios, TernFS implements a lightweight snapshotting system. When files or directories are deleted, their contents aren't actually deleted. Instead, a weak reference to them is created. We call such weak references <em>snapshot</em> directory entries.</p>
<p>Snapshot entries are not be visible through the kernel module or the S3 gateway, but are visible through <a href="#speaking-ternfs">the direct API</a>, and at XTX we have developed internal tooling to easily recover deleted files through it.<sup><a href="#f21">[21]</a></sup> Deleted files are also visible through the TernFS web UI.</p>

<p>Given that 'normal' file operations do not delete files, but rather make them a snapshot, the task of freeing up space is delegated to an external Go process, the <em>garbage collector</em>. The garbage collector traverses the filesystem and removes expired snapshots, which involves deleting their blocks permanently. Snapshot expiry is predictably regulated by <a href="#directory-policies">directory policies</a>.</p>
<h3>Keeping TernFS healthy</h3>
<p>This last section covers how we (humans of XTX) notice problems in TernFS, and how TernFS self-heals when things go wrong — both key topics if we want to ensure no data loss and notice performance problems early.</p>
<h4>Performance metrics</h4>
<p>TernFS exposes a plethora of performance metrics through the HTTP <a href="https://docs.influxdata.com/influxdb/v2/reference/syntax/line-protocol/">InfluxDB line protocol</a>. While connecting TernFS to a service which ingests these metrics is optional, it is <em>highly</em> recommended for any production service.</p>
<p>Moreover, the kernel module exposes many performance metrics itself through DebugFS.</p>
<p>Both types of metrics, especially when used in tandem, have proved invaluable to resolve performance problems quickly.</p>
<h4>Logging and alerts</h4>
<p>TernFS services log their output to files in a simple line-based format. The internal logging API is extremely simple and includes support for syslog levels out-of-the-box. At XTX we run TernFS as normal systemd services and use journalctl to view logs.</p>
<p>As with metrics, the kernel module includes various logging facilities as well. The first type of logging is just through dmesg, but the kernel module also includes numerous tracepoints for low-overhead opt-in logging of many operations.</p>
<p>TernFS is also integrated with XTX's internal alerting system, called <em>XMon</em>, to page on call developers when things go wrong. XMon is not open source, but all the alerts are also rendered as error lines in logs. <sup><a href="#f22">[22]</a></sup> We plan to eventually move to having alerts feed off performance metrics, which would make them independent from XMon, although we don't have plans to do so in the short-term. <a name="migrations"></a></p>

<h4>Migrations</h4>
<p>Finally, there's the question of what to do when drives die — and they will die, frequently, when you have 50,000 of them. While drives dying is not surprising, we've been surprised at the variety of different drive failures. <sup><a href="#f23">[23]</a></sup> A malfunctioning drive might:</p>

<ul>
<li>Produce IO errors when reading specific files. This is probably due to a single bad sector.</li>
<li>Produce IO errors when reading or writing anything. This might happen because enough bad sectors have gone bad and the drive cannot remap them, or for a variety of other reasons.</li>
<li>Return wrong data. This is usually caught by the built-in error correction codes in the hard drives, but not always.</li>
<li>Lie about data being successfully persisted. This can manifest in a variety of ways: file size being wrong on open, file contents being partially zero'd out, and so on.</li>
<li>Disappear from the mount list, only to reappear when the machine is rebooted, but missing some data.</li>
</ul>
<p>When clients fail to read from a drive, they'll automatically fall back on other drives to reconstruct the missing data, which is extremely effective in hiding failures from the end-user. That said, something needs to be done about the bad drives, and <a href="#block-service-picking">done quickly to avoid permanent data loss</a>.</p>
<p>The TernFS registry allows marking drives as faulty. Faulty drives are then picked up by the <em>migrator</em>, a Go process which waits for bad drives and then stores all its blocks onto freshly picked block services.</p>
<p>TernFS also tries to mark drives as bad automatically using a simple heuristic based on the rate of IO errors the drive is experiencing. The number of drives automatically marked as faulty is throttled to avoid having this check go awry and mark the whole cluster as faulty, which would not be catastrophic but would still be messy to deal with.</p>
<p>Moreover, drives that are faulty in subtle ways might not be picked up by the heuristics, which means that occasionally a sysadmin will need to mark a drive as faulty manually, after which the migrator will evacuate them.</p>
<h2>Closing thoughts</h2>
<p>At XTX we feel strongly about utilizing our resources efficiently. When it comes to software, this means having software that gets close to some theoretical optimum when it comes to total cost of ownership. This culture was borne out by competing hard for technological excellence when doing on-exchange trading at first, and by our ever-growing hardware costs as our business has grown later.</p>
<p>Such idealized tools might not exist or be available yet, in which case we're happy to be the tool makers. TernFS is a perfect example of this and we're excited to open source this component of our business for the community.</p>
<p>Crucially, the cost of implementation of a new solution is often overblown compared to the cost of tying yourself to an ill-fitting, expensive third-party solution. Designing and implementing a solution serving exactly your needs allows for much greater simplicity. If the requirements do change, as often happens, changes can be implemented very quickly, again only catering to your needs.</p>
<p>That said, we believe that TernFS' set of trade-offs are widely shared across many organizations dealing with large-scale storage workloads, and we hope we'll contribute to <a href="https://xkcd.com/927/">at least slowing down the seemingly constant stream of new filesystems</a>.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AWS outage shows internet users 'at mercy' of too few providers, experts say (252 pts)]]></title>
            <link>https://www.theguardian.com/technology/2025/oct/20/amazon-web-services-aws-outage-hits-dozens-websites-apps</link>
            <guid>45646649</guid>
            <pubDate>Mon, 20 Oct 2025 17:32:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/technology/2025/oct/20/amazon-web-services-aws-outage-hits-dozens-websites-apps">https://www.theguardian.com/technology/2025/oct/20/amazon-web-services-aws-outage-hits-dozens-websites-apps</a>, See on <a href="https://news.ycombinator.com/item?id=45646649">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Experts have warned of the perils of relying on a small number of companies for operating the global internet after a glitch at Amazon’s cloud computing service brought down apps and websites around the world.</p><p>The affected platforms included <a href="https://www.theguardian.com/technology/snapchat" data-link-name="in body link" data-component="auto-linked-tag">Snapchat</a>, Roblox, Signal and Duolingo as well as a host of Amazon-owned operations including its main retail site and the Ring doorbell company.</p><p>More than 2,000 companies worldwide have been affected, according to Downdetector, a site that monitors internet outages, with 8.1m reports of problems from users including 1.9m reports in the US, 1m in the UK and 418,000 in Australia.</p><p>In the UK, Lloyds bank was affected, as well as its subsidiaries Halifax and Bank of Scotland, while there were also problems accessing the HM Revenue and Customs website on Monday morning. Also in the UK, Ring users complained on social media that their doorbells were not working.</p><p>In the UK alone, reports of problems on individual apps ran into the tens of thousands for each platform. Other affected platforms around the world included Wordle, Coinbase, Duolingo, Slack, Pokémon Go, Epic Games, PlayStation Network and Peloton.</p><p>By 10.30am UK time, Amazon was reporting that the problem, which first emerged at about 8am, was being resolved as AWS was “seeing significant signs of recovery”.</p><p>However, after reporting further positive progress by late morning in the UK, Amazon still appeared to be struggling to overcome the glitch this afternoon as it acknowledged it was still experiencing elevated errors.</p><p>“We can confirm significant API errors and connectivity issues across multiple services … We are investigating,” AWS said in an update around 7am Pacific time and 3pm UK time.</p><p>To aid the recovery, AWS said it was putting in place limits on the number of requests that could be made on its platform.</p><p>Experts said the outage underlined the dangers of the internet’s reliance on a small number of tech companies, with Amazon, Microsoft and Google playing a key role in the cloud market.</p><p>Dr Corinne Cath-Speth, the head of digital at human rights organisation Article 19, said: “We urgently need diversification in cloud computing. The infrastructure underpinning democratic discourse, independent journalism and secure communications cannot be dependent on a handful of companies.”</p><p>Cori Crider, the executive director of the Future of Technology<em> </em>Institute, a thinktank that supports a sovereign technology framework for Europe, said: “The UK can’t keep leaving its critical infrastructure at the mercy of US tech giants. With Amazon Web Services down, we’ve seen the lights go out across the modern economy – from banking to communications.”</p><p>Madeline Carr, professor of global politics and cybersecurity at University College London, said it was “hard to disagree” with warnings about the over-reliance of the global internet on a small number of companies.</p><p>“The counter-argument is that it’s these large hyper-scaling companies that have the financial resources to provide a secure, global and resilient service. But most people outside those companies would argue that is a risky position for the world to be in.”</p><p>Last year, airports, healthcare services and businesses worldwide were hit by the “largest outage in history”, caused by a botched software upgrade from cybersecurity company CrowdStrike that <a href="https://www.theguardian.com/australia-news/article/2024/jul/19/microsoft-windows-pcs-outage-blue-screen-of-death" data-link-name="in body link">hit Microsoft’s Windows operating system</a>.</p><p>Amazon reported that the problem on Monday originated in the east coast of the US at Amazon Web Services, a unit that provides vital web infrastructure for a host of companies, which rent out space on Amazon servers. AWS is the world’s largest cloud computing platform.</p><p>Shortly after midnight (PDT) in the US (8am BST) on Monday, Amazon confirmed “increased error rates and latencies” for AWS services in a region on the east coast of the US. The ripple effect hit services around the world, with Downdetector reporting problems with the same sites in multiple continents.</p><p>Cisco’s Thousand Eyes, a service that <a href="https://www.thousandeyes.com/outages/" data-link-name="in body link">tracks internet outages</a>, also reported a surge in problems on Monday morning, with many of them located in Virginia, the location of Amazon’s US-East-1 region, where AWS said the problems began and where AWS has a number of datacentres.</p><p>Experts said the outage appeared to be an IT issue rather than a cyber-attack. AWS’s online health dashboard referred to DynamoDB, its database system where AWS customers store their data. Amazon appeared to rule out foul play, saying the root cause was an internal subsystem responsible for monitoring its load balancers, which prevent traffic from overloading its servers.</p><p>“The incident appears to have been caused by some accident within AWS, rather than being the result of any malicious intent,” said Steven Murdoch, a professor of security engineering at University College London.</p><p>The UK government has said it is in contact with Amazon over the outage.</p><p>A spokesperson said:<em> </em>“We are aware of an incident affecting Amazon Web Services, and several online services which rely on their infrastructure. Through our established incident response arrangements, we are in contact with the company, who are working to restore services as quickly as possible.”</p><p>The House of Commons’ treasury committee in the UK has written to the economic secretary to the Treasury, Lucy Rigby, to ask why the government had not yet designated Amazon a “critical third party” to the UK’s financial services sector – which would expose the tech firm to financial regulatory oversight.</p><p>The committee chair, Meg Hillier, pointed out that Amazon had recently told the committee that financial services customers were using AWS to support their “resilience” and that AWS offered “multiple layers of protection”.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dutch spy services have restricted intelligence-sharing with the United States (290 pts)]]></title>
            <link>https://intelnews.org/2025/10/20/01-3416/</link>
            <guid>45646572</guid>
            <pubDate>Mon, 20 Oct 2025 17:25:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://intelnews.org/2025/10/20/01-3416/">https://intelnews.org/2025/10/20/01-3416/</a>, See on <a href="https://news.ycombinator.com/item?id=45646572">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrap">

					

					

					<p><img data-attachment-id="23271" data-permalink="https://intelnews.org/2025/10/20/01-3416/first-post-h-889/" data-orig-file="https://intelnews.org/wp-content/uploads/2025/10/first-post-h-1.jpg" data-orig-size="629,291" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Mark Rutte NATO Trump" data-image-description="<p>Mark Rutte NATO Trump</p>
" data-image-caption="" data-medium-file="https://intelnews.org/wp-content/uploads/2025/10/first-post-h-1.jpg?w=300" data-large-file="https://intelnews.org/wp-content/uploads/2025/10/first-post-h-1.jpg?w=629" src="https://intelnews.org/wp-content/uploads/2025/10/first-post-h-1.jpg" alt="Mark Rutte NATO Trump" width="629" height="291">INTELLIGENCE SERVICES IN THE Netherlands have restricted intelligence-sharing with their United States counterparts due to political developments in Washington, according to two leading Dutch intelligence officials. This development—which may typify Europe’s current approach to transatlantic intelligence-sharing—was confirmed last week by the heads of the Netherlands’ two largest intelligence agencies in a joint <a title="H. MODDERKOLK &quot;Nederlandse diensten delen minder informatie met de VS - ‘Soms vertellen we dingen niet meer’&quot; De Volkskrant [18oct2025]" href="https://www.volkskrant.nl/binnenland/nederlandse-diensten-delen-minder-informatie-met-de-vs-soms-vertellen-we-dingen-niet-meer~b4882f19/">interview</a> with <em>De Volkskrant</em> newspaper.</p>
<p>The joint interview was given to <em>De Volkskrant</em> by Erik Akerboom, director of the General Intelligence and Security Service (AIVD), and Peter Reesink , director of the General Intelligence and Security Service (MIVD)—AIVD’s civilian counterpart.</p>
<p>Both men stressed that inter-agency relations between Dutch and American intelligence organizations remain “excellent”. However, they added that the Netherlands has grown more selective about what it chooses to share with American intelligence agencies—particularly the Central Intelligence Agency and the National Security Agency. “That we sometimes don’t share things anymore, that’s true,” Reesink said, referring to sharing information with American intelligence agencies. Akerboom added: “sometimes you have to think case by case.” He went on to say: “We can’t say what we will or won’t share. But we can say that we are more critical.”</p>
<p>According to the two senior officials, Dutch spies have been intensifying intelligence cooperation and sharing with their European counterparts. This is particularly applicable to a collection of central and northern European intelligence services from countries like Scandinavia, France, Germany, the United Kingdom, and Poland, according to <em>De Volkskrant</em>.</p>
<p>► <strong>Author</strong>: Ian Allen | <strong>Date</strong>: 20 October 2025 | <a href="https://intelnews.org/2025/10/20/01-3416/">Permalink</a></p>

					
					<!--
					<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
			xmlns:dc="http://purl.org/dc/elements/1.1/"
			xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
		<rdf:Description rdf:about="https://intelnews.org/2025/10/20/01-3416/"
    dc:identifier="https://intelnews.org/2025/10/20/01-3416/"
    dc:title="Dutch spy services have restricted intelligence-sharing with the United States:&nbsp;report"
    trackback:ping="https://intelnews.org/2025/10/20/01-3416/trackback/" />
</rdf:RDF>					-->

				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chess grandmaster Daniel Naroditsky has passed away (383 pts)]]></title>
            <link>https://old.reddit.com/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/</link>
            <guid>45646561</guid>
            <pubDate>Mon, 20 Oct 2025 17:24:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/">https://old.reddit.com/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/</a>, See on <a href="https://news.ycombinator.com/item?id=45646561">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="siteTable_t3_1obnbmu"><div id="thing_t1_nkh9edt" onclick="click_thing(this)" data-fullname="t1_nkh9edt" data-type="comment" data-gildings="0" data-subreddit="chess" data-subreddit-prefixed="r/chess" data-subreddit-fullname="t5_2qhr7" data-subreddit-type="public" data-author="TheStarfrost" data-author-fullname="t2_1v11zz43qp" data-replies="0" data-permalink="/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/nkh9edt/"><p>[–]<a href="https://old.reddit.com/user/TheStarfrost">TheStarfrost</a><span></span> <span title="348">348 points</span><span title="349">349 points</span><span title="350">350 points</span> <time title="Mon Oct 20 16:56:00 2025 UTC" datetime="2025-10-20T16:56:00+00:00">2 hours ago</time><time title="last edited 1 hour ago" datetime="2025-10-20T17:37:08+00:00">*</time>&nbsp;(1 child)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_nkh9edt1my"><div><p>Heartbreaking. </p>

<p>Danya was an incredible, positive influence, not only on Chess, but on the world around him. </p>

<p>He brought so much joy to so many people, from his speedruns to his commentary to his chess itself. </p>

<p>I cannot believe he's gone. We are all lesser for it.</p>

<p>He will be sorely missed.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/nkh9edt/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li>report</li><li>reply</li></ul></div><div id="thing_t1_nkhaozo" onclick="click_thing(this)" data-fullname="t1_nkhaozo" data-type="comment" data-gildings="0" data-subreddit="chess" data-subreddit-prefixed="r/chess" data-subreddit-fullname="t5_2qhr7" data-subreddit-type="public" data-author="KnightFlorianGeyer" data-author-fullname="t2_ezcujy04s" data-replies="0" data-permalink="/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/nkhaozo/"><p>[–]<a href="https://old.reddit.com/user/Solid_Crab_4748">Solid_Crab_4748</a><span></span> <span title="6">6 points</span><span title="7">7 points</span><span title="8">8 points</span> <time title="Mon Oct 20 19:10:34 2025 UTC" datetime="2025-10-20T19:10:34+00:00">19 minutes ago</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_nkhij1i599"><div><p>Nobody goes to check on someone after they haven't said something in a day</p>

<p>There are also signs in some of how he spoke more recently and for those who really payed attention you could tell he was struggling.</p>

<p>I sure do hope so. And I don't think speculating helps tbf but there's a lot more to suggest one way than the other :(</p>
</div></form><ul><li><a href="https://old.reddit.com/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/nkhij1i/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#nkhhqfi" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div><div id="thing_t1_nkh96ir" onclick="click_thing(this)" data-fullname="t1_nkh96ir" data-type="comment" data-gildings="0" data-subreddit="chess" data-subreddit-prefixed="r/chess" data-subreddit-fullname="t5_2qhr7" data-subreddit-type="public" data-author="__Jimmy__" data-author-fullname="t2_z9rec" data-replies="0" data-permalink="/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/nkh96ir/"><p>[–]<a href="https://old.reddit.com/user/ChaoticBoltzmann">ChaoticBoltzmann</a><span></span> <span title="47">47 points</span><span title="48">48 points</span><span title="49">49 points</span> <time title="Mon Oct 20 17:32:21 2025 UTC" datetime="2025-10-20T17:32:21+00:00">1 hour ago</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_nkhan31nzv"><div><p>He was easily the most passionate, deliberate, masterful teacher of Chess.</p>

<p>With Danya, you could believe that if you had spent the time and given the effort, you, too, could reach the levels of mastery that he effortlessly displayed.</p>

<p>He was a gentlemen of the highest caliber, an ambassador for the game, and he will forever be remembered as such.</p>

<p>Life is fragile folks, ...</p>
</div></form><ul><li><a href="https://old.reddit.com/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/nkhan31/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#nkh9qno" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div><div id="thing_t1_nkhb0nd" onclick="click_thing(this)" data-fullname="t1_nkhb0nd" data-type="comment" data-gildings="0" data-subreddit="chess" data-subreddit-prefixed="r/chess" data-subreddit-fullname="t5_2qhr7" data-subreddit-type="public" data-author="daynighttrade" data-author-fullname="t2_b5v0spvp" data-replies="0" data-permalink="/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/nkhb0nd/"><p>[–]<a href="https://old.reddit.com/user/daynighttrade">daynighttrade</a><span></span> <span title="23">23 points</span><span title="24">24 points</span><span title="25">25 points</span> <time title="Mon Oct 20 17:42:31 2025 UTC" datetime="2025-10-20T17:42:31+00:00">1 hour ago</time>&nbsp;(3 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_nkhb0nd9ax"><div><p>I'm shattered. I can't imagine the pain his loved ones, friends and family would be feeling. Such a bright person passing away, and living his last days of his life under mental agony. I wanted him to bounce back so much, that I'm having difficulty processing this news.  </p>

<p>I hope people take some lesson from this. It's very easy to be a keyboard warrior behind anonymity cloak, but there's a real person at the other end.</p>

<p>I also hope Kramnik realizes what he's done and stops his baseless allegations.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/nkhb0nd/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li>report</li><li>reply</li></ul></div><div id="thing_t1_nkhbkpw" onclick="click_thing(this)" data-fullname="t1_nkhbkpw" data-type="comment" data-gildings="0" data-subreddit="chess" data-subreddit-prefixed="r/chess" data-subreddit-fullname="t5_2qhr7" data-subreddit-type="public" data-author="01_vampyr" data-author-fullname="t2_1zwg66hpu4" data-replies="0" data-permalink="/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/nkhbkpw/"><p>[–]<a href="https://old.reddit.com/user/01_vampyr">01_vampyr</a><span></span> <span title="16">16 points</span><span title="17">17 points</span><span title="18">18 points</span> <time title="Mon Oct 20 17:57:54 2025 UTC" datetime="2025-10-20T17:57:54+00:00">1 hour ago</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_nkhbkpwz8v"><div><p>Danya was THE "everyday person" to be able to compete at the absolute highest level (if not above). He would regularly destroy the top of the crop in online chess, and would do so while showing emotion, entertaining viewers, and making humble content for beginners.</p>

<p>To be completely honest, I was first attracted to his streams by his fits of rage on stream... partly because they were funny, but partly also because it was relatable, and "normalized" a chess GM. Once he said he got control of it, his content got even better, and I noticed that he quickly became not just the biggest Chess streamer (I'm certain he was for several years), but one of the biggest streamers on Twitch. Like, he went from a "uni kid" raging for slipping pieces etc during games in between uni-classes, to netting $100k+/month streaming and being positive. And on top of displaying the highest level of chess on streams, he would, like I said, make beginner content, commentate tournaments, etc.</p>

<p>Never did he shill any crappy products (or far below that, like unregulated gamba, like some of the other top GM's like Hikaru), and never did it seem like any of his success got to his head. He didn't "minmax" his streams by only focusing on drama or whatever would from time to time boost Chess interest. He always did what originally got people interested in him, and what he actually wanted to do.</p>

<p>I don't know the details, but I'd be shocked if this <em>wasn't</em> suicide. Something just tells me that it was, everything into consideration. It's just so sad. The world has truly lost out on a great person, a great influencer (if you can even believe those can exist in today's world), and for chess, an irreplicable asset.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/nkhbkpw/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li>report</li><li>reply</li></ul></div><div id="thing_t1_nkhac78" onclick="click_thing(this)" data-fullname="t1_nkhac78" data-type="comment" data-gildings="0" data-subreddit="chess" data-subreddit-prefixed="r/chess" data-subreddit-fullname="t5_2qhr7" data-subreddit-type="public" data-author="Particular_Text17" data-author-fullname="t2_1ljmklf18n" data-replies="0" data-permalink="/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/nkhac78/"><p>[–]<a href="https://old.reddit.com/user/Particular_Text17">Particular_Text17</a><span></span> <span title="5">5 points</span><span title="6">6 points</span><span title="7">7 points</span> <time title="Mon Oct 20 17:23:58 2025 UTC" datetime="2025-10-20T17:23:58+00:00">2 hours ago</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_nkhac78qk4"><div><p>Shocking. Had a shiver down my spine reading this.</p>

<p>I've been watching Danya ever since his days at Stanford streaming from his dorm and classrooms. Very bright, had every door open back then.</p>

<p>He always had some minor mental health issues (e.g. destroying objects, raging really hard) and was staying up super late every night, but nothing unusual for other gamers too, to be fair. People found his antics funny, his chat was memeing mostly.</p>

<p>Really hard to tell what happened. Something must've derailed really badly. Probably something personal, not even connected to chess or his career.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/nkhac78/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li>report</li><li>reply</li></ul></div><div id="thing_t1_nkhed7l" onclick="click_thing(this)" data-fullname="t1_nkhed7l" data-type="comment" data-gildings="0" data-subreddit="chess" data-subreddit-prefixed="r/chess" data-subreddit-fullname="t5_2qhr7" data-subreddit-type="public" data-author="youwin10" data-author-fullname="t2_6iw43kjn" data-replies="0" data-permalink="/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/nkhed7l/"><p>[–]<a href="https://old.reddit.com/user/youwin10">youwin10</a><span></span> <span title="7">7 points</span><span title="8">8 points</span><span title="9">9 points</span> <time title="Mon Oct 20 18:43:50 2025 UTC" datetime="2025-10-20T18:43:50+00:00">46 minutes ago</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_nkhed7luby"><div><p>His family should destroy Kramnik now.   </p>

<p>They should lawsuit the shit out of him and destroy whatever is left of his reputation.</p>

<p>They should take everything, it's still not gonna be enough.</p>

<p>This is one of the most disgusting, evil, unholy things I've even seen in my life.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/nkhed7l/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li>report</li><li>reply</li></ul></div><div id="siteTable_deleted" onclick="click_thing(this)" data-fullname="t1_nkh9m1q" data-type="comment" data-gildings="0" data-subreddit="chess" data-subreddit-prefixed="r/chess" data-subreddit-fullname="t5_2qhr7" data-subreddit-type="public" data-author="JustinSlick" data-author-fullname="t2_6qwv2" data-replies="0" data-permalink="/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/nkh9cmp/"><p>[–]<a href="https://old.reddit.com/user/JustinSlick">JustinSlick</a><span></span> <span title="12">12 points</span><span title="13">13 points</span><span title="14">14 points</span> <time title="Mon Oct 20 17:02:22 2025 UTC" datetime="2025-10-20T17:02:22+00:00">2 hours ago</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_nkh9m1qrk7"><div><p>Post was from @charlottechesscenter - as far as I know nobody else has reported it. </p>

<blockquote>
<p>It is with great sadness that we share the unexpected passing of Daniel Naroditsky. Daniel was a talented chess player, educator, and cherished member of the chess community. He was also a loving son, brother, and loyal friend. </p>

<p>We ask for privacy for Daniel’s family during this extremely difficult time. Let us honor Daniel by remembering his passion for chess and the inspiration he brought to us all.</p>
</blockquote>
</div></form><ul><li><a href="https://old.reddit.com/r/chess/comments/1obnbmu/grandmaster_daniel_naroditsky_has_passed_away/nkh9m1q/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li>report</li><li>reply</li></ul></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Production RAG: what I learned from processing 5M+ documents (466 pts)]]></title>
            <link>https://blog.abdellatif.io/production-rag-processing-5m-documents</link>
            <guid>45645349</guid>
            <pubDate>Mon, 20 Oct 2025 15:55:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.abdellatif.io/production-rag-processing-5m-documents">https://blog.abdellatif.io/production-rag-processing-5m-documents</a>, See on <a href="https://news.ycombinator.com/item?id=45645349">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><section><p>October 20, 2025<!-- --> • <!-- -->3 min read</p><article><p>I've spent the last 8 months in the RAG trenches, I want to share what actually worked vs. wasted our time. We built RAG for Usul AI (9M pages) and an unnamed legal AI enterprise (4M pages).</p>
<h2 id="langchain-llamaindex"><a href="#langchain-llamaindex"></a>Langchain + Llamaindex</h2>
<p>We started out with youtube tutorials. First Langchain → Llamaindex. Got to a working prototype in a couple of days and were optimistic with the progress. We run tests on subset of the data (100 documents) and the results looked great. We spent the next few days running the pipeline on the production dataset and got everything working in a week — incredible.</p>
<p>Except it wasn't, the results were subpar and only the end users could tell. We spent the following few months rewriting pieces of the system, one at a time, until the performance was at the level we wanted. Here are things we did ranked by ROI.</p>
<h2 id="what-moved-the-needle"><a href="#what-moved-the-needle"></a>What moved the needle</h2>
<ol>
<li><strong>Query Generation</strong>: not all context can be captured by the user's last query. We had an LLM review the thread and generate a number of semantic + keyword queries. We processed all of those queries in parallel, and passed them to a reranker. This made us cover a larger surface area and not be dependent on a computed score for hybrid search.</li>
<li><strong>Reranking</strong>: the highest value 5 lines of code you'll add. The chunk ranking shifted <em>a lot</em>. More than you'd expect. Reranking can many times make up for a bad setup if you pass in enough chunks. We found the ideal reranker set-up to be 50 chunk input -&gt; 15 output.</li>
<li><strong>Chunking Strategy</strong>: this takes a lot of effort, you'll probably be spending most of your time on it. We built a custom flow for both enterprises, make sure to understand the data, review the chunks, and check that a) chunks are not getting cut mid-word or sentence b) ~each chunk is a logical unit and captures information on its own</li>
<li><strong>Metadata to LLM</strong>: we started by passing the chunk text to the LLM, we ran an experiment and found that injecting relevant metadata as well (title, author, etc.) improves context and answers by a lot.</li>
<li><strong>Query routing</strong>: many users asked questions that can't be answered by RAG (e.g. summarize the article, who wrote this). We created a small router that detects these questions and answers them using an API call + LLM instead of the full-blown RAG set-ups.</li>
</ol>
<h2 id="our-stack"><a href="#our-stack"></a>Our stack</h2>
<ul>
<li><strong>Vector database</strong>: Azure -&gt; Pinecone -&gt; Turbopuffer (cheap, supports keyword search natively)</li>
<li><strong>Document Extraction</strong>: Custom</li>
<li><strong>Chunking</strong>: Unstructured.io by default, custom for enterprises (heard that Chonkie is good)</li>
<li><strong>Embedding</strong>: text-embedding-large-3, haven't tested others</li>
<li><strong>Reranker</strong>: None -&gt; Cohere 3.5 -&gt; Zerank (less known but actually good)</li>
<li><strong>LLM</strong>: GPT 4.1 -&gt; GPT 5 -&gt; GPT 4.1, covered by Azure credits</li>
</ul>
<h2 id="going-open-source"><a href="#going-open-source"></a>Going Open-source</h2>
<p>We put all our learning into an open-source project: <a target="_blank" rel="noopener noreferrer" href="https://github.com/agentset-ai/agentset">agentset-ai/agentset</a> under an MIT license. Feel free to <a target="_blank" rel="noopener noreferrer" href="mailto:abdellatif@agentset.ai">reach out</a> if you have any questions.</p></article></section><!--$--><!--/$--><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Postman which I thought worked locally on my computer, is down (473 pts)]]></title>
            <link>https://status.postman.com</link>
            <guid>45645172</guid>
            <pubDate>Mon, 20 Oct 2025 15:40:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://status.postman.com">https://status.postman.com</a>, See on <a href="https://news.ycombinator.com/item?id=45645172">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <div>
                  <p><strong>Update</strong> - <span>We have seen significant recovery of the features. We are continuing to monitor for any further issues.</span>
                    <br>
                      <small><span data-datetime-unix="1760973602000"></span>Oct <var data-var="date">20</var>, <var data-var="year">2025</var> - <var data-var="time">08:20</var> PDT</small>
                  </p>
                  <p><strong>Monitoring</strong> - <span>Majority of the services have recovered. We are continuing to monitor.</span>
                    <br>
                      <small><span data-datetime-unix="1760966473000"></span>Oct <var data-var="date">20</var>, <var data-var="year">2025</var> - <var data-var="time">06:21</var> PDT</small>
                  </p>
                  <p><strong>Update</strong> - <span>We are seeing significant recovery and are continuing to monitor.</span>
                    <br>
                      <small><span data-datetime-unix="1760965004000"></span>Oct <var data-var="date">20</var>, <var data-var="year">2025</var> - <var data-var="time">05:56</var> PDT</small>
                  </p>
                  <p><strong>Update</strong> - <span>We are seeing significant recovery and are continuing to monitor.</span>
                    <br>
                      <small><span data-datetime-unix="1760964720000"></span>Oct <var data-var="date">20</var>, <var data-var="year">2025</var> - <var data-var="time">05:52</var> PDT</small>
                  </p>
                  <p><strong>Identified</strong> - <span>We are currently experiencing significantly increased error rates which is impacting functionality on Postman. There is a major issue with our underlying cloud provider and we are working with them to restore full access as quickly as possible.</span>
                    <br>
                      <small><span data-datetime-unix="1760963977000"></span>Oct <var data-var="date">20</var>, <var data-var="year">2025</var> - <var data-var="time">05:39</var> PDT</small>
                  </p>
              </div>



        <div>
      
    <div>
          <div>
      
<div data-component-id="myy1xf2243k7" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman Platform on Desktop
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="Desktop application for Windows, Mac &amp;amp; Linux to help you build, test &amp;amp; manage your APIs." data-original-title="Desktop application for Windows, Mac &amp;amp; Linux to help you build, test &amp;amp; manage your APIs." role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="yd763bkzgtgp" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman Platform on Browser
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="Postman experience on the web such as working within a workspace, sending requests, etc" data-original-title="Postman experience on the web such as working within a workspace, sending requests, etc" role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="6p1fq1q27p48" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman Login
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="Signing up, signing in to your Postman Account on your browser or when using the desktop applications." data-original-title="Signing up, signing in to your Postman Account on your browser or when using the desktop applications." role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="2qwj5yj4dd1p" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman Monitors
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="Run your collections &amp;amp; environments in the Postman Cloud to monitor your API uptime, health &amp;amp; response." data-original-title="Run your collections &amp;amp; environments in the Postman Cloud to monitor your API uptime, health &amp;amp; response." role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="4rvnts3yqmcr" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman Mocks
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="API mocking service using your collections as a backing store." data-original-title="API mocking service using your collections as a backing store." role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="zgyq9tfp50r8" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman API
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="api.getpostman.com service that lets you programmatically access all your data stored on Postman Cloud" data-original-title="api.getpostman.com service that lets you programmatically access all your data stored on Postman Cloud" role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="k14n5hnrpqss" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman API Network (API Explore)
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="Platform for the distribution of collections, allowing for easy &amp;amp; quick access to your APIs." data-original-title="Platform for the distribution of collections, allowing for easy &amp;amp; quick access to your APIs." role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      


      
<div data-component-id="pb0wgwm8s804" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman Search
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="This component serves search results on Postman Application." data-original-title="This component serves search results on Postman Application." role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="z20p2dbm7l9c" data-component-status="operational" data-js-hook="">

   <p><span>
      Public Collection Documentation
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="Published documentation served from documenter.getpostman.com or your own custom domain." data-original-title="Published documentation served from documenter.getpostman.com or your own custom domain." role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      


      
<div data-component-id="xbbdyty488ls" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman Learning Center and Documentation
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="Sub-parts of postman.com domain that deals with documentation and pages around the Postman product along with wiki and knowledge-base on https://learning.postman.com" data-original-title="Sub-parts of postman.com domain that deals with documentation and pages around the Postman product along with wiki and knowledge-base on https://learning.postman.com" role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="8gbj8vqp6q0v" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman Support and  Community Forum
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="Postman support subsystem at https://support.postman.com/ and https://community.postman.com" data-original-title="Postman support subsystem at https://support.postman.com/ and https://community.postman.com" role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="1v2l8kpl7blf" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman Integrations
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="All integrations outlined at https://www.postman.com/integrations/" data-original-title="All integrations outlined at https://www.postman.com/integrations/" role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="8rvcdymgz9dr" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman Interceptor
   </span>


  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="4h2f09y1y6gw" data-component-status="operational" data-js-hook="">

   <p><span>
      Marketing Website
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="Static content that is marketing related and served via www.postman.com.  This includes www.postman.com/downloads/, https://www.postman.com/release-notes/, and www.postman.com/pricing/." data-original-title="Static content that is marketing related and served via www.postman.com.  This includes www.postman.com/downloads/, https://www.postman.com/release-notes/, and www.postman.com/pricing/." role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      


      
<div data-component-id="mbqgc0cwjyf4" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman VS Code extension
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="The Postman VS Code extension enables you to develop and test your APIs in Postman directly from Visual Studio Code." data-original-title="The Postman VS Code extension enables you to develop and test your APIs in Postman directly from Visual Studio Code." role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="tpym246hbkmn" data-component-status="operational" data-js-hook="">

   <p><span>
      API Builder
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="Create and manage APIs within your workspace" data-original-title="Create and manage APIs within your workspace" role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="1wj634bs58jp" data-component-status="operational" data-js-hook="">

   <p><span>
      API Specifications
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="Create and manage Specifications within a workspace, generating collections from specification" data-original-title="Create and manage Specifications within a workspace, generating collections from specification" role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="x08ythhdpfvx" data-component-status="operational" data-js-hook="">

   <p><span>
      Collection Runner
   </span>


  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      


  </div>

          <div>
      
<div data-component-id="gtytnqfzhhq1" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman Platform on Desktop
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="Desktop application for Windows, Mac &amp;amp; Linux to help you build, test &amp;amp; manage your APIs." data-original-title="Desktop application for Windows, Mac &amp;amp; Linux to help you build, test &amp;amp; manage your APIs." role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="ry1cld662pcm" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman Platform on Browser
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="Postman experience on the web such as working within a workspace, sending requests, etc" data-original-title="Postman experience on the web such as working within a workspace, sending requests, etc" role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="2tqmljfrlmhx" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman Login
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="Signing up, signing in to your Postman Account on your browser or when using the desktop applications." data-original-title="Signing up, signing in to your Postman Account on your browser or when using the desktop applications." role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="s6f85vsfgs0d" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman Mocks
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="API mocking service using your collections as a backing store." data-original-title="API mocking service using your collections as a backing store." role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="3vjknn1q8nsm" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman API
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="api.getpostman.com service that lets you programmatically access all your data stored on Postman Cloud" data-original-title="api.getpostman.com service that lets you programmatically access all your data stored on Postman Cloud" role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

      
<div data-component-id="bv6g2w5c9bvy" data-component-status="operational" data-js-hook="">

   <p><span>
      Postman Search
   </span>

    <span data-js-hook="tooltip" tabindex="0" aria-label="This component serves search results on Postman Application." data-original-title="This component serves search results on Postman Application." role="tooltip">?</span>

  <span title="">

    Operational

  </span>

  <span title="Operational"></span></p>

</div>

  </div>

    </div>
    <div>
  <p><span></span>
    Operational
  </p>
  <p><span></span>
    Degraded Performance
  </p>
  <p><span></span>
    Partial Outage
  </p>
  
  <p><span></span>
    Major Outage
  </p>
  <p><span></span>
    Maintenance
  </p>
</div>

  </div>

    

    








        


      <div>
        <h2 id="past-incidents">Past Incidents</h2>
          
  <div>
    <p>Oct <var data-var="date">20</var>, <var data-var="year">2025</var></p>
      <p>Unresolved incident: Users may encounter issues with accessing or using Postman.</p>
  </div>

          
  <div>
    <p>Oct <var data-var="date">19</var>, <var data-var="year">2025</var></p>
        <p>No incidents reported.</p>
  </div>

          
  <div>
    <p>Oct <var data-var="date">18</var>, <var data-var="year">2025</var></p>
        <p>No incidents reported.</p>
  </div>

          
  <div>
    <p>Oct <var data-var="date">17</var>, <var data-var="year">2025</var></p>
        <p>No incidents reported.</p>
  </div>

          
  <div>
    <p>Oct <var data-var="date">16</var>, <var data-var="year">2025</var></p>
        <p>No incidents reported.</p>
  </div>

          
  <div>
    <p>Oct <var data-var="date">15</var>, <var data-var="year">2025</var></p>
        <p>No incidents reported.</p>
  </div>

          
  <div>
    <p>Oct <var data-var="date">14</var>, <var data-var="year">2025</var></p>
        <p>No incidents reported.</p>
  </div>

          
  <div>
    <p>Oct <var data-var="date">13</var>, <var data-var="year">2025</var></p>
        <p>No incidents reported.</p>
  </div>

          
  <div>
    <p>Oct <var data-var="date">12</var>, <var data-var="year">2025</var></p>
          <div>
    <!-- postmortem -->

    <!-- incident updates -->
      <p><strong>Completed</strong> -
      	<span>The scheduled maintenance has been completed.</span>

        <br>

        <small>
            Oct <var data-var="date">12</var>, <var data-var="time">08:52</var> PDT
        </small>
      </p>
      <p><strong>Verifying</strong> -
      	<span>Verification is currently underway for the maintenance items.</span>

        <br>

        <small>
            Oct <var data-var="date">12</var>, <var data-var="time">08:49</var> PDT
        </small>
      </p>
      <p><strong>In progress</strong> -
      	<span>Scheduled maintenance is currently in progress. We will provide updates as necessary.</span>

        <br>

        <small>
            Oct <var data-var="date">12</var>, <var data-var="time">08:00</var> PDT
        </small>
      </p>
      <p><strong>Scheduled</strong> -
      	<span>We will be performing a planned database upgrade in the US data centre to ensure the latest version and security patches are applied. During this time, users with data in US region may experience temporary service disruptions while using Postman across all platforms.<p>Maintenance is expected to last approximately 2 hours and we will provide regular updates on the progress. Please note, this downtime only affects users with data in US region. Services for users in our EU data centre will remain unaffected.</p><p>If you have any questions, please don't hesitate to reach out to our support team at <a target="_blank" href="mailto:support@postman.com">support@postman.com</a>. <br>We appreciate your patience and understanding during this maintenance window.</p></span>

        <br>

        <small>
            Oct <var data-var="date"> 9</var>, <var data-var="time">06:30</var> PDT
        </small>
      </p>
  </div>

  </div>

          
  <div>
    <p>Oct <var data-var="date">11</var>, <var data-var="year">2025</var></p>
        <p>No incidents reported.</p>
  </div>

          
  <div>
    <p>Oct <var data-var="date">10</var>, <var data-var="year">2025</var></p>
          <div>
    <!-- postmortem -->

    <!-- incident updates -->
      <p><strong>Completed</strong> -
      	<span>Scheduled database maintenance has been successfully completed.</span>

        <br>

        <small>
            Oct <var data-var="date">10</var>, <var data-var="time">21:40</var> PDT
        </small>
      </p>
      <p><strong>Verifying</strong> -
      	<span>We are in the process of verifying the maintenance items.</span>

        <br>

        <small>
            Oct <var data-var="date">10</var>, <var data-var="time">21:23</var> PDT
        </small>
      </p>
      <p><strong>In progress</strong> -
      	<span>Scheduled maintenance is currently in progress. We will provide updates as necessary.</span>

        <br>

        <small>
            Oct <var data-var="date">10</var>, <var data-var="time">20:30</var> PDT
        </small>
      </p>
      <p><strong>Scheduled</strong> -
      	<span>We have a scheduled database maintenance coming up to enable upgrades to the service. Users may experience intermittent service disruptions when doing collection runs via Collection Runner or Postman CLI. Postman Monitors and Scheduled Collections will remain unaffected. We expect to complete maintenance within two hours and will provide frequent updates throughout the process.<p>Contact our support team at <a target="_blank" href="mailto:support@postman.com">support@postman.com</a> with any questions.<br>We appreciate your patience during this maintenance window.</p></span>

        <br>

        <small>
            Oct <var data-var="date"> 7</var>, <var data-var="time">09:15</var> PDT
        </small>
      </p>
  </div>

  </div>

          
  <div>
    <p>Oct <var data-var="date"> 9</var>, <var data-var="year">2025</var></p>
        <p>No incidents reported.</p>
  </div>

          
  <div>
    <p>Oct <var data-var="date"> 8</var>, <var data-var="year">2025</var></p>
        <p>No incidents reported.</p>
  </div>

          
  <div>
    <p>Oct <var data-var="date"> 7</var>, <var data-var="year">2025</var></p>
        <p>No incidents reported.</p>
  </div>

          
  <div>
    <p>Oct <var data-var="date"> 6</var>, <var data-var="year">2025</var></p>
        <p>No incidents reported.</p>
  </div>

      </div>


      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I created a cross-platform GUI for the JJ VCS (Git compatible) (130 pts)]]></title>
            <link>https://judojj.com</link>
            <guid>45645120</guid>
            <pubDate>Mon, 20 Oct 2025 15:35:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://judojj.com">https://judojj.com</a>, See on <a href="https://news.ycombinator.com/item?id=45645120">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <nav aria-label="Primary">
      <a href="https://judojj.com/releases/">Releases</a>
      <a href="https://judojj.com/roadmap/">Roadmap</a>
    </nav>
    <header>
      <div>
        
        <h2>The full-featured GUI for <a href="https://github.com/jj-vcs/jj" target="_blank" rel="noreferrer">JJ VCS</a></h2>
        <p>(works with Git repos too!)</p>
      </div>
      <img src="https://judojj.com/assets/images/app-image-main.png" alt="Judo application screenshot">
    </header>

    

    <section aria-label="Key features">
      <div>
        <p><img src="https://judojj.com/assets/images/op-log.png" alt="Operation Log"></p><p>
          <h3>Restore your repo to any point in time with the Operation Log. Undo and redo any change.</h3>
        </p>
      </div>

      <div>
        <p><img src="https://judojj.com/assets/images/combined-diff.png" alt="Combined diffs"></p><p>
          <h3>View combined diffs of multiple commits, or the diff between commits</h3>
        </p>
      </div>

      <div>
        <p><img src="https://judojj.com/assets/images/revert-hunk.png" alt="Apply or revert hunks"></p><p>
          <h3>Apply or revert hunks of any diff, files, commits, or even multiple commits at once</h3>
        </p>
      </div>

      <div>
        <p><img src="https://judojj.com/assets/images/revsets.png" alt="Custom revsets"></p><p>
          <h3>Use custom <a href="https://jj-vcs.github.io/jj/latest/revsets/#examples" target="_blank" rel="noreferrer">revsets</a> to select which commits are shown. Filter by descriptions, authors, ancestry, and more.</h3>
        </p>
      </div>

      <div>
        <p><img src="https://judojj.com/assets/images/drag-drop-rebase.png" alt="Drag and drop rebase"></p><p>
          <h3>Drag and drop rebase</h3>
        </p>
      </div>

      <div>
        <p><img src="https://judojj.com/assets/images/squash.png" alt="Advanced operations"></p><p>
          <h3>Duplicate, split, abandon, revert, absorb, squash, and more</h3>
        </p>
      </div>

      <div>
        <p><img src="https://judojj.com/assets/images/bookmarks.png" alt="Bookmark management"></p><p>
          <h3>Keep your bookmarks managed</h3>
        </p>
      </div>
    </section>

    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How much Anthropic and Cursor spend on Amazon Web Services (152 pts)]]></title>
            <link>https://www.wheresyoured.at/costs/</link>
            <guid>45644777</guid>
            <pubDate>Mon, 20 Oct 2025 15:05:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wheresyoured.at/costs/">https://www.wheresyoured.at/costs/</a>, See on <a href="https://news.ycombinator.com/item?id=45644777">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
          <p>So, I originally planned for this to be on my premium newsletter, but decided it was better to publish on my free one so that you could all enjoy it. If you liked it, please consider subscribing to support my work. <a href="https://edzitronswheresyouredatghostio.outpost.pub/public/promo-subscription/28fs01k51c?ref=wheresyoured.at"><u>Here’s $10 off the first year of annual</u></a>.</p><p>I’ve also recorded an episode about this on my podcast Better Offline (<a href="https://www.omnycontent.com/d/playlist/e73c998e-6e60-432f-8610-ae210140c5b1/cf0c25ad-cf01-4da5-ae1c-b0fc015f790e/53ed270b-7147-4f70-81c2-b0fc015fe4ed/podcast.rss?ref=wheresyoured.at"><u>RSS feed</u></a>, <a href="https://podcasts.apple.com/us/podcast/better-offline/id1730587238?i=1000732667889&amp;ref=wheresyoured.at" rel="noreferrer">Apple</a>, <a href="https://open.spotify.com/episode/5Tyqfmo2EIIj2IzzQl9z1Z?ref=wheresyoured.at" rel="noreferrer">Spotify</a>, <a href="https://podcasts.apple.com/us/podcast/better-offline/id1730587238?i=1000732667889&amp;ref=wheresyoured.at" rel="noreferrer">iHeartRadio</a>), it’s a little different but both handle the same information, just subscribe and it'll pop up.&nbsp;</p><hr><p>Over the last two years I have written <a href="https://www.wheresyoured.at/the-haters-gui/#companies-built-on-top-of-large-language-models-dont-make-much-money-in-fact-theyre-likely-all-deeply-unprofitable"><u>again</u></a> and <a href="https://www.wheresyoured.at/why-everybody-is-losing-money-on-ai/"><u>again</u></a> about the ruinous costs of running generative AI services, and today I’m coming to you with real proof.</p><p>Based on discussions with sources with direct knowledge of their AWS billing, I am able to disclose the amounts that AI firms are spending, specifically Anthropic and AI coding company Cursor, <a href="https://www.vincentschmalbach.com/cursor-is-anthropics-largest-customer-and-maxing-out-their-gpus/?ref=wheresyoured.at"><u>its largest customer</u></a>.</p><p>I can exclusively reveal today Anthropic’s spending on Amazon Web Services for the entirety of 2024, and for every month in 2025 up until September, and that that Anthropic’s spend on compute far exceeds that previously reported.&nbsp;</p><p>Furthermore, I can confirm that <strong>through September, Anthropic has spent more than 100% of its estimated revenue (based on reporting in the last year) on Amazon Web Services, spending $2.66 billion on compute on an estimated $2.55 billion in revenue.</strong></p><p>Additionally, Cursor’s Amazon Web Services bills more than doubled from $6.2 million in May 2025 to $12.6 million in June 2025, exacerbating a cash crunch that began when Anthropic introduced Priority Service Tiers, <a href="https://www.wheresyoured.at/anthropic-and-openai-have-begun-the-subprime-ai-crisis/"><u>an aggressive rent-seeking measure that begun what I call the Subprime AI Crisis</u></a>, where model providers begin jacking up the prices on their previously subsidized rates.</p><p>Although Cursor obtains the majority of its compute from Anthropic — with AWS contributing a relatively small amount, and likely also taking care of other parts of its business — the data seen reveals an overall direction of travel, where the costs of compute <em>only keep on going up</em>.&nbsp;</p><p>Let’s get to it.</p><h2 id="some-initial-important-details">Some Initial Important Details</h2><ul><li><strong>I do not have all the answers!</strong> I am going to do my best to go through the information I’ve obtained and give you a thorough review and analysis. This information provides a revealing — though incomplete — insight into the costs of running Anthropic and Cursor, but does not include other costs, like salaries and compute obtained from other providers. I cannot tell you (and do not have insight into) Anthropic’s actual private moves. Any conclusions or speculation I make in this article will be based on my interpretations of the information I’ve received, as well as other publicly-available information.</li><li>I have used estimates of Anthropic’s revenue based on reporting across the last ten months. Any estimates I make are detailed and they are brief.&nbsp;</li><li><strong>These costs are inclusive of every product bought on Amazon Web Services, including EC2, storage and database services (as well as literally everything else they pay for).</strong></li><li>Anthropic works with both Amazon Web Services and Google Cloud for compute. I do not have any information about its Google Cloud spend.<ul><li>The reason I bring this up is that Anthropic’s revenue is already being eaten up by its AWS spend. It’s likely billions <em>more</em> in the hole from Google Cloud and other operational expenses.</li></ul></li><li>I have confirmed with sources that every single number I give around Anthropic and Cursor’s AWS spend is <strong>the final cash paid to Amazon after any discounts or credits.</strong></li><li>While I cannot disclose the identity of my source, I am 100% confident in these numbers, and have verified their veracity with other sources. </li></ul><h2 id="anthropic%E2%80%99s-compute-costs-are-likely-much-higher-than-reported-%E2%80%94-135-billion-in-2024-on-aws-alone">Anthropic’s Compute Costs Are Likely Much Higher Than Reported — $1.35 Billion in 2024 on AWS Alone</h2><p>In February of this year, <a href="https://www.theinformation.com/articles/anthropic-projects-soaring-growth-to-34-5-billion-in-2027-revenue?rc=kz8jh3&amp;ref=wheresyoured.at"><u>The information reported</u></a> that Anthropic burned $5.6 billion in 2024, and made somewhere between $400 million and $600 million in revenue:</p><blockquote>It’s not publicly known how much revenue Anthropic generated in 2024, although its monthly revenue rose to about $80 million by the end of the year, compared to around $8 million at the start. That suggests full-year revenue in the $400 million to $600 million range.<p>…Anthropic told investors it expects to burn $3 billion this year, substantially less than last year, when it burned $5.6 billion. Last year’s cash burn was nearly $3 billion more than Anthropic had previously projected. That’s likely due to the fact that more than half of the cash burn came from a one-off payment to access the data centers that power its technology, according to one of the people who viewed the pitch.</p></blockquote><p>While I don’t know about prepayment for services, I can confirm from a source with direct knowledge of billing that Anthropic spent $1.35 billion on Amazon Web Services in 2024, and has already spent $2.66 billion on Amazon Web Services through the end of September.</p><p>Assuming that Anthropic made $600 million in revenue, this means that Anthropic spent $6.2 billion in 2024, leaving $4.85 billion in costs unaccounted for.&nbsp;</p><p>The Information’s piece also brings up another point:</p><blockquote>The costs to develop AI models accounted for a major portion of Anthropic’s expenses last year. The company spent $1.5 billion on servers for training AI models. OpenAI was on track to spend as much as $3 billion on training costs last year, though that figure includes additional expenses like paying for data.</blockquote><p>Before I go any further, I want to be clear that The Information’s reporting is sound, and I trust that their source (I have no idea who they are or what information was provided) was operating in good faith with good data. </p><p>However, Anthropic is telling people it spent $1.5 billion on <em>just</em> training when it has an Amazon Web Services bill of $1.35 billion, which heavily suggests that its actual compute costs are significantly higher than we thought, because, to quote SemiAnalysis, “<a href="https://newsletter.semianalysis.com/p/amazons-ai-resurgence-aws-anthropics-multi-gigawatt-trainium-expansion?ref=wheresyoured.at#:~:text=A%20large%20share%20of%20Anthropic%E2%80%99s%20spending%20is%20going%20to%20Google%20Cloud%20%E2%80%93%20one%20of%20Anthropic%E2%80%99s%20first%20major%20investors%20(%24300M%20round%20late%2D2022)%20and%20preferred%20cloud%20partner%20in%202023%20and%202024%2C%20before%20the%20expanded%20AWS%20deal."><u>a large share of Anthropic’s spending is going to Google Cloud</u></a>.”&nbsp;</p><p>I am guessing, because I do not know, but with $4.85 billion of other expenses to account for, it’s reasonable to believe Anthropic spent an amount similar to its AWS spend on Google Cloud. I do not have any information to confirm this, but given the discrepancies mentioned above, this is an explanation that makes sense.</p><p>I also will add that there is some sort of undisclosed cut that Amazon gets of Anthropic’s revenue, though it’s unclear how much. <a href="https://www.theinformation.com/articles/anthropic-projects-soaring-growth-to-34-5-billion-in-2027-revenue?rc=kz8jh3&amp;ref=wheresyoured.at"><u>According to The Information</u></a>, “Anthropic previously told some investors it paid a substantially higher percentage to Amazon [than OpenAI’s 20% revenue share with Microsoft] when companies purchase Anthropic models through Amazon.”</p><p>I cannot confirm whether a similar revenue share agreement exists between Anthropic and Google.</p><p>This also makes me wonder exactly where Anthropic’s money is going.</p><h2 id="where-is-anthropic%E2%80%99s-money-going">Where Is Anthropic’s Money Going?</h2><p>Anthropic has, based on what I can find, raised $32 billion in the last two years, starting out&nbsp;2023 with <a href="https://www.aboutamazon.com/news/company-news/amazon-aws-anthropic-ai?ref=wheresyoured.at"><u>a $4 billion investment from Amazon from September 2023</u></a> (bringing the total to $37.5 billion), where Amazon was named its “primary cloud provider” nearly eight months after <a href="https://www.cnbc.com/2025/01/22/google-agrees-to-new-1-billion-investment-in-anthropic.html?ref=wheresyoured.at"><u>Anthropic announced Google was Anthropic’s “cloud provider.,”</u></a> which <a href="https://www.cnbc.com/2023/10/27/google-commits-to-invest-2-billion-in-openai-competitor-anthropic.html?ref=wheresyoured.at"><u>Google responded to a month later by investing another $2 billion on October 27 2023</u></a>, “involving a $500 million upfront investment and an additional $1.5 billion to be invested over time,” bringing its total funding from 2023 to $6 billion.</p><p>In 2024, it would raise several more rounds — one in January for $750 million, another in March for $884.1 million, another in May for $452.3 million, and <a href="https://www.anthropic.com/news/anthropic-amazon-trainium?ref=wheresyoured.at"><u>another $4 billion from Amazon in November 2024</u></a>, which also saw it name AWS as Anthropic’s “primary cloud and training partner,” bringing its 2024 funding total to $6 billion.</p><p>In 2025 so far, it’s raised <a href="https://www.cnbc.com/2025/01/22/google-agrees-to-new-1-billion-investment-in-anthropic.html?ref=wheresyoured.at"><u>a $1 billion round from Google</u></a>, <a href="https://techcrunch.com/2025/03/03/anthropic-raises-3-5b-to-fuel-its-ai-ambitions/?ref=wheresyoured.at"><u>a $3.5 billion venture round</u></a> in March, opened <a href="https://www.cnbc.com/2025/05/16/anthropic-ai-credit-facility.html?ref=wheresyoured.at"><u>a $2.5 billion credit facility</u></a> in May, and completed <a href="https://www.cnbc.com/2025/09/02/anthropic-raises-13-billion-at-18-billion-valuation.html?ref=wheresyoured.at"><u>a $13 billion venture round in September, valuing the company at $183 billion</u></a>. This brings its total 2025 funding to $20 billion.&nbsp;</p><p>While I do not have Anthropic’s 2023 numbers, its spend on AWS in 2024 — around $1.35 billion — leaves (as I’ve mentioned) $4.85 billion in costs that are unaccounted for. The Information reports that <a href="https://www.theinformation.com/articles/anthropic-projects-soaring-growth-to-34-5-billion-in-2027-revenue?rc=kz8jh3&amp;ref=wheresyoured.at"><u>costs for Anthropic’s 521 research and development staff reached $160 million in 2024</u></a>, leaving 394 other employees unaccounted for (for 915 employees total), and also adding that Anthropic expects its headcount to increase to 1900 people by the end of 2025.</p><p>The Information also adds that Anthropic “expects to stop burning cash in 2027.”</p><p>This leaves two unanswered questions:</p><ul><li>Where is the rest of Anthropic’s money going?</li><li>How will it “stop burning cash” when its operational costs explode as its revenue increases?</li></ul><p>An optimist might argue that Anthropic is just growing its pile of cash so it’s got a warchest to burn through in the future, but I have my doubts. <a href="https://www.wired.com/story/anthropic-dario-amodei-gulf-state-leaked-memo/?ref=wheresyoured.at"><u>In a memo revealed by WIRED</u></a>, Anthropic CEO Dario Amodei stated that “if [Anthropic wanted] to stay on the frontier, [it would] gain a very large benefit from having access to this capital,” with “this capital” referring to money from the Middle East.&nbsp;</p><p>Anthropic and Amodei’s sudden willingness to take large swaths of capital from the Gulf States does not suggest that it’s not at least a <em>little</em> desperate for capital, especially given Anthropic has, <a href="https://archive.ph/t2HiF?ref=wheresyoured.at"><u>according to Bloomberg</u></a>, “recently held early funding talks with Abu Dhabi-based investment firm MGX” <a href="https://www.cnbc.com/2025/09/02/anthropic-raises-13-billion-at-18-billion-valuation.html?ref=wheresyoured.at"><u>a month after raising $13 billion</u></a>.</p><p>In my opinion — and this is just my gut instinct — I believe that it is either significantly more expensive to run Anthropic than we know, or Anthropic’s leaked (and stated) revenue numbers are worse than we believe. I do not know one way or another, and will only report what I know.</p><h2 id="how-much-did-anthropic-and-cursor-spend-on-amazon-web-services-in-2025">How Much Did Anthropic and Cursor Spend On Amazon Web Services In 2025?</h2><p>So, I’m going to do this a little differently than you’d expect, in that I’m going to lay out how much these companies spent, and draw throughlines from that spend to its reported revenue numbers and product announcements or events that may have caused its compute costs to increase.</p><p>I’ve only got Cursor’s numbers from January through September 2025, but I have Anthropic’s AWS spend for both the entirety of 2024 and through September 2025.</p><h2 id="what-does-%E2%80%9Cannualized%E2%80%9D-mean">What Does “Annualized” Mean?</h2><p>So, this term is one of the most abused terms in the world of software, <em>but in this case</em>, I am sticking to the idea that it means “month times 12.” So, if a company made $10m in January, you would say that its annualized revenue is $120m. Obviously, there’s a lot of (when you think about it, really obvious) problems with this kind of reporting — and thus, you only ever see it when it comes to pre-IPO firms — but that’s besides the point.</p><p>I give you this explanation because, when contrasting Anthropic’s AWS spend with its revenues, I’ve had to work back from whatever annualized revenues were reported for that month.&nbsp;</p><h2 id="anthropic%E2%80%99s-amazon-web-services-spend-in-20241359-billionestimated-revenue-400-million-to-600-million">Anthropic’s Amazon Web Services Spend In 2024 - $1.359 Billion - Estimated Revenue $400 Million to $600 Million</h2><p>Anthropic’s 2024 revenues are a little bit of a mystery, but, as mentioned above, <a href="https://www.theinformation.com/articles/anthropic-projects-soaring-growth-to-34-5-billion-in-2027-revenue?ref=wheresyoured.at"><u>The Information</u></a> says it might be between $400 million and $600 million.</p><p>Here’s its monthly AWS spend.&nbsp;</p><ul><li>January 2024 - $52.9 million</li><li>February 2024 - $60.9 million</li><li>March 2024 - $74.3 million</li><li>April 2024 - $101.1 million</li><li>May 2024 - $100.1 million</li><li>June 2024 - $101.8 million</li><li>July 2024 - $118.9 million</li><li>August 2024 - $128.8 million</li><li>September 2024 - $127.8 million</li><li>October 2024 - $169.6 million</li><li>November 2024 - $146.5 million</li><li>December 2024 - $176.1 million</li></ul><h2 id="analysis-anthropic-spent-at-least-200-of-its-2024-revenue-on-amazon-web-services-in-2024">Analysis: Anthropic Spent At Least 200% of Its 2024 Revenue On Amazon Web Services In 2024</h2><p>I’m gonna be <em>nice</em> here and say that Anthropic made $600 million in 2024 — the higher end of The Information’s reporting — meaning that it spent around 226% of its revenue ($1.359 billion) on Amazon Web Services.</p><p><strong>[Editor's note:</strong> this copy originally had incorrect maths on the %. Fixed now.]<strong> </strong></p><h2 id="anthropic%E2%80%99s-amazon-web-services-spend-in-2025-through-september-2025266-billionestimated-revenue-through-september-255-billion104-of-revenue-spent-on-aws">Anthropic’s Amazon Web Services Spend In 2025 Through September 2025 - $2.66 Billion - Estimated Revenue Through September $2.55 Billion - 104% Of Revenue Spent on AWS</h2><p><a href="https://www.wheresyoured.at/howmuchmoney/#anthropic-has-made-around-15-billion-through-july-in-2025-so-far-and-could-hit-9-billion-annualized-by-end-of-2025"><u>Thanks to my own analysis</u></a> and reporting from outlets like The Information and Reuters, we have a pretty good idea of Anthropic’s revenues for much of the year. That said, July, August, and September get a little weirder, because we’re relying on “almosts” and “approachings,” as I’ll explain as we go.</p><p>I’m also gonna do an analysis on a month-by-month basis, because it’s necessary to evaluate these numbers in context.&nbsp;</p><h3 id="january-20251885-million-in-aws-spend-7291-or-83-million-in-revenue227-of-revenue-spent-on-aws">January 2025 - $188.5 million In AWS Spend, $72.91 or $83 Million In Revenue - 227% Of Revenue Spent on AWS</h3><p>In this month, Anthropic’s reported revenue was somewhere from <a href="https://www.theinformation.com/articles/lightspeed-pays-a-pretty-penny-for-anthropic?ref=wheresyoured.at&amp;rc=kz8jh3"><u>$875 million</u></a> to <a href="https://www.cnbc.com/2025/03/03/amazon-backed-ai-firm-anthropic-valued-at-61point5-billion-after-latest-round.html?ref=wheresyoured.at"><u>$1 billion annualized</u></a>, meaning either $72.91 million or $83 million for the month of January.</p><h3 id="february-20251812-million-in-aws-spend-116-million-in-revenue156-of-revenue-spent-on-aws181-of-revenue-spent-on-aws">February 2025 - $181.2 million in AWS Spend, $116 Million In Revenue - 156% Of Revenue Spent On AWS - 181% Of Revenue Spent On AWS</h3><p>In February, <a href="https://www.theinformation.com/articles/anthropics-claude-drives-strong-revenue-growth-while-powering-manus-sensation?offer=exp-ann-25&amp;utm_campaign=Expansion%3A+AI+Agenda&amp;utm_content=7427&amp;utm_medium=email&amp;utm_source=cio&amp;utm_term=5088&amp;rc=kz8jh3"><u>as reported by The Information</u></a>, Anthropic hit $1.4 billion annualized revenue, or around $116 million each month.</p><h3 id="march-20252403-million-in-aws-spend166-million-in-revenue144-of-revenue-spent-on-awslaunch-of-claude-sonnet-37-claude-code-research-preview-february-24">March 2025 - $240.3 million in AWS Spend - $166 Million In Revenue - 144% Of Revenue Spent On AWS - Launch of Claude Sonnet 3.7 &amp; Claude Code Research Preview (February 24)</h3><p>In March, <a href="https://www.reuters.com/business/anthropic-hits-3-billion-annualized-revenue-business-demand-ai-2025-05-30/?ref=wheresyoured.at"><u>as reported by Reuters</u></a>, Anthropic hit $2 billion in annualized revenue, or $166 million in revenue.</p><p>Because February is a short month, and the launch took place on February 24 2025, I’m considering the <a href="https://www.anthropic.com/news/claude-3-7-sonnet?ref=wheresyoured.at"><u>launches of Claude 3.7 Sonnet and Claude Code’s research preview</u></a> to be a cost burden in the month of March.</p><p>And man, what a burden! Costs increased by $59.1 million, primarily across compute categories, but with a large ($2 million since January) increase in monthly costs for S3 storage.</p><h3 id="april-20252216-million-in-aws-spend204-million-in-revenue108-of-revenue-spent-on-aws">April 2025 - $221.6 million in AWS Spend - $204 Million In Revenue - 108% Of Revenue Spent On AWS</h3><p>I estimate, based on a 22.4% compound growth rate, that Anthropic hit around $2.44 billion in annualized revenue in April, or $204 million in revenue.</p><p>Interestingly, <a href="https://www.anthropic.com/news/max-plan?ref=wheresyoured.at"><u>this was the month where Anthropic launched its $100 and $200 dollar a month “Max” plan</u></a>s, and it doesn’t seem to have dramatically increased its costs. Then again, Max is also the gateway to things like Claude Code, which I’ll get to shortly.</p><h3 id="may-20252867-million-in-aws-spend250-million-in-revenue114-of-revenue-spent-on-awssonnet-4-opus-4-general-availability-of-claude-code-may-22-service-tiers-may-30">May 2025 - $286.7 million in AWS Spend - $250 Million In Revenue - 114% Of Revenue Spent On AWS - Sonnet 4, Opus 4, General Availability Of Claude Code (May 22) Service Tiers (May 30)</h3><p>In May, <a href="https://www.cnbc.com/2025/05/30/anthropic-hits-3-billion-in-annualized-revenue-on-business-demand-for-ai.html?ref=wheresyoured.at"><u>as reported by CNBC</u></a>, Anthropic hit $3 billion in annualized revenue, or $250 million in monthly average revenue.</p><p>This was a <em>big month</em> for Anthropic, with <a href="https://www.anthropic.com/news/claude-4?ref=wheresyoured.at"><u>two huge launches on May 22 2025</u></a> — its new, “more powerful” models Claude Sonnet and Opus 4, as well as the general availability of its AI coding environment Claude Code.</p><p>Eight days later, on May 30 2025, a page on Anthropic's API documentation appeared for the first time: "<a href="https://web.archive.org/web/20250530132140/https://docs.anthropic.com/en/api/service-tiers"><u>Service Tiers</u></a>":</p><blockquote>Different tiers of service allow you to balance availability, performance, and predictable costs based on your application’s needs.<p>We offer three service tiers:</p><p>- Priority Tier: Best for workflows deployed in production where time, availability, and predictable pricing are important</p><p>Standard: Best for bursty traffic, or for when you’re trying a new idea</p><p>Batch: Best for asynchronous workflows which can wait or benefit from being outside your normal capacity</p></blockquote><p><a href="https://web.archive.org/web/20250530132140/https://docs.anthropic.com/en/api/service-tiers#get-started-with-priority-tier"><u>Accessing the priority tier requires you to make an up-front commitment to Anthropic</u></a>, and said commitment is based on a number of months (1, 3, 6 or 12) and the number of input and output tokens you estimate you will use each minute.&nbsp;</p><h4 id="what%E2%80%99s-a-priority-tier-why-is-it-significant">What’s a Priority Tier? Why Is It Significant?</h4><p>As I’ll get into in my June analysis, Anthropic’s Service Tiers exist specifically for it to “guarantee” your company won’t face rate limits or any other service interruptions, requiring a minimum spend, minimum token throughput, and for you to pay higher rates when writing to the cache — which is, as I’ll explain, a big part of running an AI coding product like Cursor.</p><p>Now, the jump in costs — $65.1 million or so between April and May — likely comes as a result of the final training for Sonnet and Opus 4, as well as, I imagine, some sort of testing to make sure Claude Code was ready to go.</p><h3 id="june-20253214-million-in-aws-spend333-million-in-revenue965-of-revenue-spent-on-awsanthropic-cashes-in-on-service-tier-tolls-that-add-an-increased-charge-for-prompt-caching-directly-targeting-companies-like-cursor">June 2025 - $321.4 million in AWS Spend - $333 Million In Revenue - 96.5% Of Revenue Spent On AWS - Anthropic Cashes In On Service Tier Tolls That Add An Increased Charge For Prompt Caching, Directly Targeting Companies Like Cursor</h3><p>In June, as reported by The Information, Anthropic hit $4 billion in annualized revenue, or $333 million.</p><p>Anthropic’s revenue spiked by $83 million this month, and so did its costs by $34.7 million.&nbsp;</p><h3 id="anthropic-started-the-subprime-ai-crisis-in-june-2025-increasing-costs-on-its-largest-customer-doubling-its-aws-spend-in-a-month">Anthropic Started The Subprime AI Crisis In June 2025, Increasing Costs On Its Largest Customer, Doubling Its AWS Spend In A Month</h3><p>I have, for a while, talked about <a href="https://www.wheresyoured.at/subprimeai/#:~:text=I%20hypothesize%20a%20kind,to%20justify%20the%20expense."><u>the Subprime AI Crisis</u></a>, where big tech and companies like Anthropic, after offering subsidized pricing to entice in customers, raise the rates on their customers to start covering more of their costs, leading to a cascade where businesses are forced to raise their prices to handle their new, exploding costs.</p><p>And I was god damn <em>right. </em>Or, at least, it sure looks like I am. I’m hedging, forgive me. I cannot say for certain, but I see a pattern.&nbsp;</p><p>It’s likely the June 2025 spike in revenue came from the introduction of service tiers, which specifically target prompt caching, increasing the amount of tokens you’re charged for as an enterprise customer based on the term of the contract, and your forecast usage. </p><p><a href="https://www.wheresyoured.at/anthropic-and-openai-have-begun-the-subprime-ai-crisis/#how-anthropics-tiered-service-may-have-harmed-its-largest-customer-cursor:~:text=a%20minimum%20spend.-,Furthermore,-%2C%20the%20way%20that"><u>Per my reporting in July</u></a>:&nbsp;</p><blockquote>You see, Anthropic specifically notes on its "service tiers" page that requests at the priority tier are "prioritized over all other requests to Anthropic," a rent-seeking measure that effectively means a company must either:<p>-	Commit to at least a month, though likely 3-12 months of specific levels of input and output tokens a minute, based on what they believe they will use in the future, regardless of whether they do.</p><p>-	Accept that access to Anthropic models will be slower at some point, in some way that Anthropic can't guarantee.Furthermore, the way that Anthropic is charging almost feels intentionally built to fuck over any coding startup that would use its service. Per the service tier page, Anthropic charges 1.25 for every time you write a token to the cache with a 5 minute TTL — or 2 tokens if you have a 1 hour TTL — and a longer cache is effectively essential for any background task where an agent will be working for more than 5 minutes, such as restructuring a particularly complex series of code, you know, the exact things that Cursor is well-known and marketed to do. </p><p>Furthermore, the longer something is in the cache, the better autocomplete suggestions for your code will be. It's also important to remember you're, at some point, caching the prompts themselves — so the instructions of what you want Cursor to do, meaning that the more complex the operation, the more expensive it'll now be for Cursor to provide the service with reasonable uptime.</p></blockquote><p>Cursor, as Anthropic’s largest client (the second largest being Github Copilot), represents a material part of its revenue, and its surging popularity meant it was sending more and more revenue Anthropic’s way.&nbsp; Anysphere, the company that develops Cursor, <a href="https://techcrunch.com/2025/06/05/cursors-anysphere-nabs-9-9b-valuation-soars-past-500m-arr/?ref=wheresyoured.at"><u>hit $500 million annualized revenue ($41.6 million) by the end of May</u></a>, which Anthropic chose to celebrate by increasing its costs.</p><p>On June 16 2025, <a href="https://web.archive.org/web/20250619080155/https://www.cursor.com/en/blog/new-tier"><u>Cursor launched a $200-a-month “Ultra” plan</u></a>, <a href="https://cursor.com/blog/new-tier?ref=wheresyoured.at"><u>as well as dramatic changes to its $20-a-month Pro pricing</u></a> that, instead of offering 500 “fast” responses using models from Anthropic and OpenAI, now effectively provided you with “at least” whatever you paid a month (so $20-a-month got at least $20 of credit), <a href="https://techcrunch.com/2025/07/07/cursor-apologizes-for-unclear-pricing-changes-that-upset-users/?ref=wheresyoured.at"><u>massively increasing the costs for users</u></a>, with one calling the changes a “rug pull” <a href="https://x.com/0ni_x4/status/1940885976127283342?ref=wheresyoured.at"><u>after spending $71 in a single day</u></a>.</p><p>As I’ll get to later in the piece, Cursor’s costs exploded from $6.19 million in May 2025 to $12.67 million in June 2025, and I believe this is a direct result of Anthropic’s sudden and aggressive cost increases.&nbsp;</p><p>Similarly, Replit, <a href="https://blog.replit.com/effort-based-pricing?ref=wheresyoured.at"><u>another AI coding startup, moved to “Effort-Based Pricing” on June 18 2025</u></a>. I have not got any information around its AWS spend.</p><p>I’ll get into this a bit later, but I find this whole situation disgusting.</p><h3 id="july-2025-3232-million-in-aws-spend416-million-in-revenue777-of-revenue-spent-on-aws">July 2025 $323.2 million in AWS Spend - $416 Million In Revenue - 77.7% Of Revenue Spent On AWS</h3><p>In July, <a href="https://archive.ph/KWhkD?ref=wheresyoured.at"><u>as reported by Bloomberg</u></a>, Anthropic hit $5 billion in annualized revenue, or $416 million.</p><p>While July wasn’t a huge month for announcements, it was allegedly the month that Claude Code was generating “nearly $400 million in annualized revenue,” or $33.3 million (<a href="https://www.theinformation.com/articles/anthropic-revenue-pace-nears-5-billion-run-mega-round?ref=wheresyoured.at&amp;rc=kz8jh3"><u>according to The Information</u></a>, who says Anthropic was “approaching” $5 billion in annualized revenue - which likely means LESS than that - but I’m going to go with the full $5 billion annualized for sake of fairness.&nbsp;</p><p>There’s roughly an $83 million bump in Anthropic’s revenue between June and July 2025, and I think Claude Code and its new rates are a big part of it. What’s <em>fascinating</em> is that cloud costs didn’t increase too much — by only $1.8 million, to be specific.</p><h3 id="august-20253837-million-in-aws-spend416-million-in-revenue92-of-revenue-spent-on-aws">August 2025 - $383.7 million in AWS Spend - $416 Million In Revenue - 92% Of Revenue Spent On AWS</h3><p>In August, according to Anthropic, its run-rate “<a href="https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation?ref=wheresyoured.at"><u>reached over $5 billion</u></a>,” or in or around $416 million. I am not giving it anything more than $5 billion, especially considering in July Bloomberg’s reporting said “about $5 billion.”</p><p>Costs grew by $60.5 this month, potentially due to the <a href="https://www.anthropic.com/news/claude-opus-4-1?ref=wheresyoured.at"><u>launch of Claude Opus 4.1</u></a>, Anthropic’s more aggressively expensive model, though revenues do not appear to have grown much along the way.</p><p>Yet what’s <em>very</em> interesting is that Anthropic — <a href="https://techcrunch.com/2025/07/28/anthropic-unveils-new-rate-limits-to-curb-claude-code-power-users/?ref=wheresyoured.at"><u>starting August 28</u></a> — launched weekly rate limits on its Claude Pro and Max plans. I wonder why?</p><h3 id="september-20255189-million-in-aws-spend583-million-in-revenue889-of-revenue-spent-on-aws">September 2025 - $518.9 million in AWS Spend - $583 Million In Revenue - 88.9% Of Revenue Spent On AWS</h3><p>Oh fuck! Look at that massive cost explosion!</p><p>Anyway, according to Reuters, <a href="https://archive.ph/EjLeA?ref=wheresyoured.at"><u>Anthropic’s run rate is “approaching $7 billion” in October</u></a>, and for the sake of <em>fairness</em>, I am going to just say it has $7 billion annualized, <strong>though I believe this number to be lower. </strong>“Approaching” can mean a lot of different things — $6.1 billion, $6.5 billion — and because I already anticipate a lot of accusations of “FUD,” I’m going to err on the side of <em>generosity.</em></p><p>If we assume a $6.5 billion annualized rate, that would make this month’s revenue $541.6 million, or 95.8% of its AWS spend.&nbsp;&nbsp;</p><p>Nevertheless, Anthropic’s costs exploded in the space of a month by $135.2 million (35%) - likely due to the fact that users, <a href="https://www.wheresyoured.at/anthropic-is-bleeding-out/"><u>as I reported in mid-July, were costing it thousands or tens of thousands of dollars in compute</u></a>, a problem it still faces to this day, with <a href="https://www.viberank.app/?ref=wheresyoured.at"><u>VibeRank showing a user currently spending $51,291 in a calendar month on a $200-a-month subscription</u></a>.</p><p>If there were other costs, they likely had something to do with the training runs for <a href="https://www.anthropic.com/news/claude-sonnet-4-5?ref=wheresyoured.at"><u>the launches of Sonnet 4.5</u></a> on September 29 2025 and <a href="https://www.anthropic.com/claude/haiku?ref=wheresyoured.at"><u>Haiku 4.5 in October 2025</u></a>.</p><h2 id="anthropic%E2%80%99s-monthly-aws-costs-have-increased-by-174-since-januaryand-with-its-potential-google-cloud-spend-and-massive-staff-anthropic-is-burning-billions-in-2025">Anthropic’s Monthly AWS Costs Have Increased By 174% Since January - And With Its Potential Google Cloud Spend and Massive Staff, Anthropic Is Burning Billions In 2025</h2><p>While these costs only speak to one part of its cloud stack — Anthropic has an unknowable amount of cloud spend on Google Cloud, and the data I have only covers AWS — it is simply remarkable how much this company spends on AWS, and how rapidly its costs seem to escalate as it grows.</p><p>Though things improved slightly over time — in that Anthropic is no longer burning over 200% of its revenue on AWS alone — these costs have still dramatically escalated, and done so in an aggressive and arbitrary manner.&nbsp;</p><h2 id="anthropic%E2%80%99s-aws-costs-increase-linearly-with-revenue-consuming-the-majority-of-each-dollar-anthropic-makesas-a-reminder-it-also-spends-hundreds-of-millions-or-billions-on-google-cloud-too">Anthropic’s AWS Costs Increase Linearly With Revenue, Consuming The Majority Of Each Dollar Anthropic Makes - <strong>As A Reminder, It Also Spends Hundreds Of Millions Or Billions On Google Cloud Too</strong></h2><p>So, I wanted to visualize this part of the story, because I think it’s important to see the various different scenarios.</p><h2 id="an-estimate-of-anthropic%E2%80%99s-potential-cloud-compute-spend-through-september">An Estimate of Anthropic’s Potential Cloud Compute Spend Through September</h2><p><strong><em><u>THE NUMBERS I AM USING ARE ESTIMATES CALCULATED BASED ON 25%, 50% and 100% OF THE AMOUNTS THAT ANTHROPIC HAS SPENT ON AMAZON WEB SERVICES THROUGH SEPTEMBER.&nbsp;</u></em></strong></p><p>I apologize for all the noise, I just want it to be crystal clear what you see next.&nbsp;&nbsp;</p><figure><img src="https://www.wheresyoured.at/content/images/2025/10/data-src-image-a18dda28-2ac7-494a-8aad-1e3124ebda42.png" alt="" loading="lazy" title="Chart" width="1600" height="960" srcset="https://www.wheresyoured.at/content/images/size/w600/2025/10/data-src-image-a18dda28-2ac7-494a-8aad-1e3124ebda42.png 600w, https://www.wheresyoured.at/content/images/size/w1000/2025/10/data-src-image-a18dda28-2ac7-494a-8aad-1e3124ebda42.png 1000w, https://www.wheresyoured.at/content/images/2025/10/data-src-image-a18dda28-2ac7-494a-8aad-1e3124ebda42.png 1600w" sizes="(min-width: 720px) 720px"></figure><p>As you can see, all it takes is for Anthropic to spend (I am estimating) around 25% of its Amazon Web Services bills (for a total of around $3.33 billion in compute costs through the end of September) to savage any and all revenue ($2.55 billion) it’s making.&nbsp;</p><p>Assuming Anthropic spends half of its&nbsp; AWS spend on Google Cloud, this number climbs to $3.99 billion, and if you assume - and to be clear, this is <em><strong>an estimate </strong></em>- that it spends around the same on <em>both</em> Google Cloud and AWS, Anthropic has spent $5.3 billion on compute through the end of September.</p><p>I can’t tell you which it is, just that we know for certain that Anthropic is spending money on Google Cloud, and because Google <a href="https://www.nytimes.com/2025/03/11/technology/google-investment-anthropic.html?ref=wheresyoured.at"><u>owns 14% of the company</u></a> — rivalling <a href="https://www.morningstar.com/news/marketwatch/20250903243/why-anthropics-fresh-183-billion-valuation-is-good-news-for-amazon?ref=wheresyoured.at"><u>estimates saying Amazon owns around 15-19%</u></a> — it’s fair to assume that there’s a significant spend.</p><h2 id="anthropic%E2%80%99s-costs-are-out-of-control-consistently-and-aggressively-outpacing-revenueand-amazon%E2%80%99s-revenue-from-anthropic-of-266-billion-is-25-of-its-2025-capex">Anthropic’s Costs Are Out Of Control, Consistently And Aggressively Outpacing Revenue - And Amazon’s Revenue from Anthropic Of $2.66 Billion Is 2.5% Of Its 2025 Capex</h2><p>I have sat with these numbers for a great deal of time, and I can’t find any evidence that Anthropic has any path to profitability outside of aggressively increasing the prices on their customers to the point that its services will become untenable for consumers and enterprise customers alike.</p><p>As you can see from these estimated and reported revenues, Anthropic’s AWS costs appear to increase in a near-linear fashion with its revenues, meaning that the current pricing — including rent-seeking measures like Priority Service Tiers — isn’t working to meet the burden of its costs.</p><p>We do not know its Google Cloud spend, but I’d be shocked if it was anything less than 50% of its AWS bill. If that’s the case, Anthropic is in real trouble - the cost of the services underlying its business increase the more money they make.</p><p>It’s becoming increasingly apparent that Large Language Models are not a profitable business. While I cannot speak to Amazon Web Services’ actual costs, it’s making $2.66 billion from Anthropic, which is the <em>second largest foundation model company in the world.&nbsp;</em></p><p>Is that really worth <a href="https://www.barrons.com/articles/amazon-stock-ai-aws-207f82d8?gaa_at=eafs&amp;gaa_n=AWEtsqdB5ei2Pp549YWJtX_F8ImxHOmK8IsGY7lELJ-noRSvlrNczjmyppY2gbP5KHk%3D&amp;gaa_ts=68f2c6ea&amp;gaa_sig=fWk4e5ZMICsxF_Jico93PSq67-vn6xdcEFxzgEYijc2HE7uEipCfxyojd9hqbSDlTD_JG-85XbK4EyckM3NKhQ%3D%3D&amp;ref=wheresyoured.at"><u>$105 billion in capital expenditures</u></a>? Is that really worth building <a href="https://futurism.com/the-byte/amazon-anthropic-ai-data-center?ref=wheresyoured.at"><u>a giant 1200 acre data center in Indiana</u></a> with 2.2GW of electricity?</p><p>What’s the plan, exactly? Let Anthropic burn money for the foreseeable future until it dies, and then pick up the pieces? Wait until Wall Street gets mad at you and then pull the plug?</p><p>Who knows.&nbsp;</p><p>But let’s change gears and talk about Cursor — Anthropic’s largest client and, at this point, a victim of circumstance.</p><h2 id="cursor%E2%80%99s-amazon-web-services-spend-in-2025-through-september-20256999-million">Cursor’s Amazon Web Services Spend In 2025 Through September 2025 - $69.99 Million</h2><h3 id="an-important-note-about-cursor%E2%80%99s-compute-spend">An Important Note About Cursor’s Compute Spend</h3><p><a href="https://aws.amazon.com/bedrock/anthropic/?ref=wheresyoured.at"><u>Amazon sells Anthropic’s models through Amazon Bedrock</u></a>, <strong>and I believe that AI startups are compelled to spend some of their AI model compute costs through Amazon Web Services.</strong> Cursor <em>also sends money directly to Anthropic and OpenAI, meaning that these costs are only one piece of its overall compute costs. </em><strong>In any case, it’s very clear that Cursor buys some degree of its Anthropic model spend through Amazon.</strong></p><p>I’ll also add that <a href="https://www.newcomer.co/p/cursors-popularity-has-come-at-a?ref=wheresyoured.at"><u>Tom Dotan of Newcomer reported</u></a> a few months ago that an investor told him that “Cursor is spending 100% of its revenue on Anthropic.”</p><p>Unlike Anthropic, we lack thorough reporting of the month-by-month breakdown of Cursor’s revenues. I will, however, mention them in the month I have them.</p><p>For the sake of readability — and because we really don’t have much information on Cursor’s revenues beyond a few months — I’m going to stick to a bullet point list.&nbsp;</p><h2 id="another-note-about-cursor%E2%80%99s-aws-spendit-likely-funnels-some-model-spend-through-aws-but-the-majority-goes-directly-to-providers-like-anthropic">Another Note About Cursor’s AWS Spend - It Likely Funnels Some Model Spend Through AWS, But The Majority Goes Directly To Providers Like Anthropic</h2><p>As discussed above, <a href="https://web.archive.org/web/20250619080155/https://www.cursor.com/en/blog/new-tier"><u>Cursor announced (along with their price change and $200-a-month plan) several multi-year partnerships</u></a> with xAI, Anthropic, OpenAI and Google, suggesting that it has direct agreements with Anthropic itself versus one with AWS to guarantee “this volume of compute at a predictable price.”&nbsp;</p><p>Based on its spend with AWS, I do not see a strong “minimum” spend that would suggest that they have a similar deal with Amazon — likely because Amazon handles more than its infrastructure than <em>just</em> compute, but incentivizes it to spend on Anthropic’s models through AWS by offering discounts, something I’ve confirmed with a source.&nbsp;</p><p>In any case, here’s what Cursor spent on AWS.</p><ul><li>January 2025 - $1.459 million<ul><li>This, apparently, is the month <a href="https://x.com/AnjneyMidha/status/1879306784525222193?ref=wheresyoured.at"><u>that Cursor hit $100 million annualized revenue</u></a> — or $8.3 million, meaning it spent 17.5% of its revenue on AWS.</li></ul></li><li>February 2025 - $2.47 million</li><li>March 2025 - $4.39 million</li><li>April 2025 - $4.74 million<ul><li><a href="https://www.theinformation.com/briefings/cursor-hits-200-million-annual-recurring-revenue?rc=kz8jh3&amp;ref=wheresyoured.at"><u>Cursor hit $200 million annualized ($16.6 million) at the end of March 2025</u></a>, according to The Information, working out to spending 28% of its revenue on AWS.&nbsp;&nbsp;</li></ul></li><li>May 2025 - $6.19 million</li><li>June 2025 - $12.67 million<ul><li>So, <a href="https://archive.ph/GnzEC?ref=wheresyoured.at"><u>Bloomberg reported that Cursor hit $500 million on June 5 2025</u></a>, along with raising a $900 million funding round. Great news! Turns out it’d need to start handing a lot of that to Anthropic.</li><li>This was, as I’ve discussed above, the month when Anthropic forced it to adopt “Service Tiers”.<a href="https://www.wheresyoured.at/ai-is-a-money-trap/#cursor-is-a-systemic-risk-to-the-ai-industry"><u> I go into detail about the situation here</u></a>, but the long and short of it is that Anthropic increased the amount of tokens you burned by writing stuff to the cache (think of it like RAM in a computer), and AI coding startups are very cache heavy, meaning that Cursor immediately took on what I believed would be massive new costs. As I discuss in what I just linked, this led Cursor to aggressively change its product, thereby vastly increasing its customers’ costs if they wanted to use the same service.</li><li>That same month, Cursor’s AWS costs — which I believe are the <em>minority</em> of its cloud compute costs — exploded by 104% (or by $6.48 million), and never returned to their previous levels.</li><li>It’s conceivable that this surge is due to the compute-heavy nature of the latest Claude 4 models released that month — or, perhaps, Cursor sending more of its users to other models that it runs on Bedrock.&nbsp;</li></ul></li><li>July 2025 - $15.5 million<ul><li>As you can see, Cursor’s costs continue to balloon in July, and I am guessing it’s because of the Service Tiers situation — which, I believe, indirectly resulted in Cursor pushing more users to models that it runs on Amazon’s infrastructure.</li></ul></li><li>August 2025 - $9.67 million<ul><li>So, I can only guess as to why there was a drop here. User churn? It could be <a href="https://cursor.com/blog/gpt-5?ref=wheresyoured.at"><u>the launch of GPT-5 on Cursor</u></a>, which gave users a week of free access to OpenAI’s new models.</li><li>What’s also interesting is that this was the month when <a href="https://cursor.com/blog/aug-2025-pricing?ref=wheresyoured.at"><u>Cursor announced</u></a> that its previously free “auto” model (where Cursor would select the best available premium model or its own model) would now bill at “<a href="https://cursor.com/docs/account/pricing?ref=wheresyoured.at#auto"><u>competitive token rates</u></a>,” by which I mean it went from charging nothing to $1.25 per million input and $6 per million output tokens. This change would take effect on September 15 2025.</li><li><a href="http://newcomer.co/p/cursors-popularity-has-come-at-a?ref=wheresyoured.at"><u>On August 10 2025</u></a>, Tom Dotan of Newcomer reported that Cursor was “well above” $500 million in annualized revenue based on commentary from two sources.</li></ul></li><li>September 2025 - $12.91 million<ul><li>Per the above, this is the month when Cursor started charging for its “auto” model.</li></ul></li></ul><h2 id="what-anthropic-may-have-done-to-cursor-is-disgustingand-is-a-preview-of-what%E2%80%99s-to-come-for-ai-startups">What Anthropic May Have Done To Cursor Is Disgusting - And Is A Preview Of What’s To Come For AI Startups</h2><p>When I wrote that <a href="https://www.wheresyoured.at/anthropic-and-openai-have-begun-the-subprime-ai-crisis/"><u>Anthropic and OpenAI had begun the Subprime AI Crisis</u></a> back in July, I assumed that the increase in costs was <em>burdensome,</em> but having the information from its AWS bills, it seems that Anthropic’s actions directly caused Cursor’s costs to explode by over 100%.&nbsp;</p><p>While I can’t definitively say “this is exactly what did it,” the timelines match up exactly, the costs have never come down, Amazon offers <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-use.html?ref=wheresyoured.at"><u>provisioned throughput</u></a>, and, more than likely, Cursor needs to keep a standard of uptime similar to that of Anthropic’s own direct API access.</p><p>If this is what happened, it’s deeply shameful.&nbsp;</p><p>Cursor, <a href="https://www.vincentschmalbach.com/cursor-is-anthropics-largest-customer-and-maxing-out-their-gpus/?ref=wheresyoured.at"><u>Anthropic’s largest customer</u></a>, in the very same month it hit $500 million in annualized revenue, immediately had its AWS and Anthropic-related costs explode to the point that it had to dramatically reduce the value of its product just as it hit the apex of its revenue growth.&nbsp;</p><h2 id="anthropic-timed-its-rent-seeking-service-tier-price-increases-on-cursor-with-the-launch-of-a-competitive-productwhich-is-what%E2%80%99s-coming-to-any-ai-startup-that-builds-on-top-of-its-products">Anthropic Timed Its Rent-Seeking Service Tier Price Increases on Cursor With The Launch Of A Competitive Product - Which Is What’s Coming To Any AI Startup That Builds On Top Of Its Products</h2><p>It’s very difficult to see Service Tiers as anything other than an aggressive rent-seeking maneuver.</p><p>Yet another undiscussed part of the story is that the launch of Claude 4 Opus and Sonnet — and the subsequent launch of Service Tiers — <a href="https://www.anthropic.com/news/claude-4?ref=wheresyoured.at"><u>coincided with the launch of Claude Code</u></a>, a product that directly competes with Cursor, without the burden of having to pay itself for the cost of models or, indeed, having to deal with its own “Service Tiers.”</p><p>Anthropic may have increased the prices on its largest client at the time it was launching a competitor, <strong>and I believe that this is what awaits any product built on top of OpenAI or Anthropic’s models.&nbsp;</strong></p><h2 id="the-subprime-ai-crisis-is-real-and-it-can-hurt-you">The Subprime AI Crisis Is Real, And It Can Hurt You</h2><p>I realize this has been a long, number-stuffed article, but the long-and-short of it is simple: Anthropic is burning all of its revenue on compute, and Anthropic will willingly increase the prices on its customers if it’ll help it burn less money, even though that doesn’t seem to be working.</p><p>What I believe happened to Cursor will likely happen to every AI-native company, because in a very real sense, Anthropic’s products are a wrapper for its own models, except it only has to pay the (unprofitable) costs of running them on Amazon Web Services and Google Cloud.</p><p>As a result, both OpenAI and Anthropic can (and may very well!) devour the market of any company that builds on top of their models.&nbsp;</p><p>OpenAI may have given Cursor free access to its GPT-5 models in August, but a month later <a href="https://openai.com/index/introducing-upgrades-to-codex/?ref=wheresyoured.at"><u>on September 15 2025</u></a> it debuted massive upgrades to its competitive “Codex” platform.&nbsp;</p><p>Any product built on top of an AI model that shows any kind of success can be cloned immediately by OpenAI and Anthropic, and I believe that we’re going to see multiple price increases on AI-native companies in the next few months. After all, <a href="https://openai.com/api-priority-processing/?ref=wheresyoured.at"><u>OpenAI already has its own priority processing product, which it launched shortly after Anthropic’s in June</u></a>.</p><p>The ultimate problem is that there really are no<em> winners</em> in this situation. If Anthropic kills Cursor through aggressive rent-seeking, that directly eats into its own revenues. If Anthropic lets Cursor succeed, that’s <em>revenue</em>, but it’s also clearly <em>unprofitable revenue</em>. Everybody loses, but nobody loses more than Cursor’s (and other AI companies’) customers.&nbsp;</p><h2 id="anthropic-is-in-real-troubleand-the-current-cost-of-doing-business-is-unsustainable-meaning-prices-must-increase">Anthropic Is In Real Trouble - And The Current Cost Of Doing Business Is Unsustainable, Meaning Prices Must Increase</h2><p>I’ve come away from this piece with a feeling of dread.</p><p>Anthropic’s costs are out of control, and as things get more desperate, it appears to be lashing out at its customers, both companies like Cursor and <a href="https://techcrunch.com/2025/07/28/anthropic-unveils-new-rate-limits-to-curb-claude-code-power-users/?ref=wheresyoured.at"><u>Claude Code customers facing weekly rate limits on their more-powerful models</u></a> who are chided for using a product they pay for. Again, I cannot say for certain, but the spike in costs is clear, and it feels like more than a coincidence to me.&nbsp;</p><p>There is no period of time that I can see in the just under two years of data I’ve been party to that suggests that Anthropic has any means of — or any success doing — cost-cutting, and the only thing this company seems capable of doing is increasing the amount of money it burns on a monthly basis.&nbsp;</p><p>Based on what I have been party to, the more successful Anthropic becomes, the more its services cost. <a href="https://www.wheresyoured.at/how-to-argue-with-an-ai-booster/#if-you-plotted-the-curve-of-how-the-cost-of-inference-has-been-falling-over-time-%E2%80%94-false-the-cost-of-inference-has-gone-up-over-time"><u>The cost of inference is clearly increasing for customers</u></a>, but based on its escalating monthly costs, the cost of inference appears to be high for Anthropic too, though it’s impossible to tell how much of its compute is based on training versus running inference.</p><p>In any case, these costs seem to increase with the amount of money Anthropic makes, meaning that the current pricing of both subscriptions and API access seems unprofitable, and must increase dramatically — from my calculations, a 100% price increase might work, but good luck retaining every single customer and their customers too! — for this company to ever become sustainable.&nbsp;</p><p>I don’t think that people would pay those prices. If anything, I think what we’re seeing in these numbers is a company bleeding out from costs that escalate the more that its user base grows. This is just my opinion, of course.&nbsp;</p><p>I’m tired of watching these companies burn billions of dollars to destroy our environment and steal from everybody. I’m tired that so many people have tried to pretend there’s a justification for burning billions of dollars every year, clinging to empty tropes about how <a href="https://www.wheresyoured.at/how-to-argue-with-an-ai-booster/#ultimate-booster-quip-openai-and-anthropic-are-%E2%80%9Cjust-like-uber%E2%80%9D-because-uber-burned-25-billion-over-the-course-of-15-or-so-years-and-is-now-profitable-this-proves-that-openai-a-totally-different-company-with-different-economics-will-be-fine"><u>this is just like Uber</u></a> or <a href="https://www.wheresyoured.at/the-haters-gui/#ed-amazon-web-services-took-years-to-become-profitable-people-said-amazon-would-fail"><u>Amazon Web Services</u></a>, when Anthropic has built something far more mediocre.&nbsp;</p><p>Mr. Amodei, I am sure you will read this piece, and I can make time to chat in person on my show Better Offline. Perhaps this Friday? I even have some studio time on the books.&nbsp;</p>
        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Commodore 64 Ultimate (138 pts)]]></title>
            <link>https://www.commodore.net/product-page/commodore-64-ultimate-basic-beige-batch1</link>
            <guid>45644654</guid>
            <pubDate>Mon, 20 Oct 2025 14:55:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.commodore.net/product-page/commodore-64-ultimate-basic-beige-batch1">https://www.commodore.net/product-page/commodore-64-ultimate-basic-beige-batch1</a>, See on <a href="https://news.ycombinator.com/item?id=45644654">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span><span>ex. tax</span></span></p><div id="comp-mcpbk490" data-testid="richTextElement"><h4>Preorder FAQ</h4>

<h3>Is this a Kickstarter?</h3>
<p>Not quite - this is an official Commodore® preorder with a money-back guarantee. Similar to crowdfunding, every penny goes into manufacturing first, and then to the mission to reboot Commodore itself. By skipping Kickstarter, we avoid big platform fees and pass the savings on to you - just like our founder Jack Tramiel would’ve wanted.&nbsp;</p>

<h3>Am I charged now or at shipping, and what currency?</h3>
<p>You’re charged now, in USD (your local currency shows until checkout). Like Kickstarter and others, your preorder funds production at the factory. The rest helps reboot the Commodore brand.</p>

<h3>Can I get a refund?</h3>
<p>Absolutely. You’re in control. Cancel anytime before shipping with our <strong>no quibble money-back guarantee</strong> for a full refund - no questions asked. Have an account? <a href="https://www.commodore.net/my-orders" target="_blank"><u>Cancel there</u></a> with one click. No account? Use our <a href="https://www.commodore.net/contact-us" target="_blank"><u>support form</u></a> and we’ll sort it fast.</p>

<h3>How does shipping work?</h3>
<p>We deliver from Commodore &amp; Commodore partner hubs in the USA, UK, and EU (including our original 1980s Corby building). If you’re farther out, we ship via trusted couriers like DHL, FedEx, UPS, or Chickenlips Express.</p>

<h3>Will I pay sales tax?</h3>
<p>USA: Sales tax is added at checkout where required.</p>
<p>UK &amp; Europe: VAT/IVA/MwSt is added at checkout.</p>

<h3>What about tariffs?</h3>
<p>If your country charges import tariffs (e.g. the USA), you’ll see a “Tariff Tax” at checkout. This covers everything upfront, so there’s nothing more to pay later. We don’t control these fees, but we avoid surprises. If tariffs drop after you pay, we’ll refund you at shipping. If they rise a lot before shipping, we may adjust- but only if truly needed to protect Commodore’s future.&nbsp;</p>

<h3>What about customs duties?</h3>
<p>We aim to prepay duties (DDP) where possible. In some places (e.g. Norway, Australia, New Zealand, Singapore), you might get a tax refund before shipping. Some countries may still charge customs fees on arrival - these aren’t included and as is commonplace, these are your responsibility.</p>

<h3>Isn't this just an emulator or rebadged something-or-other?</h3>
<p>The Commodore 64 Ultimate from the only original Commodore® brand (est. 1958) is brand new hardware-based Commodore 64 technology. It features SID chip-reactive LEDs (case, keyboard, power light*), the world's first transparent keyboard PCB*, original and modern creators’ autographs etched in copper, and an updated FPGA that replicates the original C64 motherboard (not emulation). All customisable via a new, easy main menu. It’s a fully authentic new build from Commodore - who else?</p>
<p><em>*except beige version</em></p>

<h3>Will Commodore 64 Ultimate units still ship if the acquisition doesn’t complete?</h3>
<p>Good news - as of 31st July 2025 we paid the sellers in full, ahead of schedule, and signed the final contracts to complete the acquisition. Even before that, we had a contract guaranteeing these machines would be made no matter what.</p>

<h3>So what are the risks?</h3>
<p>All launches have some risk, whether you're Apple or Commodore. But this preorder is unusually safe. Most parts are already in production. The motherboard is a proven design. Cases shipped in January 2025. Keycaps shipped in 2024. Just a few parts remain, like the keyboard base, made by a trusted partner since the Apple II era. All components are pro-designed to fit together. Add the box, manual, and power supply, and we’re set.</p>

<h3>Is there a warranty?</h3>
<p>Yes. Our products include a 1-year limited warranty covering defects in materials and workmanship. For customers in the EU or UK, your purchase also complies with local consumer protection laws, including the EU Consumer Rights Directive and UK Consumer Rights Act, which provide additional rights. You can also add an extended warranty <a href="https://www.commodore.net/product-page/commodore-warranty" target="_blank"><u>here</u></a>.</p>

<h3>Can I email/DM you to suggest a feature change?</h3>
<p>We’re not planning hardware changes right now. Chances are we already debated it over chickenlips snacks. We’re reviewing ideas from our launch form and will reach out if your Commodore 256 Ultra-Turbo-CD-Lightgun-Keyring makes the cut.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[BERT Is Just a Single Text Diffusion Step (412 pts)]]></title>
            <link>https://nathan.rs/posts/roberta-diffusion/</link>
            <guid>45644328</guid>
            <pubDate>Mon, 20 Oct 2025 14:31:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nathan.rs/posts/roberta-diffusion/">https://nathan.rs/posts/roberta-diffusion/</a>, See on <a href="https://news.ycombinator.com/item?id=45644328">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img alt="Text Diffusion" src="https://nathan.rs/images/roberta-diffusion.gif"></p><p>A while back, Google DeepMind unveiled <a href="https://deepmind.google/models/gemini-diffusion/">Gemini Diffusion</a>, an experimental language model that generates text using diffusion. Unlike traditional GPT-style models that generate one word at a time, Gemini Diffusion creates whole blocks of text by refining random noise step-by-step.</p><p>I read the paper <a href="https://arxiv.org/abs/2502.09992">Large Language Diffusion Models</a> and was surprised to find that discrete language diffusion is just a generalization of masked language modeling (MLM), something we’ve been doing since <a href="https://arxiv.org/abs/1810.04805">2018</a>.
The first thought I had was, “can we finetune a BERT-like model to do text generation?” I decided to try a quick proof of concept out of curiosity.</p><blockquote><p>NOTE: After I wrote the article I stumbled upon the paper <a href="https://arxiv.org/abs/2211.15029">DiffusionBERT</a> which does essentially the same thing but with more rigorous testing! Check it out if this post interested you.</p></blockquote><h2 id="a-short-history-of-transformers">A Short History of Transformers<a href="#a-short-history-of-transformers">#</a></h2><hr><p>The original Transformer architecture, introduced in <a href="https://arxiv.org/abs/1706.03762">2017</a>, was an encoder-decoder model. In 2018, researchers realized that the encoder and decoder components of the model could be separated (with the advent of <a href="https://arxiv.org/abs/1810.04805">BERT</a> and <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT</a>), and two distinct families of models were created:</p><ol><li><strong>Encoder-only models (BERT-style, bidirectional)</strong></li></ol><p>Encoder models used masked language modeling (MLM) as a training objective: randomly mask out a subset of tokens of each input and train the encoder to reconstruct the missing tokens (fill in the blanks).
The model sees the entire (partially masked) context at once and learns bidirectional representations.
This architecture excelled at tasks requiring a full‐sentence (or paragraph) representation (e.g., classification and retrieval).</p><ol start="2"><li><strong>Decoder-only models (GPT-style, autoregressive)</strong></li></ol><p>Decoder models used next‐token prediction as a training objective: at each position $t$, predict the token at position $t + 1$ given all tokens up to $t$ as context. Only the left context is used to predict future values (unidirectional).
This architecture excelled at generative tasks where you produce text one token at a time, such as open‐ended generation, summarization, and translation.</p><p>Originally, BERT saw immediate use in tasks such as classification, whereas GPT-style models didn’t become popular until later (due to initial limited capabilities). Eventually, the generation capabilities of autoregressive (decoder) transformers vastly improved. The general training objective of “next token prediction” means a much larger space of use cases when compared to encoder models.</p><h2 id="discrete-language-diffusion-models">Discrete Language Diffusion Models<a href="#discrete-language-diffusion-models">#</a></h2><hr><p>Diffusion models were first popularized in image generation. In image generation, diffusion models gradually add Gaussian noise to an image (forward process) and then train a neural network to iteratively denoise it (reverse process). A high‐level summary of continuous diffusion with images is:</p><ol><li><strong>Forward process</strong>: Start from a clean image <em>x₀</em>, then add small amounts of (usually Gaussian) noise at each timestep until you end up with near‐pure noise.</li><li><strong>Reverse process</strong>: Train a model (often a U‐Net) to predict the noise at each timestep, gradually recovering the original image in discrete denoising steps.</li></ol><p>Applying this idea to language means we need a way to add noise to text and then remove it in stages.
The simplest way to do this is a <strong>masking‐based noise process</strong>:</p><ol><li><p><strong>Forward (masking) process</strong>:</p><ul><li>At timestep <em>t = 0</em>, you have a fully uncorrupted text sequence.</li><li>At each subsequent timestep <em>t &gt; 0</em>, randomly replace a fraction of tokens with a special <code>&lt;MASK&gt;</code> token according to a pre‐defined schedule (e.g., gradually increasing the masked proportion from 0% to 100%).</li><li>By the final timestep <em>T</em>, the entire sequence may be masked (all tokens are <code>&lt;MASK&gt;</code>).</li></ul></li><li><p><strong>Reverse (denoising) process</strong>:</p><ul><li>Train a model (often a standard Transformer encoder) to predict the original token IDs given a partially masked sequence at timestep <em>t</em>.</li><li>This is akin to performing masked language modeling at varying mask rates: at early timesteps, only a few tokens are masked (easy to predict); at later timesteps, many tokens are masked (harder).</li><li>By chaining together predictions from high‐mask‐rate back down to zero, you can recover (or generate) a full sequence.</li></ul></li></ol><p>In this discrete text diffusion framework, the model learns a likelihood bound on the data distribution by optimizing a sum of denoising losses over all timesteps, rather than a single MLM objective at a fixed mask probability.</p><p>As we can see, BERT’s masked language modeling objective is the <strong>same training objective as text diffusion, but just for a subset of masking rates</strong>.
By introducing variable masking rates (from 0 to 1) and a scheduled sequence of denoising steps (inspired by diffusion theory), we can transform BERT’s masked language modeling objective into a full generative procedure.</p><h2 id="roberta-diffusion">RoBERTa Diffusion<a href="#roberta-diffusion">#</a></h2><hr><p>In 2019, <a href="https://arxiv.org/abs/1907.11692">RoBERTa</a> was released. It was essentially just an enhancement of the original BERT model, with better hyperparameters, data training size, and a more simple training objective (MLM only, removed next sentence prediction).</p><p>Here we use the HuggingFace <code>transformers</code> and <code>dataset</code> libraries to pull in the original RoBERTa weights, tokenizer, and the Trainer class to easily finetune the model on the WikiText dataset.
The main code (<a href="https://github.com/nathan-barry/RoBERTaDiffusion">full code here</a>) looks like this below:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Load and tokenize dataset and instantiate the model</span>
</span></span><span><span>dataset <span>=</span> load_dataset(<span>"wikitext"</span>, <span>"wikitext-2-raw-v1"</span>)
</span></span><span><span>tokenizer <span>=</span> RobertaTokenizerFast<span>.</span>from_pretrained(<span>"roberta-base"</span>)
</span></span><span><span>model <span>=</span> RobertaForMaskedLM<span>.</span>from_pretrained(<span>"roberta-base"</span>)
</span></span><span><span>
</span></span><span><span><span># Create the training args and Trainer instance</span>
</span></span><span><span>training_args <span>=</span> TrainingArguments(
</span></span><span><span>    output_dir<span>=</span><span>"finetuned-roberta-diffusion"</span>,
</span></span><span><span>    overwrite_output_dir<span>=</span><span>True</span>,
</span></span><span><span>    num_train_epochs<span>=</span>NUM_EPOCHS,
</span></span><span><span>    per_device_train_batch_size<span>=</span>BATCH_SIZE,
</span></span><span><span>    save_strategy<span>=</span><span>"epoch"</span>,
</span></span><span><span>    save_total_limit<span>=</span><span>1</span>,
</span></span><span><span>    logging_steps<span>=</span><span>200</span>,
</span></span><span><span>)
</span></span><span><span>
</span></span><span><span>trainer <span>=</span> Trainer(
</span></span><span><span>    model<span>=</span>model,
</span></span><span><span>    args<span>=</span>training_args,
</span></span><span><span>    train_dataset<span>=</span>tokenized[<span>"train"</span>],
</span></span><span><span>    eval_dataset<span>=</span>tokenized[<span>"validation"</span>],
</span></span><span><span>    data_collator<span>=</span>diffusion_collator, <span># custom implementation</span>
</span></span><span><span>    tokenizer<span>=</span>tokenizer,
</span></span><span><span>)
</span></span><span><span>
</span></span><span><span><span># Train &amp; save</span>
</span></span><span><span>trainer<span>.</span>train()
</span></span><span><span>trainer<span>.</span>save_model(<span>"finetuned-roberta-diffusion"</span>)</span></span></code></pre></div><p>Currently we have 10 diffusion steps, so we randomly sample a percentage $p$ out of <code>mask_probs</code> (1.0, 0.9, 0.9, …, 0.1) and mask that percent of the tokens each batch.
The custom <code>diffusion_collator</code> function (<a href="https://github.com/nathan-barry/RoBERTaDiffusion/blob/main/finetune.py#L77">see code here</a>) samples one mask-probability <code>p</code> from <code>mask_probs</code> per batch and sets each token to <code>&lt;MASK&gt;</code> with <code>p</code> probability.</p><p>To be able to condition the generation on a “prompt”, we currently never mask the first 16 tokens. That means that during training, each step will always have the first 16 tokens as context for generation.</p><p>Simplified code for the <code>diffusion_collator</code> looks like:</p><div><pre tabindex="0"><code data-lang="python"><span><span>  <span>def</span> <span>diffusion_collator</span>(examples):
</span></span><span><span>      batch <span>=</span> tokenizer<span>.</span>pad(examples, return_tensors<span>=</span><span>"pt"</span>)
</span></span><span><span>
</span></span><span><span>      <span># Randomly select masking probability for this batch</span>
</span></span><span><span>      mask_prob <span>=</span> random<span>.</span>choice([<span>1.0</span>, <span>0.9</span>, <span>0.8</span>, <span>0.7</span>, <span>0.6</span>, <span>0.5</span>, <span>0.4</span>, <span>0.3</span>, <span>0.2</span>, <span>0.1</span>])
</span></span><span><span>
</span></span><span><span>      <span># Never mask the first PREFIX_LEN tokens (preserved context)</span>
</span></span><span><span>      maskable_positions <span>=</span> batch<span>.</span>input_ids[:, PREFIX_LEN:]
</span></span><span><span>
</span></span><span><span>      <span># Create random mask for the chosen probability</span>
</span></span><span><span>      mask <span>=</span> torch<span>.</span>rand(maskable_positions<span>.</span>shape) <span>&lt;</span> mask_prob
</span></span><span><span>
</span></span><span><span>      <span># Apply masking</span>
</span></span><span><span>      batch<span>.</span>input_ids[:, PREFIX_LEN:][mask] <span>=</span> tokenizer<span>.</span>mask_token_id
</span></span><span><span>      batch<span>.</span>labels <span>=</span> batch<span>.</span>input_ids<span>.</span>clone()
</span></span><span><span>
</span></span><span><span>      <span>return</span> batch</span></span></code></pre></div><p>For inference, we start with an input which is a tensor of size 256 (since we are generating blocks of 256 tokens). The first 16 positions are the token ids that correspond to the prompt, and the last 240 are just <code>&lt;MASK&gt;</code> tokens. We iterate through the denoising schedule and each step, we generate a prediction and then remask the sequence again. The process looks like this:</p><pre tabindex="0"><code>Step 0: [PREFIX] &lt;mask&gt; &lt;mask&gt; &lt;mask&gt; &lt;mask&gt; &lt;mask&gt; ...     (100% masked)
Step 1: [PREFIX] will &lt;mask&gt; over &lt;mask&gt; control ...        (90% masked)
Step 2: [PREFIX] will begin &lt;mask&gt; greater control ...      (80% masked)
...
Step 10: [PREFIX] will begin to assert greater control ...  (0% masked - DONE)</code></pre><p>Simplified code for generation looks like:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Generate text through iterative denoising</span>
</span></span><span><span><span>for</span> step, mask_prob <span>in</span> <span>enumerate</span>(mask_probs):
</span></span><span><span>    <span># Forward pass: predict masked tokens</span>
</span></span><span><span>    <span>with</span> torch<span>.</span>no_grad():
</span></span><span><span>        outputs <span>=</span> model(input_ids<span>=</span>input_ids, attention_mask<span>=</span>attention_mask)
</span></span><span><span>        predictions <span>=</span> outputs<span>.</span>logits  <span># shape: (1, MAX_LEN, vocab_size)</span>
</span></span><span><span>
</span></span><span><span>    <span># For each masked position, sample from top-k/top-p filtered distribution</span>
</span></span><span><span>    <span>for</span> pos <span>in</span> <span>range</span>(PREFIX_LEN, MAX_LEN):
</span></span><span><span>        <span>if</span> input_ids[<span>0</span>, pos] <span>==</span> tokenizer<span>.</span>mask_token_id:
</span></span><span><span>            logits <span>=</span> predictions[<span>0</span>, pos, :]
</span></span><span><span>            <span># Apply top-k and top-p filtering</span>
</span></span><span><span>            filtered_logits <span>=</span> top_k_top_p_filtering(logits, top_k<span>=</span>TOP_K, top_p<span>=</span>TOP_P)
</span></span><span><span>            probs <span>=</span> F<span>.</span>softmax(filtered_logits, dim<span>=-</span><span>1</span>)
</span></span><span><span>            <span># Sample token</span>
</span></span><span><span>            sampled_token <span>=</span> torch<span>.</span>multinomial(probs, <span>1</span>)
</span></span><span><span>            input_ids[<span>0</span>, pos] <span>=</span> sampled_token
</span></span><span><span>
</span></span><span><span>    <span># Re-mask a portion of non-prefix tokens for next iteration</span>
</span></span><span><span>    <span>if</span> mask_prob <span>&gt;</span> <span>0</span>:
</span></span><span><span>        mask_indices <span>=</span> torch<span>.</span>rand(MAX_LEN <span>-</span> PREFIX_LEN) <span>&lt;</span> mask_prob
</span></span><span><span>        input_ids[<span>0</span>, PREFIX_LEN:][mask_indices] <span>=</span> tokenizer<span>.</span>mask_token_id</span></span></code></pre></div><p>Here is an example output generation of the fine-tuned model after training on an H200 for 30 minutes (the first line is the initial prompt):</p><pre tabindex="0"><code>Following their victory in the French and Indian War, Britain began to assert
greater...

...dominion over Europe beginning about the early 19th. There conflict took
place on the island, between British and Irish Ireland. British officials 
administered British Ireland, a Celtic empire under the control of the Irish 
nationalist authorities, defined as a dominion of Britain. As the newly Fortic 
states acquired independent and powerful status, many former English colonies
played their part in this new, British @-@ controlled colonial system. Following
this period the Non @-@ Parliamentaryist Party won its influence in Britain in 
1890, led by the support of settlers from the Irish colonies. Looking inwards, 
Sinclair, Lewis questioned, and debated the need to describe " The New Britain "</code></pre><p>The output looks surprisingly coherent! Most of the quirks present are actually just quirks from the formatting of WikiText (spaces around punctuation <code>"</code>, turning hyphens <code>-</code> into <code>@-@</code>).</p><p>Below is a comparison between our diffusion model and GPT-2:</p><p><img alt="RoBERTa Diffusion vs GPT" src="https://nathan.rs/images/roberta-diffusion-gpt.gif"></p><p>We see GPT-2’s output is more coherent and slightly faster (~9 seconds vs ~13) but I’m pleasantly surprised with how good my simple implementation was. It is a good proof of concept, and with new approaches like AR-Diffusion and Skip-Step Diffusion (and a more optimized implementation), the quality and speed can be drastically improved.</p><h2 id="conclusion">Conclusion<a href="#conclusion">#</a></h2><hr><p>We’ve seen that masked language models like RoBERTa, originally designed for fill-in-the-blank tasks, can be repurposed into fully generative engines by interpreting variable-rate masking as a discrete diffusion process. By gradually corrupting text with <code>&lt;MASK&gt;</code> tokens and training the model to iteratively denoise at increasing mask intensities, we effectively turn the standard MLM objective into a step-by-step generation procedure.</p><p>Even without architectural changes, a fine-tuned RoBERTa can generate coherent looking text after slightly modifying the training objective, validating the idea that BERT-style models are essentially just text diffusion models trained on one masking rate.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The FTC Is Disappearing Blog Posts About AI Published During Lina Khan's Tenure (111 pts)]]></title>
            <link>https://www.wired.com/story/ftc-removes-blog-posts-about-ai-authored-by-by-lina-khan/</link>
            <guid>45643776</guid>
            <pubDate>Mon, 20 Oct 2025 13:38:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/ftc-removes-blog-posts-about-ai-authored-by-by-lina-khan/">https://www.wired.com/story/ftc-removes-blog-posts-about-ai-authored-by-by-lina-khan/</a>, See on <a href="https://news.ycombinator.com/item?id=45643776">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><p><span>In late July</span> 2024, <a href="https://www.wired.com/story/ftc-chair-lina-khans-democrats-donors-harris/">Lina Khan</a>, then the chair of the US Federal Trade Commission, gave a speech at an event hosted by the San Francisco startup accelerator Y Combinator in which she <a href="https://www.wired.com/story/open-source-ai-y-combinator/">positioned herself</a> as an advocate for open source artificial intelligence.</p><p>The event took place as California lawmakers were considering a landmark bill called SB 1047 that would have imposed new testing and safety requirements on AI companies. Critics of the legislation, which was later vetoed by California governor Gavin Newsom, argued it would hamper the development and release of open source AI models. Khan called for a less restrictive approach and said that, with open models available to them, “smaller players can bring their ideas to market.”</p><p>In the days leading up to the event, Khan’s staff published a blog on the agency’s website emphasizing similar talking points. The piece noted that “open source” had been used to describe AI models with a variety of different characteristics. The authors instead suggested adopting the term “open-weight,” meaning a model that has its <a href="https://www.wired.com/story/openai-just-released-its-first-open-weight-models-since-gpt-2/">training weights</a> released publicly, allowing anyone to inspect, modify, or reuse it.</p><p>The Trump administration has since removed that blog post, two sources familiar with the matter tell WIRED. The Internet Archive’s Wayback Machine shows that the July 10, 2024, FTC blog titled “On Open-Weights Foundation Models” <a href="https://web.archive.org/web/20250000000000*/https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/2024/07/open-weights-foundation-models">was redirected</a> on September 1 of this year to a landing page for the FTC’s Office of Technology.</p><p>Another post from October 2023 titled “Consumers Are Voicing Concerns About AI,” authored by two FTC technologists, now similarly redirects back to the agency’s Office of Technology landing page. According to the Wayback Machine, the <a href="https://web.archive.org/web/20250000000000*/https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/2023/10/consumers-are-voicing-concerns-about-ai">redirect occurred</a> in late August of this year.</p><p>A third FTC post about AI that was authored by Khan’s staff and published on January 3, 2025, titled “AI and the Risk of Consumer Harm,” now leads to an error screen that says “Page not found.” According to the Wayback Machine, that blog post was still live on the FTC’s website as of August 12, but by August 15 it had been removed from the internet. In the original post, Khan’s staff had written that the agency was “increasingly taking note of AI’s potential for real-world instances of harm—from incentivizing commercial surveillance to enabling fraud and impersonation to perpetuating illegal discrimination.”</p><p>It’s not clear why the blog posts were removed from the internet. An FTC spokesperson did not respond to a request for comment. Khan, through a spokesperson, declined to comment.</p></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><p>Former FTC public affairs director Douglas Farrar tells WIRED he was especially surprised that the open weights blog was removed, given the agency’s role as a key AI market regulator. “I was shocked to see the Ferguson FTC be so out of line with the Trump White House on this signal to the market,” he says, referring to newly appointed FTC chair Andrew Ferguson.</p><p>The <a href="https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf">Trump administration’s AI Action Plan</a> from July argues that “we need to ensure America has leading open models founded on American values” and “the Federal government should create a supportive environment for open models.” (The FTC did not respond to questions about whether these deletions represent a shift in policy.)</p><p>Several advisers working for the Trump administration on technology issues, including David Sacks, special adviser to the White House on AI and crypto, and Sriram Krishnan, a senior policy adviser to the White House on AI, have also advocated for open source AI, framing it as a critical means for the US to maintain its technological dominance.</p><p>Since President Trump returned to the White House in January, the FTC has removed hundreds of blogs and business guidance for the tech industry published during Khan’s tenure, <a href="https://www.wired.com/story/federal-trade-commission-removed-blogs-critical-of-ai-amazon-microsoft/">WIRED previously reported</a>.</p><p>In March, the FTC removed some 300 posts related to AI, consumer protection, and the agency’s lawsuits against tech giants like Amazon and Microsoft. One post titled “<a data-offer-url="https://archive.is/42RSh" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://archive.is/42RSh&quot;}" href="https://archive.is/42RSh" rel="nofollow noopener" target="_blank">The Luring Test: AI and the engineering of consumer trust</a>” offered guidance to tech companies about how to avoid building deceptive AI chatbots. The blog post had <a data-offer-url="https://techprimers.aspendigital.org/hall-of-fame/2023-05-01/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://techprimers.aspendigital.org/hall-of-fame/2023-05-01/&quot;}" href="https://techprimers.aspendigital.org/hall-of-fame/2023-05-01/" rel="nofollow noopener" target="_blank">won an award</a> from the Aspen Institute in 2023 for its accessible descriptions of artificial intelligence.</p><p>An FTC source told WIRED in March that removing public blog posts “raises serious compliance concerns under the Federal Records Act and the Open Government Data Act,” which require government agencies to preserve records that have administrative, legal, or historical value and make them accessible to the public. During the Biden administration, FTC leadership placed “warning” labels on business directives and other guidance published during previous administrations that it disagreed with.</p></div><div data-journey-hook="grid-wrapper" data-testid="BodyWrapper"><p>More than 200 posts and statements authored by Khan herself were still available on the FTC’s website at the time of publication. This includes a September 2024 blog on <a href="https://www.ftc.gov/news-events/news/press-releases/2024/09/ftc-announces-crackdown-deceptive-ai-claims-schemes">enforcement actions</a> the agency took against allegedly deceptive AI schemes, a 2024 <a href="https://www.ftc.gov/legal-library/browse/joint-statement-competition-generative-ai-foundation-models-ai-products">joint statement</a> from the FTC and other groups on competition in the market for generative AI foundation models, and remarks from <a href="https://www.ftc.gov/system/files/ftc_gov/pdf/remarks-chair-khan-at-the-creative-economy-and-ai-roundtable.pdf">a 2023 roundtable on generative AI</a>, in which Khan said the agency was “looking closely at how AI can turbocharge fraud” and “entrench the dominance of the firms that control necessary raw inputs,” among other harms.</p></div></div></div>]]></description>
        </item>
    </channel>
</rss>