<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 15 Jun 2025 00:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Seven replies to the viral Apple reasoning paper and why they fall short (179 pts)]]></title>
            <link>https://garymarcus.substack.com/p/seven-replies-to-the-viral-apple</link>
            <guid>44278403</guid>
            <pubDate>Sat, 14 Jun 2025 19:52:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://garymarcus.substack.com/p/seven-replies-to-the-viral-apple">https://garymarcus.substack.com/p/seven-replies-to-the-viral-apple</a>, See on <a href="https://news.ycombinator.com/item?id=44278403">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>The Apple paper on </span><a href="https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf" rel="">limitations in the “reasoning” of Large Reasoning Models</a><span>, which raised challenges for the latest scaling hypothesis, has clearly touched a nerve. Tons of media outlets covered it; huge numbers of people on social media are discussing. </span></p><p><span>M</span><a href="https://open.substack.com/pub/garymarcus/p/a-knockout-blow-for-llms?r=8tdk6&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false" rel="">y own post here laying out the Apple paper in historical and scientific context </a><span>was so popular that well over 150,000 people read it, biggest in this newsletter’s history. </span><em>The Guardian</em><span> published an adaptation of my post (“</span><a href="https://www.theguardian.com/commentisfree/2025/jun/10/billion-dollar-ai-puzzle-break-down" rel="">When billion-dollar AIs break down over puzzles a child can do, it’s time to rethink the hype</a><span>”) The editor tells me readers spent a long time reading it, notably longer than usual, as if people really wanted to understand the arguments in detail.  (The ACM computer science society is reposting the essay, too, and </span><a href="https://legrandcontinent.eu/fr/2025/06/10/ia-llm-marcus/" rel="">there is now a French version as well</a><span>).</span></p><p>Tons of GenAI optimists took cracks at the Apple paper (see below), and it is worth considering their arguments. Overall I have seen roughly seven different efforts at rebuttal, ranging from nitpicking and ad hominem to the genuinely clever. Most (not all) are based on grains of truth, but are any of them actually compelling?</p><p> Let’s consider.</p><ol><li><p><strong>Humans have trouble with complex problems and memory demands</strong><span>. True! But incomplete.  We have every right to expect machines to do things we can’t. Cars have more stamina, calculators don’t make arithmetical errors. That’s why we invented computers: to do repetitive calculation without errors. And in many cases (including the Tower of Hanoi, which featured prominently in the paper)) we have existing systems that work perfectly without errors. AGI should be a step forward; in many cases LLMs are a step backwards. And note the bait and switch from “we’re going to build AGI that can revolutionize the world” to “give us some credit, our systems make errors and humans do, too”. The real takeaway from the Apple paper is that LLMs can’t be trusted to run algorithms as complexity and distance from the training distribution grows (just as humans shouldn’t serve as calculators). If we want to get to AGI, we will have to better.</span></p></li><li><p><strong>The Large Reasoning Models (LRMs) couldn’t possibly solve the problem, because the outputs would require too many output tokens</strong><span> (which is to say the correct answer would be too long for the LRMs to produce). Partial truth, and a clever observation: LRMs (which are enhanced LLMs) have a shortcoming, which is a limit on how long their outputs can be. The correct answer to Tower of Hanoi with 12 moves would be too long for some LRMs to spit out, and the authors should have addressed that.  But crucially (i) this objection, clever as it is, doesn’t actually explain the overall pattern of results. The LRMs failed on  Tower of Hanoi with 8 discs, where the optimal solution is 255 moves, well within so-called token limits; (ii) well-written symbolic AI systems generally don’t suffer from this problem, and AGI should not either. The length limit on LLM is a bug, and most certainly not a feature. And look, if an LLM can’t reliably execute something as basic as Hanoi, what makes you think it is going to compute military strategy (especially with the fog of war) or molecular biology (with many unknowns) correctly? What the Apple team asked for was way easier than what the real world often demands.</span></p></li><li><p><strong>The paper was written by an intern</strong><span>. This one, which infuriated me because is a form of ad hominem argument rather than substance, is misguided and only barely true – and utterly lacking in context. It is </span><em>true</em><span> that the first author was an intern at Apple, Parshin Shojaee, but (i) she (not </span><em>he</em><span> as some fool I won’t name presumed, without spending two seconds of research) also happens to be </span><a href="https://parshinsh.github.io/" rel="">a very promising third year Ph.D. student with many paper presentations at leading conferences</a><span>, (ii) the article, if you actually read it, makes it clear she shared lead responsibility with </span><a href="https://imirzadeh.me/" rel="">Iman Mirzadeh</a><span>, who has a Ph.D.;  (iii) the paper actually has six authors, not one, and four have Ph.D.s; for good measure one is Yoshua Bengio’s brother </span><a href="https://bengio.abracadoudou.com/" rel="">Samy Bengio</a><span>, well-known and very distinguished in his own right within the machine learning community; (iv) it is a common practice in many scientific fields to put the junior author first, senior author last, as this paper did; thousands of major articles have done the same (and never been criticized for doing so — it’s a true desperation measure); (v) what really matters is the quality of the work. Alfred Sturtevant was an </span><em>undergraduate</em><span> when he invented gene maps. </span></p></li><li><p><strong>Bigger models might to do better</strong><span>. True, and this is always the case (I have seen one report that o3-pro can do at least one of the problems, at least some of the time; I have not seen a thorough study yet).  Bigger models sometimes do better because of genuine improvements in the model, sometimes because of problem-specific training. (From the outside we can never know which). But here’s the thing, we can’t know in advance what model is big enough (if any) for any given problem. And Apple’s result that some pretty large models could succeed at 6 discs, giving the illusion of mastery, but collapse by 8, is ominous.  One is left simply having to test everything, all the time, with little guarantees of anything. Some model might be big enough for task T of size S and fail on the next size, or on Task T’ that is slightly different, etc. It all becomes a crapshoot. Not good.</span></p></li><li><p><strong>The systems can solve the puzzles with code.</strong><span> True in some cases, and a huge win for neurosymbolic AI, given that they can’t reliably solve the puzzles without code, and given that code is symbolic.  </span><em>Huge</em><span> vindication for what I have been saying all along: we need AI that integrates both neural networks and symbolic algorithms and representations (such as logic, code, knowledge graphs, etc). But also, we need to do so reliably, and in a general way and we haven’t yet crossed that threshold. (Importantly, the point of the Apple paper goal was to see how LRM’s unaided explore a space of solutions via reasoning and backtracking, not see how well it could use preexisting code retrieved from the web.) An analogy: A student might complain about a math exam requiring integration or differentiation by hand, even though math software can produce the correct answer instantly. The teacher’s goal in assigning the problem, though, isn’t finding the answer to that question (presumably the teacher already know the answer),  but to assess the student’s conceptual understanding. Do LLM’s </span><em>conceptually</em><span> understand Hanoi?  That’s what the Apple team was getting at. (Can LLMs download the right code? Sure. But downloading code without conceptual understanding  is of less help in the case of new problems, dynamically changing environments, and so on.)</span></p></li><li><p><strong>The paper has only four examples, and at least one of them (Hanoi) isn’t perfect</strong><span>. I doubt any of them are perfect, but the four together provide converging evidence with dozens of other prior papers (including some of my own), and I am confident many more examples will be uncovered.  I already found a couple of analogous failures in algorithm application myself, which I will write about in a few days. Tal Linzen at NYU </span><a href="https://x.com/tallinzen/status/1933184078821360084?s=61" rel="">just published yet another example</a><span>, with “</span><em>models .. able to do the right thing for simple versions of [a language] problem (small grammars, short strings), but [with] accuracy drop[ping] very quickly as the problem becomes more complex</em><span>.” Mark my words: in time, we will see scores of papers reinforcing the Apple results. </span></p></li><li><p><strong>The paper is not news; we already knew these models generalize poorly</strong><span>. True! (I personally have been trying to tell people this for almost thirty years; Subbarao Rao Kambhampati has been trying his best, too). But then why do we think these models are the royal road to AGI? The real news here, aside from the fact that this was a clever study nailing down an important point, is that </span><em>people are finally starting to pay attention,</em><span> to (one of the) two biggest Achilles’ Heels of generative AI, and to appreciate its significance. (Tune in to this newsletter on the weekend to hear about the other.)  Hilarious, by the way, is hearing both “it’s wrong” and “we knew it all along”, simultaneously. In at least one case I saw a single person say both, minutes apart!</span></p></li></ol><p>Bottom line? None of the rejoinders are compelling. If people like Sam Altman are sweating, it’s because they should. The Apple paper is yet another clear sign that scaling is not the answer; for once, people are paying attention. </p><p>§</p><p>The kicker? A Salesforce paper also just posted, that many people missed:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg" width="1251" height="1791" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1791,&quot;width&quot;:1251,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:523217,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://garymarcus.substack.com/i/165791446?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>In the “multi-turn” condition, which presumably would require reasoning and algorithmic precision, performance was only 35%. </p><p>Talk about convergence evidence. Taking the SalesForce report together with the Apple paper, it’s clear the current tech is not to be trusted.</p><p><em><strong>Gary Marcus</strong><span>, professor emeritus at NYU, and author of The Algebraic Mind and “Deep learning is hitting a wall”, both of which anticipated the correct results, is thrilled to see people finally realize that scaling is not enough to get us to AGI. Now perhaps we can begin to build better AI.</span></em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Waymo's market share in San Francisco exceeds Lyft's (119 pts)]]></title>
            <link>https://underscoresf.com/in-san-francisco-waymo-has-now-bested-lyft-uber-is-next/</link>
            <guid>44277355</guid>
            <pubDate>Sat, 14 Jun 2025 16:44:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://underscoresf.com/in-san-francisco-waymo-has-now-bested-lyft-uber-is-next/">https://underscoresf.com/in-san-francisco-waymo-has-now-bested-lyft-uber-is-next/</a>, See on <a href="https://news.ycombinator.com/item?id=44277355">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="primary"><article id="post-141765"><div><p><img width="810" height="608" src="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?fit=810%2C608&amp;ssl=1" alt="" decoding="async" srcset="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?w=5356&amp;ssl=1 5356w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?resize=1500%2C1125&amp;ssl=1 1500w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?resize=768%2C576&amp;ssl=1 768w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?resize=1536%2C1152&amp;ssl=1 1536w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?resize=2048%2C1536&amp;ssl=1 2048w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?w=1620&amp;ssl=1 1620w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?w=2430&amp;ssl=1 2430w" sizes="(max-width: 810px) 100vw, 810px" data-attachment-id="141769" data-permalink="https://underscoresf.com/in-san-francisco-waymo-has-now-bested-lyft-uber-is-next/san-francisco-ca-usa-california-street-autonomes-fahrzeug/" data-orig-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?fit=5356%2C4017&amp;ssl=1" data-orig-size="5356,4017" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;8&quot;,&quot;credit&quot;:&quot;Dietmar Rabich&quot;,&quot;camera&quot;:&quot;Canon EOS 5D Mark IV&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1656635759&quot;,&quot;copyright&quot;:&quot;Dietmar Rabich, D\u00fclmen&quot;,&quot;focal_length&quot;:&quot;105&quot;,&quot;iso&quot;:&quot;100&quot;,&quot;shutter_speed&quot;:&quot;0.005&quot;,&quot;title&quot;:&quot;San Francisco (CA, USA), California Street, autonomes Fahrzeug (&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="San Francisco (CA, USA), California Street, autonomes Fahrzeug (" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?fit=300%2C225&amp;ssl=1" data-large-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?fit=810%2C608&amp;ssl=1"></p><h2><span>If a data-backed trend plays out, Waymo could become San Francisco’s biggest ride-hailing service before the year ends.</span></h2><p><a href="https://www.wsj.com/tech/waymo-cars-self-driving-robotaxi-tesla-uber-0777f570?gaa_at=eafs&amp;gaa_n=ASWzDAhnWF-iL0nB_VD1P9sE3oEvTzxfPRciScEXu2atNMFFerpPTdeL9vKlknl-ljg%3D&amp;gaa_ts=684048ca&amp;gaa_sig=T-9NFT-D_HBZgQzoyvZnU-0sp1-gS4FSVJMIo57FFlhxVVVMK8UFbvp6Eu_ndwTO23KoDxeZ9sZoC09rHkiDCg%3D%3D"><span>Waymo’s celestial ascent into the cultural zeitgeist</span></a><span> — a rise that has been propelled by dystopian memes and sheer, futuristic novelty — has only been matched by how it continues swallowing its competition. The Alphabet-owned autonomous driving company saw explosive exponential growth in ridership in 2024, with driverless rides increasing from </span><a href="https://underscoresf.com/self-driving-world-record-was-quietly-broken-in-sf-bay-area-last-week/"><span>77,000 to more than 312,000 lifts</span></a><span> by August of last year alone, according to the California DMV; as of publishing, Waymo asserts that 30% of their rides are to local small businesses.</span></p><figure id="attachment_141767" aria-describedby="caption-attachment-141767"><img data-recalc-dims="1" decoding="async" data-attachment-id="141767" data-permalink="https://underscoresf.com/in-san-francisco-waymo-has-now-bested-lyft-uber-is-next/502321336_17879766099336220_8766268074646908687_n/" data-orig-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?fit=1236%2C919&amp;ssl=1" data-orig-size="1236,919" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="502321336_17879766099336220_8766268074646908687_n" data-image-description="<p>Screenshot: Courtesy of YipitData</p>
" data-image-caption="" data-medium-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?fit=300%2C223&amp;ssl=1" data-large-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?fit=810%2C602&amp;ssl=1" src="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?resize=810%2C602&amp;ssl=1" alt="" width="810" height="602" srcset="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?w=1236&amp;ssl=1 1236w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?resize=300%2C223&amp;ssl=1 300w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?resize=768%2C571&amp;ssl=1 768w" sizes="(max-width: 810px) 100vw, 810px"><figcaption id="caption-attachment-141767">Screenshot: Courtesy of YipitData</figcaption></figure><p><span>Independent contractors for Lyft and Uber have been </span><a href="https://www.theregister.com/2024/09/07/uber_driver_waymo/"><span>saying they’re “cooked”</span></a><span> for a while, citing massive declines in available requests as a result of Waymo’s success. (This, however, is a tandem issue: Waymo’s ride-hailing operations in San Francisco coincided with the increased number of regional rideshare drivers that began working during and after the pandemic.) But now factual data is showing that the aforementioned broiling is, indeed, happening … and at a rate quicker than once thought.</span></p><p><span>According to YipitData, a data and analytics firm based out of New York City, Waymo’s gross bookings from August of 2023 to April of this year have surpassed Lyft’s in market share. The twenty-month data analysis highlighted Uber’s dominance in San Francisco ridership — well over 50% of all trips booked via a ridesharing application were done on Uber throughout the analysis — but showed, perhaps more surprisingly, how quickly Waymo clambered into the commonplace. Waymo is also currently beating Lyft, a company that has operated rides in San Francisco since 2012, in total gross bookings.&nbsp;</span></p><p><span>In a staggeringly short amount of time, Waymo, which is about to celebrate its first anniversary of city-wide ride-hailing operations, has gone from effectively 0% market share of bookings in San Francisco to over 25%. Lyft has continuously gathered fewer bookings than Uber, but still managed to hold onto a roughly 30% market share since 2023. </span><span><br> </span><span><br> </span><span>Waymo has now flipped that stake, surpassing Lyft to become San Francisco’s second most-popular ride-hailing service. If the research published by YipitData is extrapolated outwardly, Waymo could easily beat Uber to become SF’s foremost taxi-like service by early next year. Or sooner.</span></p><p><span>What does this mean for San Francisco, the city that launched ridesharing services as we know them today? On the roads, not much; self-driving Jaguar iPaces would become even more prominent. But on an economic level, a subset of blue-collar workers (which numbers in the tens of thousands in San Francisco) would find themselves either regionally displaced or outright vocationally exterminated by a branch of artificial intelligence.&nbsp;</span></p><p><span>We don’t need a graph to tell you how that window into a </span><a href="https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic"><span>looming dystopian landscape plays out.</span></a><span> But hey, at least you won’t have to surrender to mind-numbing small talk on </span><a href="https://missionlocal.org/2025/03/sf-waymo-sfo-airport-robotaxis-autonomous-vehicles-teamsters/"><span>your way to SFO.</span></a></p><hr><p><em>Photo: Courtesy of Wikimedia Commons</em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Inside the Apollo "8-Ball" FDAI (Flight Director / Attitude Indicator) (114 pts)]]></title>
            <link>https://www.righto.com/2025/06/inside-apollo-fdai.html</link>
            <guid>44277051</guid>
            <pubDate>Sat, 14 Jun 2025 15:43:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.righto.com/2025/06/inside-apollo-fdai.html">https://www.righto.com/2025/06/inside-apollo-fdai.html</a>, See on <a href="https://news.ycombinator.com/item?id=44277051">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-2250215887377196568" itemprop="description articleBody">
<p>During the Apollo flights to the Moon, the astronauts observed the spacecraft's orientation on a special instrument
called the FDAI (Flight Director / Attitude Indicator).
This instrument showed the spacecraft's attitude—its orientation—by rotating a ball.
This ball was nicknamed the "8-ball" because it was black (albeit only on one side).
The instrument also acted as a flight director, using three yellow needles to indicate how the astronauts should maneuver
the spacecraft. Three more pointers showed how fast the spacecraft was rotating.</p>
<p><a href="https://static.righto.com/images/fdai/fdai-opened.jpg"><img alt="An Apollo FDAI (Flight Director/Attitude Indicator) with the case removed. This FDAI is on its side to avoid crushing the needles." height="511" src="https://static.righto.com/images/fdai/fdai-opened-w500.jpg" title="An Apollo FDAI (Flight Director/Attitude Indicator) with the case removed. This FDAI is on its side to avoid crushing the needles." width="500"></a></p><p>An Apollo FDAI (Flight Director/Attitude Indicator) with the case removed. This FDAI is on its side to avoid crushing the needles.</p>
<p>Since the spacecraft rotates along three axes (roll, pitch, and yaw), the ball also rotates along three axes.
It's not obvious how the ball can rotate to an arbitrary orientation while remaining attached.
In this article, I look inside an FDAI from Apollo that was repurposed for a Space Shuttle simulator<span id="fnref:simulator"><a href="#fn:simulator">1</a></span> and explain how it operates. (Spoiler: the ball mechanism is firmly attached
at the "equator" and rotates in two axes. What you see is two hollow shells around the ball mechanism that spin around the third axis.)</p>

<p>For the missions to the Moon, the Lunar Module had two FDAIs, as shown below: one on the left for the Commander (Neil Armstrong in Apollo 11) and
one on the right for the Lunar Module Pilot (Buzz Aldrin in Apollo 11).
With their size and central positions, the FDAIs dominate the instrument panel, a sign of their importance.
(The Command Module for Apollo also had two FDAIs, but with a different design; I won't discuss them here.<span id="fnref:honeywell"><a href="#fn:honeywell">2</a></span>)</p>
<p><a href="https://static.righto.com/images/fdai/lm-panel.jpg"><img alt="The instrument panel in the Lunar Module. From Apollo 15 Lunar Module, NASA, S71-40761. If you're looking for the DSKY, it is in the bottom center, just out of the picture." height="500" src="https://static.righto.com/images/fdai/lm-panel-w600.jpg" title="The instrument panel in the Lunar Module. From Apollo 15 Lunar Module, NASA, S71-40761. If you're looking for the DSKY, it is in the bottom center, just out of the picture." width="600"></a></p><p>The instrument panel in the Lunar Module. From <a href="https://archive.org/details/S71-40761">Apollo 15 Lunar Module</a>, NASA, S71-40761. If you're looking for the DSKY, it is in the bottom center, just out of the picture.</p>
<p>Each Lunar Module FDAI could display inputs from multiple sources, selected by switches on the panel.<span id="fnref:lm-fdai"><a href="#fn:lm-fdai">3</a></span> The ball could display attitude from either the
Inertial Measurement Unit
or from the backup Abort Guidance System, selected by the "ATTITUDE MON" toggle switch next to either FDAI.
The pitch attitude could also be supplied by an electromechanical unit called ORDEAL (Orbital Rate Display Earth And Lunar)
that simulates a circular orbit.
The error indications came from the Apollo Guidance Computer, the Abort Guidance System, the landing radar,
or the rendezvous radar (controlled by the "RATE/ERROR MON" switches).
The pitch, roll, and yaw rate displays were driven by the Rate Gyro Assembly (RGA).
The rate indications were scaled by a switch below the FDAI, selecting 25°/sec or 5°/sec.</p>
<h2>The FDAI mechanism</h2>
<p>The ball inside the indicator shows rotation around three axes.
I'll first explain these axes in the context of an aircraft, since the axes of a spacecraft are more arbitrary.<span id="fnref:axes"><a href="#fn:axes">4</a></span>
The roll axis indicates the aircraft's angle if it rolls side-to-side along its axis of flight, raising one wing
and lowering the other.
Thus, the indicator shows the tilt of the horizon as the aircraft rolls.
The pitch axis indicates the aircraft's angle if it pitches up or down, with the indicator showing the horizon
moving down or up in response.
Finally, the yaw axis indicates the compass direction that the aircraft is heading,
changing as the aircraft turns left or right.
(A typical aircraft attitude indicator omits yaw.)</p>
<p>I'll illustrate how the FDAI rotates the ball in three axes, using an orange as an example.
Imagine pinching the horizontal axis between two fingers with your arm extended.
Rotating your arm will roll the ball counter-clockwise or clockwise (red arrow).
In the FDAI, this rotation is accomplished by a motor turning the frame that holds the ball.
For pitch, the ball rotates forward or backward around the horizontal axis (yellow arrow).
The FDAI has a motor inside the ball to produce this rotation.
Yaw is a bit more difficult to envision: imagine hemisphere-shaped shells attached to the top and bottom shafts.
When a motor rotates these shells (green arrow), the hemispheres will rotate, even though
the ball mechanism (the orange) remains stationary.</p>
<p><a href="https://static.righto.com/images/fdai/orange.jpg"><img alt="A sphere, showing the three axes." height="334" src="https://static.righto.com/images/fdai/orange-w400.jpg" title="A sphere, showing the three axes." width="400"></a></p><p>A sphere, showing the three axes.</p>
<p>The diagram below shows the mechanism inside the FDAI.
The indicator uses three motors to move the ball.
The roll motor is attached to the FDAI's frame, while the pitch and yaw motors are inside the ball.
The roll motor rotates the roll gimbal through gears, causing the ball to rotate clockwise or counterclockwise.
The roll gimbal is attached to the ball mechanism at two points along the "equator";
these two points define the pitch axis.
Numerous wires on the roll gimbal enter the ball along the pitch axis.
The roll control transformer provides position feedback, as will be explained below.</p>
<p><a href="https://static.righto.com/images/fdai/fdai-internals-labeled.jpg"><img alt="The main components inside the FDAI." height="399" src="https://static.righto.com/images/fdai/fdai-internals-labeled-w700.jpg" title="The main components inside the FDAI." width="700"></a></p><p>The main components inside the FDAI.</p>
<p>Removing the hemispherical shells reveals the 
mechanism inside the ball.
When the roll gimbal is rotated, this mechanism rotates with it.
The pitch motor causes the ball mechanism to rotate around the pitch axis.
The yaw motor and control transformer are not visible in this photo; they are behind the pitch components, oriented
perpendicularly.
The yaw motor turns the vertical shaft, with
the two hemisphere shells attached to the top and bottom of the shaft.
Thus, the yaw motor rotates the ball shells around the yaw axis, while the mechanism itself
remains stationary.
The control transformers for pitch and yaw provide position feedback.</p>
<p><a href="https://static.righto.com/images/fdai/ball-labeled.jpg"><img alt="The components inside the ball of the FDAI." height="479" src="https://static.righto.com/images/fdai/ball-labeled-w550.jpg" title="The components inside the ball of the FDAI." width="550"></a></p><p>The components inside the ball of the FDAI.</p>
<p>Why doesn't the wiring get tangled up as the ball rotates?
The solution is two sets of slip rings to implement the electrical connections.
The photo below shows the first slip ring assembly, which handles rotation around the roll axis.
These slip rings connect the stationary part of the FDAI to the
rotating roll gimbal.
The vertical metal brushes are stationary; there are 23 pairs of brushes, one for each connection to the ball mechanism.
Each pair of brushes contacts one metal ring on the striped shaft, maintaining contact as the shaft rotates.
Inside the shaft, 23 wires connect the circular metal contacts to the roll gimbal.</p>
<p><a href="https://static.righto.com/images/fdai/sliprings.jpg"><img alt="The slip ring assembly in the FDAI." height="447" src="https://static.righto.com/images/fdai/sliprings-w450.jpg" title="The slip ring assembly in the FDAI." width="450"></a></p><p>The slip ring assembly in the FDAI.</p>
<p>A second set of slip rings inside the ball handles rotation around the pitch axis.
These rings provide the electrical connection between the
wiring on the roll gimbal and the ball mechanism.
The yaw axis does not use slip rings since only the hemisphere shells rotate around the yaw axis;
no wires are involved.</p>
<h2>Synchros and the servo loop</h2>
<p>In this section, I'll explain how the FDAI is controlled by synchros and servo loops.
In the 1950s and 1960s, the standard technique for transmitting a rotational signal electrically was through a synchro.
Synchros were used for everything from rotating an instrument indicator in avionics to rotating the gun on a navy battleship.
A synchro produces an output that depends on the shaft's rotational position, and transmits this output signal
on three wires.
If you connect these wires to a second synchro, you can use the first synchro to control the second one:
the shaft of the second synchro will rotate to the same angle as
the first shaft.
Thus, synchros are a convenient way to send a control signal electrically.</p>
<p>The photo below shows a typical synchro, with the input shaft on the top and five wires
at the bottom: two for power and three for the output.</p>
<p><a href="https://static.righto.com/images/fdai/synchro.jpg"><img alt="A synchro transmitter." height="324" src="https://static.righto.com/images/fdai/synchro-w200.jpg" title="A synchro transmitter." width="200"></a></p><p>A synchro transmitter.</p>
<p>Internally, the synchro has a rotating winding called the rotor that is driven with 400 Hz AC.
Three fixed stator windings provide the three AC output signals. As the shaft rotates, the voltages of the
output signals change, indicating the angle.
(A synchro resembles a transformer with three variable secondary windings.)
If two connected synchros have different angles, the magnetic fields create a torque that rotates the shafts into alignment.</p>
<p><a href="https://static.righto.com/images/fdai/synchro-schematic.png"><img alt="The schematic symbol for a synchro transmitter or receiver." height="192" src="https://static.righto.com/images/fdai/synchro-schematic-w200.png" title="The schematic symbol for a synchro transmitter or receiver." width="200"></a></p><p>The schematic symbol for a synchro transmitter or receiver.</p>
<p>The downside of synchros is that they don't produce a lot of torque.
The solution is to use a more powerful motor, controlled by the synchro and a feedback loop called a servo loop.
The servo loop drives the motor in the appropriate direction to eliminate the error between the desired position and the
current position.</p>
<p>The diagram below shows how the servo loop is constructed from a combination of electronics and mechanical components.
The goal is to rotate the output shaft to an angle that exactly matches the input angle,
specified by the three synchro wires.
The control transformer compares the input angle and the output shaft position, producing an error signal.
The amplifier uses this error signal to drive the motor in the appropriate direction until the error signal drops to zero.
To improve the dynamic response of the servo loop, the tachometer signal is used as a negative feedback voltage.
The feedback slows the motor as the system gets closer to the right position, so the motor doesn't overshoot the position and oscillate.
(This is sort of like a PID controller.)</p>
<p><a href="https://static.righto.com/images/fdai/servo-diagram.jpg"><img alt="This diagram shows the structure of the servo loop, with a feedback loop ensuring that the rotation angle of the output shaft matches the input angle." height="228" src="https://static.righto.com/images/fdai/servo-diagram-w600.jpg" title="This diagram shows the structure of the servo loop, with a feedback loop ensuring that the rotation angle of the output shaft matches the input angle." width="600"></a></p><p>This diagram shows the structure of the servo loop, with a feedback loop ensuring that the rotation angle of the output shaft matches the input angle.</p>
<p>A control transformer
is similar to a synchro in appearance and construction, but the rotating shaft operates as an input, not the output.
In a control transformer, the three stator windings receive the inputs and the rotor winding provides the error output.
If the rotor angle of the synchro transmitter and control transformer are the same, the signals cancel out and there is
no error voltage.
But as the difference between the two shaft angles increases, the rotor winding produces an error signal. The phase of the
error signal indicates the direction of the error.</p>
<p>In the FDAI, the motor is a special <a href="https://www.righto.com/2024/02/bendix-cadc-servomotor-tachometer.html">motor/tachometer</a>, a device that was often used in avionics servo loops.
This motor is more complicated than a regular electric motor.
The motor is powered by 115 volts AC at 400 hertz, but this won't spin the motor on its own.
The motor also has two low-voltage control windings. Energizing the control windings with the proper phase causes the
motor to spin in one direction or the other.
The motor/tachometer unit also contains a tachometer to measure its speed for the feedback loop.
The tachometer is driven by another 115-volt AC winding and generates a low-voltage AC signal that is proportional
to the motor's rotational speed.</p>
<p><a href="https://static.righto.com/images/fdai/motor-disassembled.jpg"><img alt="A motor/tachometer similar (but not identical) to the one in the FDAI." height="262" src="https://static.righto.com/images/fdai/motor-disassembled-w500.jpg" title="A motor/tachometer similar (but not identical) to the one in the FDAI." width="500"></a></p><p>A motor/tachometer similar (but not identical) to the one in the FDAI.</p>
<p>The photo above shows a motor/tachometer with the rotor removed.
The unit has many wires because of its multiple windings.
The rotor has two drums. The drum on the left, with the spiral stripes, is for the motor. This drum is a "squirrel-cage rotor",
which spins due to induced currents.
(There are no electrical connections to the rotor; the drums interact with the windings through magnetic fields.)
The drum on the right is the tachometer rotor; it induces a signal in the output winding proportional to the speed due to eddy currents.
The tachometer signal is at 400 Hz like the driving signal, either in phase or 180º out of phase, depending on the direction
of rotation.
For more information on how a motor/tachometer works, see my <a href="https://www.righto.com/2024/02/bendix-cadc-servomotor-tachometer.html">teardown</a>.</p>
<h2>The amplifiers</h2>
<p>The FDAI has three servo loops—one for each axis—and each servo loop has a separate control transformer, motor, and amplifier.
The photo below shows one of the three amplifier boards. The construction is unusual and somewhat chaotic,
with some components stacked on top of others to save space.
Some of the component leads are long and protected with clear plastic sleeves.<span id="fnref:pcb"><a href="#fn:pcb">5</a></span>
The cylindrical pulse transformer in the middle has five colorful wires coming out of it.
At the left are the two transistors that drive the motor's control windings, with two capacitors between them.
The transistors are mounted on a heat sink that is screwed down to the case of the amplifier assembly for cooling.
Each amplifier is connected to the FDAI through seven wires with pins that
plug into the sockets on the right of the board.<span id="fnref:jumpers"><a href="#fn:jumpers">6</a></span></p>
<p><a href="https://static.righto.com/images/fdai/amplifier-board.jpg"><img alt="One of the three amplifier boards. At the right front of the board, you can see a capacitor stacked on top of a resistor. The board is shiny because it is covered with conformal coating." height="351" src="https://static.righto.com/images/fdai/amplifier-board-w600.jpg" title="One of the three amplifier boards. At the right front of the board, you can see a capacitor stacked on top of a resistor. The board is shiny because it is covered with conformal coating." width="600"></a></p><p>One of the three amplifier boards. At the right front of the board, you can see a capacitor stacked on top of a resistor. The board is shiny because it is covered with conformal coating.</p>
<p>The function of the board is to amplify the error signal so the motor rotates in the appropriate direction.
The amplifier also uses the tachometer output from the motor unit to slow the motor as the error signal decreases, preventing
overshoot.
The inputs to the amplifier are 400 hertz AC signals, with the magnitude indicating the amount of error or speed and the
phase indicating the direction.
The two outputs from the amplifier drive the two control windings of the motor, determining which direction the motor rotates.</p>
<p>The schematic for the amplifier board is below. <span id="fnref:zener"><a href="#fn:zener">7</a></span>
The two transistors on the left amplify the error and tachometer signals, driving the pulse transformer.
The outputs of the pulse transformer will have opposite phases, driving the output transistors for opposite halves of
the 400 Hz cycle.
This activates the motor control winding, causing the motor to spin in the desired direction.<span id="fnref:control"><a href="#fn:control">8</a></span></p>
<p><a href="https://static.righto.com/images/fdai/amplifier-schematic.jpg"><img alt="The schematic of an amplifier board." height="262" src="https://static.righto.com/images/fdai/amplifier-schematic-w500.jpg" title="The schematic of an amplifier board." width="500"></a></p><p>The schematic of an amplifier board.</p>
<h2>History of the FDAI</h2>
<p>Bill Lear, born in 1902, was a prolific inventor with over 150 patents,
creating everything from the 8-track tape to the Learjet, the iconic
private plane of the 1960s.
He created multiple companies in the 1920s as well as inventing one of the first car radios for Motorola before starting Lear Avionics,
a company that specialized in aerospace instruments.<span id="fnref:lear"><a href="#fn:lear">9</a></span>
Lear produced innovative aircraft instruments and flight control systems such as
the <a href="https://archive.org/details/sim_flight-operations_1951-01_35_1/page/38/">F-5 automatic pilot</a>, which received a trophy as the "greatest aviation achievement in America" for 1950.</p>
<p>Bill Lear went on to solve an indicator problem for the Air Force:
the supersonic F-102 Delta Dagger interceptor (1953) could climb at steep angles, but existing
attitude indicators could not handle nearly vertical flight. 
Lear developed a remote two-gyro platform that drove the cockpit indicator while avoiding "gimbal lock" during vertical
flight.
For the experimental X-15 rocket-powered aircraft (1959), Lear improved this indicator to handle three axes:
roll, pitch, and yaw.</p>
<p>Meanwhile, the Siegler Corporation started in 1950 to manufacture space heaters for homes. A few years later, Siegler was acquired
by John Brooks, an entrepreneur who was enthusiastic about acquisitions. In 1961, Lear Avionics became his latest acquisition, and
the merged company was called Lear Siegler Incorporated, often known as LSI.
(Older programmers may know Lear Siegler through the <a href="https://en.wikipedia.org/wiki/ADM-3A">ADM-3A</a>, an inexpensive video display terminal from 1976 that 
housed the display and keyboard in a stylish white case.)</p>
<p>The X-15's attitude indicator became the basis of the indicator for the F-4 fighter plane
(the <a href="https://www.righto.com/2024/09/f4-attitude-indicator.html">ARU/11-A</a>).
Then, after "<a href="https://ntrs.nasa.gov/api/citations/19680016105/downloads/19680016105.pdf#page=120">a minimum of modification</a>",
the attitude-director indicator was used in the Gemini space program.
In total, Lear Siegler provided 11 instruments in the Gemini instrument panel, with the attitude director the most important.
Next, Gemini's indicator was modified to become the FDAI (flight director-attitude indicator) in the Lunar Module for Apollo.<span id="fnref:parts"><a href="#fn:parts">10</a></span>
Lear Siegler provided numerous components for the Apollo program, from a directional gyro for the Lunar Rover
to the electroluminescent display for the Apollo Guidance Computer's Display/Keyboard (DSKY).</p>
<p><a href="https://static.righto.com/images/fdai/lsi-instruments.jpg"><img alt="An article titled &quot;LSI Instruments Aid in Moon Landing&quot; from LSI's internal LSI Log publication, July 1969. (Click for a larger version.)" height="385" src="https://static.righto.com/images/fdai/lsi-instruments-w600.jpg" title="An article titled &quot;LSI Instruments Aid in Moon Landing&quot; from LSI's internal LSI Log publication, July 1969. (Click for a larger version.)" width="600"></a></p><p>An article titled "LSI Instruments Aid in Moon Landing" from LSI's internal LSI Log publication, July 1969. (Click for a larger version.)</p>
<p>In 1974, Lear Siegler obtained a contract to develop the Attitude-Director Indicator (ADI) for the Space Shuttle, producing
a dozen ADI units for the Space Shuttle.
However, by this time, Lear Siegler was losing enthusiasm for low-volume space avionics.
The Instrument Division president said that "the business that we were in was an engineering business and engineers
love a challenge."
However, manufacturing refused to deal with the special procedures required for space manufacturing,
so the Shuttle units were built by the engineering department.
Lear Siegler didn't bid on later Space Shuttle avionics and the Shuttle ADI became its last space product.
In the early 2000s, the Space Shuttle's instruments were upgraded to a "glass cockpit" with 11 flat-panel displays known
as the Multi-function Electronic Display System (MEDS).
The MEDS was produced by Lear Siegler's long-term competitor, Honeywell.</p>
<p>Getting back to Bill Lear, he wanted to manufacture aircraft, not just aircraft instruments, so
he created the Learjet, the first mass-produced business jet.
The first Learjet flew in 1963, with over 3000 eventually delivered.
In the early 1970s, Lear designed a steam turbine automobile engine. Rather than water, the turbine
used a secret fluorinated hydrocarbon called "Learium". Lear had visions of thousands of low-pollution "<a href="https://www.nytimes.com/1971/04/04/archives/bill-lear-thinks-hell-have-the-last-laugh.html">Learmobiles</a>", but the engine failed
to catch on. 
Lear had been on the verge of bankruptcy in the 1960s; one of his VPs explained that
"the great creative minds can't be bothered with withholding taxes and investment credits and all this crap".
But by the time of his death in 1978, Lear had a fortune estimated at $75 million.</p>
<h2>Comparing the ARU/11-A and the FDAI</h2>
<p>Looking inside our FDAI sheds more details on the evolution of Lear Siegler's attitude directors.
The photo below compares the Apollo FDAI (top) to the earlier ARU/11-A used in the F-4 aircraft (bottom).
While the basic mechanism and the electronic amplifiers are the same between the two indicators, there are
also substantial changes.</p>
<p><a href="https://static.righto.com/images/fdai/aru-vs-fdai.jpg"><img alt="Comparison of an FDAI (top) with an ARU-11/A (bottom). The amplifier boards and needles have been removed from the FDAI." height="482" src="https://static.righto.com/images/fdai/aru-vs-fdai-w600.jpg" title="Comparison of an FDAI (top) with an ARU-11/A (bottom). The amplifier boards and needles have been removed from the FDAI." width="600"></a></p><p>Comparison of an FDAI (top) with an ARU-11/A (bottom). The amplifier boards and needles have been removed from the FDAI.</p>
<p>The biggest difference between the ARU-11/A indicator and the FDAI is that the electronics for the ARU-11/A 
are in a separate module that was plugged into the back of the indicator, while the FDAI includes the electronics
internally, with boards mounted on the instrument frame.
Specifically, the ARU-11/A has a separate unit containing a multi-winding transformer, a power supply board,
and three amplifier boards (one for each axis), while the FDAI contains these components internally.
The amplifier boards in the ARU-11/A and the FDAI are identical, constructed from germanium transistors rather than
silicon.<span id="fnref:amplifiers"><a href="#fn:amplifiers">11</a></span>
The unusual 11-pin transformers are also the same.
However, the power supply boards are different, probably because the boards also contain scaling resistors
that vary between the units.<span id="fnref:resistors"><a href="#fn:resistors">12</a></span>
The power supply boards are also different shapes to fit the available space.</p>
<p>The ball assemblies of the ARU/11-A and the FDAI are almost the same, with the same motor assemblies and slip ring
mechanism. 
The gearing has minor changes. In particular, the FDAI has two plastic gears, while the ARU/11-A uses
exclusively metal gears.</p>
<p>The ARU/11-A has a <a href="https://patents.google.com/patent/US2941305A">patented</a> pitch trim feature that was
mostly—but not entirely—removed from the Apollo FDAI.
The motivation for this feature is that an aircraft in level flight will be pitched up a few degrees, the "angle of attack".
It is desirable for the attitude indicator to show the aircraft as horizontal, so a pitch trim knob allows the
angle of attack to be canceled out on the display.
The problem is that if you fly your fighter plane vertically, you want the indicator to show precisely
vertical flight, rather than applying the pitch trim adjustment.
The solution in the ARU-11/A is a special 8-zone potentiometer on the pitch axis that will apply the pitch trim
adjustment in level flight but not in vertical flight, while providing a smooth transition between the regions.
This special potentiometer is mounted inside the ball of the ARU-11/A.
However, this pitch trim adjustment is meaningless for a spacecraft, so it is not implemented in the Apollo
or Space Shuttle instruments.
Surprisingly, the shell of the potentiometer still exists in our FDAI, but without the potentiometer itself or the
wiring.
Perhaps it remained to preserve the balance of the ball.
In the photo below, the cylindrical potentiometer shell is indicated by an arrow. Note the holes in the front
of the shell; in the ARU-11/A, the potentiometer's wiring terminals protrude through these holes, but in the 
FDAI, the holes are covered with tape.</p>
<p><a href="https://static.righto.com/images/fdai/potentiometer.jpg"><img alt="Inside the ball of the FDAI. The potentiometer shell is indicated with an arrow." height="354" src="https://static.righto.com/images/fdai/potentiometer-w400.jpg" title="Inside the ball of the FDAI. The potentiometer shell is indicated with an arrow." width="400"></a></p><p>Inside the ball of the FDAI. The potentiometer shell is indicated with an arrow.</p>
<p>Finally, the mounting of the ball hemispheres is slightly different.
The ARU/11-A uses four screws at the pole of each hemisphere.
Our FDAI, however, uses a single screw at each pole; the screw is tightened with a Bristol Key, causing the
shaft to expand and hold the hemisphere in place.</p>
<p>To summarize, the Apollo FDAI occupies a middle ground: while it isn't simply a repurposed ARU-11/A, neither is
it a complete redesign.
Instead, it preserves the old design where possible, while stripping out undesired features such as pitch trim.
The separate amplifier and mechanical units of the ARU/11-A were combined to form the larger FDAI.</p>
<h2>Differences from Apollo</h2>
<p>The FDAI that we examined is a special unit:
it was originally built for Apollo but was repurposed for a Space Shuttle simulator.
Our FDAI is labeled Model 4068F, which is a Lunar Module part number.
Moreover, the FDAI is internally stamped with the date "Apr. 22 1968", over a year before the first Moon landing.</p>
<p>However, a closer look shows that several key components were modified to make the Apollo FDAI work in the Shuttle Simulator.<span id="fnref:systems-handbook"><a href="#fn:systems-handbook">14</a></span>
The Apollo FDAI (and the Shuttle ADI) used resolvers as inputs to control the ball, while our FDAI uses synchros.
(Resolvers and synchros are similar, except resolvers use sine and cosine inputs, 90° apart, on two wire pairs, while
synchros use three inputs, 120° apart, on three wires.)
NASA must have replaced the three resolver control transformers in the FDAI with synchro control transformers for use
in the simulator.</p>
<p>The Apollo FDAI used electroluminescent lighting for the display, while ours uses eight small incandescent bulbs.
The metal case of our FDAI has a Dymo <a href="https://en.wikipedia.org/wiki/Embossing_tape">embossed tape</a> label "INCANDESCENT
LIGHTING", alerting users to the change from Apollo's illumination.
Our FDAI also contains a step-down transformer to convert the 115 VAC input into 5 VAC to power the bulbs,
while the Shuttle powered its ADI illumination directly from <a href="https://airandspace.si.edu/collection-media/NASM-49BE0A798E632_006">5 volts</a>.</p>
<p>The dial of our FDAI was repainted to match the dial of the Shuttle FDAI.
The Apollo FDAI had red bands on the left and right of the dial.
A close examination of our dial shows that black paint was carefully applied over the red paint, but a few specks of
red paint are still visible (below).
Moreover, the edges of the lines and the lozenge show slight unevenness from the repainting.
Second, the Apollo FDAI had the text "ROLL RATE", "PITCH RATE", and "YAW RATE" in white next to the needle scales.
In our FDAI, this text has been hidden by black paint to match the Shuttle display.<span id="fnref:panel"><a href="#fn:panel">13</a></span>
Third, the Apollo LM FDAI had a crosshair in the center of the instrument, while our FDAI has a white U-shaped 
indicator, the same as the Shuttle (and the Command Module's FDAI).
Finally, the ball of the Apollo FDAI has red circular regions at the poles to warn of orientations that can cause
gimbal lock. Our FDAI (like the Shuttle) does not have these circles. We couldn't see any evidence that these
regions were repainted, so we suspect that our FDAI has Shuttle hemispheres on the ball.</p>
<p><a href="https://static.righto.com/images/fdai/repaint.jpg"><img alt="A closeup of the dial on our FDAI shows specks of red paint around the dial markings. The color is probably Switzer DayGlo Rocket Red." height="279" src="https://static.righto.com/images/fdai/repaint-w400.jpg" title="A closeup of the dial on our FDAI shows specks of red paint around the dial markings. The color is probably Switzer DayGlo Rocket Red." width="400"></a></p><p>A closeup of the dial on our FDAI shows specks of red paint around the dial markings. The color is probably Switzer DayGlo Rocket Red.</p>
<p>Our FDAI has also been modified electrically. 
Small green connectors (Micro-D MDB1) have been added between the slip rings and the motors, as well as on the gimbal arm.
We think these connectors were added post-Apollo, since they are attached somewhat sloppily with glue and don't
look flight-worthy.
Perhaps these connectors were added to make disassembly and modification easier.
Moreover, our FDAI has an elapsed time indicator, also mounted with glue.</p>
<p>The back of our FDAI is completely different from Apollo.
First, the connector's pinout is completely different.
Second, each of the six indicator needles has a mechanical adjustment as well as a trimpot (<a href="https://space1.com/Artifacts/Artifacts_FOR_SALE/FS__Shuttle_Sim_Avionics/FS__Shuttle_Sim_ADI/fs__shuttle_sim_adi.html">details</a>).
Finally, each of the three axes has an adjustment potentiometer.</p>
<h2>The Shuttle's ADI (Attitude Director Indicator)</h2>
<p>Each Space Shuttle had three ADIs (Attitude Director Indicators), which were very similar to the Apollo FDAI, despite
the name change.
The photo below shows the two octagonal ADIs in the forward flight deck, one on the left in front of the Commander,
and one on the right in front of the Pilot.
The <a href="https://catalog.archives.gov/id/22919771">aft flight deck station</a> had a third ADI.<span id="fnref:MEDS"><a href="#fn:MEDS">15</a></span></p>
<p><a href="https://static.righto.com/images/fdai/shuttle-flight-deck.jpg"><img alt="This photo shows Discovery's forward flight deck on STS-063 (1999). The ADIs are indicated with arrows. The photo is from the National Archives." height="385" src="https://static.righto.com/images/fdai/shuttle-flight-deck-w600.jpg" title="This photo shows Discovery's forward flight deck on STS-063 (1999). The ADIs are indicated with arrows. The photo is from the National Archives." width="600"></a></p><p>This photo shows Discovery's forward flight deck on STS-063 (1999). The ADIs are indicated with arrows. The photo is from the <a href="https://catalog.archives.gov/id/23894173">National Archives</a>.</p>
<p>Our FDAI appears to have been significantly modified for use in the Shuttle simulator, as described above.
However, it is much closer to the Apollo FDAI than the ADI used in the Shuttle, as I'll show in this section.
My hypothesis is that the simulator was built before the Shuttle's ADI was created, so the Apollo FDAI was pressed
into service.</p>
<p>The Shuttle's ADI was much more complicated electrically than the Apollo FDAI and our FDAI, providing improved
functionality.<span id="fnref:shuttle-adi"><a href="#fn:shuttle-adi">16</a></span>
For instance, while the Apollo FDAI had a simple "OFF" indicator flag to show that the indicator had lost power,
the Shuttle's ADI had extensive error detection. 
It contained voltage level monitors to check its five power supplies. (The Shuttle ADI used three DC power sources
and two AC power sources, compared to the single AC supply for Apollo.)
The Shuttle's ADI also monitored the ball servos to detect position errors. Finally, it received an external "Data OK" signal.
If a fault was detected by any of these monitors, the "OFF" flag was deployed to indicate that the ADI could not
be trusted.</p>
<p>The Shuttle's ADI had six needles, the same as Apollo, but the Shuttle used feedback to make the positions more accurate.
Specifically, each Shuttle needle had a feedback sensor, a Linear Variable Differential Transformer (LVDT) that generates
a voltage based on the needle position.
The LVDT output drove a servo feedback loop to ensure that the needle was in the exact desired position.
In the Apollo FDAI, on the other hand, the needle input voltage drove a galvanometer, swinging the needle proportionally,
but there was no closed loop to ensure accuracy.</p>
<p>I assume that the Shuttle's ADI had integrated circuit electronics to implement this new functionality, considerably
more modern than the germanium transistors in the Apollo FDAI.
The Shuttle probably used the same mechanical structures to rotate the ball, but I can't confirm that.</p>
<h2>Conclusions</h2>
<p>The FDAI was a critical instrument in Apollo, indicating the orientation of the spacecraft in three axes.
It wasn't obvious to me how the "8-ball" can rotate in three axes while still being securely connected to the
instrument.
The trick is that most of the mechanism rotates in two axes, while hollow hemispherical shells provide the
third rotational axis.</p>
<p>The FDAI has an interesting evolutionary history, from the experimental X-15 rocket plane and the F-4 fighter to
the Gemini, Apollo, and Space Shuttle flights.
Our FDAI has an unusual position in this history: since it was modified from Apollo to function in a Space Shuttle
simulator, it shows aspects of both Apollo and the Space Shuttle indicators.
It would be interesting to compare the design of a Shuttle ADI to the Apollo FDAI, but I haven't been able to find
interior photos of a Shuttle ADI (or of an unmodified Apollo FDAI).<span id="fnref:photos"><a href="#fn:photos">17</a></span></p>
<p>You can see a brief video of the FDAI in motion <a href="https://bsky.app/profile/did:plc:svh6dgjnpkdl4dhxahj4xvkv/post/3lrlbsnh5z22j">here</a>. For more, follow me on
 Bluesky (<a href="https://bsky.app/profile/righto.com">@righto.com</a>),
Mastodon (<a href="https://oldbytes.space/@kenshirriff">@<span data-cfemail="6d0608031e05041f1f040b0b2d0201090f1419081e431e1d0c0e08">[email&nbsp;protected]</span></a>),
or <a href="https://www.righto.com/feeds/posts/default">RSS</a>. (I've given up on Twitter.)
I worked on this project with CuriousMarc, Mike Stewart, and Eric Schlapfer, so expect a
video at some point. Thanks to Richard for providing the FDAI.
I wrote about the F-4 fighter plane's attitude indicator <a href="https://www.righto.com/2024/09/f4-attitude-indicator.html">here</a>.</p>
<p><a href="https://static.righto.com/images/fdai/fdai-opened2.jpg"><img alt="Inside the FDAI. The amplifier boards have been removed for this photo." height="505" src="https://static.righto.com/images/fdai/fdai-opened2-w700.jpg" title="Inside the FDAI. The amplifier boards have been removed for this photo." width="700"></a></p><p>Inside the FDAI. The amplifier boards have been removed for this photo.</p>
<h2>Notes and references</h2>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I have reimplemented Stable Diffusion 3.5 from scratch in pure PyTorch (320 pts)]]></title>
            <link>https://github.com/yousef-rafat/miniDiffusion</link>
            <guid>44276476</guid>
            <pubDate>Sat, 14 Jun 2025 13:56:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/yousef-rafat/miniDiffusion">https://github.com/yousef-rafat/miniDiffusion</a>, See on <a href="https://news.ycombinator.com/item?id=44276476">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">miniDiffusion</h2><a id="user-content-minidiffusion" aria-label="Permalink: miniDiffusion" href="#minidiffusion"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/yousef-rafat/miniDiffusion/blob/main/assets/display.png"><img src="https://github.com/yousef-rafat/miniDiffusion/raw/main/assets/display.png" alt="SD3 Diagram"></a></p>
<p dir="auto">miniDiffusion is a reimplementation of the Stable Diffusion 3.5 model in pure PyTorch with minimal dependencies. It's designed for educational, experimenting, and hacking purposes.
It's made with the mindset of having the least amount of code necessary to recreate Stable Diffusion 3.5 from scratch, with only ~2800 spanning from VAE to DiT to the Train and Dataset scripts.</p>
<p dir="auto"><strong>-Files:</strong> The main Stable Diffusion model code is located in dit.py, dit_components.py, and attention.py. The dit.py file contains the main model, dit_components.py contains the embedding, normalization, patch embedding, and help functions for the DiT code, and attention.py contains the Joint Attention implementation.
The noise.py is where the Euler Scheduler is located for solving the ODE of Rectified Flow.</p>
<p dir="auto">The text encoders are in t5_encoder.py and clip.py, and their tokenizers are both in tokenizer.py. The metrics.py implements the Fréchet inception distance (FID).</p>
<p dir="auto">The common.py is a place for helper functions for training, the common_ds.py is an implementation of an iterable dataset that converts image data to trainable data for the DiT model.</p>
<p dir="auto"><strong>-Folders:</strong> The model folder saves the model's checkpoint and logs after training. The encoders folder saves other modules' checkpoints (e.g., VAE, CLIP).</p>
<blockquote>
<p dir="auto"><g-emoji alias="warning">⚠️</g-emoji> <strong>Warning</strong>:
This repository still has experimental features and requires more testing.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Components</h2><a id="user-content-components" aria-label="Permalink: Components" href="#components"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Core Image Generation Modules</h3><a id="user-content-core-image-generation-modules" aria-label="Permalink: Core Image Generation Modules" href="#core-image-generation-modules"></a></p>
<ul dir="auto">
<li>Implementations of VAE, CLIP, and T5 Text Encoders</li>
<li>Implementation of Byte-Pair &amp; Unigram tokenizers</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">SD3 Components</h3><a id="user-content-sd3-components" aria-label="Permalink: SD3 Components" href="#sd3-components"></a></p>
<ul dir="auto">
<li>Multi-Modal Diffusion Transformer Model</li>
<li>Flow-Matching Euler Scheduler</li>
<li>Logit-Normal Sampling</li>
<li>Joint Attention</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Train and Inference Scripts For SD3</h3><a id="user-content-train-and-inference-scripts-for-sd3" aria-label="Permalink: Train and Inference Scripts For SD3" href="#train-and-inference-scripts-for-sd3"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">Get the repo</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone &quot;https://github.com/yousef-rafat/miniDiffusion&quot;"><pre>git clone <span><span>"</span>https://github.com/yousef-rafat/miniDiffusion<span>"</span></span></pre></div>
<p dir="auto">Install Dependencies</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<p dir="auto">Install Checkpoints for Models</p>
<ul dir="auto">
<li><em>Add a Hugging Face Token in get_checkpoints.py before running the script.</em></li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="python3 encoders/get_checkpoints.py"><pre>python3 encoders/get_checkpoints.py</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is under the MIT License and is made for educational and experimental purposes.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unsupervised Elicitation of Language Models (116 pts)]]></title>
            <link>https://arxiv.org/abs/2506.10139</link>
            <guid>44276041</guid>
            <pubDate>Sat, 14 Jun 2025 12:32:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2506.10139">https://arxiv.org/abs/2506.10139</a>, See on <a href="https://news.ycombinator.com/item?id=44276041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wen,+J" rel="nofollow">Jiaxin Wen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ankner,+Z" rel="nofollow">Zachary Ankner</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Somani,+A" rel="nofollow">Arushi Somani</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hase,+P" rel="nofollow">Peter Hase</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Marks,+S" rel="nofollow">Samuel Marks</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Goldman-Wetzler,+J" rel="nofollow">Jacob Goldman-Wetzler</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Petrini,+L" rel="nofollow">Linda Petrini</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sleight,+H" rel="nofollow">Henry Sleight</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Burns,+C" rel="nofollow">Collin Burns</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+H" rel="nofollow">He He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Feng,+S" rel="nofollow">Shi Feng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Perez,+E" rel="nofollow">Ethan Perez</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Leike,+J" rel="nofollow">Jan Leike</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2506.10139">View PDF</a>
    <a href="https://arxiv.org/html/2506.10139v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>To steer pretrained language models for downstream tasks, today's post-training paradigm relies on humans to specify desired behaviors. However, for models with superhuman capabilities, it is difficult or impossible to get high-quality human supervision. To address this challenge, we introduce a new unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune pretrained language models on their own generated labels, \emph{without external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward modeling tasks, our method matches the performance of training on golden supervision and outperforms training on crowdsourced human supervision. On tasks where LMs' capabilities are strongly superhuman, our method can elicit those capabilities significantly better than training on human labels. Finally, we show that our method can improve the training of frontier LMs: we use our method to train an unsupervised reward model and use reinforcement learning to train a Claude 3.5 Haiku-based assistant. Both the reward model and the assistant outperform their human-supervised counterparts.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Jiaxin Wen [<a href="https://arxiv.org/show-email/5affddae/2506.10139" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 11 Jun 2025 19:40:08 UTC (1,235 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Model Once, Represent Everywhere: UDA (Unified Data Architecture) at Netflix (123 pts)]]></title>
            <link>https://netflixtechblog.com/uda-unified-data-architecture-6a6aee261d8d</link>
            <guid>44275575</guid>
            <pubDate>Sat, 14 Jun 2025 10:56:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://netflixtechblog.com/uda-unified-data-architecture-6a6aee261d8d">https://netflixtechblog.com/uda-unified-data-architecture-6a6aee261d8d</a>, See on <a href="https://news.ycombinator.com/item?id=44275575">Hacker News</a></p>
Couldn't get https://netflixtechblog.com/uda-unified-data-architecture-6a6aee261d8d: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Last fifty years of integer linear programming: Recent practical advances (169 pts)]]></title>
            <link>https://inria.hal.science/hal-04776866v1</link>
            <guid>44274567</guid>
            <pubDate>Sat, 14 Jun 2025 06:15:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://inria.hal.science/hal-04776866v1">https://inria.hal.science/hal-04776866v1</a>, See on <a href="https://news.ycombinator.com/item?id=44274567">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en"><p>Mixed-integer linear programming (MILP) has become a cornerstone of operations research. This is driven by the enhanced efficiency of modern solvers, which can today find globally optimal solutions within seconds for problems that were out of reach a decade ago. The versatility of these solvers allowed successful applications in many areas, such as transportation, logistics, supply chain management, revenue management, finance, telecommunications, and manufacturing. Despite the impressive success already obtained, many challenges remain, and MILP is still a very active field.</p><p>This article provides an overview of the most significant results achieved in advancing the MILP solution methods. Given the immense literature on this topic, we made deliberate choices to focus on computational aspects and recent practical performance improvements, emphasizing research that reports computational experiments. We organize our survey into three main parts, dedicated to branch-and-cut methods, Dantzig-Wolfe decomposition, and Benders decomposition. The paper concludes by highlighting ongoing challenges and future opportunities in MILP research.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Cloud Incident Report – 2025-06-13 (167 pts)]]></title>
            <link>https://status.cloud.google.com/incidents/ow5i3PPK96RduMcb1SsW</link>
            <guid>44274563</guid>
            <pubDate>Sat, 14 Jun 2025 06:13:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://status.cloud.google.com/incidents/ow5i3PPK96RduMcb1SsW">https://status.cloud.google.com/incidents/ow5i3PPK96RduMcb1SsW</a>, See on <a href="https://news.ycombinator.com/item?id=44274563">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <tr> <td> <psd-status-icon> <svg aria-label="Available status" fill="none" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M9.001.666A8.336 8.336 0 0 0 .668 8.999c0 4.6 3.733 8.334 8.333 8.334s8.334-3.734 8.334-8.334S13.6.666 9 .666Zm0 15a6.676 6.676 0 0 1-6.666-6.667A6.676 6.676 0 0 1 9 2.333a6.676 6.676 0 0 1 6.667 6.666A6.676 6.676 0 0 1 9 15.666Zm-1.666-4.833L5.168 8.666 4.001 9.833l3.334 3.333L14 6.499l-1.166-1.166-5.5 5.5Z" fill="#1E8E3E" fill-rule="evenodd"></path></svg> </psd-status-icon> </td> <td>13 Jun 2025</td> <td>16:45 PDT</td> <td><h2>Incident Report</h2>
<h2><strong>Summary</strong></h2>
<p><em>Google Cloud, Google Workspace and Google Security Operations products experienced increased 503 errors in external API requests, impacting customers.</em></p>
<p><em><strong>We deeply apologize for the impact this outage has had.  Google Cloud customers and their users trust their businesses to Google, and we will do better.  We apologize for the impact this has had not only on our customers’ businesses and their users but also on the trust of our systems.  We are committed to making improvements to help avoid outages like this moving forward.</strong></em></p>
<h3><strong>What happened?</strong></h3>
<p>Google and Google Cloud APIs are served through our Google API management and control planes.  Distributed regionally, these management and control planes are responsible for ensuring each API request that comes in is authorized, has the policy and appropriate checks (like quota) to meet their endpoints.  The core binary that is part of this policy check system is known as Service Control.  Service Control is a regional service that has a regional datastore that it reads quota and policy information from. This datastore metadata gets replicated almost instantly globally to manage quota policies for Google Cloud and our customers.</p>
<p>On May 29, 2025, a new feature was added to Service Control for additional quota policy checks. This code change and binary release went through our region by region rollout, but the code path that failed was never exercised during this rollout due to needing a policy change that would trigger the code.  As a safety precaution, this code change came with a red-button to turn off that particular policy serving path.  The issue with this change was that it did not have appropriate error handling nor was it feature flag protected.  Without the appropriate error handling, the null pointer caused the binary to crash. Feature flags are used to gradually enable the feature region by region per project, starting with internal projects, to enable us to catch issues.  If this had been flag protected, the issue would have been caught in staging.</p>
<p>On June 12, 2025 at ~10:45am PDT, a policy change was inserted into the regional Spanner tables that Service Control uses for policies.  Given the global nature of quota management, this metadata was replicated globally within seconds.  This policy data contained unintended blank fields.  Service Control, then regionally exercised quota checks on policies in each regional datastore. This pulled in blank fields for this respective policy change and exercised the code path that hit the null pointer causing the binaries to go into a crash loop.  This occurred globally given each regional deployment.</p>
<p>Within 2 minutes, our Site Reliability Engineering team was triaging the incident.  Within 10 minutes, the root cause was identified and the red-button (to disable the serving path) was being put in place.  The red-button was ready to roll out ~25 minutes from the start of the incident.  Within 40 minutes of the incident, the red-button rollout was completed, and we started seeing recovery across regions, starting with the smaller ones first.</p>
<p>Within some of our larger regions, such as us-central-1, as Service Control tasks restarted, it created a herd effect on the underlying infrastructure it depends on (i.e. that Spanner table), overloading the infrastructure.  Service Control did not have the appropriate randomized exponential backoff implemented to avoid this.  It took up to ~2h 40 mins to fully resolve in us-central-1 as we throttled task creation to minimize the impact on the underlying infrastructure and routed traffic to multi-regional databases to reduce the load.  At that point, Service Control and API serving was fully recovered across all regions.  Corresponding Google and Google Cloud products started recovering with some taking longer depending upon their architecture.</p>
<h3><strong>What is our immediate path forward?</strong></h3>
<p>Immediately upon recovery, we froze all changes to the Service Control stack and manual policy pushes until we can completely remediate the system.</p>
<h3><strong>How did we communicate?</strong></h3>
<p>We posted our first incident report to Cloud Service Health about ~1h after the start of the crashes, due to the Cloud Service Health infrastructure being down due to this outage. For some customers, the monitoring infrastructure they had running on Google Cloud was also failing, leaving them without a signal of the incident or an understanding of the impact to their business and/or infrastructure. We will address this going forward.</p>
<h3><strong>What’s our approach moving forward?</strong></h3>
<p>Beyond freezing the system as mentioned above, we will prioritize and safely complete the following:</p>
<ul>
<li>We will modularize Service Control’s architecture, so the functionality is isolated and fails open.  Thus, if a corresponding check fails, Service Control can still serve API requests.</li>
<li>We will audit all systems that consume globally replicated data.  Regardless of the business need for near instantaneous consistency of the data globally (i.e. quota management settings are global), data replication needs to be propagated incrementally with sufficient time to validate and detect issues.</li>
<li>We will enforce all changes to critical binaries to be feature flag protected and disabled by default.</li>
<li>We will improve our static analysis and testing practices to correctly handle errors and if need be fail open.</li>
<li>We will audit and ensure our systems employ randomized exponential backoff.</li>
<li>We will improve our external communications, both automated and human, so our customers get the information they need asap to react to issues, manage their systems and help their customers.</li>
<li>We'll ensure our monitoring and communication infrastructure remains operational to serve customers even when Google Cloud and our primary monitoring products are down, ensuring business continuity.</li>
</ul>
<hr>
</td> </tr><tr> <td> <psd-status-icon> <svg aria-label="Available status" fill="none" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M9.001.666A8.336 8.336 0 0 0 .668 8.999c0 4.6 3.733 8.334 8.333 8.334s8.334-3.734 8.334-8.334S13.6.666 9 .666Zm0 15a6.676 6.676 0 0 1-6.666-6.667A6.676 6.676 0 0 1 9 2.333a6.676 6.676 0 0 1 6.667 6.666A6.676 6.676 0 0 1 9 15.666Zm-1.666-4.833L5.168 8.666 4.001 9.833l3.334 3.333L14 6.499l-1.166-1.166-5.5 5.5Z" fill="#1E8E3E" fill-rule="evenodd"></path></svg> </psd-status-icon> </td> <td>12 Jun 2025</td> <td>23:34 PDT</td> <td><h2>Mini Incident Report</h2>
<p>We are deeply sorry for the impact to all of our users and their customers that this service disruption/outage caused. Businesses large and small trust Google Cloud with your workloads and we will do better. In the coming days, we will publish a full incident report of the root cause, detailed timeline and robust remediation steps we will be taking. Given the size and impact of this incident, we would like to provide some information below.</p>
<p>Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Cloud Support using <a href="https://cloud.google.com/support">https://cloud.google.com/support</a>  or to Google Workspace Support using help article <a href="https://support.google.com/a/answer/1047213">https://support.google.com/a/answer/1047213</a>.</p>
<p><strong>(All Times US/Pacific)</strong></p>
<p><strong>Incident Start:</strong> 12 June, 2025 10:49</p>
<p><strong>All regions except us-central1 mitigated:</strong> 12 June, 2025 12:48</p>
<p><strong>Incident End:</strong> 12 June, 2025 13:49</p>
<p><strong>Duration:</strong> 3 hours</p>
<p><strong>Regions/Zones:</strong> Global</p>
<p><strong>Description:</strong></p>
<p>Multiple Google Cloud and Google Workspace products experienced increased 503 errors in external API requests, impacting customers.</p>
<p>From our initial analysis, the issue occurred due to an invalid automated quota update to our API management system which was distributed globally, causing external API requests to be rejected. To recover we bypassed the offending quota check, which allowed recovery in most regions within 2 hours. However, the quota policy database in us-central1 became overloaded, resulting in much longer recovery in that region. Several products had moderate residual impact (e.g. backlogs) for up to an hour after the primary issue was mitigated and a small number recovering after that.</p>
<p>Google will complete a full Incident Report in the following days that will provide a detailed root cause.</p>
<p><strong>Customer Impact:</strong></p>
<p>Customers had intermittent API and user-interface access issues to the impacted services. Existing streaming and IaaS resources were not impacted.</p>
<p><strong>Additional details:</strong></p>
<p>This incident should not have happened, and we will take the following measures to prevent future recurrence:</p>
<ul>
<li>Prevent our API management platform from failing due to invalid or corrupt data.</li>
<li>Prevent metadata from propagating globally without appropriate protection, testing and monitoring in place.</li>
<li>Improve system error handling and comprehensive testing for handling of invalid data.</li>
</ul>
<p><strong>Affected Services and Features:</strong></p>
<p><strong>Google Cloud Products:</strong></p>
<ul>
<li>Identity and Access Management</li>
<li>Cloud Build</li>
<li>Cloud Key Management Service</li>
<li>Google Cloud Storage</li>
<li>Cloud Monitoring</li>
<li>Google Cloud Dataproc</li>
<li>Cloud Security Command Center</li>
<li>Artifact Registry</li>
<li>Cloud Workflows</li>
<li>Cloud Healthcare</li>
<li>Resource Manager API</li>
<li>Dataproc Metastore</li>
<li>Cloud Run</li>
<li>VMWare engine</li>
<li>Dataplex</li>
<li>Migrate to Virtual Machines</li>
<li>Google BigQuery</li>
<li>Contact Center AI Platform</li>
<li>Google Cloud Deploy</li>
<li>Media CDN</li>
<li>Colab Enterprise</li>
<li>Vertex Gemini API</li>
<li>Cloud Data Fusion</li>
<li>Cloud Asset Inventory</li>
<li>Datastream</li>
<li>Integration Connectors</li>
<li>Apigee</li>
<li>Google Cloud NetApp Volumes</li>
<li>Google Cloud Bigtable</li>
<li>Looker (Google Cloud core)</li>
<li>Looker Studio</li>
<li>Google Cloud Functions</li>
<li>Cloud Load Balancing</li>
<li>Traffic Director</li>
<li>Document AI</li>
<li>AutoML Translation</li>
<li>Pub/Sub Lite</li>
<li>API Gateway</li>
<li>Agent Assist</li>
<li>AlloyDB for PostgreSQL</li>
<li>Cloud Firestore</li>
<li>Cloud Logging</li>
<li>Cloud Shell</li>
<li>Cloud Memorystore</li>
<li>Cloud Spanner</li>
<li>Contact Center Insights</li>
<li>Database Migration Service</li>
<li>Dialogflow CX</li>
<li>Dialogflow ES</li>
<li>Google App Engine</li>
<li>Google Cloud Composer</li>
<li>Google Cloud Console</li>
<li>Google Cloud DNS</li>
<li>Google Cloud Pub/Sub</li>
<li>Google Cloud SQL</li>
<li>Google Compute Engine</li>
<li>Identity Platform</li>
<li>Managed Service for Apache Kafka</li>
<li>Memorystore for Memcached</li>
<li>Memorystore for Redis</li>
<li>Memorystore for Redis Cluster</li>
<li>Persistent Disk</li>
<li>Personalized Service Health</li>
<li>Speech-to-Text</li>
<li>Text-to-Speech</li>
<li>Vertex AI Search</li>
<li>Retail API</li>
<li>Vertex AI Feature Store</li>
<li>BigQuery Data Transfer Service</li>
<li>Google Cloud Marketplace</li>
<li>Cloud NAT</li>
<li>Hybrid Connectivity</li>
<li>Cloud Vision</li>
<li>Network Connectivity Center</li>
<li>Cloud Workstations</li>
<li>Google Security Operations</li>
</ul>
<p><strong>Google Workspace Products:</strong></p>
<ul>
<li>AppSheet</li>
<li>Gmail</li>
<li>Google Calendar</li>
<li>Google Drive</li>
<li>Google Chat</li>
<li>Google Voice</li>
<li>Google Docs</li>
<li>Google Meet</li>
<li>Google Cloud Search</li>
<li>Google Tasks</li>
</ul>
</td> </tr><tr> <td> <psd-status-icon> <svg aria-label="Available status" fill="none" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M9.001.666A8.336 8.336 0 0 0 .668 8.999c0 4.6 3.733 8.334 8.333 8.334s8.334-3.734 8.334-8.334S13.6.666 9 .666Zm0 15a6.676 6.676 0 0 1-6.666-6.667A6.676 6.676 0 0 1 9 2.333a6.676 6.676 0 0 1 6.667 6.666A6.676 6.676 0 0 1 9 15.666Zm-1.666-4.833L5.168 8.666 4.001 9.833l3.334 3.333L14 6.499l-1.166-1.166-5.5 5.5Z" fill="#1E8E3E" fill-rule="evenodd"></path></svg> </psd-status-icon> </td> <td>12 Jun 2025</td> <td>18:27 PDT</td> <td><p>Vertex AI Online Prediction is full recovered as of 18:18 PDT.</p>
<p>All the services are fully recovered from the service issue</p>
<p>We will publish analysis of this incident once we have completed our internal investigation.</p>
<p>We thank you for your patience while we worked on resolving the issue.</p>
</td> </tr><tr> <td> <psd-status-icon> <svg fill="none" aria-label="Disruption status" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M8.168 11.5h1.667v1.666H8.168v-1.667Zm0-6.667h1.667v5H8.168v-5ZM8.993.666C4.393.666.668 4.399.668 8.999s3.725 8.334 8.325 8.334c4.608 0 8.342-3.734 8.342-8.334S13.6.666 8.993.666Zm.008 15a6.665 6.665 0 0 1-6.666-6.667A6.665 6.665 0 0 1 9 2.333a6.665 6.665 0 0 1 6.667 6.666A6.665 6.665 0 0 1 9 15.666Z" fill="#E37400" fill-rule="evenodd"></path></svg> </psd-status-icon> </td> <td>12 Jun 2025</td> <td>17:59 PDT</td> <td><p><strong>Vertex AI Online Prediction:</strong>
The issue causing elevated 5xx errors with some Model Garden models was fully resolved as of 17:05 PDT. Vertex AI serving is now back to normal in all regions except europe-west1 and asia-southeast1. Engineers are actively working to restore normal serving capacity in these two regions.</p>
<p>The ETA for restoring normal serving capacity in europe-west1 and asia-southeast1 is 19:45 PDT.</p>
<p>We will provide an update by Thursday, 2025-06-12 19:45 PDT with current details.</p>
</td> </tr><tr> <td> <psd-status-icon> <svg fill="none" aria-label="Outage status" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M11.16 5.666 9 7.824 6.843 5.666 5.668 6.841l2.158 2.158-2.158 2.159 1.175 1.175 2.158-2.159 2.159 2.159 1.175-1.175-2.159-2.159 2.159-2.158-1.175-1.175ZM9 .666A8.326 8.326 0 0 0 .668 8.999a8.326 8.326 0 0 0 8.333 8.334 8.326 8.326 0 0 0 8.334-8.334A8.326 8.326 0 0 0 9 .666Zm0 15a6.676 6.676 0 0 1-6.666-6.667A6.676 6.676 0 0 1 9 2.333a6.676 6.676 0 0 1 6.667 6.666A6.676 6.676 0 0 1 9 15.666Z" fill="#D93025" fill-rule="evenodd"></path></svg> </psd-status-icon> </td> <td>12 Jun 2025</td> <td>17:33 PDT</td> <td><p>The impact on Personalized Service Health is now resolved and the updates should be reflected without any issues.</p>
<p>The issue with Google Cloud Dataflow is fully resolved as of 17:10 PDT</p>
<p>The only remaining impact is on Vertex AI Online Prediction as follows:</p>
<p><strong>Vertex AI Online Prediction:</strong> Customers may continue to experience elevated 5xx errors with some of the models available in the Model Garden. We are seeing gradual decrease in error rates as our engineers perform appropriate mitigation actions.</p>
<p>The ETA for full resolution of these 5xx errors is 22:00 PDT</p>
<p>We will provide an update by Thursday, 2025-06-12 22:00 PDT with current details.</p>
</td> </tr><tr> <td> <psd-status-icon> <svg fill="none" aria-label="Outage status" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M11.16 5.666 9 7.824 6.843 5.666 5.668 6.841l2.158 2.158-2.158 2.159 1.175 1.175 2.158-2.159 2.159 2.159 1.175-1.175-2.159-2.159 2.159-2.158-1.175-1.175ZM9 .666A8.326 8.326 0 0 0 .668 8.999a8.326 8.326 0 0 0 8.333 8.334 8.326 8.326 0 0 0 8.334-8.334A8.326 8.326 0 0 0 9 .666Zm0 15a6.676 6.676 0 0 1-6.666-6.667A6.676 6.676 0 0 1 9 2.333a6.676 6.676 0 0 1 6.667 6.666A6.676 6.676 0 0 1 9 15.666Z" fill="#D93025" fill-rule="evenodd"></path></svg> </psd-status-icon> </td> <td>12 Jun 2025</td> <td>17:06 PDT</td> <td><p>The following Google Cloud products are still experiencing residual impact:</p>
<p><strong>Google Cloud Dataflow:</strong>  Dataflow backlog has cleared up in all regions except us-central1.  Customers may experience delays with Dataflow operations in us-central1 as the backlog  clears up gradually. We do not have an ETA for Cloud Dataflow recovery in us-central1.</p>
<p><strong>Vertex AI Online Prediction:</strong> Customers may continue to experience elevated 5xx errors with some of the models available in the Model Garden. We are seeing gradual decrease in error rates as our engineers perform appropriate mitigation actions. The ETA for full resolution of these 5xx errors is 22:00 PDT</p>
<p><strong>Personalized Service Health:</strong> Updates on the Personalized Service Health are delayed and we recommend customers to continue using Cloud Service Health dashboard for updates.</p>
<p>We will provide an update by Thursday, 2025-06-12 17:45 PDT with current details.</p>
</td> </tr><tr> <td> <psd-status-icon> <svg fill="none" aria-label="Outage status" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M11.16 5.666 9 7.824 6.843 5.666 5.668 6.841l2.158 2.158-2.158 2.159 1.175 1.175 2.158-2.159 2.159 2.159 1.175-1.175-2.159-2.159 2.159-2.158-1.175-1.175ZM9 .666A8.326 8.326 0 0 0 .668 8.999a8.326 8.326 0 0 0 8.333 8.334 8.326 8.326 0 0 0 8.334-8.334A8.326 8.326 0 0 0 9 .666Zm0 15a6.676 6.676 0 0 1-6.666-6.667A6.676 6.676 0 0 1 9 2.333a6.676 6.676 0 0 1 6.667 6.666A6.676 6.676 0 0 1 9 15.666Z" fill="#D93025" fill-rule="evenodd"></path></svg> </psd-status-icon> </td> <td>12 Jun 2025</td> <td>16:13 PDT</td> <td><p>The following Google Cloud products are still experiencing residual impact:</p>
<p><strong>Google Cloud Dataflow:</strong>  Customers may experience delays with Dataflow operations as the backlog is clearing up gradually.</p>
<p><strong>Vertex AI Online Prediction:</strong> Customers may continue to experience elevated 5xx errors with some of the models available in the Model Garden.</p>
<p><strong>Personalized Service Health:</strong> Updates on the Personalized Service Health are delayed and we recommend customers to continue using Cloud Service Health dashboard for updates.</p>
<p>We currently do not have an ETA for full mitigation of the above services.</p>
<p>We will provide an update by Thursday, 2025-06-12 17:00 PDT with current details.</p>
</td> </tr><tr> <td> <psd-status-icon> <svg fill="none" aria-label="Outage status" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M11.16 5.666 9 7.824 6.843 5.666 5.668 6.841l2.158 2.158-2.158 2.159 1.175 1.175 2.158-2.159 2.159 2.159 1.175-1.175-2.159-2.159 2.159-2.158-1.175-1.175ZM9 .666A8.326 8.326 0 0 0 .668 8.999a8.326 8.326 0 0 0 8.333 8.334 8.326 8.326 0 0 0 8.334-8.334A8.326 8.326 0 0 0 9 .666Zm0 15a6.676 6.676 0 0 1-6.666-6.667A6.676 6.676 0 0 1 9 2.333a6.676 6.676 0 0 1 6.667 6.666A6.676 6.676 0 0 1 9 15.666Z" fill="#D93025" fill-rule="evenodd"></path></svg> </psd-status-icon> </td> <td>12 Jun 2025</td> <td>15:16 PDT</td> <td><p>Most of the Google Cloud products are fully recovered as of 13:45 PDT.</p>
<p>There is some residual impact for the products currently marked as affected on the dashboard. Please continue to monitor the services and the dashboard for individual product recoveries.</p>
<p>We will provide an update by Thursday, 2025-06-12 16:00 PDT with current details.</p>
</td> </tr><tr> <td> <psd-status-icon> <svg fill="none" aria-label="Outage status" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M11.16 5.666 9 7.824 6.843 5.666 5.668 6.841l2.158 2.158-2.158 2.159 1.175 1.175 2.158-2.159 2.159 2.159 1.175-1.175-2.159-2.159 2.159-2.158-1.175-1.175ZM9 .666A8.326 8.326 0 0 0 .668 8.999a8.326 8.326 0 0 0 8.333 8.334 8.326 8.326 0 0 0 8.334-8.334A8.326 8.326 0 0 0 9 .666Zm0 15a6.676 6.676 0 0 1-6.666-6.667A6.676 6.676 0 0 1 9 2.333a6.676 6.676 0 0 1 6.667 6.666A6.676 6.676 0 0 1 9 15.666Z" fill="#D93025" fill-rule="evenodd"></path></svg> </psd-status-icon> </td> <td>12 Jun 2025</td> <td>14:23 PDT</td> <td><p>Most of the Google Cloud products have confirmed full service recovery.</p>
<p>A few services are still seeing some residual impact and the respective engineering teams are actively working on recovery of those services.</p>
<p>We expect the recovery to complete in less than an hour.</p>
<p>We will provide an update by Thursday, 2025-06-12 15:00 PDT with current details.</p>
</td> </tr><tr> <td> <psd-status-icon> <svg fill="none" aria-label="Outage status" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M11.16 5.666 9 7.824 6.843 5.666 5.668 6.841l2.158 2.158-2.158 2.159 1.175 1.175 2.158-2.159 2.159 2.159 1.175-1.175-2.159-2.159 2.159-2.158-1.175-1.175ZM9 .666A8.326 8.326 0 0 0 .668 8.999a8.326 8.326 0 0 0 8.333 8.334 8.326 8.326 0 0 0 8.334-8.334A8.326 8.326 0 0 0 9 .666Zm0 15a6.676 6.676 0 0 1-6.666-6.667A6.676 6.676 0 0 1 9 2.333a6.676 6.676 0 0 1 6.667 6.666A6.676 6.676 0 0 1 9 15.666Z" fill="#D93025" fill-rule="evenodd"></path></svg> </psd-status-icon> </td> <td>12 Jun 2025</td> <td>14:00 PDT</td> <td><p>We have implemented mitigation for the issue in us-central1 and multi-region/us and we are seeing signs of recovery.</p>
<p>We have received confirmation from our internal monitoring and customers that the Google Cloud products are also seeing recovery in multiple regions and are also seeing signs of some recovery in us-central1 and mutli-region/us.</p>
<p>We expect the recovery to complete in less than an hour.</p>
<p>We will provide an update by Thursday, 2025-06-12 14:30 PDT with current details.</p>
</td> </tr><tr> <td> <psd-status-icon> <svg fill="none" aria-label="Outage status" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M11.16 5.666 9 7.824 6.843 5.666 5.668 6.841l2.158 2.158-2.158 2.159 1.175 1.175 2.158-2.159 2.159 2.159 1.175-1.175-2.159-2.159 2.159-2.158-1.175-1.175ZM9 .666A8.326 8.326 0 0 0 .668 8.999a8.326 8.326 0 0 0 8.333 8.334 8.326 8.326 0 0 0 8.334-8.334A8.326 8.326 0 0 0 9 .666Zm0 15a6.676 6.676 0 0 1-6.666-6.667A6.676 6.676 0 0 1 9 2.333a6.676 6.676 0 0 1 6.667 6.666A6.676 6.676 0 0 1 9 15.666Z" fill="#D93025" fill-rule="evenodd"></path></svg> </psd-status-icon> </td> <td>12 Jun 2025</td> <td>13:16 PDT</td> <td><p>We have identified the root cause and applied appropriate mitigations.
Our infrastructure has recovered in all regions except us-central1.</p>
<p>Google Cloud products that rely on the affected infrastructure are seeing recovery in multiple locations.</p>
<p>Our engineers are aware of the customers still experiencing issues on us-central1 and multi-region/us and are actively working on full recovery.</p>
<p>We do not have an ETA for full recovery.</p>
<p>We will provide an update by Thursday, 2025-06-12 14:00 PDT with current details.</p>
</td> </tr><tr> <td> <psd-status-icon> <svg fill="none" aria-label="Outage status" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M11.16 5.666 9 7.824 6.843 5.666 5.668 6.841l2.158 2.158-2.158 2.159 1.175 1.175 2.158-2.159 2.159 2.159 1.175-1.175-2.159-2.159 2.159-2.158-1.175-1.175ZM9 .666A8.326 8.326 0 0 0 .668 8.999a8.326 8.326 0 0 0 8.333 8.334 8.326 8.326 0 0 0 8.334-8.334A8.326 8.326 0 0 0 9 .666Zm0 15a6.676 6.676 0 0 1-6.666-6.667A6.676 6.676 0 0 1 9 2.333a6.676 6.676 0 0 1 6.667 6.666A6.676 6.676 0 0 1 9 15.666Z" fill="#D93025" fill-rule="evenodd"></path></svg> </psd-status-icon> </td> <td>12 Jun 2025</td> <td>12:41 PDT</td> <td><p>Our engineers have identified the root cause and have applied appropriate mitigations.</p>
<p>While our engineers have confirmed that the underlying dependency is recovered in all locations except us-central1, <em><strong>we are aware that customers are still experiencing varying degrees of impact on individual google cloud products</strong></em>. All the respective engineering teams are actively engaged and working on service recovery.</p>
<p>We do not have an ETA for full service recovery.</p>
<p>We will provide an update by Thursday, 2025-06-12 13:30 PDT with current details.</p>
</td> </tr><tr> <td> <psd-status-icon> <svg fill="none" aria-label="Outage status" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M11.16 5.666 9 7.824 6.843 5.666 5.668 6.841l2.158 2.158-2.158 2.159 1.175 1.175 2.158-2.159 2.159 2.159 1.175-1.175-2.159-2.159 2.159-2.158-1.175-1.175ZM9 .666A8.326 8.326 0 0 0 .668 8.999a8.326 8.326 0 0 0 8.333 8.334 8.326 8.326 0 0 0 8.334-8.334A8.326 8.326 0 0 0 9 .666Zm0 15a6.676 6.676 0 0 1-6.666-6.667A6.676 6.676 0 0 1 9 2.333a6.676 6.676 0 0 1 6.667 6.666A6.676 6.676 0 0 1 9 15.666Z" fill="#D93025" fill-rule="evenodd"></path></svg> </psd-status-icon> </td> <td>12 Jun 2025</td> <td>12:30 PDT</td> <td><p>All locations except us-central1 have fully recovered. us-central1 is mostly recovered. We do not have an ETA for full recovery in us-central1.</p>
<p>We will provide an update by Thursday, 2025-06-12 13:00 PDT with current details.</p>
</td> </tr><tr> <td> <psd-status-icon> <svg fill="none" aria-label="Outage status" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M11.16 5.666 9 7.824 6.843 5.666 5.668 6.841l2.158 2.158-2.158 2.159 1.175 1.175 2.158-2.159 2.159 2.159 1.175-1.175-2.159-2.159 2.159-2.158-1.175-1.175ZM9 .666A8.326 8.326 0 0 0 .668 8.999a8.326 8.326 0 0 0 8.333 8.334 8.326 8.326 0 0 0 8.334-8.334A8.326 8.326 0 0 0 9 .666Zm0 15a6.676 6.676 0 0 1-6.666-6.667A6.676 6.676 0 0 1 9 2.333a6.676 6.676 0 0 1 6.667 6.666A6.676 6.676 0 0 1 9 15.666Z" fill="#D93025" fill-rule="evenodd"></path></svg> </psd-status-icon> </td> <td>12 Jun 2025</td> <td>12:09 PDT</td> <td><p>Our engineers are continuing to mitigate the issue and we have confirmation that the issue is recovered in some locations.</p>
<p>We do not have an ETA on full mitigation at this point.</p>
<p>We will provide an update by Thursday, 2025-06-12 12:45 PDT with current details.</p>
</td> </tr><tr> <td> <psd-status-icon> <svg fill="none" aria-label="Outage status" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M11.16 5.666 9 7.824 6.843 5.666 5.668 6.841l2.158 2.158-2.158 2.159 1.175 1.175 2.158-2.159 2.159 2.159 1.175-1.175-2.159-2.159 2.159-2.158-1.175-1.175ZM9 .666A8.326 8.326 0 0 0 .668 8.999a8.326 8.326 0 0 0 8.333 8.334 8.326 8.326 0 0 0 8.334-8.334A8.326 8.326 0 0 0 9 .666Zm0 15a6.676 6.676 0 0 1-6.666-6.667A6.676 6.676 0 0 1 9 2.333a6.676 6.676 0 0 1 6.667 6.666A6.676 6.676 0 0 1 9 15.666Z" fill="#D93025" fill-rule="evenodd"></path></svg> </psd-status-icon> </td> <td>12 Jun 2025</td> <td>11:59 PDT</td> <td><p><strong>Summary:</strong>
Multiple GCP products are experiencing Service issues with API requests</p>
<p><strong>Description</strong>
We are experiencing service issues with multiple GCP products beginning at Thursday, 2025-06-12 10:51 PDT.</p>
<p>Our engineering team continues to investigate the issue.</p>
<p>We will provide an update by Thursday, 2025-06-12 12:15 PDT with current details.</p>
<p>We apologize to all who are affected by the disruption.</p>
<p><strong>Symptoms:</strong>
Multiple GCP products are experiencing varying level of service impacts with API requests.</p>
<p><strong>Workaround:</strong>
None at this time.</p>
</td> </tr><tr> <td> <psd-status-icon> <svg fill="none" aria-label="Outage status" height="18" width="18" xmlns="http://www.w3.org/2000/svg"><path clip-rule="evenodd" d="M11.16 5.666 9 7.824 6.843 5.666 5.668 6.841l2.158 2.158-2.158 2.159 1.175 1.175 2.158-2.159 2.159 2.159 1.175-1.175-2.159-2.159 2.159-2.158-1.175-1.175ZM9 .666A8.326 8.326 0 0 0 .668 8.999a8.326 8.326 0 0 0 8.333 8.334 8.326 8.326 0 0 0 8.334-8.334A8.326 8.326 0 0 0 9 .666Zm0 15a6.676 6.676 0 0 1-6.666-6.667A6.676 6.676 0 0 1 9 2.333a6.676 6.676 0 0 1 6.667 6.666A6.676 6.676 0 0 1 9 15.666Z" fill="#D93025" fill-rule="evenodd"></path></svg> </psd-status-icon> </td> <td>12 Jun 2025</td> <td>11:46 PDT</td> <td><p><strong>Summary:</strong>
Multiple GCP products are experiencing Service issues</p>
<p><strong>Description</strong>
We are experiencing service issues with multiple GCP products beginning at Thursday, 2025-06-12 10:51 PDT.</p>
<p>Our engineering team continues to investigate the issue.</p>
<p>We will provide an update by Thursday, 2025-06-12 12:15 PDT with current details.</p>
<p>We apologize to all who are affected by the disruption.</p>
<p><strong>Symptoms:</strong>
Multiple GCP products are experiencing varying level of service impacts.</p>
<p><strong>Workaround:</strong>
None at this time.</p>
</td> </tr> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[$100 Hamburger (121 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/$100_hamburger</link>
            <guid>44274031</guid>
            <pubDate>Sat, 14 Jun 2025 03:41:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/$100_hamburger">https://en.wikipedia.org/wiki/$100_hamburger</a>, See on <a href="https://news.ycombinator.com/item?id=44274031">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div lang="en" dir="ltr" id="mw-content-text">

<p><i><b>$100 hamburger</b></i> ("hundred-dollar hamburger") is <a href="https://en.wikipedia.org/wiki/Aviation" title="Aviation">aviation</a> <a href="https://en.wikipedia.org/wiki/Slang" title="Slang">slang</a> for the excuse a <a href="https://en.wikipedia.org/wiki/General_aviation" title="General aviation">general aviation</a> pilot might use to fly.<sup id="cite_ref-1"><a href="#cite_note-1"><span>[</span>1<span>]</span></a></sup><sup id="cite_ref-2"><a href="#cite_note-2"><span>[</span>2<span>]</span></a></sup>
</p>

<p>A $100 hamburger trip typically involves flying a short distance (less than two hours), eating at an <a href="https://en.wikipedia.org/wiki/Airport" title="Airport">airport</a> <a href="https://en.wikipedia.org/wiki/Restaurant" title="Restaurant">restaurant</a>, and then flying home.  "$100" originally<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Dates_and_numbers#Chronological_items" title="Wikipedia:Manual of Style/Dates and numbers"><span title="The time period mentioned near this tag is ambiguous.">when?</span></a></i>]</sup> referred to the approximate cost of <a href="https://en.wikipedia.org/wiki/Renting" title="Renting">renting</a> or operating a light general aviation <a href="https://en.wikipedia.org/wiki/Aircraft" title="Aircraft">aircraft</a>, such as a <a href="https://en.wikipedia.org/wiki/Cessna_172" title="Cessna 172">Cessna 172</a>, for the time it took to fly round-trip to a nearby airport.  However, increasing <a href="https://en.wikipedia.org/wiki/Jet_fuel" title="Jet fuel">fuel</a> prices have since caused an increase in hourly operating costs for most <a href="https://en.wikipedia.org/wiki/Airplane" title="Airplane">airplanes</a>, and a Cessna 172 now<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Dates_and_numbers#Chronological_items" title="Wikipedia:Manual of Style/Dates and numbers"><span title="The time period mentioned near this tag is ambiguous.">when?</span></a></i>]</sup> costs US$95–180<sup id="cite_ref-3"><a href="#cite_note-3"><span>[</span>3<span>]</span></a></sup> per <a href="https://en.wikipedia.org/wiki/Hobbs_meter" title="Hobbs meter">Hobbs</a> hour to rent, including fuel.<sup id="cite_ref-4"><a href="#cite_note-4"><span>[</span>4<span>]</span></a></sup>
</p><p>In <a href="https://en.wikipedia.org/wiki/Perth,_Western_Australia" title="Perth, Western Australia">Perth, Western Australia</a>, a similar mentality resulted in the 'Rotto Bun Run'. A group of pilots who had run out of <a href="https://en.wikipedia.org/wiki/Hot_cross_bun" title="Hot cross bun">hot cross buns</a> on <a href="https://en.wikipedia.org/wiki/Good_Friday" title="Good Friday">Good Friday</a> decided to fly to the closest open bakery on <a href="https://en.wikipedia.org/wiki/Rottnest_Island" title="Rottnest Island">Rottnest Island</a>. The run is now an annual charity event.<sup id="cite_ref-5"><a href="#cite_note-5"><span>[</span>5<span>]</span></a></sup>
</p>

<div><ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span><cite id="CITEREFPreusch2007">Preusch, Matthew (October 26, 2007). <a rel="nofollow" href="https://www.nytimes.com/2007/10/26/travel/escapes/26burger.html">"Cleared for Lunching: The $100 Hamburger"</a>. <i><a href="https://en.wikipedia.org/wiki/The_New_York_Times" title="The New York Times">The New York Times</a></i><span>. Retrieved <span>May 16,</span> 2011</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=Cleared+for+Lunching%3A+The+%24100+Hamburger&amp;rft.date=2007-10-26&amp;rft.aulast=Preusch&amp;rft.aufirst=Matthew&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2007%2F10%2F26%2Ftravel%2Fescapes%2F26burger.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A%24100+hamburger"></span></span>
</li>
<li id="cite_note-2"><span><b><a href="#cite_ref-2">^</a></b></span> <span><cite id="CITEREFDaniels2020">Daniels, Lauren Drewes (2020-02-17). <a rel="nofollow" href="https://www.dallasobserver.com/restaurants/100-burgers-a-plane-ride-away-from-dallas-11870397">"Come Fly With Us to Pilots' Favorite '$100 Hamburgers'<span></span>"</a>. <i>Dallas Observer</i><span>. Retrieved <span>2021-07-06</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Dallas+Observer&amp;rft.atitle=Come+Fly+With+Us+to+Pilots%27+Favorite+%27%24100+Hamburgers%27&amp;rft.date=2020-02-17&amp;rft.aulast=Daniels&amp;rft.aufirst=Lauren+Drewes&amp;rft_id=https%3A%2F%2Fwww.dallasobserver.com%2Frestaurants%2F100-burgers-a-plane-ride-away-from-dallas-11870397&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A%24100+hamburger"></span></span>
</li>
<li id="cite_note-3"><span><b><a href="#cite_ref-3">^</a></b></span> <span><cite><a rel="nofollow" href="https://sancarlosflight.com/fleet/">"Fleet Aircraft"</a>. <i>San Carlos Flight Center</i><span>. Retrieved <span>2020-04-29</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=San+Carlos+Flight+Center&amp;rft.atitle=Fleet+Aircraft&amp;rft_id=https%3A%2F%2Fsancarlosflight.com%2Ffleet%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A%24100+hamburger"></span></span>
</li>
<li id="cite_note-4"><span><b><a href="#cite_ref-4">^</a></b></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20220523194457/https://flights-inc.com/fleet.asp?cat=0">"Flights Inc. - Flight Training and Aircraft Rental"</a>. <i>www.flights-inc.com</i>. Archived from <a rel="nofollow" href="http://www.flights-inc.com/fleet.asp?cat=0">the original</a> on 2022-05-23<span>. Retrieved <span>2017-06-02</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.flights-inc.com&amp;rft.atitle=Flights+Inc.+-+Flight+Training+and+Aircraft+Rental&amp;rft_id=http%3A%2F%2Fwww.flights-inc.com%2Ffleet.asp%3Fcat%3D0&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A%24100+hamburger"></span></span>
</li>
<li id="cite_note-5"><span><b><a href="#cite_ref-5">^</a></b></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20110217035257/http://royalaeroclubwa.com.au/club-flying/commemorative-flights"><i>Royal Aero Club of Western Australia - Commemorative Flights</i></a>, 2011-01-30, archived from <a rel="nofollow" href="http://www.royalaeroclubwa.com.au/club-flying/commemorative-flights">the original</a> on 2011-02-17</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Royal+Aero+Club+of+Western+Australia+-+Commemorative+Flights&amp;rft.date=2011-01-30&amp;rft_id=http%3A%2F%2Fwww.royalaeroclubwa.com.au%2Fclub-flying%2Fcommemorative-flights&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A%24100+hamburger"></span></span>
</li>
</ol></div>

<!-- 
NewPP limit report
Parsed by mw‐web.eqiad.main‐5c65c9987f‐4cwz4
Cached time: 20250614114711
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.251 seconds
Real time usage: 0.342 seconds
Preprocessor visited node count: 726/1000000
Revision size: 2605/2097152 bytes
Post‐expand include size: 15639/2097152 bytes
Template argument size: 789/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 1/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 23697/5000000 bytes
Lua time usage: 0.176/10.000 seconds
Lua memory usage: 4840126/52428800 bytes
Number of Wikibase entities loaded: 0/500
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  317.287      1 -total
 34.01%  107.899      1 Template:Reflist
 24.74%   78.500      1 Template:Aviation-stub
 24.17%   76.680      1 Template:Asbox
 22.81%   72.380      1 Template:Cite_news
 20.06%   63.654      1 Template:Short_description
 11.40%   36.183      2 Template:When
 11.19%   35.502      2 Template:Pagetype
 10.00%   31.742      1 Template:Fix
  8.79%   27.891      1 Template:Wikt
-->

<!-- Saved in parser cache with key enwiki:pcache:1828689:|#|:idhash:canonical and timestamp 20250614114711 and revision id 1295543766. Rendering was triggered because: page-view
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SIMD-friendly algorithms for substring searching (192 pts)]]></title>
            <link>http://0x80.pl/notesen/2016-11-28-simd-strfind.html</link>
            <guid>44274001</guid>
            <pubDate>Sat, 14 Jun 2025 03:31:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://0x80.pl/notesen/2016-11-28-simd-strfind.html">http://0x80.pl/notesen/2016-11-28-simd-strfind.html</a>, See on <a href="https://news.ycombinator.com/item?id=44274001">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="simd-friendly-algorithms-for-substring-searching">

<table>
<colgroup><col>
<col>
</colgroup><tbody>
<tr><th>Author:</th>
<td>Wojciech Muła</td></tr>
<tr><th>Added on:</th><td>2016-11-28</td>
</tr>
<tr><th>Updated on:</th><td>2018-02-14 (spelling), 2017-04-29 (ARMv8 results)</td>
</tr>
</tbody>
</table>
<div id="introduction">
<h2>Introduction</h2>
<p>Popular programming languages provide methods or functions which locate a
substring in a given string. In C it is the function <tt>strstr</tt>, the C++
class <tt><span>std::string</span></tt> has the method <tt>find</tt>, Python's <tt>string</tt> has methods
<tt>pos</tt> and <tt>index</tt>, and so on, so forth. All these APIs were designed for
<strong>one-shot searches</strong>.  During past decades several algorithms to solve this
problem were designed, an excellent page by <strong>Christian Charras</strong> and
<strong>Thierry Lecroq</strong> <a href="http://www-igm.univ-mlv.fr/~lecroq/string/">lists most of them</a> (if not all). Basically these
algorithms could be split into two major categories: (1) based on
Deterministic Finite Automaton, like Knuth-Morris-Pratt, Boyer Moore, etc.,
and (2) based on a simple comparison, like the Karp-Rabin algorithm.</p>
<p>The main problem with these standard algorithms is a silent assumption
that comparing a pair of characters, looking up in an extra table and
conditions are cheap, while comparing two substrings is expansive.</p>
<p>But current desktop CPUs do not meet this assumption, in particular:</p>
<ul>
<li>There is no difference in comparing one, two, four or 8 bytes on a 64-bit
CPU.  When a processor supports SIMD instructions, then comparing vectors
(it means 16, 32 or even 64 bytes) is as cheap as comparing a single byte.</li>
<li>Thus comparing short sequences of chars can be faster than fancy algorithms
which avoids such comparison.</li>
<li>Looking up in a table costs one memory fetch, so at least a L1 cache round
(~3 cycles). Reading char-by-char also cost as much cycles.</li>
<li>Mispredicted jumps cost several cycles of penalty (~10-20 cycles).</li>
<li>There is a short chain of dependencies: read char, compare it, conditionally
jump, which make hard to utilize out-of-order execution capabilities present
in a CPU.</li>
</ul>
<div id="contents">
<p>Contents</p>
<ul>
<li><a href="#introduction" id="toc-entry-1">Introduction</a></li>
<li><a href="#solution" id="toc-entry-2">Solution</a></li>
<li><a href="#algorithm-1-generic-simd" id="toc-entry-3">Algorithm 1: Generic SIMD</a><ul>
<li><a href="#algorithm" id="toc-entry-4">Algorithm</a><ul>
<li><a href="#example" id="toc-entry-5">Example</a></li>
<li><a href="#first-and-last" id="toc-entry-6">First and last?</a></li>
</ul>
</li>
<li><a href="#implementation" id="toc-entry-7">Implementation</a><ul>
<li><a href="#sse-avx2" id="toc-entry-8">SSE &amp; AVX2</a></li>
<li><a href="#swar" id="toc-entry-9">SWAR</a></li>
<li><a href="#avx512f" id="toc-entry-10">AVX512F</a></li>
<li><a href="#arm-neon-32-bit-code" id="toc-entry-11">ARM Neon (32 bit code)</a></li>
<li><a href="#aarch64-64-bit-code" id="toc-entry-12">AArch64 (64 bit code)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#algorithm-2-sse-specific-mpsadbw" id="toc-entry-13">Algorithm 2: SSE-specific (MPSADBW)</a><ul>
<li><a href="#algorithm-1" id="toc-entry-14">Algorithm</a></li>
<li><a href="#implementation-1" id="toc-entry-15">Implementation</a><ul>
<li><a href="#sse" id="toc-entry-16">SSE</a></li>
<li><a href="#avx512f-1" id="toc-entry-17">AVX512F</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#algorithm-3-sse4-2-specific-pcmpestrm" id="toc-entry-18">Algorithm 3: SSE4.2-specific (PCMPESTRM)</a><ul>
<li><a href="#algorithm-2" id="toc-entry-19">Algorithm</a></li>
<li><a href="#implementation-2" id="toc-entry-20">Implementation</a><ul>
<li><a href="#sse-1" id="toc-entry-21">SSE</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#performance-results" id="toc-entry-22">Performance results</a><ul>
<li><a href="#x64-computers" id="toc-entry-23">x64 computers</a></li>
<li><a href="#arm-computers" id="toc-entry-24">ARM computers</a></li>
<li><a href="#conclusions-and-remarks" id="toc-entry-25">Conclusions and remarks</a></li>
</ul>
</li>
<li><a href="#acknowledgments" id="toc-entry-26">Acknowledgments</a></li>
<li><a href="#source-code" id="toc-entry-27">Source code</a></li>
<li><a href="#history" id="toc-entry-28">History</a></li>
</ul>
</div>
</div>
<div id="solution">
<h2>Solution</h2>
<p>This article shows two approaches utilizing SIMD instructions which I've
already described in <a href="http://0x80.pl/notesen/2014-03-11-simd-friendly-karp-rabin.html">SIMD-friendly Rabin-Karp modification</a> and <a href="http://0x80.pl/notesen/2008-05-27-sse4-substring-locate.html">SSE4
string search — modification of Karp-Rabin algorithm</a>.  I merged the
articles, compared these two methods and extended material.  Article shows
also performance results for various implementations, ranging from
<a href="http://en.wikipedia.org/wiki/SWAR">SWAR</a> to AVX512F.</p>
<p>The Karp-Rabin algorithm does the exact substring comparison whenever <strong>weak
hashes</strong> are equal. One hash is calculated just once for searched substring,
and another one is calculated for string's portion; in every iteration the
second hash is updated at small cost. Following code shows the idea:</p>
<pre><span>k</span>  <span>:=</span> <span>substring</span> <span>length</span>
<span>h1</span> <span>:=</span> <span>hash</span><span>(</span><span>substring</span><span>)</span>
<span>h2</span> <span>:=</span> <span>hash</span><span>(</span><span>string</span><span>[</span><span>i</span> <span>..</span> <span>i</span> <span>+</span> <span>k</span><span>])</span>
<span>for</span> <span>i</span> <span>in</span> <span>0</span> <span>..</span> <span>n</span> <span>-</span> <span>k</span> <span>loop</span>
    <span>if</span> <span>h1</span> <span>==</span> <span>h2</span> <span>then</span>
        <span>if</span> <span>substring</span> <span>==</span> <span>string</span><span>[</span><span>i</span> <span>..</span> <span>i</span> <span>+</span> <span>k</span><span>]</span> <span>then</span>
            <span>return</span> <span>i</span>
        <span>end</span> <span>if</span>
    <span>end</span> <span>if</span>

    <span>h</span> <span>=</span> <span>next_hash</span><span>(</span><span>h</span><span>,</span> <span>...</span><span>)</span> <span>#</span> <span>this</span> <span>meant</span> <span>to</span> <span>be</span> <span>cheap</span>
<span>end</span> <span>loop</span>
</pre>
<p>SIMD solutions replace the hash predicate with <strong>a vector predicate</strong>, which
is calculated in parallel and, hopefully, is calculated fast.  For each
"true" element of the predicate vector an exact comparison of substrings is
performed.</p>
<p>This is one source of improvement, another is a careful implementation.
A generic implementation calls a function like <tt>memcmp</tt> to compare
substrings.  But while we know the length of searched substring, we may
provide specialisations for certain lengths, where a subprocedure call
is replaced by a few CPU instructions, even just one. Thanks to that
the cost of calling the procedure and all internal <tt>memcmp</tt> costs are
simply ridden off.</p>
</div>
<div id="algorithm-1-generic-simd">
<h2>Algorithm 1: Generic SIMD</h2>
<div id="algorithm">
<h2>Algorithm</h2>
<p>This algorithm is suitable for all SIMD instruction sets and also SWAR approach.  It
uses as a predicate equality of <strong>the first</strong> and <strong>the last</strong> characters from the
substring.</p>
<p>These two characters are populated in two registers, <strong>F</strong> and <strong>L</strong>
respectively.  Then in each iteration two chunks of strings are loaded.  The
first chunk (<strong>A</strong>) is read from offset <tt>i</tt> (where <tt>i</tt> is the current
offset) and the second chunk (<strong>B</strong>) is read from offset <tt>i + k - 1</tt>, where
<tt>k</tt> is substring's length.</p>
<p>Then we compute a vector expression <tt>F == A and B == L</tt>. This step yields a
byte vector (or a bit mask), where "true" values denote position of potential
substring occurrences.  Finally, just at these positions an exact comparisons of
substrings are performed.</p>
<div id="example">
<h3>Example</h3>
<p>Let's assume 8-byte registers. We're searching for word "cat", thus:</p>
<pre>F    = [ c | c | c | c | c | c | c | c ]
L    = [ t | t | t | t | t | t | t | t ]
</pre>
<p>We're searching in the string "a_cat_tries". In the first iteration the register
<strong>A</strong> gets data from offset 0, <strong>B</strong> from offset 2:</p>
<pre>A    = [ a | _ | c | a | t | _ | t | r ]
B    = [ c | a | t | _ | t | r | i | e ]
</pre>
<p>Now we compare:</p>
<pre>AF   = (A == F)
     = [ 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 ]

BL   = (B == L)
     = [ 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 ]
</pre>
<p>After merging comparison results, i.e. <tt>AF &amp; BL</tt>, we get following mask:</p>
<pre>mask = [ 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 ]
</pre>
<p>Since the mask is non-zero, it means there are possible substring occurrences.
As we see, there is only one non-zero element at index 2, thus only one
substring comparison must be performed.</p>
</div>
<div id="first-and-last">
<h3>First and last?</h3>
<p>Choosing the first and the last character from a substring is not always a
wise decision. Consider following scenario: a string contains mostly 'A'
characters, and a user wants to find "AjohndoeA" — in such situation the
number of char-wise would be large.</p>
<p>In order to prevent such situations an implementation can pick "last" character
as the farthest character not equal to the first one. If there is no such
character, it means that all characters in substring are the same (for example
"AAAAA"). A specialised procedure may be used to handle such patterns.</p>
</div>
</div>
<div id="implementation">
<h2>Implementation</h2>
<div id="sse-avx2">
<h3>SSE &amp; AVX2</h3>
<p>Both SSE and AVX2 versions are practically the same, and both use the
minimum number of instruction. Below is a generic AVX2 version.</p>
<p>It's worth to note that since we already know that the first and the last
characters match, we don't need to compare them again with <tt>memcmp</tt>.</p>
<pre><span>size_t</span><span> </span><span>avx2_strstr_anysize</span><span>(</span><span>const</span><span> </span><span>char</span><span>*</span><span> </span><span>s</span><span>,</span><span> </span><span>size_t</span><span> </span><span>n</span><span>,</span><span> </span><span>const</span><span> </span><span>char</span><span>*</span><span> </span><span>needle</span><span>,</span><span> </span><span>size_t</span><span> </span><span>k</span><span>)</span><span> </span><span>{</span><span>

    </span><span>const</span><span> </span><span>__m256i</span><span> </span><span>first</span><span> </span><span>=</span><span> </span><span>_mm256_set1_epi8</span><span>(</span><span>needle</span><span>[</span><span>0</span><span>]);</span><span>
    </span><span>const</span><span> </span><span>__m256i</span><span> </span><span>last</span><span>  </span><span>=</span><span> </span><span>_mm256_set1_epi8</span><span>(</span><span>needle</span><span>[</span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span>]);</span><span>

    </span><span>for</span><span> </span><span>(</span><span>size_t</span><span> </span><span>i</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>n</span><span>;</span><span> </span><span>i</span><span> </span><span>+=</span><span> </span><span>32</span><span>)</span><span> </span><span>{</span><span>

        </span><span>const</span><span> </span><span>__m256i</span><span> </span><span>block_first</span><span> </span><span>=</span><span> </span><span>_mm256_loadu_si256</span><span>(</span><span>reinterpret_cast</span><span>&lt;</span><span>const</span><span> </span><span>__m256i</span><span>*&gt;</span><span>(</span><span>s</span><span> </span><span>+</span><span> </span><span>i</span><span>));</span><span>
        </span><span>const</span><span> </span><span>__m256i</span><span> </span><span>block_last</span><span>  </span><span>=</span><span> </span><span>_mm256_loadu_si256</span><span>(</span><span>reinterpret_cast</span><span>&lt;</span><span>const</span><span> </span><span>__m256i</span><span>*&gt;</span><span>(</span><span>s</span><span> </span><span>+</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span>));</span><span>

        </span><span>const</span><span> </span><span>__m256i</span><span> </span><span>eq_first</span><span> </span><span>=</span><span> </span><span>_mm256_cmpeq_epi8</span><span>(</span><span>first</span><span>,</span><span> </span><span>block_first</span><span>);</span><span>
        </span><span>const</span><span> </span><span>__m256i</span><span> </span><span>eq_last</span><span>  </span><span>=</span><span> </span><span>_mm256_cmpeq_epi8</span><span>(</span><span>last</span><span>,</span><span> </span><span>block_last</span><span>);</span><span>

        </span><span>uint32_t</span><span> </span><span>mask</span><span> </span><span>=</span><span> </span><span>_mm256_movemask_epi8</span><span>(</span><span>_mm256_and_si256</span><span>(</span><span>eq_first</span><span>,</span><span> </span><span>eq_last</span><span>));</span><span>

        </span><span>while</span><span> </span><span>(</span><span>mask</span><span> </span><span>!=</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>

            </span><span>const</span><span> </span><span>auto</span><span> </span><span>bitpos</span><span> </span><span>=</span><span> </span><span>bits</span><span>::</span><span>get_first_bit_set</span><span>(</span><span>mask</span><span>);</span><span>

            </span><span>if</span><span> </span><span>(</span><span>memcmp</span><span>(</span><span>s</span><span> </span><span>+</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span> </span><span>+</span><span> </span><span>1</span><span>,</span><span> </span><span>needle</span><span> </span><span>+</span><span> </span><span>1</span><span>,</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>2</span><span>)</span><span> </span><span>==</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>
                </span><span>return</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span>;</span><span>
            </span><span>}</span><span>

            </span><span>mask</span><span> </span><span>=</span><span> </span><span>bits</span><span>::</span><span>clear_leftmost_set</span><span>(</span><span>mask</span><span>);</span><span>
        </span><span>}</span><span>
    </span><span>}</span><span>

    </span><span>return</span><span> </span><span>std</span><span>::</span><span>string</span><span>::</span><span>npos</span><span>;</span><span>
</span><span>}</span>
</pre>
</div>
<div id="swar">
<h3>SWAR</h3>
<p>In SWAR approach, comparison for equality uses bit xor operation, which yields zero when
two bytes are equal. Therefore instead of anding partial results, the bitwise
or is used. Clearly this part of algorithm has the same complexity as the
SSE/AVX2 code.</p>
<p>However, SWAR requires more effort to locate zero bytes. Following procedure
calculates <strong>an exact byte mask</strong>, where MSBs of <tt>zeros</tt> are set when the
corresponding byte in <tt>x</tt> is zero.</p>
<pre><span>// 7th bit set if lower 7 bits are zero
</span><span>const</span><span> </span><span>uint64_t</span><span> </span><span>t0</span><span> </span><span>=</span><span> </span><span>(</span><span>~</span><span>x</span><span> </span><span>&amp;</span><span> </span><span>0x7f7f7f7f7f7f7f7fllu</span><span>)</span><span> </span><span>+</span><span> </span><span>0x0101010101010101llu</span><span>;</span><span>
</span><span>// 7th bit set if 7th bit is zero
</span><span>const</span><span> </span><span>uint64_t</span><span> </span><span>t1</span><span> </span><span>=</span><span> </span><span>(</span><span>~</span><span>x</span><span> </span><span>&amp;</span><span> </span><span>0x8080808080808080llu</span><span>);</span><span>
</span><span>uint64_t</span><span> </span><span>zeros</span><span> </span><span>=</span><span> </span><span>t0</span><span> </span><span>&amp;</span><span> </span><span>t1</span><span>;</span>
</pre>
<p>Below is the C++ implementation for 64-bit vectors. The while loop contains an
additional condition which might look not optimal. But searching the first set
bit and later clearing it (as the SSE version does) is slower.</p>
<pre><span>size_t</span><span> </span><span>swar64_strstr_anysize</span><span>(</span><span>const</span><span> </span><span>char</span><span>*</span><span> </span><span>s</span><span>,</span><span> </span><span>size_t</span><span> </span><span>n</span><span>,</span><span> </span><span>const</span><span> </span><span>char</span><span>*</span><span> </span><span>needle</span><span>,</span><span> </span><span>size_t</span><span> </span><span>k</span><span>)</span><span> </span><span>{</span><span>

    </span><span>const</span><span> </span><span>uint64_t</span><span> </span><span>first</span><span> </span><span>=</span><span> </span><span>0x0101010101010101llu</span><span> </span><span>*</span><span> </span><span>static_cast</span><span>&lt;</span><span>uint8_t</span><span>&gt;</span><span>(</span><span>needle</span><span>[</span><span>0</span><span>]);</span><span>
    </span><span>const</span><span> </span><span>uint64_t</span><span> </span><span>last</span><span>  </span><span>=</span><span> </span><span>0x0101010101010101llu</span><span> </span><span>*</span><span> </span><span>static_cast</span><span>&lt;</span><span>uint8_t</span><span>&gt;</span><span>(</span><span>needle</span><span>[</span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span>]);</span><span>

    </span><span>uint64_t</span><span>*</span><span> </span><span>block_first</span><span> </span><span>=</span><span> </span><span>reinterpret_cast</span><span>&lt;</span><span>uint64_t</span><span>*&gt;</span><span>(</span><span>const_cast</span><span>&lt;</span><span>char</span><span>*&gt;</span><span>(</span><span>s</span><span>));</span><span>
    </span><span>uint64_t</span><span>*</span><span> </span><span>block_last</span><span>  </span><span>=</span><span> </span><span>reinterpret_cast</span><span>&lt;</span><span>uint64_t</span><span>*&gt;</span><span>(</span><span>const_cast</span><span>&lt;</span><span>char</span><span>*&gt;</span><span>(</span><span>s</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span>));</span><span>

    </span><span>for</span><span> </span><span>(</span><span>auto</span><span> </span><span>i</span><span>=</span><span>0u</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>n</span><span>;</span><span> </span><span>i</span><span>+=</span><span>8</span><span>,</span><span> </span><span>block_first</span><span>++</span><span>,</span><span> </span><span>block_last</span><span>++</span><span>)</span><span> </span><span>{</span><span>
        </span><span>const</span><span> </span><span>uint64_t</span><span> </span><span>eq</span><span> </span><span>=</span><span> </span><span>(</span><span>*</span><span>block_first</span><span> </span><span>^</span><span> </span><span>first</span><span>)</span><span> </span><span>|</span><span> </span><span>(</span><span>*</span><span>block_last</span><span> </span><span>^</span><span> </span><span>last</span><span>);</span><span>

        </span><span>const</span><span> </span><span>uint64_t</span><span> </span><span>t0</span><span> </span><span>=</span><span> </span><span>(</span><span>~</span><span>eq</span><span> </span><span>&amp;</span><span> </span><span>0x7f7f7f7f7f7f7f7fllu</span><span>)</span><span> </span><span>+</span><span> </span><span>0x0101010101010101llu</span><span>;</span><span>
        </span><span>const</span><span> </span><span>uint64_t</span><span> </span><span>t1</span><span> </span><span>=</span><span> </span><span>(</span><span>~</span><span>eq</span><span> </span><span>&amp;</span><span> </span><span>0x8080808080808080llu</span><span>);</span><span>
        </span><span>uint64_t</span><span> </span><span>zeros</span><span> </span><span>=</span><span> </span><span>t0</span><span> </span><span>&amp;</span><span> </span><span>t1</span><span>;</span><span>
        </span><span>size_t</span><span> </span><span>j</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>

        </span><span>while</span><span> </span><span>(</span><span>zeros</span><span>)</span><span> </span><span>{</span><span>
            </span><span>if</span><span> </span><span>(</span><span>zeros</span><span> </span><span>&amp;</span><span> </span><span>0x80</span><span>)</span><span> </span><span>{</span><span>
                </span><span>const</span><span> </span><span>char</span><span>*</span><span> </span><span>substr</span><span> </span><span>=</span><span> </span><span>reinterpret_cast</span><span>&lt;</span><span>char</span><span>*&gt;</span><span>(</span><span>block_first</span><span>)</span><span> </span><span>+</span><span> </span><span>j</span><span> </span><span>+</span><span> </span><span>1</span><span>;</span><span>
                </span><span>if</span><span> </span><span>(</span><span>memcmp</span><span>(</span><span>substr</span><span>,</span><span> </span><span>needle</span><span> </span><span>+</span><span> </span><span>1</span><span>,</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>2</span><span>)</span><span> </span><span>==</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>
                    </span><span>return</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>j</span><span>;</span><span>
                </span><span>}</span><span>
            </span><span>}</span><span>

            </span><span>zeros</span><span> </span><span>&gt;&gt;=</span><span> </span><span>8</span><span>;</span><span>
            </span><span>j</span><span> </span><span>+=</span><span> </span><span>1</span><span>;</span><span>
        </span><span>}</span><span>
    </span><span>}</span><span>

    </span><span>return</span><span> </span><span>std</span><span>::</span><span>string</span><span>::</span><span>npos</span><span>;</span><span>
</span><span>}</span>
</pre>
</div>
<div id="avx512f">
<h3>AVX512F</h3>
<p>AVX512F lacks of operations on bytes, the smallest vector item is a
32-bit word. The limitation forces us to use SWAR techniques.</p>
<ol>
<li>Using AVX512F instructions we compare two vectors, like in SWAR version,
i.e. two xors joined with bitwise or.
There is only one difference, a single <a href="http://0x80.pl/notesen/2015-03-22-avx512-ternary-functions.html">ternary logic instruction</a>
expresses one xor and bitwise or.</li>
<li>Using AVX512F instructions we locate which 32-bit elements
contain any zero byte.</li>
<li>Then for such 32-bit element check four substrings for equality.</li>
</ol>
<p>Unlike the SWAR procedure, where we need a precise mask for zero bytes, an
AVX512F procedure requires just information "a word has zero byte".  A
simpler algorithm, described in <a href="https://graphics.stanford.edu/~seander/bithacks.html">Bit Twiddling Hacks</a> is used; below
is its C++ implementation.</p>
<pre><span>__mmask16</span><span> </span><span>zero_byte_mask</span><span>(</span><span>const</span><span> </span><span>__m512i</span><span> </span><span>v</span><span>)</span><span> </span><span>{</span><span>

    </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>v01</span><span>  </span><span>=</span><span> </span><span>_mm512_set1_epi32</span><span>(</span><span>0x01010101u</span><span>);</span><span>
    </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>v80</span><span>  </span><span>=</span><span> </span><span>_mm512_set1_epi32</span><span>(</span><span>0x80808080u</span><span>);</span><span>

    </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>v1</span><span>   </span><span>=</span><span> </span><span>_mm512_sub_epi32</span><span>(</span><span>v</span><span>,</span><span> </span><span>v01</span><span>);</span><span>
    </span><span>// tmp1 = (v - 0x01010101) &amp; ~v &amp; 0x80808080
</span><span>    </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>tmp1</span><span> </span><span>=</span><span> </span><span>_mm512_ternarylogic_epi32</span><span>(</span><span>v1</span><span>,</span><span> </span><span>v</span><span>,</span><span> </span><span>v80</span><span>,</span><span> </span><span>0x20</span><span>);</span><span>

    </span><span>return</span><span> </span><span>_mm512_test_epi32_mask</span><span>(</span><span>tmp1</span><span>,</span><span> </span><span>tmp1</span><span>);</span><span>
</span><span>}</span>
</pre>
<p>Generic C++ implementation.</p>
<pre><span>#define _mm512_set1_epu8(c) _mm512_set1_epi32(uint32_t(c) * 0x01010101u)
</span><span>
</span><span>size_t</span><span> </span><span>avx512f_strstr_v2_anysize</span><span>(</span><span>const</span><span> </span><span>char</span><span>*</span><span> </span><span>string</span><span>,</span><span> </span><span>size_t</span><span> </span><span>n</span><span>,</span><span> </span><span>const</span><span> </span><span>char</span><span>*</span><span> </span><span>needle</span><span>,</span><span> </span><span>size_t</span><span> </span><span>k</span><span>)</span><span> </span><span>{</span><span>

    </span><span>assert</span><span>(</span><span>n</span><span> </span><span>&gt;</span><span> </span><span>0</span><span>);</span><span>
    </span><span>assert</span><span>(</span><span>k</span><span> </span><span>&gt;</span><span> </span><span>0</span><span>);</span><span>

    </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>first</span><span> </span><span>=</span><span> </span><span>_mm512_set1_epu8</span><span>(</span><span>needle</span><span>[</span><span>0</span><span>]);</span><span>
    </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>last</span><span>  </span><span>=</span><span> </span><span>_mm512_set1_epu8</span><span>(</span><span>needle</span><span>[</span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span>]);</span><span>

    </span><span>char</span><span>*</span><span> </span><span>haystack</span><span> </span><span>=</span><span> </span><span>const_cast</span><span>&lt;</span><span>char</span><span>*&gt;</span><span>(</span><span>string</span><span>);</span><span>
    </span><span>char</span><span>*</span><span> </span><span>end</span><span>      </span><span>=</span><span> </span><span>haystack</span><span> </span><span>+</span><span> </span><span>n</span><span>;</span><span>

    </span><span>for</span><span> </span><span>(</span><span>/**/</span><span>;</span><span> </span><span>haystack</span><span> </span><span>&lt;</span><span> </span><span>end</span><span>;</span><span> </span><span>haystack</span><span> </span><span>+=</span><span> </span><span>64</span><span>)</span><span> </span><span>{</span><span>

        </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>block_first</span><span> </span><span>=</span><span> </span><span>_mm512_loadu_si512</span><span>(</span><span>haystack</span><span> </span><span>+</span><span> </span><span>0</span><span>);</span><span>
        </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>block_last</span><span>  </span><span>=</span><span> </span><span>_mm512_loadu_si512</span><span>(</span><span>haystack</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span>);</span><span>

        </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>first_zeros</span><span> </span><span>=</span><span> </span><span>_mm512_xor_si512</span><span>(</span><span>block_first</span><span>,</span><span> </span><span>first</span><span>);</span><span>
        </span><span>// zeros = first_zeros | (block_last ^ last)
</span><span>        </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>zeros</span><span> </span><span>=</span><span> </span><span>_mm512_ternarylogic_epi32</span><span>(</span><span>first_zeros</span><span>,</span><span> </span><span>block_last</span><span>,</span><span> </span><span>last</span><span>,</span><span> </span><span>0xf6</span><span>);</span><span>

        </span><span>uint32_t</span><span> </span><span>mask</span><span> </span><span>=</span><span> </span><span>zero_byte_mask</span><span>(</span><span>zeros</span><span>);</span><span>
        </span><span>while</span><span> </span><span>(</span><span>mask</span><span>)</span><span> </span><span>{</span><span>

            </span><span>const</span><span> </span><span>uint64_t</span><span> </span><span>p</span><span> </span><span>=</span><span> </span><span>__builtin_ctz</span><span>(</span><span>mask</span><span>);</span><span>

            </span><span>if</span><span> </span><span>(</span><span>memcmp</span><span>(</span><span>haystack</span><span> </span><span>+</span><span> </span><span>4</span><span>*</span><span>p</span><span> </span><span>+</span><span> </span><span>0</span><span>,</span><span> </span><span>needle</span><span>,</span><span> </span><span>k</span><span>)</span><span> </span><span>==</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>
                </span><span>return</span><span> </span><span>(</span><span>haystack</span><span> </span><span>-</span><span> </span><span>string</span><span>)</span><span> </span><span>+</span><span> </span><span>4</span><span>*</span><span>p</span><span> </span><span>+</span><span> </span><span>0</span><span>;</span><span>
            </span><span>}</span><span>

            </span><span>if</span><span> </span><span>(</span><span>memcmp</span><span>(</span><span>haystack</span><span> </span><span>+</span><span> </span><span>4</span><span>*</span><span>p</span><span> </span><span>+</span><span> </span><span>1</span><span>,</span><span> </span><span>needle</span><span>,</span><span> </span><span>k</span><span>)</span><span> </span><span>==</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>
                </span><span>return</span><span> </span><span>(</span><span>haystack</span><span> </span><span>-</span><span> </span><span>string</span><span>)</span><span> </span><span>+</span><span> </span><span>4</span><span>*</span><span>p</span><span> </span><span>+</span><span> </span><span>1</span><span>;</span><span>
            </span><span>}</span><span>

            </span><span>if</span><span> </span><span>(</span><span>memcmp</span><span>(</span><span>haystack</span><span> </span><span>+</span><span> </span><span>4</span><span>*</span><span>p</span><span> </span><span>+</span><span> </span><span>2</span><span>,</span><span> </span><span>needle</span><span>,</span><span> </span><span>k</span><span>)</span><span> </span><span>==</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>
                </span><span>return</span><span> </span><span>(</span><span>haystack</span><span> </span><span>-</span><span> </span><span>string</span><span>)</span><span> </span><span>+</span><span> </span><span>4</span><span>*</span><span>p</span><span> </span><span>+</span><span> </span><span>2</span><span>;</span><span>
            </span><span>}</span><span>

            </span><span>if</span><span> </span><span>(</span><span>memcmp</span><span>(</span><span>haystack</span><span> </span><span>+</span><span> </span><span>4</span><span>*</span><span>p</span><span> </span><span>+</span><span> </span><span>3</span><span>,</span><span> </span><span>needle</span><span>,</span><span> </span><span>k</span><span>)</span><span> </span><span>==</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>
                </span><span>return</span><span> </span><span>(</span><span>haystack</span><span> </span><span>-</span><span> </span><span>string</span><span>)</span><span> </span><span>+</span><span> </span><span>4</span><span>*</span><span>p</span><span> </span><span>+</span><span> </span><span>3</span><span>;</span><span>
            </span><span>}</span><span>

            </span><span>mask</span><span> </span><span>=</span><span> </span><span>bits</span><span>::</span><span>clear_leftmost_set</span><span>(</span><span>mask</span><span>);</span><span>
        </span><span>}</span><span>
    </span><span>}</span><span>

    </span><span>return</span><span> </span><span>size_t</span><span>(</span><span>-1</span><span>);</span><span>
</span><span>}</span>
</pre>
</div>
<div id="arm-neon-32-bit-code">
<h3>ARM Neon (32 bit code)</h3>
<p>The algorithm can be also easily realised using ARM Neon instructions, having
128-bit SIMD registers.  The only problem is caused by long round trip from
the Neon unit back to the CPU.</p>
<p>It was solved by saving back the comparison result in a 64 bit word in memory:
lower nibbles come from the lower half of a SIMD register, likewise higher
nibbles come from the higher half of the register.</p>
<p>Comparison is done in two loops, separately for lower and higher nibbles.
This split is required to detect substring occurrences in the correct order.</p>
<p>Below is a sample implementation.</p>
<pre><span>size_t</span><span> </span><span>FORCE_INLINE</span><span> </span><span>neon_strstr_anysize</span><span>(</span><span>const</span><span> </span><span>char</span><span>*</span><span> </span><span>s</span><span>,</span><span> </span><span>size_t</span><span> </span><span>n</span><span>,</span><span> </span><span>const</span><span> </span><span>char</span><span>*</span><span> </span><span>needle</span><span>,</span><span> </span><span>size_t</span><span> </span><span>k</span><span>)</span><span> </span><span>{</span><span>

    </span><span>assert</span><span>(</span><span>k</span><span> </span><span>&gt;</span><span> </span><span>0</span><span>);</span><span>
    </span><span>assert</span><span>(</span><span>n</span><span> </span><span>&gt;</span><span> </span><span>0</span><span>);</span><span>

    </span><span>const</span><span> </span><span>uint8x16_t</span><span> </span><span>first</span><span> </span><span>=</span><span> </span><span>vdupq_n_u8</span><span>(</span><span>needle</span><span>[</span><span>0</span><span>]);</span><span>
    </span><span>const</span><span> </span><span>uint8x16_t</span><span> </span><span>last</span><span>  </span><span>=</span><span> </span><span>vdupq_n_u8</span><span>(</span><span>needle</span><span>[</span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span>]);</span><span>
    </span><span>const</span><span> </span><span>uint8x8_t</span><span>  </span><span>half</span><span>  </span><span>=</span><span> </span><span>vdup_n_u8</span><span>(</span><span>0x0f</span><span>);</span><span>

    </span><span>const</span><span> </span><span>uint8_t</span><span>*</span><span> </span><span>ptr</span><span> </span><span>=</span><span> </span><span>reinterpret_cast</span><span>&lt;</span><span>const</span><span> </span><span>uint8_t</span><span>*&gt;</span><span>(</span><span>s</span><span>);</span><span>

    </span><span>union</span><span> </span><span>{</span><span>
        </span><span>uint8_t</span><span>  </span><span>tmp</span><span>[</span><span>8</span><span>];</span><span>
        </span><span>uint32_t</span><span> </span><span>word</span><span>[</span><span>2</span><span>];</span><span>
    </span><span>};</span><span>

    </span><span>for</span><span> </span><span>(</span><span>size_t</span><span> </span><span>i</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>n</span><span>;</span><span> </span><span>i</span><span> </span><span>+=</span><span> </span><span>16</span><span>)</span><span> </span><span>{</span><span>

        </span><span>const</span><span> </span><span>uint8x16_t</span><span> </span><span>block_first</span><span> </span><span>=</span><span> </span><span>vld1q_u8</span><span>(</span><span>ptr</span><span> </span><span>+</span><span> </span><span>i</span><span>);</span><span>
        </span><span>const</span><span> </span><span>uint8x16_t</span><span> </span><span>block_last</span><span>  </span><span>=</span><span> </span><span>vld1q_u8</span><span>(</span><span>ptr</span><span> </span><span>+</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span>);</span><span>

        </span><span>const</span><span> </span><span>uint8x16_t</span><span> </span><span>eq_first</span><span> </span><span>=</span><span> </span><span>vceqq_u8</span><span>(</span><span>first</span><span>,</span><span> </span><span>block_first</span><span>);</span><span>
        </span><span>const</span><span> </span><span>uint8x16_t</span><span> </span><span>eq_last</span><span>  </span><span>=</span><span> </span><span>vceqq_u8</span><span>(</span><span>last</span><span>,</span><span> </span><span>block_last</span><span>);</span><span>
        </span><span>const</span><span> </span><span>uint8x16_t</span><span> </span><span>pred_16</span><span>  </span><span>=</span><span> </span><span>vandq_u8</span><span>(</span><span>eq_first</span><span>,</span><span> </span><span>eq_last</span><span>);</span><span>
        </span><span>const</span><span> </span><span>uint8x8_t</span><span> </span><span>pred_8</span><span>    </span><span>=</span><span> </span><span>vbsl_u8</span><span>(</span><span>half</span><span>,</span><span> </span><span>vget_low_u8</span><span>(</span><span>pred_16</span><span>),</span><span> </span><span>vget_high_u8</span><span>(</span><span>pred_16</span><span>));</span><span>

        </span><span>vst1_u8</span><span>(</span><span>tmp</span><span>,</span><span> </span><span>pred_8</span><span>);</span><span>

        </span><span>if</span><span> </span><span>((</span><span>word</span><span>[</span><span>0</span><span>]</span><span> </span><span>|</span><span> </span><span>word</span><span>[</span><span>1</span><span>])</span><span> </span><span>==</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>
            </span><span>continue</span><span>;</span><span>
        </span><span>}</span><span>

        </span><span>for</span><span> </span><span>(</span><span>int</span><span> </span><span>j</span><span>=</span><span>0</span><span>;</span><span> </span><span>j</span><span> </span><span>&lt;</span><span> </span><span>8</span><span>;</span><span> </span><span>j</span><span>++</span><span>)</span><span> </span><span>{</span><span>
            </span><span>if</span><span> </span><span>(</span><span>tmp</span><span>[</span><span>j</span><span>]</span><span> </span><span>&amp;</span><span> </span><span>0x0f</span><span>)</span><span> </span><span>{</span><span>
                </span><span>if</span><span> </span><span>(</span><span>memcmp</span><span>(</span><span>s</span><span> </span><span>+</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>j</span><span> </span><span>+</span><span> </span><span>1</span><span>,</span><span> </span><span>needle</span><span> </span><span>+</span><span> </span><span>1</span><span>,</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>2</span><span>)</span><span> </span><span>==</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>
                    </span><span>return</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>j</span><span>;</span><span>
                </span><span>}</span><span>
            </span><span>}</span><span>
        </span><span>}</span><span>

        </span><span>for</span><span> </span><span>(</span><span>int</span><span> </span><span>j</span><span>=</span><span>0</span><span>;</span><span> </span><span>j</span><span> </span><span>&lt;</span><span> </span><span>8</span><span>;</span><span> </span><span>j</span><span>++</span><span>)</span><span> </span><span>{</span><span>
            </span><span>if</span><span> </span><span>(</span><span>tmp</span><span>[</span><span>j</span><span>]</span><span> </span><span>&amp;</span><span> </span><span>0xf0</span><span>)</span><span> </span><span>{</span><span>
                </span><span>if</span><span> </span><span>(</span><span>memcmp</span><span>(</span><span>s</span><span> </span><span>+</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>j</span><span> </span><span>+</span><span> </span><span>1</span><span> </span><span>+</span><span> </span><span>8</span><span>,</span><span> </span><span>needle</span><span> </span><span>+</span><span> </span><span>1</span><span>,</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>2</span><span>)</span><span> </span><span>==</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>
                    </span><span>return</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>j</span><span> </span><span>+</span><span> </span><span>8</span><span>;</span><span>
                </span><span>}</span><span>
            </span><span>}</span><span>
        </span><span>}</span><span>
    </span><span>}</span><span>

    </span><span>return</span><span> </span><span>std</span><span>::</span><span>string</span><span>::</span><span>npos</span><span>;</span><span>
</span><span>}</span>
</pre>
<p>It appeared that unrolling the two inner loops brought about 1.2 speedup.</p>
</div>
<div id="aarch64-64-bit-code">
<h3>AArch64 (64 bit code)</h3>
<p>AArch64 code is almost the exact copy of the above ARM Neon procedure.
The only exception is direct reading of SIMD registers lanes, as the
architecture made this operation fast.</p>
</div>
</div>
</div>
<div id="algorithm-2-sse-specific-mpsadbw">
<h2>Algorithm 2: SSE-specific (MPSADBW)</h2>
<div id="algorithm-1">
<h2>Algorithm</h2>
<p>SSE4.1 and AVX2 provide instruction <tt>MPSADBW</tt>, which calculates eight
<a href="http://en.wikipedia.org/wiki/Taxicab_geometry">Manhattan distances</a> (L1) between given 4-byte
sub-vector from one register and eight subsequent 4-byte sub-vector from
second register. The instruction returns vector of eight words (16-bit values).</p>
<p>When two sub-vectors are equal, then the L1 distance is 0, and we may use this
property to locate possible substring locations. In other words <strong>equality of
four leading characters</strong> is used as a predicate.</p>
<p>Albeit it seems to be a stronger predicate than matching the first and the last
characters, a quadratic complexity is unavoidable.  For example, when the
searched string contains one letter "a", and we're looking for "aaaabcde", then
the predicate obviously will be true for all input characters.</p>
<p>If it isn't enough, there are following problems:</p>
<ul>
<li>This method handles substring not shorter than four characters.
Handling three-char substrings is viable, but require additional code.</li>
<li>SSE variant of <tt>MPSADBW</tt> processes only 8 bytes at once, while the
generic SIMD variant uses the whole register.</li>
<li>AVX2 variant of <tt>MPSADBW</tt> works on lanes, i.e. 128-bit helves of
a register rather than the whole 256-bit register. This imposes
additional code to properly load data.</li>
<li>Latency of the instruction is pretty hight — 5 or 7 cycles,
depending on CPU architecture. Luckily throughput is 1 or 2 cycles,
thus unrolling a loop can hide latency.</li>
</ul>
</div>
<div id="implementation-1">
<h2>Implementation</h2>
<div id="sse">
<h3>SSE</h3>
<p>The generic, simplest implementation.</p>
<pre><span>size_t</span><span> </span><span>sse4_strstr_anysize</span><span>(</span><span>const</span><span> </span><span>char</span><span>*</span><span> </span><span>s</span><span>,</span><span> </span><span>size_t</span><span> </span><span>n</span><span>,</span><span> </span><span>const</span><span> </span><span>char</span><span>*</span><span> </span><span>needle</span><span>,</span><span> </span><span>size_t</span><span> </span><span>needle_size</span><span>)</span><span> </span><span>{</span><span>

    </span><span>const</span><span> </span><span>__m128i</span><span> </span><span>prefix</span><span> </span><span>=</span><span> </span><span>_mm_loadu_si128</span><span>(</span><span>reinterpret_cast</span><span>&lt;</span><span>const</span><span> </span><span>__m128i</span><span>*&gt;</span><span>(</span><span>needle</span><span>));</span><span>
    </span><span>const</span><span> </span><span>__m128i</span><span> </span><span>zeros</span><span>  </span><span>=</span><span> </span><span>_mm_setzero_si128</span><span>();</span><span>

    </span><span>for</span><span> </span><span>(</span><span>size_t</span><span> </span><span>i</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>n</span><span>;</span><span> </span><span>i</span><span> </span><span>+=</span><span> </span><span>8</span><span>)</span><span> </span><span>{</span><span>

        </span><span>const</span><span> </span><span>__m128i</span><span> </span><span>data</span><span>   </span><span>=</span><span> </span><span>_mm_loadu_si128</span><span>(</span><span>reinterpret_cast</span><span>&lt;</span><span>const</span><span> </span><span>__m128i</span><span>*&gt;</span><span>(</span><span>s</span><span> </span><span>+</span><span> </span><span>i</span><span>));</span><span>
        </span><span>const</span><span> </span><span>__m128i</span><span> </span><span>result</span><span> </span><span>=</span><span> </span><span>_mm_mpsadbw_epu8</span><span>(</span><span>data</span><span>,</span><span> </span><span>prefix</span><span>,</span><span> </span><span>0</span><span>);</span><span>

        </span><span>const</span><span> </span><span>__m128i</span><span> </span><span>cmp</span><span>    </span><span>=</span><span> </span><span>_mm_cmpeq_epi16</span><span>(</span><span>result</span><span>,</span><span> </span><span>zeros</span><span>);</span><span>

        </span><span>unsigned</span><span> </span><span>mask</span><span> </span><span>=</span><span> </span><span>_mm_movemask_epi8</span><span>(</span><span>cmp</span><span>)</span><span> </span><span>&amp;</span><span> </span><span>0x5555</span><span>;</span><span>

        </span><span>while</span><span> </span><span>(</span><span>mask</span><span> </span><span>!=</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>

            </span><span>const</span><span> </span><span>auto</span><span> </span><span>bitpos</span><span> </span><span>=</span><span> </span><span>bits</span><span>::</span><span>get_first_bit_set</span><span>(</span><span>mask</span><span>)</span><span>/</span><span>2</span><span>;</span><span>

            </span><span>if</span><span> </span><span>(</span><span>memcmp</span><span>(</span><span>s</span><span> </span><span>+</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span> </span><span>+</span><span> </span><span>4</span><span>,</span><span> </span><span>needle</span><span> </span><span>+</span><span> </span><span>4</span><span>,</span><span> </span><span>needle_size</span><span> </span><span>-</span><span> </span><span>4</span><span>)</span><span> </span><span>==</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>
                </span><span>return</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span>;</span><span>
            </span><span>}</span><span>

            </span><span>mask</span><span> </span><span>=</span><span> </span><span>bits</span><span>::</span><span>clear_leftmost_set</span><span>(</span><span>mask</span><span>);</span><span>
        </span><span>}</span><span>
    </span><span>}</span><span>

    </span><span>return</span><span> </span><span>std</span><span>::</span><span>string</span><span>::</span><span>npos</span><span>;</span><span>
</span><span>}</span>
</pre>
</div>
<div id="avx512f-1">
<h3>AVX512F</h3>
<p>Although AVX512F doesn't support <tt>MPSADBW</tt> (AVX512BW defines it) we still
can use 4-byte prefix equality as a predicate, utilizing fact that 32-bit
elements are natively supported.</p>
<p>In each iteration we generate four AVX512 vectors containing all possible
4-byte prefixes. Example:</p>
<pre>string = "the-cat-tries-to-eat..."

vec0  = [ t | h | e | - ][ c | a | t | - ][ t | r | i | e ][ s | - | t | o ][ ... ]
vec1  = [ h | e | - | c ][ a | t | - | t ][ r | i | e | s ][ - | t | o | - ][ ... ]
vec2  = [ e | - | c | a ][ t | - | t | r ][ i | e | s | - ][ t | o | - | e ][ ... ]
vec3  = [ - | c | a | t ][ - | t | r | i ][ e | s | - | t ][ o | - ] e | a ][ ... ]
</pre>
<p>Vector <tt>vec0</tt> contains prefixes for position 0, 4, 8, 12, ...;
<tt>vec1</tt> — 1, 5, 9, 13, ..., <tt>vec2</tt> — 2, 6, 10, 14, ...;
<tt>vec3</tt> — 3, 7, 11, 15, etc.</p>
<p>Building each vector require two shifts and one bitwise or. In each iteration
four vector comparison are performed and then four bitmasks are examined. This
make a loop, which compares substrings, quite complicated.</p>
<p>Moreover, to properly fill the last elements of vectors we need four bytes
beyond vector.  This is accomplished by having two adjacent vectors per
iterations (one load per iteration is needed, though). Finally, instruction
<tt>VPALIGNR</tt> is used to extract required data.</p>
<pre><span>size_t</span><span> </span><span>avx512f_strstr_long</span><span>(</span><span>const</span><span> </span><span>char</span><span>*</span><span> </span><span>string</span><span>,</span><span> </span><span>size_t</span><span> </span><span>n</span><span>,</span><span> </span><span>const</span><span> </span><span>char</span><span>*</span><span> </span><span>needle</span><span>,</span><span> </span><span>size_t</span><span> </span><span>k</span><span>)</span><span> </span><span>{</span><span>

    </span><span>__m512i</span><span> </span><span>curr</span><span>;</span><span>
    </span><span>__m512i</span><span> </span><span>next</span><span>;</span><span>
    </span><span>__m512i</span><span> </span><span>v0</span><span>,</span><span> </span><span>v1</span><span>,</span><span> </span><span>v2</span><span>,</span><span> </span><span>v3</span><span>;</span><span>

    </span><span>char</span><span>*</span><span> </span><span>haystack</span><span> </span><span>=</span><span> </span><span>const_cast</span><span>&lt;</span><span>char</span><span>*&gt;</span><span>(</span><span>string</span><span>);</span><span>
    </span><span>char</span><span>*</span><span> </span><span>last</span><span>     </span><span>=</span><span> </span><span>haystack</span><span> </span><span>+</span><span> </span><span>n</span><span>;</span><span>

    </span><span>const</span><span> </span><span>uint32_t</span><span> </span><span>prf</span><span>   </span><span>=</span><span> </span><span>*</span><span>(</span><span>uint32_t</span><span>*</span><span>)</span><span>needle</span><span>;</span><span> </span><span>// the first 4 bytes of needle
</span><span>    </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>prefix</span><span> </span><span>=</span><span> </span><span>_mm512_set1_epi32</span><span>(</span><span>prf</span><span>);</span><span>

    </span><span>next</span><span> </span><span>=</span><span> </span><span>_mm512_loadu_si512</span><span>(</span><span>haystack</span><span>);</span><span>

    </span><span>for</span><span> </span><span>(</span><span>/**/</span><span>;</span><span> </span><span>haystack</span><span> </span><span>&lt;</span><span> </span><span>last</span><span>;</span><span> </span><span>haystack</span><span> </span><span>+=</span><span> </span><span>64</span><span>)</span><span> </span><span>{</span><span>

        </span><span>curr</span><span> </span><span>=</span><span> </span><span>next</span><span>;</span><span>
        </span><span>next</span><span> </span><span>=</span><span> </span><span>_mm512_loadu_si512</span><span>(</span><span>haystack</span><span> </span><span>+</span><span> </span><span>64</span><span>);</span><span>
        </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>shft</span><span> </span><span>=</span><span> </span><span>_mm512_alignr_epi32</span><span>(</span><span>next</span><span>,</span><span> </span><span>curr</span><span>,</span><span> </span><span>1</span><span>);</span><span>

        </span><span>v0</span><span> </span><span>=</span><span> </span><span>curr</span><span>;</span><span>

        </span><span>{</span><span>
            </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>t1</span><span> </span><span>=</span><span> </span><span>_mm512_srli_epi32</span><span>(</span><span>curr</span><span>,</span><span> </span><span>8</span><span>);</span><span>
            </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>t2</span><span> </span><span>=</span><span> </span><span>_mm512_slli_epi32</span><span>(</span><span>shft</span><span>,</span><span> </span><span>24</span><span>);</span><span>
            </span><span>v1</span><span> </span><span>=</span><span> </span><span>_mm512_or_si512</span><span>(</span><span>t1</span><span>,</span><span> </span><span>t2</span><span>);</span><span>
        </span><span>}</span><span>
        </span><span>{</span><span>
            </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>t1</span><span> </span><span>=</span><span> </span><span>_mm512_srli_epi32</span><span>(</span><span>curr</span><span>,</span><span> </span><span>16</span><span>);</span><span>
            </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>t2</span><span> </span><span>=</span><span> </span><span>_mm512_slli_epi32</span><span>(</span><span>shft</span><span>,</span><span> </span><span>16</span><span>);</span><span>
            </span><span>v2</span><span> </span><span>=</span><span> </span><span>_mm512_or_si512</span><span>(</span><span>t1</span><span>,</span><span> </span><span>t2</span><span>);</span><span>
        </span><span>}</span><span>
        </span><span>{</span><span>
            </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>t1</span><span> </span><span>=</span><span> </span><span>_mm512_srli_epi32</span><span>(</span><span>curr</span><span>,</span><span> </span><span>24</span><span>);</span><span>
            </span><span>const</span><span> </span><span>__m512i</span><span> </span><span>t2</span><span> </span><span>=</span><span> </span><span>_mm512_slli_epi32</span><span>(</span><span>shft</span><span>,</span><span> </span><span>8</span><span>);</span><span>
            </span><span>v3</span><span> </span><span>=</span><span> </span><span>_mm512_or_si512</span><span>(</span><span>t1</span><span>,</span><span> </span><span>t2</span><span>);</span><span>
        </span><span>}</span><span>

        </span><span>uint16_t</span><span> </span><span>m0</span><span> </span><span>=</span><span> </span><span>_mm512_cmpeq_epi32_mask</span><span>(</span><span>v0</span><span>,</span><span> </span><span>prefix</span><span>);</span><span>
        </span><span>uint16_t</span><span> </span><span>m1</span><span> </span><span>=</span><span> </span><span>_mm512_cmpeq_epi32_mask</span><span>(</span><span>v1</span><span>,</span><span> </span><span>prefix</span><span>);</span><span>
        </span><span>uint16_t</span><span> </span><span>m2</span><span> </span><span>=</span><span> </span><span>_mm512_cmpeq_epi32_mask</span><span>(</span><span>v2</span><span>,</span><span> </span><span>prefix</span><span>);</span><span>
        </span><span>uint16_t</span><span> </span><span>m3</span><span> </span><span>=</span><span> </span><span>_mm512_cmpeq_epi32_mask</span><span>(</span><span>v3</span><span>,</span><span> </span><span>prefix</span><span>);</span><span>

        </span><span>int</span><span> </span><span>index</span><span> </span><span>=</span><span> </span><span>64</span><span>;</span><span>
        </span><span>while</span><span> </span><span>(</span><span>m0</span><span> </span><span>|</span><span> </span><span>m1</span><span> </span><span>|</span><span> </span><span>m2</span><span> </span><span>|</span><span> </span><span>m3</span><span>)</span><span> </span><span>{</span><span>
            </span><span>if</span><span> </span><span>(</span><span>m0</span><span>)</span><span> </span><span>{</span><span>
                </span><span>int</span><span> </span><span>pos</span><span> </span><span>=</span><span> </span><span>__builtin_ctz</span><span>(</span><span>m0</span><span>)</span><span> </span><span>*</span><span> </span><span>4</span><span> </span><span>+</span><span> </span><span>0</span><span>;</span><span>
                </span><span>m0</span><span> </span><span>=</span><span> </span><span>m0</span><span> </span><span>&amp;</span><span> </span><span>(</span><span>m0</span><span> </span><span>-</span><span> </span><span>1</span><span>);</span><span>

                </span><span>if</span><span> </span><span>(</span><span>pos</span><span> </span><span>&lt;</span><span> </span><span>index</span><span> </span><span>&amp;&amp;</span><span> </span><span>memcmp</span><span>(</span><span>haystack</span><span> </span><span>+</span><span> </span><span>pos</span><span> </span><span>+</span><span> </span><span>4</span><span>,</span><span> </span><span>needle</span><span> </span><span>+</span><span> </span><span>4</span><span>,</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>4</span><span>)</span><span> </span><span>==</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>
                    </span><span>index</span><span> </span><span>=</span><span> </span><span>pos</span><span>;</span><span>
                </span><span>}</span><span>
            </span><span>}</span><span>

            </span><span>if</span><span> </span><span>(</span><span>m1</span><span>)</span><span> </span><span>{</span><span>
                </span><span>int</span><span> </span><span>pos</span><span> </span><span>=</span><span> </span><span>__builtin_ctz</span><span>(</span><span>m1</span><span>)</span><span> </span><span>*</span><span> </span><span>4</span><span> </span><span>+</span><span> </span><span>1</span><span>;</span><span>
                </span><span>m1</span><span> </span><span>=</span><span> </span><span>m1</span><span> </span><span>&amp;</span><span> </span><span>(</span><span>m1</span><span> </span><span>-</span><span> </span><span>1</span><span>);</span><span>

                </span><span>if</span><span> </span><span>(</span><span>pos</span><span> </span><span>&lt;</span><span> </span><span>index</span><span> </span><span>&amp;&amp;</span><span> </span><span>memcmp</span><span>(</span><span>haystack</span><span> </span><span>+</span><span> </span><span>pos</span><span> </span><span>+</span><span> </span><span>4</span><span>,</span><span> </span><span>needle</span><span> </span><span>+</span><span> </span><span>4</span><span>,</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>4</span><span>)</span><span> </span><span>==</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>
                    </span><span>index</span><span> </span><span>=</span><span> </span><span>pos</span><span>;</span><span>
                </span><span>}</span><span>
            </span><span>}</span><span>

            </span><span>if</span><span> </span><span>(</span><span>m2</span><span>)</span><span> </span><span>{</span><span>
                </span><span>int</span><span> </span><span>pos</span><span> </span><span>=</span><span> </span><span>__builtin_ctz</span><span>(</span><span>m2</span><span>)</span><span> </span><span>*</span><span> </span><span>4</span><span> </span><span>+</span><span> </span><span>2</span><span>;</span><span>
                </span><span>m2</span><span> </span><span>=</span><span> </span><span>m2</span><span> </span><span>&amp;</span><span> </span><span>(</span><span>m2</span><span> </span><span>-</span><span> </span><span>1</span><span>);</span><span>

                </span><span>if</span><span> </span><span>(</span><span>pos</span><span> </span><span>&lt;</span><span> </span><span>index</span><span> </span><span>&amp;&amp;</span><span> </span><span>memcmp</span><span>(</span><span>haystack</span><span> </span><span>+</span><span> </span><span>pos</span><span> </span><span>+</span><span> </span><span>4</span><span>,</span><span> </span><span>needle</span><span> </span><span>+</span><span> </span><span>4</span><span>,</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>4</span><span>)</span><span> </span><span>==</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>
                    </span><span>index</span><span> </span><span>=</span><span> </span><span>pos</span><span>;</span><span>
                </span><span>}</span><span>
            </span><span>}</span><span>

            </span><span>if</span><span> </span><span>(</span><span>m3</span><span>)</span><span> </span><span>{</span><span>
                </span><span>int</span><span> </span><span>pos</span><span> </span><span>=</span><span> </span><span>__builtin_ctz</span><span>(</span><span>m3</span><span>)</span><span> </span><span>*</span><span> </span><span>4</span><span> </span><span>+</span><span> </span><span>3</span><span>;</span><span>
                </span><span>m3</span><span> </span><span>=</span><span> </span><span>m3</span><span> </span><span>&amp;</span><span> </span><span>(</span><span>m3</span><span> </span><span>-</span><span> </span><span>1</span><span>);</span><span>

                </span><span>if</span><span> </span><span>(</span><span>pos</span><span> </span><span>&lt;</span><span> </span><span>index</span><span> </span><span>&amp;&amp;</span><span> </span><span>memcmp</span><span>(</span><span>haystack</span><span> </span><span>+</span><span> </span><span>pos</span><span> </span><span>+</span><span> </span><span>4</span><span>,</span><span> </span><span>needle</span><span> </span><span>+</span><span> </span><span>4</span><span>,</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>4</span><span>)</span><span> </span><span>==</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>
                    </span><span>index</span><span> </span><span>=</span><span> </span><span>pos</span><span>;</span><span>
                </span><span>}</span><span>
            </span><span>}</span><span>
        </span><span>}</span><span>

        </span><span>if</span><span> </span><span>(</span><span>index</span><span> </span><span>&lt;</span><span> </span><span>64</span><span>)</span><span> </span><span>{</span><span>
            </span><span>return</span><span> </span><span>(</span><span>haystack</span><span> </span><span>-</span><span> </span><span>string</span><span>)</span><span> </span><span>+</span><span> </span><span>index</span><span>;</span><span>
        </span><span>}</span><span>
    </span><span>}</span><span>

    </span><span>return</span><span> </span><span>size_t</span><span>(</span><span>-1</span><span>);</span><span>
</span><span>}</span>
</pre>
</div>
</div>
</div>
<div id="algorithm-3-sse4-2-specific-pcmpestrm">
<h2>Algorithm 3: SSE4.2-specific (PCMPESTRM)</h2>
<div id="algorithm-2">
<h2>Algorithm</h2>
<p>SSE4.2 introduced <a href="http://en.wikipedia.org/wiki/SSE4#SSE4.2">String and Text New Instructions</a> (STNI), a set of very
complex instructions that were meant to be a building block for string
operations.  Unfortunately, Intel practically discontinued STNI in newer
processors, hasn't introduced AVX2 versions of STNI and make them extremely
slow (11 cycles latency is unacceptable).</p>
<p>Basically <tt>PCMPxSTRx</tt> instruction exists in four variants, which differs
only in:</p>
<ol>
<li>A way to determine string length: the length might be given explicitly
or the first zero byte marks string end, as in traditional C strings.</li>
<li>How the result is saved, it might be either a bit-mask/byte-mask or
the number of first/last bit set in the bit-mask.</li>
</ol>
<p>Additional instruction's argument (immediate constant) defines several aspects
of execution, specifically the algorithm of comparison.  There are four
different algorithms available, one we're using is called <strong>range ordered</strong>.
Despite the name, this algorithm locates substring, or its prefix if a
substring goes beyond register width.</p>
<p>For example, when we're searching "ABCD" in "ABCD_ABC_ABCD_AB" the instruction
returns bitmask 0b0100001000000001, treating suffix "AB" as a match. Thus we
can safely assume that only <strong>the first character matches</strong>, as tail might or
might not be present in a register. (Of course it can be determined, but
require additional calculations which is not very handy.)</p>
<p>Below is a code snippet which does the above operation.</p>
<pre><span>const</span><span> </span><span>char</span><span>*</span><span> </span><span>s1</span><span> </span><span>=</span><span> </span><span>"ABCD_ABC_ABCD_AB"</span><span>;</span><span>
</span><span>const</span><span> </span><span>char</span><span>*</span><span> </span><span>s2</span><span> </span><span>=</span><span> </span><span>"ABCD____________"</span><span>;</span><span>

</span><span>const</span><span> </span><span>int</span><span> </span><span>mode</span><span> </span><span>=</span><span> </span><span>_SIDD_UBYTE_OPS</span><span>
               </span><span>|</span><span> </span><span>_SIDD_CMP_EQUAL_ORDERED</span><span>
               </span><span>|</span><span> </span><span>_SIDD_BIT_MASK</span><span>;</span><span>

</span><span>const</span><span> </span><span>__m128i</span><span> </span><span>N</span><span>   </span><span>=</span><span> </span><span>_mm_loadu_si128</span><span>((</span><span>__m128i</span><span>*</span><span>)(</span><span>s2</span><span>));</span><span>
</span><span>const</span><span> </span><span>__m128i</span><span> </span><span>D</span><span>   </span><span>=</span><span> </span><span>_mm_loadu_si128</span><span>((</span><span>__m128i</span><span>*</span><span>)(</span><span>s1</span><span>));</span><span>
</span><span>const</span><span> </span><span>__m128i</span><span> </span><span>res</span><span> </span><span>=</span><span> </span><span>_mm_cmpestrm</span><span>(</span><span>N</span><span>,</span><span> </span><span>4</span><span>,</span><span> </span><span>D</span><span>,</span><span> </span><span>16</span><span>,</span><span> </span><span>mode</span><span>);</span><span>
</span><span>uint64_t</span><span> </span><span>mask</span><span> </span><span>=</span><span> </span><span>_mm_cvtsi128_si64</span><span>(</span><span>res</span><span>);</span><span> </span><span>// = 0b0100001000000001</span>
</pre>
</div>
<div id="implementation-2">
<h2>Implementation</h2>
<div id="sse-1">
<h3>SSE</h3>
<pre><span>size_t</span><span> </span><span>sse42_strstr_anysize</span><span>(</span><span>const</span><span> </span><span>char</span><span>*</span><span> </span><span>s</span><span>,</span><span> </span><span>size_t</span><span> </span><span>n</span><span>,</span><span> </span><span>const</span><span> </span><span>char</span><span>*</span><span> </span><span>needle</span><span>,</span><span> </span><span>size_t</span><span> </span><span>k</span><span>)</span><span> </span><span>{</span><span>

    </span><span>const</span><span> </span><span>__m128i</span><span> </span><span>N</span><span> </span><span>=</span><span> </span><span>_mm_loadu_si128</span><span>((</span><span>__m128i</span><span>*</span><span>)</span><span>needle</span><span>);</span><span>

    </span><span>for</span><span> </span><span>(</span><span>size_t</span><span> </span><span>i</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>n</span><span>;</span><span> </span><span>i</span><span> </span><span>+=</span><span> </span><span>16</span><span>)</span><span> </span><span>{</span><span>

        </span><span>const</span><span> </span><span>int</span><span> </span><span>mode</span><span> </span><span>=</span><span> </span><span>_SIDD_UBYTE_OPS</span><span>
                       </span><span>|</span><span> </span><span>_SIDD_CMP_EQUAL_ORDERED</span><span>
                       </span><span>|</span><span> </span><span>_SIDD_BIT_MASK</span><span>;</span><span>

        </span><span>const</span><span> </span><span>__m128i</span><span> </span><span>D</span><span>   </span><span>=</span><span> </span><span>_mm_loadu_si128</span><span>((</span><span>__m128i</span><span>*</span><span>)(</span><span>s</span><span> </span><span>+</span><span> </span><span>i</span><span>));</span><span>
        </span><span>const</span><span> </span><span>__m128i</span><span> </span><span>res</span><span> </span><span>=</span><span> </span><span>_mm_cmpestrm</span><span>(</span><span>N</span><span>,</span><span> </span><span>k</span><span>,</span><span> </span><span>D</span><span>,</span><span> </span><span>n</span><span> </span><span>-</span><span> </span><span>i</span><span>,</span><span> </span><span>mode</span><span>);</span><span>
        </span><span>uint64_t</span><span> </span><span>mask</span><span> </span><span>=</span><span> </span><span>_mm_cvtsi128_si64</span><span>(</span><span>res</span><span>);</span><span>

        </span><span>while</span><span> </span><span>(</span><span>mask</span><span> </span><span>!=</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>

            </span><span>const</span><span> </span><span>auto</span><span> </span><span>bitpos</span><span> </span><span>=</span><span> </span><span>bits</span><span>::</span><span>get_first_bit_set</span><span>(</span><span>mask</span><span>);</span><span>

            </span><span>// we know that at least the first character of needle matches
</span><span>            </span><span>if</span><span> </span><span>(</span><span>memcmp</span><span>(</span><span>s</span><span> </span><span>+</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span> </span><span>+</span><span> </span><span>1</span><span>,</span><span> </span><span>needle</span><span> </span><span>+</span><span> </span><span>1</span><span>,</span><span> </span><span>k</span><span> </span><span>-</span><span> </span><span>1</span><span>)</span><span> </span><span>==</span><span> </span><span>0</span><span>)</span><span> </span><span>{</span><span>
                </span><span>return</span><span> </span><span>i</span><span> </span><span>+</span><span> </span><span>bitpos</span><span>;</span><span>
            </span><span>}</span><span>

            </span><span>mask</span><span> </span><span>=</span><span> </span><span>bits</span><span>::</span><span>clear_leftmost_set</span><span>(</span><span>mask</span><span>);</span><span>
        </span><span>}</span><span>
    </span><span>}</span><span>

    </span><span>return</span><span> </span><span>std</span><span>::</span><span>string</span><span>::</span><span>npos</span><span>;</span><span>
</span><span>}</span>
</pre>
</div>
</div>
</div>
<div id="performance-results">
<h2>Performance results</h2>
<p>Performance of various SIMD implementations were measured.  Test programs also
have got specialisation for short substrings, that are selected at run time.
Performance of a C <tt>strstr</tt> is included for comparison.  I omitted C++
<tt><span>string::find</span></tt> due to <a href="http://0x80.pl/notesen/2016-10-08-slow-std-string-find.html">performance bug in GNU libc</a> which makes the method
10 times slower than <tt>strstr</tt>.</p>
<p>Test programs were run three times. Following computers, running either
Debian or Ubuntu, were tested:</p>
<ol>
<li>Westmere i5 M540, GCC 6.2.0,</li>
<li>Bulldozer FX-8150 CPU, GCC 4.8.4</li>
<li>Haswell i7 4470, GCC 5.4.1,</li>
<li>Skylake i7 6700, GCC 5.4.1,</li>
<li>Knights Landing (KNL) 7210, GCC 5.3.0.</li>
<li>ARMv7 (Raspberry Pi 3, 32-bit code), GCC 4.9.2</li>
<li>ARMv8 (<a href="https://shop.softiron.com/product/overdrive-1000/">ARM Cortex A57 - AMD Opteron A1100</a>, 64-bit code), Clang 3.8.0</li>
</ol>
<div id="x64-computers">
<h2>x64 computers</h2>
<table>
<colgroup>
<col width="34%">
<col width="13%">
<col width="13%">
<col width="13%">
<col width="13%">
<col width="13%">
</colgroup>
<thead>
<tr><th rowspan="2">procedure</th>
<th colspan="5">time in seconds</th>
</tr>
<tr><th>Westemere</th>
<th>Bulldozer</th>
<th>Haswell</th>
<th>Skylake</th>
<th>KNL</th>
</tr>
</thead>
<tbody>
<tr><td>std::strstr</td>
<td>0.82246</td>
<td>9.37792</td>
<td>0.52786</td>
<td>0.66148</td>
<td>4.94606</td>
</tr>
<tr><td>std::string::find</td>
<td>---</td>
<td>---</td>
<td>---</td>
<td>---</td>
<td>---</td>
</tr>
<tr><td><a href="#generic-swar">SWAR 64-bit (generic)</a></td>
<td>2.49859</td>
<td>2.93836</td>
<td>1.57715</td>
<td>1.40404</td>
<td>8.17075</td>
</tr>
<tr><td><a href="#generic-sse-avx2">SSE2 (generic)</a></td>
<td><strong>0.74589</strong></td>
<td><strong>0.78871</strong></td>
<td>0.55435</td>
<td><strong>0.48863</strong></td>
<td>6.10786</td>
</tr>
<tr><td><a href="#mpsadbw-sse-avx2">SSE4.1 (MPSADBW)</a></td>
<td>1.45040</td>
<td>1.98863</td>
<td>0.89775</td>
<td><strong>0.63875</strong></td>
<td>18.71666</td>
</tr>
<tr><td>SSE4.1 (MPSADBW unrolled)</td>
<td>1.23849</td>
<td>2.06008</td>
<td>0.99647</td>
<td>0.87919</td>
<td>13.72486</td>
</tr>
<tr><td><a href="#pcmpstr-sse">SSE4.2 (PCMPESTRM)</a></td>
<td>1.69968</td>
<td>2.00681</td>
<td>1.55992</td>
<td>1.39063</td>
<td>6.28869</td>
</tr>
<tr><td><a href="#mpsadbw-sse-avx2">AVX2 (MPSADBW)</a></td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>0.61578</td>
<td><strong>0.56981</strong></td>
<td>13.15136</td>
</tr>
<tr><td><a href="#generic-sse-avx2">AVX2 (generic)</a></td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><strong>0.38653</strong></td>
<td><strong>0.36309</strong></td>
<td><strong>4.09478</strong></td>
</tr>
<tr><td><a href="#mpsadbw-avx512f">AVX512F (MPSADBW-like)</a></td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><strong>2.32616</strong></td>
</tr>
<tr><td><a href="#generic-avx512f">AVX512F (generic)</a></td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><strong>1.14057</strong></td>
</tr>
<tr><td>biggest speed-up to <tt>strstr</tt></td>
<td>1.10</td>
<td>---</td>
<td>1.37</td>
<td>1.82</td>
<td>4.33</td>
</tr>
</tbody>
</table>
<ul>
<li>Performance of <tt>strstr</tt> on the machine with Bulldozer is terrible, it'd pointless to use
it as a reference.</li>
</ul>
</div>
<div id="arm-computers">
<h2>ARM computers</h2>
<table>
<colgroup>
<col width="62%">
<col width="19%">
<col width="19%">
</colgroup>
<thead>
<tr><th rowspan="2">procedure</th>
<th colspan="2">time in seconds</th>
</tr>
<tr><th>ARMv7</th>
<th>ARMv8</th>
</tr>
</thead>
<tbody>
<tr><td>std::strstr</td>
<td>7.30405</td>
<td>3.37546</td>
</tr>
<tr><td>std::string::find</td>
<td>4.17131</td>
<td>1.81368</td>
</tr>
<tr><td><a href="#generic-swar">SWAR 64-bit (generic)</a></td>
<td>36.65012</td>
<td>0.46269</td>
</tr>
<tr><td>SWAR 32-bit (generic)</td>
<td>2.45058</td>
<td>0.81075</td>
</tr>
<tr><td><a href="#generic-neon32">ARM Neon (32-bit, generic)</a></td>
<td>1.29861</td>
<td>0.40699</td>
</tr>
<tr><td><a href="#generic-aarch64">AArch64  (64-bit, generic)</a></td>
<td>---</td>
<td>0.27897</td>
</tr>
<tr><td>biggest speed-up to <tt><span>std::string::find</span></tt></td>
<td>3.1</td>
<td>6.5</td>
</tr>
</tbody>
</table>
</div>
<div id="conclusions-and-remarks">
<h2>Conclusions and remarks</h2>
<ul>
<li>The generic SIMD algorithm outperforms C <tt>strstr</tt> on all platforms.
An implementation should use the highest SIMD version available on
a certain CPU.</li>
<li><tt>MPSADBW</tt> performs pretty bad, with an exception for Skylake.
Knights Landing performance is terrible.</li>
<li><tt>PCMPESTRM</tt> performs worse than <tt>MPSADBW</tt>.</li>
<li>ARM Neon performance is pretty good even for SWAR implementation.
The SWAR version is 1.7 times faster than <tt><span>string::find</span></tt>, SIMD
version is 3.1 times faster.</li>
<li>AArch64 performance of scalar SWAR64 is almost as good as 32-bit SIMD
procedure.</li>
<li>Comparison with <tt>strstr</tt> might be considered unfair, as the
procedure deals with string of unknown length, while my implementations
get lengths and take advantage of this. I fully agree.</li>
<li>Procedures I implemented are also unsafe, because might read data off
the input string. This may lead to access violation if strings are
located just before unmapped memory. And for sure address sanitizers
will complain. Making the procedures safe is feasible, but it wasn't
my goal.</li>
</ul>
</div>
</div>
<div id="acknowledgments">
<h2>Acknowledgments</h2>
<p><a href="http://lemire.me/">Daniel Lemire</a> has gave me access to Haswell, Skylake, KNL, Bulldozer
and ARMv8 machines, where I compiled and run test programs. Thank you!</p>
</div>
<div id="source-code">
<h2>Source code</h2>
<p>All implementations and tests programs are available at <a href="https://github.com/WojciechMula/sse4-strstr">github</a>.</p>
</div>
<div id="history">
<h2>History</h2>
<ul>
<li>2017-04-29 — ARMv8 results</li>
<li>2017-01-30 — better timings from Raspberry Pi 3</li>
<li>2017-01-26 — ARM Neon &amp; results from Raspberry Pi 3</li>
<li>2017-01-25 — spelling</li>
<li>2016-12-22 — added results from Bulldozer</li>
<li>2016-11-30 — spelling</li>
</ul>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Filedb: Disk-based key-value store inspired by Bitcask (113 pts)]]></title>
            <link>https://github.com/rajivharlalka/filedb</link>
            <guid>44273857</guid>
            <pubDate>Sat, 14 Jun 2025 02:45:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/rajivharlalka/filedb">https://github.com/rajivharlalka/filedb</a>, See on <a href="https://news.ycombinator.com/item?id=44273857">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">filedb</h2><a id="user-content-filedb" aria-label="Permalink: filedb" href="#filedb"></a></p>
<p dir="auto">A key-value store inspired by Bitcask.</p>
<hr>
<p dir="auto">FileDB is a Zig-implementation of Bitcask by Riak<sup><a href="#user-content-fn-1-09af71fecd8555ac6378a8c84d5da9a8" id="user-content-fnref-1-09af71fecd8555ac6378a8c84d5da9a8" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> paper.</p>
<ul dir="auto">
<li>FileDB stores record metadata in a log-structured hashtable and parallely keeps 1 disk file open for inserting records in append-only mode. On restarts or <code>MAX_FILE_REACHED</code>, the disk file is rotated and all the oldfiles are kept open for reading <strong>only</strong>.</li>
<li>A compaction process running every <code>config.compactionInterval</code> seconds, reads all the disk files and combines them into one file while updating the metadata hashtable.</li>
<li>A sync process syncs the open disk files once every <code>config.syncInterval</code>. Sync also can be done on every request if <code>config.alwaysFsync</code> is True.</li>
</ul>
<p dir="auto">Read about internals in-depth at <a href="https://rajivharlalka.in/posts/filedb" rel="nofollow">FileDb</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benefits:</h2><a id="user-content-benefits" aria-label="Permalink: Benefits:" href="#benefits"></a></p>
<ol dir="auto">
<li>Since the metadata keeps an exact location of file and position in file for a record, fetching records become O(1) operation.</li>
<li>All metadata records are constant size, so irrespective of the size of the value of a record the in-memory store keeps a constant sized metadata.</li>
<li>Provides high throughput by using the open file in append only mode.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Methods</h2><a id="user-content-methods" aria-label="Permalink: Methods" href="#methods"></a></p>
<ol dir="auto">
<li><code>init(allocator: std.mem.Allocator, options: ?config.Options)</code> : Intialized FileDB</li>
<li><code>deinit()</code>: Deinitalizes FileDB</li>
<li><code>put(key:[]const u8, value: []const u8)</code>: Inserts a key-value pair in the database to be tracked.</li>
<li><code>get(key:[]const u8)</code>: Retrieved a key-value pair from the database.</li>
<li><code>delete(key: []const u8)</code>: Delete a key-value pair from the database</li>
<li><code>list(allocator: std.mem.Allocator)</code>: Returns a list of keys stored in the database.</li>
<li><code>sync()</code>: Syncs the current open datafile on the disk</li>
<li><code>storeHashMap()</code>: Creates the HINTS file</li>
<li><code>loadKeyDir()</code>: Loads the hashmap from the HINTS file</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Redis Compatible:</h2><a id="user-content-redis-compatible" aria-label="Permalink: Redis Compatible:" href="#redis-compatible"></a></p>
<p dir="auto">Along with the library, a Redis-compatible client is available.</p>
<div dir="auto" data-snippet-clipboard-copy-content="127.0.0.1:6379> RING
(error) ERR unknown command
127.0.0.1:6379> PING
PONG
127.0.0.1:6379> get abcd
(nil)
127.0.0.1:6379> set abcd def
OK
127.0.0.1:6379> get abcd
&quot;def&quot;"><pre>127.0.0.1:<span>6379&gt;</span> RING
(error) ERR unknown <span>command</span>
127.0.0.1:<span>6379&gt;</span> PING
PONG
127.0.0.1:<span>6379&gt;</span> get abcd
(nil)
127.0.0.1:<span>6379&gt;</span> <span>set</span> abcd def
OK
127.0.0.1:<span>6379&gt;</span> get abcd
<span><span>"</span>def<span>"</span></span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Redis Benchmark</h2><a id="user-content-redis-benchmark" aria-label="Permalink: Redis Benchmark" href="#redis-benchmark"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="redis-benchmark -p 6379 -t set -n 10000 -r 100000000
Summary:
  throughput summary: 13736.26 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        3.615     0.088     3.455     6.831     8.831    14.919
      
redis-benchmark -p 6379 -t set -n 200000 -r 100000000
Summary:
  throughput summary: 14375.04 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        3.452     0.072     3.087     6.767    10.647   114.303

redis-benchmark -p 6379 -t get -n 100000 -r 100000000
Summary:
  throughput summary: 44286.98 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        0.573     0.088     0.519     0.967     1.447     7.495

redis-benchmark -p 6379 -t get -n 1000000 -r 1000000000 --threads 10
Summary:
  throughput summary: 104876.77 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        0.405     0.032     0.375     0.831     1.295    26.047"><pre>redis-benchmark -p 6379 -t <span>set</span> -n 10000 -r 100000000
Summary:
  throughput summary: 13736.26 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        3.615     0.088     3.455     6.831     8.831    14.919
      
redis-benchmark -p 6379 -t <span>set</span> -n 200000 -r 100000000
Summary:
  throughput summary: 14375.04 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        3.452     0.072     3.087     6.767    10.647   114.303

redis-benchmark -p 6379 -t get -n 100000 -r 100000000
Summary:
  throughput summary: 44286.98 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        0.573     0.088     0.519     0.967     1.447     7.495

redis-benchmark -p 6379 -t get -n 1000000 -r 1000000000 --threads 10
Summary:
  throughput summary: 104876.77 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        0.405     0.032     0.375     0.831     1.295    26.047</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">References:</h2><a id="user-content-references" aria-label="Permalink: References:" href="#references"></a></p>
<ol dir="auto">
<li><a href="https://riak.com/assets/bitcask-intro.pdf" rel="nofollow">Bitcask Paper by Riak</a></li>
<li><a href="https://github.com/mr-karan/barreldb">Go Implementation of Bitcask</a></li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Zig Resources:</h2><a id="user-content-zig-resources" aria-label="Permalink: Zig Resources:" href="#zig-resources"></a></p>
<ol dir="auto">
<li><a href="https://www.openmymind.net/Basic-MetaProgramming-in-Zig/" rel="nofollow">https://www.openmymind.net/Basic-MetaProgramming-in-Zig/</a></li>
<li><a href="https://pedropark99.github.io/zig-book/" rel="nofollow">https://pedropark99.github.io/zig-book/</a></li>
<li><a href="https://zig.guide/standard-library/" rel="nofollow">https://zig.guide/standard-library/</a></li>
<li><a href="https://zighelp.org/" rel="nofollow">https://zighelp.org</a></li>
</ol>
<section data-footnotes=""><h2 id="footnote-label" dir="auto">Footnotes</h2>
<ol dir="auto">
<li id="user-content-fn-1-09af71fecd8555ac6378a8c84d5da9a8">
<p dir="auto"><a href="https://riak.com/assets/bitcask-intro.pdf">https://riak.com/assets/bitcask-intro.pdf</a> <a href="#user-content-fnref-1-09af71fecd8555ac6378a8c84d5da9a8" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
</ol>
</section>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Tech Job Meltdown (164 pts)]]></title>
            <link>https://www.professoraxelrod.com/p/the-tech-job-meltdown</link>
            <guid>44273790</guid>
            <pubDate>Sat, 14 Jun 2025 02:22:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.professoraxelrod.com/p/the-tech-job-meltdown">https://www.professoraxelrod.com/p/the-tech-job-meltdown</a>, See on <a href="https://news.ycombinator.com/item?id=44273790">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><blockquote><p><em><span>He wrote me a prescription; he said “You are depressed</span><br><span>I'm glad you came to see me to get this off your chest</span><br><span>Come back and see me later, next patient please</span><br><span>Send in another victim of industrial disease”</span></em><br><em><strong>Industrial Disease</strong></em><span>, Dire Straits</span></p></blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c9872ad-e90e-49c8-be04-2f03ac64653b_1167x486.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c9872ad-e90e-49c8-be04-2f03ac64653b_1167x486.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c9872ad-e90e-49c8-be04-2f03ac64653b_1167x486.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c9872ad-e90e-49c8-be04-2f03ac64653b_1167x486.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c9872ad-e90e-49c8-be04-2f03ac64653b_1167x486.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c9872ad-e90e-49c8-be04-2f03ac64653b_1167x486.webp" width="1167" height="486" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2c9872ad-e90e-49c8-be04-2f03ac64653b_1167x486.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:486,&quot;width&quot;:1167,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:243242,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.professoraxelrod.com/i/165486167?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c9872ad-e90e-49c8-be04-2f03ac64653b_1167x486.webp&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c9872ad-e90e-49c8-be04-2f03ac64653b_1167x486.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c9872ad-e90e-49c8-be04-2f03ac64653b_1167x486.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c9872ad-e90e-49c8-be04-2f03ac64653b_1167x486.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c9872ad-e90e-49c8-be04-2f03ac64653b_1167x486.webp 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption><span>The Google campus doesn’t look as friendly as it used to. (this ia actually from Bartertown in </span><em><strong>Mad Max 3</strong></em><span> - you can see Thunderdome in the middle)</span></figcaption></figure></div><p><span>Since the start of 2023, more than half-a-million tech workers have been laid off. This isn’t the impact of COVID, this isn’t a sudden realization that tech workers are under-performing, this isn’t (much) a wave of AI making tech workers more efficient, and the other usual shibboleths like “it was overhiring during the pandemic” or “it’s a wave of H1B workers” or “all knowledge worker jobs are being replaced by LLMs” are only vaguely correct. You could make a pretty reasonable case that it was </span><em><strong>the end of Zero Interest Rate Policy</strong></em><span> (ZIRP) and the corresponding impact of cost of capital - that the cost of borrowing went up, thus venture capital became a less attractive investment class than other areas, so less money went to building new companies and it was harder for existing firms to borrow, as investors went elsewhere for better returns. </span><em><strong>That is correct - and it would have had its impact</strong></em><span> - though that impact would have basically been the slowdown of new venture backed firms, not layoffs at the Big Tech Giants - and that did in fact happen. </span><em><span>(There </span><strong>is definitely</strong><span> a knock-on effect at the Big Tech Giants where a lack of tech startups does bad things to the large parts of the ecosystem - but that effect is not as immediate as it was back in in the 2000 dot com crash.)</span></em><span> But there’s a much more immediate bottom line reason.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e72374-3470-4078-a9cf-64a042cf6b25_498x298.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e72374-3470-4078-a9cf-64a042cf6b25_498x298.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e72374-3470-4078-a9cf-64a042cf6b25_498x298.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e72374-3470-4078-a9cf-64a042cf6b25_498x298.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e72374-3470-4078-a9cf-64a042cf6b25_498x298.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e72374-3470-4078-a9cf-64a042cf6b25_498x298.gif" width="498" height="298" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/06e72374-3470-4078-a9cf-64a042cf6b25_498x298.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:298,&quot;width&quot;:498,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:70629,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/gif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.professoraxelrod.com/i/165486167?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e72374-3470-4078-a9cf-64a042cf6b25_498x298.gif&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e72374-3470-4078-a9cf-64a042cf6b25_498x298.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e72374-3470-4078-a9cf-64a042cf6b25_498x298.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e72374-3470-4078-a9cf-64a042cf6b25_498x298.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e72374-3470-4078-a9cf-64a042cf6b25_498x298.gif 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>Section 174 of the Internal Revenue Code governs the tax treatment of research and development (R&amp;D) expenditures. For roughly 70 years, American companies could deduct 100% of “qualified research and development spending” in the year they incurred the costs, and this was generally interpreted pretty liberally. Salaries, software, contractor payments… if it contributed to creating or improving a product, it could be deducted “off the top” of a firm’s taxable income. The </span><a href="https://www.thetaxadviser.com/issues/2024/jun/rights-for-the-research-development-credit-and-sec-174/#:~:text=Originally%20enacted%20in%201954%2C%20Sec,product%20development%20expenses%20under%20Sec." rel="">deduction was </a><span>originally codified by Section 174 of the IRS Code of 1954, and under the provision, R&amp;D flourished in the U.S. It gave us the dominance of Bell Labs, Microsoft, Apple, Google, Facebook - pretty much all the US technology booms you’ve lived through unless you’re quire venerable.</span></p><p><span>So the way these regs are written: These expenditures must be for activities intended to discover information that eliminates uncertainty about the development or improvement of a product. </span><em>(Kind of open-ended.)</em><span> Prior to 2022, taxpayers could immediately deduct R&amp;D expenditures in the year they were incurred, providing a significant tax benefit for businesses investing in innovation. Alternatively, taxpayers could capitalize these costs and amortize them over a period (e.g., at least 60 months) if they chose to defer the deduction. But it was pretty rare to do this, because you could directly manage your R&amp;D payroll costs versus income to mitigate the tax hit. And societally, we accepted that - we were investing in growing the American economy. </span></p><p><span>But, the Tax Cuts and Jobs Act (TCJA) of 2017 amended Section 174, effective for tax years beginning after December 31, 2021. Starting in 2022, R&amp;D expenditures must be capitalized and amortized over 5 years for domestic research </span><em>(and 15 years for foreign research… which is pretty untenable.)</em><span> This change eliminated the option to immediately deduct R&amp;D costs, increasing tax liability for companies with significant research budgets in the short term. Even more annoying, amortization begins at the midpoint of the taxable year in which the expenses are incurred, using a straight-line method.</span></p><p><span>The short version is: this rule change has </span><em><strong>increased taxable income for businesses in the short term, as they can no longer deduct R&amp;D costs immediately</strong></em><span>. Now, there is actually a category of tax law which you can still use (IRS Section 41 Research and Development Tax Credits) which are different - if you think I’m about to advocate tax reform, yes, but I’m </span><em><strong>always</strong></em><span> doing that and I feel like I’m talking to a brick wall. But it’s not as broadly useful, and it’s not a simple recategorization, or we’d all have done it.</span></p><p><span>Anyway, let me see if I can summarize how it works today. A U.S. company incurs $1 million in domestic R&amp;D (Section 174) expenses in 2025 and let’s assume they can’t reasonably reclassify any of it under Section 41. Under Section 174, it must capitalize these costs and amortize them over 5 years. Amortization begins mid-year, so in 2025, the company can deduct $100,000 (1/10th of $1 million,  since you only get to count half the first year. The remaining $900,000 is deducted evenly over the next 4.5 years ($200,000 per year). These are basically “tax credits”.  So in some respects, they may be long term beneficial - </span><em><strong>but they are a short term drag, which is why you see layoffs at the moment</strong></em><span>. Also they create a lot more compliance paperwork </span><em>(and potentially you’ll see companies change hands just for their accumulated tax credits, which is a little… unhelpful.)</em></p><p>And of course, this hit the “real world” of companies focused on building tomorrow’s products in some pretty obvious ways.</p><p>The inability to immediately deduct R&amp;D costs reduced cash flow, particularly for cash-strapped startups and small tech firms reliant on R&amp;D. Companies had to either take out high-interest loans - since of course, interest rates have recently gone up, cut costs, or face bankruptcy. Many chose layoffs to free up cash to cover these tax liabilities. For instance, a small company might lay off a software engineer earning $200,000 to cover a $189,000 tax bill.</p><p><span>Now, you might think the 15-year amortization period for foreign R&amp;E expenditures would make hiring non-U.S. engineers less tax-advantageous and help bring jobs back to the US, but this </span><em><strong>mostly</strong></em><span> did not actually work out this way. You see, larger companies responded by offshoring R&amp;D to countries with more favorable tax regimes, leading to U.S. job losses. For example, Google reportedly shifted some work to Germany, and Microsoft moved a bunch of research work to China - both because pay rates were better and because the local subsidiary company in that jurisdiction operated under the national laws for that nation, which … were not the US tax laws. They were much more like the previous US tax laws, because the rest of the world had realized “hey, we also want to encourage people to invest in R&amp;D and grow the next trillion dollar company here!” And this new tax ruling doesn’t precisely say “we don’t want to do that” but it does say “we don’t want you to be quick about it” - which everyone who believes in the Amazing Growth Story thinks is anathema to their strategy.</span></p><p><span>Anyway, the impact of this tax strategy turned out to be: </span><em><strong>layoffs of U.S.-based engineers </strong></em><span>while companies restructured operations abroad.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd833dee0-007f-44b0-86d3-41de3cef17d6_628x356.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd833dee0-007f-44b0-86d3-41de3cef17d6_628x356.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd833dee0-007f-44b0-86d3-41de3cef17d6_628x356.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd833dee0-007f-44b0-86d3-41de3cef17d6_628x356.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd833dee0-007f-44b0-86d3-41de3cef17d6_628x356.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd833dee0-007f-44b0-86d3-41de3cef17d6_628x356.jpeg" width="628" height="356" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d833dee0-007f-44b0-86d3-41de3cef17d6_628x356.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:356,&quot;width&quot;:628,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:147865,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.professoraxelrod.com/i/165486167?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd833dee0-007f-44b0-86d3-41de3cef17d6_628x356.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd833dee0-007f-44b0-86d3-41de3cef17d6_628x356.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd833dee0-007f-44b0-86d3-41de3cef17d6_628x356.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd833dee0-007f-44b0-86d3-41de3cef17d6_628x356.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd833dee0-007f-44b0-86d3-41de3cef17d6_628x356.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>This is </span><em><strong>NOT</strong></em><span> working out how it was planned.</span></figcaption></figure></div><p><span>Now back in 2017, when Congress passed the </span><a href="https://www.investopedia.com/taxes/trumps-tax-reform-plan-explained/" rel="">Tax Cuts and Jobs Act</a><span> (TCJA), the signature legislative achievement of President Donald Trump’s first term, it slashed the corporate tax rate from 35% to 21%, which looked like a massive revenue loss “on paper” for the federal government. So in order to make the 2017 bill comply with Senate budget rules, lawmakers had to offset the cost… therefore, they put in a future tax hike </span><em>(well, several)</em><span> that wouldn’t kick in right away, wouldn’t provoke immediate backlash from businesses, and could, in theory, be quietly repealed later.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bd3ab5b-825a-4438-a6ce-7ab87497fabf_600x449.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bd3ab5b-825a-4438-a6ce-7ab87497fabf_600x449.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bd3ab5b-825a-4438-a6ce-7ab87497fabf_600x449.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bd3ab5b-825a-4438-a6ce-7ab87497fabf_600x449.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bd3ab5b-825a-4438-a6ce-7ab87497fabf_600x449.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bd3ab5b-825a-4438-a6ce-7ab87497fabf_600x449.jpeg" width="600" height="449" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7bd3ab5b-825a-4438-a6ce-7ab87497fabf_600x449.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:449,&quot;width&quot;:600,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:178290,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.professoraxelrod.com/i/165486167?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bd3ab5b-825a-4438-a6ce-7ab87497fabf_600x449.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bd3ab5b-825a-4438-a6ce-7ab87497fabf_600x449.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bd3ab5b-825a-4438-a6ce-7ab87497fabf_600x449.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bd3ab5b-825a-4438-a6ce-7ab87497fabf_600x449.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bd3ab5b-825a-4438-a6ce-7ab87497fabf_600x449.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>But they didn’t repeal it, so when this went into effect, companies panicked as the tax steamroller ran over them.</figcaption></figure></div><p>The delayed change to Section 174, from immediate expensing of R&amp;D to “mandatory amortization”, meaning that companies must spread the deduction out in smaller chunks over five or even 15-year periods… that’s one of them, as I’m sure you’ve figured out. The delayyed start meant that it would not begin affecting the budget until 2022, but it helped the TCJA appear “deficit neutral” over the 10-year window used for legislative “scoring”.</p><p><span>The delay wasn’t legally </span><em><strong>required</strong></em><span>, mind you, it was a way to game the system. These kind of political tactics are</span><a href="https://www.pbs.org/newshour/politics/as-house-gop-grinds-ahead-new-cbo-report-says-trumps-big-tax-cuts-bill-will-add-to-deficit?utm_source=chatgpt.com" rel=""> commononplace in tax legislation</a><span>. Phase-ins and or delayed-start provisions let lawmakers game how the Congressional Budget Office (CBO) -  Congress’ nonpartisan analyst of how bills impact budgets and deficits - “grades” legislation, because it kicks the costs down the road, outside the “official” forecasting windows. Those of you who play table-top games or RPGs would call this a way to “cheese the system” or the traditional euphemism used to be something along the lines of “robbing Peter to pay Paul”.</span></p><p><a href="https://technical.ly/civic-news/section-174-small-software-companies-taxes-explainer/" rel="">Many businesses expected Congress to repeal </a><span>or delay the Section 174 changes before they took effect in 2022, as there </span><em><strong>definitely</strong></em><span> was bipartisan support for immediate expensing. However, inaction led to a “shock” when 2022 tax bills arrived in 2023, forcing rapid cost-cutting, including layoffs. Small software firms, in particular, faced “extinction-level” tax bills, with some reporting taxable income tripling overnight, prompting layoffs or salary cuts. And bigger firms - Amazon, Meta/Facebook, Alphabet/Google, etc, Microsoft, Salesforce, etc - have had widespread layoffs in the US and have moved jobs overseas. Twilio cut 22% of its domestic workforce in 2023. Shopify cut 30% </span><em>(they’re based in Canada, but much of their R&amp;D was in the US - guess what, it isn’t anymore)</em><span>. Coinbase cut 36% of their team and there are still a heck of a lot of crypto bros, so I think they are probably not in a doomsday situation. </span></p><p><span>Now, I don’t want to say this was the </span><em><strong>only </strong></em><span>thing in 2023 that did this. There was </span><em><strong>a lot of economic turmoil </strong></em><span>on the horizon then: rising interest rates, reduced venture capital funding, supply chain problems, and post-pandemic over-hiring corrections, all amplifying financial pressures at the time. Companies like Meta announced layoffs during their “Year of Efficiency” in 2023, partly due to these tax changes and corresponding changes in ad spending.  While not the sole cause, the Section 174 change drove </span><em>(or accelerated)</em><span> layoffs that otherwise would probably have been unnecessary.</span></p><p><span>Congress has made </span><a href="https://technical.ly/startups/r-d-tax-change-reversal-startups/" rel="">noise previously about a bipartisan reversal</a><span> of this tax code change, and I rather hope they do - it would be a welcome boost to many sectors of the American economy, from manufacturing to pharmaceutical to technology to education to electrnics to scientific research and consulting.</span></p><p><span>Don’t think of this as just a problem for the tech space. I mean, the tech sector is ridiculously dominant for the S&amp;P 500, the “Magnificent Seven” are a third of the value of the S&amp;P 500. (That’s Alphabet, Amazon, Apple, Nvidia, Microsoft, Meta, and Tesla.) But if this does get repealed - it will be very good for those seven stocks - </span><em><strong>this is not an official stock tip, shush, you SEC guys.</strong></em></p><p>Throughout the 2010s, a broad swath of startups, direct-to-consumer brands, and internet-first firms… heck, basically every company that you would recognize from Instagram or Facebook ads… built their growth models around a kind of synthetic carefully engineered break-even. Lot of online firms, a lot of handheld or wearable tech firms, personal entertainment devices, ride-hailing firms, anything self-driving, all the recent buzzy things. </p><p><span>The tax code allowed them to spend aggressively on product and engineering, then write it all off as R&amp;D, keeping their taxable income close to zero by design. It worked because taxable income and actual cash flow were often not</span><em> </em><span>quite</span><em> </em><span>the same thing under what’s known as </span><a href="https://www.investopedia.com/terms/g/gaap.asp" rel="">GAAP</a><span> accounting practices. Basically, as long as spending counted as R&amp;D, companies could report losses to investors while owing almost nothing to the IRS. In short, </span><em><strong>it costs a lot to invent - and market - the future. Building a better tomorrow can be expensive!</strong></em><span> Investors generally bought into this, gave them </span><em><strong>another round of venture capital, and let them defer a public offering</strong></em><span>. </span><em>(This is actually another problem with this model - companies have stayed private far too long - but I’ll address that at some other point.)</em></p><p><span>But the Section 174 tax change absolutely cratered that model. Once those same expenses had to be spread out, or amortized, over multiple years, suddenly you couldn’t write these off - basically, the tax shield vanished or the accounting rules changed, depending on how politely you want to phrase it. But the mechanics of it are the same: companies that were still burning cash suddenly looked </span><em><strong>profitable on paper</strong></em><span>, triggering </span><em><strong>real tax bills on imaginary gains</strong></em><span>. </span><em>(If this reminds you of some of the past economic crises - it should - this is one of the things that burned people back in the dotcom crash of 2000 amongst other mark-to-market problems in 2008, though the 2008 crisis mostly wasn’t this.)</em></p><p><span>The logic that once fueled a generation of digital-first research-focused growth ran straight into a IRS-shaped brick wall and put a mighty dent in the work force. If you were already public and profitable - well, your management team wasn’t going let you suddenly become unprofitable just because of a tax law change, so the answer was “cut expenses” and that mostly became “slow CapEx on data centers </span><em>- servers are expensive -</em><span>  and lay off employees” so as to preserve profit margins and keep the stock price high. After all, reasoned management, if we had to put up with these crazy new rules, we basically just had to bank up R&amp;D credits for a few years and then we were back to par. It was a couple of years of drag on the economy - </span><em><strong>and a particularly bad time</strong></em><span> to fall behind on technology leadership or a chance to reshore manufacturing or stabilize the American economy. One suspects that the current administration, if they had noticed that, might have kicked out a repeal or other clever plan as part of their new budget package </span><em>(what’s currently being bundled together under the slightly goofy name of the Big Beautiful Bill)</em><span> to juice economic recovery.</span></p><p><span>So it wasn’t just tech experiencing effects. From 1954 until 2022, the U.S. tax code had encouraged businesses of all stripes to behave at </span><em><strong>least a little bit </strong></em><span>like we think of tech companies behaving, by which I mean “investing in R&amp;D” and more generally “investing in software” for the latter half of that. From retail to logistics, healthcare to media, if firms built internal tools, customized a software stack, or invested in business intelligence and data-driven product development, they could expense those costs - and the IRS generally agreed, which is slightly miraculous. The write-off incentivized in-house builds and fast growth well outside what people generally call the “tech sector”. For comparison, check the </span><a href="https://www.oecd.org/content/dam/oecd/en/publications/reports/2020/09/the-effects-of-r-d-tax-incentives-and-their-role-in-the-innovation-policy-mix_b7f9884d/65234003-en.pdf" rel="">OECD research</a><span> showing that immediate deductions foster innovation more than spread-out ones.</span></p><p><span>And American companies loved that logic and invested according to it. According to government data, U.S. businesses reported about $500 billion in </span><a href="https://ncses.nsf.gov/pubs/nsf22314" rel="">R&amp;D expenditures</a><span> in 2019 alone, and almost half of that came from industries outside traditional tech. The Bureau of Economic Analysis estimates that this sector, the broader digital economy, </span><a href="https://itif.org/publications/2023/05/30/six-tech-industries-accounted-for-more-than-one-third-of-gdp-growth-in-the-last-decade/" rel="">accounts</a><span> for another 10% of GDP. Probably 20% if you count Big Tech. And there’s a secondary market - all the people who support and are downstream from those workers and those industries </span><em>(see below)</em><span>; we’re actually introducing relatively-avoidable friction into about a quarter of the American economy here with this particular tax change.</span></p><p><span>The result? A tax policy aimed at raising short-term revenue basically defined the growth engine for a huge chunk of American companies. But when that rug got pulled out, it also yanked out the incentive for hiring American engineers or investing in American-made tech and digital products. It made building tech companies in America suddenly not economically viable, and the last time we did this with a stupid policy change (Fed policy tightening “to defeat the Wealth Effect” and cool down speculative fervor … </span><em><strong>yup, it sure did! Lots of wealth went away, was that a good idea?</strong></em><span>) caused the big Dot Com Crash in 2000. It would probably be a good idea to put this tax credit back in place </span><em>(or some facsimile thereof)</em><span> before it continues to strip jobs out of the American economy - let alone all the folks (realtors, contractors, restaurants, nannies, tutors, personal trainers, et al) that are one step downstream from the tech sector jobs.</span></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
    </channel>
</rss>