<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 28 Jun 2024 19:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Supreme Court overrules Chevron deference [pdf] (109 pts)]]></title>
            <link>https://www.supremecourt.gov/opinions/23pdf/22-451_7m58.pdf</link>
            <guid>40821007</guid>
            <pubDate>Fri, 28 Jun 2024 14:36:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.supremecourt.gov/opinions/23pdf/22-451_7m58.pdf">https://www.supremecourt.gov/opinions/23pdf/22-451_7m58.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=40821007">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Supreme Court overturns 40-year-old "Chevron deference" doctrine (332 pts)]]></title>
            <link>https://www.axios.com/2024/06/28/supreme-court-chevron-doctrine-ruling</link>
            <guid>40820949</guid>
            <pubDate>Fri, 28 Jun 2024 14:31:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.axios.com/2024/06/28/supreme-court-chevron-doctrine-ruling">https://www.axios.com/2024/06/28/supreme-court-chevron-doctrine-ruling</a>, See on <a href="https://news.ycombinator.com/item?id=40820949">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-cy="au-image" data-chromatic="ignore"><img data-cy="StoryImage" alt="The U.S. Supreme Court in April 2024." fetchpriority="high" width="1920" height="1080" decoding="async" data-nimg="1" sizes="100vw" srcset="https://images.axios.com/0HWtMwcrXgCb_5lFNMsNTUU6Yy8=/0x900:8640x5760/320x180/2024/05/16/1715879188297.jpg?w=320 320w, https://images.axios.com/0HWtMwcrXgCb_5lFNMsNTUU6Yy8=/0x900:8640x5760/320x180/2024/05/16/1715879188297.jpg?w=320 320w, https://images.axios.com/7ZxL6bKpkNxWL6ubjcOmQOyctKo=/0x900:8640x5760/640x360/2024/05/16/1715879188297.jpg?w=640 640w, https://images.axios.com/7ZxL6bKpkNxWL6ubjcOmQOyctKo=/0x900:8640x5760/640x360/2024/05/16/1715879188297.jpg?w=640 640w, https://images.axios.com/lSDvbXXvNLIXeF2xPC0G-oouy-c=/0x900:8640x5760/768x432/2024/05/16/1715879188297.jpg?w=768 768w, https://images.axios.com/lSDvbXXvNLIXeF2xPC0G-oouy-c=/0x900:8640x5760/768x432/2024/05/16/1715879188297.jpg?w=768 768w, https://images.axios.com/hXC9UdkVp4dGz5KhHQvsDxPQfXw=/0x900:8640x5760/1024x576/2024/05/16/1715879188297.jpg?w=1024 1024w, https://images.axios.com/hXC9UdkVp4dGz5KhHQvsDxPQfXw=/0x900:8640x5760/1024x576/2024/05/16/1715879188297.jpg?w=1024 1024w, https://images.axios.com/Fqe49_mapPRZ-bwhEVixaLwVxSg=/0x900:8640x5760/1366x768/2024/05/16/1715879188297.jpg?w=1366 1366w, https://images.axios.com/Fqe49_mapPRZ-bwhEVixaLwVxSg=/0x900:8640x5760/1366x768/2024/05/16/1715879188297.jpg?w=1366 1366w, https://images.axios.com/99g12vnypLK_H6s5a3GwwpWuoss=/0x900:8640x5760/1600x900/2024/05/16/1715879188297.jpg?w=1600 1600w, https://images.axios.com/99g12vnypLK_H6s5a3GwwpWuoss=/0x900:8640x5760/1600x900/2024/05/16/1715879188297.jpg?w=1600 1600w, https://images.axios.com/9RMqkLL-qeuwtkEPKrMjkj_KObk=/0x900:8640x5760/1920x1080/2024/05/16/1715879188297.jpg?w=1920 1920w, https://images.axios.com/9RMqkLL-qeuwtkEPKrMjkj_KObk=/0x900:8640x5760/1920x1080/2024/05/16/1715879188297.jpg?w=1920 1920w" src="https://images.axios.com/9RMqkLL-qeuwtkEPKrMjkj_KObk=/0x900:8640x5760/1920x1080/2024/05/16/1715879188297.jpg?w=1920"><figcaption><p>The U.S. Supreme Court in April 2024. Photo; Bill Clark/CQ-Roll Call, Inc via Getty Images</p></figcaption></div><div data-chromatic="ignore"><p><span data-schema="smart-brevity"><p>The <a data-vars-link-text="Supreme Court" data-vars-click-url="https://www.axios.com/2024/06/20/supreme-court-rulings-trump-abortion" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/06/20/supreme-court-rulings-trump-abortion" target="_self">Supreme Court</a> on Friday curtailed the executive branch's ability to interpret laws it's charged with implementing, giving the judiciary more say in what federal agencies can do.</p><p><strong>Why it matters:</strong> The<strong> </strong><a data-vars-link-text="landmark 6-3 ruling" data-vars-click-url="https://www.supremecourt.gov/opinions/23pdf/22-451_7m58.pdf" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.supremecourt.gov/opinions/23pdf/22-451_7m58.pdf" target="_blank">landmark 6-3 ruling</a> along ideological lines overturns<strong> </strong>the court's 40-year-old "Chevron deference" doctrine. It could make it harder for executive agencies to tackle<strong> </strong>a wide array of policy areas, including <a data-vars-link-text="environmental" data-vars-click-url="https://www.axios.com/2024/01/17/supreme-court-chevron-environment" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/01/17/supreme-court-chevron-environment" target="_self">environmental</a> and <a data-vars-link-text="health" data-vars-click-url="https://www.axios.com/2024/01/17/supreme-court-chevron-deference-medicare-medicaid" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/01/17/supreme-court-chevron-deference-medicare-medicaid" target="_self">health</a> regulations and <a data-vars-link-text="labor and employment laws" data-vars-click-url="https://www.bloomberglaw.com/external/document/X1N1TE8K000000/labor-relations-professional-perspective-apres-moi-le-deluge-big" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.bloomberglaw.com/external/document/X1N1TE8K000000/labor-relations-professional-perspective-apres-moi-le-deluge-big" target="_blank">labor and employment laws</a>.</p></span></p><p><strong>Driving the news:</strong> Chief Justice John Roberts, writing the opinion of the court, argued Chevron "defies the command of" the Administrative Procedure Act, which governs federal administrative agencies.</p><ul><li>He said it "requires a court to ignore, not follow, 'the reading the court would have reached had it exercised its independent judgment as required by the APA.'"</li><li>Further, he said it "is misguided" because "agencies have no special competence in resolving statutory ambiguities. Courts do."</li></ul><p><strong>Roberts noted </strong>the court's decision did not call into question prior cases that relied on Chevron, including holdings pertaining to the Clean Air Act, because they "are still subject to statutory stare decisis despite our change in interpretive methodology."</p><ul><li>"Mere reliance on Chevron cannot constitute a 'special justification' for overruling such a holding," he said.</li></ul><p><strong>Justice Elena Kagan,</strong> in a dissenting opinion, wrote that the ruling Friday was "yet another example of the Court's resolve to roll back agency authority, despite congressional direction to the contrary."</p><ul><li>"Congress knows that it does not — in fact cannot — write perfectly complete regulatory statutes," she wrote. "It knows that those statutes will inevitably contain ambiguities that some other actor will have to resolve, and gaps that some other actor will have to fill. And it would usually prefer that actor to be the responsible agency, not a court.<strong>"</strong></li><li>She warned the decision "is likely to produce large-scale disruption." </li><li>"In one fell swoop, the majority today gives itself exclusive power over every open issue — no matter how expertise-driven or policy-laden — involving the meaning of regulatory law."</li><li>"The majority disdains restraint, and grasps for power."</li></ul><p><strong>Context: </strong>The ruling <a data-vars-link-text="marks another major victory" data-vars-click-url="https://www.axios.com/2023/05/25/supreme-court-epa-wetlands-clean-water-act" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2023/05/25/supreme-court-epa-wetlands-clean-water-act" target="_self">marks another major victory</a> for conservatives, who for decades have sought to limit the federal government's ability to regulate businesses.</p><ul><li>In the wake of the court's ruling, it's expected that more federal rules will be challenged in the courts and judges will have greater discretion to invalidate agency actions.</li><li>The decision comes one day after the Supreme Court <a data-vars-link-text="curtailed federal agencies' use of administrative law judges" data-vars-click-url="https://www.axios.com/2024/06/27/scotus-sec-jarkesy-decision" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/06/27/scotus-sec-jarkesy-decision" target="_self">curtailed federal agencies' use of administrative law judges</a> in another blow to the administrative state.</li></ul><p><strong>How it works:</strong> <a data-vars-link-text="The doctrine" data-vars-click-url="https://sgp.fas.org/crs/misc/R44954.pdf" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://sgp.fas.org/crs/misc/R44954.pdf" target="_blank">The doctrine</a> was created by the Reagan-era Supreme Court in Chevron U.S.A. v. Natural Resources Defense Council in 1984 and has since become <a data-vars-link-text="the most cited" data-vars-click-url="https://www.yalejreg.com/nc/most-cited-supreme-court-administrative-law-decisions-by-chris-walker/" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.yalejreg.com/nc/most-cited-supreme-court-administrative-law-decisions-by-chris-walker/" target="_blank">the most cited</a> Supreme Court decision in administrative law.</p><ul><li>Under Chevron deference, courts would defer to how to expert federal agencies interpret the laws they are charged with implementing provided their reading is reasonable — even if it's not the only way the law can be interpreted.</li><li>It allowed Congress to rely on the expertise within the federal government when implementing everything from health and safety regulations to environmental and financial laws.</li></ul><p><strong>Zoom in: </strong>However, Chevron was challenged in two separate cases over a National Marine Fisheries Service regulation meant to prevent overfishing on commercial fishing vessels.</p><ul><li>Fishing companies challenging the regulation claimed the doctrine violated Article III of the Constitution by shifting the authority to interpret federal law from the courts to the executive branch.</li><li> They also claimed it violated Article I by allowing agencies to formulate policy when only Congress should have lawmaking power.</li></ul><p><strong>The other side:</strong> The government argued that the doctrine had  safeguards within it that prevented agencies from usurping Congress's lawmaking authority.</p><ul><li>It noted that Chevron only applied to ambiguous text in laws passed by Congress and instances in which lawmakers had given interpretive authority to an agency.</li><li>The doctrine was also necessary to limit federal judges' abilities to make public policy when they may not have the expertise to do so and aren't subject to democratic accountability, the government said.</li></ul><p><strong>Between the lines: </strong>Lawyers who worked pro bono to represent fishing companies involved in the cases are also staff attorneys for Americans for Prosperity, a libertarian political advocacy group funded by billionaire Charles Koch, the <a data-vars-link-text="New York Times" data-vars-click-url="https://www.nytimes.com/2024/01/16/climate/koch-chevron-deference-supreme-court.html" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.nytimes.com/2024/01/16/climate/koch-chevron-deference-supreme-court.html" target="_blank">New York Times</a> reported earlier this year.</p><ul><li>The political network associated with Charles Koch and his late brother, David Koch, have long championed efforts to get cases before the Supreme Court that, if decided in their favor, would roll back the federal government's regulatory powers.</li><li>The Koch network also successfully attracted Supreme Court Justice Clarence Thomas, who voted against the doctrine, to speak at at least one of its <a data-vars-link-text="donor events" data-vars-click-url="https://www.axios.com/2023/09/22/clarence-thomas-koch-brothers-donor-events" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2023/09/22/clarence-thomas-koch-brothers-donor-events" target="_self">donor events</a> in 2018, <a data-vars-link-text="ProPublica" data-vars-click-url="https://www.propublica.org/article/clarence-thomas-secretly-attended-koch-brothers-donor-events-scotus" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.propublica.org/article/clarence-thomas-secretly-attended-koch-brothers-donor-events-scotus" target="_blank">ProPublica</a> reported last year.</li><li>It's unclear who purchased Thomas' flight to the 2018 event, as he never  reported it in his annual financial disclosure form. Thomas has attended at least two of such events in past years.</li></ul><p><strong>The big picture: </strong>In recent years, Chevron had fallen out of favor of the <a data-vars-link-text="conservative-majority Supreme Court" data-vars-click-url="https://crsreports.congress.gov/product/pdf/LSB/LSB11061" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://crsreports.congress.gov/product/pdf/LSB/LSB11061" target="_blank">conservative-majority Supreme Court</a>, which had declined to apply it or cite it in cases which it may once have applied.</p><ul><li>The ruling comes as some federal judges have taken a more active role in overruling agency expertise.</li><li>For example, Texas District Judge Matthew Kacsmaryk last April <a data-vars-link-text="paused" data-vars-click-url="https://www.axios.com/2024/03/23/abortion-pill-access-supreme-court-mifepristone-fda" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/03/23/abortion-pill-access-supreme-court-mifepristone-fda" target="_self">paused</a> the FDA's original 23-year-old approval of the abortion pill mifepristone in a case that's now to be decided by the Supreme Court.</li></ul><p><strong>Go deeper: </strong><a data-vars-link-text="Supreme Court brushes off payday lenders' challenge to consumer watchdog's funding" data-vars-click-url="https://www.axios.com/2024/05/16/supreme-court-cfpb-funding" data-vars-content-id="319f7ddc-f588-4484-955d-818b37e93d50" data-vars-headline="Supreme Court guts agency power in seismic Chevron ruling" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2024/05/16/supreme-court-cfpb-funding" target="_self">Supreme Court brushes off payday lenders' challenge to consumer watchdog's funding</a></p><p><em>Editor's note: This story was updated with details from the court's ruling.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple II graphics: More than you wanted to know (106 pts)]]></title>
            <link>https://nicole.express/2024/phasing-in-and-out-of-existence.html</link>
            <guid>40820311</guid>
            <pubDate>Fri, 28 Jun 2024 13:14:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nicole.express/2024/phasing-in-and-out-of-existence.html">https://nicole.express/2024/phasing-in-and-out-of-existence.html</a>, See on <a href="https://news.ycombinator.com/item?id=40820311">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>The Apple ][ is one of the most iconic vintage computers of all time. But since Wozniak’s monster lasted all the way until 1993 (1995 if you could the IIe card, which I won’t count until I get one), it can be easy to forget that in 1977, it was a video <em>extravaganza</em>. The competitors– even much bigger and established companies like Commodore and Tandy– generally only had text modes, let alone pixel-addressable graphics, and they certainly didn’t have sixteen colors. (Gray and grey are different colors, right?)</p>

<h2 id="preliminary">Preliminary</h2>

<p>My main source here is going to be <em>Understanding the Apple II</em> by Jim Sather. You might say, why should I read this post then, when I can go to the source? And honestly <a href="https://archive.org/details/utaii">yeah go do that</a>. What I’ll do here is try to digest it for myself by writing it in a form I find understandable, focused on details I find interesting. I’ll also throw together some looks at my own personal Apple II, maybe an oscilloscope, that sort of thing.</p>

<p><img src="https://nicole.express/assets/a2p-2.JPG" title="This photo is from one of the earliest posts on this blog" alt="The Apple II, plugged into a greenscreen monitor"></p>

<p>Now, my personal Apple ][<sub><i>plus</i></sub> and the book have something in common: they predate the Apple IIe. So this blog post will focus on the original Apple ][ designed machines. So when I talk about graphics mode, you won’t see the 80 column or double-width graphics modes. Those were IIe features; there were no provisions for such things on the original models. If this post proves interesting maybe I’ll dig into them later; I have an Apple IIgs, so certainly I <em>can</em> explore IIe exclusives. (And Jim Sather even wrote a follow-up book, <em>Understanding the Apple IIe</em>)</p>



<p><img src="https://nicole.express/assets/img/2apple2furious/lang.jpeg" title="taking out the language card is a pain so I left it in" alt="Apple II motherboard"></p>

<p>Here is the motherboard of my Apple II plus. It’s serial number 820-0044-01, which despite the 1979 copyright date, implies it’s definitely one of the later Apple IIs of its type– in 1981, the 820-0044-XX motherboard series was created by Apple in order to try to reduce radio-frequency interference (RFI), so this is known as the “RFI” motherboard. Go dig into <a href="https://archive.org/details/apple-ii-circuit-description/page/n157/mode/2up"><em>The Apple II Circuit Description</em></a> for all the nitty-gritty on motherboard variants.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/serial.jpeg" title="I want a Rev 0 so bad" alt="Apple II motherboard serial number"></p>

<p>It’s worth noting for those not familiar with the Apple II that the “Apple II” and the “Apple II plus” are the same system, whose major difference is just the ROM. After the introduction of the new ROMs in 1979, there was a period where the same motherboard, when sold with the original Integer BASIC ROM set, it was an Apple II; when Applesoft (Microsoft BASIC for the Apple) was baked in instead, the badge was changed to II plus. Eventually all Apples shipped with Applesoft and the original II badges stopped being used. The internal capabilities are identical, including all the graphics modes I’ll talk about.</p>

<h2 id="everything-but-the-kitchen-sync">Everything but the kitchen sync</h2>

<p>Television video systems predate computers wanting to use them. Therefore, they are greedy– a video signal must produce the expected signals at the expected times, or your television will lose synchronization with the signal. Regaining synchronization will likely result in a delay, and definitely a loss of visual signals.</p>

<p>So when the video signal is being drawn, everything else has to bow to the video system’s will. The <a href="https://nicole.express/2022/the-nes-as-an-artifact.html">Nintendo Entertainment System</a> creates a separate world, the PPU bus, for the video system to inhabit, and the developer should avoid touching it unless it’s convenient. Other machine did things differently– the Atari 130XE I recently <a href="https://nicole.express/2024/have-you-typed-atari-today.html">upgraded the keyboard</a> on uses a variant of the 6502 processor with an extra pin whose sole purpose is to halt the CPU whenever the video chip needs extra time to access RAM. Both the 130XE and the NES have a 1.7MHz CPU, but the NES can run its just a little faster. (The 130XE has literally 64 times the CPU-accessible RAM, it’ll be fine)</p>



<p>The Apple II is a little bit different than that. The CPU can access RAM whenever it wants. The graphics system, known as the video scanner (it’s not one chip), can also access the RAM it needs whenever it wants. How did Woz do this? A clue is hidden in the <a href="http://www.6502.org/documents/datasheets/synertek/">6502 datasheets</a>– specifically, this diagram is from the August 1976 “SY6500/MCS6500 Microcomputer Family Hardware Manual”.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/book.png" title="This diagram is vintage" alt="Timing diagram for the 6502, showing that the address and data only both need be valid when the clock is down"></p>

<p>The important thing to note is to look at φ<sub>2</sub>, the input clock. The data output of external memory only needs to be valid at the very end of the period in which φ<sub>2</sub> is low. The <em>entire</em> period when φ<sub>2</sub> is high, the 6502 is doing internal stuff, and you don’t actually want its signals to show up on the data bus. So if you have sufficiently fast memory, you can have your memory off doing something else while φ<sub>2</sub> is low, and the 6502 will never know the difference.</p>



<p>Now, my understanding is that the Commodore VIC-20 interlaces its memory accesses the same way. But there are three major differences between the Apple II and the Commodore VIC-20. Well, okay, there’s more than that. But there are a few particularly relevant ones:</p>

<ol>
  <li>The VIC-20 uses static RAM. The Apple II’s video scanner also handles DRAM refresh, while the Commodore doesn’t need to worry about that.</li>
  <li>The VIC-20 has no directly-accessible-pixel screen modes.</li>
  <li>The VIC-20’s Video Interface Chip is, well, a chip. A highly integrated circuit that is opaque to exterior analysis, and with room for extra logic to simplify the external interface. The Apple II video scanner is constructed out of easily-analyzable discrete logic, and wears its implementation details on its sleeve.</li>
</ol>

<p>It’s that last one that I think is really important here. A lot of the fiddly details of the Apple II’s video that a programmer has to put up with could have been papered over with a few extra logic gates, internal registers, and buffers. On an integrated circuit this wouldn’t be a big deal as long as everything still fit within the planned mask size. But Steve Wozniak was building the Apple II out of discrete logic, and Apple paid for each one of those chips. So it was in the interest of cost-effectiveness that Apple offloaded some complexity to the programmer.</p>

<h3 id="what-frequency-is-it-anyways">What frequency is it anyways?</h3>

<p>Let’s talk about pixels. The Apple II has a core oscillator at 14.318180MHz (“14M”), which is divided by two to create a 7.15909MHz (“7M”) signal, and then divided again to create a 3.579545MHz signal. This latter suspiciously-specific frequency is the NTSC “colorburst” frequency. 7M is our pixel clock; during active display, a pixel is output every (1 / 7.15909MHz). A division of 7M by 7 gives you 1.0227MHz, which sounds like the 1MHz CPU clock. <em>But is it?</em></p>

<p>The horizontal scanning rate of NTSC television is 15.734kHz. PAL is 15.625kHz, but we’ll ignore that which challenges us. That means we have 63.56μs to finish a line, or a quick trip to <a href="https://www.wolframalpha.com/input?i=%281+%2F+15.734kHz%29+%2F+%281+%2F+14.318180MHz%29">Wolfram Alpha</a> says that’s 910 clicks of our 14M clock. 65 clicks of 1.0227MHz, 455 pixels (including in blanking periods; 280 pixels the screen actually draws), and therefore 227.5 ticks of our color reference. Which isn’t evenly divided.</p>

<p>That’s actually correct and how the spec expects it, however, we need to keep all the accesses perfectly synchronized, and we want the color reference to also be constant relative to the CPU. (Ever wonder why systems like PC clones don’t always have consistent artifact colors?) So the Apple II lengthens that last 65th clock– it’s the “long cycle”, taking an extra tick. Now the scanline frequency is dropped to 15.700kHz (fine for most TVs), but also now the Apple II CPU clock is <strong>not constant</strong>, it varies based off of where the screen is drawing. It’s 1.0205MHz on average, but only on average.</p>

<p>Combine that with the knowledge of the Apple II’s <a href="https://nicole.express/2021/stop-mocking-me.html">audio system</a> and the stock Apple II’s lack of any hardware timers, and this is actually worth knowing. Unfortunately, the Apple II doesn’t give the programmer any ability to know where in its cycle the video scanner is at any given time. (<i>Understanding the Apple II</i> has a few possible mods you can do to your computer to let it know, though!)</p>

<h2 id="text-mode">TEXT mode</h2>

<p>Many vintage computers are defined by their fonts. The PET’s PETSCII is iconic, of course (though probably moreso for its use on the Commodore 64). The TRS-80 had its “pseudographics” characters allowing for very blocky pixel graphics despite only having text mode. And I’ve always enjoyed the thick letters of the Atari 8-bit font, which not only look nice, but also help readability on a system whose text mode is single-color (varying only in luminance) and would be viewed by most users over a noisy RF modulator.</p>

<p><img src="https://nicole.express/assets/img/dasatari/name-intro.jpg" title="Please exclusively refer to me as (5-note tune) from now on" alt="The point in the software described above"></p>

<p>And then we get to the Apple II, with its stark simple character forms, and absolutely no special characters to speak of. (Though at least it has built in inverse and flashing text modes, always useful?) One can easily imagine Steve Jobs in the mindset that would lead to the lack of arrow keys on the first Macintosh keyboard, insisting that since the Apple II’s standout feature was its graphics modes, there would be no special incentives to create pseudographics with text mode.</p>

<p><video src="https://nicole.express/assets/img/2apple2furious/txt.mp4" width="640px" autoplay="" loop="" muted=""> You don't have a video tag support or something? So you can't see this footage of The Apple II, showing its narrow font? Ah too bad.</video></p>

<p>You could think that, but you’d be wrong. Early revisions of the Apple II used the Signetics 2513 character generator ROM. This was a commercial, off the shelf part. You can go find its datasheet online.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/sign.png" title="Understanding the Apple II says it was a GI version but everything was second and third sourced those days" alt="Signetics logo advertising a 2513 HIGH SPEED 64x7x5 CHARACTER GENERATOR"></p>

<p>This was a popular part that Woz had used earlier in the Apple 1, and was a popular use for hobbyist projects like the famous 1973 <a href="https://en.wikipedia.org/wiki/TV_Typewriter">TV Typewriter</a>. So the message the Apple II font actually sends is “hey tinkerers, this is for you”. Now later models of the Apple II, like my II plus, use a more standard mask ROM instead of this weird 5-bit character-specific ROM; you can even mod it to put your own EEPROM in. On my RFI board, it’s “ROM SPCL” deep under the keyboard.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/spcl.jpeg" title="a special rom for a special computer <3" alt="ROM SPCL is underneath the keyboard PCB. It is only visible by the edge of its socket"></p>

<p>Despite the Signetics ROM being only 5 pixels wide, text characters on the Apple II are 7 pixels wide. But why 7? Well, the reason is all that screen math again. The memory access clock is 7M divided by 7 to get the memory timing; so you have one memory cycle to get 7 pixels. We have 40 columns in the visible area, and 24 rows, fine for low resolution text mode. Pretty basic, right?</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/steve.jpg" title="I swear this won't become a habit but I couldn't resist" alt="Steve Jobs saying 'One More Thing'"></p>

<h3 id="memory-layout">Memory layout</h3>

<p>The official Apple <em>Apple II Reference Manual</em>, signed by Woz on the cover, provides some detail on the memory layout that starts to be a bit concerning.</p>

<blockquote>
  <p>The area of memory which is used for the primary text page starts at location number 1024 and extends to location number 2047. The secondary screen begins at location number 2048 and extends to location 3071. In machine language, the primary page is from hexadecimal address $400 to address $7FF; the secondary page is from $800 to $BFF. Each of these pages is 1,024 bytes long. Those of you intrepid enough to do the multiplication will realize that there are only 960 characters displayed on the screen. The remaining 64 bytes in each page which are not displayed on the screen are used as temporary storage locations by programs stored in PROM on Apple Intelligent Interface (r) peripheral boards (see page 82).</p>
</blockquote>

<p>You might wonder why they’re giving memory addresses in decimal– well, that was pretty normal for 70’s and 80’s computer manuals. You might also wonder why they’re so desperate for RAM that such a small amount of extra RAM would be in demand for peripheral cards– well, the original Apple II was sold with a base RAM configuration of 4kiB, so no addresses above <code>0x1000</code> would exist.</p>

<p>But the real question is how that memory is laid out. The Reference Manual just gives a screen map, but it doesn’t tell you <em>why</em> it is the way it is. My scan of this isn’t great, but you can see that the rows are very much not sequential.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/text.png" title="and you thought microsoft was bad at counting to ten" alt="Screen layout diagram"></p>

<p>I’ve called the Apple II screen memory layout bizarre before, and from a programmer’s perspective, it really is. But I was also criticized for that– because Steve Wozniak is really doing something quite impressive here. You have to understand the constraints he was under.</p>

<ol>
  <li><strong>Use as few chips as possible</strong>. Each piece of discrete logic costs money. So wherever possible work with the signals you have– the binary counters that are used for all that counting logic, for example.</li>
  <li><strong>But don’t waste RAM</strong>. The constraints of the television standard give us 40 columns wide. That’s not a simple binary number; you could have a gap after every text line, but that’d give 24 small areas of wasted RAM.</li>
  <li><strong>Refresh DRAM</strong>. DRAM addressing is pretty complicated, relying on “row” and “column” signals. But long story short, when going through the screen to display video, you also need to access every “row” address every 2ms. A 60Hz frame is 16.67ms. (Note that each chip is either 4kiB or 16kiB, so if you refresh the right range in one chip you can simultaneously refresh the rest)</li>
</ol>

<p>I’m not going to go into the full detail of the design because I think I’d just be repeating Jim Sather’s book in full here. But more-or-less, the screen is divided into three areas: top, middle, and bottom, each of eight rows. Then, the memory page, let’s use the primary text page <code>0x400</code>, is divided into eight subsections of 128 bytes each– 128 bytes gives us something our binary counters can easily catch. Each of these 128 byte sections is as follows:</p>

<ul>
  <li>One row of 40 characters for the top area</li>
  <li>One row of 40 characters for the middle area</li>
  <li>One row of 40 characters for the bottom area</li>
  <li>One 8 byte “screen hole” given to the Apple Intelligent Interface (r) peripheral boards</li>
</ul>

<p>In order to refresh a larger part of the screen during TEXT and LORES modes, the video scanner actually accesses different addresses during the horizontal blanking period, which allows it to refresh a wider range. These are wrong for video, but there’s no video during the blanking period. It doesn’t need to do this in HIRES mode, so it doesn’t.</p>



<p>From the programmer’s perspective, this usually just is papered over with a lookup table, and isn’t a big deal in the end.</p>

<h2 id="hires-mode">HIRES mode</h2>

<p>The Apple II offers no ability to customize the blocks in text mode; that Signetics ROM could not be replaced with RAM. This was the case for all three machines of the 1977 “Trinity”, but later Commodore machines would allow it. Unlike the other two “Trinity” machines, though, Apple lets you address the screen pixels directly.</p>

<p>With 40 bytes per row, and 24 * 8 = 192 rows in the visible area, you’d need at least 7.5kiB for just one screen. So we’ve abandoned the 4kiB Apple II users here– the 4kiB Apple II was not on the market very long though, and the upgraded 16kiB was the low-end model for most of the late 70’s. By the time of later models like my plus, 48kiB was more or less assumed anyways. With 16kiB you get one HIRES page at <code>0x2000</code>, with 48kiB you can get a second one at <code>0x4000</code>. (Double-buffering in 1977?) Of course, that’s a lot of RAM, so if you don’t need it a program will probably use it for something else.</p>

<p>Now, there’s an interesting problem that HIRES memory has to handle– the addresses and layout for text mode are very carefully chosen and set up to allow DRAM refresh. But now we need to get eight times as many addresses, with as few changes as possible. How do we do it? HIRES mode uses <em>higher</em> address bits, which are mapped to the DRAM “columns”, mostly not impacting the careful dance of refresh. But this creates a pretty wild memory layout.</p>

<p>
<svg width="640" height="860" viewBox="-10 0 320 420" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Memory map">
    
    <defs>
        <pattern id="striped" viewBox="0,0,7,8" width="4.16%" height="4.16%">
            <rect fill="#0000ff" x="-2" y="0" width="40" height="1"></rect>
            <rect fill="#00a0a0" x="-2" y="1" width="40" height="1"></rect>
            <rect fill="#00ff00" x="-2" y="2" width="40" height="1"></rect>
            <rect fill="#a0a000" x="-2" y="3" width="40" height="1"></rect>
            <rect fill="#ff0000" x="-2" y="4" width="40" height="1"></rect>
            <rect fill="#a0a0ff" x="-2" y="5" width="40" height="1"></rect>
            <rect fill="#ffa0a0" x="-2" y="6" width="40" height="1"></rect>
            <rect fill="#a000a0" x="-2" y="7" width="40" height="1"></rect>
        </pattern>
    </defs>
    <rect fill="#fff" x="-20" y="-20" width="320" height="450"></rect>
    <text x="0" y="12">Screen</text>
    <rect fill="url(#striped)" height="192" width="240" x="0" y="20"></rect>
    <text x="0" y="227">Memory</text>
    <rect fill="#0000ff" x="0" y="230" width="240" height="24"></rect>
    <rect fill="#00a0a0" x="0" y="254" width="240" height="24"></rect>
    <rect fill="#00ff00" x="0" y="278" width="240" height="24"></rect>
    <rect fill="#a0a000" x="0" y="302" width="240" height="24"></rect>
    <rect fill="#ff0000" x="0" y="326" width="240" height="24"></rect>
    <rect fill="#a0a0ff" x="0" y="350" width="240" height="24"></rect>
    <rect fill="#ffa0a0" x="0" y="374" width="240" height="24"></rect>
    <rect fill="#a000a0" x="0" y="398" width="240" height="24"></rect>
</svg>
</p>

<p>Apologies to the colorblind for the graph above! In fact, maybe I should just apologize to everyone with eyes. The SVGs are an experiment, we’ll see how they go.</p>

<p>Essentially, the HIRES memory space is divided into eight sections. The first section is the top row of pixels for each 7x8 text mode tile, the second section the second row, etc. etc. Each section (the large colored blocks in memory above) is itself laid out the same way as TEXT mode, complete with some screen holes. Confusing? Sure, but again, most programmers made a lookup table or two and called it a day. Each byte has seven pixels, the first bit being ignored. (The bits are also pushed to the screen in <em>opposite</em> order to how they’re usually written, but this is all just convention anyway)</p>

<h3 id="color">Color</h3>

<p><img src="https://nicole.express/assets/img/softcard/monitor3.jpg" title="Probably because it looks awesome" alt="The Monitor III sitting on the Apple II plus"></p>

<p>Everything I just described is good enough for business software and users of the monochrome Monitor ///. But this is an Apple II, the ultimate gaming PC of 1977. We want <em>color</em>. You probably know about “NTSC artifacts”, but what does that mean? And that’s where all our timing synchronization comes into play.</p>

<p>Imagine the Apple II drawing an alternating pixel pattern, <code>0101</code>. It draws those pixels at the rate of its pixel clock, <code>7M</code>. An important thing to remember about square waves is that the frequency is the frequency of a <em>complete cycle</em>, both the “up” and “down” of the wave.</p>

<p>
<svg width="500" height="325" viewBox="0 0 100 65" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <text x="5" y="7">7M (pixel clock)</text>
    <polyline points="10,10 10,20 20,20 20,10 30,10 30,20 40,20 40,10 50,10 50,20 60,20 60,10 70,10 70,20 80,20 80,10 90,10 90,20" stroke="red" stroke-width="2" fill="none"></polyline>
    <text x="5" y="27">3.5M (color signal)</text>
    <polyline points="10,30 10,40 30,40 30,30 50,30 50,40 70,40 70,30 90,30" stroke="blue" stroke-width="2" fill="none"></polyline>
    <text x="5" y="47">Pixels</text>
    <rect fill="#000" x="10" y="50" width="20" height="10" rx="4"></rect>
    <rect fill="#000" x="50" y="50" width="20" height="10" rx="4"></rect>
</svg>
</p>

<p>That is to say, if you alternate pixels, you’re creating a signal that repeats at the colorburst frequency! This is a real color signal, just like you’d generate if you had one of those fancy TMS9918As or something, but it’s being generated using the same mechanism that generates the pixels. (Sure, it’s a square wave here, but that’s what signal filters and such are for) Also, as the programmer, you get to control it directly.</p>

<p><img src="https://nicole.express/assets/img/pang-sparts/rf2av.jpg" title="Yep it's a rainbow" alt="Pong with rainbow backgrounds and art"></p>

<p>This is a screenshot from <a href="https://nicole.express/2024/super-duper-rainbow-pong.html">Atari’s <em>Pong Sports IV</em></a>, which like Atari’s other <em>Home Pong</em> series of consoles, uses a slightly out of phase crystal to create a cool rainbow effect. Obviously that isn’t possible here– with such strict pixel timing, we can only create two color phase shifts. And now you know why the “long cycle” that keeps the memory accesses in sync with the colorburst frequency is so important, or the phase shifts would also be different on each scanline.</p>

<p>
<svg width="500" height="400" viewBox="0 0 100 80" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <text x="5" y="7">7M (pixel clock)</text>
    <polyline points="10,10 10,20 20,20 20,10 30,10 30,20 40,20 40,10 50,10 50,20 60,20 60,10 70,10 70,20 80,20 80,10 90,10 90,20" stroke="red" stroke-width="2" fill="none"></polyline>
    <text x="5" y="27">3.5M (color signal)</text>
    <polyline points="10,30 10,40 30,40 30,30 50,30 50,40 70,40 70,30 90,30" stroke="blue" stroke-width="2" fill="none"></polyline>
    <text x="5" y="47">Patterns</text>
    <polyline points="10,50 10,60 30,60 30,50 50,50 50,60 70,60 70,50 90,50" stroke="orange" stroke-width="2" fill="none"></polyline>
    <polyline points="10,75 10,65 30,65 30,75 50,75 50,65 70,65 70,75 90,75" stroke="orange" stroke-width="2" fill="none"></polyline>
    <rect fill="rgba(0, 0, 0, 0.5)" x="10" y="50" width="20" height="10" rx="4"></rect>
    <rect fill="rgba(0, 0, 0, 0.5)" x="50" y="50" width="20" height="10" rx="4"></rect>
    <rect fill="rgba(0, 0, 0, 0.5)" x="30" y="65" width="20" height="10" rx="4"></rect>
    <rect fill="rgba(0, 0, 0, 0.5)" x="70" y="65" width="20" height="10" rx="4"></rect>
</svg>
</p>

<p>I deliberately used the chosen colors above because they are <em>not</em> the colors generated, because I don’t necessarily have the phase relationships perfectly correct. Take the diagrams above as basic conceptual scribbles, not necessarily oscilloscope traces– the point is, there are two signals: one in phase with the colorburst, one 180° out of phase with the colorburst. But the colorburst is itself defined as 180° out of phase with the color carrier, so this is a bit complex.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/snek.png" title="no step on snek" alt="Ultima II showing a green dorky-looking snake in a dungeon"></p>

<p>Anyway long story short, as <em>Ultima II</em> shows us above, it’s pink and green, the colors are pink and green. Well, pink is looking awfully purplish today, but that’s the wonder of NTSC. (And the horizontal lines visible on the green snake are the wonder of using square waves and this particular upscaler-capture combo) Notice those horizontal lines also picked up some color– if you want to guarantee white, you’ll need to create a signal with a frequency that <em>isn’t</em> a color carrier. The line above is just one pixel, but two pixels next to each other will do it.</p>

<p>
<svg width="500" height="325" viewBox="0 0 100 65" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <text x="5" y="7">7M (pixel clock)</text>
    <polyline points="10,10 10,20 20,20 20,10 30,10 30,20 40,20 40,10 50,10 50,20 60,20 60,10 70,10 70,20 80,20 80,10 90,10 90,20" stroke="red" stroke-width="2" fill="none"></polyline>
    <text x="5" y="27">3.5M (color signal)</text>
    <polyline points="10,30 10,40 30,40 30,30 50,30 50,40 70,40 70,30 90,30" stroke="blue" stroke-width="2" fill="none"></polyline>
    <text x="5" y="47">Pixels</text>
    <polyline points="10,50 10,60 50,60 50,50 90,50 90,60" stroke="orange" stroke-width="2" fill="none"></polyline>
    <rect fill="rgba(0, 0, 0, 0.5)" x="10" y="50" width="20" height="10" rx="4"></rect>
    <rect fill="rgba(0, 0, 0, 0.5)" x="30" y="50" width="20" height="10" rx="4"></rect>
</svg>
</p>

<p>Even if you alternate groups of two pixels, that signal isn’t at 3.579545MHz, so your television won’t be able to pull out any color information from it– it’ll just be treated as monochrome white. Modulo some higher-frequency fringes, after all, this <em>is</em> still good old-fashioned <a href="https://nicole.express/2021/shouldve-had-field-sequential.html">composite video</a> and no filter is perfect. Apple II users got used to some color fringing, or used a monochrome monitor.</p>

<p>Now, unfortunately, there is another catch here. Our addressable pixel areas are 7 pixels wide, which is not evenly divisible by two.<sup><i>citation needed</i></sup> This means that if you have a pattern, <code>x0101010</code>, whether it’s pink or green will depend on its position relative to the beginning of the line. Odd addresses will have one color, even addresses the other. This is why many Apple II games, like <em>Ultima III: Exodus</em>, create a grid of 14-pixel wide tiles that things move about on.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/exy.png" title="let my people go?" alt="Exodus, showing a tiled area"></p>

<p>This is still HIRES graphics mode, not a tile-based mode– the developer is just implementing tiles to make their lives easier. And on this title screen they only do so where things will move around in the bottom half.</p>

<h3 id="whats-the-big-deal">What’s the big deal?</h3>

<p>Now you might have noticed something about the screenshot above. It’s got colors that aren’t pink and green– it’s got blue water, and red lava. That’s true, but it’s only true because my Apple II isn’t a Revision 0. Those early adopters only have a three-color HIRES mode. The rest of us have something better.</p>

<p>Remember that first bit? It’d be pretty wasteful to just leave that unused. This is especially true from the perspective of the video scanner– this bit is fully decoded and just sitting there, waiting to have a use applied for it. What Woz did was have it delay the output of pixels by one cycle of the <em>14M</em> master clock, breaking the 7M pixel clock, but creating new phase shifts.</p>

<p>
<svg width="500" height="400" viewBox="0 0 100 80" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <text x="5" y="7">7M (pixel-ish clock)</text>
    <polyline points="10,10 10,20 20,20 20,10 30,10 30,20 40,20 40,10 50,10 50,20 60,20 60,10 70,10 70,20 80,20 80,10 90,10 90,20" stroke="red" stroke-width="2" fill="none"></polyline>
    <text x="5" y="27">3.5M (color signal)</text>
    <polyline points="10,30 10,40 30,40 30,30 50,30 50,40 70,40 70,30 90,30" stroke="blue" stroke-width="2" fill="none"></polyline>
    <text x="5" y="47">Patterns (offset)</text>
    <polyline points="9,50 20,50 20,60 40,60 40,50 60,50 60,60 80,60 80,50 90,50" stroke="#0d0" stroke-width="2" fill="none"></polyline>
    <polyline points="9,75 20,75 20,65 40,65 40,75 60,75 60,65 80,65 80,75 90,75" stroke="#0d0" stroke-width="2" fill="none"></polyline>
    <polyline points="10,50 10,60 30,60 30,50 50,50 50,60 70,60 70,50 90,50" stroke="rgba(0,0,0,0.2)" stroke-width="2" fill="none"></polyline>
    <polyline points="10,75 10,75 10,65 30,65 30,75 50,75 50,65 70,65 70,75 90,75" stroke="rgba(0,0,0,0.2)" stroke-width="2" fill="none"></polyline>
</svg>
</p>

<p>This gives us two more phases to work with, 90° out of phase with the pink and green colors we had before– blue and red-orange. But remember, we’re limited to using them within groups of 7 pixels. You can almost think of this as like being able to choose a palette for a 1-pixel high and 7-pixel wide area, except for…</p>

<h3 id="the-boundaries">The boundaries</h3>

<p>An interesting thing can be seen in this screenshot from Sega’s <em>Frogger</em>. You should now understand why Frogger is white, and why the colored areas are laid out the way they are. But take a look at where the coast meets the water. The green areas and the blue areas don’t quite line up.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/f1.png" title="Frogger on the TRS-80 has horizontal scrolling" alt="Frogger port"></p>

<p><img src="https://nicole.express/assets/img/2apple2furious/zoom.png" title="plus turtles arent as big as trucks, inaccurate" alt="Zoomed in to show misaligned blocks"></p>

<p>Why don’t they line up? It’s not the developer’s fault, it’s because they can’t. Take a look at what happens if I plug the Apple II’s output into the component luma input on the OSSC. (This is actually how I got most of the text mode captures too, to avoid unnecessary color noise)</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/f2.jpeg" title="the wormhole aliens HATE the apple ii because it is linear" alt="Frogger port all made out of lines"></p>

<p>The lines just don’t line up. The Apple II can get you in a mindset trap; of <em>course</em> it can’t line up, you might start to think. But on a system where the luminance and chrominance are set separately, and where it can output analog values, not just 0 or 1, of course it can. On the Apple II, all sorts of weirdness will happen where non-delayed pixels interact with delayed pixels. Say, at the edge of the screen, when we enter the screen border, the seven pixels will be abruptly cut off, leaving a half-pixel at the edge.</p>

<p>
<svg width="500" height="250" viewBox="0 40 100 50" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <text x="30" y="45">Screen boundary</text>
    <line stroke-width="2" x1="70" y1="0" x2="70" y2="100" stroke="#f00"></line>
    <polyline points="9,50 20,50 20,60 40,60 40,50 60,50 60,60 90,60" stroke="#0d0" stroke-width="2" fill="none"></polyline>
    <polyline points="9,75 20,75 20,65 40,65 40,75 60,75 60,65 70,65 70,75 90,75" stroke="#0d0" stroke-width="2" fill="none"></polyline>
</svg>
</p>

<p>Now, do these effects matter? On a CRT, probably not, but you might see some fun color fringes here and there. But if you’re wondering why your Apple II image capture is looking so much worse than your other systems, even over composite? Well, stuff like this doesn’t help.</p>

<p>HIRES is by far the most important graphics mode on the Apple II; more games used it than any of the other options, and even business software used it to do things like implement 80-column text in software. A bit awkward and weird? People got over it. I’ll end this discussion with one of my more nostalgic vintage HIRES intro sequences, from <em>Ultima II</em>. Sorry about the lack of audio; I kept the internal speaker wired up separately when I installed the Mockingboard, and this game only uses the internal beeper.</p>

<p><video src="https://nicole.express/assets/img/2apple2furious/u2.mp4" width="640px" controls="">You can't see this due to lacking video tag support. It's pretty cool, a dragon shows up and breathes fire.</video></p>

<p>I especially like the color animation on the “II” from toggling that seventh bit.</p>

<h2 id="kill-it-with-fire-or-a-transistor-will-do">Kill it with fire, or a transistor will do</h2>

<p>With all the above in mind, text mode <em>should</em> have the same color fringing everywhere that you see in HIRES graphics mode. The pixels are the same size, and Apple didn’t even design the font, so it’d be pretty impressive if it had been optimized to not fringe. But on most displays you’ll see nice pure white in Apple II text mode. How come?</p>

<p>Well, take a look at a screenshot of <em>Mission Asteroid</em> by Sierra. This uses a mixed mode, which I won’t really go into detail on how it works, but basically has four lines worth of text mode at the bottom of the screen underneath the graphics mode. And in this mode, the text fringes quite a bit. All those single-pixel horizontal lines suffer the same problem as the horizontal lines the snake was hanging out in in <em>Ultima II</em>.</p>

<p><img src="https://nicole.express/assets/img/yellow/mission.jpg" title="this is art hang it in the louvre" alt="A secretary sits at a desk. The game is a text adventure with a prompt at the bottom of the screen."></p>

<p>If I had a Revision 0 Apple II, I’d have the same experience with the pure text mode. But I don’t– that’s because later revisions of the Apple II like mine added a circuit called the “color killer”, which removes the color burst when in text mode. In theory, a signal without a color burst should always be interpreted by the TV as a monochrome signal, because NTSC is backwards-compatible. The problem is, the color killer isn’t great– it merely reduces the color burst.</p>

<p>Here’s the signal with the color burst present, which I obtained by booting BASIC and typing <code>HGR</code>.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/color1.png" title="doctor please my color's burst" alt="Oscilloscope trace showing color burst"></p>

<p>And here it is with the color burst on, which I obtained by typing <code>TEXT</code> with the same trace showing. I’m kind of surprised this ever doesn’t work, honestly– I guess that little bit of a cycle must be enough to confuse a sufficiently sensitive detector. Interesting, <em>Understanding the Apple II</em> suggests you mod your TV to detect the color burst, only suggesting modding the computer as a last resort, despite the many other computer mods recommended.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/color2.png" title="dont like all that noise" alt="Oscilloscope trace showing color burst gone, except for a cycle"></p>

<p>This impact for me has always shown up on higher-quality scalers, which are desperate to try to extract a color signal. The color killer works fine on a cheap AV2HD box, but the Micromsoft Framemeister ends up with a fringy mess. Check out my <a href="https://nicole.express/2021/composite-conflict-completed.html#test-10-the-apple-">composite scaler competition</a> post for more details on that.</p>

<p><img src="https://nicole.express/assets/img/composhoot/apii-meister.gif" title="I talk about the color killer way too much" alt="Poorly color killed signal"></p>

<h2 id="lores-mode">LORES mode</h2>

<p>The Apple II’s LORES graphics mode is very impressive. It can display any pixel on screen in any of 16 colors. Well, 15. More or less. There’s two greys that are usually the same. But still, far more colors than HIRES, and with pure pixel-level color selection. So what’s the catch? The resolution is a whopping 40x48. When even <a href="https://nicole.express/2024/radio-keith-orpheum.html">RCA Studio II</a> fans think you could use a few more pixels, you’re in trouble.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/duke1.png" title="Duke Nukem accepts all babes, regardless of how few pixels" alt="Duke Nukem port in LORES mode"></p>

<p>That’s not to say LORES mode is useless. <a href="http://deater.net/weave/vmwprod/duke/">Deater</a> has done quite a few demakes into LORES mode; such a low resolution makes it fast to update the whole screen even doing things like parallax, and the mixed text/graphics mode means you can use text mode for things that absolutely have to be readable, like scores and such.</p>

<p><img src="https://nicole.express/assets/img/mock-me/little-brick-out.png" title="cuz she's a brick... OUT" alt="Game Over screen in Little Brick Out. Only a few bricks are broken and the game is telling me my score is not too good."></p>

<p>And honestly LORES mode is part of the heart of the Apple II. It’s the mode that Wozniak created so that, having done <em>Breakout</em> in hardware for Atari, he could now do it in software with <em>Little Brick Out</em>. It’s the mode classic business simulation <em>Lemonade Stand</em> used. It justified the rainbow Apple logo. So how does it work?</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/lores.png" title="I took a new version of this picture, because I love you" alt="LORES color palette from the Diagonstics II plus disk"></p>

<p>The Koryuu I’m using here has a filter option that kind of blurs everything horizontally; I generally keep it disabled, but it does at least blur the lines together and gets rid of the high frequency noise. Of course, as we’ll see, that noise is the point. By the way, these color bars are from the Apple Diagnostics II+ disk.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/blurry.png" title="fuzzy" alt="LORES color palette from the Diagonstics II plus disk, blurred a bit"></p>

<p>The LORES mode is twice the height of TEXT mode, and that’s no coincidence. The same screen data as text mode is used, with the same layout– the difference is, the two “nybbles” of each byte each become one of 16 colors, stacked on top of each other. A LORES pixel is 7 HIRES pixels wide, and four HIRES pixels tall.</p>

<p>But how does LORES get so many colors?</p>

<p>
<svg width="500" height="325" viewBox="0 0 100 65" xmlns="http://www.w3.org/2000/svg" title="oh gods please age well SVG" alt="Pixel timing diagram">
    
    <rect fill="#fff" x="0" y="0" width="100" height="100"></rect>
    <line stroke-width="2" x1="50" y1="0" x2="50" y2="100" stroke="#ccc"></line>
    <text x="5" y="07">14M (oscillator)</text>
    <polyline points="10,10 10,20 15,20 15,10 20,10 20,20 25,20 25,10 30,10 30,20 35,20 35,10 40,10 40,20 45,20 45,10 50,10 50,20 55,20 55,10 60,10 60,20 65,20 65,10 70,10 70,20 75,20 75,10 80,10 80,20 85,20 85,10 90,10 90,20" stroke="green" stroke-width="2" fill="none"></polyline>
    <text x="5" y="27">7M (TEXT pixel clock)</text>
    <polyline points="10,30 10,40 20,40 20,30 30,30 30,40 40,40 40,30 50,30 50,40 60,40 60,30 70,30 70,40 80,40 80,30 90,30 90,40" stroke="red" stroke-width="2" fill="none"></polyline>
    <text x="5" y="47">3.5M (color signal)</text>
    <polyline points="10,50 10,60 30,60 30,50 50,50 50,60 70,60 70,50 90,50" stroke="blue" stroke-width="2" fill="none"></polyline>
</svg>
</p>

<p>Notice that for every four cycles of the <code>14M</code> master oscillator, there’s one cycle of the 3.5MHz color burst signal. So if you repeat a four-bit pattern at the rate of the 14MHz clock, you’ll create a signal with a period of 3.5MHz. That’s all <code>LORES</code> mode is doing to create its colors. Sure, there will be components of the signal at other frequencies– you can see the OSSC trying to show the high-frequency lines, while other devices like an RF modulator might blur the high-frequency signal out into a flat luminance.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/lores2.png" title="i am the lores, i speak for the pixels" alt="LORES color palette in monochrome, showing lines"></p>

<p>And the repeating patterns? That’s the genius part– they <em>are</em> the nybble in question. Take a look at the chart again. Repeating <code>0000</code> over and over again? Of course that’s a pure black. Repeating <code>1111</code> again and again? That’s pure white. What are the grey patterns? <code>0101</code> (5) and <code>1010</code> (10), which alternate fast enough that they don’t really have a low-frequency component, so no color to pick up on. There are two alternating patterns, so two greys. (For homework, consider what happens when those two greys are next to each other)</p>

<p>There is a bit more to it; for example, the phase inversion caused by having 7-pixel-wide slots is compensated for in LORES, but in general this is really a very clever graphics mode. Double HIRES mode on the 80-column Apple IIe uses the same pixel patterns, but that’s a story for another time.</p>

<h2 id="apple-ii-forever">Apple II Forever</h2>

<p><img src="https://nicole.express/assets/img/2apple2furious/oregon.png" title="not going to lie I have more nostalgia for the monochrome Mac oregon trail" alt="Oregon Trail: NICOLE has cholera"></p>

<p>The Apple II is one of the oldest computers I recall using; my kindergarten in 1995 had an old machine they let the kids bang on to avoid them breaking anything new. And yet that computer still fascinates me to this day. I think it’s because it’s not only a useful machine, with a lot of history, but also that despite its rougher edges, it’s something that beckons to be understood. And is well documented to boot. Definitely beats the arcade boards for that.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/timezone.png" title="this is the opening to TIME ZONE, the biggest Apple II game of all time, and the most expensive at release. if you can afford this game, you can afford this big house" alt="YOU ARE IN FRONT OF YOUR OWN HOUSE. Underneath an image of a house, from the Apple II game TIME ZONE."></p>

<p>Since Jim Sather’s book was crucial to the completion of this blog post, I think it’s only fair that we end with a quote from <em>Understanding the Apple 2</em>.</p>

<p><img src="https://nicole.express/assets/img/2apple2furious/suzy.png" title="what are you talking about" alt="...bus system. This means that a peripheral card can control all hardware features of the Apple. It is as if you could plug a Suzy brain into Johnny and have the Suzy brain control Johnny's body, a concept much in vogue in some circles."></p>

<p>Jim Sather is in way cooler circles than me.</p>

  </div>

  
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New ways to catch gravitational waves (141 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-024-02003-6</link>
            <guid>40820063</guid>
            <pubDate>Fri, 28 Jun 2024 12:41:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-024-02003-6">https://www.nature.com/articles/d41586-024-02003-6</a>, See on <a href="https://news.ycombinator.com/item?id=40820063">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>In September 2015, a vibration lasting just one-fifth of a second changed the history of physics. It was the <a href="https://www.nature.com/articles/530261a" data-track="click" data-label="https://www.nature.com/articles/530261a" data-track-category="body text link">first direct detection of gravitational waves</a> — perturbations in the geometry of space-time that move across the Universe at the speed of light.</p><p>Astronomers say it was like gaining a new sense — as if, until 2015, they had only been able to ‘see’ cosmic events, and now could ‘hear’ them, too. Since then, it has become almost a matter of daily routine to record the passage of gravitational waves at the two massive facilities of the Laser Interferometer Gravitational-wave Observatory (LIGO) in Louisiana and Washington state, along with their sibling Virgo observatory near Pisa, Italy.</p><p>The detection of gravitational waves has provided <a href="https://www.nature.com/articles/nature.2016.19337" data-track="click" data-label="https://www.nature.com/articles/nature.2016.19337" data-track-category="body text link">new ways to explore the laws of nature and the history of the Universe</a>, including clues about the life story of black holes and the large stars they originated from. For many physicists, the birth of gravitational-wave science was a rare bright spot in the past decade, says Chiara Caprini, a theoretical physicist at the University of Geneva in Switzerland. Other promising fields of exploration have disappointed: <a href="https://www.nature.com/articles/d41586-020-02741-3" data-track="click" data-label="https://www.nature.com/articles/d41586-020-02741-3" data-track-category="body text link">dark-matter searches</a> have kept coming up empty handed; the Large Hadron Collider near Geneva has found nothing beyond the Higgs boson; and even some promising hints of <a href="https://www.nature.com/articles/d41586-023-02532-6" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02532-6" data-track-category="body text link">new physics</a> seem to be fading. “In this rather flat landscape, the arrival of gravitational waves was a breath of fresh air,” says Caprini.</p><p>That rare bright spot looks set to become brighter.</p><p>All of the more than 100 gravitational-wave events spotted so far have been just a tiny sample of what physicists think is out there. The window opened by LIGO and Virgo was rather narrow, limited mostly to frequencies in the range 100–1,000 hertz. As pairs of heavy stars or black holes slowly spiral towards each other, over millions of years, they produce gravitational waves of slowly increasing frequency, until, in the final moments before the objects collide, the waves ripple into this detectable range. But this is only one of <a href="https://www.nature.com/articles/nature.2016.19337" data-track="click" data-label="https://www.nature.com/articles/nature.2016.19337" data-track-category="body text link">many kinds of phenomenon</a> that are expected to produce gravitational waves.</p><p>LIGO and Virgo are laser interferometers: they work by detecting small differences in travel time for lasers fired along perpendicular arms, each a few kilometres long. The arms expand and contract by minuscule amounts as gravitational waves wash over them. Researchers are now working on several next-generation LIGO-type observatories, both on Earth and, in space, the Laser Interferometer Space Antenna; some have even proposed building one on the Moon<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>. Some of these could be sensitive to gravitational waves at frequencies as low as 1 Hz.</p><p>But physicists are also exploring entirely different techniques to detect gravitational waves. These strategies, which range from watching pulsars to measuring quantum fluctuations, hope to catch a much wider variety of gravitational waves, with frequencies in the megahertz to nanohertz range (see ‘Opening the window on gravitational waves’).</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257006.png?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257006.png?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Opening the window on gravitational waves: graphic that shows a range of new detectors, and the range of frequencies from different sources that they will be able to detect." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257006.png">
  <figcaption>
   
  </figcaption>
 </picture>
</figure><p>By broadening their observational window, astronomers should be able to watch black holes circling each other for days, weeks or even years, rather than just catching the last few seconds before collision. And they’ll be able to spot waves made by totally different cosmic phenomena — including mega black holes and even the start of the Universe itself. All this, they say, will crack open many remaining secrets of the cosmos.</p><h2>Pulsar timing array: catching waves that last a decade</h2><p>Last year, one viable alternative to interferometers entered the game.</p><p>Since the early 2000s, radio astronomers have been attempting to use the entire Galaxy as a gravitational-wave detector. The trick is to monitor dozens of neutron stars called pulsars. These spin on their axis hundreds of times per second while emitting a radio-frequency beam, producing what looks like a pulse of light on each turn.</p><p>Gravitational waves sweeping the Galaxy would change the distance between Earth and each pulsar, creating anomalies in detected pulsar frequencies from one year to the next. Observations of a collection or array of pulsars — called a pulsar timing array (PTA) — should be able to detect changes induced by gravitational waves with frequencies of just nanohertz, as might be produced by pairs of supermassive black holes, for example. It takes tens of years for successive crests of such waves to pass a given vantage point, meaning that tens of years of observations are needed to spot them.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-02203-6" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_25559284.jpg"><p>Giant gravitational waves: why scientists are so excited</p></a>
 </article><p>In 2023, the PTA technique <a href="https://www.nature.com/articles/d41586-023-02203-6" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02203-6" data-track-category="body text link">began to bear fruit</a>. Four separate collaborations, in North America, Europe, Australia and China, <a href="https://www.nature.com/articles/d41586-023-02167-7" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02167-7" data-track-category="body text link">unveiled tantalizing hints</a> of a pattern expected from a random ‘stochastic background’ of gravitational waves that make Earth slosh around, probably caused by a cacophony of supermassive black-hole binaries, says astrophysicist Chiara Mingarelli at Yale University in New Haven, Connecticut.</p><p>The teams have not yet used the word ‘discovery’, because the evidence that each collaboration unveiled is not yet rock solid. But three of the groups — all but the Chinese one — are now pooling their data and conducting a joint analysis in the hope of getting to the ‘D’ word. This requires painstaking work, because each group processed its raw data in slightly different ways, and so it could take at least another year to get to publication, says Scott Ransom, an astrophysicist at the US National Radio Astronomy Observatory in Charlottesville, Virginia, and a senior member of the North American collaboration.</p><p>“In our current data, we almost certainly have the hints of individual supermassive black-hole binaries out there,” says Ransom. With each extra year of observation, they should get closer to resolving single black-hole pairs out of the cacophony, he adds. “Things are just going to get better and better.”</p><h2>Microwave telescopes: spotting waves from the Big Bang</h2><p>A year before LIGO’s 2015 detection, a team of cosmologists using a South Pole telescope called BICEP2 claimed to have spotted gravitational waves — not directly, but in the pattern of light called the cosmic microwave background (CMB), sometimes described as the afterglow of the Big Bang.</p><p>The <a href="https://www.nature.com/articles/nature.2014.15440" data-track="click" data-label="https://www.nature.com/articles/nature.2014.15440" data-track-category="body text link">BICEP2 claim turned out to be premature</a>, but cosmologists are now doubling down on this idea. An array of telescopes much more powerful than BICEP2, called the <a href="https://www.nature.com/articles/d41586-024-00333-z" data-track="click" data-label="https://www.nature.com/articles/d41586-024-00333-z" data-track-category="body text link">Simons Observatory</a>, is being set up on a mountaintop in northern Chile’s Atacama Desert. Some researchers are holding out hope for an even more powerful array called CMB-S4 (originally proposed to include 12 telescopes in Chile and at the South Pole) — although in May, plans for that project were put on hold because of the disrepair of the US South Pole base.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/nature.2016.19337" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257160.jpg"><p>Gravitational waves: 6 cosmic questions they can tackle</p></a>
 </article><p>What cosmologists are looking for in the CMB is a specific ‘B mode’ pattern in the swirls of its polarization — the preferential directions in which the microwaves wiggle — that would have been imprinted by the passage of gravitational waves. The theory is that such waves should have been produced by inflation, a quick burst of exponential cosmic expansion thought to have happened around the time of the Big Bang<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup>. Inflation would explain many of the Universe’s most striking properties, such as its flatness and how mass is distributed. The gravitational waves that inflation produced would have started at high frequencies, but would by now be at incredibly low frequencies of around 10<sup>−14</sup> Hz.</p><p>Although inflation is a cornerstone of accepted cosmological theory, there’s no proof of it yet. The B-mode pattern would be the smoking gun and, moreover, would reveal the energy scales involved, which would be a first step towards understanding what powered inflation.</p><p>The problem is, no one knows whether that energy scale was large enough to have left a noticeable mark. “Inflation predicts the B modes, but we don’t know if it’s big enough to be detected,” says Marc Kamionkowski, a theoretical astrophysicist at Johns Hopkins University in Baltimore, Maryland. But if the leading models are right, either the Simons Observatory or CMB-S4 should eventually find it, he says.</p><h2>Atom interferometry: closing the gap</h2><p>Although many of these projects push gravitational-wave science towards lower frequencies, they leave a crucial gap just below 1 Hz.</p><p>Detecting such frequencies could reveal mergers of black holes much more massive than those seen by LIGO (which spots waves from collapsing stars that weigh at most a few tens of solar masses). “This is an unexplored region, but it could be populated with lots of black holes,” says Caprini.</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27252636.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27252636.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Physicists Jason Hogan and Mark Kasevich pictured next to equipment they are developing for measuring gravitational waves." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27252636.jpg">
  <figcaption>
   <p><span>Jason Hogan (left) and Mark Kasevich work on an atom interferometer — a device that could reveal mergers of black holes much more massive than those seen by current laser interferometers.</span><span>Credit: L.A. Cicero and Stanford University</span></p>
  </figcaption>
 </picture>
</figure><p>A nascent technique could come to the rescue, according to physicist Oliver Buchmüller at Imperial College London. “Atom interferometry sits in that gap which we currently cannot explore with any other technology,” he says. An atom interferometer is a vertical high-vacuum pipe in which atoms can be released and allowed to fall under gravity. As they do so, physicists tickle the atoms with laser light to toggle them between an excited and a relaxed state — the same principle used by atomic clocks. “We’re trying to push this atomic-clock technique to what’s ultimately possible,” says Jason Hogan, a physicist at Stanford University in California.</p><p>To detect gravitational waves, physicists plan to drop two or more sets of atoms at different heights inside the same vertical pipe, and to measure the time it takes for a laser pulse to travel from one set of atoms to the next, says Hogan. The passage of gravitational waves would result in light spending either slightly less or slightly more time travelling between them — a variation smaller than one part in 100 billion billion.</p><p>Pioneering experiments at Stanford University have developed atom interferometers with 10-metre drops, but detecting gravitational waves would require devices at least 1 kilometre in height, which could be installed in a mine shaft, say, or even in space. As a first step, several groups around the world are planning to build 100-m atom interferometers as test beds. One such facility, called MAGIS-100, is already under construction in an existing shaft at the Fermi National Accelerator Laboratory outside Chicago, Illinois, and is scheduled for completion in 2027.</p><h2>Desktop detectors: pushing the frequency up</h2><p>Other researchers are exploring ways of detecting gravitational waves with much, much smaller (and cheaper) detectors — including some that could fit on a desktop. These are designed to watch for extremely high-frequency gravity waves. Known phenomena probably don’t produce such waves, but some speculative theories do predict them.</p><p>The Levitated Sensor Detector (LSD) at Northwestern University in Evanston, Illinois looks like a toy LIGO: it bounces lasers between pairs of mirrors just 1 metre apart. The LSD is a prototype for a new type of instrument designed to sense gravitational waves using resonance: the same principle by which even little pushes can make a child on a swing go higher and higher if they are timed just right<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup>.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d43978-024-00069-4" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257164.jpg"><p>Will the Einstein Telescope be split in two?</p></a>
 </article><p>In a vacuum inside each of the LSD’s arms, laser light suspends a particle just micrometres wide. As with an interferometer, the passage of gravitational waves will alternately elongate and compress the length of each arm. If the frequency of the gravitational waves resonates with that of the device, the lasers will then give many tiny kicks to the particle. The LSD can track the particle’s motion with a precision of femtometres, says Northwestern physicist Andrew Geraci, who is leading the project.</p><p>The LSD is designed to be sensitive to gravitational waves with frequencies of around 100 kHz. This prototype might already have a shot at detecting some, if the team can keep experimental noise in check — and provided that such waves exist. “Depending how optimistic you are, we may be able to measure a real signal in that band even with a 1-m instrument,” Geraci says. Future versions could be scaled up to 100-m-long arms, he adds, which would increase their sensitivity.</p><p>Theoretical physicist Ivette Fuentes at the University of Southampton, UK, has an idea for making an even smaller resonant detector. She aims to exploit sound waves in an exotic state of matter called a Bose–Einstein condensate (BEC) — a cloud of atoms kept at temperatures as low as a few millionths of a degree above absolute zero. If a gravitational wave passes through at a frequency that resonates with the sound wave, it can be detected. Because the act of looking for this signal destroys the BEC, a new flood of atoms needs to be released every second. The process might need to be repeated for months for a successful detection, Fuentes says.</p><p>In principle, a BEC-based detector could expand the search for gravitational waves to extremely high frequencies of 1 MHz or more — again, provided they exist. Fuentes says her scheme would require pushing BEC techniques just a little beyond the current state of the art. “I think the idea is very bold,” she says. Physicists have posited that high-frequency gravitational waves could reveal exotic physics that went on in the first second or so after the Big Bang. “We could use it to study the state of the Universe at very high energies,” says Caprini.</p><h2>Quantum crystal: only takes a second</h2><p>A final, more radical proposal for detecting gravitational waves involves putting objects in two places at once.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-024-00333-z" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-02003-6/d41586-024-02003-6_27257162.jpg"><p>‘Best view ever’: observatory will map Big Bang’s afterglow in new detail</p></a>
 </article><p>Sougato Bose, a physicist at University College London, has proposed a device in which a micrometre-sized diamond crystal is put in a superposition of two quantum states. In his scheme, the crystal’s two ‘personas’ would be pushed apart by as much as 1 metre and then brought together again — an extremely delicate procedure that has been compared to putting the nursery-rhyme character Humpty Dumpty back together after a fall. The passage of gravitational waves would make one persona travel further than the other when apart, putting them out of sync — in a measurable way — when reunited. The whole process would take around one second to complete, which would make the device sensitive to gravitational waves of around 1 Hz.</p><p>This idea is extremely ambitious: such quantum tricks have so far been shown to work only for objects the size of molecules, and no one has ever tested whether quantum weirdness can be pushed to such extremes. “Putting Humpty Dumpty back together has never been demonstrated for crystals,” says Bose.</p><p>But if the technique can be perfected, then table-top experiments such as this one could take gravitational-wave detection out of the hands of just a few large-scale labs. Together, these techniques could blow open the window on what can be seen. “The outlook is very positive,” says Caprini.</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is Clear Air Turbulence becoming more common? (152 pts)]]></title>
            <link>https://www.flightradar24.com/blog/is-cat-more-common/</link>
            <guid>40819784</guid>
            <pubDate>Fri, 28 Jun 2024 12:04:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.flightradar24.com/blog/is-cat-more-common/">https://www.flightradar24.com/blog/is-cat-more-common/</a>, See on <a href="https://news.ycombinator.com/item?id=40819784">Hacker News</a></p>
Couldn't get https://www.flightradar24.com/blog/is-cat-more-common/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[A Novel way to curb poaching, injecting radioisotopes into 20 live rhinoceros (117 pts)]]></title>
            <link>https://www.wits.ac.za/news/latest-news/general-news/2024/2024-06/a-novel-way-to-save-rhinos-.html</link>
            <guid>40819617</guid>
            <pubDate>Fri, 28 Jun 2024 11:35:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wits.ac.za/news/latest-news/general-news/2024/2024-06/a-novel-way-to-save-rhinos-.html">https://www.wits.ac.za/news/latest-news/general-news/2024/2024-06/a-novel-way-to-save-rhinos-.html</a>, See on <a href="https://news.ycombinator.com/item?id=40819617">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
          <article data-contentid="3353674">
  <span id="d.en.3353674"></span>
  
  <p>
    <time datetime="2024-06-25 08:54">25 June 2024</time> - Wits University 
  </p>
  <p>The Rhisotope Project at Wits is entering a new testing phase with the insertion of radioisotopes into 20 live rhinoceros.</p>
  <p>After three years of meticulous and dedicated hard work, the Rhisotope Project at Wits University has successfully inserted low doses of radioisotopes into 20 live rhinoceros.</p>
<p>In this final phase of the research project, Professor James Larkin from the University of the Witwatersrand’s Radiation and Health Physics Unit (RHPU) in collaboration with a team of experts who are leaders in the world of rhino conservation and veterinary work, will closely monitor the health and vital statistics of the rhinos over a period of six months, in order to determine the viability of this approach. .</p>
<p>The Rhisotope Project’s intention is to use nuclear technology in the form of small, measured quantities of radioisotopes and to insert these into the horns of rhinoceros, which can be picked up by radiation detection portal monitors at international borders, including at harbours, airports and land-crossings. These radioisotopes will provide an affordable, safe and easily applicable method to create long-lasting and detectable horn markers that cause no harm to the animals and environment. At a later stage, the work will expand to elephants, pangolins and other fauna and flora.</p>
<p>Being pioneered in the UNESCO Waterberg Biosphere Reserve, the Project aims to benefit from existing, sophisticated multi-billion-dollar nuclear security infrastructure that already exists throughout the world. Over 11 000 radiation detection portal monitors are installed at airports, harbours and other ports of entry, including thousands of trained personnel equipped with radiation detectors, all of which can detect the smallest radioactive particles. In contrast to this, the infrastructure and number of trained officials to detect wildlife trafficking at ports of entry internationally is extremely limited.</p>
  <section>
  <article>
    <iframe width="100%" height="315" src="//www.youtube.com/embed/Wy3hqdexWy0" frameborder="0" allowfullscreen=""></iframe>
  </article>
</section>
  
  <p>“Every 20 hours in South Africa a rhino dies for its horn. These poached horns are then trafficked across the world and used for traditional medicines, or as status symbols. This has led to their horns currently being the most valuable false commodity in the black-market trade, with a higher value even than gold, platinum, diamonds and cocaine. Sadly, rhino horns play a large role in funding a wide variety of criminal activities globally,” says Professor James Larkin. “Ultimately, the aim is to try to devalue rhinoceros horn in the eyes of the end users, while at the same time making the horns easier to detect as they are being smuggled across borders.”</p>
<p>Starting on Monday, 24 June 2004, Professor Larkin and his team carefully sedated the 20 rhinos &nbsp;and drilled a small hole into each of their horns to insert the non-toxic radioisotopes. The rhinos were then released under the care of a highly qualified crew that will monitor the animals on a 24-hour basis for the next six months. “Each insertion was closely monitored by expert veterinarians and extreme care was taken to prevent any harm to the animals,” says Larkin. “Over months of research and testing we have also ensured that the inserted radioisotopes hold no health or any other risk for the animals or those who care for them.”&nbsp;&nbsp;&nbsp;&nbsp;</p>
<p>The development and application of the Rhisotope Project nuclear technology has the capacity to help deter poaching, increase the detection capabilities of smuggled horns, increase prosecution success, reveal smuggling routes and deter end-user markets.</p>
<p>Rhino poaching reached crisis levels since 2008 where close to 10 000 rhinos were lost to poaching in South Africa, with wildlife trafficking being the third biggest organised crime globally.</p>
<p>Professor Lynn Morris, the Deputy Vice-Chancellor: Research and Innovation at Wits University says: “This is an example of how cross-disciplinary research and innovation makes a real difference. This novel approach pioneered by Prof Larkin and his colleagues has the potential to eradicate the threat of extinction our unique wild-life species&nbsp;&nbsp; , especially in South Africa and on the continent. This is one of many projects at Wits that demonstrates research with impact, and which helps to address some of the local and global challenges of the 21<sup>st</sup> Century.”</p>
<p>The Rhisotope Project at Wits was set up by a small team of likeminded individuals as a South African-based conservation initiative in January 2021 with the intention of becoming a global leader in harnessing nuclear technology to protect threatened and endangered species of fauna and flora as well as communities of people.</p>
<p>Aside from developing a solution to combat the illicit trade and trafficking of wildlife products, the Rhisotope Project seeks to provide education and social upliftment to empower people and local communities. A special focus is aimed at uplifting the girls and women of rural communities, who are often the backbone of these communities in the remote areas where endangered species are found and are the greatest components of success in changing the hearts and minds of local communities thereby creating rhino ambassadors and champions.</p>
</article>
<h5>Share</h5>


<span displaytext="Facebook"></span>
<span displaytext="LinkedIn"></span>

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Two million checkboxes (written in Elixir) (108 pts)]]></title>
            <link>https://twomillioncheckboxes.com</link>
            <guid>40819184</guid>
            <pubDate>Fri, 28 Jun 2024 10:05:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twomillioncheckboxes.com">https://twomillioncheckboxes.com</a>, See on <a href="https://news.ycombinator.com/item?id=40819184">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><a href="https://twomillioncheckboxes.com/" data-phx-link="redirect" data-phx-link-state="push">
      <span>Two</span> Million Checkboxes
    </a></p><p>(checking a box checks it for everyone)</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Parkinson's Link to Gut Bacteria Suggests Unexpected, Simple Treatment (127 pts)]]></title>
            <link>https://www.sciencealert.com/parkinsons-link-to-gut-bacteria-suggests-unexpected-simple-treatment</link>
            <guid>40818987</guid>
            <pubDate>Fri, 28 Jun 2024 09:23:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencealert.com/parkinsons-link-to-gut-bacteria-suggests-unexpected-simple-treatment">https://www.sciencealert.com/parkinsons-link-to-gut-bacteria-suggests-unexpected-simple-treatment</a>, See on <a href="https://news.ycombinator.com/item?id=40818987">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><img width="642" height="260" src="https://www.sciencealert.com/images/2023/04/microbes_epithelium_gut_AD-642x260.jpg" alt="Artist's impression of microbes on the gut lining." loading="eager" decoding="async" fetchpriority="high" srcset="https://www.sciencealert.com/images/2023/04/microbes_epithelium_gut_AD-642x260.jpg 642w, https://www.sciencealert.com/images/2023/04/microbes_epithelium_gut_AD.jpg 1024w, https://www.sciencealert.com/images/2023/04/microbes_epithelium_gut_AD-768x311.jpg 768w, https://www.sciencealert.com/images/2023/04/microbes_epithelium_gut_AD-600x243.jpg 600w" sizes="(-webkit-min-device-pixel-ratio: 2) 50vw,
			(min-resolution: 192dpi) 50vw,
			(min-resolution: 2dppx) 50vw,
			(-webkit-min-device-pixel-ratio: 3) 33.33vw,
			(min-resolution: 288dpi) 33.33vw,
			(min-resolution: 3dppx) 33.33vw"> <span>
<span>Illustration of bacteria on the colon epithelium.</span> <span>(Nanoclustering/Science Photo Library/Getty Images)</span> </span>
</p><div>
<p>Researchers have suspected for<a href="https://www.sciencealert.com/new-evidence-suggests-parkinson-s-might-start-in-the-gut-before-spreading-to-the-brain"> some time</a> that the link between our gut and brain plays a role in the development of <a href="https://www.sciencealert.com/go/IYl" data-linkid="73029" data-postid="130757" rel="nofollow" target="_self">Parkinson's</a> disease.</p><p>A new study just identified gut microbes likely to be involved and linked them with decreased <a href="https://en.wikipedia.org/wiki/Riboflavin">riboflavin</a> ( <a href="https://www.sciencealert.com/what-are-vitamins-and-do-we-really-need-to-take-them" data-linkid="73116" data-postid="130757" rel="nofollow" target="_self">vitamin</a> B2) and <a href="https://en.wikipedia.org/wiki/Biotin">biotin</a> (vitamin B7), pointing the way to an unexpectedly simple treatment that may help: B <a href="https://www.sciencealert.com/what-are-vitamins-and-do-we-really-need-to-take-them" data-linkid="73116" data-postid="130757" rel="nofollow" target="_self">vitamins</a>.</p>
<p>"Supplementation of riboflavin and/or biotin is likely to be beneficial in a subset of Parkinson's disease patients, in which gut dysbiosis plays pivotal roles," Nagoya University medical researcher Hiroshi Nishiwaki and colleagues <a href="https://doi.org/10.1038/s41531-024-00724-z">write</a> in their published paper.</p>
<p>The neurodegenerative disease impacts <a href="https://www.who.int/news-room/fact-sheets/detail/parkinson-disease">almost 10 million people globally</a>, who at best can hope for <a href="https://www.sciencealert.com/this-is-the-first-time-drug-shows-signs-of-slowing-parkinsons-disease">therapies that slow and alleviate symptoms</a>.</p>
<p>Symptoms typically begin with constipation and sleep problems, up to 20 years before progressing into dementia and the debilitating loss of muscle control.</p><figure id="attachment_131231" aria-describedby="caption-attachment-131231"><img decoding="async" src="https://www.sciencealert.com/images/2024/06/SHakingHandPIckingUpGlass-1.jpg" alt="Hands picking up a glass" width="642" height="500" srcset="https://www.sciencealert.com/images/2024/06/SHakingHandPIckingUpGlass-1.jpg 642w, https://www.sciencealert.com/images/2024/06/SHakingHandPIckingUpGlass-1-533x415.jpg 533w, https://www.sciencealert.com/images/2024/06/SHakingHandPIckingUpGlass-1-600x467.jpg 600w" sizes="(max-width: 642px) 100vw, 642px" loading="lazy"><figcaption id="caption-attachment-131231">(<a href="https://www.canva.com/photos/MAEGCRiWJIk/">pixelshot/Canva Pro</a>)</figcaption></figure><p><a href="https://www.sciencealert.com/changes-in-gut-bacteria-are-present-long-before-signs-of-parkinsons-appear">Previous research</a> found people with Parkinson's disease also experience changes in their microbiome long before other signs appear.</p>
<p>Analyzing fecal samples from 94 patients with Parkinson's disease and 73 relatively healthy controls in Japan, Nishiwaki and team compared their results with data from China, Taiwan, Germany, and the US.</p>
<p>While different groups of bacteria were involved in the different countries examined, they all influenced pathways that synthesize B vitamins in the body. The researchers found the changes in gut bacteria communities were associated with a decrease in <a href="https://en.wikipedia.org/wiki/Riboflavin">riboflavin</a> and <a href="https://en.wikipedia.org/wiki/Biotin">biotin</a> in people with Parkinson's disease.</p>
<p>Nishiwaki and colleagues then showed the lack of B vitamins was linked to a decrease in <a href="https://en.wikipedia.org/wiki/Short-chain_fatty_acid">short-chain fatty acids</a> (SCFAs) and <a href="https://en.wikipedia.org/wiki/Polyamine">polyamines</a>: molecules that help create a healthy mucus layer in the intestines.</p>
<p>"Deficiencies in polyamines and SCFAs could lead to thinning of the intestinal mucus layer, increasing intestinal permeability, both of which have been observed in Parkinson's disease," Nishiwaki <a href="https://www.nagoya-u.ac.jp/researchinfo/result-en/2024/06/20240618-01.html">explains</a>.</p><figure id="attachment_131227" aria-describedby="caption-attachment-131227"><img decoding="async" src="https://www.sciencealert.com/images/2024/06/GraphicOfParkinsonsVitaminBConnection642.jpg" alt="A graphic depicting the process of gut bacteria depleting B vitamins and leading to symptoms of Parkinson's disease" width="642" height="950" srcset="https://www.sciencealert.com/images/2024/06/GraphicOfParkinsonsVitaminBConnection642.jpg 642w, https://www.sciencealert.com/images/2024/06/GraphicOfParkinsonsVitaminBConnection642-280x415.jpg 280w, https://www.sciencealert.com/images/2024/06/GraphicOfParkinsonsVitaminBConnection642-600x888.jpg 600w" sizes="(max-width: 642px) 100vw, 642px" loading="lazy"><figcaption id="caption-attachment-131227">Summary of findings from the study and speculations from previous research. (Nishiwaki et al., <a href="https://doi.org/10.1038/s41531-024-00724-z"><em>npj Parkinson's Disease</em></a>, 2024)</figcaption></figure><p>They suspect the weakened protective layer exposes the intestinal nervous system to more of the toxins we now encounter more regularly. These include <a href="https://www.sciencealert.com/dry-cleaning-chemical-could-be-major-cause-of-parkinsons-scientists-warn">cleaning chemicals,</a> <a href="https://www.sciencealert.com/two-pathways-to-parkinsons-could-point-to-a-single-way-to-prevent-it">pesticides</a>, and herbicides.</p>
<p>Such toxins lead to the overproduction of α-synuclein fibrils – molecules <a href="https://doi.org/10.1016/j.nbd.2024.106411">known to amass</a> in <a href="https://www.sciencealert.com/new-test-detects-parkinsons-7-years-before-most-symptoms-show">dopamine-producing cells</a> in the <a href="https://www.sciencealert.com/scientists-pinpoint-exactly-which-brain-cells-die-in-parkinson-s-disease">substantia nigra</a> part of our brains, and increased nervous system inflammation, eventually leading to the more debilitating motor and dementia symptoms of Parkinson's.</p>
<p>A <a href="https://www.scielo.br/j/bjmbr/a/BM4WLJBtjxF8Cx3wFsjFhKb/?lang=en">2003 study</a> found high doses of riboflavin can assist in recovering some motor functions in patients who also eliminated red meat from their diets.</p>
<p>So it's possible that high doses of vitamin B may prevent some of the damage, Nishiwaki and team propose.</p><figure id="attachment_131228" aria-describedby="caption-attachment-131228"><img decoding="async" src="https://www.sciencealert.com/images/2024/06/VitaminBInBlood642.jpg" alt="Illustration of a vitamin B2 molecule in the blood. " width="642" height="424" srcset="https://www.sciencealert.com/images/2024/06/VitaminBInBlood642.jpg 642w, https://www.sciencealert.com/images/2024/06/VitaminBInBlood642-628x415.jpg 628w, https://www.sciencealert.com/images/2024/06/VitaminBInBlood642-600x396.jpg 600w" sizes="(max-width: 642px) 100vw, 642px" loading="lazy"><figcaption id="caption-attachment-131228">Illustration of a riboflavin (B2) molecule in the blood. (Nemes Laszlo/Science Photo Library/Getty Images)</figcaption></figure><p>This all suggests ensuring patients have healthy gut microbiomes may also prove protective, as would <a href="https://www.hopkinsmedicine.org/health/conditions-and-diseases/parkinsons-disease/can-environmental-toxins-cause-parkinson-disease">reducing the toxic pollutants</a> in our environment.</p>
<p>Of course, with such a complicated chain of events involved in Parkinson's disease, not all patients likely experience the same causes, so each individual would need to be assessed.</p>
<p>"We could perform gut microbiota analysis on patients or conduct fecal metabolite analysis," <a href="https://www.nagoya-u.ac.jp/researchinfo/result-en/2024/06/20240618-01.html">explains</a> Nishiwak.</p>
<p>"Using these findings, we could identify individuals with specific deficiencies and administer oral riboflavin and biotin supplements to those with decreased levels, potentially creating an effective treatment."</p><p>This research was published in <a href="https://doi.org/10.1038/s41531-024-00724-z"><em>npj Parkinson's Disease</em></a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: What is the best code base you ever worked on? (148 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40818809</link>
            <guid>40818809</guid>
            <pubDate>Fri, 28 Jun 2024 08:40:44 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40818809">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="40818809">
      <td><span></span></td>      <td><center><a id="up_40818809" href="https://news.ycombinator.com/vote?id=40818809&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=40818809">Ask HN: What is the best code base you ever worked on?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_40818809">109 points</span> by <a href="https://news.ycombinator.com/user?id=pcatach">pcatach</a> <span title="2024-06-28T08:40:44"><a href="https://news.ycombinator.com/item?id=40818809">9 hours ago</a></span> <span id="unv_40818809"></span> | <a href="https://news.ycombinator.com/hide?id=40818809&amp;goto=item%3Fid%3D40818809">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20What%20is%20the%20best%20code%20base%20you%20ever%20worked%20on%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=40818809&amp;auth=5ee72ab5243c5242c20ff4d4f59b5526ab0dd94a">favorite</a> | <a href="https://news.ycombinator.com/item?id=40818809">96&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>And what made it so good?</p><p>Was there someone enforcing good practices top down? Just being in a group of great engineers? Or something else?</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table><table>
            <tbody><tr id="40823142"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40823142" href="https://news.ycombinator.com/vote?id=40823142&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Google's monorepo, and it's not even close - primarily for the tooling:</p><p>* Creating a mutable snapshot of the entire codebase takes a second or two.</p><p>* Builds are perfectly reproducible, and happen on build clusters. Entire C++ servers with hundreds of lines of code can be built from scratch in a minute or two tops.</p><p>* The build config language is really simple and concise.</p><p>* Code search across the entire codebase is instant.</p><p>* File history loads in an instant.</p><p>* Line-by-line blame loads in a few seconds.</p><p>* Nearly all files in supported languages have instant symbol lookup.</p><p>* There's a consistent style enforced by a shared culture, auto-linters, and presubmits.</p><p>* Shortcuts for deep-linking to a file/version/line make sharing code easy-peasy.</p><p>* A ton of presubmit checks ensure uniform code/test quality.</p><p>* Code reviews are required, and so is pairing tests with code changes.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40823346"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40823346" href="https://news.ycombinator.com/vote?id=40823346&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>&gt; Entire C++ servers with hundreds of lines of code can be built from scratch in a minute or two tops.</p><p>Hundreds, huh? Is this a typo? It makes me wonder if the whole comment is facetious. Or do C++ programmers just have very low expectations for build time?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40823383"><td></td></tr>
                        <tr id="40819222"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819222" href="https://news.ycombinator.com/vote?id=40819222&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>The one in my previous job, which was an admin board for a market intelligence application. Ultimately, the reason it was good was because the engineers had zero ego on top of having excellent skills. The team that set the codebase were basically 4 seniors and 3 principals (the client actually did pay for top talent in this case) so not only everything was based on industry standards, written elegantly and organized perfectly, but every time some new requirement came up, these senior / principal engineers would discuss it in the most civilized matter I have ever seen.</p><p>E.g, "we need to come up with a way to implement X". Person A gives their idea, person B gives another idea and so on until everybody shared their thoughts. Then someone would say "I think what person C said makes the most sense" and everybody would agree and that was it. 30 minutes to hear everybody out, 3 minutes to discuss who will do it and when and the meeting was over.</p><p>I think the biggest testament to this code base was that when junior members joined the team, they were able to follow the existing code for adding new features. It was that easy to navigate and understand the big picture of.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40823111"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40823111" href="https://news.ycombinator.com/vote?id=40823111&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I've seen a few people say 'google3'.</p><p>Q: is it actually the code that you loved, or simply the tooling that exists?</p><p>(and if it's tooling, why can't that type of tooling be replicated for other codebases outside of google?)</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40823510"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40823510" href="https://news.ycombinator.com/vote?id=40823510&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Its both. The tooling has a very direct impact on the quality of the code.</p><p>I think the reason its not easy replicable is:</p><p>1. It takes a ton of initial investment and ongoing maintenance but its worth it when your code base is gigantic.</p><p>2. There is a consistent set of top down enforced rules. With the consistency it becomes much, much easier to build tight integrations between tools.</p><p>(almost?) everything is buildable by a single build system (blaze). When anyone can consistently build/test/run anything in your codebase it becomes a lot easier to build a whole host of potential tools like code search.</p><p>Probably someone can dive deeper than I can. But one thing I learned the most important property for a code base to be maintainable/scalable is consistency.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40823459"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40823459" href="https://news.ycombinator.com/vote?id=40823459&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>&gt; (and if it's tooling, why can't that type of tooling be replicated for other codebases outside of google?)</p><p>The elegance of the tooling from what I hear is that there's tons of different tools maintained by different teams that work seamlessly (and fast) together to produce google3 and all of its supporting pieces.</p><p>But to answer your question, sure it can. But good luck building your own. Google has been doing this since the 2000s.</p><p>And if you're a big company already, you've already bought into your existing patterns &amp; design choices; things like that are VERY hard to change.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40822429"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40822429" href="https://news.ycombinator.com/vote?id=40822429&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Postgres. I don't code in C if I can avoid it, since it often feels like an awful lot of extra typing while still having to worry about memory safety. But the Postgres codebase is extraordinarily well organized and respects the humans that work with it with its intelligent handling of memory and judicious use of macros.</p><p>I consider the core Postgres codebase to be the gold standard in development even though it's in a language I do not prefer to write in if given the choice.</p><p>Shout out to the pgrx folks. You're awesome! <a href="https://github.com/pgcentralfoundation/pgrx">https://github.com/pgcentralfoundation/pgrx</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819157"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819157" href="https://news.ycombinator.com/vote?id=40819157&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Google3 codebase. It's just so vast and it works. It's a miracle. I feel lucky to have seen it. Everytime you change it, it reruns the dependencies. Everyone has different view concurrently. Commits are efficient immutable snapshots. It's just incredible multiplayer. So massively beyond what can be done with GitHub. Really I feel it's peak codebase. I've not seen what the other big techs do but it blew my mind</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40823120"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40823120" href="https://news.ycombinator.com/vote?id=40823120&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>+1</p><p>I can not understate how much I agree with parent comment.</p><p>The opposite of move fast, build a shitty prototype and iterate is a deliberate problem solving approach  undertaken by the highest caliber of engineers. The actual challenges to be addressed are effectively addressed right at the design stage.</p><p>The result is a thing of immense beauty and elegance.</p><p>I will forever be grateful for the opportunity I had to see this magnificent piece of engineering in action.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819316"><td></td></tr>
            <tr id="40822973"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40822973" href="https://news.ycombinator.com/vote?id=40822973&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Google3 codebase very consistently has <i>clean</i> code, but some of the architecture there is very much not great.</p><p>Some is great, some not so much.</p><p>Some of Verizon's code was <i>much</i> more elegant (though much smaller scope) from an API perspective, and really leaned into advanced type systems in a way Google has not.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819202"><td></td></tr>
                <tr id="40819223"><td></td></tr>
                <tr id="40819247"><td></td></tr>
                              <tr id="40819146"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819146" href="https://news.ycombinator.com/vote?id=40819146&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>For me, the most eye-opening codebase of my career was Cocotron, around 2007:</p><p><a href="https://github.com/cjwl/cocotron">https://github.com/cjwl/cocotron</a></p><p>I was looking for a way to port my native Mac Cocoa apps to Windows. I had been already disappointed by the aimless sprawl of GNUstep.</p><p>This one-person project implemented all the essential APIs for both Foundation and AppKit. Reading the code was a revelation: can it really be this simple and at the same time this effortlessly modular for cross-platform support?</p><p>I contributed a few missing classes, and successfully used Cocotron for some complex custom GUI apps that needed the dual-platform support.</p><p>Cocotron showed me that one person with a vision can build something that will rival or even outgun large teams at the big tech companies. But focus is essential. Architecture astronauts usually never get down from their high orbits to ship something.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819140"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819140" href="https://news.ycombinator.com/vote?id=40819140&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Worked on a codebase for a large safety-critical system where everything was 100% documented, and the development guide for the project was followed so closely that you couldn't tell, across millions of lines of code, that the whole thing wasn't written by one person. Absolutely impressive levels of attention to detail everywhere, down to not even being able to find typographical errors in comments or documentation (a typo in a comment was treated just as seriously as any other bug).</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819164"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819164" href="https://news.ycombinator.com/vote?id=40819164&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Let me guess, it was very well funded and there were no fake deadlines and cross-team dependencies, am I correct or am I very correct?</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40823140"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40823140" href="https://news.ycombinator.com/vote?id=40823140&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>I am almost sure this is because the system would have to pass a certification procedure somewhere, and for that they would need this level of clarity. Am I right?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40819254"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819254" href="https://news.ycombinator.com/vote?id=40819254&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Last year I worked for a client that gave me a lot of time, money and autonomy to lead dev on a critical software rewrite.</p><p>We got a small team of competent people, with domain experts to peer code with the devs.</p><p>It was wonderful. We could test, document and clean up. Having people who knew the trade and users at hand removed second guessing.</p><p>The result was so good we found bugs even in competitors' implementations.</p><p>We also got x5 in perfs compared to the system it was replacing and more features.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40823353"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40823353" href="https://news.ycombinator.com/vote?id=40823353&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Similar thing.</p><p>Had time and autonomy from a client, so took sweet time examining the domain, the existing systems et al. Spent a few months writing the basis and the framework around what will be done, based on years and years of experience I had with bad frameworks and codebases, combined with working on the same domain for their parent company years ago.</p><p>And it worked. We delivered features insanely fast, hundreds of forms were migrated, feature generators would create 90% of the boilerplate code and the code was small, readable and neatly clustered. Maintaining it was a piece of cake, leading to us not having enough work after a while so we I negotiated our time to half a week for the same money.</p><p>After a while, client deemed us too expensive to pay for only 2.5 days of work - after all, how does it make sense - if we are paying them that much, they should work 5 days!</p><p>So they cut us out. Two things happened:</p><p>1. Devs that got moved to other projects in the company told me they didn't know development could be so smooth and tried to replicate it in future projects, even tho they say it failed a lot of lessons they picked up from the framework were highly relevant in their future careers.</p><p>2. The company found a cheaper developer, he said "this is unusable and has to be rewritten" and rewrote it into "clean code", taking longer than the original project took. At least he works 5 days a week now.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819277"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819277" href="https://news.ycombinator.com/vote?id=40819277&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Good for you. Magical moments in careers are hard to find in my experience but they are so satisfying when you get there.</p><p>Glad whomever was over this didnt just drop the "dont rewrite" joel spolsky article and fight making it happen.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40822182"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40822182" href="https://news.ycombinator.com/vote?id=40822182&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Joel has since said that that he doesn't really agree with that advice anymore, at least not in the same way. Super annoying that it gets parroted over and over again as though it's the word of the lord.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819354"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40819354" href="https://news.ycombinator.com/vote?id=40819354&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I actually was the one telling them not to rewrite, lol.</p><p>But the original code was a mess of matlab spaghetti, they couldn't find a way to hire for that. Not to mention turning it into a web service was already a big hack of java parsing a raw dump of matlab datastructures that nobody dared to touch.</p><p>I had to read the matlab code, and it tooks hours to decypher a few lines. Plus the language doesn't have great debugging and error handling capabilities and the tooling is quite terrible.</p><p>So rewriting to python won, and for once, I must say it was a good call.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40819274"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819274" href="https://news.ycombinator.com/vote?id=40819274&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>One of them stands out, due to being super-productive, over years, and then decades.</p><p>A large system that was originally written by only two super-productive engineers (I mean real engineers, both with PhDs in an area of Engineering).  And a comparably capable and essential IT person.</p><p>The reasons for the super-productivity include one of the developers choosing great technology and using it really well, to build a foundation with "force multiplier" effects, and the other developer able to build out bulk with that, while understanding the application domain.</p><p>Another reason was understanding and being pretty fully in control of the code base, so that, as needs grew and changed, over years, someone could figure out how to do whatever was needed.</p><p>One of the costs was that most things had to be built from scratch.  Over time that also proved to be an advantage, because whenever they needed (put loosely) a "framework" to something it couldn't do, they effectively owned the framework, and could make dramatic changes.</p><p>When I said "costs", I mean things like, many times they needed to make a component from scratch that would be an off-the-shelf component in some other ecosystem.  So if someone looked closely at how time was sometimes spent, without really understanding it or knowing how that panned out, it would look like a cost that they could optimize away.  But if they looked at the bigger picture, they'd see a few people consistently, again and again, accomplishing what you'd think would take a lot more people to do.</p><p>It helped that the first programmer also became the director for that area of business, and made sure that smart engineering kept happening.</p><p>Someone might look for a reason this couldn't work, and think of bus factor.  What I think helped there was the fact that the work involved one of those niche languages that attract way more super programmers than there are jobs.  "Gosh, if only we had access to a secret hiring pool of super programmers who were capable of figuring out how to take up where the other person left off, and we had a way to get them to talk with us...")</p><p>It was easy to imagine a competitor with 100 developers, not able to keep up, and at many points getting stuck with a problem that none of them were able to solve.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40823146"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40823146" href="https://news.ycombinator.com/vote?id=40823146&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>I assume you avoided identifying (or even hinting at) this "great technology" on purpose, but could you persuaded to divulge what it was?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40819718"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819718" href="https://news.ycombinator.com/vote?id=40819718&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Pretty much any internal tool/TUI/CLI/library I've created. If I had to guess I'd say at most 25% of the company projects I've worked on have launched AND have consistent usage. Working hard on something just for it to wither crushes my soul but internal projects are different. They're all skunk works projects. No tickets. No project/board. No PM pushing back on how many points (read: hours) something should be. I'm solving real problems that directly impact the quality of life for myself and my coworkers. The best part is getting real, genuine, feedback. If something sucks they'll tell you and they won't sugarcoat it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819727"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819727" href="https://news.ycombinator.com/vote?id=40819727&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>I love this take.  What language(s) do you typically use to write CLI programs?  I'm also interested in learning about what types of internal TUI tools you have created.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40822493"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40822493" href="https://news.ycombinator.com/vote?id=40822493&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>The run time library for Turbo Pascal/Delphi for Windows was completely documented, sane, and very easy to work with. The working examples really helped.</p><p>The free Pascal RTL seems opaque in comparison. Their reliance on and archaic help file build system keeps contributors away. Thus it's poorly documented at best.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819043"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819043" href="https://news.ycombinator.com/vote?id=40819043&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>My favourite projects are small, with very focused goals and features.</p><p>I have a Laravel project that I have maintained for a customer for seven years.
The app is straightforward and allows users to create portals that list files and metadata, such as expiration dates and tags.</p><p>Every other year, they ask me to add a new batch of features or update the UI to reflect the business's branding.
As the app is so small, I have the opportunity to review every part of the app and refactor or completely rewrite parts I am not happy with.</p><p>It is a joy to work on and I always welcome new requests.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40823296"><td></td></tr>
            <tr id="40819262"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819262" href="https://news.ycombinator.com/vote?id=40819262&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>My past three employers code bases: mono-repos, Bazel, lots ot C++ and Python, thousands of libraries and tools, code generation and modeling tools that are fully integrated into the build, easy cross compilation, large integration tests just one bazel test invocation away, hermetic and uniform dependencies...</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819125"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819125" href="https://news.ycombinator.com/vote?id=40819125&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>The latest Go micro-service I have built.</p><p>About once a year roughly, for the last couple years, the opportunity has arisen to greenfield a Go micro-service with pretty loose deadlines.</p><p>Each time I have come into it with more knowledge about what went well and what I wasn't particularly happy with the last time. Each one has been better than the last.</p><p>I've been building software professionally for twenty years, and these micro-services have been one of the few projects in that time that have had clear unified vision and time to build with constant adjustments in the name of code quality.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819013"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819013" href="https://news.ycombinator.com/vote?id=40819013&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Although I'm only on job three and have not had that much involvement with open source, I think my current employer (Attio) has one of the best codebases I've seen.</p><p>Qualitatively, I experience this in a few ways:
* Codebase quality improves over time, even as codebase and team size rapidly increase
* Everything is easy to find. Sub-packages are well-organised. Files are easy to search for
* Scaling is now essentially solved and engineers can put 90% of their time into feature-focused work instead of load concerns</p><p>I think there are a few reasons for this:</p><p>* We have standard patterns for our common use cases
* Our hiring bar is high and everyone is expected to improve code quality over time
* Critical engineering decisions have been consistently well-made. For example, we are very happy to have chosen our current DB architecture, avoided GraphQL and used Rust for some performance-critical areas
* A TypeScript monorepo means code quality spreads across web/mobile/backend
* Doing good migrations has become a core competency. Old systems get migrated out and replaced by better, newer ones
* GCP makes infra easy
* All the standard best practices: code review, appropriate unit testing, feature flagging, ...</p><p>Of course, there are still some holes. We have one or two dark forest features that will eventually need refactoring/rebuilding; testing needs a little more work. But overall, I'm confident these things will get fixed and the trajectory is very good.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819307"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819307" href="https://news.ycombinator.com/vote?id=40819307&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>One that had a sort of improvised facade/adapter pattern (it didn't really follow either) in a clearly cut multilayered and pipelined structure, with actor model bits where it made sense.</p><p>The code wasn't simple, at all. It took active training of new arrivals for them to understand it. But it was very well thought out, with very few warts given the complexity, and extremely easy to extend (that was the main requirement, given constant changes in APIs and clients).</p><p>We had an API, with multiple concurrent versions, that transformed requests into an intermediate model, on which our business logic operated, later targetted external APIs (dozens of them, some REST, some SOAP, some under NDAs, some also with multiple versions), whose responses turned again into the intermediate model, with more business logic on our end, and a final response through our API. Each transaction got its context serialized so we could effectively have what was an, again improvised, "async/await"-like syntax in what was (trigger warning) C++03 code.</p><p>The person who engineered it didn't have formal CS background.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819022"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819022" href="https://news.ycombinator.com/vote?id=40819022&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>google3, all the devex tooling was taken care of by other teams. Tons of useful library functions available to import, accumulated over decades.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40820893"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40820893" href="https://news.ycombinator.com/vote?id=40820893&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Probably the microservices-based one for async video messaging (i.e. Slack for video) for workers in the field. Each service was small enough that we could do a blue-green deploy to prod in about 2 minutes running only the service's tests and a tiny (intentionally limited) set of about 8 system journey tests <i>(can onboard a new user, user can create a contact/group, user can send a common content-type message, user can receive messages, user can react/respond to a message)</i>. Every commit to main/master automatically either deployed to prod or broke the CI/CD pipeline and needed to be fixed ASAP. Each service was also well-known by team members that it literally could be rewritten in a week or two if desired to change a key part of its design.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40821122"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40821122" href="https://news.ycombinator.com/vote?id=40821122&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Thinking about what I wrote, I suppose my criteria for a good codebase is one that has the lowest friction to change: a fast edit/run/test/debug loop, a meaningful sense of security (without dogma), and fast automated deployment/revert (via blue/green). Given those a bad codebase can become a good one again (by uncoordinated action of individuals) without <i>everyone having to buy into</i> a large investment.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40820818"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40820818" href="https://news.ycombinator.com/vote?id=40820818&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Realistically any code base where the engineers had at least a basic understanding of programming. You do not know suffering until you've seen someone hard code basic variables, we're talking about strings all over the place, and then they just copy the function again to replace the strings .</p><p>I've legitimately left jobs over bad code. We're talking about code that did nothing in reality.
The best code bases have been ones where I've been able to lead the direction. I get to know exactly how things work. I'm privileged to have a job where I essentially created the initial framework right now .</p><p>Plus I'm fully remote, life is pretty good.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819061"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819061" href="https://news.ycombinator.com/vote?id=40819061&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Considering the biases, the one I wrote for the company I created.</p><p>When we have the opportunity to be in this context, keeping in mind what bothered us in the codebases with which we were able to work in the past, we can force ourselves not to reproduce the same errors. Like the unmaintained unit and integration tests, the lack of refactoring, other developers that use fancy technologies instead of simpler concepts more for the opportunity to play with technologies than real need..</p><p>And also, I guess, because we are more aware that the code is a reflection of the company that we want to have, that the simpler the better is a key point when we need to debug.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819304"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819304" href="https://news.ycombinator.com/vote?id=40819304&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>The best codebase is the one you fully understand. I prefer codebases that are small enough to understand within a week. This is why I like Microservices. Large codebases can be overwhelming and even senior developers working a decade in the company of might not fully understand them. Instead, I prefer maintaining a few Microservices that our team fully comprehends, where the entire codebase fits into a clear mental model. We then interact with other codebases, that have active mental models in other teams, via APIs.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819115"><td></td></tr>
            <tr id="40819057"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819057" href="https://news.ycombinator.com/vote?id=40819057&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Any codebase that I had complete control over.</p><p>No, but more seriously, I've found that familiarity with the codebase is more important than having it be perfectly engineered. Once you're really familiar with the codebase, you know where dragons be, and you can make changes more easily. And God (PM) forbid, if you ever find yourself with some extra free time you might even reduce the size of dragons over time.</p><p>This brings me to my final point. Any codebase that I really enjoyed working with was the one that was constantly evolving. I don't mean rewriting everything from scratch every few months, but as long as I have permission (and time) to refactor the things that have been bothering me for months as patterns emerge, I'm a happy bee.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819086"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819086" href="https://news.ycombinator.com/vote?id=40819086&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>&gt; Any codebase that I had complete control over.</p><p>No excuses. Code ownership is important. Sometimes it works for a team, sometimes only for individuals.</p><p>But not having to submit to core teams, architects and self-proclaimed experts of all kinds is a blessing.</p><p>I now work for an organization that discourages code ownership, and it struggles  on many fronts:</p><pre><code>        1. core teams are dysfunctional
        2. people find niches and stick to them
        3. top talent is leaving, although pay is good and business creates real value for citizens
        4. there is virtually no horizontal communication
        5. mediocre ones rise to the level of their incompetence and  infest the lives of others
        6. and so on and so forth...
</code></pre><p>
And I think the root cause of all this is lack of individual (code) ownership.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819367"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40819367" href="https://news.ycombinator.com/vote?id=40819367&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>I’ve had exactly the same issues but because I couldn’t change anything without getting approval from 16.5 code owners on every PR submitted. It’s a real pain if you start modifying your coding for ‘least code owners hit’ instead of ‘best architecture’.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819893"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40819893" href="https://news.ycombinator.com/vote?id=40819893&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>I like how my workplace does it -- there are rigorous codeowners and usually you only need approval from 1-2. if you do need approval from 5+, you can request a select 'super' codeowner review which will approve it for all.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40819644"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40819644" href="https://news.ycombinator.com/vote?id=40819644&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I think core teams being helpful or harmful really comes down to the individuals.</p><p>The problem is at this level, most orgs don't have anyone to really judge or contest competency, so they hire the salesmen rather than the doers and when they don't, they tend to cheap out and just get inexperienced people.</p><p>Logically it makes a bunch of sense, though.</p><p>Why rebuild yet another platform?  Why is your central platform bad?  Usually it's not self-service, sometimes it's because it's built in cumbersome ways, other times its because it actually enforces good standards on you rather than just giving app your apps admin.</p><p>It's difficult for the person who hires the core team to differentiate between those complaints, unless they themselves both have the technical competency and the empathy to really understand the problem.  They usually don't.</p><p>Point being, done well, it's great, but most folks can't do it well.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40821957"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40821957" href="https://news.ycombinator.com/vote?id=40821957&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>&gt; And God (PM) forbid, if you ever find yourself with some extra free time you might even reduce the size of dragons over time.</p><p>Honest question, what is the company like where you can do that? Everywhere I've worked (only been working in industry for 6 years) has had such rigid agile development that even when I do find myself with free time, there's no flexibility to work on things that haven't been assigned to you and the best I can do is work on profiling/debugging tools.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819258"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819258" href="https://news.ycombinator.com/vote?id=40819258&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I also echo the code ownership part.</p><p>I have a part-time gig where I maintain accounting software for a former client of mine. It takes up a few months' weekends a year.</p><p>I wrote about 60-70% of it when I was working for the owner of the software. It's something where as long as the client's happy, and they get new integrations and updates on time, they could keep using it for a decade longer.</p><p>I had almost complete ownership of the architecting of the software. It's broken down into a few microservices (think database, core business logic, reporting, auth, logging etc).
The best thing I did at the time was pushing to use gRPC even though management felt it was too new tech.</p><p>The UI is in Angular, pain-free periodic upgrades. I've even rewritten some perf-sensitive code in Rust, and everyone's happy with snappier calculations.</p><p>The code hygiene is relatively good.</p><p>The only downside's that if someone else were to take over the code, they'd struggle (it's one of those things where I'm wearing many hats). I've been fortunate to be a professional accountant who moved into software engineering, so everything makes sense to me.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819251"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819251" href="https://news.ycombinator.com/vote?id=40819251&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>&gt; Once you're really familiar with the codebase, you know where dragons be, and you can make changes more easily.</p><p>This is an interesting, and often overlooked, point. A month or two ago someone asked us, the Fossil SCM maintainers, if we'd be open to them refactoring the tree to something which better matches modern sensibilities (i'm paraphrasing here). Despite its many proverbial dragons, the long-time maintainers are comfortable with it and know where those dragons are and how to defeat them (or, in some cases, sneak around them), so, despite our collective geek tendencies to prefer "perfect code," we're happier with the dragons we know than those we don't. (That's not to say that fossil's code is awful, but its structure varies significantly from what one might write today if one was to start such a project from the ground up.)</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819200"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819200" href="https://news.ycombinator.com/vote?id=40819200&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>This is very much true. Initially when I joined the industry I used to work for a product from its inception. So I was aware of what section of the code affects what part of the product. Although I hadn't worked on all of those, I kept an eye for the all the changes that were coming in. I knew where I should look immediately a bug is reported even if it is not something related to my line of work.</p><p>Recently I switched teams and now I find myself taking up bugs that are only related to my line of work. Not being familiar with the codebase decreases productivity and wants you to rely on other people in the team for most of the time.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819174"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819174" href="https://news.ycombinator.com/vote?id=40819174&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>&gt; Any codebase that I had complete control over.</p><p>Anyone other than myself would instantly observe it as the the worst codebase they have ever seen.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40820078"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40820078" href="https://news.ycombinator.com/vote?id=40820078&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>For that, there is a saying: "code as if your kids will maintain it".</p><p>But I think it does not convey the right meaning. When I code something I will have to maintain for a long time, I try to make it as simple as possible for my future, older, less motivated and weary self.</p><p>The worst codebases are written by people who landed the gig a few months before and do not expect to stay around longer than a year or two.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40819097"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819097" href="https://news.ycombinator.com/vote?id=40819097&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Also nice is when an entire community has agreed to architect a codebase more or less the same. You're basically psychic when that happens.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819275"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819275" href="https://news.ycombinator.com/vote?id=40819275&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>&gt; Any codebase that I had complete control over.</p><p>This is probably one of the pillars of good codebases, or at least decoupling the bits that you don't control as well as you can (this includes external services). I remember needing to write a wrapper around another JWT library, but because it was quite important, I aimed for &gt;95% test coverage, some of the tests acted as documentation for how to use the code, there was also a nice README, there was CI configuration for pushing the build artifacts to Maven and suddenly even managing dependency updates became easy because of the test suite. Years later, it's been integrated in a bunch of services and "just works".</p><p>Come to think of it, things always get less pleasant once you add a bunch of complex dependencies and libraries/frameworks. Need to make a few RESTful Web APIs in Java? Something like Dropwizard will probably give you fewer headaches than Spring (or Spring Boot) and all of its inherent complexity, in the case of the former you might even need to do configuration in XML and that has honestly never been pleasant. If you need to integrate with a bunch of other stuff and want something opinionated, going for Spring Boot will make sense then, but for simple use cases it's overkill. Same for ASP.NET, Ruby on Rails, Laravel and many others, while they might be easier to use, updates (especially across major versions) and breaking changes will give you a headache and just add a bunch of churn to the project.</p><p>Similarly, if you need a message queue, externalizing that into RabbitMQ might make a lot of sense, same with storing files in an S3 compatible store, using something like Redis or Valkey for key-value storage, as well as not trying to shove too much logic into your RDBMS. Just pick whatever tool feels best for the given task at hand, instead of shaping things into what they're not (using the database for blob storage, for example), unless you have a whole bunch of constraints to contend with. Otherwise, sometimes you just get the worst of both worlds, like needing to use Oracle for a project, not having easily launchable local environments (because Oracle XE doesn't support some features), having to share DB instances with others during development and also running into weird obscure issues like DATABASE LINK queries taking 100 times longer in some cases, even when executing the same SELECT query on the remote DB works without issues.</p><p>To not go into a rant, I'd sum it up like this: be in control, isolate the things that you cannot control, pick the correct technologies, do the simplest thing that you can get away with without overengineering and think about the person who'll have to maintain, debug and grow the system in the following months and years (which might also be yourself, sans knowledge about what the code did, if you don't make it explain itself or don't comment the non-trivial stuff).</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40819736"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819736" href="https://news.ycombinator.com/vote?id=40819736&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>The ones that were straightforward and close to the business. It starts at the obvious, works in an obvious way and has comment blocks at hard parts.</p><p>For this reason I despise most modern [web] projects, which have a weak start, immediately drop into “services” and “components”, do one action per source file per 30-50 lines of code, which are mostly imports and boilerplate, and have hundreds of these files. You can never tell what it does because it does almost nothing except formalities.</p><p>I also noted a tendency to use wrong paradigms for a language. E.g. it may have no closures (imagine that in 202x) so they use events as continuations for asynchronicity, which results in a mess. Or it isn’t immutable/functional, but they pretend it is, which results in fragility.</p><p>The best projects are both close to their business and written in a paradigm of the language used.</p><p><i>Was there someone enforcing good practices top down?</i></p><p>Natural time pressure is the best bs cleaner, imo. You write effing code, maybe have few hours a week to refactor strange parts. With no time pressure a project naturally becomes massaged by all members into the “likeable” form of their age.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819135"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819135" href="https://news.ycombinator.com/vote?id=40819135&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>My open source projects. One in particular I've been working on for about 10 years. The code is consistent and always getting better, even though there is a lot more work that could be done.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819550"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819550" href="https://news.ycombinator.com/vote?id=40819550&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I was debugging some issues with Thanos and had pretty good success tweaking the codebase to add additional telemetry.</p><p>The code was fairly well organized and more importantly worked out of the box with a Makefile and IDE integration (GoLand). All it took was `git clone` and opening GoLand to get started.</p><p>For C (maybe it's C++), fluentbit seemed pretty straight forward (I don't have much experience in C though)</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819194"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819194" href="https://news.ycombinator.com/vote?id=40819194&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I haven’t contributed code to it, but I’ve read lots of code in it ( for work reasons ) - I really like the golang compiler and library codebase.</p><p>About codebases I’ve written code for, the best one strived for simplicity, and was driven by very strong engineers who actively considered code hygiene ( in the broadest possible sense ) a first class citizen.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40820282"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40820282" href="https://news.ycombinator.com/vote?id=40820282&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>I can't name a good one, but i have strong
 dislike for big project codebases where
 even finding one file out of thousands where
the relevant code resides is a challenge:
its never something isolated but acts 
like some "component".
The best i can think of is one-person projects
where organization is streamlined as its
actually has to be used by the author,
not like a cog in a giant project.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819305"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819305" href="https://news.ycombinator.com/vote?id=40819305&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I don't think the 'code' side of code base can be considered in isolation.</p><p>What makes a <i>project</i> objectively good (from subjective experience) is a combination of code, design, documentation, and often the humans involved.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819110"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819110" href="https://news.ycombinator.com/vote?id=40819110&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Any code base that doesn't use the advanced features of it's language(s) are always better in my experience. Heavy usage of e.g. meta-programming in Python or perhaps uber's fx (dependency injection) in Go makes projects infinitely harder to get into.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819409"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819409" href="https://news.ycombinator.com/vote?id=40819409&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I worked at GOV.UK for a few years on what was effectively specialised CMSs all written in Rails. Mostly basic CRUD stuff. A contractor came along and built the most insane CMS I've ever seen. I found out later it was flavor of the Java Repository Pattern using dependency injection. It became so difficult to work with that the last thing I worked on there was to delete it and rebuild it using standard Rails patterns.</p><p>The KISS philosophy exists for a reason and that includes over-using advanced language features just to show off.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819510"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40819510" href="https://news.ycombinator.com/vote?id=40819510&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Besides just KISS, a lot of messes I've seen have been implementing patterns outside the framework or implementing complex patterns that didn't add value.</p><p>Besides KISS (or maybe as an extensive), try to keep framework-based codebases as close to the official documented setup as possible. You automatically get to s of free, high-quality documentation available on the Internet.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40822224"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40822224" href="https://news.ycombinator.com/vote?id=40822224&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Props for gov.uk! I’ve looked at its documentation and design system and see both as peak user experience and clarity.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40822758"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40822758" href="https://news.ycombinator.com/vote?id=40822758&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I think one should not use advanced language features just because, but I also think one should not avoid using advanced language features where it is useful.</p><p>Why would the code base be worse when advanced language features are used?</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819218"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819218" href="https://news.ycombinator.com/vote?id=40819218&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Depends who is providing it.</p><p>Django and pydantic meta programming usually make the code easier to deal with.</p><p>In shop written meta programming usually sucks.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819168"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819168" href="https://news.ycombinator.com/vote?id=40819168&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Such a great point. I audibly groan when I come across Python meta-programming.</p><p>While not an advanced feature, I have a similar response when I see lots of decorators in Python. They quickly become a debugging nightmare.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819211"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819211" href="https://news.ycombinator.com/vote?id=40819211&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I can see a few cases where that depends...</p><p>Really simple languages: Ruling out meta-programming is really going to limit you in Lua for example. Just being able to do `mySocket:close()` instead of `Socket.close(mySocket)` involves meta-programming.</p><p>Older languages: For C++ the "simple" features are going to include raw pointers and macros. Maybe it's not so bad to allow smart pointers and templates to avoid those</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819783"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40819783" href="https://news.ycombinator.com/vote?id=40819783&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Both of these examples are examples of an under-programmed core though. Lua is notorious for lacking batteries, so everyone has to reinvent their own. There’s literally no serious Lua program without some sort of classes, but they still resist adding them into lauxlib.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40819457"><td></td></tr>
                  <tr id="40819080"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819080" href="https://news.ycombinator.com/vote?id=40819080&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Generally code that hasn't been tested commercially. Unencumbered by pesky client driven features, just code for dreamt up features that are fun to code but perhaps will never be used.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819010"><td></td></tr>
                <tr id="40819324"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819324" href="https://news.ycombinator.com/vote?id=40819324&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>So true!</p><p>I spend so much time obsessing over how what I am about to write ties in with what has already been written; or fuming over the stupidity of earlier decisions (usually made by myself). A blank slate is incredibly refreshing.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819098"><td></td></tr>
                <tr id="40819327"><td></td></tr>
                        <tr id="40819453"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819453" href="https://news.ycombinator.com/vote?id=40819453&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Ruby on Rails.</p><p>It is the only framework I have read top to bottom.</p><p>Also the FreeBSD kernel, if you want to see a C code base that's quite beautiful (for C).</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819569"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819569" href="https://news.ycombinator.com/vote?id=40819569&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>In this category I would nominate Django as well. It's very well designed (opinionated, but usually for good reasons).</p><p>In terms of large C code bases I enjoy reading the PostgreSQL source code.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40820203"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40820203" href="https://news.ycombinator.com/vote?id=40820203&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>whenever I'm stuck on how to structure some code, I ask myself how would Laravel do it? and look up their code and structure mine similarly</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40819297"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819297" href="https://news.ycombinator.com/vote?id=40819297&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>No name because the project code name changes with each new contract, but the first-level ground processing system for the NRO that turns raw satellite downlinks into some kind of human-intelligible files.</p><p>I think a lot was just the restrictions built into the way development happens required a high level of discipline, care, and planning. Also, requirements were pretty tightly coupled to sensor platform capabilities, which are known well in advance and don't change unexpectedly, so waterfall development actually works and you don't have to deal with the chaos of not really knowing what will and won't work and customers constantly changing their minds.</p><p>Code base was overwhelmingly C++, some Fortran, a lot of it was very old. It was all developed on airgapped networks, and the difficulty of bringing in external dependencies meant there largely were not any. All of the library functionality we required was mostly developed in-house, so we had extremely well-documented and stable functions available to do damn near anything you could want, with a good chance that whoever first wrote it was still there. All development had always been tied to a ticketing system of some sort that included all of the original discussion, design documents, and that kind of thing might add process overhead upfront, but it means that forever new developers can simply read the history and learn exactly why everything works the way it works.</p><p>The system itself was very Unixy. In production, it was meant to be run as a server with many instances striped across high-performance compute nodes, but it did not have to be run that way. Every individual product flow could also be built as its own transient executable, so that working on a single component could easily be done locally. You didn't have to rebuild the world or spin up your own server. Performance requirements were enough that we had our own customized networking stack and filesystem in production, but nothing depended on this for function, so you could still develop and test on plain Linux with ext4.</p><p>The culture was terrific. We were part of one of the big five defense contractors, but an acquisition and this program was largely still staffed by employees of the original company that had been acquired. We were de facto exempted from most of the bullshit any other program had to deal with. I don't know if that was part of the original terms of being acquired or just a consequence of having so many long-time developers that couldn't afford to be lost if you subjected them to nonsense. This was the kind of project that people intentionally stayed on and retired from because the experience was so much better than any other project you could get assigned to.</p><p>Ironically, it had none of the characteristics that high-performing companies often tout. You work in private. The rest of the company, including your own management chain, doesn't even know what you're working on. You'll never get any recognition or publicity. The pay is mediocre. We weren't attracting the best and brightest from all of the world. You had to be American, have a top-secret clearance, and be geographically close enough to the facility to get there every day, so this was a pretty constrained hiring pool. I still worked with some of the smartest people and best engineers I've ever known. The upside of this kind of environment is you have no mercenaries or publicity hounds. Everyone who sticks around is a person who really loves and cares about what they're working on, and a lot of people did stick around. The sanity and organization of the code was heavily facilitated by having a whole lot of people working on it who'd been working on it for 30+ years.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819180"><td></td></tr>
            <tr id="40819267"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819267" href="https://news.ycombinator.com/vote?id=40819267&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>Laravel, once you get the hang of it everything just works, and using a debug bar to optimize database calls is very satisfying.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40821046"><td></td></tr>
            <tr id="40819100"><td></td></tr>
            <tr id="40819131"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819131" href="https://news.ycombinator.com/vote?id=40819131&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Every code base I have ever worked on was a legacy nightmare. Every "greenfield" project I joined turned into a legacy nightmare within weeks. I have never encountered enjoyable code. I had the displeasure of wading through Spring, Hibernate and Apache HTTP client code before and they were all an incomprehensible mess.</p><p>My conclusion: You know the claim "any medication that really has an effect must also have side effects". I would like to adapt that for code: Any code that does a lot of useful and complex things must be an arcane, barely maintainable mess that can only be understood by deep study.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819485"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819485" href="https://news.ycombinator.com/vote?id=40819485&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Something in your answer triggered a flash back to when I worked for a phone company, 20 years ago. One team, under the leadership of our questionable chief architect, had produced our new, all encompassing backend system. This was to be the corner stone of all future development and integration, dog slow and complicated as it where. I worked as a .Net developer and had the misfortune to be among the first to integrate with this monster. Try as I might, I could not get .Net to interoperate with these services. Finally I figured out that the SOAP services was using some old deprecated versions. Going back to the architect, I ask "did you build this on Apache Axis, and please say Axis2", but no, it was just Axis, a deprecated version, that would generate webservices not supported by newer .Net version. That wasn't an problem, because: "None of our project have upgraded to those .Net versions yet"... DUDE, we've launched a brand new system based on that .Net version a year ago, and what was you f-ing plan for the future, to redo every single service using Axis2?</p><p>This guy had based a brand new system on a framework/library that was no longer maintained, even before the system was launched.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819334"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819334" href="https://news.ycombinator.com/vote?id=40819334&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>IMHO library code is especially challenging, as cruft has a tendency to accumulate, historical behavior needs to be preserved, and APIs are set in stone once they’re built.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40819418"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40819418" href="https://news.ycombinator.com/vote?id=40819418&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>"Any code that does a lot of useful and complex things must be an arcane, barely maintainable mess that can only be understood by deep study."</p><p>"Every code base I have ever worked on was a legacy nightmare. Every "greenfield" project I joined turned into a legacy nightmare within weeks"</p><p>I am sorry to say this, but it really sounds like you were either really, really unlucky, or part of the reason why it became a mess.</p><p>Complicated things are complicated. Nothing can ever change that and it requires study to understand it.</p><p>But it still does matter a lot, how one organizes the whole thing. How it is structured, documented(!), refactored. Are there competent people in charge who understand it all and kick peoples asses if they make even a temporary mess or forget to document, or do random people make changes wherever they see fit, because a deadline is ticking?</p><p>Modularisation is usually the key. Small modules do one thing and are as seperated as much as possible with as little side effects as possible.</p><p>And if one has to ship things, it is not always possible to keep it pure and if the code is not intended to live forever then this is often fine. But if the codebase is supposed to stay, then there needs to be the time to clean up the hacks. Or don't and then you end up scared touching anything.</p><p>That being said, the technologies you mentioned, I would not like to touch either..</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819494"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40819494" href="https://news.ycombinator.com/vote?id=40819494&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I think that's a bit cruel. Usually mess happens because lots of people with different ideas about the future needs and best structure meet up in the code - and it's hard to develop a consistent culture.</p><p>Also codebases get too large for any one person to refactor them into shape in the time they have each day. So you end up needing people who are responsible just for keeping things in shape.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819611"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40819611" href="https://news.ycombinator.com/vote?id=40819611&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>Well, that is why I said with those words it <i>sounds</i> like this to me, not that he or she <i>was</i> in fact responsible. (at least this is what I meant)</p><p>"Also codebases get too large for any one person to refactor them into shape in the time they have each day."</p><p>Which is why modules, or subprojects were invented. Or however you want to call it, if one person is only responsible for a small part and not everyone for everything. And yes, there is also the non trivial problem of time management.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40819704"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_40819704" href="https://news.ycombinator.com/vote?id=40819704&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>yup, but the style of the modules starts to diverge as different people maintain them.</p><p>IMO one does want people whose fulltime job is to looking at a codebase  orthogonally to those who are just implementing a feature. People who make sure it builds fast, is secure tries to be consistent to some degree etc.</p><p>Many companies call teams/indivuduals like this a "cost centre" and disparage it because they are dolts.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40823366"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_40823366" href="https://news.ycombinator.com/vote?id=40823366&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>If those code checkers are experienced programmers, who do not annoy people with arbitary guidelines and standards, then yes, this might also work. But I do not think they are always necessary as a seperate full time role, if everything works normal otherwise. And with working normal, I mean there is enough time to refactor and document and clean up and is actually done.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                                          <tr id="40819108"><td></td></tr>
            <tr id="40819477"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819477" href="https://news.ycombinator.com/vote?id=40819477&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>I just cannot face the thought of calling any of them "best".</p><p>Every one had good and bad features though. One or two were OS-sized and I think a codebase that compiles and links to 85GB of output for 20+ devices without being a total disaster inside is harder to do than a neat small python module or whatever.</p><p>GOOD FEATURES:</p><p>Maintenance of the build and test: 
I worked on tools that helped builds go faster so I saw a lot of codebases where people were not maintaining the build partly because nobody had that a s a responsibility. There was bad management of dependencies leading to build failures, poor performance, incorrect build output.  Android would be a counter example to that - I don't know if people like developing in it but it was always hard to accelerate it as the maintainers fixed performance problems regularly leaving our tools with little to improve.</p><p>Using appropriate languages.  Writing everything in C++ was a fad at one time. All projects work better, port better, have faster build times, are easier to test etc if they use memory safe "build once" languages to a maximum (e.g. java) and unsafe ones (e.g. C/C++ which have to be rebuilt and tested for each device/os) to a minimum. IMO Android beat Symbian amongst other reasons because it wasn't all C/C++ and that meant a lot of code didn't have to be rebuilt for different devices.  This made builds faster and fast builds lead to better quality because of a short dev-test cycle.</p><p>Use of faster compilers over "better" compilers. Ultimate code performance and quality depends on a fast development cycle more (IMO) than on having the most optimizing compiler. GCC versus the older ARM compilers for example.  Now the ARM compiler is based on LLVM and I know that happened indirectly from a suggestion I made to someone who then made it to ARM who then did it.</p><p>The setup and build of one codebase I worked on was as easy as one could expect, the build errored out if you tried to use the wrong tools so you never ended up debugging weird failures because of an incorrect tool in your PATH somewhere. I made this feature happen :-D. With big codebases the tools could be included in the version control system so you knew you had the right compiler, right object dumper etc.  This is another strength of Android and yet I was in a project for Symbian to do the opposite because of some utter bonehead who never touched a build in his life who was trying to make a name for himself with his slimy bosses as a "doer" and "reformer."</p><p>Codebases (especially big ones) benefit a lot from some kind of codesearch/index where you could find out where some function/class/variable was defined and what version of the source base it was introduced in.</p><p>BAD FEATURES:</p><p>Exclusively Owned code - we need to know who understands code best and who is best to review it but I don't think anyone should have totally exclusive control. It was a nightmare for me at one job - trying to get another team to make some needed change (like fixing their stupid makefiles to work properly in parallel).  We (build team) should have been able to do it ourselves - maybe including them in the PR.  Sometimes ownership is entirely theoretical - nobody who wrote it is still employed and nobody among the notional owners understands it and none of them want to approach it within 100 metres in case it blows up and becomes their problem.  I simply <i>had</i> to approach such code - no choice - but I kept having to send diffs to people who didn't want to bother to look at them. It was a case of pushing wet spaghetti and took forever to do very simple changes.</p><p>Insufficient tests that run infrequently. What else is there to say?</p><p>Complicated code with no "why" or "what this is for" type comments.   The kind of thing you trawl around in for weeks and cannot make head nor tail of what is going on overall.</p><p>Code with so much dependency injection and general SOLID that you have to bounce all over the place to understand a very simple action.</p><p>Code where writing tests is an enormous ballache.  In one Go codebase the reason was because somone decided that the standard Go practise of an array of test data being run through a kind of "test engine" was the only way anyone should be allowed to write tests.  Hence you had to do lots of weird things to make your test cases into data.  Generally we use a kind of "religious" approach to try to get consistency out of a group of people but then take it much too far.</p><p>codebases without automated reformatting - so everyone wastes time arguing about line spacing or camel-case names or whatever in their PRs.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40820509"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40820509" href="https://news.ycombinator.com/vote?id=40820509&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div><p>&gt; Code with so much dependency injection and general SOLID that you have to bounce all over the place to understand a very simple action.</p><p>I find that happens when people get religious about patterns and methodology; without understanding the "why", the language, and how a computer works.</p><p>Case in point: I once worked on a C# project that used a port of Spring for dependency injection: Ultimately, it was near impossible to know when something was constructed, and what was calling what. There were classes that couldn't call themselves through "this" because of certain weird dependency injection features used.</p><p>Later, I decided to use dependency injection as a design pattern: Instead of a complicated DI framework, there was just a few files of code. It was very easy for newcomers to understand. It was also easy to swap in mock objects, and easy to swap dependencies based on the target platform. It was also easy to see when a dependency was constructed; because it wasn't hidden behind a giant framework.</p></div></td></tr>
        </tbody></table></td></tr>
                            <tr id="40819191"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40819191" href="https://news.ycombinator.com/vote?id=40819191&amp;how=up&amp;goto=item%3Fid%3D40818809"></a></center>    </td><td><br><div>
                  <p>I worked with a very experienced engineer who created his own WAF (software-based) using Java.
From a purely architectural perspective, it might not be the best (I might have used an ISAPI filter back then), but the code itself was very efficient, well-written and documented. I used it several times as a teaching example.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Frame.work laptop now available in Denmark, Finland, and Sweden (206 pts)]]></title>
            <link>https://community.frame.work/t/now-available-in-denmark-finland-and-sweden/53690</link>
            <guid>40818422</guid>
            <pubDate>Fri, 28 Jun 2024 07:09:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://community.frame.work/t/now-available-in-denmark-finland-and-sweden/53690">https://community.frame.work/t/now-available-in-denmark-finland-and-sweden/53690</a>, See on <a href="https://news.ycombinator.com/item?id=40818422">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
<meta itemprop="headline" content="Now available in Denmark, Finland, and Sweden!">

<meta itemprop="datePublished" content="2024-06-27T14:26:19Z">
<meta itemprop="articleSection" content="Framework Team">
<meta itemprop="keywords" content="">

<div itemprop="text" id="post_1">
<p><img src="https://static-community.frame.work/original/3X/b/a/baac1e3b5fae1ef1b029a20497386120161a80a8.gif" alt="unnamed (1)" data-base62-sha1="qDnzodVI39tnb30sRjG8gUkxJA4" width="686" height="500"></p>
<p>Our products are now available for shipment to Denmark, Finland, and Sweden! That includes the in-stock <a href="https://u15405850.ct.sendgrid.net/ls/click?upn=u001.TVrR6I1vPj77I3R6mcli0RGI2wiBOolLjDWLvH8gf0pb07X1MoweDuqqdczMfmkZBwh5YsUbqzL4MyIrfSzf0w0lccbfHugn45qTPcDmggZrgKtY3e67-2Be1k8w-2FQZxKjO-2BRsfgr0AYGt1LGQmEjd2IIWZecVAYT1FSmmtbZBbDA2y0Zqx6KWrHYUWgOZJXlubmPgeJkwZW7g6eEDYpsF3vyItB8RxGE8CBAlUxNzoI7ZJNO2afsAaunuq0GUi3OzeICfIg2A9o29Qox-2B31IkPUbWS7n6CQcYYuaQd3p4lWzXsQIsClZ51cKpH-2FqkXyi0wGneBXQEgM1OyH8plXfwMA-3D-3DN-Ue_LI0JivCtpL2LeDHSKL-2Bm40FXeyVeHu6yfOdZGNMQhuDMKKii8bUF9QrH-2FTJ82JsNujpMR9orSD-2FZSqUnHxuzWz3BJwvIw9GdPS5e12oLTiGM-2B4vN0iJcgha5DU3bAQ6T51Fup5yWN0v5vu1KBHDJ1zaNUs10WmIwzBXndu53YN2zZGubASykGhMQ-2FBFdKBQrTQsxONgO1uubBj0fyIAMLT8yT7MAhe02nstC8P-2BtnUtqu8Nn7KsBtrLvEvnUAnmtRzL7AITJ7OmNtDMdc8xkZR2m7q0kxTDJhEo6kroA9YC12qv9jzmF6OL7hlivlqoAVwkHHX0XNTmJDJCkou2Yng-3D-3D">Framework Laptop 13</a> and <a href="https://u15405850.ct.sendgrid.net/ls/click?upn=u001.TVrR6I1vPj77I3R6mcli0RGI2wiBOolLjDWLvH8gf0q760Nn7xLf2EymdsshjKWqx50VZKkmVi9qbiefFntOrt2739EfVef2lNgVAy7hODLzSf8CAT8ZusIFcaeyZw-2BMqnj-2Fo66KIxMKA9JqPOu94sXz6NWmx3zGYsG0FDOJgBWoV6prdnIeu5pZRsdFhMi1H814VsOG270M7xr0kFyjZ5TsrGlqxJm-2B1HGK6CE-2Bfkr-2BSNKIKnmCGUBOFoOedaA04wJEKpglvTl0gL2o4PWJUMKI-2BPP8sNYIU7fTWSLObhwtSIbaYf9TjXNAvso0Wh6ojYay11HLZCKn8WPEnJgcpw-3D-3DPdop_LI0JivCtpL2LeDHSKL-2Bm40FXeyVeHu6yfOdZGNMQhuDMKKii8bUF9QrH-2FTJ82JsNujpMR9orSD-2FZSqUnHxuzWz3BJwvIw9GdPS5e12oLTiGM-2B4vN0iJcgha5DU3bAQ6T51Fup5yWN0v5vu1KBHDJ1zaNUs10WmIwzBXndu53YN0JFpnnNc1jcpCtvAj2g1oIrUZhtb3NXRAihb3ieJuIbi76yFJITR2sjEcu26XucGu0r-2Bv67rfIfmuPwmyRnA2W0s9wPLiHrOzAopRexRoySrtu6WADBnTQ9metr84BIpF-2Bg3-2BjCIGmaj-2BAskJabP6qTST3GPLhSK6Hib0hA3xWig-3D-3D">Framework Laptop 16</a>, pre-orders for the new <a href="https://u15405850.ct.sendgrid.net/ls/click?upn=u001.TVrR6I1vPj77I3R6mcli0RGI2wiBOolLjDWLvH8gf0pb07X1MoweDuqqdczMfmkZWBXR3yXuompK-2FgRQ1cHFLt2oUuta82PDPTsbq8CttOmaIllOJkQJvRTzlmu9oWkJpCE87Fs8WxErnMr9qG25RbQSz6-2FEMo0bGHixnPw2Wjdb0Ccpev7WjBYUZ-2FfIk-2BBkgy9WlTxYq2aqL76Wdkul-2F6meXw1IIRWQfp-2FJAWMeSZMvROd9Y0jBP8-2B1lhZPM2AYnC-2FnZl6tQddjLR-2BgmmNkIr3CfQlIXkq0lxb-2B-2BnIB-2FrnWjDnWLzS7ETGAy6pwxAY9h-2FOPibXOB3CFEyAyVpbdIu65J2UDlkl3bX5e-2F29ogIM-3D3NHf_LI0JivCtpL2LeDHSKL-2Bm40FXeyVeHu6yfOdZGNMQhuDMKKii8bUF9QrH-2FTJ82JsNujpMR9orSD-2FZSqUnHxuzWz3BJwvIw9GdPS5e12oLTiGM-2B4vN0iJcgha5DU3bAQ6T51Fup5yWN0v5vu1KBHDJ1zaNUs10WmIwzBXndu53YN1-2FAFWs20rLj1FZJBNA-2BOfIWAbsY4oVCQcwb9luQ4tHE-2F0hezA-2B2wXLq-2BXuxAXzxIJLcqX-2Byr3Wro6jvcOVOjqXWSTTBsxg0Z8AvtfqdzH6n7oS-2Frq8M10FglwCTe7kMehafFumCkhRmGOT3WIIQigh7A6MXUQqrI4Hd4Fy7MCP7w-3D-3D">Framework Laptop 13 with Intel Core Ultra Series 1 processors</a>, as well as the modules in the <a href="https://u15405850.ct.sendgrid.net/ls/click?upn=u001.TVrR6I1vPj77I3R6mcli0YTF1XZmNP1buE3hFacQHq1DVSN31yfaINgicg2SWe0JmWj-2Fgwg-2B3nrwspRenX7YD5mHP6oFl1s5nS9mlOQiydlkaO2SzE5HuDGw31zMInqDZWMJhvSNfFxoPzoaPK6LUgEQCsK4cKvfzI77Z9ByE7csHYKKJcIa-2Bd4F4dM8FQdGT71b2jK8tpTErAPK7fun-2Bf9LpnV-2BvuFunECfT1OZ75pkYaMlntyPy1P71AetOg1qlKZP4e-2BFLCTVvpwk0FH1oxPmh-2BdyCReatpjUH-2BOwuNRbcXgFChxfjl-2BsbkLmeOqgK9mA_LI0JivCtpL2LeDHSKL-2Bm40FXeyVeHu6yfOdZGNMQhuDMKKii8bUF9QrH-2FTJ82JsNujpMR9orSD-2FZSqUnHxuzWz3BJwvIw9GdPS5e12oLTiGM-2B4vN0iJcgha5DU3bAQ6T51Fup5yWN0v5vu1KBHDJ1zaNUs10WmIwzBXndu53YN0vNNfknVN2gV5cngc6JwGnVd7p207EVetZiaK6SBIiVcAODwiP59hfQLS1AbearNloREr1wYHXIXCBuMxsy-2BkgDFyU6ltMRvw9u5RqOF4lcqY9QqhUdLg0OEQ3YZbcOn6MtisFk4mFViZp1nhPJXGTZoGFnmPFB2NGinzPEr00fw-3D-3D">Framework Marketplace</a>.</p>
<p>With this launch, we’re adding Danish and Swedish/Finnish keyboards that you can order now with a Framework Laptop 13 or as a standalone Keyboard or Input Cover Kit. For Framework Laptop 16, these keyboards will be available in late July this year. You can <a href="https://u15405850.ct.sendgrid.net/ls/click?upn=u001.TVrR6I1vPj77I3R6mcli0dN1nOnZEAhsT8LywqPPWBxni94AQW9IPESaYEr6OuPSXcwtBGJI19EWOIUZgNE3JG56M-2Fj4P-2BQGle4SyJ1Hv9ovhLera16Y0BPBd53NCG8IDtyHN7BM8T4CbgDh1v0IEVnqaHPtLqII5Q1j7eYbkClO7QmIwJ0aWIdLSCqcr6d6KU3iA9Z-2FBienSJ1nadmbGnT2KdKGDmDebej7r0pfp5efkJpZwnqPbuO8BY3c42Q0zClLh8fE3lOhoW1mJoGkDgtUlTo1mvMHNaU3z25Ls6c-3DGjXI_LI0JivCtpL2LeDHSKL-2Bm40FXeyVeHu6yfOdZGNMQhuDMKKii8bUF9QrH-2FTJ82JsNujpMR9orSD-2FZSqUnHxuzWz3BJwvIw9GdPS5e12oLTiGM-2B4vN0iJcgha5DU3bAQ6T51Fup5yWN0v5vu1KBHDJ1zaNUs10WmIwzBXndu53YN0uCh4rBKz3F6cU4zflKEfXaIAeVeaoQgMQk7v8g-2BPTUZ6uWjXbxp7tcUGVQNgH82RNTMu72wEMR63ULISIYG700TtlgoetQo41RVfKeC8B6OD4evZ2bHMyQU201DM2SvK7SwGk4rdpjnQtrvR8Z3BlRxZDdUrbbi6VdkA08AP-2B8A-3D-3D">sign up for an email alert</a> on the product page and we’ll notify you when they are in stock.</p>
</div>
<div id="post_2" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
<p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
<a itemprop="url" href="https://community.frame.work/u/inffy"><span itemprop="name">inffy</span></a>
</span>
<span>
<time itemprop="datePublished" datetime="2024-06-27T14:53:18Z">
June 27, 2024, 2:53pm
</time>
<meta itemprop="dateModified" content="2024-06-27T14:53:18Z">
<span itemprop="position">2</span>
</span>
</p>
<p>Niice, preordered AMD with the new screen</p>


</div>

<div itemprop="comment" id="post_4" itemscope="" itemtype="http://schema.org/Comment">
<p>Amazing! I immediately went to order a Swedish keyboard.</p>
<p>I notice there’s a lot less available to order on the Swedish store than, say, the German one. I was hoping to order a WiFi antenna unit for my standalone mainboard and a couple of other parts. Are those parts expected to become available in Sweden soon?</p>
<p>Edit: I refreshed the store and a few more items are available. Looks like I just need to be patient <img src="https://community.frame.work/images/emoji/apple/stuck_out_tongue.png?v=12" title=":stuck_out_tongue:" alt=":stuck_out_tongue:" loading="lazy" width="20" height="20"></p>
</div>
<div id="post_5" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">

<p>I now have the option to buy the proper Swedish keyboard or keep my blank one so I keep getting to see the look on peoples faces when they mean in to use my laptop. Hard choice.</p>


</div>
<div itemprop="comment" id="post_6" itemscope="" itemtype="http://schema.org/Comment">
<p>I’m in exactly the same situation!</p>
<p>Weirdly enough, on the Swedish store I can only order the Swedish keyboard for my Framework 13 as part of an entire new assembly with a trackpad and etc. If I switch to the German store, I can order the Swedish keyboard on its own.</p>
</div>
<div id="post_8" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">

<p>the options should be the same for both regions, can you please check again?</p>


</div>
<div id="post_9" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">

<p>I can’t see any loose F13 keyboards on the Swedish store at all. 16 inch is there and input covers but no loose keyboards.</p>


</div>
<div id="post_10" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
<p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
<a itemprop="url" href="https://community.frame.work/u/iKenndac"><span itemprop="name">iKenndac</span></a>
</span>
<span>
<time itemprop="datePublished" datetime="2024-06-27T15:45:15Z">
June 27, 2024, 3:45pm
</time>
<meta itemprop="dateModified" content="2024-06-27T15:45:15Z">
<span itemprop="position">10</span>
</span>
</p>
<p>I just checked again — the Framework Marketplace landing page shows 230 total products when the website is set to Germany, and 56 total products when the website is set to Sweden.</p>


</div>
<div id="post_11" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">

<p>It appears to have been fixed now. I can see all products while having the site det to Sweden.</p>


</div>
<div id="post_12" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
<p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
<a itemprop="url" href="https://community.frame.work/u/junaruga"><span itemprop="name">junaruga</span></a>
</span>
<span>
<time itemprop="datePublished" datetime="2024-06-27T16:32:12Z">
June 27, 2024, 4:32pm
</time>
<meta itemprop="dateModified" content="2024-06-27T16:32:12Z">
<span itemprop="position">12</span>
</span>
</p>
<div itemprop="text">
<p><a href="https://community.frame.work/u/destroya">@Destroya</a> Congratulations!</p>
<p>I hope Framework will update the following knowledge base article too.</p>

</div>


</div>
<div id="post_13" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
<p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
<a itemprop="url" href="https://community.frame.work/u/2-4601"><span itemprop="name">2-4601</span></a>
</span>
<span>
<time itemprop="datePublished" datetime="2024-06-27T16:40:56Z">
June 27, 2024, 4:40pm
</time>
<meta itemprop="dateModified" content="2024-06-27T16:40:56Z">
<span itemprop="position">13</span>
</span>
</p>
<p>Been waiting for this day a long time. Unfortunately my order did not go through. Card(s) kept getting declined. Already contacted support. Just a bit frustrated at this last minute obstacle… <img src="https://community.frame.work/images/emoji/apple/sweat_smile.png?v=12" title=":sweat_smile:" alt=":sweat_smile:" loading="lazy" width="20" height="20"></p>


</div>
<div id="post_14" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
<p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
<a itemprop="url" href="https://community.frame.work/u/Destroya"><span itemprop="name">Destroya</span></a>
</span>
<span>
<time itemprop="datePublished" datetime="2024-06-27T16:53:05Z">
June 27, 2024, 4:53pm
</time>
<meta itemprop="dateModified" content="2024-06-27T16:53:05Z">
<span itemprop="position">14</span>
</span>
</p>
<p>Sorry to hear that, they should check your help request soon and help you!</p>


</div>
<div id="post_15" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
<p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
<a itemprop="url" href="https://community.frame.work/u/Destroya"><span itemprop="name">Destroya</span></a>
</span>
<span>
<time itemprop="datePublished" datetime="2024-06-27T16:53:34Z">
June 27, 2024, 4:53pm
</time>
<meta itemprop="dateModified" content="2024-06-27T16:53:34Z">
<span itemprop="position">15</span>
</span>
</p>
<p>Thanks! We should be updating it today!</p>


</div>
<div id="post_16" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
<p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
<a itemprop="url" href="https://community.frame.work/u/Destroya"><span itemprop="name">Destroya</span></a>
</span>
<span>
<time itemprop="datePublished" datetime="2024-06-27T16:55:25Z">
June 27, 2024, 4:55pm
</time>
<meta itemprop="dateModified" content="2024-06-27T16:55:25Z">
<span itemprop="position">16</span>
</span>
</p>
<p>I see 230 for Sweden now, I guess we needed a bit of time <img src="https://community.frame.work/images/emoji/apple/wink.png?v=12" title=":wink:" alt=":wink:" loading="lazy" width="20" height="20"></p>


</div>
<div id="post_17" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
<p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
<a itemprop="url" href="https://community.frame.work/u/inffy"><span itemprop="name">inffy</span></a>
</span>
<span>
<time itemprop="datePublished" datetime="2024-06-27T16:59:32Z">
June 27, 2024, 4:59pm
</time>
<meta itemprop="dateModified" content="2024-06-27T16:59:32Z">
<span itemprop="position">17</span>
</span>
</p>
<p>How will the order process go? I usually use a card that i don’t keep extra money on, will FW inform us when they are preparing to take rest of the payment?</p>


</div>
<div id="post_18" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
<p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
<a itemprop="url" href="https://community.frame.work/u/Monni"><span itemprop="name">Monni</span></a>
</span>
<span>
<time itemprop="datePublished" datetime="2024-06-27T17:10:09Z">
June 27, 2024, 5:10pm
</time>
<meta itemprop="dateModified" content="2024-06-27T17:10:09Z">
<span itemprop="position">18</span>
</span>
</p>
<div itemprop="text">
<p>Yes! Have been looking forward to this for a while!</p>
<p>I’m looking to buy a 13 factory seconds version, but I don’t seem to be able to buy the Fin/Swe plain keyboard with one. Trying to buy both at the same time whines about</p>
<blockquote>
<p>Your bag contains items that ship from different warehouses. At this time, we can only ship from one warehouse for each order placed. Please place a separate order for the other items.</p>
</blockquote>
<p>and trying to order only the keyboard tells that</p>
<blockquote>
<p>Sorry, we only have stock of that item at one warehouse. You can only purchase it when combined with a laptop order.</p>
</blockquote>
<p>Should the plain keyboards be available for purchase separately?</p>
</div>


</div>
<div id="post_19" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
<p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
<a itemprop="url" href="https://community.frame.work/u/Monni"><span itemprop="name">Monni</span></a>
</span>
<span>
<time itemprop="datePublished" datetime="2024-06-27T17:17:16Z">
June 27, 2024, 5:17pm
</time>
<meta itemprop="dateModified" content="2024-06-27T17:17:16Z">
<span itemprop="position">19</span>
</span>
</p>
<p>It’s working now, that got resolved quickly <img src="https://community.frame.work/images/emoji/apple/smiley.png?v=12" title=":smiley:" alt=":smiley:" loading="lazy" width="20" height="20"></p>


</div>
<div id="post_20" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
<p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
<a itemprop="url" href="https://community.frame.work/u/Destroya"><span itemprop="name">Destroya</span></a>
</span>
<span>
<time itemprop="datePublished" datetime="2024-06-27T17:26:31Z">
June 27, 2024, 5:26pm
</time>
<meta itemprop="dateModified" content="2024-06-27T17:26:31Z">
<span itemprop="position">20</span>
</span>
</p>
<p>For the pre-orders, we send a “batch preparation” email 3 business days before we start charging, you should have plenty of time to arrange things on your end.</p>


</div>
<div id="post_21" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
<p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
<a itemprop="url" href="https://community.frame.work/u/Destroya"><span itemprop="name">Destroya</span></a>
</span>
<span>
<time itemprop="datePublished" datetime="2024-06-27T17:27:55Z">
June 27, 2024, 5:27pm
</time>
<meta itemprop="dateModified" content="2024-06-27T17:27:55Z">
<span itemprop="position">21</span>
</span>
</p>
<p>Thanks for the question, as far as I know, factory seconds laptop does not come with all keyboard layout options, this is expected.</p>


</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Software galaxies (516 pts)]]></title>
            <link>https://anvaka.github.io/pm/</link>
            <guid>40817852</guid>
            <pubDate>Fri, 28 Jun 2024 04:54:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anvaka.github.io/pm/">https://anvaka.github.io/pm/</a>, See on <a href="https://news.ycombinator.com/item?id=40817852">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Aeon: OpenSUSE for Lazy Developers (117 pts)]]></title>
            <link>https://lwn.net/Articles/977987/</link>
            <guid>40817199</guid>
            <pubDate>Fri, 28 Jun 2024 02:30:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/977987/">https://lwn.net/Articles/977987/</a>, See on <a href="https://news.ycombinator.com/item?id=40817199">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<center>
           <div><b>LWN.net needs you!</b><p>Without subscribers, LWN would simply not exist.  Please consider
       <a href="https://lwn.net/subscribe/">signing up for a subscription</a> and helping
       to keep LWN publishing</p></div>
           </center>
           
<p>The openSUSE project recently <a href="https://news.opensuse.org/2024/05/28/aeon-desktop-brings-new-features-in-rctwo-release/">announced</a>
the second release candidate (RC2) of its <a href="https://aeondesktop.github.io/">Aeon Desktop</a>, formerly known
as MicroOS Desktop GNOME. Aside from the new coat of naming paint,
Aeon breaks ground in a few other ways by dabbling with technologies not found in other openSUSE releases. The goal for Aeon is to provide
automated system updates using snapshots that can be applied
atomically, removing the burden of system maintenance for
"<q>lazy developers</q>" who want to focus on their work rather than desktop
administration. System-tinkerers need not apply.</p>

<p>The idea behind Aeon, as with other immutable (or <a href="https://lwn.net/Articles/946526/">image-based</a>) Linux
distributions, is to provide the core of the distribution as a read-only image or
filesystem that is updated atomically and can be rolled back if
needed. Google's ChromeOS was the first popular Linux-based desktop
operating system to follow this model. Since the
release of ChromeOS a number of interesting immutable
implementations have cropped up, such as <a href="https://fedoraproject.org/atomic-desktops/silverblue/">Fedora
Silverblue</a>, <a href="https://projectbluefin.io/">Project Bluefin</a>
(<a href="https://lwn.net/Articles/954059/">covered here</a> in
December 2023), 
openSUSE's <a href="https://microos.opensuse.org/">MicroOS</a> (<a href="https://lwn.net/Articles/927373/">covered here</a> in March
2023), and <a href="https://ubuntu.com/core">Ubuntu Core</a>.</p>

<p>What makes up the core software and how the immutable bits are
composed, deployed, and managed varies quite a bit between distributions.
Aeon uses a utility called <a href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</a>
(with openSUSE's Zypper package manager under the hood) and <a href="https://en.opensuse.org/SDB:BTRFS">Btrfs subvolumes</a> to
<a href="https://kubic.opensuse.org/documentation/transactional-update-guide/transactional-update.html">create
and update system snapshots</a>. Basically, instead of installing and
updating a system with individual packages while it is running,
the updates are applied in the background to a separate Btrfs snapshot
and then the system is rebooted into that snapshot. The <tt>/home</tt>
and <tt>/var</tt> directories are, of course, writable Btrfs volumes;
<tt>/etc</tt> uses <a href="https://docs.kernel.org/filesystems/overlayfs.html">overlayfs</a>
to apply local changes on top of the default configuration files.</p>

<p>User-installed software for these distributions is separated from the rest of the system
software on a mutable filesystem using some type of application-containerization technology like <a href="https://flatpak.org/">Flatpak</a>, <a href="https://podman.io/">Podman</a>, or <a href="https://snapcraft.io/about">Snap</a>. In Aeon's case, user
applications are generally installed via Flatpak, or using a
traditional package manager inside a containerized Linux distribution
of the user's choice that is managed with <a href="https://github.com/89luca89/distrobox">Distrobox</a>. 
(On Aeon, Distrobox uses Podman to run containers.)</p>

<h4>Installation and updates</h4>

<p>The openSUSE YaST team has been <a href="https://yast.opensuse.org/blog/2024-05-17/agama-8">working
on</a> a new installer called <a href="https://github.com/openSUSE/agama">Agama</a>, but Aeon has its
own installer, the <a href="https://github.com/sysrich/tik">Transactional Installation
Kit</a> (tik). The tik installer is designed to deploy operating
system images to UEFI hardware, which means that users with older
hardware are (at least currently) out of luck when it comes to
installing Aeon. It can also make use of the <a href="https://en.opensuse.org/Portal:MicroOS/Ignition">Ignition</a>
and <a href="https://en.opensuse.org/Portal:MicroOS/Combustion">Combustion</a>
configuration tools to create users, enable services,
install SSH keys, and more, automatically at install time.</p>

<p>As with the rest of the distribution, the philosophy for tik is
"minimal". Tik does not ask the user to make any
choices about software, how disk partitions should be laid out, or
much else, except to confirm that the installation should
proceed. Users are prompted to choose <em>which</em> disk to use,
when a system has multiple disks. If the target system has a existing MicroOS
installation, tik will offer to back up (and restore) existing users and data as
long as the USB stick with the installer has more free space than the
data stored in the home directory.</p>

<p>After installation, there is a first-run wizard to perform system
configuration. It asks for the usual information: the language
and keyboard layout to use, wireless network connection information,
time zone, and user information. Aeon does not configure a root user,
so the first system user is set up as an administrator and can use
<tt>sudo</tt> to perform any administrative tasks.</p>

<h4>Truly minimal desktop</h4>

<p>Aeon is composed from packages in the openSUSE Tumbleweed and
MicroOS repositories, so Aeon RC2 provides users with an up-to-date <a href="https://lwn.net/Articles/966187/">GNOME&nbsp;46</a>
desktop using Wayland. (GNOME&nbsp;46.2, to be exact.) It also includes Linux
6.9.3, systemd v255, and glibc 2.39.</p>

<p>Many Linux distributions offer "minimal" desktop package
selections, but openSUSE Aeon takes this farther than most. Aside from
some basic applications one would expect with GNOME—such as the
file manager, settings application, and other utilities—Aeon
comes with almost no software installed as part of the base
system. Firefox and GNOME's text editor, calculator, and terminal
application are all installed as Flatpaks as a second step after the
desktop user logs in the first time. If a user wants a media player,
office suite, email application, or even image and PDF viewers, they
have to be installed separately. That <em>may</em> be taking
minimalism a bit too far, but it does give users a lot of control over
what is installed on their system.</p>

<blockquote>
<a href="https://lwn.net/Articles/978229/"><img src="https://static.lwn.net/images/2024/openSUSE-Aeon-sm.png" alt="[openSUSE Aeon minimal
install]" title="openSUSE Aeon minimal install"></a>
</blockquote>

<p>Aeon is touted as a distribution for users who do not want to
hassle with system administration, but its sparse selection of
software ensures that some up-front work is required to reach the
payoff. For example, aside from having to
install expected utilities like a PDF viewer, the <tt>man</tt> command
and man pages are not part of the default software. Probably the best
way to deal with that is to create a Distrobox container to do one's work in,
though having the Distrobox man pages available would be
helpful when doing so. (Distrobox <a href="https://distrobox.it/#quick-start">documentation</a> is available
online, of course, but I've always reached for man pages first.)</p>

<p>One attractive, and seemingly unique, feature that Aeon offers is
automatic updates for the operating system, installed Flatpaks, and
any distroboxes that one may have set up. It's not unusual to offer
system and Flatpak updates, but updating distroboxes is usually a task
left to the user. It can become unwieldy quickly if one has several
distroboxes set up that need to be updated manually.</p>

<p>According to the RC2 announcement: there are a few features
targeting better performance. Aeon is the first openSUSE
edition to use the <a href="https://en.wikipedia.org/wiki/Zram">zram</a> kernel module for
system swap. This is supposed to improve system performance by
avoiding swapping to disk. Aeon's transactional update system is also
supposed to automatically choose packages compiled with <a href="https://news.opensuse.org/2023/03/02/tw-gains-optional-optimizations/">x86-64-v3
optimizations</a> for CPUs with Advanced Vector Extensions version two
(AVX2) extensions, if they are available and the system supports them.</p>

<p>Overall, Aeon looks like a good choice for users who want a
minimal, immutable openSUSE GNOME desktop system. It would be
especially attractive for users who want a distraction-free system
with just a few applications, like an IDE and web browser for
development work. It is probably not a great choice for openSUSE users
who are happy with Tumbleweed or Leap and installing software with Zypper.</p>

<p>Readers interested in trying openSUSE Aeon should check out the <a href="https://en.opensuse.org/Portal:Aeon/InstallGuide">install
guide</a> and <a href="https://en.opensuse.org/Portal:Aeon">overview</a>. The
distribution is still in a release-candidate stage, so rough edges are
to be expected—but it is solid enough to test drive and <a href="https://bugzilla.opensuse.org/enter_bug.cgi?product=openSUSE+Aeon&amp;format=guided">report
bugs</a> if they're found.</p><br clear="all"><hr><p>
           (<a href="https://lwn.net/Login/?target=/Articles/977987/">Log in</a> to post comments)
           </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Death of NYC Congestion Pricing (159 pts)]]></title>
            <link>https://www.apricitas.io/p/the-death-of-nyc-congestion-pricing</link>
            <guid>40817079</guid>
            <pubDate>Fri, 28 Jun 2024 02:01:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apricitas.io/p/the-death-of-nyc-congestion-pricing">https://www.apricitas.io/p/the-death-of-nyc-congestion-pricing</a>, See on <a href="https://news.ycombinator.com/item?id=40817079">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><strong>Thanks for reading! If you haven’t subscribed, please click the button below:</strong></p><p><strong>By subscribing you’ll join over 42,000 people who read Apricitas weekly!</strong></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9ac9984-69fa-4008-bcb3-61cf00f7bdac_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9ac9984-69fa-4008-bcb3-61cf00f7bdac_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9ac9984-69fa-4008-bcb3-61cf00f7bdac_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9ac9984-69fa-4008-bcb3-61cf00f7bdac_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9ac9984-69fa-4008-bcb3-61cf00f7bdac_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9ac9984-69fa-4008-bcb3-61cf00f7bdac_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c9ac9984-69fa-4008-bcb3-61cf00f7bdac_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1046162,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9ac9984-69fa-4008-bcb3-61cf00f7bdac_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9ac9984-69fa-4008-bcb3-61cf00f7bdac_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9ac9984-69fa-4008-bcb3-61cf00f7bdac_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9ac9984-69fa-4008-bcb3-61cf00f7bdac_2886x1843.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>New York City and its surrounding suburbs </span><a href="https://en.wikipedia.org/wiki/List_of_cities_by_GDP" rel="">make up the largest urban economic cluster on planet Earth.</a><span> </span><a href="https://data.bls.gov/timeseries/ENU3606110010" rel="">2.4 million people work</a><span> in the 23 square miles of land that make up the borough of Manhattan, and collectively they produce </span><a href="https://fred.stlouisfed.org/graph/?g=1oEcN" rel="">$886B in economic output</a><span>, a nominal sum larger than </span><a href="https://data.worldbank.org/indicator/NY.GDP.MKTP.CD?most_recent_value_desc=true&amp;locations=VN" rel="">Vietnam</a><span> and </span><a href="https://data.worldbank.org/indicator/NY.GDP.MKTP.CD?most_recent_value_desc=true&amp;locations=PK" rel="">Pakistan’s</a><span> GDP combined. Most of those workers take the subway.</span></p><p>Globally, that is nothing notable—in most urban cores a majority of workers take public transportation for work and daily activities. Increasing the density of jobs, hospitals, restaurants, homes, schools, and more is key to the agglomeration effects that make cities such economic powerhouses, and as density grows mass transit becomes essential since it can far surpass the maximum throughput capacity of even the largest roadways.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d9b5079-1b8a-4d3d-af12-11be814806eb_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d9b5079-1b8a-4d3d-af12-11be814806eb_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d9b5079-1b8a-4d3d-af12-11be814806eb_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d9b5079-1b8a-4d3d-af12-11be814806eb_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d9b5079-1b8a-4d3d-af12-11be814806eb_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d9b5079-1b8a-4d3d-af12-11be814806eb_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2d9b5079-1b8a-4d3d-af12-11be814806eb_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:132950,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d9b5079-1b8a-4d3d-af12-11be814806eb_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d9b5079-1b8a-4d3d-af12-11be814806eb_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d9b5079-1b8a-4d3d-af12-11be814806eb_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d9b5079-1b8a-4d3d-af12-11be814806eb_2886x1843.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>Yet in the US, it’s extremely unusual for most workers to take public transportation—in 2022 </span><a href="https://data.census.gov/table/ACSST1Y2022.S0802?q=means%20of%20transportation%20to%20work" rel="">only about 3.1% of Americans took trains or buses to their jobs</a><span> and </span><a href="https://data.census.gov/table?q=means%20of%20transportation%20to%20work&amp;g=010XX00US_310XX00US35620" rel="">about 45% of those transit commuters lived in the Greater New York area.</a><span> </span><a href="https://data.census.gov/table?q=means%20of%20transportation%20to%20work&amp;g=160XX00US0667000,1150000,1714000,2507000,4260000" rel="">Chicago, Boston, DC, LA, San Francisco, and Philadelphia are the only other major American cities where a substantial number of workers take transit, and in none of them is the transit mode share above 25%.</a><span> New Yorkers’ inflated sense of self-importance is </span><a href="https://vintageposters.us/wp-content/uploads/2023/12/00550-scaled.jpg" rel="">one of the longest-running jokes about the city</a><span>, but it’s true that in this way the Big Apple is fundamentally America’s only major city. Thus New York’s public transit policy is fundamentally America’s public transit policy—and NYC’s urban development is in many ways America’s urban development.</span></p><p><span>That’s why it was so important when </span><a href="https://www.nbcnewyork.com/news/mta-pauses-new-york-city-congestion-pricing-plans-indefinitely/5479563/" rel="">earlier this month</a><span>, New York State Governor Kathy Hochul announced that America’s first congestion pricing scheme would be “indefinitely paused”. The policy would have charged vehicles for entering the perpetually gridlocked streets of Lower Manhattan, using the money raised to then fund many of the city’s transit projects—and it was slated to officially begin operating less than 30 days from the announcement. In the short term, the flip-flop will be extremely costly for the city and state, but more broadly the cancellation and the dynamics that led up to it are emblematic of many of the problems holding back American cities &amp; infrastructure.</span></p><p><span>In one sense, the congestion pricing scheme is a radical new experiment in American transportation policy. Cars compete for scarce urban road space with more efficient modes of transportation like buses, </span><a href="https://www.reddit.com/r/newyorkcity/comments/12uoj82/good_thing_there_are_no_people_in_the_congestion/" rel="">emit large amounts of noxious chemicals and particulate matter from tailpipes and tires</a><span>, </span><a href="https://www.usatoday.com/money/blueprint/auto-insurance/fatal-car-crash-statistics/" rel="">are the leading cause of death for Americans under 55</a><span>, and in large quantities become disruptively loud for workers and residents. Lower Manhattan is among the few places in America where substantial transit alternatives to driving exist, so a congestion tax should induce mode shifts away from cars and toward the subway—to the benefit of the local and global climate. Wealthier residents are more likely to drive and poorer residents more likely to take transit, so congestion pricing serves redistributive purposes as well. New York’s scheme promised to be a pioneering first for the nation, a pilot project that if successful could be copied in cities throughout the US.</span></p><p><span>Yet in another sense, the plan should be entirely unremarkable. </span><a href="https://en.wikipedia.org/wiki/Area_Licensing_Scheme" rel="">Congestion pricing has existed in some form throughout cities across the world for 50 years</a><span>, and is fundamentally just a fancy version of the normal tolling systems that are common on bridges, tunnels, and other roadways. People who drove into Lower Manhattan were already “paying” for the privilege to do so, just with the time and gasoline lost to bumper-to-bumper traffic rather than in hard cash. As with any heavily overcrowded bridge or highway, mispricing was causing economic inefficiency—charging for entry would allow priority vehicles to get where they’re going faster, incentivize better use of existing roadways, and open up a funding stream that could be used to expand beyond current infrastructure constraints. Economists would recommend some form of congestion pricing just for the benefits to road users alone. Plus, </span><a href="https://x.com/NatJYang/status/1799463825164693999" rel="">once implemented, these schemes are usually popular among city residents</a><span>—</span><a href="https://x.com/NatJYang/status/1799463825164693999" rel="">public opinion of London’s congestion charge rose nearly 20 percentage points after implementation</a><span> and </span><a href="https://en.wikipedia.org/wiki/Stockholm_congestion_tax" rel="">Stockholm’s congestion tax was approved by voters after a 7-month trial period.</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d3ddb37-fcd8-42e5-be7c-e4997c6e4658_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d3ddb37-fcd8-42e5-be7c-e4997c6e4658_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d3ddb37-fcd8-42e5-be7c-e4997c6e4658_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d3ddb37-fcd8-42e5-be7c-e4997c6e4658_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d3ddb37-fcd8-42e5-be7c-e4997c6e4658_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d3ddb37-fcd8-42e5-be7c-e4997c6e4658_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9d3ddb37-fcd8-42e5-be7c-e4997c6e4658_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:289337,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d3ddb37-fcd8-42e5-be7c-e4997c6e4658_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d3ddb37-fcd8-42e5-be7c-e4997c6e4658_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d3ddb37-fcd8-42e5-be7c-e4997c6e4658_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d3ddb37-fcd8-42e5-be7c-e4997c6e4658_2886x1843.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>In a more practical sense, New York’s Metropolitan Transportation Authority (MTA) needed new post-COVID funding streams. After ridership (understandably) cratered during the early pandemic, the system had to be bailed out by federal cash infusions from the CARES Act and American Rescue Plan alongside </span><a href="https://neweast.mta.info/press-release/federal-funding-provides-mta-financial-stability-through-first-half-of-2024-no-fare#:~:text=In%202020%2C%20the%20MTA%20utilized,the%20maximum%20%242.9%20billion%20allowed." rel="">nearly $3B borrowed from the Federal Reserve.</a><span> Remote work enabled shifts out of New York’s expensive real estate market and into the city’s suburbs and other metro areas, making it even more difficult for city bus &amp; rail ridership to recover.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75bd8e48-3432-425b-8a00-8e3a13ee4fcc_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75bd8e48-3432-425b-8a00-8e3a13ee4fcc_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75bd8e48-3432-425b-8a00-8e3a13ee4fcc_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75bd8e48-3432-425b-8a00-8e3a13ee4fcc_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75bd8e48-3432-425b-8a00-8e3a13ee4fcc_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75bd8e48-3432-425b-8a00-8e3a13ee4fcc_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/75bd8e48-3432-425b-8a00-8e3a13ee4fcc_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:299356,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75bd8e48-3432-425b-8a00-8e3a13ee4fcc_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75bd8e48-3432-425b-8a00-8e3a13ee4fcc_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75bd8e48-3432-425b-8a00-8e3a13ee4fcc_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75bd8e48-3432-425b-8a00-8e3a13ee4fcc_2886x1843.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Yet while transit ridership continues to struggle, </span><a href="https://new.mta.info/document/135371" rel="">MTA bridge &amp; tunnel toll revenue has more than completely recovered to pre-COVID levels</a><span>. People were increasingly driving into the city and underutilizing transit infrastructure—thus congestion pricing, while initially envisioned for a pre-pandemic world, rapidly became a necessary part of plans for a post-COVID recovery. Taxing traffic, with all its negative externalities, was certainly much more politically palatable and economically beneficial than attempting to raise payroll taxes further, which would have strained New York’s already-slow post-COVID jobs recovery. Plus, the incidence of congestion taxes would also fall more on tourists and residents of New Jersey/Connecticut, helping to recapture some of the economic value lost to the suburbs as a result of remote work. The policy should have been an easy win for the city.</span></p><p><span>Yet just before implementation, congestion pricing was indefinitely paused at a press conference framed around inflation and cost of living concerns, highlighting how much the tax side of the scheme loomed large in voters’ minds. That makes sense for drivers from Bergen County in northern New Jersey, for whom the system was (financially) all cost and no benefit—they would pay to enter downtown, but the revenue collected would all go to transit investments outside their county, and their primary benefit would thus only be experiencing less traffic when driving through Manhattan. But Governor Hochul does not represent New Jerseyans, and for New Yorkers (</span><a href="https://urbanstats.org/comparison.html?longnames=%5B%22New+York%2C+USA%22%2C%22New+York+city%2C+New+York%2C+USA%22%5D" rel="">especially the 55% of city households who don’t own a car</a><span>), the tradeoff was supposed to be clearer—impose costs on a small subset of drivers to deliver benefits in the form of transit investments that would serve the majority of people. Those benefits, however, felt relatively small to many city and state residents.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cf95e54-3699-4d2a-bc7d-c4d3c1b06ff8_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cf95e54-3699-4d2a-bc7d-c4d3c1b06ff8_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cf95e54-3699-4d2a-bc7d-c4d3c1b06ff8_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cf95e54-3699-4d2a-bc7d-c4d3c1b06ff8_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cf95e54-3699-4d2a-bc7d-c4d3c1b06ff8_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cf95e54-3699-4d2a-bc7d-c4d3c1b06ff8_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5cf95e54-3699-4d2a-bc7d-c4d3c1b06ff8_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:256463,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cf95e54-3699-4d2a-bc7d-c4d3c1b06ff8_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cf95e54-3699-4d2a-bc7d-c4d3c1b06ff8_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cf95e54-3699-4d2a-bc7d-c4d3c1b06ff8_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cf95e54-3699-4d2a-bc7d-c4d3c1b06ff8_2886x1843.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>A lot of congestion-pricing criticism posits that the MTA already has “enough money” and just needs to spend it better. In many ways, they have a great point—</span><a href="http://web.mta.info/budgetdashboard/Budget_Transparencyd.html" rel="">the MTA has a total operating budget of $19B</a><span>, </span><a href="https://en.wikipedia.org/wiki/List_of_U.S._state_budgets" rel="">an amount larger than many states</a><span>, </span><a href="https://new.mta.info/capital/2020CapitalProgram" rel="">plus a capital investment budget of roughly $11B a year</a><span>, and they build relatively little with it. High costs for new public transportation infrastructure is an America-wide problem but is at its absolute worst within the city—of the 5 most expensive projects tracked by the </span><a href="https://transitcosts.com/new-data/" rel="">NYU-Marron Institute Transit Costs Project</a><span>, 4 are in New York City. It would be great if New York could build transit infrastructure at something approaching the cost-per-mile of other dense urban areas in much of Asia, continental Europe, or South America, but even reaching the still-boondoggle levels of the Bay Area Rapid Transit extension to San Jose would be a massive improvement. Even as billions are spent, construction takes much longer to complete in NYC—in the 15 years it took to complete the </span><a href="https://en.wikipedia.org/wiki/East_Side_Access" rel="">East Side Access project</a><span>, </span><a href="https://en.wikipedia.org/wiki/Delhi_Metro" rel="">the Delhi Metro in India added 7 new lines</a><span> and increased ridership by more than 1B trips per year.</span></p><p><span>These MTA projects are still worthwhile—cost-per-rider projections for NYC transit actually tend to be relatively benign (by the very low standards of American transit) because the city’s expensive projects are still moving through some of the densest neighborhoods in the country and link to an extremely comprehensive existing network. But high costs force New Yorkers to settle for a much lower-quality, lower-capacity system. Take the Interborough Express, a planned orbital transit line between Brooklyn and Queens designed to improve access in transit deserts throughout the city and connect with 17 existing subway lines. </span><a href="https://www.youtube.com/watch?v=yLQHKggYb6c&amp;t=170s" rel="">The line will be lower capacity light rail, not the kind of heavy rail that the subway runs on, will mix with aboveground street traffic throughout some subsections of Queens, be built mostly by repurposing preexisting freight right-of-way,</a><span> and </span><a href="https://new.mta.info/document/87606" rel="">is still projected to cost $5.5B.</a><span> The fact that with billions of dollars in new revenue the MTA could only promise marginal expansions rather than transformative system-changing investments reduced the perceived benefits of the congestion pricing scheme—allowing narrow dedicated opposition among a portion of suburban drivers to topple the program.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1356888-c47a-4fa7-90ec-b950c3f0b6ea_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1356888-c47a-4fa7-90ec-b950c3f0b6ea_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1356888-c47a-4fa7-90ec-b950c3f0b6ea_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1356888-c47a-4fa7-90ec-b950c3f0b6ea_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1356888-c47a-4fa7-90ec-b950c3f0b6ea_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1356888-c47a-4fa7-90ec-b950c3f0b6ea_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b1356888-c47a-4fa7-90ec-b950c3f0b6ea_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:214692,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1356888-c47a-4fa7-90ec-b950c3f0b6ea_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1356888-c47a-4fa7-90ec-b950c3f0b6ea_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1356888-c47a-4fa7-90ec-b950c3f0b6ea_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1356888-c47a-4fa7-90ec-b950c3f0b6ea_2886x1843.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Yet I think congestion pricing opponents miss how their victory is emblematic of—and exacerbating—the city’s transit cost problems. The Governor is turning the MTA into a </span><a href="https://pedestrianobservations.com/2024/06/05/hochul-suspends-congestion-pricing/" rel="">nightmare customer and a horrible coworker here</a><span>, as the cancellation of congestion pricing jeopardizes key funding and matching federal dollars for a suite of construction projects at the 11th hour. </span><a href="https://x.com/DaveCoIon/status/1799257709935468869/photo/1" rel="">The MTA’s CFO was forced to announce that the agency will now need to “reorganize the [2020-2024 Capital] Program to prioritize the most basic and urgent needs” because it “cannot award contracts that do not have a committed, identified funding source.”</a><span> That means tens of billions of dollars in investments could be shelved, including Phase II of the Second Avenue Subway, the </span><a href="https://new.mta.info/project/penn-station-access" rel="">Metro-North Penn Station Access</a><span> expansions, a host of station upgrades, and much more. Contractors who work with the MTA on these projects already charge a premium for having to manage the substantial risk that construction is delayed or canceled—introducing “the Governor might suddenly decide your job is over” as another risk to worry about will just increase those cost premiums, thin the pool of companies who are capable of working with the MTA, and reward the politically savvy.</span></p><p>The fact that infrastructure plans can go through years of study, environmental review, public consultation, pilot projects, revisions, lawsuits, construction, delays, and more only to then be shelved days before final implementation is a large part of why public works are so expensive in the first place. The number of veto points for American infrastructure is extremely high—even one “no” across from any number of officials throughout construction can stop a project dead—and that veto power makes it easy to extract money and concessions from projects. Pick more costly construction methods to minimize surface disruption, reduce daily construction time to leave existing traffic unimpeded, move infrastructure to suboptimal locations to appease powerful voting constituencies, expand stations to meet interagency demands for back office space, and in the absolute worst cases just use discretionary powers to do quid-pro-quo corruption. Also, imagine if you were among the hundreds of state, local, and federal civil servants who worked on any part of the congestion pricing or MTA capital investment plans—would you want to remain at your job after having years of work tossed aside at the last minute? The hollowing out of institutional capacity within transportation agencies is a serious driver of cost problems, with planning becoming a bespoke project-by-project activity heavily dependent on expensive outside consultants rather than a regular in-house activity.</p><p><span>As a more practical matter, hundreds of millions were already (over)spent on congestion pricing between years of studies, protracted legal battles, construction of physical infrastructure, contracts for implementation, and much more—all that money is wasted by pulling the plug at the last possible second. The cancellation of congestion pricing without an alternative funding stream will hurt the MTA’s credit rating, </span><a href="https://x.com/ReinventAlbany/status/1800272448774144022" rel="">increase its debt-servicing expenses</a><span>, and functionally impose even more costs on New Yorkers. Before the congestion pricing cancellation, the MTA had actually made some rare positive efforts to control infrastructure construction costs, including </span><a href="https://marroninstitute.nyu.edu/blog/mta-finds-savings-for-second-avenue-subway-phase-2" rel="">saving $1.3B in Phase II of the Second Avenue Subway project</a><span>, yet instead of building on that the Governor has decided to throw the entire infrastructure program into jeopardy.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40a3da86-1f39-4893-b2d4-100d7540ea8a_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40a3da86-1f39-4893-b2d4-100d7540ea8a_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40a3da86-1f39-4893-b2d4-100d7540ea8a_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40a3da86-1f39-4893-b2d4-100d7540ea8a_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40a3da86-1f39-4893-b2d4-100d7540ea8a_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40a3da86-1f39-4893-b2d4-100d7540ea8a_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/40a3da86-1f39-4893-b2d4-100d7540ea8a_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:358926,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40a3da86-1f39-4893-b2d4-100d7540ea8a_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40a3da86-1f39-4893-b2d4-100d7540ea8a_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40a3da86-1f39-4893-b2d4-100d7540ea8a_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40a3da86-1f39-4893-b2d4-100d7540ea8a_2886x1843.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>America invests little into public transportation infrastructure by the standards of similarly-wealthy European and Asian countries (or even relative to Canada and Mexico). Pre-pandemic, it was normal for urban mass transit to receive only 6-9% of all state and local land transportation spending, and for bus and train passenger terminals to receive roughly another 3-5%. However, relative spending on both items has cratered post-COVID, with the share of construction spending going to mass transit falling to the lowest levels in a decade this year. Even when these investments are made, American cities struggle to make the best of new infrastructure.</p><p><span>Public transit is a network system that depends on scale to be successful—a two-line metro is more than twice as valuable as a 1-line metro, and a metro that goes between dense downtown neighborhoods is much more valuable than one that meanders through sprawly suburbs. A large part of why American transit infrastructure projects fail is because even when built they are placed in the middle of nowhere or are undercut by local governments that prevent the necessary complementary neighborhood housing and commercial developments. Take the new Silver Line Extension of Washington DC’s Metro—stations like Loudoun Gateway are currently transit to nowhere, </span><a href="https://www.google.com/maps/@38.9937482,-77.46077,1217m/data=!3m1!1e3?entry=ttu" rel="">situated near a field and highway interchange just outside Dulles Airport</a><span>, and </span><a href="https://new.mta.info/project/east-side-access" rel="">thus serve only a couple hundred riders per day.</a><span> Meanwhile, existing stations that were coupled with substantial housing development, like </span><a href="https://imgur.com/Tfrrt2o" rel="">Navy Yard-Ballpark</a><span>, had some of the fastest-growing ridership of the entire system pre-COVID. The immediate vicinity of the </span><a href="https://www.google.com/maps/place/North+Berkeley/@37.8738506,-122.2841245,437m/data=!3m1!1e3!4m6!3m5!1s0x80857ebe7ffc5585:0xee00d72faf6fdce3!8m2!3d37.8739752!4d-122.2834383!16s%2Fg%2F1hdz6wk_d?entry=ttu" rel="">North Berkeley BART stop in California is surrounded on all sides by parking lots</a><span>, and </span><a href="https://www.bart.gov/sites/default/files/2024-05/Ridership_202404.xlsx" rel="">thus the station currently sees only about 1,700 riders per weekday.</a><span> A redevelopment is finally planned for the land surrounding the station, </span><a href="https://sfyimby.com/2024/02/pre-application-filed-for-north-berkeley-bart-redevelopment.html" rel="">but this will still likely bring less than 750 new homes to the area.</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccc13671-a4ea-4690-b24c-f52f0f95109f_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccc13671-a4ea-4690-b24c-f52f0f95109f_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccc13671-a4ea-4690-b24c-f52f0f95109f_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccc13671-a4ea-4690-b24c-f52f0f95109f_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccc13671-a4ea-4690-b24c-f52f0f95109f_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccc13671-a4ea-4690-b24c-f52f0f95109f_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ccc13671-a4ea-4690-b24c-f52f0f95109f_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:202946,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccc13671-a4ea-4690-b24c-f52f0f95109f_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccc13671-a4ea-4690-b24c-f52f0f95109f_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccc13671-a4ea-4690-b24c-f52f0f95109f_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccc13671-a4ea-4690-b24c-f52f0f95109f_2886x1843.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>In New York, the high cost of the East Side Access project would be more palatable if more progress had been made on </span><a href="https://www.6sqft.com/master-planning-for-huge-sunnyside-yard-project-to-begin-this-summer/" rel="">Sunnyside Yard—a plan to build housing and a new station on top of an existing train yard like in the Hudson Yards</a><span> redevelopment—and if the proposed number of housing units hadn’t already been scaled back. More broadly, </span><a href="https://socds.huduser.gov/permits/output_annual.odb?outpref=xls&amp;geoval=state&amp;datatype=annual&amp;varlist=1&amp;yearlist=1990%231991%231992%231993%231994%231995%231996%231997%231998%231999%232000%232001%232002%232003%232004%232005%232006%232007%232008%232009%232010%232011%232012%232013%232014%232015%232016%232017%232018%232019%232020%232021%232022%232023&amp;statelist=34%2336&amp;msalist=+&amp;cbsalist=+&amp;bppllist=34017246000%2336005082500%2336061431000&amp;cntylist=34017%2336005%2336061&amp;COUNTYSUM=+&amp;COUNTYALL=+&amp;COUNTYGRP=+&amp;STATESUM=+&amp;STATEALL=+&amp;METROSUM=+&amp;METROALL=+&amp;METRO=+&amp;CBSA=+&amp;PLACEGRP=+&amp;CSUMNAME=&amp;JSUMNAME=+&amp;geo=state&amp;chron=annual&amp;__utma=200893484.681316835.1717929820.1717929820.1717929820.1&amp;__utmc=200893484&amp;__utmz=200893484.1717929820.1.1.utmcsr%3D%28direct%29%7Cutmccn%3D%28direct%29%7Cutmcmd%3D%28none%29&amp;__utmt=1&amp;__utmb=200893484.1.10.1717929820" rel="">over the last 6 years more housing has been built across the Harlem River in the Bronx and across the Hudson River in Jersey City than in the extremely transit-accessible and high-demand neighborhoods of Manhattan</a><span>. The city’s housing policies are, among a myriad of other issues, making the MTA less effective—had the city built more in the 2010s, COVID outmigration wouldn’t have been so severe and the MTA’s network could draw from a much stronger ridership base. The defeat of reform efforts like </span><a href="https://www.nytimes.com/article/nyc-housing-hochul-long-island-westchester.html" rel="">Governor Hochul’s housing compact</a><span>, which would have allowed for more dense housing construction around transit stations, are entrenching this extremely flawed status quo.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4049ee-9757-466b-ae0a-4305afb84dca_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4049ee-9757-466b-ae0a-4305afb84dca_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4049ee-9757-466b-ae0a-4305afb84dca_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4049ee-9757-466b-ae0a-4305afb84dca_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4049ee-9757-466b-ae0a-4305afb84dca_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4049ee-9757-466b-ae0a-4305afb84dca_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5f4049ee-9757-466b-ae0a-4305afb84dca_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:197477,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4049ee-9757-466b-ae0a-4305afb84dca_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4049ee-9757-466b-ae0a-4305afb84dca_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4049ee-9757-466b-ae0a-4305afb84dca_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4049ee-9757-466b-ae0a-4305afb84dca_2886x1843.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>These cost and ineffectiveness problems are also mirrored in another infrastructure area where spending is actually increasing—intercity rail. State and local spending on rail construction has hit a new record high post-COVID, rising above $3B a year for the first time. The project most emblematic of this construction surge is California High Speed Rail, which aimed to be America’s first modern HSR system but has hit a </span><a href="https://en.wikipedia.org/wiki/2008_California_Proposition_1A" rel="">long series of snags since it was first authorized by voters in 2008.</a><span> Slow land acquisition, lengthy environmental review, and repeated legal battles continually delayed the project and increased its costs. Along with other issues, this has meant the line is only scheduled to start operation on a segment between Bakersfield and Merced in the early 2030s, and the full price to connect San Francisco to LA is likely to exceed $100B.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102cec94-d47e-4ce1-9c09-be49a74a2ccf_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102cec94-d47e-4ce1-9c09-be49a74a2ccf_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102cec94-d47e-4ce1-9c09-be49a74a2ccf_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102cec94-d47e-4ce1-9c09-be49a74a2ccf_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102cec94-d47e-4ce1-9c09-be49a74a2ccf_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102cec94-d47e-4ce1-9c09-be49a74a2ccf_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/102cec94-d47e-4ce1-9c09-be49a74a2ccf_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:266810,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102cec94-d47e-4ce1-9c09-be49a74a2ccf_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102cec94-d47e-4ce1-9c09-be49a74a2ccf_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102cec94-d47e-4ce1-9c09-be49a74a2ccf_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102cec94-d47e-4ce1-9c09-be49a74a2ccf_2886x1843.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>At current cost estimates, the Bakersfield-Merced initial operating segment is already expensive by global standards and the SF-LA line would be among the most expensive high-speed rail projects in history. This is with generous assumptions for CAHSR—especially the assumption there won’t be further cost increases when construction starts on more difficult rail segments—and still the only lines with similar price profiles are those built largely underground or those in the UK where cost inflation is often even worse. Even accepting that the initial operating segment can open on schedule in the 2030s, </span><a href="https://www.kcra.com/article/california-bullet-train-project-funding-san-francisco-los-angeles/60181448" rel="">there is still no timeline for the project to connect to either San Francisco or Los Angeles, and another source of funding will need to be found before construction to those cities can continue.</a><span> While not complete transit-to-nowhere, the fact that the highest-speed train in the Americas will be only shuttling people between some of California’s smaller car-dependent cities for years is emblematic of the struggles of US transportation planning. Good infrastructure has large diffuse benefits and small narrow costs, but with enough twisting you can make the benefits smaller and the costs larger.</span></p><p><span>The practical future of the congestion pricing scheme remains up in the air. There are </span><a href="https://www.nytimes.com/2024/06/11/nyregion/congestion-pricing-lawsuits.html" rel="">inevitable legal battles that will have to be settled regarding Hochul’s attempt to pause the project</a><span>, and the appetite for a payroll tax increase seems to be low, leading to talk of raiding the state’s general fund or issuing more debt as a stop-gap until another source of workable revenue can be found. Uncertainty is extremely high, but congestion pricing is </span><a href="https://manifold.markets/capablemonkey/will-congestion-pricing-in-nyc-be-e" rel="">not completely buried just ye</a><span>t and the lack of viable alternatives may push the Governor’s Mansion to eventually revive the scheme. For New York, a permanent killing of congestion pricing would help solidify the state’s position as a shrinking share of American jobs, residents, and GDP in the post-COVID world.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0e840e-4456-4fd1-8840-39345c784ca1_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0e840e-4456-4fd1-8840-39345c784ca1_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0e840e-4456-4fd1-8840-39345c784ca1_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0e840e-4456-4fd1-8840-39345c784ca1_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0e840e-4456-4fd1-8840-39345c784ca1_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0e840e-4456-4fd1-8840-39345c784ca1_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fd0e840e-4456-4fd1-8840-39345c784ca1_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:288944,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0e840e-4456-4fd1-8840-39345c784ca1_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0e840e-4456-4fd1-8840-39345c784ca1_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0e840e-4456-4fd1-8840-39345c784ca1_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0e840e-4456-4fd1-8840-39345c784ca1_2886x1843.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>These all still might just seem like local political problems, especially if you live outside NYC or are among the vast majority of Americans who aren’t reliant on public transit infrastructure. However, many of the issues facing urban public transportation plague all forms of US infrastructure spending. Real per-capita investment in all mass transit (including airports) has been stagnant for 20 years—but real per-capita investment in highways &amp; streets is also stuck at pre-WWII levels. </span><a href="https://www.aeaweb.org/articles?id=10.1257/app.20200398" rel="">Cost-per-mile for US highway construction has skyrocketed over time, especially in wealthy areas with stronger local opposition,</a><span> America just more readily pays those higher costs rather than the higher cost for transit infrastructure—which is why it is necessary to get cost problems under control rather than just abandon the idea of transit infrastructure entirely.</span></p><p><span>American transportation agencies have a notable reluctance to adopt best practices from abroad, yet even within the US there are valuable success stories to learn from. Los Angeles is the city most synonymous with American highway urbanism, but </span><a href="https://www.youtube.com/watch?v=wpfaH-LhTYM" rel="">LA Metro is currently engaged in one of the most transformative transit expansion projects in modern American history for a high-but-not-ludicrous price tag.</a><span> Meanwhile, </span><a href="https://www.vox.com/cities-and-urbanism/24125535/dc-metro-transit-wmata-urbanism-cities-commuting" rel="">DC’s Metro has shuffled through a series of operational reforms under new leadership</a><span> that have dramatically improved ridership for a system that was extremely at risk given how many city residents now work remotely. The death of congestion pricing was among the largest setbacks for American urban infrastructure since the start of the pandemic, but it’s still necessary and possible for US cities to expand and improve their transportation networks.</span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everything I Knew About Stretching Was Wrong (101 pts)]]></title>
            <link>https://tylertringas.com/mobility/</link>
            <guid>40816795</guid>
            <pubDate>Fri, 28 Jun 2024 00:56:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tylertringas.com/mobility/">https://tylertringas.com/mobility/</a>, See on <a href="https://news.ycombinator.com/item?id=40816795">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									<p data-pm-slice="1 1 []">In the past 12 months my body’s mobility and flexibility went from abysmal, a source of persistent pain impacting my quality of life, to pretty darn good. I’m not about to become a stretching influencer, but after a year of researching, trial and error, and hard work I feel back on track and like I have tools that will really serve me and my body for decades. I’m honestly grateful for the big wake up call and I thought I would I share the most important things I learned.</p>
<p>A year ago my 37 year old body was completely wrecked. It snuck up on me. In the pandemic I started seriously strength training for the first time. I spent two years lifting heavy 3-4 days per week and it worked. I gained 20 pounds of muscle. For the first time in my life I got really strong, bench pressing my bodyweight and deadlifting/squatting around 2x that.</p>
<p>But throughout this process, I completely utterly neglected my flexibility and mobility. Rock climbing and yoga have been main exercise for most of my adult life, but climbing gyms and yoga studios were closed or inaccessible for most of those 2 years. So I packed on layers and layers of new muscle fibers with virtually no mobility work.</p>
<p>On top of that, I had the most stressful two years of my life on both personal and professional fronts. I always thought the idea that “we hold stress in our {body part}” was woo woo nonsense. It isn’t. Persistent stress and tension has a real affect on our bodies.</p>
<p>The problem is the typical weight lifting session doesn’t require a ton of full body mobility. The barbell that stabilizes the weight, allowing you to exert the most force possible, also limits the range of motion you actually utilize at the gym.</p>
<p>Without my noticing it my mobility fell off a cliff. My shoulders were frozen stiff. I couldn’t raise my arms over my head, hold a basic down dog, or touch my hand to my back or shoulder blades at all. I couldn’t touch my toes or even sit on the floor comfortably. Every major muscle group was hard as a rock all the time, as if I was flexing it as hard as I could. My range of motion was laughable and everything was gobbed up with layers of fascia.</p>
<p>Then I started trying to use my body to do things again—rock climb, ski, bike, yoga, or just carry things up a 4th floor walk up—and my whole body imploded. First, <em>everything</em> felt unbelievably challenging because every single body movement felt like it was pulling against heavy resistance bands. Next, everything hurt all the time. I had persistent muscle soreness, tightness, and pain all the time. Then the last straw was the nerve pain. I started getting numbness in my fingers and periodic electric shocks of pain going up both my arms doing simple day to day things like opening a door or picking up a cooking pan.</p>
<p>It sucked. I felt like I was 90 years old, decrepit and in constant pain. It was seriously degrading my quality of life and I was willing to try anything and everything. So here’s what I did:</p>
<ul>
<li data-list-kind="bullet">I went for a massage 2x/week</li>
<li data-list-kind="bullet">I bought a fancy Theragun massage tool</li>
<li data-list-kind="bullet">I went to two different local physical therapists</li>
<li data-list-kind="bullet">I started going to yoga 2x/week</li>
<li data-list-kind="bullet">I start stretching and warming up before climbing workouts</li>
</ul>
<p>3 months of all of that did exactly <em>nothing</em> for my mobility issues.</p>
<p>Massages and percussive massage tools can provide some temporary relief but have no lasting effect.</p>
<p>Sorry, but it seems that most PTs have absolutely no idea how to systematically treat these kind of issues. Like a massage therapist, they would use a kind of scraping tool or some other treatment that would provide some relief, and tell me to do nerve glides, charge me $250, and tell me they’d see me next week. But the process didn’t seem to have much of a real methodology and each week felt like zero progress. My experience seems to be similar to that of many others dealing with similar issues.</p>
<p>Yoga and pre-workout stretching are fine for maintaining flexibility, but not barely effective at <em>creating</em> flexibility, especially when you are severely limited.</p>
<h2 data-pm-slice="1 1 []">The turning point</h2>
<p>As I have often done in my life when the conventional solutions to a problem are failing me, I turned to the Twitter community for help and also went deep down YouTube and other internet rabbit holes. And that’s how I learned that everything I thought I knew about stretching was wrong.</p>
<p>The turning point was when I got connected to and did a virtual consult with <a href="https://twitter.com/movebettersam" data-lasso-id="435">Sam Martin from Move Better Project</a> (huge thanks to Sam Segar for making that intro). Sam explained to me that the numbess and nerve pain was likely Thoracic Outlet Syndrome or something similar where my stiff gunked up inflexible muscles were compressing and trapping the median nerve that runs from the shoulder through the arm and down to the fingers.</p>
<p>Sam gave me a bunch of simple exercises to target each muscle group (including some hard to find ones). But he also gave me a radically better overall understanding of&nbsp;<em>how</em> stretching and mobility actually works. These fundamental ideas explained why none of the stuff I’d tried before had any real effect and laid the groundwork for me to effectively build an approach to my own mobility.</p>
<p>I realized that I was a bit of an extreme case. Interventions that worked for most people, felt like they barely made a dent in my issues. That’s when I discovered a number of great YouTube channels dedicated to mobility for power lifters and other beefcakes. These guys were preaching similar methods as Sam, but devising more intense variations that I needed to unlock by extremely laminated muscle fibers.</p>
<p>I’ll share some specific examples below, but here are the main ideas I learned that completely changed my approach to mobility.</p>
<h2 data-pm-slice="1 1 []">What I learned</h2>
<p><strong>Hold stretches for 2 minutes (or more!). </strong>The kind of brief stretching you might do in a vinyasa yoga class poses or while warming up for a workout will have almost no effect on increasing your flexibility. To move the needle on your flexibility you need to be holding each position for <em>at least</em> two minutes, and sometimes more like 5+ minutes. Yes, this takes a long time. For several months while I was rebuilding my mobility I was spending 30-45 minutes almost every day on mobility.</p>
<p>These long holds, initially needing up to 5 minutes each, were the key to getting my shoulder mobility back into a good range.</p>
<p>The good news is that, at least in my experience, this only necessary when you’re trying to <em>increase </em>your mobility. Once you get things in a pretty good place, shorter regular stretching will do a good job of maintaining.</p>
<p><strong>Smash it, don’t rub it. </strong> Rubbing your muscles mainly just increases bloodflow, which is nice, but isn’t going to break up the years of fascia and adhesions or release chronically stiff muscles. You need to <em>smash</em> them instead.</p>
<ol>
<li data-list-kind="ordered">Create pressure in the meaty part of the muscle with something like a lacrosse ball. For me I usually have to move it around until I find a particularly stiff (and painful) spot to place it.</li>
<li data-list-kind="ordered">Once you find the spot, DON’T MOVE THE BALL AROUND. Keep it in that same spot and hold that pressure for ~2 minutes or until you feel like the muscle release. For me this can feel like a long time where nothing is happening except this ball is painfully jabbing into my muscles, and then out of nowhere the muscles just melt all at once. Stay in the same place until you feel that change happen.</li>
<li data-list-kind="ordered">To increase the intensity and get even more relief, keep the pressure point in the same place, but start moving the muscle or limbs around it through their normal range of motion while still holding the pressure point in the same place. To understand what i mean, watch <a href="https://youtu.be/9u8_6RdBdL8?feature=shared&amp;t=167" data-lasso-id="436">Sam in this tricep smash video</a> and how he moves his whole arm while keeping the ball in place.</li>
</ol>
<p>This smashing technique was the essential ingredient in finally releasing extreme tightness I developed in my pecs, from bench press-type exercises, and in my shoulder blades, from a severe muscle imbalance. More on that below.</p>
<p><strong>Train your nervous system, don’t just stretch. </strong>It’s easy to think of your muscles as just some kind of rubber band and flexibility is just a matter of mechanically stretching them out so they get looser. But that’s not how it works. Most of the time your body is already physically capable of achieving the position, but you need to train your nervous system that it’s safe for it to relax into that position. So it’s not very productive to just contort your body, grit your teeth and sweat for two minutes while trying to watch TV. You will definitely get some benefit from this, but the far better approach is to consciously focus on steady breathing and mindfully relaxing the muscles one by one.</p>
<p>This kind of mindful breath-focused stretching has been essential for my lower body hip and hamstring mobility in particular.</p>
<p><strong>Persistent tightness is as much a strength issue as a flexibility one. </strong>If you’ve figured out how to smash a muscle and stretch it and it still keeps getting tight, it’s probably a strength imbalance issue. Other muscles around it may be overdeveloped and are essentially pulling it out of balance all the time. This will feel a little counter-productive, because strength training a muscle will often make it feel tighter in the short term. But over the long-term a combination of smashing, stretching, and strengthening is often the right balance.</p>
<h2 data-pm-slice="1 2 []">Mobility tools that actually work</h2>
<p>I was in so much pain that I bought every mobility gizmo I could find. My home workout area looked like a medieval torture chamber. Most of those things are gathering dust now. Here is the short list of, mostly very cheap, mobility tools that actually work:</p>
<ul>
<li data-list-kind="bullet"><a href="https://www.amazon.com/Kieba-Massage-Lacrosse-Myofascial-Therapy/dp/B017V7UKW2/ref=sr_1_6?crid=20HJHQ65PYQZU&amp;keywords=lacrosse%20ball&amp;qid=1707073055&amp;sprefix=lacrosse%2Caps%2C280&amp;sr=8-6&amp;tag=tyletrin-20&amp;tag=tyletrin-20" data-lasso-id="457">Lacrosse ball</a>: this is your go to smashing tool.</li>
<li data-list-kind="bullet"><a href="https://www.amazon.com/Thrival-Bullseye-Adjustable-Massage-Release/dp/B0B54373DG/ref=pd_ci_mcx_mh_mcx_views_0?pd_rd_w=BP8zC&amp;content-id=amzn1.sym.225b4624-972d-4629-9040-f1bf9923dd95%3Aamzn1.symc.40e6a10e-cbc4-4fa5-81e3-4435ff64d03b&amp;pf_rd_p=225b4624-972d-4629-9040-f1bf9923dd95&amp;pf_rd_r=7PQJ3S2KRGNXND48GC3R&amp;pd_rd_wg=uPrY6&amp;pd_rd_r=b4aeff7b-b4f0-4eed-a220-1cf684f6303e&amp;pd_rd_i=B0B54373DG&amp;tag=tyletrin-20&amp;tag=tyletrin-20" data-lasso-id="458">Psoas tool</a>: This is the nuclear option for smashing. It’s designed to target deep muscles like the psoas but I also found it incredibly helpful for releasing muscles in my shoulders and back.</li>
<li data-list-kind="bullet"><a href="https://www.amazon.com/WIKDAY-Resistance-Exercise-Stretching-Crossfit/dp/B08C7K77PS/ref=sr_1_1_sspa?crid=3CENYXTSV4HDL&amp;keywords=resistance%20bands&amp;qid=1707073084&amp;sprefix=resistance%20ban%2Caps%2C277&amp;sr=8-1-spons&amp;sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&amp;psc=1&amp;tag=tyletrin-20&amp;tag=tyletrin-20" data-lasso-id="459">Resistance bands</a>: I used these both for building strength to correct imbalances and as a way to stretch my shoulders. I’d get a pack like this one with a door anchor unless you have something at home to anchor them to.</li>
<li data-list-kind="bullet"><a href="https://www.amazon.com/Trideer-Eco-Friendly-Accessories-Stretching-Workouts/dp/B0B154V8LM/ref=sr_1_1_sspa?crid=3JLWZIJIP6TER&amp;keywords=yoga%20blocks&amp;qid=1707073137&amp;sprefix=yoga%20blocks%2Caps%2C278&amp;sr=8-1-spons&amp;sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&amp;psc=1&amp;tag=tyletrin-20&amp;tag=tyletrin-20" data-lasso-id="460">Yoga blocks</a>: Great for support getting into positions where your flexibility is very limited like a split. Can also be used to increase the intensity of a stretch.</li>
<li data-list-kind="bullet"><a href="https://www.amazon.com/ActiveGear-Large-Yoga-Mat-Comfortable/dp/B08TFBSY6R/ref=sr_1_2_sspa?crid=1NWPF645FE2V8&amp;keywords=large%20square%20yoga%20mat&amp;qid=1707073167&amp;sprefix=large%20squaryoga%20mat%2Caps%2C295&amp;sr=8-2-spons&amp;sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&amp;psc=1&amp;tag=tyletrin-20&amp;tag=tyletrin-20" data-lasso-id="461">Giant yoga mat</a>: I found it really helpful to have a large surface area to move through the positions I needed and this over-sized yoga mat was perfect.</li>
</ul>
<h2 data-pm-slice="1 1 []">Recommended Internet Rabbit Holes</h2>
<p>Here are some of my favorite resources for learning more in general and finding specific suggestions for how to diagnose and target individual mobility issues.</p>
<p><strong>Sam Martin, Move Better Project (YouTube):</strong>&nbsp;<a href="https://www.youtube.com/@MoveBetterProject/videos" data-lasso-id="437">Sam’s YouTube page</a> is filled with tons of great short videos for specific exercises. I also booked Sam for a virtual consultation which was one of the highest value things I did all year.</p>
<p><strong>Smashwerx (YouTube)</strong>: This was my favorite of the channels targeting more hardcore power lifters. In particular <a href="https://www.youtube.com/watch?v=tChmqOdSOas" data-lasso-id="462">this video</a>, involving dangling a kettlebell while you smash your pec into a bar, was the only thing I found that finally unglued my pec muscles.</p>
<p data-pm-slice="1 1 []"><strong>Strength Side (YouTube)</strong>: This is a fantastic all around strength and fitness channel and they have a bunch of good mobility routines, <a href="https://youtu.be/W5LvKNElKH4" data-lasso-id="463">like this lower body one</a>, to easily incorporate into your day to day.</p>
<p><strong>Instagram</strong>: My two favorite follows right now are <a href="https://www.instagram.com/kruseelite" data-lasso-id="464">@kruseelite</a> and <a href="https://www.instagram.com/twstraining/?hl=en" data-lasso-id="465">@twstraining</a>.</p>
<p>Lastly, despite all these resources, I never found a video of anything that could release the tightness in my shoulder blade that I just could not get rid of. I discovered this nuclear option muscle release using the psoas tool. <a href="https://x.com/tylertringas/status/1754222793992925464?s=20" data-lasso-id="466">Watch the video here</a>.</p>

 <div data-ck-version="6">     <h3>Subscribe to future posts</h3>                 <!--  Form starts here  -->        </div>   									
																	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Infrastructure set-up & open-source scripts to train a 70B model from bare metal (248 pts)]]></title>
            <link>https://imbue.com/research/70b-infrastructure/</link>
            <guid>40816158</guid>
            <pubDate>Thu, 27 Jun 2024 23:08:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://imbue.com/research/70b-infrastructure/">https://imbue.com/research/70b-infrastructure/</a>, See on <a href="https://news.ycombinator.com/item?id=40816158">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main><p><em>We would like to thank Voltage Park, Dell, H5, and NVIDIA for their invaluable partnership and help with setting up our cluster. A special thanks to Ozan, Melissa, Drew, Michael, and David at Voltage Park for their dedicated support throughout the project. Setting up a cluster of this size is an enormous challenge and we couldn’t have done it without them.</em></p>
<h2 id="introduction">Introduction</h2>
<p>In the span of a few months, with a small team of researchers and engineers, we trained a 70B parameter model from scratch on our own infrastructure that outperformed zero-shot GPT-4o on reasoning-related tasks.</p>
<p>Today, we’re sharing an end-to-end guide for setting up the required infrastructure: from bringing up the initial cluster and installing the OS, to automatically recovering from errors encountered during training. In each step, we detail the challenges we encountered and how we resolved them. Along with our learnings, we’re releasing many of the infrastructure scripts we developed to ensure healthy hosts, so that other teams can more easily create stable infrastructure for their own model training.</p>
<p>Along with our detailed process, we are releasing:</p>
<ul>
<li><a href="https://github.com/imbue-ai/cluster-health/tree/master/health_checks">Host-level health checks</a>: scripts to ensure that a given host is free from known errors</li>
<li>An NVIDIA Collective Communication Library (NCCL) patch that improves logging around errors and stalls</li>
<li><a href="https://github.com/imbue-ai/cluster-health/tree/master/gpu_stress_test">A stress test</a> to confirm that GPUs are able to allocate large tensors and perform standard operations</li>
<li><a href="https://github.com/imbue-ai/cluster-health/tree/master/host_validation">Networking tests</a> to check that the GPUs on a given machine are able to communicate with each other (via NVLink) and with GPUs on other machines (via InfiniBand)</li>
<li>A <a href="https://github.com/imbue-ai/cluster-health/tree/master/ufm_events">script which parses the Unified Fabric Manager (UFM) event log</a>, checks for relevant events, and determines which network ports should be disabled</li>
<li>A <a href="https://github.com/imbue-ai/cluster-health/tree/master/ib_burn">script which generates a comprehensive burn-in workload for InfiniBand fabrics</a>, aiming to exercise every available link</li>
</ul>
<p>Throughout the process, members of our engineering team worked with our partners at Voltage Park to prepare the cluster for production use. The full process involved:</p>
<ol>
<li>Provisioning individual machines</li>
<li>Provisioning InfiniBand</li>
<li>Ensuring fully healthy machines</li>
<li>Diagnosing common training issues</li>
<li>Improving infrastructure tooling</li>
</ol>
<p>Each of these steps is described in greater detail below.</p>
<h2 id="background-how-this-is-supposed-to-work">Background: How this is supposed to work</h2>
<p>The purpose of our compute was to enable rapid experimentation with large-scale language models. To do this, we needed a large number of fast GPUs that could communicate with each other at high speeds.</p>
<p>This post focuses on one cluster that had 4,092 H100 GPUs spread across 511 computers, with eight GPUs to a computer. There were 511 computers with GPUs because some connections needed to be reserved for the Unified Fabric Manager nodes, which managed the InfiniBand network. On the 511 hosts with GPUs, each GPU was directly connected to a ConnectX-7 card that could simultaneously transmit and receive at 400 Gbps to any other GPU on the InfiniBand network through its own ConnectX-7 card.</p>
<p>Our InfiniBand network topology was called “fully non-blocking” because every GPU could, in theory, simultaneously talk to another GPU at the maximum rate. This was enabled by a three-tier InfiniBand network architecture: the three levels of InfiniBand switches, when properly connected, enabled this high level of throughput over the entire network. See below for an overview of the InfiniBand network:</p>
<a href="https://imbue.com/70b-blog/cluster-topology.png" target="_blank"><div><p><img src="https://imbue.com/70b-blog/cluster-topology.png" alt=""></p><p>Click to enlarge.</p></div></a>
<p>Note that the communication for training networks happened over InfiniBand, not over Ethernet. While the machines were also connected to an Ethernet network, that network was used to transfer datasets, checkpoints, and other data. It would have been far slower to send data over Ethernet because data would first travel from the GPU to the CPU, and then out one of the 100 Gbps Ethernet cards. While it would have been possible to train over Ethernet using a technique called <a href="https://docs.nvidia.com/networking/display/MLNXOFEDv23070512/RDMA+over+Converged+Ethernet+(RoCE)">RDMA over Converged Ethernet</a> (RoCE), that would require a significant amount of additional work on both the hardware and software side, and would generally be less reliable than InfiniBand (see <a href="https://arxiv.org/pdf/2402.15627">this paper</a> that outlines the extensive process).</p>
<p>There was also a secondary Ethernet network used purely for configuration and management, enabling access to the controller interface for the basic input/output system (BIOS), power supplies, and other low level machine interfaces. Without this management network, we would have had to manually set up our nodes with a USB drive, keyboard, and monitor, which would not have been not a sustainable approach for hundreds of machines.</p>
<p>Using our cluster for high performance training meant that every component — InfiniBand, Ethernet, GPUs, and the nodes themselves — had to work near perfectly. If even a single one of the over 12,000 connections was a little flaky, it could slow down the entire training run.</p>
<p>The rest of this post details the process of actually getting to a point where everything works perfectly, and ensuring that it stays that way.</p>

<h2 id="provisioning-individual-machines">Provisioning individual machines</h2>
<p>After establishing an initial Ethernet connection to the cluster via the management network, we obtained access credentials to the baseboard management controller (BMC). The BMC is a specialized service processor that remotely monitors a host system and is typically wired to a separate network. It allowed us to interact with every machine as if we were physically present and provided additional APIs for hardware health, BIOS setting, and power management.</p>
<p>With these components in place, we rolled up our sleeves and began setting up the cluster.</p>
<h3 id="step-0-getting-one-machine-provisioned">Step 0: Getting one machine provisioned</h3>
<p>We first used iDRAC (Dell’s baseboard management controller) to install Ubuntu 22.04 on a single server, which would be used to set up everything else. Among other things, iDRAC allowed us to mount and boot off of an ISO image from a local computer, with a virtual console provided in the browser. This would ideally be the only manual installation in this process.</p>
<h3 id="step-1-installing-an-os-on-every-machine">Step 1: Installing an OS on every machine</h3>
<p>With patient zero taken care of, we proceeded to install Ubuntu’s Metal-as-a-Service (MAAS) software to help provision the remaining servers. With the Preboot eXecution Environment protocol (PXE) booting and automated iDRAC tools, we instructed every machine to boot off the network and configured MAAS to respond to the PXE boot requests. When performing the initial network boot, servers received an IP from MAAS via a dynamic IP allocation protocol (DHCP) and an initial kernel without needing to have anything installed on the local drives. This bare-bones environment was automatically used to perform a persistent OS installation. In theory, we would wait for the first boot, and everything would be taken care of. In practice, however, the MAAS integration with BMC was not reliable, so we used the iDRAC API to collect the MAC address (a unique physical hardware identifier) for every machine in advance.</p>
<p>Throughout the training process, MAAS was a generally reliable component of the stack. However, we experienced some hiccups at the beginning that were fairly specific to our setup. For instance, during the first few provisions, the clocks were so far off that HTTPS certificate validation issues prevented anything from being installed via apt. Relatedly, because the MAAS server had to take care of so many responsibilities (DHCP server, DNS server for hostname to IP resolution, HTTP proxy between hosts and official Ubuntu package servers, NTP server, cloud-init configuration management, and ground truth database for connecting MAC addresses to IPs to hostnames to custom metadata), we had difficulty tracking issues to the root cause component. Adding to this was the learning curve around the MAAS provisioning lifecycle, as it was designed to handle the complexity of managing greenfield deployments as well as gradual migration of nodes and various debugging/unhealthy intermediate states.</p>
<h3 id="step-2-diagnosing-broken-machines">Step 2: Diagnosing broken machines</h3>
<p>As is typical in setting up large GPU clusters, we found that about 10% of the machines failed to boot, mostly due to physical issues with the servers. Some issues we encountered included: unconnected or miswired Ethernet cables, hardware issues in iDRAC, broken power supply units, bad NVME (nonvolatile memory express) drives, missing internal wires, and network cards or GPUs failing to show up. We automated checks for these issues, passed some machines back to Dell for re-testing, and filed appropriate tickets for the data center staff. An advantage of taking the cluster setup into our own hands was that we were immediately able to put the healthy machines to use while awaiting maintenance on the others.</p>
<h3 id="step-3-minimal-viable-observable-metal">Step 3: Minimal <del>viable</del> observable metal</h3>
<p>We proceeded to set up the following on every server:</p>
<ol>
<li>Docker (for more easily running services and training jobs)</li>
<li>Data center GPU drivers</li>
<li>Prometheus node exporter (for exporting a steady stream of hardware / OS metric)</li>
<li>DCGM exporter (additional metrics from NVIDIA for GPU state / clocks / utilization)</li>
<li>RAIDZ ZFS pool on all the non-OS drives (this enables machines to survive one drive being down, and also gives transparent compression for free, which is particularly helpful for plain-text datasets and repetitive logs, routinely enabling us to use about 10 times more space than we would have otherwise been able to)</li>
</ol>
<p>We then ran basic GPU diagnostics to determine whether the GPUs were mostly functional — those that weren’t would typically experience hardware issues within a couple hours.</p>
<p>During this time we ran into some bandwidth bottlenecks when we tried to install software packages across all 400 nodes in parallel. This was also the first time we started to receive heat alerts for high temperatures on various components in the data center deployment. This first batch of heat issues was mostly remediated via firmware updates.</p>
<h3 id="step-4-single-node-gpu-training">Step 4: Single-node GPU training</h3>
<p>The next step was ensuring that every machine could handle real GPU workloads in isolation. Many couldn’t, due to a number of issues:</p>
<ul>
<li>GPU-related errors, which were mostly fixed by reseating the cards in their slots: physically sliding out the 200-pound server from the rack, removing all the cables in between the cover and the GPUs, then taking the GPUs out and putting them in again before replacing all the cables and reracking the server.</li>
<li>A number of the wires between the GPUs and the Peripheral Component Interconnect Express (PCIe) buses or network cards reported <em>“limited width: x4 &lt; x16”</em> according to Ubuntu server logs. After updating the PCIe switch bus firmware, we found that the internal PCIe cables needed to be reseated for around a quarter of the hosts in the cluster — presumably because the rather fragile cables were situated between the casing and the GPUs, which meant that they would be jostled or unplugged any time anyone wanted to do maintenance on the GPUs.</li>
<li>A number of miscellaneous failures affected single-digit hosts. Dell helped us fix these via firmware upgrades:<!-- -->
<ul>
<li>NVMe drives didn’t show up as faulty, but locked up the entire machine when touched.</li>
<li>Hard drives showing up in random order under Linux, which confused MAAS and caused the OS to get installed on the wrong drive.</li>
<li>Wrong temperature readings, which caused fans to spin up to 100% all the time. This was caused in part by having a bad NVIDIA driver, which we resolved by downgrading to the previous driver version.</li>
<li>Dynamic frequency scaling for CPUs going haywire, limiting to 2 GHz on the active cores.</li>
<li>Direct GPU-GPU communication (GDR, or GPUDirect RDMA Peer Memory Client) was impossible to apply successfully.</li>
</ul>
</li>
</ul>
<h2 id="provisioning-infiniband">Provisioning InfiniBand</h2>
<h3 id="step-0-installing-the-ufm">Step 0: Installing the UFM</h3>
<p>One advantage of InfiniBand was its centralized design, as it has one brain for the entire network. Therefore, we only had to deal with one entity for the 320 network switches in the fabric. Our first task was to figure out which switch connected to which machines, then to correlate that with the wiring diagram and rename the switches by their physical location.</p>
<h3 id="step-1-time-for-rewiring">Step 1: Time for rewiring</h3>
<p>Initially, the UFM couldn’t detect the 320 network switches, let alone all of the hosts expected to be present on the fabric. After conferring with our data center partners, we confirmed that the switches were powered on and wired in, yet remained undetectable. Upon examining the network wiring list, we noticed that the top level of the fabric was misdesigned: instead of a single unified fabric, we had eight disjointed networks with no common routing paths. After rewiring the connections, we added checks to verify that all the physical connections lined up with the new design.</p>
<h3 id="step-2-ten-thousand-temperature-alerts">Step 2: Ten thousand temperature alerts</h3>
<p>After resolving the physical wiring issues, the UFM successfully established contact with all InfiniBand switches in the fabric. However, almost every switch port began reporting excessively high temperatures, sometimes exceeding 70 degrees Celsius, even though they weren’t transmitting data yet. We discovered that the issue stemmed from open spaces between switches in the same networking racks, which caused hot air to recirculate back to the front. Our data center partners helped us diagnose the issue quickly and develop a suitable workaround.</p>
<h3 id="step-3-eighteen-hundred-alerts">Step 3: Eighteen hundred alerts</h3>
<p>Many ports also exhibited high error rates or fluctuated between working and broken states, known as “flapping.” These issues only surfaced when the ports were actively used, so preemptive detection proved challenging, as the entire fabric consisted of 10,000 links with a high degree of redundancy. Our data center partners helped clean and reseat alerting ports, and we disabled the remaining alerting transceivers while waiting for replacements.</p>
<p>Although InfiniBand is highly resilient to hardware failures, once about 10% of the fabric began experiencing issues, features like adaptive routing couldn’t function reliably enough to work around haphazardly dropping links.</p>
<p>During this period, we managed to conduct multi-node training runs with 100 to 200 machines. Our process was largely improvised: we sometimes launched on a random set of nodes, observed their performance, and tried to stay running on as many of those nodes as possible. This approach allowed us to find a reliable subset of the InfiniBand fabric, but proved tricky because every time we would change the set of nodes used for training, the default set of InfiniBand links would change.</p>
<h3 id="step-4-burn-infiniband-burn-disco-inferno">Step 4: Burn InfiniBand burn, disco inferno</h3>
<p>To diagnose InfiniBand issues more efficiently, we devised a specialized workload for the entire cluster that focused on simultaneously pushing as much data as possible through every port on the entire fabric. This was not the same as running one large all-reduce workload across the cluster, which would utilize NCCL to optimize communication within individual nodes by having GPUs communicate using NVLink via Server PCIe Module (SXM) sockets.</p>
<p>Instead, we opted for a brute force approach, and succeeded handily. The UFM began sending alerts about data transmission exceeding 97% of the theoretical capacity through most ports, and some switches temporarily crashed. Every port left standing at the end of the day was deemed robust enough to proceed, and the rest were disabled or handed off for later repair.</p>
<h3 id="step-5-gpudirect-rdma">Step 5: GPUDirect RDMA</h3>
<p>For the GPUs to communicate without incurring CPU overhead, we enabled a feature called GPUDirect RDMA, which allowed direct communication with the InfiniBand network cards. This involved two key steps:</p>
<ol>
<li><a href="https://download.nvidia.com/XFree86/Linux-x86_64/535.183.01/README/nvidia-peermem.html">Enabling an extra kernel module</a></li>
<li><a href="https://docs.nvidia.com/deeplearning/nccl/archives/nccl_2114/user-guide/docs/troubleshooting.html#pci-access-control-services-acs">Ensuring PCIe Access Control Service (ACS) was disabled to prevent immediate hangs</a></li>
</ol>
<h3 id="step-6-grow-the-golden-set-of-servers">Step 6: Grow the golden set of servers</h3>
<p>A rule of thumb for GPU clusters using the newest hardware: expect about 3% of machines to break every week.</p>
<p>There is a crucial nuance that often gets lost, however: it’s not that every machine has a uniform 3% chance of failing; rather, a small number of malcontent machines repeatedly break in different ways until they’re properly fixed. This highlighted the advantage of having a large number of machines on the same fabric. Instead of playing whack-a-mole on our large training run with random machines, we could instead focus on growing a set of known reliable, or “golden,” machines.</p>
<h3 id="step-7-maintenance">Step 7: Maintenance</h3>
<p>InfiniBand maintenance mostly involved responding to UFM alerts, swapping failing cables and transceivers, and occasionally diagnosing more difficult errors, such as faulty switches. Large scale regressions were usually caused by two factors:</p>
<ol>
<li>Firmware upgrades, especially if applied to only half of the cluster, which could corrupt the UFM state and necessitate UFM restarts on all of the InfiniBand switches.</li>
<li>Mass restarts of GPU boxes at the same time, which could flood the UFM state with updates and similarly necessitate a UFM service restart.</li>
</ol>
<h2 id="ensuring-fully-healthy-machines">Ensuring fully healthy machines</h2>
<p>Throughout this process, we discovered numerous ways individual machines could fail or slow down training runs. Many of these failure modes were not immediately obvious, so we wrote a number of health checks to determine which hosts were healthy enough to use for training. We have released the code for these <a href="https://github.com/imbue-ai/cluster-health">here</a>.</p>
<p>Note that many of these health checks are specific to our runtime environments, are not necessarily related to the base hardware, or are extremely trivial to fix or automate. This is by design: for the overall goal of making our machines ready for training, we wanted a single entrypoint that would answer <em>“yes”</em> or <em>“no,”</em> and would abstract over any number of “trivial” details.</p>
<h3 id="gpu-health-check">GPU Health Check</h3>
<p>We checked that we had the correct number of GPUs, that ECC (error correction code) checking was enabled, and that there were no ECC errors. We also checked that the NVLink topology, which connects the GPUs to each other, was up and error-free.</p>
<h3 id="disk-space-health-check">Disk Space Health Check</h3>
<p>We checked that hosts had no more than 95% disk space utilization.</p>
<h3 id="docker-health-check">Docker Health Check</h3>
<p>We checked that Docker was able to run a container with GPUs attached (i.e. NVIDIA Container Runtime was working properly), and that all Docker containers relevant for monitoring/profiling were active and given the correct host permissions.</p>
<h3 id="dmesg-health-check">Dmesg Health Check</h3>
<p>We checked that there were no hardware Xids or SXid errors (faults thrown by the NVIDIA GPUs or inter-GPU NVIDIA switches) in dmesg. We also read out all dmesg log lines to verify that they were all categorizable in a list of <em>“usual/expected log lines”</em>.</p>
<h3 id="idrac-health-check">iDRAC Health Check</h3>
<p>We checked for iDRAC errors on the machine, ignoring non-fatal error messages. This is specific to our Dell machines and is not part of the health checks that we are open sourcing.</p>
<h3 id="disk-health-check">Disk Health Check</h3>
<p>We checked that zpool was mounted, Docker was connected to it properly, and that it could actually be touched without the CPU locking up.</p>
<h3 id="infiniband-health-check">InfiniBand Health Check</h3>
<p>We checked for the presence of increased InfiniBand error rates and/or outdated driver firmware.</p>
<h3 id="nvlink-health-check">Nvlink Health Check</h3>
<p>We checked for NVLink errors on the machine. Empirically, this didn’t seem to result in training failures, but it may result in slowdowns.</p>
<h3 id="gdr-health-check">GDR Health Check</h3>
<p>We checked that GDR was enabled on the machine.</p>
<h3 id="vbios-health-check">VBIOS Health Check</h3>
<p>We checked that the VBIOS version of the GPUs, as well as the H100 baseboard firmware were up to date.</p>
<h3 id="flint-health-check">Flint Health Check</h3>
<p>We used flint and <code>hca_self_test</code> to check that we have the right version of the Mellanox OFED driver, card firmware, and transceiver firmware, and that they were compiled correctly against the NVIDIA drivers.</p>
<h3 id="psb-health-check">PSB Health Check</h3>
<p>We queried PCIe devices to check that the speed and width of connections is what we expected between the GPUs, the PSB (PCIe Switch Bus), and the network cards. We also checked that the switch firmware was on the current version. This script was developed by Dell and not Imbue, so we are currently unable to share it.</p>
<p>In addition to these quick health checks, we also had a couple of more involved health checks that included:</p>
<ul>
<li>Initializing a matrix computation via PyTorch and measuring the NVLink bandwidth and GPU computation speed and memory. We set the proper GDR flags to test both InfiniBand and NVLink.</li>
<li>Using <code>ib_write_bw</code> with <code>–use_cuda</code> to send data over the IB cards and measure the PCIe and InfiniBand card bandwidth. We ran this for a longer period (~15 mins) to ensure we caught flappy InfiniBand links.</li>
<li>Running a multinode diagnostic run to check NCCL initialization ability and whether it would randomly stall. If it stalled, our forked NCCL code added additional logging. This could take 12 to 24 hours to detect issues, so we typically only ran this for new nodes, or when we suspected an issue.</li>
<li>Checking the DCGM exports for any GPU clock throttle events (excluding the expected <code>gpu_idle</code> and <code>power_cap</code>). Multinode training that exercised all GPUs, InfiniBand cards, and CPU and disk simultaneously was the best way to exercise these power events.</li>
</ul>
<h2 id="diagnosing-common-training-issues">Diagnosing common training issues</h2>
<p>Once the hardware began working properly, it was time to begin training.</p>
<p>In this section, we share some concrete debugging steps and insights revealed through our experience running large language model training jobs on our cluster.</p>
<h3 id="crashing-on-startup">Crashing on startup</h3>
<p>In some ways, this was the best error to encounter, because it would (theoretically) be easy to reproduce and iterate on.</p>
<p>We first checked whether we were running our code on the correct version, configurations, and environment variables. While basic, we found that it was critical to ensure that launching training was reproducible and easily inspectable, especially since intermediate abstractions like Docker image caching or opaque secrets configurations could muddy the waters.</p>
<p>Another basic check conducted was ensuring all our machines were online, and that the emitted stack traces or logs could be easily aggregated and inspected. We used a Loki, Prometheus, and Grafana stack, but any suitable log aggregation or tracing SaaS would be appropriate. Due to the synchronous, distributed nature of these runs, often the first error to trigger would cause a cascade of unrelated errors. Here, health checks also helped instantly detect obvious issues such as broken hard drives or missing or invalid GPUs.</p>
<p>We built a system to automatically relaunch on failure, which made log and error aggregation even more important to avoid mixing up errors from different relaunches. Some common errors we encountered included:</p>
<ol>
<li>Errors like <code>Forward order differs across ranks: rank 0 is all-gathering 43 parameters while rank 1228 is all-gathering 1 parameters</code>. We found that this was a quirk of PyTorch Fully Sharded Data Parallel (FSDP) implementation that could be resolved by a relaunch.</li>
<li>GPU Out of Memory (OOM) Errors, that looked like <code>CUDA out of memory. Tried to allocate …</code> We fixed these by by double-checking our configurations and code, and backing out any recent code changes that might have caused extra utilization of GPU#0 due to improper PyTorch device specification during startup.</li>
<li>CPU/RAM OOM Errors, which were less easily spotted from error logs, and were typically best detected via dmesg logs from the host outside Docker containers. We saw them mostly as <code>CalledProcessError</code> or <code>ConnectionError</code>, when a forked process or network peer was reaped by OOM Killer invocation. We preferred to just fail health checks and restart the box when an OOM Killer invocation was detected from dmesg. We also checked our code path had a sufficient amount of manual garbage collection (see below sections on how to disable it), and wasn’t accidentally trying to do computations or move tensors on to the CPU.</li>
</ol>
<h3 id="crashing-in-the-middle-of-training">Crashing in the middle of training</h3>
<p>The first order of business was to automate systems that would rerun all diagnostic health checks (see previous sections), then auto-restart the run without unhealthy hosts. We encountered a few random hardware faults, including Xid and SXid errors which could crash the run without emitting meaningful Python stack traces. Some instances, like row remapping, were recoverable by a restart. Others, like uncorrectable ECC errors, often needed hardware maintenance or replacement parts.</p>
<p>In addition, we observed crashes caused by particularly malformed training data. For instance, a very large single document in the corpus could cause OOM errors in either the GPU or CPU. To prevent these, we had a fully deterministic data loader, which made every crash easily reproducible via correlation with the epoch or step number. We found it helpful to disable dataloading or substitute fake data (such as all zeroes) to confirm whether the data was truly the root cause.</p>
<p>Finally, it was also helpful to record network and general node health statistics via any preferred method of metrics aggregation. Issues like Ethernet briefly cutting out or running out of disk space may not show up as helpful error messages, but could be easily correlated with collected data.</p>
<h3 id="hanging-with-no-stacktrace-information-possibly-followed-by-a-timeout">Hanging with no stacktrace information (possibly followed by a timeout)</h3>
<p>These types of errors were extremely frustrating to debug, due to the lack of helpful information and the fact that they were often difficult to reliably reproduce.</p>
<p>The most memorable type was characterized by error messages like</p>
<p><code>Watchdog caught collective operation timeout: WorkNCCL(SeqNum=408951, OpType=_ALLGATHER_BASE, … , Timeout(ms)=600000) ran for 600351 milliseconds before timing out</code></p>
<p>simultaneously appearing across all GPU workers in the training run.</p>
<p>What this meant was that one or more of the hosts failed to complete a NCCL operation, or even crashed out of the NCCL and InfiniBand connection, causing all other hosts to block synchronously on the particular tensor op until the <code>NCCL_TIMEOUT</code> was reached. Unfortunately, the nature of the NCCL library made it incredibly difficult to find which particular host(s) was the culprit.</p>
<p>We made some logging changes to the NCCL library (see our fork <a href="https://github.com/boweiliu/nccl">here</a>) to better surface which messages or operations were in-flight when the crash happened, and thereby identify which was the host or GPU that seemed to prevent the runs.</p>
<p>Note that in order to identify misbehaving hosts, we often needed to figure out which hosts did not produce certain log messages. The lack of such messages indicated that the workers on that host were stragglers or had crashed.</p>
<p>Other instances of unresponsiveness without helpful error messages could typically be associated with hardware-related issues, such as the aforementioned Xid/SXid/ECC errors causing the NVIDIA driver or NVIDIA docker communication driver to lock up. To distinguish the NCCL hangs from driver hangs and from a race condition or deadlock in Python code, we used tools including Py-Spy and GNU Project Debugger (GDB) to live-debug stalled processes wherever we encountered them. Using this method, we were able to catch one particular issue where, due to a misconfiguration in the Python threading settings, we were unable to launch the eight multi-threaded NCCL GPU processes properly on certain hosts which hit a race condition during pre-PyTorch initialization code.</p>
<h3 id="training-slowdowns-as-measured-by-mfu">Training slowdowns (as measured by MFU)</h3>
<p>Lack of instrumentation could make these types of issues even more frustrating than the previous category. In addition to breaking out Py-Spy, stack trace inspection, and GDB, we also spun up NVIDIA Nsight and profiling tools to help, some of which were difficult to work with in a highly distributed setup.</p>
<p>Sadly, generic slowdowns or lower-than-previously-demonstrated model flops utilization (MFU) could be caused by a variety of reasons.</p>
<p>First, it proved helpful to double-check configurations, code, and environment variables. We experienced running the wrong model, the wrong batch size, the wrong UFM or NCCL settings, the wrong <code>CUDA_DEVICE_MAX_CONNECTIONS</code>, which all caused suboptimal performance.</p>
<p>We also found it useful to measure instantaneous (i.e. per-batch) MFU rather than a smoothed or windowed average, as the pre-smoothed shape of the MFU curve often helped us diagnose the class of issue. Issues included:</p>
<h4 id="training-immediately-started-off-at-extremely-low-mfu-less-than-110th-of-expected-and-remained-stable">Training immediately started off at extremely low MFU (less than 1/10th of expected) and remained stable</h4>
<p>This was most often a hardware issue with the InfiniBand networking, such as a dead switch at the T2 or T3 layer. It could also be caused by hardware issues between the GPU and the NIC, showing up in dmesg as <code>PCIe x16 lanes limited by …</code></p>
<h4 id="training-immediately-started-off-at-30-of-expected-mfu-and-remained-stable">Training immediately started off at 30% of expected MFU and remained stable</h4>
<p>This could be caused by one host with improperly set GDR (NVIDIA Peer Memory), or incorrect GDR environment variables.</p>
<h4 id="training-immediately-started-off-at-60-80-of-expected-mfu-and-remained-stable">Training immediately started off at ~60-80% of expected MFU and remained stable</h4>
<p>Most commonly, this was caused by degraded or faulty InfiniBand links, especially if a single particular GPU had a faulty associated InfiniBand NIC, causing NCCL to try to route the traffic over local NVLink and use the NIC on another GPU on the same host. It could also be caused by CPU throttling, which required tweaking some BIOS settings for particular hosts.</p>
<h4 id="sudden-drastic-dips-by-10x-for-single-batches-that-occurred-regularly">Sudden drastic dips (by 10x) for single batches that occurred regularly</h4>
<p>This was almost certainly related to checkpointing or evaluations — verifiable by checking against epoch or step counts. Annoyingly, this causes many false positives if automated alerting is set just to trigger off of MFU anomalies.</p>
<h4 id="sudden-drastic-dips-by-10x-for-single-batches-that-occurred-randomly-and-rather-rarely-on-the-order-of-every-15-minutes-and-achieved-full-recovery-to-good-mfu-immediately-afterward">Sudden drastic dips (by 10x) for single batches that occurred randomly and rather rarely (on the order of every 15 minutes), and achieved full recovery to good MFU immediately afterward</h4>
<p>This seemed to be most commonly caused by other CPU-heavy workloads scheduled on one of the hosts in the run. Rather than build profiling tooling to identify the particular host, we found it easier to crudely monitor CPU usage by PID. This could also be attributable to sporadically poor networking, such as dataloader bottlenecks. We used metrics monitoring and added Python code timing logs for the dataloading, checkpoints, and any non-NCCL code, which proved quite reliable.</p>
<h4 id="mfu-graph-gradually-sagged-downward-over-the-course-of-a-run-but-returned-to-100-upon-any-restart">MFU graph gradually sagged downward over the course of a run, but returned to 100% upon any restart</h4>
<p>Theoretically, this would be caused by heat accumulation on the switches, but we never saw that. Instead, we used Python and NVIDIA profilers to determine that the degradation seemed to be the result of automatic garbage collection.</p>
<div><p><img src="https://imbue.com/70b-blog/windowed-mfu.png" alt=""></p></div>
<p>While debugging these slowdowns, we noticed a pattern of periodic dips in throughput that almost appeared deterministic. As the training run progressed, the dips impacted a progressively larger percentage of distributed operations. This led to a hypothesis that the dips could be related to automatic garbage collection, which we validated by profiling and testing. Once we disabled automatic garbage collection and scheduled garbage collection to occur at specific intervals across all hosts, these throughput “sags” disappeared.</p>
<p>We used a synchronous distributed training algorithm, FSDP, which is based on <a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">ZeRO-3</a>. During a blocking operation, a single worker process running garbage collection could slow down every other worker. With hundreds of worker processes, this could result in significant slowdowns.</p>
<h4 id="good-performance-in-the-beginning-then-sudden-dips-to-70-of-expected-that-persisted-at-high-frequency-every-15-seconds">Good performance in the beginning, then sudden dips (to 70% of expected) that persisted at high frequency (every 15 seconds)</h4>
<p>We observed this was correlated with NVIDIA GPU “clock throttle reasons,” which we collected via applying the proper settings to NVIDIA DCGM. Heat issues (GPU temperatures or broken/degraded host cooling fans) or power supply failures caused this. Also, some of our hosts with specific power supply hardware had voltage problems when we maxed out all 8 GPU utilization and 8x NIC InfiniBand utilization and CPU/RAM/disk at the same time, but only when all were being used — typically only during an actual training run.</p>
<h4 id="good-performance-but-a-bit-noisier-than-usual-high-frequency-white-noise-variance-between-90-and-100-of-expected-mfu">Good performance but a bit “noisier” than usual (high-frequency white noise variance between 90% and 100% of expected MFU)</h4>
<p>This was also InfiniBand hardware related, but typically due to moderately degraded or flapping links higher up in the network rather than at the less redundant host to T2 layer.</p>
<p>Unfortunately, many of these issues are not easily pinnable to a particular host, and the InfiniBand-related issues were especially hard to nail down because of the topology-aware nature of the InfiniBand switch technology. InfiniBand seemed to prefer adjacent hosts in the InfiniBand fat-tree design, and the UFM could route packets in ways that would result in asymmetric link speeds.</p>
<p>Here’s a quick summary/flowchart/sanity checklist for debugging throughput regressions:</p>
<ul>
<li>Did it ever work?</li>
<li>Did you change something recently (e.g. merged code, updated drivers)?</li>
<li>Are you running on healthy hosts? Are all your dependent services running, including third party SaaS, e.g. Docker Hub, GitHub, or whatever else your stack depends on?</li>
<li>Are you sure you ran with the exact same code, environment, configurations, versions, host list, rank order, random seed as the last time (if possible)?</li>
<li>Is it reproducible?</li>
<li>Is it correlated with anything else? Other processes? Daily crontab? Host or DCGM or UFM metrics?</li>
<li>Are your tools to measure metrics correct?</li>
<li>Does the issue still occur when running reduced code (smaller model, faked data, no checkpoint saving or loading)?</li>
</ul>

<p>Upon completing the above steps, one can achieve good performance when training a model…at least until something inevitably breaks.</p>
<p>In this section, we cover a few different tools and systems that we made to ensure that training continued running smoothly, ideally with a minimal amount of human intervention. Because we are a small team, we simply didn’t have enough people to constantly make manual repairs, so we attempted to automate as much of the process as possible.</p>
<p>Almost all of our training run problems could be pinpointed to faulty machines or network components. These failures occur frequently in a large cluster, so it was essential to automate the process of disabling faulty machines and network components and requesting repairs.</p>
<h3 id="faulty-machines">Faulty machines</h3>
<p>We developed a system for automatically relaunching crashed runs from the most recent checkpoint. The relaunch process would begin by running our health checks on every available machine and classifying each machine’s health based on which health checks it passes; it would then attempt to relaunch the training job on the healthiest machines.</p>
<h3 id="faulty-network-components">Faulty network components</h3>
<p>All the network component failures we observed were detected by the UFM and registered in the UFM event log, so responding to network component failures was just a matter of parsing the UFM log and taking the appropriate action for each event.</p>
<p>The UFM events system is quite complicated, containing dozens of event types. In practice, however, we found that only a handful of events were problematic, mostly related to links going down or high symbol error counts. After identifying these events, we were able to write scripts to parse the UFM event log, disable links and ports implicated in recent events, file maintenance tickets on those network components, and re-enable those components once maintenance was finished.</p>
<h3 id="local-mirror-file-system">Local mirror file system</h3>
<p>It became obvious early on that one of the bottlenecks to large distributed training runs would be Ethernet speed into and out of the cluster. A shared Ethernet connection with a bandwidth of about 10Gbit/s would quickly become saturated if hundreds of workers tried to download datasets and model checkpoints simultaneously.</p>
<p>As a result, we decided to build a local file system within our cluster to mirror cloud storage and essentially serve as a cache to reduce the number of files we needed to fetch from S3. To deal with cluster churn (machines would often be disabled or swapped out for maintenance reasons) we did three-fold replication of each file, using <a href="https://en.wikipedia.org/wiki/Consistent_hashing">consistent hashing</a> to distribute load evenly in a way that minimized file movements during churn. Limited disk space on the cluster meant that we also had to develop various tools for keeping track of file lifecycles and clearing out files that were no longer relevant.</p>
<h3 id="local-distributed-docker-registry">Local distributed Docker registry</h3>
<p>We also made use of <a href="https://github.com/uber/kraken">Kraken</a>, a fantastic open source software to enable peer-to-peer transfer of Docker images. We had almost no issues with it, which was a bit surprising given the complexity of both the task and the implementation.</p>
<h3 id="various-performance-monitoring-tools">Various performance-monitoring tools</h3>
<p>We set up the default Torch profiler as well as NVIDIA’s Nsight Systems. The latter was helpful for understanding exactly how long forward/backward passes and NCCL communications take, and in determining if we were bottlenecked by communications or compute for a given model size and worker count. However, Nsight Systems was somewhat difficult to use, because it required running Docker in privileged mode, disabling security checks related to performance monitoring events, and because saving out the profiles often required stopping the entire training process.</p>
<p>In addition, we found it helpful to write tools to detect slow training batches and understand potential causes of slowness. The most useful of these was a tool which monitored how long each batch took and dumped the stack trace of every worker when a batch was unusually slow - this made it easier to identify specific hosts with subtle hardware or software issues.</p>
<h3 id="subdividing-machine-groups-to-pinpoint-faulty-hosts">Subdividing machine groups to pinpoint faulty hosts</h3>
<p>During our first few months using the cluster (when our health checks were not as thorough as they are now), we often ran into a situation where a training run on a specific set of machines was failing but it wasn’t clear which machine was at fault. To pinpoint faulty hosts, we developed tools to make it easy to partition the set of machines into subsets and launch a smaller job on each subset of machines.</p>
<p>For instance, if a job on a group of 48 machines was failing, we would launch smaller runs on six groups of eight machines, and then launch smaller runs on eight groups of six machines. It would often be the case that only a single run would fail on each of these two stages, allowing us to conclude with high confidence that the machine which was part of faulty runs in both stages was problematic.</p>
<h2 id="reflections-and-learnings">Reflections and learnings</h2>
<p>Over the course of setting up and maintaining our infrastructure, we gleaned a few useful learnings on the overall process:</p>
<ul>
<li><strong>Being able to swap out machines for each other is extremely useful.</strong> For any given training run we found it helpful to have 10-20% more machines than necessary for the run, so that we could easily relaunch in case of machine failures. Setting up the cluster networking in such a way that every machine is closely connected with every other machine meant that we could essentially use any working subset of the machines.</li>
<li><strong>It’s worth writing tests and automated solutions for every kind of hardware or software failure you experience</strong>, since every issue encountered during training will reoccur. Similarly, for every opaque error message, it’s worthwhile to write tools to make the error more interpretable.</li>
<li><strong>Reproducibility is the key to good science.</strong> One rule we quickly adopted is “change only one thing at a time,” even for the simplest things.</li>
<li><strong>Trust, but verify.</strong> Whenever we introduced an external tool to the process or onboarded a new person, either externally or internally, we made sure to double-check their claims, especially if subsequent steps depended on those results.</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>Training large language models requires complex infrastructure to even get started. We chose to be heavily involved in the details of the infrastructure set-up both because we believe it is important to fully understand the systems we work with, and because we suspected that it would ultimately prove more efficient. Now, having gone through the full process, we’re very glad we took this approach — it ended up being critical to have full control over our infrastructure and to be able to easily debug problems at every level of abstraction.
While this process required extensive supervision and iteration, it allowed us to deeply understand the underlying procedures, build a series of tools to ensure healthy hosts, learn how to automate systems to ensure continual smooth training, and ultimately create infrastructure that has allowed us to rapidly iterate on the training of cutting edge language models.</p>
<p>This infrastructure process exemplifies our approach to researching and building a robust foundation for AI agents: probing the nitty-gritty details, continually improving upon existing processes, and building useful tools and systems that enable our scrappy team to tackle larger challenges.</p>
<h3 id="if-this-full-stack-approach-resonates-with-you-were-always-hiring">If this full-stack approach resonates with you, we’re always <a href="https://imbue.com/careers">hiring</a>!</h3></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[200 people charged in $2.7B health care fraud crackdown (398 pts)]]></title>
            <link>https://apnews.com/article/justice-department-health-care-fraud-garland-24948b951896d0f265c29ba3fcacf858</link>
            <guid>40815139</guid>
            <pubDate>Thu, 27 Jun 2024 21:09:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/justice-department-health-care-fraud-garland-24948b951896d0f265c29ba3fcacf858">https://apnews.com/article/justice-department-health-care-fraud-garland-24948b951896d0f265c29ba3fcacf858</a>, See on <a href="https://news.ycombinator.com/item?id=40815139">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>WASHINGTON (AP) — Nearly 200 people have been charged in a sweeping nationwide crackdown on health care fraud schemes with false claims topping $2.7 billion, <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/hub/us-department-of-justice">the Justice Department</a></span> said on Thursday. </p><p><span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/hub/merrick-garland">Attorney General Merrick Garland</a></span> announced the charges against doctors, nurse practitioners and others across the U.S. accused of a variety of scams, including a $900 million scheme in Arizona targeting dying patients. </p><p>“It does not matter if you are a trafficker in a drug cartel or a corporate executive or medical professional employed by a health care company,” Garland told reporters. “If you profit from the unlawful distribution of controlled substances, you will be held accountable.” </p><p>In the Arizona case, prosecutors have accused two owners of wound care companies of accepting more than $330 million in kickbacks as part of a scheme to fraudulently bill Medicare for amniotic wound grafts, which are dressings to help heal wounds. </p>
    

<p>Nurse practitioners were pressured to apply the wound grafts to elderly patients who didn’t need them, including people in hospice care, the Justice Department said. Some patients died the day they received the grafts or within days, court papers say. </p>



<p>In less than two years, more than $900 million in bogus claims were submitted to Medicare for grafts that were used on fewer than 500 patients, prosecutors said. </p><p>The owners of the wound care companies, Alexandra Gehrke and Jeffrey King, were arrested this month at the Phoenix airport as they were boarding a flight to London, according to court papers urging a judge to keep them behind bars while they await trial. An attorney for Gehrke declined to comment, and a lawyer for King didn’t immediately respond to an email from The Associated Press.</p>
    
<p>Authorities allege Gehrke and King, who got married this year, knew charges were coming and had been preparing to flee. At their home, authorities found a book titled “How To Disappear: Erase Your Digital Footprint, Leave False Trails, and Vanish Without a Trace,” according to court papers. In one of their bags packed for their flight, there was a book titled “Criminal Law Handbook: Know Your Rights, Survive The System,” the papers say.</p>
    

<p>Gehrke and King lived lavishly off the scheme, prosecutors allege, citing luxury cars, a nearly $6 million home and more than $520,000 in gold bars, coins and jewelry. Officials seized more than $52 million from Gehrke’s personal and business bank accounts after her arrest, prosecutors say. </p><p>In total, 193 people — including 76 doctors, nurse practitioners, and other licensed medical professionals — were charged in a series of separate cases brought over about two weeks in the nationwide health care fraud sweep. Authorities seized more than $230 million in cash, luxury cars and other assets. The Justice Department carries out these sweeping health care fraud efforts periodically to help deter other potential wrongdoers. </p><p>In another scheme targeting Native Americans, phony sober living homes were set up promising addiction treatment. Claims were then submitted for services that were never actually performed, officials said. </p>
    

<p>Another case alleges a scheme in Florida to distribute misbranded HIV drugs. Prosecutors say drugs were bought on the black market and resold to unsuspecting pharmacies, which then provided the medications to patients. </p><p>Some patients were given bottles that contained different drugs than the label showed. One patient ended up unconscious for 24 hours after taking what he was led to believe was his HIV medication but was actually an anti-psychotic drug, prosecutors say. </p><h2>___</h2><p>Follow the AP’s coverage of the U.S. Department of Justice at <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/hub/us-department-of-justice">https://apnews.com/hub/us-department-of-justice</a></span>.</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Python grapples with Apple App Store rejections (238 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/979671/4fb7c1827536d1ae/</link>
            <guid>40815130</guid>
            <pubDate>Thu, 27 Jun 2024 21:09:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/979671/4fb7c1827536d1ae/">https://lwn.net/SubscriberLink/979671/4fb7c1827536d1ae/</a>, See on <a href="https://news.ycombinator.com/item?id=40815130">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>

<h2>[LWN subscriber-only content]</h2>
</p><div>
<blockquote>
<div>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider <a href="https://lwn.net/subscribe/">subscribing to LWN</a>.  Thank you
for visiting LWN.net!
</p></div>
</blockquote>

<p>An upgrade from Python 3.11 to 3.12 has led to the rejection of
some Python apps by Apple's app stores. That led to Eric Froemling submitting a <a href="https://github.com/python/cpython/issues/120522">bug report</a>
against CPython. That, in turn, led to an interesting
discussion among Python developers about how far the project was
willing to go to accommodate app store review processes. Developers
reached a quick consensus, and a solution that may arrive as soon as
Python&nbsp;3.13.</p>

<p>The problem at hand is that Apple's macOS App Store is
automatically rejecting apps that contain the string
"itms-services". That is the URL scheme for apps that
want to ask Apple's iTunes Store to install another
app. Software distributed via Apple's macOS store is
sandboxed, and sandboxed apps are prohibited from using URLs
with the <tt>itms-services</tt> scheme. That string is in the
<tt><a href="https://docs.python.org/3/library/urllib.html">urllib</a></tt>
parser in Python's standard library, though an application may never actually use
the <tt>itms-services</tt> handler.</p>

<p>Of course, Apple did not do anything so
straightforward as to <em>explain</em> this to Froemling. Once
he filed an appeal with Apple about the rejection, Apple finally told him that
<tt>parse.py</tt> and <tt>parse.pyc</tt> were the offending
files. After that, <a href="https://github.com/python/cpython/issues/120522#issuecomment-2169031927">he
said</a>, it was not hard to track down the problem:</p>

<blockquote>
Now in retrospect I'm frustrated I didn't think to run a full text
search for itms-services over Python itself earlier or stumble across
one of the <a href="https://github.com/beeware/briefcase/issues/1655">other
cases</a> of folks hitting this.
</blockquote>

<p>Russell Keith-Magee <a href="https://discuss.python.org/t/handling-incompatibilities-with-app-store-review-processes/56011">started the discussion</a> in the Python Core
Development discussion forum on June&nbsp;17. He wanted to know whether "<q>acceptable to app
stores</q>" should be a design goal for CPython, or if that compliance
should be a problem left to the tools that generate application
bundles for app stores.</p>

<h4>Paranoid and inscrutable</h4>

<p>Keith-Magee noted in his initial question that Apple's review processes were the
most "<q>paranoid and inscrutable</q>" of app-store-review processes,
but that other app stores also had "<q>validation and acceptance processes
that are entirely opaque</q>". One solution might be to obfuscate the
offending string to pass review, but that might "<q>lead to an obfuscation arms race</q>" and
there were no guarantees this would be the last time the project had
to resolve app-validation problems. The other option, he said, was to
consider this to be a distribution problem and leave it to tools like
<a href="https://beeware.org/project/projects/tools/briefcase/">Briefcase</a>,
<a href="https://pypi.org/project/py2app/">py2app</a>, and <a href="https://pypi.org/project/buildozer/">buildozer</a> to
solve. Traditionally, they have had to patch CPython anyway, he said,
because it did not support Android or iOS "out of the box". But that
will change with Python 3.13 when no patching should be required for
those platforms.</p>

<!-- middle-ad -->

<p>Alex Gaynor <a href="https://discuss.python.org/t/handling-incompatibilities-with-app-store-review-processes/56011/3">suggested</a> that the project try a
an approach that Keith-Magee had not put forward inspired by
Gaynor's experience with the <a href="https://cryptography.io/en/latest/">cryptography</a> library.
The project often receives complaints that the library refuses
to parse a certificate that is technically invalid, but was in wide
use. He said that the policy was to accept pull
requests that work around those issues "<q>provided they are small,
localized, and generally aren't too awful</q>". 
But, he added, these patches should only be accepted on the condition
that someone complains to the third party (in this case Apple), and extracts
some kind of commitment that they would do something about it. He
suggested that the workaround be time-limited, to give users a decent
experience "<q>while also not letting large firms simply externalize
their bizarre issues onto OSS projects</q>".</p>

<p>Brandt Bucher <a href="https://discuss.python.org/t/handling-incompatibilities-with-app-store-review-processes/56011/4">wondered</a>
whether obfuscation was even allowed, or if it would be seen as
circumventing the review process. That was a question no one seemed to
have an answer to; and Keith-Magee <a href="https://discuss.python.org/t/handling-incompatibilities-with-app-store-review-processes/56011/6">responded</a> with an&nbsp;8-Ball emoji and
the phrase "<q>ask again later</q>." He added that Gaynor's approach
sounded appealing, but it would be like screaming into the
void. Apple, he said, barely has an appeals process and there is no
channel available to the Python project "<q>to raise a complaint that
we could reasonably believe would result in a change of
policy</q>".</p>

<p>Another approach, <a href="https://discuss.python.org/t/handling-incompatibilities-with-app-store-review-processes/56011/12">suggested</a>
by Alyssa Coghlan, would be to use a JSON configuration file that
<tt>urllib</tt> would read to set up its module level attributes
"<q>rather than hardcoding its knowledge of all the relevant schemes</q>".
That could allow app generators to drop "itms-services"
from the configuration file rather than patching <tt>urllib.py</tt>
directly. Keith-Magee <a href="https://discuss.python.org/t/handling-incompatibilities-with-app-store-review-processes/56011/13">said</a> that could work, but "<q>it strikes me as a
bit of overkill for an edge case</q>" that could be handled by
obfuscation or distribution-level patching.</p>

<p>On June&nbsp;20, Keith-Magee <a href="https://discuss.python.org/t/handling-incompatibilities-with-app-store-review-processes/56011/16">wrote</a>
that he had thought of another approach: adding a build-time option
called "<tt>--with-app-store-patch</tt>" that removes code that is
known to be problematic. He said it would be enabled
by default for the iOS platform, and disabled elsewhere. It could be
used when building an application for macOS, if the developer intended
to distribute that application via the macOS App Store. He suggested
that the option could also accept a path to a file with a patch, to
allow distributors to provide an updated patch if an app store changes
its rules after the maintenance window for a Python release has
closed.</p>

<h4>Let's paint the bikeshed</h4>

<p>Coghlan <a href="https://discuss.python.org/t/handling-incompatibilities-with-app-store-review-processes/56011/20">asked</a>
if it was now time to "<q>paint a config option bikeshed</q>". She
said that the proposed option name was both too broad and too
narrow. The "app-store" component of the name was too broad, because
it could encompass any app store, not only Apple app stores. The
"patch" component was too narrow, because patch specifies the
method of complying with policies rather than intent. There may be
other methods required to comply with app-store-compliance
checks. Keith-Magee liked the suggestion about dropping "patch" from
the option name, and suggested painting the bikeshed a nice shade of
"<tt>--with-app-store-compliance</tt>" that would interact with
platform identification to sort out what is required.</p>

<p>On June&nbsp;25, Keith-Magee <a href="https://discuss.python.org/t/handling-incompatibilities-with-app-store-review-processes/56011/31">thanked</a>
participants in the discussion for their input, and pointed to a <a href="https://github.com/python/cpython/pull/120984">pull request</a>
that would implement the <tt>--with-app-store-compliance</tt>
configuration option. In the request, he noted that it would be
possible to use the option with platforms other than iOS or macOS, but
there were no use cases for that at present. If all goes well, it
should be available in Python&nbsp;3.13.</p>

<p>It is frustrating that free-software projects like Python have to
waste time finding ways around opaque review processes just so
developers can write software for non-free platforms. However, the
approach taken by Keith-Magee and other CPython developers seems to be
the least-bad option that offers the best experience for Python
application developers. It will almost certainly not be the last time that a
project runs into this problem.</p><br clear="all">
               <br clear="all">
               <hr><p>
           (<a href="https://lwn.net/Login/?target=/Articles/979671/">Log in</a> to post comments)
           </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Maker of RStudio launches new R and Python IDE (171 pts)]]></title>
            <link>https://www.infoworld.com/article/3715702/maker-of-rstudio-launches-new-r-and-python-ide.html</link>
            <guid>40815097</guid>
            <pubDate>Thu, 27 Jun 2024 21:05:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.infoworld.com/article/3715702/maker-of-rstudio-launches-new-r-and-python-ide.html">https://www.infoworld.com/article/3715702/maker-of-rstudio-launches-new-r-and-python-ide.html</a>, See on <a href="https://news.ycombinator.com/item?id=40815097">Hacker News</a></p>
<div id="readability-page-1" class="page"><section role="main" id="page-wrapper">
	
		
		
		

		<article itemscope="" itemtype="http://schema.org/NewsArticle">

		















<!-- Events Header -->

 
 
	
		
		
		
		
		
					    			
		
	
	
	
	
	

 
 
 
 
  
  
 
 












	
	
	
	
	
	
	
	
	
	
	
	
	
	
	


<!-- //end Events Header -->


	
	
	
	
	
	
		
	



	
		
	
	
		






















	
	
	
		
	



















	
		
	
	
















	
	




	
			












	
	
		<meta itemprop="keywords" content="integrated development environments, development tools, python, r language, data science, analytics, software development ">
	







<header>

	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
		
		
			
		
	
	

	
	
	
	
	
		<section>
			<h3 itemprop="description">Posit, formerly RStudio, has released a beta of Positron, a ‘next generation’ data science development environment based on Visual Studio Code. </h3>
		</section>
	
	
	
	
	
	
	
	
			
	
	
	

	
		
			
				
				<div>
						
						<p>
						
							Executive Editor, Data &amp; Analytics, 
								
									
								












<span itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><span itemprop="name">InfoWorld</span><meta itemprop="url" content="https://www.infoworld.com"><meta itemprop="logo" content="https://idge.staticworld.net/ifw/IFW_logo_social_300x300.png"></span> <span>|</span>
							
	
							<span itemprop="datePublished" content="2024-06-27T11:30-0700"><span></span>
								
							</span>
						</p>
					</div>
			
		
	
	
		
	
	
		
			
			
				
				
				
				
					
					
						
					
				
				
			
		
	
	
	
	
	
</header>




	<div>
		
		
		
		
			
				<div>
					
					
					
						
						
							
				                
				                	
				                	
				                
				                
								
								
								
							
							
								
							
							
							<figure itemprop="image" itemscope="" itemtype="http://schema.org/ImageObject">
								<meta itemprop="representativeOfPage" content="1">
								<meta itemprop="height" content="511">
								<meta itemprop="width" content="1200">
								<meta itemprop="url" content="https://images.idgesg.net/images/article/2023/07/shutterstock_2140364053-100942940-large.jpg?auto=webp&amp;quality=85,70">
							    								
								
								
								
								
								
								<img src="https://images.idgesg.net/images/article/2023/07/shutterstock_2140364053-100942940-large.jpg?auto=webp&amp;quality=85,70" loading="eager" width="620" height="264" alt="LLM, NLP, data science" itemprop="contentUrl">

					    		<figcaption>
					    			
					    			
					    				<span itemprop="copyrightHolder">
					    					
							    				
							    				TippaPatt/Shutterstock
						    				
				
							    			
							    				
							    				
							    				
												
												  	
												
												  	
												
												  	
												
												  	
												
												  	
												
												  	
												
												  	
												
												  	
												
												  	
												
												  	
												
												  	
												
												  	
												
							    				
							    				
												
											
									
										</span>
					    			
									
					    		</figcaption>
							</figure>
							
				
						
						
					
				</div>
			
			
		
	





</div>














		
		



		<section>

			
				
				
					












	
		



























	



	
	
	
	
		
	
	
	
	
	
	
	
	
		
		
			
			
			
			
		
	
	
	
	
	
	
			
	
	
	
	
	
	
		
		
			
		
	
	
	
	
	
		
			
				
				
				
					
				
				
			
		
		
	
	
		
		
			<div id="drr-container" itemprop="articleBody">
		
		
		
		
	
		
			
			
				<p>The company best known for RStudio, the leading integrated development environment (IDE) for R programmers, has quietly launched a “next-generation” IDE designed specifically for both R and <a href="https://www.infoworld.com/article/3204016/what-is-python-powerful-intuitive-programming.html">Python</a>. The <a href="https://github.com/posit-dev/positron" rel="nofollow">Positron IDE</a> is available in public beta as of today for macOS, Windows, and Linux.</p><p>Created by Boston-based Posit PBC, formerly RStudio, Positron is based on Microsoft’s <a href="https://www.infoworld.com/article/3666488/what-is-visual-studio-code-microsofts-extensible-code-editor.html">Visual Studio Code</a>. Users of VS Code will likely find Positron’s look and feel rather familiar, with panels for writing code and viewing code output, consoles and terminals, and an activity bar at the far left offering options for file navigation, version control, debugging, and extensions. However, Positron is packaged out of the box to be easier to set up, especially for R users but also for Python.</p><p>There’s no need to install extensions in order to get R up and running in Positron, as is <a href="https://www.infoworld.com/article/3625488/how-to-run-r-in-visual-studio-code.html%20">the case with VS Code</a>. Likewise you don’t have to install an extension for Positron to run Python. In fact, you’re cautioned <em>not</em> to install the usual VS Code extensions for R and Python in Positron, since the IDE already comes with that functionality built-in. For both languages, you should have the basic language files installed on your system, as well as the <a href="https://pypi.org/project/ipykernel/" rel="nofollow">IPykernel</a> package to run Python. Positron easily found both my R and Python installations at first launch.</p><figure><a href="https://images.idgesg.net/images/article/2024/06/positron_opening_screen-100963305-orig.jpg?auto=webp&amp;quality=85,70" rel="nofollow"><img alt="positron opening screen" width="1200" height="739" data-imageid="100963305" data-license="IDG" data-original="https://images.idgesg.net/images/article/2024/06/positron_opening_screen-100963305-large.jpg?auto=webp&amp;quality=85,70" loading="lazy" src="https://images.idgesg.net/images/article/2024/06/positron_opening_screen-100963305-large.jpg?auto=webp&amp;quality=85,70"></a> <small>IDG</small></figure><p>You can install other VS Code extensions in Positron if you want them, though. Because Microsoft does not allow third-party IDEs to access the official VS Code Marketplace, Positron extensions are installed via the <a href="https://open-vsx.org/" rel="nofollow">OpenVSX registry</a>. “Posit is a major sponsor of OpenVSX,” Posit noted in its Positron wiki. Not all VS Code extension authors also submit and regularly update their projects to OpenVSX, however.</p><p>Posit calls the project “a next-generation data science IDE” and “an extensible polyglot tool for writing code and exploring data.” It has a built-in, easy-to-use data and variable explorer, which includes options like sorting and filtering data frames. It can be accessed by clicking an icon, for both R and Python data.</p><p>“The Data Explorer is intended to complement code-first exploration of data, allowing you to display data in a spreadsheet-like grid, temporarily filter and sort data, and provide useful summary statistics directly inside of Positron,” according to the project wiki. “The goal of the Data Explorer is not to replace code-based workflows, but rather supplement with ephemeral views of the data or summary statistics as you further explore or modify the data via code.”</p><p>There are other welcome little tweaks in Positron, such as cmd/ctrl + enter running one line of a Python script and then moving your cursor to the next line of code. This can be surprisingly helpful for quick code examination outside of the debug tool. If you are working on a project that combines both R and Python scripts, which I increasingly do as an R user working with generative AI, the IDE also easily pops up the correct console when you switch between scripts in both languages.</p><p>The repo cautions that Positron is “an early stage project under active development.” Users should keep that in mind when weighing how and when to try it out.</p>
			
		
		
	
		
		
			
		
		
		
		
			
			
			
			











		
		
	</div>
	
	
	
		
		
		
	
	
	
	
	
	
	
	
	    
	
	
	
	
	
	
	
	
	
			
	   		












	
	
		<div>	
			<p>Sharon Machlis is Director of Editorial Data &amp; Analytics at Foundry, where she works on data analysis and in-house editor tools in addition to writing. Her book <a href="https://www.amazon.com/Practical-Mass-Communication-Journalism-Chapman/dp/1138726915/">Practical R for Mass Communication and Journalism</a> was published by CRC Press. She was named Digital Analytics Association's 2021 Top (Data) Practitioner, winner of the 2023 Jesse H. Neal journalism award for best instructional content, 2014 Azbee national gold award for investigative reporting, and 2017 Azbee gold for how-to article, among other awards. You can find her on Mastodon at <a rel="me" href="https://masto.machlis.com/@smach">masto.machlis.com/@smach</a>.</p><!-- end .author-info -->
						
		</div>
	



	   		
	   		
			
	
	
	
	
		












	






	<p>Copyright © 2024 IDG Communications, Inc.</p>

	
		 
	
	
	
	
	
		
		
	

















	 
	







	



	

	
	
	
	
	
	












	
	
	
	
	
	
	
	
	
	
	
	
	
	
	


























		

	
	















		






	
	
	
	




					










				
			
		
		</section><!-- /.bodee -->

		














 




	
		
			
				
				
					
						
					

				
			
			
		

		
			
			
		
			
				
					
							
	
	



	
		
			












	
	
	

			
		
	
	




		</article>

	</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TeamViewer Security Breach (102 pts)]]></title>
            <link>https://www.teamviewer.com/en/resources/trust-center/statement/</link>
            <guid>40815074</guid>
            <pubDate>Thu, 27 Jun 2024 21:02:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.teamviewer.com/en/resources/trust-center/statement/">https://www.teamviewer.com/en/resources/trust-center/statement/</a>, See on <a href="https://news.ycombinator.com/item?id=40815074">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p data-cmp-data-layer="{&quot;title-70c117e726&quot;:{&quot;@type&quot;:&quot;teamviewer/components/tenants/website/_atoms/title&quot;,&quot;dc:title&quot;:&quot;Security Update – June 28, 2024, 12:10pm CEST&quot;,&quot;xdm:linkURL&quot;:&quot;&quot;}}" id="title-70c117e726">
    <h2>Security Update – June 28, 2024, 12:10pm CEST</h2>
</p>

    

</div><div data-cmp-data-layer="{&quot;text-295aa2c938&quot;:{&quot;@type&quot;:&quot;teamviewer/components/tenants/website/_atoms/text&quot;,&quot;xdm:text&quot;:&quot;<p>A comprehensive taskforce consisting of TeamViewer’s security team together with globally leading cyber security experts has worked 24/7 on investigating the incident with all means available. We are in constant exchange with additional threat intelligence providers and relevant authorities to inform the investigation.</p>\r\n<p>Current findings of the investigation point to an attack on Wednesday, June 26, tied to credentials of a standard employee account within our Corporate IT environment. Based on continuous security monitoring, our teams identified suspicious behavior of this account and immediately put incident response measures into action. Together with our external incident response support, we currently attribute this activity to the threat actor known as APT29 / Midnight Blizzard. Based on current findings of the investigation, the attack was contained within the Corporate IT environment and there is no evidence that the threat actor gained access to our product environment or customer data.</p>\r\n<p>Following best-practice architecture, we have a strong segregation of the Corporate IT, the production environment, and the TeamViewer connectivity platform in place. This means we keep all servers, networks, and accounts strictly separate to help prevent unauthorized access and lateral movement between the different environments. This segregation is one of multiple layers of protection in our ‘defense in-depth’ approach.</p>\r\n<p>Security is of utmost importance for us, it is deeply rooted in our DNA. Therefore, we commit to transparent communication to stakeholders. We will continue to update the status of our investigations in <a href=\&quot;/content/teamviewer/website/eu/en/resources/trust-center/statement.html\&quot;>our Trust Center</a> as new information becomes available. We expect to post the next update by end of today CEST.</p>\r\n&quot;}}" id="text-295aa2c938">
    <p>A comprehensive taskforce consisting of TeamViewer’s security team together with globally leading cyber security experts has worked 24/7 on investigating the incident with all means available. We are in constant exchange with additional threat intelligence providers and relevant authorities to inform the investigation.</p>
<p>Current findings of the investigation point to an attack on Wednesday, June 26, tied to credentials of a standard employee account within our Corporate IT environment. Based on continuous security monitoring, our teams identified suspicious behavior of this account and immediately put incident response measures into action. Together with our external incident response support, we currently attribute this activity to the threat actor known as APT29 / Midnight Blizzard. Based on current findings of the investigation, the attack was contained within the Corporate IT environment and there is no evidence that the threat actor gained access to our product environment or customer data.</p>
<p>Following best-practice architecture, we have a strong segregation of the Corporate IT, the production environment, and the TeamViewer connectivity platform in place. This means we keep all servers, networks, and accounts strictly separate to help prevent unauthorized access and lateral movement between the different environments. This segregation is one of multiple layers of protection in our ‘defense in-depth’ approach.</p>
<p>Security is of utmost importance for us, it is deeply rooted in our DNA. Therefore, we commit to transparent communication to stakeholders. We will continue to update the status of our investigations in <a href="https://www.teamviewer.com/en/resources/trust-center/statement/">our Trust Center</a> as new information becomes available. We expect to post the next update by end of today CEST.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FCC rule would make carriers unlock all phones after 60 days (172 pts)]]></title>
            <link>https://techcrunch.com/2024/06/27/fcc-rule-would-make-carriers-unlock-all-phones-after-60-days/</link>
            <guid>40814728</guid>
            <pubDate>Thu, 27 Jun 2024 20:27:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/06/27/fcc-rule-would-make-carriers-unlock-all-phones-after-60-days/">https://techcrunch.com/2024/06/27/fcc-rule-would-make-carriers-unlock-all-phones-after-60-days/</a>, See on <a href="https://news.ycombinator.com/item?id=40814728">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">The FCC wants to make it significantly easier for consumers to unlock their phones from their carriers, proposing that all devices must be unlockable just 60 days after purchase. How this will mesh with current plans and phone-buying trends, however, is something the agency is hoping to learn before putting such a rule into effect.</p>

<p>Mobile phones purchased from a carrier are generally locked to that carrier until either the contract is up or the phone is paid off. But despite <a href="https://techcrunch.com/2015/11/15/if-you-want-tech-freedom-congress-needs-to-change-a-law/">improvements to the process over the years </a>(unlocking was <a href="https://techcrunch.com/2013/01/26/unlocking-your-phone-is-now-illegal-but-what-does-that-mean-for-you/">flat-out illegal </a>not long ago), it still isn’t quite clear to all consumers when and how they can unlock their phone and take it to the carrier (or country) of their choice.</p>

	
	


<p>To be clear, this is not about opening up your phone using a face, fingerprint or password, but changing settings in its software to allow it to work with different mobile networks.</p>

<p>FCC Chairwoman Jessica Rosenworcel announced the Notice of Proposed Rulemaking, or NPRM, <a rel="nofollow" href="https://www.fcc.gov/document/fcc-chairwoman-proposes-mobile-phone-unlocking-requirement">in a press release Thursday</a>. “When you buy a phone, you should have the freedom to decide when to change service to the carrier you want and not have the device you own stuck by practices that prevent you from making that choice,” she wrote. “That is why we are proposing clear, nationwide mobile phone unlocking rules.”</p>

	
	


<p>Specifically, the release says, carriers would simply have to provide unlocking services 60 days after activation. A welcome standard, but it may run afoul of today’s phone and wireless markets.</p>

<p>For instance, although the dreaded two-year contract is no longer forced on most consumers, many still opt for them to lock in the price and get other benefits. And perhaps more to the point, the phones themselves are often paid for in what amount to installment plans: You get a phone for “free” and then pay it off over the next few years.</p>

<p>The NPRM is the stage of FCC rulemaking where it has a draft rule but has not yet solicited public feedback. Only July 18, the agency will publish the full document and open up commentary on the above issues. And you can be sure there will be some squawking from mobile providers!</p>


	
	


<p>Not knowing the specifics of the proposed rule, we can’t be sure how it would mix with these common pay-over-time details. But unlocking a phone doesn’t free someone from needing to pay the device off — they can just use it on other networks if they want. And if a carrier lets you buy a phone outright from it but locks it to the bands for six months or a year out of sheer greed, this would offer an early exit.</p>

	
	


<p>As Rosenworcel said, the point of the rule is to offer consistency and transparency: a simple, national rule from regulators setting a reasonable limit on how and whether carriers can lock down devices. We’ll know more in July when the full NPRM is published.</p>

<figure></figure>
</div></div>]]></description>
        </item>
    </channel>
</rss>