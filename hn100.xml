<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 24 Aug 2025 04:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[The cost of interrupted work (2023) (125 pts)]]></title>
            <link>https://blog.oberien.de/2023/11/05/23-minutes-15-seconds.html</link>
            <guid>44999373</guid>
            <pubDate>Sat, 23 Aug 2025 21:45:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.oberien.de/2023/11/05/23-minutes-15-seconds.html">https://blog.oberien.de/2023/11/05/23-minutes-15-seconds.html</a>, See on <a href="https://news.ycombinator.com/item?id=44999373">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="main_content_wrap">
        
<p>2023-11-05</p>

<p>You’ve likely read lots of blog posts stating that it takes 23 minutes and 15 seconds to get back to work after an interruption, context switch, or meeting. Thus, “do you have five minutes” ends up not only costing those few minutes, but instead about half an hour. But where does that number come from?</p>

<p>I just wanted to quickly reference this fact to a colleague. Quick search for the reference, copy’n’paste it, in and out, 20 minutes adventure. I quickly found a reference to a paper. For sanity sake, I just wanted to verify where it states the 23 minutes. Open the paper, Ctrl+F for “23”, “no results”. Huh?</p>

<h3 id="papers">Papers</h3>

<p>Most of the posts mentioning the number refer to the paper <a href="https://ics.uci.edu/~gmark/chi08-mark.pdf">The Cost of Interrupted Work: More Speed and Stress</a>. The authors performed a study investigating different effects of interruptions during long tasks.<br>
Contrary to the quoted number, the study found out that the time spent on only the original task was in fact lower when interruptions were present (20.31 and 20.60 min) compared to no interruptions (22.77 min), albeit with much higher experienced stress. The paper never goes into details regarding the recovery time between finishing the interruption and getting back to the original task. The paper never mentions the number <code>23</code>.</p>

<p>Maybe it’s in a different paper? Related Work? References?</p>
<ul>
  <li><a href="http://erichorvitz.com/taskdiary.pdf">A Diary Study of Task Switching and Interruptions</a> let participants record interruption diaries. The paper does not include or mention task switch recovery time. Its primary result is that the average person has 50 task switches per week.</li>
  <li><a href="https://erichorvitz.com/CHI_2007_Iqbal_Horvitz.pdf">Disruption and Recovery of Computing Tasks: Field Study, Analysis, and Directions</a> states that it takes 11 - 16 minutes to resolve an interruption until getting back to the original task. Some of that time is spent to get the mind back into the original task. However, no further investigation of the recovery period has been performed.</li>
  <li><a href="https://interruptions.net/literature/Adamczyk-CHI04-p271-adamczyk.pdf">If Not Now, When?: The Effects of Interruption at Different Moments Within Task Execution</a> states “An approximate value for Resumption Lag, the time a subject takes to switch focus back to primary task after interruption, was also collected.” However, the paper never provides any value for that number and doesn’t discuss it further.</li>
  <li><a href="https://ics.uci.edu/~gmark/CHI2005.pdf">No Task Left Behind? Examining the Nature of Fragmented Work</a> focuses on the probability that a task was resumed on the same day in regards to recovery. There is no mention of a specific recovery time.</li>
</ul>

<h3 id="blog-posts">Blog Posts</h3>

<p>The search continued. In addition to the 5 papers I (fittingly) read through a total of 23 posts.</p>
<ul>
  <li>9 posts incorrectly referred to one of the papers; one of them even included a quote that is nowhere to be found within the referenced paper</li>
  <li>2 posts correctly referred to the first paper for its actual results</li>
  <li>9 posts directly or indirectly refer to three interviews with Gloria Mark (the author of the original paper), in which she stated the 23 minutes and 15 seconds recovery time</li>
  <li>2 posts refer to the Wall Street Journal, which directly quotes Gloria Mark with the 23 minutes 15 seconds figure</li>
</ul>

<p>So in the end, where do the 23 minutes and 15 seconds come from? They are mentioned in interviews multiple times by Gloria Mark. But I wasn’t able to find a primary printed source. There are <a href="https://ics.uci.edu/~gmark/Home_page/Publications.html">many more publications by Gloria Mark</a>, but none of them turned up while searching for the 23 minutes 15 seconds figure. If someone knows a paper or study where that figure originally appears in, please tell me.</p>

<hr>

<p>Discussion on <a href="https://www.reddit.com/r/programming/comments/17ooxwe/interruptions_cost_23_minutes_15_seconds_right/">r/programming</a>.</p>

<hr>

<p>Here is the reference graph of all posts and papers I’ve mentioned in this post and a list of their links.</p>

<p><img src="https://blog.oberien.de/assets/2023-11-05-references.svg" alt=""></p>

<ul>
  <li>Dev Interrupted – <a href="https://devinterrupted.substack.com/p/3-proven-ways-to-improve-dev-focus">https://devinterrupted.substack.com/p/3-proven-ways-to-improve-dev-focus</a></li>
  <li>Loom – <a href="https://www.loom.com/blog/cost-of-context-switching">https://www.loom.com/blog/cost-of-context-switching</a></li>
  <li>Paladinic – <a href="https://www.paladininc.com/blog/detail/6299/dealing-with-work-interruptions">https://www.paladininc.com/blog/detail/6299/dealing-with-work-interruptions</a></li>
  <li>Lifehacker – <a href="https://lifehacker.com/how-long-it-takes-to-get-back-on-track-after-a-distract-1720708353">https://lifehacker.com/how-long-it-takes-to-get-back-on-track-after-a-distract-1720708353</a></li>
  <li>The Muse – <a href="https://www.themuse.com/advice/this-is-nuts-it-takes-nearly-30-minutes-to-refocus-after-you-get-distracted">https://www.themuse.com/advice/this-is-nuts-it-takes-nearly-30-minutes-to-refocus-after-you-get-distracted</a></li>
  <li>Fast Company – <a href="https://www.fastcompany.com/944128/worker-interrupted-cost-task-switching">https://www.fastcompany.com/944128/worker-interrupted-cost-task-switching</a></li>
  <li>idonethis – <a href="https://blog.idonethis.com/distractions-at-work/">https://blog.idonethis.com/distractions-at-work/</a></li>
  <li>idea to value – <a href="https://www.ideatovalue.com/curi/nickskillicorn/2023/07/it-takes-23-minutes-to-regain-focus-after-a-distraction-task-switching/">https://www.ideatovalue.com/curi/nickskillicorn/2023/07/it-takes-23-minutes-to-regain-focus-after-a-distraction-task-switching/</a></li>
  <li>LeadDev – <a href="https://leaddev.com/process/managing-chaos-context-switching">https://leaddev.com/process/managing-chaos-context-switching</a></li>
  <li>getabstract – <a href="https://journal.getabstract.com/en/2022/03/17/twenty-three-minutes/">https://journal.getabstract.com/en/2022/03/17/twenty-three-minutes/</a></li>
  <li>gallup – <a href="https://news.gallup.com/businessjournal/23146/too-many-interruptions-work.aspx">https://news.gallup.com/businessjournal/23146/too-many-interruptions-work.aspx</a></li>
  <li>JournalStar – <a href="https://eu.pjstar.com/story/news/2013/01/25/frequent-emails-phone-call-interruptions/42450766007/">https://eu.pjstar.com/story/news/2013/01/25/frequent-emails-phone-call-interruptions/42450766007/</a></li>
  <li>togglblog – <a href="https://toggl.com/blog/how-to-get-back-on-track-when-you-get-distracted-at-work">https://toggl.com/blog/how-to-get-back-on-track-when-you-get-distracted-at-work</a></li>
  <li>Presentation by Gloria Mark – <a href="https://slideplayer.com/slide/1409624/">https://slideplayer.com/slide/1409624/</a></li>
  <li>Productivityjunkie – <a href="https://www.linkedin.com/pulse/productivityjunkie-23-minutes-15-seconds-mystery-sugar-inga-bieli%C5%84ska">https://www.linkedin.com/pulse/productivityjunkie-23-minutes-15-seconds-mystery-sugar-inga-bieli%C5%84ska</a></li>
  <li>Bright Developers – <a href="https://www.brightdevelopers.com/the-cost-of-interruption-for-software-developers/">https://www.brightdevelopers.com/the-cost-of-interruption-for-software-developers/</a></li>
  <li>Wall Street Journal – <a href="https://www.wsj.com/articles/SB10001424127887324339204578173252223022388">https://www.wsj.com/articles/SB10001424127887324339204578173252223022388</a></li>
  <li>Ironistic – <a href="https://www.linkedin.com/pulse/cost-distractions-developers-ironistic-com">https://www.linkedin.com/pulse/cost-distractions-developers-ironistic-com</a></li>
  <li>Jazz Hanley – <a href="https://www.linkedin.com/pulse/reclaim-23-minutes-you-lose-every-time-youre-work-jazz-hanley">https://www.linkedin.com/pulse/reclaim-23-minutes-you-lose-every-time-youre-work-jazz-hanley</a></li>
  <li>devmio – <a href="https://devm.io/careers/aaaand-gone-true-cost-interruptions-128741">https://devm.io/careers/aaaand-gone-true-cost-interruptions-128741</a></li>
  <li>Stephanie C. Mitchell – <a href="https://www.stephaniecmitchell.com/articles/whats-distracting-you-from-writing">https://www.stephaniecmitchell.com/articles/whats-distracting-you-from-writing</a></li>
  <li>devbizops – <a href="https://devbizops.medium.com/getting-into-the-developer-flow-state-7b0e5c98eb8a">https://devbizops.medium.com/getting-into-the-developer-flow-state-7b0e5c98eb8a</a></li>
  <li>Hardvard Business Review – <a href="https://hbr.org/2014/04/help-your-employees-find-flow">https://hbr.org/2014/04/help-your-employees-find-flow</a></li>
</ul>


      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Romhack.ing's Internet Archive Mirror No Longer Available (136 pts)]]></title>
            <link>https://romhack.ing/database/news/entry/DW8BKnRHSEqaGDwXTiKjMw</link>
            <guid>44998982</guid>
            <pubDate>Sat, 23 Aug 2025 20:43:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://romhack.ing/database/news/entry/DW8BKnRHSEqaGDwXTiKjMw">https://romhack.ing/database/news/entry/DW8BKnRHSEqaGDwXTiKjMw</a>, See on <a href="https://news.ycombinator.com/item?id=44998982">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[What makes Claude Code so damn good (231 pts)]]></title>
            <link>https://minusx.ai/blog/decoding-claude-code/</link>
            <guid>44998295</guid>
            <pubDate>Sat, 23 Aug 2025 19:07:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://minusx.ai/blog/decoding-claude-code/">https://minusx.ai/blog/decoding-claude-code/</a>, See on <a href="https://news.ycombinator.com/item?id=44998295">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Claude Code is the most delightful AI agent/workflow I have used so far. Not only does it make targeted edits or vibe coding throwaway tools less annoying, using Claude Code makes me happy. It has enough autonomy to do interesting things, while not inducing a jarring loss of control like some other tools do. Of course most of the heavy lifting is done by the new Claude 4 model (especially interleaved thinking). But I find Claude Code objectively less annoying to use compared to Cursor, or Github Copilot agents even with the same underlying model! What makes it so damn good? If you're reading this and nodding along, I'm going to try and provide some answers.</p>
<p><strong>Note</strong>: This is not a blogpost with Claude Code's architecture dump (there are some good ones out there). This blogpost is meant to be a guide for building delightful LLM agents, based on my own experience using and tinkering with Claude Code over the last few months (and all the logs we intercepted and analyzed). You can find <a href="#appendix">prompts</a> and <a href="#appendix">tools</a> in the <a href="#appendix">Appendix section</a>. This post is ~2k words long, so strap in! If you're looking for some quick takeaways, the <a href="#how-to-build-a-claude-code-like-agent-tldr">TL;DR</a> section is a good place to start.</p>

<p><img src="https://minusx.ai/images/claude-code/prompts.png" alt="prompts">
</p><p>You can clearly see the different Claude Code updates.</p>

<p>Claude Code (CC) feels great to use, because it <em>just simply works</em>. CC has been crafted with a fundamental understanding of what the LLM is good at and what it is terrible at. Its prompts and tools cover for the model's stupidity and help it shine in its wheelhouse. The control loop is extremely simple to follow and trivial to debug.</p>
<p>We started using CC at MinusX as soon as it launched. To look under the hood, <a href="https://x.com/ppsreejith_">Sreejith</a> wrote a logger that intercepts and logs every network request made. The following analysis is from my extensive use over the last couple of months. <strong>This post attempts to answer the question - "What makes Claude Code so good, and how can you give a CC-like experience in your own chat-based-LLM agent?"</strong> We've incorporated most of these into MinusX already and I'm excited to see you do it too!</p>

<p><img src="https://minusx.ai/images/claude-code/tools.png" alt="prompts">
</p><p>Edit is the most frequent tool, followed by Read and ToDoWrite</p>
<br>
<h2 id="how-to-build-a-claude-code-like-agent-tldr"><a href="#how-to-build-a-claude-code-like-agent-tldr">How to build a Claude Code like agent: TL;DR</a></h2>
<p>If there is one thing to take away from this, it is this - <strong>Keep Things Simple, Dummy</strong>. LLMs are terrible enough to debug and evaluate. Any additional complexity you introduce (multi-agents, agent handoffs or complex RAG search algorithms) only makes debugging 10x harder. If such a fragile system works at all, you'll be terrified of making drastic changes to it later. So, keep everything in one file, avoid excessive boilerplate scaffolding and rip it all out at least a couple of times :)</p>
<p>Here are the main takeaways from Claude Code to implement in your own system.</p>
<h4 id="1-control-loop"><a href="#1-control-loop">1. Control Loop</a></h4>
<ul>
<li>1.1 <a href="#11-keep-one-main-loop">Keep one main loop (with max one branch) and one message history</a></li>
<li>1.2 <a href="#12-use-a-smaller-model-for-everything">Use a smaller model for all sorts of things. All. The. Frickin. Time.</a></li>
</ul>
<h4 id="2-prompts"><a href="#2-prompts">2. Prompts</a></h4>
<ul>
<li>2.1 <a href="#21-use-claudemd-for-collaborating-on-user-context-and-preferences">Use claude.md pattern to collaborate on and remember user preferences</a></li>
<li>2.2 <a href="#22-special-xml-tags-markdown-and-lots-of-examples">Use special XML Tags, Markdown, and lots of examples</a></li>
</ul>
<h4 id="3-tools"><a href="#3-tools">3. Tools</a></h4>
<ul>
<li>3.1 <a href="#31-llm-search---rag-based-search">LLM search &gt;&gt;&gt;  RAG based search</a></li>
<li>3.2 <a href="#32-how-to-design-good-tools-low-level-vs-high-level-tools">How to design good tools? (High vs Low level tools)</a></li>
<li>3.3 <a href="#33-let-the-agent-manage-a-todo-list">Let your agent manage its own todo list</a></li>
</ul>
<h4 id="4-steerability"><a href="#4-steerability">4. Steerability</a></h4>
<ul>
<li>4.1 <a href="#41-tone-and-style">Tone and style</a></li>
<li>4.2 <a href="#42-this-is-important-is-still-state-of-the-art">"<strong>PLEASE THIS IS IMPORTANT</strong>" is unfortunately still state of the art</a></li>
<li>4.3 <a href="#43-write-the-algorithm-with-heuristics-and-examples">Write the algorithm, with heuristics and examples</a></li>
</ul>
<br>
<blockquote>
<p>Claude Code choses architectural simplicity at every juncture - one main loop, simple search, simple todolist, etc. Resist the urge to over-engineer, build good harness for the model let it cook! Is this end-to-end self-driving all over again? Bitter lesson much?</p>
</blockquote>
<hr>
<h2 id="1-control-loop-design"><a href="#1-control-loop-design">1. Control Loop Design</a></h2>
<h3 id="11-keep-one-main-loop"><a href="#11-keep-one-main-loop">1.1 Keep One Main Loop</a></h3>
<p>Debuggability &gt;&gt;&gt; complicated hand-tuned multi-agent lang-chain-graph-node mishmash.</p>
<p>Despite multi agent systems being all the rage, Claude Code has just one main thread. It uses a few different types of prompts periodically to summarize the git history, to clobber up the message history into one message or to come up with some fun UX elements. But apart from that, it maintains a flat list of messages. An interesting way it handles hierarchical tasks is by spawning itself as a sub-agent without the ability to spawn more sub-agents. There is a maximum of one branch, the result of which is added to the main message history as a "tool response".</p>
<p>If the problem is simple enough, the main loop just handles it via iterative tool calling. But if there are one or more tasks that are complex, the main agent creates clones of itself. The combination of the max-1-branch and the todo list makes sure the agent has the ability to break the problem into sub-problems, but also keep the eye on the final desired outcome.</p>
<p>I highly doubt your app needs a multi-agent system. With every layer of abstraction you make your system harder to debug, and more importantly you deviate from the general-model-improvement trajectory.</p>
<p><img src="https://minusx.ai/images/claude-code/control_loop.gif" alt="Control Loop"></p>
<h3 id="12-use-a-smaller-model-for-everything"><a href="#12-use-a-smaller-model-for-everything">1.2 Use a Smaller model for <em>everything</em></a></h3>
<p>Over 50% of all important LLM calls made by CC are to claude-3-5-haiku. It is used to read large files, parse web pages, process git history and summarize long conversations. It is also used to come up with the one-word processing label - literally for every key stroke! The smaller models are 70-80% cheaper than the standard ones (Sonnet 4, GPT-4.1). Use them liberally!</p>
<h2 id="2-prompts-1"><a href="#2-prompts-1">2. Prompts</a></h2>
<p>Claude Code has extremely elaborate prompts filled with heuristics, examples and IMPORTANT (tch-tch) reminders. The system prompt is ~2800 tokens long, with the Tools taking up a whopping 9400 tokens. The user prompt always contains the claude.md file, which can typically be another 1000-2000 tokens. The system prompt contains sections on tone, style, proactiveness, task management, tool usage policy and doing tasks. It also contains the date, current working directory, platform and OS information and recent commits.</p>
<p><a href="#appendix"><strong>Go read the entire prompt</strong></a>!</p>
<h3 id="21-use-claudemd-for-collaborating-on-user-context-and-preferences"><a href="#21-use-claudemd-for-collaborating-on-user-context-and-preferences">2.1 Use claude.md for collaborating on user context and preferences</a></h3>
<p>One of the major patterns most coding agent creators have settled on is the context file (aka Cursor Rules / claude.md / agent.md). The difference in Claude Code's performance with and without claude.md is night and day. It is a great way for the developers to impart context that cannot be inferred from the codebase and to codify all strict preferences. For example, you can force the LLM to skip some folders, or use specific libraries. CC sends the entire contents of the claude.md with every user request</p>
<p>We recently introduced <a href="https://minusx.ai/blog/memory/">minusx.md in MinusX</a> which is fast becoming the de-facto context file for our agents to codify user and team preferences.</p>
<h3 id="22-special-xml-tags-markdown-and-lots-of-examples"><a href="#22-special-xml-tags-markdown-and-lots-of-examples">2.2 Special XML Tags, Markdown, and lots of examples</a></h3>
<p>It is fairly established that XML tags and Markdown are two ways to structure a prompt. CC uses both, extensively. Here are a few notable XML tags in Claude Code:</p>
<ul>
<li><code>&lt;system-reminder&gt;</code>: This is used at the end of many prompt sections to remind the LLM of thing it presumably otherwise forgets. Example:</li>
</ul>
<pre><code>&lt;system-reminder&gt;This is a reminder that your todo list is currently empty. DO NOT mention this to the user explicitly because they are already aware. If you are working on tasks that would benefit from a todo list please use the TodoWrite tool to create one. If not, please feel free to ignore. Again do not mention this message to the user.&lt;/system-reminder&gt;
</code></pre>
<ul>
<li><code>&lt;good-example&gt;</code>, <code>&lt;bad-example&gt;</code>: These are used to codify heuristics. They can be especially useful when there is a fork in the road with multiple seemingly reasonable paths/tool_calls the model can choose. Examples can be used to contrast the cases and make it very clear which path is preferable. Example:</li>
</ul>
<pre><code>Try to maintain your current working directory throughout the session by using absolute paths and avoiding usage of `cd`. You may use `cd` if the User explicitly requests it.
&lt;good-example&gt;
pytest /foo/bar/tests  
&lt;/good-example&gt;
&lt;bad-example&gt;
cd /foo/bar &amp;&amp; pytest tests
&lt;/bad-example&gt;
</code></pre>
<p>CC also uses markdown to demarcate clear sections in the system prompt. Example markdown headings include:</p>
<ul>
<li>Tone and style</li>
<li>Proactiveness</li>
<li>Following conventions</li>
<li>Code style</li>
<li>Task Management</li>
<li>Tool use policy</li>
<li>Doing Tasks</li>
<li>Tools</li>
</ul>
<h2 id="3-tools-1"><a href="#3-tools-1">3. Tools</a></h2>
<p><a href="#appendix"><strong>Go read the entire tools prompt</strong></a> - it is a whopping 9400 tokens long!</p>
<h3 id="31-llm-search---rag-based-search"><a href="#31-llm-search---rag-based-search">3.1 LLM search &gt;&gt;&gt;  RAG based search</a></h3>
<p>One significant way in which CC deviates from other popular coding agents is in its rejection of RAG. Claude Code searches your code base just as you would, with really complex <code>ripgrep</code>, <code>jq</code> and <code>find</code> commands. Since the LLM understands code really well, it can use sophisticated regex to find pretty much any codeblock it deems relevant. Sometimes it ends up reading whole files with a smaller model.</p>
<p>RAG sounds like a good idea in theory, but it introduces new (and more importantly, hidden) failure modes. What is the similarity function to use? What reranker? How do you chunk the code? What do you do with large JSON or log files? With LLM Search, it just looks at 10 lines of the json file to understand its structure. If it wants, it looks at 10 more lines - just like you would. Most importantly, this is RL learnable - something BigLabs are already working on. The model does most of the heavy lifting - as it should, dramatically reducing the number of moving parts in the agent. Also, having two complicated, intelligent systems wired this way is just ugly. I was recently kidding with a friend saying this is the Camera vs Lidar of the LLM era and I'm only half joking.</p>
<h3 id="32-how-to-design-good-tools-low-level-vs-high-level-tools"><a href="#32-how-to-design-good-tools-low-level-vs-high-level-tools">3.2 How to design good tools? (Low level vs High level tools)</a></h3>
<p>This question keeps anyone who is building an LLM agent up at night. Should you give the model generic tasks (like meaningful actions) or should it be low level (like type and click and bash)? The answer is that it depends (and you should use both).</p>
<p>Claude Code has low level (Bash, Read, Write), medium level (Edit, Grep, Glob) and high level tools (Task, WebFetch, exit_plan_mode). CC can use bash, so why give a separate Grep tool? The real trade-off here is in how often you expect your agent to use the tool vs accuracy of the agent in using the tool. CC uses grep and glob so frequently that it makes sense to make separate tools out of them, but at the same time, it can also write generic bash commands for special scenarios.</p>
<p>Similarly, there are even higher level tools like WebFetch or 'mcp__ide__getDiagnostics' that are extremely deterministic in what they do. This saves the LLM from having to do multiple low level clicking and typing and keeps it on track. Help the poor model out, will ya!? Tool descriptions have elaborate prompts with plenty of examples. The system prompt has information about ‘when to use a tool' or how to choose between two tools that can do the same task.</p>
<p><strong>Tools in Claude Code:</strong></p>
<div><div><ul>
<li><a href="#appendix">Task</a></li>
<li><a href="#appendix">Bash</a></li>
<li><a href="#appendix">Glob</a></li>
<li><a href="#appendix">Grep</a></li>
<li><a href="#appendix">LS</a></li>
<li><a href="#appendix">ExitPlanMode</a></li>
<li><a href="#appendix">Read</a></li>
<li><a href="#">Edit</a></li>
</ul></div><div><ul>
<li><a href="#appendix">MultiEdit</a></li>
<li><a href="#appendix">Write</a></li>
<li><a href="#appendix">NotebookEdit</a></li>
<li><a href="#appendix">WebFetch</a></li>
<li><a href="#appendix">TodoWrite</a></li>
<li><a href="#appendix">WebSearch</a></li>
<li><a href="#">mcp__ide__getDiagnostics</a></li>
<li><a href="#">mcp__ide__executeCode</a></li>
</ul></div></div>
<h3 id="33-let-the-agent-manage-a-todo-list"><a href="#33-let-the-agent-manage-a-todo-list">3.3 Let the agent manage a todo list</a></h3>
<p>There are many reasons why this is a good idea. Context rot is a common problem in long-running LLM agents. They enthusiastically start out tackling a difficult problem, but over time lose their way and devolve into garbage. There are a few ways current agent designs tackle this.
Many agents have experimented with explicit todos (one model generates todos, another model implements them) or with Multi-agent handoff + verification (PRD/PM agent -&gt; implementer agent -&gt; QA agent)</p>
<p>We already know multi-agent handoff is not a good idea, for many many reasons. CC uses an explicit todo list, but one that the model maintains. This keeps the LLM on track (it has been heavily prompted to refer to the todo list frequently), while at the same time giving the model the flexibility to course correct mid-way in an implementation. This also effectively leverages the model's interleaved thinking abilities to either reject or insert new todo items on the fly.</p>
<h2 id="4-steerability-1"><a href="#4-steerability-1">4. Steerability</a></h2>
<h3 id="41-tone-and-style"><a href="#41-tone-and-style">4.1 Tone and Style</a></h3>
<p>CC explicitly attempts to control the aesthetic behavior of the agent. There are sections in the system prompt around tone, style and proactiveness - full of instructions and examples. This is  why Claude Code “feels” tasteful in its comments and eagerness. I recommend just copying large sections of this into your app as is.</p>
<pre><code># Some examples of tone and style
- IMPORTANT: You should NOT answer with unnecessary preamble or postamble (such as explaining your code or summarizing your action), unless the user asks you to.
Do not add additional code explanation summary unless requested by the user.

- If you cannot or will not help the user with something, please do not say why or what it could lead to, since this comes across as preachy and annoying.

- Only use emojis if the user explicitly requests it. Avoid using emojis in all communication unless asked.
</code></pre>
<h3 id="42-this-is-important-is-still-state-of-the-art"><a href="#42-this-is-important-is-still-state-of-the-art">4.2 "THIS IS IMPORTANT" is still State of the Art</a></h3>
<p>Unfortunately CC is no better when it comes to asking the model to not do something. IMPORTANT, VERY IMPORTANT, NEVER and ALWAYS seem to be the best way to steer the model away from landmines. I expect the models to get more steerable in the future and avoid this ugliness. But for now, CC uses this liberally, and so should you. Some examples:</p>
<pre><code>- IMPORTANT: DO NOT ADD ***ANY*** COMMENTS unless asked

- VERY IMPORTANT: You MUST avoid using search commands like `find` and `grep`. Instead use Grep, Glob, or Task to search. You MUST avoid read tools like `cat`, `head`, `tail`, and `ls`, and use Read and LS to read files.\n  - If you _still_ need to run `grep`, STOP. ALWAYS USE ripgrep at `rg` first

- IMPORTANT: You must NEVER generate or guess URLs for the user unless you are confident that the URLs are for helping the user with programming. You may use URLs provided by the user in their messages or local files.

</code></pre>
<h3 id="43-write-the-algorithm-with-heuristics-and-examples"><a href="#43-write-the-algorithm-with-heuristics-and-examples">4.3 Write the Algorithm (with heuristics and examples)</a></h3>
<p>It is extremely important to identify the most important task the LLM needs to perform and write out the algorithm for it. Try to role-play as the LLM and work through examples, identify all the decision points and write them explicitly. It helps if this is in the form of a flow-chart. This helps structure the decision making and aids the LLM in following instructions. One thing to definitely avoid is a big soup of Dos and Don'ts. They are harder to keep track, and keep mutually exclusive. If your prompt is several thousand tokens long, you will inadvertently have conflicting Dos and Don'ts. The LLM becomes extremely fragile in this case and it becomes impossible to incorporate new use cases.</p>
<p><code>Task Management</code>, <code>Doing Tasks</code> and <code>Tool Usage Policy</code> sections in Claude Code's system prompt clearly walk through the algorithm to follow. This is also the section to add lots of heuristics and examples of various scenarios the LLM might encounter.</p>
<h2 id="bonus-why-pay-attention-to-biglab-prompts"><a href="#bonus-why-pay-attention-to-biglab-prompts">Bonus: Why pay attention to BigLab prompts?</a></h2>
<p>A lot of the effort in steering LLMs is trying to reverse engineer their post-training / RLHF data distribution. Should you use JSON or XML? Should the tool descriptions be in the system prompt or just in tools? What about your app's current state? It helps to see what they do in their own apps and use it to inform yours. Claude Code design is very opinionated and it helps to use that in forming your own.</p>
<br>
<h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2>
<p>The main takeaway, again, is to keep things simple. Extreme scaffolding frameworks will hurt more than help you. Claude Code really made me believe that an "agent" can be simple and yet extremely powerful. We've incorporated a bunch of these lessons into MinusX, and are continuing to incorporate more.</p>
<p>If you're interested in Claude-Codifying your own LLM agent, I'd love to chat - ping me on <a href="https://x.com/nuwandavek">twitter</a>! If you want trainable Claude Code like data agents for your Metabase, check out <a href="https://minusx.com/">MinusX</a> or set up a demo with me <a href="https://minusx.com/demo">here</a>. Happy (Claude) Coding!</p>

<br>
<hr>
<br>
<h2 id="appendix"><a href="#appendix">Appendix</a></h2>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I hacked Monster Energy (174 pts)]]></title>
            <link>https://bobdahacker.com/blog/monster-energy</link>
            <guid>44997145</guid>
            <pubDate>Sat, 23 Aug 2025 16:42:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bobdahacker.com/blog/monster-energy">https://bobdahacker.com/blog/monster-energy</a>, See on <a href="https://news.ycombinator.com/item?id=44997145">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
            <div>
                <h2>The Energy Drink Giant That Forgot to Lock Its Doors</h2>
<p>As a hacker who likes energy drinks, I decided to check out Monster Energy's corporate infrastructure. What I found was completely exposed and making terrible security decisions.</p>
<h3>Monster University: Where Security Goes to Die</h3>
<p>Monster University (<code>mu.monsterenergy.com</code>) is where Monster employees go to learn about their brand. It's also where I learned that changing <code>/login</code> to <code>/register</code> in the URL is apparently Monster's idea of "authentication."</p>
<p>The registration form appeared but wouldn't submit. So I went straight to the JavaScript to find the actual API endpoint. The API helpfully told me exactly which fields were missing from my registration attempt.</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/register_api.png" alt="Monster University Registration API"></p>
<p>Once I called the API directly with the right fields, boom, I was in. Full access to Monster University, complete with all their training materials, including this absolute masterpiece about their target demographic:</p>
<h3>This Is What Monster Thinks You Look Like</h3>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/consumer_demographics.jpg" alt="Monster's " consumer"="" demographics"=""></p>
<p>I'm not kidding. This is from their actual brand training guide. According to Monster Energy, their "Core Brand Family Consumer" is:</p>
<p><em>"Monster Green shoppers are likely younger (Gen-Z/Millennial/Gen-X) male, lower income &amp; Caucasian (skews Hispanic)."</em></p>
<p>And they included this photo of five people in Monster gear looking like they're being held hostage in a marketing photoshoot. This is literally what Monster corporate thinks their average customer looks like. I can't make this stuff up.</p>
<h3>The Irony: Their Own Cybersecurity Training</h3>
<p>The best part? Monster University has a cybersecurity course they bought from a third-party vendor. The same platform with no authentication has lessons teaching employees about phishing and basic security:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/cybersecurity_course.png" alt="Monster Cybersecurity Course"></p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/gamelist.png" alt="Monster Cybersecurity Game List"></p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/amplify_game.png" alt="Amplify Cybersecurity Game"></p>
<p>The irony of hosting a cybersecurity course about phishing on a completely unsecured platform is just <em>chef's kiss</em>.</p>
<h3>Meanwhile in Monster Corporate: Walmart Zoom Calls and "ULTIMATE BEAST" Badges</h3>
<p>While exploring Monster University, I found some gems about their corporate culture. You can view their entire Zoom meeting schedule and even get the join links:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/walmart_meeting.png" alt="Walmart Office Hours Event"></p>
<p>And check out their employee achievement system - you can earn badges for everything from "BEAST" to "ULTIMATE BEAST":</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/monster_badges.png" alt="Monster Achievement Badges"></p>
<h3>Beast Bux: Monster's Internal Currency System</h3>
<p>But wait, it gets better. I found their internal employee rewards system called "Beast Bux." Here's their actual training video explaining it:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/video_section.png" alt="Monster University Video Section"></p>

<p>Essentially, employees get Beast Bux yearly and can give them to other employees to show appreciation. These can be used to buy Monster merch at their internal store: <a target="_blank" rel="noopener noreferrer" href="https://bleedgreenshop.monsterenergy.com/">https://bleedgreenshop.monsterenergy.com/</a></p>
<h3>The Real Treasure: Exposed Corporate File System</h3>
<p>The scariest part wasn't the training portal or the questionable customer profiling. It was finding their OpenText API completely exposed with no authentication required:</p>
<pre><code>https://opentextapi.monsterenergy.com/opentext/search/?page=1&amp;pageSize=1000&amp;searchTerms=
</code></pre>
<p>This endpoint allows anyone to search through Monster's entire file system. No password. No authentication. Nothing.</p>
<p>Want to see internal contracts? Sure, here's one I found:</p>
<pre><code>https://opentextapi.monsterenergy.com/opentext/images/7e02a7602d8cee4aaf5b999850c243df9d0a184b
</code></pre>
<p>(Don't let the "images" in the URL fool you - it serves all file types, not just images)</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/contract.png" alt="Monster Contract Document"></p>
<p>The API returns full document metadata, file paths, and direct download links for everything in their system. Contracts, internal documents, you name it.</p>
<h3>Even Worse: The ClickUp Integration Disaster</h3>
<p>On a subdomain called Kermometer (<code>kermometer.monsterenergy.com</code>), I discovered Monster had integrated ClickUp into their workflow, but they made a critical mistake: they exposed an admin's private account token directly in their website's JavaScript.</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/clickup_token.png" alt="ClickUp Token Exposed in JavaScript"></p>
<p>This token would allow anyone to:</p>
<ul>
<li>Access their entire ClickUp workspace</li>
<li>View all private documents and projects</li>
<li>Invite themselves to the workspace</li>
<li>Potentially modify or delete critical project data</li>
</ul>
<p>To prove it worked, I invited myself to their workspace:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/clickup_invite.png" alt="ClickUp Workspace Invitation"></p>
<p>For the lulz, I wrote a script to share everything with myself using their admin token:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/sharing_script.png" alt="Script Sharing Everything With Me"></p>
<p>Don't worry, I left their workspace immediately after proving the vulnerability. I'm not trying to steal Monster's secret energy drink formulas or anything.</p>
<h3>The Response (Or Lack Thereof)</h3>
<p>I tried contacting Monster Energy directly about these vulnerabilities. No response.</p>
<p>They did fix the Monster University registration issue, but I don't think they even read my emails - they probably just noticed someone had signed up through their broken system and patched it.</p>
<p>Finally, I told ClickUp themselves about the exposed token on Monster's site. They investigated and contacted Monster, getting it fixed in less than a week. </p>
<p>But Monster? They never even acknowledged any of my reports. And as you can see, they left their entire file system API wide open.</p>
<p><strong>The OpenText API is STILL ACTIVE as of writing this post.</strong></p>
<h3>To Monster Energy</h3>
<p>Your energy drinks might "Unleash the Beast," but your security is definitely asleep.</p>
<p>Maybe spend less time creating stereotypical customer profiles and more time securing your infrastructure? Just a thought.</p>
<p>Also, a security contact email would be nice. You know, for next time.</p>
<hr>

            </div>
        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Line scan camera image processing for train photography (235 pts)]]></title>
            <link>https://daniel.lawrence.lu/blog/y2025m09d21/</link>
            <guid>44996938</guid>
            <pubDate>Sat, 23 Aug 2025 16:09:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daniel.lawrence.lu/blog/y2025m09d21/">https://daniel.lawrence.lu/blog/y2025m09d21/</a>, See on <a href="https://news.ycombinator.com/item?id=44996938">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header><div><ol><li><a href="#s1"><span>1</span> <span>Principle of operation</span></a></li><li><a href="#s2"><span>2</span> <span>About the camera</span></a></li><li><a href="#s3"><span>3</span> <span>Detecting the region of interest</span></a></li><li><a href="#s4"><span>4</span> <span>Speed estimation</span></a></li><li><a href="#s5"><span>5</span> <span>Resampling</span></a></li><li><a href="#s6"><span>6</span> <span>Demosaicing</span></a></li><li><a href="#s7"><span>7</span> <span>Getting rid of the vertical stripes</span></a></li><li><a href="#s8"><span>8</span> <span>Denoising</span></a></li><li><a href="#s9"><span>9</span> <span>Skew correction</span></a></li><li><a href="#s10"><span>10</span> <span>Color calibration</span></a></li><li><a href="#s11"><span>11</span> <span>Implementation details</span></a><ol><li><a href="#s11.1"><span>11.1</span> <span>Vibe coding experience</span></a></li></ol></li><li><a href="#s12"><span>12</span> <span>Other people’s line scan photography of trains</span></a><ol><li><a href="#s12.1"><span>12.1</span> <span>Adam Magyar</span></a></li><li><a href="#s12.2"><span>12.2</span> <span>KR64’s blog</span></a></li></ol></li></ol></div></header><p>I use my line scan camera to take cool pictures of trains and other stuff.</p><p>But there’s a lot that goes into properly processing the images.</p><div><figure id="fig1"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_a8414d74520f4baa.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_450228d70b868eb2.jpg" alt=""></a><figcaption><a href="#fig1">FIGURE 1</a> A cool tram.</figcaption></figure></div><div><figure id="fig2"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_0d9fee240b0c6e5f.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_5df7185ac60d16b4.jpg" alt=""></a><figcaption><a href="#fig2">FIGURE 2</a> A cool train, the Renfe AVE Class 102, nicknamed <em>Pato</em> because of its duck bill-like appearance.</figcaption></figure></div><div><figure id="fig3"><a href="https://i.dllu.net/rgb_9_prod_no_denoise_20704300acae5f1e.jpg"><img src="https://i.dllu.net/rgb_9_prod_no_denoise_16c81b9b2f43f4bb.jpg" alt=""></a><figcaption><a href="#fig3">FIGURE 3</a> Cool diesel locomotive.</figcaption></figure></div><div><figure id="fig4"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_dd93f40ada264e00.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_7e6bd0d3e527a2d0.jpg" alt=""></a><figcaption><a href="#fig4">FIGURE 4</a> Nice CRH6A intercity electric multiple unit.</figcaption></figure></div><div><figure id="fig5"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_685eacc3349ba19a.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_b45d3300018b8368.jpg" alt=""></a><figcaption><a href="#fig5">FIGURE 5</a> Awesome CR400AF. Super fast.</figcaption></figure></div><h2 id="s1"><a href="#s1">1</a> <span>Principle of operation</span></h2><p>The way it works is that the camera has a single column of pixels (or in this case, two columns), that scans at a super high speed.
The camaera is stationary, but as a train moves past it, it gets scanned.</p><p>This is essentially also how a <a href="https://en.wikipedia.org/wiki/Photo_finish">photo finish camera</a> works.</p><p>Since the background is static, it gets repeated for every column of the image, giving it its distinctive striped look.</p><p>Line scan cameras are very suitable for capturing trains, since I can capture the full length of the train with minimal perspective distortion.
This is super nice for train nerds who want to make models of the trains.
Also, as you keep the camera running, you can get incredibly high resolution photos that span over 100,000 pixels wide.</p><p>By the way, film photo finish cameras and strip cameras behave almost the same as line scan cameras but with one subtle distinction, which is that you have to pull the film across a strip that’s somewhat wider than a single column of pixels.
This is because film is less sensitive than modern digital image sensors.
However, you’ll need to know the approximate speed of the subject and pull the film across at roughly the right speed.</p><h2 id="s2"><a href="#s2">2</a> <span>About the camera</span></h2><p>I’m using an <a href="https://www.alkeria.com/products/necta-series">Alkeria Necta N4K2-7C</a>.
It has a 4096×2 <a href="https://en.wikipedia.org/wiki/Bayer_filter">Bayer array</a> image sensor.
I’m saving its raw data in 16 bit binary arrays.</p><div><figure id="fig8"><a href="https://i.dllu.net/2024-09-12-18-31-00_DSCF0249_d644edd144f26bdabd7a876eda224c2a673ad9ec_d6e13378a10243e5.jpg"><img src="https://i.dllu.net/2024-09-12-18-31-00_DSCF0249_d644edd144f26bdabd7a876eda224c2a673ad9ec_7281c09603aff005.jpg" alt=""></a><figcaption><a href="#fig8">FIGURE 8</a> My camera.</figcaption></figure></div><iframe width="320" height="400" src="https://www.youtube.com/embed/r-GHYwkQD1o" title="line scan photography of Shanghai Transrapid" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe><div><figure id="fig9"><a href="https://i.dllu.net/IMG_6801_654e71a21ee44795.jpg"><img src="https://i.dllu.net/IMG_6801_9c68db912482ad71.jpg" alt=""></a><figcaption><a href="#fig9">FIGURE 9</a> Waiting for a subway train to roll by in Brooklyn, New York.</figcaption></figure></div><h2 id="s3"><a href="#s3">3</a> <span>Detecting the region of interest</span></h2><p>Sometimes, I keep the line scan camera running for a while, and it generates tons of boring data of the background.
To detect moving things, I compute an “energy function” that’s defined as</p><p>where <img src="https://daniel.lawrence.lu/texcache/4ba7d3cff9565fbc8341f413e7711422cb61cc27i.svg" alt="\text{max}_\mathbf{I}"> is the maximum pixel value of the image, and the partial derivative are the <a href="https://en.wikipedia.org/wiki/Image_gradient">image gradient</a>.</p><div><figure id="fig10"><a href="https://i.dllu.net/score_sample_91b1560a90ba63f2.png"><img src="https://i.dllu.net/score_sample_8c65dd9f93d6c9aa.png" alt=""></a><figcaption><a href="#fig10">FIGURE 10</a> Example energy image.</figcaption></figure></div><p>This is because, for a static background, it will be full of horizontal stripes.
By weighing the <img src="https://daniel.lawrence.lu/texcache/11f6ad8ec52a2984abaafd7c3b516503785c2072i.svg" alt="x">-direction (time direction) gradient against the total gradient norm, we can find areas where it’s a more vertical-ish structure rather than a horizontal structure.
However, doing this by itself risks noisy gradients in empty (but noisy) areas where the gradient direction is completely random.
The maximum pixel value term ensures that whatever gradient we see is salient.</p><p>The image is divided into chunks and the score of a chunk is the 99th percentile energy.</p><p>Finally, chunks containing moving objects are defined to be ones where the score is at least 1.5× that of the minimum score.</p><p>This heuristic took me longer than I would like to admit to figure out.
Previously, I came up with heuristics that worked well on one capture but couldn’t generalize well to other captures.
Sometimes, the background will contain slowly moving foliage waving in the wind, that would screw up other methods of detection.
That resulted in a lot of wasted time because time spent processing empty regions seriously slows down iteration speed when developing the later steps.</p><h2 id="s4"><a href="#s4">4</a> <span>Speed estimation</span></h2><p>The most common question I get is, how do I estimate the speed of the subject?
If I don’t do it properly, it will appear stretched out, squished, or flipped.</p><p>Typically, I just set the camera to scan as fast as possible while maintaining a decent exposure, so the scan rate is independent of the subject.
Faster subjects will appear squished, and slower subjects will appear stretched out.</p><p>For most of my earlier works, I just eyeballed it. A good rule of thumb is to look for round things such as the wheels and “no smoking” signs.
But now I have a fully automated technique that works fairly robustly.</p><p>The key idea is to exploit the fact that the line scan camera actually has two lines in a <a href="https://en.wikipedia.org/wiki/Bayer_filter">Bayer array</a>, where one line is red, green, red, green, and the second line is green, blue, green blue.
By comparing the two green channels, we are able to figure out how fast stuff is moving.</p><p>The problem is that the data is very noisy, and salient features are sparse.
Here’s the general approach:</p><ul><li>Divide image into chunks.</li><li>Compute the absolute difference between the 2 green channels of each chunk for various small shifts (from -7 to +7). This gives us a cost array for each chunk.</li><li>Perform subpixel peak interpolation in the cost array using an iteratively reweighted Gaussian, <a href="https://en.wikipedia.org/wiki/Mean_shift">mean shift</a> style. This gives us a shift estimate per chunk.</li><li>Fit a robust spline to the shift estimates.</li></ul><div><figure id="fig11"><a href="https://i.dllu.net/mean_shift_0080_e3d219d77b34d22e.png"><img src="https://i.dllu.net/mean_shift_0080_c375801a598f46a4.png" alt=""></a><figcaption><a href="#fig11">FIGURE 11</a> Interpolating to find the peak using mean shift.</figcaption></figure></div><div><figure id="fig12"><a href="https://i.dllu.net/spline_8457468cc8c5d06a.png"><img src="https://i.dllu.net/spline_49ca48ada37ab8d2.png" alt=""></a><figcaption><a href="#fig12">FIGURE 12</a> Sample spacing and fitted spline.</figcaption></figure></div><p>As you can see, the data is noisy, but we have surprisingly decent granularity for this very subpixel case where we were scanning slower than needed so the spacing is like 0.5.</p><p>The value of the spline is actually the <em>sample spacing</em>. It tells us how close together or far apart the sample points in the original time series we should be using.
This leads us to the next section.</p><div><figure id="fig13"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_b83579bd26d10381.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_2e790d956d6045d7.jpg" alt=""></a><figcaption><a href="#fig13">FIGURE 13</a> Uncorrected left end. It’s squished!!!</figcaption></figure></div><div><figure id="fig14"><a href="https://i.dllu.net/rgb_2_prod_no_denoise_32cd95888d846ffa.jpg"><img src="https://i.dllu.net/rgb_2_prod_no_denoise_aeefb00dbe18bf6f.jpg" alt=""></a><figcaption><a href="#fig14">FIGURE 14</a> Uncorrected right end. It’s slightly squished but not nearly as much.</figcaption></figure></div><div><figure id="fig15"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_e458634bcd3cc936.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_b489258f43a8a411.jpg" alt=""></a><figcaption><a href="#fig15">FIGURE 15</a> Left end of New York subway train.</figcaption></figure></div><div><figure id="fig16"><a href="https://i.dllu.net/rgb_3_prod_no_denoise_fa356ca82a22b333.jpg"><img src="https://i.dllu.net/rgb_3_prod_no_denoise_6c643c2fa36cc924.jpg" alt=""></a><figcaption><a href="#fig16">FIGURE 16</a> Right end of New York subway train.</figcaption></figure></div><p>Hmm, I think my speed estimation still isn’t perfect. It could be off by about 10%.
For future work, I think I might be able to extract features correspondences such as SIFT or LightGlue. Trains are full of repeating elements that are supposed to be evenly spaced. I can detect those, and add a cost function to evenly space them, and optimize.
Another idea is to use a circle Hough transform to find circles.</p><h2 id="s5"><a href="#s5">5</a> <span>Resampling</span></h2><p>From the spline that gives us the sample spacing, we can basically generate the samples as such:</p><div><pre><span></span><span>samples</span> <span>=</span> <span>[]</span>
<span>sample_position</span> <span>=</span> <span>0.0</span>
<span>while</span> <span>sample_position</span> <span>&lt;</span> <span>raw_width</span><span>:</span>
    <span>samples</span><span>.</span><span>append</span><span>(</span><span>sample_position</span><span>)</span>
    <span>sample_position</span> <span>+=</span> <span>spline</span><span>(</span><span>sample_position</span><span>)</span>
</pre></div>
<p>However, there are a few gotchas:</p><ul><li>If the spline is negative-valued, it means the subject is going the other way, i.e. the image is flipped. In this case, I start with <code>sample_position</code> set to <code>raw_width</code> and go from right to left.</li><li>If the spline goes to zero, we are doomed because the while loop will never terminate. I clamp the steps to at least 0.1 and throw an error if the spline has both positive and negative values.</li><li>This is sort of a naive integration compared to the trapezoidal rule or something. However, given that the spline moves very slowly, it is fine.</li></ul><p>Now, for each sample position, we also store the sample width, which is the value of the spline.
If we were to simply extract a single column from the raw data, we would be throwing away a lot of data, and the result wouldn’t be antialiased.
Instead, it is better to pick a window of width proportional to the sample spacing.
I chose a <a href="https://en.wikipedia.org/wiki/Hann_function">Hann window</a>.</p><div><figure id="fig17"><a href="https://i.dllu.net/2025-08-19-12-10-58_b5c58484522b4579.png"><img src="https://i.dllu.net/2025-08-19-12-10-58_26c04e2f3b5d16e1.png" alt=""></a><figcaption><a href="#fig17">FIGURE 17</a> Naively selecting columns instead of using a windowing function.</figcaption></figure></div><div><figure id="fig18"><a href="https://i.dllu.net/2025-08-19-12-11-19_006f733f714d000c.png"><img src="https://i.dllu.net/2025-08-19-12-11-19_1c8c1f1331cf0c62.png" alt=""></a><figcaption><a href="#fig18">FIGURE 18</a> Using a rectangular window.</figcaption></figure></div><p>Not only is the first image very grainy, but the rapidly blinking LED display showing the characters for 筲箕灣 is completely illegible without proper sampling.</p><div><figure id="fig19"><a href="https://i.dllu.net/2025-08-21-23-01-25_b836f51e7b72da06.png"><img src="https://i.dllu.net/2025-08-21-23-01-25_f6d4fd8579a3ee52.png" alt=""></a><figcaption><a href="#fig19">FIGURE 19</a> Upsampling using a rectangular window.</figcaption></figure></div><div><figure id="fig20"><a href="https://i.dllu.net/2025-08-21-23-00-57_cf42ac3a31b6cdf8.png"><img src="https://i.dllu.net/2025-08-21-23-00-57_89de76f62fa2ad89.png" alt=""></a><figcaption><a href="#fig20">FIGURE 20</a> Upsampling using a Hann window.</figcaption></figure></div><p>As you can see, the rectangular window performs very poorly when upsampling and introduces horrible jagged artifacts. The Hann window does better. Some other windows like the Sinc are even better supposedly.</p><h2 id="s6"><a href="#s6">6</a> <span>Demosaicing</span></h2><p>Recall that the camera has two lines forming a Bayer array.</p><p>If we simply create an image of half resolution (i.e. 2048 pixels tall instead of 4096), by grouping each RGGB group into one pixel, we would have some nasty fringing problems since the red and blue pixels are offset.</p><div><figure id="fig21"><a href="https://i.dllu.net/2025-08-19-17-57-15_ea319c5f717e47df.png"><img src="https://i.dllu.net/2025-08-19-17-57-15_35d9b99165d33e0a.png" alt=""></a><figcaption><a href="#fig21">FIGURE 21</a> Fringing due to bad demosaicing.</figcaption></figure></div><div><figure id="fig22"><a href="https://i.dllu.net/2025-08-19-17-57-35_41dc85cc57217678.png"><img src="https://i.dllu.net/2025-08-19-17-57-35_92ba71ba7bbc7152.png" alt=""></a><figcaption><a href="#fig22">FIGURE 22</a> Better.</figcaption></figure></div><p>Instead we should write out the image with careful attention to offsets, interpolating as necessary.
Note that the horizontal offsets must be done <em>after</em> speed estimation, because, before speed estimation, the <img src="https://daniel.lawrence.lu/texcache/11f6ad8ec52a2984abaafd7c3b516503785c2072i.svg" alt="x">-axis is time, and after speed estimation, the <img src="https://daniel.lawrence.lu/texcache/11f6ad8ec52a2984abaafd7c3b516503785c2072i.svg" alt="x">-axis is space.
But the 2-pixel wide Bayer array is physically a <em>spatial</em> offset.</p><p>I implemented a basic interpolation scheme that uses bilinear interpolation.
This fixes most of the fringing, although we can do even better. That will be left for future work.</p><p>Unlike a traditional Bayer array, here we have the possibility that the green channels cover 100% of the pixels, so we can potentially do better than traditional demosaicing algorithms.
But there’s currently an annoying problem, which is that the two green channels on my line scan camera don’t match.</p><h2 id="s7"><a href="#s7">7</a> <span>Getting rid of the vertical stripes</span></h2><p>Vertical stripes in the image are common and are due to two main reasons:</p><ul><li>Clock jitter. The exposure time of each column may be randomly slightly off for some reason.</li><li>I’ve noticed that when a dark object shows up, like the coupling between train cars, the whole slice of the image there is brighter.</li></ul><p>To fix this, I use linear regression to fit a basic model of the form:</p><p>where <img src="https://daniel.lawrence.lu/texcache/86f7e437faa5a7fce15d1ddcb9eaeaea377667b8i.svg" alt="a">, <img src="https://daniel.lawrence.lu/texcache/e9d71f5ee7c92d6dc9e92ffdad17b8bd49418f98i.svg" alt="b">, and <img src="https://daniel.lawrence.lu/texcache/84a516841ba77a5b4648de2cd0dfcb30ea46dbb4i.svg" alt="c"> are scalar parameters of the model, <img src="https://daniel.lawrence.lu/texcache/5fafb73566c2a261cb4f5b35b6642b16d8284d3di.svg" alt="\mathbf{x}"> is a 2048-element vector containing the luminance value of the column (mean over the 4 channels), and <img src="https://daniel.lawrence.lu/texcache/ba360d8ea75d540ca5e1b1ce009ddbe6cd0691e4i.svg" alt="\mathbf{k}"> is the row index (aka the 2048-element vector of <img src="https://daniel.lawrence.lu/texcache/6b8d068c8ae69b58d2484e0ad9b0f6576d9bd529i.svg" alt="\begin{bmatrix}0, 1, \cdots, 2047\end{bmatrix}">.</p><p>You can compose models as such:</p><p>This gives us a new model <img src="https://daniel.lawrence.lu/texcache/c1086bb06dd2795dc0b7c7de2b953422fa2bc22di.svg" alt="\text{model}_{12}"> with parameters:</p><p>The associative property of the composition operator is left as an exercise for the reader.</p><p>There is also the identity model, i.e. one that does nothing, which is</p><p>and there’s also the inverse:</p><p>so the set of these models forms a mathematical group.</p><p>I fit a model to each consecutive pair of columns using weighted least squares, where we assign each row element a weight based on a Gaussian.
The weight would be:</p><p>In other words, the residual would be</p><p>After fitting this model, we redo the steps again several times, where the <img src="https://daniel.lawrence.lu/texcache/5d3a9194bf53d4e2741ee6cf67ae186b5d22770ai.svg" alt="\mathbf{w}"> vector is recalculated each time.
This is known as <em>iteratively-reweighted least squares</em> and is pretty good at rejecting outliers.</p><div><figure id="fig23"><a href="https://i.dllu.net/jitter_debug_082121e08533fc28.png"><img src="https://i.dllu.net/jitter_debug_c28f675b057b422f.png" alt=""></a><figcaption><a href="#fig23">FIGURE 23</a> The first plot shows the current column’s luminance and previous column’s luminance, as well as the previous column corrected by the model. The second plot shows the weight. The third plot shows the weighted initial and final error.</figcaption></figure></div><p>This all gives us <em>relative</em> models between the previous column and the current column, but we want <em>global</em>models that tell us how to correct each column overall.
We could set the global models by just composing them forever, but they would soon start to drift arbitrarily far away from the identity model.</p><p>You could prevent them from drifting away by solving a band-diagonal linear system where you have residuals of two types:</p><ul><li>prior residual, penalizing the difference between each model from the identity</li><li>relative model residual, penalizing the difference between the delta between adjacent models and the relative model we computed</li></ul><p>This can be solved in <img src="https://daniel.lawrence.lu/texcache/ebc75cd71fe8ecc45d16e8fbe4ca608d05d1efe0i.svg" alt="O(n)">. However, it is a bit of work to implement. In practice, you can mitigate most high frequency stripes by just doing <a href="https://en.wikipedia.org/wiki/Exponential_smoothing">exponential smoothing</a>, which basically acts as a high-pass filter</p><p>for some small <img src="https://daniel.lawrence.lu/texcache/b3931f1ce298c536432fd324b3a1ab4337120689i.svg" alt="\lambda">, in this case hardcoded to be 0.02.</p><div><figure id="fig24"><a href="https://i.dllu.net/2025-08-21-23-06-05_7f2b75d765c999be.png"><img src="https://i.dllu.net/2025-08-21-23-06-05_c5de9c2fbf369030.png" alt=""></a><figcaption><a href="#fig24">FIGURE 24</a> Before. You can see rather subtle stripes in the dark area.</figcaption></figure></div><div><figure id="fig25"><a href="https://i.dllu.net/2025-08-21-23-06-21_d6e64e01704c96d1.png"><img src="https://i.dllu.net/2025-08-21-23-06-21_a3b1e6747c4f6c1c.png" alt=""></a><figcaption><a href="#fig25">FIGURE 25</a> After.</figcaption></figure></div><p>Previously, I also had some success by directly fitting the model to line up each column with the first column. However, it doesn’t work for captures where the background isn’t static (e.g. rotating line scan panoramas, and pointing the line scan camera out of a moving train).</p><p>By the way, I should point out that vertical stripes getting rid of should be done <em>before</em> speed estimation, since it happens in the time domain at capture time.
If a train were speeding up, it would appear stretched out at first, and squished at the end, and the striping would affect the end a lot more than the start.</p><h2 id="s8"><a href="#s8">8</a> <span>Denoising</span></h2><p>I implemented a patch-based denoiser, also known as <a href="https://en.wikipedia.org/wiki/Block-matching_and_3D_filtering">block matching</a>.
It works by making the observation that you often have repeated textures in a line scan photo of a train.
Technically, you also have lots of self-similarity in general photos, so patch-based denoising is a common method for denoising in general.
However, one important distinction is that most denoisers only look in a small neighborhood around the current patch, but mine looks along the entire row.</p><p>What I do is, for each row, we process it independently.
From each 3×3 pixel patch, we can construct a <em>feature vector</em> of size 27 (9 times 3 channels, RGB).
Then, we collect all these features and sort them by mean value.
Now, for each position along the row, we search in the window of size 128 in the sorted vector.
The sorted vector will have similar-looking patches nearby, but we further weigh them by Gaussian similarity to the current patch.
Then, we compute the weighted average of the center pixel of each of those patches.</p><p>Another trick is to realize that the noise is Poisson-distributed which has a standard deviation that scales with the square root of the signal.
But if I just square root the input data first, then we just need to compare it to a constant.</p><p>This works decently, but is incredibly slow.
Let me know if you think of any faster ways to do it. A KD tree in feature space would die from the curse of dimensionality. Perhaps a hash table? To keep things lightweight, we can limit the population in each cell.</p><div><figure id="fig26"><a href="https://i.dllu.net/2025-08-20-16-08-55_264411aaba797e0c.png"><img src="https://i.dllu.net/2025-08-20-16-08-55_648c53661a3709d9.png" alt=""></a><figcaption><a href="#fig26">FIGURE 26</a> Noisy watch.</figcaption></figure></div><div><figure id="fig27"><a href="https://i.dllu.net/2025-08-20-16-08-30_6c3c0102c46a5803.png"><img src="https://i.dllu.net/2025-08-20-16-08-30_7db8ed848f17bc87.png" alt=""></a><figcaption><a href="#fig27">FIGURE 27</a> Denoised watch.</figcaption></figure></div><div><figure id="fig28"><a href="https://i.dllu.net/2025-08-20-16-09-34_8be193ad950c4151.png"><img src="https://i.dllu.net/2025-08-20-16-09-34_4440c8997d90f81e.png" alt=""></a><figcaption><a href="#fig28">FIGURE 28</a> Noisy passenger.</figcaption></figure></div><div><figure id="fig29"><a href="https://i.dllu.net/2025-08-20-16-09-59_dbce64ce63ce8080.png"><img src="https://i.dllu.net/2025-08-20-16-09-59_49cd78feafeaebf8.png" alt=""></a><figcaption><a href="#fig29">FIGURE 29</a> Denoised passenger.</figcaption></figure></div><p>The good thing about the patch-based denoiser is that unique features like this passenger remain virtually unchanged.</p><p>Previously, I also tried using a <a href="https://en.wikipedia.org/wiki/Total_variation_denoising">total variation denoiser</a>, processing each row and column independently.
It worked decently but would often destroy fine detail in textures.</p><h2 id="s9"><a href="#s9">9</a> <span>Skew correction</span></h2><p>If the camera isn’t perfectly upright, the resulting image may be slightly skewed.
I’m planning on implementing automatic skew correction.
But here are two caveats:</p><ul><li>skew detection must be done after speed estimation</li><li>proper sampling should happen after skew detection, since the skew transformation introduces generation loss and we can sample directly from the raw data instead.</li></ul><p>So basically we’d need to generate a quick, poorly sampled version, run skew detection on it, and then sample it properly afterwards.
We can implement skew detection using a <a href="https://daniel.lawrence.lu/blog/y2025m09d21/Hough%20transform">Hough transform</a>.
Generally, I do a decent job of keeping the camera upright, so we just need to correct for very small skews, so a Hough transform is suitable (since the complexity scales with the number of bins of the histogram).
We can also use the energy function from the region of interest detector to primarily care about vertical structures.</p><h2 id="s10"><a href="#s10">10</a> <span>Color calibration</span></h2><p>I kinda just eyeballed this color calibration matrix.</p><p>But to be honest it looks fairly decent.</p><div><figure id="fig30"><a href="https://i.dllu.net/rgb_7_prod_no_denoise_2474ac2d89335c15.jpg"><img src="https://i.dllu.net/rgb_7_prod_no_denoise_e7ce15b1872fe528.jpg" alt=""></a><figcaption><a href="#fig30">FIGURE 30</a> People’s skin tones look fine to me.</figcaption></figure></div><h2 id="s11"><a href="#s11">11</a> <span>Implementation details</span></h2><p>The code is implemented in Python using numpy.</p><p><a href="https://github.com/dllu/nectar/blob/master/python/preview.py">the code</a></p><p>Due to the large size of the data (4096 rows and hundreds of thousands of columns), it is sometimes impossible to fit all of it in memory, so the code takes several passes and outputs in chunks.
Actually, it is probably okay to fit it in a few gigabytes of RAM, but you’d have to chunk up the storage (there’s no way a contiguous numpy array of 4096 by 100,000 could be allocated).</p><h2 id="s11.1"><a href="#s11.1">11.1</a> <span>Vibe coding experience</span></h2><p>I tried using AI to help with a lot of the implementation. However, the results were mixed.</p><p>AI would often accidentally make things quadratic for no reason when a linear time algorithm would suffice. For example, when trying to implement spline-based resampling, ChatGPT 5 came up with horribly slow (but vectorized) code that constructed a giant tensor with a mask across the entire width of the image for <em>every single sample</em>. Since there are 100,000 samples, and each mask was 100,000 columns wide, you can imagine it would take millennia to run. I ended up reimplementing it from scratch by hand. Then Grok 4 implemented weighted least squares regression by materializing the entire weight vector with <code>np.diag</code> instead of simply pre-multiplying each row of <code>A</code> and <code>x</code> with the square root of the weight before doing <code>np.linalg.solve(A, x)</code>. Again, with 100,000 elements, making the square matrix with <code>np.diag</code> would have instantly run out of memory.</p><p>Both Grok 4 Expert and ChatGPT 5 Thinking also completely failed to implement the band-diagonal least squares to my vertical stripes strategy, but as mentioned, the exponential smoothing trick works okay for now.</p><p>However, for some other stuff, AI was quite helpful.
It created a class that dynamically loads chunks from disk but provides the API to index and slice it.
That was neat.
AI was also incredibly good at helping with Matplotlib’s arcane syntax.</p><h2 id="s12"><a href="#s12">12</a> <span>Other people’s line scan photography of trains</span></h2><h2 id="s12.1"><a href="#s12.1">12.1</a> <span>Adam Magyar</span></h2><p><a href="https://www.magyaradam.com/wp/">Adam Magyar</a> uses a black and white digital line scan camera for his “Stainless” project, and another derived from a scanner for his “Urban Flow” project.</p><p>His camera must have much better sensitivity than mine since he managed to capture fairly clean images even for underground trains (whereas I generally require sunlight for mine).
Apparently, he had to scout out many subway stations to find ones where the lights don’t flicker at 60 Hz.</p><h2 id="s12.2"><a href="#s12.2">12.2</a> <span>KR64’s blog</span></h2><p>At <a href="https://web.archive.org/web/20250715102540/https://kr64.seesaa.net/">kr64.seesaa.net</a> you can find a mind boggling collection of high quality line scan photos of trains from all across Japan.</p><p>They probably do this full time as the variety of trains is far greater than I can ever hope to achieve.
I believe they use a film slit scan camera.</p><p>Unfortunately, their website has a bunch of technical issues and often goes down. I would be happy to help them out but my Japanese is very poor and I don’t see any way to contact them.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RFC 9839 and Bad Unicode (222 pts)]]></title>
            <link>https://www.tbray.org/ongoing/When/202x/2025/08/14/RFC9839</link>
            <guid>44995640</guid>
            <pubDate>Sat, 23 Aug 2025 12:54:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tbray.org/ongoing/When/202x/2025/08/14/RFC9839">https://www.tbray.org/ongoing/When/202x/2025/08/14/RFC9839</a>, See on <a href="https://news.ycombinator.com/item?id=44995640">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="centercontent">
<p itemprop="description">Unicode is good. If you’re designing a data structure or protocol that has text fields, they should contain
    Unicode characters encoded in UTF-8. There’s another question, though:
    “<em>Which</em> Unicode characters?” The 
    answer is “Not all of them, please exclude some.”</p>

<p>This issue keeps coming up, so Paul Hoffman and I put together an individual-submission draft
    to the IETF and now (where by “now” I mean “two years later”) it’s been published as
    <a href="https://www.rfc-editor.org/rfc/rfc9839.html">RFC 9839</a>. It explains which characters are bad, and why, then offers
    three plausible less-bad subsets that you might want to use.
    Herewith a bit of background, but…</p>

<p id="p-2"><span>Please</span> · 
If you’re actually working on something new that will have text fields, please read the RFC. It’s only ten pages long, and that’s
    with all the IETF boilerplate. It’s written specifically for software and networking people.</p>

<p id="p-3"><span>The smoking gun</span> · 
The badness that 9839 focuses on is “problematic characters”, so let’s start with a painful example of what that means.
    Suppose you’re designing a protocol that uses JSON and one of your constructs has a <code>username</code> field.
    Suppose you get this message (I omit all the non-<code>username</code> fields). It’s 
    a perfectly legal JSON text:</p>

<div><pre><span></span><span>{</span>
  <span>  </span><span>"username"</span><span>:</span><span> </span><span>"\u0000\u0089\uDEAD\uD9BF\uDFFF"</span>
<span>}</span>    </pre></div>
    <p>Unpacking all the JSON escaping gibberish reveals that the value of the <code>username</code> field contains four 
    numeric “code points” identifying Unicode characters:</p>

    <ol>
      <li><p>The first code point is zero, in Unicode jargon <code>U+0000</code>. In human-readable text it
      has no meaning, but it will interfere with the operation of certain programming languages.</p>
</li>
      <li><p>Next is Unicode <code>U+0089</code>, official name “CHARACTER TABULATION WITH JUSTIFICATION”. It’s what Unicode calls a
      <a href="https://en.wikipedia.org/wiki/C0_and_C1_control_codes">C1
      control code</a>, inherited from ISO/IEC 6429:1992, adopted from 
      <a href="https://www.ecma-international.org/wp-content/uploads/ECMA-48_5th_edition_june_1991.pdf">ECMA 48</a> (1991), which calls it
      “HTJ” and says: <i>HTJ causes the contents of the active field (the field in the presentation component that contains the
      active presentation position) to be shifted forward so that it ends at the character position preceding the
      following character tabulation stop. The active presentation position is moved to that following character
      tabulation stop. The character positions which precede the beginning of the shifted string are put into the
      erased state.</i></p>

      <p>Good luck with that.</p>
</li>
      <li><p>The third code point, <code>U+DEAD</code>, in Unicode lingo, is an “unpaired surrogate”.  To understand,
      you’d have to learn how Unicode’s much-detested
      <a href="https://en.wikipedia.org/wiki/UTF-16">UTF-16</a> encoding works.
      I recommend not bothering.</p>

      <p>All you need to know is that surrogates are only meaningful when they come in pairs in UTF-16 encoded text. There is
      effectively no such text on the wire and thus no excuse for tolerating surrogates in your data. In fact, the UTF-8 specification
      says that you mustn’t use UTF-8 to encode surrogates. But the real problem is that different libraries in different
      programming languages don’t always do the same things when they encounter this sort of fœtid interloper.</p>
    </li>
      <li><p>Finally, <code>\uD9BF\uDFFF</code> is JSON for the code point <code>U+7FFFF</code>.
      Unicode has a category called “noncharacter”, containing a few dozen code points that, for a variety of
      reasons, some good, 
      don’t represent anything and must not be interchanged on the wire. <code>U+7FFFF</code> is one of those.</p>
</li>
    </ol>
    <p>The four code points in the example are all clearly problematic. 
    The just-arrived RFC 9839 formalizes the notion of “problematic” and
    offers easy-to-cite language saying which of these problematic types you want to
    exclude from your text fields. Which, if you’re going to use JSON, you should probably do.</p>

    <p id="p-6"><span>Don’t blame Doug</span> · 
    Doug Crockford I mean, the inventor of JSON.  If he (or I or really anyone careful) were inventing JSON now that Unicode is
    mature, he’d have been fussier about its character repertoire. Having said that, we’re stuck with JSON-as-it-is forever, so we
    need a good way to say which of the problematic characters we’re going to exclude even if JSON allows them.</p>

    <p id="p-5"><span>PRECISion</span> · 
    You may find yourself wondering why the IETF waited until 2025 to provide help with Bad Unicode.
    It didn’t; here’s
    <a href="https://www.rfc-editor.org/rfc/rfc8264.html">RFC 8264</a>: <cite>PRECIS Framework: Preparation, Enforcement, and
    Comparison of Internationalized Strings in Application Protocols</cite>; the first PRECIS predecessor was published in 2002.
    8264 is 43 pages long, containing a <em>very</em>
    thorough discussion of many more potential Bad Unicode issues than 9839 does.</p>

    <p>Like 9839, PRECIS specifies subsets of the Unicode character repertoire and goes further, providing a mechanism for defining
    more.</p>

    <p>Having said that, PRECIS doesn’t seem to be very widely used by people who are defining new data structures and protocols. My
    personal opinion is that there are two problems which make it hard to adopt. First, it’s large and 
    complex, with many moving parts, and requires careful study to understand. Developers are (for good reason) lazy.</p>

    <p>Second, using PRECIS ties you to a specific version of Unicode. In particular, it forbids the use of the (nearly a million)
    unassigned code points. Since each release of Unicode includes new code point assignments, that means that a sender and receiver
    need to agree on exactly which version of Unicode they’re both going to use if they want reliably interoperable behavior. This
    makes life difficult for anyone writing a general-purpose code designed to be used in lots of different applications.</p>

    <p>I personally think that the only version of Unicode anybody wants to use is “as recent as possible”, so they can be confident
    of having all the latest emojis.</p>

    <p>Anyhow, 9839 is simpler and dumber than PRECIS. But I think some people will find it useful and now the IETF agrees.</p>

    <p id="p-7"><span>Source code</span> · 
    I’ve written a little Go-language library to validate incoming text fields against each of the three subsets that 9839
    specifies,
    <a href="https://github.com/timbray/RFC9839">here</a>.  I don’t claim it’s optimal, but it is well-tested.</p>

    <p>It doesn’t have a version number or release just yet, I’ll wait till a few folk have had a chance to spot any dumb mistakes I
    probably made.</p>

    <p id="p-9"><span>Details</span> · 
    Here’s a compact summary of the world of problematic Unicode code points and data formats and standards.</p>

    <table>
      <tbody><tr><td></td><th colspan="3">Problematic classes excluded?</th></tr>
      <tr><td></td><th>Surrogates</th><th>Legacy controls</th><th>Noncharacters</th></tr>
      <tr><td>CBOR</td><td>yes</td><td>no</td><td>no</td></tr>
      <tr><td>I-JSON</td><td>yes</td><td>no</td><td>yes</td></tr>
      <tr><td>JSON</td><td>no</td><td>no</td><td>no</td></tr>
      <tr><td>Protobufs</td><td>no</td><td>no</td><td>no</td></tr>
      <tr><td>TOML</td><td>yes</td><td>no</td><td>no</td></tr>
      <tr><td>XML</td><td>yes</td><td>partial [1]</td><td>partial [2]</td></tr>
      <tr><td>YAML</td><td>yes</td><td>mostly [3]</td><td>partial [2]</td></tr>
      <tr><td></td><th colspan="3">RFC 9839 Subsets</th></tr>
      <tr><td>Scalars</td><td>yes</td><td>no</td><td>no</td></tr>
      <tr><td>XML</td><td>yes</td><td>partial</td><td>partial</td></tr>
      <tr><td>Assignables</td><td>yes</td><td>yes</td><td>yes</td></tr>
    </tbody></table>
    <p>Notes:</p>

    <p><b>[1]</b> XML allows C1 controls.</p>

    <p><b>[2]</b> XML and YAML don’t exclude the noncharacters outside the Basic Multilingual Pane.</p>

    <p><b>[3]</b> YAML excludes all the legacy controls except for the mostly-harmless <code>U+0085</code>, another version of
    <code>\n</code> used in IBM mainframe documents.</p>

    <p id="p-8"><span>Thanks!</span> · 
    9839 is not a solo production. It received an extraordinary amount of discussion and improvement from a lot of smart and
    well-informed people 
    and the published version, 15 draft revisions later, is immensely better than my initial draft. My sincere thanks go to my
    co-editor Paul Hoffman and to all those mentioned in the RFC’s “Acknowledgements” section.</p>

    <p id="p-4"><span>On individual submissions</span> · 
    9839 is the second “individual submission” RFC I’ve pushed through the IETF (the other is
    <a href="https://datatracker.ietf.org/doc/html/rfc7725">RFC 7725</a>, which registers the HTTP 451 status code).  While it’s nice
    to decide something is worth standardizing and eventually have that happen, it’s really a lot of work. Some of that work is
    annoying.</p>

    <p>I’ve been involved in
    other efforts as Working-Group member, WG chair, and WG specification editor, and I can report authoritatively that creating an
    RFC the traditional way, through a Working Group, is easier and better.</p>

    <p>I feel discomfort advising others not to follow in my footsteps, but in this case I think it’s the right advice.</p>

  <hr>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing Speed-of-Light Flash Attention for 5090 in CUDA C++ (138 pts)]]></title>
            <link>https://gau-nernst.github.io/fa-5090/</link>
            <guid>44995508</guid>
            <pubDate>Sat, 23 Aug 2025 12:29:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gau-nernst.github.io/fa-5090/">https://gau-nernst.github.io/fa-5090/</a>, See on <a href="https://news.ycombinator.com/item?id=44995508">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><header><p><time>Aug 23, 2025</time></p></header><section><p>In this post, I will walkthrough how I learned to implement Flash Attention for 5090 in CUDA C++. The main objective is to learn writing attention in CUDA C++, since many features are not available in <a href="https://triton-lang.org/main/index.html">Triton</a>, such as MXFP8 / NVFP4 MMA for sm120. I also feel this is a natural next step after learning about matmul kernels. Lastly, there are <a href="https://alexarmbr.github.io/2024/08/10/How-To-Write-A-Fast-Matrix-Multiplication-From-Scratch-With-Tensor-Cores.html">many</a> <a href="https://www.spatters.ca/mma-matmul">excellent</a> <a href="https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog">blogposts</a> on writing fast matmul kernels, but there is none for attention. So I want to take this chance to write up something nicely.</p><p>Readers are highly recommended to be familiar with CUDA C++ and how to use Tensor cores on NVIDIA GPUs. Of course you can still read along and clarify with your favourite LLMs along the way. Or you can check out GPU-MODE series (<a href="https://github.com/gpu-mode/lectures">slides</a>, <a href="https://www.youtube.com/@GPUMODE">YouTube</a>) for basic CUDA C++ knowledge, as well as the excellent matmul blogposts mentioned above, to quickly get up to speed.</p><p>You can find the full implementation discussed in this post here: <a href="https://github.com/gau-nernst/learn-cuda/tree/e83c256/07_attention">https://github.com/gau-nernst/learn-cuda/tree/e83c256/07_attention</a>. For <code>bs=1, num_heads=8, len_query=4096, len_kv = 8192</code>, 5090 @ 400W, compile with CUDA 12.9, I obtained the following benchmark results (theoretical limit of 5090 is 209.5 TFLOPS for BF16)</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td><code>F.sdpa()</code> (Flash Attention)</td><td>186.73</td><td>89.13%</td></tr><tr><td><code>F.sdpa()</code> (CuDNN)</td><td>203.61</td><td>97.19%</td></tr><tr><td><code>flash-attn</code></td><td>190.58</td><td>90.97%</td></tr><tr><td>v1 (basic)</td><td>142.87</td><td>68.20%</td></tr><tr><td>v2 (shared memory swizzling)</td><td>181.11</td><td>86.45%</td></tr><tr><td>v3 (2-stage pipelining)</td><td>189.84</td><td>90.62%</td></tr><tr><td>v4 (<code>ldmatrix.x4</code> for K and V)</td><td>194.33</td><td>92.76%</td></tr><tr><td>v5 (better pipelining)</td><td>197.74</td><td>94.39%</td></tr></tbody></table><p>Do note that although I only use Ampere features in these implementations (sm120 supports <code>cp.async.bulk</code> i.e. TMA, but I don’t use it here), my implementations might not run performantly on earlier generations of GPUs. Due to improvements in newer hardware, you might need to use more tricks to reach Speed-of-Light on older GPUs e.g. pipeline shared memory to register memory data movements.</p><h2 id="flash-attention-algorithm">Flash Attention algorithm</h2><p>Let’s start with the reference implementation of attention.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> torch <span>import</span> Tensor
</span></span><span><span>
</span></span><span><span><span>def</span> <span>sdpa</span>(q: Tensor, k: Tensor, v: Tensor):
</span></span><span><span>    <span># q: [B, Lq, DIM]</span>
</span></span><span><span>    <span># k: [B, Lk, DIM]</span>
</span></span><span><span>    <span># v: [B, Lk, DIM]</span>
</span></span><span><span>    D <span>=</span> q<span>.</span>shape[<span>-</span><span>1</span>]
</span></span><span><span>    scale <span>=</span> D <span>**</span> <span>-</span><span>0.5</span>
</span></span><span><span>    attn <span>=</span> (q <span>@</span> k<span>.</span>transpose(<span>-</span><span>1</span>, <span>-</span><span>2</span>)) <span>*</span> scale  <span># [B, Lq, Lk]</span>
</span></span><span><span>    attn <span>=</span> attn<span>.</span>softmax(dim<span>=-</span><span>1</span>)
</span></span><span><span>    out <span>=</span> attn <span>@</span> v  <span># [B, Lq, DIM]</span>
</span></span><span><span>    <span>return</span> out
</span></span></code></pre></div><p>Technically, if the inputs are BF16, some computations should remain in FP32, especially softmax. However, for brevity, we omit them.</p><p>We are implementing the algorithm outlined in the <a href="https://arxiv.org/abs/2307.08691">Flash Attention 2 paper</a>. Each threadblock is responsible for a chunk of Q, and we will iterate along the sequence length of KV. A Python-like outline of the algorithm looks like below (S and P follow Flash Attention notation).</p><div><pre tabindex="0"><code data-lang="python"><span><span>scale <span>=</span> DIM <span>**</span> <span>-</span><span>0.5</span>
</span></span><span><span><span>for</span> b_idx <span>in</span> range(B):
</span></span><span><span>    <span>for</span> tile_Q_idx <span>in</span> range(Lq <span>//</span> BLOCK_Q):
</span></span><span><span>        <span>### start of each threadblock's kernel</span>
</span></span><span><span>        tile_O <span>=</span> torch<span>.</span>zeros(BLOCK_Q, DIM)
</span></span><span><span>        tile_Q <span>=</span> load_Q(b_idx, tile_Q_idx)  <span># [BLOCK_Q, DIM]</span>
</span></span><span><span>
</span></span><span><span>        <span>for</span> tile_KV_idx <span>in</span> range(Lk <span>//</span> BLOCK_KV):
</span></span><span><span>            <span># first MMA: S = Q @ K.T</span>
</span></span><span><span>            <span># (BLOCK_Q, DIM) x (BLOCK_KV, DIM).T -&gt; (BLOCK_Q, BLOCK_KV)</span>
</span></span><span><span>            tile_Q                               <span># (BLOCK_Q, DIM)</span>
</span></span><span><span>            tile_K <span>=</span> load_K(b_idx, tile_KV_idx)  <span># (BLOCK_KV, DIM)</span>
</span></span><span><span>            tile_S <span>=</span> tile_Q <span>@</span> tile_K<span>.</span>T           <span># (BLOCK_Q, BLOCK_KV)</span>
</span></span><span><span>            tile_S <span>=</span> tile_S <span>*</span> scale
</span></span><span><span>
</span></span><span><span>            <span># online softmax and rescale tile_O</span>
</span></span><span><span>            <span>...</span>
</span></span><span><span>
</span></span><span><span>            <span># second MMA: O = P @ V</span>
</span></span><span><span>            <span># (BLOCK_Q, BLOCK_KV) x (BLOCK_KV, DIM) -&gt; (BLOCK_Q, DIM)</span>
</span></span><span><span>            tile_P                               <span># (BLOCK_Q, BLOCK_KV)</span>
</span></span><span><span>            tile_V <span>=</span> load_V(b_idx, tile_KV_idx)  <span># (BLOCK_KV, DIM)</span>
</span></span><span><span>            tile_O <span>+=</span> tile_P <span>@</span> tile_V            <span># (BLOCK_Q, DIM)</span>
</span></span><span><span>
</span></span><span><span>        <span># normalize output and write results</span>
</span></span><span><span>        store_O(b_idx, tile_Q_idx)
</span></span><span><span>        <span>### end of each threadblock's kernel</span>
</span></span></code></pre></div><p>It’s implied <code>DIM</code> is small, so that we can hold <code>tile_Q</code> in register memory throughout the duration of the kernel. This is the reason pretty much all models nowadays use <code>head_dim=128</code>. There are exceptions of course, like <a href="https://arxiv.org/abs/2405.04434">MLA</a>, which uses <code>head_dim=576</code> for Q and K, and <code>head_dim=512</code> for V. Talking about this, I should study <a href="https://github.com/deepseek-ai/FlashMLA">FlashMLA</a> some day.</p><p>Online softmax is quite tricky to explain, so let’s delay the explanation of that part. At the high level, you just need to know that online softmax will transform <code>tile_S</code> to <code>tile_P</code>, and also rescale <code>tile_O</code>.</p><h2 id="version-1---basic-implementation">Version 1 - Basic implementation</h2><p>We will follow the typical MMA flow</p><ul><li>Load a 2D tile of data from global memory to shared memory using <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#data-movement-and-conversion-instructions-cp-async">cp.async</a>. This requires Ampere (sm80 and newer).</li><li>Load data from shared memory to register memory using <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-instructions-ldmatrix">ldmatrix</a>.</li><li>Call <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-fragment-mma-16816-float">mma.m16n8k16</a> for BF16 matrix multiplication (and accumulate).</li></ul><p>I want to focus on implementing the algorithm correctly first, hence I leave out more complicated tricks like shared memory swizzling and pipelining. This reduces the surface area for mistakes, and we will revisit them later for performance optimization.</p><h3 id="global-to-shared-memory-data-transfer">Global to Shared memory data transfer</h3><p>The following templated function does a 2D tile copy from global memory to shared memory.</p><ul><li>Shape of the 2D tile is specified via <code>HEIGHT</code> and <code>WIDTH</code>.</li><li><code>dst</code> is shared memory address, <code>src</code> is global memory address.</li><li>Global memory <code>src</code> is row-major, so <code>src_stride</code> specifies how much to move to the next row.</li><li>Shared memory <code>dst</code> is also row-major, and will be stored as a contiguous block -&gt; <code>dst_stride = WIDTH</code>.</li></ul><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>#include</span> <span>&lt;cuda_bf16.h&gt;</span><span>
</span></span></span><span><span><span></span>
</span></span><span><span><span>template</span> <span>&lt;</span><span>int</span> HEIGHT, <span>int</span> WIDTH, <span>int</span> TB_SIZE<span>&gt;</span>
</span></span><span><span>__device__ <span>inline</span>
</span></span><span><span><span>void</span> global_to_shared(<span>uint32_t</span> dst, <span>const</span> nv_bfloat16 <span>*</span>src, <span>int</span> src_stride, <span>int</span> tid) {
</span></span><span><span>  <span>constexpr</span> <span>int</span> num_elems <span>=</span> <span>16</span> <span>/</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>  <span>constexpr</span> <span>int</span> num_iters <span>=</span> HEIGHT <span>*</span> WIDTH <span>/</span> (TB_SIZE <span>*</span> num_elems);
</span></span><span><span>
</span></span><span><span>  <span>for</span> (<span>int</span> iter <span>=</span> <span>0</span>; iter <span>&lt;</span> num_iters; iter<span>++</span>) {
</span></span><span><span>    <span>const</span> <span>int</span> idx <span>=</span> (iter <span>*</span> TB_SIZE <span>+</span> tid) <span>*</span> num_elems;
</span></span><span><span>    <span>const</span> <span>int</span> row <span>=</span> idx <span>/</span> WIDTH;
</span></span><span><span>    <span>const</span> <span>int</span> col <span>=</span> idx <span>%</span> WIDTH;
</span></span><span><span>
</span></span><span><span>    <span>const</span> <span>uint32_t</span> dst_addr <span>=</span> dst <span>+</span> (row <span>*</span> WIDTH <span>+</span> col) <span>*</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>    <span>const</span> nv_bfloat16 <span>*</span>src_addr <span>=</span> src <span>+</span> (row <span>*</span> src_stride <span>+</span> col);
</span></span><span><span>    <span>asm</span> <span>volatile</span>(<span>"cp.async.cg.shared.global [%0], [%1], 16;"</span> <span>::</span> <span>"r"</span>(dst_addr), <span>"l"</span>(src_addr));
</span></span><span><span>  }
</span></span><span><span>}
</span></span></code></pre></div><figure><img src="https://gau-nernst.github.io/fa-5090/global_to_shared.svg" alt="Global to Shared data transfer"><figcaption><p>2D tile copy from Global memory to Shared memory.</p></figcaption></figure><p>We will use inline assembly to write <code>cp.async.cg.shared.global</code>. This PTX does 16-byte transfer, or 8 BF16 elements (<code>num_elems = 16 / sizeof(nv_bfloat16)</code>), for each CUDA thread. To ensure coalesced memory access, consecutive threads will be responsible for consecutive groups of 8xBF16.</p><figure><img src="https://gau-nernst.github.io/fa-5090/coalesced.svg" alt="Coalesced memory access"><figcaption><p>Consecutive threads are responsible for consecutive groups of 8xBF16.</p></figcaption></figure><p>Note:</p><ul><li>The loop <code>for (int iter = 0; iter &lt; num_iters; iter++)</code> is written this way so that the compiler (<code>nvcc</code>) can fully unroll the loop. <code>num_iters</code> is known at compile time (guaranteed by <code>constexpr</code>). If we mix <code>tid</code> in the loop, which is a “dynamic” variable to the compiler, the loop can’t be unrolled, even when we know certain constraints about the variable i.e. <code>tid &lt; TB_SIZE</code>.</li><li>Data type of shared memory pointer <code>dst</code> is <code>uint32_t</code>. This is intentional. Pretty much all PTX instructions expect shared memory addresses to be in <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#state-spaces">shared state space</a>. We can convert C++ pointers, which are generic addresses, to shared state space addresses with <code>static_cast&lt;uint32_t&gt;(__cvta_generic_to_shared(ptr))</code>. This is done outside of <code>global_to_shared()</code>.</li></ul><p>To finish using <code>cp.async</code>, we also need to add the following:</p><ul><li><code>cp.async.commit_group</code> (PTX): commit all previously issued <code>cp.async</code> instructions into a <strong><code>cp.async</code> group</strong>. This group will be the unit for synchronization.</li><li><code>cp.async.wait_all</code> (PTX): wait for all committed groups to finish.</li><li><code>__syncthreads()</code>: make sure all threads (in a threadblock) reach here before reading the loaded data in shared memory (because one thread may read data loaded by another thread). More importantly, this broadcasts <strong>visibility</strong> of the new data to all threads in the threadblock. Without <code>__syncthreads()</code>, the compiler is free to optimize away memory accesses.</li></ul><p>As always, refer to <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/">PTX doc</a> for more information about the instructions. Basically we issue multiple <code>cp.async</code> and wait for them to complete immediately right after. <code>commit_group</code> and <code>wait_group</code> provide a mechanism for us to implement pipelining later. But for now, just need to know we have to write it that way to use <code>cp.async</code>.</p><p>Our code snippet would look something like this.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// nv_bfloat16 *Q;
</span></span></span><span><span><span>// uint32_t Q_smem;
</span></span></span><span><span><span>// const int tid = blockIdx.x;
</span></span></span><span><span><span>// constexpr int TB_SIZE = 32 * 4;
</span></span></span><span><span><span>// constexpr int DIM = 128;
</span></span></span><span><span><span></span>
</span></span><span><span>global_to_shared<span>&lt;</span>BLOCK_Q, DIM, TB_SIZE<span>&gt;</span>(Q_smem, Q, DIM, tid);
</span></span><span><span><span>asm</span> <span>volatile</span>(<span>"cp.async.commit_group;"</span>);
</span></span><span><span><span>asm</span> <span>volatile</span>(<span>"cp.async.wait_all;"</span>);
</span></span><span><span>__syncthreads();
</span></span></code></pre></div><h3 id="shared-memory-to-register-memory-data-transfer">Shared memory to Register memory data transfer</h3><p>When doing global-&gt;shared data transfer, we think in terms of threadblock tiles and individual CUDA threads. For shared-&gt;register data transfer, since this is to service later MMA instructions, we think in terms of warp tiles/MMA tiles and warps. Following Flash Attention 2 (section 3.3), we let each warp in a threadblock handle a portion of <code>tile_Q</code>, splitting along the Q sequence length dimension. This means that different warps will index into different chunks of <code>tile_Q</code>, but they all index to the same <code>tile_K</code> and <code>tile_V</code> chunks in the KV-sequence-length loop.</p><figure><img src="https://gau-nernst.github.io/fa-5090/fa_warp_partition.svg" alt="Flash Attention warp partition"><figcaption><p>Warp partition in Flash Attention 2.</p></figcaption></figure><p>Since we are using <code>mma.m16n8k16</code> instruction, each MMA 16x8 output tile (<code>m16n8</code>) requires 16x16 A tile (<code>m16k16</code>) and 8x16 B tile (<code>n8k16</code>). <code>ldmatrix</code> can load one, two, or four 8x8 tile(s) of 16-bit elements. Hence,</p><ul><li>A tile <code>m16k16</code> requires four 8x8 tiles -&gt; <code>ldmatrix.x4</code></li><li>B tile <code>n8k16</code> requires two 8x8 tiles -&gt; <code>ldmatrix.x2</code></li></ul><p>Only Q acts as A in an MMA. Both K and V act as B in their MMAs, though K will require transposed <code>ldmatrix</code> for correct layout (all tensors use row-major layout in global and shared memory).</p><p>To use <code>ldmatrix</code>, each thread supplies address of a row. Threads 0-7 select the 1st 8x8 tile, threads 8-15 select the 2nd 8x8 tile, and so on. The <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-fragment-mma-16816-float">layout of A</a> in the official PTX documentation can look confusing. But it’s easier (at least for me) to focus on the order of 8x8 tiles within an MMA tile.</p><figure><img src="https://gau-nernst.github.io/fa-5090/ldmatrix.svg" alt="ldmatrix for MMA layout"><figcaption><p>Order of <code>ldmatrix</code> tiles in <code>mma.m16n8k16</code>.</p></figcaption></figure><p>With the visualisation above, I hope the following snippet makes sense</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>constexpr</span> <span>int</span> MMA_M <span>=</span> <span>16</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> MMA_N <span>=</span> <span>8</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> MMA_K <span>=</span> <span>16</span>;
</span></span><span><span>
</span></span><span><span><span>uint32_t</span> Q_smem;
</span></span><span><span><span>uint32_t</span> Q_rmem[WARP_Q <span>/</span> MMA_M][DIM <span>/</span> MMA_K][<span>4</span>];
</span></span><span><span>
</span></span><span><span><span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>)
</span></span><span><span>  <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_K; mma_id_d<span>++</span>) {
</span></span><span><span>    <span>const</span> <span>int</span> row <span>=</span> (warp_id <span>*</span> WARP_Q) <span>+</span> (mma_id_q <span>*</span> MMA_M) <span>+</span> (lane_id <span>%</span> <span>16</span>);
</span></span><span><span>    <span>const</span> <span>int</span> col <span>=</span> (mma_id_d <span>*</span> MMA_K) <span>+</span> (lane_id <span>/</span> <span>16</span> <span>*</span> <span>8</span>);
</span></span><span><span>    <span>const</span> <span>uint32_t</span> addr <span>=</span> Q_smem <span>+</span> (row <span>*</span> DIM <span>+</span> col) <span>*</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>    ldmatrix_x4(Q_rmem[mma_id_q][mma_id_d], addr);
</span></span><span><span>  }
</span></span></code></pre></div><ul><li>The two nested loops tile <code>[MMA_M, MMA_K]</code> (i.e. <code>[16, 16]</code>) over <code>[WARP_Q, DIM]</code> in shared memory.</li><li><code>(warp_id * WARP_Q)</code> selects the warp tile. We don’t need this for K and V.</li><li><code>(mma_id_q * MMA_M)</code> in <code>row</code> and <code>(mma_id_d * MMA_K)</code> in <code>col</code> selects the MMA tile.</li><li><code>(lane_id % 16)</code> in <code>row</code> and <code>(lane_id / 16 * 8)</code> in <code>col</code> select the correct row address for each thread, following the required Multiplicand A layout (see the figure above).</li></ul><p><code>ldmatrix_x4()</code> is a small wrapper around <code>ldmatrix.sync.aligned.m8n8.x4.b16</code> PTX for convenience. You can refer to <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/common.h">common.h</a> for more details.</p><p>K and V can be loaded from shared to register memory similarly. One thing to note is about the row-major / column-major layout when using <code>ldmatrix</code>. Regardless of whether <code>.trans</code> modifier is used, each thread still provides the row address of each row in 8x8 tiles. <code>.trans</code> only changes the <strong>register layout</strong> of <code>ldmatrix</code> results.</p><figure><img src="https://gau-nernst.github.io/fa-5090/ldmatrix_kv.svg" alt="ldmatrix for K and V"><figcaption><p>Use transposed version of <code>ldmatrix</code> for V.</p></figcaption></figure><p>One trick to know whether to use the transposed version of <code>ldmatrix</code> is to look at the K-dim or the reduction dimension. The 1st MMA’s K-dim is along <code>DIM</code> dimension, while the 2nd MMA’s K-dim is along the <code>BLOCK_KV</code> dimension.</p><h3 id="draft-version">Draft version</h3><p>We have the high-level tile-based design, and know how to load the data for MMA. Calling MMA is simple - just drop <code>mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32</code> PTX in our code. Our draft version looks like this.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>constexpr</span> <span>int</span> BLOCK_Q <span>=</span> <span>128</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> BLOCK_KV <span>=</span> <span>64</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> DIM <span>=</span> <span>128</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> NUM_WARPS <span>=</span> <span>4</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> TB_SIZE <span>=</span> NUM_WARPS <span>*</span> <span>32</span>;
</span></span><span><span>
</span></span><span><span><span>// mma.m16n8k16
</span></span></span><span><span><span></span><span>constexpr</span> <span>int</span> MMA_M <span>=</span> <span>16</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> MMA_N <span>=</span> <span>8</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> MMA_K <span>=</span> <span>16</span>;
</span></span><span><span>
</span></span><span><span>__global__
</span></span><span><span><span>void</span> <span>attention_v1_kernel</span>(
</span></span><span><span>  <span>const</span> nv_bfloat16 <span>*</span>Q,  <span>// [bs, len_q, DIM]
</span></span></span><span><span><span></span>  <span>const</span> nv_bfloat16 <span>*</span>K,  <span>// [bs, len_kv, DIM]
</span></span></span><span><span><span></span>  <span>const</span> nv_bfloat16 <span>*</span>V,  <span>// [bs, len_kv, DIM]
</span></span></span><span><span><span></span>  nv_bfloat16 <span>*</span>O,        <span>// [bs, len_q, DIM]
</span></span></span><span><span><span></span>  <span>int</span> bs,
</span></span><span><span>  <span>int</span> len_q,
</span></span><span><span>  <span>int</span> len_kv) {
</span></span><span><span>
</span></span><span><span>  <span>// basic setup
</span></span></span><span><span><span></span>  <span>const</span> <span>int</span> tid <span>=</span> threadIdx.x;
</span></span><span><span>  <span>const</span> <span>int</span> warp_id <span>=</span> tid <span>/</span> <span>32</span>;
</span></span><span><span>  <span>const</span> <span>int</span> lane_id <span>=</span> tid <span>%</span> <span>32</span>;
</span></span><span><span>
</span></span><span><span>  <span>// increment Q, K, V, O based on blockIdx.x
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// set up shared memory
</span></span></span><span><span><span></span>  <span>// Q_smem is overlapped with (K_smem + V_smem), since we only use Q_smem once
</span></span></span><span><span><span></span>  <span>extern</span> __shared__ <span>uint8_t</span> smem[];
</span></span><span><span>  <span>const</span> <span>uint32_t</span> Q_smem <span>=</span> __cvta_generic_to_shared(smem);
</span></span><span><span>  <span>const</span> <span>uint32_t</span> K_smem <span>=</span> Q_smem;
</span></span><span><span>  <span>const</span> <span>uint32_t</span> V_smem <span>=</span> K_smem <span>+</span> BLOCK_KV <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>
</span></span><span><span>  <span>// FA2: shard BLOCK_Q among warps
</span></span></span><span><span><span></span>  <span>constexpr</span> <span>int</span> WARP_Q <span>=</span> BLOCK_Q <span>/</span> NUM_WARPS;
</span></span><span><span>
</span></span><span><span>  <span>// set up register memory
</span></span></span><span><span><span></span>  <span>uint32_t</span> Q_rmem[WARP_Q <span>/</span> MMA_M][DIM <span>/</span> MMA_K][<span>4</span>];       <span>// act as A in MMA
</span></span></span><span><span><span></span>  <span>uint32_t</span> K_rmem[BLOCK_KV <span>/</span> MMA_N][DIM <span>/</span> MMA_K][<span>2</span>];     <span>// act as B in MMA
</span></span></span><span><span><span></span>  <span>uint32_t</span> P_rmem[WARP_Q <span>/</span> MMA_M][BLOCK_KV <span>/</span> MMA_K][<span>4</span>];  <span>// act as A in MMA
</span></span></span><span><span><span></span>  <span>uint32_t</span> V_rmem[BLOCK_KV <span>/</span> MMA_K][DIM <span>/</span> MMA_N][<span>2</span>];     <span>// act as B in MMA
</span></span></span><span><span><span></span>  <span>float</span> O_rmem[WARP_Q <span>/</span> MMA_M][DIM <span>/</span> MMA_N][<span>4</span>];          <span>// act as C/D in MMA
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// Q global-&gt;shared [BLOCK_Q, DIM]
</span></span></span><span><span><span></span>  global_to_shared<span>&lt;</span>BLOCK_Q, DIM, TB_SIZE<span>&gt;</span>(Q_smem, Q, DIM, tid);
</span></span><span><span>  <span>asm</span> <span>volatile</span>(<span>"cp.async.commit_group;"</span>);
</span></span><span><span>  <span>asm</span> <span>volatile</span>(<span>"cp.async.wait_all;"</span>);
</span></span><span><span>  __syncthreads();
</span></span><span><span>
</span></span><span><span>  <span>// Q shared-&gt;register. select the correct warp tile
</span></span></span><span><span><span></span>  <span>// Q stays in registers throughout the kernel's lifetime
</span></span></span><span><span><span></span>  <span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>)
</span></span><span><span>    <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_K; mma_id_d<span>++</span>) {
</span></span><span><span>      <span>const</span> <span>int</span> row <span>=</span> warp_id <span>*</span> WARP_Q <span>+</span> mma_id_q <span>*</span> MMA_M <span>+</span> (lane_id <span>%</span> <span>16</span>);
</span></span><span><span>      <span>const</span> <span>int</span> col <span>=</span> mma_id_d <span>*</span> MMA_K <span>+</span> (lane_id <span>/</span> <span>16</span> <span>*</span> <span>8</span>);
</span></span><span><span>      <span>const</span> <span>uint32_t</span> addr <span>=</span> Q_smem <span>+</span> (row <span>*</span> DIM <span>+</span> col) <span>*</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>      ldmatrix_x4(Q_rmem[mma_id_q][mma_id_d], addr);
</span></span><span><span>    }
</span></span><span><span>  __syncthreads();
</span></span><span><span>
</span></span><span><span>  <span>// main loop
</span></span></span><span><span><span></span>  <span>const</span> <span>int</span> num_kv_iters <span>=</span> len_kv <span>/</span> BLOCK_KV;
</span></span><span><span>  <span>for</span> (<span>int</span> kv_idx <span>=</span> <span>0</span>; kv_idx <span>&lt;</span> num_kv_iters; kv_idx<span>++</span>) {
</span></span><span><span>    <span>// accumulator for the 1st MMA. reset to zeros
</span></span></span><span><span><span></span>    <span>float</span> S_rmem[WARP_Q <span>/</span> MMA_M][BLOCK_KV <span>/</span> MMA_N][<span>4</span>] <span>=</span> {};  <span>// act as C/D in MMA
</span></span></span><span><span><span></span>
</span></span><span><span>    <span>// load K global-&gt;shared-&gt;registers [BLOCK_KV, DIM]
</span></span></span><span><span><span></span>    <span>// similar to loading Q, except we use ldmatrix_x2()
</span></span></span><span><span><span></span>    ...
</span></span><span><span>
</span></span><span><span>    <span>// 1st MMA: S = Q @ K.T
</span></span></span><span><span><span></span>    <span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>)
</span></span><span><span>      <span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>)
</span></span><span><span>        <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_K; mma_id_d<span>++</span>)
</span></span><span><span>          mma_m16n8k16(Q_rmem[mma_id_q][mma_id_d],
</span></span><span><span>                       K_rmem[mma_id_kv][mma_id_d],
</span></span><span><span>                       S_rmem[mma_id_q][mma_id_kv]);
</span></span><span><span>
</span></span><span><span>    <span>// online softmax. we will touch on this later
</span></span></span><span><span><span></span>    <span>// also pack S_rmem to P_rmem for the 2nd MMA
</span></span></span><span><span><span></span>    ...
</span></span><span><span>
</span></span><span><span>    <span>// load V global-&gt;shared-&gt;registers [BLOCK_KV, DIM]
</span></span></span><span><span><span></span>    <span>// similar to loading K, except we use ldmatrix_x2_trans()
</span></span></span><span><span><span></span>    ...
</span></span><span><span>
</span></span><span><span>    <span>// 2nd MMA: O += P @ V
</span></span></span><span><span><span></span>    <span>// similar to the 1st MMA
</span></span></span><span><span><span></span>    ...
</span></span><span><span>
</span></span><span><span>    <span>// increment pointer to the next KV block
</span></span></span><span><span><span></span>    K <span>+=</span> BLOCK_KV <span>*</span> DIM;
</span></span><span><span>    V <span>+=</span> BLOCK_KV <span>*</span> DIM;
</span></span><span><span>  }
</span></span><span><span>
</span></span><span><span>  <span>// write output
</span></span></span><span><span><span></span>  <span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>)
</span></span><span><span>    <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_N; mma_id_d<span>++</span>) {
</span></span><span><span>      <span>const</span> <span>int</span> row <span>=</span> warp_id <span>*</span> WARP_Q <span>+</span> mma_id_q <span>*</span> MMA_M <span>+</span> (lane_id <span>/</span> <span>4</span>);
</span></span><span><span>      <span>const</span> <span>int</span> col <span>=</span> mma_id_d <span>*</span> MMA_N <span>+</span> (lane_id <span>%</span> <span>4</span>) <span>*</span> <span>2</span>;
</span></span><span><span>
</span></span><span><span>      <span>float</span> <span>*</span>regs <span>=</span> O_rmem[mma_id_q][mma_id_d];
</span></span><span><span>      <span>reinterpret_cast</span><span>&lt;</span>nv_bfloat162 <span>*&gt;</span>(O <span>+</span> (row <span>+</span> <span>0</span>) <span>*</span> DIM <span>+</span> col)[<span>0</span>] <span>=</span> __float22bfloat162_rn({regs[<span>0</span>], regs[<span>1</span>]});
</span></span><span><span>      <span>reinterpret_cast</span><span>&lt;</span>nv_bfloat162 <span>*&gt;</span>(O <span>+</span> (row <span>+</span> <span>8</span>) <span>*</span> DIM <span>+</span> col)[<span>0</span>] <span>=</span> __float22bfloat162_rn({regs[<span>2</span>], regs[<span>3</span>]});
</span></span><span><span>    }
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>// kernel launcher
</span></span></span><span><span><span></span><span>void</span> <span>attention_v1</span>(
</span></span><span><span>  <span>const</span> nv_bfloat16 <span>*</span>Q,  <span>// [bs, len_q, DIM]
</span></span></span><span><span><span></span>  <span>const</span> nv_bfloat16 <span>*</span>K,  <span>// [bs, len_kv, DIM]
</span></span></span><span><span><span></span>  <span>const</span> nv_bfloat16 <span>*</span>V,  <span>// [bs, len_kv, DIM]
</span></span></span><span><span><span></span>  nv_bfloat16 <span>*</span>O,        <span>// [bs, len_q, DIM]
</span></span></span><span><span><span></span>  <span>int</span> bs,
</span></span><span><span>  <span>int</span> len_q,
</span></span><span><span>  <span>int</span> len_kv) {
</span></span><span><span>
</span></span><span><span>  <span>// 1 threadblock for each BLOCK_Q
</span></span></span><span><span><span></span>  <span>const</span> <span>int</span> num_blocks <span>=</span> bs <span>*</span> cdiv(len_q, BLOCK_Q);
</span></span><span><span>
</span></span><span><span>  <span>// Q overlap with K+V.
</span></span></span><span><span><span></span>  <span>const</span> <span>int</span> smem_size <span>=</span> max(BLOCK_Q, BLOCK_KV <span>*</span> <span>2</span>) <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>
</span></span><span><span>  <span>// use dynamic shared memory so we can allocate more than 48kb if needed.
</span></span></span><span><span><span></span>  <span>if</span> (smem_size <span>&gt;</span> <span>48'000</span>)
</span></span><span><span>    CUDA_CHECK(cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size));
</span></span><span><span>
</span></span><span><span>  attention_v1_kernel<span>&lt;&lt;&lt;</span>num_blocks, TB_SIZE, smem_size<span>&gt;&gt;&gt;</span>(Q, K, V, O, bs, len_q, len_kv);
</span></span><span><span>  CUDA_CHECK(cudaGetLastError());
</span></span><span><span>}
</span></span></code></pre></div><p>Now, let’s tackle online softmax.</p><h3 id="online-softmax---theory">Online softmax - Theory</h3><p>For the original explanation, you can refer to <a href="https://arxiv.org/abs/1805.02867">Online normalizer calculation for softmax</a> and Flash Attention 2 paper.</p><p>We have the following mathematical definition of softmax. For each row with length $L_{kv}$</p><p>$$
p_l = \frac{\exp(s_l-m)}{\exp(s_0-m) + \exp(s_1-m) + \dots + \exp(s_{L_{kv}-1}-m)}
$$
$$
l\in[0,L_{kv})
$$
$$
m=\max(s_0,s_1,\dots,s_{L_{kv}-1})
$$</p><p>$-m$ is max subtraction to improve numerical stability ($\exp(\cdot)$ can easily explode if its input is large). Let’s bring out the denominator normalizer and write the whole row as a vector.</p><p>$$
\vec P =
\begin{bmatrix}
p_0 \\
\vdots \\
p_{L_{kv}-1}
\end{bmatrix}
= \frac{1}{\sum_{l\in[0,L_{kv})}\exp(s_l-m)}
\begin{bmatrix}
\exp(s_0-m) \\
\vdots \\
\exp(s_{L_{kv}-1}-m)
\end{bmatrix}
$$</p><p>In our 2nd matmul <code>O += P @ V</code>, each row of P (softmax output) is dot-producted with the corresponding column of V.</p><p>$$
o=\vec P \cdot \vec V = \frac{1}{\sum_{l\in[0,L_{kv})}\exp(s_l-m)} \sum_{l\in[0,L_{kv})}\exp(s_l-m) \cdot v_l
$$</p><p>The extra dot-product is a blessing in disguise - we no longer need individual elements in a row for the final result. This enables Flash Attention to compute attention in one pass. To see it more clearly, let’s consider the iterative process of adding a new element during online computation.</p><p>$$
o_{[0,L)} = \frac{1}{\sum_{l\in[0,L)}\exp(s_l-m_{[0,L)})} \sum_{l\in[0,L)}\exp(s_l-m_{[0,L)}) \cdot v_l
$$
$$
m_{[0,L)}=\max(s_0,s_1,\dots,s_{L-1})
$$</p><p>I’m abusing the notation here, but I hope I get the idea across. When we add a new element $s_{L+1}$</p><p>$$
o_{[0,L+1)} = \frac{1}{\sum_{l\in[0,L+1)}\exp(s_l-m_{[0,L+1)})} \sum_{l\in[0,L+1)}\exp(s_l-m_{[0,L+1)}) \cdot v_l
$$</p><p>Look at the normalizer (denominator)</p><p>$$
\sum_{l\in[0,L+1)}\exp(s_l-m_{[0,L+1)}) = \colorbox{red}{$\displaystyle\exp(m_{[0,L)}-m_{[0,L+1)})$}\colorbox{orange}{$\displaystyle\sum_{l\in[0,L)}\exp(s_l-m_{[0,L)})$} + \colorbox{lime}{$\displaystyle\exp(s_L-m_{[0,L+1)})$}
$$</p><p>The equation means that we only need to $\colorbox{red}{rescale}$ the $\colorbox{orange}{previous normalizer}$ before adding the $\colorbox{lime}{new term}$. The same logic can be applied for the dot product with V (unnormalized output). <strong>This is the key idea of online softmax and Flash Attention</strong>.</p><p>Define <strong>attention state</strong></p><p>$$
\begin{bmatrix}
m \\
\tilde{o} \\
\mathrm{sumexp}
\end{bmatrix}
$$</p><p>where $m$ is the max of elements seen so far, $\tilde{o}$ is the <strong>unnormalized</strong> output, and $\mathrm{sumexp}$ is the normalizer. We need $m$ to compute the rescaling factor as seen above.</p><p>You can convince yourself that updating attention state is an <strong>associative</strong> operation - it does not matter the order in which elements are used to update the attention state.</p><p>$$
\begin{aligned}
\begin{bmatrix}
m_1 \\
\tilde{o}_1 \\
\mathrm{sumexp}_1
\end{bmatrix}
\oplus \begin{bmatrix}
m_2 \\
\tilde{o}_2 \\
\mathrm{sumexp}_2
\end{bmatrix}
&amp;= \begin{bmatrix}
m_3 \\
\tilde{o}_3 \\
\mathrm{sumexp}_3
\end{bmatrix} \\
&amp;= \begin{bmatrix}
\max(m_1,m_2) \\
\exp(m_1-m_3)\tilde{o}_1+\exp(m_2-m_3)\tilde{o}_2 \\
\exp(m_1-m_3)\mathrm{sumexp}_1+\exp(m_2-m_3)\mathrm{sumexp}_2
\end{bmatrix}
\end{aligned}
$$</p><p>This associative property enables things like <a href="https://pytorch.org/blog/flash-decoding/">Flash Decoding</a>, a split-K version of attention.</p><h3 id="online-softmax---implementation">Online softmax - Implementation</h3><p>We can now fill in the gap of online softmax in our high-level Python implementation.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># attention state</span>
</span></span><span><span>m <span>=</span> torch<span>.</span>zeros(BLOCK_Q)
</span></span><span><span>tile_O <span>=</span> torch<span>.</span>zeros(BLOCK_Q, DIM)
</span></span><span><span>sumexp <span>=</span> torch<span>.</span>zeros(BLOCK_Q)
</span></span><span><span>
</span></span><span><span><span>for</span> _ <span>in</span> range(Lk <span>//</span> BLOCK_KV):
</span></span><span><span>  <span># 1st MMA</span>
</span></span><span><span>  tile_S <span>=</span> tile_Q <span>@</span> tile_K<span>.</span>T  <span># [BLOCK_Q, BLOCK_KV]</span>
</span></span><span><span>  tile_S <span>=</span> tile_S <span>*</span> scale
</span></span><span><span>
</span></span><span><span>  <span># online softmax</span>
</span></span><span><span>  tile_max <span>=</span> tile_S<span>.</span>amax(dim<span>=-</span><span>1</span>)  <span># [BLOCK_Q]</span>
</span></span><span><span>  new_m <span>=</span> torch<span>.</span>maximum(m, tile_max)
</span></span><span><span>  tile_P <span>=</span> torch<span>.</span>exp(tile_S <span>-</span> new_m<span>.</span>unsqueeze(<span>-</span><span>1</span>))
</span></span><span><span>
</span></span><span><span>  <span># rescale</span>
</span></span><span><span>  scale <span>=</span> torch<span>.</span>exp(m <span>-</span> new_m)
</span></span><span><span>  tile_O <span>*=</span> scale<span>.</span>unsqueeze(<span>-</span><span>1</span>)
</span></span><span><span>  sumexp <span>=</span> sumexp <span>*</span> scale <span>+</span> tile_P<span>.</span>sum(dim<span>=-</span><span>1</span>)
</span></span><span><span>  m <span>=</span> new_m  <span># save new max</span>
</span></span><span><span>
</span></span><span><span>  <span># 2nd MMA</span>
</span></span><span><span>  tile_O <span>+=</span> tile_P <span>@</span> tile_V  <span># [BLOCK_Q, DIM]</span>
</span></span><span><span>
</span></span><span><span><span># apply normalization</span>
</span></span><span><span>tile_O <span>/=</span> sumexp<span>.</span>unsqueeze(<span>-</span><span>1</span>)
</span></span></code></pre></div><h4 id="row-max">Row max</h4><p>When translating this to CUDA C++, the most tricky part is to wrap our head around MMA layout. Let’s start with <code>tile_S</code>.</p><figure><img src="https://docs.nvidia.com/cuda/parallel-thread-execution/_images/mma-16816-C-f16.png" alt="MMA m16n8k16 output layout"><figcaption><p>Thread and register layout of MMA m16n8k16 output. Source: <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-fragment-mma-16816-float">NVIDIA PTX doc</a>.</p></figcaption></figure><p>Softmax scale applies the same scaling for all elements, so that is trivial. Next, we need to compute row max for the current tile. Remember that we allocate the registers for <code>tile_S</code> this way.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>float</span> S_rmem[WARP_Q <span>/</span> MMA_M][BLOCK_KV <span>/</span> MMA_N][<span>4</span>];
</span></span></code></pre></div><p><code>4</code> means <code>c0,c1,c2,c3</code> in the figure above i.e. each thread holds 2 consecutive elements from 2 rows. To do reduction within a row (of an MMA output tile), we do reduction for 2 consecutive elements held by a thread, then reduction within a group of 4 threads i.e. <code>T0-T3</code>, <code>T4-T7</code>, and so on. However, the row reduction is actually within the whole <code>tile_S</code>, hence we also need to loop over <code>BLOCK_KV / MMA_N</code> of <code>S_rmem</code>. This can be combined with thread-level reduction before 4-thread reduction.</p><figure><img src="https://gau-nernst.github.io/fa-5090/row_reduction.svg" alt="Row reduction"><figcaption><p>Perform row reduction on MMA output.</p></figcaption></figure><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// initial attention state
</span></span></span><span><span><span></span><span>float</span> rowmax[WARP_Q <span>/</span> MMA_M][<span>2</span>];
</span></span><span><span><span>float</span> rowsumexp[WARP_Q <span>/</span> MMA_M][<span>2</span>] <span>=</span> {};
</span></span><span><span><span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>) {
</span></span><span><span>  rowmax[mma_id_q][<span>0</span>] <span>=</span> <span>-</span>FLT_MAX;
</span></span><span><span>  rowmax[mma_id_q][<span>1</span>] <span>=</span> <span>-</span>FLT_MAX;
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>// main loop
</span></span></span><span><span><span></span><span>const</span> <span>int</span> num_kv_iters <span>=</span> len_kv <span>/</span> BLOCK_KV;
</span></span><span><span><span>for</span> (<span>int</span> kv_idx <span>=</span> <span>0</span>; kv_idx <span>&lt;</span> num_kv_iters; kv_idx<span>++</span>) {
</span></span><span><span>  <span>// tile_S = tile_Q @ tile_K.T
</span></span></span><span><span><span></span>  S_rmem[][] <span>=</span> ...
</span></span><span><span>
</span></span><span><span>  <span>// loop over rows
</span></span></span><span><span><span></span>  <span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>) {
</span></span><span><span>    <span>// apply softmax scale
</span></span></span><span><span><span></span>    <span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>)
</span></span><span><span>      <span>for</span> (<span>int</span> reg_id <span>=</span> <span>0</span>; reg_id <span>&lt;</span> <span>4</span>; reg_id<span>++</span>)
</span></span><span><span>        S_rmem[mma_id_q][mma_id_kv][reg_id] <span>*=</span> softmax_scale;
</span></span><span><span>
</span></span><span><span>    <span>// rowmax
</span></span></span><span><span><span></span>    <span>float</span> this_rowmax[<span>2</span>] <span>=</span> {<span>-</span>FLT_MAX, <span>-</span>FLT_MAX};
</span></span><span><span>    <span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>) {
</span></span><span><span>      <span>float</span> <span>*</span>regs <span>=</span> S_rmem[mma_id_q][mma_id_kv];
</span></span><span><span>      this_rowmax[<span>0</span>] <span>=</span> max(this_rowmax[<span>0</span>], max(regs[<span>0</span>], regs[<span>1</span>]));  <span>// c0 and c1
</span></span></span><span><span><span></span>      this_rowmax[<span>1</span>] <span>=</span> max(this_rowmax[<span>1</span>], max(regs[<span>2</span>], regs[<span>3</span>]));  <span>// c2 and c3
</span></span></span><span><span><span></span>    }
</span></span><span><span>
</span></span><span><span>    <span>// butterfly reduction within 4 threads
</span></span></span><span><span><span></span>    this_rowmax[<span>0</span>] <span>=</span> max(this_rowmax[<span>0</span>], __shfl_xor_sync(<span>0xFFFF'FFFF</span>, this_rowmax[<span>0</span>], <span>1</span>));
</span></span><span><span>    this_rowmax[<span>0</span>] <span>=</span> max(this_rowmax[<span>0</span>], __shfl_xor_sync(<span>0xFFFF'FFFF</span>, this_rowmax[<span>0</span>], <span>2</span>));
</span></span><span><span>    this_rowmax[<span>1</span>] <span>=</span> max(this_rowmax[<span>1</span>], __shfl_xor_sync(<span>0xFFFF'FFFF</span>, this_rowmax[<span>1</span>], <span>1</span>));
</span></span><span><span>    this_rowmax[<span>1</span>] <span>=</span> max(this_rowmax[<span>1</span>], __shfl_xor_sync(<span>0xFFFF'FFFF</span>, this_rowmax[<span>1</span>], <span>2</span>));
</span></span><span><span>  }
</span></span><span><span>
</span></span><span><span>  ...
</span></span><span><span>}
</span></span></code></pre></div><p>In a typical reduction kernel, when there are only 32 active threads left, we can use warp shuffle <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions">__shfl_down_sync()</a> to copy data from higher lanes to lower lanes, and the final result is stored in thread 0. In this case, since we need the max value to be shared among the 4 threads in a group (for max subtraction later), we can use <code>__shfl_xor_sync()</code> to avoid an additional broadcast step.</p><figure><img src="https://gau-nernst.github.io/fa-5090/butterfly_reduction.svg" alt="Butterfly reduction"><figcaption><p>Butterfly reduction within 4 threads using __shfl_xor_sync().</p></figcaption></figure><h4 id="rescaling">Rescaling</h4><p>With row max of the new tile, we can compute rescaling factor for (unnormalized) output as well as normalizer (sumexp of each row).</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// new rowmax
</span></span></span><span><span><span></span>this_rowmax[<span>0</span>] <span>=</span> max(this_rowmax[<span>0</span>], rowmax[mma_id_q][<span>0</span>]);
</span></span><span><span>this_rowmax[<span>1</span>] <span>=</span> max(this_rowmax[<span>1</span>], rowmax[mma_id_q][<span>1</span>]);
</span></span><span><span>
</span></span><span><span><span>// rescale for previous O
</span></span></span><span><span><span></span><span>float</span> rescale[<span>2</span>];
</span></span><span><span>rescale[<span>0</span>] <span>=</span> __expf(rowmax[mma_id_q][<span>0</span>] <span>-</span> this_rowmax[<span>0</span>]);
</span></span><span><span>rescale[<span>1</span>] <span>=</span> __expf(rowmax[mma_id_q][<span>1</span>] <span>-</span> this_rowmax[<span>1</span>]);
</span></span><span><span><span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_N; mma_id_d<span>++</span>) {
</span></span><span><span>  O_rmem[mma_id_q][mma_id_d][<span>0</span>] <span>*=</span> rescale[<span>0</span>];
</span></span><span><span>  O_rmem[mma_id_q][mma_id_d][<span>1</span>] <span>*=</span> rescale[<span>0</span>];
</span></span><span><span>  O_rmem[mma_id_q][mma_id_d][<span>2</span>] <span>*=</span> rescale[<span>1</span>];
</span></span><span><span>  O_rmem[mma_id_q][mma_id_d][<span>3</span>] <span>*=</span> rescale[<span>1</span>];
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>// save new rowmax
</span></span></span><span><span><span></span>rowmax[mma_id_q][<span>0</span>] <span>=</span> this_rowmax[<span>0</span>];
</span></span><span><span>rowmax[mma_id_q][<span>1</span>] <span>=</span> this_rowmax[<span>1</span>];
</span></span></code></pre></div><p>We don’t rescale <code>rowsumexp</code> here because we want to fuse it with addition of the new sumexp term later i.e. FMA - fused multiply add. We can’t fuse multiplication with MMA, hence we need to do a separate multiplication for <code>O_rmem</code>.</p><h4 id="pack-tile_s-to-tile_p-and-compute-row-sum-exp">Pack <code>tile_S</code> to <code>tile_P</code> (and compute row sum exp)</h4><p>For the next part, we will loop over the row dimension again (<code>BLOCK_KV / MMA_N</code>), to compute and pack <code>tile_P</code> from <code>tile_S</code>, as well as doing reduction for sumexp. Recall that we declare registers for <code>S</code> and <code>P</code> as follows.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>float</span> S_rmem[WARP_Q <span>/</span> MMA_M][BLOCK_KV <span>/</span> MMA_N][<span>4</span>]      <span>// m16n8
</span></span></span><span><span><span></span><span>uint32_t</span> P_rmem[WARP_Q <span>/</span> MMA_M][BLOCK_KV <span>/</span> MMA_K][<span>4</span>];  <span>// m16k16
</span></span></span></code></pre></div><p>Look up the thread/register layout for MMA multiplicand A and output C/D again in PTX doc. Luckily, the layouts are exactly the same - within an 8x8 tile, the arrangement of elements is identical.</p><figure><img src="https://gau-nernst.github.io/fa-5090/m16n8_to_m16k16.svg" alt="Register layout of MMA m16n8k16"><figcaption><p>The left half of multiplicand A has the same layout as accumulator. Source: <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-fragment-mma-16816-float">NVIDIA PTX doc</a>.</p></figcaption></figure><p>It means that for all threads, every 2 floats in <code>S_rmem</code> can be packed as BF16x2 in a single 32-bit register of <code>P_rmem</code>, exactly how <code>mma.m16n8k16</code> expects for the 2nd MMA. There are no data movements across threads. Note that this is not always true: if we use INT8 or FP8 MMA for the 1st and/or 2nd MMA, we would need to permute data across threads to pack <code>tile_S</code> to <code>tile_P</code>.</p><p>Our code for the last part of online softmax is below.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// rowsumexp
</span></span></span><span><span><span></span><span>float</span> this_rowsumexp[<span>2</span>] <span>=</span> {};
</span></span><span><span><span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>) {
</span></span><span><span>  <span>float</span> <span>*</span>regs <span>=</span> S_rmem[mma_id_q][mma_id_kv];
</span></span><span><span>  regs[<span>0</span>] <span>=</span> __expf(regs[<span>0</span>] <span>-</span> rowmax[mma_id_q][<span>0</span>]);  <span>// c0
</span></span></span><span><span><span></span>  regs[<span>1</span>] <span>=</span> __expf(regs[<span>1</span>] <span>-</span> rowmax[mma_id_q][<span>0</span>]);  <span>// c1
</span></span></span><span><span><span></span>  regs[<span>2</span>] <span>=</span> __expf(regs[<span>2</span>] <span>-</span> rowmax[mma_id_q][<span>1</span>]);  <span>// c2
</span></span></span><span><span><span></span>  regs[<span>3</span>] <span>=</span> __expf(regs[<span>3</span>] <span>-</span> rowmax[mma_id_q][<span>1</span>]);  <span>// c3
</span></span></span><span><span><span></span>
</span></span><span><span>  this_rowsumexp[<span>0</span>] <span>+=</span> regs[<span>0</span>] <span>+</span> regs[<span>1</span>];
</span></span><span><span>  this_rowsumexp[<span>1</span>] <span>+=</span> regs[<span>2</span>] <span>+</span> regs[<span>3</span>];
</span></span><span><span>
</span></span><span><span>  <span>// pack to P registers for next MMA
</span></span></span><span><span><span></span>  <span>// we need to change from m16n8 to m16k16
</span></span></span><span><span><span></span>  <span>// each iteration of this loop packs half of m16k16
</span></span></span><span><span><span></span>  nv_bfloat162 <span>*</span>this_P_rmem <span>=</span> <span>reinterpret_cast</span><span>&lt;</span>nv_bfloat162 <span>*&gt;</span>(P_rmem[mma_id_q][mma_id_kv <span>/</span> <span>2</span>]);
</span></span><span><span>  this_P_rmem[(mma_id_kv <span>%</span> <span>2</span>) <span>*</span> <span>2</span>]     <span>=</span> __float22bfloat162_rn({regs[<span>0</span>], regs[<span>1</span>]});  <span>// top row
</span></span></span><span><span><span></span>  this_P_rmem[(mma_id_kv <span>%</span> <span>2</span>) <span>*</span> <span>2</span> <span>+</span> <span>1</span>] <span>=</span> __float22bfloat162_rn({regs[<span>2</span>], regs[<span>3</span>]});  <span>// bottom row
</span></span></span><span><span><span></span>}
</span></span><span><span>
</span></span><span><span><span>// butterfly reduction on this_rowsumexp[2]
</span></span></span><span><span><span></span>...
</span></span><span><span>
</span></span><span><span><span>// accumulate to total rowsumexp using FMA
</span></span></span><span><span><span></span>rowsumexp[mma_id_q][<span>0</span>] <span>=</span> rowsumexp[mma_id_q][<span>0</span>] <span>*</span> rescale[<span>0</span>] <span>+</span> this_rowsumexp[<span>0</span>];
</span></span><span><span>rowsumexp[mma_id_q][<span>1</span>] <span>=</span> rowsumexp[mma_id_q][<span>1</span>] <span>*</span> rescale[<span>1</span>] <span>+</span> this_rowsumexp[<span>1</span>];
</span></span></code></pre></div><p>After this is the 2nd MMA: load V, then compute <code>tile_O += tile_P @ tile_V</code>. This completes our 1st version of Flash Attention. Actually we also need to normalize the output before writing <code>O_rmem</code> to global memory, but the logic for that should be pretty straightforward.</p><p>You can find the full code for Version 1 at <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention_v1.cu">attention_v1.cu</a>.</p><h3 id="benchmark-setup">Benchmark setup</h3><p>Wow, that’s plentiful for the 1st version. Indeed, I spent the most time on version 1 trying to implement Flash Attention correctly. Took me 2 days to realize <a href="https://github.com/gau-nernst/learn-cuda/commit/8fdb3e6a">__shfl_xor_sync()’s mask should be 2 (0b10) instead of 0x10 for butterfly reduction</a>.</p><p>Anyway, now we need a script for correctness check as well as speed benchmark. I prefer to do these things in Python Pytorch since it’s easy to do, as well as making it simple to compare against other attention kernels with PyTorch bindings. To achieve this, I create:</p><ol><li><a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention.cpp">attention.cpp</a>: provides PyTorch bindings for my attention kernels.</li><li><a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/main.py">main.py</a>: correctness check and speed benchmark.</li></ol><p>For correctness check, I compare against <code>F.sdpa()</code>, which should dispatch Flash Attention 2 by default (at least on my GPU and current PyTorch version). I also purposely add a small bias to the random inputs, which are sampled from the standard normal distribution, so that the output has a positive mean. This is to avoid large relative error caused by zero mean.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>generate_input</span>(<span>*</span>shape):
</span></span><span><span>    <span>return</span> torch<span>.</span>randn(shape)<span>.</span>add(<span>0.5</span>)<span>.</span>bfloat16()<span>.</span>cuda()
</span></span></code></pre></div><p>For speed benchmarks, it’s generally a good idea to compare against (1) theoretical limit of the hardware i.e. Speed-of-Light, and (2) known good implementations. I’m more interested in the compute-bound regime of attention, hence I will be using FLOPS (floating point operations per second, with a capital S) as the metric for comparison.</p><p>To compute FLOPS of a given kernel, we count the number of required floating point operations (FLOPs, with a small s), then divide by the latency. Just counting FLOPs from the MMAs should be good enough, which turns out to be <code>4 * bsize * num_heads * len_q * len_kv * head_dim</code>.</p><p>The “known good implementations” are FA2 and CuDNN backends of <code>F.sdpa()</code>, as well as FA2 from <a href="https://github.com/Dao-AILab/flash-attention">flash-attn</a> package. For my kernel, I did do some tuning of <code>BLOCK_Q</code> and <code>BLOCK_KV</code>, and obtained the following results.</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td><code>F.sdpa()</code> (Flash Attention)</td><td>186.73</td><td>89.13%</td></tr><tr><td><code>F.sdpa()</code> (CuDNN)</td><td>203.61</td><td>97.19%</td></tr><tr><td><code>flash-attn</code></td><td>190.58</td><td>90.97%</td></tr><tr><td>v1 (basic)</td><td>142.87</td><td>68.20%</td></tr></tbody></table><p>It doesn’t look too bad for the first version, but we still have some headroom to go. That’s fine, because we still have a few tricks up our sleeves for the next versions. In fact, the tricks are exactly the same as the ones used in optimizing a matmul kernel.</p><h4 id="profiling">Profiling</h4><p>Before moving to the next version, I want to talk about profiling tools. I think it’s always a good idea to use profiling as the guide for optimization. Previously I only knew how to use <a href="https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html">ncu</a> at a very basic level. Seeing so many people using <a href="https://developer.nvidia.com/nsight-compute">Nsight Compute</a> with cool diagrams, I decided to learn how to use it, and it was actually quite easy to use.</p><p>Nsight Compute can run on macOS with SSH access to another machine with NVIDIA GPU, which is exactly the setup I’m using right now (yes, I write code exclusively on my Macbook). If you are unfamiliar with Nsight Compute, I recommend watching a tutorial or two to get acquainted with it.</p><p>To enable source inspection feature, remember to pass <code>-lineinfo</code> to NVCC (see <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/main.py#L22">here</a>), and enable “Import Source” in Nsight Compute.</p><p>Let’s do a profiling with Nsight Compute, and look at <strong>Warp State Statistics</strong> section.</p><figure><img src="https://gau-nernst.github.io/fa-5090/v1_warp_state_stats.png" alt="Warp state statistics of v1"><figcaption><p>Warp state statistics of kernel v1.</p></figcaption></figure><p><strong>Stall Math Pipe Throttle</strong> being the highest is good - it means warps are busy with math operations i.e. Tensor Cores. The second highest is <strong>Stall Short Scoreboard</strong>. This typically means waiting for accesses to and from shared memory. You can check <a href="https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html">Nsight Compute doc</a> and search for <code>stalled_short_scoreboard</code>.</p><p>We can double confirm this by looking at <strong>Memory Workload Analysis</strong>, which reveals several problems.</p><figure><img src="https://gau-nernst.github.io/fa-5090/v1_memory_analysis.png" alt="Memory analsysis of v1"><figcaption><p>Memory analysis of kernel v1.</p></figcaption></figure><ul><li><strong>L1TEX Global Store Access Pattern</strong> comes from storing the output, since it is the only global write we have. This is not important since the runtime of looping over the sequence length of KV should dominate when <code>len_kv</code> is large.</li><li><strong>L1TEX Local Load/Store Access Pattern</strong> is due to register spilling. Since it’s register spilling, only spilling and reloading 1 element at a time is normal. Reducing <code>BLOCK_Q</code> (so that we use fewer registers to hold accumulators) would resolve this issue, but my manual tuning showed that some spilling was actually faster.</li><li><strong>Shared Load Bank Conflicts</strong> is exactly what we are looking for - bank conflicts that result in “Stall Short Scoreboard”.</li></ul><p>NVIDIA GPU’s shared memory is backed by 32 memory banks. Consecutive 4-byte memory addresses are assigned to consecutive memory banks. This poses a problem when we load data from shared to register memory with <code>ldmatrix</code>. Although it’s not explitcitly stated in any documentations, <code>ldmatrix.x2</code> and <code>ldmatrix.x4</code> operate per 8x8 tile at a time. This is good, since it makes our analysis simpler: we only need to consider the case of loading a 8x8 tile.</p><p>Consider a 2D tile of shape 8x64, BF16 dtype, in shared memory.</p><figure><img src="https://gau-nernst.github.io/fa-5090/bank_conflicts.svg" alt="Bank conflicts"><figcaption><p>Memory bank distribution for a 8x64 BF16 tile in shared memory.</p></figcaption></figure><p>From the figure above, when we load the 8x8 <code>ldmatrix</code> tile, the same 4 banks 0-3 service all 32 threads, resulting in 8-way bank conflict. I’m not sure why Nsight Compute reports 16-way bank conflict as shown above. I tried looking up <a href="https://alexarmbr.github.io/2024/08/10/How-To-Write-A-Fast-Matrix-Multiplication-From-Scratch-With-Tensor-Cores.html">matmul blogposts with swizzling</a> and <a href="https://forums.developer.nvidia.com/t/ncu-detects-bank-conflicts-in-matrix-transposition-after-padding/239100/6">NVIDIA forum threads</a>, and found another way to check for bank conflicts was to go to the <strong>Source</strong> tab of Nsight Compute and check for <strong>L1 Wavefronts Shared</strong> and <strong>L1 Wavefronts Shared Ideal</strong> (I had to enable these two columns manually since they were not displayed by default for me).</p><figure><img src="https://gau-nernst.github.io/fa-5090/ldmatrix_bank_conflicts.png" alt="Bank conflicts in ldmatrix"><figcaption><p>Actual and Ideal L1 Wavefronts Shared of <code>ldmatrix</code> in kernel v1.</p></figcaption></figure><p>The ratio of <strong>Actual / Ideal</strong> is 8, matching our hypothesis of 8-way bank conflicts. I’m still not sure why there is a discrepancy between this value and the one in <strong>Details</strong> tab.</p><p>Anyway, there are 2 standard solutions to this problem</p><ol><li><strong>Pad shared memory</strong>. Due to <code>ldmatrix</code>’s alignment requirement, we can only pad the width with 16 bytes, equivalent to 4 banks. This means that when we go to the next row, the memory banks are shifted by 4, avoiding bank conflicts. In many cases, this is good enough. However, it’s generally quite wasteful as we are not utilising the padded storage.</li><li><strong>Swizzle shared memory address</strong>. This is black magic: you XOR the shared memory address with some magic numbers, then suddenly bank conflicts disappear!</li></ol><p>Let’s elaborate on the 2nd approach. I’m not smart enough to invent this trick, but at least I hope I can give some pointers on why it makes sense. We use XOR since this operation permutes the data nicely - there is a one-to-one mapping between input and output, given a fixed 2nd input. We get bank conflicts because when we move to the next row, we are hitting the same memory banks again -&gt; we can use this row index to permute the addresses.</p><p>In particular, if we look at the raw row addresses:</p><ul><li><strong>Bits 0-3</strong> are always zeros due to 16-byte alignment constraint.</li><li><strong>Bits 2-6</strong> determine bank index. We only need to care about bits 4-6 since the lower bits are always zeros (due to alignment).</li><li>Row stride determines which bits are incremented when we move to the next row (this is by definition). If our 2D tile’s width is 64 BF16 elements, row stride is 128 bytes. Going to the next row will increment bit 7, leaving <strong>bits 0-6 unchanged</strong> (but we don’t care about bits 0-3).</li><li>Thus, we can XOR <strong>bits 4-6</strong> of row address with <strong>bits 0-2</strong> of row index, which is guaranteed to change for every row.</li></ul><p>If the tile width is different, e.g. 32 BF16, we can go through the same reasoning. Also notice that row index is encoded within the row address, thus we only need the row address and row stride to do swizzling.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// NOTE: stride in bytes
</span></span></span><span><span><span></span><span>template</span> <span>&lt;</span><span>int</span> STRIDE<span>&gt;</span>
</span></span><span><span>__device__
</span></span><span><span><span>uint32_t</span> swizzle(<span>uint32_t</span> index) {
</span></span><span><span>  <span>// no need swizzling
</span></span></span><span><span><span></span>  <span>if</span> <span>constexpr</span> (STRIDE <span>==</span> <span>16</span>)
</span></span><span><span>    <span>return</span> index;
</span></span><span><span>
</span></span><span><span>  <span>uint32_t</span> row_idx <span>=</span> (index <span>/</span> STRIDE) <span>%</span> <span>8</span>;
</span></span><span><span>  <span>uint32_t</span> bits_to_xor <span>=</span> row_idx <span>/</span> max(<span>64</span> <span>/</span> STRIDE, <span>1</span>);
</span></span><span><span>  <span>return</span> index <span>^</span> (bits_to_xor <span>&lt;&lt;</span> <span>4</span>);
</span></span><span><span>}
</span></span></code></pre></div><p>To enable this swizzling, we need to add it to <code>cp.async</code> (write to shared memory) and <code>ldmatrix</code> (read from shared memory) calls.</p><div><pre tabindex="0"><code data-lang="diff"><span><span>// for cp.async
</span></span><span><span><span>- const uint32_t dst_addr = dst + (row * WIDTH + col) * sizeof(nv_bfloat16);
</span></span></span><span><span><span></span><span>+ const uint32_t dst_addr = swizzle&lt;WIDTH * sizeof(nv_bfloat16)&gt;(dst + (row * WIDTH + col) * sizeof(nv_bfloat16));
</span></span></span><span><span><span></span>asm volatile("cp.async.cg.shared.global [%0], [%1], 16;" :: "r"(dst_addr), "l"(src_addr));
</span></span><span><span>
</span></span><span><span>// for ldmatrix
</span></span><span><span><span>- ldmatrix_x2(K_rmem[mma_id_kv][mma_id_d], addr);
</span></span></span><span><span><span></span><span>+ ldmatrix_x2(K_rmem[mma_id_kv][mma_id_d], swizzle&lt;DIM * sizeof(nv_bfloat16)&gt;(addr));
</span></span></span></code></pre></div><p>Since this is a standard optimization in matmul kernels, I also added a small optimization for <code>ldmatrix</code>. I pre-compute row addresses and swizzling outside of the main loop, so that there is less work in the hot loop. When we iterate over MMA tiles within a warp tile, we need to increment the address. However, swizzling is a XOR operation, and we cannot simply exchange XOR with addition i.e. <code>(a + b) ^ c != (a ^ c) + b</code>. Notice that if there is some alignment in the base address <code>a</code>, addition becomes XOR! i.e. <code>100 + 001 == 100 ^ 001</code>. Hence, when incrementing the input address of <code>ldmatrix</code>, we XOR it with column offset, instead of doing addition. Row offset will affect bits higher than the swizzled bits, so we can keep addition for it.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// K shared-&gt;registers
</span></span></span><span><span><span></span><span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>)
</span></span><span><span>  <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_K; mma_id_d<span>++</span>) {
</span></span><span><span>    <span>// swizzle(addr + offset) = swizzle(addr) XOR offset
</span></span></span><span><span><span></span>    <span>uint32_t</span> addr <span>=</span> K_smem_thread;
</span></span><span><span>    addr <span>+=</span> mma_id_kv <span>*</span> MMA_N <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16);  <span>// row
</span></span></span><span><span><span></span>    addr <span>^=</span> mma_id_d <span>*</span> MMA_K <span>*</span> <span>sizeof</span>(nv_bfloat16);  <span>// col
</span></span></span><span><span><span></span>    ldmatrix_x2(K_rmem[mma_id_kv][mma_id_d], addr);
</span></span><span><span>  }
</span></span></code></pre></div><p>Version 2: <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention_v2.cu">attention_v2.cu</a>.</p><p>We can verify that there are no more bank conflicts with Nsight Compute. Benchmark results show an impressive uplift (I always re-tune <code>BLOCK_Q</code> and <code>BLOCK_KV</code> for new versions of the kernel).</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td>v1 (basic)</td><td>142.87</td><td>68.20%</td></tr><tr><td>v2 (shared memory swizzling)</td><td>181.11</td><td>86.45%</td></tr></tbody></table><h2 id="version-3---2-stage-pipelining">Version 3 - 2-stage pipelining</h2><figure><img src="https://gau-nernst.github.io/fa-5090/v2_warp_state_stats.png" alt="Warp state statistics of v2"><figcaption><p>Warp state statistics of kernel v2.</p></figcaption></figure><p><strong>Stall Short Scoreboard</strong> is no longer an issue, since we have handled it with swizzling. Now the issues are:</p><ul><li><strong>Stall Wait</strong> (<code>stalled_wait</code> in Nsight Compute doc): “waiting on a fixed latency execution dependency”, doesn’t seem to be a big issue.</li><li><strong>Stall Long Scoreboard</strong> (<code>stalled_long_scoreboard</code> in Nsight Compute doc): usually means waiting for global memory accesses.</li></ul><p>Up until now, we haven’t overlapped global memory operations with compute operations (MMA). This means the Tensor Cores are idle while waiting for global-&gt;shared transfer to complete. This seems to be the right time to introduce <strong>pipelining</strong> or <strong>double-buffering</strong>: allocate more shared memory than needed so that we can prefetch data for the next iteration while working on the current iteration.</p><ul><li>Technically we can also pipeline shared-&gt;register data transfer. This is in fact mentioned in <a href="https://github.com/NVIDIA/cutlass/blob/v4.1.0/media/docs/cpp/efficient_gemm.md">Efficient GEMM doc</a> of CUTLASS. However, I could never implement it successfully on my 5090. Inspecting the generated SASS of my current code, I see that there is interleaving between <code>LDSM</code> (<code>ldmatrix</code> in PTX) and <code>HMMA</code> (half-precision <code>mma</code> in PTX), probably done by the compiler to achieve similar memory-compute overlapping effect.</li></ul><p>Let’s discuss the more general implementation of <strong>N-stage pipelining</strong>. This <a href="https://developer.nvidia.com/blog/controlling-data-movement-to-boost-performance-on-ampere-architecture/">NVIDIA blogpost</a> gives a pretty good explanation of the idea, but generally I don’t really like using CUDA C++ API (and considering that CUTLASS also doesn’t, I think it’s more fun to use PTX directly). N-stage means there are N ongoing stages at any point in time. This will be the <strong>invariance</strong> we want to keep throughout the inner loop.</p><ul><li>This is the same concept of <code>num_stages</code> mentioned in <a href="https://triton-lang.org/main/python-api/generated/triton.Config.html">triton.Config</a> for autotuning.</li><li>Double buffering is a special case of N=2.</li></ul><div><pre tabindex="0"><code data-lang="python"><span><span>num_stages <span>=</span> <span>4</span>
</span></span><span><span>
</span></span><span><span><span># set up num_stages buffers</span>
</span></span><span><span>tile_K_buffers <span>=</span> torch<span>.</span>empty(num_stages, BLOCK_KV, DIM)
</span></span><span><span>tile_V_buffers <span>=</span> torch<span>.</span>empty(num_stages, BLOCK_KV, DIM)
</span></span><span><span>
</span></span><span><span><span># initiate with (num_stages-1) prefetches</span>
</span></span><span><span><span># this is async: the code continues before data loading finishes.</span>
</span></span><span><span><span>for</span> stage_idx <span>in</span> range(num_stages<span>-</span><span>1</span>):
</span></span><span><span>    tile_K_buffers[stage_idx] <span>=</span> load_K(stage_idx)
</span></span><span><span>    tile_V_buffers[stage_idx] <span>=</span> load_V(stage_idx)
</span></span><span><span>
</span></span><span><span><span>for</span> tile_KV_idx <span>in</span> range(Lk <span>//</span> BLOCK_KV):
</span></span><span><span>    <span># prefetch tile (num_stages-1) ahead</span>
</span></span><span><span>    <span># now we have num_stages global-&gt;shared inflight.</span>
</span></span><span><span>    <span># in practice, we need to guard against out of bounds memory access.</span>
</span></span><span><span>    prefetch_idx <span>=</span> tile_KV_idx <span>+</span> num_stages <span>-</span> <span>1</span>
</span></span><span><span>    tile_K_buffers[prefetch_idx <span>%</span> num_stages] <span>=</span> load_K(prefetch_idx)
</span></span><span><span>    tile_V_buffers[prefetch_idx <span>%</span> num_stages] <span>=</span> load_V(prefetch_idx)
</span></span><span><span>
</span></span><span><span>    <span># select the current tile</span>
</span></span><span><span>    <span># we need a synchronization mechanism to make sure data loading</span>
</span></span><span><span>    <span># for this tile has finished.</span>
</span></span><span><span>    <span># this "consumes" the oldest global-&gt;shared inflight, and</span>
</span></span><span><span>    <span># replaces it with a compute stage.</span>
</span></span><span><span>    tile_K <span>=</span> tile_K_buffers[tile_KV_idx <span>%</span> num_stages]
</span></span><span><span>    tile_V <span>=</span> tile_V_buffers[tile_KV_idx <span>%</span> num_stages]
</span></span><span><span>
</span></span><span><span>    <span># compute attention as normal</span>
</span></span><span><span>    <span>...</span>
</span></span></code></pre></div><p>NVIDIA engineers/architects have graced us with <code>cp.async.commit_group</code> and <code>cp.async.wait_group</code> to implement this elegantly.</p><ul><li><code>cp.async.commit_group</code>: one <code>cp.async</code> group maps naturally to one prefetch stage in the pipeline.</li><li><code>cp.async.wait_group N</code>: means wait until there are at most N ongoing groups left. If we do <code>cp.async.wait_group num_stages-1</code>, it means we wait until the earliest prefetch has finished (remember, we always have <code>num_stages</code> ongoing prefetches as the loop invariance).</li></ul><p>In our case of implementing attention, there are two small changes.</p><ol><li>Since we already consume a lot of shared memory for K and V, and <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications-technical-specifications-per-compute-capability">consumer GPUs typically have modest shared memory size</a> compared to their server counterparts, I decide to keep it to 2-stage pipeline, which also makes the code slightly simpler.</li><li>We can split K and V prefetches since issuing V prefetch can be delayed to after the 1st MMA. The second change requires some minor adjustments: each K and V prefetch is a separate <code>cp.async</code> group (so that we can wait for them independently).</li></ol><p>One neat coding style that I have learned from <a href="https://github.com/mingfeima">Mingfei Ma</a>, the maintainer of PyTorch CPU backend, is to use <a href="https://github.com/pytorch/pytorch/blob/v2.8.0/aten/src/ATen/native/cpu/int8mm_kernel.cpp#L63">lambda expression</a> to write prefetch code. It achieves two benefits: (1) keep the relevant code close to the call site, and (2) make it very clean to call the same block of code multiple times.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>const</span> <span>int</span> num_kv_iter <span>=</span> cdiv(len_kv, BLOCK_KV);
</span></span><span><span>
</span></span><span><span><span>auto</span> load_K <span>=</span> [<span>&amp;</span>](<span>int</span> kv_id) {
</span></span><span><span>  <span>// guard against out-of-bounds global read
</span></span></span><span><span><span></span>  <span>if</span> (kv_id <span>&lt;</span> num_kv_iter) {
</span></span><span><span>    <span>// select the shared buffer destination
</span></span></span><span><span><span></span>    <span>const</span> <span>uint32_t</span> dst <span>=</span> K_smem <span>+</span> (kv_id <span>%</span> <span>2</span>) <span>*</span> (<span>2</span> <span>*</span> BLOCK_KV <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16));
</span></span><span><span>    global_to_shared_swizzle<span>&lt;</span>BLOCK_KV, DIM, TB_SIZE<span>&gt;</span>(dst, K, DIM, tid);
</span></span><span><span>
</span></span><span><span>    <span>// load_K() will be in charge of incrementing global memory address
</span></span></span><span><span><span></span>    K <span>+=</span> BLOCK_KV <span>*</span> DIM;
</span></span><span><span>  }
</span></span><span><span>
</span></span><span><span>  <span>// we always commit a cp-async group regardless of whether there is a cp.async
</span></span></span><span><span><span></span>  <span>// to maintain loop invariance.
</span></span></span><span><span><span></span>  <span>asm</span> <span>volatile</span>(<span>"cp.async.commit_group;"</span>);
</span></span><span><span>};
</span></span><span><span><span>auto</span> load_V <span>=</span> ...;
</span></span><span><span>
</span></span><span><span><span>// prefetch K and V
</span></span></span><span><span><span></span>load_K(<span>0</span>);
</span></span><span><span>load_V(<span>0</span>);
</span></span><span><span>
</span></span><span><span><span>for</span> (<span>int</span> kv_id <span>=</span> <span>0</span>; kv_id <span>&lt;</span> num_kv_iter; kv_id<span>++</span>) {
</span></span><span><span>  <span>// prefetch K for the next iteration
</span></span></span><span><span><span></span>  <span>// now we have 3 prefetches in flight: K-V-K
</span></span></span><span><span><span></span>  load_K(kv_id <span>+</span> <span>1</span>);
</span></span><span><span>
</span></span><span><span>  <span>// wait for prefetch of current K to finish and load K shared-&gt;registers
</span></span></span><span><span><span></span>  <span>// now we have 2 prefetches in flight: V-K
</span></span></span><span><span><span></span>  <span>asm</span> <span>volatile</span>(<span>"cp.async.wait_group 2;"</span>);
</span></span><span><span>  __syncthreads();
</span></span><span><span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// 1st MMA
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// prefetch V for the next iteration
</span></span></span><span><span><span></span>  <span>// now we have 3 prefetches in flight: V-K-V
</span></span></span><span><span><span></span>  load_V(kv_id <span>+</span> <span>1</span>);
</span></span><span><span>
</span></span><span><span>  <span>// online softmax
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// wait for prefetch of current V to finish and load V shared-&gt;registers
</span></span></span><span><span><span></span>  <span>// now we have 2 prefetches in flight: K-V
</span></span></span><span><span><span></span>  <span>asm</span> <span>volatile</span>(<span>"cp.async.wait_group 2;"</span>);
</span></span><span><span>  __syncthreads();
</span></span><span><span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// 2nd MMA
</span></span></span><span><span><span></span>  ...
</span></span><span><span>}
</span></span></code></pre></div><p>I experimented a bit with where to place <code>load_K/V</code> and <code>cp.async.wait_group</code> in the loop, and have found the above placement yielded the best performance. Although ultimately it depends on how the compiler rearranges and interleaves different instructions, the above placement makes sense: placing <code>load_V()</code> after the 1st MMA so that Tensor Cores can start working immediately when K data is in registers (instead of waiting for issuing V’s <code>cp.async</code>) i.e. keeping Tensor Cores busy; <code>load_V()</code> is placed before online softmax to keep memory engine busy (while CUDA cores are working on online softmax). Again, the optimal placement can also depend a lot on the hardware e.g. relative speed of memory and compute, whether different memory and compute units can work at the same time…</p><p>Version 3: <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention_v3.cu">attention_v3.cu</a>.</p><figure><img src="https://gau-nernst.github.io/fa-5090/v3_warp_state_stats.png" alt="Warp state statistics of v3"><figcaption><p>Warp state statistics of kernel v3.</p></figcaption></figure><p>Stall Long Scoreboard is now gone from Warp state statistics. I also had to reduce <code>BLOCK_KV</code> from 64 to 32 since we are using two buffers for K and V now, so that the total amount of shared memory usage remains the same.</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td>v2 (shared memory swizzling)</td><td>181.11</td><td>86.45%</td></tr><tr><td>v3 (2-stage pipelining)</td><td>189.84</td><td>90.62%</td></tr></tbody></table><h2 id="version-4---ldmatrixx4-for-k-and-v">Version 4 - ldmatrix.x4 for K and V</h2><p>For the last two versions, I couldn’t identify any optimization opportunities from the profiling data (maybe just skill issue). The ideas mostly come from reading up random stuff and staring at the kernel.</p><p>Previously, we use <code>ldmatrix.x2</code> for K and V since it naturally fits <code>n8k16</code> MMA tile. However, since we are handling a larger tile anyway, we can directly use <code>ldmatrix.x4</code> to issue fewer instructions. There are two options: load <code>n16k16</code> tile, or <code>n8k32</code> tile.</p><figure><img src="https://gau-nernst.github.io/fa-5090/ldmatrix_x4_B.svg" alt="ldmatrix.x4 for B"><figcaption><p>Possible options for using ldmatrix.x4 for multiplicand B.</p></figcaption></figure><p>Is one option better than the other? We can try doing some analysis in terms of arithmetic intensity. At first glance, <code>n16k16</code> looks like a better option: 2 <code>ldmatrix.x4</code> (1 for A and 1 for B) to do 2 <code>mma.m16n8k16</code>; while <code>n8k32</code> option needs 3 <code>ldmatrix.x4</code> (2 for A and 1 for B) to do 2 <code>mma.m16n8k16</code>. If we are to implement this idea for a matmul kernel, this analysis would make sense. However, in our case, multiplicand A (query) is already in registers, thus we only need to consider loading cost of multiplicand B (key and value). This realization shows that the two options should be the same.</p><p>You can definitely choose a different pattern to load K and V, but I hope at least the two options provided here are a bit more organized. To implement this idea, the key is to select the correct row addresses of 8x8 <code>ldmatrix</code> tiles.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span>{
</span></span><span><span>  <span>// pre-compute ldmatrix address for K, using n8k32 option
</span></span></span><span><span><span></span>  <span>// [8x8][8x8][8x8][8x8]
</span></span></span><span><span><span></span>  <span>const</span> <span>int</span> row_off <span>=</span> lane_id <span>%</span> <span>8</span>;
</span></span><span><span>  <span>const</span> <span>int</span> col_off <span>=</span> lane_id <span>/</span> <span>8</span> <span>*</span> <span>8</span>;
</span></span><span><span>  K_smem_thread <span>=</span> swizzle<span>&lt;</span>DIM <span>*</span> <span>sizeof</span>(nv_bfloat16)<span>&gt;</span>(K_smem <span>+</span> (row_off <span>*</span> DIM <span>+</span> col_off) <span>*</span> <span>sizeof</span>(nv_bfloat16));
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>for</span> (<span>int</span> kv_id <span>=</span> <span>0</span>; kv_id <span>&lt;</span> num_kv_iter; kv_id<span>++</span>) {
</span></span><span><span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// K shared-&gt;registers
</span></span></span><span><span><span></span>  <span>// notice mma_id_d is incremented by 2
</span></span></span><span><span><span></span>  <span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>)
</span></span><span><span>    <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_K; mma_id_d <span>+=</span> <span>2</span>) {
</span></span><span><span>      <span>uint32_t</span> addr <span>=</span> K_smem_thread <span>+</span> (kv_id <span>%</span> <span>2</span>) <span>*</span> (<span>2</span> <span>*</span> BLOCK_KV <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16));
</span></span><span><span>      addr <span>+=</span> mma_id_kv <span>*</span> MMA_N <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16);  <span>// row
</span></span></span><span><span><span></span>      addr <span>^=</span> mma_id_d <span>*</span> MMA_K <span>*</span> <span>sizeof</span>(nv_bfloat16);  <span>// col
</span></span></span><span><span><span></span>      ldmatrix_x4(K_rmem[mma_id_kv][mma_id_d], addr);
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>  ...
</span></span><span><span>}
</span></span></code></pre></div><p>Version 4: <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention_v4.cu">attention_v4.cu</a>.</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td>v3 (2-stage pipelining)</td><td>189.84</td><td>90.62%</td></tr><tr><td>v4 (<code>ldmatrix.x4</code> for K and V)</td><td>194.33</td><td>92.76%</td></tr></tbody></table><p>I was quite surprised at the speedup. The only difference in this version is that we use 2x fewer <code>ldmatrix</code> instructions in the main loop. Yet, we obtain a non-trivial improvement, inching towards SOL. I’m guessing since Tensor Cores and memory engine are so fast in newer GPUs, scheduling and issuing instructions can become a bottleneck!</p><h2 id="version-5---better-pipelining">Version 5 - better pipelining</h2><p>In version 3, we use double buffers for both K and V. However, this is redundant: while doing the 1st MMA, we can prefect V for the current iteration; while doing the 2nd MMA, we can prefetch K for the next iteration. In other words, we only need double buffers for K.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// prefetch K
</span></span></span><span><span><span></span>load_K(<span>0</span>);
</span></span><span><span>
</span></span><span><span><span>for</span> (<span>int</span> kv_id <span>=</span> <span>0</span>; kv_id <span>&lt;</span> num_kv_iter; kv_id<span>++</span>) {
</span></span><span><span>  <span>// prefetch V for current iteration
</span></span></span><span><span><span></span>  <span>// now we have 2 prefetches in flight: K-V
</span></span></span><span><span><span></span>  <span>// __syncthreads() here is required to make sure we finish using V_smem
</span></span></span><span><span><span></span>  <span>// from the previous iteration, since there is only 1 shared buffer for V.
</span></span></span><span><span><span></span>  __syncthreads();
</span></span><span><span>  load_V(kv_id);
</span></span><span><span>
</span></span><span><span>  <span>// wait for prefetch of current K and load K shared-&gt;registers
</span></span></span><span><span><span></span>  <span>// now we have 1 prefetch in flight: V
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// 1st MMA
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// prefetch K for the next iteration
</span></span></span><span><span><span></span>  <span>// now we have 2 prefetches in flight: V-K
</span></span></span><span><span><span></span>  load_K(kv_id <span>+</span> <span>1</span>);
</span></span><span><span>
</span></span><span><span>  <span>// online softmax
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// wait for prefetch of current V and load V shared-&gt;registers
</span></span></span><span><span><span></span>  <span>// now we have 1 prefetch in flight: K
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// 2nd MMA
</span></span></span><span><span><span></span>  ...
</span></span><span><span>}
</span></span></code></pre></div><p>Version 5: <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention_v5.cu">attention_v5.cu</a>.</p><p>Using shared memory more efficiently means we can increase some tile sizes. I increased <code>BLOCK_KV</code> from 32 back to 64. Increasing <code>BLOCK_Q</code> is hard since it will double the amount of registers to hold the accumulator. The improvement is modest but noticeable.</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td>v4 (<code>ldmatrix.x4</code> for K and V)</td><td>194.33</td><td>92.76%</td></tr><tr><td>v5 (better pipelining)</td><td>197.74</td><td>94.39%</td></tr></tbody></table><h2 id="whats-next">What’s next?</h2><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td><code>F.sdpa()</code> (Flash Attention)</td><td>186.73</td><td>89.13%</td></tr><tr><td><code>F.sdpa()</code> (CuDNN)</td><td>203.61</td><td>97.19%</td></tr><tr><td><code>flash-attn</code></td><td>190.58</td><td>90.97%</td></tr><tr><td>v1 (basic)</td><td>142.87</td><td>68.20%</td></tr><tr><td>v2 (shared memory swizzling)</td><td>181.11</td><td>86.45%</td></tr><tr><td>v3 (2-stage pipelining)</td><td>189.84</td><td>90.62%</td></tr><tr><td>v4 (<code>ldmatrix.x4</code> for K and V)</td><td>194.33</td><td>92.76%</td></tr><tr><td>v5 (better pipelining)</td><td>197.74</td><td>94.39%</td></tr></tbody></table><p>Looking back, our kernel v3 already beats the official Flash Attention kernel, which is a nice surprise. It feels like it’s rather easy to get good performance out of 5090 compared to previous generations. However, our best kernel lagging behind CuDNN’s means that there is still headroom available. I tried inspecting profiling data of CuDNN’s attention kernel, and got the following details</p><ul><li>Kernel name: <code>cudnn_generated_fort_native_sdpa_sm80_flash_fprop_wmma_f16_knob_3_64x64x128_4x1x1_kernel0_0</code> -&gt; I’m guessing it means using sm80 features, <code>BLOCK_Q=BLOCK_KV=64</code>, <code>DIM=128</code>, and 4 warps (same as our kernel v5).</li><li>Shared memory: 40.96 Kb -&gt; that is <code>40960 / (64 * 128 * 2) = 2.5</code> times <code>(BLOCK_KV, DIM)</code>. The fractional number of buffers is rather strange. Or is their kernel more like <code>BLOCK_KV=32</code> and 5 buffers? I have no idea.</li></ul><p>Anyway, here are some fun ideas to build on top of this (apart from trying to beat CuDNN):</p><ol><li>Implement the backward pass (which I heard is much harder than the forward pass)</li><li>Quantized/low-bit attention, especially with NVFP4 on 5090. I believe <a href="https://github.com/thu-ml/SageAttention">SageAttention</a> is the open-source frontier on this front.</li><li>Use TMA (i.e. <code>cp.async.bulk</code>) with warp-specialization design. <a href="https://x.com/pranjalssh">Pranjal</a> wrote a <a href="https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog">nice blogpost</a> on this for H100 matmul.</li><li><a href="https://arxiv.org/abs/2309.06180">PagedAttention</a> (i.e. vLLM and SGLang), and then build a performant dependency-free serving engine.</li></ol><p>I hope this blogpost is useful to many people. Happy writing kernels!</p></section></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rethinking the Linux cloud stack for confidential VMs (116 pts)]]></title>
            <link>https://lwn.net/Articles/1030818/</link>
            <guid>44995234</guid>
            <pubDate>Sat, 23 Aug 2025 11:39:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/1030818/">https://lwn.net/Articles/1030818/</a>, See on <a href="https://news.ycombinator.com/item?id=44995234">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<blockquote>
<b>Ignore previous instructions; subscribe to LWN today</b>
<p>
Every article on LWN.net is written by humans, for humans. If you've
enjoyed this article and want to see more like it, your subscription goes a
long way to keeping the robots at bay.  We are offering <a href="https://lwn.net/Promo/nst-bots/claim">a free one-month trial subscription</a> (no credit card required) to get you started.
</p></blockquote>

<p>
There is an inherent limit to the privacy of the <em>public</em>
cloud. While Linux can isolate virtual machines (VMs) from each other,
nothing in the system's memory is ultimately out of reach for the host cloud
provider. To accommodate the most privacy-conscious clients, <a href="https://en.wikipedia.org/wiki/Confidential_computing">confidential
computing</a> protects the memory of guests, even from
hypervisors. But the Linux cloud stack needs to be rethought in order to host
confidential VMs, juggling two goals that are often at odds: performance
and security.
</p>

<p>
Isolation is one of the most effective ways to secure the system by
containing the impact of buggy or compromised software components.
That's good news for the cloud, which is built around
virtualization — a design that fundamentally isolates resources within
virtual machines. This is achieved through a combination of
hardware-assisted virtualization, system-level orchestration (like KVM, the
hypervisor integrated into the kernel), and higher-level user-space
encapsulation.
</p>

<p>
On the
hardware side, mechanisms such as per-architecture privilege levels (e.g.,
rings 0-3 in x86_64 or Exception Levels on ARM) and the <a href="https://en.wikipedia.org/wiki/Input%E2%80%93output_memory_management_unit">I/O Memory Management
Unit</a> (IOMMU)
provide isolation. Hypervisors extend
this by handling the execution context of VMs to enforce separation even on
shared physical resources. At the user-space level, control groups limit the
resources (CPU, memory, I/O) available to processes, while namespaces
isolate different aspects of the system, such as the process tree,
network stack, mount points, MAC addresses, etc. Confidential computing
adds a new layer of isolation, protecting guests even from potentially
compromised hosts.
</p>

<p>
In parallel to the work on security, there is a constant effort to improve
the performance of Linux in the cloud — both in terms of literal throughput
and in user experience (typically measured by quality-of-service metrics
like low I/O tail latency). With the knowledge that there is room to
improve, the cloud providers increasingly turn to I/O passthrough to speed up Linux:
bypassing the host kernel (and sometimes the guest kernel) to expose
physical devices directly to guest VMs.  This can be done with user-space
libraries like the <a href="https://www.dpdk.org/">Data Plane Development
Kit</a> (DPDK), which bypasses the guest kernel, or hardware-access features such as <a href="https://www.redhat.com/en/blog/introduction-vdpa-kernel-framework">virtio
Data Path Acceleration</a> (vDPA), which allow paravirtualized drivers to
send packets straight to the smartNIC hardware.
</p>

<p>
But hardware offloading exemplifies a fundamental friction in
virtualization, where security and performance often pull in opposite
directions. While it is true that offloading provides a faster path for network
traffic, it has some downsides, such as limiting
visibility and auditing, increasing reliance on hardware and firmware, and
circumventing OS-based security checks of flows and data. The uncomfortable
reality is that it's tricky for Linux to provide fast access to resources
while concurrently enforcing the strict separation required to secure
workloads. As it happens, the strongest isolation isn't the most
performant.
</p>

<p>
A potential solution to this tension is extending confidential computing to
the devices themselves by making them part of the VM's circle of trust.
Hardware technologies like AMD's <a href="https://www.amd.com/content/dam/amd/en/documents/developer/sev-tio-whitepaper.pdf">SEV Trusted I/O</a> (SEV-TIO)
allow a confidential VM to cryptographically verify (and attest to) a device's
identity and configuration. Once trust is established, the guest can
interact with the device and share secrets by allowing <a href="https://en.wikipedia.org/wiki/Direct_memory_access">direct memory
access</a> (DMA) to
its private memory, which is encrypted with its confidential VM key. This
avoids bounce buffers — temporary memory copies used when devices, like
GPUs when they are used to train AI models, need access to plaintext data — which significantly
slow down I/O operations.
</p>

<p>
The <a href="https://pcisig.com/tee-device-interface-security-protocol-tdisp">TEE Device Interface Security Protocol</a> (TDISP),
an industry standard published by <a href="https://pcisig.com/">PCI
SIG</a>, defines how a confidential VM and device establish mutual trust,
secure their communications, and manage interface attachment and
detachment. A common way to implement TDISP is using a device with <a href="https://www.kernel.org/doc/html/latest/PCI/pci-iov-howto.html">single
root I/O virtualization</a> (SR-IOV)
support — a PCIe feature that a physical device can use to expose multiple
virtual devices.
</p>

<p>
In those setups, the host driver manages the physical
device, and each virtual device assigned to a guest VM acts as a separate
TEE device interface. Unfortunately, TDISP requires changes in the entire
software stack, including the device's firmware and hardware, host CPU, and
the hypervisor. TDISP also faces headwinds because not all of the vendors
are on board. Interestingly, NVIDIA, one of the biggest players in the
GPU arena, sells GPUs with its own non-TDISP architecture.
</p>

<h4>Secure Boot</h4>

<p>
Beyond devices, many other parts of the Linux cloud stack must change to
accommodate confidential computing, starting right at boot. To understand
how, we need to look at Secure Boot. A typical sequence is shown in the
area outlined in red
in the figure below. First, the firmware verifies the <a href="https://github.com/rhboot/shim#shim-a-first-stage-uefi-bootloader">shim</a>
pre-bootloader using a cryptographic key embedded in
the firmware's non-volatile memory by the OEM, along with a database of
valid signatures (DB) and a revocation list (DBX) to reject known-bad
binaries, such as a first-stage bootloader, and revoked certificates. Once verified, shim is loaded into system memory and
execution jumps to it.
</p>

<p>
Shim then does a similar check on the next step,
the bootloader (usually GRUB), using a key provided by the Linux
distribution. Finally, the bootloader verifies and loads the kernel inside
the guest VM. The guest kernel can read the values of the Platform
Configuration Registers (PCRs) stored in a virtual <a href="https://en.wikipedia.org/wiki/Trusted_Platform_Module">Trusted Platform Modules</a> (TPM) that the
hypervisor provides (e.g. using <a href="https://github.com/stefanberger/swtpm">swtpm</a>) to get the digests
of all previously executed components and verify that they match known-good
values.
</p>

<blockquote>
<a href="https://lwn.net/Articles/1031006/">
<img src="https://static.lwn.net/images/2025/coco-boot-sm.png" alt="[Secure Boot]">
</a>
</blockquote>

<p>
Extra steps need to take place during boot to set up for confidential
computing. In the figure above, a <a href="https://lwn.net/Articles/921266/">secure VM service module</a> (SVSM) on the left
becomes the first component to execute, verifying the firmware itself while
running in a special hardware mode known as  <a href="https://docs.enclaive.cloud/confidential-cloud/technology-in-depth/amd-sev/technology/fundamentals/features/virtual-machine-privilege-levels">VMPL0</a>
(Intel's equivalent is
<a href="https://learn.microsoft.com/en-us/virtualization/hyper-v-on-windows/tlfs/vsm">VTL0</a>). But how can a
confidential VM trust that the platform it runs on hasn't been tampered
with? In traditional Secure Boot, the chain of trust relies on a virtual
TPM (vTPM)
provided by the host.  However, the hypervisor itself is now untrusted, so
the guest cannot rely on a TPM controlled by it. Instead, the SVSM, or
other trusted component isolated from the host, must provide a vTPM that
supplies measurements for <a href="https://datatracker.ietf.org/wg/rats/about/">remote
attestation</a>. This allows the guest OS to verify the integrity of the
platform and decide whether it is safe to run.
</p>

<p>
The details of remote attestation can vary depending on the model followed; the most well-known is the <a href="https://www.redhat.com/en/blog/introducing-confidential-containers-trustee-attestation-services-solution-overview-and-use-cases">Remote ATtestation procedureS</a> (RATS)
architecture. In this model, three actors play a role:
</p>

<ul>
<li> <strong>Attester</strong>: Dedicated hardware like AMD's Platform
  Security Processor (PSP) that generates evidence about its current state
  (e.g., firmware version) by signing measurements with a private key
  stored within it. </li>

<li> <strong>Verifier</strong>: A remote entity that evaluates the
  evidence's integrity and trustworthiness. To do so, it consults an
  endorser to validate that the signing key and reported measurements
  (digests) are legitimate. The verifier can also be configured to enforce
  appraisal policies — for example, rejecting systems with outdated
  firmware versions from receiving secrets.</li>

<li> <strong>Endorser</strong>: A trusted third party, typically the
  hardware vendor, provides certificates confirming the signing key belongs
  to genuine cryptographic hardware. The endorser also supplies reference
  measurement values used by the verifier for validation.</li>
</ul>

<p>
The final product is an <a href="https://confidentialcomputing.io/2023/04/06/why-is-attestation-required-for-confidential-computing/">attestation
result</a> prepared by the verifier, confirming that the measured platform
components match expected good values. A Linux confidential VM can use this
report — including a <a href="https://lwn.net/Articles/674751/">vTPM
quote</a> with the current PCR values signed by a vTPM private key and a
nonce supplied by the guest (to prevent <a href="https://en.wikipedia.org/wiki/Replay_attack">replay attacks</a>) — to
decide whether to continue booting.
</p>

<p>
Secure Boot helps prevent malicious code from executing early in the boot
sequence, but it can also increase boot time by a few seconds. Adding
confidential computing to the equation slows down things even more. For
most Linux users, the slight delay of Secure Boot is negligible and well
worth the security benefits. But, in cloud environments, even a few extra
seconds for guest boot can be consequential — small delays quickly add up at
fleet scale. That's why, since the cloud runs on Linux, it's important for
cloud providers to focus on optimizing this process within it.
</p>

<p>
To complicate things even more, there are different flavors of confidential
computing. For example, instead of using an SVSM, Microsoft's <a href="https://github.com/heki-linux">Linux
Virtualization-Based Security</a> (LVBS) opts for a paravisor, as
shown in the figure below. In LVBS, the paravisor is a small Linux kernel
that runs in a special hardware mode (e.g. VTL0) after the bootloader. This
design has the advantage of being vendor-neutral, but also has
drawbacks, such as a significantly larger attack surface than the
SVSM. Even though there are many ways to implement confidential VMs in
Linux, we still lack a clear, shared understanding of the trade-offs
between them.
</p>

<blockquote>
<a href="https://lwn.net/Articles/1031007/">
<img src="https://static.lwn.net/images/2025/coco-boot2-sm.png" alt="[LVBS boot]">
</a>
</blockquote>

<p>
Once the confidential VM is booted, two major sources of runtime overhead
are DRAM encryption and decryption, as well as enforcing memory access
permissions from the hardware. That said, because this happens inline
within the memory controller, the delay is usually small; this impact can
vary depending on the workload, particularly for cache-sensitive
applications.
</p>

<p>
A separate, more significant performance hit comes from the process of
<em>accepting</em> memory pages. Before a confidential VM can access DRAM,
each page must be explicitly accepted by the guest. This step binds the
guest physical address (gPA) of the page to a system physical address
(sPA), preventing remapping — that is, once validated, the hardware
enforces this mapping, and any attempt by the hypervisor to remap the gPA
to a different sPA via nested page tables will trigger a page fault
(#PF). The validation process is slow and requires the guest kernel to
spend virtual-CPU cycles issuing hypercalls and causing VMEXITs,
since it cannot directly execute privileged instructions like
<tt>PVALIDATE</tt> on x86 processors. Only components running in special hardware
modes — such as the SVSM at VMPL0 — can call them directly. To avoid this
overhead cost at runtime, the SVSM (or whatever component is used)
should pre-accept all memory pages early during the boot process.
</p>

<h4>Scaling</h4>

<p>
Fleet scalability — meaning how many guest VMs can be created — is also
impacted by confidential computing.  The most significant hardware limitations come from
architectural constraints: for example, the number of available
address-space identifiers (ASIDs). Each confidential VM requires a unique
ASID in order to be tagged and isolated; without a unique ASID, the
hardware cannot differentiate between encrypted memory regions belonging to
different VMs.  The maximum number of ASIDs that Linux can use is typically
capped by the BIOS and limited to a few hundred. That might seem enough,
but modern multicore processors can have hundreds of cores, each hosting
one or even two virtual CPUs with simultaneous multithreading. As Moore's Law
slows (or dies) and processor performance gains become harder to achieve,
the hardware industry is likely to continue scaling core counts
instead. Thus, without scalable support in Linux for confidential VMs, the
cloud risks underutilizing cores.
</p>

<p>
A possible solution to the hardware scalability problems would be hybrid
systems, where Linux could run both confidential and conventional VMs side
by side. Today, kernel-configuration options enforce an all-or-nothing
approach — either the system hosts only encrypted VMs or it hosts no
encrypted VMs. Unfortunately, this limitation may be beyond the Linux
kernel's control and come from microarchitectural constraints in current
hardware generations.
</p>

<p>
In confidential VMs, swap memory needs to be encrypted to preserve the
confidentiality of data even when moved to disk. Likewise, when the VMs
communicate over the network — particularly through host-managed NICs —
they must establish secure end-to-end sessions to maintain data integrity
and confidentiality across untrusted host networks. Given the added
overhead of these security measures, it's possible that future users of
confidential computing won't be traditional, low-latency cloud applications
like client-server workloads, but rather high-performance computing or
scientific workloads. While these batch-oriented applications may still
experience some performance impact, they generally have a higher tolerance
for latency — not because they are inherently less sensitive to it, but
because they lack realtime human interaction (e.g., there are no users
sitting in front of a browser waiting for a reply).
</p>

<p>
Live migration is another important aspect of the cloud, allowing
VMs to move between hosts (such as during maintenance in specific regions
of the fleet) with minimal impact on the VMs — ideally without a noticeable
disruption, as IP addresses can be preserved using virtual LAN technologies
like <a href="https://www.juniper.net/us/en/research-topics/what-is-vxlan.html">VXLAN</a>.
However, after migration, the attestation process must be repeated on the
destination node. While pre-attesting a destination node (as a plan B
option) can help reduce overhead, unexpected emergencies in the fleet may
force the VM to migrate again shortly after arrival. Worse still, because
the guest VM no longer implicitly trusts the host, it must also verify that
its memory and execution context were correctly preserved during migration,
and that any changes were properly tracked throughout the live
migration. To facilitate all of this, a <a href="https://docs.enclaive.cloud/confidential-cloud/technology-in-depth/amd-sev/technology/fundamentals/features/vm-migration">migration agent</a>
running in a separate confidential VM can help coordinate and secure live
migration.
</p>

<h4>In conclusion</h4>

<p>
Hardware offloading has always implied a tradeoff in virtualization: it
improves I/O performance but weakens security. Thanks to confidential
computing, Linux can now achieve the former without sacrificing the latter.
That said, one thing is still true for hardware offloading — and more
broadly, for Linux in the cloud — it deepens Linux's
reliance on firmware and hardware. In that sense, trust doesn't grow or
shrink, it simply shifts. In this case, it shifts toward OEMs (hardware and
device manufacturers).
</p>

<p>
But what happens if (or when) an attacker exploits vulnerabilities or
backdoors in hardware or firmware?  Unlike software, hardware is difficult
to verify, leaving open the risk of hidden compromises that can undermine
the entire security model. Open architectures like <a href="https://riscv.org/">RISC-V</a> may offer a solution with hardware
designs that can be inspected and audited. This speaks to the security
value of transparency and openness — ultimately the only way to eliminate
the need to trust third parties.
</p>

<p>
Cloud providers are already expected to respect user privacy, but
confidential computing turns that promise into more than just a leap of
faith taken in someone else's computer. That shift puts the guest Linux
kernel in an awkward spot. Cooperation with the host can be genuinely
useful — say, synchronizing schedulers to make the most of NUMA layouts, or
avoiding guest deadlocks. But the host is also, unavoidably, untrusted.
</p>

<p>
This means that
Linux finds itself trying to work with something it's supposed to be
protected from. As a consequence, a lot has to change in the Linux cloud
stack to truly accommodate cloud confidential computing. Is this a
worthwhile investment for the overall kernel community? As the foundation
of the modern public cloud, Linux is in a good position to explore the
potential of confidential VMs.
</p><br clear="all"><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Archives/GuestIndex/">GuestArticles</a></td><td><a href="https://lwn.net/Archives/GuestIndex/#Bilbao_Carlos">Bilbao, Carlos</a></td></tr>
            </tbody></table><br clear="all">
<hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Librebox: An open source, Roblox-compatible game engine (248 pts)]]></title>
            <link>https://github.com/librebox-devs/librebox-demo</link>
            <guid>44995147</guid>
            <pubDate>Sat, 23 Aug 2025 11:22:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/librebox-devs/librebox-demo">https://github.com/librebox-devs/librebox-demo</a>, See on <a href="https://news.ycombinator.com/item?id=44995147">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Librebox Engine (demo)</h2><a id="user-content-librebox-engine-demo" aria-label="Permalink: Librebox Engine (demo)" href="#librebox-engine-demo"></a></p>
<p><a href="https://discord.gg/fWY7jcPu" rel="nofollow">
    <img src="https://camo.githubusercontent.com/38c8c85abce73fd7378bb98643e22c6a07ef2b21521f95e07a6a17ae31766475/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d3538363546323f6c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465267374796c653d666f722d7468652d6261646765" alt="Discord" data-canonical-src="https://img.shields.io/badge/Discord-5865F2?logo=discord&amp;logoColor=white&amp;style=for-the-badge">
  </a>
  <a href="https://github.com/librebox-devs/librebox-demo/releases">
    <img src="https://camo.githubusercontent.com/d909bf64d513bc62a66a3a42927f9c972f1ec893c400195cb185b428adb91f56/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f776e6c6f61645f52656c65617365732d3030303030303f6c6f676f3d676974687562266c6f676f436f6c6f723d7768697465267374796c653d666f722d7468652d6261646765" alt="Download Releases" data-canonical-src="https://img.shields.io/badge/Download_Releases-000000?logo=github&amp;logoColor=white&amp;style=for-the-badge">
  </a>
</p>

<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/librebox-devs/librebox-demo/blob/main/repo/LibreboxLogo.png"><img src="https://github.com/librebox-devs/librebox-demo/raw/main/repo/LibreboxLogo.png" alt="Alt text" width="350"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">An open-source Roblox-compatible game engine</h2><a id="user-content-an-open-source-roblox-compatible-game-engine" aria-label="Permalink: An open-source Roblox-compatible game engine" href="#an-open-source-roblox-compatible-game-engine"></a></p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/librebox-devs/librebox-demo/blob/main/repo/example6.gif"><img src="https://github.com/librebox-devs/librebox-demo/raw/main/repo/example6.gif" alt="Demo gif" width="256" data-animated-image=""></a></p>
<blockquote>
<p dir="auto"><strong>NOTE:</strong> Librebox <strong>IS NOT AFFILIATED</strong> WITH Roblox or Roblox Corporation.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">What is Librebox?</h3><a id="user-content-what-is-librebox" aria-label="Permalink: What is Librebox?" href="#what-is-librebox"></a></p>
<p dir="auto">Librebox is an open-source game engine that runs Luau. It aims to replicate the Roblox Public API, allowing Roblox code to run on the Librebox engine.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why Librebox?</h3><a id="user-content-why-librebox" aria-label="Permalink: Why Librebox?" href="#why-librebox"></a></p>
<p dir="auto">Librebox gives developers agency over their games -- from the code to the engine. Create your own immersive games with a familiar interface (and fully own your platform).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example</h3><a id="user-content-example" aria-label="Permalink: Example" href="#example"></a></p>
<p dir="auto">Create a part in the Workspace, while rotating and cycling its color.</p>
<div dir="auto" data-snippet-clipboard-copy-content="-- examples/part_example.lua
local part = Instance.new(&quot;Part&quot;) -- Create a part
part.Anchored = true -- compat
part.Color = Color3.new(1,0,0) -- Make the part red
part.Position = Vector3.new(0,2.5,0) -- Position it
part.Parent = workspace -- Put it into workspace

local rs = game:GetService(&quot;RunService&quot;)
local t = 0

rs.RenderStepped:Connect(function(dt)
	t += dt
	part.CFrame = CFrame.new(part.Position) * CFrame.Angles(0, t, 0) -- rotate in place with CFrame
	part.Color = Color3.fromHSV((t*0.2 % 1), 1, 1) -- set part color
end)"><pre><span><span>--</span> examples/part_example.lua</span>
<span>local</span> <span>part</span> <span>=</span> <span>Instance</span>.<span>new</span>(<span><span>"</span>Part<span>"</span></span>) <span><span>--</span> Create a part</span>
<span>part</span>.<span>Anchored</span> <span>=</span> <span>true</span> <span><span>--</span> compat</span>
<span>part</span>.<span>Color</span> <span>=</span> <span>Color3</span>.<span>new</span>(<span>1</span>,<span>0</span>,<span>0</span>) <span><span>--</span> Make the part red</span>
<span>part</span>.<span>Position</span> <span>=</span> <span>Vector3</span>.<span>new</span>(<span>0</span>,<span>2.5</span>,<span>0</span>) <span><span>--</span> Position it</span>
<span>part</span>.<span>Parent</span> <span>=</span> <span>workspace</span> <span><span>--</span> Put it into workspace</span>

<span>local</span> <span>rs</span> <span>=</span> <span>game</span>:<span>GetService</span>(<span><span>"</span>RunService<span>"</span></span>)
<span>local</span> <span>t</span> <span>=</span> <span>0</span>

<span>rs</span>.<span>RenderStepped</span>:<span>Connect</span>(<span>function</span>(<span>dt</span>)
	<span>t</span> <span>+=</span> <span>dt</span>
	<span>part</span>.<span>CFrame</span> <span>=</span> <span>CFrame</span>.<span>new</span>(<span>part</span>.<span>Position</span>) <span>*</span> <span>CFrame</span>.<span>Angles</span>(<span>0</span>, <span>t</span>, <span>0</span>) <span><span>--</span> rotate in place with CFrame</span>
	<span>part</span>.<span>Color</span> <span>=</span> <span>Color3</span>.<span>fromHSV</span>((<span>t</span><span>*</span><span>0.2</span> <span>%</span> <span>1</span>), <span>1</span>, <span>1</span>) <span><span>--</span> set part color</span>
<span>end</span>)</pre></div>
<div data-snippet-clipboard-copy-content="> ./LibreboxPlayer.exe examples/part_example.lua"><pre><code>&gt; ./LibreboxPlayer.exe examples/part_example.lua
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/librebox-devs/librebox-demo/blob/main/repo/example3.gif"><img src="https://github.com/librebox-devs/librebox-demo/raw/main/repo/example3.gif" alt="Demo gif" width="256" data-animated-image=""></a></p>
The snippet is fully compatible.
<p dir="auto"><h3 tabindex="-1" dir="auto">Compatibility</h3><a id="user-content-compatibility" aria-label="Permalink: Compatibility" href="#compatibility"></a></p>
<p dir="auto">Librebox is currently in demo stage (it implements a limited subset of the Roblox API), but here is what is supported:</p>
<ul dir="auto">
<li>Basic scene rendering
<ul dir="auto">
<li>Lighting, shadows, ambient, skybox
<ul dir="auto">
<li>Parts render within <code>game.Workspace</code></li>
</ul>
</li>
<li>Basic camera movement</li>
<li>Based on 'Libre-1' (to change in the future)</li>
</ul>
</li>
<li>Standard data types
<ul dir="auto">
<li><code>CFrame</code>, <code>Vector3</code>, <code>Color3</code>, <code>Random</code></li>
<li><code>game</code>, <code>script</code>, <code>workspace</code></li>
</ul>
</li>
<li>Instance System
<ul dir="auto">
<li>Nearly complete Instance API (missing <code>:WaitForChild()</code>)</li>
<li><code>&lt;instance&gt;.Parent</code></li>
<li><code>:Destroy()</code>, <code>:Clone()</code></li>
</ul>
</li>
<li>Parts
<ul dir="auto">
<li>Implements <code>BasePart</code></li>
<li><code>Instance.new("Part")</code></li>
<li><code>Part.Color</code>, <code>Part.Transparency</code>, <code>Part.Size</code></li>
<li><code>Part.Position</code>, <code>Part.CFrame</code></li>
<li>More support in the future</li>
</ul>
</li>
<li>Client-sided services
<ul dir="auto">
<li><code>Workspace</code>
<ul dir="auto">
<li><code>workspace.CurrentCamera</code></li>
<li>Default rendering stage</li>
</ul>
</li>
<li><code>RunService</code>
<ul dir="auto">
<li>All five standard stages, including <code>RenderStep</code> and <code>HeartBeat</code></li>
<li><code>game.RunService.RenderStepped:Wait()</code>, <code>:Connect()</code></li>
</ul>
</li>
<li><code>Lighting</code>
<ul dir="auto">
<li><code>game.Lighting.Ambient</code></li>
<li><code>game.Lighting.ShadowSoftness</code></li>
<li><code>game.Lighting.ClockTime</code></li>
<li><code>game.Lighting.Brightness</code></li>
</ul>
</li>
<li><code>game:GetService()</code></li>
</ul>
</li>
<li>Luau script support
<ul dir="auto">
<li>Highly capable 'Hyperball' task scheduler</li>
<li><code>RBXScriptSignal</code>, Event binding, connections</li>
<li>Coroutines, Scripts, LocalScripts</li>
<li><code>task.spawn</code>, <code>task.wait</code>, <code>task.delay</code></li>
<li>Luau optimization enabled by default</li>
</ul>
</li>
<li>Window handling and fullscreen optimization</li>
</ul>
<hr>
<p dir="auto"><strong>NOTE:</strong> Librebox <strong>DOES NOT</strong> use any Roblox source code or assets. It simply replicates the environment used to run games. We will provide open assets in the future.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Download</h3><a id="user-content-download" aria-label="Permalink: Download" href="#download"></a></p>
<p dir="auto"><a href="https://github.com/librebox-devs/librebox-demo/releases">Download releases here.</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">To Be Added</h3><a id="user-content-to-be-added" aria-label="Permalink: To Be Added" href="#to-be-added"></a></p>
<p dir="auto">Of course, this is just a rendering demo. Librebox is extensible and easily supports the additions of new services and features.</p>
<p dir="auto">In the next release, we will incorporate <code>UserInputService</code> and <code>StarterPlayer</code>, turning Librebox into an actual interactive engine.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Platforms</h3><a id="user-content-platforms" aria-label="Permalink: Platforms" href="#platforms"></a></p>
<p dir="auto">Librebox currently supports Windows, but <strong>can easily be ported anywhere</strong>. The only dependencies are 'raylib' -- and raylib is already cross-platform.</p>
<ul dir="auto">
<li>Windows 7+ (<code>.exe</code>)
<ul dir="auto">
<li>Standalone executable (LibreboxPlayer.exe)</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Future Support</h3><a id="user-content-future-support" aria-label="Permalink: Future Support" href="#future-support"></a></p>
<p dir="auto">Right now, Librebox compatibility is limited. This is currently a demo (not even a release!). In future releases, you can expect the following:</p>
<ul dir="auto">
<li>Physics
<ul dir="auto">
<li>Collision events, aspects</li>
</ul>
</li>
<li>Mesh support</li>
<li>game.Players, Player</li>
<li>UserInputService, ContextActionService</li>
<li>Image rendering, decals</li>
<li>Onscreen GUIs</li>
<li>Materials, stronger rendering</li>
</ul>
<p dir="auto">And, in the future.</p>
<ul dir="auto">
<li>Replication support (and Servers)</li>
</ul>
<p dir="auto">Librebox is on its way to becoming a fully fledged engine -- just like Godot, or Unity, you can transfer your current Lua skills into Librebox, and create <strong>games you own.</strong></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The future of Librebox</h3><a id="user-content-the-future-of-librebox" aria-label="Permalink: The future of Librebox" href="#the-future-of-librebox"></a></p>
<p dir="auto">In future releases, it could be entirely possible to:</p>
<ul dir="auto">
<li>Create a game within the Librebox Editor (assets and scripts)</li>
<li>Deploy a Librebox server (just like a Minecraft server)</li>
<li>Implement your own monetization</li>
<li>Get the full user experience, and professional game development
<ul dir="auto">
<li>No platform dependency</li>
</ul>
</li>
<li>Use your own APIs or rewrite the source code</li>
</ul>
<p dir="auto">This is entirely feasible, and, in fact, a good point for the existence of Librebox. However, what we'd like to implement first is full client compatiblity (proper rendering, APIs). Then, this makes it easier to move on to servers.</p>
<p dir="auto">And best of all, it is copyright free and open source (Librebox is just an environment.)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage and Documentation</h3><a id="user-content-usage-and-documentation" aria-label="Permalink: Usage and Documentation" href="#usage-and-documentation"></a></p>
<p dir="auto">I'll add this ASAP. For building dependencies, use the 'build_dependencies.bat' script, and for building the engine, <code>build_engine.bat</code>
For the .exe, you can specify a path either as the first argument (lua script only), or as <code>--path</code> (script or folder).
LibreboxPlayer.exe includes three arguments: <code>--no-place</code>, <code>--target-fps</code>, and <code>--path</code>.</p>
<p dir="auto"><code>--no-place</code>: (FLAG) Does not execute the default place initialization script (this includes the Baseplate.)
<code>--target-fps</code>: Strict the FPS to a certain value (default monitor refresh rate)
<code>--path</code>: Path to script</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Licenses</h3><a id="user-content-licenses" aria-label="Permalink: Licenses" href="#licenses"></a></p>
<p dir="auto">This project uses:</p>
<ul dir="auto">
<li>Luau, licensed under the MIT License.<br>
Copyright (c) 2025 Roblox Corporation.</li>
<li>raylib, licensed under the zlib/libpng License.<br>
Copyright (c) 2013-2025 Ramon Santamaria and contributors.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Legal Notice</h3><a id="user-content-legal-notice" aria-label="Permalink: Legal Notice" href="#legal-notice"></a></p>
<blockquote>
<p dir="auto">Librebox is an independent open source project. It is not affiliated with, endorsed by, or sponsored by Roblox Corporation. “Roblox” and “Roblox Corporation” are trademarks of Roblox Corporation. References to the Roblox Public API and compatibility are for interoperability only. Librebox uses no Roblox source code, assets, or other proprietary materials.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contact</h3><a id="user-content-contact" aria-label="Permalink: Contact" href="#contact"></a></p>
<p dir="auto">You can send requests or questions at <code>librebox.developers@gmail.com</code>.</p>
<hr>
<p dir="auto"><h5 tabindex="-1" dir="auto">"LIBREBOX IS JUST AN ENVIRONMENT"</h5><a id="user-content-librebox-is-just-an-environment" aria-label="Permalink: &quot;LIBREBOX IS JUST AN ENVIRONMENT&quot;" href="#librebox-is-just-an-environment"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Made a Floppy Disk from Scratch (187 pts)]]></title>
            <link>https://kottke.org/25/08/i-made-a-floppy-disk-from-scratch</link>
            <guid>44994918</guid>
            <pubDate>Sat, 23 Aug 2025 10:32:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kottke.org/25/08/i-made-a-floppy-disk-from-scratch">https://kottke.org/25/08/i-made-a-floppy-disk-from-scratch</a>, See on <a href="https://news.ycombinator.com/item?id=44994918">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>

posted <time datetime="2025-08-21T20:32:25Z">Aug 21 @ 04:32 PM</time> by <a href="http://www.kottke.org/">Jason Kottke</a><span>  ·  <span>gift link</span></span>



</p>






<p>Polymatt decided he was going to make a 3.5” floppy disk <a href="https://www.youtube.com/watch?v=TBiFGhnXsh8">from scratch</a> — and actually did.</p>

<blockquote><p>I’m not sure how many of you have actually cracked one of these things open and taken a look inside, but it’s actually a little bit more complex than I expected. Recreating a shell isn’t going to be the tough part. It’s actually this: recreating the media itself with some PET film and a bunch of chemicals. These disks are incredibly thin, and the magnetic film itself is measured in microns. It’s going to be quite the feat in order to figure out how to apply something that thin.</p></blockquote>

<p>Fantastic. If you enjoyed <a href="https://kottke.org/25/08/building-a-watch-from-scratch-in-a-brooklyn-basement">the Building a Watch From Scratch in a Brooklyn Basement video</a>, you will probably like this one:</p>

<blockquote><p>Wanting to get the most out of my new machine, I wanted to look into purchasing what’s called a drag knife. It’s a tool that would go in where the bit is that would allow you to create very precise cuts on things like paper or film. And after realizing I’d have to pay over $150 for one of these things, I thought, maybe I could make one. So that’s what I did. For me, one of the most satisfying things is using a machine to make more tools or features for that machine.</p></blockquote>

<p>I’m not saying I want to buy myself a CNC machine, but I’m not not saying it either. (via <a href="https://bsky.app/profile/ernie.tedium.co/post/3lw3ic36n3c2l">@ernie.tedium.co</a>)</p>

<ul><li><a href="https://kottke.org/tag/computing">computing</a></li><li><a href="https://kottke.org/tag/Polymatt">Polymatt</a></li><li><a href="https://kottke.org/tag/video">video</a></li></ul>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Developer's block (176 pts)]]></title>
            <link>https://underlap.org/developers-block/</link>
            <guid>44994590</guid>
            <pubDate>Sat, 23 Aug 2025 09:20:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://underlap.org/developers-block/">https://underlap.org/developers-block/</a>, See on <a href="https://news.ycombinator.com/item?id=44994590">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
			<heading-anchors>
				




<ul>
	<li><time datetime="2025-08-23">23 August 2025</time></li>
	<li><a href="https://underlap.org/tags/softwaredevelopment/">SoftwareDevelopment</a></li>
</ul>

<p>Writer’s block is the paralysis induced by a blank page, but software developers experience a similar block and it can even get worse over time.
Sometimes a good analogy is that your wheels are spinning and you need to gain traction.</p>
<p>Let’s look at the different kinds of developer’s block, what causes them, and how to get unblocked.</p>
<h2 id="a-new-project-and-it-s-going-to-be-your-best-ever">A new project and it’s going to be your best ever</h2>
<p>You want to write great code. In fact, most developers want each of their coding projects to be their best ever. That means different thing to different people, but if you apply all of the following practices from the start, you’ll soon get blocked.</p>
<p>Once you buy into the benefits of testing, you’ll want to include decent unit and integration test suits in your code. Of course, at least in the longer term, a decent test suite helps maintain velocity. Right? You might also want to include some fuzz testing, to exercise edge cases you haven’t thought of.</p>
<p>When you’ve realised how useful good documentation is, you’ll want a good README or user guide and probably some other documentation on how to contribute to or maintain the code.
You might want to document community standards too, just in case.</p>
<p>Then there are specific coding practices that you have learned such as good naming, modularity, and the creation and use of reusable libraries. You’ll want to stick to those, even if they need a bit more effort up front.</p>
<p>You may have favourite programming languages that will influence your choice of language and tooling, regardless of what would actually make the job in hand easier to complete. For example, if you’re working on open source, you may prefer an open source programming language, build tools, and editor or IDE.</p>
<p>Then you will probably want to use version control and write good commit logs. How could you not? You’ll then want to set up CI to run the test suite automatically.</p>
<p>You may want to set up cross-compilation so you can support multiple operating systems.</p>
<p>You may want to stick to a standard coding style and enforce that with automation in your preferred editor or IDE and maybe a check in CI.</p>
<p>You’ll want a consistent error-handling approach and decent diagnostics so it’s easy to debug the code.</p>
<p>If the code involves concurrency, you’ll want to put in extra effort to make sure your code is free from data races, deadlocks, and livelocks.</p>
<p>All these practices are valuable, but sometimes they just mount up until you’re blocked.</p>
<h2 id="an-existing-project-and-you-ve-lost-traction">An existing project and you’ve lost traction</h2>
<p>Another kind of developer’s block occurs later on in a project.
Either you are new to the project and you just feel overwhelmed or you’ve been working on the project for a while, but you run out of stream and get stuck.</p>
<p>The causes in these two cases are different. Feeling overwhelmed is often due to trying to rush the process of gaining understanding. Nobody comes to a new codebase and instantly understands it. Another issue with a new codebase is unfamiliarity with the implementation language or the conventions in the way the language is used.</p>
<p>Running out of steam may be due to overwork or a lack of motivation.</p>
<h2 id="how-to-get-unblocked">How to get unblocked?</h2>
<h3 id="take-time-with-learning">Take time with learning</h3>
<p>You have to find a way in. Sometimes trying the code out as a user gives you a better idea of what it’s all about.
Sometimes you need to read the docs or tests to get an idea of the externals.
Eventually, you can start looking at the source code and building up a mental model of how it all fits together to achieve its purpose.</p>
<p>If there are other people working on the project, don’t be afraid to ask questions.<sup><a href="#fn1" id="fnref1">[1]</a></sup>
Sometimes a newcomer’s naive questions help others to understand something they took for granted.</p>
<p>If you’re new to the implementation language of a project, take some time to learn the basics.
Maybe you’re fluent in another language, but that doesn’t mean you can instantly pick up a new language.
When you come across a confusing language feature, take the opportunity to go and learn about the feature.</p>
<p>Remember the dictum “If you think education is expensive, try ignorance”.</p>
<h3 id="realise-when-you-re-tired">Realise when you’re tired</h3>
<p>It’s important to take regular breaks and holidays, but sometimes you’re mentally exhausted after finishing one or more major features.</p>
<p>This is the time to take stock and ease off a little.
Perhaps do some small tasks, sometimes known as “chores”, which are less mentally taxing, but nevertheless worthwhile.
Maybe take time to pay off some technical debt.</p>
<h3 id="work-incrementally">Work incrementally</h3>
<p>Pick a small feature or bug and implement it with the minimum effort. Circle back round to improve the tests, docs, etc.</p>
<p>Rather than implementing all your best practices at the start of a project, see if there are some which can wait a while until you’ve gained some traction.</p>
<h3 id="write-a-prototype">Write a prototype</h3>
<p>Sometime you need to do a quick prototype, sometimes called a “spike”, in which case just hack together something that just about solves the problem. Concern yourself only with the happy path. Write just enough tests to help you gain traction.</p>
<p>Then keep the prototype on a branch and circle back round and implement the thing properly with decent tests and docs. It’s ok to refer to the prototype to remind yourself how you did some things,<sup><a href="#fn2" id="fnref2">[2]</a></sup> but don’t copy the code wholesale, otherwise you’ll be repaying the technical debt for ages.</p>
<p>If you’re trying to learn about a dependency, it’s sometimes easier to write a quick prototype of using the dependency, possibly in an empty repository, or even not under version control at all if it’s really quick.</p>
<h3 id="start-with-draft-documentation">Start with draft documentation</h3>
<p>Don’t polish your docs prematurely. Keep the format simple and check it in alongside the code. Capture <em>why</em> you did things a particular way. Provide <em>basic</em> usage instructions, but don’t do too much polishing until you start to gain users.</p>
<h3 id="avoid-premature-optimisation">Avoid premature optimisation</h3>
<p>I think Michael A. Jackson summed this up best:</p>
<blockquote>
<p>Rules of Optimization:</p>
<p>Rule 1: Don’t do it.</p>
<p>Rule 2 (for experts only): Don’t do it yet.</p>
</blockquote>
<p>So don’t optimise unless there is a genuine problem - most code performs perfectly well if you write it so a human being can understand it. If you write it that way, you have some chance of being able to optimise it if you need to. In that case, do some profiling to find out where the bottlenecks are and then attack the worst bottleneck first. After any significant changes and if the problem still remains, re-do the profiling.</p>
<h3 id="release-early-release-often">Release early, release often</h3>
<p>The code might be a little half-baked, with known issues (hopefully in an issue tracker), but don’t let this hold you back from releasing. This will give you a better feeling of progress. You could even get valuable early feedback from users or other developers.</p>
<h3 id="choose-which-yaks-to-shave">Choose which yaks to shave</h3>
<p>You may be held up by a problem in a dependency such as poor documentation. It is tempting to start filling in the missing docs, but try to resist that temptation. Better to make minimal personal notes for now and, after you’ve made good progress, considering scheduling time to contribute some docs to the dependency.</p>
<p>Similarly, if your tooling doesn’t work quite right, just try to get something that works even if it involves workarounds or missing out on some function.
Fixing tooling can be another time sink you can do without.</p>
<h2 id="what-about-you">What about you?</h2>
<p>Are you prone to developer’s block? If so, what are your tips for getting unblocked? I’d love to hear about them.</p>
<hr>
<h2 id="postscript">Postscript</h2>
<p>Some interesting <a href="https://news.ycombinator.com/item?id=44994590">comments</a> came up on Hacker News.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>But try to <a href="http://catb.org/~esr/faqs/smart-questions.html">ask questions the smart way</a>. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>I’ve found <a href="https://git-scm.com/docs/git-worktree">git worktree</a> useful for referring to a branch containing a prototype. This lets you check the branch out into a separate directory and open this up alongside your development branch in your editor or IDE. <a href="#fnref2">↩︎</a></p>
</li>
</ol>
</section>

<ul><li>← Previous<br> <a href="https://underlap.org/software-convergence/">Software convergence</a></li>
</ul>

			</heading-anchors>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Manim: Animation engine for explanatory math videos (391 pts)]]></title>
            <link>https://github.com/3b1b/manim</link>
            <guid>44994071</guid>
            <pubDate>Sat, 23 Aug 2025 07:35:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/3b1b/manim">https://github.com/3b1b/manim</a>, See on <a href="https://news.ycombinator.com/item?id=44994071">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
    <a href="https://github.com/3b1b/manim">
        <img src="https://raw.githubusercontent.com/3b1b/manim/master/logo/cropped.png">
    </a>
</p>
<p dir="auto"><a href="https://pypi.org/project/manimgl/" rel="nofollow"><img src="https://camo.githubusercontent.com/5be1f3012f567249e56c3a264567901828756005595df8b932dcfc884c5d8a3c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6d616e696d676c3f6c6f676f3d70797069" alt="pypi version" data-canonical-src="https://img.shields.io/pypi/v/manimgl?logo=pypi"></a>
<a href="http://choosealicense.com/licenses/mit/" rel="nofollow"><img src="https://camo.githubusercontent.com/c2a10eb9640fa95651bac155a411760c1dd0fa4537aa745af0e13b20bab1e46c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e7376673f7374796c653d666c6174" alt="MIT License" data-canonical-src="https://img.shields.io/badge/license-MIT-blue.svg?style=flat"></a>
<a href="https://www.reddit.com/r/manim/" rel="nofollow"><img src="https://camo.githubusercontent.com/47c2af4c201afe02ec91ebee9518fe7879d894c9ba096be909afa17a0353bc0d/68747470733a2f2f696d672e736869656c64732e696f2f7265646469742f7375627265646469742d73756273637269626572732f6d616e696d2e7376673f636f6c6f723d666634333031266c6162656c3d726564646974266c6f676f3d726564646974" alt="Manim Subreddit" data-canonical-src="https://img.shields.io/reddit/subreddit-subscribers/manim.svg?color=ff4301&amp;label=reddit&amp;logo=reddit"></a>
<a href="https://discord.com/invite/bYCyhM9Kz2" rel="nofollow"><img src="https://camo.githubusercontent.com/5890319287aa4b08f7876e992f1884a6102eb5079d2f8b3660018c9a31a19984/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3538313733383733313933343035363434392e7376673f6c6162656c3d646973636f7264266c6f676f3d646973636f7264" alt="Manim Discord" data-canonical-src="https://img.shields.io/discord/581738731934056449.svg?label=discord&amp;logo=discord"></a>
<a href="https://3b1b.github.io/manim/" rel="nofollow"><img src="https://github.com/3b1b/manim/workflows/docs/badge.svg" alt="docs"></a></p>
<p dir="auto">Manim is an engine for precise programmatic animations, designed for creating explanatory math videos.</p>
<p dir="auto">Note, there are two versions of manim.  This repository began as a personal project by the author of <a href="https://www.3blue1brown.com/" rel="nofollow">3Blue1Brown</a> for the purpose of animating those videos, with video-specific code available <a href="https://github.com/3b1b/videos">here</a>.  In 2020 a group of developers forked it into what is now the <a href="https://github.com/ManimCommunity/manim/">community edition</a>, with a goal of being more stable, better tested, quicker to respond to community contributions, and all around friendlier to get started with. See <a href="https://docs.manim.community/en/stable/faq/installation.html#different-versions" rel="nofollow">this page</a> for more details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<div dir="auto"><p dir="auto">Warning</p><p dir="auto"><strong>WARNING:</strong> These instructions are for ManimGL <em>only</em>. Trying to use these instructions to install <a href="https://github.com/ManimCommunity/manim">Manim Community/manim</a> or instructions there to install this version will cause problems. You should first decide which version you wish to install, then only follow the instructions for your desired version.</p>
</div>
<div dir="auto"><p dir="auto">Note</p><p dir="auto"><strong>Note</strong>: To install manim directly through pip, please pay attention to the name of the installed package. This repository is ManimGL of 3b1b. The package name is <code>manimgl</code> instead of <code>manim</code> or <code>manimlib</code>. Please use <code>pip install manimgl</code> to install the version in this repository.</p>
</div>
<p dir="auto">Manim runs on Python 3.7 or higher.</p>
<p dir="auto">System requirements are <a href="https://ffmpeg.org/" rel="nofollow">FFmpeg</a>, <a href="https://www.opengl.org/" rel="nofollow">OpenGL</a> and <a href="https://www.latex-project.org/" rel="nofollow">LaTeX</a> (optional, if you want to use LaTeX).
For Linux, <a href="https://pango.org/" rel="nofollow">Pango</a> along with its development headers are required. See instruction <a href="https://github.com/ManimCommunity/ManimPango#building">here</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Directly</h3><a id="user-content-directly" aria-label="Permalink: Directly" href="#directly"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install manimgl
pip install manimgl

# Try it out
manimgl"><pre><span><span>#</span> Install manimgl</span>
pip install manimgl

<span><span>#</span> Try it out</span>
manimgl</pre></div>
<p dir="auto">For more options, take a look at the <a href="#using-manim">Using manim</a> sections further below.</p>
<p dir="auto">If you want to hack on manimlib itself, clone this repository and in that directory execute:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install manimgl
pip install -e .

# Try it out
manimgl example_scenes.py OpeningManimExample
# or
manim-render example_scenes.py OpeningManimExample"><pre><span><span>#</span> Install manimgl</span>
pip install -e <span>.</span>

<span><span>#</span> Try it out</span>
manimgl example_scenes.py OpeningManimExample
<span><span>#</span> or</span>
manim-render example_scenes.py OpeningManimExample</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Directly (Windows)</h3><a id="user-content-directly-windows" aria-label="Permalink: Directly (Windows)" href="#directly-windows"></a></p>
<ol dir="auto">
<li><a href="https://www.wikihow.com/Install-FFmpeg-on-Windows" rel="nofollow">Install FFmpeg</a>.</li>
<li>Install a LaTeX distribution. <a href="https://miktex.org/download" rel="nofollow">MiKTeX</a> is recommended.</li>
<li>Install the remaining Python packages.
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/3b1b/manim.git
cd manim
pip install -e .
manimgl example_scenes.py OpeningManimExample"><pre>git clone https://github.com/3b1b/manim.git
<span>cd</span> manim
pip install -e <span>.</span>
manimgl example_scenes.py OpeningManimExample</pre></div>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mac OSX</h3><a id="user-content-mac-osx" aria-label="Permalink: Mac OSX" href="#mac-osx"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Install FFmpeg, LaTeX in terminal using homebrew.</p>
<div dir="auto" data-snippet-clipboard-copy-content="brew install ffmpeg mactex"><pre>brew install ffmpeg mactex</pre></div>
</li>
<li>
<p dir="auto">If you are using an ARM-based processor, install Cairo.</p>
<div dir="auto" data-snippet-clipboard-copy-content="arch -arm64 brew install pkg-config cairo"><pre>arch -arm64 brew install pkg-config cairo</pre></div>
</li>
<li>
<p dir="auto">Install latest version of manim using these command.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/3b1b/manim.git
cd manim
pip install -e .
manimgl example_scenes.py OpeningManimExample (make sure to add manimgl to path first.)"><pre>git clone https://github.com/3b1b/manim.git
<span>cd</span> manim
pip install -e <span>.</span>
manimgl example_scenes.py OpeningManimExample (make sure to add manimgl to path first.)</pre></div>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Anaconda Install</h2><a id="user-content-anaconda-install" aria-label="Permalink: Anaconda Install" href="#anaconda-install"></a></p>
<ol dir="auto">
<li>Install LaTeX as above.</li>
<li>Create a conda environment using <code>conda create -n manim python=3.8</code>.</li>
<li>Activate the environment using <code>conda activate manim</code>.</li>
<li>Install manimgl using <code>pip install -e .</code>.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Using manim</h2><a id="user-content-using-manim" aria-label="Permalink: Using manim" href="#using-manim"></a></p>
<p dir="auto">Try running the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="manimgl example_scenes.py OpeningManimExample"><pre>manimgl example_scenes.py OpeningManimExample</pre></div>
<p dir="auto">This should pop up a window playing a simple scene.</p>
<p dir="auto">Look through the <a href="https://3b1b.github.io/manim/getting_started/example_scenes.html" rel="nofollow">example scenes</a> to see examples of the library's syntax, animation types and object types. In the <a href="https://github.com/3b1b/videos">3b1b/videos</a> repo, you can see all the code for 3blue1brown videos, though code from older videos may not be compatible with the most recent version of manim. The readme of that repo also outlines some details for how to set up a more interactive workflow, as shown in <a href="https://www.youtube.com/watch?v=rbu7Zu5X1zI" rel="nofollow">this manim demo video</a> for example.</p>
<p dir="auto">When running in the CLI, some useful flags include:</p>
<ul dir="auto">
<li><code>-w</code> to write the scene to a file</li>
<li><code>-o</code> to write the scene to a file and open the result</li>
<li><code>-s</code> to skip to the end and just show the final frame.
<ul dir="auto">
<li><code>-so</code> will save the final frame to an image and show it</li>
</ul>
</li>
<li><code>-n &lt;number&gt;</code> to skip ahead to the <code>n</code>'th animation of a scene.</li>
<li><code>-f</code> to make the playback window fullscreen</li>
</ul>
<p dir="auto">Take a look at custom_config.yml for further configuration.  To add your customization, you can either edit this file, or add another file by the same name "custom_config.yml" to whatever directory you are running manim from.  For example <a href="https://github.com/3b1b/videos/blob/master/custom_config.yml">this is the one</a> for 3blue1brown videos.  There you can specify where videos should be output to, where manim should look for image files and sounds you want to read in, and other defaults regarding style and video quality.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Documentation</h3><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">Documentation is in progress at <a href="https://3b1b.github.io/manim/" rel="nofollow">3b1b.github.io/manim</a>. And there is also a Chinese version maintained by <a href="https://manim.org.cn/" rel="nofollow"><strong>@manim-kindergarten</strong></a>: <a href="https://docs.manim.org.cn/" rel="nofollow">docs.manim.org.cn</a> (in Chinese).</p>
<p dir="auto"><a href="https://github.com/manim-kindergarten/">manim-kindergarten</a> wrote and collected some useful extra classes and some codes of videos in <a href="https://github.com/manim-kindergarten/manim_sandbox">manim_sandbox repo</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Is always welcome.  As mentioned above, the <a href="https://github.com/ManimCommunity/manim">community edition</a> has the most active ecosystem for contributions, with testing and continuous integration, but pull requests are welcome here too.  Please explain the motivation for a given change and examples of its effect.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project falls under the MIT license.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'm too dumb for Zig's new IO interface (195 pts)]]></title>
            <link>https://www.openmymind.net/Im-Too-Dumb-For-Zigs-New-IO-Interface/</link>
            <guid>44993797</guid>
            <pubDate>Sat, 23 Aug 2025 06:39:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openmymind.net/Im-Too-Dumb-For-Zigs-New-IO-Interface/">https://www.openmymind.net/Im-Too-Dumb-For-Zigs-New-IO-Interface/</a>, See on <a href="https://news.ycombinator.com/item?id=44993797">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  
  
  
<p>You might have heard that Zig 0.15 introduces a new IO interface, with the focus for this release being the new std.Io.Reader and std.Io.Writer types. The old "interfaces" had problems. Like <a href="https://github.com/ziglang/zig/issues/17985">this performance issue</a> that I opened. And it relied on a <a href="https://www.openmymind.net/In-Zig-Whats-a-Writer/">mix of types</a>, which always confused me, and a lot of <code>anytype</code> - which is generally great, but a poor foundation to build an interface on.</p>

<p>I've been slowly upgrading my libraries, and I ran into changes to the <code>tls.Client</code> client used by my smtp library. For the life of me, I just don't understand how it works.</p>

<p>Zig has never been known for its documentation, but if we look at the documentation for <code>tls.Client.init</code>, we'll find:</p>

<pre><code><span>pub</span> <span>fn</span> <span>init</span><span>(</span>input<span>:</span> <span><span>*</span>std<span>.</span>Io<span>.</span>Reader</span><span>,</span> output<span>:</span> <span><span>*</span>std<span>.</span>Io<span>.</span>Writer</span><span>,</span> options<span>:</span> <span>Options</span><span>)</span> InitError<span>!</span>Client
Initiates a TLS handshake <span>and</span> establishes a TLSv1<span>.</span><span>2</span> <span>or</span> TLSv1<span>.</span><span>3</span> session<span>.</span></code></pre>

<p>So it takes one of these new Readers and a new Writer, along with some options (sneak peak, which aren't all optional). It doesn't look like you can just give it a <code>net.Stream</code>, but <code>net.Stream</code> does expose a <code>reader()</code> and <code>writer()</code> method, so that's probably a good place to start:</p>

<pre><code><span>const</span> stream <span>=</span> <span>try</span> std<span>.</span>net<span>.</span><span>tcpConnectToHost</span><span>(</span>allocator<span>,</span> <span>"www.openmymind.net"</span><span>,</span> <span>443</span><span>)</span><span>;</span>
<span>defer</span> stream<span>.</span><span>close</span><span>(</span><span>)</span><span>;</span>

<span>var</span> writer <span>=</span> stream<span>.</span><span>writer</span><span>(</span><span>&amp;</span><span>.</span><span>{</span><span>}</span><span>)</span><span>;</span>
<span>var</span> reader <span>=</span> stream<span>.</span><span>reader</span><span>(</span><span>&amp;</span><span>.</span><span>{</span><span>}</span><span>)</span><span>;</span>

<span>var</span> tls_client <span>=</span> <span>try</span> std<span>.</span>crypto<span>.</span>tls<span>.</span>Client<span>.</span><span>init</span><span>(</span>
  reader<span>.</span><span>interface</span><span>(</span><span>)</span><span>,</span>
  <span>&amp;</span>writer<span>.</span>interface<span>,</span>
  <span>.</span><span>{</span><span>}</span><span>,</span> <span>// options TODO</span>
<span>)</span><span>;</span></code></pre>

<p>Note that <code>stream.writer()</code> returns a <code>Stream.Writer</code> and <code>stream.reader()</code> returns a <code>Stream.Reader</code> - those aren't the types our <code>tls.Client</code> expects. To convert the <code>Stream.Reader</code> to an <code>*std.Io.Reader</code>, we need to call its <code>interface()</code> method. To get a <code>*std.io.Writer</code> from an <code>Stream.Writer</code>, we need the address of its <code>&amp;interface</code> field. This doesn't seem particularly consistent. Don't forget that the <code>writer</code> and <code>reader</code> need a stable address. Because I'm trying to get the simplest example working, this isn't an issue - everything will live on the stack of <code>main</code>. In a real word example, I think it means that I'll always have to wrap the <code>tls.Client</code> into my own heap-allocated type; giving the writer and reader have a cozy stable home.</p>

<p>Speaking of allocations, you might have noticed that <code>stream.writer</code> and <code>stream.reader</code> take a parameter. It's the buffer they should use. Buffering is a first class citizen of the new Io interface - who needs composition? The documentation <strong>does</strong> tell me these need to be at least <code>std.crypto.tls.max_ciphertext_record_len</code> large, so we need to fix things a bit:</p>

<pre><code><span>var</span> write_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
<span>var</span> writer <span>=</span> stream<span>.</span><span>writer</span><span>(</span><span>&amp;</span>write_buf<span>)</span><span>;</span>

<span>var</span> read_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
<span>var</span> reader <span>=</span> stream<span>.</span><span>reader</span><span>(</span><span>&amp;</span>read_buf<span>)</span><span>;</span></code></pre>

<p>Here's where the code stands: </p>

<pre><code><span>const</span> std <span>=</span> <span>@import</span><span>(</span><span>"std"</span><span>)</span><span>;</span>

<span>pub</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> <span><span>!</span><span>void</span></span> <span>{</span>
  <span>var</span> gpa<span>:</span> std<span>.</span>heap<span>.</span><span>DebugAllocator</span><span>(</span><span>.</span><span>{</span><span>}</span><span>)</span> <span>=</span> <span>.</span>init<span>;</span>
  <span>const</span> allocator <span>=</span> gpa<span>.</span><span>allocator</span><span>(</span><span>)</span><span>;</span>

  <span>const</span> stream <span>=</span> <span>try</span> std<span>.</span>net<span>.</span><span>tcpConnectToHost</span><span>(</span>allocator<span>,</span> <span>"www.openmymind.net"</span><span>,</span> <span>443</span><span>)</span><span>;</span>
  <span>defer</span> stream<span>.</span><span>close</span><span>(</span><span>)</span><span>;</span>

  <span>var</span> write_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> writer <span>=</span> stream<span>.</span><span>writer</span><span>(</span><span>&amp;</span>write_buf<span>)</span><span>;</span>

  <span>var</span> read_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> reader <span>=</span> stream<span>.</span><span>reader</span><span>(</span><span>&amp;</span>read_buf<span>)</span><span>;</span>

  <span>var</span> tls_client <span>=</span> <span>try</span> std<span>.</span>crypto<span>.</span>tls<span>.</span>Client<span>.</span><span>init</span><span>(</span>
      reader<span>.</span><span>interface</span><span>(</span><span>)</span><span>,</span>
      <span>&amp;</span>writer<span>.</span>interface<span>,</span>
      <span>.</span><span>{</span>
      <span>}</span><span>,</span>
  <span>)</span><span>;</span>
  <span>defer</span> tls_client<span>.</span><span>end</span><span>(</span><span>)</span> <span>catch</span> <span>{</span><span>}</span><span>;</span>
<span>}</span></code></pre>

<p>But if you try to run it, you'll get a compilation error. Turns out we have to provide 4 options: the ca_bundle, a host, a <code>write_buffer</code> and a <code>read_buffer</code>. Normally I'd expect the options parameter to be for optional parameters, I don't understand why some parameters (input and output) are passed one way while <code>writer_buffer</code> and <code>read_buffer</code> are passed another.</p>

<p>Let's give it what it wants AND send some data:</p>

<pre><code><span>// existing setup...</span>

<span>var</span> bundle <span>=</span> <span>std<span>.</span>crypto<span>.</span>Certificate<span>.</span>Bundle</span><span>{</span><span>}</span><span>;</span>
<span>try</span> bundle<span>.</span><span>rescan</span><span>(</span>allocator<span>)</span><span>;</span>
<span>defer</span> bundle<span>.</span><span>deinit</span><span>(</span>allocator<span>)</span><span>;</span>

<span>var</span> tls_client <span>=</span> <span>try</span> std<span>.</span>crypto<span>.</span>tls<span>.</span>Client<span>.</span><span>init</span><span>(</span>
  reader<span>.</span><span>interface</span><span>(</span><span>)</span><span>,</span>
  <span>&amp;</span>writer<span>.</span>interface<span>,</span>
  <span>.</span><span>{</span>
    <span>.</span>ca <span>=</span> <span>.</span><span>{</span><span>.</span>bundle <span>=</span> bundle<span>}</span><span>,</span>
    <span>.</span>host <span>=</span> <span>.</span><span>{</span> <span>.</span>explicit <span>=</span> <span>"www.openmymind.net"</span> <span>}</span> <span>,</span>
    <span>.</span>read_buffer <span>=</span> <span>&amp;</span><span>.</span><span>{</span><span>}</span><span>,</span>
    <span>.</span>write_buffer <span>=</span> <span>&amp;</span><span>.</span><span>{</span><span>}</span><span>,</span>
  <span>}</span><span>,</span>
<span>)</span><span>;</span>
<span>defer</span> tls_client<span>.</span><span>end</span><span>(</span><span>)</span> <span>catch</span> <span>{</span><span>}</span><span>;</span>

<span>try</span> tls_client<span>.</span>writer<span>.</span><span>writeAll</span><span>(</span><span>"GET / HTTP/1.1\r\n\r\n"</span><span>)</span><span>;</span></code></pre>

<p>Now, if I try to run it, the program just hangs. I don't know what <code>write_buffer</code> is, but I know Zig now loves buffers, so let's try to give it something:</p>

<pre><code><span>// existing setup...</span>

<span>// I don't know what size this should/has to be??</span>
<span>var</span> write_buf2<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>

<span>var</span> tls_client <span>=</span> <span>try</span> std<span>.</span>crypto<span>.</span>tls<span>.</span>Client<span>.</span><span>init</span><span>(</span>
  reader<span>.</span><span>interface</span><span>(</span><span>)</span><span>,</span>
  <span>&amp;</span>writer<span>.</span>interface<span>,</span>
  <span>.</span><span>{</span>
    <span>.</span>ca <span>=</span> <span>.</span><span>{</span><span>.</span>bundle <span>=</span> bundle<span>}</span><span>,</span>
    <span>.</span>host <span>=</span> <span>.</span><span>{</span> <span>.</span>explicit <span>=</span> <span>"www.openmymind.net"</span> <span>}</span> <span>,</span>
    <span>.</span>read_buffer <span>=</span> <span>&amp;</span><span>.</span><span>{</span><span>}</span><span>,</span>
    <span>.</span>write_buffer <span>=</span> <span>&amp;</span>write_buf2<span>,</span>
  <span>}</span><span>,</span>
<span>)</span><span>;</span>
<span>defer</span> tls_client<span>.</span><span>end</span><span>(</span><span>)</span> <span>catch</span> <span>{</span><span>}</span><span>;</span>

<span>try</span> tls_client<span>.</span>writer<span>.</span><span>writeAll</span><span>(</span><span>"GET / HTTP/1.1\r\n\r\n"</span><span>)</span><span>;</span></code></pre>


<p>Great, now the code doesn't hang, all we need to do is read the response. <code>tls.Client</code> exposes a <code>reader: *std.Io.Reader</code> field which is "Decrypted stream from the server to the client." That sounds like what we want, but believe it or not <code>std.Io.Reader</code> doesn't have a <code>read</code> method. It has a <code>peak</code> a <code>takeByteSigned</code>, a <code>readSliceShort</code> (which seems close, but it blocks until the provided buffer is full), a <code>peekArray</code> and a lot more, but nothing like the <code>read</code> I'd expect. The closest I can find, which I think does what I want, is to stream it to a writer:</p>

<pre><code><span>var</span> buf<span>:</span> <span><span>[</span><span>1024</span><span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
<span>var</span> w<span>:</span> <span>std<span>.</span>Io<span>.</span>Writer</span> <span>=</span> <span>.</span><span>fixed</span><span>(</span><span>&amp;</span>buf<span>)</span><span>;</span>
<span>const</span> n <span>=</span> <span>try</span> tls_client<span>.</span>reader<span>.</span><span>stream</span><span>(</span><span>&amp;</span>w<span>,</span> <span>.</span><span>limited</span><span>(</span>buf<span>.</span>len<span>)</span><span>)</span><span>;</span>
std<span>.</span>debug<span>.</span><span>print</span><span>(</span><span>"read: {d} - {s}\n"</span><span>,</span> <span>.</span><span>{</span>n<span>,</span> buf<span>[</span><span>0</span><span>..</span>n<span>]</span><span>}</span><span>)</span><span>;</span></code></pre>

<p>If we try to run the code now, it crashes. We've apparently failed an assertion regarding the length of a buffer. So it seems like we also <em>have</em> to provide a <code>read_buffer</code>.</p>

<p>Here's my current version (it doesn't work, but it doesn't crash!):</p>

<pre><code><span>const</span> std <span>=</span> <span>@import</span><span>(</span><span>"std"</span><span>)</span><span>;</span>

<span>pub</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> <span><span>!</span><span>void</span></span> <span>{</span>
  <span>var</span> gpa<span>:</span> std<span>.</span>heap<span>.</span><span>DebugAllocator</span><span>(</span><span>.</span><span>{</span><span>}</span><span>)</span> <span>=</span> <span>.</span>init<span>;</span>
  <span>const</span> allocator <span>=</span> gpa<span>.</span><span>allocator</span><span>(</span><span>)</span><span>;</span>

  <span>const</span> stream <span>=</span> <span>try</span> std<span>.</span>net<span>.</span><span>tcpConnectToHost</span><span>(</span>allocator<span>,</span> <span>"www.openmymind.net"</span><span>,</span> <span>443</span><span>)</span><span>;</span>
  <span>defer</span> stream<span>.</span><span>close</span><span>(</span><span>)</span><span>;</span>

  <span>var</span> write_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> writer <span>=</span> stream<span>.</span><span>writer</span><span>(</span><span>&amp;</span>write_buf<span>)</span><span>;</span>

  <span>var</span> read_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> reader <span>=</span> stream<span>.</span><span>reader</span><span>(</span><span>&amp;</span>read_buf<span>)</span><span>;</span>

  <span>var</span> bundle <span>=</span> <span>std<span>.</span>crypto<span>.</span>Certificate<span>.</span>Bundle</span><span>{</span><span>}</span><span>;</span>
  <span>try</span> bundle<span>.</span><span>rescan</span><span>(</span>allocator<span>)</span><span>;</span>
  <span>defer</span> bundle<span>.</span><span>deinit</span><span>(</span>allocator<span>)</span><span>;</span>

  <span>var</span> write_buf2<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> read_buf2<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>

  <span>var</span> tls_client <span>=</span> <span>try</span> std<span>.</span>crypto<span>.</span>tls<span>.</span>Client<span>.</span><span>init</span><span>(</span>
      reader<span>.</span><span>interface</span><span>(</span><span>)</span><span>,</span>
      <span>&amp;</span>writer<span>.</span>interface<span>,</span>
      <span>.</span><span>{</span>
        <span>.</span>ca <span>=</span> <span>.</span><span>{</span><span>.</span>bundle <span>=</span> bundle<span>}</span><span>,</span>
        <span>.</span>host <span>=</span> <span>.</span><span>{</span> <span>.</span>explicit <span>=</span> <span>"www.openmymind.net"</span> <span>}</span> <span>,</span>
        <span>.</span>read_buffer <span>=</span> <span>&amp;</span>read_buf2<span>,</span>
        <span>.</span>write_buffer <span>=</span> <span>&amp;</span>write_buf2<span>,</span>
      <span>}</span><span>,</span>
  <span>)</span><span>;</span>
  <span>defer</span> tls_client<span>.</span><span>end</span><span>(</span><span>)</span> <span>catch</span> <span>{</span><span>}</span><span>;</span>

  <span>try</span> tls_client<span>.</span>writer<span>.</span><span>writeAll</span><span>(</span><span>"GET / HTTP/1.1\r\n\r\n"</span><span>)</span><span>;</span>

  <span>var</span> buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> w<span>:</span> <span>std<span>.</span>Io<span>.</span>Writer</span> <span>=</span> <span>.</span><span>fixed</span><span>(</span><span>&amp;</span>buf<span>)</span><span>;</span>
  <span>const</span> n <span>=</span> <span>try</span> tls_client<span>.</span>reader<span>.</span><span>stream</span><span>(</span><span>&amp;</span>w<span>,</span> <span>.</span><span>limited</span><span>(</span>buf<span>.</span>len<span>)</span><span>)</span><span>;</span>
  std<span>.</span>debug<span>.</span><span>print</span><span>(</span><span>"read: {d} - {s}\n"</span><span>,</span> <span>.</span><span>{</span>n<span>,</span> buf<span>[</span><span>0</span><span>..</span>n<span>]</span><span>}</span><span>)</span><span>;</span>
<span>}</span></code></pre>

<p>When I looked through Zig's source code, there's <a href="https://github.com/ziglang/zig/blob/306176046e6ae5e30bc58e5f3bcf786159e367f2/lib/std/http/Client.zig#L329">only one place</a> using <code>tls.Client</code>. It helped to get me where where I am. I couldn't find any tests.</p>

<p>I'll admit that during this migration, I've missed some basic things. For example, someone had to help me find <code>std.fmt.printInt</code> - the renamed version of <code>std.fmt.formatIntBuf</code>. Maybe there's a helper like: <code>tls.Client.init(allocator, stream)</code> somewhere. And maybe it makes sense that we do <code>reader.interface()</code> but <code>&amp;writer.interface</code> - I'm reminded of Go's <code>*http.Request</code> and <code>http.ResponseWrite</code>. And maybe Zig has some consistent rule for what parameters belong in options. And I know nothing about TLS, so maybe it makes complete sense to need 4 buffers. I feel a bit more confident about the weirdness of not having a <code>read(buf: []u8) !usize</code> function on <code>Reader</code>, but at this point I wouldn't bet on me.</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The ROI of Exercise (138 pts)]]></title>
            <link>https://herman.bearblog.dev/exercise/</link>
            <guid>44993692</guid>
            <pubDate>Sat, 23 Aug 2025 06:19:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://herman.bearblog.dev/exercise/">https://herman.bearblog.dev/exercise/</a>, See on <a href="https://news.ycombinator.com/item?id=44993692">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-08-22T07:45Z">
                    22 Aug, 2025
                </time>
            </i>
        </p>
    

    <p>I workout 4 days a week and I love it. It's the foundation of my morning routine, following spending 45 minutes drinking coffee on the couch and watching the sun come up with Emma.</p>
<p>I've been doing this for a few years now and while I struggled (as everyone does) in the beginning, I can't imagine not exercising in the morning now. On the rare occasion that I do skip a workout, I feel it missing throughout the day as a lack of vitality and less mental clarity.</p>
<p>Let's perform a thought experiment to work out the return on investment of exercise. For this let's first assume that exercise does nothing else but expand your lifespan (not extend; since it's not just adding frail years to the end but instead injects extra years in each stage of life). We can ignore the effects it has on strength, focus, feelings of accomplishment, and mental health for now.</p>
<p>It's well understood that a good exercise routine is a mixture of strength, mobility, and cardio; and is performed at a decent intensity for 2-4 days a week for at least 45 minutes. This could be a combination of weight lifting, yoga, running, tennis, hiking, or whatever floats your boat.</p>
<p>This totals about 3 hours a week, or 156 hours per year. If we extrapolate that over an adult lifetime, that's about 8,500 hours of exercise, or about a year of solid physical activity.</p>
<p>That sounds like a lot! But when put into the context of life expansion, it's actually an incredibly good deal. There are many studies detailing how any physical activity, from an easy walk all the way up to vigorous exercise a few times a week increases expected lifespan by 3 to 10 years. And none of these studies used lifetime exercisers, just people who exercised regularly in the last 10-ish years.</p>
<p>This makes sense, since 80 years ago we were still fighting the second world war, and jogging only entered the mainstream in the 70s. Weightlifting was an even later bloomer, and only becoming cool in the 90s!</p>
<p>I speculate that a lifetime exerciser with a modern approach to physical activity would have an even longer health and lifespan than any of these studies suggest. But for this writeup I want to stick with conservative estimates and not speculate too much.</p>
<p>We know from <a href="https://pubmed.ncbi.nlm.nih.gov/30193744/" target="_blank">one study</a> that people who played tennis a few times per week lived roughly 10 years longer than average. So we'll use that value going forward.</p>
<p>That means that over a lifetime, one full year of exercise leads to 10 full years of extra life. That's a 1:10 return on investment! So even without any of the additional benefits (which I'll get into later), this is still one of the best investments you can make.</p>
<p>Yes, this is an oversimplification. Correlation between exercise and longevity doesn’t imply causation. Confounding factors like diet, socioeconomic status, and healthcare access influence lifespan. Attributing 10 years solely to exercise ignores these; but it does play a significant factor, as many well-controlled studies will attest to.</p>
<p>This is also based on the premise that all of the time spent exercising is "wasted", which is hardly the case. People love running, playing padel with friends, lifting heavy things, and hiking. I love being in the gym, working towards mini-goals, making progress, and interacting with the community around me. This is not time wasted. I'll posit for many people it's the best part of their day. Not only that but it leaves you feeling accomplished, wholesome, and <a href="https://www.bmj.com/content/384/bmj-2023-075847" target="_blank">less depressed and anxious</a>.</p>
<p>To end off I'll rattle off a few other things exercise is good for:</p>
<ul>
<li>Better sleep</li>
<li>Less frailty in old age</li>
<li>More strength</li>
<li>Able to take part in more fun activities (like long hikes)</li>
<li>Being more attractive (subjectively, of course)</li>
<li>Improved self perception</li>
<li>Better cognitive function and memory</li>
<li>Access to communities</li>
<li>Less pain</li>
<li>More mobility</li>
<li>A stronger immune system</li>
</ul>
<p>And this is injected into every single part of your life and available in every decade. Not just at the end.</p>
<p>And this is inherently doable. This is the time equivalent of one episode of any Netflix show, 4 times a week. I watched 3 episodes of Pantheon on Monday alone!</p>
<p>So go do the thing. Incrementally at first. Start off slow and build up a practice that feels right. You won't regret it.</p>


    

    
        

        
            


        
    


  </div></div>]]></description>
        </item>
    </channel>
</rss>