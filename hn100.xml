<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 03 Jul 2023 17:00:11 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Who is hiring? (July 2023) (174 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36573871</link>
            <guid>36573871</guid>
            <pubDate>Mon, 03 Jul 2023 15:00:52 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36573871">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="36573871">
      <td><span></span></td>      <td><center><a id="up_36573871" href="https://news.ycombinator.com/vote?id=36573871&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=36573871">Ask HN: Who is hiring? (July 2023)</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_36573871">175 points</span> by <a href="https://news.ycombinator.com/user?id=whoishiring">whoishiring</a> <span title="2023-07-03T15:00:52"><a href="https://news.ycombinator.com/item?id=36573871">1 hour ago</a></span> <span id="unv_36573871"></span> | <a href="https://news.ycombinator.com/hide?id=36573871&amp;goto=item%3Fid%3D36573871">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Who%20is%20hiring%3F%20(July%202023)&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=36573871&amp;auth=27820869a7938cc40b67d8da8daa04aaa254d150">favorite</a> | <a href="https://news.ycombinator.com/item?id=36573871">120&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>Please state the location and include REMOTE, INTERNS and/or VISA
when that sort of candidate is welcome. When remote work is <i>not</i> an option,
include ONSITE.</p><p>Please only post if you personally are part of the hiring company—no
recruiting firms or job boards. One post per company. If it isn't a household name,
explain what your company does.</p><p>Commenters: please don't reply to job posts to complain about
something. It's off topic here.</p><p>Readers: please only email if you are personally interested in the job.</p><p>Searchers: try <a href="https://hnhired.fly.dev/" rel="nofollow noreferrer">https://hnhired.fly.dev</a>, <a href="https://kennytilton.github.io/whoishiring/" rel="nofollow noreferrer">https://kennytilton.github.io/whoishiring/</a>,
<a href="https://hnjobs.emilburzo.com/" rel="nofollow noreferrer">https://hnjobs.emilburzo.com</a>, <a href="https://news.ycombinator.com/item?id=10313519">https://news.ycombinator.com/item?id=10313519</a>.</p><p>Don't miss these other fine threads:</p><p><i>Who wants to be hired?</i> <a href="https://news.ycombinator.com/item?id=36573869">https://news.ycombinator.com/item?id=36573869</a></p><p><i>Freelancer? Seeking freelancer?</i> <a href="https://news.ycombinator.com/item?id=36573870">https://news.ycombinator.com/item?id=36573870</a></p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table><table>
            <tbody><tr id="36575683"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575683" href="https://news.ycombinator.com/vote?id=36575683&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Core Digital | Hybrid | US-Los Angeles | Full-time | Frontend Engineer<p><a href="https://jobs.lever.co/coredigitalmedia/2b29f047-446f-49f3-8581-f7b3d4a600b4" rel="nofollow noreferrer">https://jobs.lever.co/coredigitalmedia/2b29f047-446f-49f3-85...</a></p><p>Core Digital Media brands empower people to improve their financial lives every day.  We are one of the largest advertisers on the internet and tens of millions of people come to our sites each month. 
We have helped many of them save thousands of dollars off of a variety of recurring expenses such as their mortgage, insurance premiums, credit card fees, etc. Our websites include LowerMyBills.com, itsHome.com, QuickenCompare.com, and Refily.com, and they provide our consumers with valuable content, tools, service providers and solutions as they navigate some of life's biggest financial decisions.</p><p>Core Digital Media is a proud member of the Rocket Companies family [“RKT” on NYSE] and works in close partnership with its sister companies Rocket Mortgage, Rocket Homes, Rocket Loans and Rocket Money.  These companies are all market leaders in their industries and are obsessed with helping their clients achieve the dream of home ownership and financial freedom.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575651"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575651" href="https://news.ycombinator.com/vote?id=36575651&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Superblocks // NYC // Full-Time // Series A<p>-- The Team--</p><p>Ex-Observability/DB experts building a programmable internal tooling platform for developers.</p><p>-- About Superblocks--</p><p>The all-in-one programmable platform for developers to build beautiful, secure, enterprise-grade internal tools fast: build any internal app, automate any workflow and schedule any job. Developers use the Superblocks development environment to bind their datasources and UI components together using drag and drop or code. Many programming languages are already supported, including Javascript, Python and SQL. Superblocks apps, workflows, or jobs are then deployed onto a custom URL with permissions, SSO, audit logging, and other enterprise functionality.</p><p>-- Resources --</p><p><a href="https://docs.superblocks.com/" rel="nofollow noreferrer">https://docs.superblocks.com/</a></p><p><a href="https://superblocks.com/careers" rel="nofollow noreferrer">https://superblocks.com/careers</a></p><p>-- Notes --</p><p>We're looking for senior frontend engineers. React expertise is a + but not required.</p><p>-- Shortcut --</p><p>Reach out to Careers@Superblockshq.com with your resume or LinkedIn mentioning you saw us on HN. Tell us why Superblocks looks interesting!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575506"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575506" href="https://news.ycombinator.com/vote?id=36575506&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Ramp | Full-Time | Front-End Engineer (All Levels) | Remote Anywhere (NYC offices) | $250k+ TC<p>Ramp is looking for talented front-end folks who live and breathe UX Our front-end stack is React, TypeScript, and a shiny component library.</p><p>Email your resume to nico@ramp.com.</p><p>See the job posting at [senior-frontend-job]</p><p>We're also hiring for other roles, see all openings at [all-jobs]. Particularly interested in Android (Kotlin), iOS (SwiftUI), and Backend (Python) engineers as well.</p><p>===</p><p>Ramp is building the world's first finance automation platform designed to save businesses time and money. Ramp offers 5-in-1 software that consolidates corporate cards, expense management, bill payments, accounting, and reporting into one simple and free solution. We're a young hypergrowth startup building top tier product in the B2B fintech space (3y old, $8.1B valuation, Series C, ~500 employees, ~100 engineers)</p><p>Cool breakdown of our business: [breakdown]</p><p>More recent article on the economy: [july-2022-update]</p><p>[senior-frontend-job]: <a href="https://jobs.ashbyhq.com/ramp/4f9dd90d-f32e-46d5-b0c0-c69272b2f8d8?utm_source=q6xpBD7lnE">https://jobs.ashbyhq.com/ramp/4f9dd90d-f32e-46d5-b0c0-c69272...</a></p><p>[all-jobs]: <a href="https://jobs.ashbyhq.com/ramp?utm_source=Pk8jzEeRLO">https://jobs.ashbyhq.com/ramp?utm_source=Pk8jzEeRLO</a></p><p>[breakdown]: <a href="https://www.notboring.co/p/ramping-up" rel="nofollow noreferrer">https://www.notboring.co/p/ramping-up</a></p><p>[july-2022-update]: <a href="https://ramp.com/blog/ramp-july-2022-update" rel="nofollow noreferrer">https://ramp.com/blog/ramp-july-2022-update</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575487"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575487" href="https://news.ycombinator.com/vote?id=36575487&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Weekend Health | Hybrid (M/W/Th) in San Francisco | Full-time | $160-200k base<p>Weekend Health's mission is to make healthcare more accessible, especially for those with chronic diseases like obesity, diabetes, and heart disease. After launching in January 2022, we grew from 0 to $20m ARR by the end of the year, got acquired by WeightWatchers in April, and are now looking to scale our eng team of 8.</p><p>We're looking for motivated full-stack and mobile engineers who are passionate about using their eng skills to make a positive impact in the world. You'd be building software that allows patients to get insurance to cover medications that would otherwise be prohibitively expensive while working with a tight-knit team of serial entrepreneurs and all-star performers.</p><p>Our stack: TypeScript | React | React Native | Node | AWS</p><p>More details about the roles here: <a href="https://www.weekendhealth.com/careers" rel="nofollow noreferrer">https://www.weekendhealth.com/careers</a></p><p>If you're interested, please reach out to me (and mention HN!) at calvin.young@weekendhealth.com.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575535"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575535" href="https://news.ycombinator.com/vote?id=36575535&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Bernard Health | Software QA Analyst | ONSITE | Nashville, TN | Full-time | <a href="https://jobs.bernieportal.com/afa31732" rel="nofollow noreferrer">https://jobs.bernieportal.com/afa31732</a><p>Company Overview:</p><p>HR is hard. Nobody starts a company because they passionately want to run payroll and handle benefit administration for their employees.</p><p>We provide software to make the “running a business” side of things easier, and in the process we get to help thousands of people daily.</p><p>To make sure that we continue to help these people the best that we can, we’re looking for a QA Analyst to join the team and make sure we’re doing it right.</p><p>Job Overview:</p><p>As a a QA Analyst, you’re in a unique position. Your role is the only manual-automation hybrid in the company, and you interface between the product/development teams and the support teams. You replicate and write up unexpected behaviors within the application, manually verify our weekly releases before and after deployment (on Staging, and Production respectively), and you write automated tests to reduce future manual test cases.</p><p>If you noticed the grammatical error in this section, you’re probably the kind of person we’re looking for — let’s talk.</p><p>Read more details and apply at <a href="https://jobs.bernieportal.com/afa31732" rel="nofollow noreferrer">https://jobs.bernieportal.com/afa31732</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575678"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575678" href="https://news.ycombinator.com/vote?id=36575678&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Sirch.<p>The search engine of the future. Tired of 23938384 results in 0.00003 seconds to sift through each time you query?</p><p>Sirch gives you what you need. No sifting. How?
AI + Democracy</p><p>We have an MVP that folks seem to like so far, and a small team. Most of us are a bit weird. Fully remote. Hiring: AI engineer + Consumer app growth expert. Joining us is very high risk/reward.</p><p>josh@sirch.org
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575467"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575467" href="https://news.ycombinator.com/vote?id=36575467&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Packfleet (<a href="https://packfleet.com/" rel="nofollow noreferrer">https://packfleet.com/</a>) | Senior Engineer (Routing) | Full time, £90-110k + meaningful equity | Hybrid, Onsite | London, UK | VISA<p>Packfleet is a fast growing seed-stage delivery startup based in London, founded by early employees of Monzo. We're out to make fully electric next-day deliveries the new normal, while improving every aspect of the delivery experience using modern tech. We're looking for a Senior engineer with routing or optimization experience, to join our small team and help build our in-house vehicle router in Rust. You would be the first engineer focused exclusively on routing and get to own and develop a core piece of tech in the company.</p><p>Some of the most interesting problems we’re working on at the moment are:</p><p>- Using a 3D distance matrix to generate routes that are time-of-day accurate.</p><p>- Taking into account the charge levels of our 100% electric fleet.</p><p>- Scaling time-windowed deliveries to thousands per day while maintaining efficiency.</p><p>- Incorporating on-demand same-day deliveries and address changes into our routes.</p><p>- Simulating demand and scale so we’re always one step ahead of our growth.</p><p>Apply at <a href="https://apply.workable.com/packfleet/j/4F21C14939/" rel="nofollow noreferrer">https://apply.workable.com/packfleet/j/4F21C14939/</a>, or e-mail robin@packfleet.com.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575526"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575526" href="https://news.ycombinator.com/vote?id=36575526&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Screenly (<a href="https://screenly.io/" rel="nofollow noreferrer">https://screenly.io</a>) | Full Time | Front-end Dev/Dev Rel | Remote<p>Screenly is a popular digital signage platform (industry jargon for display management). Think screens on walls with advertisements or dashboards. We’re the most developer friendly digital signage platform, and you will be in the center of this, joining us working on our new Edge Apps.</p><p>As an Edge App, you will work closely with our graphical design team to translate UI/UX design into code. You will prominently be working on our open source example apps in our on GitHub, but also on some custom work to solve customer specific problems.</p><p><a href="https://apply.workable.com/screenly/j/1002C12C10/" rel="nofollow noreferrer">https://apply.workable.com/screenly/j/1002C12C10/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575546"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575546" href="https://news.ycombinator.com/vote?id=36575546&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Resemble AI | San Francisco Bay Area or Toronto, ON | Full-Time | Full-Stack Engineer, Deep Learning Researcher, Frontend Engineer, Product Designer<p>Resemble AI creates high-quality synthetic voices that capture human emotion. We're a venture-backed high-growth startup that's looking to shake up an entire industry with state of the art AI. Our product changes the way that thousands of brands, media companies, creative agencies, and game studios create speech content. We’re a remote-first team that thrives on flexibility and creativeness. We cover expenses for office space, equipment, and all of the other perks and benefits that make you productive. We also believe that to build an enticing product and solid team is by encouraging innovation is by enabling continuous education. That's why every Friday is a day that you can use to work on anything you want, Resemble-related or not. We're hiring for four roles:</p><p>Full Stack Engineer - Can you break the entire stack? You're the right person for this job. Work on our Rails app, with sprinkles of React, and Python for the deep learning. Everything is dockerized, and we use Kubernetes to deploy.</p><p>Deep Learning Researcher - Come build large ML models for voice synthesis! We already have the SoTA Speech Synthesis models, so this role is all about thinking outside of the box and running huge experiments on tons of compute. Experience in deep speech fields like TTS and ASR is desirable alongside proficiency in Python and DL/ML frameworks like PyTorch, TensorFlow, and Keras.</p><p>Frontend Engineer - We're hiring a Frontend Engineer proficient in React, TypeScript, and Ruby on Rails to shape our user experience. Join our team to develop user-friendly interfaces and collaborate on building exceptional web experiences.</p><p>Product Designer - As a Product Designer, you will lead the end-to-end design process, from concept to implementation, ensuring a seamless and delightful user experience. You will collaborate with cross-functional teams to define product vision, conduct user research, create visually compelling interfaces, and develop interactive prototypes.</p><p>If interested, reach out directly to me: zohaib [at] resemble.ai
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574033"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574033" href="https://news.ycombinator.com/vote?id=36574033&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Quatt.io | Amsterdam, Netherlands | Full-time | Hybrid/ONSITE | <a href="https://quatt.io/" rel="nofollow noreferrer">https://quatt.io</a> | climate tech<p>I'm head of Software at Quatt, a quickly growing startup building hybrid heatpumps to help fix climate change. Heatpumps have a 10x higher return on investment for CO2 saved per Euro invested than electric cars, and we're building the most accessible and smartest heatpump on the market. Our product is live, being installed daily, and I really like the impact we're having.</p><p>I'm looking for full stack app developers (Node/typescript + React Native) and a Cloud Architect (AWS) for my team, as we believe having the best software will allow us to have the best product.</p><p>Now is a great time to join, as the software team is still small but growing quickly
Other vacancies are on our careers page: <a href="https://quatt.recruitee.com/" rel="nofollow noreferrer">https://quatt.recruitee.com</a>.
Email me directly ( hacker news username@quatt.io ) for questions.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36575550"><td></td></tr>
                  <tr id="36573933"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36573933" href="https://news.ycombinator.com/vote?id=36573933&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Apple, Inc. | Austin |  London | Paris | Prague | Santa Clara Valley | Seattle | Full-time | Security Software Engineering, Research, Analysis, Operations<p>We’re perfectionists. Idealists. Inventors. Forever tinkering with products and processes, always on the lookout for better. Whether you work at one of our global offices, offsite, or even at home, a job at Apple will be demanding. But it also rewards bright, original thinking and hard work. And none of us here would have it any other way.</p><p>Where do you see yourself at Apple?</p><p>Security Engineering &amp; Architecture is looking for engineers, researchers, and operations professionals who can help make sense of complex hardware and software systems and contribute to the development of the most secure devices on the planet. We have a few open roles, related to secret storage, core operating system internals, secure infrastructure and endpoints, vulnerability research, and operations/program management. We are looking for individuals that can build scalable distributed services, analyze highly-complex threats, identify vulnerabilities, design pragmatic processes, and evolve operating system security mechanisms across our platforms. Ideal candidates will demonstrate clear communication, consistent follow-through, and the capacity to collaborate globally. These roles are a unique opportunity to be part of the team that architects and delivers groundbreaking security to more than a billion devices.</p><p>See our open roles and send a résumé to sear-recruiting@group.apple.com with “[HN] &lt;role of interest&gt;” in the email subject if interested: <a href="https://jobs.apple.com/en-us/search?search=SEAR2023" rel="nofollow noreferrer">https://jobs.apple.com/en-us/search?search=SEAR2023</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36574418"><td></td></tr>
                  <tr id="36575096"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575096" href="https://news.ycombinator.com/vote?id=36575096&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>SpruceID (YC W21) | Full REMOTE | Multiple Roles | Full-Time | spruceid.com<p>Spruce lets users control their data across the web. We are reimagining trusted interactions by creating the world’s best open source software for packaging beliefs digitally, while innovating on global standards in identity.</p><p>We hire programmers who love technology and are committed to intellectual honesty, user privacy, and innovation. Our products are composed from a combination of industry-trusted frameworks, applied cryptography, new interoperable identity standards (W3C, ISO, IETF, and OpenID), and custom backend libraries.</p><p>Select roles:</p><p>Technical Success Manager: Work with external customers and stakeholders, maintaining implementation roadmaps, collaborating on product roadmaps, and documenting user stories, timelines, and implementation strategy at the customer level.</p><p>Software Engineer, SRE: Implement reliable and performant features in Rust that is cross-compiled to servers, mobile devices, and WASM.</p><p>Software Engineer, Android: Build software prototypes and product features from start to finish for Android and iOS, embedding our Rust core.</p><p>See all roles here: <a href="https://jobs.lever.co/sprucesystems/" rel="nofollow noreferrer">https://jobs.lever.co/sprucesystems/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575459"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575459" href="https://news.ycombinator.com/vote?id=36575459&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Loop | loop.com | Product/Design/Engineering | On-site San Francisco, CA, Chicago, IL | H1B OK<p>We build connected finance for logistics. We want to make the billing and payment process as smooth as consumers paying for online goods while unlocking hundreds of billions of working capital for businesses. Loop can help trillions of dollars move more efficiently and improve millions of people’s livelihoods.</p><p>1. Raised $65m from JPM GEP, Founders Fund, 8VC, Susa Ventures, Flexport, Index, and Expa.</p><p>2. 35 paying enterprise customers with multiple-year contracts; 60+ customers in the pipeline.</p><p>3. High-caliber team of engineers from Google, Scale AI, Flexport, Uber, Bolt, Rakuten, Square, Meta, Stanford and Yale.</p><p>4. 9+ years cash runway</p><p>Product Manager, Payment - <a href="https://boards.greenhouse.io/loop/jobs/4826377004" rel="nofollow noreferrer">https://boards.greenhouse.io/loop/jobs/4826377004</a></p><p>Brand Designer - <a href="https://boards.greenhouse.io/loop/jobs/4866545004" rel="nofollow noreferrer">https://boards.greenhouse.io/loop/jobs/4866545004</a></p><p>Product Designer - <a href="https://boards.greenhouse.io/loop/jobs/4126037004" rel="nofollow noreferrer">https://boards.greenhouse.io/loop/jobs/4126037004</a></p><p>Fullstack Engineer San Francisco - <a href="https://boards.greenhouse.io/loop/jobs/4102236004" rel="nofollow noreferrer">https://boards.greenhouse.io/loop/jobs/4102236004</a></p><p>Fullstack Engineer Chicago - <a href="https://boards.greenhouse.io/loop/jobs/4830548004" rel="nofollow noreferrer">https://boards.greenhouse.io/loop/jobs/4830548004</a></p><p>If you do not see your role list here but are excited about what Loop is building, send a note to founders at loop.com.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575408"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575408" href="https://news.ycombinator.com/vote?id=36575408&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>AWS | SoC Driver Developer | Austin, TX or Cupertino, CA | Full-time<p>I'm an engineer in Annapurna Labs, the small company-within-a-company that designs AWS custom silicon.  My team designs and runs the Trainium and Inferentia machine learning (ML) accelerators.  We're looking for an engineer to help maintain the driver stack for new and existing chips.</p><p>This is definitely a system software-oriented role; no ML experience is needed (actually, we're really looking for people who have strong opinions about system software).  But if you're interested in ML, you'll be adjacent to people who have tons of experience with high performance compute, chip design, and ML algorithms.</p><p>Ping me if you're wanting to chat about the team or the role: ghilliar -at- amazon.com.  I'm an embedded systems engineer and I don't bite :)</p><p>And here's the job description link: <a href="https://www.amazon.jobs/en/jobs/2393159" rel="nofollow noreferrer">https://www.amazon.jobs/en/jobs/2393159</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574936"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574936" href="https://news.ycombinator.com/vote?id=36574936&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>OpenRent | London, UK | Full-Time | ONSITE+PART REMOTE | <a href="https://www.openrent.co.uk/" rel="nofollow noreferrer">https://www.openrent.co.uk</a><p>What sucked the last time you rented a house or flat? Come and fix it.</p><p>OpenRent is a force for good in an industry tarnished by rip-off agencies. Enabled by an unrelenting focus on technology, we now let more properties than any agency in the UK. In the last 12m we let over £45 billion worth of property, to over 5 million registered users, without ever charging any admin fees.</p><p>You'll be working on things that vary from machine learning models to predict the right price of a property, to the future of property management, all to help tenants find their dream home, and landlords their ideal tenant.</p><p>We're VC backed, profitable, and have plenty of ambition to maintain our fast growth in this absolutely massive market.</p><p>We have a bunch of roles available at the moment and keen to find great people like you!</p><p>- Senior Web Designer | Equity | 75k-100k+ (based on experience) + Quarterly Bonus</p><p>- Full Stack Engineer (C#) | Equity | £45k-£120k+ (based on experience) + Quarterly Bonus</p><p>- Senior DevOps Engineer | Equity | 75k-100k+ (based on experience) + Quarterly Bonus</p><p>All roles visible here: <a href="https://www.openrent.co.uk/jobs" rel="nofollow noreferrer">https://www.openrent.co.uk/jobs</a></p><p>OpenRent is already impacting millions of tenants and landlords each year, join us and make every tenancy better than the last.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574001"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574001" href="https://news.ycombinator.com/vote?id=36574001&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>AI Research Residency Program (1 year) | Zug/Zurich area | Full time | On site | 100-200k CHF/annum<p>I am launching a new AI research organisation in the Zug/Zurich area. The goal of the organisation is to make a meaningful contribution towards solving AGI.</p><p>I am looking for employees number no. 1 and 2. This will ideally be someone with a history of interesting research and a clear agenda for future research. I am personally interested in LLM's, MuZero (<a href="https://arxiv.org/abs/1911.08265" rel="nofollow noreferrer">https://arxiv.org/abs/1911.08265</a>), Dreamer (<a href="https://arxiv.org/pdf/2301.04104v1.pdf" rel="nofollow noreferrer">https://arxiv.org/pdf/2301.04104v1.pdf</a>) and DreamCoder (<a href="https://arxiv.org/abs/2006.08381" rel="nofollow noreferrer">https://arxiv.org/abs/2006.08381</a>), and think there are some interesting systems to be found by interpolating between these existing areas (however, please still apply even if your research isn't in these areas).</p><p>The project will be financed by my current company which has made extensive use of deep learning for the past 4 years. You will have access to the &gt;100 top-end GPUs owned by this company.</p><p>The job will be structured as a research residency program in the first year, with the hope of transitioning successful candidates into full-time roles.</p><p>In your application, please include a high-level research proposal stating what you would work on.</p><p>To apply, email me at : not_a_cat at fastmail. co[m].
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575323"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575323" href="https://news.ycombinator.com/vote?id=36575323&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Orum | Staff Software Engineer | Remote (US and Canada only) | Full-time | <a href="https://orum.com/" rel="nofollow noreferrer">https://orum.com/</a><p>Orum efficiently connects sales teams to their target prospects, helping them sell to their ideal customers. Using Orum's parallel dialer, sales reps can dial multiple numbers at once, and Orum's speech recognition will automatically recognize and connect representatives with humans that pick up. This leads to consistent conversations, faster training, and more meetings booked.</p><p>Our current technology stack includes NodeJS with Typescript, Ruby, Postgres, Google Cloud Platform, and FreeSWITCH.</p><p>What we would love to see:</p><p>* 8+ years of experience as a software engineer</p><p>* 5+ years of experience as a mentor, tech lead, or leading an engineering team</p><p>* Minimum of 1+ years in an architect, principal, or staff engineer role</p><p>* Proven track record of owning products and features through their lifecycles - from discovery to deployment</p><p>* Strong familiarity with NodeJS, TypeScript, PostgreSQL, and React</p><p>* Experience measuring and tuning the performance of NodeJS applications</p><p>* Familiar with Unix shell scripting</p><p>More details here: <a href="https://www.orum.com/careers" rel="nofollow noreferrer">https://www.orum.com/careers</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575352"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575352" href="https://news.ycombinator.com/vote?id=36575352&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Ordergroove | Engineering Manager | REMOTE | FULL-TIME | <a href="https://www.ordergroove.com/jobs/?gh_jid=6653210002" rel="nofollow noreferrer">https://www.ordergroove.com/jobs/?gh_jid=6653210002</a><p>We are looking for an entrepreneurial Engineer Manager to take over the Frontend platform team at Ordergroove. If you love building great self-service Control Panels or Admin Panels, you will thrive at this role.</p><p>Ordergroove is a fast growing SaaS that enables DTC merchants to provide their products as a subscription. We are used by some of the top brands in the industry such as Dollar Shaving Club, P&amp;G, L'Oreal, KIND, and many more!</p><p>Feel free to apply here: <a href="https://www.ordergroove.com/jobs/?gh_jid=6653210002" rel="nofollow noreferrer">https://www.ordergroove.com/jobs/?gh_jid=6653210002</a></p><p>Or email me directly if you have any questions: luthfur.chowdhury@ordergroove.com
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575337"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575337" href="https://news.ycombinator.com/vote?id=36575337&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>ISRG / Let's Encrypt | General Counsel | Half Time | Fully Remote<p>If you’re an attorney looking for a flexible half-time position with great work/life balance and a mission that matters, this could be the job for you!</p><p>Internet Security Research Group (ISRG) has changed the Internet for nearly everyone using it. You’d be our next staff member (our 24th), joining a highly motivated and future-focused organization. We make the Internet more secure and privacy-respecting. Our largest project, Let’s Encrypt, helps more than 300 million websites protect their visitors. We operate as a 501(c)(3) nonprofit so we can stay focused on what’s best for the people using the Internet. This model lets us take on tough problems and we’ve built up the staff capability to do it.</p><p><a href="https://boards.greenhouse.io/isrg/jobs/4247229006" rel="nofollow noreferrer">https://boards.greenhouse.io/isrg/jobs/4247229006</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574927"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574927" href="https://news.ycombinator.com/vote?id=36574927&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Roboflow (<a href="https://roboflow.com/careers">https://roboflow.com/careers</a>) | Software Engineers, ML Research Engineer, Former Founders | Full-time | SF, NYC, Remote<p>We make tools that enable developers to make the world programmable. Over 250k developers, including those from half the Fortune 100, use our computer vision tools to improve datasets, models, and deployments. For example, Roboflow is Snap's partner for building custom vision models into AR lenses [1].</p><p>We have SF/NYC/distributed roles. Every team member also has an annual travel stipend [2] to spend coworking with others, anywhere.</p><p>[1] <a href="https://twitter.com/SnapAR/status/1671985144524165120/photo/1" rel="nofollow noreferrer">https://twitter.com/SnapAR/status/1671985144524165120/photo/...</a>
[2] <a href="https://blog.roboflow.com/remote-not-distant/">https://blog.roboflow.com/remote-not-distant/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575286"><td></td></tr>
            <tr id="36575332"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575332" href="https://news.ycombinator.com/vote?id=36575332&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Atlas | Senior Software Engineer (Product) | Full-time | SF, Remote<p>At Atlas, our goal is to accelerate financial empowerment. We’re focussed on over 100M Americans who are underserved by the current financial system. By making it easier to access credit and premium card rewards, we believe we can make the system work better for everyone. We see a massive opportunity for impact though better products. We're off to a strong start and have been growing over 100% a month since we launched 10 months ago.</p><p>We are a product and technology obsessed team with past experience at Microsoft, Amazon, Facebook, and Thumbtack. We’re growing quickly as a team and business. Join us and help us build a modern credit card product built around digitized income, asset and liability data.</p><p>Learn more here - <a href="https://www.atlasfin.com/careers/full-stack-software-engineer" rel="nofollow noreferrer">https://www.atlasfin.com/careers/full-stack-software-enginee...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575372"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575372" href="https://news.ycombinator.com/vote?id=36575372&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Brain Corporation | Multiple Roles | Full Time | Hybrid (Onsite as needed) | San Diego<p>Brain Corp is a team of innovators, surfers and, well,  brainiacs, that love bringing robots to life through pioneering AI software technology. We take great pride in helping our customers build smarter operations around autonomous robots that are safe, intelligent, and easy to use so they can help people in everyday jobs within environments such as retail stores, malls, airports, hospitals, and more.</p><p>We are hiring for multiple roles including a Senior Data Engineer and a Cloud Software Engineer.</p><p>To apply see <a href="https://braincorp.com/company/careers/" rel="nofollow noreferrer">https://braincorp.com/company/careers/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575272"><td></td></tr>
            <tr id="36575400"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575400" href="https://news.ycombinator.com/vote?id=36575400&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Cruise | Senior React Native Engineer | Full Time | San Francisco or US Remote | $166,600 - $245,100 + bonus + equity<p>Cruise's ridehail team is looking for a mobile expert to come in and help build out our iOS and Android app using React Native. Come help build the future of self driving cars.</p><p><a href="https://getcruise.com/careers/jobs/2537876/" rel="nofollow noreferrer">https://getcruise.com/careers/jobs/2537876/</a></p><p>Feel free to reach out at firstname.lastname@getcruise.com I'm one of the software devs on the team
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574549"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574549" href="https://news.ycombinator.com/vote?id=36574549&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Railway | engineering, design, dev rel | Remote | <a href="https://railway.app/careers" rel="nofollow noreferrer">https://railway.app/careers</a><p>Tired of trying to beat kube into shape? Does writing YAML to ship code fill you with utter dread? Dream of a future where deploying software is simple, and you don't need an army of infrastructure engineers to build that perfect janky bash script™ to make life easy?</p><p>We're Railway, and we think infrastructure can be better. So far we've built out a platform loved by hundreds of thousands of users who simply tell us "Give me Postgres", "Deploy this repo", and we make it happen</p><p>Fair warning! The problems are complex: home-rolled hypervisors, cut-above container orchestration, over/under/whateverlay networks, virtio device drivers, edge proxies, IAM that doesn't suck, kitchen sinks - we need to build it and we're looking for likeminded individuals who think this stuff is fun.</p><p>If that sounds like you, please apply at railway.app/careers. We have a number of roles, but are prioritizing the following four roles:</p><p>+Platform Engineer - General</p><p>+Developer Relations</p><p>+Support Engineer</p><p>+Product Designer</p><p>+Solutions Architect</p><p>See you soon, and happy shipping.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36575243"><td></td></tr>
                  <tr id="36575216"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575216" href="https://news.ycombinator.com/vote?id=36575216&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Vannevar Labs | REMOTE-FIRST | FULL-TIME | Offices in DC and NYC<p>Vannevar Labs brings innovative, robust, and scalable software design to the public servants working in defense and national security keeping our country safe. As a team, we exist because we believe in public service, and we think that our democracy and government improve only if we put serious, collective effort into improving them, including the technology our government uses. Our founders have 30 years of combined experience across national security, government sales and CIA, In-Q-Tel, DoD/JSOC, Harvard, and Stanford. We are a profitable Series B company and are looking to scale out the engineering team.</p><p>We're hiring across a number of engineering roles - feel free to apply on our website <a href="https://jobs.lever.co/vannevarlabs-2" rel="nofollow noreferrer">https://jobs.lever.co/vannevarlabs-2</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574064"><td></td></tr>
            <tr id="36575198"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575198" href="https://news.ycombinator.com/vote?id=36575198&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>PostHog | Remote (US/Europe timezones) | Full stack engineer, technical ex-founder, tech lead | <a href="https://posthog.com/">https://posthog.com</a><p>PostHog is the only open-source Product OS, combining product analytics, session recordings, feature flags, cdp and a data warehouse in one.</p><p>We have a culture of written async communication (see our handbook [0]), lots of individual responsibility and an opportunity to make a huge impact. Being fully remote means we're able to create a team that is truly diverse. We're based all over the world, and the team includes former YC founders, CTOs turned developers and recent grads.</p><p>To apply see <a href="https://posthog.com/careers">https://posthog.com/careers</a> or email us careers@posthog.com</p><p>[0] <a href="https://posthog.com/handbook/">https://posthog.com/handbook/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574337"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574337" href="https://news.ycombinator.com/vote?id=36574337&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Stream | Multiple Positions | Amsterdam, NL and Boulder, US | Remote possible | Full Time | Visa Sponsorship<p>Stream's Chat Messaging &amp; Activity Feed APIs are used by over a billion end-users, and are powered by Go, RocksDB, and Raft - with response times typically measured in single-digit milliseconds.
We reached a new industry benchmark for scaling real-time chat as-a-service, with 5 million concurrent connections supported in a single chat channel.
We’re currently one of the fastest growing startups in Europe and Colorado on our Series B round of $38 mln: <a href="https://tcrn.ch/3peCYXl" rel="nofollow noreferrer">https://tcrn.ch/3peCYXl</a></p><p>We’re hiring for the following positions:</p><p>* Backend Software Developer (Python/Django)</p><p>* Backend Software Developer (Go)</p><p>* Android SDK Team Lead (Jetpack Compose)</p><p>* React Native SDK Developer</p><p>As a part of Stream, you'll have a chance to make a huge impact on the product within a team of the strongest engineers all over the world (over 35 countries aboard).
If you are interested in becoming a part of what we do, apply now! <a href="https://getstream.io/team/#jobs" rel="nofollow noreferrer">https://getstream.io/team/#jobs</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575269"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575269" href="https://news.ycombinator.com/vote?id=36575269&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Slim.AI | CTO Innovation Office R&amp;D Engineer | Full-time | Remote or WA State<p>I'm the CTO/founder. I also created SlimToolkit (aka DockerSlim), which recently became a CNCF Sandbox project. If you have a background and passion for container internals, cloud native technology and security this might be an interesting role for you. I'm looking for an engineer (the exact title depends on your background/experience) to join the CTO Innovation Office team to work on advanced research projects and experiments. There'll be AI related experiments too :-)</p><p>If you think it's a good match for you and you want to learn more email me: cto@slim.ai
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575133"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575133" href="https://news.ycombinator.com/vote?id=36575133&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Luma AI (<a href="https://lumalabs.ai/join" rel="nofollow noreferrer">https://lumalabs.ai/join</a>) - ML and Backend Engineering, Bay Area | Full Time | Visa sponsored<p>We are making 3D creation into an art form that allows every artist to become a 3D artist. To this end, we are training large 3D generative models and working on ML graphics and neural rendering. Our research and product team: <a href="https://lumalabs.ai/team" rel="nofollow noreferrer">https://lumalabs.ai/team</a></p><p>ML engineering at Luma is about large-scale data, large model training pipelines, and working on and shipping incredible things to passionate and creative users. We have a modern tech stack, have raised 25m, and our team will help you grow and do rewarding and challenging work!</p><p>To apply → <a href="https://lumalabs.ai/join" rel="nofollow noreferrer">https://lumalabs.ai/join</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575354"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575354" href="https://news.ycombinator.com/vote?id=36575354&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Upwave | Senior Product Engineer | $150k-$175k + equity + benefits | Remote-first (US Timezones) | Full-time | <a href="https://www.upwave.com/careers" rel="nofollow noreferrer">https://www.upwave.com/careers</a><p>About Upwave:</p><p>Upwave is a YC/venture-funded SF-based startup that helps large enterprises plan, measure, and optimize brand advertising campaigns. We offer a comprehensive set of tools that help our Fortune 500 clients understand and improve how they’re perceived in the marketplace, how aware consumers are of the value they provide, and how much consumers trust them to provide the value they promise.
We've been rapidly expanding our list of enterprise customers and deepening our integrations with "household name" partners like Amazon and Clorox. As we grow, we need engineers to help us build even more unique, differentiated, valuable, scalable, and robust analytical SaaS tools and products.</p><p>About the Role:</p><p>Upwave is a startup and we need an experienced generalist “startup engineer” who’s excited about doing the wide variety of technical work that modern SaaS startups need. That includes backend web development (JVM frameworks), frontend SPA development (mostly React), AWS-based cloud engineering, and some data engineering. We are also particularly interested in someone who knows (or is interested in learning) the intricacies of our industry and our business and who is comfortable leading conversations with customers and partners to scope, define, and execute the technical integrations that have become one of our major levers of growth. We're not fixated on years of experience, but this role is unlikely to be right for someone with less than 10.
Our team emphasizes high-quality, high-velocity, sustainable software development in a collaborative and inclusive team environment. We’re a small team that gives our engineers a lot of autonomy and empowerment, and we want people who are excited to step in to learn whatever skills or technologies are needed, to contribute wherever in the R&amp;D process they're needed, and to grow their careers by taking on pivotal responsibilities early in their tenure.</p><p>If you are interested, email me directly: matus.faro@upwave.com or check out the specific role <a href="https://www.upwave.com/job/6815819002" rel="nofollow noreferrer">https://www.upwave.com/job/6815819002</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36575391"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36575391" href="https://news.ycombinator.com/vote?id=36575391&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><br><div>
                  <p><span>How and why would I feel that I'm helping society/humanity/planet/world anyhow by working for Upware? Who would I really be helping to achieve what exactly?</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36575256"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575256" href="https://news.ycombinator.com/vote?id=36575256&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Wasmer (<a href="https://wasmer.io/">https://wasmer.io</a>) | Rust Engineer | Full Time | Remote (EU or east-coast timezone)<p>Wasmer is building the Operating System for the Edge.</p><p>We are seeking a skilled Rust Engineer with industrial-strength software engineering skills to help us enable WebAssembly on any infrastructure compiled from any programing language. Bonus points: WebAssembly, libc or unix Kernel-like experience :)</p><p><a href="https://wasmer.io/values-and-culture">https://wasmer.io/values-and-culture</a> is a good place to learn more about our culture.</p><p>If interested, email me (founder): syrus+hn@wasmer.io
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575150"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575150" href="https://news.ycombinator.com/vote?id=36575150&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Sourcetable.com | Full Stack Engineer (React / Python / C#) (Senior/Staff) | Full Time | San Francisco or Mexico City<p>Sourcetable is building a spreadsheet-based data platform to make data accessible to everyone.</p><p>We’re hiring for a staff level full-stack engineer to join the team, working on our React.js / Python codebase (C# is helpful too).</p><p>You do not need a degree to apply, but you must be able to demonstrate that you have worked on hard things and have experience commensurate with the role. It will be helpful if you have previous experience at a startup.</p><p><a href="https://sourcetable.com/about" rel="nofollow noreferrer">https://sourcetable.com/about</a> is a good place to learn more about what we're up to. You will be joining as person no.8 on the team.</p><p>If interested, email me (founder): eoin+hn@sourcetable.com
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574950"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574950" href="https://news.ycombinator.com/vote?id=36574950&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Datadog | Software Engineers | ONSITE (Boston, Lisbon, Madrid, NYC, Paris, Tel Aviv) and REMOTE | Full-time<p>Datadog is a monitoring, tracing, logs system, and more, for your infrastructure and services. We build our own tsdb, event store [1][2], distributed tracing tools, cutting edge visualizations, and more. We love shipping great experiences for customers just like us and are growing fast! We write a lot of Go, Java, Python, Typescript (with React), and a bit of other languages. We run on k8s, and are multi-region and multi-cloud.</p><p>We're looking for people who can build systems at scale as we process trillions of events per day. Let us know if that's you!</p><p><a href="https://dtdg.co/hnwhoshiring" rel="nofollow noreferrer">https://dtdg.co/hnwhoshiring</a></p><p>[1] <a href="https://www.datadoghq.com/blog/engineering/introducing-husky" rel="nofollow noreferrer">https://www.datadoghq.com/blog/engineering/introducing-husky</a></p><p>[2] <a href="https://www.datadoghq.com/blog/engineering/husky-deep-dive/" rel="nofollow noreferrer">https://www.datadoghq.com/blog/engineering/husky-deep-dive/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575012"><td></td></tr>
            <tr id="36574925"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574925" href="https://news.ycombinator.com/vote?id=36574925&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Talk360 | Development Team Lead for Payments &amp; Marketplace - Hybrid (Based in NL or ZA)| Amsterdam, Johannesburg
Are you ready to make a tangible difference in people's lives in Africa? Do you dream of being part of a thrilling and rapidly expanding international scale-up? How about spearheading the introduction of groundbreaking technologies, tools, and features? If your answer is a resounding yes, then Talk360 is the ideal destination for you!<p>We are currently seeking an exceptional Development Team Lead to head our payment and marketplace division, where you will have the opportunity to revolutionize ecommerce and digital payments in Africa.</p><p>Your Challenge: Lead the payment &amp; marketplace development
You will be responsible for driving the implementation of payment solutions that enable African consumers to convert using their preferred payment methods and currencies, addressing a hot topic in the industry and maximizing our market reach.
Actively contribute as a developer on Node.js projects for the Marketplace, utilizing your expertise to architect and implement high-quality code for consumer-facing web applications, collaborating closely with the team to optimize software features.
Provide visionary leadership to your development team, championing agile software methodologies and establishing best practices that prioritize performance, reliability, simplicity, and maintainability.</p><p>Tech Stack: As Team Lead, be an expert in NodeJS and Type script, and be familiar with Express.js, Nest.js, RESTful APIs, React and Vue.js.
More info here: <a href="https://talk360.recruitee.com/o/lead-developer-for-payments-marketplace-hybrid-based-in-nl-or-za" rel="nofollow noreferrer">https://talk360.recruitee.com/o/lead-developer-for-payments-...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574167"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574167" href="https://news.ycombinator.com/vote?id=36574167&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Atomic Object | Senior Software Developer &amp; Consultant | ONSITE | Full Time | Raleigh, NC<p><a href="https://atomicobject.com/" rel="nofollow noreferrer">https://atomicobject.com/</a></p><p>As a senior software developer in a recently established office, you will have a unique opportunity to help build and shape the development team from the ground up. Your experience and expertise can help establish a strong foundation of technical best practices and contribute to our hiring process for future team members. Additionally, you will have broad influence shaping the technology and technical processes used to ensure the delivery of high-quality software that exceeds clients’ expectations.</p><p>More details and application here - <a href="https://atomicobject.applytojob.com/apply/4GSNOJOkrx/Senior-" rel="nofollow noreferrer">https://atomicobject.applytojob.com/apply/4GSNOJOkrx/Senior-</a>...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36573965"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36573965" href="https://news.ycombinator.com/vote?id=36573965&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Culture Biosciences | Sr Software Engineer - Hardware Automation | Hybrid ( South San Francisco , CA, USA) | $170,000 - $190,000
Culture’s mission is to make bioprocess development and scale-up as fast and easy as scaling software. Towards this aim, Culture’s offering enables biopharma, biotechnology, and synthetic biology companies to run their bioprocessing R&amp;D in the cloud. Clients design, manage, and analyze bioprocess experiments in Culture’s Console web application. Culture’s offering enables customers to focus on designing and improving their process versus spending time and effort building out their own high-throughput process development laboratory. While customers remotely observe the process and analyze data, the experiments are executed in Culture’s cloud bioreactor facility in South San Francisco. The facility is enabled by Culture’s proprietary robotic 250mL and 5L single-use bioreactor technology and software systems.<p>We’re looking for a software developer to help us create robust automation tools and streamline the biomanufacturing R&amp;D process, specifically focusing on the automation of the bioreactor systems. An ideal candidate has worked on application code, cloud and/or local infrastructure and automation.</p><p>In this position you'll be collaborating with mechanical engineers, embedded systems engineers, and other software engineers, so thriving in a cross-functional environment is a must.</p><p>You'll be responsible for integrating new hardware systems to the Culture's software backend. The ability and interest in designing user interfaces for those systems is also strongly desired.</p><p>Please Apply Here: <a href="https://www.culturebiosciences.com/careers?gh_jid=5615370003">https://www.culturebiosciences.com/careers?gh_jid=5615370003</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36574010"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36574010" href="https://news.ycombinator.com/vote?id=36574010&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>I’m working as a data science as at a biotech company, but I have a chemical engineering degree.<p>I want to highlight how cool bioreactor process optimization is. There is so much complexity and coolness to the problem. For example, scaling up does not involve making all of the parameters bigger because of different scaling relationships.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36574007"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574007" href="https://news.ycombinator.com/vote?id=36574007&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>CrazyGames | <a href="https://about.crazygames.com/" rel="nofollow noreferrer">https://about.crazygames.com/</a> | REMOTE | Full-time | Product Engineer<p>With recent technologies such as WebGPU and WebAssembly, the browser has become a powerful gaming platform. High-quality 3D graphics and near-native level performance are becoming possible without the need for downloads, apps, or platform-specific development. Our browser games platform is already reaching more than 25 million people per month. We are self-funded, profitable, remote-first, and fast-growing. We are currently looking for a Product Engineer to build our product together with a small elite engineering team and a world-class designer:</p><p>* <a href="https://crazygames.recruitee.com/o/remote-product-engineer" rel="nofollow noreferrer">https://crazygames.recruitee.com/o/remote-product-engineer</a></p><p>More information here: <a href="https://about.crazygames.com/" rel="nofollow noreferrer">https://about.crazygames.com/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36574870"><td></td></tr>
                  <tr id="36574212"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574212" href="https://news.ycombinator.com/vote?id=36574212&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Zattoo | Berlin | Full Time | ONSITE / HYBRID | Principal QA Engineer - Video Technology<p>Zattoo: Back in 2005, we pioneered Europe’s first TV streaming service. Today, we’re the world’s first certified climate neutral TV streaming provider. 3 million users across three countries. Over 230 colleagues of 47 nationalities.</p><p>// Principal QA Engineer - Video Technology <a href="https://grnh.se/e978a822teu" rel="nofollow noreferrer">https://grnh.se/e978a822teu</a></p><p>Video Technology at Zattoo: Our video teams develop the software which drives our carrier-grade streaming platform, covering the full processing chain from content ingest, transcoding, storage and stream delivery as well as putting the streams into action with our own playback SDKs for many platforms. Additionally we operate our inhouse SSAI solution based on the streaming stack mentioned before.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575024"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575024" href="https://news.ycombinator.com/vote?id=36575024&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Assembled | Software Engineer, Senior Software Engineer, Engineering Manager, UX Developer | Full Time | SF, NYC<p>We're transforming customer support for the most modern companies in the world. Our customers include Stripe, Etsy, Zoom, and Asana among others. We solve deep technical and algorithmic problems (how do you forecast support demand, how do you schedule thousands of agents within shift constraints, etc.) while priding ourselves on our user experience.</p><p>If you're interested in working in a dynamic, fast paced environment with lots of ownership, you'd be a great fit!</p><p>We target 90th percentile compensation across all roles against companies of similar size/funding.</p><p><a href="https://www.assembled.com/careers-at-assembled" rel="nofollow noreferrer">https://www.assembled.com/careers-at-assembled</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574903"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574903" href="https://news.ycombinator.com/vote?id=36574903&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Fictive Reality | Founding Engineers in AI and Game Dev | REMOTE or ONSITE (Sweden)<p>We're startup building a platform using conversational AI to enable practice, recruiting, coaching and tutoring of employees, students, and individuals. Use cases such as sales, customer support, patient care and more. We have also been given a grant to help those that struggle to use and understand digital government services.</p><p>We are looking for experienced engineers to form the core of the tech team and evolve our beta product. Be prepared to solve whatever needs solving, but the focus is LLMs, ML, optimize audio streaming, latency and optimizing Unity apps across web and other platforms.</p><p>Stack: Generative AI, ML, mixed reality, Unity, WebGL, iOS, Android, WebRTC, Python, React.</p><p>Culture: Honesty, teamwork, and freedom to work and live the way you want.</p><p><a href="https://thehub.io/startups/fictive-reality" rel="nofollow noreferrer">https://thehub.io/startups/fictive-reality</a> or dev@fictivereality.com
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575108"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575108" href="https://news.ycombinator.com/vote?id=36575108&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Arcol | CAD Software Developer | Remote / NYC / San Jose
At Arcol, we are building "Figma for BIM" -- a web-first collaborative design and documentation tool for the AEC industry.<p>We are building a fast, scalable geometry kernel using modern browser technologies like web workers and WASM.</p><p>We've raised VC from an amazing group of supporters, people like Dylan Field (CEO of Figma) and Tooey Courtemanche (CEO of Procore). We have a waitlist of over 16,000 people.</p><p>We are hiring CAD experts, graphics experts, and web performance gurus. We love software developers who live and breathe in the world of architecture and building design.</p><p>Send mail to paul@arcol.io to apply.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574602"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574602" href="https://news.ycombinator.com/vote?id=36574602&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Nooks (<a href="https://nooks.ai/" rel="nofollow noreferrer">https://nooks.ai</a>) | Software Engineers (full-stack &amp; backend/ML infra) | Full-time | SF or Remote<p>Hi, I’m one of the founders of Nooks — we’re trying to re-invent how sales teams work with real-time collaboration and AI.</p><p>Nooks is: - A virtual office: you can work from anywhere in the world, but still work with your team and get feedback and coaching like you’re sitting side-by-side. - and an AI co-pilot. We learn how your team works, identify “winning” strategies behaviors, then share them with the whole team during and after calls!</p><p>We’re working on fun engineering challenges including complex distributed systems, low-latency algorithms &amp; infrastructure, and modeling sales calls with large language models. You'll also have the chance to make a huge impact on our customers (sales reps who spend 80% of their work day on Nooks.)</p><p>Right now we’re a small team (18 ppl) growing super quickly - doubling revenue every quarter, and recently hit $1.4MM ARR!</p><p>Looking for experienced engineers who love tackling difficult product questions and working closely with customers. Excitement about language models, few-shot learning and other recent AI advances is also a plus :)</p><p>I’d love to chat: reach me at nikhil [at] nooks.in or apply at the link below: <a href="https://nooks.breezy.hr/p/bbc06c1a1bb4-software-engineer" rel="nofollow noreferrer">https://nooks.breezy.hr/p/bbc06c1a1bb4-software-engineer</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574610"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574610" href="https://news.ycombinator.com/vote?id=36574610&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><br><div>
                  <p><span>Bold Penguin | Remote (US)* | Full Time | www.boldpenguin.com
Bold Penguin products help insurance agencies successfully quote and bind small business insurance policies with the click of button! We are growing fast and continuing to expand our teams. Take it as you need it PTO, Slack/Zoom/Mac, and you can use OSS tools that you like using. Remote-first!<p>If you don’t meet all of the qualifications please don’t worry. Apply anyway! If you’re a current SWE that doesn’t mean we wouldn’t consider you for a senior position. We don’t bite ;)</p><p>Some current open roles:</p><pre><code>  Senior Software Engineer (Ruby on Rails) ~&gt; https://www.thegravityapp.com/shared/job?clientId=8a7883d0676d10a10167856a16554ede&amp;id=8a7887ac85cc28b80185cf9ff28b2de3&amp;u=1685555509&amp;v=9&amp;token=eyJ1aWQiOjQzNjY3LCJwcm92aWRlciI6ImJvdW5jZSIsInR5cGUiOiJlbWFpbCJ9.mL_wfqEbWYg666jl7RuE-KvV1E0

  Site Reliability Engineer ~&gt; https://www.thegravityapp.com/shared/job?clientId=8a7883d0676d10a10167856a16554ede&amp;id=8a78839e85eb5036018603a447ed2adc&amp;u=1688125450&amp;v=9&amp;token=eyJ1aWQiOjQzNjY3LCJwcm92aWRlciI6ImJvdW5jZSIsInR5cGUiOiJlbWFpbCJ9.fhh3fOI2fZ4__nJn6kD460p6RaE

  Technical Analyst ~&gt; https://www.thegravityapp.com/shared/job?clientId=8a7883d0676d10a10167856a16554ede&amp;id=8a7887a1876de7420187801e961325d7&amp;u=1685022697&amp;v=9&amp;token=eyJ1aWQiOjQzNjY3LCJwcm92aWRlciI6ImJvdW5jZSIsInR5cGUiOiJlbWFpbCJ9.HJmSzti3yEY1z74qolEpdTkUV1E
  

</code></pre>
* Today you have to live within 150 miles of a few cities as we are working on creating "hubs" but it's still remote!</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36574542"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574542" href="https://news.ycombinator.com/vote?id=36574542&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>NextEra Mobility | Senior Software Engineers | REMOTE US* or San Francisco or Florida | Full Time<p>NextEra Energy Resources is the largest deployer of wind and solar power generation in the United States. Our mission is to decarbonize the U.S. economy. Our Mobility team pursues that mission for road transportation as it electrifies, in part by writing software that will play a key role in abating millions of tonnes of CO2e[1]—and reduce the terrible health impacts of diesel emissions[2]. We are hiring senior software engineers able to work on the Python data engineering stack:</p><p>* <a href="https://jobs.nexteraenergy.com/job-invite/73949/" rel="nofollow noreferrer">https://jobs.nexteraenergy.com/job-invite/73949/</a></p><p>Engineering roles are remote-first* with a great San Francisco office. We gather twice a year, once in SF and once in Florida. Join us! You can reach me directly at samuel.penrose at nexteraenergy.com</p><p>[1] <a href="https://electrek.co/2023/04/27/daimler-just-announced-a-650m-us-wide-ev-charging-network-for-trucks/" rel="nofollow noreferrer">https://electrek.co/2023/04/27/daimler-just-announced-a-650m...</a></p><p>[2] <a href="https://pubs.acs.org/doi/10.1021/acs.estlett.0c00424" rel="nofollow noreferrer">https://pubs.acs.org/doi/10.1021/acs.estlett.0c00424</a></p><p>* We can employ residents of most U.S. states
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36574639"><td></td></tr>
                <tr id="36574854"><td></td></tr>
                <tr id="36574873"><td></td></tr>
                              <tr id="36574680"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574680" href="https://news.ycombinator.com/vote?id=36574680&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>ML6 | Machine Learning Engineer, Data Engineer | Python, TensorFlow, Google Cloud Platform | Full-time | Amsterdam, Berlin, Ghent (EU)<p>We are a Machine Learning consulting company that builds end-to-end Machine Learning solutions. By applying the latest AI research, we keep our clients at the forefront of innovation.</p><p>If you are interested check out: <a href="https://www.ml6.eu/resources/resource-library" rel="nofollow noreferrer">https://www.ml6.eu/resources/resource-library</a> and <a href="https://www.ml6.eu/client-cases" rel="nofollow noreferrer">https://www.ml6.eu/client-cases</a></p><p>You will mostly work with TensorFlow and Python to solve hard Machine Learning tasks and help to put these into production. As a Premier Google Cloud Service Partner, ML6 has a very strong relationship with Google, providing you options to collaborate and alpha test a lot of their latest ML tools.</p><p>We are looking for:</p><p>• Machine Learning Engineer</p><p>• Data Engineer</p><p>• Project Manager</p><p>• Business Development Consultant</p><p>• Cloud Partner (AWS, Azure)</p><p>• Junior Accountant</p><p>• Administration Officer</p><p>Apply at: <a href="https://ml6.eu/join-us" rel="nofollow noreferrer">https://ml6.eu/join-us</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36573875"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36573875" href="https://news.ycombinator.com/vote?id=36573875&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Snout <a href="https://www.snoutplans.com/" rel="nofollow noreferrer">https://www.snoutplans.com/</a> | Senior Software Engineer | Remote US | Full Time | $120k + equity and benefits<p>Join us at Snout on our mission to ensure no one ever has to make a health decision for their pet based on the cash in their back account. Snout plans pay for 100% of routine veterinary care, unlimited visits, and additional member benefits - think pet insurance, that you will actually use every year.</p><p>You will be working on our core wellness product, launched late-2022. Work includes the addition of new wellness plan features and capabilities, enhancement of user experience, testing and operation of the wellness platform, and special projects that arise from time to time. We frequently work cross-functionally and you can expect to write code and perform technical operations for our marketing, sales, and support efforts, as well.</p><p>Our tech stack includes Node.js, React, PostgreSQL, AWS, and Tailwind. We use both JavaScript and TypeScript heavily.</p><p>We’re a small startup team consisting of fewer than ten people. Successful team members are comfortable participating in spirited and detailed debates to establish product and technical plans, and then taking initiative and ownership to deliver on those plans quickly and effectively. Our team is collaborative. You can expect to meet with the team on a daily basis, pair regularly, and participate in slack discussions throughout the day. We keep pacific work hours and expect the team to be available during 9a-5p at a minimum. If you’re looking for a team where you can carve out your area of responsibility, work with experienced partners who have your back, grow your role alongside the growth of the company, and take a product to the moon, then we should talk.</p><p>To apply, please email kyle@snoutplans.com.</p><p>Our typical process is a one hour technical interview followed by a casual 30 minute meeting with our whole team.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574902"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574902" href="https://news.ycombinator.com/vote?id=36574902&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Foodee (<a href="https://www.food.ee/" rel="nofollow noreferrer">https://www.food.ee/</a>) | CTO | Western Canada or US (Office in Vancouver)<p>Are you passionate about food technology? We’re Foodee, a team of food experts on a mission to connect local restaurants to businesses through our meal planning platform. We help corporations spread the power of good food by curating menus from top local restaurants and delivering them to your office.</p><p>We are seeking a Chief Technology Officer, whose role will be to oversee Foodee's technical strategy, execution and team. We are seeking candidates based in Western US or Western Canada (BC).</p><p>Apply at: <a href="https://foodee.bamboohr.com/careers/541" rel="nofollow noreferrer">https://foodee.bamboohr.com/careers/541</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574621"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574621" href="https://news.ycombinator.com/vote?id=36574621&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Shortwave | Sr. Software Engineer | ONSITE in San Francisco | $140k-$215k base + generous equity
I previously cofounded Firebase (YC S11) and sold it to Google, and most of our team is ex-Google / ex-Firebase.<p>Shortwave (<a href="https://www.shortwave.com/" rel="nofollow noreferrer">https://www.shortwave.com/</a>) is building an email client with a built-in AI executive assistant. We're bringing the power of LLMs &amp; a modern messaging UX to 4.5 billion email users.</p><p>Imagine an AI virtual assistant with perfect knowledge of every message, attachment, receipt, newsletter, etc you've ever sent or received. Your AI assistant can answer questions about your email ("which candidates in my inbox have experience with Kotlin?") and act on your behalf ("Draft responses asking them to schedule a time with me").</p><p>We want the best and are willing to pay for it. We like generalists with strong CS fundamentals, app development experience, scrappiness, speed, and grit.</p><p>Come help us build the future. Email if interested: joinourteam@shortwave.com
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574526"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574526" href="https://news.ycombinator.com/vote?id=36574526&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Solar Monkey | Platform Engineer | REMOTE or On-site | The Netherlands - The Hague | Full-time | <a href="https://jobs.solarmonkey.nl/?utm_campaign=2023-07-PlatformEngineer&amp;utm_source=HackerNews&amp;utm_medium=blogpost" rel="nofollow noreferrer">https://jobs.solarmonkey.nl?utm_campaign=2023-07-PlatformEng...</a><p>At Solar Monkey (<a href="https://solarmonkey.io/" rel="nofollow noreferrer">https://solarmonkey.io/</a>), our goal is to enable solar to be the world’s leading power supply, and we accomplish that with software that makes solar power systems more affordable, secure and reliable. We do so in a very open and warm environment, where everybody can have his or her say on company values, structure, and policies.</p><p>We are well funded and expanding, and currently looking for a full time Platform Engineer who is keen to help our growing development teams manage their infrastructure. We run a moderate cloud stack on the Google Cloud Platform and use Hashicorp’s tools for deployment. In our case a Consul + Vault cluster, and Nomad to schedule (container) jobs.</p><p>Most of the work is in improving the experience of other developers, and making it easier for them to take ownership over their domain, including the operations side. So if you enjoy sharing knowledge and building a development environment that everyone enjoys, you are the Platform Engineer we are looking for.</p><p>Interested? Here’s the job link: <a href="https://jobs.solarmonkey.nl/o/platform-engineer?utm_campaign=2023-07-PlatformEngineer&amp;utm_source=HackerNews&amp;utm_medium=blogpost" rel="nofollow noreferrer">https://jobs.solarmonkey.nl/o/platform-engineer?utm_campaign...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574391"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574391" href="https://news.ycombinator.com/vote?id=36574391&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Orbit | Full Stack Engineer | REMOTE, or London (UK), or Hamburg (Germany)<p>Orbit is the operating system for logistics &amp; delivery – modular, cloud-native, made for pros. 
We strive to make the world of logistics a better place by providing the world’s leading logistics operating system and helping companies to achieve more with less: Do more. With Orbit.</p><p>Read more &amp; apply --&gt; <a href="https://bit.ly/3JCNVh2" rel="nofollow noreferrer">https://bit.ly/3JCNVh2</a></p><p><i>We care about design.</i> 
We fundamentally believe that great design comes with clarity of thought – and vice-versa. Design isn’t shallow: We can differentiate ourselves with design – by thinking outside the box and pushing boundaries.</p><p><i>Software. Done right.</i> 
We perceive software as art &amp; craftsmanship. And as craftsmen we care about <i>how</i> things are made. Our code is robust, high-quality and built to last.</p><p><i>Platform-centric thinking.</i>
Orbit is an inter-connected operating system – this is why we always think holistic. Ask not what your feature can do for the platform  — ask what the platform can do for your feature.</p><p><i>Easy-(and-enjoyable)-to-use &amp; Self-serviceable to the core.</i>
Orbit is enterprise-grade technology and used in complex organisations. Although Orbit is ‘Made for Pros’, ease of use, enjoyable UX and self-service capabilities are non-negotiable.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574347"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574347" href="https://news.ycombinator.com/vote?id=36574347&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>PlantingSpace | Full-time | Remote (EU time zone) with quarterly gatherings | <a href="https://planting.space/" rel="nofollow noreferrer">https://planting.space</a><p>We build a system that can accurately represent knowledge and handle uncertainty, to help us discover insights and solve problems based on composition and explainable reasoning. We envision applications to automate analysis and speed up research in domains such as Finance, Strategy Consulting, Material Sciences, Engineering, and more.</p><p>&gt;&gt;&gt; Senior Business Development Engineer
As we are moving into our product development phase, you will be essential in helping us define strategically optimal use cases and develop the most value-adding partnerships to set our initial product direction. You bring at least 10 years of experience in developing business opportunities for AI/ML-driven SaaS products.</p><p>A bit more about us:
To pursue our goal, we leverage emerging scientific fields such as category theory, probabilistic programming, deep learning, and cognitive science. Our team is distributed across many countries, but we get together every quarter in nice locations around Europe to solve problems, connect, and learn.</p><p>Read about our company and more job openings on our website: <a href="https://planting.space/joinus/" rel="nofollow noreferrer">https://planting.space/joinus/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36574507"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36574507" href="https://news.ycombinator.com/vote?id=36574507&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Just as some random feedback, I skimmed the site and had no idea what you do, I read it a second time with full attention and I'm still a little confused though I think I have an idea.<p>Some examples / showcases would be nice IMHO unless you wanted it to be sort of vague on purpose.</p><p>The idea I left with is some sort of chatGPT for more global and inter-disciplined issues?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36574880"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36574880" href="https://news.ycombinator.com/vote?id=36574880&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Thanks for the feedback. It is somewhat vague on purpose, but we are starting to work on a new version of the site which should be more concrete.<p>To build on your initial idea - our system can use language models under the hood, but more importantly, beyond that, it can also:
1. Perform computations and reason in exact or statistically correct manner.
2. Provide sources of information that it used to answer questions.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574630"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36574630" href="https://news.ycombinator.com/vote?id=36574630&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><br><div>
                  <p><span>From the website, this looks like a boutique data solutions consulting company, I agree that it's hard to understand what this is supposed to be</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36574947"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36574947" href="https://news.ycombinator.com/vote?id=36574947&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><br><div>
                  <p><span>We are building a product, working on the initial release now, we do not plan to do consulting if that is what you meant. Users should, amongst other things, be able to obtain data solutions by interacting with our system.
Thanks for sharing your impressions.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36574756"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36574756" href="https://news.ycombinator.com/vote?id=36574756&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><br><div>
                  <p><span>What does 'boutique' mean?
I interpret it as sth 'special/extraordinary' but in what way?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36574824"><td></td></tr>
                        <tr id="36574914"><td></td></tr>
                  <tr id="36574587"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36574587" href="https://news.ycombinator.com/vote?id=36574587&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>&gt; we hire predominantly in European time zones<p>Because of European 5-figure salaries? Does it make sense to apply from North America? (6-figure)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36574784"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36574784" href="https://news.ycombinator.com/vote?id=36574784&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><br><div>
                  <p><span>Our team is mostly located in European time zone and it makes collaboration easier, but we do consider candidates from the east coast.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="36574442"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574442" href="https://news.ycombinator.com/vote?id=36574442&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>1/2<p>Lloyds Banking Group | Bristol, London, Leeds, Edinburgh, Manchester, Halifax, United Kingdom | Full-time UK (currently UK-hybrid - 2 days in office) | <a href="https://bit.ly/LBGCareersSite" rel="nofollow noreferrer">https://bit.ly/LBGCareersSite</a></p><p>At Lloyds Banking Group we’re driven by a clear purpose: to help Britain prosper. As part of our diverse team you’ll impact the lives of more than 26 million customers through our brands, including Lloyds Bank, Halifax, Bank of Scotland, and Scottish Widows. With your help we'll continue to innovate and adapt to meet the ongoing digital needs of our customers. Join us and you can make a real difference.</p><p>Some of our current openings are listed below. For other opportunities visit our careers site: <a href="https://bit.ly/LBGTechTransformation" rel="nofollow noreferrer">https://bit.ly/LBGTechTransformation</a></p><p>All have similar benefits: | Performance Bonus | 4% Cash Sum | Private Medical Insurance | Pension (we contribute up to 15%) | Share Plans | 30 days holiday (plus bank holidays)</p><p>## Please be aware that these role are live at the time of posting, however they will expire within a week for two ##
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36574455"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36574455" href="https://news.ycombinator.com/vote?id=36574455&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>2/2<p>Lloyds Banking Group | Bristol, London, Leeds, Edinburgh, Manchester, Halifax, United Kingdom | Full-time UK (currently UK-hybrid - 2 days in the office) | <a href="https://bit.ly/LBGCareersSite" rel="nofollow noreferrer">https://bit.ly/LBGCareersSite</a></p><p>## Please be aware that these role are live at the time of posting, however they will expire within a week for two ##</p><p>Test Engineer - Leeds  - £61,911 to £68,790 - <a href="https://bit.ly/3CYvJdV" rel="nofollow noreferrer">https://bit.ly/3CYvJdV</a></p><p>FileNet Software Engineer - UK Wide  - £41,715 to £60,255 - <a href="https://bit.ly/3NXARoT" rel="nofollow noreferrer">https://bit.ly/3NXARoT</a></p><p>Associate Director - Credit Analytics – Business and SME Banking - London  - £81,252 to £90,280 - <a href="https://bit.ly/3PDceyX" rel="nofollow noreferrer">https://bit.ly/3PDceyX</a></p><p>Director - Credit Analytics – Business and SME Banking - London  - £112,209 to £132,000 - <a href="https://bit.ly/447kP1b" rel="nofollow noreferrer">https://bit.ly/447kP1b</a></p><p>Technical Product Owner - London  - £98,685 to £116,100 - <a href="https://bit.ly/46rP9FD" rel="nofollow noreferrer">https://bit.ly/46rP9FD</a></p><p>Senior Data Science Analyst (Audit) - London, Edinburgh, Bristol  - £56,322 to £62,580 - <a href="https://bit.ly/3PENvKI" rel="nofollow noreferrer">https://bit.ly/3PENvKI</a></p><p>Associate Product Manager, Data Management Solutions - Bristol  - £38,232 to £42,480 - <a href="https://bit.ly/3CUuOey" rel="nofollow noreferrer">https://bit.ly/3CUuOey</a></p><p>Automation and Dev Ops Engineer - Bristol  - £66,861 to £74,290 - <a href="https://bit.ly/44lC1jC" rel="nofollow noreferrer">https://bit.ly/44lC1jC</a></p><p>Automated Test Engineer - Edinburgh, Halifax  - £35,656 to £44,570 - <a href="https://bit.ly/41RxrIJ" rel="nofollow noreferrer">https://bit.ly/41RxrIJ</a></p><p>Engineering Manager - Manchester  - £70,784 to £88,480 - <a href="https://bit.ly/3PGQT7P" rel="nofollow noreferrer">https://bit.ly/3PGQT7P</a></p><p>PEGA Software Engineer - Manchester, Bristol, Halifax, Leeds  - £61,911 to £89,427 - <a href="https://bit.ly/448CiXj" rel="nofollow noreferrer">https://bit.ly/448CiXj</a></p><p>Cloud Software Engineer (GCP) - Manchester, London  - £52,912 to £85,982 - <a href="https://bit.ly/46xrTWE" rel="nofollow noreferrer">https://bit.ly/46xrTWE</a></p><p>Firewall Engineer - London, Halifax  - £52,560 to £58,400 - <a href="https://bit.ly/3PBoiAV" rel="nofollow noreferrer">https://bit.ly/3PBoiAV</a></p><p>Firewall Shift Engineer - London, Halifax  - £52,560 to £58,400 - <a href="https://bit.ly/446elje" rel="nofollow noreferrer">https://bit.ly/446elje</a></p><p>Engineering Lead - Edinburgh  - £73,616 to £92,020 - <a href="https://bit.ly/3NtFi9m" rel="nofollow noreferrer">https://bit.ly/3NtFi9m</a></p><p>Technical Manager, Model Risk and Validation - UK Wide  - £76,428 to £84,920 - <a href="https://bit.ly/3retpgp" rel="nofollow noreferrer">https://bit.ly/3retpgp</a></p><p>Quality Engineer - Stockport  - £45,054 to £50,060 - <a href="https://bit.ly/3pCN09h" rel="nofollow noreferrer">https://bit.ly/3pCN09h</a></p><p>Associate Director - London  - £69,448 to £86,810 - <a href="https://bit.ly/3pBtBFK" rel="nofollow noreferrer">https://bit.ly/3pBtBFK</a></p><p>Data Engineer - Halifax  - £57,078 to £63,420 - <a href="https://bit.ly/3NEQSyR" rel="nofollow noreferrer">https://bit.ly/3NEQSyR</a></p><p>Service &amp; Release Engineer - Stockport  - £41,292 to £45,880 - <a href="https://bit.ly/3PAopg6" rel="nofollow noreferrer">https://bit.ly/3PAopg6</a></p><p>Software Engineer - Stockport  - £45,054 to £67,581 - <a href="https://bit.ly/3PJKusz" rel="nofollow noreferrer">https://bit.ly/3PJKusz</a></p><p>DevOps Engineer - Edinburgh, Halifax, Leeds  - £66,861 to £100,292 - <a href="https://bit.ly/3NyTjTr" rel="nofollow noreferrer">https://bit.ly/3NyTjTr</a></p><p>Digital Workplace Messaging &amp; Surveillance Engineer - Manchester, Leeds  - £66,861 to £74,290 - <a href="https://bit.ly/3pqe6AF" rel="nofollow noreferrer">https://bit.ly/3pqe6AF</a></p><p>Data Product Implementation Manager - Bristol  - £78,634 to £92,510 - <a href="https://bit.ly/46BhUjh" rel="nofollow noreferrer">https://bit.ly/46BhUjh</a></p><p>Software Engineer - Chester  - £61,911 to £89,427 - <a href="https://bit.ly/44avo3C" rel="nofollow noreferrer">https://bit.ly/44avo3C</a></p><p>DevOps Senior Engineer (Cisco ACI) - UK Wide  - £77,301 to £115,952 - <a href="https://bit.ly/3NJ6Wj8" rel="nofollow noreferrer">https://bit.ly/3NJ6Wj8</a></p><p>Solution Architect - Edinburgh  - £66,861 to £74,290 - <a href="https://bit.ly/436CxAJ" rel="nofollow noreferrer">https://bit.ly/436CxAJ</a></p><p>Solution Architect - Leeds  - £85,255 to £100,300 - <a href="https://bit.ly/44xSJwd" rel="nofollow noreferrer">https://bit.ly/44xSJwd</a></p><p>Back End Engineer - Chester  - £61,911 to £89,427 - <a href="https://bit.ly/3NJ6WQa" rel="nofollow noreferrer">https://bit.ly/3NJ6WQa</a></p><p>BPS Event attendees only - Edinburgh  - £66,861 to £100,292 - <a href="https://bit.ly/3NElwIo" rel="nofollow noreferrer">https://bit.ly/3NElwIo</a></p><p>GCP DevOps Engineer - Bristol - £61,911 to £89,427 - <a href="https://bit.ly/447dFdx" rel="nofollow noreferrer">https://bit.ly/447dFdx</a></p><p>DBA Technical Specialist - Gloucester - £66,861 to £74,290 - <a href="https://bit.ly/44tvpPX" rel="nofollow noreferrer">https://bit.ly/44tvpPX</a></p><p>Operations Specialist-1 - Gloucester - £56,421 to £62,690 - <a href="https://bit.ly/437Ui2M" rel="nofollow noreferrer">https://bit.ly/437Ui2M</a></p><p>Data &amp; AI Ethics Product Owner - London  - £85,208 to £106,510 - <a href="https://bit.ly/3ne5hJ9" rel="nofollow noreferrer">https://bit.ly/3ne5hJ9</a></p><p>Service Engineer - Edinburgh  - £57,078 to £63,420 - <a href="https://bit.ly/3JHfyFu" rel="nofollow noreferrer">https://bit.ly/3JHfyFu</a></p><p>Service Engineer - Halifax  - £57,078 to £63,420 - <a href="https://bit.ly/434vdWo" rel="nofollow noreferrer">https://bit.ly/434vdWo</a></p><p>Lead Network Security Engineer - UK Wide  - £85,255 to £100,300 - <a href="https://bit.ly/3reohJ5" rel="nofollow noreferrer">https://bit.ly/3reohJ5</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36574660"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574660" href="https://news.ycombinator.com/vote?id=36574660&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Spill (<a href="https://www.spill.chat/" rel="nofollow noreferrer">https://www.spill.chat/</a>) | Senior Frontend Engineer, Senior Backend Engineer &amp; Lead Product Designer | London, UK (HQ) | ONSITE 1-2 days | Full-time<p>Our aim is to make counselling and therapy free at the point of use for as many people in the word as possible. We do this by selling a mental health support product to businesses.</p><p>We're hiring a Senior Software Engineers and a Lead Product Designer into our small product and engineering team of 10.</p><p>For the SWE roles you'll be expected to help deliver thoughtful features for our customers and share responsibility for architectural decisions.</p><p>For the Lead Product Designer role you'll be our first full-time product designer hire and will be expected to bring a new level of thoughtfulness to the Spill product.</p><p>email calvin[at]spill[dot]chat for a more thorough job specs
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574232"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574232" href="https://news.ycombinator.com/vote?id=36574232&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>StatsBomb | Data Platform Engineer | REMOTE, or Bath (UK), or Cairo (Egypt)<p>StatsBomb is a sports analytics startup, covering football (both the soccer and American varieties) and soon basketball. We sell data products as well as analysis tools to sports, media and gambling organisations, with a tech pipeline that includes computer vision, machine learning, stream processing, and web-based dataviz. We count many of the biggest names in football as customers, and your work will have a direct impact on our ability to deliver insights to those customers, driving success on the field.</p><p>We're hiring a Data Platform Engineer to help manage our data warehouse and other key components of our data infrastructure:</p><p>- Apply at: <a href="https://statsbomb.bamboohr.com/careers/172" rel="nofollow noreferrer">https://statsbomb.bamboohr.com/careers/172</a></p><p>If you'd like to find out more about football analytics:</p><p>- Play with our open data: <a href="https://github.com/statsbomb/open-data">https://github.com/statsbomb/open-data</a></p><p>- Read our articles: <a href="https://statsbomb.com/articles/" rel="nofollow noreferrer">https://statsbomb.com/articles/</a></p><p>- Browse our conference videos: <a href="https://www.youtube.com/channel/UCmZ2ArreL9muPvH49Gaw0Bw">https://www.youtube.com/channel/UCmZ2ArreL9muPvH49Gaw0Bw</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574571"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574571" href="https://news.ycombinator.com/vote?id=36574571&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Connectly | Machine Learning/LLM engineer(SF) | Frontend engineer(Remote)<p>We Connectly (<a href="https://connectly.ai/" rel="nofollow noreferrer">https://connectly.ai</a>) are a fast-growing startup that is building LLM-driven conversational commerce for messaging platforms such as WhatsApp and SMS. Our goal is to enable users to discover and purchase products seamlessly on WhatsApp and other messaging apps.</p><p>Founders: Head of FB Messenger + CTO of Strava</p><p>Team - 40 people &amp; growing.</p><p>Product - Bring the full shopping experience to WhatsApp in 5 mins</p><p>Funding - have raised a large amount from top investors in Silicon Valley</p><p>Revenue - multiple million dollars &amp; growing fast</p><p>Stack - typescript, react, python &amp; golang,</p><p>Culture - we pay people generously and treat each other with respect.</p><p>Join us if you are self-driven, move fast and would like to make something great together! Email me at careers@connectly.ai or apply at <a href="https://jobs.lever.co/connectly" rel="nofollow noreferrer">https://jobs.lever.co/connectly</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574352"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574352" href="https://news.ycombinator.com/vote?id=36574352&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Hack Club Bank | Engineering Manager | Shelburne, Vermont | $80k-$110k<p>We're Hack Club Bank, an initiative of Hack Club, a nonprofit platform dedicated to streamlining the process for groups seeking nonprofit status. Through our platform, we've greatly reduced the time, cost, and complexity that typically goes with gaining and maintaining nonprofit status.</p><p>Hack Club Bank started as a solution for our own clubs and hackathons needing nonprofit status, and now we serve as a nonprofit fiscal sponsor for over 1,100 organizations, making us the largest fiscal sponsor in America serving young people.</p><p>Our specific Hack Club Bank team consists of three operations staff, two full-time engineers, a group of contractors, and Hack Club teenagers who use and contribute to the platform. We're looking to add an Engineering Manager to this team.</p><p>We offer a salary range of $80k to $110k for this position. As part of our commitment to transparency, you can view our finances at <a href="https://bank.hackclub.com/hq/" rel="nofollow noreferrer">https://bank.hackclub.com/hq/</a></p><p>Our future plans include open-sourcing the platform, adding grant-making tools, and building further tools to increase financial transparency for all.</p><p>We are located in Shelburne, Vermont, and are part of the broader Hack Club team of 20. To apply, please visit <a href="https://hiring.hackclub.com/27568" rel="nofollow noreferrer">https://hiring.hackclub.com/27568</a></p><p>You can learn more about our platform at <a href="https://hackclub.com/bank/" rel="nofollow noreferrer">https://hackclub.com/bank/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574935"><td></td></tr>
            <tr id="36574395"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574395" href="https://news.ycombinator.com/vote?id=36574395&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Bytedance (Security Engineering)  | bytedance.com | Singapore, Sydney, Australia | Onsite | Full Time | Visa sponsorship available<p>As part of our team, you'll engage in unique and high-impact projects, tackling security challenges on a scale not typically seen in the tech world. You will design and implement new approaches for system security, apply trusted computing, and track cutting-edge security technologies.</p><p>We're hiring for:
Site Reliability Engineers - Singapore: <a href="https://job.toutiao.com/s/i2chcXA" rel="nofollow noreferrer">https://job.toutiao.com/s/i2chcXA</a></p><p>Site Reliability Lead - Sydney, Australia: <a href="https://job.toutiao.com/s/i2vTUqD" rel="nofollow noreferrer">https://job.toutiao.com/s/i2vTUqD</a></p><p>Site Reliability Engineer - Sydney, Australia: <a href="https://job.toutiao.com/s/i2c8kSV" rel="nofollow noreferrer">https://job.toutiao.com/s/i2c8kSV</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574832"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574832" href="https://news.ycombinator.com/vote?id=36574832&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Opvia (YC S20) | Sales Development Representatives| REMOTE (US based)| opvia.io<p>Opvia wants to be the software layer for regulated manufacturing companies. Regulated industries (Pharma, biotech, F&amp;B) need better software to go to market faster, get insights from their data, and comply with the regulations. Opvia makes it easier to build exactly the procedures and workflows companies need with its no-code platform.</p><p>Drop me a line (alberto@opvia.io) if you need further info or are looking for a different position
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574085"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574085" href="https://news.ycombinator.com/vote?id=36574085&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Advicerco. | Staff Software Engineer | REMOTE (US) | Full Time/Contract | $100k-$200k<p>Advicerco is enabling people to maximize the efficiency of every dollar by building a cash-flow based management platform for their finances.  It takes into account their goals and assets and uses them, the the help of an advisor, to assemble an automated flow based system to manage their funds.  By changing the way that people view and use their money, we are both changing the perception of money, altering people’s relationship with it, and reducing the everyday stress of dealing with income and budgets.</p><p>We are currently looking for full stack engineers with a penchant for product, who are willing to both help find the best solution for how we interact with our finances, as well as building out a system for handling those finances in the background.  We believe strongly in testing, as money is not a thing to be trifled with.</p><p>We're looking for:</p><p>* You an experienced developer, with at least 5 years of experience, knowledge of javascript/typescript or a fierce desire to learn.
* Experience building event based, and highly asynchronous solutions
* A lack of fear in solving hard problems, and writing tests to prove it.</p><p>Our stack is typescript with react, built in AWS.  While you don't have to know everything, you should be familiar with lambda and how it works in different contexts.  We also use postgres as our database.</p><p>If you're interested, come check out a better description and apply here:
<a href="https://advicerco.breezy.hr/p/37c0a0dd3aa2-staff-software-engineer" rel="nofollow noreferrer">https://advicerco.breezy.hr/p/37c0a0dd3aa2-staff-software-en...</a></p><p>Alternatively, send an email to jobs@advicerco.com
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574117"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574117" href="https://news.ycombinator.com/vote?id=36574117&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Oso | <a href="https://www.osohq.com/" rel="nofollow noreferrer">https://www.osohq.com</a><p>Oso is building authorization for the next billion developers. We are building the product in Rust, which we think is both a good design choice for our use case and a fun choice for the team. I recently joined the team and absolutely love the culture and I'm inspired by the talented engineers I get to work with :) Current open roles:</p><p>- Engineering Manager (NYC) 
- Software Engineer (NYC or Remote)
- Developer Experience Engineer (NYC or Remote) 
- Head of Engineering (NYC)</p><p>All open roles - <a href="https://www.osohq.com/company/jobs" rel="nofollow noreferrer">https://www.osohq.com/company/jobs</a>
A bit more about Authorization: <a href="https://www.osohq.com/academy" rel="nofollow noreferrer">https://www.osohq.com/academy</a>
Our Slack community: <a href="http://join-slack.osohq.com/" rel="nofollow noreferrer">http://join-slack.osohq.com/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36574432"><td></td></tr>
                <tr id="36574891"><td></td></tr>
                                  <tr id="36573964"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36573964" href="https://news.ycombinator.com/vote?id=36573964&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Pioneer | Founding Senior Software Engineer | Climate Tech and LLMs | Seed stage | SF Bay Area, DC, NY, remote, <i>USA only</i> | $140K+, equity<p><i>Apply here:</i> <a href="https://usepioneer.com/careers" rel="nofollow noreferrer">https://usepioneer.com/careers</a></p><p>About: Pioneer takes the pain out of the government application process by using LLMs to gradually reduce the effort required to identify, qualify, apply, and comply with government awards. We are passionate about climate impact and creating a supportive growth environment based on the fundamentals of Radical Candor. We also have a proven business model and rapidly growing revenue.</p><p>Culture: Ask us about our culture: toms, laps, feedback, and decision journals! Our newest team member said <i>“Every interaction is “kind”, coming from a good place of respecting each other.”</i></p><p>You: Owner mindset, collaborative, have curiosity for technology like LLMs, start sentences with <i>“What if…“</i>, enjoy building full stack, and enjoy the 0-to-1 phase.</p><p>Stack: We TypeScript, React, Next.JS, and other modern tools.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36574743"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36574743" href="https://news.ycombinator.com/vote?id=36574743&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><br><div>
                  <p><span>I tried to email founder@usepioneer, founder@pioneerclimate and join@pioneerclimate, but got bounced emails from Gmail saying addresses don't exist.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36574853"><td></td></tr>
                        <tr id="36574188"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574188" href="https://news.ycombinator.com/vote?id=36574188&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Ministudio AI | {ML, Computer Graphics, iOS} engineer | Remote / NYC | Full-Time<p>Ministudio is a funded AI startup based in NYC build creative tools for children to unleash their imagination with the power of AI. We combine traditional computer graphics / vision methods with modern advances in generative models, e.g., LLMs and Stable Diffusion. You will be joining a stellar team of experienced founders working on cutting edge technology to make creative tools accessible to children. As an early stage employee you will have the autonomy and responsibility to drive our next phase of growth.</p><p>We are looking for folks proficient in computer graphics/vision, game development, applied machine learning, AI research, or iOS development.</p><p>Please get in touch with CV and/or links to projects you've worked on: akash@ministudio.ai
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575046"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575046" href="https://news.ycombinator.com/vote?id=36575046&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>trivago | Multiple Positions | Düsseldorf, Germany | Onsite (3 days per week) | Full Time | Visa Sponsorship<p>trivago is a well-know travel tech company enabling users to find the right accommodation. We save our users time and empower them to get more out of travel by comparing accommodation from leading booking sites. At trivago we have talents from over 80 different countries and work on some of the most interesting technical challenges within the accommodation meta-search space.</p><p>We are looking for:</p><p>• Data Analyst – Product Intelligence</p><p>• Site Reliability Engineer</p><p>• Senior Quantitative UX Researcher</p><p>• Senior Data Analyst – UX Research</p><p>• Backend Lead – Ranking &amp; Search</p><p>Apply at: <a href="https://careers.trivago.com/join/open-positions/" rel="nofollow noreferrer">https://careers.trivago.com/join/open-positions/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36575112"><td></td></tr>
                <tr id="36575188"><td></td></tr>
                        <tr id="36573906"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36573906" href="https://news.ycombinator.com/vote?id=36573906&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Shortform | REMOTE | Full-stack Engineer | Full-time / Part-time | $100-200k<p><a href="https://www.shortform.com/" rel="nofollow noreferrer">https://www.shortform.com/</a></p><p>Do you feel overwhelmed by the amount of stuff to learn? Do you have hundreds of books and articles in your to-read list, but wish you could extract the main ideas faster?</p><p>The goal of Shortform is to make the world's best ideas more accessible. We started by creating the world's best book guides, with in-depth chapter-by-chapter summaries, original insights building on the book, and distillation of ideas into actionables. They're like superpowered book summaries.</p><p>Our next goal is to index every idea on Earth and make sense of it. Specifically, we want to summarize and contextualize every meaningful idea published each day, in every form of media (books, articles, podcasts, videos, news, research papers).</p><p>Our first step was to build a Chrome extension that summarizes any page in your browser (<a href="https://extension.shortform.com/" rel="nofollow noreferrer">https://extension.shortform.com/</a>). Next, we're building content products to comprehend the world's information - for example, an idea explorer lets you see how a specific idea has been expressed throughout human history; a news summarizer shows the entire spectrum of opinions on a topic to help avoid groupthink. If ideas like these interest you, please apply.</p><p>--------</p><p>Company status: We launched 4 years ago. Shortform was funded by the founder (who has built multiple bootstrapped profitable companies in the past) and has taken on no outside investment. Our financials allow us to be profitable, but currently we're reinvesting in growth. Engineering team consists of 8 full-stack engineers.</p><p>Technology: Vue.js on frontend, Flask / Python on backend, with heavy R&amp;D in AI / LLM / GPT. 
(Keywords for searchers with tech flexibility: Angular, React, Django | BERT, LLaMA, Natural Language Processing / NLP, Large Language Models, ML)</p><p>About the role: Ideally you're fullstack on the web, and comfortable on both front-end and back-end. Position is remote (current team is worldwide). Fulltime / parttime roles available. We're able to pay competitive market salaries (i.e. not decreased by equity).</p><p>Email jobs+engineerhn@shortform.com to get more details about the company, what we're building, and the application process. In your email, please discuss your experience for our stack, and also your favorite non-fiction book (to help us screen spam emails).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36574248"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36574248" href="https://news.ycombinator.com/vote?id=36574248&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><br><div>
                  <p><span>What sort of part time roles do you have? What are your requirements for part time, like hours per week and availability?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36574437"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36574437" href="https://news.ycombinator.com/vote?id=36574437&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Time - We typically ask 20 hours per week as a minimum. Below this we've found that not enough gets done for the relationship to be worthwhile for either party.<p>Availability - we try to be as async as possible. Right now the only live meeting is a daily standup at a time that supports very different time zones. Otherwise, we keep trying to cut live meetings down to the bone.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="36574325"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574325" href="https://news.ycombinator.com/vote?id=36574325&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>MorphMarket.com | Senior Django Backend or Django Full-Stack Developer | Remote | Full-time | Contract | <a href="https://www.morphmarket.com/" rel="nofollow noreferrer">https://www.morphmarket.com</a><p>I'm the founder of MorphMarket.com. We are an online marketplace that connects breeders and keepers of lizards, geckos, snakes, frogs, spiders and other captive-bred pets. Launched in 2015, we are the largest reptile-related website in the world with about 5,000 active sellers. Our users passionately love the least loved animals.</p><p>We are seeking to add another experienced developer to our remote team. This position requires expertise in Python/Django, and it is even better if the applicant is a Full Stack developer with React expertise.</p><p>Our reptile site is implemented in Python, of course. We use a Django/Postgres/Redis backend and frontend development is in React/Typescript.  Dev-ops mostly involve AWS, Heroku, CloudFlare, and NewRelic. As a dev on our team, you could think of yourself as a Python handler. :)~</p><p>You can see some of our recent work in this demo: <a href="https://www.youtube.com/watch?v=nz05fICRnNI">https://www.youtube.com/watch?v=nz05fICRnNI</a></p><p>Key skills: Django, Django Rest Framework, Python, SQL. Good knowledge of: browser caching, performance profiling, web performance fundamentals, system-wide debugging, unix.  Nice to have:  React, TypeScript, SCSS, React Native, AWS, webpack. Pragmatic: ability to balance quality and speed.</p><p>There are about 10 people on our product team. We collaborate with Github and Slack. Our business has done well even through the ups and downs of the economy. We are committed to a positive culture for the dev team.</p><p>This is a full-time, long-term engagement. Work-hours are expected to overlap a few hours a day with the US time zone. We have multiple team members in Europe and they shift their work-day a few hours forward to make it work.</p><p>Please email a CV jobs@morphmarket.com and fill out our short application form here <a href="https://forms.gle/Fr5Buqt6BkWvTiNfA" rel="nofollow noreferrer">https://forms.gle/Fr5Buqt6BkWvTiNfA</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36574365"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36574365" href="https://news.ycombinator.com/vote?id=36574365&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><br><div>
                  <p><span>I don't yet feel comfortable calling myself an "experienced" Djangoist, and even less an experienced React writer, but it's heartening to open up WIH and see my exact chosen stack as the first result.</span></p></div></td></tr>
        </tbody></table></td></tr>
                            <tr id="36573963"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36573963" href="https://news.ycombinator.com/vote?id=36573963&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Infinia ML | <a href="https://infiniaml.com/" rel="nofollow noreferrer">https://infiniaml.com</a> | Durham, NC, USA | REMOTE US | Full-time | Senior infrastructure and full-stack engineers<p>Infinia ML is a machine learning startup, originally spun out of a research lab at Duke University in 2017. We’re building a SaaS intelligent document processing platform that automates custom data-intensive workflows for our clients.</p><p>We’re hiring for:</p><p>* Lead Infrastructure Engineer</p><p>* Senior Infrastructure Engineer</p><p>* Senior Full-stack Engineer</p><p>Tech stack: Python (Django / FastAPI / Celery), Vue.js, Tailwind, a dash of Go, Postgres, Redis, RabbitMQ, K8s, and AWS.</p><p>Apply at: <a href="https://infiniaml.com/about/careers/" rel="nofollow noreferrer">https://infiniaml.com/about/careers/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574041"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574041" href="https://news.ycombinator.com/vote?id=36574041&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Countfire | Senior or Mid-level Fullstack Developer | Remote (but must based in UK) | www.countfire.com<p>Countfire makes software for the construction industry. Our software is well loved because we try to design solutions that are a magnitude more efficient than existing tools.</p><p>We use all sorts of tech, but mostly MobX, React, Typescript, Python, Hasura and Postgres. Looking for people with a good understanding of React.</p><p>Previously based in London, now seeking remote devs across the UK.</p><p>More info here <a href="https://www.countfire.com/careers/" rel="nofollow noreferrer">https://www.countfire.com/careers/</a></p><p>Feel free to contact me directly aidan@countfire.com</p><p>Note, applicants must be resident in the UK. No recruiters.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574112"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574112" href="https://news.ycombinator.com/vote?id=36574112&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>PivotCX | Indianapolis, IN USA | Senior Developer - Python, Go, Vue.js | Full-time | $100-$130K<p>PivotCX is communications hub, that plugs in to ATS, HRIS, CRMs and enables automated and person to person communication over SMS, chat, voice, and video. We're looking for a senior developer who can help us build our app and development team. We are pre-series A, and</p><p>We're hiring: Senior Developers and Front End Developers</p><p>Apply here: <a href="https://api.pivotcx.io/c/dHd6" rel="nofollow noreferrer">https://api.pivotcx.io/c/dHd6</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36573899"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36573899" href="https://news.ycombinator.com/vote?id=36573899&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>lowRISC <a href="https://lowrisc.org/" rel="nofollow noreferrer">https://lowrisc.org</a> | Non-profit | Cambridge, UK | Zurich, Switzerland | Design Verification Engineers | Infrastructure Engineer | HYBRID | Full time<p>lowRISC's mission is to bring open source silicon to the hardware world and see it shipping in volume in commercial applications. We want to see open source silicon occupy a similar position to open source software (e.g. look at Linux, it's the default choice in many applications, we'd like open source silicon to be used for similar foundational technologies in the hardware world).</p><p>Our major project focus is OpenTitan: <a href="https://github.com/lowRISC/opentitan">https://github.com/lowRISC/opentitan</a> it’s a silicon root of trust being built and funded by a collaboration of major companies, such as Google, Western Digital, Seagate, Winbond and Rivos amongst others. lowRISC stewards the project as well as performing a significant proportion of the engineering work.</p><p>We’ve just announced the RTL freeze for the first OpenTitan tapeout, a discrete chip, named Earl Grey: <a href="https://lowrisc.org/blog/2023/06/opentitans-rtl-freeze-leveraging-transparency-to-create-trustworthy-computing/" rel="nofollow noreferrer">https://lowrisc.org/blog/2023/06/opentitans-rtl-freeze-lever...</a></p><p>We’re looking for verification and infrastructure engineers to join us. We work in System Verilog and use UVM, though plan to expand our use of formal verification. We’re also keen to explore new innovative ways to verify designs.</p><p>A key responsibility for lowRISC is maintaining the CI and regression infrastructure for OpenTitan. This is a complex system running many different tools across different machines (both cloud and on-site) and involves FPGAs and custom hardware. We use ansible and terraform to manage it all. We’re seeking an infrastructure engineer to maintain and scale the system as well as architect and build new facets of it.</p><p>lowRISC is headquartered in Cambridge, UK and we have an office in Zürich, Switzerland. We utilize a hybrid working model.</p><p>We offer competitive salaries (see job ads for ranges) and a generous pension (12.5% employer contribution in the UK), you can find our individual job postings here: <a href="https://lowrisc.applytojob.com/apply/" rel="nofollow noreferrer">https://lowrisc.applytojob.com/apply/</a></p><p>Feel free to email me at gac@lowrisc.org if you’ve got any questions.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574116"><td></td></tr>
            <tr id="36574478"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574478" href="https://news.ycombinator.com/vote?id=36574478&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Powertools Technologies | Junior Engineer | Lisbon, Portugal | Full-time | ONSITE<p>Looking for a junior engineer for work on software related to Electronic Design Automation and/or Software Development. Candidate should at least have (or graduate shortly) a 3 year university degree in engineering. Most suitably Electronic/Computer Engineering or Informatics. Software Developers are more than welcome to apply.</p><p>Site: <a href="https://www.powertools-tech.com/" rel="nofollow noreferrer">https://www.powertools-tech.com</a> . Growing a small experienced team with international industrial and academic track, willing to train new hire in fairly uncommon skill set. Candidate should be capable of quality detail work, and have good communication abilities, to provide support to international design teams in fabless semiconductor companies.</p><p>Email your interest and CV to hr@powertools-tech.com, please.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575221"><td></td></tr>
            <tr id="36573982"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36573982" href="https://news.ycombinator.com/vote?id=36573982&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>iPify | Legal Tech | Remote Europe or UK | Full Time<p>Do you want to build the leading platform for the intellectual property industry? We want you to join us, a team of seasoned industry professionals growing an ambitious project in a niche market.</p><p>Bring your enthusiasm and expertise to help us build the B2B collaborative ecosystem supporting patent and trademark legal specialists with their business network management, legal workflows and financial flows.</p><p>Job description: <a href="https://ipify.notion.site/Senior-Python-Developer-37de517efa9d4ec793f5a2b4f4d753da" rel="nofollow noreferrer">https://ipify.notion.site/Senior-Python-Developer-37de517efa...</a></p><p>Company website: <a href="https://www.ipify.app/" rel="nofollow noreferrer">https://www.ipify.app/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574487"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574487" href="https://news.ycombinator.com/vote?id=36574487&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>acreom | DevRel (remote) or Prague (Czechia)<p><a href="https://acreom.com/" rel="nofollow noreferrer">https://acreom.com</a> is a markdown knowledge base with tasks for developers. We're building a delightful and integrated interface developers love using alongside their code editors to organise their work.</p><p>reach out to me directly /martin at acreom dot com/ for more info.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574267"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574267" href="https://news.ycombinator.com/vote?id=36574267&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Witt Software LLC | <a href="https://www.weatherthetrip.com/" rel="nofollow noreferrer">https://www.weatherthetrip.com</a><p>Seeking someone to extend their daily rigor in marketing and business development for a software-as-a-service. Remote only.</p><p>equity stake for you up to 20%
annual compensation is negotiable but is dependent on performance of the saas</p><p>@jasonmarks__ on twitter
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575303"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575303" href="https://news.ycombinator.com/vote?id=36575303&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><br><div>
                  <p><span>Tesla | Various Roles | Palo Alto | ONSITE | <a href="https://www.tesla.com/careers" rel="nofollow noreferrer">https://www.tesla.com/careers</a> Tesla is hiring for software positions across a variety of disciplines. If you're interested in helping us transition to sustainable energy production/storage/consumption, we'd love to hear from you! Please apply online at <a href="https://www.tesla.com/careers" rel="nofollow noreferrer">https://www.tesla.com/careers</a> and send us a email highlighting some exceptional things you've done in the past so we can pull your resume from the stack.
You can reach us at vehiclesoftwarerecruiting at tesla. Make sure to reference the url for the job(s) you've applied to!<p><a href="https://www.tesla.com/careers/search/job/software-validation-engineer-ethernet-communication-architecture-177415(test" rel="nofollow noreferrer">https://www.tesla.com/careers/search/job/software-validation...</a> validation, test infrastructure, SIL, HIL, integration, Python) Location: Palo Alto, CA</p><p><a href="https://www.tesla.com/careers/search/job/firmware-engineer-embedded-software-engineer-body-controls-164776" rel="nofollow noreferrer">https://www.tesla.com/careers/search/job/firmware-engineer-e...</a> (robotics, embedded, C) Location: Palo Alto, CA</p><p><a href="https://www.tesla.com/careers/search/job/firmware-validation-engineer-embedded-software-low-voltage-power-distribution-153049" rel="nofollow noreferrer">https://www.tesla.com/careers/search/job/firmware-validation...</a> (hardware, validation, python, C) Location: Palo Alto, CA</p><p><a href="https://www.tesla.com/careers/search/job/firmware-engineer-embedded-software-engineer-thermal-hvac-166402" rel="nofollow noreferrer">https://www.tesla.com/careers/search/job/firmware-engineer-e...</a> (sensors, actuators, embedded, C) Location: Palo Alto, CA</p><p><a href="https://www.tesla.com/careers/search/job/software-qa-engineer-chassis-systems-168868" rel="nofollow noreferrer">https://www.tesla.com/careers/search/job/software-qa-enginee...</a> (python, c, HIL/SIL, mechatronics, brakes, suspension) Location: Palo Alto, CA</p><p><a href="https://www.tesla.com/careers/search/job/software-validation-engineer-hardware-in-the-loop-crash-safety-104529" rel="nofollow noreferrer">https://www.tesla.com/careers/search/job/software-validation...</a>  (HIL, validation, firmware, c, python) Location: Palo Alto, CA</p><p><a href="https://www.tesla.com/careers/search/job/thermal-systems-integration-engineer-all-levels-120369" rel="nofollow noreferrer">https://www.tesla.com/careers/search/job/thermal-systems-int...</a> (firmware, integration, thermal, python) Location: Palo Alto, CA</p><p><a href="https://www.tesla.com/careers/search/job/software-integration-engineer-tesla-electronic-park-brake-168862" rel="nofollow noreferrer">https://www.tesla.com/careers/search/job/software-integratio...</a> (firmware, integration, electronic park brake) Location: Palo Alto, CA</p><p><a href="https://www.tesla.com/careers/search/job/software-validation-engineer-thermal-systems-123212" rel="nofollow noreferrer">https://www.tesla.com/careers/search/job/software-validation...</a> (system validation, thermal, Python) Location: Palo Alto, CA</p><p><a href="https://www.tesla.com/en_CA/careers/search/job/vehicle-firmware-embedded-systems-engineering-internship-fall-2023-168106" rel="nofollow noreferrer">https://www.tesla.com/en_CA/careers/search/job/vehicle-firmw...</a> (internship, embedded, validation, thermal) Location: Palo Alto, CA.
              </p></span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36574881"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574881" href="https://news.ycombinator.com/vote?id=36574881&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>MixRank (YC S11) | Software Engineers | 100% REMOTE (Global) | Full-Time<p>MixRank processes petabytes of data every month from web crawling. We have hundreds of customers using our data products including Google, Amazon, Facebook, Intel, and Adobe, across industries Sales, Marketing, Finance, and Security.</p><p>Team is 34 full-time, full-remote from 17+ countries. We're growing, profitable, employee-owned, no dependence on outside funding. Applicants from all geographies and backgrounds are welcome.</p><p>We are looking for passionate individuals for whom programming is not just a job but it’s something they love to do. We're obsessed with computers, programming, big data, databases, compilers, hardware, math, data science, and the internet. Does this sound like you? Please apply to join our team.</p><p>Our code base is very friendly to new contributors. You'll have a fully-functional development environment within hours (fully automated) and be pushing commits on your first day. Deployments to production happen multiple times per day and finish in less than 2 minutes. Effectively all of our codebase is written in Python, Rust, SQL, Javascript/TypeScript, and Nix. The core technologies you'll need familiarity with to be productive are Python, PostgreSQL, Linux, and Git.</p><p>We operate at a larger scale than typical startups. We operate two datacenters with high performance servers we've built that are capable of dealing with the volumes of data we process. We've implemented our own distributed file system. We do full-scale web crawls. We download and perform static analysis on the entire universe of Android APKs and iOS IPAs that are published. Unlike a typical startup where you'll spend half of your time in meetings, and the other half fixing bugs from Jira tickets— at MixRank you'll get to challenge yourself with difficult technical problems that will help you to grow as an individual.</p><p>--</p><p>Junior Software Engineer - Remote (Global), Full-Time</p><p>We're looking for remote junior engineers that have 0-3 years of professional experience in software, and 5+ years of curiosity exploring computers, programming, and technical hobby projects. This is an open-ended entry role with mentorship and diverse opportunities to work on all areas of our product: databases, distributed systems, infrastructure and tooling, data analysis, machine learning, frontend/backend web development, APIs, data mining, data modeling, and more. To stand out, please highlight what makes you unique: passion for computing, curiosity and side projects, work ethic, niche research, etc.</p><p>Ideally you've already graduated, but if you still have one or more years left of school, please feel free to apply anyway, and if you're the right fit for the team we'll figure out a way to accommodate your schedule.</p><p>--</p><p>Software Engineer - Remote (Global), Full-Time</p><p>We're hiring generalist software engineers to work on web applications, data mining, machine learning/data science, data transformation/ETL, data modeling, database scaling, infrastructure, devops, and more. We'll cater the role to whatever subset of these areas match your interests.</p><p>Beneficial experience includes PostgreSQL, Python, Rust, Linux, TypeScript, Nix, frontend/backend web development, and data mining.</p><p>--</p><p>I'm Scott, Founder/CEO/CTO. Please apply here: <a href="https://www.ycombinator.com/companies/mixrank/jobs">https://www.ycombinator.com/companies/mixrank/jobs</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574178"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574178" href="https://news.ycombinator.com/vote?id=36574178&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Graphistry (graphistry.com + louie.ai) | DC | US | Australia | REMOTE | Full-time and Contract-to-hire | Senior data engineering, backend engineering, research engineering, security data science, security engineering<p>Graphistry is a Gartner-awarded startup used by fraud/cyber/BI/supplychain/... analysts to see and correlate using our visual graph AI platform, and now with louie.ai (generative AI), they can talk with their many databases in natural language while having L.O.U.I.E. generate the queries, GPU visualizations, &amp; analyses for them. We partner with companies like Nvidia &amp; AWS to enable end-to-end GPU compute so operational teams can do all this at previously impossible scales and interactivity.</p><p>We're looking for collaborative area owners especially around generative AI infra (OpenSearch, vector DBs, NLP &amp; knowledge graphs, ...), cybersecurity (Splunk, graph neural nets, ...), and osint. Despite the broader market slowdowns, we're 2-3X'ing this year, just as we did last year... and entirely on revenue. Many of our wins cannot be written about publicly, so we're happy to chat about details more privately. We're a small and high-leverage team, so looking for folks who thrive on bringing emerging technologies to meaningful problems in fast-but-intentional environments and iterating closely with our design partners at our bigger customers (banks, govs, late-stage startups, ..).</p><p>Non-traditional backgrounds are welcome. Think veterans, those transitioning from MSPs &amp; IaaS =&gt; software &amp; solutions, etc. We are generally hiring senior. Even if the current openings are not a fit, I encourage contacting us as we expect even more openings later this year as our no/low-code tools hit general availability for anyone with a database and we expect even faster growth. For senior candidates, we look for experiences like having started &amp; run projects &amp; startups, been a key contributor to open source, or other marks of engineering innovation, hard-earned wisdom, teamwork, and working holistically vs myopically.</p><p>You can find more info here: <a href="https://www.graphistry.com/careers" rel="nofollow noreferrer">https://www.graphistry.com/careers</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574172"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574172" href="https://news.ycombinator.com/vote?id=36574172&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Evertas Insurance | US-based REMOTE | Full-time | <a href="https://www.evertas.com/" rel="nofollow noreferrer">https://www.evertas.com/</a> | <a href="https://www.evertas.com/careers/" rel="nofollow noreferrer">https://www.evertas.com/careers/</a><p>Evertas is the world's first cryptoasset insurance company. We're a Bermuda carrier, Lloyd's of London coverholder, and have been around for over 5 years. (We raised a $15mm Series A in late 2022.) We write real policies which actually cover significant risk for clients, quantify premiums based on the effectiveness of deployed controls, and do professional services for clients to put better controls in place (or helping other insurance companies with underwriting and claims adjustments).</p><p>The best pitch for the company is: Join us to prevent the next SBF/FTX/3AC/Luna/Celsius... meltdown, by providing tools for the industry to police itself, provide meaningful risk transference, ensure best practices are documented and commercially required, and avoid uninformed and unproductive government regulatory retaliation instead of meaningful public/private beneficial governance -- and make a bunch of money for everyone while doing so.</p><p>(I'm Chief Security Officer; have been involved in crypto since the 1990s (blinded tokens), and I love insurance as a powerful tool to improve security across the industry. The rest of the team are also cryptocurrency and security OGs, with industry and institutional experience.)</p><p>We're looking to hire 1-2 Security Engineers. Primarily this is for underwriting and professional services in the cryptocurrency space, as well as research into risks in the space (and mitigations). A great thing about insurance is it's not "call in the middle of the night over a holiday with a crisis", but solving things prospectively for clients, but still having significant and meaningful impact.</p><p>We're open to security experts moving into cryptocurrency, or cryptocurrency experts moving into security, but obviously having interest in both is key (and expertise in both would be amazing). We highly value professionalism, integrity, and a problem-solving nature. Would be very interested in people who currently or recently worked at downsizing, technically excellent security teams in the crypto industry or in security-critical adjacent industries.</p><p>(We would be open to remote hires outside USA at some point; the complexity is handling nexus and HR and such with a small team.)</p><p>Please apply through the form at <a href="https://www.evertas.com/careers/" rel="nofollow noreferrer">https://www.evertas.com/careers/</a> (PLEASE NO RECRUITERS)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574134"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574134" href="https://news.ycombinator.com/vote?id=36574134&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Open edX / OpenCraft | Full-time | TRUE REMOTE (async/worldwide) | Senior Open Source Developer on Open edX - React, Python/Django, AWS/Kubernetes<p>Do you care about contributing to open-source, and appreciate a good challenge? We do too! :)</p><p>Open-source</p><p>We are a team of veteran open-source developers, working on educational and community-based projects in an open-first environment – and we are looking for new members. By joining us, you will work full-time on open-source, pushing your changes to free software projects upstream through pull requests, contributing features, documentation, or help on public forums.</p><p>We care deeply about contributing our work upstream. You will see the results of your work reused and recognized across the educational community, increasing access to quality education for everyone, everywhere.</p><p>Remote-first</p><p>Unlike companies who reluctantly started to accept remote workers recently, we have embraced it from day 1. For the past 7 years, we have based and refined our way of working around remote-friendly workflows, from the ground up. No day-long video meetings, mandatory work hours, or risk of being forced back into an office one day -- as long as you have a good internet connection, it’s none of our business when or where you work from. :)</p><p>We are all working remotely, from all continents (except Antarctica, at least so far - applicants welcome!). We use remote-friendly and timezone-agnostic workflows based on asynchronous principles and good documentation practices.</p><p>Online education</p><p>We are one of the main contributors to the Open edX project, the main open-source MOOC platform created by MIT, Harvard and many other top universities. It powers sites like edX.org, the MIT Open Learning Library, and the national online learning platform for France. We provide development and hosting for institutions like Harvard Medical School, Harvard LabXchange, Cloudera, Autodesk, and several governments. We are not affiliated with edX.org, but we contribute and work with them on various projects.</p><p>Our handbook, like much of our work, is publicly viewable and you can find it at <a href="https://handbook.opencraft.com/" rel="nofollow noreferrer">https://handbook.opencraft.com/</a>.</p><p>Apply for this Position</p><p>See the full details and apply at <a href="https://opencraft.com/jobs/open-source-developer/" rel="nofollow noreferrer">https://opencraft.com/jobs/open-source-developer/</a></p><p>Job description: <a href="https://docs.google.com/document/d/1VgA7geR5oAsHzTRnldm6KU7LK9c7pvbMAbSkt0ihNho/edit" rel="nofollow noreferrer">https://docs.google.com/document/d/1VgA7geR5oAsHzTRnldm6KU7L...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36573895"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36573895" href="https://news.ycombinator.com/vote?id=36573895&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>Temporal Technologies | Multiple positions in United States - WORK FROM HOME | FULL-TIME.<p>Temporal offers an entirely new way to build scalable and reliable applications. Temporal enables developers to focus on writing important business logic, and not on managing state or worrying about the underlying infrastructure. Sequoia Capital led our recent round of funding and our team has experience from start-ups and larger companies like Microsoft, Google, Amazon, Uber, and more.</p><p>Temporal Investors Expand Funding: <a href="https://temporal.io/news/temporal-investors-expand-funding-with-usd75m-round" rel="nofollow noreferrer">https://temporal.io/news/temporal-investors-expand-funding-w...</a></p><p>Temporal in 7 minutes: <a href="https://temporal.io/tldr" rel="nofollow noreferrer">https://temporal.io/tldr</a></p><p>We're looking for senior level engineers for multiple roles - see here - <a href="https://www.temporal.io/careers" rel="nofollow noreferrer">https://www.temporal.io/careers</a></p><p>FEATURED ROLES:</p><p>Senior Product Manager - Platform - Read more and apply here  <a href="https://jobs.lever.co/temporal/d4ebe2cd-27fa-43e2-ae0b-57ec316c8f4a" rel="nofollow noreferrer">https://jobs.lever.co/temporal/d4ebe2cd-27fa-43e2-ae0b-57ec3...</a></p><p>Senior Technical Curriculum Developer - Read more and apply here <a href="https://jobs.lever.co/temporal/a09daac8-f296-4330-a31b-59e56445e6f8" rel="nofollow noreferrer">https://jobs.lever.co/temporal/a09daac8-f296-4330-a31b-59e56...</a></p><p>Staff+/Tech Lead - Distributed Systems Software Engineer - Read more and apply here <a href="https://jobs.lever.co/temporal/28a290fa-087f-447b-934c-2960e769b963" rel="nofollow noreferrer">https://jobs.lever.co/temporal/28a290fa-087f-447b-934c-2960e...</a></p><p>Developer Success Engineer - Read more and apply here  <a href="https://jobs.lever.co/temporal/ff1b33fe-fd12-4d8c-a266-4b1a83295182" rel="nofollow noreferrer">https://jobs.lever.co/temporal/ff1b33fe-fd12-4d8c-a266-4b1a8...</a></p><p>For all employees, we offer: competitive salary, stock options, fully covered premiums for medical, dental (and ortho), vision, and life insurance benefits, HSA, 401K, unlimited time-off, work from home perks, monthly wellness / food $ allowance, an access pass to a WeWork location if you so choose.Send resume to careers AT temporal.io or apply here <a href="https://www.temporal.io/careers/" rel="nofollow noreferrer">https://www.temporal.io/careers/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574770"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36574770" href="https://news.ycombinator.com/vote?id=36574770&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>JetBrains | Software Developer (Kotlin Multiplatform Tooling) | Munich, Berlin, Amsterdam, Prague, Cyprus, … | Onsite, Hybrid | Visa | Full-time<p>Are you an ambitious and talented software developer with a passion for creating world-class developer tools? If so, we have an exciting opportunity for you at JetBrains, a rapidly growing software development company that's on a mission to build the best development tools in the world. Our products are used by millions of developers worldwide, helping them to be more productive and achieve better results faster.</p><p>We are looking for a motivated software developer to join our Kotlin Multiplatform Tooling team, working on innovative Integrated Development Environments (IDEs). You will have the opportunity to be at the forefront of software development and make your mark in the industry. You will collaborate with the team to design, implement, and improve our tooling for Kotlin, Swift, and Compose on Android, iOS, and desktop platforms, crafting a cohesive development experience for Kotlin Multiplatform developers.</p><p>At JetBrains, we understand the importance of a supportive and empowering workplace culture for software developers. Our company was founded by developers, for developers, and we strive to create an environment where you can do your best work, learn from your peers, and grow in your career. We encourage innovation, engage with our customers to better understand their needs, and value diversity, teamwork, and open communication.</p><p>We are committed to your professional growth, offering extensive mentorship opportunities, attendance at technical conferences, and various career advancement paths to align with your goals and strengths. We believe in a healthy work-life balance and provide flexible schedules, remote working opportunities, and a supportive work environment that encourages you to bring your whole self to work.</p><p>Our development process is fully transparent, and anyone can participate in any task. All team members communicate with our users, discuss newly proposed features, take part in troubleshooting, and provide bug reports. We are in constant communication with each other, both in person and via Slack.</p><p>Our source code is stored in Git repositories, with most of it written in Java and Kotlin, while low-level libraries use Rust and C/C++. All changes are covered by automated tests, code is peer-reviewed, and then extensively tested by QA to maintain high product quality and avoid regressions.</p><p>JetBrains has a strong presence in seven R&amp;D centers across Europe, and our tools are used by 99 of the Fortune 100 companies. We offer a competitive salary, excellent opportunities for professional and personal growth, 6 weeks of paid annual leave, relocation assistance, visa sponsorship, and many other benefits.</p><p>To apply and learn more about what we offer, check out our job posting for this position [0] and the many more we have open for all kinds of roles. If you have any questions at all, feel free to contact me directly at florian.kistner+hn@jetbrains.com.</p><p>Join our team and help us build the tools that empower developers to do their best work. We look forward to welcoming you to JetBrains!</p><p>[0]: <a href="https://www.jetbrains.com/careers/jobs/software-developer-kotlin-multiplatform-tooling-1061/" rel="nofollow noreferrer">https://www.jetbrains.com/careers/jobs/software-developer-ko...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36575415"><td></td></tr>
            <tr id="36575076"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36575076" href="https://news.ycombinator.com/vote?id=36575076&amp;how=up&amp;goto=item%3Fid%3D36573871"></a></center>    </td><td><p><span>TheNotCompany | Machine Learning Manager | San Francisco, CA | Hybrid (2-days/week)<p>Apply here: <a href="https://www.linkedin.com/jobs/view/3604523243" rel="nofollow noreferrer">https://www.linkedin.com/jobs/view/3604523243</a> or drop a line to aadit@thenotcompany.com</p><p>Hiring for a Machine Learning Manager to build ML-enabled software for formulators and scientists.</p><p>We are using a combination of Generative AI (jargony, but we started with VAE a few years ago and moving to LLMs now) and human-in-the-loop optimization to create sustainable formulations for CPG food, personal care and more. We've also extended our technology to solve wet-lab R&amp;D problems at the bench and pilot scale for synbio and other tangential industries.</p><p>This is a great role if you like working with scientific data and possess creative problem solving skills to envision how a suite of ML models can form into a comprehensive product for our end users. We work with internal teams of analytical scientists and formulators to collect data (GCMS, HPLC, FTIR) and test AI-generated/screened formulations. Our flagship product, NotMilk, is formulated with pineapple &amp; cabbage, which was suggested by our algorithm.</p><p>The team size is 7 ML Engineers with an overall Eng/Product/Design org of 40 (global). You will be inheriting a strongly built ML team (Stanford, Harvard, Berkeley, PUC Chile)</p><p>401k w/ match, Health/Dental/Vision, 24 days PTO.</p><p>Quick Summary:</p><p>* 7 year old food-tech company.</p><p>* $400M raised to date; last raise of $70M in Dec 2022 @ $1.5B valuation.</p><p>* Entered into a joint venture with Kraft-Heinz to develop their plant-based portfolio in 2022 using our technology and have already launched 2 products.</p><p>* Currently getting sued by the dairy industry in Chile, but we turned it into an ad-campaign and won a Cannes Lions.</p><p>Sites:  
<a href="http://notco.com/" rel="nofollow noreferrer">http://notco.com</a>  
<a href="http://notco.ai/" rel="nofollow noreferrer">http://notco.ai</a></p><p>News:
[1]<a href="https://techcrunch.com/2021/07/26/notco-gets-its-horn-following-235m-round-to-expand-plant-based-food-products/" rel="nofollow noreferrer">https://techcrunch.com/2021/07/26/notco-gets-its-horn-follow...</a>
[2]<a href="https://www.washingtonpost.com/food/2021/06/16/notmilk-non-dairy-drink/" rel="nofollow noreferrer">https://www.washingtonpost.com/food/2021/06/16/notmilk-non-d...</a>
[3]<a href="https://vegnews.com/2023/2/chilean-unicorn-notco-ai-starbucks-dunkin-vegan" rel="nofollow noreferrer">https://vegnews.com/2023/2/chilean-unicorn-notco-ai-starbuck...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36574762"><td></td></tr>
            <tr id="36574709"><td></td></tr>
                <tr id="36574774"><td></td></tr>
                  <tr id="36574826"><td></td></tr>
                <tr id="36574841"><td></td></tr>
                  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FoundationDB: A Distributed Key-Value Store (105 pts)]]></title>
            <link>https://cacm.acm.org/magazines/2023/6/273229-foundationdb-a-distributed-key-value-store/fulltext</link>
            <guid>36572658</guid>
            <pubDate>Mon, 03 Jul 2023 13:34:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cacm.acm.org/magazines/2023/6/273229-foundationdb-a-distributed-key-value-store/fulltext">https://cacm.acm.org/magazines/2023/6/273229-foundationdb-a-distributed-key-value-store/fulltext</a>, See on <a href="https://news.ycombinator.com/item?id=36572658">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="container">



<hr>
<div id="articleFullText">
<p><span>
By Jingyu Zhou, Meng Xu, Alexander Shraer, Bala Namasivayam, Alex Miller, Evan Tschannen, Steve Atherton, Andrew J. Beamon, Rusty Sears, John Leach, Dave Rosenthal, Xin Dong, Will Wilson, Ben Collins, David Scherer, Alec Grieser, Yang Liu, Alvin Moore, Bhaskar Muppana, Xiaoge Su, Vishesh Yadav
<br>
Communications of the ACM,
June 2023,
Vol. 66 No. 6, Pages 97-105<br>
10.1145/3592838<br>
<a href="#comments">Comments</a>
</span></p>


<div id="asset-45537">
<figure>
<img alt="FoundationDB logo" src="https://cacm.acm.org/system/assets/0004/5537/051523_VectorLogo_FoundationDB.large.jpg?1684170206&amp;1684170206" title="FoundationDB logo">
<figcaption>
<p>Credit: Vector Logo Zone</p>
</figcaption>
</figure>
</div>


<p>FoundationDB is an open-source transactional key-value store created more than 10 years ago. It is one of the first systems to combine the flexibility and scalability of NoSQL architectures with the power of ACID transactions. FoundationDB adopts an unbundled architecture that decouples an in-memory transaction management system, a distributed storage system, and a built-in distributed configuration system. Each sub-system can be independently provisioned and configured to achieve scalability, high availability, and fault tolerance. FoundationDB includes a deterministic simulation framework, used to test every new feature under a myriad of possible faults. This rigorous testing makes FoundationDB extremely stable and allows developers to introduce and release new features in a rapid cadence. FoundationDB offers a minimal and carefully chosen feature set, which has enabled a range of disparate systems to be built as layers on top. FoundationDB is the underpinning of cloud infrastructure at Apple, Snowflake, and other companies, due to its consistency, robustness, and availability for storing user data, system metadata and configuration, and other critical information.</p>

<p><a href="#PageTop">Back to Top</a></p>


<h3>1. Introduction</h3>
<p>Many cloud services rely on scalable, distributed storage backends for persisting application state. Such storage systems must be fault tolerant and highly available, and at the same time provide sufficiently strong semantics and flexible data models to enable rapid application development. Such services must scale to billions of users, petabytes or exabytes of stored data, and millions of requests per second.</p>
<p>More than a decade ago, NoSQL storage systems emerged offering ease of application development, making it simple to scale and operate storage systems, offering fault-tolerance and supporting a wide range of data models (instead of the traditional rigid relational model). In order to scale, these systems sacrificed transactional semantics, and instead provided eventual consistency, forcing application developers to reason about interleavings of updates from concurrent operations. </p>
<p>FoundationDB (FDB)<sup><a href="#R3">3</a></sup> was created in 2009 and gets its name from the focus on providing what we saw as the foundational set of building blocks required to build higher-level distributed systems. It is an ordered, transactional, key-value store natively supporting multi-key strictly serializable transactions across its entire key space. Unlike most databases, which bundle together a storage engine, data model, and query language, forcing users to choose all three or none, FDB takes a modular approach: it provides a highly scalable, transactional storage engine with a minimal yet carefully chosen set of features. It provides no structured semantics, no query language, data model or schema management, secondary indices, or many other features one normally finds in a transactional database. Offering these would benefit some applications but others that do not require them (or do so in a slightly different form) would need to work around them. Instead, The NoSQL model leaves application developers with great flexibility. Applications can manage data stored as simple key-value pairs, but at the same time implement advanced features, such as consistent secondary indices and referential integrity checks.<sup><a href="#R10">10</a></sup> FDB defaults to strictly serializable transactions but allows relaxing these semantics for applications that don't require them with flexible, fine-grained controls over conflicts.</p>
<p>One of the reasons for its popularity and growing open source community is FoundationDB's focus on the "lower half" of a database, leaving the rest to its "layers"—stateless applications developed on top to provide various data models and other capabilities. With this, applications that would traditionally require completely different types of storage systems, can instead all leverage FDB. Indeed, the wide range of layers that have been built on FDB in recent years is evidence of the usefulness of this unusual design. For example, the FoundationDB Record Layer<sup><a href="#R10">10</a></sup> adds back much of what users expect from a relational database, and JanusGraph,<sup><a href="#R6">6</a></sup> a graph database, provides an implementation as a FoundationDB layer.<sup><a href="#R5">5</a></sup> In its newest release, CouchDB<sup><a href="#R1">1</a></sup> (arguably the first NoSQL system) is being rebuilt as a layer on top of FoundationDB.</p>
<p>Testing and debugging distributed systems is at least as hard as building them. Unexpected process and network failures, message reorderings, and other sources of non-determinism can expose subtle bugs and implicit assumptions that break in reality, which are extremely difficult to reproduce or debug. The consequences of such subtle bugs are especially severe for database systems, which purport to offer perfect fidelity to an unambiguous contract. Moreover, the stateful nature of a database system means that any such bug can result in subtle data corruption that may not be discovered for months. Model-checking techniques can verify the correctness of distributed protocols but often fall short of checking the actual implementation. Deep bugs,<sup><a href="#R18">18</a></sup> which only happen when multiple crashes or restarts occur in a particular sequence, pose a challenge even for end-to-end testing infrastructure. FDB took a radical approach—before building the database itself, we built a deterministic database simulation framework that can simulate a network of interacting processes and a variety of disk, process, network, and request-level failures and recoveries, all within a single physical process. A syntactic extension to C++, called Flow,<sup><a href="#R2">2</a></sup> was created specifically for this purpose. This rigorous testing in simulation makes FDB extremely stable and allows its developers to introduce new features and releases in a rapid cadence.</p>
<p>FDB adopts an unbundled architecture<sup><a href="#R19">19</a></sup> that comprises a control plane and a data plane. The control plane manages the metadata of the cluster and uses Active Disk Paxos<sup><a href="#R9">9</a></sup> for high availability. The data plane consists of a transaction management system, responsible for processing updates, and a distributed storage layer serving reads; both can be independently scaled out. FDB achieves strict serializability through a combination of optimistic concurrency control (OCC)<sup><a href="#R17">17</a></sup> and multi-version concurrency control (MVCC).<sup><a href="#R8">8</a></sup> One of the features distinguishing FDB from other distributed databases is its approach to handling failures. Unlike most similar systems, FDB does not rely on quorums to mask failures, but rather tries to eagerly detect and recover from them by reconfiguring the system. This allows us to achieve the same level of fault tolerance with significantly fewer resources: FDB can tolerate <em>f</em> failures with only <em>f</em> + 1 (rather than 2<em>f</em> + 1) replicas. This approach is best suited for deployments in a local or metro area. For WAN deployments, FDB offers a novel strategy that avoids cross-region write latencies while providing automatic failover between regions without losing data.</p>
<p>This paper makes three primary contributions. First, we describe an open-source distributed storage system, FoundationDB, combining NoSQL and ACID, used in production at Apple, Snowflake, and other companies. Second, an integrated deterministic simulation framework makes FoundationDB one of the most stable systems of its kind. Third, we describe a unique architecture and approach to transaction processing, fault tolerance, and high availability.</p>

<p><a href="#PageTop">Back to Top</a></p>


<h3>2. Design</h3>
<p>The main design principles of FDB are:</p>
<ul>
<li><em>Divide-and-Conquer (or separation of concerns).</em> FDB decouples the transaction management system (write path) from the distributed storage (read path) and scales them independently. Within the transaction management system, processes are assigned various roles representing different aspects of transaction management. Furthermore, cluster-wide orchestrating tasks, such as overload control and load balancing are also divided and serviced by additional heterogeneous roles.</li>
<li><em>Make failure a common case.</em> For distributed systems, failure is a norm rather than an exception. To cope with failures in the transaction management system of FDB, we handle all failures through the recovery path: the transaction system proactively shuts down when it detects a failure. Thus, all failure handling is reduced to a single recovery operation, which becomes a common and well-tested code path. To improve availability, FDB strives to minimize Mean-Time-To-Recovery (MTTR). In our production clusters, the total time is usually less than five seconds.</li>
<li><em>Simulation testing.</em> FDB relies on a randomized, deterministic simulation framework for testing the correctness of its distributed database. Simulation tests not only expose deep bugs,<sup><a href="#R18">18</a></sup> but also boost developer productivity and the code quality of FDB.</li>
</ul>
<p><img alt="*" src="http://dl.acm.org/images/bullet.gif">&nbsp;<strong>2.1. Architecture</strong></p>
<p>An FDB cluster has a control plane for managing critical system metadata and cluster-wide orchestration, and a data plane for transaction processing and data storage, as illustrated in <a href="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f1.jpg" onclick="window.open(this.href, '', 'resizable=yes,status=no,location=no,toolbar=no,menubar=no,fullscreen=no,scrollbars=no,dependent=no,width=681,height=763'); return false;">Figure 1</a>.</p>
<p><a href="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f1.jpg" onclick="window.open(this.href, '', 'resizable=yes,status=no,location=no,toolbar=no,menubar=no,fullscreen=no,scrollbars=no,dependent=no,width=681,height=763'); return false;"><img alt="f1.jpg" height="465" src="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f1.jpg" width="415"></a><br>
<strong>Figure 1. Architecture and transaction processing.</strong></p>
<p><strong>Control plane.</strong> The control plane is responsible for persisting critical system metadata, that is, the configuration of transaction systems, on <code>Coordinators.</code> These <code>Coordinators</code> form a Paxos group<sup><a href="#R9">9</a></sup> and elect a <code>ClusterController.</code> The <code>ClusterController</code> monitors all servers in the cluster and recruits three processes, <code>Sequencer</code> (described in Section 2.1.2), <code>DataDistributor</code>, and <code>Ratekeeper</code>, which are re-recruited if they fail or crash. The <code>DataDistributor</code> is responsible for monitoring failures and balancing data among <code>StorageServers. Ratekeeper</code> provides overload protection for the cluster.</p>
<p><strong>Data plane.</strong> FDB targets OLTP workloads that are read-mostly, read and write a small set of keys per transaction, have low contention, and require scalability. FDB chooses an unbundled architecture<sup><a href="#R19">19</a></sup>: a distributed transaction management system (TS) consists of a <code>Sequencer</code>, <code>Proxies</code>, and <code>Resolvers</code>, all of which are stateless processes. A log system (LS) stores Write-Ahead-Log (WAL) for TS, and a separate distributed storage system (SS) is used for storing data and servicing reads. The LS contains a set of <code>LogServers</code> and the SS has a number of <code>StorageServers.</code> This scales well with Apple's largest transactional workloads.<sup><a href="#R10">10</a></sup></p>
<p>The <code>Sequencer</code> assigns a read and a commit version to each transaction. <code>Proxies</code> offer MVCC read versions to clients and orchestrate transaction commits. <code>Resolvers</code> check for conflicts among transactions. <code>LogServers</code> act as replicated, sharded, distributed persistent queues, each queue storing WAL data for a <code>StorageServer.</code></p>
<p>The SS consists of a number of <code>StorageServers</code>, each storing a set of data shards, that is, contiguous key ranges, and serving client reads. <code>StorageServers</code> are the majority of processes in the system, and together they form a distributed B-tree. Currently, the storage engine on each <code>StorageServer</code> is an enhanced version of SQLite,<sup><a href="#R15">15</a></sup> with enhancements that make range clears faster, defer deletion to a background task, and add support for asynchronous programming.</p>
<p><strong>Read-write separation and scaling.</strong> As mentioned above, processes are assigned different roles; FDB scales by adding new processes for each role. Clients read from sharded <code>StorageServers</code>, so reads scale linearly with the number of <code>StorageServers.</code> Writes are scaled by adding more <code>Proxies</code>, <code>Resolvers</code>, and <code>LogServers.</code> The control plane's singleton processes (e.g., <code>ClusterController</code> and <code>Sequencer</code>) and <code>Coordinators</code> are not performance bottlenecks; they only perform limited metadata operations.</p>
<p><strong>Bootstrapping.</strong> FDB has no dependency on external coordination services. All user data and most system metadata (keys that start with <code>0xFF</code> prefix) are stored in <code>StorageServers.</code> The metadata about <code>StorageServers</code> is persisted in <code>LogServers</code>, and the <code>LogServers</code> configuration data is stored in all <code>Coordinators.</code> The <code>Coordinators</code> are a disk Paxos group; servers attempt to become the <code>ClusterController</code> if one does not exist. A newly elected <code>ClusterController</code> reads the old LS configuration from the <code>Coordinators</code> and spawns a new TS and LS. <code>Proxies</code> recover system metadata from the old LS, including information about all <code>StorageServers.</code> The <code>Sequencer</code> waits until the new TS finishes recovery (Section 2.2.4), then writes the new LS configuration to all <code>Coordinators.</code> The new transaction system is then ready to accept client transactions.</p>
<p><strong>Reconfiguration.</strong> The <code>Sequencer</code> process monitors the health of <code>Proxies</code>, <code>Resolvers</code>, and <code>LogServers.</code> Whenever there is a failure in the TS or LS, or the database configuration changes, the <code>Sequencer</code> terminates. The <code>ClusterController</code> detects the <code>Sequencer</code> failure, then recruits and bootstraps a new TS and LS. In this way, transaction processing is divided into epochs, where each epoch represents a generation of the transaction management system with its own <code>Sequencer.</code></p>
<p><img alt="*" src="http://dl.acm.org/images/bullet.gif">&nbsp;<strong>2.2. Transaction management</strong></p>
<p>This section describes end-to-end transaction processing and strict serializability, then discusses logging and recovery.</p>
<p><strong>End-to-end transaction processing.</strong> As illustrated in <a href="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f1.jpg" onclick="window.open(this.href, '', 'resizable=yes,status=no,location=no,toolbar=no,menubar=no,fullscreen=no,scrollbars=no,dependent=no,width=681,height=763'); return false;">Figure 1</a>, a client transaction starts by contacting one of the <code>Proxies</code> to obtain a read version (i.e., a timestamp). The <code>Proxy</code> then asks the <code>Sequencer</code> for a read version that is at least as large as all previously issued transaction commit versions, and sends this read version back to the client. The client may then issue reads to <code>StorageServers</code> and obtain values at that specific read version. Client writes are buffered locally without contacting the cluster and read-your-write semantics are preserved by combining results from database look-ups with uncommitted writes of the transaction. At commit time, the client sends the transaction data, including the read and write sets (i.e., key ranges), to one of the <code>Proxies</code> and waits for a commit or abort response. If the transaction cannot commit, the client may choose to restart it.</p>
<p>A <code>Proxy</code> commits a client transaction in three steps. First, it contacts the <code>Sequencer</code> to obtain a commit version that is larger than any existing read versions or commit versions. The <code>Sequencer</code> chooses the commit version by advancing it at a rate of one million versions per second. Then, the <code>Proxy</code> sends the transaction information to range-partitioned <code>Resolvers</code>, which implement FDB's optimistic concurrency control by checking for <em>read-write</em> conflicts. If all <code>Resolvers</code> return with no conflict, the transaction can proceed to the final commit stage. Otherwise, the <code>Proxy</code> marks the transaction as aborted. Finally, committed transactions are sent to a set of <code>LogServers</code> for persistence. A transaction is considered committed after all designated <code>LogServers</code> have replied to the <code>Proxy</code>, which reports the committed version to the <code>Sequencer</code> (to ensure that later transactions' read versions are after this commit) and then replies to the client. <code>StorageServers</code> continuously pull mutation logs from <code>LogServers</code> and apply committed updates to disks.</p>
<p>In addition to the above <em>read-write transactions</em>, FDB also supports <em>read-only transactions</em> and <em>snapshot reads.</em> A read-only transaction in FDB is both serializable (happens at the read version) and performant (thanks to the MVCC), and the client can commit these transactions locally without contacting the database. This is particularly important because the majority of transactions are read-only. Snapshot reads in FDB selectively relax the isolation property of a transaction by reducing conflicts, that is, concurrent writes will not conflict with snapshot reads.</p>
<p><strong>Strict serializability.</strong> FDB implements Serializable Snapshot Isolation (SSI) by combining OCC with MVCC. Recall that a transaction <em>T<sub>x</sub></em> gets both its read version and commit version from the <code>Sequencer</code>, where the read version is guaranteed to be no less than any committed version when <em>T<sub>x</sub></em> starts and the commit version is larger than any existing read or commit versions. This commit version defines a serial history for transactions and serves as a Log Sequence Number (LSN). Because <em>T<sub>x</sub></em> observes the results of all previously committed transactions, FDB achieves strict serializability. To ensure there are no gaps between LSNs, the <code>Sequencer</code> returns the previous commit version (i.e., previous LSN) with each commit version. A Proxy sends both LSN and the previous LSN to <code>Resolvers</code> and <code>LogServers</code> so that they can serially process transactions in the order of LSNs. Similarly, <code>StorageServers</code> pull log data from <code>LogServers</code> in increasing LSN order.</p>
<p><code>Resolvers</code> use a lock-free conflict detection algorithm similar to <em>write-snapshot isolation</em>,<sup><a href="#R22">22</a></sup> with the difference that in FDB the commit version is chosen before conflict detection. This allows FDB to efficiently batch-process both version assignments and conflict detection.</p>
<p>The entire key space is divided among <code>Resolvers</code> allowing conflict detection to be performed in parallel. A transaction can commit only when all <code>Resolvers</code> admit the transaction. Otherwise, the transaction is aborted. It is possible that an aborted transaction is admitted by a subset of <code>Resolvers</code>, and they have already updated their history of potentially committed transactions, which may cause other transactions to conflict (i.e., a false positive). In practice, this has not been an issue for our production workloads, because transactions' key ranges usually fall into one <code>Resolver.</code> Additionally, because the modified keys expire after the MVCC window, such false positives are limited to only happen within the short MVCC window time (i.e., 5 seconds).</p>
<p>The OCC design of FDB avoids the complicated logic of acquiring and releasing (logical) locks, which greatly simplifies interactions between the TS and the SS. The price is wasted work done by aborted transactions. In our multitenant production workload transaction conflict rate is very low (less than 1%) and OCC works well. If a conflict happens, the client can simply restart the transaction.</p>
<p><strong>Logging protocol.</strong> After a <code>Proxy</code> decides to commit a transaction, it sends a message to all <code>LogServers</code>: mutations are sent to <code>LogServers</code> responsible for the modified key ranges, while other <code>LogServers</code> receive an empty message body. The log message header includes both the current and previous LSN obtained from the <code>Sequencer</code>, as well as the largest known committed version (KCV) of this <code>Proxy. LogServers</code> reply to the <code>Proxy</code> once the log data is made durable, and the <code>Proxy</code> updates its KCV to the LSN if all replica <code>LogServers</code> have replied and this LSN is larger than the current KCV.</p>
<p>Shipping the redo log from the LS to the SS is not a part of the commit path and is performed in the background. In FDB, <code>StorageServers</code> apply non-durable redo logs from <code>LogServers</code> to an in-memory index. In the common case, this happens before any read versions that reflect the commit is handed out to a client, allowing very low latency for serving multi-version reads. Therefore, when client read requests reach <code>StorageServers</code>, the requested version (i.e., the latest committed data) is usually already available. If fresh data is not available to read at a <code>StorageServer</code> replica, the client either waits for the data to become available or reissues the request at another replica.<sup><a href="#R12">12</a></sup> If both read time out, the client can simply restart the transaction.</p>
<p>Since log data is already durable on <code>LogServers</code>, <code>StorageServers</code> can buffer updates in memory and persist batches of data to disks periodically, thus improving I/O efficiency.</p>
<p><strong>Transaction system recovery.</strong> Traditional database systems often employ the ARIES recovery protocol.<sup><a href="#R20">20</a></sup> During recovery, the system processes redo log records from the last checkpoint by re-applying them to relevant data pages. This brings the database to a consistent state; transactions that were in flight during the crash can be rolled back by executing the undo log records.</p>
<p>In FDB, recovery is purposely made very cheap—there is no need to apply undo log entries. This is possible because of a greatly simplifying design choice: redo log processing is the same as the normal log forward path. In FDB, <code>StorageServers</code> pull logs from <code>LogServers</code> and apply them in the background. The recovery process starts by detecting a failure and recruiting a new transaction system. The new TS can accept transactions before all the data in the old <code>LogServers</code> is processed. Recovery only needs to find out the end of the redo log: At that point (as in normal forward operation) <code>StorageServers</code> asynchronously replay the log.</p>
<p>For each epoch, the <code>ClusterController</code> executes recovery in several steps. First, it reads the previous TS configuration from <code>Coordinators</code> and locks this information to prevent another concurrent recovery. Next, it recovers previous TS system states, including information about older <code>LogServers</code>, stops them from accepting transactions, and recruits a new set of <code>Sequencer</code>, <code>Proxies</code>, <code>Resolvers</code>, and <code>LogServers.</code> After previous <code>LogServers</code> are stopped and a new TS is recruited, the <code>ClusterController</code> writes the new TS information to the <code>Coordinators.</code> Because <code>Proxies</code> and <code>Resolvers</code> are stateless, their recoveries have no extra work. In contrast, <code>LogServers</code> save the logs of committed transactions, and we need to ensure all such transactions are durable and retrievable by <code>StorageServers.</code></p>
<p>The essence of the recovery of old <code>LogServers</code> is to determine the end of the redo log, that is, a Recovery Version (RV). Rolling back undo the log is essentially discarding any data after RV in the old <code>LogServers</code> and <code>StorageServers.</code> <a href="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f2.jpg" onclick="window.open(this.href, '', 'resizable=yes,status=no,location=no,toolbar=no,menubar=no,fullscreen=no,scrollbars=no,dependent=no,width=718,height=274'); return false;">Figure 2</a> illustrates how RV is determined by the <code>Sequencer.</code> Recall that a <code>Proxy</code> request to <code>LogServers</code> piggybacks its KCV, the maximum LSN that this <code>Proxy</code> has committed, along with the LSN of the current transaction. Each <code>LogServer</code> keeps the maximum KCV received and a Durable Version (DV), which is the maximum LSN persisted by the <code>LogServer</code> (DV is ahead of KCV since it corresponds to in-flight transactions). During recovery, the <code>Sequencer</code> attempts to stop all <em>m</em> old <code>LogServers</code>, where each response contains the DV and KCV on that <code>LogServer.</code> Assume the replication degree for <code>LogServers</code> is <em>k.</em> Once the <code>Sequencer</code> has received more than <em>m</em> − <em>k</em> replies, the <code>Sequencer</code> knows the previous epoch has committed transactions up to the maximum of all KCVs, which becomes the previous epoch's end version (PEV). All data before this version has been fully replicated. For the current epoch, its start version is <em>PEV</em> + 1 and the <code>Sequencer</code> chooses the minimum of all DVs to be the RV. Logs in the range of [<em>PEV</em> + 1, <em>RV</em>] are copied from the previous epoch's <code>LogServers</code> to the current ones, for healing the replication degree in case of <code>LogServer</code> failures. The overhead of copying this range is very small because it only contains a few seconds' log data.</p>
<p><a href="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f2.jpg" onclick="window.open(this.href, '', 'resizable=yes,status=no,location=no,toolbar=no,menubar=no,fullscreen=no,scrollbars=no,dependent=no,width=718,height=274'); return false;"><img alt="f2.jpg" height="158" src="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f2.jpg" width="415"></a><br>
<strong>Figure 2. An illustration of RV and PEV.</strong></p>
<p>When <code>Sequencer</code> accepts new transactions, the first is a special recovery transaction that informs <code>StorageServers</code> of the RV so that they can roll back any data larger than the RV. The current FDB storage engine consists of an unversioned SQLite<sup><a href="#R15">15</a></sup> B-tree and in-memory multi-versioned redo log data. Only mutations leaving the MVCC window (i.e., committed data) are written to SQLite. The rollback is simply discarding in-memory multi-versioned data in <code>StorageServers.</code> Then <code>StorageServers</code> pull any data larger than version <em>PEV</em> from new <code>LogServers.</code></p>
<p><img alt="*" src="http://dl.acm.org/images/bullet.gif">&nbsp;<strong>2.3. Replication</strong></p>
<p>FDB uses a combination of various replication strategies for different data to tolerate <em>f</em> failures:</p>
<ul>
<li><em>Metadata replication.</em> System metadata of the control plane is stored on <code>Coordinators</code> using Active Disk Paxos.<sup><a href="#R9">9</a></sup> As long as a quorum (i.e., majority) of <code>Coordinators</code> are live, this metadata can be recovered.</li>
<li><em>Log replication.</em> When a <code>Proxy</code> writes logs to <code>LogServers</code>, each sharded log record is synchronously replicated on <em>k</em> = <em>f</em> + 1 <code>LogServers.</code> Only when all <em>k</em> have replied with successful persistence can the <code>Proxy</code> send back the commit response to the client. Failure of <code>LogServer</code> results in a transaction system recovery.</li>
<li><em>Storage replication.</em> Every shard, that is, a key range, is asynchronously replicated to <em>k</em> = <em>f</em> + 1 <code>StorageServers</code>, which is called a <em>team.</em> A <code>StorageServer</code> usually hosts a number of shards so that its data is evenly distributed across many teams. A failure of a <code>StorageServer</code> triggers <code>DataDistributor</code> to move data from teams containing the failed process to other healthy teams.</li>
</ul>
<p>Note the storage team abstraction is more sophisticated than Copysets.<sup><a href="#R11">11</a></sup> To reduce the chance of data loss due to simultaneous failures, FDB ensures that at most one process in a replica group is placed in a <em>fault domain</em>, for example, a host, rack, or availability zone. Each team is guaranteed to have at least one process live and there is no data loss if any one of the respective fault domains remains available.</p>

<p><a href="#PageTop">Back to Top</a></p>


<h3>3. Simulation Testing</h3>
<p>Testing and debugging distributed systems is a challenging and inefficient process. This problem is particularly acute for FDB—any failure of its strong concurrency control contract can produce almost arbitrary corruption in systems layered on top. Accordingly, an ambitious approach to end-to-end testing was adopted from the beginning: the real database software is run, together with randomized synthetic workloads and fault injection, in a deterministic discrete-event simulation. The harsh simulated environment quickly provokes bugs in the database, and determinism guarantees that every such bug can be reproduced and investigated.</p>
<p><strong>Deterministic simulator.</strong> FDB was built from the ground up to enable this testing approach. All database code is deterministic and multithreaded concurrency is avoided (instead, one database node is deployed per core). <a href="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f3.jpg" onclick="window.open(this.href, '', 'resizable=yes,status=no,location=no,toolbar=no,menubar=no,fullscreen=no,scrollbars=no,dependent=no,width=617,height=314'); return false;">Figure 3</a> illustrates the simulator process of FDB, where all sources of nondeterminism and communication are abstracted, including network, disk, time, and pseudorandom number generator. FDB is written in Flow,<sup><a href="#R2">2</a></sup> a novel syntactic extension to C++ adding async/await-like concurrency primitives with automatic cancellation, permitting highly concurrent code to execute deterministically. Flow provides the Actor programming model<sup><a href="#R7">7</a></sup> that abstracts various actions of the FDB server process into a number of actors that are scheduled by the Flow runtime library. The simulator process is able to spawn multiple FDB servers that communicate with each other through a simulated network in a single discrete-event simulation. The production implementation is a simple shim to the relevant system calls.</p>
<p><a href="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f3.jpg" onclick="window.open(this.href, '', 'resizable=yes,status=no,location=no,toolbar=no,menubar=no,fullscreen=no,scrollbars=no,dependent=no,width=617,height=314'); return false;"><img alt="f3.jpg" height="211" src="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f3.jpg" width="415"></a><br>
<strong>Figure 3. The FDB deterministic simulator.</strong></p>
<p>The simulator runs multiple workloads (written in Flow) that communicate with simulated FDB servers through the simulated network. These workloads include fault injection instructions, mock applications, database configuration changes, and internal database functionality invocations. Workloads are composable to exercise various features and are reused to construct comprehensive test cases.</p>
<p><strong>Test oracles.</strong> FDB uses a variety of test oracles to detect failures in simulation. Most of the synthetic workloads have assertions built in to verify the contracts and properties of the database, for example, by checking invariants in their data that can only be maintained through transaction atomicity and isolation. Assertions are used throughout the code base to check properties that can be verified "locally." Properties like recoverability (eventual availability) can be checked by returning the modeled hardware environment (after a set of failures sufficient to break the database's availability) to a state in which recovery should be possible and verifying that the cluster eventually recovers.</p>
<p><strong>Fault injection.</strong> Simulation injects machine, rack, and data-center failures and reboots, a variety of network faults, partitions, and latency problems, disk behavior (e.g., the corruption of unsynchronized writes when machines reboot), and randomizes event times. This variety of fault injection both tests the database's resilience to specific faults and increases the diversity of states in simulation. Fault injection distributions are carefully tuned to avoid driving the system into a small state space caused by an excessive fault rate.</p>
<p>FDB itself cooperates with the simulation in making rare states and events more common, through a high-level fault injection technique informally referred to as "buggification." At many places in its code base, the simulation is allowed to inject some unusual (but not contract-breaking) behavior such as unnecessarily returning an error from an operation that usually succeeds, injecting a delay in an operation that is usually fast, or choosing an unusual value for a tuning parameter, etcetera. This complements fault injection at the network and hardware levels. Randomization of tuning parameters also ensures that specific performance tuning values do not accidentally become necessary for correctness.</p>
<p>Swarm testing<sup><a href="#R14">14</a></sup> is extensively used to maximize the diversity of simulation runs. Each run uses a random cluster size and configuration, random workloads, random fault injection parameters, random tuning parameters, and enables and disables a random subset of buggification points. We have open-sourced the swarm testing framework for FDB.<sup><a href="#R4">4</a></sup></p>
<p>Conditional coverage macros are used to evaluate and tune the effectiveness of the simulation. For example, a developer concerned that a new piece of code may rarely be invoked with a full buffer can add the line <code>TEST(buffer.is_full())</code>; and analysis of simulation results will tell them how many distinct simulation runs achieved that condition. If the number is too low, or zero, they can add buggification, workload, or fault injection functionality to ensure that scenario is adequately tested.</p>
<p><strong>Latency to bug discovery.</strong> Finding bugs quickly is important both so that they are encountered in testing before production, and for engineering productivity (since bugs found immediately in an individual commit can be trivially traced to that commit). Discrete-event simulation can run arbitrarily faster than real-time if CPU utilization within the simulation is low, as the simulator can fast-forward clock to the next event. Many distributed systems bugs take time to play out, and running simulations with long stretches of low utilization allows many more of these to be found per core second than in "real-world" end-to-end tests.</p>
<p>Additionally, randomized testing is embarrassingly parallel and FDB developers can and do "burst" the amount of testing they do before major releases, in the hopes of catching exceptionally rare bugs that have thus far eluded the testing process. Since the search space is effectively infinite, running more tests results in more code being covered and more potential bugs being found, in contrast to scripted functional or system testing.</p>
<p><strong>Limitations.</strong> Simulation cannot reliably detect performance issues, such as an imperfect load-balancing algorithm. It is also unable to test third-party libraries or dependencies, or even first-party code not implemented in Flow. As a consequence, we have largely avoided taking dependencies on external systems. Finally, bugs in critical dependent systems, such as a filesystem or the operating system, or misunderstandings of their contract, can lead to bugs in FDB. For example, several bugs have resulted from the true operating system contract being weaker than it was believed to be.</p>

<p><a href="#PageTop">Back to Top</a></p>


<h3>4. Evaluation</h3>
<p>This section studies the scalability of FDB and provides some data on the time of reconfiguration.</p>
<p><img alt="*" src="http://dl.acm.org/images/bullet.gif">&nbsp;<strong>4.1. Scalability test</strong></p>
<p>The experiments were conducted on a test cluster of 27 machines in a single data center. Each machine has a 16-core 2.5 GHz Intel Xeon CPU with hyper-threading enabled, 256 GB memory, 8 SSD disks, connected via 10 Gigabit Ethernet. Each machine runs 14 <code>StorageServers</code> on 7 SSD disks and reserves the remaining SSD for <code>LogServer.</code> In the experiments, we use the same number of <code>Proxies</code> and <code>LogServers.</code> The replication degrees for both <code>LogServers</code> and <code>StorageServers</code> are set to three.</p>
<p>We use a synthetic workload to evaluate the performance of FDB. Specifically, there are four types of transactions: (1) <em>blind writes</em> that update a configured number of random keys; (2) <em>range reads</em> that fetch a configured number of continuous keys starting at a random key; (3) <em>point reads</em> that fetch 10 random keys; and (4) <em>point writes</em> that fetch 5 random keys and update another 5 random keys. We use blind writes and range reads to evaluate the write and read performance, respectively. Point reads and point writes are used together to evaluate mixed read-write performance. For instance, 90% reads and 10% writes (90/10 read-write) is constructed with 80% point reads and 20% point writes transactions. Each key is 16 bytes and the value size is uniformly distributed between 8 and 100 bytes (averaging 54 bytes). The database is pre-populated with data using the same size distribution. In the experiments, we make sure the dataset cannot be completely cached in the memory of <code>StorageServers.</code></p>
<p><a href="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f4.jpg" onclick="window.open(this.href, '', 'resizable=yes,status=no,location=no,toolbar=no,menubar=no,fullscreen=no,scrollbars=no,dependent=no,width=690,height=354'); return false;">Figure 4</a> illustrates the scalability test of FDB from 4 to 24 machines using 2 to 22 <code>Proxies</code> or <code>LogServers.</code> <a href="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f4.jpg" onclick="window.open(this.href, '', 'resizable=yes,status=no,location=no,toolbar=no,menubar=no,fullscreen=no,scrollbars=no,dependent=no,width=690,height=354'); return false;">Figure 4a</a> shows that the write throughput scales from 67 to 391 MBps (5.84X) for 100 operations per transaction (T100), and from 73 to 467 MBps (6.40X) for 500 operations per transaction (T500). Note the raw write throughput is three times higher because each write is replicated three times to <code>LogServers</code> and <code>StorageServers. LogServers</code> are CPU saturated at the maximum write throughput. Read throughput increases from 2946 to 10,096 MBps (3.43X) for T100, and from 5055 to 21,830 MBps (4.32X) for T500, where <code>StorageServers</code> are saturated. For both reads and writes, increasing the number of operations in a transaction boosts throughput. However, increasing operations further (e.g., to 1000) doesn't bring significant changes. <a href="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f4.jpg" onclick="window.open(this.href, '', 'resizable=yes,status=no,location=no,toolbar=no,menubar=no,fullscreen=no,scrollbars=no,dependent=no,width=690,height=354'); return false;">Figure 4b</a> shows the operations per second for 90/10 read-write traffic, which increases from 593k to 2779k (4.69X). In this case, <code>Resolvers</code> and <code>Proxies</code> are CPU saturated.</p>
<p><a href="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f4.jpg" onclick="window.open(this.href, '', 'resizable=yes,status=no,location=no,toolbar=no,menubar=no,fullscreen=no,scrollbars=no,dependent=no,width=690,height=354'); return false;"><img alt="f4.jpg" height="213" src="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f4.jpg" width="415"></a><br>
<strong>Figure 4. Scalability test.</strong></p>
<p>The above experiments study saturated performance. <a href="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f5.jpg" onclick="window.open(this.href, '', 'resizable=yes,status=no,location=no,toolbar=no,menubar=no,fullscreen=no,scrollbars=no,dependent=no,width=696,height=274'); return false;">Figure 5</a> illustrates the client performance on a 24-machine cluster with varying operation rates of 90/10 read-write load. This configuration has 2 <code>Resolvers</code>, 22 <code>LogServers</code>, 22 <code>Proxies</code>, and 336 <code>StorageServers.</code> <a href="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f5.jpg" onclick="window.open(this.href, '', 'resizable=yes,status=no,location=no,toolbar=no,menubar=no,fullscreen=no,scrollbars=no,dependent=no,width=696,height=274'); return false;">Figure 5a</a> shows that the throughput scales linearly with more operations per second (Ops) for both reads and writes. For latency, <a href="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f5.jpg" onclick="window.open(this.href, '', 'resizable=yes,status=no,location=no,toolbar=no,menubar=no,fullscreen=no,scrollbars=no,dependent=no,width=696,height=274'); return false;">Figure 5b</a> shows that when Ops is below 100k, the mean latencies remain stable: about 0.35 ms to read a key, 2 ms to commit, and 1ms to get a read version (GRV). Read is a single-hop operation, thus is faster than the two-hop GRV request. The commit request involves multiple hops and persistence to three <code>LogServers</code>, thus higher latency than reads and GRVs. When Ops exceeds 100 k, all these latencies increase because of more queuing time. At 2m Ops, <code>Resolvers</code> and <code>Proxies</code> are saturated. Batching helps to sustain the throughput but commits latency spike to 368 ms due to saturation.</p>
<p><a href="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f5.jpg" onclick="window.open(this.href, '', 'resizable=yes,status=no,location=no,toolbar=no,menubar=no,fullscreen=no,scrollbars=no,dependent=no,width=696,height=274'); return false;"><img alt="f5.jpg" height="163" src="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f5.jpg" width="415"></a><br>
<strong>Figure 5. Throughput and average latency for different operation rates on a 24-machine cluster configuration.</strong></p>
<p><img alt="*" src="http://dl.acm.org/images/bullet.gif">&nbsp;<strong>4.2. Reconfiguration duration</strong></p>
<p>We collected 289 reconfigurations (i.e., transaction system recovery) traces from our production clusters that typically host hundreds of TBs data. Because of the client-facing nature, short reconfiguration time is critical for the high availability of these clusters. <a href="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f6.jpg" onclick="window.open(this.href, '', 'resizable=yes,status=no,location=no,toolbar=no,menubar=no,fullscreen=no,scrollbars=no,dependent=no,width=420,height=271'); return false;">Figure 6</a> illustrates the cumulative distribution function (CDF) of the reconfiguration times. The median and 90-percentile are 3.08 and 5.28 seconds, respectively. The reason for these short recovery times is that they are not bounded by the data or transaction log size, and are only related to the system metadata sizes. During the recovery, read-write transactions were temporarily blocked and were retried after the timeout. However, client reads were not impacted. The causes of these reconfigurations include automatic failure recovery from software or hardware faults, software upgrades, database configuration changes, and the manual mitigation of production issues.</p>
<p><a href="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f6.jpg" onclick="window.open(this.href, '', 'resizable=yes,status=no,location=no,toolbar=no,menubar=no,fullscreen=no,scrollbars=no,dependent=no,width=420,height=271'); return false;"><img alt="f6.jpg" height="268" src="https://dl.acm.org/cms/attachment/html/10.1145/3592838/assets/html/f6.jpg" width="415"></a><br>
<strong>Figure 6. CDF plot for reconfiguration duration in seconds.</strong></p>

<p><a href="#PageTop">Back to Top</a></p>


<h3>5. Lessons Learned</h3>
<p>This section discusses our experience and lessons of FDB.</p>
<p><img alt="*" src="http://dl.acm.org/images/bullet.gif">&nbsp;<strong>5.1. Architecture design</strong></p>
<p>The divide-and-conquer design principle has proven to be an enabling force for flexible cloud deployment, making the database extensible as well as performant. First, separating the transaction system from the storage layer enables greater flexibility in placing and scaling compute and storage resources independently. An added benefit of <code>LogServers</code> is that they are akin to witness replicas; in some of our multi-region production deployments, <code>LogServers</code> significantly reduce the number of <code>StorageServers</code> (full replicas) required to achieve the same high-availability properties. Further, operators are free to place heterogeneous roles of FDB on different server instance types, optimizing for performance and costs. Second, the decoupling design makes it possible to extend the database functionality, such as our ongoing work of supporting RocksDB<sup><a href="#R13">13</a></sup> as a drop-in replacement for the current SQLite engine. Finally, many of the recent performance improvements are specializing functionality as dedicated roles, for example, separating <code>DataDistributor</code> and <code>Ratekeeper</code> from <code>Sequencer</code>, adding storage cache, dividing <code>Proxies</code> into get-read-version Proxy and commit Proxy. This design pattern successfully allows new features and capabilities to be added frequently.</p>
<p><img alt="*" src="http://dl.acm.org/images/bullet.gif">&nbsp;<strong>5.2. Simulation testing</strong></p>
<p>Simulation testing has enabled FDB to maintain a very high development velocity with a small team by shortening the latency between a bug being introduced and a bug being found, and by allowing deterministic reproduction of issues. Adding additional logging, for instance, generally does not affect the deterministic ordering of events, so an exact reproduction is guaranteed. The productivity of this debugging approach is so much higher than normal production debugging, that in the rare circumstances when a bug was first found "in the wild," the debugging process was almost always first to improve the capabilities or the fidelity of the simulation until the issue could be reproduced there, and only then to begin the normal debugging process. Rigorous correctness testing via simulation makes FDB extremely reliable. In the past several years, CloudKit<sup><a href="#R21">21</a></sup> has deployed FDB for more than 0.5M disk years without a single data corruption event.</p>
<p>It is hard to measure the productivity improvements stemming from increased confidence in the testability of the system. On numerous occasions, the FDB team executed ambitious, ground-up rewrites of major subsystems. Without simulation testing, many of these projects would have been deemed too risky or too difficult, and not even attempted.</p>
<p>The success of simulation has led us to continuously push the boundary of what is amenable to simulation testing by eliminating dependencies and reimplementing them ourselves in Flow. For example, early versions of FDB depended on Apache Zookeeper for coordination, which was deleted after real-world fault injection found two independent bugs in Zookeeper (circa 2010) and was replaced by a de novo Paxos implementation written in Flow. No production bugs have ever been reported since.</p>
<p><img alt="*" src="http://dl.acm.org/images/bullet.gif">&nbsp;<strong>5.3. Fast recovery</strong></p>
<p>Fast recovery is not only useful for improving availability but also greatly simplifies software upgrades and configuration changes and makes them faster. The traditional wisdom of upgrading a distributed system is to perform rolling upgrades so that rollback is possible when something goes wrong. The duration of rolling upgrades can last from hours to days. In contrast, FoundationDB upgrades can be performed by restarting all processes at the same time, which usually finishes within a few seconds. Because this upgrade path has been extensively tested in simulation, all upgrades in Apple's production clusters are performed in this way. Additionally, this upgrade path simplifies protocol compatibility between different versions—we only need to make sure on-disk data is compatible. There is no need to ensure the compatibility of RPC protocols between different software versions.</p>
<p>An interesting discovery is that fast recovery sometimes can automatically heal latent bugs, which is similar to software rejuvenation.<sup><a href="#R16">16</a></sup> For instance, after we separated the <code>DataDistributor</code> role from the <code>Sequencer</code>, we were surprised to discover several unknown bugs in the <code>DataDistributor.</code> This is because, before the change, <code>DataDistributor</code> is restarted with <code>Sequencer</code>, which effectively reinitializes and heals the states of the <code>DataDistributor.</code> After the separation, we made <code>DataDistributor</code> a long-running process independent of transaction system recovery (including <code>Sequencer</code> restart). As a result, the erroneous states of the <code>DataDistributor</code> are never healed and cause test failures.</p>
<p><img alt="*" src="http://dl.acm.org/images/bullet.gif">&nbsp;<strong>5.4. 5s MVCC window</strong></p>
<p>FDB chooses a 5-second MVCC window to limit the memory usage of the transaction system and storage servers because the multi-version data is stored in the memory of <code>Resolvers</code> and <code>StorageServers</code>, which in turn restricts transaction sizes. From our experience, this 5s window is long enough for the majority of OLTP use cases. If a transaction exceeds the time limit, it is often the case that the client application is doing something inefficient, for example, issuing reads one by one instead of parallel reads. As a result, exceeding the time limit often exposes inefficiency in the application.</p>
<p>For some transactions that may span more than 5s, many can be divided into smaller transactions. For instance, the continuous backup process of FDB will scan through the key space and create snapshots of key ranges. Because of the 5s limit, the scanning process is divided into a number of smaller ranges so that each range can be performed within 5s. In fact, this is a common pattern: one transaction creates a number of jobs and each job can be further divided or executed in a transaction. FDB has implemented such a pattern in an abstraction called <code>TaskBucket</code> and the backup system heavily depends on it.</p>

<p><a href="#PageTop">Back to Top</a></p>


<h3>6. Conclusion</h3>
<p>FoundationDB is a key value store designed for OLTP cloud services. The main idea is to decouple transaction processing from logging and storage. Such an unbundled architecture enables the separation and horizontal scaling of both read and write handling. The transaction system combines OCC and MVCC to ensure strict serializability. The decoupling of logging and the determinism in transaction orders greatly simplify recovery, thus, allowing unusually quick recovery time and improving availability. Finally, deterministic and randomized simulation has ensured the correctness of the database implementation.</p>
<p><img alt="*" src="http://dl.acm.org/images/bullet.gif">&nbsp;<strong>Acknowledgments</strong></p>
<p>FoundationDB is still an ongoing effort. We thank Zhe Wu, He Liu, Dan Lambright, Hao Fu, Neethu H. Bingi, Renxuan Wang, Sreenath Bodagala, Yao Xiao, Aaron Molitor, our SRE team, and the Snowflake team for their continuous improvement to the project.</p>

<p><a href="#PageTop">Back to Top</a></p>

<div id="article-references"><h3>References</h3>
<p><a name="R1"></a>1. CouchDB. <a href="https://couchdb.apache.org/">https://couchdb.apache.org/</a>.</p>
<p><a name="R2"></a>2. Flow. <a href="https://github.com/apple/foundationdb/tree/master/flow">https://github.com/apple/foundationdb/tree/master/flow</a>.</p>
<p><a name="R3"></a>3. FoundationDB. <a href="https://github.com/apple/foundationdb">https://github.com/apple/foundationdb</a>.</p>
<p><a name="R4"></a>4. FoundationDB Joshua. <a href="https://github.com/FoundationDB/fdb-joshua">https://github.com/FoundationDB/fdb-joshua</a>.</p>
<p><a name="R5"></a>5. Foundationdb storage adapter for janusgraph. <a href="https://github.com/JanusGraph/janusgraph-foundationdb">https://github.com/JanusGraph/janusgraph-foundationdb</a>.</p>
<p><a name="R6"></a>6. Janusgraph. <a href="https://janusgraph.org/">https://janusgraph.org/</a>.</p>
<p><a name="R7"></a>7. Agha, G. <em>Actors: A Model of Concurrent Computation in Distributed Systems.</em> MIT Press Cambridge, MA, USA, 1986.</p>
<p><a name="R8"></a>8. Bernstein, P.A., Hadzilacos, V., Goodman, N. <em>Concurrency Control and Recovery in Database Systems.</em> Addison-Wesley Boston, MA, USA, 1987.</p>
<p><a name="R9"></a>9. Chockler, G., Malkhi, D. Active disk paxos with infinitely many processes. In <em>ACM PODC</em> (2002).</p>
<p><a name="R10"></a>10. Chrysafis, C., Collins, B., Dugas, S., Dunkelberger, J., Ehsan, M., Gray, S., et al. FoundationDB record layer: A multi-tenant structured datastore. In <em>ACM SIGMOD</em> (2019).</p>
<p><a name="R11"></a>11. Cidon, A., Rumble, S., Stutsman, R., Katti, S., Ousterhout, J., Rosenblum, M. Copysets: Reducing the frequency of data loss in cloud storage. In <em>USENIX Annual Technical Conference</em> (2013).</p>
<p><a name="R12"></a>12. Dean, J., Barroso, L.A. The tail at scale. <em>Commun. ACM 56</em>, 2 (Feb. 2013), 74–80.</p>
<p><a name="R13"></a>13. Facebook. Rocksdb. <a href="https://rocksdb.org/">https://rocksdb.org</a>.</p>
<p><a name="R14"></a>14. Groce, A., Zhang, C., Eide, E., Chen, Y., Regehr, J. Swarm testing. In <em>ACM ISSTA</em> (2012).</p>
<p><a name="R15"></a>15. Hipp, R.D. SQLite. 2020. <a href="https://www.sqlite.org/index.html">https://www.sqlite.org/index.html</a>.</p>
<p><a name="R16"></a>16. Huang, Y., Kintala, C., Kolettis, N., Fulton, N.D. Software rejuvenation: Analysis, module and applications. In <em>Twenty-Fifth International Symposium on Fault-Tolerant Computing</em> (1995), 381–390.</p>
<p><a name="R17"></a>17. Kung, H.T., Robinson, J.T. On optimistic methods for concurrency control. <em>ACM Trans. Database Syst. 6</em>, 2 (1981), 213–226.</p>
<p><a name="R18"></a>18. Leesatapornwongsa, T., Hao, M., Joshi, P., Lukman, J.F., Gunawi, H.S. Samc: Semantic-aware model checking for fast discovery of deep bugs in cloud systems. In <em>USENIX OSDI</em> (2014).</p>
<p><a name="R19"></a>19. Lomet, D., Fekete, A., Weikum, G., Zwilling, M.J. Unbundling transaction services in the cloud. In <em>CIDR</em> (2009).</p>
<p><a name="R20"></a>20. Mohan, C., Haderle, D., Lindsay, B.G., Pirahesh, H., Schwarz, P.M. Aries: A transaction recovery method supporting fine-granularity locking and partial rollbacks using write-ahead logging. <em>ACM Trans. Database Syst. 17</em>, 1 (1992), 94–162.</p>
<p><a name="R21"></a>21. Shraer, A., Aybes, A., Davis, B., Chrysafis, C., Browning, D., Krugler, E., et al. Cloudkit: Structured storage for mobile applications. <em>Proc. VLDB Endow. 11</em>, 5 (Jan. 2018) 540–552.</p>
<p><a name="R22"></a>22. Yabandeh, M., Gómez Ferro, D. A Critique of snapshot isolation. In <em>Proceedings of the 7<sup>th</sup> ACM European Conference on Computer Systems, EuroSys'12</em>, Bern, Switzerland 2012, 155–168.</p>
</div>

<p><a href="#PageTop">Back to Top</a></p>

<div id="article-authorinfo"><h3>Authors</h3>
<p><strong>Jingyu Zhou</strong> (<a href="mailto:jingyu_zhou@apple.com">jingyu_zhou@apple.com</a>), Apple Inc., Cupertino, CA, USA.</p>
<p><strong>Meng Xu</strong> (<a href="mailto:meng_xu@apple.com">meng_xu@apple.com</a>), Apple Inc., Cupertino, CA, USA.</p>
<p><strong>Alexander Shraer</strong> (<a href="mailto:alexander@cockroachlabs.com">alexander@cockroachlabs.com</a>), Cockroach Labs, New York, NY, USA.</p>
<p><strong>Bala Namasivayam</strong> (<a href="mailto:bnamasivayam@apple.com">bnamasivayam@apple.com</a>), Apple Inc., Cupertino, CA, USA.</p>
<p><strong>Alex Miller</strong> (<a href="mailto:alex.r.miller@snowflake.com">alex.r.miller@snowflake.com</a>), Snowflake Inc., San Mateo, CA, USA.</p>
<p><strong>Evan Tschannen</strong> (<a href="mailto:evan.tschannen@snowflake.com">evan.tschannen@snowflake.com</a>), Snowflake Inc., San Mateo, CA, USA.</p>
<p><strong>Steve Atherton</strong> (<a href="mailto:steve.atherton@snowflake.com">steve.atherton@snowflake.com</a>), Snowflake Inc., San Mateo, CA, USA.</p>
<p><strong>Andrew J. Beamon</strong> (<a href="mailto:aj.beamon@snowflake.com">aj.beamon@snowflake.com</a>), Snowflake Inc., San Mateo, CA, USA.</p>
<p><strong>Rusty Sears</strong> (<a href="mailto:russell_sears@apple.com">russell_sears@apple.com</a>), Apple Inc., Cupertino, CA, USA.</p>
<p><strong>John Leach</strong> (<a href="mailto:john_leach@apple.com">john_leach@apple.com</a>), Apple Inc., Cupertino, CA, USA.</p>
<p><strong>Dave Rosenthal</strong> (<a href="mailto:daver@apple.com">daver@apple.com</a>), Apple Inc., Cupertino, CA, USA.</p>
<p><strong>Xin Dong</strong> (<a href="mailto:xind@apple.com">xind@apple.com</a>), Apple Inc., Cupertino, CA, USA.</p>
<p><strong>Will Wilson</strong> (<a href="mailto:will.wilson@antithesis.com">will.wilson@antithesis.com</a>), Vienna, VA, USA.</p>
<p><strong>Ben Collins</strong> (<a href="mailto:ben.collins@antithesis.com">ben.collins@antithesis.com</a>), Vienna, VA, USA.</p>
<p><strong>David Scherer</strong> (<a href="mailto:david.scherer@antithesis.com">david.scherer@antithesis.com</a>), Vienna, VA, USA.</p>
<p><strong>Alec Grieser</strong> (<a href="mailto:agrieser@apple.com">agrieser@apple.com</a>), Apple Inc., Cupertino, CA, USA.</p>
<p><strong>Yang Liu</strong> (<a href="mailto:yliu68@apple.com">yliu68@apple.com</a>), Apple Inc., Cupertino, CA, USA.</p>
<p><strong>Alvin Moore</strong> (<a href="mailto:alvinm@apple.com">alvinm@apple.com</a>), Apple Inc., Cupertino, CA, USA.</p>
<p><strong>Bhaskar Muppana</strong> (<a href="mailto:g.muppana@snowflake.com">g.muppana@snowflake.com</a>), Snowflake Inc., San Mateo, CA, USA.</p>
<p><strong>Xiaoge Su</strong> (<a href="mailto:xiaoge_su@apple.com">xiaoge_su@apple.com</a>), Apple Inc., Cupertino, CA, USA.</p>
<p><strong>Vishesh Yadav</strong> (<a href="mailto:vishesh_yadav@apple.com">vishesh_yadav@apple.com</a>), Apple Inc., Cupertino, CA, USA.</p>
</div>

<p><a href="#PageTop">Back to Top</a></p>

<div id="article-footnotes"><h3>Footnotes</h3>
<p>To view the accompanying Technical Perspective, visit <a href="http://doi.acm.org/10.1145/3592837/html">doi.acm.org/10.1145/3592837</a></p>
<p>The original version of this paper, entitled "FoundationDB: A Distributed Unbundled Transactional Key Value Store," was published in SIGMOD '21, June 20–25, 2021, Virtual Event, China; <a href="https://doi.org/10.1145/3448016.3457559">https://doi.org/10.1145/3448016.3457559</a></p>
<p>Before 7.1 release, the <code>ClusterController</code> delegates this work to the new <code>Sequencer.</code></p>
</div>

<div id="article-permission">
<hr><p>Copyright held by authors/owners. Publication rights licensed to ACM.<br>
Request permission to publish from <a href="mailto:permissions@acm.org">permissions@acm.org</a></p>
</div>

<p>The Digital Library is published by the Association for Computing Machinery. Copyright&nbsp;©&nbsp;2023 ACM, Inc.</p>


<hr>

<p>
No entries found
</p>

</div>

<a href="https://cacm.acm.org/magazines/2023/6/273229-foundationdb-a-distributed-key-value-store/fulltext"></a>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Self-Driving Cars Are Surveillance Cameras on Wheels (206 pts)]]></title>
            <link>https://www.schneier.com/blog/archives/2023/07/self-driving-cars-are-surveillance-cameras-on-wheels.html</link>
            <guid>36572401</guid>
            <pubDate>Mon, 03 Jul 2023 13:11:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.schneier.com/blog/archives/2023/07/self-driving-cars-are-surveillance-cameras-on-wheels.html">https://www.schneier.com/blog/archives/2023/07/self-driving-cars-are-surveillance-cameras-on-wheels.html</a>, See on <a href="https://news.ycombinator.com/item?id=36572401">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-67500">

	<div>

		
		<p>Police are <a href="https://www.bloomberg.com/news/articles/2023-06-29/self-driving-car-video-from-waymo-cruise-give-police-crime-evidence?sref=P6Q0mxvj">already using</a> self-driving car footage as video evidence:</p>
<blockquote><p>While security cameras are commonplace in American cities, self-driving cars represent a new level of access for law enforcement ­ and a new method for encroachment on privacy, advocates say. Crisscrossing the city on their routes, self-driving cars capture a wider swath of footage. And it’s easier for law enforcement to turn to one company with a large repository of videos and a dedicated response team than to reach out to all the businesses in a neighborhood with security systems.</p>
<p>“We’ve known for a long time that they are essentially surveillance cameras on wheels,” said Chris Gilliard, a fellow at the Social Science Research Council. “We’re supposed to be able to go about our business in our day-to-day lives without being surveilled unless we are suspected of a crime, and each little bit of this technology strips away that ability.”</p>
<p>[…]</p>
<p>While self-driving services like Waymo and Cruise have yet to achieve the same level of market penetration as Ring, the wide range of video they capture while completing their routes presents other opportunities. In addition to the San Francisco homicide, Bloomberg’s review of court documents shows police have sought footage from Waymo and Cruise to help solve hit-and-runs, burglaries, aggravated assaults, a fatal collision and an attempted kidnapping.</p>
<p>In all cases reviewed by Bloomberg, court records show that police collected footage from Cruise and Waymo shortly after obtaining a warrant. In several cases, Bloomberg could not determine whether the recordings had been used in the resulting prosecutions; in a few of the cases, law enforcement and attorneys said the footage had not played a part, or was only a formality. However, video evidence has become a lynchpin of criminal cases, meaning it’s likely only a matter of time.</p></blockquote>

		
			<p>
				<span>Tags: <a href="https://www.schneier.com/tag/cars/" rel="tag">cars</a>, <a href="https://www.schneier.com/tag/crime/" rel="tag">crime</a>, <a href="https://www.schneier.com/tag/law-enforcement/" rel="tag">law enforcement</a>, <a href="https://www.schneier.com/tag/privacy/" rel="tag">privacy</a>, <a href="https://www.schneier.com/tag/surveillance/" rel="tag">surveillance</a></span>			</p>

		
		
		<p>
			<a href="https://www.schneier.com/blog/archives/2023/07/self-driving-cars-are-surveillance-cameras-on-wheels.html" rel="bookmark">Posted on July 3, 2023 at 7:04 AM</a>			•
			<a href="https://www.schneier.com/blog/archives/2023/07/self-driving-cars-are-surveillance-cameras-on-wheels.html#comments">4 Comments</a>		</p>

		
	</div>

</article><p id="powered">Sidebar photo of Bruce Schneier by Joe MacInnis.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Data-Oriented Design (2018) (198 pts)]]></title>
            <link>https://www.dataorienteddesign.com/dodbook/dodmain.html</link>
            <guid>36571110</guid>
            <pubDate>Mon, 03 Jul 2023 10:41:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dataorienteddesign.com/dodbook/dodmain.html">https://www.dataorienteddesign.com/dodbook/dodmain.html</a>, See on <a href="https://news.ycombinator.com/item?id=36571110">Hacker News</a></p>
<div id="readability-page-1" class="page">


<!--End of Navigation Panel-->
<tt>Online release of Data-Oriented Design : <br>This is the free, online, reduced version. Some inessential chapters are excluded from this version, but in the spirit of this being an education resource, the essentials are present for anyone wanting to learn about data-oriented design.<br>Expect some odd formatting and some broken images and listings as this is auto generated and the Latex to html converters available are not perfect. If the source code listing is broken, you should be able to find the referenced source on <a href="https://github.com/raspofabs/dodbooksourcecode/">github</a>. If you like what you read here, consider purchasing the real paper book from <a href="https://www.amazon.com/dp/1916478700">here</a>, as not only will it look a lot better, but it will help keep this version online for those who cannot afford to buy it. Please send any feedback to <a href="mailto:support@dataorienteddesign.com">support@dataorienteddesign.com</a></tt>


<p><strong>Richard Fabian</strong>
</p>
<hr>

<br><hr>
<!--Table of Child-Links-->
<a name="CHILD_LINKS"></a>

<ul>
<li><a name="tex2html46" href="https://www.dataorienteddesign.com/dodbook/node1.html">Contents</a>
</li><li><a name="tex2html47" href="https://www.dataorienteddesign.com/dodbook/node2.html">Data-Oriented Design</a>
<ul>
<li><a name="tex2html48" href="https://www.dataorienteddesign.com/dodbook/node2.html#SECTION00210000000000000000">It's all about the data</a>
</li><li><a name="tex2html49" href="https://www.dataorienteddesign.com/dodbook/node2.html#SECTION00220000000000000000">Data is not the problem domain</a>
</li><li><a name="tex2html50" href="https://www.dataorienteddesign.com/dodbook/node2.html#SECTION00230000000000000000">Data and statistics</a>
</li><li><a name="tex2html51" href="https://www.dataorienteddesign.com/dodbook/node2.html#SECTION00240000000000000000">Data can change</a>
</li><li><a name="tex2html52" href="https://www.dataorienteddesign.com/dodbook/node2.html#SECTION00250000000000000000">How is data formed?</a>
</li><li><a name="tex2html53" href="https://www.dataorienteddesign.com/dodbook/node2.html#SECTION00260000000000000000">The framework</a>
</li><li><a name="tex2html54" href="https://www.dataorienteddesign.com/dodbook/node2.html#SECTION00270000000000000000">Conclusions and takeaways</a>
</li></ul>
<br>
</li><li><a name="tex2html55" href="https://www.dataorienteddesign.com/dodbook/node3.html">Relational Databases</a>
<ul>
<li><a name="tex2html56" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00310000000000000000">Complex state</a>
</li><li><a name="tex2html57" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00320000000000000000">The framework</a>
</li><li><a name="tex2html58" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00330000000000000000">Normalising your data</a>
</li><li><a name="tex2html59" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00340000000000000000">Normalisation</a>
<ul>
<li><a name="tex2html60" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00341000000000000000">Primary keys</a>
</li><li><a name="tex2html61" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00342000000000000000">1<sup>st</sup> Normal Form</a>
</li><li><a name="tex2html62" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00343000000000000000">2<sup>nd</sup> Normal Form</a>
</li><li><a name="tex2html63" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00344000000000000000">3<sup>rd</sup> Normal Form</a>
</li><li><a name="tex2html64" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00345000000000000000">Boyce-Codd Normal Form</a>
</li><li><a name="tex2html65" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00346000000000000000">Domain Key / Knowledge</a>
</li><li><a name="tex2html66" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00347000000000000000">Reflections</a>
</li></ul>
</li><li><a name="tex2html67" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00350000000000000000">Operations</a>
</li><li><a name="tex2html68" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00360000000000000000">Summing up</a>
</li><li><a name="tex2html69" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00370000000000000000">Stream Processing</a>
</li><li><a name="tex2html70" href="https://www.dataorienteddesign.com/dodbook/node3.html#SECTION00380000000000000000">Why does database technology matter?</a>
</li></ul>
<br>
</li><li><a name="tex2html71" href="https://www.dataorienteddesign.com/dodbook/node4.html">Existential Processing</a>
<ul>
<li><a name="tex2html72" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00410000000000000000">Complexity</a>
</li><li><a name="tex2html73" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00420000000000000000">Debugging</a>
</li><li><a name="tex2html74" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00430000000000000000">Why use an if</a>
</li><li><a name="tex2html75" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00440000000000000000">Types of processing</a>
</li><li><a name="tex2html76" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00450000000000000000">Don't use booleans</a>
</li><li><a name="tex2html77" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00460000000000000000">Don't use enums quite as much</a>
</li><li><a name="tex2html78" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00470000000000000000">Prelude to polymorphism</a>
</li><li><a name="tex2html79" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00480000000000000000">Dynamic runtime polymorphism</a>
</li><li><a name="tex2html80" href="https://www.dataorienteddesign.com/dodbook/node4.html#SECTION00490000000000000000">Event handling</a>
</li></ul>
<br>
</li><li><a name="tex2html81" href="https://www.dataorienteddesign.com/dodbook/node5.html">Component Based Objects</a>
<ul>
<li><a name="tex2html82" href="https://www.dataorienteddesign.com/dodbook/node5.html#SECTION00510000000000000000">Components in the wild</a>
</li><li><a name="tex2html83" href="https://www.dataorienteddesign.com/dodbook/node5.html#SECTION00520000000000000000">Away from the hierarchy</a>
</li><li><a name="tex2html84" href="https://www.dataorienteddesign.com/dodbook/node5.html#SECTION00530000000000000000">Towards managers</a>
</li><li><a name="tex2html85" href="https://www.dataorienteddesign.com/dodbook/node5.html#SECTION00540000000000000000">There is no entity</a>
</li></ul>
<br>
</li><li><a name="tex2html86" href="https://www.dataorienteddesign.com/dodbook/node6.html">Hierarchical Level of Detail</a>
<ul>
<li><a name="tex2html87" href="https://www.dataorienteddesign.com/dodbook/node6.html#SECTION00610000000000000000">Existence</a>
</li><li><a name="tex2html88" href="https://www.dataorienteddesign.com/dodbook/node6.html#SECTION00620000000000000000">Mementos</a>
</li><li><a name="tex2html89" href="https://www.dataorienteddesign.com/dodbook/node6.html#SECTION00630000000000000000">JIT mementos</a>
</li><li><a name="tex2html90" href="https://www.dataorienteddesign.com/dodbook/node6.html#SECTION00640000000000000000">Alternative axes</a>
<ul>
<li><a name="tex2html91" href="https://www.dataorienteddesign.com/dodbook/node6.html#SECTION00641000000000000000">The true measure</a>
</li><li><a name="tex2html92" href="https://www.dataorienteddesign.com/dodbook/node6.html#SECTION00642000000000000000">Beyond space</a>
</li></ul>
</li><li><a name="tex2html93" href="https://www.dataorienteddesign.com/dodbook/node6.html#SECTION00650000000000000000">Collective LOD</a>
</li></ul>
<br>
</li><li><a name="tex2html94" href="https://www.dataorienteddesign.com/dodbook/node7.html">Searching</a>
<ul>
<li><a name="tex2html95" href="https://www.dataorienteddesign.com/dodbook/node7.html#SECTION00710000000000000000">Indexes</a>
</li><li><a name="tex2html96" href="https://www.dataorienteddesign.com/dodbook/node7.html#SECTION00720000000000000000">Data-oriented Lookup</a>
</li><li><a name="tex2html97" href="https://www.dataorienteddesign.com/dodbook/node7.html#SECTION00730000000000000000">Finding low and high</a>
</li><li><a name="tex2html98" href="https://www.dataorienteddesign.com/dodbook/node7.html#SECTION00740000000000000000">Finding random</a>
</li></ul>
<br>
</li><li><a name="tex2html99" href="https://www.dataorienteddesign.com/dodbook/node8.html">Sorting</a>
<ul>
<li><a name="tex2html100" href="https://www.dataorienteddesign.com/dodbook/node8.html#SECTION00810000000000000000">Do you need to?</a>
</li><li><a name="tex2html101" href="https://www.dataorienteddesign.com/dodbook/node8.html#SECTION00820000000000000000">Maintaining</a>
</li><li><a name="tex2html102" href="https://www.dataorienteddesign.com/dodbook/node8.html#SECTION00830000000000000000">Sorting for your platform</a>
</li></ul>
<br>
</li><li><a name="tex2html103" href="https://www.dataorienteddesign.com/dodbook/node9.html">Optimisations</a>
<ul>
<li><a name="tex2html104" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00910000000000000000">When should we optimise?</a>
</li><li><a name="tex2html105" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00920000000000000000">Feedback</a>
<ul>
<li><a name="tex2html106" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00921000000000000000">Know your limits</a>
</li></ul>
</li><li><a name="tex2html107" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00930000000000000000">A strategy</a>
<ul>
<li><a name="tex2html108" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00931000000000000000">Define the problem</a>
</li><li><a name="tex2html109" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00932000000000000000">Measure</a>
</li><li><a name="tex2html110" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00933000000000000000">Analyse</a>
</li><li><a name="tex2html111" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00934000000000000000">Implement</a>
</li><li><a name="tex2html112" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00935000000000000000">Confirm</a>
</li><li><a name="tex2html113" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00936000000000000000">Summary</a>
</li></ul>
</li><li><a name="tex2html114" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00940000000000000000">Tables</a>
</li><li><a name="tex2html115" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00950000000000000000">Transforms</a>
</li><li><a name="tex2html116" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00960000000000000000">Spatial sets</a>
</li><li><a name="tex2html117" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00970000000000000000">Lazy evaluation</a>
</li><li><a name="tex2html118" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00980000000000000000">Necessity</a>
</li><li><a name="tex2html119" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION00990000000000000000">Varying length sets</a>
</li><li><a name="tex2html120" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION009100000000000000000">Joins as intersections</a>
</li><li><a name="tex2html121" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION009110000000000000000">Data-driven techniques</a>
<ul>
<li><a name="tex2html122" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION009111000000000000000">SIMD</a>
</li></ul>
</li><li><a name="tex2html123" href="https://www.dataorienteddesign.com/dodbook/node9.html#SECTION009120000000000000000">Structs of arrays</a>
</li></ul>
<br>
</li><li><a name="tex2html124" href="https://www.dataorienteddesign.com/dodbook/node10.html">Helping the compiler</a>
<ul>
<li><a name="tex2html125" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001010000000000000000">Reducing order dependence</a>
</li><li><a name="tex2html126" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001020000000000000000">Reducing memory dependency</a>
</li><li><a name="tex2html127" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001030000000000000000">Write buffer awareness</a>
</li><li><a name="tex2html128" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001040000000000000000">Aliasing</a>
</li><li><a name="tex2html129" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001050000000000000000">Return value optimisation</a>
</li><li><a name="tex2html130" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001060000000000000000">Cache line utilisation</a>
</li><li><a name="tex2html131" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001070000000000000000">False sharing</a>
</li><li><a name="tex2html132" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001080000000000000000">Speculative execution awareness</a>
</li><li><a name="tex2html133" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION001090000000000000000">Branch prediction</a>
</li><li><a name="tex2html134" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION0010100000000000000000">Don't get evicted</a>
</li><li><a name="tex2html135" href="https://www.dataorienteddesign.com/dodbook/node10.html#SECTION0010110000000000000000">Auto vectorisation</a>
</li></ul>
<br>
</li><li><a name="tex2html136" href="https://www.dataorienteddesign.com/dodbook/node11.html">Maintenance and reuse</a>
<ul>
<li><a name="tex2html137" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001110000000000000000">Cosmic hierarchies</a>
</li><li><a name="tex2html138" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001120000000000000000">Debugging</a>
<ul>
<li><a name="tex2html139" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001121000000000000000">Lifetimes</a>
</li><li><a name="tex2html140" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001122000000000000000">Avoiding pointers</a>
</li><li><a name="tex2html141" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001123000000000000000">Bad State</a>
</li></ul>
</li><li><a name="tex2html142" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001130000000000000000">Reusability</a>
</li><li><a name="tex2html143" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001140000000000000000">Reusable functions</a>
</li><li><a name="tex2html144" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001150000000000000000">Unit testing</a>
</li><li><a name="tex2html145" href="https://www.dataorienteddesign.com/dodbook/node11.html#SECTION001160000000000000000">Refactoring</a>
</li></ul>
<br>
</li><li><a name="tex2html146" href="https://www.dataorienteddesign.com/dodbook/node12.html">What's wrong?</a>
<ul>
<li><a name="tex2html147" href="https://www.dataorienteddesign.com/dodbook/node12.html#SECTION001210000000000000000">The harm</a>
</li><li><a name="tex2html148" href="https://www.dataorienteddesign.com/dodbook/node12.html#SECTION001220000000000000000">Mapping the problem</a>
</li><li><a name="tex2html149" href="https://www.dataorienteddesign.com/dodbook/node12.html#SECTION001230000000000000000">Internalised state</a>
</li><li><a name="tex2html150" href="https://www.dataorienteddesign.com/dodbook/node12.html#SECTION001240000000000000000">Instance oriented development</a>
</li><li><a name="tex2html151" href="https://www.dataorienteddesign.com/dodbook/node12.html#SECTION001250000000000000000">Hierarchical design vs change</a>
</li><li><a name="tex2html152" href="https://www.dataorienteddesign.com/dodbook/node12.html#SECTION001260000000000000000">Divisions of labour</a>
</li><li><a name="tex2html153" href="https://www.dataorienteddesign.com/dodbook/node12.html#SECTION001270000000000000000">Reusable generic code</a>
</li></ul>
<br>
</li><li><a name="tex2html154" href="https://www.dataorienteddesign.com/dodbook/node13.html">About this document ...</a>
</li></ul>
<!--End of Table of Child-Links-->
<br><hr>
<address>
Richard Fabian
2018-10-08
</address>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wikipedia switches to CC BY-SA 4.0 license (121 pts)]]></title>
            <link>https://diff.wikimedia.org/2023/06/29/stepping-into-the-future-wikimedia-projects-transition-to-creative-commons-4-0-license/</link>
            <guid>36570714</guid>
            <pubDate>Mon, 03 Jul 2023 09:35:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diff.wikimedia.org/2023/06/29/stepping-into-the-future-wikimedia-projects-transition-to-creative-commons-4-0-license/">https://diff.wikimedia.org/2023/06/29/stepping-into-the-future-wikimedia-projects-transition-to-creative-commons-4-0-license/</a>, See on <a href="https://news.ycombinator.com/item?id=36570714">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-96580">
	<div>
			
<p><em>The Wikimedia Foundation is excited to announce the update to our license to Creative Commons Attribution-ShareAlike 4.0, one of the main changes of our recent Terms of Use update, which brings the Wikimedia projects up to the latest version of the Creative Commons licenses.</em></p>



<figure><a href="https://diff.wikimedia.org/?attachment_id=96581"><img decoding="async" src="https://diff.wikimedia.org/wp-content/uploads/2023/06/Creative-Commons-Attribution-ShareAlike-badge.png?w=1024&amp;resize=823%2C288" alt="An image of the Creative Commons Attribution-ShareAlike badge" width="823" height="288" srcset="https://diff.wikimedia.org/wp-content/uploads/2023/06/Creative-Commons-Attribution-ShareAlike-badge.png?w=2560 2560w, https://diff.wikimedia.org/wp-content/uploads/2023/06/Creative-Commons-Attribution-ShareAlike-badge.png?w=300 300w, https://diff.wikimedia.org/wp-content/uploads/2023/06/Creative-Commons-Attribution-ShareAlike-badge.png?w=768 768w, https://diff.wikimedia.org/wp-content/uploads/2023/06/Creative-Commons-Attribution-ShareAlike-badge.png?w=1024&amp;resize=823%2C288 1024w, https://diff.wikimedia.org/wp-content/uploads/2023/06/Creative-Commons-Attribution-ShareAlike-badge.png?w=1536 1536w, https://diff.wikimedia.org/wp-content/uploads/2023/06/Creative-Commons-Attribution-ShareAlike-badge.png?w=2048 2048w" sizes="(max-width: 823px) 100vw, 823px" data-recalc-dims="1"></a><figcaption>The Creative Commons Attribution-ShareAlike badge. Image by <a href="https://creativecommons.org/">Creative Commons</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">CC BY-SA 4.0</a>, via <a href="https://creativecommons.org/about/downloads/">Creative Commons</a>.</figcaption></figure>



<div><p>We are glad to announce an exciting change that is now effective on many Wikimedia projects as part of our recent <a href="https://meta.wikimedia.org/wiki/Wikimedia_Foundation_Legal_department/2023_ToU_updates">Terms of Use (ToU) update</a>: Wikimedia project licensing has been updated from Creative Commons Attribution-ShareAlike 3.0 (CC BY-SA 3.0) to Creative Commons Attribution-ShareAlike 4.0 (CC BY-SA 4.0).</p><p>In this blog post, we’ll discuss what this update means to users and how it’ll positively impact the millions of volunteers who contribute to, reshare, and remix Wikipedia daily.</p></div>



<h2><strong>What are Creative Commons licenses, and why are they important to free knowledge and Wikimedia projects?</strong></h2>



<div><p>Creative Commons licenses are legal tools that enable everyone contributing to a massive, decentralized project like Wikipedia to create free culture content! Content that’s contributed to the Wikimedia projects under a Creative Commons license gives others permission to reuse and remix that content. That enables the projects to make knowledge available to people around the world, free of charge.&nbsp;</p><p>Creative Commons licensing allows creators—like Wikipedia editors—to retain copyright while permitting reusers—people who use Creative Commons content in their own works to help share knowledge—to copy, distribute, and republish the work in question as long as they follow two main conditions: The “Attribution” limitation means that a reuser must give credit to the original author; the “Share Alike” limitation means that reusers must share any adaptations they make under the same or an equivalent license. In this way, Creative Commons licenses enable knowledge to be widely adapted, help keep new knowledge freely available as people develop it, and contribute to making sure that the people who receive and use the knowledge in question are informed where that knowledge comes from so they can know its source and continue to contribute to it themselves.</p></div>



<h2><strong>Why did we update our license to CC BY-SA 4.0?</strong></h2>



<p>Wikipedia’s update to the <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> license, from the previous <a href="https://creativecommons.org/licenses/by-sa/3.0/">CC BY-SA 3.0</a> license, helps to make the project more accessible, globally compatible, simplified, and readable.</p>



<p><span>1. New sources for knowledge</span></p>



<p>For years, new knowledge created under the 4.0 license could not be <em>directly</em> added to Wikipedia. Now it can! For example, United Nations (UN) bodies and <a href="https://wiki.creativecommons.org/wiki/Government_use_of_Creative_Commons">national governments</a> have licensed their publications under 4.0, and these did not match the older versions of the license. Now, publications such as those from the UN can be uploaded directly to the Wikimedia projects to provide sources, quotations, or descriptions in articles. This opens a large area of new material for the projects to work with and build on, and helps everyone move closer to the vision of making the world’s knowledge freely available for anyone, anywhere.&nbsp;</p>



<p><span>2. Internationalization</span></p>



<p>The 4.0 license has been designed to be a <em>global </em>license. With 3.0, there were multiple versions of the license, which varied depending on the country in which they were used. Now, with 4.0, the single license is applicable worldwide, <a href="https://wiki.creativecommons.org/wiki/Legal_Tools_Translation">with official translations in over 30 languages—and with many more to come!</a> This eliminates the need for different ported versions of the license, that is to say, with changed language made for use in a specific jurisdiction. It also eliminates the need for users to make translations of those ports themselves. This universality fosters increased global collaboration across projects and national borders, hence making this change crucial to achieving the 2030 Wikimedia movement goals.</p>



<p><span>3. Attribution requirements are streamlined</span></p>



<p>With version 4.0, it is easier for reusers to understand how to credit the original author of the work. For example, it clarifies that linking to a webpage with attribution information is allowable, which is helpful since doing so has already become a common method of providing attribution. It also enables people to fix attribution mistakes within a reasonable time: this is important to help address simple mistakes without the need for overly aggressive copyright enforcement demands, and makes free knowledge content safer to use for people, anywhere in the world, who may be discovering and experimenting with contributing free content for the first time.&nbsp;</p>



<p>4<span>. The license text itself is more readable</span></p>



<p>The side-by-side comparison of the licenses that we have provided below shows how the license language is now clearer to read. This is important because it helps people who contribute to projects like Wikipedia, as well as people who want to use information on Wikipedia in their own work, understand the license. Clearer, better-organized texts mean fewer attribution errors and broader use of this material for the public good.</p>



<figure><a href="https://diff.wikimedia.org/?attachment_id=96587"><img decoding="async" loading="lazy" width="1024" height="543" src="https://diff.wikimedia.org/wp-content/uploads/2023/06/Screen-Shot-2023-06-28-at-1.06.51-PM.png?resize=1024%2C543" alt="An image featuring a side-by-side comparison of some of the text from the CC BY-SA 3.0 and CC BY-SA 4.0 licenses, which shows that the new version is more concise " srcset="https://diff.wikimedia.org/wp-content/uploads/2023/06/Screen-Shot-2023-06-28-at-1.06.51-PM.png?w=1206 1206w, https://diff.wikimedia.org/wp-content/uploads/2023/06/Screen-Shot-2023-06-28-at-1.06.51-PM.png?w=300 300w, https://diff.wikimedia.org/wp-content/uploads/2023/06/Screen-Shot-2023-06-28-at-1.06.51-PM.png?w=768 768w, https://diff.wikimedia.org/wp-content/uploads/2023/06/Screen-Shot-2023-06-28-at-1.06.51-PM.png?w=1024 1024w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></a><figcaption>Side-by-side comparison of some of the text from the CC BY-SA 3.0 (left) and CC BY-SA 4.0 (right) licenses, illustrating how the new version is more concise.</figcaption></figure>



<p>You can find out more about what is new in 4.0 in <a href="https://creativecommons.org/version4/">this explainer from Creative Commons</a>.</p>



<h2><strong>Conclusion</strong></h2>



<p>The update to CC BY-SA 4.0 will help the Wikimedia projects continue to thrive as open, collaborative platforms for sharing free knowledge. The update makes the projects’ content more adaptable and usable for the global community, makes large amounts of new material compatible with the Wikimedia projects, and aligns our platform with the latest standards in open licensing.&nbsp;</p>



<p>We are excited about this new chapter of more modern, flexible, and easy-to-use licensing of free knowledge. Volunteer editors have already started the process of updating the relevant policy documents on-wiki so that we can operationalize this change and better communicate it to volunteers across the various Wikimedia projects and languages. If you see a webpage with an out-of-date license version, please feel free to update it!</p>
				<div id="translate-post">
					<p><img src="https://diff.wikimedia.org/wp-content/themes/interconnection/assets/images/translate-post.jpg" alt="">
					</p>

					<div>
						<h2>Can you help us translate this article?</h2>

						<p>In order for this article to reach as many people as possible we would like your help. Can you translate this article to get the message out?</p>

													<p><a href="https://diff.wikimedia.org/wp-login.php?redirect_to=%2F2023%2F06%2F29%2Fstepping-into-the-future-wikimedia-projects-transition-to-creative-commons-4-0-license%2F%23translate-post">Start translation</a>
												</p></div>
				</div>
				
		</div>

	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Perl 5.38 (152 pts)]]></title>
            <link>https://perldoc.perl.org/perl5380delta</link>
            <guid>36569727</guid>
            <pubDate>Mon, 03 Jul 2023 07:04:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://perldoc.perl.org/perl5380delta">https://perldoc.perl.org/perl5380delta</a>, See on <a href="https://news.ycombinator.com/item?id=36569727">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapperlicious">
        
        <h2><a id="toc">CONTENTS</a></h2>
                  <ul>
              <li>
                <a href="#NAME">NAME</a>
              </li>
              <li>
                <a href="#DESCRIPTION">DESCRIPTION</a>
              </li>
              <li>
                <a href="#Core-Enhancements">Core Enhancements</a>
                            <ul>
              <li>
                <a href="#New-class-Feature">New class Feature</a>
              </li>
              <li>
                <a href="#Unicode-15.0-is-supported">Unicode 15.0 is supported</a>
              </li>
              <li>
                <a href="#Deprecation-warnings-now-have-specific-subcategories">Deprecation warnings now have specific subcategories</a>
              </li>
              <li>
                <a href="#%25%7B%5EHOOK%7D-API-introduced">%{^HOOK} API introduced</a>
              </li>
              <li>
                <a href="#PERL_RAND_SEED">PERL_RAND_SEED</a>
              </li>
              <li>
                <a href="#Defined-or-and-logical-or-assignment-default-expressions-in-signatures">Defined-or and logical-or assignment default expressions in signatures</a>
              </li>
              <li>
                <a href="#@INC-Hook-Enhancements-and-$INC-and-INCDIR">@INC Hook Enhancements and $INC and INCDIR</a>
              </li>
              <li>
                <a href="#Forbidden-control-flow-out-of-defer-or-finally-now-detected-at-compile-time">Forbidden control flow out of defer or finally now detected at compile-time</a>
              </li>
              <li>
                <a href="#Optimistic-Eval-in-Patterns">Optimistic Eval in Patterns</a>
              </li>
              <li>
                <a href="#REG_INF-has-been-raised-from-65,536-to-2,147,483,647">REG_INF has been raised from 65,536 to 2,147,483,647</a>
              </li>
              <li>
                <a href="#New-API-functions-optimize_optree-and-finalize_optree">New API functions optimize_optree and finalize_optree</a>
              </li>
              <li>
                <a href="#Some-gotos-are-now-permitted-in-defer-and-finally-blocks">Some gotos are now permitted in defer and finally blocks</a>
              </li>
              <li>
                <a href="#New-regexp-variable-$%7B%5ELAST_SUCCESSFUL_PATTERN%7D">New regexp variable ${^LAST_SUCCESSFUL_PATTERN}</a>
              </li>
              <li>
                <a href="#Locale-category-LC_NAME-now-supported-on-participating-platforms">Locale category LC_NAME now supported on participating platforms</a>
              </li>
          </ul>

              </li>
              <li>
                <a href="#Incompatible-Changes">Incompatible Changes</a>
                            <ul>
              <li>
                <a href="#readline()-no-longer-clears-the-stream-error-and-eof-flags">readline() no longer clears the stream error and eof flags</a>
              </li>
              <li>
                <a href="#INIT-blocks-no-longer-run-after-an-exit()-in-BEGIN">INIT blocks no longer run after an exit() in BEGIN</a>
              </li>
              <li>
                <a href="#Syntax-errors-no-longer-produce-%22phantom-error-messages%22">Syntax errors no longer produce "phantom error messages"</a>
              </li>
              <li>
                <a href="#utf8::upgrade()">utf8::upgrade()</a>
              </li>
              <li>
                <a href="#Changes-to-%22thread-safe%22-locales">Changes to "thread-safe" locales</a>
              </li>
          </ul>

              </li>
              <li>
                <a href="#Deprecations">Deprecations</a>
                            <ul>
              <li>
                <a href="#Use-of-'-as-a-package-name-separator-is-deprecated">Use of ' as a package name separator is deprecated</a>
              </li>
              <li>
                <a href="#Switch-and-Smart-Match-operator">Switch and Smart Match operator</a>
              </li>
          </ul>

              </li>
              <li>
                <a href="#Performance-Enhancements">Performance Enhancements</a>
              </li>
              <li>
                <a href="#Modules-and-Pragmata">Modules and Pragmata</a>
                            <ul>
              <li>
                <a href="#Updated-Modules-and-Pragmata">Updated Modules and Pragmata</a>
              </li>
          </ul>

              </li>
              <li>
                <a href="#Documentation">Documentation</a>
                            <ul>
              <li>
                <a href="#New-Documentation">New Documentation</a>
                            <ul>
              <li>
                <a href="#perlclass">perlclass</a>
              </li>
              <li>
                <a href="#perlclassguts">perlclassguts</a>
              </li>
          </ul>

              </li>
              <li>
                <a href="#Changes-to-Existing-Documentation">Changes to Existing Documentation</a>
                            <ul>
              <li>
                <a href="#perlapi">perlapi</a>
              </li>
              <li>
                <a href="#perldeprecation">perldeprecation</a>
              </li>
              <li>
                <a href="#perlintern">perlintern</a>
              </li>
              <li>
                <a href="#perlexperiment">perlexperiment</a>
              </li>
              <li>
                <a href="#perlfunc">perlfunc</a>
              </li>
              <li>
                <a href="#perlhacktips">perlhacktips</a>
              </li>
              <li>
                <a href="#perlop">perlop</a>
              </li>
              <li>
                <a href="#perlvar">perlvar</a>
              </li>
          </ul>

              </li>
          </ul>

              </li>
              <li>
                <a href="#Diagnostics">Diagnostics</a>
                            <ul>
              <li>
                <a href="#New-Diagnostics">New Diagnostics</a>
                            <ul>
              <li>
                <a href="#New-Errors">New Errors</a>
              </li>
              <li>
                <a href="#New-Warnings">New Warnings</a>
              </li>
          </ul>

              </li>
              <li>
                <a href="#Changes-to-Existing-Diagnostics">Changes to Existing Diagnostics</a>
              </li>
          </ul>

              </li>
              <li>
                <a href="#Configuration-and-Compilation">Configuration and Compilation</a>
              </li>
              <li>
                <a href="#Testing">Testing</a>
              </li>
              <li>
                <a href="#Platform-Support">Platform Support</a>
                            <ul>
              <li>
                <a href="#Discontinued-Platforms">Discontinued Platforms</a>
              </li>
              <li>
                <a href="#Platform-Specific-Notes">Platform-Specific Notes</a>
              </li>
          </ul>

              </li>
              <li>
                <a href="#Internal-Changes">Internal Changes</a>
              </li>
              <li>
                <a href="#Selected-Bug-Fixes">Selected Bug Fixes</a>
              </li>
              <li>
                <a href="#Acknowledgements">Acknowledgements</a>
              </li>
              <li>
                <a href="#Reporting-Bugs">Reporting Bugs</a>
              </li>
              <li>
                <a href="#Give-Thanks">Give Thanks</a>
              </li>
              <li>
                <a href="#SEE-ALSO">SEE ALSO</a>
              </li>
          </ul>

      <h2 id="NAME"><a href="#NAME">#</a>NAME</h2>

<p>perldelta - what is new for perl v5.38.0</p>

<h2 id="DESCRIPTION"><a href="#DESCRIPTION">#</a>DESCRIPTION</h2>

<p>This document describes differences between the 5.36.0 release and the 5.38.0 release.</p>

<h2 id="Core-Enhancements"><a href="#Core-Enhancements">#</a><a id="Core"></a>Core Enhancements</h2>

<h2 id="New-class-Feature"><a href="#New-class-Feature">#</a><a id="New"></a>New <code>class</code> Feature</h2>

<p>A new <b>experimental</b> syntax is now available for defining object classes, where per-instance data is stored in "field" variables that behave like lexicals.</p>

<pre><code>use feature 'class';

class Point
{
    field $x;
    field $y;

    method zero { $x = $y = 0; }
}</code></pre>

<p>This is described in more detail in <a href="https://perldoc.perl.org/perlclass">perlclass</a>. Notes on the internals of its implementation and other related details can be found in <a href="https://perldoc.perl.org/perlclassguts">perlclassguts</a>.</p>

<p>This remains a new and experimental feature, and is very much still under development. It will be the subject of much further addition, refinement and alteration in future releases. As it is experimental, it yields warnings in the <code>experimental::class</code> category. These can be silenced by a <code>no warnings</code> statement.</p>

<pre><code>use feature 'class';
no warnings 'experimental::class';</code></pre>

<h2 id="Unicode-15.0-is-supported"><a href="#Unicode-15.0-is-supported">#</a><a id="Unicode"></a>Unicode 15.0 is supported</h2>

<p>See <a href="https://www.unicode.org/versions/Unicode15.0.0/">https://www.unicode.org/versions/Unicode15.0.0/</a> for details.</p>

<h2 id="Deprecation-warnings-now-have-specific-subcategories"><a href="#Deprecation-warnings-now-have-specific-subcategories">#</a><a id="Deprecation"></a>Deprecation warnings now have specific subcategories</h2>

<p>All deprecation warnings now have their own specific deprecation category which can be disabled individually. You can see a list of all deprecated features in <a href="https://perldoc.perl.org/perldeprecation">perldeprecation</a>, and in <a href="https://perldoc.perl.org/warnings">warnings</a>. The following list is from <a href="https://perldoc.perl.org/warnings">warnings</a>:</p>

<pre><code>+- deprecated ----+
|                 |
|                 +- deprecated::apostrophe_as_package_separator
|                 |
|                 +- deprecated::delimiter_will_be_paired
|                 |
|                 +- deprecated::dot_in_inc
|                 |
|                 +- deprecated::goto_construct
|                 |
|                 +- deprecated::smartmatch
|                 |
|                 +- deprecated::unicode_property_name
|                 |
|                 +- deprecated::version_downgrade</code></pre>

<p>It is still possible to disable all deprecation warnings in a single statement with</p>

<pre><code>no warnings 'deprecated';</code></pre>

<p>but now is possible to have a finer grained control. As has historically been the case these warnings are automatically enabled with</p>

<pre><code>use warnings;</code></pre>

<h2 id="%{^HOOK}-API-introduced"><a href="#%25%7B%5EHOOK%7D-API-introduced">#</a><a id="HOOK-API-introduced"></a>%{^HOOK} API introduced</h2>

<p>For various reasons it can be difficult to create subroutine wrappers for some of perls keywords. Any keyword which has an undefined prototype simply cannot be wrapped with a subroutine, and some keywords which perl permits to be wrapped are in practice very tricky to wrap. For example <code>require</code> is tricky to wrap, it is possible but doing so changes the stack depth, and the standard methods of exporting assume that they will be exporting to a package at certain stack depth up the stack, and the wrapper will thus change where functions are exported to unless implemented with a great deal of care. This can be very awkward to deal with.</p>

<p>Accordingly we have introduced a new hash called <code>%{^HOOK}</code> which is intended to facilitate such cases. When a keyword supports any kind of special hook then the hook will live in this new hash. Hooks in this hash will be named after the function they are called by, followed by two underbars and then the phase they are executed in, currently either before or after the keyword is executed.</p>

<p>In this initial release we support two hooks <code>require__before</code> and <code>require__after</code>. These are provided to make it easier to perform tasks before and after a require statement.</p>

<p>See <a href="https://perldoc.perl.org/perlvar">perlvar</a> for more details.</p>

<h2 id="PERL_RAND_SEED"><a href="#PERL_RAND_SEED">#</a>PERL_RAND_SEED</h2>

<p>Added a new environment variable <code>PERL_RAND_SEED</code> which can be used to cause a perl program which uses <code>rand</code> without using <code>srand()</code> explicitly or which uses <code>srand()</code> with no arguments to be repeatable. See <a href="https://perldoc.perl.org/perlrun">perlrun</a>. This feature can be disabled at compile time by passing</p>

<pre><code>-Accflags=-DNO_PERL_RAND_SEED</code></pre>

<p>to <i>Configure</i> during the build process.</p>

<h2 id="Defined-or-and-logical-or-assignment-default-expressions-in-signatures"><a href="#Defined-or-and-logical-or-assignment-default-expressions-in-signatures">#</a><a id="Defined"></a>Defined-or and logical-or assignment default expressions in signatures</h2>

<p>The default expression for a subroutine signature parameter can now be assigned using the <code>//=</code> or <code>||=</code> operators, to apply the defaults whenever the caller provided an undefined or false value (respectively), rather than simply when the parameter is missing entirely. For more detail see the documentation in <a href="https://perldoc.perl.org/perlsub">perlsub</a>.</p>

<h2 id="@INC-Hook-Enhancements-and-$INC-and-INCDIR"><a href="#@INC-Hook-Enhancements-and-$INC-and-INCDIR">#</a><a id="INC-Hook-Enhancements-and-INC-and-INCDIR"></a>@INC Hook Enhancements and $INC and INCDIR</h2>

<p>The internals for <code>@INC</code> hooks have been hardened to handle various edge cases and should no longer segfault or throw assert failures when hooks modify <code>@INC</code> during a require operation. As part of this we now ensure that any given hook is executed at most once during a require call, and that any duplicate directories do not trigger additional directory probes.</p>

<p>To provide developers more control over dynamic module lookup, a new hook method <code>INCDIR</code> is now supported. An object supporting this method may be injected into the <code>@INC</code> array, and when it is encountered in the module search process it will be executed, just like how INC hooks are executed, and its return value used as a list of directories to search for the module. Returning an empty list acts as a no-op. Note that since any references returned by this hook will be stringified and used as strings, you may not return a hook to be executed later via this API.</p>

<p>When an <code>@INC</code> hook (either <code>INC</code> or <code>INCDIR</code>) is called during require, the <code>$INC</code> variable will be localized to be the value of the index of <code>@INC</code> that the hook came from. If the hook wishes to override what the "next" index in <code>@INC</code> should be it may update <code>$INC</code> to be one less than the desired index (<code>undef</code> is equivalent to <code>-1</code>). This allows an <code>@INC</code> hook to completely rewrite the <code>@INC</code> array and have perl restart its directory probes from the beginning of <code>@INC</code>.</p>

<p>Blessed CODE references in <code>@INC</code> that do not support the <code>INC</code> or <code>INCDIR</code> methods will no longer trigger an exception, and instead will be treated the same as unblessed coderefs are, and executed as though they were an <code>INC</code> hook.</p>

<h2 id="Forbidden-control-flow-out-of-defer-or-finally-now-detected-at-compile-time"><a href="#Forbidden-control-flow-out-of-defer-or-finally-now-detected-at-compile-time">#</a><a id="Forbidden"></a>Forbidden control flow out of <code>defer</code> or <code>finally</code> now detected at compile-time</h2>

<p>It is forbidden to attempt to leave a <code>defer</code> or <code>finally</code> block by means of control flow such as <code>return</code> or <code>goto</code>. Previous versions of perl could only detect this when actually attempted at runtime.</p>

<p>This version of perl adds compile-time detection for many cases that can be statically determined. This may mean that code which compiled successfully on a previous version of perl is now reported as a compile-time error with this one. This only happens in cases where it would have been an error to actually execute the code anyway; the error simply happens at an earlier time.</p>

<h2 id="Optimistic-Eval-in-Patterns"><a href="#Optimistic-Eval-in-Patterns">#</a><a id="Optimistic"></a>Optimistic Eval in Patterns</h2>

<p>The use of <code>(?{ ... })</code> and <code>(??{ ... })</code> in a pattern disables various optimisations globally in that pattern. This may or may not be desired by the programmer. This release adds the <code>(*{ ... })</code> equivalent. The only difference is that it does not and will never disable any optimisations in the regex engine. This may make it more unstable in the sense that it may be called more or less times in the future, however the number of times it executes will truly match how the regex engine functions. For example, certain types of optimisation are disabled when <code>(?{ ... })</code> is included in a pattern, so that patterns which are O(N) in normal use become O(N*N) with a <code>(?{ ... })</code> pattern in them. Switching to <code>(*{ ... })</code> means the pattern will stay O(N).</p>

<h2 id="REG_INF-has-been-raised-from-65,536-to-2,147,483,647"><a href="#REG_INF-has-been-raised-from-65,536-to-2,147,483,647">#</a><a id="REG_INF"></a><a id="REG_INF-has-been-raised-from-65-536-to-2-147-483-647"></a>REG_INF has been raised from 65,536 to 2,147,483,647</h2>

<p>Many regex quantifiers used to be limited to <code>U16_MAX</code> in the past, but are now limited to <code>I32_MAX</code>, thus it is now possible to write <code>/(?:word){1000000}/</code> for example. Note that doing so may cause the regex engine to run longer and use more memory.</p>

<h2 id="New-API-functions-optimize_optree-and-finalize_optree"><a href="#New-API-functions-optimize_optree-and-finalize_optree">#</a><a id="New1"></a>New API functions optimize_optree and finalize_optree</h2>

<p>There are two new API functions for operating on optree fragments, ensuring you can invoke the required parts of the optree-generation process that might otherwise not get invoked (e.g. when creating a custom LOGOP). To get access to these functions, you first need to set a <code>#define</code> to opt-in to using these functions.</p>

<pre><code>#define PERL_USE_VOLATILE_API</code></pre>

<p>These functions are closely tied to the internals of how the interpreter works, and could be altered or removed at any time if other internal changes make that necessary.</p>

<h2 id="Some-gotos-are-now-permitted-in-defer-and-finally-blocks"><a href="#Some-gotos-are-now-permitted-in-defer-and-finally-blocks">#</a><a id="Some"></a>Some <code>goto</code>s are now permitted in <code>defer</code> and <code>finally</code> blocks</h2>

<p>Perl version 5.36.0 added <code>defer</code> blocks and permitted the <code>finally</code> keyword to also add similar behaviour to <code>try</code>/<code>catch</code> syntax. These did not permit any <code>goto</code> expression within the body, as it could have caused control flow to jump out of the block. Now, some <code>goto</code> expressions are allowed, if they have a constant target label, and that label is found within the block.</p>

<pre><code>use feature 'defer';

defer {
  goto LABEL;
  print "This does not execute\n";
  LABEL: print "This does\n";
}</code></pre>

<h2 id="New-regexp-variable-${^LAST_SUCCESSFUL_PATTERN}"><a href="#New-regexp-variable-$%7B%5ELAST_SUCCESSFUL_PATTERN%7D">#</a><a id="New2"></a><a id="New-regexp-variable-LAST_SUCCESSFUL_PATTERN"></a>New regexp variable ${^LAST_SUCCESSFUL_PATTERN}</h2>

<p>This allows access to the last succesful pattern that matched in the current scope. Many aspects of the regex engine refer to the "last successful pattern". The empty pattern reuses it, and all of the magic regex vars relate to it. This allows access to its pattern. The following code</p>

<pre><code>if (m/foo/ || m/bar/) {
    s//PQR/;
}</code></pre>

<p>can be rewritten as follows</p>

<pre><code>if (m/foo/ || m/bar/) {
    s/${^LAST_SUCCESSFUL_PATTERN}/PQR/;
}</code></pre>

<p>and it will do the exactly same thing.</p>

<h2 id="Locale-category-LC_NAME-now-supported-on-participating-platforms"><a href="#Locale-category-LC_NAME-now-supported-on-participating-platforms">#</a><a id="Locale"></a>Locale category LC_NAME now supported on participating platforms</h2>

<p>On platforms that have the GNU extension <code>LC_NAME</code> category, you may now use it as the category parameter to <a href="https://perldoc.perl.org/POSIX#setlocale">"setlocale" in POSIX</a> to set and query its locale.</p>

<h2 id="Incompatible-Changes"><a href="#Incompatible-Changes">#</a><a id="Incompatible"></a>Incompatible Changes</h2>

<h2 id="readline()-no-longer-clears-the-stream-error-and-eof-flags"><a href="#readline()-no-longer-clears-the-stream-error-and-eof-flags">#</a><a id="readline"></a><a id="readline-no-longer-clears-the-stream-error-and-eof-flags"></a>readline() no longer clears the stream error and eof flags</h2>

<p><code>readline()</code>, also spelled <code>&lt;&gt;</code>, would clear the handle's error and eof flags after an error occurred on the stream.</p>

<p>In nearly all cases this clear is no longer done, so the error and eof flags now properly reflect the status of the stream after readline().</p>

<p>Since the error flag is no longer cleared calling close() on the stream may fail and if the stream was not explicitly closed, the implicit close of the stream may produce a warning.</p>

<p>This has resulted in two main types of problems in downstream CPAN modules, and these may also occur in your code:</p>

<ul>

<li><p>If your code reads to end of file, and then rebinds the handle to a new file descriptor, previously since the eof flag wasn't set you could continue to read from the stream. You now need to clear the eof flag yourself with <code>$handle-&gt;clearerr()</code> to continue reading.</p>

</li>
<li><p>If your code encounters an error on the stream while reading with readline() you will need to call <code>$handle-&gt;clearerr</code> to continue reading. The one case this occurred the underlying file descriptor was marked non-blocking, so the read() system call was failing with <code>EAGAIN</code>, which resulted in the error flag being set on the stream.</p>

</li>
</ul>

<p>The only case where error and eof flags continue to cleared on error is when reading from the child process for glob() in <i>miniperl</i>. This allows it to correctly report errors from the child process on close(). This is unlikely to be an issue during normal perl development.</p>

<p>[<a href="https://github.com/Perl/perl5/issues/20060">GH #20060</a>]</p>

<h2 id="INIT-blocks-no-longer-run-after-an-exit()-in-BEGIN"><a href="#INIT-blocks-no-longer-run-after-an-exit()-in-BEGIN">#</a><a id="INIT"></a><a id="INIT-blocks-no-longer-run-after-an-exit-in-BEGIN"></a><code>INIT</code> blocks no longer run after an <code>exit()</code> in <code>BEGIN</code></h2>

<p><code>INIT</code> blocks will no longer run after an <code>exit()</code> performed inside of a <code>BEGIN</code>. This means that the combination of the <code>-v</code> option and the <code>-c</code> option no longer executes a compile check as well as showing the perl version. The <code>-v</code> option executes an exit(0) after printing the version information inside of a <code>BEGIN</code> block, and the <code>-c</code> check is implemented by using <code>INIT</code> hooks, resulting in the <code>-v</code> option taking precedence.</p>

<p>[<a href="https://github.com/Perl/perl5/issues/1537">GH #1537</a>] [<a href="https://github.com/Perl/perl5/issues/20181">GH #20181</a>]</p>

<h2 id="Syntax-errors-no-longer-produce-&quot;phantom-error-messages&quot;"><a href="#Syntax-errors-no-longer-produce-%22phantom-error-messages%22">#</a><a id="Syntax"></a><a id="Syntax-errors-no-longer-produce-phantom-error-messages"></a>Syntax errors no longer produce "phantom error messages"</h2>

<p>Generally perl will continue parsing the source code even after encountering a compile error. In many cases this is helpful, for instance with misspelled variable names it is helpful to show as many examples of the error as possible. But in the case of syntax errors continuing often produces bizarre error messages and may even cause segmentation faults during the compile process. In this release the compiler will halt at the first syntax error encountered. This means that any code expecting to see the specific error messages we used to produce will be broken. The error that is emitted will be one of the diagnostics that used to be produced, but in some cases some messages that used to be produced will no longer be displayed.</p>

<p>See <a href="#Changes-to-Existing-Diagnostics">"Changes to Existing Diagnostics"</a> for more details.</p>

<h2 id="utf8::upgrade()"><a href="#utf8::upgrade()">#</a><a id="utf8"></a><a id="utf8::upgrade"></a><a href="https://perldoc.perl.org/utf8#Utility-functions"><code>utf8::upgrade()</code></a></h2>

<p>Starting in this release, if the input string is <code>undef</code>, it remains <code>undef</code>. Previously it would be changed into a defined, zero-length string.</p>

<h2 id="Changes-to-&quot;thread-safe&quot;-locales"><a href="#Changes-to-%22thread-safe%22-locales">#</a><a id="Changes"></a><a id="Changes-to-thread-safe-locales"></a>Changes to "thread-safe" locales</h2>

<p>Perl 5.28 introduced "thread-safe" locales on systems that supported them, namely modern Windows, and systems supporting POSIX 2008 locale operations. These systems accomplish this by having per-thread locales, while continuing to support the older global locale operations for code that doesn't take the steps necessary to use the newer per-thread ones.</p>

<p>It turns out that some POSIX 2008 platforms have or have had buggy implementations, which forced perl to not use them. The <code>${^SAFE_LOCALES}</code> scalar variable contains 0 or 1 to indicate whether or not the current platform is considered by perl to have a working thread-safe implementation. Some implementations have been fixed already, but FreeBSD and Cygwin have been newly discovered to be sufficiently buggy that the thread-safe operations are no longer used by perl, starting in this release. Hence, <code>${^SAFE_LOCALES}</code> is now 0 for them. Older versions of perl can be configured to avoid these buggy implementations by adding the <i>Configure</i> option <code>-DNO_POSIX_2008_LOCALE</code>.</p>

<p>And v5.38 fixes a bug in all previous perls that led to locales not being fully thread-safe. The first thread that finishes caused the main thread (named <code>thread0</code>) to revert to the global locale in effect at startup, discarding whatever the thread's locale had been previously set to. If any other thread had switched to the global locale by calling <code>switch_to_global_locale()</code> in XS code, those threads would all share the global locale, and <code>thread0</code> would not be thread-safe.</p>

<h2 id="Deprecations"><a href="#Deprecations">#</a>Deprecations</h2>

<h2 id="Use-of-'-as-a-package-name-separator-is-deprecated"><a href="#Use-of-'-as-a-package-name-separator-is-deprecated">#</a><a id="Use"></a><a id="Use-of-as-a-package-name-separator-is-deprecated"></a>Use of <code>'</code> as a package name separator is deprecated</h2>

<p>Using <code>'</code> as package separator in a variable named in a double-quoted string has warned since 5.28. It is now deprecated in both string interpolation and non-interpolated contexts, and will be removed in Perl 5.42.</p>

<h2 id="Switch-and-Smart-Match-operator"><a href="#Switch-and-Smart-Match-operator">#</a><a id="Switch"></a>Switch and Smart Match operator</h2>

<p>The "switch" feature and the smartmatch operator, <code>~~</code>, were introduced in v5.10. Their behavior was significantly changed in v5.10.1. When the "experiment" system was added in v5.18.0, switch and smartmatch were retroactively declared experimental. Over the years, proposals to fix or supplement the features have come and gone.</p>

<p>In v5.38.0, we are declaring the experiment a failure. Some future system may take the conceptual place of smartmatch, but it has not yet been designed or built.</p>

<p>These features will be entirely removed from perl in v5.42.0.</p>

<h2 id="Performance-Enhancements"><a href="#Performance-Enhancements">#</a><a id="Performance"></a>Performance Enhancements</h2>

<ul>

<li><p>Additional optree optimizations for common OP patterns. For example, multiple simple OPs replaced by a single streamlined OP, so as to be more efficient at runtime. [<a href="https://github.com/Perl/perl5/pull/19943">GH #19943</a>], [<a href="https://github.com/Perl/perl5/pull/20063">GH #20063</a>], [<a href="https://github.com/Perl/perl5/pull/20077">GH #20077</a>].</p>

</li>
<li><p>Creating an anonymous sub no longer generates an <code>srefgen</code> op, the reference generation is now done in the <code>anoncode</code> or <code>anonconst</code> op, saving runtime. [<a href="https://github.com/Perl/perl5/pull/20290">GH #20290</a>]</p>

</li>
</ul>

<h2 id="Modules-and-Pragmata"><a href="#Modules-and-Pragmata">#</a><a id="Modules"></a>Modules and Pragmata</h2>

<h2 id="Updated-Modules-and-Pragmata"><a href="#Updated-Modules-and-Pragmata">#</a><a id="Updated"></a>Updated Modules and Pragmata</h2>

<ul>

<li><p>Added the <code>is_tainted()</code> builtin function. [<a href="https://github.com/Perl/perl5/issues/19854">GH #19854</a>]</p>

</li>
<li><p>Added the <code>export_lexically()</code> builtin function as per <a href="https://github.com/Perl/PPCs/blob/main/ppcs/ppc0020-lexical-export.md">PPC 0020</a>. [<a href="https://github.com/Perl/perl5/issues/19895">GH #19895</a>]</p>

</li>
<li><p>Support for <a href="https://github.com/Perl/PPCs/blob/main/ppcs/ppc0018-module-true.md">PPC 0018</a>, <code>use feature "module_true";</code> has been added to the default feature bundle for v5.38 and later. It may also be used explicitly. When enabled inside of a module the module does not need to return true explicitly, and in fact the return will be forced to a simple true value regardless of what it originally was.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Attribute::Handlers">Attribute::Handlers</a> has been upgraded from version 1.02 to 1.03.</p>

</li>
<li><p><a href="https://perldoc.perl.org/attributes">attributes</a> has been upgraded from version 0.34 to 0.35.</p>

</li>
<li><p><a href="https://perldoc.perl.org/autodie">autodie</a> has been upgraded from version 2.34 to 2.36.</p>

</li>
<li><p><a href="https://perldoc.perl.org/B">B</a> has been upgraded from version 1.83 to 1.88.</p>

</li>
<li><p><a href="https://perldoc.perl.org/B::Concise">B::Concise</a> has been upgraded from version 1.006 to 1.007.</p>

</li>
<li><p><a href="https://perldoc.perl.org/B::Deparse">B::Deparse</a> has been upgraded from version 1.64 to 1.74.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Benchmark">Benchmark</a> has been upgraded from version 1.23 to 1.24.</p>

</li>
<li><p><a href="https://perldoc.perl.org/bignum">bignum</a> has been upgraded from version 0.65 to 0.66.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Carp">Carp</a> has been upgraded from version 1.52 to 1.54.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Class::Struct">Class::Struct</a> has been upgraded from version 0.66 to 0.68.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Compress::Raw::Bzip2">Compress::Raw::Bzip2</a> has been upgraded from version 2.103 to 2.204_001.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Compress::Raw::Zlib">Compress::Raw::Zlib</a> has been upgraded from version 2.105 to 2.204_001.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Config::Perl::V">Config::Perl::V</a> has been upgraded from version 0.33 to 0.36.</p>

</li>
<li><p><a href="https://perldoc.perl.org/CPAN">CPAN</a> has been upgraded from version 2.33 to 2.36.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Data::Dumper">Data::Dumper</a> has been upgraded from version 2.184 to 2.188.</p>

</li>
<li><p><a href="https://perldoc.perl.org/DB_File">DB_File</a> has been upgraded from version 1.857 to 1.858.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Devel::Peek">Devel::Peek</a> has been upgraded from version 1.32 to 1.33.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Devel::PPPort">Devel::PPPort</a> has been upgraded from version 3.68 to 3.71.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Digest::MD5">Digest::MD5</a> has been upgraded from version 2.58 to 2.58_01.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Digest::SHA">Digest::SHA</a> has been upgraded from version 6.02 to 6.04.</p>

</li>
<li><p><a href="https://perldoc.perl.org/DynaLoader">DynaLoader</a> has been upgraded from version 1.52 to 1.54.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Encode">Encode</a> has been upgraded from version 3.17 to 3.19.</p>

</li>
<li><p><a href="https://perldoc.perl.org/encoding::warnings">encoding::warnings</a> has been upgraded from version 0.13 to 0.14.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Env">Env</a> has been upgraded from version 1.05 to 1.06.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Errno">Errno</a> has been upgraded from version 1.36 to 1.37.</p>

</li>
<li><p><a href="https://perldoc.perl.org/experimental">experimental</a> has been upgraded from version 0.028 to 0.031.</p>

</li>
<li><p><a href="https://perldoc.perl.org/ExtUtils::CBuilder">ExtUtils::CBuilder</a> has been upgraded from version 0.280236 to 0.280238.</p>

</li>
<li><p><a href="https://perldoc.perl.org/ExtUtils::Install">ExtUtils::Install</a> has been upgraded from version 2.20 to 2.22.</p>

</li>
<li><p><a href="https://perldoc.perl.org/ExtUtils::MakeMaker">ExtUtils::MakeMaker</a> has been upgraded from version 7.64 to 7.70.</p>

</li>
<li><p><a href="https://perldoc.perl.org/ExtUtils::Miniperl">ExtUtils::Miniperl</a> has been upgraded from version 1.11 to 1.13.</p>

</li>
<li><p><a href="https://perldoc.perl.org/ExtUtils::ParseXS">ExtUtils::ParseXS</a> has been upgraded from version 3.45 to 3.51.</p>

</li>
<li><p><a href="https://perldoc.perl.org/ExtUtils::PL2Bat">ExtUtils::PL2Bat</a> has been upgraded from version 0.004 to 0.005.</p>

</li>
<li><p><a href="https://perldoc.perl.org/ExtUtils::Typemaps">ExtUtils::Typemaps</a> has been upgraded from version 3.45 to 3.51.</p>

</li>
<li><p><a href="https://perldoc.perl.org/feature">feature</a> has been upgraded from version 1.72 to 1.82.</p>

</li>
<li><p><a href="https://perldoc.perl.org/File::Basename">File::Basename</a> has been upgraded from version 2.85 to 2.86.</p>

</li>
<li><p><a href="https://perldoc.perl.org/File::Copy">File::Copy</a> has been upgraded from version 2.39 to 2.41.</p>

</li>
<li><p><a href="https://perldoc.perl.org/File::Find">File::Find</a> has been upgraded from version 1.40 to 1.43.</p>

</li>
<li><p><a href="https://perldoc.perl.org/File::Glob">File::Glob</a> has been upgraded from version 1.37 to 1.40.</p>

</li>
<li><p><a href="https://perldoc.perl.org/File::Spec">File::Spec</a> has been upgraded from version 3.84 to 3.89.</p>

</li>
<li><p><a href="https://perldoc.perl.org/File::stat">File::stat</a> has been upgraded from version 1.12 to 1.13.</p>

</li>
<li><p><a href="https://perldoc.perl.org/FileHandle">FileHandle</a> has been upgraded from version 2.03 to 2.05.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Filter::Util::Call">Filter::Util::Call</a> has been upgraded from version 1.60 to 1.64.</p>

</li>
<li><p><a href="https://perldoc.perl.org/GDBM_File">GDBM_File</a> has been upgraded from version 1.23 to 1.24.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Getopt::Long">Getopt::Long</a> has been upgraded from version 2.52 to 2.54.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Hash::Util">Hash::Util</a> has been upgraded from version 0.28 to 0.30.</p>

</li>
<li><p><a href="https://perldoc.perl.org/HTTP::Tiny">HTTP::Tiny</a> has been upgraded from version 0.080 to 0.083.</p>

</li>
<li><p><a href="https://perldoc.perl.org/I18N::Langinfo">I18N::Langinfo</a> has been upgraded from version 0.21 to 0.22.</p>

</li>
<li><p><a href="https://perldoc.perl.org/IO">IO</a> has been upgraded from version 1.50 to 1.52.</p>

</li>
<li><p>IO-Compress has been upgraded from version 2.106 to 2.204.</p>

</li>
<li><p><a href="https://perldoc.perl.org/IO::Socket::IP">IO::Socket::IP</a> has been upgraded from version 0.41 to 0.41_01.</p>

<p>On DragonflyBSD, detect setsockopt() not actually clearing <code>IPV6_V6ONLY</code> even when setsockopt() returns success. [<a href="https://rt.cpan.org/Ticket/Display.html?id=148293">cpan #148293</a>]</p>

</li>
<li><p><a href="https://perldoc.perl.org/IO::Zlib">IO::Zlib</a> has been upgraded from version 1.11 to 1.14.</p>

</li>
<li><p><a href="https://perldoc.perl.org/JSON::PP">JSON::PP</a> has been upgraded from version 4.07 to 4.16.</p>

</li>
<li><p>libnet has been upgraded from version 3.14 to 3.15.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Locale::Maketext">Locale::Maketext</a> has been upgraded from version 1.31 to 1.33.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Math::BigInt">Math::BigInt</a> has been upgraded from version 1.999830 to 1.999837.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Math::BigInt::FastCalc">Math::BigInt::FastCalc</a> has been upgraded from version 0.5012 to 0.5013.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Math::BigRat">Math::BigRat</a> has been upgraded from version 0.2621 to 0.2624.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Math::Complex">Math::Complex</a> has been upgraded from version 1.5902 to 1.62.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Memoize">Memoize</a> has been upgraded from version 1.03_01 to 1.16.</p>

</li>
<li><p><a href="https://perldoc.perl.org/MIME::Base64">MIME::Base64</a> has been upgraded from version 3.16 to 3.16_01.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Module::CoreList">Module::CoreList</a> has been upgraded from version 5.20220520 to 5.20230520.</p>

</li>
<li><p><a href="https://perldoc.perl.org/mro">mro</a> has been upgraded from version 1.26 to 1.28.</p>

</li>
<li><p><a href="https://perldoc.perl.org/NDBM_File">NDBM_File</a> has been upgraded from version 1.15 to 1.16.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Net::Ping">Net::Ping</a> has been upgraded from version 2.74 to 2.76.</p>

</li>
<li><p><a href="https://perldoc.perl.org/ODBM_File">ODBM_File</a> has been upgraded from version 1.17 to 1.18.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Opcode">Opcode</a> has been upgraded from version 1.57 to 1.64.</p>

</li>
<li><p><a href="https://perldoc.perl.org/overload">overload</a> has been upgraded from version 1.35 to 1.37.</p>

</li>
<li><p><a href="https://perldoc.perl.org/parent">parent</a> has been upgraded from version 0.238 to 0.241.</p>

</li>
<li><p><a href="https://perldoc.perl.org/PerlIO::via::QuotedPrint">PerlIO::via::QuotedPrint</a> has been upgraded from version 0.09 to 0.10.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Pod::Checker">Pod::Checker</a> has been upgraded from version 1.74 to 1.75.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Pod::Html">Pod::Html</a> has been upgraded from version 1.33 to 1.34.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Pod::Usage">Pod::Usage</a> has been upgraded from version 2.01 to 2.03.</p>

</li>
<li><p><a href="https://perldoc.perl.org/podlators">podlators</a> has been upgraded from version 4.14 to 5.01.</p>

</li>
<li><p><a href="https://perldoc.perl.org/POSIX">POSIX</a> has been upgraded from version 2.03 to 2.13.</p>

</li>
<li><p><a href="https://perldoc.perl.org/re">re</a> has been upgraded from version 0.43 to 0.44.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Safe">Safe</a> has been upgraded from version 2.43 to 2.44.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Scalar::Util">Scalar::Util</a> has been upgraded from version 1.62 to 1.63.</p>

</li>
<li><p><a href="https://perldoc.perl.org/SDBM_File">SDBM_File</a> has been upgraded from version 1.15 to 1.17.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Socket">Socket</a> has been upgraded from version 2.033 to 2.036.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Storable">Storable</a> has been upgraded from version 3.26 to 3.32.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Sys::Hostname">Sys::Hostname</a> has been upgraded from version 1.24 to 1.25.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Term::Cap">Term::Cap</a> has been upgraded from version 1.17 to 1.18.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Test::Simple">Test::Simple</a> has been upgraded from version 1.302190 to 1.302194.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Text::Balanced">Text::Balanced</a> has been upgraded from version 2.04 to 2.06.</p>

</li>
<li><p><a href="https://perldoc.perl.org/threads">threads</a> has been upgraded from version 2.27 to 2.36.</p>

</li>
<li><p><a href="https://perldoc.perl.org/threads::shared">threads::shared</a> has been upgraded from version 1.64 to 1.68.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Tie::File">Tie::File</a> has been upgraded from version 1.06 to 1.07.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Time::HiRes">Time::HiRes</a> has been upgraded from version 1.9770 to 1.9775.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Time::Piece">Time::Piece</a> has been upgraded from version 1.3401 to 1.3401_01.</p>

</li>
<li><p><a href="https://perldoc.perl.org/Unicode::Normalize">Unicode::Normalize</a> has been upgraded from version 1.31 to 1.32.</p>

</li>
<li><p><a href="https://perldoc.perl.org/UNIVERSAL">UNIVERSAL</a> has been upgraded from version 1.14 to 1.15.</p>

</li>
<li><p><a href="https://perldoc.perl.org/User::grent">User::grent</a> has been upgraded from version 1.03 to 1.04.</p>

</li>
<li><p><a href="https://perldoc.perl.org/User::pwent">User::pwent</a> has been upgraded from version 1.01 to 1.02.</p>

</li>
<li><p><a href="https://perldoc.perl.org/utf8">utf8</a> has been upgraded from version 1.24 to 1.25.</p>

</li>
<li><p><a href="https://perldoc.perl.org/warnings">warnings</a> has been upgraded from version 1.58 to 1.65.</p>

</li>
<li><p><a href="https://perldoc.perl.org/XS::APItest">XS::APItest</a> has been upgraded from version 1.22 to 1.32.</p>

</li>
<li><p><a href="https://perldoc.perl.org/XSLoader">XSLoader</a> has been upgraded from version 0.31 to 0.32.</p>

</li>
</ul>

<h2 id="Documentation"><a href="#Documentation">#</a>Documentation</h2>

<h2 id="New-Documentation"><a href="#New-Documentation">#</a><a id="New3"></a>New Documentation</h2>

<h3 id="perlclass"><a href="#perlclass">#</a><a href="https://perldoc.perl.org/perlclass">perlclass</a></h3>

<p>Describes the new <code>class</code> feature.</p>

<h3 id="perlclassguts"><a href="#perlclassguts">#</a><a href="https://perldoc.perl.org/perlclassguts">perlclassguts</a></h3>

<p>Describes the internals of the new <code>class</code> feature.</p>

<h2 id="Changes-to-Existing-Documentation"><a href="#Changes-to-Existing-Documentation">#</a><a id="Changes1"></a>Changes to Existing Documentation</h2>

<p>We have attempted to update the documentation to reflect the changes listed in this document. If you find any we have missed, open an issue at <a href="https://github.com/Perl/perl5/issues">https://github.com/Perl/perl5/issues</a>.</p>

<p>Additionally, the following selected changes have been made:</p>

<h3 id="perlapi"><a href="#perlapi">#</a><a href="https://perldoc.perl.org/perlapi">perlapi</a></h3>

<ul>

<li><p>Documented <a href="https://perldoc.perl.org/perlapi#hv_ksplit"><code>hv_ksplit</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#hv_name_set"><code>hv_name_set</code></a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perlapi#hv_store"><code>hv_store</code></a> and <a href="https://perldoc.perl.org/perlapi#hv_stores"><code>hv_stores</code></a> documentation have been greatly improved.</p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#gv_autoload_pv"><code>gv_autoload_pv</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#gv_autoload_pvn"><code>gv_autoload_pvn</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#gv_autoload_sv"><code>gv_autoload_sv</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#gv_name_set"><code>gv_name_set</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#start_subparse"><code>start_subparse</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#SV_CHECK_THINKFIRST_COW_DROP"><code>SV_CHECK_THINKFIRST_COW_DROP</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#SV_CHECK_THINKFIRST"><code>SV_CHECK_THINKFIRST</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#SvPV_shrink_to_cur"><code>SvPV_shrink_to_cur</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#save_aelem"><code>save_aelem</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#save_aelem_flags"><code>save_aelem_flags</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#save_helem"><code>save_helem</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlapi#save_helem_flags"><code>save_helem_flags</code></a></p>

</li>
</ul>

<h3 id="perldeprecation"><a href="#perldeprecation">#</a><a href="https://perldoc.perl.org/perldeprecation">perldeprecation</a></h3>

<ul>

<li><p>Added information about unscheduled deprecations and their categories.</p>

</li>
<li><p>Added category information for existing scheduled deprecations.</p>

</li>
<li><p>Added smartmatch and apostrophe as a package separator deprecation data.</p>

</li>
</ul>

<h3 id="perlintern"><a href="#perlintern">#</a><a href="https://perldoc.perl.org/perlintern">perlintern</a></h3>

<ul>

<li><p>Documented <a href="https://perldoc.perl.org/perlintern#save_pushptr"><code>save_pushptr</code></a></p>

</li>
<li><p>Documented <a href="https://perldoc.perl.org/perlintern#save_scalar_at"><code>save_scalar_at</code></a></p>

</li>
<li><p>Entries have been added to <a href="https://perldoc.perl.org/perlguts">perlguts</a> for the new <code>newAV_alloc_x</code>, <code>newAV_alloc_xz</code> and <code>*_simple</code> functions.</p>

</li>
<li><p>References to the now-defunct PrePAN service have been removed from <a href="https://perldoc.perl.org/perlcommunity">perlcommunity</a> and <a href="https://perldoc.perl.org/perlmodstyle">perlmodstyle</a>.</p>

</li>
<li><p>A section on symbol naming has been added to <a href="https://perldoc.perl.org/perlhacktips">perlhacktips</a>.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perlexperiment">perlexperiment</a> has been edited to properly reference the warning categories for the defer block modifier and extra paired delimiters for quote-like operators.</p>

</li>
</ul>

<h3 id="perlexperiment"><a href="#perlexperiment">#</a><a href="https://perldoc.perl.org/perlexperiment">perlexperiment</a></h3>

<ul>

<li><p>Smartmatch has been moved from experimental status to deprecated status. Unfortunately the experiment did not work out.</p>

</li>
</ul>

<h3 id="perlfunc"><a href="#perlfunc">#</a><a href="https://perldoc.perl.org/perlfunc">perlfunc</a></h3>

<ul>

<li><p>Some wording improvements have been made for the <code>ucfirst</code>, <code>push</code>, <code>unshift</code> and <code>bless</code> functions, as well as additional examples added.</p>

</li>
</ul>

<h3 id="perlhacktips"><a href="#perlhacktips">#</a>perlhacktips</h3>

<ul>

<li><p>A new section, <a href="https://perldoc.perl.org/perlhacktips#Writing-safer-macros">"Writing safer macros" in perlhacktips</a> has been added to discuss pitfalls and solutions to using C macros in C and XS code.</p>

</li>
<li><p>A new section, <a href="https://perldoc.perl.org/perlhacktips#Choosing-good-symbol-names">"Choosing good symbol names" in perlhacktips</a>, has been added to discuss unexpected gotchas with names.</p>

</li>
</ul>

<h3 id="perlop"><a href="#perlop">#</a><a href="https://perldoc.perl.org/perlop">perlop</a></h3>

<ul>

<li><p>Document the behavior of matching the empty pattern better and specify its relationship to the new <code>${^LAST_SUCCESSFUL_PATTERN}</code> properly.</p>

</li>
</ul>

<h3 id="perlvar"><a href="#perlvar">#</a><a href="https://perldoc.perl.org/perlvar">perlvar</a></h3>

<ul>

<li><p>Added a section on "Scoping Rules of Regex Variables", and other wording improvements made throughout.</p>

</li>
<li><p>Added information on the new <code>%{^HOOK}</code> interface, and the new <code>require__before</code> and <code>require__after</code> hooks which it exposes.</p>

</li>
<li><p>Correct information on the regex variables <code>${^PREMATCH}</code>, <code>${^MATCH}</code> and <code>${^POSTMATCH}</code>, all of which were incorrectly documented due to an oversight. Specifically they only work properly after a regex operation that used the /p modifier to enable them.</p>

</li>
<li><p>Added information on the new regex variable <code>${^LAST_SUCCESSFUL_PATTERN}</code>, which represents the pattern of the last successful regex match in scope.</p>

</li>
</ul>

<h2 id="Diagnostics"><a href="#Diagnostics">#</a>Diagnostics</h2>

<p>The following additions or changes have been made to diagnostic output, including warnings and fatal error messages. For the complete list of diagnostic messages, see <a href="https://perldoc.perl.org/perldiag">perldiag</a>.</p>

<h2 id="New-Diagnostics"><a href="#New-Diagnostics">#</a><a id="New4"></a>New Diagnostics</h2>

<h3 id="New-Errors"><a href="#New-Errors">#</a><a id="New5"></a>New Errors</h3>

<ul>

<li><p>A new syntax error has been added for the error that a <code>catch</code> block does not have its required variable declaration. See <a href="https://perldoc.perl.org/perldiag#catch-block-requires-a-%28VAR%29">catch block requires a (VAR)</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Too-many-nested-BEGIN-blocks%2C-maximum-of-%25d-allowed">Too many nested BEGIN blocks, maximum of %d allowed</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Execution-of-%25s-aborted-due-to-compilation-errors.">Execution of %s aborted due to compilation errors.</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Can%27t-locate-object-method-%22INC%22%2C-nor-%22INCDIR%22-nor-string-overload-via-package-%22%25s%22-%25s-in-%40INC">Can't locate object method "INC", nor "INCDIR" nor string overload via package "%s" %s in @INC</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Attempt-to-bless-into-a-class">Attempt to bless into a class</a></p>

<p>(F) You are attempting to call <code>bless</code> with a package name that is a new-style <code>class</code>. This is not necessary, as instances created by the constructor are already in the correct class. Instances cannot be created by other means, such as <code>bless</code>.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Cannot-assign-%3Aparam%28%25s%29-to-field-%25s-because-that-name-is-already-in-use">Cannot assign :param(%s) to field %s because that name is already in use</a></p>

<p>(F) An attempt was made to apply a parameter name to a field, when the name is already being used by another field in the same class, or one of its parent classes. This would cause a name clash so is not allowed.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Cannot-create-class-%25s-as-it-already-has-a-non-empty-%40ISA">Cannot create class %s as it already has a non-empty @ISA</a></p>

<p>(F) An attempt was made to create a class out of a package that already has an <code>@ISA</code> array, and the array is not empty. This is not permitted, as it would lead to a class with inconsistent inheritance.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Cannot-invoke-a-method-of-%22%25s%22-on-an-instance-of-%22%25s%22">Cannot invoke a method of "%s" on an instance of "%s"</a></p>

<p>(F) You tried to directly call a <code>method</code> subroutine of one class by passing in a value that is an instance of a different class. This is not permitted, as the method would not have access to the correct instance fields.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Cannot-invoke-method-on-a-non-instance">Cannot invoke method on a non-instance</a></p>

<p>(F) You tried to directly call a <code>method</code> subroutine of a class by passing in a value that is not an instance of that class. This is not permitted, as the method would not then have access to its instance fields.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Cannot-%27%25s%27-outside-of-a-%27class%27">Cannot '%s' outside of a 'class'</a></p>

<p>(F) You attempted to use one of the keywords that only makes sense inside a <code>class</code> definition, at a location that is not inside such a class.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Cannot-reopen-existing-class-%22%25s%22">Cannot reopen existing class "%s"</a></p>

<p>(F) You tried to begin a <code>class</code> definition for a class that already exists. A class may only have one definition block.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Can%27t-bless-an-object-reference">Can't bless an object reference</a></p>

<p>(F) You attempted to call <code>bless</code> on a value that already refers to a real object instance.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#can%27t-convert-empty-path">can't convert empty path</a></p>

<p>(F) On Cygwin, you called a path conversion function with an empty path. Only non-empty paths are legal.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Class-already-has-a-superclass%2C-cannot-add-another">Class already has a superclass, cannot add another</a></p>

<p>(F) You attempted to specify a second superclass for a <code>class</code> by using the <code>:isa</code> attribute, when one is already specified. Unlike classes whose instances are created with <code>bless</code>, classes created via the <code>class</code> keyword cannot have more than one superclass.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Class-attribute-%25s-requires-a-value">Class attribute %s requires a value</a></p>

<p>(F) You specified an attribute for a class that would require a value to be passed in parentheses, but did not provide one. Remember that whitespace is <b>not</b> permitted between the attribute name and its value; you must write this as</p>

<pre><code>class Example::Class :attr(VALUE) ...</code></pre>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Class-%3Aisa-attribute-requires-a-class-but-%22%25s%22-is-not-one">Class :isa attribute requires a class but "%s" is not one</a></p>

<p>(F) When creating a subclass using the <code>class</code> <code>:isa</code> attribute, the named superclass must also be a real class created using the <code>class</code> keyword.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Field-already-has-a-parameter-name%2C-cannot-add-another">Field already has a parameter name, cannot add another</a></p>

<p>(F) A field may have at most one application of the <code>:param</code> attribute to assign a parameter name to it; once applied a second one is not allowed.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Field-attribute-%25s-requires-a-value">Field attribute %s requires a value</a></p>

<p>(F) You specified an attribute for a field that would require a value to be passed in parentheses, but did not provide one. Remember that whitespace is <b>not</b> permitted between the attribute name and its value; you must write this as</p>

<pre><code>field $var :attr(VALUE) ...</code></pre>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Field-%25s-is-not-accessible-outside-a-method">Field %s is not accessible outside a method</a></p>

<p>(F) An attempt was made to access a field variable of a class from code that does not appear inside the body of a <code>method</code> subroutine. This is not permitted, as only methods will have access to the fields of an instance.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Field-%25s-of-%22%25s%22-is-not-accessible-in-a-method-of-%22%25s%22">Field %s of "%s" is not accessible in a method of "%s"</a></p>

<p>(F) An attempt was made to access a field variable of a class, from a method of another class nested inside the one that actually defined it. This is not permitted, as only methods defined by a given class are permitted to access fields of that class.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Only-scalar-fields-can-take-a-%3Aparam-attribute">Only scalar fields can take a :param attribute</a></p>

<p>(F) You tried to apply the <code>:param</code> attribute to an array or hash field. Currently this is not permitted.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Required-parameter-%27%25s%27-is-missing-for-%25s-constructor">Required parameter '%s' is missing for %s constructor</a></p>

<p>(F) You called the constructor for a class that has a required named parameter, but did not pass that parameter at all.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Unexpected-characters-while-parsing-class-%3Aisa-attribute%3A-%25s">Unexpected characters while parsing class :isa attribute: %s</a></p>

<p>(F) You tried to specify something other than a single class name with an optional trailing version number as the value for a <code>class</code> <code>:isa</code> attribute. This confused the parser.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Unrecognized-class-attribute-%25s">Unrecognized class attribute %s</a></p>

<p>(F) You attempted to add a named attribute to a <code>class</code> definition, but perl does not recognise the name of the requested attribute.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Unrecognized-field-attribute-%25s">Unrecognized field attribute %s</a></p>

<p>(F) You attempted to add a named attribute to a <code>field</code> definition, but perl does not recognise the name of the requested attribute.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#%24%7B%5EHOOK%7D%7B%25s%7D-may-only-be-a-CODE-reference-or-undef">${^HOOK}{%s} may only be a CODE reference or undef</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Attempt-to-set-unknown-hook-%27%25s%27-in-%25%7B%5EHOOK%7D">Attempt to set unknown hook '%s' in %{^HOOK}</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Missing-or-undefined-argument-to-%25s-via-%25%7B%5EHOOK%7D%7Brequire__before%7D">Missing or undefined argument to %s via %{^HOOK}{require__before}</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Too-many-capture-groups-%28limit-is-%25d%29-in-regex-m%2F%25s%2F">Too many capture groups (limit is %d) in regex m/%s/</a></p>

</li>
</ul>

<h3 id="New-Warnings"><a href="#New-Warnings">#</a><a id="New6"></a>New Warnings</h3>

<ul>

<li><p><a href="https://perldoc.perl.org/perldiag#Unknown-locale-category-%25d">Unknown locale category %d</a></p>

<p>This is a shortened form of an already existing diagnostic, for use when there is no new locale being switched to. The previous diagnostic was misleading in such circumstances.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Locale-%27%25s%27-is-unsupported%2C-and-may-crash-the-interpreter.">Locale '%s' is unsupported, and may crash the interpreter.</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Treating-%25s%3A%3AINIT-block-as-BEGIN-block-as-workaround">Treating %s::INIT block as BEGIN block as workaround</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Filehandle-STD%25s-reopened-as-%25s-only-for-input">Filehandle STD%s reopened as %s only for input</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#%25s-on-BEGIN-block-ignored">%s on BEGIN block ignored</a></p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#ADJUST-is-experimental">ADJUST is experimental</a></p>

<p>(S experimental::class) This warning is emitted if you use the <code>ADJUST</code> keyword of <code>use feature 'class'</code>. This keyword is currently experimental and its behaviour may change in future releases of Perl.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#class-is-experimental">class is experimental</a></p>

<p>(S experimental::class) This warning is emitted if you use the <code>class</code> keyword of <code>use feature 'class'</code>. This keyword is currently experimental and its behaviour may change in future releases of Perl.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Method-%25s-redefined">Method %s redefined</a></p>

<p>(W redefine) You redefined a method. To suppress this warning, say</p>

<pre><code>{
   no warnings 'redefine';
   *name = method { ... };
}</code></pre>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Odd-number-of-elements-in-hash-field-initialization">Odd number of elements in hash field initialization</a></p>

<p>(W misc) You specified an odd number of elements to initialise a hash field of an object. Hashes are initialised from a list of key/value pairs so there must be a corresponding value to every key. The final missing value will be filled in with undef instead.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Old-package-separator-%22%27%22-deprecated">Old package separator "'" deprecated</a></p>

<p>(W deprecated, syntax) You used the old package separator "'" in a variable, subroutine or package name. Support for the old package separator will be removed in Perl 5.40.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#field-is-experimental">field is experimental</a></p>

<p>(S experimental::class) This warning is emitted if you use the <code>field</code> keyword of <code>use feature 'class'</code>. This keyword is currently experimental and its behaviour may change in future releases of Perl.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#method-is-experimental">method is experimental</a></p>

<p>(S experimental::class) This warning is emitted if you use the <code>method</code> keyword of <code>use feature 'class'</code>. This keyword is currently experimental and its behaviour may change in future releases of Perl.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Can%27t-call-destructor-for-0x%25p-in-global-destruction">Can't call destructor for 0x%p in global destruction</a></p>

</li>
</ul>

<h2 id="Changes-to-Existing-Diagnostics"><a href="#Changes-to-Existing-Diagnostics">#</a><a id="Changes2"></a>Changes to Existing Diagnostics</h2>

<ul>

<li><p>The compiler will now stop parsing on the first syntax error it encounters. Historically the compiler would attempt to "skip past" the error and continue parsing so that it could list multiple errors. For things like undeclared variables under strict this makes sense. For syntax errors however it has been found that continuing tends to result in a storm of unrelated or bizarre errors that mostly just obscure the true error. In extreme cases it can even lead to segfaults and other incorrect behavior.</p>

<p>Therefore we have reformed the continuation logic so that the parse will stop after the first seen syntax error. Semantic errors like undeclared variables will not stop the parse, so you may still see multiple errors when compiling code. However if there is a syntax error it will be the last error message reported by perl and all of the errors that you see will be something that actually needs to be fixed.</p>

</li>
<li><p>Error messages that output class or package names have been modified to output double quoted strings with various characters escaped so as to make the exact value clear to a reader. The exact rules on which characters are escaped may change over time but currently are that printable ASCII codepoints, with the exception of <code>"</code> and <code>\</code>, and unicode word characters whose codepoint is over 255 are output raw, and any other symbols are escaped much as Data::Dumper might escape them, using <code>\n</code> for newline and <code>\"</code> for double quotes, etc. Codepoints in the range 128-255 are always escaped as they can cause trouble on unicode terminals when output raw.</p>

<p>In older versions of perl the one liner</p>

<pre><code>$ perl -le'"thing\n"-&gt;foo()'</code></pre>

<p>would output the following error message exactly as shown here, with text spread over multiple lines because the "\n" would be emitted as a raw newline character:</p>

<pre><code>Can't locate object method "foo" via package "thing
" (perhaps you forgot to load "thing
"?) at -e line 1.</code></pre>

<p>As of this release we would output this instead (as one line):</p>

<pre><code>Can't locate object method "foo" via package "thing\n"
  (perhaps you forgot to load "thing\n"?) at -e line 1.</code></pre>

<p>Notice the newline in the package name has been quoted and escaped, and thus the error message is a single line. The text is shown here wrapped to two lines only for readability.</p>

</li>
<li><p>When package or class names in errors are very large the middle excess portion will be elided from the message. As of this release error messages will show only up to the first 128 characters and the last 128 characters in a package or class name in error messages. For example</p>

<pre><code>$ perl -le'("Foo" x 1000)-&gt;new()'</code></pre>

<p>will output the following as one line:</p>

<pre><code>Can't locate object method "new" via package "FooFooFooFooFooFooFoo
FooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFoo
FooFooFooFooFooFooFooFooFooFooFooFooFooFo"..."oFooFooFooFooFooFooFoo
FooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFoo
FooFooFooFooFooFooFooFooFooFooFooFooFoo" (perhaps you forgot to load
"FooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFoo
FooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFo"...
"oFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFoo
FooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFooFoo"?)
at -e line 1.</code></pre>

<p>Notice the <code> "prefix"..."suffix" </code> form of the package name in this case. In previous versions of perl the complete string would have been shown making the error message over 6k long and there was no upper limit on the length of the error message at all. If you accidentally used a 1MB string as a class name then the error message would be over 2MB long. In this perl the upper limit should be around 2k when eliding and escaping are taken into account.</p>

</li>
<li><p>Removed <code>Complex regular subexpression recursion limit (%d) exceeded</code></p>

<p>The regular expresion engine has not used recursion in some time. This warning no longer makes sense.</p>

<p>See [<a href="https://github.com/Perl/perl5/pull/19636">GH #19636</a>].</p>

</li>
<li><p>Various warnings that used to produce parenthesized hints underneath the main warning message and after its "location data" were chanaged to put the hint inline with the main message. For instance:</p>

<pre><code>Bareword found where operator expected at -e line 1, near "foo bar"
    (Do you need to predeclare foo?)</code></pre>

<p>will now look like this but as one line:</p>

<pre><code>Bareword found where operator expected (Do you need to predeclare
foo?) at -e line 1, near "foo bar"</code></pre>

<p>as a result such warnings will no longer trigger <code>$SIG{__WARN__}</code> twice, and the hint will be visible when fatal warnings is in effect.</p>

</li>
<li><p>The error message that is produced when a <code>require</code> or <code>use</code> statement fails has been changed. It used to contain the words <code>@INC contains:</code>, and it used to show the state of <code>@INC</code> *after* the require had completed and failed. The error message has been changed to say <code>@INC entries checked:</code> and to reflect the actual directories or hooks that were executed during the require statement. For example:</p>

<pre><code>perl -e'push @INC, sub {@INC=()}; eval "require Frobnitz"
    or die $@'
Can't locate Frobnitz.pm in @INC (you may need to install the
Frobnitz module) (@INC contains:) at (eval 1) line 1.</code></pre>

<p>Will change to (with some output elided for clarity):</p>

<pre><code>perl -e'push @INC, sub {@INC=()}; eval "require Frobnitz"
    or die $@'
Can't locate Frobnitz.pm in @INC (you may need to install the
Frobnitz module) (@INC entries checked:
.../site_perl/5.38.0/x86_64-linux .../site_perl/5.38.0
.../5.38.0/x86_64-linux .../5.38.0 CODE(0x562745e684b8))
at (eval 1) line 1.</code></pre>

<p>thus showing the actual directories checked. Code that checks for <code>@INC contains:</code> in error messages should be hardened against any future wording changes between the <code>@INC</code> and <code>:</code>, for instance use <code>qr/\@INC[ \w]+:/</code> instead of using <code>qr/\@INC contains:/</code> or <code>qr/\@INC entries checked:/</code> in tests as this will ensure both forward and backward compatibility.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Old-package-separator-used-in-string">Old package separator used in string</a></p>

<p>This diagnostic is now also part of the <code>deprecated</code> category.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#given-is-deprecated">given is deprecated</a> replaces <code>given is experimental</code>.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#when-is-deprecated">when is deprecated</a> replaces <code>when is experimental</code>.</p>

</li>
<li><p><a href="https://perldoc.perl.org/perldiag#Smartmatch-is-deprecated">Smartmatch is deprecated</a> replaces <code>Smartmatch is experimental</code>.</p>

</li>
</ul>

<h2 id="Configuration-and-Compilation"><a href="#Configuration-and-Compilation">#</a><a id="Configuration"></a>Configuration and Compilation</h2>

<ul>

<li><p><code>make -j6 minitest</code> could fail due to a build conflict in building <code>$(MINIPERL_EXE)</code> between the main make process and a child process. [<a href="https://github.com/Perl/perl5/issues/19829">GH #19829</a>]</p>

</li>
<li><p>Properly populate osvers on Dragonfly BSD when the hostname isn't set.</p>

</li>
<li><p>Fix typos for C99 macro name <code>PRIX64</code>.</p>

</li>
<li><p>Remove ancient and broken GCC for VMS support</p>

</li>
<li><p>Remove vestigial reference to <code>/VAXC</code> qualifier</p>

</li>
<li><p>Remove sharedperl option on VMS</p>

</li>
<li><p>VMS now has mkostemp</p>

</li>
<li><p><code>Configure</code> now properly handles quoted elements outputted by gcc. [<a href="https://github.com/Perl/perl5/issues/20606">GH #20606</a>]</p>

</li>
<li><p><code>Configure</code> probed for the return type of malloc() and free() by testing whether declarations for those functions produced a function type mismatch with the implementation. On Solaris, with a C++ compiler, this check always failed, since Solaris instead imports malloc() and free() from <code>std::</code> with <code>using</code> for C++ builds. Since the return types of malloc() and free() are well defined by the C standard, skip probing for them. <code>Configure</code> command-line arguments and hints can still override these type in the unlikely case that is needed. [<a href="https://github.com/Perl/perl5/issues/20806">GH #20806</a>]</p>

</li>
</ul>

<h2 id="Testing"><a href="#Testing">#</a>Testing</h2>

<p>Tests were added and changed to reflect the other additions and changes in this release. Furthermore, these significant changes were made:</p>

<ul>

<li><p>Unicode normalization tests have been added.</p>

</li>
<li><p>t/test.pl: Add ability to cancel an watchdog timer</p>

</li>
</ul>

<h2 id="Platform-Support"><a href="#Platform-Support">#</a><a id="Platform"></a>Platform Support</h2>

<h2 id="Discontinued-Platforms"><a href="#Discontinued-Platforms">#</a><a id="Discontinued"></a>Discontinued Platforms</h2>

<dl>

<dt id="Ultrix"><a href="#Ultrix">#</a>Ultrix</dt>
<dd>

<p>Support code for DEC Ultrix has been removed. Ultrix was the native Unix-like operating system for various Digital Equipment Corporation machines. Its final release was in 1995.</p>

</dd>
</dl>

<h2 id="Platform-Specific-Notes"><a href="#Platform-Specific-Notes">#</a><a id="Platform1"></a>Platform-Specific Notes</h2>

<dl>

<dt id="DragonflyBSD"><a href="#DragonflyBSD">#</a>DragonflyBSD</dt>
<dd>

<p>Skip tests to workaround an apparent bug in <code>setproctitle()</code>. [<a href="https://github.com/Perl/perl5/issues/19894">GH #19894</a>]</p>

</dd>
<dt id="FreeBSD"><a href="#FreeBSD">#</a>FreeBSD</dt>
<dd>

<p>FreeBSD no longer uses thread-safe locale operations, to avoid <a href="https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=265950">a bug in FreeBSD</a></p>

<p>Replace the first part of archname with <code>uname -p</code> [<a href="https://github.com/Perl/perl5/issues/19791">GH #19791</a>]</p>

</dd>
<dt id="Solaris"><a href="#Solaris">#</a>Solaris</dt>
<dd>

<p>Avoid some compiler and compilation issues on NetBSD/Solaris from regexec.c and regcomp.c.</p>

</dd>
<dt id="Synology"><a href="#Synology">#</a>Synology</dt>
<dd>

<p>Update Synology Readme for DSM 7.</p>

</dd>
<dt id="Windows"><a href="#Windows">#</a>Windows</dt>
<dd>

<p>Fix win32 memory alignment needed for gcc-12 from vmem.h.</p>

<p>utimes() on Win32 would print a message to stderr if it failed to convert a supplied <code>time_t</code> to to a <code>FILETIME</code>. [<a href="https://github.com/Perl/perl5/issues/19668">GH #19668</a>]</p>

<p>In some cases, timestamps returned by <a href="https://perldoc.perl.org/perlfunc#stat">stat()</a> and <a href="https://perldoc.perl.org/perlfunc#lstat">lstat()</a> failed to take daylight saving time into account. [<a href="https://github.com/Perl/perl5/issues/20018">GH #20018</a>] [<a href="https://github.com/Perl/perl5/issues/20061">GH #20061</a>]</p>

<p>stat() now works on <code>AF_UNIX</code> socket files. [<a href="https://github.com/Perl/perl5/issues/20204">GH #20204</a>]</p>

<p>readlink() now returns the <code>PrintName</code> from a symbolic link reparse point instead of the <code>SubstituteName</code>, which should make it better match the name the link was created with. [<a href="https://github.com/Perl/perl5/pull/20271">GH #20271</a>]</p>

<p>lstat() on Windows now returns the length of the link target as the size of the file, as it does on POSIX systems. [<a href="https://github.com/Perl/perl5/issues/20476">GH #20476</a>]</p>

<p>symlink() on Windows now replaces any <code>/</code> in the target with <code>\</code>, since Windows does not recognise <code>/</code> in symbolic links. The reverse translation is <b>not</b> done by readlink(). [<a href="https://github.com/Perl/perl5/issues/20506">GH #20506</a>]</p>

<p>symlink() where the target was an absolute path to a directory was incorrectly created as a file symbolic link. [<a href="https://github.com/Perl/perl5/issues/20533">GH #20533</a>]</p>

<p><code>POSIX::dup2</code> no longer creates broken sockets. [<a href="https://github.com/Perl/perl5/issues/20920">GH #20920</a>]</p>

<p>Closing a busy pipe could cause Perl to hang. [<a href="https://github.com/Perl/perl5/issues/19963">GH #19963</a>]</p>

</dd>
</dl>

<h2 id="Internal-Changes"><a href="#Internal-Changes">#</a><a id="Internal"></a>Internal Changes</h2>

<ul>

<li><p>Removed many deprecated C functions.</p>

<p>These have been deprecated for a long time. See <a href="https://github.com/perl/perl5/commit/7008caa915ad99e650acf2aea40612b5e48b7ba2">https://github.com/perl/perl5/commit/7008caa915ad99e650acf2aea40612b5e48b7ba2</a> for a full list.</p>

</li>
<li><p><code>get_op_descs</code>, <code>get_op_names</code>, <code>get_opargs</code>, <code>get_no_modify</code> and <code>get_ppaddr</code> have been marked deprecated.</p>

</li>
<li><p><code>hv_free_ent</code> has been marked as internal API.</p>

</li>
<li><p><code>save_pushptr</code>, <code>save_pushptrptr</code>, and <code>save_pushi32ptr</code> have been marked as internal API.</p>

</li>
<li><p>New bool related functions and macros have been added to complement the new bool type introduced in 5.36:</p>

<p>The functions are:</p>

<dl>

<dt id="newSVbool(const-bool-bool_val)"><a href="#newSVbool(const-bool-bool_val)">#</a><a id="newSVbool"></a><a id="newSVbool-const-bool-bool_val"></a><a href="https://perldoc.perl.org/perlapi#newSVbool"><code>newSVbool(const bool bool_val)</code></a></dt>
<dd>

</dd>
<dt id="newSV_true()"><a href="#newSV_true()">#</a><a id="newSV_true"></a><a href="https://perldoc.perl.org/perlapi#newSV_true"><code>newSV_true()</code></a></dt>
<dd>

</dd>
<dt id="newSV_false()"><a href="#newSV_false()">#</a><a id="newSV_false"></a><a href="https://perldoc.perl.org/perlapi#newSV_false"><code>newSV_false()</code></a></dt>
<dd>

</dd>
<dt id="sv_set_true(SV-*sv)"><a href="#sv_set_true(SV-*sv)">#</a><a id="sv_set_true"></a><a id="sv_set_true-SV-sv"></a><a href="https://perldoc.perl.org/perlapi#sv_set_true"><code>sv_set_true(SV *sv)</code></a></dt>
<dd>

</dd>
<dt id="sv_set_false(SV-*sv)"><a href="#sv_set_false(SV-*sv)">#</a><a id="sv_set_false"></a><a id="sv_set_false-SV-sv"></a><a href="https://perldoc.perl.org/perlapi#sv_set_false"><code>sv_set_false(SV *sv)</code></a></dt>
<dd>

</dd>
<dt id="sv_set_bool(SV-*sv,-const-bool-bool_val)"><a href="#sv_set_bool(SV-*sv,-const-bool-bool_val)">#</a><a id="sv_set_bool"></a><a id="sv_set_bool-SV-sv-const-bool-bool_val"></a><a href="https://perldoc.perl.org/perlapi#sv_set_bool"><code>sv_set_bool(SV *sv, const bool bool_val)</code></a></dt>
<dd>

</dd>
</dl>

<p>The macros are:</p>

<dl>

<dt id="SvIandPOK(sv)"><a href="#SvIandPOK(sv)">#</a><a id="SvIandPOK"></a><a id="SvIandPOK-sv"></a><a href="https://perldoc.perl.org/perlapi#SvIandPOK"><code>SvIandPOK(sv)</code></a></dt>
<dd>

</dd>
<dt id="SvIandPOK_off(sv)"><a href="#SvIandPOK_off(sv)">#</a><a id="SvIandPOK_off"></a><a id="SvIandPOK_off-sv"></a><a href="https://perldoc.perl.org/perlapi#SvIandPOK_off"><code>SvIandPOK_off(sv)</code></a></dt>
<dd>

</dd>
<dt id="SvIandPOK_on"><a href="#SvIandPOK_on">#</a><a href="https://perldoc.perl.org/perlapi#SvIandPOK_on"><code>SvIandPOK_on</code></a></dt>
<dd>

</dd>
</dl>

</li>
<li><p>Perl is no longer manipulating the <code>environ</code> array directly. The variable <code>PL_use_safe_putenv</code> has been removed and <code>PERL_USE_SAFE_PUTENV</code> is always defined. This means XS modules can now call <code>setenv</code> and <code>putenv</code> without causing segfaults. [<a href="https://github.com/Perl/perl5/issues/19399">perl #19399</a>]</p>

</li>
<li><p>Internal C API functions are now hidden with <code>__attribute__((hidden))</code> on the platforms that support it. This means they are no longer callable from XS modules on those platforms.</p>

<p>It should be noted that those functions have always been hidden on Windows. This change merely brings that to the other platforms. [<a href="https://github.com/Perl/perl5/pull/19655">perl #19655</a>]</p>

</li>
<li><p>New formatting symbols were added for printing values declared as <code>U32</code> or <code>I32</code>:</p>

<dl>

<dt id="I32df-Like-%d"><a href="#I32df-Like-%25d">#</a><a id="I32df"></a><a id="I32df----Like-d"></a>I32df -- Like %d</dt>
<dd>

</dd>
<dt id="U32of-Like-%o"><a href="#U32of-Like-%25o">#</a><a id="U32of"></a><a id="U32of----Like-o"></a>U32of -- Like %o</dt>
<dd>

</dd>
<dt id="U32uf-Like-%u"><a href="#U32uf-Like-%25u">#</a><a id="U32uf"></a><a id="U32uf----Like-u"></a>U32uf -- Like %u</dt>
<dd>

</dd>
<dt id="U32xf-Like-%x"><a href="#U32xf-Like-%25x">#</a><a id="U32xf"></a><a id="U32xf----Like-x"></a>U32xf -- Like %x</dt>
<dd>

</dd>
<dt id="U32Xf-Like-%X"><a href="#U32Xf-Like-%25X">#</a><a id="U32Xf"></a><a id="U32Xf----Like-X"></a>U32Xf -- Like %X</dt>
<dd>

</dd>
</dl>

<p>These are used in the same way already existing similar symbols, such as <code>IVdf</code>, are used. See <a href="https://perldoc.perl.org/perlapi#I%2FO-Formats">"I/O Formats" in perlapi</a>.</p>

</li>
<li><p>new 'HvHasAUX' macro</p>

</li>
<li><p>regexec.c: Add some branch predictions reorder conds</p>

</li>
<li><p>locale: Change macro name to be C conformant</p>

</li>
<li><p>Rename the <code>PADNAMEt_*</code> constants to <code>PADNAMEf_*</code></p>

</li>
<li><p>Changes all the API macros that retrieve a PV into a call to an inline function so as to evaluate the parameter just once.</p>

</li>
<li><p>regexec.c: multiple code refactor to make the code more readable</p>

</li>
<li><p>perl.h: Change macro name to be C conformant (remove leading _ from NOT_IN_NUMERIC macros)</p>

</li>
<li><p>regcomp.h: add new <code>BITMAP_BIT</code> macro in addition to the existing <code>BITMAP_BYTE</code> and <code>BITMAP_TEST</code> ones.</p>

</li>
<li><p>Create new regnode type ANYOFH. populate_ANYOF_from_invlist was renamed to populate_bitmap_from_invlist</p>

</li>
<li><p>regex: Refactor bitmap vs non-bitmap of qr/[]/</p>

</li>
<li><p>regcomp.c: add new functions to convert from an inversion list to a bitmap (and vice versa) <code>populate_bitmap_from_invlist</code> and <code>populate_invlist_from_bitmap</code>.</p>

</li>
<li><p>Add <code>newAVav()</code> to create an AV from an existing AV. Add <code>newAVhv()</code> to create an AV using keys and values from an existing HV.</p>

</li>
<li><p>Fix definition of <code>Perl_atof</code>.</p>

</li>
<li><p>Fix undefined behavior with overflow related <code>OPTIMIZE_INFTY</code> and delta in <i>regcomp.c</i>.</p>

</li>
<li><p>Fix regnode pointer alignment issue in <i>regcomp.h</i>.</p>

</li>
<li><p>The <code>CVf_METHOD</code> CV flag and associated <code>CvMETHOD</code> macro has been renamed to <code>CVf_NOWARN_AMBIGUOUS</code> and <code>CvNOWARN_AMBIGUOUS</code>. This closer reflects its actual behaviour (it suppresses a warning that would otherwise be generated about ambiguous names), in order to be less confusing with <code>CvIsMETHOD</code>, which indicates that a CV is a <code>method</code> subroutine relating to the <code>class</code> feature.</p>

</li>
<li><p>The <code>OPf_SPECIAL</code> flag is no longer set on the <code>OP_ENTERSUB</code> op constructed to call the <code>VERSION</code>, <code>import</code> and <code>unimport</code> methods as part of a <code>use</code> statement and attribute application, nor when assigning to an <code>:lvalue</code> subroutine.</p>

</li>
<li><p>A new CV flag <code>CVf_REFCOUNTED_ANYSV</code> has been added, which indicates that the CV is an XSUB and stores an SV pointer in the <code>CvXSUBANY.any_sv</code> union field. Perl core operations such as cloning or destroying the CV will maintain the reference count of the pointed-to SV, destroying it when required.</p>

</li>
<li><p>A new API function <a href="https://perldoc.perl.org/perlapi#Perl_localeconv">"<code>Perl_localeconv</code>" in perlapi</a> is added. This is the same as <a href="https://perldoc.perl.org/POSIX#localeconv"><code>POSIX::localeconv</code></a> (returning a hash of the <code>localeconv()</code> fields), but directly callable from XS code.</p>

</li>
<li><p>A new API function, <a href="https://perldoc.perl.org/perlapi#Perl_langinfo8">"<code>Perl_langinfo8</code>" in perlapi</a> is added. This is the same as plain <a href="https://perldoc.perl.org/perlapi#Perl_langinfo">"<code>Perl_langinfo</code>" in perlapi</a>, but with an extra parameter that allows the caller to simply and reliably know if the returned string is UTF-8.</p>

</li>
<li><p>We have introduced a limit on the number of nested <code>eval EXPR</code>/<code>BEGIN</code> blocks and <code>require</code>/<code>BEGIN</code> (and thus <code>use</code> statements as well) to prevent C stack overflows. This variable can also be used to forbid <code>BEGIN</code> blocks from executing during <code>eval EXPR</code> compilation. The limit defaults to <code>1000</code> but can be overridden by setting the <code>${^MAX_NESTED_EVAL_BEGIN_BLOCKS}</code> variable. The default itself can be changed at compile time with</p>

<pre><code>-Accflags='-DPERL_MAX_NESTED_EVAL_BEGIN_BLOCKS_DEFAULT=12345'</code></pre>

<p>Note that this value relates to the size of your C stack and if you choose an inappropriately large value Perl may segfault, be conservative about what you choose.</p>

</li>
<li><p>A new magic type <code>PERL_MAGIC_extvalue</code> has been added. This is available for use like <code>PERL_MAGIC_ext</code>, but is a value magic: upon localization the new value will not be magical.</p>

</li>
<li><p>The <code>SSNEW()</code>, <code>SSNEWt()</code>, <code>SSNEWa()</code> and <code>SSNEWat()</code> APIs now return a <code>SSize_t</code> value. The <code>SSPTR()</code> and <code>SSPTRt()</code> macros now expect a <code>SSize_t</code> parameter, and enforce that on debugging builds. [<a href="https://github.com/Perl/perl5/issues/20411">GH #20411</a>]</p>

</li>
<li><p>Filenames in cops are now refcounted under threads. Under threads we were copying the filenames into each opcode. This is because in theory opcodes created in one thread can be destroyed in another. The change adds a new struct/type <code>RCPV</code>, which is a refcounted string using shared memory. This is implemented in such a way that code that previously used a char * can continue to do so, as the refcounting data is located a specific offset before the char * pointer itself.</p>

</li>
<li><p>Added <code>HvNAMEf</code> and <code>HvNAMEf_QUOTEDPREFIX</code> special formats. They take an <code>HV *</code> as an argument and use <code>HvNAME()</code> and related macros to determine the string, its length, and whether it is utf8.</p>

</li>
<li><p>The underlying <code>Perl_dowantarray</code> function implementing the long-deprecated <a href="https://perldoc.perl.org/perlapi#GIMME"><code>GIMME</code></a> macro has been marked as deprecated, so that use of the macro emits a compile-time warning. <code>GIMME</code> has been documented as deprecated in favour of <a href="https://perldoc.perl.org/perlapi#GIMME_V"><code>GIMME_V</code></a> since Perl v5.6.0, but had not previously issued a warning.</p>

</li>
<li><p>The API function <a href="https://perldoc.perl.org/perlapi#utf8_length">"utf8_length" in perlapi</a> is now more efficient.</p>

</li>
<li><p>Added <code>SAVERCPV()</code> and <code>SAVEFREERCPV()</code> for better support for working with <code>RCPV</code> (reference counted string/pointer value) structures which currently are used in opcodes to share filename and warning bit data in a memory efficient manner.</p>

</li>
<li><p>Added <code>MORTALSVFUNC_SV()</code> and <code>MORTALDESTRUCTOR_SV()</code> macros, which make it possible to create a destructor which is fired at the end of the current statement. This uses the <code>PERL_MAGIC_destruct</code> magic to use "free" magic to trigger an action when a variable is freed. The action can be specified as a C function or as a Perl code reference.</p>

</li>
<li><p>Added the <code>%{^HOOK}</code> api and related <code>PERL_MAGIC_hook</code> and <code>PERL_MAGIC_hookelem</code> for providing ways to hook selected perl functions which for one reason or another are problematic to wrap with a customized subroutine.</p>

</li>
<li><p>Added support for <code>${^HOOK}{require__before}</code> which can be used to rewrite the filename that <code>require</code> will try to load, and also to block <code>require</code> from loading a specific module, even via fully qualified filename. The hook can also be used to perform "pre-require" and "post-require" actions.</p>

</li>
<li><p>Added support for <code>${^HOOK}{require__after}</code> which can be used to track what modules have been required after the fact.</p>

</li>
<li><p>Regular expression opcodes (regops) now use a standardized structure layout that uses unions to expose data in different format. This means it should be much easier to extend or modify regops to use more memory. This has been used to make a number of regops track how many parens they contain.</p>

</li>
</ul>

<h2 id="Selected-Bug-Fixes"><a href="#Selected-Bug-Fixes">#</a><a id="Selected"></a>Selected Bug Fixes</h2>

<ul>

<li><p>Avoid recursion and stack overflow parsing 'pack' template</p>

<p>[<a href="https://github.com/Perl/perl5/issues/16319">GH #16319</a>]</p>

</li>
<li><p>An eval() as the last statement in a regex code block could trigger an interpreter panic; e.g.</p>

<pre><code>/(?{ ...; eval {....}; })/</code></pre>

<p>[<a href="https://github.com/Perl/perl5/issues/19680">GH #19680</a>]</p>

</li>
<li><p>Disabling the <code>bareword_filehandles</code> feature no longer treats <code>print Class-&gt;method</code> as an error. [<a href="https://github.com/Perl/perl5/issues/19704">GH #19704</a>]</p>

</li>
<li><p>When a Perl subroutine tail-calls an XS subroutine using <code>goto &amp;xs_sub</code>, the XS subroutine can now correctly determine its calling context. Previously it was always reported as scalar.</p>

<p>In addition, where the Perl subroutine is freed at the same time:</p>

<pre><code>sub foo { *foo = sub {}; goto &amp;xs_sub }</code></pre>

<p>this formerly could lead to crashes if the XS subroutine tried to use the value of <code>PL_op</code>, since this was being set to NULL. This is now fixed.</p>

<p>[<a href="https://github.com/Perl/perl5/issues/19936">GH #19936</a>]</p>

</li>
<li><p>setsockopt() now uses the mechanism added in 5.36 to better distinguish between numeric and string values supplied as the <code>OPTVAL</code> parameter. [<a href="https://github.com/Perl/perl5/issues/18761">GH #18761</a>]</p>

</li>
<li><p>4-argument <code>select()</code> now rejects strings with code points above 255. Additionally, for code points 128-255, this operator will now always give the corresponding octet to the OS, regardless of how Perl stores such code points in memory. (Previously Perl leaked its internal string storage to the OS.) [<a href="https://github.com/Perl/perl5/issues/19882">GH #19882</a>]</p>

</li>
<li><p>Fix panic issue from <code>val {} inside /(?{...})/</code> [<a href="https://github.com/Perl/perl5/issues/19390">GH #19390</a>]</p>

</li>
<li><p>Fix multiple compiler warnings from <i>regexp.c</i>, <i>locale.c</i> [<a href="https://github.com/Perl/perl5/issues/19915">GH #19915</a>]</p>

</li>
<li><p>Fix a bug with querying locales on platforms that don't have <code>LC_NUMERIC</code> [<a href="https://github.com/Perl/perl5/issues/19890">GH #19890</a>]</p>

</li>
<li><p>Prevent undefined behaviour in <code>S_maybe_multideref()</code>.</p>

</li>
<li><p>Avoid signed integer overflow in <code>use integer</code> ops.</p>

</li>
<li><p>Avoid adding an offset to a NULL pointer in <code>hv_delete_common</code>.</p>

</li>
<li><p>PerlIO::get_layers will now accept IO references too</p>

<p>Previously it would only take glob references or names of globs. Now it will also accept IO references.</p>

</li>
<li><p>Fixes to memory handling for <code>PL_splitstr</code>:</p>

<ul>

<li><p>If a thread was created the allocated string would be freed twice.</p>

</li>
<li><p>If two <code>-F</code> switches were supplied the memory allocated for the first switch wouldn't be freed.</p>

</li>
</ul>

</li>
<li><p>Correctly handle <code>OP_ANONCODE</code> ops generated by CPAN modules that don't include the OPf_REF flag when propagating lvalue context. [<a href="https://github.com/Perl/perl5/pull/20532">GH #20532</a>]</p>

</li>
<li><p><a href="https://perldoc.perl.org/POSIX#strxfrm">POSIX::strxfrm</a> now uses the <code>LC_CTYPE</code> locale category to specify its collation, ignoring any differing <code>LC_COLLATE</code>. It doesn't make sense for a string to be encoded in one locale (say, ISO-8859-6, Arabic) and to collate it based on another (like ISO-8859-7, Greek). Perl assumes that the current <code>LC_CTYPE</code> locale correctly represents the encoding, and collates accordingly.</p>

<p>Also, embedded <code>NUL</code> characters are now allowed in the input.</p>

<p>If locale collation is not enabled on the platform (<code>LC_COLLATE</code>), the input is returned unchanged.</p>

</li>
<li><p>Double FETCH during stringification of tied scalars returning an overloaded object have been fixed. The FETCH method should only be called once, but prior to this release was actually called twice. [<a href="https://github.com/Perl/perl5/pull/20574">GH #20574</a>]</p>

</li>
<li><p>Writing to a magic variables associated with the selected output handle, <code>$^</code>, <code>$~</code>, <code>$=</code>, <code>$-</code> and <code>$%</code>, no longer crashes perl if the IO object has been cleared from the selected output handle. [<a href="https://github.com/Perl/perl5/issues/20733">GH #20733</a>]</p>

</li>
<li><p>Redefining a <code>use constant</code> list constant with <code>use constant</code> now properly warns. This changes the behaviour of <code>use constant</code> but is a core change, not a change to <i>constant.pm</i>. [<a href="https://github.com/Perl/perl5/issues/20742">GH #20742</a>]</p>

</li>
<li><p>Redefining a <code>use constant</code> list constant with an empty prototype constant sub would result in an assertion failure. [<a href="https://github.com/Perl/perl5/issues/20742">GH #20742</a>]</p>

</li>
<li><p>Fixed a regression where the <code>INC</code> method for objects in <code>@INC</code> would not be resolved by <code>AUTOLOAD</code>, while it was in 5.36. The <code>INCDIR</code> method for objects in <code>@INC</code> cannot be resolved by <code>AUTOLOAD</code> as <code>INC</code> would have been resolved first. [<a href="https://github.com/Perl/perl5/issues/20665">GH #20665</a>]</p>

</li>
<li><p><code>$SIG{__DIE__}</code> will now be called from eval when the code dies during compilation regardless of how it dies. This means that code expecting to be able to upgrade <code>$@</code> into an object will be called consistently. In earlier versions of perl <code>$SIG{__DIE__}</code> would not be called for certain compilation errors, for instance undeclared variables. For other errors it might be called if there were more than a certain number of errors, but not if there were less. Now you can expect that it will be called in every case.</p>

</li>
<li><p>Compilation of code with errors used to inconsistently stop depending on the count and type of errors encountered. The intent was that after 10 errors compilation would halt, but bugs in this logic meant that certain types of error would be counted, but would not trigger the threshold check to stop compilation. Other errors would. With this release after at most 10 errors compilation will terminate, regardless of what type of error they were.</p>

<p>Note that you can change the maximum count by defining <code>PERL_STOP_PARSING_AFTER_N_ERRORS</code> to be something else during the configuration process. For instance</p>

<pre><code>./Configure ... -Accflags='-DPERL_STOP_PARSING_AFTER_N_ERRORS=100'</code></pre>

<p>would allow up to 100 errors.</p>

</li>
<li><p>The API function <a href="https://perldoc.perl.org/perlapi#my_snprintf">"my_snprintf" in perlapi</a> now prints a non-dot decimal point if the perl code it ultimately is called from is in the scope of <code>use locale</code> and the locale in effect calls for that.</p>

</li>
<li><p>A number of bugs related to capture groups in quantified groups in regular expression have been fixed, especially in alternations. For example in a pattern like:</p>

<pre><code>"foobazfoobar" =~ /((foo)baz|foo(bar))+/</code></pre>

<p>the regex variable <code>$2</code> will not be "foo" as it once was, it will be undef.</p>

</li>
<li><p>Bugs with regex backreference operators that are inside of a capture group have been fixed. For instance:</p>

<pre><code>"xa=xaaa" =~ /^(xa|=?\1a){2}\z/</code></pre>

<p>will now correctly not match. [<a href="https://github.com/Perl/perl5/issues/10073">GH #10073</a>]</p>

</li>
<li><p><code>SSGROW()</code> and <code>SSCHECK()</code> have been reworked to ensure that the requested space is actually allocated. <code>SSCHECK()</code> is now an alias for <code>SSGROW()</code>.</p>

</li>
</ul>

<h2 id="Acknowledgements"><a href="#Acknowledgements">#</a>Acknowledgements</h2>

<p>Perl 5.38.0 represents approximately 12 months of development since Perl 5.36.0 and contains approximately 290,000 lines of changes across 1,500 files from 100 authors.</p>

<p>Excluding auto-generated files, documentation and release tools, there were approximately 190,000 lines of changes to 970 .pm, .t, .c and .h files.</p>

<p>Perl continues to flourish into its fourth decade thanks to a vibrant community of users and developers. The following people are known to have contributed the improvements that became Perl 5.38.0:</p>

<p>Alex, Alexander Nikolov, Alex Davies, Andreas König, Andrew Fresh, Andrew Ruthven, Andy Lester, Aristotle Pagaltzis, Arne Johannessen, A. Sinan Unur, Bartosz Jarzyna, Bart Van Assche, Benjamin Smith, Bram, Branislav Zahradník, Brian Greenfield, Bruce Gray, Chad Granum, Chris 'BinGOs' Williams, chromatic, Clemens Wasser, Craig A. Berry, Dagfinn Ilmari Mannsåker, Dan Book, danielnachun, Dan Jacobson, Dan Kogai, David Cantrell, David Golden, David Mitchell, E. Choroba, Ed J, Ed Sabol, Elvin Aslanov, Eric Herman, Felipe Gasper, Ferenc Erki, Firas Khalil Khana, Florian Weimer, Graham Knop, Håkon Hægland, Harald Jörg, H.Merijn Brand, Hugo van der Sanden, James E Keenan, James Raspass, jkahrman, Joe McMahon, Johan Vromans, Jonathan Stowe, Jon Gentle, Karen Etheridge, Karl Williamson, Kenichi Ishigaki, Kenneth Ölwing, Kurt Fitzner, Leon Timmermans, Li Linjie, Loren Merritt, Lukas Mai, Marcel Telka, Mark Jason Dominus, Mark Shelor, Matthew Horsfall, Matthew O. Persico, Mattia Barbon, Max Maischein, Mohammad S Anwar, Nathan Mills, Neil Bowers, Nicholas Clark, Nicolas Mendoza, Nicolas R, Paul Evans, Paul Marquess, Peter John Acklam, Peter Levine, Philippe Bruhat (BooK), Reini Urban, Renee Baecker, Ricardo Signes, Richard Leach, Russ Allbery, Scott Baker, Sevan Janiyan, Sidney Markowitz, Sisyphus, Steve Hay, TAKAI Kousuke, Todd Rinaldo, Tomasz Konojacki, Tom Stellard, Tony Cook, Tsuyoshi Watanabe, Unicode Consortium, vsfos, Yves Orton, Zakariyya Mughal, Zefram, 小鸡.</p>

<p>The list above is almost certainly incomplete as it is automatically generated from version control history. In particular, it does not include the names of the (very much appreciated) contributors who reported issues to the Perl bug tracker.</p>

<p>Many of the changes included in this version originated in the CPAN modules included in Perl's core. We're grateful to the entire CPAN community for helping Perl to flourish.</p>

<p>For a more complete list of all of Perl's historical contributors, please see the <i>AUTHORS</i> file in the Perl source distribution.</p>

<h2 id="Reporting-Bugs"><a href="#Reporting-Bugs">#</a><a id="Reporting"></a>Reporting Bugs</h2>

<p>If you find what you think is a bug, you might check the perl bug database at <a href="https://github.com/Perl/perl5/issues">https://github.com/Perl/perl5/issues</a>. There may also be information at <a href="http://www.perl.org/">http://www.perl.org/</a>, the Perl Home Page.</p>

<p>If you believe you have an unreported bug, please open an issue at <a href="https://github.com/Perl/perl5/issues">https://github.com/Perl/perl5/issues</a>. Be sure to trim your bug down to a tiny but sufficient test case.</p>

<p>If the bug you are reporting has security implications which make it inappropriate to send to a public issue tracker, then see <a href="https://perldoc.perl.org/perlsec#SECURITY-VULNERABILITY-CONTACT-INFORMATION">"SECURITY VULNERABILITY CONTACT INFORMATION" in perlsec</a> for details of how to report the issue.</p>

<h2 id="Give-Thanks"><a href="#Give-Thanks">#</a><a id="Give"></a>Give Thanks</h2>

<p>If you wish to thank the Perl 5 Porters for the work we had done in Perl 5, you can do so by running the <code>perlthanks</code> program:</p>

<pre><code>perlthanks</code></pre>

<p>This will send an email to the Perl 5 Porters list with your show of thanks.</p>

<h2 id="SEE-ALSO"><a href="#SEE-ALSO">#</a><a id="SEE"></a>SEE ALSO</h2>

<p>The <i>Changes</i> file for an explanation of how to view exhaustive details on what changed.</p>

<p>The <i>INSTALL</i> file for how to build Perl.</p>

<p>The <i>README</i> file for general stuff.</p>

<p>The <i>Artistic</i> and <i>Copying</i> files for copyright information.</p>


      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia’s H100: Funny L2, and Tons of Bandwidth (118 pts)]]></title>
            <link>https://chipsandcheese.com/2023/07/02/nvidias-h100-funny-l2-and-tons-of-bandwidth/</link>
            <guid>36569044</guid>
            <pubDate>Mon, 03 Jul 2023 04:55:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2023/07/02/nvidias-h100-funny-l2-and-tons-of-bandwidth/">https://chipsandcheese.com/2023/07/02/nvidias-h100-funny-l2-and-tons-of-bandwidth/</a>, See on <a href="https://news.ycombinator.com/item?id=36569044">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>GPUs started out as devices meant purely for graphics rendering, but their highly parallel nature made them attractive for certain compute tasks too. As the GPU compute scene grew over the past couple decades, Nvidia made massive investments to capture the compute market. Part of this involved recognizing that compute tasks have different needs than graphics tasks, and diverging their GPU lineup to better target each market.</p>
<p>H100 is the latest member of Nvidia’s line of compute-oriented GPUs. It uses the Hopper architecture, and is built on a massive 814 mm<sup>2</sup> die using TSMC’s 4N process with 80 billion transistors. This giant die implements 144 Streaming Multiprocessors (SMs), 60 MB of L2 cache, and 12 512-bit HBM memory controllers. We’re testing H100’s PCIe version on Lambda Cloud, which enables 114 of those SMs, 50 MB of L2 cache, and 10 HBM2 memory controllers. The card can draw up to 350 W.</p>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sxm5.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18547" data-permalink="https://chipsandcheese.com/h100_sxm5/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sxm5.png?fit=599%2C323&amp;ssl=1" data-orig-size="599,323" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_sxm5" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sxm5.png?fit=599%2C323&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sxm5.png?fit=599%2C323&amp;ssl=1" decoding="async" width="599" height="323" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sxm5.png?resize=599%2C323&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sxm5.png?resize=599%2C323&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>Nvidia’s rendering of H100 in the SXM form factor, from the H100 whitepaper</figcaption></figure>
<p>Nvidia also offers a SXM form factor H100, which can draw up to 700W and has 132 SMs enabled. The SXM H100 also uses HBM3 memory, providing additional bandwidth to feed those extra shaders.</p>
<figure><table><tbody><tr><td></td><td>H100</td><td>A100</td></tr><tr><td>Reported Device Name</td><td>H100 PCIe</td><td>A100-SXM4-40GB</td></tr><tr><td>CPU</td><td>Intel Xeon Platinum 8480+</td><td>AMD EPYC 7J13</td></tr><tr><td>Max Core Boost Clock</td><td>1755 MHz</td><td>1410 MHz</td></tr><tr><td>Idle Core Clock</td><td>345 MHz</td><td>210 MHz</td></tr><tr><td>Memory Clock (Does not change under load)</td><td>1593 MHz</td><td>1215 MHz</td></tr></tbody></table><figcaption>I also did some limited testing on an A100 instance for comparison purposes</figcaption></figure>
<h2>Brief Note on Clock Speeds</h2>
<p>The H100 features a much higher boost clock than the A100. When microbenchmarking, the H100 occasionally dropped down to as low as 1395 MHz, or just under 80% of its maximum boost clock. Other metrics from nvidia-smi suggest we could be hitting a power limit, particularly when pulling data from L2. The H100 PCIe version has a power limit of 350W, and gets right up against that when bandwidth testing.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18669" data-permalink="https://chipsandcheese.com/2023/07/02/nvidias-h100-funny-l2-and-tons-of-bandwidth/h100_clk-1/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?fit=1023%2C736&amp;ssl=1" data-orig-size="1023,736" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_clk-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?fit=1023%2C736&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?fit=688%2C495&amp;ssl=1" decoding="async" loading="lazy" width="688" height="495" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?resize=688%2C495&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?w=1023&amp;ssl=1 1023w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?resize=768%2C553&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?w=1023&amp;ssl=1 1023w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?resize=768%2C553&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_clk-1.png?resize=688%2C495&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Server cooling is able to keep the H100 at a very low temperature, even when the GPU is sucking down over 300W. Memory temperatures are a bit higher, but still well within reason.</p>
<p>A100 saw different behavior. Core clocks went to 1410 MHz under load and stays there. Power draw is also quite high, but the SXM4 version of A100 has a <a href="https://www.servethehome.com/new-nvidia-a100-pcie-add-in-card-launched/nvidia-a100-specs-sxm-and-pcie/">higher 400W power limit</a>. Probably because of that, we don’t see any clock speed drops even as power draw passes 350W.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18670" data-permalink="https://chipsandcheese.com/2023/07/02/nvidias-h100-funny-l2-and-tons-of-bandwidth/a100_clk-1/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?fit=927%2C701&amp;ssl=1" data-orig-size="927,701" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="a100_clk-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?fit=927%2C701&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?fit=688%2C520&amp;ssl=1" decoding="async" loading="lazy" width="688" height="520" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?resize=688%2C520&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?w=927&amp;ssl=1 927w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?resize=768%2C581&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?w=927&amp;ssl=1 927w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?resize=768%2C581&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?resize=200%2C150&amp;ssl=1 200w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/a100_clk-1.png?resize=688%2C520&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Like H100, A100 enjoys very low core temperatures. Passively cooled cards seem to thrive in a server chassis with plenty of airflow. A100’s memory temperatures are also lower than H100’s.</p>
<h2>Cache and Memory Setup</h2>
<p>Computers have been limited by memory speed for just about all of eternity. We’ve seen consumer GPUs counter this with increasingly sophisticated cache setups. AMD’s RX 6900 XT used a four level cache hierarchy with 128 MB of last level caching capacity, while Nvidia’s RTX 4090 extended L2 capacity to 72 MB. Nvidia’s compute GPUs have seen increased caching capacities too, but the strategy there is a bit different.</p>
<p>Streaming Multiprocessors (SMs) are Nvidia’s basic GPU building block. Nvidia has consistently emphasized SM-private caching in prior datacenter oriented GPUs. For most Nvidia architectures, a SM has a private chunk of memory that can be flexibly partitioned between L1 cache and Shared Memory (a software managed scratchpad) usage. GK210 Kepler SMs had 128 KB of memory for that compared to 64 KB on client implementations. A100 had 192 KB compared to 128 KB on client Ampere. Now, H100 brings L1/Shared Memory capacity to 256 KB.</p>
<p>We can do limited testing of L1 cache allocations by using Nvidia’s proprietary API. We usually test with OpenCL or Vulkan because many vendors support those APIs, letting tests run unmodified across a large variety of GPUs. But CUDA gives limited control over L1 and Shared Memory splits. Specifically, we can ask the GPU to prefer L1 caching capacity, prefer an equal split, or prefer Shared Memory capacity. Asking for larger L1 cache allocations doesn’t come with any latency penalty.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18442" data-permalink="https://chipsandcheese.com/h100_l1_alloc/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?fit=1372%2C601&amp;ssl=1" data-orig-size="1372,601" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_l1_alloc" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?fit=1372%2C601&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?fit=688%2C301&amp;ssl=1" decoding="async" loading="lazy" width="688" height="301" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?resize=688%2C301&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?w=1372&amp;ssl=1 1372w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?resize=768%2C336&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?resize=1200%2C526&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?resize=1320%2C578&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?w=1372&amp;ssl=1 1372w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?resize=768%2C336&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?resize=1200%2C526&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?resize=1320%2C578&amp;ssl=1 1320w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l1_alloc.png?resize=688%2C301&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>Testing memory access latency using CUDA, which lets us specify a preferred L1/Shared Memory split. </figcaption></figure></div>
<p>When we ask CUDA to prefer L1 caching capacity, we see 208 KB of L1 cache. With this setting, H100 has more first level data caching capacity than any other GPU. Even if we account for AMD’s strategy of using separate memories for caching and scratchpad purposes, H100 continues to be ahead. Adding up RDNA 3’s L0 vector cache, scalar cache, and LDS (scratchpad) capacity only gives 208 KB of storage, compared to 256 KB on Hopper.</p>
<p>Against A100, H100’s L1 is both higher capacity and lower latency. It’s a welcome improvement, and the trend of being slightly better than A100 continues further down the cache hierarchy.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18443" data-permalink="https://chipsandcheese.com/h100_latency_vs_a100/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?fit=1372%2C601&amp;ssl=1" data-orig-size="1372,601" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_latency_vs_a100" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?fit=1372%2C601&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?fit=688%2C301&amp;ssl=1" decoding="async" loading="lazy" width="688" height="301" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?resize=688%2C301&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?w=1372&amp;ssl=1 1372w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?resize=768%2C336&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?resize=1200%2C526&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?resize=1320%2C578&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?w=1372&amp;ssl=1 1372w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?resize=768%2C336&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?resize=1200%2C526&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?resize=1320%2C578&amp;ssl=1 1320w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_a100.png?resize=688%2C301&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>For data that can’t fit within L1, H100 has a 50 MB L2. When A100 launched in 2020, its 40 MB L2 gave it more last level caching capacity than any Nvidia GPU at the time. H100 slightly increases cache capacity, but it’s nothing special today. Nvidia’s RTX 4090 features 72 MB of L2, while AMD’s high end RDNA 2 and RDNA 3 GPUs have 128 MB of 96 MB of last level cache respectively.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18454" data-permalink="https://chipsandcheese.com/gh100_block_diagram/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?fit=1247%2C631&amp;ssl=1" data-orig-size="1247,631" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh100_block_diagram" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?fit=1247%2C631&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?fit=688%2C348&amp;ssl=1" decoding="async" loading="lazy" width="688" height="348" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?resize=688%2C348&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?w=1247&amp;ssl=1 1247w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?resize=768%2C389&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?resize=1200%2C607&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?w=1247&amp;ssl=1 1247w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?resize=768%2C389&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?resize=1200%2C607&amp;ssl=1 1200w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/gh100_block_diagram.jpg?resize=688%2C348&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>H100 block diagram from Nvidia’s whitepaper, showing two L2 partitions and a link between them</figcaption></figure></div>
<p>H100 also inherits A100’s split L2 configuration. Any thread running on the GPU can access the full 50 MB of cache, but not at the same speed. Accessing the “far” partition takes nearly twice as long. It has about as much latency as VRAM on the RX 6900 XT, making it more useful for bandwidth than for getting individual warps or wavefronts to finish faster.</p>
<div>
<figure><img data-lazy-fallback="1" data-attachment-id="18464" data-permalink="https://chipsandcheese.com/h100_latency_vs_rdna2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?fit=1372%2C601&amp;ssl=1" data-orig-size="1372,601" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_latency_vs_rdna2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?fit=1372%2C601&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?fit=688%2C301&amp;ssl=1" decoding="async" loading="lazy" width="688" height="301" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?resize=688%2C301&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?w=1372&amp;ssl=1 1372w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?resize=768%2C336&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?resize=1200%2C526&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?resize=1320%2C578&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?w=1372&amp;ssl=1 1372w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?resize=768%2C336&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?resize=1200%2C526&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?resize=1320%2C578&amp;ssl=1 1320w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_latency_vs_rdna2.png?resize=688%2C301&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption>Using OpenCL to compare to AMD’s client RDNA 2 architecture</figcaption></figure></div>
<p>H100’s L2 cache feels like a two-level setup rather than a single level of cache. A thread running on H100 can access the “far” L2 cache a bit faster than on A100, so Nvidia has improved compared to the prior generation. A100 was a bit of a pioneer for Nvidia when it came to implementing large caches, and H100’s setup is a natural evolution of A100’s. But this isn’t the low latency, efficient cache setup used on modern client GPUs.</p>
<p>Out in VRAM, H100 sees slightly lower latency than A100, and is comparable to some older client GPUs. For example, the GTX 980 Ti has about 354 ns of VRAM latency.</p>
<h2>No More Constant Cache?</h2>
<p>Nvidia has long used a separate constant cache hierarchy, typically with a 2 KB constant cache backed by a 32 to 64 KB mid-level constant cache. The constant cache offers very low latency access, but is read-only and backed by a limited memory space. H100 handles constant memory differently. Nvidia can allocate up to 64 KB of constant memory (a limitation dating back to the Tesla architecture), and latency is constant throughout that range. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_constant.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18633" data-permalink="https://chipsandcheese.com/h100_constant/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_constant.png?fit=740%2C354&amp;ssl=1" data-orig-size="740,354" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_constant" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_constant.png?fit=740%2C354&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_constant.png?fit=688%2C329&amp;ssl=1" decoding="async" loading="lazy" width="688" height="329" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_constant.png?resize=688%2C329&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_constant.png?resize=688%2C329&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Furthermore, latency looks nearly identical to L1 cache latency. H100 might be using the L1 data cache to hold constant data. Validating this assumption would require additional testing that I can’t currently make the time investment for, due to real life and day job demands. But whatever Nvidia did, it provides a clear improvement over A100’s constant caching, with lower latency across the board. Ada Lovelace enjoys lower latency if it can serve requests from a tiny and fast 2 KB constant cache, but also falls behind if there’s a lot of constant data.</p>
<h2>Local Memory Latency</h2>
<p>As mentioned before, H100’s SMs have a large block of private storage that can be split between L1 caching and Shared Memory use. Shared Memory is Nvidia’s term for a software managed scratchpad that offers consistently high performance. AMD’s equivalent is called the Local Data Share (LDS). On Intel GPUs, it’s called Shared Local Memory (SLM). OpenCL refers to this memory type as local memory. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_lds.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18566" data-permalink="https://chipsandcheese.com/h100_lds/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_lds.png?fit=480%2C288&amp;ssl=1" data-orig-size="480,288" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_lds" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_lds.png?fit=480%2C288&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_lds.png?fit=480%2C288&amp;ssl=1" decoding="async" loading="lazy" width="480" height="288" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_lds.png?resize=480%2C288&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_lds.png?resize=480%2C288&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Even though H100 allocates it out of the same block of storage, Shared Memory is faster than L1 cache access because it doesn’t require tag comparisons and status checks to make sure there’s a hit. H100 does well compared to a wide range of GPUs, even though it can allocate more Shared Memory capacity than any other current GPU.</p>
<h2>Atomics</h2>
<p>Shared Memory (or local memory) is also useful for synchronizing threads within the same workgroup. Here, we’re testing OpenCL’s atomic_cmpxchg function, which does compare and exchange operations with a guarantee that nothing else appears to touch the memory its working with between those operations.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_localatomic.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18581" data-permalink="https://chipsandcheese.com/h100_localatomic/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_localatomic.png?fit=480%2C288&amp;ssl=1" data-orig-size="480,288" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_localatomic" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_localatomic.png?fit=480%2C288&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_localatomic.png?fit=480%2C288&amp;ssl=1" decoding="async" loading="lazy" width="480" height="288" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_localatomic.png?resize=480%2C288&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_localatomic.png?resize=480%2C288&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>H100 does reasonably well with this atomic operation, though it’s a bit behind what consumer GPUs can do. Surprisingly, that applies to older GPUs that run at lower clocks as well, like the GTX 980 Ti. H100 however does do better than the A100.</p>
<p>If we perform the same operation on global memory (i.e. memory backed by VRAM), latency is far worse. It’s slightly higher than L2 latency, so perhaps H100 is handling cross-SM synchronization at the L2 cache.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_globalatomic.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18583" data-permalink="https://chipsandcheese.com/h100_globalatomic/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_globalatomic.png?fit=480%2C288&amp;ssl=1" data-orig-size="480,288" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_globalatomic" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_globalatomic.png?fit=480%2C288&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_globalatomic.png?fit=480%2C288&amp;ssl=1" decoding="async" loading="lazy" width="480" height="288" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_globalatomic.png?resize=480%2C288&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_globalatomic.png?resize=480%2C288&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>Again, H100 slightly improves over A100, and falls short compared to consumer GPUs. But this time, the gap is much larger. The RX 6900 XT leaves both the H100 and A100 far behind. The old GTX 980 Ti also performs quite a bit better. I guess synchronizing things across a massive 814 mm<sup>2</sup> or 826 mm<sup>2</sup> die is quite challenging.</p>
<h3>Distributed Shared Memory</h3>
<p>To mitigate the cost of transferring data across the gigantic die, H100 has a feature called Distributed Shared Memory (DSMEM). Using this feature, applications can keep data within a GPC, or a cluster of SMs. This should allow for lower latency data sharing than the global atomics mentioned above, while being able to share data across more threads than would fit in a workgroup.</p>
<p>Testing this feature would involve paying $2 per hour for a H100 instance while learning a new API, and then testing with no other GPU to sanity check results against. Writing, debugging, and validating a test usually takes many hours even under favorable conditions. Nvidia claims DSMEM is <a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">typically 7x faster </a>than exchanging data through global memory.</p>

<p>Latency is only part of the picture. GPUs like the H100 are designed for extremely parallel compute workloads, and probably don’t have to deal with cases where available parallelism is low. That contrasts with consumer GPUs, which occasionally face less parallel tasks like geometry processing or small draw calls. So, H100 emphasizes massive bandwidth. Starting at the L2 cache, we see over 5.5 TB/s of read bandwidth. We measured about 5.7 TB/s of read bandwidth from the RX 7900 XTX’s L2, so H100 gets almost the same amount of bandwidth with much higher caching capacity.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18444" data-permalink="https://chipsandcheese.com/h100_l2_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?fit=1026%2C420&amp;ssl=1" data-orig-size="1026,420" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_l2_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?fit=1026%2C420&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?fit=688%2C282&amp;ssl=1" decoding="async" loading="lazy" width="688" height="282" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?resize=688%2C282&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?w=1026&amp;ssl=1 1026w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?resize=768%2C314&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?w=1026&amp;ssl=1 1026w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?resize=768%2C314&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_l2_bw.png?resize=688%2C282&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>Tested using OpenCL</figcaption></figure></div>
<p>Compared to A100, H100 enjoys a small but noticeable bandwidth boost. But that only applies to the “near” L2 partition. As noted before, A100 and H100’s L2 isn’t really a single level cache. Bandwidth is significantly worse if we exceed “near” L2 capacity. H100 also regresses compared to A100 when accessing the entire 50 MB L2, with 3.8 TB/s compared to the A100’s 4.5 TB/s. Nvidia may have determined that few workloads were L2 bandwidth bound on A100, so dropping a bit of cross-partition L2 bandwidth wasn’t a big deal.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18445" data-permalink="https://chipsandcheese.com/h100_big_l2_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?fit=1028%2C420&amp;ssl=1" data-orig-size="1028,420" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_big_l2_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?fit=1028%2C420&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?fit=688%2C281&amp;ssl=1" decoding="async" loading="lazy" width="688" height="281" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?resize=688%2C281&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?w=1028&amp;ssl=1 1028w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?resize=768%2C314&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?w=1028&amp;ssl=1 1028w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?resize=768%2C314&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_big_l2_bw.png?resize=688%2C281&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>In an absolute sense, H100’s 50 MB L2 still offers a ton of bandwidth even when requests have to go across the cache’s partitions. For comparison, RDNA 2’s Infinity Cache offers around 2 TB/s of bandwidth, while RDNA 3’s Infinity Cache stops just short of 3 TB/s. H100 therefore offers a bit less caching capacity than the Infinity Cache on AMD’s top end client GPUs, but makes up for it with more bandwidth.</p>
<p>However, I feel like Nvidia could bring some of their client side engineering into their compute oriented GPUs. Their RTX 4090 offers about 5 TB/s of L2 bandwidth, and has a lot more L2 caching capacity. On the bright side, H100’s L2 offers much higher bandwidth than VRAM, even when requests have to cross partitions. That’s a compliment, because H100 has a ridiculous amount of VRAM bandwidth.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18446" data-permalink="https://chipsandcheese.com/h100_vram/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?fit=1024%2C420&amp;ssl=1" data-orig-size="1024,420" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_vram" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?fit=1024%2C420&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?fit=688%2C282&amp;ssl=1" decoding="async" loading="lazy" width="688" height="282" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?resize=688%2C282&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?w=1024&amp;ssl=1 1024w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?resize=768%2C315&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?w=1024&amp;ssl=1 1024w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?resize=768%2C315&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vram.png?resize=688%2C282&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>With six stacks of HBM2e, the H100 can pull just short of 2 TB/s from VRAM. Thus, the H100’s VRAM bandwidth is actually quite close to RDNA 2’s Infinity Cache bandwidth. It also represents a significant improvement over A100. A100 used HBM2 and still has more VRAM bandwidth than any consumer GPU, but its lower memory clocks let H100 pull ahead.</p>
<p>H100’s VRAM bandwidth will be very useful for massive working sets without cache-friendly access patterns. Consumer GPUs have trended towards good caching instead of massive VRAM setups. The few consumer GPUs that did use HBM have turned in a mediocre performance compared to ones with a modest GDDR setup but excellent caching. That’s because caches lower latency, making it easier to keep the execution units fed even with small workloads. From how Nvidia and AMD have been building compute GPUs, it looks like compute workloads favor the opposite. A100 was already tuned for large workloads. H100 takes that further, with a lead over A100 if you can fill more than half the GPU, but falls a bit behind if you don’t.</p>
<h2>Compute Throughput</h2>
<p>A100’s SMs offered higher theoretical occupancy and FP64 performance than client Ampere, but only had half the FP32 throughput. H100 remediates that by giving each SM sub partition (SMSP) 32 FP32 units, letting it execute one warp instruction per clock.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18495" data-permalink="https://chipsandcheese.com/h100_vs_a100_sm/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?fit=1251%2C829&amp;ssl=1" data-orig-size="1251,829" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_vs_a100_sm" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?fit=1251%2C829&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?fit=688%2C456&amp;ssl=1" decoding="async" loading="lazy" width="688" height="456" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?resize=688%2C456&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?w=1251&amp;ssl=1 1251w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?resize=768%2C509&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?resize=1200%2C795&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?w=1251&amp;ssl=1 1251w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?resize=768%2C509&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?resize=1200%2C795&amp;ssl=1 1200w" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_vs_a100_sm.jpg?resize=688%2C456&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>Figures from the A100 and H100’s respective whitepapers</figcaption></figure></div>
<p>Alongside FP32 performance, FP64 performance doubles too. Each H100 SMSP can execute a FP64 warp instruction every two cycles, compared to once every four cycles on A100. That makes H100 a much better performer than A100 for scientific applications that need increased precision.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sm_throughput.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18499" data-permalink="https://chipsandcheese.com/h100_sm_throughput/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sm_throughput.png?fit=743%2C684&amp;ssl=1" data-orig-size="743,684" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_sm_throughput" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sm_throughput.png?fit=743%2C684&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sm_throughput.png?fit=688%2C633&amp;ssl=1" decoding="async" loading="lazy" width="688" height="633" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sm_throughput.png?resize=688%2C633&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_sm_throughput.png?resize=688%2C633&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>INT32 addition throughput on A100 is definitely a measuring error. And sadly, Nvidia does not support OpenCL’s FP16 extension, so FP16 throughput couldn’t be tested</figcaption></figure></div>
<p>At the same time, H100 inherits Nvidia’s strength in integer multiplication. Specifically, INT32 multiplies execute at half rate, compared to quarter rate on AMD GPUs. On the other hand, AMD GPUs can execute 16-bit integer operations at double rate, while Nvidia GPUs can’t.</p>
<p>At the GPU level, H100 features a small SM count increase and a substantial increase in clock speeds. The result is a significant increase in compute throughput across the board. Thanks to SM-level changes, H100’s FP32 and FP64 throughput blows A100 out of the water. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_gpu_throughput.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="18507" data-permalink="https://chipsandcheese.com/h100_gpu_throughput/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_gpu_throughput.png?fit=743%2C765&amp;ssl=1" data-orig-size="743,765" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_gpu_throughput" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_gpu_throughput.png?fit=743%2C765&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_gpu_throughput.png?fit=688%2C708&amp;ssl=1" decoding="async" loading="lazy" width="688" height="708" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_gpu_throughput.png?resize=688%2C708&amp;ssl=1" alt="" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/06/h100_gpu_throughput.png?resize=688%2C708&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></figure></div>
<p>H100’s improvements should bring performance benefits across a very wide variety of applications, because it’s hard to think of any GPGPU program that doesn’t use FP32 or FP64. Doubling throughput for those operations along with a SM count and clock speed increase is going to make workloads finish faster.</p>
<p>Going beyond vector compute performance, H100 doubles tensor core throughput. Tensor cores specialize in doing matrix multiplication by breaking the SIMT model, and storing matrices across a warp’s registers. I don’t have a test written for tensor cores and writing one in the near future is beyond what I have time for with a free time hobby project. But, I trust Nvidia’s whitepaper on this topic.</p>
<h2>Final Words</h2>
<p>Consumer GPUs in recent years have moved towards maintaining good performance when faced with small workloads. They’re still very wide of course, but AMD and Nvidia have struck a balance between throughput and latency. RDNA 2/3 and Ada Lovelace run well over 2 GHz, meaning their clock speeds approach those of server CPUs. Alongside high clock speeds, sophisticated cache hierarchies provide latency benefits along with high bandwidth, provided access patterns are cache friendly. Meanwhile, expensive memory solutions have fallen out of favor. The few client GPUs with HBM never did well against their GDDR equipped competition, despite having more memory bandwidth and often more compute throughput to back that up.</p>
<p>But that evidently doesn’t apply to compute GPUs, because they’ve gone in the opposite direction. H100 is a massively wide GPU running at relatively low clocks, which emphasizes performance per watt over absolute performance. 1755 MHz was typical for Pascal, an architecture launched seven years ago. Cache capacity and latency are mediocre compared to recent client GPUs. Meanwhile, Nvidia has not compromised bandwidth. H100’s L2 does not fall behind client GPUs when it comes to bandwidth. After L2, VRAM bandwidth is monstrous thanks to a giant HBM configuration. H100, like A100 and AMD’s CDNA GPUs, is meant to run large, long running jobs. Based on the emphasis on VRAM bandwidth over caching capacity, these jobs probably fall into the category where if you can’t catch the access pattern with a few dozen megabytes of cache, doubling cache capacity probably won’t help.</p>
<p>H100 differentiates itself from client designs at the SM level too. More memory for L1 or Shared Memory use means carefully crafted programs can keep a lot of data very close to the execution units. Across H100’s 144 physical SMs, there’s 36.8 MB of L1 and Shared Memory capacity, which makes for a significant die area investment. Nvidia has also spent SM area to track more warps in flight in order to cope with higher L1 miss latency. H100 can track 64 warps per SM, compared to 48 on client Ampere’s and Ada Lovelace. Additional SM area gets spent to double FP32, FP64, and tensor throughput.</p>
<p>Client GPUs continue to provide reasonable compute capability, and datacenter GPUs <a href="https://www.techpowerup.com/310298/nvidia-h100-hopper-gpu-tested-for-gaming-slower-than-integrated-gpu">can be forced to render graphics</a> if you hate yourself enough. But for the foreseeable future, compute and graphics oriented architectures will likely continue to diverge. Ada Lovelace and H100 feature plenty of differences, even if they’re based off a similar base. On the AMD side, RDNA and CDNA continue to diverge too, though both can trace their ISA’s roots back to the venerable GCN architecture. This kind of divergence is natural as process node progress slows down and everyone tries to specialize to get the most out of each transistor.<br>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p>

<div data-post_id="10949" data-instance_id="1" data-additional_class="pp-multiple-authors-layout-boxed.multiple-authors-target-the-content" data-original_class="pp-multiple-authors-boxes-wrapper pp-multiple-authors-wrapper box-post-id-10949 box-instance-id-1">

<ul>
<li>
<p><img data-lazy-fallback="1" alt="clamchowder" src="https://secure.gravatar.com/avatar/83de286347cdfc84e1bb10146350467e?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/83de286347cdfc84e1bb10146350467e?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80" loading="lazy" decoding="async" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"> </p>

</li>
</ul>
</div>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[55 GiB/s FizzBuzz (2021) (465 pts)]]></title>
            <link>https://codegolf.stackexchange.com/questions/215216/high-throughput-fizz-buzz/236630#236630</link>
            <guid>36568192</guid>
            <pubDate>Mon, 03 Jul 2023 02:54:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://codegolf.stackexchange.com/questions/215216/high-throughput-fizz-buzz/236630#236630">https://codegolf.stackexchange.com/questions/215216/high-throughput-fizz-buzz/236630#236630</a>, See on <a href="https://news.ycombinator.com/item?id=36568192">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<h2>x86-64+AVX2 assembly language (Linux, <code>gcc</code>+<code>gas</code>)</h2>

<h3>Build and usage instructions</h3>
<p>This program is most conveniently built using <code>gcc</code>. Save it as <code>fizzbuzz.S</code> (that's a capital <code>S</code> as the extension), and build using the commands</p>
<pre><code>gcc -mavx2 -c fizzbuzz.S
ld -o fizzbuzz fizzbuzz.o
</code></pre>
<p>Run as <code>./fizzbuzz</code> piped into one command, e.g. <code>./fizzbuzz | pv &gt; /dev/null</code> (as suggested in the question), <code>./fizzbuzz | cat</code>, or <code>./fizzbuzz | less</code>. To simplify the I/O, this will not work (producing an error on startup) if you try to output to a file/terminal/device rather than a pipe. Additionally, this program may produce incorrect output if piped into two commands (e.g. <code>./fizzbuzz | pv | cat &gt; fizzbuzz.txt</code>), but only in the case where the middle command uses the <code>splice</code> system call; this is either a bug in Linux (very possible with system calls this obscure!) or a mistake in the documentation of the system calls in question (also possible). However, it should work correctly for the use case in the question, which is all that matters on CGCC.</p>
<p>This program is somewhat system-specific; it requires the operating system to be a non-ancient version of Linux, and the processor to be an x86-64 implementation that supports AVX2. (Most moderately recent processors by Intel and AMD have AVX2 support, including the Ryzen 9 mentioned in the question, and almost all use the x86-64 instruction set.) However, it avoids assumptions about the system it's running on beyond those mentioned in the header, so there's a decent chance that if you can run Linux, you can run this.</p>
<p>The program outputs a quintillion lines of FizzBuzz and then exits (going further runs into problems related to the sizes of registers). This would take tens of years to accomplish, so hopefully counts as "a very high astronomical number" (although it astonishes me that it's a small enough timespan that it might be theoretically possible to reach a number as large as a quintillion without the computer breaking).</p>
<p>As a note: this program's performance is dependent on whether it and the program it outputs to are running on sibling CPUs or not, something which will be determined arbitrarily by the kernel when you start it. If you want to compare the two possible timings, use <code>taskset</code> to force the programs onto particular CPUs:
<code>taskset 1 ./fizzbuzz | taskset 2 pv &gt; /dev/null</code> versus <code>taskset 1 ./fizzbuzz | taskset 4 pv &gt; /dev/null</code>. (The former will probably run faster, but might be slower on some CPU configurations.)</p>
<h3>Discussion</h3>
<p>I've spent months working on this program. I've long thought that "how fast can you make a FizzBuzz" would be a really interesting question for learning about high-performance programming, and when I subsequently saw this question posted on CGCC, I pretty much had to try.</p>
<p>This program aims for the maximum possible single-threaded performance. In terms of the FizzBuzz calculation itself, it is intended to sustain a performance of 64 bytes of FizzBuzz per 4 clock cycles (and is future-proofed where possible to be able to run faster if the relevant processor bottleneck – L2 cache write speed – is ever removed). This is faster than a number of standard functions. In particular, it's faster than <code>memcpy</code>, which presents interesting challenges when it comes to I/O (if you try to output using <code>write</code> then the copies in <code>write</code> will take up almost all the runtime – replacing the I/O routine here with <code>write</code> causes the performance on my CPU to drop by a factor of 5). As such, I needed to use much more obscure system calls to keep I/O-related copies to a minimum (in particular, the generated FizzBuzz text is only sent to main memory if absolutely necessary; most of the time it's stored in the processor's L2 cache and piped into the target program from there, which is why reading it from a sibling CPU can boost performance – the physical connection to the L2 cache is shorter and higher bandwidth than it would be to a more distant CPU).</p>
<p>On my computer (which has a fairly recent, but not particularly powerful, Intel processor), this program generates around 31GiB of FizzBuzz per second. I'll be interested to see how it does on the OP's computer.</p>
<p>I did experiment with multithreaded versions of the program, but was unable to gain any speed. Experiments with simpler programs show that it could be possible, but any gains may be small; the cost of communication between CPUs is sufficiently high to negate most of the gains you could get by doing work in parallel, assuming that you only have one program reading the resulting FizzBuzz (and anything that writes to memory will be limited by the write speed of main memory, which is slower than the speed with which the FizzBuzz can be generated).</p>
<h3>The program</h3>
<p>This isn't <a href="https://codegolf.stackexchange.com/questions/tagged/code-golf" title="show questions tagged 'code-golf'" rel="tag">code-golf</a>, so my explanation of the program and its algorithm are given as comments in the program itself. (I still had to lightly golf the program, and especially the explanation, to fit this post within the 65536 byte size limit.)</p>
<p>The program is written in a "literate" assembly style; it will be easiest to understand if you read it in order, from start to end. (I also added a number of otherwise useless line labels to separate the program into logical groups of instructions, in order to make the disassembly easier to read, if you're one of the people who prefers to read assembly code like that.)</p>
<pre><code>.intel_syntax prefix

// Header files.
#include &lt;asm/errno.h&gt;
#include &lt;asm/mman.h&gt;
#include &lt;asm/unistd.h&gt;
#define F_SETPIPE_SZ 1031 // not in asm headers, define it manually

// The Linux system call API (limited to 4 arguments, the most this
// program uses). 64-bit registers are unsuffixed; 32-bit have an "e"
// suffix.
#define ARG1 %rdi
#define ARG1e %edi
#define ARG2 %rsi
#define ARG2e %esi
#define ARG3 %rdx
#define ARG3e %edx
#define ARG4 %r10
#define ARG4e %r10d
#define SYSCALL_RETURN %rax
#define SYSCALL_RETURNe %eax
#define SYSCALL_NUMBER %eax

// %rax, %rcx, %rdx, %ymm0-3 are general-purpose temporaries. Every
// other register is used for just one or two defined purposes; define
// symbolic names for them for readability. (Bear in mind that some of
// these will be clobbered sometimes, e.g. OUTPUT_LIMIT is clobbered
// by `syscall` because it's %r11.)
#define OUTPUT_PTR %rbx
#define BYTECODE_IP %rbp
#define SPILL %rsi
#define BYTECODE_GEN_PTR %rdi
#define REGEN_TRIGGER %r8
#define REGEN_TRIGGERe %r8d
#define YMMS_AT_WIDTH %r9
#define YMMS_AT_WIDTHe %r9d
#define BUZZ %r10
#define BYTECODE_NEG_LEN %r10
#define FIZZ %r11
#define FIZZe %r11d
#define OUTPUT_LIMIT %r11
#define BYTECODE_END %r12
#define BYTECODE_START %r13
#define BYTECODE_STARTe %r13d
#define PIPE_SIZE %r13
#define LINENO_WIDTH %r14
#define LINENO_WIDTHe %r14d
#define GROUPS_OF_15 %r15
#define GROUPS_OF_15e %r15d
#define LINENO_LOW %ymm4
#define LINENO_MID %ymm5
#define LINENO_MIDx %xmm5
#define LINENO_TOP %ymm6
#define LINENO_TOPx %xmm6
#define LINENO_MID_TEMP %ymm7
#define ENDIAN_SHUFFLE %ymm8
#define ENDIAN_SHUFFLEx %xmm8
#define LINENO_LOW_INCR %ymm9
#define LINENO_LOW_INCRx %xmm9

// The last six vector registers are used to store constants, to avoid
// polluting the cache by loading their values from memory.
#define LINENO_LOW_INIT %ymm10
#define LINENO_MID_BASE %ymm11
#define LINENO_TOP_MAX %ymm12
#define ASCII_OFFSET %ymm13
#define ASCII_OFFSETx %xmm13
#define BIASCII_OFFSET %ymm14
#define BASCII_OFFSET %ymm15


// Global variables.
.bss
.align 4 &lt;&lt; 20
// The most important global variables are the IO buffers. There are
// two of these, each with 2MiB of memory allocated (not all of it is
// used, but putting them 2MiB apart allows us to simplify the page
// table; this gives a 30% speedup because page table contention is
// one of the main limiting factors on the performance).
io_buffers:
.zero 2 * (2 &lt;&lt; 20)
// The remaining 2MiB of memory stores everything else:
iovec_base:          // I/O config buffer for vmsplice(2) system call
.zero 16
error_write_buffer:  // I/O data buffer for write(2) system call
.zero 1
.p2align 9,0
bytecode_storage:    // the rest is a buffer for storing bytecode
.zero (2 &lt;&lt; 20) - 512


// The program starts here. It doesn't use the standard library (or
// indeed any libraries), so the start point is _start, not main.
.text
.globl _start
_start:

// This is an AVX2 program, so check for AVX2 support by running an
// AVX2 command. This is a no-op, but generates SIGILL if AVX2 isn't
// supported.
vpand %ymm0, %ymm0, %ymm0

// Initialize constant registers to their constant values.
vmovdqa LINENO_LOW_INIT, [%rip + lineno_low_init]
vmovdqa LINENO_MID_BASE, [%rip + lineno_mid_base]
vmovdqa LINENO_TOP_MAX, [%rip + lineno_top_max]
vmovdqa ASCII_OFFSET, [%rip + ascii_offset]
vmovdqa BIASCII_OFFSET, [%rip + biascii_offset]
vmovdqa BASCII_OFFSET, [%rip + bascii_offset]

// Initialize global variables to their initial values.
vmovdqa ENDIAN_SHUFFLE, [%rip + endian_shuffle_init]
vmovdqa LINENO_TOP, [%rip + lineno_top_init]

// Check the size of the L2 cache.
//
// This uses the CPUID interface. To use it safely, check what range
// of command numbers is legal; commands above the legal range have
// undefined behaviour, commands within the range might not be
// implemented but will return all-zeros rather than undefined values.
// CPUID clobbers a lot of registers, including some that are normally
// call-preserved, so this must be done first.
mov %eax, 0x80000000 // asks which CPUID extended commands exist
cpuid                // returns the highest supported command in %eax
cmp %eax, 0x80000006 // does 0x80000006 give defined results?
jb bad_cpuid_error

mov %eax, 0x80000006 // asks about the L2 cache size
cpuid                // returns size in KiB in the top half of %ecx
shr %ecx, 16
jz bad_cpuid_error   // unsupported commands return all-0s

// Calculate the desired pipe size, half the size of the L2 cache.
// This value is chosen so that the processor can hold a pipeful of
// data being output, plus a pipeful of data being calculated, without
// needing to resort to slow L3 memory operations.
shl %ecx, 10 - 1     // convert KiB to bytes, then halve
mov PIPE_SIZE, %rcx

// Ask the kernel to resize the pipe on standard output.
mov ARG1e, 1
mov ARG2e, F_SETPIPE_SZ
mov ARG3e, %ecx
mov SYSCALL_NUMBER, __NR_fcntl
syscall
cmp SYSCALL_RETURNe, -EBADF
je pipe_error
cmp SYSCALL_RETURNe, -EPERM
je pipe_perm_error
call exit_on_error
cmp SYSCALL_RETURN, PIPE_SIZE
jne pipe_size_mismatch_error

// Ask the kernel to defragment the physical memory backing the BSS
// (read-write data) segment. This simplifies the calculations needed
// to find physical memory addresses, something that both the kernel
// and processor would otherwise spend a lot of time doing, and
// speeding the program up by 30%.
lea ARG1, [%rip + io_buffers]
mov ARG2e, 3 * (2 &lt;&lt; 20)
mov ARG3e, MADV_HUGEPAGE
mov SYSCALL_NUMBER, __NR_madvise
syscall
call exit_on_error

// From now on, OUTPUT_PTR is permanently set to the memory location
// where the output is being written. This starts at the start of the
// first I/O buffer.
lea OUTPUT_PTR, [%rip + io_buffers]


///// First phase of output
//
// The FizzBuzz output is produced in three distinct phases. The first
// phase is trivial; just a hardcoded string, that's left in the
// output buffer, to be output at the end of the second phase.

first_phase:

.section .rodata
fizzbuzz_intro:
.ascii "1\n2\nFizz\n4\nBuzz\nFizz\n7\n8\nFizz\n"
.text
vmovdqu %ymm0, [%rip + fizzbuzz_intro]
vmovdqu [OUTPUT_PTR], %ymm0
add OUTPUT_PTR, 30


///// Second phase of output
//
// This is a routine implementing FizzBuzz in x86-64+AVX2 assembler in
// a fairly straightforward and efficient way. This isn't as fast as
// the third-phase algorithm, and can't handle large numbers, but will
// introduce some of the basic techniques this program uses.

second_phase_init:

// The outer loop of the whole program breaks the FizzBuzz output into
// sections where all the line numbers contain the same number of
// digits. From now on, LINENO_WIDTH tracks the number of digits in
// the line number. This is currently 2; it ranges from 2-digit
// numbers to 18-digit numbers, and then the program ends.
mov LINENO_WIDTHe, 2

// GROUPS_OF_15 is permanently set to the number of groups of 15 lines
// that exist at this line number width; it's multiplied by 10 whenever
// LINENO_WIDTH is incremented.
//
// A general note about style: often the program uses numbers that are
// statically known to fit into 32 bits, even in a register that's
// conceptually 64 bits wide (like this one). In such cases, the
// 32-bit and 64-bit versions of a command will be equivalent (as the
// 32-bit version zero-extends to 64-bits on a 64-bit processor); this
// program generally uses the 32-bit version, both because it
// sometimes encodes to fewer bytes (saving cache pressure), and
// because some processors recognise zeroing idioms only if they're 32
// bits wide.
mov GROUPS_OF_15e, 6

// Some constants used throughout the second phase, which permanently
// stay in their registers. Note that short string literals can be
// stored in normal integer registers - the processor doesn't care.
mov FIZZ, 0x0a7a7a6946  // "Fizz\n"
mov BUZZ, 0x0a7a7a7542  // "Buzz\n"

.section .rodata
.p2align 5, 0
second_phase_constants:
.byte 0, 0, 0, 0, 0, 0, 0, 0
.byte 1, 0, 0, 0, 0, 0, 0, 0
.text
vmovdqa %xmm3, [%rip + second_phase_constants]

// This program makes extensive use of a number format that I call
// "high-decimal". This is a version of decimal where the digit 0 is
// encoded as the byte 246, the digit 1 as the byte 247, ..., the
// digit 9 as the byte 255. The bytes are stored in the normal
// endianness for the processor (i.e. least significant first), and
// padded to a known length (typically 8 digits) with leading zeroes.
//
// The point of high-decimal is that it allows us to use arithmetic
// operators intended for binary on high-decimal numbers, and the
// carries will work the same way (i.e. the same digits will carry,
// although carries will be 0-based rather than 246-based); all that's
// required is to identify the digits that carried and add 246 to
// them. That means that the processor's binary ALU can be used to do
// additions directly in decimal - there's no need for loops or
// anything like that, and no need to do binary/decimal conversions.
//
// The first use for high-decimal is to store the line number during
// the second phase (it's stored differently in the third phase).
// It's stored it in the top half of %xmm1 (although it's only 64 bits
// wide, it needs to be in a vector register so that it can be
// interpreted as 8 x 8 bits when necessary; general-purpose
// registers can't do that). The bottom half of %xmm1 is unused, and
// frequently overwritten with arbitrary data.
.section .rodata
line_number_init:
#define REP8(x) x,x,x,x,x,x,x,x
.byte REP8(0)
.byte 246, 247, 246, 246, 246, 246, 246, 246
.text
vmovdqa %xmm1, [%rip + line_number_init]

// Writing line numbers is nontrivial because x86-64 is little-endian
// but FizzBuzz output is big-endian; also, leading zeroes aren't
// allowed. ENDIAN_SHUFFLE is used to fix both these problems; when
// used to control the vector shuffler, it reverses the order of a
// vector register, and rotates the elements to put the first digit
// (based on LINENO_WIDTH) into the first byte. (This method is used
// by both the second and third phases; the second phase uses only the
// bottom half, with the top half used by the third phase, but they
// are both initialized together.)
.section .rodata
endian_shuffle_init:
.byte 9, 8, 7, 6, 5, 4, 3, 2
.byte 1, 0, 255, 254, 253, 252, 251, 250
.byte 3, 2, 1, 0, 255, 254, 253, 252
.byte 251, 250, 249, 248, 247, 246, 245, 244
.text


second_phase_per_width_init:

// The second phase writing routines are macros.
//
// Fizz and Buzz are trivial. (This writes a little beyond the end of
// the string, but that's OK; the next line will overwrite them.)
#define WRITE_FIZZ   mov [OUTPUT_PTR], FIZZ; add OUTPUT_PTR, 5
#define WRITE_BUZZ   mov [OUTPUT_PTR], BUZZ; add OUTPUT_PTR, 5

// For FizzBuzz, output 32 bits of FIZZ to write "Fizz" with no
// newline, then write a "Buzz" after that.
#define WRITE_FIZZBUZZ \
  mov [OUTPUT_PTR], FIZZe; mov [OUTPUT_PTR + 4], BUZZ; \
  add OUTPUT_PTR, 9

// To write a line number, add 58 to each byte of the line number
// %xmm1, fix the endianness and width with a shuffle, and write a
// final newline.
.section .rodata
ascii_offset:
.byte REP8(58), REP8(58), REP8(58), REP8(58)
.text
#define WRITE_LINENO \
  vpaddb %xmm0, ASCII_OFFSETx, %xmm1; \
  vpshufb %xmm0, %xmm0, ENDIAN_SHUFFLEx; \
  vmovdqu [OUTPUT_PTR], %xmm0; \
  lea OUTPUT_PTR, [OUTPUT_PTR + LINENO_WIDTH + 1]; \
  mov byte ptr [OUTPUT_PTR - 1], 10  // 10 = newline

// Incrementing the line number is fairly easy: add 1 (in the usual
// binary notation, taken from %xmm3) to the high-decimal number, then
// convert any bytes that produced a carry to high-decimal 0s by
// max-ing with 246.
//
// Normally I'd use a separate constant for this, but there randomly
// happens to be an %xmm register with 246s in its top half already
// (it's intended for an entirely different purpose, but it'll do for
// this one too).
#define INC_LINENO \
  vpaddq %xmm1, %xmm3, %xmm1; vpmaxub %xmm1, LINENO_TOPx, %xmm1

// Avoid modulus tests by unrolling the FizzBuzz by 15. (Bear in mind
// that this starts at 10, not 0, so the pattern will have a different
// phase than usual.)
mov %ecx, GROUPS_OF_15e
fifteen_second_phase_fizzbuzz_lines:
WRITE_BUZZ; INC_LINENO
WRITE_LINENO; INC_LINENO
WRITE_FIZZ; INC_LINENO
WRITE_LINENO; INC_LINENO
WRITE_LINENO; INC_LINENO
WRITE_FIZZBUZZ; INC_LINENO
WRITE_LINENO; INC_LINENO
WRITE_LINENO; INC_LINENO
WRITE_FIZZ; INC_LINENO
WRITE_LINENO; INC_LINENO
WRITE_BUZZ; INC_LINENO
WRITE_FIZZ; INC_LINENO
WRITE_LINENO; INC_LINENO
WRITE_LINENO; INC_LINENO
WRITE_FIZZ; INC_LINENO
dec %ecx
jnz fifteen_second_phase_fizzbuzz_lines

second_phase_increment_width:

lea GROUPS_OF_15e, [GROUPS_OF_15 + GROUPS_OF_15 * 4]
add GROUPS_OF_15e, GROUPS_OF_15e
inc LINENO_WIDTHe

// Increment every element of the low half of ENDIAN_SHUFFLE to
// adjust it for the new width, while leaving the top half unchanged.
vpcmpeqb %xmm0, %xmm0, %xmm0
vpsubb ENDIAN_SHUFFLE, ENDIAN_SHUFFLE, %ymm0

// The second phase handles line numbers with 2 to 5 digits.
cmp LINENO_WIDTHe, 6
jne second_phase_per_width_init

///// The output routine
//
// Most FizzBuzz routines produce output with `write` or a similar
// system call, but these have the disadvantage that they need to copy
// the data being output from userspace into kernelspace. It turns out
// that when running full speed (as seen in the third phase), FizzBuzz
// actually runs faster than `memcpy` does, so `write` and friends are
// unusable when aiming for performance - this program runs five times
// faster than an equivalent that uses `write`-like system calls.
//
// To produce output without losing speed, the program therefore needs
// to avoid copies, or at least do them in parallel with calculating
// the next block of output. This can be accomplished with the
// `vmsplice` system call, which tells the kernel to place a reference
// to a buffer into a pipe (as opposed to copying the data into the
// pipe); the program at the other end of this pipe will then be able
// to read the output directly out of this program's memory, with no
// need to copy the data into kernelspace and then back into
// userspace. In fact, it will be reading out of this program's
// processor's L2 cache, without main memory being touched at all;
// this is the secret to high-performance programming, because the
// cache is much faster than main memory is.
//
// Of course, it's therefore important to avoid changing the output
// buffer until the program connected to standard output has actually
// read it all. This is why the pipe size needed to be set earlier; as
// long as the amount of output is always at least as large as the
// pipe size, successfully outputting one buffer will ensure that none
// of the other buffer is left in the pipe, and thus it's safe to
// overwrite the memory that was previously output. There is some need
// to jump through hoops later on to make sure that `swap_buffers` is
// never called with less than one pipeful of data, but it's worth it
// to get the huge performance boost.

mov %rdx, OUTPUT_PTR
and %edx, (2 &lt;&lt; 20) - 1

call swap_buffers
jmp third_phase_init

// Takes the amount of data to output in %rdx, and outputs from the
// buffer containing OUTPUT_PTR.
swap_buffers:
and OUTPUT_PTR, -(2 &lt;&lt; 20)  // rewind to the start of the buffer
mov [%rip + iovec_base], OUTPUT_PTR
mov [%rip + iovec_base + 8], %rdx
mov ARG1e, 1
lea ARG2, [%rip + iovec_base]
mov ARG3e, 1
xor ARG4e, ARG4e

// As with most output commands, vmsplice can do a short write
// sometimes, so it needs to be called in a loop in order to ensure
// that all the output is actually sent.
1: mov SYSCALL_NUMBER, __NR_vmsplice
syscall
call exit_on_error
add [ARG2], SYSCALL_RETURN
sub [ARG2 + 8], SYSCALL_RETURN
jnz 1b

xor OUTPUT_PTR, (2 &lt;&lt; 20)  // swap to the other buffer
ret


///// Third phase of output
//
// This is the heart of this program. It aims to be able to produce a
// sustained output rate of 64 bytes of FizzBuzz per four clock cycles
// in its main loop (with frequent breaks to do I/O, and rare breaks
// to do more expensive calculations).
//
// The third phase operates primarily using a bytecode interpreter; it
// generates a program in "FizzBuzz bytecode", for which each byte of
// bytecode generates one byte of output. The bytecode language is
// designed so that it can be interpreted using SIMD instructions; 32
// bytes of bytecode can be loaded from memory, interpreted, and have
// its output stored back into memory using just four machine
// instructions. This makes it possible to speed up the FizzBuzz
// calculations by hardcoding some of the calculations into the
// bytecode (this is similar to how JIT compilers can create a version
// of the program with some variables hardcoded, and throw it away on
// the rare occasions that those variables' values change).

third_phase_init:

// Reinitialize ENDIAN_SHUFFLE by copying the initializer stored in
// its high half to both halves. This works in the same way as in the
// second phase.
vpermq ENDIAN_SHUFFLE, ENDIAN_SHUFFLE, 0xEE

// Up to this point, PIPE_SIZE has held the size of the pipe. In order
// to save on registers, the pipe size is from now on encoded via the
// location in which the bytecode program is stored; the bytecode is
// started at iovec_base + PIPE_SIZE (which will be somewhere within
// bytecode_storage), so the same register can be used to find the
// bytecode and to remember the pipe size.
lea %rax, [%rip + iovec_base]
add BYTECODE_START, %rax  // BYTECODE_START is a synonym for PIPE_SIZE

// The bytecode program always holds instructions to produce exactly
// 600 lines of FizzBuzz. At width 6, those come to 3800 bytes long.
lea BYTECODE_END, [BYTECODE_START + 3800]

mov REGEN_TRIGGER, -1  // irrelevant until much later, explained there


third_phase_per_width_init:

// Calculate the amount of output at this LINENO_WIDTH. The result
// will always be divisible by 32, and thus is stored as the number of
// 32-byte units at this width; storing it in bytes would be more
// convenient, but sadly would overflow a 64-bit integer towards the
// end of the program.
lea %ecx, [LINENO_WIDTH * 8 + 47]   // bytes per 15 lines
mov YMMS_AT_WIDTH, GROUPS_OF_15
shr YMMS_AT_WIDTH, 5   // to avoid overflow, divide by 32 first
imul YMMS_AT_WIDTH, %rcx

// This program aims to output 64 bytes of output per four clock
// cycles, which it achieves via a continuous stream of 32-byte writes
// calculated by the bytecode program. One major complication here is
// that the 32-byte writes won't correspond to lines of FizzBuzz; a
// single processor instruction may end up outputting multiple
// different line numbers. So it's no longer possible to have a simple
// line number register, like it was in the second phase.
//
// Instead, the program stores an *approximation* of the line number,
// which is never allowed to differ by 100 or more from the "actual"
// line number; the bytecode program is responsible for fixing up the
// approximation to work out the correct line number to output (this
// allows the same CPU instruction to output digits from multiple
// different line numbers, because the bytecode is being interpreted
// in a SIMD way and thus different parts of the bytecode can fix the
// line number up differently within a single instruction.
//
// The line number is split over three processor registers:
// - LINENO_LOW: stores the line number modulo 200
// - LINENO_MID: stores the hundreds to billions digits
// - LINENO_TOP: stores the ten-billions and more significant digits
// (The parity of the 100s digit is duplicated between LINENO_MID and
// LINENO_LOW; this allows a faster algorithm for LINENO_MID updates.)
//
// Because there's only a need to be within 100 of the real line
// number, the algorithm for updating the line numbers doesn't need to
// run all that often (saving processor cycles); it runs once every
// 512 bytes of output, by simply adding a precalculated value
// (LINENO_LOW_INCR) to LINENO_LOW, then processing the carry to
// LINENO_MID (see later for LINENO_TOP). The amount by which the line
// number increases per 512 bytes of output is not normally going to
// be an integer; LINENO_LOW is therefore stored as a 64-bit fixpoint
// number (in which 2**64 represents "200", e.g. 2**63 would be the
// representation of "the line number is 100 mod 200"), in order to
// delay the accumulation of rounding errors as long as possible. It's
// being stored in a vector register, so there are four copies of its
// value; two of them have 50 (i.e 2**62) added, and two of them have
// 50 subtracted, in order to allow for more efficient code to handle
// the carry to LINENO_MID. Additionally, LINENO_LOW is interpreted as
// a signed number (an older version of this program was better at
// checking for signed than unsigned overflow and I had no reason to
// change).
//
// LINENO_LOW and LINENO_MID are reset every LINENO_WIDTH increase
// (this is because the program can calculate "past" the width
// increase due to not being able to break out of every instruction of
// the main loop, which may cause unwanted carries into LINENO_MID and
// force a reset).

.section .rodata
lineno_low_init:
.byte 0, 0, 0, 0, 0, 0, 0, 192
.byte 0, 0, 0, 0, 0, 0, 0, 64
.byte 0, 0, 0, 0, 0, 0, 0, 192
.byte 0, 0, 0, 0, 0, 0, 0, 64
.text
vmovdqa LINENO_LOW, LINENO_LOW_INIT

// %ecx is the number of bytes in 15 lines. That means that the number
// of 200-line units in 512 bytes is 38.4/%ecx, i.e. 384/(%ecx*10).
// Multiply by 2**64 (i.e. 384*2**64/(%ecx*10) to get LINENO_LOW_INCR.
lea %ecx, [%rcx + %rcx * 4]
add %ecx, %ecx
mov %edx, 384
xor %eax, %eax
div %rcx  // 128-bit divide, %rax = %rdx%rax / %rcx
vpxor LINENO_LOW_INCR, LINENO_LOW_INCR, LINENO_LOW_INCR
vpinsrq LINENO_LOW_INCRx, LINENO_LOW_INCRx, %rax, 0
vpermq LINENO_LOW_INCR, LINENO_LOW_INCR, 0

// LINENO_MID is almost stored in high-decimal, as four eight-digit
// numbers. However, the number represented is the closest line number
// that's 50 mod 100, stored as the two closest multiples of 100 (e.g.
// if the true line number is 235, it's approximated as 250 and then
// stored using the representations for 200 and 300), which is why
// LINENO_LOW needs the offsets of 50 and -50 to easily do a carry. A
// ymm vector holds four 64-bit numbers, two of which hold the value
// that's 0 mod 200, two which hold the value that's 100 mod 200. So
// carries on it are handled using a vector of mostly 246s, with 247s
// in the two locations which are always odd.
.section .rodata
lineno_mid_base:
.byte 246, 246, 246, 246, 246, 246, 246, 246
.byte 247, 246, 246, 246, 246, 246, 246, 246
.byte 246, 246, 246, 246, 246, 246, 246, 246
.byte 247, 246, 246, 246, 246, 246, 246, 246
.text

// This code is some fairly complex vector manipulation to initialise
// LINENO_MID to a power of 10 (handling the case where LINENO_WIDTH
// is so high that the hundreds to billions digits are all zeroes).
mov %edx, 1
mov %eax, 11
sub %eax, LINENO_WIDTHe
cmovbe %eax, %edx
shl %eax, 3
vpxor %xmm0, %xmm0, %xmm0
vpinsrq %xmm0, %xmm0, %rax, 0
vpermq %ymm0, %ymm0, 0
vpcmpeqb LINENO_MID, LINENO_MID, LINENO_MID
vpsrlq LINENO_MID, LINENO_MID, %xmm0
vpmaxub LINENO_MID, LINENO_MID_BASE, LINENO_MID
vpermq %ymm0, LINENO_MID_BASE, 0x55
vpsubb %ymm0, %ymm0, LINENO_MID_BASE
vpaddq LINENO_MID, LINENO_MID, %ymm0
vpmaxub LINENO_MID, LINENO_MID_BASE, LINENO_MID

// LINENO_TOP doesn't need to be initialized for new widths, because
// an overrun by 100 lines is possible, but by 10 billion lines isn't.
// The format consists of two 64-bit sections that hold high-decimal
// numbers (these are always the same as each other), and two that
// hold constants that are used by the bytecode generator.
.section .rodata
lineno_top_init:
.byte 198, 197, 196, 195, 194, 193, 192, 191
.byte 246, 246, 246, 246, 246, 246, 246, 246
.byte 190, 189, 188, 187, 186, 185, 184, 183
.byte 246, 246, 246, 246, 246, 246, 246, 246
.text

// When moving onto a new width, start at the start of the bytecode
// program.
mov BYTECODE_IP, BYTECODE_START


// Generating the bytecode program
//
// The bytecode format is very simple (in order to allow it to be
// interpreted in just a couple of machine instructions):
// - A negative byte represents a literal character (e.g. to produce
//   a literal 'F', you use the bytecode -'F', i.e. -70 = 0xba)
// - A byte 0..7 represents the hundreds..billions digit of the line
//   number respectively, and asserts that the hundreds digit of the
//   line number is even
// - A byte 8..15 represents the hundreds..billions digit of the line
//   number respectively, and asserts that the hundreds digit of the
//   line number is odd
//
// In other words, the bytecode program only ever needs to read from
// LINENO_MID; the information stored in LINENO_LOW and LINENO_TOP
// therefore has to be hardcoded into it. The program therefore needs
// to be able to generate 600 lines of output (as the smallest number
// that's divisible by 100 to be able to hardcode the two low digits,
// 200 to be able to get the assertions about the hundreds digits
// correct, and 3 and 5 to get the Fizzes and Buzzes in the right
// place).

generate_bytecode:

mov BYTECODE_GEN_PTR, BYTECODE_START

// FIZZ and BUZZ work just like in the second phase, except that they
// are now bytecode programs rather than ASCII.
mov FIZZ, 0xf6868697ba  // -"Fizz\n"
mov BUZZ, 0xf686868bbe  // -"Buzz\n"

// %ymm2 holds the bytecode for outputting the hundreds and more
// significant digits of a line number. The most significant digits of
// this can be obtained by converting LINENO_TOP from high-decimal to
// the corresponding bytecode, which is accomplished by subtracting
// from 198 (i.e. 256 - 10 - '0'). The constant parts of LINENO_TOP
// are 198 minus the bytecode for outputting the hundreds to billions
// digit of a number; this makes it possible for a single endian
// shuffle to deal with all 16 of the mid and high digits at once.
.section .rodata
bascii_offset:
.byte REP8(198), REP8(198), REP8(198), REP8(198)
.text
vpsubb %ymm2, BASCII_OFFSET, LINENO_TOP
vpshufb %ymm2, %ymm2, ENDIAN_SHUFFLE

#define GEN_FIZZ  mov [BYTECODE_GEN_PTR], FIZZ; add BYTECODE_GEN_PTR, 5
#define GEN_BUZZ  mov [BYTECODE_GEN_PTR], BUZZ; add BYTECODE_GEN_PTR, 5
#define GEN_FIZZBUZZ \
  mov [BYTECODE_GEN_PTR], FIZZe; \
  mov [BYTECODE_GEN_PTR + 4], BUZZ; add BYTECODE_GEN_PTR, 9
#define GEN_LINENO(units_digit) \
  vmovdqu [BYTECODE_GEN_PTR], %xmm2; \
  lea BYTECODE_GEN_PTR, [BYTECODE_GEN_PTR + LINENO_WIDTH + 1]; \
  mov [BYTECODE_GEN_PTR - 3], %al; \
  mov word ptr [BYTECODE_GEN_PTR - 2], 0xf6d0 - units_digit

// The bytecode generation loop is unrolled to depth 30, allowing the
// units digits to be hardcoded. The tens digit is stored in %al, and
// incremented every ten lines of output. The parity of the hundreds
// digit is stored in %ymm2: one half predicts the hundreds digit to
// be even, the other to be odd, and the halves are swapped every time
// the tens digit carries (ensuring the predictions are correct).
mov %eax, 0xd0
jmp 2f
inc_tens_digit:
cmp %al, 0xc7
je 1f  // jumps every 10th execution, therefore predicts perfectly
dec %eax
ret
1: mov %eax, 0xd0
vpermq %ymm2, %ymm2, 0x4e
ret

2: mov %ecx, 20
thirty_bytecode_lines:
GEN_BUZZ
GEN_LINENO(1)
GEN_FIZZ
GEN_LINENO(3)
GEN_LINENO(4)
GEN_FIZZBUZZ
GEN_LINENO(6)
GEN_LINENO(7)
GEN_FIZZ
GEN_LINENO(9)
call inc_tens_digit
GEN_BUZZ
GEN_FIZZ
GEN_LINENO(2)
GEN_LINENO(3)
GEN_FIZZ
GEN_BUZZ
GEN_LINENO(6)
GEN_FIZZ
GEN_LINENO(8)
GEN_LINENO(9)
call inc_tens_digit
GEN_FIZZBUZZ
GEN_LINENO(1)
GEN_LINENO(2)
GEN_FIZZ
GEN_LINENO(4)
GEN_BUZZ
GEN_FIZZ
GEN_LINENO(7)
GEN_LINENO(8)
GEN_FIZZ
call inc_tens_digit
dec %ecx
jnz thirty_bytecode_lines

generate_bytecode_overrun_area:

// Duplicate the first 512 bytes of the bytecode program at the end,
// so that there's no need to check to see whether BYTECODE_IP needs
// to be looped back to the start of the program any more than once
// per 512 bytes
mov %rax, BYTECODE_START
#define COPY_64_BYTECODE_BYTES(offset) \
  vmovdqa %ymm0, [%rax + offset]; \
  vmovdqa %ymm3, [%rax + (offset + 32)]; \
  vmovdqu [BYTECODE_GEN_PTR + offset], %ymm0; \
  vmovdqu [BYTECODE_GEN_PTR + (offset + 32)], %ymm3
COPY_64_BYTECODE_BYTES(0)
COPY_64_BYTECODE_BYTES(64)
COPY_64_BYTECODE_BYTES(128)
COPY_64_BYTECODE_BYTES(192)
COPY_64_BYTECODE_BYTES(256)
COPY_64_BYTECODE_BYTES(320)
COPY_64_BYTECODE_BYTES(384)
COPY_64_BYTECODE_BYTES(448)


// Preparing for the main loop
//
// Work out how long the main loop is going to iterate for.
// OUTPUT_LIMIT holds the address just beyond the end of the output
// that the main loop should produce. The aim here is to produce
// exactly one pipeful of data if possible, but to stop earlier if
// there's a change in digit width (because any output beyond that
// point will be useless: the bytecode will give it the wrong number
// of digits).
calculate_main_loop_iterations:

// Extract the pipe size from BYTECODE_START, in 32-byte units.
// During this calculation, OUTPUT_LIMIT holds the amount of output
// produced, rather than an address like normal.
mov OUTPUT_LIMIT, BYTECODE_START
lea %rdx, [%rip + iovec_base]
sub OUTPUT_LIMIT, %rdx
shr OUTPUT_LIMIT, 5

// Reduce the output limit to the end of this width, if it would be
// higher than that.
cmp OUTPUT_LIMIT, YMMS_AT_WIDTH
cmovae OUTPUT_LIMIT, YMMS_AT_WIDTH

// If there's already some output in the buffer, reduce the amount
// of additional output produced accordingly (whilst ensuring that
// a multiple of 512 bytes of output is produced).
//
// This would be buggy if the YMMS_AT_WIDTH limit were hit at the
// same time, but that never occurs as it would require two width
// changes within one pipeful of each other, and 9000000 lines of
// FizzBuzz is much more than a pipeful in size.
mov %rax, OUTPUT_PTR
and %eax, ((2 &lt;&lt; 20) - 1) &amp; -512
shr %eax, 5
sub OUTPUT_LIMIT, %rax

// The amount of output to produce is available now, and won't be
// later, so subtract it from the amount of output that needs to
// be produced now.
sub YMMS_AT_WIDTH, OUTPUT_LIMIT

// Return OUTPUT_LIMIT back to being a pointer, not an amount.
shl OUTPUT_LIMIT, 5
add OUTPUT_LIMIT, OUTPUT_PTR

prepare_main_loop_invariants:

// To save one instruction in the bytecode interpreter (which is very
// valuable, as it runs every second CPU cycle), LINENO_MID_TEMP is
// used to store a reformatted version of LINENO_MID, in which each
// byte is translated from high-decimal to ASCII, and the bytecode
// command that would access that byte is added to the result (e.g.
// the thousands digit for the hundreds-digits-odd version has 10
// added to convert from high-decimal to a pure number, '0' added to
// convert to ASCII, then 9 added because that's the bytecode command
// to access the thousands digit when the hundreds digit is odd, so
// the amount added is 10 + '0' + 9 = 57).
//
// LINENO_MID_TEMP is updated within the main loop, immediately after
// updating LINENO_MID, but because the bytecode interpreter reads
// from it it needs a valid value at the start of the loop.
.section .rodata
biascii_offset:
.byte 58, 59, 60, 61, 62, 63, 64, 65
.byte 66, 67, 68, 69, 70, 71, 72, 73
.byte 58, 59, 60, 61, 62, 63, 64, 65
.byte 66, 67, 68, 69, 70, 71, 72, 73
.text
vpaddb LINENO_MID_TEMP, BIASCII_OFFSET, LINENO_MID

// To save an instruction, precalculate minus the length of the
// bytecode. (Although the value of this is determined entirely by
// LINENO_WIDTH, the register it's stored in gets clobbered by
// system calls and thus needs to be recalculated each time.)
mov BYTECODE_NEG_LEN, BYTECODE_START
sub BYTECODE_NEG_LEN, BYTECODE_END


// The main loop

// The bytecode interpreter consists of four instructions:
// 1. Load the bytecode from memory into %ymm2;
// 2. Use it as a shuffle mask to shuffle LINENO_MID_TEMP;
// 3. Subtract the bytecode from the shuffle result;
// 4. Output the result of the subtraction.
//
// To see why this works, consider two cases. If the bytecode wants to
// output a literal character, then the shuffle will produce 0 for
// that byte (in AVX2, a shuffle with a a negative index produces an
// output of 0), and subtracting the bytecode from 0 then produces the
// character (because the bytecode encoded minus the character). If
// the bytecode instead wants to output a digit, then the shuffle will
// fetch the relevant digit from LINENO_MID_TEMP (which is the desired
// ASCII character plus the bytecode instruction that produces it),
// and subtract the bytecode instruction to just produce the character
// on its own.
//
// This produces an exactly correct line number as long as the line
// number approximation is within 100 of the true value: it will be
// correct as long as the relevant part of LINENO_MID is correct, and
// the worst case is for LINENO_MID to be storing, say, 200 and 300
// (the representation of 250) when the true line number is 400. The
// value in LINENO_MID specifically can be up to 50 away from the
// value of the line number as recorded by LINENO_MID and LINENO_LOW
// together, so as long as the line number registers are within 100,
// LINENO_MID will be within 150 (which is what is required).
//
// This doesn't update the bytecode instruction pointer or the pointer
// into the output buffer; those are updated once every 512 bytes (and
// to "advance the instruction pointer" the rest of the time, the main
// loop is unrolled, using hardcoded offsets with the pointer updates
// baked in).
//
// The bytecode instruction pointer itself is read from %rdx, not
// BYTECODE_IP, so that mid-loop arithmetic on BYTECODE_IP won't cause
// the interpreter to break.
//
// It's important to note one potential performance issue with this
// code: the read of the bytecode from memory is not only misalignable
// (`vmovdqu`); it splits a cache line 3/8 of the time. This causes L1
// split-load penalties on the 3/8 cycles where it occurs. I am not
// sure whether this actually reduces the program's performance in
// practice, or whether the split loads can be absorbed while waiting
// for writes to go through to the L2 cache. However, even if it does
// have a genuine performance cost, it seems like the least costly way
// to read the bytecode; structuring the bytecode to avoid split loads
// makes it take up substantially more memory, and the less cache that
// is used for the bytecode, the more that can be used for the output
// buffers. (In particular, increasing the bytecode to 2400 lines so
// that it's available at all four of the alignments required of it
// does not gain, because it then becomes so large that the processor
// struggles to keep it in L1 cache - it only just fits, and there
// isn't any way for it to know which parts of the cache are meant to
// stay in L1 and which are meant to leave to L2, so there's a large
// slowdown when it guesses wrong.)
#define INTERPRET_BYTECODE(bc_offset, buf_offset) \
  vmovdqu %ymm2, [%rdx + bc_offset]; \
  vpshufb %ymm0, LINENO_MID_TEMP, %ymm2; \
  vpsubb %ymm0, %ymm0, %ymm2; \
  vmovdqa [OUTPUT_PTR + buf_offset], %ymm0

// The main loop itself consists of sixteen uses of the bytecode
// interpreter, interleaved (to give the reorder buffer maximum
// flexibility) with all the other logic needed in the main loop.
// (Most modern processors can handle 4-6 instructions per clock cycle
// as long as they don't step on each others' toes; thus this loop's
// performance will be limited by the throughput of the L2 cache, with
// all the other work (bytecode interpretation, instruction decoding,
// miscellaneous other instructions, etc.) fitting into the gaps while
// the processor is waiting for the L2 cache to do its work.)

.p2align 5
main_loop:
// %rdx caches BYTECODE_IP's value at the start of the loop
mov %rdx, BYTECODE_IP
INTERPRET_BYTECODE(0, 0)

// %ymm1 caches LINENO_LOW's value at the start of the loop
vmovdqa %ymm1, LINENO_LOW
INTERPRET_BYTECODE(32, 32)

// Add LINENO_LOW_INCR to LINENO_LOW, checking for carry; it carried
// if the sign bit changed from 0 to 1. (vpandn is unintuitive; this
// is ~%ymm1 &amp; LINENO_LOW, not %ymm1 &amp; ~LINENO_LOW like the name
// suggests.)
vpaddq LINENO_LOW, LINENO_LOW_INCR, LINENO_LOW
INTERPRET_BYTECODE(64, 64)

vpandn %ymm3, %ymm1, LINENO_LOW
INTERPRET_BYTECODE(96, 96)

vpsrlq %ymm3, %ymm3, 63
INTERPRET_BYTECODE(128, 128)

// Add the carry to LINENO_MID (doubling it; LINENO_MID counts in
// units of 100 but a LINENO_LOW carry means 200).
vpaddb %ymm3, %ymm3, %ymm3
INTERPRET_BYTECODE(160, 160)

vpaddq LINENO_MID, LINENO_MID, %ymm3
INTERPRET_BYTECODE(192, 192)

vpmaxub LINENO_MID, LINENO_MID_BASE, LINENO_MID
INTERPRET_BYTECODE(224, 224)

// Update LINENO_MID_TEMP with the new value from LINENO_MID; this is
// the point at which the new value takes effect. This is done at the
// exact midpoint of the loop, in order to reduce the errors from
// updating once every 512 bytes as far as possible.
vpaddb LINENO_MID_TEMP, BIASCII_OFFSET, LINENO_MID
INTERPRET_BYTECODE(256, 256)

// Update the output and bytecode instruction pointers. The change to
// the output pointer kicks in immediately, but is cancelled out via
// the use of a negative offset until the end of the loop.
add OUTPUT_PTR, 512
INTERPRET_BYTECODE(288, -224)

add BYTECODE_IP, 512
INTERPRET_BYTECODE(320, -192)

// The change to the bytecode instruction pointer doesn't kick in
// immediately, because it might need to wrap back to the start (this
// can be done by adding BYTECODE_NEG_LEN to it); this is why the
// interpreter has a cached copy of it in %rdx.
lea %rax, [BYTECODE_IP + BYTECODE_NEG_LEN]
INTERPRET_BYTECODE(352, -160)

INTERPRET_BYTECODE(384, -128)
// Some modern processors can optimise `cmp` better if it appears
// immediately before the command that uses the comparison result, so
// a couple of commands have been moved slightly to put the `cmp` next
// to the use of its result. With modern out-of-order processors,
// there is only a marginal advantage to manually interleaving the
// instructions being used, and the `cmp` advantage outweighs that.
cmp BYTECODE_IP, BYTECODE_END

cmovae BYTECODE_IP, %rax
INTERPRET_BYTECODE(416, -96)

INTERPRET_BYTECODE(448, -64)

INTERPRET_BYTECODE(480, -32)
cmp OUTPUT_PTR, OUTPUT_LIMIT
jb main_loop

after_main_loop:
// There are two reasons the main loop might terminate: either there's
// a pipeful of output, or the line number has increased in width
// (forcing the generaion of new bytecode to put more digits in the
// numbers being printed). In the latter case, a) the output may have
// overrun slightly, and OUTPUT_PTR needs to be moved back to
// OUTPUT_LIMIT:
mov OUTPUT_PTR, OUTPUT_LIMIT
// and b) there may be less than a pipeful of output, in which case it
// wouldn't be safe to output it and the swap_buffers call needs to be
// skipped. Calculate the pipe size into %rax, the amount of output
// into %rdx (swap_buffers needs it there anyway), and compare.
lea %rax, [%rip + iovec_base]
sub %rax, BYTECODE_START
neg %eax
mov %rdx, OUTPUT_PTR
and %edx, (2 &lt;&lt; 20) - 1
cmp %edx, %eax
jb 1f
call swap_buffers

// If all the lines at this width have been exhausted, move to the
// next width.
1: test YMMS_AT_WIDTH, YMMS_AT_WIDTH
jnz check_lineno_top_carry

cmp LINENO_WIDTHe, 18  // third phase handles at most 18 digits
je fourth_phase

inc LINENO_WIDTHe
vpcmpeqb %ymm0, %ymm0, %ymm0
vpsubb ENDIAN_SHUFFLE, ENDIAN_SHUFFLE, %ymm0

lea GROUPS_OF_15, [GROUPS_OF_15 + GROUPS_OF_15 * 4]
add GROUPS_OF_15, GROUPS_OF_15

add BYTECODE_END, 320

jmp third_phase_per_width_init

// So far, the code has kept LINENO_MID and LINENO_LOW updated, but
// not LINENO_TOP. Because 10 billion lines of FizzBuzz don't normally
// have a length that's divisible by 512 (and indeed, vary in size a
// little because 10 billion isn't divisible by 15), it's possible for
// the 10-billions and higher digits to need to change in the middle
// of a main loop iteration - indeed, even in the middle of a single
// CPU instruction!
//
// It turns out that when discussing the line number registers above,
// I lied a little about the format. The bottom seven bytes of
// LINENO_MID do indeed represent the hundreds to hundred millions
// digits. However, the eighth changes in meaning over the course of
// the program. It does indeed represent the billions digit most of
// the time; but when the line number is getting close to a multiple
// of 10 billion, the billions and hundred-millions digits will always
// be the same as each other (either both 9s or both 0s). When this
// happens, the format changes: the hundred-millions digit of
// LINENO_MID represents *both* the hundred-millions and billions
// digits of the line number, and the top byte then represents the
// ten-billions digit. Because incrementing a number causes a row of
// consecutive 9s to either stay untouched, or all roll over to 0s at
// once, this effectively lets us do maths on more than 8 digits,
// meaning that the normal arithmetic code within the main loop can
// handle the ten-billions digit in addition to the digits below.
//
// Of course, the number printing code also needs to handle the new
// representation, but the number printing is done by a bytecode
// program, which can be made to output some of the digits being
// printed multiple times by repeating "print digit from LINENO_MID"
// commands within it. Those commands are generated from COUNTER_TOP
// anyway, so the program just changes the constant portion of
// COUNTER_TOP (and moves print-digit commands into the top half) in
// order to produce the appropriate bytecode changes.
//
// A similar method is used to handle carries in the hundred-billions,
// trillions, etc. digits.
//
// Incidentally, did you notice the apparent off-by-one in the
// initialisation of LINENO_MID within third_phase_per_width_init? It
// causes the "billions" digit to be initialised to 1 (not 0) when the
// line number width is 11 or higher. That's because the alternate
// representation will be in use during a line number width change (as
// higher powers of 10 are close to multiples of 10 billion), so the
// digit that's represented by that byte of LINENO_MID genuinely is a
// 1 rather than a 0.
check_lineno_top_carry:

// The condition to change line number format is:
// a) The line number is in normal format, and the hundred-millions
//    and billions digits are both 9; or
// b) The line number is in alternate format, and the hundred-millions
//    digit is 0.
// To avoid branchy code in the common case (when no format change is
// needed), REGEN_TRIGGER is used to store the specific values of the
// hundred-millions and billions digits that mean a change is needed,
// formatted as two repeats of billions, hundred-millions, 9, 9 in
// high-decimal (thus, when using normal format, REGEN_TRIGGER is
// high-decimal 99999999, i.e. -1 when interpreted as binary). The 9s
// are because vpshufd doesn't have very good resolution: the millions
// and ten-millions digits get read too, but can simply just be masked
// out. The two repeats are to ensure that both halves of LINENO_MID
// (the even-hundreds-digit and odd-hundreds-digit halves) have the
// correct value while changing (changing the format while half the
// register still ended ...98999999 would produce incorrect output).
vpshufd %xmm0, LINENO_MIDx, 0xED
vpextrq %rax, %xmm0, 0
mov %rdx, 0x0000ffff0000ffff
or %rax, %rdx
cmp %rax, REGEN_TRIGGER
jne calculate_main_loop_iterations

cmp REGEN_TRIGGER, -1
jne switch_to_normal_representation


switch_to_alternate_representation:
// Count the number of 9s at the end of LINENO_TOP. To fix an edge
// case, the top bit of LINENO_TOP is interpreted as a 0, preventing
// a 9 being recognised there (this causes 10**18-1 to increment to
// 10**17 rather than 10**18, but the program immediately exits
// before this can become a problem).
vpextrq %rdx, LINENO_TOPx, 1
mov SPILL, %rdx
shl %rdx, 1
shr %rdx, 1
not %rdx
bsf %rcx, %rdx
and %rcx, -8

// Change the format of LINENO_TOP so that the digit above the
// consecutive 9s becomes a reference to the top byte of LINENO_MID,
// and the 9s themselves references to the hundred-millions digit.
// This is done via a lookup table that specifies how to move the
// bytes around.
.section .rodata
alternate_representation_lookup_table:
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 7, 9, 10, 11, 12, 13, 14, 15
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 7, 9, 10, 11, 12, 13, 14, 15

.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 7, 10, 11, 12, 13, 14, 15
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 7, 10, 11, 12, 13, 14, 15

.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 7, 11, 12, 13, 14, 15
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 7, 11, 12, 13, 14, 15

.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 7, 12, 13, 14, 15
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 7, 12, 13, 14, 15

.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 6, 7, 13, 14, 15
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 6, 7, 13, 14, 15

.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 6, 6, 7, 14, 15
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 6, 6, 7, 14, 15

.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 6, 6, 6, 7, 15
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 6, 6, 6, 7, 15

.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 6, 6, 6, 6, 7
.byte 0, 1, 2, 3, 4, 5, 6, 6
.byte 6, 6, 6, 6, 6, 6, 6, 7
.text

lea %rax, [%rip + alternate_representation_lookup_table]
vpshufb LINENO_TOP, LINENO_TOP, [%rax + 4 * %rcx]

// The top byte of LINENO_MID also needs the appropriate digit of
// LINENO_TOP placed there.
mov %rdx, SPILL
shr %rdx, %cl
vpinsrb LINENO_MIDx, LINENO_MIDx, %edx, 7
vpinsrb LINENO_MIDx, LINENO_MIDx, %edx, 15
vpermq LINENO_MID, LINENO_MID, 0x44

// Finally, REGEN_TRIGGER needs to store the pattern of digits that
// will prompt a shift back to the normal representation (the hundred-
// millions digit must be 0, and the value of the billions digit will
// be predictable).
inc %edx
shl %edx, 24
or %edx, 0xF6FFFF
mov REGEN_TRIGGERe, %edx
shl %rdx, 32
or REGEN_TRIGGER, %rdx
jmp generate_bytecode


switch_to_normal_representation:
// Switching back is fairly easy: LINENO_TOP can almost be converted
// back into its usual format by running the bytecode program stored
// there to remove any unusual references into LINENO_MID, then
// restoring the usual references manually. Running the program will
// unfortunately convert high-decimal to ASCII (or in this case zeroes
// because there's no need to do the subtraction), but that can be
// worked around by taking the bytewise maximum of the converted and
// original LINENO_TOP values (high-decimal is higher than bytecode
// references and much higher than zero).
vpsubb %ymm2, BASCII_OFFSET, LINENO_TOP
vpshufb %ymm0, LINENO_MID, %ymm2
vpmaxub LINENO_TOP, LINENO_TOP, %ymm0

// Manually fix the constant parts of lineno_top to contain their
// usual constant values
.section .rodata
lineno_top_max:
.byte 198, 197, 196, 195, 194, 193, 192, 191
.byte 255, 255, 255, 255, 255, 255, 255, 255
.byte 190, 189, 188, 187, 186, 185, 184, 183
.byte 255, 255, 255, 255, 255, 255, 255, 255
.text
vpminub LINENO_TOP, LINENO_TOP_MAX, LINENO_TOP

// The billions digit of LINENO_MID needs to be set back to 0 (which
// is its true value at this point: the same as the hundred-thousands
// digit, which is also 0).
vpsllq LINENO_MID, LINENO_MID, 8
vpsrlq LINENO_MID, LINENO_MID, 8
vpmaxub LINENO_MID, LINENO_MID_BASE, LINENO_MID

mov REGEN_TRIGGER, -1

jmp generate_bytecode


///// Fourth phase
//
// Ending at 999999999999999999 lines would be a little unsatisfying,
// so here's a routine to write the quintillionth line and exit.
//
// It's a "Buzz", which we can steal from the first phase's constant.

fourth_phase:

mov ARG1e, 1
lea ARG2, [%rip + fizzbuzz_intro + 11]
mov ARG3, 5
mov SYSCALL_NUMBER, __NR_write
syscall
call exit_on_error
xor ARG1e, ARG1e
jmp exit


///// Error handling code
//
// This doesn't run in a normal execution of the program, and isn't
// particularly optimised; I didn't comment it much because it isn't
// very interesting and also is fairly self-explanatory.

write_stderr:
mov ARG1e, 2
mov SYSCALL_NUMBER, __NR_write
syscall
ret

inefficiently_write_as_hex:
push %rax
push %rcx
shr %rax, %cl
and %rax, 0xF
.section .rodata
hexdigits: .ascii "0123456789ABCDEF"
.text
lea %rcx, [%rip + hexdigits]
movzx %rax, byte ptr [%rcx + %rax]
mov [%rip + error_write_buffer], %al
lea ARG2, [%rip + error_write_buffer]
mov ARG3e, 1
call write_stderr
pop %rcx
pop %rax
sub %ecx, 4
jns inefficiently_write_as_hex
ret

exit_on_error:
test SYSCALL_RETURN, SYSCALL_RETURN
js 1f
ret

.section .rodata
error_message_part_1: .ascii "Encountered OS error 0x"
error_message_part_2: .ascii " at RIP 0x"
error_message_part_3: .ascii ", exiting program.\n"
.text

1: push SYSCALL_RETURN
lea ARG2, [%rip + error_message_part_1]
mov ARG3e, 23
call write_stderr
pop SYSCALL_RETURN
neg SYSCALL_RETURN
mov %rcx, 8
call inefficiently_write_as_hex
lea ARG2, [%rip + error_message_part_2]
mov ARG3e, 10
call write_stderr
pop %rax  // find the caller's %rip from the stack
sub %rax, 5  // `call exit_on_error` compiles to 5 bytes
mov %rcx, 60
call inefficiently_write_as_hex
lea ARG2, [%rip + error_message_part_3]
mov ARG3e, 19
call write_stderr
mov ARG1e, 74
// fall through

exit:
mov SYSCALL_NUMBER, __NR_exit_group
syscall
ud2

.section .rodata
cpuid_error_message:
.ascii "Error: your CPUID command does not support command "
.ascii "0x80000006 (AMD-style L2 cache information).\n"
.text
bad_cpuid_error:
lea ARG2, [%rip + cpuid_error_message]
mov ARG3e, 96
call write_stderr
mov ARG1e, 59
jmp exit

.section .rodata
pipe_error_message:
.ascii "This program can only output to a pipe "
.ascii "(try piping into `cat`?)\n"
.text
pipe_error:
lea ARG2, [%rip + pipe_error_message]
mov ARG3e, 64
call write_stderr
mov ARG1e, 73
jmp exit

.section .rodata
pipe_perm_error_message_part_1:
.ascii "Cannot allocate a sufficiently large kernel buffer.\n"
.ascii "Try setting /proc/sys/fs/pipe-max-size to 0x"
pipe_perm_error_message_part_2: .ascii ".\n"
.text
pipe_perm_error:
lea ARG2, [%rip + pipe_perm_error_message_part_1]
mov ARG3e, 96
call write_stderr
mov %rax, PIPE_SIZE
mov %ecx, 28
call inefficiently_write_as_hex
lea ARG2, [%rip + pipe_perm_error_message_part_2]
mov ARG3e, 2
call write_stderr
mov ARG1e, 77
jmp exit

.section .rodata
pipe_size_error_message_part_1:
.ascii "Failed to resize the kernel pipe buffer.\n"
.ascii "Requested size: 0x"
pipe_size_error_message_part_2: .ascii "\nActual size: 0x"
pipe_size_error_message_part_3:
.ascii "\n(If the buffer is too large, this may cause errors;"
.ascii "\nthe program could run too far ahead and overwrite"
.ascii "\nmemory before it had been read from.)\n"
.text
pipe_size_mismatch_error:
push SYSCALL_RETURN
lea ARG2, [%rip + pipe_size_error_message_part_1]
mov ARG3e, 59
call write_stderr
mov %rax, PIPE_SIZE
mov %ecx, 28
call inefficiently_write_as_hex
lea ARG2, [%rip + pipe_size_error_message_part_2]
mov ARG3e, 16
call write_stderr
pop %rax
mov %ecx, 28
call inefficiently_write_as_hex
lea ARG2, [%rip + pipe_size_error_message_part_3]
mov ARG3e, 141
call write_stderr
mov ARG1e, 73
jmp exit
</code></pre>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Are people in tech inside an AI echo chamber? (305 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36567918</link>
            <guid>36567918</guid>
            <pubDate>Mon, 03 Jul 2023 02:17:29 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36567918">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="36567918">
      <td><span></span></td>      <td><center><a id="up_36567918" href="https://news.ycombinator.com/vote?id=36567918&amp;how=up&amp;goto=item%3Fid%3D36567918"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=36567918">Ask HN: Are people in tech inside an AI echo chamber?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_36567918">120 points</span> by <a href="https://news.ycombinator.com/user?id=freelanddev">freelanddev</a> <span title="2023-07-03T02:17:29"><a href="https://news.ycombinator.com/item?id=36567918">10 hours ago</a></span> <span id="unv_36567918"></span> | <a href="https://news.ycombinator.com/hide?id=36567918&amp;goto=item%3Fid%3D36567918">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Are%20people%20in%20tech%20inside%20an%20AI%20echo%20chamber%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=36567918&amp;auth=8a337da02758aaf505247c50497f3a405fe6d8b3">favorite</a> | <a href="https://news.ycombinator.com/item?id=36567918">198&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><p>I recently spoke with a friend who is not in the tech space and he hadn’t even heard of ChatGPT. He’s a millennial &amp; a white collar worker and smart. I have had conversations with non-tech people about ChatGPT/AI, but not very frequently, which led me to think, are we just in an echo chamber? Not that this would be a bad thing, as we’re all quite aware that AI will play an increasing role in our lives (in &amp; out of the office), but maybe AI mainstream adoption will take longer than we anticipate. What do you think?</p></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla on track to smash targets after producing almost a million EVs in 6 months (171 pts)]]></title>
            <link>https://thedriven.io/2023/07/03/tesla-on-track-to-smash-targets-after-producing-almost-a-million-evs-in-first-6-months/</link>
            <guid>36567725</guid>
            <pubDate>Mon, 03 Jul 2023 01:43:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thedriven.io/2023/07/03/tesla-on-track-to-smash-targets-after-producing-almost-a-million-evs-in-first-6-months/">https://thedriven.io/2023/07/03/tesla-on-track-to-smash-targets-after-producing-almost-a-million-evs-in-first-6-months/</a>, See on <a href="https://news.ycombinator.com/item?id=36567725">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

		
		<main id="main" role="main">

			
			
				
				<article id="post-170072">
				

							
			<section>
		<figure>
			<a href="https://thedriven.io/wp-content/uploads/2023/07/Tesla-Q2-results_03.jpg">
				<img width="800" height="437" src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-800x437.jpg?lossy=1&amp;strip=0&amp;webp=1" alt="Tesla Model 3" decoding="async" loading="lazy" data-srcset="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-90x49.jpg?lossy=1&amp;strip=0&amp;webp=1 90w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-120x66.jpg?lossy=1&amp;strip=0&amp;webp=1 120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-150x82.jpg?lossy=1&amp;strip=0&amp;webp=1 150w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-180x98.jpg?lossy=1&amp;strip=0&amp;webp=1 180w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-240x131.jpg?lossy=1&amp;strip=0&amp;webp=1 240w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-300x164.jpg?lossy=1&amp;strip=0&amp;webp=1 300w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-320x175.jpg?lossy=1&amp;strip=0&amp;webp=1 320w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-388x212.jpg?lossy=1&amp;strip=0&amp;webp=1 388w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03.jpg?size=464x253&amp;lossy=1&amp;strip=0&amp;webp=1 464w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-560x306.jpg?lossy=1&amp;strip=0&amp;webp=1 560w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-640x350.jpg?lossy=1&amp;strip=0&amp;webp=1 640w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03.jpg?size=696x380&amp;lossy=1&amp;strip=0&amp;webp=1 696w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-800x437.jpg?lossy=1&amp;strip=0&amp;webp=1 800w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-1120x612.jpg?lossy=1&amp;strip=0&amp;webp=1 1120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-1160x634.jpg?lossy=1&amp;strip=0&amp;webp=1 1160w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03.jpg?lossy=1&amp;strip=0&amp;webp=1 1200w" data-sizes="(max-width: 800px) 100vw, 800px" data-src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_03-800x437.jpg?lossy=1&amp;strip=0&amp;webp=1">			</a>
						<figcaption>Tesla Model 3. Source: Unsplash – Vlad Tchompalov</figcaption>
					</figure>
	</section>

					<div>

										
			
							<section>

								<p>Tesla produced 479,700 electric vehicles in the second quarter of 2023 and delivered 466,140, smashing both quarterly production and sales records.</p>
<p>For the first half of the year Tesla has produced 920,508 EVs meaning the company is well placed to achieve its target of producing 2 million in 2023.</p>
<p>As expected, the Model 3 and Model Y made up 96% of Tesla’s production, with the Model S and X flagship cars accounting for just 4%.</p>
<p>Tesla’s Q2 sales were up 10% quarter-on-quarter from Q1’s previous record of 422,875. Year-on-year Q2 sales were up a staggering 83% over Q2 2022, as shown in the graph below from <a href="https://twitter.com/piloly/status/1675535815164805121">Roland Pircher.</a></p>

<figure id="attachment_170076" aria-describedby="caption-attachment-170076"><a href="https://thedriven.io/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg"><img loading="lazy" decoding="async" src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?lossy=1&amp;strip=0&amp;webp=1" alt="Tesla global quarterly sales" width="1200" height="675" srcset="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-90x51.jpg?lossy=1&amp;strip=0&amp;webp=1 90w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-120x68.jpg?lossy=1&amp;strip=0&amp;webp=1 120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-150x84.jpg?lossy=1&amp;strip=0&amp;webp=1 150w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-180x101.jpg?lossy=1&amp;strip=0&amp;webp=1 180w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-240x135.jpg?lossy=1&amp;strip=0&amp;webp=1 240w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-300x169.jpg?lossy=1&amp;strip=0&amp;webp=1 300w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-320x180.jpg?lossy=1&amp;strip=0&amp;webp=1 320w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-388x218.jpg?lossy=1&amp;strip=0&amp;webp=1 388w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?size=464x261&amp;lossy=1&amp;strip=0&amp;webp=1 464w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-560x315.jpg?lossy=1&amp;strip=0&amp;webp=1 560w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-640x360.jpg?lossy=1&amp;strip=0&amp;webp=1 640w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?size=696x392&amp;lossy=1&amp;strip=0&amp;webp=1 696w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-800x450.jpg?lossy=1&amp;strip=0&amp;webp=1 800w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?size=928x522&amp;lossy=1&amp;strip=0&amp;webp=1 928w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-1120x630.jpg?lossy=1&amp;strip=0&amp;webp=1 1120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-1160x653.jpg?lossy=1&amp;strip=0&amp;webp=1 1160w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?lossy=1&amp;strip=0&amp;webp=1 1200w" sizes="(max-width: 1160px) 100vw, 1160px" data-old-src="https://thedriven.io/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-srcset="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-90x51.jpg?lossy=1&amp;strip=0&amp;webp=1 90w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-120x68.jpg?lossy=1&amp;strip=0&amp;webp=1 120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-150x84.jpg?lossy=1&amp;strip=0&amp;webp=1 150w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-180x101.jpg?lossy=1&amp;strip=0&amp;webp=1 180w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-240x135.jpg?lossy=1&amp;strip=0&amp;webp=1 240w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-300x169.jpg?lossy=1&amp;strip=0&amp;webp=1 300w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-320x180.jpg?lossy=1&amp;strip=0&amp;webp=1 320w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-388x218.jpg?lossy=1&amp;strip=0&amp;webp=1 388w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?size=464x261&amp;lossy=1&amp;strip=0&amp;webp=1 464w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-560x315.jpg?lossy=1&amp;strip=0&amp;webp=1 560w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-640x360.jpg?lossy=1&amp;strip=0&amp;webp=1 640w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?size=696x392&amp;lossy=1&amp;strip=0&amp;webp=1 696w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-800x450.jpg?lossy=1&amp;strip=0&amp;webp=1 800w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?size=928x522&amp;lossy=1&amp;strip=0&amp;webp=1 928w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-1120x630.jpg?lossy=1&amp;strip=0&amp;webp=1 1120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01-1160x653.jpg?lossy=1&amp;strip=0&amp;webp=1 1160w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?lossy=1&amp;strip=0&amp;webp=1 1200w" data-src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_01.jpg?lossy=1&amp;strip=0&amp;webp=1"></a><figcaption id="caption-attachment-170076">Tesla global quarterly sales. Source: <a href="https://twitter.com/piloly/status/1675535815164805121">@piloly</a></figcaption></figure>
<p>Looking at production in Q2, Tesla produced 479,700 EVs, up 13% on Q1’s figure of 440,808. If Tesla can maintain that level of quarter-on-quarter production growth for the rest of this year, it will hit around 542,000 for Q3 and 612,500 for Q4.</p>
<p>This will put total 2023 production numbers at around 2.1 million, exceeding Tesla’s stated goal of 1.8 to 2 million.</p>
<p>If Tesla does manage to produce 2.1 million EVs for 2023, it will represent an annualised growth rate of around 60%, which is also above Tesla’s long-term target of 50% annualised production growth.</p>
<p>To get a sense of Tesla’s exponential sales growth, James Stephenson graphed Q2 trailing 12-month global deliveries over the past 10 years.</p>
<p>Based the Q2 trailing 12-month figures, Tesla sales grew 47% in 2022-23, 58% in 2021-22 and 82% in 2020-21.</p>

<figure id="attachment_170078" aria-describedby="caption-attachment-170078"><a href="https://thedriven.io/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg"><img loading="lazy" decoding="async" src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?lossy=1&amp;strip=0&amp;webp=1" alt="Trailing 12-month global Tesla deliveries Q2 2013 to Q2 2023" width="1047" height="1408" srcset="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-90x120.jpg?lossy=1&amp;strip=0&amp;webp=1 90w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-120x160.jpg?lossy=1&amp;strip=0&amp;webp=1 120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-150x202.jpg?lossy=1&amp;strip=0&amp;webp=1 150w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-180x242.jpg?lossy=1&amp;strip=0&amp;webp=1 180w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-240x323.jpg?lossy=1&amp;strip=0&amp;webp=1 240w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-297x400.jpg?lossy=1&amp;strip=0&amp;webp=1 297w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-300x403.jpg?lossy=1&amp;strip=0&amp;webp=1 300w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-320x430.jpg?lossy=1&amp;strip=0&amp;webp=1 320w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?size=464x624&amp;lossy=1&amp;strip=0&amp;webp=1 464w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-560x753.jpg?lossy=1&amp;strip=0&amp;webp=1 560w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-640x861.jpg?lossy=1&amp;strip=0&amp;webp=1 640w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?size=696x936&amp;lossy=1&amp;strip=0&amp;webp=1 696w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-800x1076.jpg?lossy=1&amp;strip=0&amp;webp=1 800w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?size=928x1248&amp;lossy=1&amp;strip=0&amp;webp=1 928w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?lossy=1&amp;strip=0&amp;webp=1 1047w" sizes="(max-width: 1047px) 100vw, 1047px" data-old-src="https://thedriven.io/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-srcset="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-90x120.jpg?lossy=1&amp;strip=0&amp;webp=1 90w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-120x160.jpg?lossy=1&amp;strip=0&amp;webp=1 120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-150x202.jpg?lossy=1&amp;strip=0&amp;webp=1 150w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-180x242.jpg?lossy=1&amp;strip=0&amp;webp=1 180w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-240x323.jpg?lossy=1&amp;strip=0&amp;webp=1 240w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-297x400.jpg?lossy=1&amp;strip=0&amp;webp=1 297w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-300x403.jpg?lossy=1&amp;strip=0&amp;webp=1 300w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-320x430.jpg?lossy=1&amp;strip=0&amp;webp=1 320w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?size=464x624&amp;lossy=1&amp;strip=0&amp;webp=1 464w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-560x753.jpg?lossy=1&amp;strip=0&amp;webp=1 560w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-640x861.jpg?lossy=1&amp;strip=0&amp;webp=1 640w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?size=696x936&amp;lossy=1&amp;strip=0&amp;webp=1 696w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02-800x1076.jpg?lossy=1&amp;strip=0&amp;webp=1 800w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?size=928x1248&amp;lossy=1&amp;strip=0&amp;webp=1 928w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?lossy=1&amp;strip=0&amp;webp=1 1047w" data-src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/07/Tesla-Q2-results_02.jpg?lossy=1&amp;strip=0&amp;webp=1"></a><figcaption id="caption-attachment-170078">Trailing 12-month global Tesla deliveries Q2 2013 to Q2 2023. Source: <a href="https://twitter.com/ICannot_Enough/status/1675543214638465026">@ICannot_Enough</a></figcaption></figure>

<div itemtype="http://schema.org/Person" itemscope="" itemprop="author"><p><img loading="lazy" src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442.jpeg?lossy=1&amp;strip=0&amp;webp=1" width="100" height="100" alt="Daniel Bleakley Profile Picture" itemprop="image" srcset="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-80x80.jpeg?lossy=1&amp;strip=0&amp;webp=1 80w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-90x90.jpeg?lossy=1&amp;strip=0&amp;webp=1 90w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-120x120.jpeg?lossy=1&amp;strip=0&amp;webp=1 120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-160x160.jpeg?lossy=1&amp;strip=0&amp;webp=1 160w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-180x180.jpeg?lossy=1&amp;strip=0&amp;webp=1 180w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-240x240.jpeg?lossy=1&amp;strip=0&amp;webp=1 240w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-320x320.jpeg?lossy=1&amp;strip=0&amp;webp=1 320w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-388x388.jpeg?lossy=1&amp;strip=0&amp;webp=1 388w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442.jpeg?size=464x464&amp;lossy=1&amp;strip=0&amp;webp=1 464w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-560x560.jpeg?lossy=1&amp;strip=0&amp;webp=1 560w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442-640x640.jpeg?lossy=1&amp;strip=0&amp;webp=1 640w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442.jpeg?size=696x696&amp;lossy=1&amp;strip=0&amp;webp=1 696w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442.jpeg?lossy=1&amp;strip=0&amp;webp=1 800w" sizes="(max-width: 800px) 100vw, 800px" data-old-src="https://thedriven.io/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2023/01/1667258499442.jpeg?size=100x100&amp;lossy=1&amp;strip=0&amp;webp=1"></p><div><p>Daniel Bleakley is a clean technology researcher and advocate with a background in engineering and business. He has a strong interest in electric vehicles, renewable energy, manufacturing and public policy.</p></div></div><!-- AI CONTENT END 1 -->

								
							</section>

										
			
						</div><!-- .entry-wrap -->

					


				</article>

				
				
			
		</main>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lumia WOA Project – Windows 10 or Windows 11 Desktop OS for Lumia 950/XL (168 pts)]]></title>
            <link>https://woa-project.github.io/LumiaWOA/</link>
            <guid>36566999</guid>
            <pubDate>Sun, 02 Jul 2023 23:42:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://woa-project.github.io/LumiaWOA/">https://woa-project.github.io/LumiaWOA/</a>, See on <a href="https://news.ycombinator.com/item?id=36566999">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-role="main">
        <div id="hero">
            <p>
                
                <h4>Full Windows for Lumia</h4>
            </p>
        </div>
        <div>
                <h3>Caution</h3>
                <h6>Experimental firmware ahead</h6>
                <p>The firmware provided is for testing purposes only. We aren't responsible for any data loss caused by
                    the firmware images. Make backups of your data prior to installing.</p>
                <p><strong>This software has not been approved for use with emergency services. By installing this
                        software, you agree to not use it as your primary phone device due to possible disruption in
                        emergency service access.</strong></p>
            </div>
        <div>
            <r-grid columns="2">
                <r-cell span-s="row">
                    
                    <h2>The Windows You Know and Love</h2>
                    <p>This project brings the Windows 10 or Windows 11 desktop operating system to your Lumia 950 and Lumia 950 XL.
                    </p>
                    <p>It's the same edition of Windows you're used to on your traditional laptop or desktop
                        computer, but it's the version for ARM64 (armv8a) processors.</p>
                    <p>It can run ARM64, ARM, x86 and x64 applications (the last two via emulation) just fine.<sup>1</sup></p>
                </r-cell>
                <r-cell span-s="row">
                    <figure>
                        <div>
                            <picture>
                                <source srcset="https://woa-project.github.io/LumiaWOA/img/Desktop.webp" type="image/webp">
                                <img loading="lazy" src="https://woa-project.github.io/LumiaWOA/img/Desktop.png" alt="A Lumia 950">
                            </picture>
                            
                        </div>
                    </figure>
                </r-cell>
            </r-grid>
        </div>
        <div>
            <figure>
                <div>
                    <picture>
                        <source srcset="https://woa-project.github.io/LumiaWOA/img/phone_continuum.webp" type="image/webp">
                        <img loading="lazy" src="https://woa-project.github.io/LumiaWOA/img/phone_continuum.png" alt="A Lumia 950">
                    </picture>
                    
                </div>
            </figure>
            <div>
                <h2></h2>
                <h2>With Continuum<sup>2</sup></h2>
                <figure>
                    <p><img src="https://woa-project.github.io/LumiaWOA/img/desktop_continuum.png">
                        
                    </p>
                </figure>
            </div>
        </div>
        <r-grid columns="2">
            <r-cell span="1" span-s="row">
                <h2></h2>
                <h2>A Mobile Twist</h2>
                <div>
                    <p>MobileShell is a fully-featured adaptive shell aiming to mimic the appearance of Windows
                        Mobile.</p>
                    <p>MobileShell brings back the navigation bar, status bar, puts your notification toasts at the top,
                        status icons at a glance, and activates only when your phone is in tablet mode.</p>
                        
                </div>
            </r-cell>
            <r-cell span="1" span-s="row">
                <figure>
                    <div>
                        <picture>
                            <source srcset="https://woa-project.github.io/LumiaWOA/img/MobilePortrait.webp" type="image/webp">
                            <img loading="lazy" src="https://woa-project.github.io/LumiaWOA/img/MobilePortrait.png" alt="A Lumia 950">
                        </picture>
                        
                    </div>
                </figure>
            </r-cell>
            <r-cell span="1" span-s="row">
                <figure>
                    <p><img src="https://woa-project.github.io/LumiaWOA/img/MobileLandscape.png">
                        
                    </p>
                </figure>
            </r-cell>
            <r-cell span="1" span-s="row">
                <p>MobileShell also supports landscape mode, adjusting perfectly to the phone's current state. Mobile
                    Shell is made by @ADeltaX and is included by default! You can also download it from the Microsoft
                    Store:</p>
                
            </r-cell>
        </r-grid>
        <div>
            <r-grid columns="2">
                <r-cell span-s="row">
                    <h2></h2>
                    <h2>Say Hello...<sup>3</sup></h2>
                    <div>
                        <p>This project backports the cellular stack from Windows 10 Mobile to Windows desktop. On
                            supported versions of Windows, you can make calls, texts, and browse the internet using a
                            cellular connection.</p>
                        <p>Dialer (previously WOA Dialer) is our custom app that allows you to make and manage calls on your device. 
                            Dialer is bundled with the project by default, along with the classic Microsoft Phone app.</p>
                    </div>
                    
                </r-cell>
                <r-cell span-s="row">
                    <figure>
                        <div>
                            <picture>
                                <source srcset="https://woa-project.github.io/LumiaWOA/img/Dialer.webp" type="image/webp">
                                <img loading="lazy" src="https://woa-project.github.io/LumiaWOA/img/Dialer.png" alt="A Lumia 950">
                            </picture>
                            
                        </div>
                    </figure>
                </r-cell>
            </r-grid>
        </div>
        <div>
            <r-grid columns="2">
                <r-cell span-s="row">
                    <figure>
                        <div>
                            <picture>
                                <source srcset="https://woa-project.github.io/LumiaWOA/img/Chat.webp" type="image/webp">
                                <img loading="lazy" src="https://woa-project.github.io/LumiaWOA/img/Chat.png" alt="A Lumia 950">
                            </picture>
                            
                        </div>
                    </figure>
                </r-cell>
                <r-cell span-s="row">
                    <h2></h2>
                    <h2>...Or Send a Message<sup>3</sup></h2>
                    <p>With the Chat application, you can recieve and send SMS messages. MMS messages remain unsupported
                        as of now.</p>
                    
                </r-cell>
            </r-grid>
        </div>
        <r-grid columns="2">
            <r-cell span="1-2" span-s="row">
                <h2>And Much More!</h2>
            </r-cell>
            <r-cell span-s="row">
                <h4>WOA Deployer</h4>
                <p>WOA Deployer allows you to deploy with ease Windows Desktop to your device, and enabling Dual Boot
                    with 2 clicks. You can pick the windows release you want, the language you want.</p>
                <a href="https://github.com/WOA-Project/WOA-Deployer-Lumia">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>BootShim</h4>
                <p>BootShim is the UEFI bootstraper. It escalates the SoC to AArch64 and starts our UEFI.</p>
                <a href="https://github.com/imbushuo/boot-shim">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Lumia950XlPkg</h4>
                <p>Lumia950XlPkg is our EDK2 port for the Lumia 950 and Lumia 950 XL. It enables us to bootstrap Windows
                    10/11 Desktop for ARM64 processors on the Lumia.</p>
                <a href="https://github.com/WOA-Project/Lumia950XLPkg">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Lumia Drivers</h4>
                <p>Lumia Drivers is the repository hosting all driver files for Windows, and INF files which had to be
                    recreated. Some additional driver patching is also done here to make things work the way they
                    should.</p>
                <a href="https://github.com/WOA-Project/Lumia-Drivers">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Lumia USB-C</h4>
                <p>Lumia USB-C is the recreation of the USB C driver for Lumia devices. The Lumia 950 USB-C solution is
                    proprietary and personalized, thus the need for a custom driver.</p>
                <a href="https://github.com/WOA-Project/LumiaUSBC">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Color Profile</h4>
                <p>Color Profile is the stack managing the personalization of the display color tint, saturation and
                    contrast.</p>
                <a href="https://github.com/WOA-Project/ColorProfile">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Advanced Info</h4>
                <p>Advanced Info displays information about your device, within the settings app.</p>
                <a href="https://github.com/WOA-Project/AdvancedInfo">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Airwaves</h4>
                <p>Airwaves allows you to listen to FM radio, right from your phone.</p>
                <a href="https://github.com/tigerw/FM-Radio-Frontend">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>RIL Init Service</h4>
                <p>RIL Init Service allows you to have the Radio Interface Layer initialized on newer versions of
                    Windows 10/11.</p>
                <a href="https://github.com/WOA-Project/RILServiceInit">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Auto Brightness Service</h4>
                <p>The auto brightness service allows you to have automatic brightness on your device.</p>
                <a href="https://github.com/WOA-Project/AutoBrightnessSvc">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Auto Rotation Service</h4>
                <p>The auto rotation service allows you to have automatic rotation on your device.</p>
                <a href="https://github.com/WOA-Project/AutoRotate">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Vibrations</h4>
                <p>The vibration stack allows you to have haptic vibrations once you get a notification, and control the
                    intensity of the vibration via a settings application.</p>
                <a href="https://github.com/WOA-Project/VibrationSettings">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>USB Function Mode Switcher</h4>
                <p>USB Function Mode Switcher allows you to switch USB function modes.</p>
                <a href="https://github.com/WOA-Project/USBFunctionModeSwitcher">View on
                    GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Data Management Service</h4>
                <p>The data management service enables cellular data connections automatically.</p>
                <a href="https://github.com/WOA-Project/DataManagementSvc">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Power Supply Notifier</h4>
                <p>Power Supply Notifier plays a sound when your device starts charging.</p>
                <a href="https://github.com/WOA-Project/PowerSupplyNotifier">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>SynapticsTouch</h4>
                <p>The Synaptics Touch driver enables touch on your device</p>
                <a href="https://github.com/imbushuo/SynapticsTouch">View on GitHub</a>
            </r-cell>
            <r-cell span-s="row">
                <h4>Display Dock Flyout</h4>
                <p>The Display Dock Flyout displays information about a connected Display Dock (HD-500)</p>
                <a href="https://github.com/WOA-Project/DisplayDockFlyout">View on GitHub</a>
            </r-cell>
        </r-grid>
        <div>
            <p><sup>1</sup> Applications compiled for the AMD64/x86-64 architecture are 
                supported only on build 21277+.</p>
            <p><sup>2</sup> Continuum currently only works wirelessly over Miracast.</p>
            <p><sup>3</sup> Cellular support is still unfinished and might be broken in some
                areas. Cellular calls are automatically enabled in up to Windows 10 November 2019 Update (version 19H2, build
                18363). Versions higher than this will <strong>only</strong> support cellular data. You can manually enable 
                calls on builds higher than 18363 by using <a href="https://woa-project.github.io/LumiaWOA/guides/ican0/">this guide</a>. SMS are supported up to Windows 10 November 2019 Update 
                (version 19H2, build 18363). Dual SIM devices may have issues fetching the default Carrier APN settings, a provisioning 
                package for APN may be required. The advanced settings page for Cellular in the Windows Settings app may crash. 
                Your experience will vary between carriers and devices. This software stack has not been approved for use with emergency services.
                As a consequence <strong>it should not be used as your primary way of communication</strong>. VoLTE (IMS) stack while
                present is not functional.</p>
        </div>
    </div><div>
            <p>© 2017-2021 The Lumia WOA Authors</p>
            <p>Snapdragon is a registered trademark of Qualcomm Incorporated. Microsoft, the Microsoft Corporate Logo,
                Windows, Lumia, Windows Hello, Continuum, Hyper-V, and DirectX are registered trademarks of Microsoft
                Corporation in the United States. Miracast is a registered trademark of the Wi-Fi Alliance. Other
                binaries may be copyright Qualcomm Incorporated and Microsoft Mobile.</p>
            <p>Hello from San Francisco (US), France, Italy, Germany, Spain, Hungary. Site built by @itsmichaelwest.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Xerox Smalltalk-80 GUI Was Weird (123 pts)]]></title>
            <link>https://collindonnell.com/the-xerox-smalltalk-80-gui-was-weird</link>
            <guid>36566638</guid>
            <pubDate>Sun, 02 Jul 2023 22:51:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://collindonnell.com/the-xerox-smalltalk-80-gui-was-weird">https://collindonnell.com/the-xerox-smalltalk-80-gui-was-weird</a>, See on <a href="https://news.ycombinator.com/item?id=36566638">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>As part of my ongoing interest in the origins of object-oriented programming and design patterns like MVC, I started to think the best way to fully grasp these things was to go back to the beginning. While there are modern Smalltalk’s like Squeak, Pharo, and Cuis that retain a lot of the old-school vibe, they’re also pretty different.</p>

<p>I could go into detail on how all of those options aren’t quite what I’m looking for, but since I want to write about that later, the short version for now is that I started looking into how you could emulate the original Smalltalk-80 environment on modern hardware. That ended up being incredibly easy when I found <a href="https://github.com/dbanay/Smalltalk">this</a> “by the Bluebook” implementation of it on GitHub.</p>

<p>After a couple minutes installing, I got it running and saw this:</p>

<p><img src="https://i.snap.as/ZzWRwDlc.png" alt="Smalltalk-80 first run"></p>

<p>At first glance, this looks <em>incredibly</em> similar to something like the desktop of the <a href="https://en.wikipedia.org/wiki/Apple_Lisa">Apple Lisa</a> or <a href="https://en.wikipedia.org/wiki/Classic_Mac_OS">early Mac OS</a>. It’s easy to see why people might think that Apple sort of <a href="https://www.youtube.com/watch?v=CBri-xgYvHQ">stole the graphical user interface</a> from it’s rich neighbor Xerox. It’s not true, though.</p>

<p>The first thing is that the Smalltalk environment wasn't really an operating system the same way something like Mac OS was. It’s more like an IDE that runs on bare hardware.</p>

<p>For example, there’s no traditional file system. Smalltalk environments were stored as images which contained the entire state of the system. That means all of the objects, code, data, whatever, were stored as Smalltalk objects. The creators of Smalltalk wanted to make a system where the person using it was also modifying and programming the system as they used it. That’s really cool as a programming environment, but not really how we think of normal people using computers today.</p>

<p>There’s also no desktop, icons, or pull down menus. The whole desktop metaphor isn’t really there, because that isn’t really what it was. It’s sort of pure in an appealing way. There aren’t many different categories of things there. Windows, pop-up menus, lists. Not a lot. You can collapse different windows you’re not using and organize them on your screen, but they’re all just still windows, as opposed to also having icons for files and folders and apps and other sorts of things.</p>

<p><img src="https://i.snap.as/tJ0LRLFc.png" alt="Collapsed windows"></p>

<p>If you want to do anything with a window, you might notice there’s no controls. All window operations are done modally, meaning you click, select what you want to do, and then do that thing. You can’t even move or resize a window by clicking and dragging. There is click and drag for scrollbars and text, but not for windows. Weird.</p>

<p><img src="https://i.snap.as/Z5lx819X.png" alt="To resize a window you use the “frame” button"></p>

<p>If you wanted to move this window you would select “move”, which collapses the window and ties it to your mouse location. You then move your mouse to wherever you want the window to be and click again to place it. Oh, and you can only click, because there are no key commands in this system whatsoever.</p>

<p>Looking at all of this together, yeah, the Lisa and Macintosh took a lot from Xerox. But, they also added a lot. The Smalltalk environment was a revolutionary GUI, but it was still a system you would have had to have been a <em>computer operator</em> or something to really use. It’s not a personal computer at all.</p>

<p>The fact Apple was able to see the potential and then figure out all of the metaphors and affordances which needed to be there for regular humans to use a computer like this, and that they got so many of those things right by the time of the original Macintosh is pretty incredible.</p>

<p><a href="https://collindonnell.com/tag:programming"><span>#</span><span>programming</span></a> <a href="https://collindonnell.com/tag:apple"><span>#</span><span>apple</span></a> <a href="https://collindonnell.com/tag:smalltalk"><span>#</span><span>smalltalk</span></a> <a href="https://collindonnell.com/tag:ui"><span>#</span><span>ui</span></a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Illegal Life Pro Tip: Want to ruin your competitors business? (400 pts)]]></title>
            <link>https://oppositeinvictus.com/illegal-life-pro-tip-want-to-ruin-your-competitors-business</link>
            <guid>36566634</guid>
            <pubDate>Sun, 02 Jul 2023 22:51:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://oppositeinvictus.com/illegal-life-pro-tip-want-to-ruin-your-competitors-business">https://oppositeinvictus.com/illegal-life-pro-tip-want-to-ruin-your-competitors-business</a>, See on <a href="https://news.ycombinator.com/item?id=36566634">Hacker News</a></p>
Couldn't get https://oppositeinvictus.com/illegal-life-pro-tip-want-to-ruin-your-competitors-business: Error: incorrect header check]]></description>
        </item>
        <item>
            <title><![CDATA[Ericsson to WhatsApp: The Story of Erlang (247 pts)]]></title>
            <link>https://thechipletter.substack.com/p/ericsson-to-whatsapp-the-story-of</link>
            <guid>36566167</guid>
            <pubDate>Sun, 02 Jul 2023 21:51:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thechipletter.substack.com/p/ericsson-to-whatsapp-the-story-of">https://thechipletter.substack.com/p/ericsson-to-whatsapp-the-story-of</a>, See on <a href="https://news.ycombinator.com/item?id=36566167">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg" width="1456" height="976" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:976,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:135418,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84d7445f-4d93-4120-b226-de74c97b6d63_2292x1536.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>In this post, we’re going to look at a piece of technology that was banned by the company that created it. It was kept alive by a small team of enthusiasts. Then, almost thirty years after its first development, it became the core technology underpinning one of the most important and lucrative startups of the 2010s.</p><p>Today, it plays a key role in services that are used by billions across the world.</p><p>We’re talking about the Erlang programming language.</p><p>Why is Erlang interesting? Well, one of the themes of this newsletter is that old technology can be incredibly valuable and useful. Erlang was often seen as an esoteric curiosity, until a small group of entrepreneurial engineers used it to create a start-up that they sold for billions of dollars.</p><p>Erlang’s history also has a number of lessons. The spread of general purpose hardware and software into specialist areas like telecoms. The power of individuals and small teams using the right software tools. The resilience of open source software. And more.</p><p>This is our first post on a programming language. Except it isn’t entirely about software, it’s about how software can make the most of the hardware resources available and the links between the hardware of computing and that of telephony.</p><p>So, what is Erlang, how did it come into being, and why has it had a renaissance recently?</p><p>We’re going to start at the place where Erlang was born. In Stockholm in Sweden.</p><p>If you’re old enough, you may remember Ericsson mobile phone handsets which were a popular choice in the pre-smartphone era. Today, Ericsson’s business is focused on telecommunications equipment.</p><p>Ericsson was founded by Lars Magnus Ericsson in Stockholm, Sweden in 1876. In 1878, it started making and selling telephones and switchboards.  Over the following decades, Ericsson had mixed fortunes and, after a period of financial difficulties, it was rescued by banks controlled by the Wallenberg family. By the 1960s, the Wallenbergs had full control of Ericsson.</p><p><span>We’ve previously met Ericsson in recounting the </span><a href="https://thechipletter.substack.com/p/the-first-risc-john-cocke-and-the" rel="">story</a><span> of the first RISC computer, the IBM 801. IBM wanted to to enter the telecoms equipment market and considered joining forces with Ericsson, with the development of a new computer at the center of a possible joint venture. After a series of secretive meetings at London’s Claridges Hotel, that came to nothing.</span></p><p>But telephony in general, and Ericsson in particular, needed computing.</p><p>In 1980, a small group of Ericsson engineers - Bjarne Däcker, Mike Williams, Göran Båge and Seved Torsten-dahl - proposed creating a Computer Science Laboratory (CSL) within the company. Their objectives were to create software technology for future telecoms systems and to help introduce that technology into existing Ericsson systems.</p><p><span>Their hardware mixed industry standard computers with specialised telecoms hardware, with a </span><a href="https://en.wikipedia.org/wiki/VAX-11" rel="">DEC VAX 11/750</a><span> minicomputer linked to an Ericsson telephone exchange.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png" width="1016" height="1312" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1312,&quot;width&quot;:1016,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:363474,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fc09f58-e02f-4ec2-8730-9a3fb48b4c16_1016x1312.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>The original setup used to develop Erlang with a VAX 11/750 minicomputer running Unix connected to an Ericsson telephone exchange</figcaption></figure></div><p>Earlier work to program telecom systems at Ericsson had used a range of languages including PL163, CHILL and modified versions of the Pascal programming language. </p><p>Now the team started to look at a range of existing programming languages, each of which brought its own approach. These languages included then mainstream languages like Ada and lesser known ones such as Concurrent Euclid, PFL, LPL, OPS4, Frames and CLU.</p><p>As the team's work progressed, it became clear that none of these individual languages by itself would meet all the team’s requirements.</p><p>In 1985, the group was joined by Joe Armstrong and Robert Virding.</p><p>Then in 1988, the Lab moved from Ericsson to Ellemtel, a joint venture between Ericsson and the Swedish national telecoms provider Televerket.</p><p>The focus now was on building software to program AXE, Ericsson’s telephone exchange, which was programmed in a language called PLEX. The motivation was to “to make something like PLEX, to run on ordinary hardware, only better.”</p><p>Carrying forward some properties of PLEX was considered essential. In particular, according to Armstrong:</p><blockquote><p>Firstly, it should be possible to change code “on the fly;” in other words, code change operations should be possible without stopping the system.</p></blockquote><p>Software systems need to be updated and maintained with new code replacing old. A telephone system can’t be taken down completely whilst a set of code updates are introduced. Customers would not be happy if they can’t make their calls!</p><p>The language also needed to support many simultaneous activities, as calls were routed between users, and it had to be able to do this efficiently. Again, according to Armstrong, the language had to take replicate this property of telecoms switching systems:</p><blockquote><p>A switching system is made from a number of individual switches. Individual switches typically handle tens to hundreds of thousands of simultaneous calls. The switching system must be capable of handling millions of calls and must tolerate the failure of individual switches, providing uninterrupted services to the user. </p></blockquote><p>To this was added the conclusions of the earlier work to test a variety of languages. In particular:</p><ul><li><p>Small languages were thought desirable.</p></li><li><p>Functional programming was liked.</p></li><li><p>Logic programming was best in terms of elegance.</p></li><li><p>Concurrency was viewed as essential for this type of problem.</p></li></ul><p>Armstrong experimented with Smalltalk, but his attention was drawn to Prolog. Experimenting with building a system in Prolog soon led to something distinct:</p><blockquote><p>What started as an experiment in “adding concurrency to Prolog” became more of a language in its own right and this language acquired a name “Erlang,” which was probably coined by Bjarne Dacker. What did the name Erlang mean? Some said it meant “Ericsson Language,” while others claimed it was named after Agner Krarup Erlang (1878 – 1929), while we deliberately encouraged this ambiguity.</p></blockquote><p>Naming Erlang after Agner Krarup Erlang the Danish Mathematician who worked in the fields of traffic engineering and queuing theory, was surely appropriate for a language used to route telephone calls!</p><p>Robert Virding was now working with Armstrong on Erlang and by the end of 1988 most of the key ideas in Erlang had been implemented. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png" width="664" height="592" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:592,&quot;width&quot;:664,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:37564,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F508ca2bc-a5fa-44d8-9e4a-d76ddec36f92_664x592.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>So what made Erlang different, and how did it support the Ericsson team’s requirements?</p><p>At the centre of Erlang was the idea of thousands and thousands of ‘processes’. These are separate pieces of code that run largely independently of each other. Each process communicates with other processes using ‘messages’, small amounts of data that are sent and received by each process.</p><p>Each process has a ‘mailbox’ which it can check from time to time and take action depending on the contents.</p><p>Furthermore, each process is extremely ‘lightweight’ in that it uses a very small proportion of the resources of the computer. A modern server can support millions of Erlang processes running at the same time.</p><p>The team built resilience into the language, allowing processes to fail, either due to bugs or to hardware failures, without bringing down the whole system – which of course would be a disaster for a telephony switching system.</p><p>They also built in the ability to scale. Processes can run on multiple physical machines, sending messages to other processes either running on the same machine or on other machines. So, an Erlang program can easily grow by just adding more physical machines.</p><p>Word of the development of Erlang spread, and Ericsson engineers wanted to try it out:</p><blockquote><p>The team wanted to prototype a new software architecture called ACS3 designed for programming telephony services on the Ericsson MD110 PABX4 and were looking for a suitable language for the project, which is how they got to hear about Erlang. A project called ACS / Dunder was started to build the prototype.</p></blockquote><p>When the work on ACS / Dunder was evaluated, there was a striking conclusion: development in Erlang was a lot faster than the alternatives.</p><p>Erlang was revealed to the world at a conference in 1989. Work continued inside Ericsson on to develop and improve Erlang over the following years. Then in 1992 there was a seemingly small, but in fact immensely significant, step forward for Erlang. A book was published on the language.</p><p>PLEX had been proprietary to Ericsson, as it was presumed that it gave the company a commercial advantage. For Erlang, it was decided to follow the example of AT&amp;T and the C language.</p><blockquote><p>The decision to publish an Erlang book and to be fairly open about what we did was therefore to avoid isolation and follow the AT&amp;T/C path rather than the Ericsson/PLEX path. Also in 1992 we ported Erlang to MS-DOS windows, the Mac, QNX and VxWorks.</p></blockquote><p>So the Erlang team started to promote the language. Including on video. We can see some of the original designers of Erlang, showcasing the system, in ‘Erlang: The Movie’. In it Bjarne Decker, Joe Armstrong, Mike Williams and Robert Virding show off the capabilities of their new language.</p><p><strong> ‘Declarative Real Time Programming Now!’, ‘Try It, You’ll like it!’</strong></p><div id="youtube2-xrIjfIjssLE" data-attrs="{&quot;videoId&quot;:&quot;xrIjfIjssLE&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/xrIjfIjssLE?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p>In December 1995, a large project at Ellemtel, called AXE-N, collapsed. According to Armstrong:</p><blockquote><p>AXE-N was a project aimed at developing a new generation of switching products ultimately to replace the AXE10 system. The AXE-N project had developed a new hardware platform and system software that was developed in C++.</p></blockquote><p>It was decided to replace the C++ software with code written in Erlang.</p><p>Again, according to Armstrong:</p><blockquote><p>This new project was to be the largest-ever Erlang project so far, with over 60 Erlang programmers. At the start of the AXD project, the entire Erlang system was the responsibility of half a dozen people in the Lab. This number was viewed as inadequate to support the needs of a large development project and so plans were immediately enacted to build a product unit, called OTP, to officially support the Erlang system.</p></blockquote><p>OTP or the  ‘Open Telecoms Platform’ provided capabilities such as ‘hot code reloading’, databases (he ‘Mnesia’ package) and many other facilities. Despite the use of the word ‘telecom’ in the name OTP is really a general purpose set of programs and doesn’t really have anything that is peculiar to telecoms systems.</p><p>So with increasing adoption Erlang’s future within Ericsson looked bright.</p><p>Then suddenly, in 1998, Erlang was banned within the Ericsson. The reason? To avoid the costs of development of a language that was unique to Ericsson, and instead to access the shared effort being put into the development of other languages.</p><blockquote><p>The selection of an implementation language implies a more long-term commitment than the selection of a processor and OS, due to the longer life cycle of implemented products. Use of a proprietary language implies a continued effort to maintain and further develop the support and the development environment. It further implies that we cannot easily benefit from, and find synergy with, the evolution following the large-scale deployment of globally used languages.</p></blockquote><p>In other words, Erlang was almost killed by the success of open-source languages.</p><p>But all was not over for Erlang.</p><p>The language continued to be maintained within Ericsson, as it was needed to support existing products.</p><p>And the efforts of the Erlang team outside Erlang would bear fruit. According to Armstrong:</p><blockquote><p>For some time, we had been distributing Erlang to interested parties outside Ericsson, although in the form of a free evaluation system subject to a non-disclosure agreement. By 1998, about 40 evaluation systems had been distributed to external users and by now the idea of releasing Erlang subject to an open-source license was formed. Recall that at the start of the Erlang era, in 1986, “open source” was unheard of, so in 1986 everything we did was secret.</p></blockquote><p>Following the ban, the Erlang team lobbied for Erlang, no longer seen as a sensitive commercial secret for Ericsson, to be released as open source. On 2 December 1998, “Open-Source Erlang” was announced.</p><p>The original members of the Erlang team soon left Ericsson to form their own company, Bluetail, to work on Erlang. It was natural for the team to turn to internet services, as this was just at the end of the era of the internet bubble.</p><blockquote><p>Given that the Bluetail system was programmed by most of the people who had designed and implemented the Erlang and OTP systems, the project was rapidly completed and had sold its first system within six months of the formation of the company. This was one of the first products built using the OTP technology for a non-telecoms application.</p></blockquote><p>Erlang had broken away from its telecoms roots and made its way onto the internet.</p><p>Other uses for the language started to appear. It found a niche in finance, helping firms that rely on ultra-fast trading to ensure that their systems were robust. Illustrious Wall Street firm Goldman Sachs used Erlang to implement their messaging system, with its robustness and reliability again key.</p><p><span>Open-source Erlang-based projects started to appear.  One of these was </span><a href="https://en.wikipedia.org/wiki/Ejabberd" rel="">ejabberd</a><span>, originally developed by Alexey Shchepin starting in 2003, which provided high-quality software to implement a system that allowed chat messages to be sent between a large number of users.</span></p><p>Then in 2009, Jan Koum came up with an idea. What about an internet-based messaging system for the iPhone?</p><p>The iPhone was launched in 2007 and the App Store in July 2008. The first version of Android was released in September 2008.</p><p>iPhone and Android users could send text messages to each other, but doing so was costly, with charges on a per-text basis. By contrast, the amount of data used for each message was tiny, even in the context of the small data transfer allowances granted to users at the time.</p><p><span>Koum was joined by Brian Acton, who he’d met whilst working at Yahoo!</span></p><p>Koum and Acton set out to build a system where users could send each other messages using their data allowances. This would dramatically undercut the pricing of the telecoms operators. With dedicated apps on each of the new mobile platforms, they could also build in more features to make using the application more useful and fun.</p><p>The underlying software base to power the ‘back end’ of this system was already available in the form of ejabberd. Koum and Acton took the open-source messaging system and adapted it to provide the underpinnings of the product that they envisaged.</p><p><span>So, </span><a href="https://en.wikipedia.org/wiki/WhatsApp" rel="">WhatsApp</a><span> was born in January 2009.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png" width="1456" height="1292" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1292,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:467063,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F904669ad-127a-45f9-9eeb-e2f8d14f1b0c_1508x1338.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>WhatsApp Logo</figcaption></figure></div><p>WhatsApp’s growth was spectacular. From 10 million users in 2010 it grew to 100 million at the end of 2012. Thanks to the efficiency of Erlang and their careful refinement of both ejabberd, the FreeBSD operating system and Erlang itself, they were able to scale quickly whilst keeping growth of both their hardware and engineering resources down.</p><p>Their ability to scale at low cost also underpinned their business strategy. They avoided showing ads to users, instead charging each user $1 each year after the first year. $1 per user may not sound much, but with their growing user base, it would soon represent significant income. It was also affordable for users even in lower income countries, and users liked the ad-free WhatsApp user experience.</p><p>By 2013, Facebook’s Mark Zuckerberg started to see WhatsApp as a threat to his own business. Facebook had its own messaging application, Facebook Messenger, but WhatsApp was winning the messaging battle.</p><p>Critically, Zuckerberg would have seen how fast WhatsApp was scaling and presumed that it would be able to continue to scale. All thanks to Erlang.</p><p>So Facebook bought WhatsApp in 2014 for $16 billion, making Koum and Acton billionaires overnight. In the previous year, WhatsApp had revenues of only $10.2 million had incurred losses of $138 million (still small for a company with 400 million users at the end of the year – and with much of this stock-based compensation rather than underlying costs). Immediately after the acquisition, Facebook dropped what had been WhatsApp’s major revenue source, by removing the $1 per year fee. At the time of the acquisition Erlang employed only 35 engineers.</p><p>Under Facebook’s ownership, WhatsApp has continued to grow, with more than 2 billion users worldwide, all supported by Erlang.</p><p>WhatsApp wasn’t the only messaging system to use Erlang. Like WhatsApp, Facebook Messenger had used Erlang and ejabberd but moved away from it after finding it difficult to recruit engineers. WeChat in China also used Erlang for a messaging system operating with billions of users.</p><p>But messaging wasn't the only area where Erlang could be built on.</p><p>Jose Valim had been a core member of the group developing the Open-Source Ruby on Rails (commonly known as ‘Rails’) web framework. Rails was started by David Heinemeier Hansson in 2003 a part of the Basecamp project management web application.</p><p>Rails provides much of the software infrastructure  that is needed to build a modern web application: the ability to create web pages, access a database with user data and so on. Rails is built using the Ruby programming language.</p><p>Valim saw the advantages of Erlang and how useful they would be to someone developing a modern web application. In particular, its scalability made it suited to applications where users need to communicate frequently with web servers.</p><p>He also saw that Erlang had some major limitations in this context, including an unfamiliar syntax. So het set out to create a language that dealt with these issues whilst retaining the core advantages of Erlang and OTP.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png" width="1000" height="418" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:418,&quot;width&quot;:1000,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:62250,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F080d62f2-69e6-4e78-a028-c9a5a88bfffe_1000x418.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>So </span><a href="https://en.wikipedia.org/wiki/Elixir_(programming_language)" rel="">Elixir</a><span> was born in 2013. Elixir provides the capabilities of Erlang (in fact, the language ‘cross compiles’ to Erlang) in a syntax that is largely familiar to Ruby programmers.</span></p><p>Just as Rails on Rails built on Ruby, Elixir soon formed the base for a new ‘web framework’, developed by Chris McCord, Jose Valim and others, Phoenix, provides open-source software that is broadly equivalent to Ruby on Rails in its capabilities whilst being less resource intensive and making it more straightforward to build web applications, such as chat, that need frequent connection to a web server.</p><p>Innovation on top of Elixir and Phoenix has continued to this day. Two notable examples are LiveView, that removes the need for the user to write JavaScript running in the browser, and Nx which adds the base capabilities required for machine learning applications.</p><p>Today, many companies are using Elixir as the basis for their web applications, including large companies such as Twitch and a number of smaller start-ups. Elixir and Phoenix provide everything that is needed for even a small team to build an application that can be rapidly scaled.</p><p>Erlang continues to be developed and supported by individuals and teams both inside and outside Ericsson. In 2022, it received a notable update that speeded up the execution of Erlang programs significantly through the introduction of an updated ‘just in time’ compiler.</p><p>Elixir and Phoenix have both reached maturity. They have both attracted a small but dedicated and enthusiastic group of supporters. The ecosystem around both continues to expand, although it still can’t match that around a framework such as Ruby on Rails.</p><p>But Elixir and Phoenix haven’t taken over the world and are still relatively niche. We’ll discuss why this is and some of the lessons that can be learned from the history of Erlang, WhatsApp, and Elixir in the paid supplement to this post coming on Tuesday. This post also contains loads of links to sources and supplementary material, including the briefest introductions to programming in Erlang and Elixir.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI and the Automation of Work (196 pts)]]></title>
            <link>https://www.ben-evans.com/benedictevans/2023/7/2/working-with-ai</link>
            <guid>36565854</guid>
            <pubDate>Sun, 02 Jul 2023 21:16:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ben-evans.com/benedictevans/2023/7/2/working-with-ai">https://www.ben-evans.com/benedictevans/2023/7/2/working-with-ai</a>, See on <a href="https://news.ycombinator.com/item?id=36565854">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-nc-base="header" data-controller="AncillaryLayout">
          

          <main>
            
              <section data-content-field="main-content">
                <article id="post-6492ff3078695b7b2d072912" data-item-id="6492ff3078695b7b2d072912">

    
      
    

    <div data-layout-label="Post Body" data-type="item" data-updated-on="1687355241466" id="item-6492ff3078695b7b2d072912"><div data-block-type="2" id="block-yui_3_17_2_1_1687355242158_5623">
  <p>Pretty much everyone in tech agrees that generative AI, Large Language Models and ChatGPT are a generational change in what we can do with software, and in what we can automate with software. There isn’t much agreement on anything else about LLMs - indeed, we’re still working out what the arguments are - but everyone agrees there’s a lot more automation coming, and entirely new kinds of automation. Automation means jobs, and people. </p><p>This is also happening very fast: ChatGPT has (apparently) over 100m users after just six months, and this data from <a href="https://productiv.com/state-of-saas-trends/">Productiv</a> suggests it’s already a top-dozen ‘shadow IT’ app. So, how many jobs is this going to take, how fast, and can there be new jobs to replace them?</p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1688229104156_55683">

<p>We should start by remembering that we’ve been automating work for 200 years. Every time we go through a wave of automation, whole classes of jobs go away, but new classes of jobs get created. There is frictional pain and dislocation in that process, and sometimes the new jobs go to different people in different places, but over time the total number of jobs doesn’t go down, and we have all become more prosperous. </p>



</div><div data-block-type="2" id="block-yui_3_17_2_1_1688084473363_14341">
  <p>When this is happening to your own generation, it seems natural and intuitive to worry that this time, there aren’t going to be those new jobs. We can <em>see</em> the jobs that are going away, but we can’t predict what the new jobs will be, and often they don’t exist yet. We know (or should know), empirically, that there always have been those new jobs in the past, and that they weren’t predictable either: no-one in 1800 would have predicted that in 1900 a million Americans would work on ‘railways’ and no-one in 1900 would have predicted ‘video post-production’ or ‘software engineer’ as employment categories. But it seems insufficient to take it on faith that this will happen now just because it always has in the past. How do you know it will happen this time? Is this different?</p><p>At this point, any first-year economics student will tell us that this is answered by, amongst other things, the ‘Lump of Labour’ fallacy. </p><p>The Lump of Labour fallacy is the misconception that there is a fixed amount of work to be done, and that if some work is taken by a machine then there will be less work for people. But if it becomes cheaper to use a machine to make, say, a pair of shoes, then the shoes are cheaper, more people can buy shoes and they have more money to spend on other things besides, and we discover new things we need or want, and new jobs. The efficient gain isn’t confined to the shoe: generally, it ripples outward through the economy and creates new prosperity and new jobs. So, we don’t know what the new jobs will be, but we have a model that says, not just that there always have been new jobs, but why that is inherent in the process. Don’t worry about AI!</p><p>The most fundamental challenge to this model today, I think, is to say that no, what’s really been happening for the last 200 years of automation is that we’ve been moving up the scale of human capability. </p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1688084473363_58771">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p>‘Barge haulers on the Volga’, Ilya Repin, 1870-73. (Note the steam boat on the horizon.)</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1688084473363_59132">
  <p>We began with humans as beasts of burden, pulling barges up the river, and moved up: we automated legs, then arms, then fingers, and now brains. We went from farm work to blue collar work to white collar work, and now we’ll automate the white-collar work as well and there’ll be nothing left. Factories were replaced by call centres, but if we automate the call centres, what else is there? </p><p>Here, I think it’s useful to look at another piece of economic and tech history: the Jevons Paradox. </p><p>In the 19th century the British navy ran on coal. Britain had a lot of coal (it was the Saudi Arabia of the steam age) but people worried what would happen when the coal ran out. Ah, said the engineers: don’t worry, because the steam engines keep getting more efficient, so we’ll use less coal. No, said Jevons: if we make steam engines more efficient, then they will be cheaper to run, and we will use more of them and use them for new and different things, and so we will use <em>mor</em>e coal. </p><p>We’ve been applying the Jevons Paradox to white collar work for 150 years. </p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1688069358241_3998">
  <p>It’s hard to imagine jobs of the future that don’t exist yet, but it’s also hard to imagine some of the jobs of the past that have already been automated away. Gogol’s <a href="https://en.wikipedia.org/wiki/The_Overcoat">downtrodden clerks</a> in 1830s St Petersburg spent their entire adult lives copying out documents, one at a time, by hand. They were human Xeroxes. By the 1880s, typewriters produced perfectly legible text at twice the words-per-minute, and carbons gave half a dozen free copies as well. Typewriters meant a clerk could produce more than 10 times the output. A few decades later, adding machines from companies like Burroughs did the same for book-keeping and accounting: instead of adding up columns with a pen, the machine does it for you, in 20% of the time, with no mistakes. </p><p>What did that do to clerical employment? People hired far more clerks. Automation plus the Jevons Paradox meant more jobs. </p><p>If one clerk with a machine can do the work of 10, then you might have fewer clerks, but you might also you might do far more with them. If, Jevons tells us, it becomes much cheaper and more efficient to do something, you might do more of it - you might do more analysis or manage more inventory. You might build a different and more efficient business that is only possible because you can automate their administration with typewriters and adding machines. </p><p>This process keeps repeating. This is Jack Lemon as CC Baxter in <a href="https://en.wikipedia.org/wiki/The_Apartment">The Apartment</a> in 1960, using an electro-mechanical adding machine from <a href="https://en.wikipedia.org/wiki/Friden,_Inc.">Friden</a> fifty years after adding machines were new and exciting. </p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1688069358241_18109">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/50363cf324ac8e905e7df861/2d3a670a-28cf-4dd9-a07e-c37c9f9861c8/featured.png" data-image="https://images.squarespace-cdn.com/content/v1/50363cf324ac8e905e7df861/2d3a670a-28cf-4dd9-a07e-c37c9f9861c8/featured.png" data-image-dimensions="1200x894" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="649de7e7206ef75742274813" data-type="image" src="https://images.squarespace-cdn.com/content/v1/50363cf324ac8e905e7df861/2d3a670a-28cf-4dd9-a07e-c37c9f9861c8/featured.png">
                
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1688069358241_18464">

<p>Everyone in that shot is a cell in a spreadsheet, and the whole building is a spreadsheet. Once a week someone on the top floor presses F9 and they recalculate. But they already had computers, and in 1965 or 1970 they bought a mainframe, and scrapped all the adding machines. Did white collar employment collapse? Or, as IBM advertised, did a computer give you 150 extra engineers? 25 years later, what did the PC revolution, and the accounting department in a box, do to accounting?</p>



</div><div data-block-type="2" id="block-yui_3_17_2_1_1688084473363_92506">
  <p>Dan Bricklin invented the computer spreadsheet in 1979: until then, ‘spreadsheets’ were paper (you can still buy them <a href="https://www.amazon.com/s?crid=J5T9EM16AQN4&amp;i=office-products&amp;k=spreadsheet%20paper&amp;ref=nb_sb_ss_ts-doa-p_1_11&amp;sprefix=spreadsheet%2Coffice-products%2C56">on Amazon</a>). He has <a href="https://qz.com/578661/dan-bricklin-invented-the-spreadsheet-but-dont-hold-that-against-him">some entertaining stories</a> about early use: ‘<em>People would tell me, “I was doing all this work, and coworkers thought I was amazing. But I was really goofing off because it only took an hour and then I took the the rest of the day off. People thought I was a wunderkind but I was using this tool.”’</em></p><p> So, what did Excel and the PC do to accounting employment? <a href="https://us.aicpa.org/interestareas/accountingeducation/newsandpublications/aicpa-trends-report">It went up</a>. </p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1688069358241_21167">

<p>40 years later, do spreadsheets mean you can goof off early? Not really.  </p>



</div><div data-block-json="{&quot;width&quot;:550,&quot;height&quot;:null,&quot;hSize&quot;:null,&quot;html&quot;:&quot;<blockquote class=\&quot;twitter-tweet\&quot;><p lang=\&quot;en\&quot; dir=\&quot;ltr\&quot;>Younger people may not believe this but before spreadsheets investment bankers worked really long hours. It&amp;#39;s only thanks to Excel that Goldman Sachs associates can get everything done and leave the office at 3pm on Fridays. Now LLMs mean they&amp;#39;ll only have to work one day a week!</p>&amp;mdash; Benedict Evans (@benedictevans) <a href=\&quot;https://twitter.com/benedictevans/status/1654514832765329411?ref_src=twsrc%5Etfw\&quot;>May 5, 2023</a></blockquote>\n<script async src=\&quot;https://platform.twitter.com/widgets.js\&quot; charset=\&quot;utf-8\&quot;></script>&quot;,&quot;url&quot;:&quot;https://twitter.com/benedictevans/status/1654514832765329411&quot;,&quot;resolvedBy&quot;:&quot;twitter&quot;,&quot;floatDir&quot;:null,&quot;providerName&quot;:&quot;Twitter&quot;,&quot;customThumbEnabled&quot;:false}" data-block-type="22" id="block-yui_3_17_2_1_1688219853387_28970"><blockquote><p lang="en" dir="ltr">Younger people may not believe this but before spreadsheets investment bankers worked really long hours. It's only thanks to Excel that Goldman Sachs associates can get everything done and leave the office at 3pm on Fridays. Now LLMs mean they'll only have to work one day a week!</p>— Benedict Evans (@benedictevans) <a href="https://twitter.com/benedictevans/status/1654514832765329411?ref_src=twsrc%5Etfw">May 5, 2023</a></blockquote>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1688219853387_29036">
  <p>New technology generally makes it cheaper and easier to do something, but that might mean you do the same with fewer people, or you might do much more with the same people. It also tends to mean that you change what you do. To begin with, we make the new tool fit the old way of working, but over time, we change how we work to fit the tool. When CC Baxter’s company bought a mainframe, they began by automating the way they already did things, but over time, new ways to run the business became possible. The machine lets a person do 10x the work, but you need the person. </p><p>So, all of this is to say that by default, we should expect LLMs to destroy, displace, create, accelerate and multiple jobs just as SAP, Excel, Mainframes or typewriters did. It’s just more automation.  </p><p>I think there are two counter-arguments to this. </p><p>The first is to say that yes, perhaps this is indeed just more of the same kind of change that we saw with the internet, PCs or computers, and perhaps again it will have no long-term effect on net employment, but this time it will happen much faster, and so that frictional pain will be much greater and it will be much harder to adjust. </p><p>LLMs and ChatGPT certainly are happening a lot faster than things like iPhones or the Internet, or indeed PCs. The Apple II shipped in 1977, the IBM PC in 1981 and the Mac in 1984, but it took until the early 1990s before  there were 100m PCs in use: there are 100m ChatGPT users today after just six months. You don’t need to wait for telcos to build broadband networks, or consumers to buy new devices, and generative AI sits on top of the whole existing stack of cloud, distributed computing and indeed a lot of the machine learning stack itself that was built over the last decade. To a user, it’s just a website.</p><p>However, your expectations might be different if you think about the implications of these charts, from <a href="https://productiv.com/state-of-saas-trends/">Productiv</a> again and from <a href="https://www.okta.com/uk/resources/whitepaper-businesses-at-work-2021/">Okta</a> (with a different methodology). They report that their typical customer now has hundreds of different software applications. </p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1688331732417_68266">

<p>And yet, enterprise cloud adoption is still barely a quarter of workflows.  </p>



</div><div data-block-type="2" id="block-yui_3_17_2_1_1688219853387_71837">
  <p>What does that mean for generative AI in the workplace? Whatever you think will happen, it will take years, not weeks. </p><p>First, the tools that people use for work, and the tasks that might now get a new layer of automation, are complicated and very specialised, and embody a lot of work and institutional knowledge. A lot of people are <em>experimenting </em>with ChatGPT, and seeing what it will do. If you’re reading this, you probably have too. That doesn’t mean that ChatGPT has replaced their existing workflows <em>yet</em>, and replacing or automating any of those tools and tasks is not trivial. </p><p>There’s a huge difference between an amazing demo of a transformative technology and something that a big complicated company holding other people’s business can use. You can rarely go to a law firm and sell them an API key to GCP’s translation or sentiment analysis: you need to wrap it in control, security, versioning, management, client privilege and a whole bunch of other things that only legal software companies know about (there’s a graveyard of machine learning companies that learnt this in the last decade). Companies generally can’t buy ‘technology’.  <a href="https://www.everlaw.com/">Everlaw</a> doesn’t sell translation and <a href="https://www.people.ai/">People.ai</a> doesn’t sell sentiment analysis - they sell tools and products, and often the AI is only one part of that. I don’t think a text prompt, a ‘go’ button and a black-box, general purpose text generation engine make up a product, and product takes time.  </p><p>Second, buying tools that manage big complicated things takes time even once the tool is built and has product-market fit. One of the most basic challenges in building an enterprise software startup is that startups run on an 18 month funding cycle and a lot of enterprises run on an 18 month decision cycle. SaaS itself accelerated this because you don’t need to get into the enterprise datacenter deployment schedule, but you still need purchase, integration and training, and companies with millions of customers and tens or hundreds of thousands of employees have very good reasons not to change things suddenly. The future takes a while, and the world outside Silicon Valley is complicated. </p><p>The second objection is that part of the paradigm shift of ChatGPT and LLMs is a shift in the layer of abstraction: this looks like a much more general purpose technology. Indeed, that’s why it’s exciting. It can answer <em>anything,</em> we are told. So, you could look at that chart of 473 enterprise SaaS apps and say that ChatGPT will disrupt that and collapse many those vertical apps into one prompt box. That would mean it would move faster, and also automate much more. </p><p>I think that misunderstands the problem. If a partner at a law firm wants a first draft of a paper, they want to be able to shape the parameters in completely different ways to a salesperson at an insurance company challenging a claim, probably with a different training set and certainly with a bunch of different tooling. Excel is ‘general purpose’ too, and so is SQL, but how many different kinds of ‘database’ are there? This is one reason I think the future of LLMs is to move from prompt boxes to GUIs and buttons - I think ‘prompt engineering’ and natural language’ are mutually contradictory. But either way, even if you can run everything as a thin wrapper on top of one giant foundational model (and there is very little agreement or clarity about this yet), even those wrappers will take time.</p><p>Meanwhile, while one could suggest that LLMs will subsume many apps on one axis, I think it’s equally likely that they will enable a whole new wave of unbundling on other axes, as startups peel off dozens more use cases from Word, Salesforce and SAP, and build a whole bunch more big companies by solving problems that no-one had realised were problems until LLMs let you solve them. That’s the process that explains why big companies already have 400 SaaS apps today, after all. </p><p>More fundamental, of course, there is the error rate. ChatGPT can <em>try </em>to answer ‘anything’ but the answer might be wrong. People call this hallucinations, making things up, lying or bullshitting - it’s the ‘overconfident undergraduate’ problem. I think these are all unhelpful framings: I think the best way to understand this is that when you type something into a prompt, you’re not actually asking it to answer a question at all. Rather, you’re asking it “what sort of answers would be people be likely to produce to questions that look like this?” You’re asking it to match a pattern. </p><p>Hence, if I ask ChatGPT4 to write a biography of myself, and then ask it again, it gives different answers. It suggests I went to Cambridge, Oxford or the LSE; my first job was in equity research, consulting or financial journalism. These are always the right pattern: it’s the right kind of university and the right kind of job (it never says Anglia Poly and then catering management). It is giving 100% correct answers to the question “what <strong>kinds</strong> of degrees and jobs are people <strong>like</strong> Benedict <strong>likely</strong> to have done?” It’s not doing a database lookup: it’s making a pattern. </p><p>You can see something similar in this image I got from MidJourney. The prompt was “A photograph of advertising people discussing creativity on stage in a panel on a beach at Cannes Lions.”</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1688228900463_18067">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/50363cf324ac8e905e7df861/f8965b47-137b-4dc8-9fe7-eec10e13781f/0_3.png" data-image="https://images.squarespace-cdn.com/content/v1/50363cf324ac8e905e7df861/f8965b47-137b-4dc8-9fe7-eec10e13781f/0_3.png" data-image-dimensions="1024x1024" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="64a05493b8d18000902a981a" data-type="image" src="https://images.squarespace-cdn.com/content/v1/50363cf324ac8e905e7df861/f8965b47-137b-4dc8-9fe7-eec10e13781f/0_3.png">
                
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1688228900463_18432">
  <p>It’s matched the pattern almost perfectly - that looks like the beach at Cannes, these people are dressed like advertising people, and they even have the right haircuts. But it doesn’t <em>know</em> anything, and so it doesn’t know that people <em>never </em>have three legs, only that it’s unlikely. This isn’t ‘lying’ or ‘making things up’ - it’s matching a pattern, imperfectly. </p><p>Whatever you call it, if you don’t understand this, you can get into trouble, as happened to <a href="https://www.nytimes.com/2023/06/08/nyregion/lawyer-chatgpt-sanctions.html">this unfortunate lawyer</a>, who did not understand that when he asked for precedents, he was actually asking for things that looked like precedents. He duly got things that looked like precedents, but weren’t. It’s not a database. </p><p>If you do understand this, then you have to ask, well, where are LLMs useful? Where is it useful to have automated undergraduates, or automated interns, who can repeat a pattern, that you might have to check? The last wave of machine learning gave you infinite interns who could read anything for you, but you had to check, and now we have infinite interns that can write anything for you, but you have to check. So where is it useful to have infinite interns? Ask Dan Bricklin - we’re back to the Jevons Paradox. </p><p>This takes me, obviously, to AGI. The really fundamental objection to everything I’ve just said is to ask what would happen if we had a system that didn’t have an error rate, didn’t hallucinate, and really could do anything that people can do. If we had that, then you might not have one accountant using Excel to get the output of ten accountants: you might just have the machine. This time, it really would be be different. Where previous waves of automation meant one person could do more, now you don’t need the person.  </p><p>Like a lot of AGI questions, though, this can become circular if you’re not careful. ‘If we had a machine that could do anything people do, without any of these limitations, could it do anything people can do, without these limitations?’</p><p>Well, indeed, and if so we might have bigger problems than middle-class employment, but is that close? You can spend weeks of your life watching three hour YouTube videos of computer scientists arguing about this, and conclude only that they don’t really know either. You might also suggest that the idea this one magic piece of software will change everything, and override all the complexity of real people, real companies and the real economy, and can now be deployed in weeks instead of years, sounds like classic tech solutionism, but turned from utopia to dystopia. </p><p>As an analyst, though, I tend to prefer Hume’s empiricism over Decartes - I can only analyse what we can know. We don’t have AGI, and without that, we have only another wave of automation, and we don’t seem to have any <em>a priori</em> reason why this must be more or less painful than all the others. </p>
</div></div>

    

    

    

  </article>





  
              </section>
            
          </main>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Disabled at 22 million commits (113 pts)]]></title>
            <link>https://programming.dev/post/355447</link>
            <guid>36565842</guid>
            <pubDate>Sun, 02 Jul 2023 21:15:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://programming.dev/post/355447">https://programming.dev/post/355447</a>, See on <a href="https://news.ycombinator.com/item?id=36565842">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="postContent"><div><p>Hey folks,</p>
<p>This is an update to: <a href="https://sh.itjust.works/post/580838">https://sh.itjust.works/post/580838</a></p>
<p>On <a href="https://github.com/csm10495/commit-ment">https://github.com/csm10495/commit-ment</a>, I made it to somewhere around 22 million commits. I can’t imagine this is not a world record for commits to a single branch. In the middle of the night, I got an email from GitHub support saying:</p>
<p><img src="https://sh.itjust.works/pictrs/image/e706b3af-9157-4d97-a5c2-0b4e3725b452.png" alt=""></p>
<p>A few minutes later, I got another email like so:</p>
<p><img src="https://sh.itjust.works/pictrs/image/96f57e50-0567-4913-a8cf-e9c4773ba45f.png" alt=""></p>
<p>I’ve asked them if the two emails are related (and I guess if the first one is some sort of error since there was no personal info in that repo). I’ve also asked if they can give any information about what triggered the email and if they can give me more info about what it looks look on their side.</p>
<p>I’ve also asked if they can re-enable it so I can give one more commit to say the final results on the readme then (public) archive it.</p>
<p>We’ll see what they say.</p>
<p>Doing a pull is interesting at the moment, it shows:</p>
<pre><code>git pull origin master --no-rebase -vvv
ERROR: Access to this repository has been disabled by GitHub staff due to
excessive resource use. Please contact support via
https://support.github.com/contact to restore access to this repository.
Read about how to decrease the size of your repository:
  https://docs.github.com/articles/what-is-my-disk-quota

fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.
</code></pre>
<p>Similar thing happens if you try to clone: <a href="https://programming.dev/cdn-cgi/l/email-protection#46212f3206212f322e33246825292b"><span data-cfemail="caada3be8aada3bea2bfa8e4a9a5a7">[email&nbsp;protected]</span></a>:csm10495/commit-ment.git</p>
<p>So yeah, I figured this would happen sooner or later. I just hope they can tell me a bit more about what it looks like on their side since managing this repo on my box is a pain, I can’t imagine what it could look like on theirs. I’m also curious how pull requests could merge at such a rate given that just doing a pull on my end could take minutes. So many questions!</p>
<p>This whole project was really just for curiosity on my end, so anything I can learn/find out is much appreciated on all ends.</p>
<p>Anyways, just figured I’d update y’all.</p>
</div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WebAuthn Is Great and It Sucks (2020) (143 pts)]]></title>
            <link>https://sec.okta.com/articles/2020/04/webauthn-great-and-it-sucks/</link>
            <guid>36565405</guid>
            <pubDate>Sun, 02 Jul 2023 20:35:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sec.okta.com/articles/2020/04/webauthn-great-and-it-sucks/">https://sec.okta.com/articles/2020/04/webauthn-great-and-it-sucks/</a>, See on <a href="https://news.ycombinator.com/item?id=36565405">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>What is WebAuthn again?</h2><p>First things first, let’s all agree that passwords suck, OK?&nbsp;</p><p>Good, glad we’re on the same page. Passwords are hard to remember, leading people to pick weak ones and reuse them over and over. Passwords are also easy to phish, with ever more subtle and believable attacks happening <a href="https://threatpost.com/clever-phishing-attack-enlists-google-translate-to-spoof-facebook-login-page/141571/">all the time</a>.&nbsp;</p><p>WebAuthn—short for Web Authentication—promises to fix passwords on the web with a strong, simple, and un-phishable standard for secure authentication. WebAuthn at its heart is a credential management API built into modern web browsers allowing web applications to strongly authenticate users, and it’s now a World Wide Web Consortium standard.</p><p>How does WebAuthn do this? Public key cryptography, which allows you to strongly authenticate without a password. Using WebAuthn, you're able to use a single authenticator (like a Yubikey, for example) on any site that supports the standard. This way, as a user, you don't need to have passwords for every site you visit, just a strong authenticator that works with WebAuthn.</p><p>In addition to offering convenience, WebAuthn provides privacy, as one site can’t figure out from the authenticator what other sites you’ve used it for. Attackers also can’t capture and successfully replay the authentication request, so malicious sites can’t use it to attack the genuine sites, eliminating <a href="https://breakdev.org/evilginx-2-next-generation-of-phishing-2fa-tokens/">man-in-the-middle attacks</a>. WebAuthn also allows you to choose your own authenticator, a device you already have (like a smartphone or computer) or an external authenticator like a USB security key.&nbsp;</p><h3>Wow, sounds great!</h3><p>Sure does!</p><h3>OK, so I looked up WebAuthn and it’s full of acronyms!</h3><p>You bet, let’s look at what they mean!</p><p><strong>FIDO</strong> is short for Fast IDentity Online. The FIDO Alliance is an open industry association with <a href="https://fidoalliance.org/members/">hundreds of member companies</a>, working to create authentication standards to help reduce the world’s over-reliance on passwords.&nbsp;</p><p><strong>FIDO2</strong> is the overarching term for the <a href="https://fidoalliance.org/specifications/download/">specifications</a> from the World Wide Web Consortium (W3C) and the FIDO Alliance. It includes both WebAuthentication (web APIs for passwordless authentication in browsers) and CTAP protocols.</p><p><strong>CTAP</strong> stands for <a href="https://fidoalliance.org/specs/fido-v2.0-ps-20190130/fido-client-to-authenticator-protocol-v2.0-ps-20190130.html">Client To Authenticator Protocol</a>. It describes how authenticators can implement second-factor and passwordless authentication. These authenticators can be built-in to devices like phones and laptops (on-device or platform authenticators), or they can be external ones (roaming authenticators or security keys) connecting over NFC, USB, and/or BLE.</p><p><strong>CTAP2</strong> is the new hotness. It enables you to use the new authenticators <strong>not only for second-factor authentication</strong>, but also for <strong><a href="https://www.w3.org/TR/webauthn/#sctn-authenticator-taxonomy">passwordless and multi-factor authentication</a></strong>.&nbsp;</p><p><strong>CTAP1</strong> is what used to be called FIDO U2F. It allows older U2F authenticators like security keys to continue to be used for second-factor authentication, i.e. as an extra step after a password.&nbsp;</p><p><img src="https://lh5.googleusercontent.com/ZO0LpBNwZXtNU7BcMFv0G97IFjEWKNGScAsD1vOBjs36YhTysbpL91AghbbSAy0QorZEHb78cLcBIvtzbuFXecY4Gaw__Re5qlWJYuORpb3KscqBv34EcVcaIX71ZD1CX_JyK7nf"><br>
Image copyright FIDO Alliance</p><p>Easy, right? Well, it could be a lot worse—just look at the <a href="https://tools.ietf.org/html/rfc6749">OAuth 2.0 specifications!</a></p><p><img src="https://lh5.googleusercontent.com/M3i3S_QJPUvtfkFRLsC17sgw2Ha_UbkgG-ULfwKu8n9vk_HDnLL4U4R0ZcPBGnBUG99uqn21abXnAEQAHBh0LofdZ7wurV2JxOpTty36qR0BYRtYFz16XDInI-YH1tu-u1CPKdHV"></p><p>If you want to see how WebAuthn works behind the scenes, watch this <a href="https://www.okta.com/resources/oktane-content/developer/#does-webauthn-signal">great video by James Fang and Payal Pan from Oktane 19</a>.</p><h2>Wait, why do I care again?</h2><p>Great question! The key promise is …. *drum roll* <strong>strong passwordless authentication</strong>!</p><p>The older FIDO U2F protocols and security keys allow for strong and phishing-resistant second-factor experiences, but now we’re talking passwordless! Just think of how smooth and seamless that will be!</p><p>These new protocols make it possible to require even stronger authentication than the user presence test of U2F protocol, where you tap the security key to authenticate. With FIDO2, <a href="https://www.w3.org/TR/webauthn/#user-verification">sites can require user verification</a> at different levels from password or PIN tied to the security key all the way to on-device biometrics, such as fingerprint readers, Face ID, or Windows Hello. This can enable single-device <a href="https://en.wikipedia.org/wiki/Multi-factor_authentication#Authentication_factors">multi-factor authentication</a>, combining the possession factor (you have the authenticator) with a knowledge factor (you know the PIN) and/or inherent factors (your biometric, like fingerprint or faceprint matches).</p><p>Now, the biggest challenge in moving past passwords is the simple fact that it has been the lowest common denominator—the easiest and cheapest thing to implement. Passwords like we know them date back 59 years (!!) going back to <a href="https://www.wired.com/2012/01/computer-password/">MIT in 1961 with the CTSS</a> operating system. Passwords are literally everywhere.&nbsp;</p><p>Every gizmo that has come since with a promise to eliminate the password has failed. There’s always been a platform, service, or system that didn’t support the latest new passwordless idea, and very few were ready to pay the cost of changing the servers, the operating systems, the applications, everything, just to get a non-compatible point solution. Passwords are cheap. Everything else is expensive.</p><p>Except now, with WebAuthn!</p><h2>Everything supports WebAuthn! Great!</h2><p>We are at a cusp of having universal support for WebAuthn!&nbsp; All major operating systems and browsers have now implemented WebAuthn.&nbsp;</p><p>As I write this in April 2020, a full <strong>83% of all the browsers in use</strong> around the world support it, as you can see in this <a href="https://caniuse.com/#search=webauthn">CanIUse</a> report.</p><p><img src="https://lh3.googleusercontent.com/2wLQQsRTDLUv1OT-jPovHPioCZpyU5XlY_-WlSa7Mb5eoDlYJ2WntAakBxKIUfSlivfmpXvp-V2IUeQS4O_8q7dIOPV4TCc5EFsPM_eiuTPYxM8S0F1ar0Eeg0icjWqCypE9NTxU"></p><p>There are dozens of different FIDO2-compatible security keys available from companies like Yubico, Feitian, Google, Kensington, and others. And developers have built support into operating systems (iOS, Android, Windows, macOS) so you can use platform authenticators like Touch ID sensors on MacBooks and facial recognition and fingerprint sensors on PCs.</p><p>Victory, hooray! Let’s go and get a FIDO2 Security Key so we can use it everywhere!</p><h2>Nothing supports WebAuthn! What?!</h2><p>Except when you go to set it up, you will find that basically <strong>no major web application</strong> supports WebAuthn the way we envisioned here as replacement for the password! D’oh!</p><p>Web applications support WebAuthn fairly well as a <strong>second factor</strong>, backward compatible to FIDO U2F, but even that support remains far from universal.</p><p>Story time! I’ve always been more than a little paranoid, an occupational hazard having worked around web security for 20+ years. Outside of work, I already had a unique, complex, and <a href="https://haveibeenpwned.com/Passwords">non-Pwned password</a> for each of the 562 websites and apps I have a login for (not counting the work apps behind Okta SSO of course). Yes, I use a <a href="https://1password.com/">password manager</a>—I’m not an animal.</p><p>Let me recount my experience when I went to set up WebAuthn on every account whose security really matters to me! Passwords begone! Here are the results!</p><h3>Let’s see the scoreboard</h3><ul><li>GMail: Yes! But alas only as a U2F security key after password.</li><li>Another email: No, but at least they support generic <a href="https://en.wikipedia.org/wiki/Time-based_One-time_Password_algorithm">TOTP second factors</a>.</li><li>Apple iCloud: Proprietary multi-factor authentication, but that’s a different story.&nbsp;</li><li>Cellular provider: LOL NOPE! Security PIN only and SMS, which they helpfully are willing to send to my kids’ phone numbers too in case they ever guess my password!</li><li>Top modern robo advisor: No, but at least they support generic TOTP.</li><li>Top online brokerage: No, but at least they support a proprietary third-party TOTP app.</li><li>Top retirement account: LOL NOPE! SMS!</li><li>Top-three credit card issuer: LOL NOPE! SMS!</li><li>Another top-three credit card issuer: LOL NOPE! No strong authentication of ANY kind.</li><li>Top-five bank: LOL NOPE! SMS! Or they’ll sell you a 90’s-style hardware TOTP token!</li><li>Local credit union: LOL NOPE! They got nothing, but the last time I tried to log in my account was locked out, so good to know that somebody’s trying to brute-force my complex, globally unique password :)</li><li>Online crypto currency wallet: Yes! But alas only as a U2F security key after password. Also, they block my FIDO2 platform authenticator and only allow USB security keys. And once you add a security key, you lose your TOTP! It’s one or the other!&nbsp;&nbsp;</li><li>My DNS / hosting provider: Yes! But alas only as a U2F security key after password.</li><li>Facebook: Yes, but just as a second factor, U2F mode.</li><li>Twitter: Yes, but just as a second factor, U2F mode, and the settings are buried deep.</li><li>Zoom: Nope. Just a well-hidden option to add a generic TOTP second factor.</li><li>Dropbox: Yes, but just as a second factor, U2F mode.</li></ul><p>So, the <strong>score is 0 (zero) out of 17 for going passwordless</strong> with WebAuthn. Sigh.</p><p>Or we could say the score is 6 out of 17, if we accept U2F mode using WebAuthn as a second factor.</p><h3>Don’t banks want better security?</h3><p>Well, they do, but they are not pushing end-user-visible and end-user-operated security tools, because today even the best ones like WebAuthn add friction in the form of inconsistencies and confusion. And as I’ll show you shortly, even with WebAuthn that friction is unfortunately real.&nbsp;</p><p>Any friction translates to confused and angry customers, which translates to millions of dollars in call-center cost and customer churn. Remember that even small banks have tens of thousands of users, large ones tens of millions! This is why banking security professionals focus so heavily on the invisible, back-end fraud detection and risk management tools. And if an attacker compromises an account and takes money, the bank can make the account holder whole again and treat it as a cost of doing business. Corporate banking portals dealing with big money transfers typically use strong authentication, as the user population is much smaller and more receptive to adopting security measures.</p><p>So, don’t look for consumer financial services to adopt passwordless WebAuthn first. That won’t happen until browsers and operating systems universally support it and <strong>not until the user experience is consistent and great</strong>.</p><h2>Why can’t we have nice things?</h2><p>So are WebAuthn and FIDO2 doomed to fail? And can it ever get us to passwordless?&nbsp;</p><p>Well, the technology doesn’t suck, the protocols work, the basic tech is kind of great, and you can and should use WebAuthn as a second-factor authenticator everywhere that matters! Security keys are one of the <a href="https://krebsonsecurity.com/2018/07/google-security-keys-neutralized-employee-phishing/">strongest practical authenticators</a> available today, and they are useful for anyone who would ever get this far into a blog post like this.</p><p>The problem is, while they technically work, the <a href="https://blog.silverorange.com/web-authn-ux-89a61ba7b555">user experience is broken</a>.&nbsp;</p><p>Every website has a different path for setting things up. What the security settings are called and where they are found are wildly inconsistent from one website to the next.&nbsp;</p><p>Every browser and operating system presents the experience in a different-looking pane or slide-over. The terminology different browsers use is inconsistent and confusing.&nbsp;</p><p>Even the very few sites that support the full passwordless WebAuthn experience have to provide other options, so you have to click on a separate link for the passwordless path. And if you make a mistake during setup, the error messages can be less than helpful.</p><p><img src="https://lh6.googleusercontent.com/b1qcKPRRquGHsuNgvomjLj1p0e9fsSTy605x88BVKynqrnKiV5bI8kU_yznvCRgHLAq_mDE20nnApEsPK9Ob5zzbah_w3l-Zcau-x8EHQ5i6i0-78NWCks5JhbcYbnU-O8Xxhmwj"></p><p>Therefore it is hard to recommend WebAuthn to the people we most want to help—our friends and family, children and parents. The way websites, browsers and OSs implement WebAuthn today does not pass my will-my-spouse-murder-me-in-my-sleep standard of deployability.&nbsp;</p><h2>Can I get passwordless WebAuthn?</h2><p>In short, probably not today. But it’s not hopeless!</p><p>Frankly, you might get passwordless first at work. Modern single sign-on providers make it possible to use WebAuthn and other modern authenticators in combination with risk scoring, device management, and other tools in the corporate IT tool box to enable end-to-end passwordless experiences. These corporate solutions, like <a href="https://www.okta.com/fastpass/">Okta FastPass</a>, combine on-device biometrics and device management solutions to get there. At work, we are <a href="https://www.okta.com/blog/2019/10/the-dogfooding-chronicles-webauthn-the-path-to-passwordless/">eating our own dogfood</a> with WebAuthn!</p><p>Outside of the workplace, across the 17 sites I looked to secure, I found none that yet supported a bona-fide WebAuthn passwordless experience.&nbsp;</p><h2>So what should I do?</h2><p>Don’t give up!</p><p>Even if we can’t have true passwordless today, <strong>adding WebAuthn as a strong second-factor authentication is absolutely worth it</strong>. I can use WebAuthn as a second factor at 6 of my 17 sites I checked. <a href="https://youtu.be/w-YDV6vC2qo">Not great, not terrible!</a> Another 4 of the 17 support adding TOTP one-time passwords. Although technically these one-time passwords are phishable, the risk is vastly reduced if you’re using a password manager app as your TOTP authenticator as well, as the password manager won’t autofill your credentials to the phishing site.&nbsp;</p><p>So using WebAuthn as a second-factor authenticator is definitely worth it and user experience is fairly seamless when combined with password managers.</p><p>But how do we ever get to the promised WebAuthn passwordless world?!</p><p>Let’s keep the pressure on the browser and operating system vendors and ask for consistency. Ask <a href="https://support.google.com/chrome/answer/95315">Chrome</a>, <a href="https://qsurvey.mozilla.com/s3/FirefoxInput/">Firefox</a>, and <a href="https://www.apple.com/feedback/safari.html">Safari</a> to standardize their naming conventions for a better user experience!</p><p>And on the other hand, let’s keep the pressure on the <a href="https://twofactorauth.org/">websites that don’t support strong authentication</a> at all and more specifically, let’s push <a href="https://www.dongleauth.info/">websites that don’t support security keys</a> to add that support.</p><p>If you’re an IT administrator and want to support WebAuthn for your employee access, it’s easy to do with modern SSO providers like <a href="https://www.okta.com/resources/whitepaper-how-webauthn-works-oad">Okta</a>.&nbsp;</p><p>And if you’re one of the people who builds these websites, consider adding WebAuthn support to it! The frameworks are there and ready for you to use, see <a href="https://developers.yubico.com/WebAuthn/WebAuthn_Developer_Guide/Overview.html">Yubico’s WebAuthn developer guide</a> and <a href="https://2018.pycon-au.org/talks/44258-webauthn-multifactor-auth-for-everyone/">this talk</a> for examples. With just a little more design and usability polish, we can all win!</p><p>Thanks for reading and stay safe!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Googling for answers costs time (2020) (119 pts)]]></title>
            <link>https://prashanth.world/seo-ruining-programming-help/</link>
            <guid>36565225</guid>
            <pubDate>Sun, 02 Jul 2023 20:16:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://prashanth.world/seo-ruining-programming-help/">https://prashanth.world/seo-ruining-programming-help/</a>, See on <a href="https://news.ycombinator.com/item?id=36565225">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>When I’m learning a new language, I tend to take a very dumb approach (and I don’t mean that in some enlightened way). I’ll usually try out a tutorial or two, but if the language is similar to another language that I know, I’ll basically just start coding without investing a lot of time learning the language features. This leads me to often googling how to do extremely basic things. Stuff like ‘python iterate over dict’. ‘javascript get object keys’. ‘elixir keyword list to map’. In the past, I’ve felt extremely adept at finding exactly what I need, but recently I’ve noticed that it’s been taking me longer and longer to really find the answers I’m looking for.</p>

<p>For example, looking at the results on Google for <a href="https://www.google.com/search?hl=en&amp;q=python%20iterate%20over%20dictionary">‘python iterate over dict’</a>, I see the following results</p>

<table>
  <thead>
    <tr>
      <th><img src="https://prashanth.world/images/google_python_dict_results.png" alt="google results"></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>Seems reasonable</em></td>
    </tr>
  </tbody>
</table>

<p>Notice that StackOverflow is the third response. I am not sure what realpython.com is, but clicking on that <a href="https://realpython.com/iterate-through-dictionary-python/">link</a> shows the following page</p>

<table>
  <thead>
    <tr>
      <th><img src="https://prashanth.world/images/real_python_image_1.png" alt="google results"></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>a whole lot of nothing</em></td>
    </tr>
  </tbody>
</table>

<p>Notice that there’s literally no content visible. Lets scroll down one page</p>

<table>
  <thead>
    <tr>
      <th><img src="https://prashanth.world/images/real_python_TOC.png" alt="google results"></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>…so…much…nonsense</em></td>
    </tr>
  </tbody>
</table>

<p>Hmm still no content. Well there’s a table of contents, but no actual content. Scrolling down to the content, it goes on and on and on and on about why dictionaries are important, what you can use dictionaries for, what are valid keys for dictionaries… The title of the post is quite literally ‘How to iterate through a dictionary in python’, but it takes NINE PAGE DOWN keystrokes just to get to the <em>section</em> called ‘the basics’.</p>

<p>How is this the number one link on Google? Specifically when looking up how to do basic things in Python, I get more and more of this filler content. I think it’s because Python attracts a lot of bootcamp and college students, people who <em>would pay</em> for training in order to learn Python. I remember one of the arguments I heard about why we were adopting Python at a previous company I worked for that was a mostly Rails (and some Elixir) shop was that Python was very easy to learn and search for on Google. I don’t think that’s true.</p>

<p>I don’t <em>know</em> if this is actually getting worse over time, but I’m noticing filler content more and more. It also seems to happen whenever searching for anything cooking related. For example: <a href="https://www.google.com/search?hl=en&amp;q=1%20cup%20flour%20to%20grams">converting 1 cup flour to grams</a>. To start, this search has a featured snippet that’s just wrong. But when you click into a few of the other links, the answer is obscured in some way (burried in a list of charts, answer is not made very apparent). Another cooking example - <a href="https://thesaltymarshmallow.com/homemade-belgian-waffle-recipe/">recipe sites</a>. It’s not a recipe unless there’s some excruciatingly long preamble, asking me to scroll at least 5 pages down before the actual recipe is available.</p>

<p>And I know the reason - ads and SEO. In order to be a good candidate for any ad revenue, you need enough ‘content’ before you can actually make any money. All this pointless content is posted just so people will spend longer on a site, which makes things more appealing for both ads and SEO. Except… well SEO should be optimising for content people actually want to see, and instead it’s optimizing for the amount of time spent on a site, regardless of whether the time on the site was actually productive.</p>

<p>But to me, the user, it feels like I’m paying for my answers with my time pointlessly scrolling around the page to find the answer I came to the site for. I can’t stand it; I feel like I am being duped, misled, being taken for a ride, just so I know how to make pancakes, or how to do a simple thing in common programming language. It feels like watching Dragon Ball Z back in the 00’s. Watching 5 episodes week after week, just for five minutes of time to elapse in the show.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Commuters prefer origin to destination transfers (154 pts)]]></title>
            <link>https://pedestrianobservations.com/2023/06/30/why-commuters-prefer-origin-to-destination-transfers/</link>
            <guid>36564608</guid>
            <pubDate>Sun, 02 Jul 2023 19:07:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pedestrianobservations.com/2023/06/30/why-commuters-prefer-origin-to-destination-transfers/">https://pedestrianobservations.com/2023/06/30/why-commuters-prefer-origin-to-destination-transfers/</a>, See on <a href="https://news.ycombinator.com/item?id=36564608">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>It’s an empirical observation that rail riders who are faced with a transfer are much more likely to make the trip if it’s near their home than near their destination. Reinhard Clever’s since-linkrotted work gives an example from Toronto, and American commuter rail rider behavior in general; <a href="https://pedestrianobservations.com/2011/10/28/why-the-7-to-secaucus-wont-work/">I was discussing it from the earliest days of this blog</a>. He points out that American and Canadian commuter rail riders drive long distances just to get to a cheaper or faster park-and-ride stations, but are reluctant to take the train if they have any transfer at the city center end.</p>



<p>This pattern is especially relevant as, due to continued job sprawl, American rail reformers keep looking for new markets for commuter rail to serve and set their eyes on commutes to the suburbs. <a href="https://blog.bimajority.org/2018/11/12/a-further-examination-of-the-agricultural-branch-for-regional-rail/">Garrett Wollman is giving an example</a>, in the context of the Agricultural Branch, a low-usage freight line linking to the Boston-Worcester commuter line that could be used for local passenger rail service. Garrett talks about the potential ridership of the line, counting people living near it and people working near it. And inadvertently, his post makes it clear why the pattern Clever saw in Toronto is as it is.</p>



<p><strong>Residential and job sprawl</strong></p>



<p>The issue at hand is that residential sprawl and job sprawl both require riders to spend some time connecting to the train. The more typical example of residential sprawl involves isotropic single-family density in a suburban region, with commuters driving to the train station to get on a train to city center; they could be parking there or being dropped off by family, but in either case, the interface to the train for them is in their own car.</p>



<p>Job sprawl is different. Garrett points out that there are 79,000 jobs within two miles of a potential station on the Ag Branch, within the range of corporate shuttles. With current development pattern, rail service on the branch could follow the best practices there are and I doubt it would get 5% of those workers as riders, for all of the following reasons:</p>



<ul>
<li>The corporate shuttle is a bus, with all the discomfort that implies; it usually is also restricted in hours even more than traditional North American commuter rail – the frequency on the LIRR or even Caltrain is low off-peak but the trains do run all day, whereas corporate shuttles have a tendency to only run at peak. There is no own-car interface involved.</li>



<li>The traditional car-commuter train interface is to jobs in areas with traffic congestion and difficult parking. The jobs in the suburbs face neither constraint. Of note, Long Islanders working in Manhattan do transfer to the subway, because driving to the East Side to avoid the transfer from Penn Station is not a realistic option.</li>



<li>The traditional car-commuter train interface is to jobs in a city center served from all directions by commuter rail. In contrast, the jobs in the suburbs are only served by commuter rail along a single axis. There is a fair amount of reverse-peak ridership from San Francisco to Silicon Valley jobs or from New York to White Plains and Stamford jobs, even if at far lower rates than the traditional peak direction – but most people working at a suburban job center live in another suburb, own a car, and either commute in a different direction from that of the train or don’t live and work close enough to a station that the car-train-shuttle trip is faster than an all-car trip. </li>
</ul>



<p>Those features are immutable without further changes in urban design. Then there are other features that interact with the current timetables and fares. North American commuter rail has so many features designed to appeal to the type of person who drives everywhere and uses the train as a shuttle  extending their car-oriented lifestyle into the city – premium fares, heavy marketing as different from normal public transit, poor integration with said normal public transit – that interface with one’s own car is especially valuable, and interface with public transit is especially unvalued.</p>



<p>And yet, it’s clearly possible to make it work. How?</p>



<p><strong>How Europe makes it work</strong></p>



<p>Commuter trains in Europe (nobody calls them regional rail here – <a href="https://pedestrianobservations.com/2019/08/20/s-bahn-and-regionalbahn/">that term is reserved for hourly long-range trains</a>) get a lot of off-peak ridership and are not at all used exclusively by 9-to-5 commuters who drive for all other purposes. Some of this is to suburban job centers. How does this work, besides timetables and other operating practices that American reformers recognize as superior to what’s available in the US and Canada?</p>



<p>The primary answer is near-center jobs. Paris and La Défense have, between them, about 37% of the total jobs of Ile-de-France. Within the same land area, 100 km^2, both New York and Boston have a similar proportion of the jobs in their respective metro areas, about 35% each, as does San Francisco within the smaller definition of the metro area, excluding Silicon Valley. Ile-de-France’s work trip modal split is about 43%, metro New York’s is 33%, metro San Francisco’s is 17%, metro Boston’s is 12%.</p>



<p>So where Boston specifically fails is not so much office park jobs, such as those on Route 128, but near-center jobs. Its urban and suburban transit networks do a poor job of getting people to job centers like Longwood, the airport, Cambridge, and the Seaport. The same is true of San Francisco. New York’s network does a better but still mediocre job at connecting to Long Island City and Downtown Brooklyn, and a rather bad job at connecting to inner-suburban New Jersey jobs, but so many of those 35% jobs in the central 100 km^2 are in fact in the central 23 km^2 of the Manhattan core, and nearly half are in the central 4 km^2 comprising Midtown, that the poor service to the other 77 km^2 can be overlooked.</p>



<p>As far as commuter rail is concerned, the main difference in ridership between the main European networks – the Paris RER, the Berlin S-Bahn, and so on – and the American ones is how useful they are for plain urban service. Nearly all Berlin S-Bahn traffic is within the city, not the suburbs; the RER’s workhorse stations are mostly in dense inner suburbs that in most other countries would have been amalgamated into the city already.</p>



<p>To the extent that this relates to American commuter rail reforms, it’s about coverage within the city: multiple city stations, good (free, frequent) connections to local urban rail, high frequency all day to encourage urban travel (a train within the city that runs every half an hour might as well not run).</p>



<p>Suburban ridership is better here as well, but this piggybacks on very strong urban service, giving strong service from the suburbs to the city. Suburb-to-suburb commutes are done largely by car – Ile-de-France’s modal split is 43%, not 80%; there are fewer of them than in most of the US, but not fewer than in New York, Boston, or San Francisco.</p>



<p>But, well, Paris’s modal split is noticeably higher than the job share within the city – a job share that does include drivers. What gives?</p>



<p><strong>Suburban transit-oriented development</strong></p>



<p>TOD in the suburbs can create a pleasant enough rail commute that the modal split is respectable, if nothing like what is seen for jobs in Paris or Manhattan. However, for this to work, planners must eliminate the expression “corporate shuttle” from their lexicon.</p>



<p>Instead, suburban job sites must be placed right on top of the train station, or within walking distance along streets that are decently walkable. I can’t think of good Berlin examples – Berlin maintains high modal split through a strong center – but I can think of several Parisian ones: Marne-la-Vallée (including Disneyland), Noisy, Evry, Cergy. Those were often built simultaneously with greenfield suburban lines that were then connected to the RER, rather than on top of preexisting commuter lines.</p>



<p>They look nothing like American job sprawl. Here, for example, is Cergy:</p>



<figure><a href="https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png"><img data-attachment-id="9549" data-permalink="https://pedestrianobservations.com/2023/06/30/why-commuters-prefer-origin-to-destination-transfers/cergy/" data-orig-file="https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png" data-orig-size="1412,876" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cergy" data-image-description="" data-image-caption="" data-medium-file="https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png?w=300" data-large-file="https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png?w=830" src="https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png?w=1024" alt="" srcset="https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png?w=1024 1024w, https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png?w=150 150w, https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png?w=300 300w, https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png?w=768 768w, https://pedestrianobservations.files.wordpress.com/2023/06/cergy.png 1412w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>There are parking garages visible near the train stations, but also a massing of mid-rise residential and commercial buildings.</p>



<p>But speaking of residential, the issue is that employers looking for sites to locate to have no real reason to build offices on top of most suburban train stations – the likeliest highest and best usage is residential. In the case of American TOD, even the secondary-urban centers, like Worcester, probably have much more demand for residential than commercial TOD within walking distance of the train station – employers who are willing to pay near-train station premium rent might as well pay the higher premium of locating within the primary city, where the commuter shed is much larger.</p>



<p>In effect, the suburban TOD model does not counter the traditional monocentric urban layout. It instead extends it to a much larger scale. In this schema, the entirety of the city, and not just its central few square kilometers, is the monocenter, served by different lines with many stations on them. Berlin is ahead of the curve by virtue of its having multiple close-by centers as a Cold War legacy, but Paris is similar (its highest-intensity commercial TOD is in La Défense and in in-city sites like Bercy, on top of former railyards attached to Gare de Lyon).</p>



<p>At no point does this model include destination-end transfers in the suburbs. In the city, it does: a single line cannot cover all urban job sites; but the transfer is within the rapid transit system. But in the suburbs, the jobs that are serviceable by public transportation are within walking distance of the station. Shuttles may exist, but are secondary, and job sites that require them are and will always be auto-centric.</p>
			
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google search's death by a thousand cuts (261 pts)]]></title>
            <link>https://matt-rickard.com/google-searchs-death-by-a-thousand-cuts</link>
            <guid>36564042</guid>
            <pubDate>Sun, 02 Jul 2023 18:14:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matt-rickard.com/google-searchs-death-by-a-thousand-cuts">https://matt-rickard.com/google-searchs-death-by-a-thousand-cuts</a>, See on <a href="https://news.ycombinator.com/item?id=36564042">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>Reddit communities are still private in protest of new API rules. Twitter moved beyond a login wall and is rate-limiting users. Users are frustrated but still using these sites.</p><p>But — what will happen to the Google Index? Millions of search results are effectively dead links. Users that refined Reddit search results via Google are now out of luck (Reddit’s search is inferior). Tweets in the search engine results page (SERP) now lead to a login wall for many users.</p><p>Advancements in <a href="https://matt-rickard.com/will-llms-disrupt-google" rel="noopener noreferrer nofollow">AI might disrupt Google Search</a> in a roundabout way:</p><p>Large models are trained on public data scraped via API. Content-heavy sites are most likely to be disrupted (why post on StackOverflow?) by models trained on their own data. Naturally, they want to restrict access and either (1) sell the data or (2) train their own models. This restriction prevents (or complicates) Google’ automatic scraping of the data for Search (and probably for training models, too).</p><p>Google will lose results, site by site — it will be Google Search’s death by a thousand cuts.</p><p>It’s estimated that Wikipedia shows up on the first page of 99% of searches on Google. What if Wikipedia started charging or restricting API access? It’s a dataset found in almost every large language model corpus. The Wikimedia Foundation is constantly looking for financial assistance (“please donate” banners) and has already launched an enterprise API product (Wikimedia Enterprise, 2021).</p><p>One by one, search results become dead links and are removed from the index. Users will start to rely on site-specific searches behind walled gardens. The first page of search results will not only be filled with ads but will be missing key results. Google may try to augment results with AI-generated answers, but (1) not all of these answers will be good enough, and (2) the data needed to train these answers will increasingly be found behind login or paywalls. Search might progressively get worse over the years until a new alternative arises.</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JavaScript Gom Jabbar (532 pts)]]></title>
            <link>https://frantic.im/javascript-gom-jabbar/</link>
            <guid>36564010</guid>
            <pubDate>Sun, 02 Jul 2023 18:12:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://frantic.im/javascript-gom-jabbar/">https://frantic.im/javascript-gom-jabbar/</a>, See on <a href="https://news.ycombinator.com/item?id=36564010">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <header>
    
    
  </header>
  <p>You have been using JavaScript for 10 years. It’s time for your test. You are sitting in front of a computer. The test is simple: you have to open a package.json file and read it. The <code>package.json</code> is full of pain. You have to read it all.</p>
<p>You look at <code>version</code>, you haven’t reached 1.0 yet. Semver causes unpleasant memories, but you’ve learned to ignore them for so long that you don’t even notice the tickling sensation in your skull.</p>
<p>You wish you used a different <code>name</code> for your package, but some random internet person has squatted that name 7 years ago and never updated their package since. It’s only mildly discomforting. Maybe the test isn’t so bad after all?</p>
<p>Both <code>main</code> and <code>browser</code> fields are present, you sense traces of Isomorphic JavaScript. In a flash, you remember requiring <code>fs</code> module from your browser bundle. These memories are very unpleasant. The hacks you had to do to make it work were even more unpleasant.</p>
<p>The <code>type</code> is set to <code>module</code>. This has something to do with the migration from <code>requires</code> to <code>imports</code>. Why do we have to care about this, again? The extensive pain you’ve experienced trying to importing ES5 modules from ESM modules and vice versa overwhelms you again.</p>
<p>You make your way to <code>scripts</code>. What a hot, painful mess it is. You can’t look at them without your heart rate going to 150. lint, lintall, lintfast, lintdiff. Parallel runs, obscure arguments, double-escaping JSON-formatted arguments. Subcommands calling npm even through you switched to yarn and then pnpm. Thousands of variations, premutations and details make you shiver. Why do these things have to be here? Why do they need to be so complicated?</p>
<p>Some scripts still use <code>watchman</code>. Gotta remember to not use symlinks because it doesn’t support them (and the issue has been open since 2015). There’s also this gulp-based script that nobody has the guts to replace with anything else that’s considered more modern. You think that there’s actually no modern version of gulp but it feels outdated and you definitely want to get rid of it. The pains spreads from your head into your neck and shoulders.</p>
<p>The pain is barely tolerable when you reach <code>dependencies</code>. So, so many of them. There’s <code>left-pad</code>, the legendary tiny package that broke all internet, collectively causing the amount of pain and drama comparable to the destruction of Alderaan.</p>
<p>Every time you modify dependency list, some of the dependencies print out screens-worth of messages to your console, asking for donations, warning about breaking changes. You gave up trying to understand these. You only hope none of them are malicious enough to steal your secrets or ruin your computer. The threat of potential pain of that magnitute is frighting.</p>
<p>There’s also moment.js. You love that library, it has a really pleasent API. But the internet decided it’s too “mutable”, too fat, it doesn’t support treeshaking and now you have to migrate to date-fns. You haven’t started yet, but you already feel the painful refactoring in your bones.</p>
<p>Looking at every package in that list causes some amount of trauma recall. But what’s even more concerning is that the version of these packages are way behind what’s considered “current”. You know that you should upgrade them. But you also have tried that before and you know how much suffering it brings. Things will break in so many ways, big and loud ways, small and subtle ways.</p>
<p>The next thing in this damn file is <code>resolutions</code>. Yes, you remember this one. It’s a suffering you choose to avoid dealing with package upgrades.</p>
<p>You scroll down to <code>devDependencies</code>. You can’t remember the time when you only needed non-dev dependencies. Why do we have this split? Yes, right, to cause more pain.</p>
<p><code>eslint</code>. Its configuration got so strict that you can’t even write code anymore. Any small misstep and you get an angry red underline. Your CI is configured to treat any lint problem as the end of the world. It gives a false sense of security to your junior engineers on the team. You survived serveral holy wars on which rules to enable. The pain is proportional to the amount of <code>eslint-ignore</code>s you have all over your codebase. There’s a lot.</p>
<p>You also notice <code>postcss</code> hiding there. This package is a mystery to you. You don’t use it directly, it’s a requirement of a dependency of a dependency. But it’s the package that’s constantly causing you pain by throwing obscure C++ compilation errors on any new platform you try to <code>npm install</code> on. If CSS itself wasn’t painful enough.</p>
<p>Oh, dear <code>jest</code>. It started as a fast test runner. But now it’s big and fat, it depends on some babel packages while the rest of your app is transpiled by a mix of esbuild and swc. Properly configuring it with ESM and TypeScript was a PhD science project.</p>
<p>You stop to count how many tools and parsers work on your codebase: TypeScript, esbuild, swc, babel, eslint, prettier, jest, webpack, rollup, terser. You are not sure if you missed any. You are not sure if you want to know. The level of pain is so high you forget about anything else.</p>
<p><code>engines</code> prominently lists <code>node</code>. And while you hate it with the depth of your soul, you are not going to Bun or Deno because you know this will not stop the pain. This will only make the pain worse.</p>
<p>It’s the end of the file now. Final closing curly brace. You close the tab and take a breath. Look around. You are still alive, your hands and your brain intact. You survived. For now.</p>

  
  






  <div>
    <p>Hello! This text lives here to convince you to subscribe. If you are reading this, consider clicking that subscribe button for more details.</p>
    <p>I write about programming, software design and side projects <a href="https://frantic.im/subscribe/" target="_blank"><svg viewBox="0 0 800 800"><path d="M493 652H392c0-134-111-244-244-244V307c189 0 345 156 345 345zm71 0c0-228-188-416-416-416V132c285 0 520 235 520 520z"></path><circle cx="219" cy="581" r="71"></circle></svg> Subscribe</a></p>
  </div>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD CPU Use Among Linux Gamers Approaching 70% Marketshare (321 pts)]]></title>
            <link>https://www.phoronix.com/news/AMD-CPU-Linux-Gaming-67p</link>
            <guid>36563979</guid>
            <pubDate>Sun, 02 Jul 2023 18:09:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phoronix.com/news/AMD-CPU-Linux-Gaming-67p">https://www.phoronix.com/news/AMD-CPU-Linux-Gaming-67p</a>, See on <a href="https://news.ycombinator.com/item?id=36563979">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="AMD" src="https://www.phoronix.com/assets/categories/amd.webp" width="100" height="100"></p><p>
Besides being curious about the Steam Survey results for indicating the size of the Linux gaming marketshare as an overall percentage, one of the interesting metrics we are curious about each month is the AMD vs. Intel CPU marketshare for Linux gaming. AMD has been on quite an upward trajectory among Linux gamers/enthusiasts in recent years not only for their Radeon graphics cards with their popular open-source driver stack but their Ryzen CPUs have become extremely popular with Linux users. With <a href="https://www.phoronix.com/news/Steam-June-2023-Statistics">the new Steam Survey results for June</a>, AMD CPUs are found on nearly 70% of Linux gaming systems polled by Steam.
</p><p>
The June results put the AMD CPU marketshare for Linux users at 67%, a remarkable 7% increase month-over-month. In part that's due to the Steam Deck being powered by an AMD SoC but it's been a trend building for some time of AMD's increasing Ryzen CPU popularity among Linux users to their open-source driver work and continuing to build more good will with the community.
</p><p><img src="https://www.phoronix.net/image.php?id=2023&amp;image=steam_june_4"></p>
<p>In comparison, last June the AMD CPU Linux gaming marketshare <a href="https://www.phoronix.com/news/Steam-Linux-June-2022">came in at 45%</a> while Intel was at 54%. Or at the start of 2023, <a href="https://www.phoronix.com/news/Steam-Survey-January-2023">AMD CPUs were at a 55% marketshare</a> among Linux gamers. Or if going back six years, <a href="https://www.phoronix.com/news/Steam-Survey-July-2017">AMD CPU use among Linux gamers was a mere 18%</a> during the early Ryzen days.
</p><p><img src="https://www.phoronix.net/image.php?id=2023&amp;image=windows_cpu_gamers_june" alt="Windows CPU stats for June"></p>
<p>It's also the direct opposite on the Windows side. When looking at the Steam Survey results for June limited to Windows, there Intel has a 68% marketshare to AMD at 32%.
</p><p><a href="https://www.phoronix.com/image-viewer.php?id=2023&amp;image=amd_gaming_cpus_june_lrg" target="_blank"><img src="https://www.phoronix.net/image.php?id=2023&amp;image=amd_gaming_cpus_june_med" alt="AMD Ryzen boxes, cheers"></a></p>
<p>Beyond the Steam Deck, it's looking like AMD's efforts around open-source drivers, <a href="https://www.phoronix.com/news/Dell-Mario-On-AMD-Linux-Team">AMD expanding their Linux client (Ryzen) development efforts</a> over the past two years, promises around <a href="https://www.phoronix.com/search/OpenSIL">OpenSIL</a>, and other efforts commonly covered on Phoronix are paying off for AMD in wooing over their Linux gaming customer base.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chinese Tech Terms Explained in English (169 pts)]]></title>
            <link>https://16x.engineer/2022/10/18/chinese-tech-terms.html</link>
            <guid>36563956</guid>
            <pubDate>Sun, 02 Jul 2023 18:06:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://16x.engineer/2022/10/18/chinese-tech-terms.html">https://16x.engineer/2022/10/18/chinese-tech-terms.html</a>, See on <a href="https://news.ycombinator.com/item?id=36563956">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
            <p>
              Chinese tech companies like ByteDance and Tencent are setting up
              engineering offices outside China. Local hires in these global
              offices are becoming a norm.
            </p>
            <p>
              These locally hired engineers typically have limited Chinese
              language proficiency. It is a challenge for them to understand the
              Chinese tech terms used in their daily work, and to communicate
              with their Chinese colleagues.
            </p>
            <p>
              Here are 5 common Chinese tech terms, along with explanations and
              examples of usage in a sentence.
            </p>
            <h2 id="huidu">Huidu</h2>
            <blockquote>
              <p>灰度 <span>huī dù</span></p>
            </blockquote>
            <p>
              <img src="https://16x.engineer/public/post-images/optimized/chinese-huidu.webp" alt="Huidu in English example screenshot">
            </p>
            <p>
              Huidu is a widely used Chinese technical term in companies like
              Tencent and Alibaba. Huidu means rolling out experimental new
              features to a small subset of users for testing.
            </p>
            <p>
              The implication of huidu is that the new feature may be launched
              officially (at a later date), or scrapped if it did not have the
              desired outcome.
            </p>
            <h2 id="huidu-in-english">Huidu in English</h2>
            <p>
              The literal translation of huidu is “grayscale”. Intuitively,
              huidu represents the transition state between black and white.
            </p>
            <p>
              Note that huidu can be used to describe product features,
              functionalities, or code deployment.
            </p>
            <p>
              When describing a product feature, the close equivalent would be
              <strong>beta release</strong> or
              <strong>phased rollout</strong> in English.
            </p>
            <p>
              When used for describe code deployment,
              <strong>canary release</strong> is also an accurate translation.
            </p>
            <h2 id="examples-of-huidu-in-a-sentence">
              Examples of huidu in a sentence
            </h2>
            <ul>
              <li>
                <p>
                  <em>WeChat is <strong>huidu</strong> testing a new feature that
                    allows users to register an additional WeChat account with
                    an existing phone number.</em>
                </p>
              </li>
              <li>
                <p>
                  <em>Lenovo started <strong>huiduing</strong> new version of
                    operating system for its tablets.</em>
                </p>
              </li>
            </ul>
            <h2 id="lunzi">Lunzi</h2>
            <blockquote><p>轮子 lún zi</p></blockquote>
            <p>
              <img src="https://16x.engineer/public/post-images/optimized/chinese-lunzi.webp" alt="Lunzi in English example screenshot">
            </p>
            <p>
              Lunzi describes tools, libraries or frameworks that have been
              invented or reinvented multiple times.
            </p>
            <p>
              Typically it is used for new things that are created to solve a
              specific problem, but bear resemblance to something existing.
            </p>
            <h2 id="lunzi-in-english">Lunzi in English</h2>
            <p>
              The literal translation for lunzi is “wheel”, and the origin of
              the word is likely from the English phrase “reinvent the wheel”.
            </p>
            <p>
              Despite the apparent origin from English, lunzi as a word has no
              equivalent translation in English.
            </p>
            <p>
              The literal translation “<strong>wheel</strong>” might be the best
              candidate.
            </p>
            <h2 id="example-of-lunzi-in-a-sentence">
              Example of lunzi in a sentence
            </h2>
            <ul>
              <li>
                <p>
                  <em>Engineers need create <strong>lunzi</strong> to fulfill
                    KPIs or get promoted.</em>
                </p>
              </li>
              <li>
                <p>
                  <em><strong>Lunzi</strong> created by others do not fulfil our
                    specific needs.</em>
                </p>
              </li>
            </ul>
            <h2 id="chendian">Chendian</h2>
            <blockquote><p>沉淀 chén diàn</p></blockquote>
            <p>
              <img src="https://16x.engineer/public/post-images/optimized/chinese-chendian.webp" alt="Chendian in English example screenshot">
            </p>
            <p>
              Chendian is not a technical term, but it is commonly used in
              Chinese tech companies. It means consolidating learnings from past
              experience.
            </p>
            <p>
              It also implies coming up with a systematic solution to a
              recurring problem.
            </p>
            <h2 id="chendian-in-english">Chendian in English</h2>
            <p>
              The literal translation for chendian is “chemical precipitation”,
              drawing from the intuition that things are consolidated from
              liquid form to solid state.
            </p>
            <p>
              At the beginning, things are messy and fluid. But as you learn and
              progress, they because more clear and structured.
            </p>
            <p>
              There seems to be no equivalent term in English to describe this
              concept.
            </p>
            <h2 id="example-of-chendian-in-a-sentence">
              Example of chendian in a sentence
            </h2>
            <ul>
              <li>
                <p>
                  <em>Team lead has some expectations for new interns, such as
                    critical thinking, <strong>chendian</strong> and knowledge
                    sharing.</em>
                </p>
              </li>
              <li>
                <p>
                  <em>Our team managed to <strong>chendian</strong> tools and
                    organizational capabilities from the double 11 sale.</em>
                </p>
              </li>
            </ul>
            <h2 id="dapan">Dapan</h2>
            <blockquote><p>大盘 dà pán</p></blockquote>
            <p>
              <img src="https://16x.engineer/public/post-images/optimized/chinese-dapan.webp" alt="Dapan in English example screenshot">
            </p>
            <p>
              In Chinese tech companies, dapan is used to describe dashboards
              with various charts, where you can monitor key system metrics or
              business metrics in real time.
            </p>
            <p>
              It is especially important for e-commerce companies like Alibaba
              and JD who run big sales events like double 11.
            </p>
            <h2 id="dapan-in-english">Dapan in English</h2>
            <p>
              Dapan originally refers to the big screens in stock exchanges
              showing market data.
            </p>
            <p>
              The close equivalent of dapan in English would be “<strong>monitoring dashboard</strong>”.
            </p>
            <h2 id="examples-of-dapan-in-a-sentence">
              Examples of dapan in a sentence
            </h2>
            <ul>
              <li>
                <p>
                  <em>Remember to monitor <strong>dapan</strong> closely when
                    performing production deployment.</em>
                </p>
              </li>
              <li>
                <p>
                  <em>Popular <strong>dapan</strong> products usually come with
                    WYSIWYG editors.</em>
                </p>
              </li>
            </ul>
            <h2 id="maidian">Maidian</h2>
            <blockquote>
              <p>埋点 <span>mái diǎn</span></p>
            </blockquote>
            <p>
              <img src="https://16x.engineer/public/post-images/optimized/chinese-maidian.webp" alt="Maidian in English example screenshot">
            </p>
            <p>
              In Chinese companies, maidian refers to embedding of tracking code
              to monitor and analyse user behaviour.
            </p>
            <p>
              This is typically required by product and data analytics teams,
              and the tracking code is embedded in the product by front-end
              engineers.
            </p>
            <h2 id="maidian-in-english">Maidian in English</h2>
            <p>
              As individual characters, “mai” means burying, and “dian” means
              points. The phrase “maidian” literally means “embedding points” or
              “burying seeds”.
            </p>
            <p>
              The intuition is that by “embedding” the “seed” in the product,
              the user journey can be mapped out later.
            </p>
            <p>
              It can be translated into <strong>tracking</strong>,
              <strong>tagging</strong> or
              <strong>user data analytics</strong> depending on the context.
            </p>
            <h2 id="examples-of-maidian-in-a-sentence">
              Examples of maidian in a sentence
            </h2>
            <ul>
              <li>
                <p>
                  <em>Data analytics team need to design a rigorous
                    <strong>maidian</strong> system and produce
                    <strong>maidian</strong> document to support subsequent user
                    behaviour analysis.</em>
                </p>
              </li>
              <li>
                <p>
                  <em>The cost of <strong>maidian</strong> is high because every
                    widget with user interactions needs to embed
                    <strong>maidian</strong> code.</em>
                </p>
              </li>
            </ul>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Economic inequality cannot be explained by individual bad choices, study finds (133 pts)]]></title>
            <link>https://phys.org/news/2023-06-economic-inequality-individual-bad-choices.html</link>
            <guid>36563815</guid>
            <pubDate>Sun, 02 Jul 2023 17:53:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phys.org/news/2023-06-economic-inequality-individual-bad-choices.html">https://phys.org/news/2023-06-economic-inequality-individual-bad-choices.html</a>, See on <a href="https://news.ycombinator.com/item?id=36563815">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
										
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2023/economic-inequality-ca.jpg" data-src="https://scx2.b-cdn.net/gfx/news/2023/economic-inequality-ca.jpg" data-sub-html="Correlation between ten biases within 3346 participants showed each bias was largely unique and not collinear with other biases assessed, with the exception of overplacement&nbsp;and overestimation (which rely on the presence of some biases). Credit: <i>Scientific Reports</i> (2023). DOI: 10.1038/s41598-023-36339-2">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2023/economic-inequality-ca.jpg" alt="Economic inequality cannot be explained by individual bad choices" title="Correlation between ten biases within 3346 participants showed each bias was largely unique and not collinear with other biases assessed, with the exception of overplacement&nbsp;and overestimation (which rely on the presence of some biases). Credit: Scientific Reports (2023). DOI: 10.1038/s41598-023-36339-2" width="685" height="530">
             <figcaption>
                Correlation between ten biases within 3346 participants showed each bias was largely unique and not collinear with other biases assessed, with the exception of overplacement&nbsp;and overestimation (which rely on the presence of some biases). Credit: <i>Scientific Reports</i> (2023). DOI: 10.1038/s41598-023-36339-2
            </figcaption>        </figure>
    </div>
<p>A global study led by a researcher at Columbia University Mailman School of Public Health and published in the journal <i>Scientific Reports</i> finds that economic inequality on a social level cannot be explained by bad choices among the poor nor by good decisions among the rich. Poor decisions were the same across all income groups, including for people who have overcome poverty.

										  
											        </p>
										 
										 											  
<p>While <a href="https://phys.org/tags/economic+inequality/" rel="tag">economic inequality</a> continues to rise within countries, efforts to address it have been largely ineffective, particularly those involving behavioral approaches. It has been often implied, but until now not tested, that choice patterns among low-income individuals may be a factor impeding behavioral interventions aimed at improving upward economic mobility.
</p><p>The study is based on online surveys in 22 languages with close to 5,000 participants from 27 countries in Asia, Europe, North America, and South America. Decision-making ability was measured through 10 individual biases, including (1) temporal discounting, not preferring immediate funds over larger future gains; (2) overestimation, or thinking you are better than you are at making decisions; (3) over-placement, or thinking you are better than the <a href="https://phys.org/tags/average+person/" rel="tag">average person</a> at making decisions; and (4) extremeness aversion, or taking the "middle option" simply because it seems safer than the highest or lowest.
</p><p>Taken along with <a href="https://phys.org/news/2022-07-economic-inequality-instability-impacts-long-term.html">related work</a> showing that temporal discounting is tied more to the broader societal economic environment rather than individual financial circumstances, the new findings are a major validation of arguments stating that poorer individuals are not uniquely prone to <a href="https://phys.org/tags/cognitive+biases/" rel="tag">cognitive biases</a> that alone explain protracted poverty.
</p><p>"Our research does not reject the notion that individual behavior and decision-making may directly relate to upward economic mobility. Instead, we narrowly conclude that biased decision-making does not alone explain a significant proportion of population-level economic inequality," says first author Kai Ruggeri, Ph.D., assistant professor in the Department of Health Policy and Management at Columbia Public Health.
</p><p>"Low-income individuals are not uniquely prone to cognitive biases linked to bad financial decisions. Instead, scarcity is more likely a greater driver of these decisions," Ruggeri adds.
										 																				
																				</p><div>
																						<p><strong>More information:</strong>
												Kai Ruggeri et al, The persistence of cognitive biases in financial decisions across economic groups, <i>Scientific Reports</i> (2023).  <a data-doi="1" href="https://dx.doi.org/10.1038/s41598-023-36339-2" target="_blank">DOI: 10.1038/s41598-023-36339-2</a>
																						
																						</p>
																					</div>
                               											
																					
                              										                                        
										<!-- print only -->
										<div>
											 <p><strong>Citation</strong>:
												Economic inequality cannot be explained by individual bad choices, study finds (2023, June 29)
												retrieved 2 July 2023
												from https://phys.org/news/2023-06-economic-inequality-individual-bad-choices.html
											 </p>
											 <p>
											 This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
											 part may be reproduced without the written permission. The content is provided for information purposes only.
											 </p>
										</div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMIGAlive – Play Amiga games online with people across the world (143 pts)]]></title>
            <link>https://www.amigalive.com/</link>
            <guid>36563780</guid>
            <pubDate>Sun, 02 Jul 2023 17:50:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.amigalive.com/">https://www.amigalive.com/</a>, See on <a href="https://news.ycombinator.com/item?id=36563780">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		<main id="main" role="main">

			
				
<article id="post-65" class="page">
	
	<div>
		

		
<h2><strong>Welcome to the AmigaLive project!</strong></h2>



<h4>AmigaLive is a front-end application which utilizes the  netplay capabilities of the FS-UAE emulator. AmigaLive makes it more simple than ever before to have 2 or more people from around the world, connect and play the same game, or even use the same software, as if sharing the same Amiga computer hardware and peripherals<strong>.</strong></h4>



<div><figure><img decoding="async" width="820" height="462" src="https://www.amigalive.com/wp-content/uploads/Amiga-Facebook-2.jpg" alt="" srcset="https://www.amigalive.com/wp-content/uploads/Amiga-Facebook-2.jpg 820w, https://www.amigalive.com/wp-content/uploads/Amiga-Facebook-2-300x169.jpg 300w, https://www.amigalive.com/wp-content/uploads/Amiga-Facebook-2-768x433.jpg 768w, https://www.amigalive.com/wp-content/uploads/Amiga-Facebook-2-128x72.jpg 128w, https://www.amigalive.com/wp-content/uploads/Amiga-Facebook-2-32x18.jpg 32w" sizes="(max-width: 820px) 100vw, 820px"></figure></div>



<h4>FS-UAE  is the friendliest Amiga emulator which integrates Amiga emulation code from WinUAE and is available for all major platforms, such as Windows, Mac OS-X and Linux.</h4>



<p><strong>*This project offers an unofficial distribution of the&nbsp;FS-UAE&nbsp;software which may not be up to date, therefore please do not send any bug reports to the developer (Frode Solheim) regarding AmigaLive issues.</strong></p>



<h2><strong>Requirements:</strong></h2>



<ul><li><a href="https://www.amigalive.com/download/"><strong>Download</strong></a><b><strong> </strong>the latest version of the AmigaLive bundle</b><br><b>(Everyone needs to have the same major version)</b></li><li><strong>A modern computer running a 64-bit OS </strong><br><strong>(Linux, Mac OS-X or Windows)</strong></li><li><strong>High speed internet<br>(Wi-Fi works but a wired LAN connection is recommended)</strong></li><li><strong>Any input device:&nbsp;Joystick, GamePad, Keyboard or Mouse.&nbsp;<br>A joystick can also be emulated by using different keyboard layouts, the default keys are: cursor keys and right Ctrl as fire 1 button<br>(A digital&nbsp;2 button Joystick/GamePad is recommended)</strong></li><li><strong><strong>You need people ready to play with you, but if you don’t know many that are interested or support Amiga emulation, w<strong><strong>e welcome you to join </strong></strong></strong>us on the <a href="https://discord.gg/r8pZTyV">AmigaLive Discord</a> and give us a shout. You can notify whoever is available by using the @here command in the Discord channels 🙂</strong></li></ul>




	</div><!-- .entry-content -->
</article><!-- #post-## -->

			
		</main><!-- #main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First 'tooth regrowth' medicine moves toward clinical trials in Japan (851 pts)]]></title>
            <link>https://mainichi.jp/english/articles/20230609/p2a/00m/0sc/026000c</link>
            <guid>36563590</guid>
            <pubDate>Sun, 02 Jul 2023 17:33:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mainichi.jp/english/articles/20230609/p2a/00m/0sc/026000c">https://mainichi.jp/english/articles/20230609/p2a/00m/0sc/026000c</a>, See on <a href="https://news.ycombinator.com/item?id=36563590">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<!-- cxenseparse_start -->

<div>
<figure>
<div>
<a data-href="https://cdn.mainichi.jp/vol1/2023/06/09/20230609p2a00m0na021000p/9.jpg?1" data-lightbox="photos" data-title="(Getty)">
<span>
<img src="https://cdn.mainichi.jp/vol1/2023/06/09/20230609p2a00m0na021000p/6.jpg?1" alt="">

</span>

</a>
</div>
<figcaption>(Getty)</figcaption>
</figure>
</div>

<div>
<figure>
<div>
<a data-href="https://cdn.mainichi.jp/vol1/2023/06/09/20230609p2a00m0na022000p/9.jpg?1" data-lightbox="photos" data-title="Katsu Takahashi, head of the dentistry and oral surgery department at the Medical Research Institute Kitano Hospital, is seen in the city of Osaka's Kita Ward on May 16, 2023. (Mainichi/Mirai Nagira)">
<span>
<img src="https://cdn.mainichi.jp/vol1/2023/06/09/20230609p2a00m0na022000p/7.jpg?1" alt="">

</span>

</a>
</div>
<figcaption>Katsu Takahashi, head of the dentistry and oral surgery department at the Medical Research Institute Kitano Hospital, is seen in the city of Osaka's Kita Ward on May 16, 2023. (Mainichi/Mirai Nagira)</figcaption>
</figure>
</div>
<p>
    TOKYO -- A Japanese research team is making progress on the development of a groundbreaking medication that may allow people to grow new teeth, with clinical trials set to begin in July 2024.
</p>
<!-- cxenseparse_end -->

<!-- cxenseparse_start -->
<p>
    The tooth regrowth medicine is intended for people who lack a full set of adult teeth due to congenital factors. The team is aiming to have it ready for general use in 2030.
</p>
<p>
    In prior animal experiments, the medicine prompted the growth of "third-generation" teeth following baby teeth and then permanent adult teeth.
</p>
<p>
    "The idea of growing new teeth is every dentist's dream. I've been working on this since I was a graduate student. I was confident I'd be able to make it happen," said Katsu Takahashi, lead researcher and head of the dentistry and oral surgery department at the Medical Research Institute Kitano Hospital in the city of Osaka.
</p>

<div>
<figure>
<div>
<a data-href="https://cdn.mainichi.jp/vol1/2023/06/09/20230609p2a00m0na023000p/7.jpg?1" data-lightbox="photos" data-title="A new tooth is seen growing in a mouse treated with the tooth regrowth medicine. (Photo provided by Katsu Takahashi, head of the dentistry and oral surgery department at the Medical Research Institute Kitano Hospital)">
<span>
<img src="https://cdn.mainichi.jp/vol1/2023/06/09/20230609p2a00m0na023000p/6.jpg?1" alt="">

</span>

</a>
</div>
<figcaption>A new tooth is seen growing in a mouse treated with the tooth regrowth medicine. (Photo provided by Katsu Takahashi, head of the dentistry and oral surgery department at the Medical Research Institute Kitano Hospital)</figcaption>
</figure>
</div>
<p>
    Anodontia is a congenital condition that causes the growth of fewer than a full set of teeth, present in around 1% of the population. Genetic factors are thought to be the major cause for the one-tenth of anodontia patients who lack six or more teeth, a condition categorized as oligodontia. These conditions are also known as tooth agenesis. People who grow up with tooth agenesis struggle with basic abilities like chewing, swallowing and speaking from a young age, which can negatively impact their development.
</p>
<p>
    After completing a dentistry degree, Takahashi went on to graduate studies in molecular biology at Kyoto University in 1991. Afterwards, he studied in the U.S.
</p>
<p>
    Around that time, research around the world had begun to pinpoint genes that, when deleted, would cause genetically modified mice to grow fewer teeth. "The number of teeth varied through the mutation of just one gene. If we make that the target of our research, there should be a way to change the number of teeth (people have)," Takahashi said of his thoughts at the time.
</p>
<p>
    <b>Global attention</b>
</p>

<div>
<figure>
<div>
<a data-href="https://cdn.mainichi.jp/vol1/2023/06/09/20230609p2a00m0na024000p/6.jpg?1" data-lightbox="photos" data-title="The front teeth of a ferret treated with tooth regrowth medicine are seen in a photo provided by Katsu Takahashi, head of the dentistry and oral surgery department at the Medical Research Institute Kitano Hospital. The medicine induced the growth of an additional seventh tooth (center).">
<span>
<img src="https://cdn.mainichi.jp/vol1/2023/06/09/20230609p2a00m0na024000p/6.jpg?1" alt="">

</span>

</a>
</div>
<figcaption>The front teeth of a ferret treated with tooth regrowth medicine are seen in a photo provided by Katsu Takahashi, head of the dentistry and oral surgery department at the Medical Research Institute Kitano Hospital. The medicine induced the growth of an additional seventh tooth (center).</figcaption>
</figure>
</div>
<p>
    It was around 2005, when he delved further into the subject at Kyoto University after returning to Japan, that he began to see a bright path for his continued research. The researchers found that mice lacking a certain gene had an increased number of teeth. A protein called USAG-1, synthesized by the gene, was found to limit the growth of teeth. In other words, blocking the action of that protein could allow more teeth to grow.
</p>
<p>
    Takahashi's research team narrowed their focus onto USAG-1, and developed a neutralizing antibody medicine able to block the protein's function. In experiments in 2018, mice with a congenitally low number of teeth were given medicine that resulted in new teeth coming through. The research results were published in a U.S. scientific paper in 2021, and gained much attention as the beginnings of the world's first tooth regeneration medicine.
</p>
<p>
    Work is now underway to get the drug ready for human use. Once confirmed to have no ill effects on the human body, it will be aimed at treating children aged 2 to 6 who exhibit anodontia. "We hope to pave the way for the medicine's clinical use," Takahashi said.
</p>
<p>
    <b> Medicine could be game-changer</b>
</p>
<p>
    If successful, a drug to regenerate teeth may be a game-changer for the entire field of dentistry.
</p>
<p>
    Animals including sharks and some reptile species can continuously regrow teeth. It's been assumed that humans only grow two sets of teeth in their lifetime, but in fact, there is evidence that we also have the "buds" for a third set.
</p>
<p>
    Around 1% of the population exhibits the converse of anodontia: hyperdontia, a congenital condition causing a higher-than-normal number of teeth. According to research by Takahashi's team, one in three such cases manifests as the growth of a third set of teeth. Takahashi believes that in most cases, humans' ability to grow a third set was lost over time.
</p>
<p>
    When the researchers applied the drug to ferrets, they grew an additional seventh front tooth. As the new teeth grew in between the existing front teeth and were of the same shape, the medicine is thought to have induced the generation of third-set teeth in the animals.
</p>
<p>
    When treatment of teeth is no longer possible due to severe cavities or erosion of the dental sockets, known as pyorrhea, people lose them and need to rely on dental appliances such as dentures. The ability to grow third-generation teeth could change that. "In any case, we're hoping to see a time when tooth-regrowth medicine is a third choice alongside dentures and implants," Takahashi said.
</p>
<p>
    For further information or inquiries about Takahashi's research, please visit https://www.kitano-hp.or.jp/toothreg/ (in Japanese).
</p>
<p>
    (Japanese original by Mirai Nagira, Science &amp; Environment News Department)
</p>
<!-- cxenseparse_end -->

<!--| tools BGN |-->

<!--| tools END |-->
</div></div>]]></description>
        </item>
    </channel>
</rss>