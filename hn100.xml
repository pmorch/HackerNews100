<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 04 Nov 2024 14:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[I've had a change of heart regarding employee metrics (358 pts)]]></title>
            <link>http://rachelbythebay.com/w/2024/11/03/metrics/</link>
            <guid>42038653</guid>
            <pubDate>Mon, 04 Nov 2024 05:06:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://rachelbythebay.com/w/2024/11/03/metrics/">http://rachelbythebay.com/w/2024/11/03/metrics/</a>, See on <a href="https://news.ycombinator.com/item?id=42038653">Hacker News</a></p>
Couldn't get http://rachelbythebay.com/w/2024/11/03/metrics/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Please just stop saying "just" (2019) (115 pts)]]></title>
            <link>https://sgringwe.com/2019/10/10/Please-just-stop-saying-just.html</link>
            <guid>42038139</guid>
            <pubDate>Mon, 04 Nov 2024 03:25:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sgringwe.com/2019/10/10/Please-just-stop-saying-just.html">https://sgringwe.com/2019/10/10/Please-just-stop-saying-just.html</a>, See on <a href="https://news.ycombinator.com/item?id=42038139">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
      <p>Do you work in Software Engineering, and have you seen messages or sentences like these before?</p>

<blockquote>
  <p>“Can’t we <em>just</em> set up a redirect to this other domain?”</p>
</blockquote>

<blockquote>
  <p>“Why don’t we <em>just</em> add some caching to speed that page up?”</p>
</blockquote>

<blockquote>
  <p>“I read their documentation, and we <em>just</em> need to inject some javascript, we should be all set!”</p>
</blockquote>

<p>Me too, and it drives me a little crazy.</p>

<p>Not because anyone is being rude, or because anything they are saying is necessarily wrong. Nobody is being intentionally malice, here. <strong>It’s that (in my opinion) the word “<em>just</em>” added as a qualifier to an idea carries with it a whole bunch of implied baggage.</strong></p>

<h2 id="the-baggage">The Baggage</h2>

<h2 id="implies-simplicity">Implies Simplicity</h2>

<p>First and foremost, the word “<em>just</em>” implies that an idea is simple. “Oh, we can <em>just</em> do this!”.</p>

<p>If there is one thing I’ve learned with Software Engineering, it’s that <em>Computers are Hard</em> and implementation details matter, especially when it comes to complexity. Very rarely can the complexity of an engineering solution be outlined in a single sentence. There is almost always more to it.</p>

<p>Let’s take one of the above examples of “<em>just</em>” injecting javascript into the page.</p>

<ul>
  <li>Do we trust this javascript to run on our page? What is the security posture of the javascript author?</li>
  <li>What dependencies does the script have, and are they trusted and up-to-date?</li>
  <li>What data is being sent from their javascript? Do we need to update our privacy policy?</li>
  <li>How will the javascript be minified?</li>
  <li>Does it work with our other libraris, such as React or Turbolinks?</li>
  <li>Do we need to load this Javascript from a third party? What if that server goes down? Can we trust the integrity of their server?</li>
  <li>Do we need to update our Content Security Policy for this script?</li>
  <li>What sort of performance impact will this have on our page load performance?</li>
  <li>Does the script need to run at any a particular point in the page load process?</li>
  <li>Does the styling of the DOM elements it produces match our brand? Is that important?</li>
  <li>What sort of impact will this have on the accessibility of our website?</li>
  <li>Who is going to maintain this javascript, and update it when it needs to be?</li>
  <li>How will we monitor if this javascript snippet is still working?</li>
  <li>If the business decides to stop using it, who will tell us and how will we know?</li>
</ul>

<h2 id="reinforces-imposter-syndrome">Reinforces Imposter Syndrome</h2>

<p>Many engineers report having Imposter Syndrome, with some data even suggesting over <a href="https://www.techrepublic.com/article/why-58-of-tech-employees-suffer-from-imposter-syndrome/">half of engineers</a> at most major Silicon Valley companies.</p>

<p>Let’s paint the picture of a very common, every-day scenario…</p>

<p>Imagine being an engineer with Imposter Syndrome, and you are working on solving a problem all day. You find yourself stuck on a few tricky details on the solutions you are considering, and so you ask for help from a Senior Engineer on your team. You sit down, start to talk through it, and your coworker says…</p>

<p><strong>“what if you <em>just</em> do this?”</strong></p>

<p>I ask you, reader:</p>

<ul>
  <li>Would you feel good or happy to hear that?</li>
  <li>If you disagreed, would you voice that disagreement with such a confident and simple answer from a Senior Engineer?</li>
  <li>If you don’t understand, do you ask “what do you mean?”</li>
</ul>

<p>My guess is probably not. How could you disagree with such a simple answer?! Or not understand it?!</p>

<h2 id="reduces-ideation">Reduces Ideation</h2>

<p>Because of the implications the word “<em>just</em>” comes with, I believe that engineers are less likely to ideate and brainstorm in that environment.</p>

<p>In the face of an idea that says to “<em>just</em> do this”, I believe engineers are less confident in voicing disagreement, and are less likely to ask “what do you mean?” to truly understand the problem and the solution.</p>

<p>A team where engineers aren’t asking for clarification, don’t understand their solutions, and aren’t voicing alternative approaches is <strong>not a team that I hope to be on</strong>. That team is missing out on so much, including great ideas.</p>

<h2 id="solution">Solution</h2>

<p>My solution is simple: stop qualifying your statements with the word “<em>just</em>”.</p>

<p>Of course, I’ll acknowledge right away that such as task is not easy. And I continue to use the word by mistake all the time! But I try hard not to, and I believe I am better for it.</p>

<p>When I catch myself typing a sentence that uses the word “just”, I pause and rephrase my sentence to avoid using it. I find that doing so helps for me to:</p>

<ul>
  <li>get better response and ideation back from others</li>
  <li>clarify my thinking with strong examples and evidence</li>
  <li>orient the discussion towards a more generative mindset</li>
  <li>produce a healthier debate with more learning and sharing from everyone</li>
</ul>

<p>So please… <em>just</em> stop saying “<em>just</em>”!</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An embarrassingly simple approach to recover unlearned knowledge for LLMs (179 pts)]]></title>
            <link>https://arxiv.org/abs/2410.16454</link>
            <guid>42037982</guid>
            <pubDate>Mon, 04 Nov 2024 02:52:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2410.16454">https://arxiv.org/abs/2410.16454</a>, See on <a href="https://news.ycombinator.com/item?id=42037982">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2410.16454">View PDF</a>
    <a href="https://arxiv.org/html/2410.16454v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Large language models (LLMs) have shown remarkable proficiency in generating text, benefiting from extensive training on vast textual corpora. However, LLMs may also acquire unwanted behaviors from the diverse and sensitive nature of their training data, which can include copyrighted and private content. Machine unlearning has been introduced as a viable solution to remove the influence of such problematic content without the need for costly and time-consuming retraining. This process aims to erase specific knowledge from LLMs while preserving as much model utility as possible. Despite the effectiveness of current unlearning methods, little attention has been given to whether existing unlearning methods for LLMs truly achieve forgetting or merely hide the knowledge, which current unlearning benchmarks fail to detect. This paper reveals that applying quantization to models that have undergone unlearning can restore the "forgotten" information. To thoroughly evaluate this phenomenon, we conduct comprehensive experiments using various quantization techniques across multiple precision levels. We find that for unlearning methods with utility constraints, the unlearned model retains an average of 21\% of the intended forgotten knowledge in full precision, which significantly increases to 83\% after 4-bit quantization. Based on our empirical findings, we provide a theoretical explanation for the observed phenomenon and propose a quantization-robust unlearning strategy to mitigate this intricate issue...
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Fali Wang [<a href="https://arxiv.org/show-email/606b1885/2410.16454" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 21 Oct 2024 19:28:37 UTC (1,232 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists glue two proteins together, driving cancer cells to self-destruct (460 pts)]]></title>
            <link>https://med.stanford.edu/news/all-news/2024/10/protein-cancer.html</link>
            <guid>42037386</guid>
            <pubDate>Mon, 04 Nov 2024 00:42:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://med.stanford.edu/news/all-news/2024/10/protein-cancer.html">https://med.stanford.edu/news/all-news/2024/10/protein-cancer.html</a>, See on <a href="https://news.ycombinator.com/item?id=42037386">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">

    
      
        <p itemprop="description">Stanford researchers hope new technique will flip lymphoma protein’s normal action — from preventing cell death to triggering it.</p>
      
    

   <p><span itemprop="datePublished">October 22, 2024</span> 
       - By Rachel Tompa</p>


    <div><div>























<wcmmode:edit>
    
</wcmmode:edit>


    
    
        
    



<div id="" data-fullscreen-modal="false">
                    <p>A new molecule developed by Stanford Medicine researchers (turquoise and yellow) tethers two proteins (purple and red) that together switch on self-destruction genes in cancer cells.<br>
<i>Ella Maru Studio</i></p>
                </div></div>
<div id="main_content">
    <p>Our bodies divest themselves of 60 billion cells every day through a natural process of cell culling and turnover called apoptosis.</p>
<p>These cells — mainly blood and gut cells — are all replaced with new ones, but the way our bodies rid themselves of material could have profound implications for cancer therapies in a new approach developed by Stanford Medicine researchers.</p>
<p>They aim to use this natural method of cell death to trick cancer cells into disposing of themselves. Their method accomplishes this by artificially bringing together two proteins in such a way that the new compound switches on a set of cell death genes, ultimately driving tumor cells to turn on themselves. The researchers describe their <a href="https://www.science.org/doi/10.1126/science.adl5361?url_ver=Z39.88-2003&amp;rfr_id=ori:rid:crossref.org&amp;rfr_dat=cr_pub%20%200pubmed">latest such compound</a> in a paper published Oct. 4 in <i>Science</i>.</p>
<p>The idea came to <a href="https://profiles.stanford.edu/gerald-crabtree">Gerald Crabtree</a>, MD, a professor of development biology, during a pandemic stroll through the forests of Kings Mountain, west of Palo Alto, California. As he walked, Crabtree, a longtime cancer biologist, was thinking about major milestones in biology.</p>
<p>One of the milestones he pondered was the 1970s-era discovery that cells trigger their own deaths for the greater good of the organism. Apoptosis turns out to be critical for many biological processes, including proper development of all organs and the fine-tuning of our immune systems. That system retains pathogen-recognizing cells but kills off self-recognizing ones, thus preventing autoimmune disease.</p>
<p>“It occurred to me, Well gee, this is the way we want to treat cancer,” said Crabtree, a co-senior author on the study who is the David Korn, MD, Professor in Pathology. “We essentially want to have the same kind of specificity that can eliminate 60 billion cells with no bystanders, so no cell is killed that is not the proper object of the killing mechanism.”</p>


</div>

<div id="main_text">
    <p>Traditional treatments for cancer — namely chemotherapy and radiation — often kill large numbers of healthy cells alongside the cancerous ones. To harness cells’ natural and highly specific self-destruction abilities, the team developed a kind of molecular glue that sticks together two proteins that normally would have nothing to do with one another.</p>
<h3>Flipping the cancer script</h3>
<p>One of these proteins, BCL6, when mutated, drives the blood cancer known as diffuse large cell B-cell lymphoma. This kind of cancer-driving protein is also referred to as an oncogene. In lymphoma, the mutated BCL6 sits on DNA near apoptosis-promoting genes and keeps them switched off, helping the cancer cells retain their signature immortality.</p>
<p>The researchers developed a molecule that tethers BCL6 to a protein known as CDK9, which acts as an enzyme that catalyzes gene activation, in this case, switching on the set of apoptosis genes that BCL6 normally keeps off.</p>
<p>“The idea is, Can you turn a cancer dependency into a cancer-killing signal?” asked <a href="https://profiles.stanford.edu/nathanael-gray">Nathanael Gray</a>, PhD, co-senior author with Crabtree, the Krishnan-Shah Family Professor and a chemical and systems biology professor. “You take something that the cancer is addicted to for its survival and you flip the script and make that be the very thing that kills it.”</p>
<p>This approach — switching something on that is off in cancer cells — stands in contrast to many other kinds of targeted cancer therapies that inhibit specific drivers of cancer, switching off something that is normally on.</p>
<p>“Since oncogenes were discovered, people have been trying to shut them down in cancer,” said Roman Sarott, PhD, a postdoctoral scholar at Stanford Medicine and co-first author on the study. “Instead, we’re trying to use them to turn signaling on that, we hope, will prove beneficial for treatment.”</p>

</div>

<div id="main_text_0">
    <p>When the team tested the molecule in diffuse large cell B-cell lymphoma cells in the lab, they found that it indeed killed the cancer cells with high potency. They also tested the molecule in healthy mice and found no obvious toxic side effects, even though the molecule killed off a specific category of of the animals’ healthy B cells, a kind of immune cell, which also depend on BCL6. They’re now testing the compound in mice with diffuse large B-cell lymphoma to gauge its ability to kill cancer in a living animal.</p>
<p>Because the technique relies on the cells’ natural supply of BCL6 and CDK9 proteins, it seems to be very specific for the lymphoma cells — the BCL6 protein is found only in this kind of lymphoma cell and in one specific kind of B cell. The researchers tested the molecule in 859 different kinds of cancer cells in the lab; the chimeric compound killed only diffuse large cell B-cell lymphoma cells.</p>
<p>And because BCL6 normally acts on 13 different apoptosis-promoting genes, the researchers hope their strategy will avoid the treatment resistance that seems so common in cancer. Cancer is often able to rapidly adapt to therapies that target only one of the disease’s weak spots, and some of these therapies may stop cancer from growing without killing the cells entirely. The research team hopes that by blasting the cells with multiple different cell death signals at once, the cancer will not be able to survive long enough to evolve resistance, although this idea remains to be tested.</p>
<p>“It’s sort of cell death by committee,” said Sai Gourisankar, PhD, a postdoctoral scholar and co-first author on the study. “And once a cancer cell is dead, that’s a terminal state.”</p>
<p>Crabtree and Gray, both members of the <a href="https://med.stanford.edu/cancer.html">Stanford Cancer Institute</a>, are co-founders of a biotech startup, Shenandoah Therapeutics, that aims to further test this molecule and a similar, <a href="https://www.nature.com/articles/s41586-023-06348-2">previously developed molecule</a> in hopes of gathering enough pre-clinical data to support launching clinical trials of the compounds. They also plan to build similar molecules that could target other cancer-driving proteins, including the oncogene Ras, which is a driver of several different kinds of cancer.</p>
<p>The study was funded by the Howard Hughes Medical Institute, the National Institutes of Health (grants CA276167, CA163915, MH126720-01 and 5F31HD103339-03), the Mary Kay Foundation, the Schweitzer Family Fund, the SPARK Translational Research Program at Stanford University and Bio-X at Stanford University.</p>

</div>

</div>


    <div>






<ul>











<li>
    
    <div>
        
            <p>
                Rachel Tompa is a freelance science writer.
            </p>
        
    </div>
</li></ul>
</div>


    
  	  

      
    
      <p>About Stanford Medicine</p>
      <p><a href="https://med.stanford.edu/">Stanford Medicine</a> is an integrated academic health system comprising the <a href="https://med.stanford.edu/school.html">Stanford School of Medicine</a> and adult and pediatric health care delivery systems. Together, they harness the full potential of biomedicine through collaborative research, education and clinical care for patients. For more information, please visit <a href="https://med.stanford.edu/">med.stanford.edu</a>.</p>
    

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hertz-dev, the first open-source base model for conversational audio (199 pts)]]></title>
            <link>https://si.inc/hertz-dev/</link>
            <guid>42036995</guid>
            <pubDate>Sun, 03 Nov 2024 23:30:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://si.inc/hertz-dev/">https://si.inc/hertz-dev/</a>, See on <a href="https://news.ycombinator.com/item?id=42036995">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    <article>
      <p>For the last few months, we at Standard Intelligence have focused on fundamental research on the frontier of audio-only speech generation. We're excited to announce that we're open-sourcing current checkpoints of our full-duplex, audio-only transformer base model, hertz-dev, with a total of 8.5 billion parameters.</p>

      <ul>
        <li><strong>hertz-codec:</strong> a convolutional audio autoencoder that takes mono, 16kHz speech and transforms it into a 8 Hz latent representation at about 1kbps bitrate. The codec at 1kbps outperforms Soundstream and Encodec at 6kbps and is on par with DAC at 8kbps in subjective evaluations, while having lower tokens per second than any popular tokenizer, critical for language modeling. The codec has 5 million encoder parameters and 95 million decoder parameters.</li>
        <li><strong>hertz-vae:</strong> a 1.8 billion parameter transformer decoder which acts as a learned prior for the audio VAE. The model uses a context of 8192 sampled latent representations (17 minutes) and predicts the next encoded audio frame as a mixture of gaussians. 15 bits of quantized information from the next token act as semantic scaffolding to steer the generation in a streamable manner.</li>
        <li><strong>hertz-dev:</strong> a 6.6 billion parameter transformer stack. The primary checkpoint is partially initialized from the weights of a pre-trained language model and then trained for a single epoch on 500B tokens with a 2048-token (4 minute) context length. We're also publishing an ablation of the language model initialization which is similarly trained on 500B tokens.</li>
      </ul>

      <p>Hertz-dev is the first publicly released audio base model of its kind. Base models are uniquely valuable as a research product because they accurately model the distribution of the data that they were trained on, as opposed to models that have had substantial RL tuning done to collapse their generation distributions. This makes base models the best starting point to fine-tune for a large number of different tasks.</p>

      <p>Hertz-dev has a theoretical latency of 65ms and a real-world average latency of 120ms on a RTX 4090. This is about 2x lower latency than any public model in the world—a prerequisite for a model that can interact with you in human-like ways instead of what feels like a delayed, choppy phone call. We're currently training a larger, more advanced version of Hertz, which will use a scaled base model recipe and RL tuning to substantially improve the raw capabilities and final coherence of the model. Hertz-dev is a glimpse at the future of real-time voice interaction, and is the easiest conversational audio model in the world for researchers to fine-tune and build on top of.</p>

      <h2>Sample Generations</h2>
      <p>To demonstrate the audio modeling capabilities of hertz-dev, we sample both one-channel and two-channel generations as well as a live conversation between the model and a human.</p>

      <h3>One-channel generation</h3>
      <div>
          <p><audio controls="">
            <source src="https://publicr2.si.inc/hertz-dev/generations/as_an_assistant.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio></p><p>2 seconds prompt.</p>
        </div>

      <h3>Two-channel generation</h3>
      <div>
          <div>
            <p><audio controls="">
                <source src="https://publicr2.si.inc/hertz-dev/generations/counting.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </p>
          </div>
          <p>22 seconds prompt.</p>
        </div>

      <h3>Interactive generation</h3>
      <div>
          <p><audio controls="">
            <source src="https://publicr2.si.inc/hertz-dev/generations/ai_talk.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio></p><p>9 seconds prompt.</p>
        </div>

      
    </article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why systemd is a problem for embedded Linux (192 pts)]]></title>
            <link>https://kevinboone.me/systemd_embedded.html</link>
            <guid>42036305</guid>
            <pubDate>Sun, 03 Nov 2024 21:42:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kevinboone.me/systemd_embedded.html">https://kevinboone.me/systemd_embedded.html</a>, See on <a href="https://news.ycombinator.com/item?id=42036305">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">





<p><img src="https://kevinboone.me/img/tux_win.png"></p>
<p>The Internet is full of rants about systemd, and I don’t want this
post to be another one. Many of the complaints people make about it
don’t stand up to much scrutiny, even the technical ones; and many
complaints are not even technical. My particular interest in Linux is
primarily for embedded applications; and there, I suggest, systemd is
creating a potential (technical) problem. In this article I will try to
articulate what the problem is; but I have no solution to offer.</p>
<h2 id="recapping-the-last-ten-years">Recapping the last ten years</h2>
<p>systemd is a set of integrated applications concerned with system
management. It replaces not only the traditional <code>init</code>
process that brings up long-lived processes, but also much of the other
system infrastructure: user session management, device management,
logging, timing, and an increasing number of other functions.</p>
<p>The majority of Linux users are uninterested in the pros and cons of
systemd. A small number are violently opposed to it, and a small number
are violently opposed to those who are opposed to it. Nevertheless, most
mainstream Linux distributions have adopted it after a shorter (Fedora)
or longer (Debian) period of argument.</p>
<p>I think there’s little argument that the main target for systemd is a
general-purpose computer, with a modern, integrated graphical desktop
(Gnome, KDE). systemd does well in systems like this because it can
handle process initialization on demand and in parallel. This
potentially makes boot times faster, and keeps resource usage down,
because it isn’t necessary to start a lot of services that are used only
occasionally. I don’t think these advantages are the main reasons for
systemd’s widespread adoption (see below), but they’re certainly
important.</p>
<h2 id="embedded-linux-the-problem">Embedded Linux: the problem</h2>
<p>Unfortunately, what makes systemd good for general-purpose desktop
applications potentially makes it unsatisfactory for embedded Linux
systems. As an illustration, I’ll show some memory figures from the
Raspberry Pi 3B that’s currently on my workbench. The board is running
the DietPi Linux distribution – probably the best fully-maintained Pi
distribution, if you want a minimal system. Although DietPi uses systemd
(it has little alternative, as I’ll explain later) it doesn’t
necessarily use the full set of components. In fact, a minimal
installation of DietPi, for console operation, installs only the systemd
service manager (the ‘init’ process), the log daemon, and the
<code>udev</code> daemon.</p>
<p>This is the resource usage, as reported by <code>top</code>, for the
systemd init process (alone) on the Raspberry Pi 3B.</p>
<pre><code>    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM
      1 root      20   0  168144  11404   8592 S   0.0   0.3</code></pre>
<p>systemd init is not a particularly greedy user of memory by
contemporary standards – its virtual address space is 168Mb, but only
~8Mb is currently mapped to RAM. That’s about 0.3% of the Pi 3’s 4Gb of
RAM. But here’s the same figures for SystemV <code>init</code>, on
exactly the same hardware:</p>
<pre><code>  PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM
    1 root      20   0    2744   1532   1436 S   0.0   0.2</code></pre>
<p>It’s <em>much</em> smaller. Just to be clear – I got the systemd
implementation and the SystemV init implementation from the same Debian
ARM binary repository. I haven’t used fiddly compiler optimizations or
anything like that, to bias the resource usage figures.</p>
<p>Now let’s look at the systemd logging daemon,
<code>systemd-journald</code>.</p>
<pre><code>  PID USER     PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
    147 root   20   0   41508   7744   6652 S   0.0   0.2   0:12.05 systemd-jour+</code></pre>
<p>Again, it’s a small, but noticeable, user of the 4Gb RAM. And, for
comparison, these are the figures from my own <a href="https://github.com/kevinboone/syslogd-lite">syslogd-lite</a>,
which I wrote specifically for embedded applications.</p>
<pre><code> PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
  117 root     20   0    1976     72      0 S   0.0   0.0   0:00.00 syslogd</code></pre>
<p>Note that the memory and CPU usage of this logging daemon are both
essentially zero. This is an unfair comparison, in a way, because I
wrote syslogd-lite specifically to minimize resource usage, and it has
few features. But it shows that it’s plausible to write utilities
that<br>
target embedded systems specifically, and sometimes it’s necessary.
systemd does not do this, and was never intended to. Running a
general-purpose software set like systemd on minimal, embedded hardware
can’t be expected to be effective.</p>
<p>With care, though, a minimal installation of systemd <em>does</em>
run on a low-resource ARM board like the Pi 3. In fact, it will run on a
board with 1Gb RAM, perhaps even lower. But, as the RAM decreases, the
proportion of it occupied by systemd increases.</p>
<p>What’s less obvious is the effect on boot time of the use of systemd
init. Proponents of systemd argue – correctly, I think – that it
decreases boot time in the kinds of system for which it was designed.
But on my Pi 3 it increases boot time quite noticeably. That is, the
systemd ‘init’ process takes an extra half-second or so to start,
compared to SystemV init. Half a second isn’t much, but in an embedded
application I actually care about boot time. I couldn’t care less on my
desktop computers, because they spend most of their lives suspended. I
rarely actually reboot them.</p>
<p>The extra start-up time of the init process is the result, I guess,
of the additional size and complexity of the systemd executable. It’s
about 200kB in size itself, and dynamically links 26 other libraries.
SystemV init, on the same hardware, is 40kB, and links only the standard
C library. The additional complexity of systemd is not wasted: it’s
needed for the additional functionality that systemd offers, in its
intended environment. But this functionality is mostly not needed in an
embedded application, so the additional complexity of systemd is a cost
without a benefit.</p>
<p>I’ve found that most of the services that systemd replaces have an
alternative that is smaller, and faster to start, in an embedded
environment. Unfortunately, some of these services don’t <em>have</em>
an alternative any more.</p>
<h2 id="so-what">So what?</h2>
<p>I’m not obliged to run systemd on my Raspberry Pi systems and, in
fact, usually I do not. I build my own Linux installation, using
binaries that I cherry-pick from the Debian repositories, and code I
write myself. Most of the binaries work without systemd. Some complain
about not finding systemd, but work anyway. Some things don’t work
without systemd: the Gnome display manager, for example, as it is built
for the standard Raspberry Pi, won’t work. It can be made to work, but
you have to build it from source. How long it will continue to work,
even if built from source, is open to question. But I’m not going to be
running Gnome on an embedded Linux board, so I don’t see this as a
problem for my applications.</p>
<p>The more fundamental problem is that <em>the people who most like
systemd are distribution managers</em>. Sure, there are administrators
who like it, and defend it vigorously; but most end users and
administrators don’t really care. But for the maintainers of mainstream
Linux distributions, systemd is like Christmas. systemd works reasonably
well for a whole range of usage scenarios and, best of all, it all comes
in one bundle. So a distribution manager doesn’t have to maintain,
integrate, and support a whole bunch of system utilities from different
sources – systemd provides everything in one huge build.</p>
<p>There are just a few Linux distributions that don’t use systemd, and
they are not widely used. Maintaining them is difficult, with just a
handful of volunteers. All the large, commercial Linux providers have
moved to systemd.</p>
<p>Only Gentoo and its derivatives (so far as I know) make systemd
optional, with fully-supported alternatives. And even Gentoo can’t be
made 100% free of systemd – not in practice.</p>
<h2 id="is-it-even-practical-to-avoid-systemd-any-more">Is it even
practical to avoid systemd any more?</h2>
<p>Take, for example, the <code>udev</code> daemon. This service
monitors the kernel, and makes configuration changes when devices are
added and removed. It’s not an essential part of Linux, but it removes
the need for a whole heap of manual configuration.</p>
<p><em>At the time of writing there is no fully-featured
<code>udev</code> implementation outside of systemd</em>. The original
<code>udev</code> code was absorbed into the systemd project about ten
years ago. The Gentoo alternative <code>eudev</code> is no longer fully
maintained. At present, if your Linux distribution requires
<code>udev</code>, you’re forced to use the version from systemd. This
version can be used without the rest of systemd, but it nevertheless
does systemd operations. In particular, it tries to communicate with
systemd over DBus. This communication (so far) fails gracefully if
systemd is not present, but it’s not clear how long this will continue
to be the case.</p>
<p>systemd is a tightly-coupled set of services. I don’t think a design
goal of the systemd maintainers is to make its components modular.
<code>udev</code> doesn’t need to talk to systemd but, clearly, there’s
some benefit to its doing so in a systemd installation. I understand
that there was a kind of ‘gentlemen’s agreement’ between the systemd
maintainers and the Gentoo maintainers, to keep <code>udev</code>
independent of the rest of systemd. I think we can see that this
agreement has broken down a little already; I suspect it will break down
more, if the systemd folks think that tighter integration will make
systemd work better.</p>
<p>Many parts of systemd continue – for now – to have non-systemd
alternatives. For example, systemd has a clock synchronizer
<code>systemd_systemtimed</code>. The systemd maintainers are perfectly
honest that this software lacks features that exist in alternatives like
Chrony and OpenNTPD, and it’s less accurate. <code>systemd_timed</code>
is included because it’s fast to start, and satisfactory for
applications where exact time synchronization is not critical.</p>
<p>At present, Chrony remains widely used, even in some distributions
(like Fedora) that use systemd. But with systemd becoming ubiquitous,
what motivation will there be to maintain non-systemd alternatives?
These alternatives could fall into disrepair, even though some are
superior to the systemd utilities – and even the systemd maintainers
admit this.</p>
<p>Similarly, systemd has a DHCP client. It isn’t the only DHCP client
that is available, but my concern is that one day it might be. In my
tests, I’ve found that the systemd components are larger, and slower to
start, than the traditional alternatives (where they still exist).
Again, this isn’t a criticism of systemd itself – I’m testing them in an
environment they were not designed for.</p>
<h2 id="so-where-does-that-leave-embedded-linux">So where does that
leave embedded Linux?</h2>
<p>I’ve found that many systemd components are less effective in an
embedded environment than the traditional alternatives. I’ve shown some
illustrative examples in this article, but I really don’t think there’s
much controversy here: this simply isn’t the environment that systemd
was designed for. But it’s getting increasingly difficult to find a
mainstream Linux distribution that doesn’t use systemd – even Raspberry
Pi distributions use it. As systemd absorbs more functionality into
itself, there’s going to be little motivation to maintain alternatives.
After all, if everybody uses systemd, what motivation is there to
support anything else? My concern is that we’re moving towards a future
where Linux is inconceivable without systemd. That will be a problem for
those environments where systemd really doesn’t shine.</p>
<p>I wish I knew the solution to this problem. There’s no point
complaining about systemd, because distribution maintainers have grown
to like it too much. And, in any event, well-reasoned, technical
concerns are drowned out by all the ranting and conspiracy theories. All
we can do – if we care – is to continue to use and support Linux
distributions that don’t insist on systemd, and stand ready to develop
or maintain alternatives to those bits of Linux that it absorbs.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Do you need Redis? PostgreSQL does queuing, locking, and pub/sub (189 pts)]]></title>
            <link>https://spin.atomicobject.com/redis-postgresql/</link>
            <guid>42036303</guid>
            <pubDate>Sun, 03 Nov 2024 21:42:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spin.atomicobject.com/redis-postgresql/">https://spin.atomicobject.com/redis-postgresql/</a>, See on <a href="https://news.ycombinator.com/item?id=42036303">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

					<p>There’s a tried-and-true architecture that I’ve seen many times for supporting your web services and applications:</p>
<ul id="condensed">
<li>PostgreSQL for data storage</li>
<li>Redis for coordinating background job queues (and some limited atomic operations)</li>
</ul>
<p>Redis is fantastic, but what if I told you that its most common use cases for this stack could actually be achieved using only PostgreSQL?</p>
<h2 id="use-case-1-job-queuing">Use Case 1: Job Queuing</h2>
<p>Perhaps the most common use of Redis I’ve seen is to coordinate dispatching of jobs from your web service to a pool of background workers. The concept is that you’d like to record the desire for some background job to be performed (perhaps with some input data) and to ensure that only one of your many background workers will pick it up. Redis helps with this because it provides a rich set of atomic operations for its data structures.</p>
<p>But since the introduction of version 9.5, PostgreSQL has a&nbsp;<code>SKIP LOCKED</code>&nbsp;option for the&nbsp;<code>SELECT ... FOR ...</code>statement (<a href="https://www.postgresql.org/docs/9.5/sql-select.html#SQL-FOR-UPDATE-SHARE">here’s the documentation</a>). When this option is specified, PostgreSQL will just ignore any rows that would require waiting for a lock to be released.</p>
<p>Consider this example from the perspective of a background worker:</p>
<pre><code>
BEGIN;

WITH job AS (
  SELECT
    id
  FROM
    jobs
  WHERE
    status = 'pending'
  LIMIT 1
  FOR UPDATE SKIP LOCKED
)
UPDATE
  jobs
SET
  status = 'running'
WHERE
  jobs.id = job.id
RETURNING
  jobs.*;
  
COMMIT;
</code></pre>

<p>By specifying&nbsp;<code>FOR UPDATE SKIP LOCKED</code>, a row-level lock is implicitly acquired for any rows returned from the&nbsp;<code>SELECT</code>. Further, because you specified&nbsp;<code>SKIP LOCKED</code>, there’s no chance of this statement blocking on another transaction. If there’s another job ready to be processed, it will be returned. There’s no concern about multiple workers running this command receiving the same row because of the row-level lock.</p>
<p>The biggest caveat for this technique is that, if you have a large number of workers trying to pull off this queue and a large number of jobs feeding them, they may spend some time stepping through jobs and trying to acquire a lock. In practice, most of the apps I’ve worked on have fewer than a dozen background workers, and the cost is not likely to be significant.</p>
<h2 id="use-case-2-application-locks">Use Case 2: Application Locks</h2>
<p>Let’s imagine that you have a synchronization routine with a third-party service, and you only want one instance of it running for any given user across all server processes. This is another common application I’ve seen for Redis: distributed locking.</p>
<p>PostgreSQL can achieve this as well using its&nbsp;<a href="https://www.postgresql.org/docs/9.2/explicit-locking.html#ADVISORY-LOCKS">advisory locks</a>. Advisory locks allow you to leverage the same locking engine PostgreSQL uses internally for your own application-defined purposes.</p>
<h2 id="use-case-3-pub-sub">Use Case 3: Pub/Sub</h2>
<p>I saved the coolest example for last: pushing events to your active clients. For example, say you need to notify a user that they have a new message available to read. Or perhaps you’d like to stream data to the client as it becomes available. Typically, web sockets are the transport layer for these events while Redis serves as the Pub/Sub engine.</p>
<p>However, since version 9, PostgreSQL also provides this functionality via the&nbsp;<a href="https://www.postgresql.org/docs/9.0/sql-listen.html"><code>LISTEN</code></a>&nbsp;and&nbsp;<a href="https://www.postgresql.org/docs/9.0/sql-notify.html"><code>NOTIFY</code></a>&nbsp;statements. Any PostgreSQL client can subscribe (<code>LISTEN</code>) to a particular message channel, which is just an arbitrary string. When any other client sends a message (<code>NOTIFY</code>) on that channel, all other subscribed clients will be notified. Optionally, a small message can be attached.</p>
<p>If you happen to be using Rails and ActionCable, using PostgreSQL is even supported out of the box.</p>
<h2 id="taking-full-advantage-of-postgresql">Taking Full Advantage of PostgreSQL</h2>
<p>Redis fundamentally fills a different niche than PostgreSQL and excels at things PostgreSQL doesn’t aspire to. Examples include caching data with TTLs and storing and manipulating ephemeral data.</p>
<p>However, PostgreSQL has a lot more capabilities than you may expect when you approach it from the perspective of just another SQL database or some mysterious entity that lives behind your ORM.</p>
<p>There’s a good chance that the things you’re using Redis for may actually be good tasks for PostgreSQL as well. It may be a worthy tradeoff to skip Redis and save on the operational costs and development complexity of relying on multiple data services.</p>

					



				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Tinder, but to Decide What to Eat (191 pts)]]></title>
            <link>https://whatdinner.com/</link>
            <guid>42036041</guid>
            <pubDate>Sun, 03 Nov 2024 20:56:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://whatdinner.com/">https://whatdinner.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42036041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <div> <div> <h2>
Swipe together - dine together
</h2>  <div> <p>
WhatDinner makes it easy for couples to <b>decide</b> on a meal <b>together</b>.
</p> <p>
Our simple <b>meal planning</b> interface is designed for couples who enjoy cooking dinner as a team.
</p> <p><a href="https://apps.apple.com/us/app/meal-planner-dinner-ideas/id6451110287"> <img src="https://whatdinner.com/images/appstore.svg"> </a> </p> </div> </div> <p><img src="https://whatdinner.com/images/screenshot.webp" alt="Hero image"></p> </div> <div> <div> <p><img src="https://whatdinner.com/images/frequency_screenshot.webp"> </p> <div> <h3>Recipe Manager</h3> <p>Manage your recipes and see at a glance how often you want to prepare them.</p> </div> </div> <div> <div> <h3>Frequency</h3> <p>Decide how often you'd like to prepare your own recipe. We respect and integrate your preferences for each day.</p> </div> <p><img src="https://whatdinner.com/images/recipes_screenshot.webp"> </p> </div> </div> <!--<div class="py-24 shadow">--> <!--  <div class="max-w-4xl mx-auto gap-2">--> <!--    <div class="pb-8 text-center">--> <!--      <h2 class="text-3xl text-comfortaa ">Decide Together</h2>--> <!--    </div>--> <!--    <div class="grid grid-cols-2">--> <!--      <div class="grid grid-cols-2 gap-4">--> <!--        <video width="886" height="1920" class="max-h-[500px]" autoplay muted>--> <!--          <source src="/videos/new_video.mp4" type="video/mp4">--> <!--          Your browser does not support the video tag.--> <!--        </video>--> <!--        <div>--> <!--          This works--> <!--        </div>--> <!--      </div>--> <!--    </div>--> <!--  </div>--> <!--</div>--> <div> <h2>
Start Deciding <br> Together
</h2> <div> <p><a href="https://apps.apple.com/us/app/meal-planner-dinner-ideas/id6451110287"> <img src="https://whatdinner.com/images/appstore.svg"> </a> </p> </div> </div>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hacker News Data Map [180MB] (176 pts)]]></title>
            <link>https://lmcinnes.github.io/datamapplot_examples/hackernews/</link>
            <guid>42035981</guid>
            <pubDate>Sun, 03 Nov 2024 20:45:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lmcinnes.github.io/datamapplot_examples/hackernews/">https://lmcinnes.github.io/datamapplot_examples/hackernews/</a>, See on <a href="https://news.ycombinator.com/item?id=42035981">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="title-container">
      <p><span>
        Hackernews Data Map
      </span>
      <br>
      <span>
        A Map of stories on Hackernews using UMAP and nomic-embed
      </span></p>
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Project Sid: Many-agent simulations toward AI civilization (345 pts)]]></title>
            <link>https://github.com/altera-al/project-sid</link>
            <guid>42035319</guid>
            <pubDate>Sun, 03 Nov 2024 19:09:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/altera-al/project-sid">https://github.com/altera-al/project-sid</a>, See on <a href="https://news.ycombinator.com/item?id=42035319">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><details open="">
  <summary>
    
    <span aria-label="Video description projectSidVideo.mp4">projectSidVideo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/111396834/382341315-a288265d-03ac-4d7d-b803-b74066267f26.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2Njk3MDMsIm5iZiI6MTczMDY2OTQwMywicGF0aCI6Ii8xMTEzOTY4MzQvMzgyMzQxMzE1LWEyODgyNjVkLTAzYWMtNGQ3ZC1iODAzLWI3NDA2NjI2N2YyNi5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMTAzJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTEwM1QyMTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT02NGM2YzJiNmFkOTYwMDM0Mzg5YTAyNTE0ZjZiMWMzN2UyZjBmZjg4YmIxZmRmNGJjNDRjOTgwMWFmZjM3ODAwJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.hbGHlPwv3Z1lEvS_fBAVSiCpEuPu0ZfI9L5-vJyZ2Co" data-canonical-src="https://private-user-images.githubusercontent.com/111396834/382341315-a288265d-03ac-4d7d-b803-b74066267f26.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2Njk3MDMsIm5iZiI6MTczMDY2OTQwMywicGF0aCI6Ii8xMTEzOTY4MzQvMzgyMzQxMzE1LWEyODgyNjVkLTAzYWMtNGQ3ZC1iODAzLWI3NDA2NjI2N2YyNi5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMTAzJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTEwM1QyMTMwMDNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT02NGM2YzJiNmFkOTYwMDM0Mzg5YTAyNTE0ZjZiMWMzN2UyZjBmZjg4YmIxZmRmNGJjNDRjOTgwMWFmZjM3ODAwJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.hbGHlPwv3Z1lEvS_fBAVSiCpEuPu0ZfI9L5-vJyZ2Co" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Project Sid: Many-agent simulations toward AI civilization</h2><a id="user-content-project-sid-many-agent-simulations-toward-ai-civilization" aria-label="Permalink: Project Sid: Many-agent simulations toward AI civilization" href="#project-sid-many-agent-simulations-toward-ai-civilization"></a></p>
<p dir="auto">This repository contains our technical report: "Project Sid: Many-agent simulations toward AI civilization"</p>
<p dir="auto">To appear on arXiv.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Abstract</h2><a id="user-content-abstract" aria-label="Permalink: Abstract" href="#abstract"></a></p>
<p dir="auto">AI agents have been evaluated in isolation or within small groups, where interactions remain limited in scope and complexity. Large-scale simulations involving many autonomous agents—reflecting the full spectrum of civilizational processes—have yet to be explored. Here, we demonstrate how 10 – 1000+ AI agents behave and progress within agent societies. We first introduce the PIANO (Parallel Information Aggregation via Neu- ral Orchestration) architecture, which enables agents to interact with humans and other agents in real-time while maintaining coherence across multiple output streams. We then evaluate agent performance in large- scale simulations using civilizational benchmarks inspired by human history. These simulations, set within a Minecraft environment, reveal that agents are capable of meaningful progress—autonomously developing specialized roles, adhering to and changing collective rules, and engaging in cultural and religious transmis- sion. These preliminary results show that agents can achieve significant milestones towards AI civilizations, opening new avenues for large-scale societal simulations, agentic organizational intelligence, and integrating AI into human civilizations.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/altera-al/project-sid/blob/main/visual_abstract.png"><img src="https://github.com/altera-al/project-sid/raw/main/visual_abstract.png" width="1200"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Paper</h2><a id="user-content-paper" aria-label="Permalink: Paper" href="#paper"></a></p>
<p dir="auto">The paper is available in two locations:</p>
<ul dir="auto">
<li>arXiv: To appear</li>
<li>PDF: <a href="https://github.com/altera-al/project-sid/blob/main/2024-10-31.pdf">2024-10-31.pdf</a> (in this repository)</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[gptel: a simple LLM client for Emacs (142 pts)]]></title>
            <link>https://github.com/karthink/gptel</link>
            <guid>42034675</guid>
            <pubDate>Sun, 03 Nov 2024 17:52:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/karthink/gptel">https://github.com/karthink/gptel</a>, See on <a href="https://news.ycombinator.com/item?id=42034675">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">gptel: A simple LLM client for Emacs</h2><a id="user-content-gptel-a-simple-llm-client-for-emacs" aria-label="Permalink: gptel: A simple LLM client for Emacs" href="#gptel-a-simple-llm-client-for-emacs"></a></p>
<p dir="auto"><a href="https://elpa.nongnu.org/nongnu/gptel.svg" rel="nofollow"><img src="https://camo.githubusercontent.com/4b6f2590c77fa98a45d56fbea0332e8d9844f89a8524ad078621a9c638e6b523/68747470733a2f2f656c70612e6e6f6e676e752e6f72672f6e6f6e676e752f677074656c2e737667" alt="https://elpa.nongnu.org/nongnu/gptel.svg" data-canonical-src="https://elpa.nongnu.org/nongnu/gptel.svg"></a> <a href="https://stable.melpa.org/packages/gptel-badge.svg" rel="nofollow"><img src="https://camo.githubusercontent.com/c595cbac5099d30bb5ce893a701f6bbcd5b99948bca3ce42201938b5ab501845/68747470733a2f2f737461626c652e6d656c70612e6f72672f7061636b616765732f677074656c2d62616467652e737667" alt="https://stable.melpa.org/packages/gptel-badge.svg" data-canonical-src="https://stable.melpa.org/packages/gptel-badge.svg"></a> <a href="https://melpa.org/#/gptel" rel="nofollow"><img src="https://camo.githubusercontent.com/c607e4545b94060506a5b0f88e06c62839e09a256a49e8ed68780706a411fa99/68747470733a2f2f6d656c70612e6f72672f7061636b616765732f677074656c2d62616467652e737667" alt="https://melpa.org/packages/gptel-badge.svg" data-canonical-src="https://melpa.org/packages/gptel-badge.svg"></a></p>
<p dir="auto">gptel is a simple Large Language Model chat client for Emacs, with support for multiple models and backends.  It works in the spirit of Emacs, available at any time and uniformly in any buffer.</p>

<p dir="auto"><b>General usage</b>: (<a href="https://www.youtube.com/watch?v=bsRnh_brggM" rel="nofollow">YouTube Demo</a>)</p>
<details open="">
  <summary>
    
    <span aria-label="Video description intro-demo.mp4">intro-demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/8607532/230516812-86510a09-a2fb-4cbd-b53f-cc2522d05a13.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzIzMDUxNjgxMi04NjUxMGEwOS1hMmZiLTRjYmQtYjUzZi1jYzI1MjJkMDVhMTMubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YzNiNzM3ZDA2OGMzZTRiZjI3MDE4NjEzOTEyNTA5ODg4NmUyMTU4MTU3MjgwNTc5MzljNGQ1Mjg1YmRkN2VlYSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.YRahKfgJxYSMeDSRF-nr_fN3GF9L4TTLm5TpM70o8tE" data-canonical-src="https://private-user-images.githubusercontent.com/8607532/230516812-86510a09-a2fb-4cbd-b53f-cc2522d05a13.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzIzMDUxNjgxMi04NjUxMGEwOS1hMmZiLTRjYmQtYjUzZi1jYzI1MjJkMDVhMTMubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YzNiNzM3ZDA2OGMzZTRiZjI3MDE4NjEzOTEyNTA5ODg4NmUyMTU4MTU3MjgwNTc5MzljNGQ1Mjg1YmRkN2VlYSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.YRahKfgJxYSMeDSRF-nr_fN3GF9L4TTLm5TpM70o8tE" controls="controls" muted="muted">

  </video>
</details>

<details open="">
  <summary>
    
    <span aria-label="Video description intro-demo-2.mp4">intro-demo-2.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/8607532/230516816-ae4a613a-4d01-4073-ad3f-b66fa73c6e45.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzIzMDUxNjgxNi1hZTRhNjEzYS00ZDAxLTQwNzMtYWQzZi1iNjZmYTczYzZlNDUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NzQ4Yjg3OTQyN2ZhYTkwZGFhNjIyNTlkYjk5OWZhZDBlNWUyYjVmNjIyNGQzZWFiZGJmYWM2MTRjODQ5OTcyOSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.wSkWb8mu7PCGMK8s0RJPCNh0A-1_8meHG2hByRGf2Dk" data-canonical-src="https://private-user-images.githubusercontent.com/8607532/230516816-ae4a613a-4d01-4073-ad3f-b66fa73c6e45.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzIzMDUxNjgxNi1hZTRhNjEzYS00ZDAxLTQwNzMtYWQzZi1iNjZmYTczYzZlNDUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NzQ4Yjg3OTQyN2ZhYTkwZGFhNjIyNTlkYjk5OWZhZDBlNWUyYjVmNjIyNGQzZWFiZGJmYWM2MTRjODQ5OTcyOSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.wSkWb8mu7PCGMK8s0RJPCNh0A-1_8meHG2hByRGf2Dk" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><b>Media support</b></p>
<details open="">
  <summary>
    
    <span aria-label="Video description gptel-image-demo-1.mp4">gptel-image-demo-1.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/8607532/375931228-1fd947e1-226b-4be2-bc68-7b22b2e3215f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM3NTkzMTIyOC0xZmQ5NDdlMS0yMjZiLTRiZTItYmM2OC03YjIyYjJlMzIxNWYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MmUwNDAwZjkzNmM5ZTNmM2NhNjE0NDVlNTBiOTIwNWQ1MTNmMzcwOTVkMmIyMjY1Y2QxMDIxZjc3MjBhOWI0ZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.U57pAV_5dvq59O388Fk6DvdZO2dt4bjdxS3ZggIQzmo" data-canonical-src="https://private-user-images.githubusercontent.com/8607532/375931228-1fd947e1-226b-4be2-bc68-7b22b2e3215f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM3NTkzMTIyOC0xZmQ5NDdlMS0yMjZiLTRiZTItYmM2OC03YjIyYjJlMzIxNWYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MmUwNDAwZjkzNmM5ZTNmM2NhNjE0NDVlNTBiOTIwNWQ1MTNmMzcwOTVkMmIyMjY1Y2QxMDIxZjc3MjBhOWI0ZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.U57pAV_5dvq59O388Fk6DvdZO2dt4bjdxS3ZggIQzmo" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><b>Multi-LLM support demo</b>:</p>
<details open="">
  <summary>
    
    <span aria-label="Video description gptel-multi.mp4">gptel-multi.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/8607532/278854024-ae1336c4-5b87-41f2-83e9-e415349d6a43.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzI3ODg1NDAyNC1hZTEzMzZjNC01Yjg3LTQxZjItODNlOS1lNDE1MzQ5ZDZhNDMubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NjQ3MjA1M2U2ZTQ4YTdhYzAzMDQwMGQxMjk0NmNlNGVkZTE0MDZkZjExNTMwZGNmNjdmMTViNTcwY2NlNTAxNCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.lQt2mdFFAu_Dlblp9agyCKtSSZwLSfDrOOW7coqJBhg" data-canonical-src="https://private-user-images.githubusercontent.com/8607532/278854024-ae1336c4-5b87-41f2-83e9-e415349d6a43.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzI3ODg1NDAyNC1hZTEzMzZjNC01Yjg3LTQxZjItODNlOS1lNDE1MzQ5ZDZhNDMubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NjQ3MjA1M2U2ZTQ4YTdhYzAzMDQwMGQxMjk0NmNlNGVkZTE0MDZkZjExNTMwZGNmNjdmMTViNTcwY2NlNTAxNCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.lQt2mdFFAu_Dlblp9agyCKtSSZwLSfDrOOW7coqJBhg" controls="controls" muted="muted">

  </video>
</details>

<ul dir="auto">
  <li>It’s async and fast, streams responses.</li>
  <li>Interact with LLMs from anywhere in Emacs (any buffer, shell, minibuffer, wherever)</li>
  <li>LLM responses are in Markdown or Org markup.</li>
  <li>Supports multiple independent conversations and one-off ad hoc interactions.</li>
  <li>Supports multi-modal models (include images, documents)</li>
  <li>Save chats as regular Markdown/Org/Text files and resume them later.</li>
  <li>You can go back and edit your previous prompts or LLM responses when continuing a conversation. These will be fed back to the model.</li>
  <li>Don’t like gptel’s workflow? Use it to create your own for any supported model/backend with a <a href="https://github.com/karthink/gptel/wiki/Defining-custom-gptel-commands">simple API</a>.</li>
</ul>
<p dir="auto">gptel uses Curl if available, but falls back to url-retrieve to work without external dependencies.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contents</h2><a id="user-content-contents" aria-label="Permalink: Contents" href="#contents"></a></p>
<ul dir="auto">
  <li><a href="#breaking-changes">Breaking changes!</a></li>
  <li><a href="#installation">Installation</a>
    <ul dir="auto">
      <li><a href="#straight">Straight</a></li>
      <li><a href="#manual">Manual</a></li>
      <li><a href="#doom-emacs">Doom Emacs</a></li>
      <li><a href="#spacemacs">Spacemacs</a></li>
    </ul>
  </li>
  <li><a href="#setup">Setup</a>
    <ul dir="auto">
      <li><a href="#chatgpt">ChatGPT</a></li>
      <li><a href="#other-llm-backends">Other LLM backends</a>
        <ul dir="auto">
          <li><a href="#azure">Azure</a></li>
          <li><a href="#gpt4all">GPT4All</a></li>
          <li><a href="#ollama">Ollama</a></li>
          <li><a href="#gemini">Gemini</a></li>
          <li><a href="#llamacpp-or-llamafile">Llama.cpp or Llamafile</a></li>
          <li><a href="#kagi-fastgpt--summarizer">Kagi (FastGPT &amp; Summarizer)</a></li>
          <li><a href="#togetherai">together.ai</a></li>
          <li><a href="#anyscale">Anyscale</a></li>
          <li><a href="#perplexity">Perplexity</a></li>
          <li><a href="#anthropic-claude">Anthropic (Claude)</a></li>
          <li><a href="#groq">Groq</a></li>
          <li><a href="#openrouter">OpenRouter</a></li>
          <li><a href="#privategpt">PrivateGPT</a></li>
          <li><a href="#deepseek">DeepSeek</a></li>
          <li><a href="#cerebras">Cerebras</a></li>
          <li><a href="#github-models">Github Models</a></li>
          <li><a href="#novita-ai">Novita AI</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#usage">Usage</a>
    <ul dir="auto">
      <li><a href="#in-any-buffer">In any buffer:</a></li>
      <li><a href="#in-a-dedicated-chat-buffer">In a dedicated chat buffer:</a>
        <ul dir="auto">
          <li><a href="#including-media-images-documents-with-requests">Including media (images, documents) with requests</a></li>
          <li><a href="#save-and-restore-your-chat-sessions">Save and restore your chat sessions</a></li>
        </ul>
      </li>
      <li><a href="#include-more-context-with-requests">Include more context with requests</a></li>
      <li><a href="#rewrite-refactor-or-fill-in-a-region">Rewrite, refactor or fill in a region</a></li>
      <li><a href="#extra-org-mode-conveniences">Extra Org mode conveniences</a></li>
    </ul>
  </li>
  <li><a href="#faq">FAQ</a>
    <ul dir="auto">
      <li><a href="#i-want-the-window-to-scroll-automatically-as-the-response-is-inserted">I want the window to scroll automatically as the response is inserted</a></li>
      <li><a href="#i-want-the-cursor-to-move-to-the-next-prompt-after-the-response-is-inserted">I want the cursor to move to the next prompt after the response is inserted</a></li>
      <li><a href="#i-want-to-change-the-formatting-of-the-prompt-and-llm-response">I want to change the formatting of the prompt and LLM response</a></li>
      <li><a href="#i-want-the-transient-menu-options-to-be-saved-so-i-only-need-to-set-them-once">I want the transient menu options to be saved so I only need to set them once</a></li>
      <li><a href="#i-want-to-use-gptel-in-a-way-thats-not-supported-by-gptel-send-or-the-options-menu">I want to use gptel in a way that’s not supported by <code>gptel-send</code> or the options menu</a></li>
      <li><a href="#doom-emacs-sending-a-query-from-the-gptel-menu-fails-because-of-a-key-conflict-with-org-mode">(Doom Emacs) Sending a query from the gptel menu fails because of a key conflict with Org mode</a></li>
      <li><a href="#chatgpt-i-get-the-error-http2-429-you-exceeded-your-current-quota">(ChatGPT) I get the error “(HTTP/2 429) You exceeded your current quota”</a></li>
      <li><a href="#why-another-llm-client">Why another LLM client?</a></li>
    </ul>
  </li>
  <li><a href="#additional-configuration">Additional Configuration</a></li>
  <li><a href="#alternatives">Alternatives</a>
    <ul dir="auto">
      <li><a href="#packages-using-gptel">Packages using gptel</a></li>
    </ul>
  </li>
  <li><a href="#acknowledgments">Acknowledgments</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Breaking changes!</h2><a id="user-content-breaking-changes" aria-label="Permalink: Breaking changes!" href="#breaking-changes"></a></p>
<ul dir="auto">
  <li><code>gptel-model</code> is now expected to be a symbol, not a string.  Please update your configuration.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">gptel can be installed in Emacs out of the box with <code>M-x package-install</code> ⏎ <code>gptel</code>.  This installs the latest commit.</p>
<p dir="auto">If you want the stable version instead, add NonGNU-devel ELPA or MELPA-stable to your list of package sources (<code>package-archives</code>), then install gptel with <code>M-x package-install⏎</code> <code>gptel</code> from these sources.</p>
<p dir="auto">(Optional: Install <code>markdown-mode</code>.)</p>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Straight</h4><a id="user-content-straight" aria-label="Permalink: Straight" href="#straight"></a></p>
</summary>
<div dir="auto" data-snippet-clipboard-copy-content="(straight-use-package 'gptel)"><pre>(<span>straight-use-package</span> <span>'gptel</span>)</pre></div>
<p dir="auto">Installing the <code>markdown-mode</code> package is optional.</p>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Manual</h4><a id="user-content-manual" aria-label="Permalink: Manual" href="#manual"></a></p>
</summary>
<p dir="auto">Clone or download this repository and run <code>M-x package-install-file⏎</code> on the repository directory.</p>
<p dir="auto">Installing the <code>markdown-mode</code> package is optional.</p>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Doom Emacs</h4><a id="user-content-doom-emacs" aria-label="Permalink: Doom Emacs" href="#doom-emacs"></a></p>
</summary>
<p dir="auto">In <code>packages.el</code></p>

<p dir="auto">In <code>config.el</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="(use-package! gptel
 :config
 (setq! gptel-api-key &quot;your key&quot;))"><pre>(use-package! gptel
 <span>:config</span>
 (setq! gptel-api-key <span><span>"</span>your key<span>"</span></span>))</pre></div>
<p dir="auto">“your key” can be the API key itself, or (safer) a function that returns the key.  Setting <code>gptel-api-key</code> is optional, you will be asked for a key if it’s not found.</p>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Spacemacs</h4><a id="user-content-spacemacs" aria-label="Permalink: Spacemacs" href="#spacemacs"></a></p>
</summary>
<p dir="auto">In your <code>.spacemacs</code> file, add <code>llm-client</code> to <code>dotspacemacs-configuration-layers</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="(llm-client :variables
            llm-client-enable-gptel t)"><pre>(llm-client <span>:variables</span>
            llm-client-enable-gptel <span>t</span>)</pre></div>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">ChatGPT</h3><a id="user-content-chatgpt" aria-label="Permalink: ChatGPT" href="#chatgpt"></a></p>
<p dir="auto">Procure an <a href="https://platform.openai.com/account/api-keys" rel="nofollow">OpenAI API key</a>.</p>
<p dir="auto">Optional: Set <code>gptel-api-key</code> to the key. Alternatively, you may choose a more secure method such as:</p>
<ul dir="auto">
  <li>Storing in <code>~/.authinfo</code>. By default, “api.openai.com” is used as HOST and “apikey” as USER.
    <pre lang="authinfo">machine api.openai.com login apikey password TOKEN
    </pre>
  </li>
  <li>Setting it to a function that returns the key.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Other LLM backends</h3><a id="user-content-other-llm-backends" aria-label="Permalink: Other LLM backends" href="#other-llm-backends"></a></p>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Azure</h4><a id="user-content-azure" aria-label="Permalink: Azure" href="#azure"></a></p>
</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content="(gptel-make-azure &quot;Azure-1&quot;             ;Name, whatever you'd like
  :protocol &quot;https&quot;                     ;Optional -- https is the default
  :host &quot;YOUR_RESOURCE_NAME.openai.azure.com&quot;
  :endpoint &quot;/openai/deployments/YOUR_DEPLOYMENT_NAME/chat/completions?api-version=2023-05-15&quot; ;or equivalent
  :stream t                             ;Enable streaming responses
  :key #'gptel-api-key
  :models '(gpt-3.5-turbo gpt-4))"><pre>(gptel-make-azure <span><span>"</span>Azure-1<span>"</span></span>             <span><span>;</span>Name, whatever you'd like</span>
  <span>:protocol</span> <span><span>"</span>https<span>"</span></span>                     <span><span>;</span>Optional -- https is the default</span>
  <span>:host</span> <span><span>"</span>YOUR_RESOURCE_NAME.openai.azure.com<span>"</span></span>
  <span>:endpoint</span> <span><span>"</span>/openai/deployments/YOUR_DEPLOYMENT_NAME/chat/completions?api-version=2023-05-15<span>"</span></span> <span><span>;</span>or equivalent</span>
  <span>:stream</span> <span>t</span>                             <span><span>;</span>Enable streaming responses</span>
  <span>:key</span> <span>#<span>'gptel-api-key</span></span>
  <span>:models</span> '(gpt-3.5-turbo gpt-4))</pre></div>
<p dir="auto">Refer to the documentation of <code>gptel-make-azure</code> to set more parameters.</p>
<p dir="auto">You can pick this backend from the menu when using gptel. (see <a href="#usage">Usage</a>).</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend"></a></p>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model 'gpt-3.5-turbo
 gptel-backend (gptel-make-azure &quot;Azure-1&quot;
                 :protocol &quot;https&quot;
                 :host &quot;YOUR_RESOURCE_NAME.openai.azure.com&quot;
                 :endpoint &quot;/openai/deployments/YOUR_DEPLOYMENT_NAME/chat/completions?api-version=2023-05-15&quot;
                 :stream t
                 :key #'gptel-api-key
                 :models '(gpt-3.5-turbo gpt-4)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model <span>'gpt-3</span>.5-turbo
 gptel-backend (gptel-make-azure <span><span>"</span>Azure-1<span>"</span></span>
                 <span>:protocol</span> <span><span>"</span>https<span>"</span></span>
                 <span>:host</span> <span><span>"</span>YOUR_RESOURCE_NAME.openai.azure.com<span>"</span></span>
                 <span>:endpoint</span> <span><span>"</span>/openai/deployments/YOUR_DEPLOYMENT_NAME/chat/completions?api-version=2023-05-15<span>"</span></span>
                 <span>:stream</span> <span>t</span>
                 <span>:key</span> <span>#<span>'gptel-api-key</span></span>
                 <span>:models</span> '(gpt-3.5-turbo gpt-4)))</pre></div>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">GPT4All</h4><a id="user-content-gpt4all" aria-label="Permalink: GPT4All" href="#gpt4all"></a></p>
</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content="(gptel-make-gpt4all &quot;GPT4All&quot;           ;Name of your choosing
 :protocol &quot;http&quot;
 :host &quot;localhost:4891&quot;                 ;Where it's running
 :models '(mistral-7b-openorca.Q4_0.gguf)) ;Available models"><pre>(gptel-make-gpt4all <span><span>"</span>GPT4All<span>"</span></span>           <span><span>;</span>Name of your choosing</span>
 <span>:protocol</span> <span><span>"</span>http<span>"</span></span>
 <span>:host</span> <span><span>"</span>localhost:4891<span>"</span></span>                 <span><span>;</span>Where it's running</span>
 <span>:models</span> '(mistral-7b-openorca.Q4_0.gguf)) <span><span>;</span>Available models</span></pre></div>
<p dir="auto">These are the required parameters, refer to the documentation of <code>gptel-make-gpt4all</code> for more.</p>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>).</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-1" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-1"></a></p>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.  Additionally you may want to increase the response token size since GPT4All uses very short (often truncated) responses by default.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-max-tokens 500
 gptel-model 'mistral-7b-openorca.Q4_0.gguf
 gptel-backend (gptel-make-gpt4all &quot;GPT4All&quot;
                 :protocol &quot;http&quot;
                 :host &quot;localhost:4891&quot;
                 :models '(mistral-7b-openorca.Q4_0.gguf)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-max-tokens <span>500</span>
 gptel-model <span>'mistral-7b-openorca</span>.Q4_0.gguf
 gptel-backend (gptel-make-gpt4all <span><span>"</span>GPT4All<span>"</span></span>
                 <span>:protocol</span> <span><span>"</span>http<span>"</span></span>
                 <span>:host</span> <span><span>"</span>localhost:4891<span>"</span></span>
                 <span>:models</span> '(mistral-7b-openorca.Q4_0.gguf)))</pre></div>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Ollama</h4><a id="user-content-ollama" aria-label="Permalink: Ollama" href="#ollama"></a></p>
</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content="(gptel-make-ollama &quot;Ollama&quot;             ;Any name of your choosing
  :host &quot;localhost:11434&quot;               ;Where it's running
  :stream t                             ;Stream responses
  :models '(mistral:latest))          ;List of models"><pre>(gptel-make-ollama <span><span>"</span>Ollama<span>"</span></span>             <span><span>;</span>Any name of your choosing</span>
  <span>:host</span> <span><span>"</span>localhost:11434<span>"</span></span>               <span><span>;</span>Where it's running</span>
  <span>:stream</span> <span>t</span>                             <span><span>;</span>Stream responses</span>
  <span>:models</span> '(mistral:latest))          <span><span>;</span>List of models</span></pre></div>
<p dir="auto">These are the required parameters, refer to the documentation of <code>gptel-make-ollama</code> for more.</p>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>)</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-2" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-2"></a></p>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model 'mistral:latest
 gptel-backend (gptel-make-ollama &quot;Ollama&quot;
                 :host &quot;localhost:11434&quot;
                 :stream t
                 :models '(mistral:latest)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model <span>'mistral:latest</span>
 gptel-backend (gptel-make-ollama <span><span>"</span>Ollama<span>"</span></span>
                 <span>:host</span> <span><span>"</span>localhost:11434<span>"</span></span>
                 <span>:stream</span> <span>t</span>
                 <span>:models</span> '(mistral:latest)))</pre></div>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Gemini</h4><a id="user-content-gemini" aria-label="Permalink: Gemini" href="#gemini"></a></p>
</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; :key can be a function that returns the API key.
(gptel-make-gemini &quot;Gemini&quot; :key &quot;YOUR_GEMINI_API_KEY&quot; :stream t)"><pre><span><span>;</span>; :key can be a function that returns the API key.</span>
(gptel-make-gemini <span><span>"</span>Gemini<span>"</span></span> <span>:key</span> <span><span>"</span>YOUR_GEMINI_API_KEY<span>"</span></span> <span>:stream</span> <span>t</span>)</pre></div>
<p dir="auto">These are the required parameters, refer to the documentation of <code>gptel-make-gemini</code> for more.</p>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>)</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-3" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-3"></a></p>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model 'gemini-pro
 gptel-backend (gptel-make-gemini &quot;Gemini&quot;
                 :key &quot;YOUR_GEMINI_API_KEY&quot;
                 :stream t))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model <span>'gemini-pro</span>
 gptel-backend (gptel-make-gemini <span><span>"</span>Gemini<span>"</span></span>
                 <span>:key</span> <span><span>"</span>YOUR_GEMINI_API_KEY<span>"</span></span>
                 <span>:stream</span> <span>t</span>))</pre></div>
</details>
<details>
<summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Llama.cpp or Llamafile</h4><a id="user-content-llamacpp-or-llamafile" aria-label="Permalink: Llama.cpp or Llamafile" href="#llamacpp-or-llamafile"></a></p>
</summary>
<p dir="auto">(If using a llamafile, run a <a href="https://github.com/Mozilla-Ocho/llamafile#other-example-llamafiles">server llamafile</a> instead of a “command-line llamafile”, and a model that supports text generation.)</p>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Llama.cpp offers an OpenAI compatible API
(gptel-make-openai &quot;llama-cpp&quot;          ;Any name
  :stream t                             ;Stream responses
  :protocol &quot;http&quot;
  :host &quot;localhost:8000&quot;                ;Llama.cpp server location
  :models '(test))                    ;Any names, doesn't matter for Llama"><pre><span><span>;</span>; Llama.cpp offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>"</span>llama-cpp<span>"</span></span>          <span><span>;</span>Any name</span>
  <span>:stream</span> <span>t</span>                             <span><span>;</span>Stream responses</span>
  <span>:protocol</span> <span><span>"</span>http<span>"</span></span>
  <span>:host</span> <span><span>"</span>localhost:8000<span>"</span></span>                <span><span>;</span>Llama.cpp server location</span>
  <span>:models</span> '(test))                    <span><span>;</span>Any names, doesn't matter for Llama</span></pre></div>
<p dir="auto">These are the required parameters, refer to the documentation of <code>gptel-make-openai</code> for more.</p>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>)</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-4" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-4"></a></p>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model   'test
 gptel-backend (gptel-make-openai &quot;llama-cpp&quot;
                 :stream t
                 :protocol &quot;http&quot;
                 :host &quot;localhost:8000&quot;
                 :models '(test)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model   <span>'test</span>
 gptel-backend (gptel-make-openai <span><span>"</span>llama-cpp<span>"</span></span>
                 <span>:stream</span> <span>t</span>
                 <span>:protocol</span> <span><span>"</span>http<span>"</span></span>
                 <span>:host</span> <span><span>"</span>localhost:8000<span>"</span></span>
                 <span>:models</span> '(test)))</pre></div>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Kagi (FastGPT &amp; Summarizer)</h4><a id="user-content-kagi-fastgpt--summarizer" aria-label="Permalink: Kagi (FastGPT &amp; Summarizer)" href="#kagi-fastgpt--summarizer"></a></p>
</summary>
<p dir="auto">Kagi’s FastGPT model and the Universal Summarizer are both supported.  A couple of notes:</p>
<ol dir="auto">
  <li>Universal Summarizer: If there is a URL at point, the summarizer will summarize the contents of the URL.  Otherwise the context sent to the model is the same as always: the buffer text upto point, or the contents of the region if the region is active.</li>
  <li>Kagi models do not support multi-turn conversations, interactions are “one-shot”.  They also do not support streaming responses.</li>
</ol>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content="(gptel-make-kagi &quot;Kagi&quot;                    ;any name
  :key &quot;YOUR_KAGI_API_KEY&quot;)                ;can be a function that returns the key"><pre>(gptel-make-kagi <span><span>"</span>Kagi<span>"</span></span>                    <span><span>;</span>any name</span>
  <span>:key</span> <span><span>"</span>YOUR_KAGI_API_KEY<span>"</span></span>)                <span><span>;</span>can be a function that returns the key</span></pre></div>
<p dir="auto">These are the required parameters, refer to the documentation of <code>gptel-make-kagi</code> for more.</p>
<p dir="auto">You can pick this backend and the model (fastgpt/summarizer) from the transient menu when using gptel.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-5" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-5"></a></p>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model 'fastgpt
 gptel-backend (gptel-make-kagi &quot;Kagi&quot;
                 :key &quot;YOUR_KAGI_API_KEY&quot;))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model <span>'fastgpt</span>
 gptel-backend (gptel-make-kagi <span><span>"</span>Kagi<span>"</span></span>
                 <span>:key</span> <span><span>"</span>YOUR_KAGI_API_KEY<span>"</span></span>))</pre></div>
<p dir="auto">The alternatives to <code>fastgpt</code> include <code>summarize:cecil</code>, <code>summarize:agnes</code>, <code>summarize:daphne</code> and <code>summarize:muriel</code>.  The difference between the summarizer engines is <a href="https://help.kagi.com/kagi/api/summarizer.html#summarization-engines" rel="nofollow">documented here</a>.</p>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">together.ai</h4><a id="user-content-togetherai" aria-label="Permalink: together.ai" href="#togetherai"></a></p>
</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Together.ai offers an OpenAI compatible API
(gptel-make-openai &quot;TogetherAI&quot;         ;Any name you want
  :host &quot;api.together.xyz&quot;
  :key &quot;your-api-key&quot;                   ;can be a function that returns the key
  :stream t
  :models '(;; has many more, check together.ai
            mistralai/Mixtral-8x7B-Instruct-v0.1
            codellama/CodeLlama-13b-Instruct-hf
            codellama/CodeLlama-34b-Instruct-hf))"><pre><span><span>;</span>; Together.ai offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>"</span>TogetherAI<span>"</span></span>         <span><span>;</span>Any name you want</span>
  <span>:host</span> <span><span>"</span>api.together.xyz<span>"</span></span>
  <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>                   <span><span>;</span>can be a function that returns the key</span>
  <span>:stream</span> <span>t</span>
  <span>:models</span> '(<span><span>;</span>; has many more, check together.ai</span>
            mistralai/Mixtral-8x7B-Instruct-v0.1
            codellama/CodeLlama-13b-Instruct-hf
            codellama/CodeLlama-34b-Instruct-hf))</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>)</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-6" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-6"></a></p>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model   'mistralai/Mixtral-8x7B-Instruct-v0.1
 gptel-backend
 (gptel-make-openai &quot;TogetherAI&quot;         
   :host &quot;api.together.xyz&quot;
   :key &quot;your-api-key&quot;                   
   :stream t
   :models '(;; has many more, check together.ai
             mistralai/Mixtral-8x7B-Instruct-v0.1
             codellama/CodeLlama-13b-Instruct-hf
             codellama/CodeLlama-34b-Instruct-hf)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model   <span>'mistralai/Mixtral-8x7B-Instruct-v0</span>.1
 gptel-backend
 (gptel-make-openai <span><span>"</span>TogetherAI<span>"</span></span>         
   <span>:host</span> <span><span>"</span>api.together.xyz<span>"</span></span>
   <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>                   
   <span>:stream</span> <span>t</span>
   <span>:models</span> '(<span><span>;</span>; has many more, check together.ai</span>
             mistralai/Mixtral-8x7B-Instruct-v0.1
             codellama/CodeLlama-13b-Instruct-hf
             codellama/CodeLlama-34b-Instruct-hf)))</pre></div>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Anyscale</h4><a id="user-content-anyscale" aria-label="Permalink: Anyscale" href="#anyscale"></a></p>
</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Anyscale offers an OpenAI compatible API
(gptel-make-openai &quot;Anyscale&quot;           ;Any name you want
  :host &quot;api.endpoints.anyscale.com&quot;
  :key &quot;your-api-key&quot;                   ;can be a function that returns the key
  :models '(;; has many more, check anyscale
            mistralai/Mixtral-8x7B-Instruct-v0.1))"><pre><span><span>;</span>; Anyscale offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>"</span>Anyscale<span>"</span></span>           <span><span>;</span>Any name you want</span>
  <span>:host</span> <span><span>"</span>api.endpoints.anyscale.com<span>"</span></span>
  <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>                   <span><span>;</span>can be a function that returns the key</span>
  <span>:models</span> '(<span><span>;</span>; has many more, check anyscale</span>
            mistralai/Mixtral-8x7B-Instruct-v0.1))</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>)</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-7" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-7"></a></p>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model   'mistralai/Mixtral-8x7B-Instruct-v0.1
 gptel-backend
 (gptel-make-openai &quot;Anyscale&quot;
                 :host &quot;api.endpoints.anyscale.com&quot;
                 :key &quot;your-api-key&quot;
                 :models '(;; has many more, check anyscale
                           mistralai/Mixtral-8x7B-Instruct-v0.1)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model   <span>'mistralai/Mixtral-8x7B-Instruct-v0</span>.1
 gptel-backend
 (gptel-make-openai <span><span>"</span>Anyscale<span>"</span></span>
                 <span>:host</span> <span><span>"</span>api.endpoints.anyscale.com<span>"</span></span>
                 <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>
                 <span>:models</span> '(<span><span>;</span>; has many more, check anyscale</span>
                           mistralai/Mixtral-8x7B-Instruct-v0.1)))</pre></div>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Perplexity</h4><a id="user-content-perplexity" aria-label="Permalink: Perplexity" href="#perplexity"></a></p>
</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Perplexity offers an OpenAI compatible API
(gptel-make-openai &quot;Perplexity&quot;         ;Any name you want
  :host &quot;api.perplexity.ai&quot;
  :key &quot;your-api-key&quot;                   ;can be a function that returns the key
  :endpoint &quot;/chat/completions&quot;
  :stream t
  :models '(;; has many more, check perplexity.ai
            pplx-7b-chat
            pplx-70b-chat
            pplx-7b-online
            pplx-70b-online))"><pre><span><span>;</span>; Perplexity offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>"</span>Perplexity<span>"</span></span>         <span><span>;</span>Any name you want</span>
  <span>:host</span> <span><span>"</span>api.perplexity.ai<span>"</span></span>
  <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>                   <span><span>;</span>can be a function that returns the key</span>
  <span>:endpoint</span> <span><span>"</span>/chat/completions<span>"</span></span>
  <span>:stream</span> <span>t</span>
  <span>:models</span> '(<span><span>;</span>; has many more, check perplexity.ai</span>
            pplx-7b-chat
            pplx-70b-chat
            pplx-7b-online
            pplx-70b-online))</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>)</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-8" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-8"></a></p>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model   'pplx-7b-chat
 gptel-backend
 (gptel-make-openai &quot;Perplexity&quot;
   :host &quot;api.perplexity.ai&quot;
   :key &quot;your-api-key&quot;
   :endpoint &quot;/chat/completions&quot;
   :stream t
   :models '(;; has many more, check perplexity.ai
             pplx-7b-chat
             pplx-70b-chat
             pplx-7b-online
             pplx-70b-online)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model   <span>'pplx-7b-chat</span>
 gptel-backend
 (gptel-make-openai <span><span>"</span>Perplexity<span>"</span></span>
   <span>:host</span> <span><span>"</span>api.perplexity.ai<span>"</span></span>
   <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>
   <span>:endpoint</span> <span><span>"</span>/chat/completions<span>"</span></span>
   <span>:stream</span> <span>t</span>
   <span>:models</span> '(<span><span>;</span>; has many more, check perplexity.ai</span>
             pplx-7b-chat
             pplx-70b-chat
             pplx-7b-online
             pplx-70b-online)))</pre></div>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Anthropic (Claude)</h4><a id="user-content-anthropic-claude" aria-label="Permalink: Anthropic (Claude)" href="#anthropic-claude"></a></p>
</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content="(gptel-make-anthropic &quot;Claude&quot;          ;Any name you want
  :stream t                             ;Streaming responses
  :key &quot;your-api-key&quot;)"><pre>(gptel-make-anthropic <span><span>"</span>Claude<span>"</span></span>          <span><span>;</span>Any name you want</span>
  <span>:stream</span> <span>t</span>                             <span><span>;</span>Streaming responses</span>
  <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>)</pre></div>
<p dir="auto">The <code>:key</code> can be a function that returns the key (more secure).</p>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>).</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-9" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-9"></a></p>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model 'claude-3-sonnet-20240229 ;  &quot;claude-3-opus-20240229&quot; also available
 gptel-backend (gptel-make-anthropic &quot;Claude&quot;
                 :stream t :key &quot;your-api-key&quot;))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model <span>'claude-3-sonnet-20240229</span> <span><span>;</span>  "claude-3-opus-20240229" also available</span>
 gptel-backend (gptel-make-anthropic <span><span>"</span>Claude<span>"</span></span>
                 <span>:stream</span> <span>t</span> <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>))</pre></div>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Groq</h4><a id="user-content-groq" aria-label="Permalink: Groq" href="#groq"></a></p>
</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Groq offers an OpenAI compatible API
(gptel-make-openai &quot;Groq&quot;               ;Any name you want
  :host &quot;api.groq.com&quot;
  :endpoint &quot;/openai/v1/chat/completions&quot;
  :stream t
  :key &quot;your-api-key&quot;                   ;can be a function that returns the key
  :models '(llama-3.1-70b-versatile
            llama-3.1-8b-instant
            llama3-70b-8192
            llama3-8b-8192
            mixtral-8x7b-32768
            gemma-7b-it))"><pre><span><span>;</span>; Groq offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>"</span>Groq<span>"</span></span>               <span><span>;</span>Any name you want</span>
  <span>:host</span> <span><span>"</span>api.groq.com<span>"</span></span>
  <span>:endpoint</span> <span><span>"</span>/openai/v1/chat/completions<span>"</span></span>
  <span>:stream</span> <span>t</span>
  <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>                   <span><span>;</span>can be a function that returns the key</span>
  <span>:models</span> '(llama-3.1-70b-versatile
            llama-3.1-8b-instant
            llama3-70b-8192
            llama3-8b-8192
            mixtral-8x7b-32768
            gemma-7b-it))</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>).  Note that Groq is fast enough that you could easily set <code>:stream nil</code> and still get near-instant responses.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-10" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-10"></a></p>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq gptel-model   'mixtral-8x7b-32768
      gptel-backend
      (gptel-make-openai &quot;Groq&quot;
        :host &quot;api.groq.com&quot;
        :endpoint &quot;/openai/v1/chat/completions&quot;
        :stream t
        :key &quot;your-api-key&quot;
        :models '(llama-3.1-70b-versatile
                  llama-3.1-8b-instant
                  llama3-70b-8192
                  llama3-8b-8192
                  mixtral-8x7b-32768
                  gemma-7b-it)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span> gptel-model   <span>'mixtral-8x7b-32768</span>
      gptel-backend
      (gptel-make-openai <span><span>"</span>Groq<span>"</span></span>
        <span>:host</span> <span><span>"</span>api.groq.com<span>"</span></span>
        <span>:endpoint</span> <span><span>"</span>/openai/v1/chat/completions<span>"</span></span>
        <span>:stream</span> <span>t</span>
        <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>
        <span>:models</span> '(llama-3.1-70b-versatile
                  llama-3.1-8b-instant
                  llama3-70b-8192
                  llama3-8b-8192
                  mixtral-8x7b-32768
                  gemma-7b-it)))</pre></div>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">OpenRouter</h4><a id="user-content-openrouter" aria-label="Permalink: OpenRouter" href="#openrouter"></a></p>
</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OpenRouter offers an OpenAI compatible API
(gptel-make-openai &quot;OpenRouter&quot;               ;Any name you want
  :host &quot;openrouter.ai&quot;
  :endpoint &quot;/api/v1/chat/completions&quot;
  :stream t
  :key &quot;your-api-key&quot;                   ;can be a function that returns the key
  :models '(openai/gpt-3.5-turbo
            mistralai/mixtral-8x7b-instruct
            meta-llama/codellama-34b-instruct
            codellama/codellama-70b-instruct
            google/palm-2-codechat-bison-32k
            google/gemini-pro))
"><pre><span><span>;</span>; OpenRouter offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>"</span>OpenRouter<span>"</span></span>               <span><span>;</span>Any name you want</span>
  <span>:host</span> <span><span>"</span>openrouter.ai<span>"</span></span>
  <span>:endpoint</span> <span><span>"</span>/api/v1/chat/completions<span>"</span></span>
  <span>:stream</span> <span>t</span>
  <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>                   <span><span>;</span>can be a function that returns the key</span>
  <span>:models</span> '(openai/gpt-3.5-turbo
            mistralai/mixtral-8x7b-instruct
            meta-llama/codellama-34b-instruct
            codellama/codellama-70b-instruct
            google/palm-2-codechat-bison-32k
            google/gemini-pro))
</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>).</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-11" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-11"></a></p>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq gptel-model   'mixtral-8x7b-32768
      gptel-backend
      (gptel-make-openai &quot;OpenRouter&quot;               ;Any name you want
        :host &quot;openrouter.ai&quot;
        :endpoint &quot;/api/v1/chat/completions&quot;
        :stream t
        :key &quot;your-api-key&quot;                   ;can be a function that returns the key
        :models '(openai/gpt-3.5-turbo
                  mistralai/mixtral-8x7b-instruct
                  meta-llama/codellama-34b-instruct
                  codellama/codellama-70b-instruct
                  google/palm-2-codechat-bison-32k
                  google/gemini-pro)))
"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span> gptel-model   <span>'mixtral-8x7b-32768</span>
      gptel-backend
      (gptel-make-openai <span><span>"</span>OpenRouter<span>"</span></span>               <span><span>;</span>Any name you want</span>
        <span>:host</span> <span><span>"</span>openrouter.ai<span>"</span></span>
        <span>:endpoint</span> <span><span>"</span>/api/v1/chat/completions<span>"</span></span>
        <span>:stream</span> <span>t</span>
        <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>                   <span><span>;</span>can be a function that returns the key</span>
        <span>:models</span> '(openai/gpt-3.5-turbo
                  mistralai/mixtral-8x7b-instruct
                  meta-llama/codellama-34b-instruct
                  codellama/codellama-70b-instruct
                  google/palm-2-codechat-bison-32k
                  google/gemini-pro)))
</pre></div>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">PrivateGPT</h4><a id="user-content-privategpt" aria-label="Permalink: PrivateGPT" href="#privategpt"></a></p>
</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content="(gptel-make-privategpt &quot;privateGPT&quot;               ;Any name you want
  :protocol &quot;http&quot;
  :host &quot;localhost:8001&quot;
  :stream t
  :context t                            ;Use context provided by embeddings
  :sources t                            ;Return information about source documents
  :models '(private-gpt))
"><pre>(gptel-make-privategpt <span><span>"</span>privateGPT<span>"</span></span>               <span><span>;</span>Any name you want</span>
  <span>:protocol</span> <span><span>"</span>http<span>"</span></span>
  <span>:host</span> <span><span>"</span>localhost:8001<span>"</span></span>
  <span>:stream</span> <span>t</span>
  <span>:context</span> <span>t</span>                            <span><span>;</span>Use context provided by embeddings</span>
  <span>:sources</span> <span>t</span>                            <span><span>;</span>Return information about source documents</span>
  <span>:models</span> '(private-gpt))
</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>).</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-12" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-12"></a></p>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq gptel-model   'private-gpt
      gptel-backend
      (gptel-make-privategpt &quot;privateGPT&quot;               ;Any name you want
        :protocol &quot;http&quot;
        :host &quot;localhost:8001&quot;
        :stream t
        :context t                            ;Use context provided by embeddings
        :sources t                            ;Return information about source documents
        :models '(private-gpt)))
"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span> gptel-model   <span>'private-gpt</span>
      gptel-backend
      (gptel-make-privategpt <span><span>"</span>privateGPT<span>"</span></span>               <span><span>;</span>Any name you want</span>
        <span>:protocol</span> <span><span>"</span>http<span>"</span></span>
        <span>:host</span> <span><span>"</span>localhost:8001<span>"</span></span>
        <span>:stream</span> <span>t</span>
        <span>:context</span> <span>t</span>                            <span><span>;</span>Use context provided by embeddings</span>
        <span>:sources</span> <span>t</span>                            <span><span>;</span>Return information about source documents</span>
        <span>:models</span> '(private-gpt)))
</pre></div>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">DeepSeek</h4><a id="user-content-deepseek" aria-label="Permalink: DeepSeek" href="#deepseek"></a></p>
</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; DeepSeek offers an OpenAI compatible API
(gptel-make-openai &quot;DeepSeek&quot;       ;Any name you want
  :host &quot;api.deepseek.com&quot;
  :endpoint &quot;/chat/completions&quot;
  :stream t
  :key &quot;your-api-key&quot;               ;can be a function that returns the key
  :models '(deepseek-chat deepseek-coder))
"><pre><span><span>;</span>; DeepSeek offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>"</span>DeepSeek<span>"</span></span>       <span><span>;</span>Any name you want</span>
  <span>:host</span> <span><span>"</span>api.deepseek.com<span>"</span></span>
  <span>:endpoint</span> <span><span>"</span>/chat/completions<span>"</span></span>
  <span>:stream</span> <span>t</span>
  <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>               <span><span>;</span>can be a function that returns the key</span>
  <span>:models</span> '(deepseek-chat deepseek-coder))
</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>).</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-13" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-13"></a></p>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq gptel-model   'deepseek-chat
      gptel-backend
      (gptel-make-openai &quot;DeepSeek&quot;     ;Any name you want
        :host &quot;api.deepseek.com&quot;
        :endpoint &quot;/chat/completions&quot;
        :stream t
        :key &quot;your-api-key&quot;             ;can be a function that returns the key
        :models '(deepseek-chat deepseek-coder)))
"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span> gptel-model   <span>'deepseek-chat</span>
      gptel-backend
      (gptel-make-openai <span><span>"</span>DeepSeek<span>"</span></span>     <span><span>;</span>Any name you want</span>
        <span>:host</span> <span><span>"</span>api.deepseek.com<span>"</span></span>
        <span>:endpoint</span> <span><span>"</span>/chat/completions<span>"</span></span>
        <span>:stream</span> <span>t</span>
        <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>             <span><span>;</span>can be a function that returns the key</span>
        <span>:models</span> '(deepseek-chat deepseek-coder)))
</pre></div>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Cerebras</h4><a id="user-content-cerebras" aria-label="Permalink: Cerebras" href="#cerebras"></a></p>
</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Cerebras offers an instant OpenAI compatible API
(gptel-make-openai &quot;Cerebras&quot;
  :host &quot;api.cerebras.ai&quot;
  :endpoint &quot;/v1/chat/completions&quot;
  :stream t                             ;optionally nil as Cerebras is instant AI
  :key &quot;your-api-key&quot;                   ;can be a function that returns the key
  :models '(llama3.1-70b
            llama3.1-8b))"><pre><span><span>;</span>; Cerebras offers an instant OpenAI compatible API</span>
(gptel-make-openai <span><span>"</span>Cerebras<span>"</span></span>
  <span>:host</span> <span><span>"</span>api.cerebras.ai<span>"</span></span>
  <span>:endpoint</span> <span><span>"</span>/v1/chat/completions<span>"</span></span>
  <span>:stream</span> <span>t</span>                             <span><span>;</span>optionally nil as Cerebras is instant AI</span>
  <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>                   <span><span>;</span>can be a function that returns the key</span>
  <span>:models</span> '(llama3.1-70b
            llama3.1-8b))</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>).</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-14" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-14"></a></p>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq gptel-model   'llama3.1-8b
      gptel-backend
      (gptel-make-openai &quot;Cerebras&quot;
        :host &quot;api.cerebras.ai&quot;
        :endpoint &quot;/v1/chat/completions&quot;
        :stream nil
        :key &quot;your-api-key&quot;
        :models '(llama3.1-70b
                  llama3.1-8b)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span> gptel-model   <span>'llama3</span>.1-8b
      gptel-backend
      (gptel-make-openai <span><span>"</span>Cerebras<span>"</span></span>
        <span>:host</span> <span><span>"</span>api.cerebras.ai<span>"</span></span>
        <span>:endpoint</span> <span><span>"</span>/v1/chat/completions<span>"</span></span>
        <span>:stream</span> <span>nil</span>
        <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>
        <span>:models</span> '(llama3.1-70b
                  llama3.1-8b)))</pre></div>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Github Models</h4><a id="user-content-github-models" aria-label="Permalink: Github Models" href="#github-models"></a></p>
</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Github Models offers an OpenAI compatible API
(gptel-make-openai &quot;Github Models&quot; ;Any name you want
  :host &quot;models.inference.ai.azure.com&quot;
  :endpoint &quot;/chat/completions&quot;
  :stream t
  :key &quot;your-github-token&quot;
  :models '(gpt-4o))"><pre><span><span>;</span>; Github Models offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>"</span>Github Models<span>"</span></span> <span><span>;</span>Any name you want</span>
  <span>:host</span> <span><span>"</span>models.inference.ai.azure.com<span>"</span></span>
  <span>:endpoint</span> <span><span>"</span>/chat/completions<span>"</span></span>
  <span>:stream</span> <span>t</span>
  <span>:key</span> <span><span>"</span>your-github-token<span>"</span></span>
  <span>:models</span> '(gpt-4o))</pre></div>
<p dir="auto">For all the available models, check the <a href="https://github.com/marketplace/models">marketplace</a>.</p>
<p dir="auto">You can pick this backend from the menu when using (see <a href="#usage">Usage</a>).</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-15" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-15"></a></p>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq gptel-model  'gpt-4o
      gptel-backend
      (gptel-make-openai &quot;Github Models&quot; ;Any name you want
        :host &quot;models.inference.ai.azure.com&quot;
        :endpoint &quot;/chat/completions&quot;
        :stream t
        :key &quot;your-github-token&quot;
        :models '(gpt-4o))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span> gptel-model  <span>'gpt-4o</span>
      gptel-backend
      (gptel-make-openai <span><span>"</span>Github Models<span>"</span></span> <span><span>;</span>Any name you want</span>
        <span>:host</span> <span><span>"</span>models.inference.ai.azure.com<span>"</span></span>
        <span>:endpoint</span> <span><span>"</span>/chat/completions<span>"</span></span>
        <span>:stream</span> <span>t</span>
        <span>:key</span> <span><span>"</span>your-github-token<span>"</span></span>
        <span>:models</span> '(gpt-4o))</pre></div>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Novita AI</h4><a id="user-content-novita-ai" aria-label="Permalink: Novita AI" href="#novita-ai"></a></p>
</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Novita AI offers an OpenAI compatible API
(gptel-make-openai &quot;NovitaAI&quot;         ;Any name you want
  :host &quot;api.novita.ai&quot;
  :endpoint &quot;/v3/openai&quot;
  :key &quot;your-api-key&quot;                   ;can be a function that returns the key
  :stream t
  :models '(;; has many more, check https://novita.ai/llm-api
            gryphe/mythomax-l2-13b
            meta-llama/llama-3-70b-instruct
            meta-llama/llama-3.1-70b-instruct))"><pre><span><span>;</span>; Novita AI offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>"</span>NovitaAI<span>"</span></span>         <span><span>;</span>Any name you want</span>
  <span>:host</span> <span><span>"</span>api.novita.ai<span>"</span></span>
  <span>:endpoint</span> <span><span>"</span>/v3/openai<span>"</span></span>
  <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>                   <span><span>;</span>can be a function that returns the key</span>
  <span>:stream</span> <span>t</span>
  <span>:models</span> '(<span><span>;</span>; has many more, check https://novita.ai/llm-api</span>
            gryphe/mythomax-l2-13b
            meta-llama/llama-3-70b-instruct
            meta-llama/llama-3.1-70b-instruct))</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>)</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-16" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-16"></a></p>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model   'gryphe/mythomax-l2-13b
 gptel-backend
 (gptel-make-openai &quot;NovitaAI&quot;         
   :host &quot;api.novita.ai&quot;
   :endpoint &quot;/v3/openai&quot;
   :key &quot;your-api-key&quot;                   
   :stream t
   :models '(;; has many more, check https://novita.ai/llm-api
             mistralai/Mixtral-8x7B-Instruct-v0.1
             meta-llama/llama-3-70b-instruct
             meta-llama/llama-3.1-70b-instruct)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model   <span>'gryphe/mythomax-l2-13b</span>
 gptel-backend
 (gptel-make-openai <span><span>"</span>NovitaAI<span>"</span></span>         
   <span>:host</span> <span><span>"</span>api.novita.ai<span>"</span></span>
   <span>:endpoint</span> <span><span>"</span>/v3/openai<span>"</span></span>
   <span>:key</span> <span><span>"</span>your-api-key<span>"</span></span>                   
   <span>:stream</span> <span>t</span>
   <span>:models</span> '(<span><span>;</span>; has many more, check https://novita.ai/llm-api</span>
             mistralai/Mixtral-8x7B-Instruct-v0.1
             meta-llama/llama-3-70b-instruct
             meta-llama/llama-3.1-70b-instruct)))</pre></div>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">(There is also a <a href="https://www.youtube.com/watch?v=bsRnh_brggM" rel="nofollow">video demo</a> showing various uses of gptel.)</p>
<markdown-accessiblity-table><table>
  <tbody><tr><th><b>To send queries</b></th><th>Description</th></tr>
  <tr><td><code>gptel-send</code></td><td>Send all text up to <code>(point)</code>, or the selection if region is active.  Works anywhere in Emacs.</td></tr>
  <tr><td><code>gptel</code></td><td>Create a new dedicated chat buffer.  Not required to use gptel.</td></tr>
  <tr><th><b>To Set options</b></th><th></th></tr>
  <tr><td><code>C-u</code> <code>gptel-send</code></td><td>Transient menu for preferences, input/output redirection etc.</td></tr>
  <tr><td><code>gptel-menu</code></td><td><i>(Same)</i></td></tr>
  <tr><th><b>To add context</b></th><th></th></tr>
  <tr><td><code>gptel-add</code></td><td>Add/remove a region or buffer to gptel’s context.  In Dired, add/remove marked files.</td></tr>
  <tr><td><code>gptel-add-file</code></td><td>Add a file (text or supported media type) to gptel’s context.  Also available from the transient menu.</td></tr>
  <tr><th><b>Org mode bonuses</b></th><th></th></tr>
  <tr><td><code>gptel-org-set-topic</code></td><td>Limit conversation context to an Org heading.  (For branching conversations see below.)</td></tr>
  <tr><td><code>gptel-org-set-properties</code></td><td>Write gptel configuration as Org properties, for per-heading chat configuration.</td></tr>
</tbody></table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">In any buffer:</h3><a id="user-content-in-any-buffer" aria-label="Permalink: In any buffer:" href="#in-any-buffer"></a></p>
<ol dir="auto">
  <li>Call <code>M-x gptel-send</code> to send the text up to the cursor. The response will be inserted below.  Continue the conversation by typing below the response.</li>
  <li>If a region is selected, the conversation will be limited to its contents.</li>
  <li>Call <code>M-x gptel-send</code> with a prefix argument (<code>C-u</code>)
    <ul dir="auto">
      <li>to set chat parameters (GPT model, system message etc) for this buffer,</li>
      <li>include quick instructions for the next request only,</li>
      <li>to add additional context – regions, buffers or files – to gptel,</li>
      <li>to read the prompt from or redirect the response elsewhere,</li>
      <li>or to replace the prompt with the response.</li>
    </ul>
  </li>
</ol>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/342004903-3562a6e2-7a5c-4f7e-8e57-bf3c11589c73.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM0MjAwNDkwMy0zNTYyYTZlMi03YTVjLTRmN2UtOGU1Ny1iZjNjMTE1ODljNzMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NTVkNWYzZjk3MTM1YzNlYTA4YzYwZWMwNmJkZDU1N2U5ZjRlYTA5ZTYwYmJmYmJkZTVmNjlkMDIxZjM4ZjFhNCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.DkcoQKgXo3SQvkCa6nK5wL70J28NOYBpImDMycLLNwo"><img src="https://private-user-images.githubusercontent.com/8607532/342004903-3562a6e2-7a5c-4f7e-8e57-bf3c11589c73.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM0MjAwNDkwMy0zNTYyYTZlMi03YTVjLTRmN2UtOGU1Ny1iZjNjMTE1ODljNzMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NTVkNWYzZjk3MTM1YzNlYTA4YzYwZWMwNmJkZDU1N2U5ZjRlYTA5ZTYwYmJmYmJkZTVmNjlkMDIxZjM4ZjFhNCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.DkcoQKgXo3SQvkCa6nK5wL70J28NOYBpImDMycLLNwo" alt="Image showing gptel's menu with some of the available query options."></a>
<p dir="auto"><h3 tabindex="-1" dir="auto">In a dedicated chat buffer:</h3><a id="user-content-in-a-dedicated-chat-buffer" aria-label="Permalink: In a dedicated chat buffer:" href="#in-a-dedicated-chat-buffer"></a></p>
<ol dir="auto">
  <li>Run <code>M-x gptel</code> to start or switch to the chat buffer. It will ask you for the key if you skipped the previous step. Run it with a prefix-arg (<code>C-u M-x gptel</code>) to start a new session.</li>
  <li>In the gptel buffer, send your prompt with <code>M-x gptel-send</code>, bound to <code>C-c RET</code>.</li>
  <li>Set chat parameters (LLM provider, model, directives etc) for the session by calling <code>gptel-send</code> with a prefix argument (<code>C-u C-c RET</code>):</li>
</ol>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/342005178-eb4867e5-30ac-455f-999f-e17123afb810.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM0MjAwNTE3OC1lYjQ4NjdlNS0zMGFjLTQ1NWYtOTk5Zi1lMTcxMjNhZmI4MTAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9Zjk4Y2ZjMmQ3NWM1MmMyMDA4NTcyMzU0NTRiMGIzMWE3NWM2Y2E5MjU3NTZmMmM5ZGQ5ZmIyNjhmZjdjNzNkMiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.WfGSV-bVk2NluLyE__PI402BY5iTVQvPlCaY8hGKiKU"><img src="https://private-user-images.githubusercontent.com/8607532/342005178-eb4867e5-30ac-455f-999f-e17123afb810.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM0MjAwNTE3OC1lYjQ4NjdlNS0zMGFjLTQ1NWYtOTk5Zi1lMTcxMjNhZmI4MTAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9Zjk4Y2ZjMmQ3NWM1MmMyMDA4NTcyMzU0NTRiMGIzMWE3NWM2Y2E5MjU3NTZmMmM5ZGQ5ZmIyNjhmZjdjNzNkMiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.WfGSV-bVk2NluLyE__PI402BY5iTVQvPlCaY8hGKiKU" alt="Image showing gptel's menu with some of the available query options."></a>
<p dir="auto">That’s it. You can go back and edit previous prompts and responses if you want.</p>
<p dir="auto">The default mode is <code>markdown-mode</code> if available, else <code>text-mode</code>.  You can set <code>gptel-default-mode</code> to <code>org-mode</code> if desired.</p>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Including media (images, documents) with requests</h4><a id="user-content-including-media-images-documents-with-requests" aria-label="Permalink: Including media (images, documents) with requests" href="#including-media-images-documents-with-requests"></a></p>
</summary>
<p dir="auto">gptel supports sending media in Markdown and Org chat buffers, but this feature is disabled by default.</p>
<ul dir="auto">
  <li>You can enable it globally, for all models that support it, by setting <code>gptel-track-media</code>.</li>
  <li>Or you can set it locally, just for the chat buffer, via the header line:</li>
</ul>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/375929517-91f6aaab-2ea4-4806-9cc9-39b4b46a8e6c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM3NTkyOTUxNy05MWY2YWFhYi0yZWE0LTQ4MDYtOWNjOS0zOWI0YjQ2YThlNmMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTEzZjU4OGFiN2RlZjgwMzg0MzNjYmYxM2NhM2M2NTI3OTRhOGRmYTkzY2JhNmU5N2ZkOTgyN2I4MmVmMmVkMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.gm0emXnTb-ycQYPMp1VAVCNDPaB156wnyuZdeRO2h5U"><img src="https://private-user-images.githubusercontent.com/8607532/375929517-91f6aaab-2ea4-4806-9cc9-39b4b46a8e6c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM3NTkyOTUxNy05MWY2YWFhYi0yZWE0LTQ4MDYtOWNjOS0zOWI0YjQ2YThlNmMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTEzZjU4OGFiN2RlZjgwMzg0MzNjYmYxM2NhM2M2NTI3OTRhOGRmYTkzY2JhNmU5N2ZkOTgyN2I4MmVmMmVkMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.gm0emXnTb-ycQYPMp1VAVCNDPaB156wnyuZdeRO2h5U" alt="Image showing a gptel chat buffer's header line with the button to toggle media support"></a>
<hr>
<p dir="auto">There are two ways to include media with requests:</p>
<ol dir="auto">
  <li>Adding media files to the context with <code>gptel-add-file</code>, described further below.</li>
  <li>Including links to media in chat buffers, described here:</li>
</ol>
<p dir="auto">To send media – images or other supported file types – with requests in chat buffers, you can include links to them in the chat buffer.  Such a link must be “standalone”, i.e. on a line by itself surrounded by whitespace.</p>
<p dir="auto">In Org mode, for example, the following are all <b>valid</b> ways of including an image with the request:</p>
<ul dir="auto">
  <li>“Standalone” file link:</li>
</ul>
<pre>Describe this picture

[[file:/path/to/screenshot.png]]

Focus specifically on the text content.
</pre>
<ul dir="auto">
  <li>“Standalone” file link with description:</li>
</ul>
<pre>Describe this picture

[[file:/path/to/screenshot.png][some picture]]

Focus specifically on the text content.
</pre>
<ul dir="auto">
  <li>“Standalone”, angle file link:</li>
</ul>
<pre>Describe this picture

&lt;file:/path/to/screenshot.png&gt;

Focus specifically on the text content.
</pre>
<p dir="auto">The following links are <b>not valid</b>, and the text of the link will be sent instead of the file contents:</p>
<ul dir="auto">
  <li>Inline link:</li>
</ul>
<pre>Describe this [[file:/path/to/screenshot.png][picture]].

Focus specifically on the text content.
</pre>
<ul dir="auto">
  <li>Link not “standalone”:</li>
</ul>
<pre>Describe this picture: 
[[file:/path/to/screenshot.png]]
Focus specifically on the text content.
</pre>
<ul dir="auto">
  <li>Not a valid Org link:</li>
</ul>
<pre>Describe the picture

file:/path/to/screenshot.png
</pre>
<p dir="auto">Similar criteria apply to Markdown chat buffers.</p>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Save and restore your chat sessions</h4><a id="user-content-save-and-restore-your-chat-sessions" aria-label="Permalink: Save and restore your chat sessions" href="#save-and-restore-your-chat-sessions"></a></p>
</summary>
<p dir="auto">Saving the file will save the state of the conversation as well.  To resume the chat, open the file and turn on <code>gptel-mode</code> before editing the buffer.</p>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">Include more context with requests</h3><a id="user-content-include-more-context-with-requests" aria-label="Permalink: Include more context with requests" href="#include-more-context-with-requests"></a></p>
<p dir="auto">By default, gptel will query the LLM with the active region or the buffer contents up to the cursor.  Often it can be helpful to provide the LLM with additional context from outside the current buffer. For example, when you’re in a chat buffer but want to ask questions about a (possibly changing) code buffer and auxiliary project files.</p>
<p dir="auto">You can include additional text regions, buffers or files with gptel’s queries.  This additional context is “live” and not a snapshot.  Once added, the regions, buffers or files are scanned and included at the time of each query.  When using multi-modal models, added files can be of any supported type – typically images.</p>
<p dir="auto">You can add a selected region, buffer or file to gptel’s context from the menu, or call <code>gptel-add</code>.  (To add a file use <code>gptel-add</code> in Dired or use the dedicated <code>gptel-add-file</code> command.)</p>
<p dir="auto">You can examine the active context from the menu:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/342006376-63cd7fc8-6b3e-42ae-b6ca-06ff935bae9c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM0MjAwNjM3Ni02M2NkN2ZjOC02YjNlLTQyYWUtYjZjYS0wNmZmOTM1YmFlOWMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZjdkMjEyNTM2NWZhZGYwOTlkZjYxMjc1MmJkYjZlNmQ2MTdhZjc3NmEyNjU3ZWZiZjk1MWRjMTRjZTQwN2JkZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.ZHy1S-7UHn0gJ0hGRQusjvyP9nWBlhNmOo4eRGjXRMk"><img src="https://private-user-images.githubusercontent.com/8607532/342006376-63cd7fc8-6b3e-42ae-b6ca-06ff935bae9c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM0MjAwNjM3Ni02M2NkN2ZjOC02YjNlLTQyYWUtYjZjYS0wNmZmOTM1YmFlOWMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZjdkMjEyNTM2NWZhZGYwOTlkZjYxMjc1MmJkYjZlNmQ2MTdhZjc3NmEyNjU3ZWZiZjk1MWRjMTRjZTQwN2JkZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.ZHy1S-7UHn0gJ0hGRQusjvyP9nWBlhNmOo4eRGjXRMk" alt="Image showing gptel's menu with the "></a>
<p dir="auto">And then browse through or remove context from the context buffer:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/342006378-79a5ffe8-3d63-4bf7-9bf6-0457ab61bf2a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM0MjAwNjM3OC03OWE1ZmZlOC0zZDYzLTRiZjctOWJmNi0wNDU3YWI2MWJmMmEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTAyYmQ4N2M5ODQ3YmI5MTA0NTMzYmEwYTQxMGRiMjk4MTVmNWZmMzBlYmIwZjlkNzNmMzRmY2NmNzJmMzNlOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.UvRFbVbq8t2ov3b_04ba4DRkuLMcYcgiE6obSKddoHM"><img src="https://private-user-images.githubusercontent.com/8607532/342006378-79a5ffe8-3d63-4bf7-9bf6-0457ab61bf2a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM0MjAwNjM3OC03OWE1ZmZlOC0zZDYzLTRiZjctOWJmNi0wNDU3YWI2MWJmMmEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTAyYmQ4N2M5ODQ3YmI5MTA0NTMzYmEwYTQxMGRiMjk4MTVmNWZmMzBlYmIwZjlkNzNmMzRmY2NmNzJmMzNlOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.UvRFbVbq8t2ov3b_04ba4DRkuLMcYcgiE6obSKddoHM" alt="Image showing gptel's context buffer."></a>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rewrite, refactor or fill in a region</h3><a id="user-content-rewrite-refactor-or-fill-in-a-region" aria-label="Permalink: Rewrite, refactor or fill in a region" href="#rewrite-refactor-or-fill-in-a-region"></a></p>
<p dir="auto">In any buffer: with a region selected, you can rewrite prose or refactor code from here:</p>
<p dir="auto"><b>Prose</b>:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/365940610-172e75cd-11e9-4f3b-9ab9-b4edfb8f6695.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM2NTk0MDYxMC0xNzJlNzVjZC0xMWU5LTRmM2ItOWFiOS1iNGVkZmI4ZjY2OTUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NTE0M2YzNmIyOTg5NzUxM2Y2Mjg4MmM1MDcxMjRmMTc5ZmFkNzEwMGFhZTZkN2Q5OGFmMmJlMzI2ZDVjYmYzYSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.yS0YIwBW1Obs97_OahUoFlsEyAc_K-GCcP0zRoTWUsE"><img src="https://private-user-images.githubusercontent.com/8607532/365940610-172e75cd-11e9-4f3b-9ab9-b4edfb8f6695.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM2NTk0MDYxMC0xNzJlNzVjZC0xMWU5LTRmM2ItOWFiOS1iNGVkZmI4ZjY2OTUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NTE0M2YzNmIyOTg5NzUxM2Y2Mjg4MmM1MDcxMjRmMTc5ZmFkNzEwMGFhZTZkN2Q5OGFmMmJlMzI2ZDVjYmYzYSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.yS0YIwBW1Obs97_OahUoFlsEyAc_K-GCcP0zRoTWUsE"></a>
<p dir="auto"><b>Code</b>:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/365940620-8670ddb3-8655-47f4-a70c-8994994e61e3.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM2NTk0MDYyMC04NjcwZGRiMy04NjU1LTQ3ZjQtYTcwYy04OTk0OTk0ZTYxZTMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ODQ3ZGM0MjU1ZTQzYTljMWRhNDJlYjVmNzhiMmJiNjUzYzEwYWJmYTEzNzJjY2NjZmM2YjY1YzA1YzhkMGRiNyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.gCNnTf2JWon0JMMzJt6X0FOCnlI1Ed-thVOQvbD8fjs"><img src="https://private-user-images.githubusercontent.com/8607532/365940620-8670ddb3-8655-47f4-a70c-8994994e61e3.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM2NTk0MDYyMC04NjcwZGRiMy04NjU1LTQ3ZjQtYTcwYy04OTk0OTk0ZTYxZTMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ODQ3ZGM0MjU1ZTQzYTljMWRhNDJlYjVmNzhiMmJiNjUzYzEwYWJmYTEzNzJjY2NjZmM2YjY1YzA1YzhkMGRiNyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.gCNnTf2JWon0JMMzJt6X0FOCnlI1Ed-thVOQvbD8fjs"></a>
<p dir="auto">When the refactor is ready, you can apply it or compare against the original:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/365940614-a0cf33b2-3ad3-4f55-aa7b-4349e54d6771.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM2NTk0MDYxNC1hMGNmMzNiMi0zYWQzLTRmNTUtYWE3Yi00MzQ5ZTU0ZDY3NzEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YjFhNDUzZmExNjFkMmVjMTdlYWM0OGZhYzFhMmE1NDA2MzY0YzRkY2FmMzBjNTEzOTdmYWU3ODk3M2RmNTRmYyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.1nvdV4vIiHVvjo1_6Iu6PewA1ZsCC4K29RwKeMIqoWk"><img src="https://private-user-images.githubusercontent.com/8607532/365940614-a0cf33b2-3ad3-4f55-aa7b-4349e54d6771.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM2NTk0MDYxNC1hMGNmMzNiMi0zYWQzLTRmNTUtYWE3Yi00MzQ5ZTU0ZDY3NzEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YjFhNDUzZmExNjFkMmVjMTdlYWM0OGZhYzFhMmE1NDA2MzY0YzRkY2FmMzBjNTEzOTdmYWU3ODk3M2RmNTRmYyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.1nvdV4vIiHVvjo1_6Iu6PewA1ZsCC4K29RwKeMIqoWk"></a>
<p dir="auto">These actions are also available directly when the cursor is inside the pending rewrite region:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/365940607-57e1fff7-f8e4-47f0-9bda-d0b698443559.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM2NTk0MDYwNy01N2UxZmZmNy1mOGU0LTQ3ZjAtOWJkYS1kMGI2OTg0NDM1NTkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTUzZTA4MDcyOTU0YmE2NzhiZGIwYjYzMzQyOTdmNTk5NjM3Y2JlOTEyMDM2MWU3ZTIyNWNhMzlhMDEzYjI2ZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.U4R5EUDylkoJiHUnOfo0hkTcgEbk1WPTt9JPpDL2ZTE"><img src="https://private-user-images.githubusercontent.com/8607532/365940607-57e1fff7-f8e4-47f0-9bda-d0b698443559.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzM2NTk0MDYwNy01N2UxZmZmNy1mOGU0LTQ3ZjAtOWJkYS1kMGI2OTg0NDM1NTkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTUzZTA4MDcyOTU0YmE2NzhiZGIwYjYzMzQyOTdmNTk5NjM3Y2JlOTEyMDM2MWU3ZTIyNWNhMzlhMDEzYjI2ZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.U4R5EUDylkoJiHUnOfo0hkTcgEbk1WPTt9JPpDL2ZTE"></a>
<p dir="auto"><h3 tabindex="-1" dir="auto">Extra Org mode conveniences</h3><a id="user-content-extra-org-mode-conveniences" aria-label="Permalink: Extra Org mode conveniences" href="#extra-org-mode-conveniences"></a></p>
<p dir="auto">gptel offers a few extra conveniences in Org mode.</p>
<ul dir="auto">
  <li>You can limit the conversation context to an Org heading with the command <code>gptel-org-set-topic</code>.</li>
  <li>You can have branching conversations in Org mode, where each hierarchical outline path through the document is a separate conversation branch.  This is also useful for limiting the context size of each query.  See the variable <code>gptel-org-branching-context</code>.
    Note: using this option requires Org 9.6.7 or higher to be available.  The <a href="https://github.com/ultronozm/ai-org-chat.el">ai-org-chat</a> package uses gptel to provide this branching conversation behavior for older versions of Org.</li>
  <li>You can declare the gptel model, backend, temperature, system message and other parameters as Org properties with the command <code>gptel-org-set-properties</code>.  gptel queries under the corresponding heading will always use these settings, allowing you to create mostly reproducible LLM chat notebooks, and to have simultaneous chats with different models, model settings and directives under different Org headings.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">I want the window to scroll automatically as the response is inserted</h4><a id="user-content-i-want-the-window-to-scroll-automatically-as-the-response-is-inserted" aria-label="Permalink: I want the window to scroll automatically as the response is inserted" href="#i-want-the-window-to-scroll-automatically-as-the-response-is-inserted"></a></p>
</summary>
<p dir="auto">To be minimally annoying, gptel does not move the cursor by default.  Add the following to your configuration to enable auto-scrolling.</p>
<div dir="auto" data-snippet-clipboard-copy-content="(add-hook 'gptel-post-stream-hook 'gptel-auto-scroll)"><pre>(<span>add-hook</span> <span>'gptel-post-stream-hook</span> <span>'gptel-auto-scroll</span>)</pre></div>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">I want the cursor to move to the next prompt after the response is inserted</h4><a id="user-content-i-want-the-cursor-to-move-to-the-next-prompt-after-the-response-is-inserted" aria-label="Permalink: I want the cursor to move to the next prompt after the response is inserted" href="#i-want-the-cursor-to-move-to-the-next-prompt-after-the-response-is-inserted"></a></p>
</summary>
<p dir="auto">To be minimally annoying, gptel does not move the cursor by default.  Add the following to your configuration to move the cursor:</p>
<div dir="auto" data-snippet-clipboard-copy-content="(add-hook 'gptel-post-response-functions 'gptel-end-of-response)"><pre>(<span>add-hook</span> <span>'gptel-post-response-functions</span> <span>'gptel-end-of-response</span>)</pre></div>
<p dir="auto">You can also call <code>gptel-end-of-response</code> as a command at any time.</p>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">I want to change the formatting of the prompt and LLM response</h4><a id="user-content-i-want-to-change-the-formatting-of-the-prompt-and-llm-response" aria-label="Permalink: I want to change the formatting of the prompt and LLM response" href="#i-want-to-change-the-formatting-of-the-prompt-and-llm-response"></a></p>
</summary>
<p dir="auto">For dedicated chat buffers: customize <code>gptel-prompt-prefix-alist</code> and <code>gptel-response-prefix-alist</code>.  You can set a different pair for each major-mode.</p>
<p dir="auto">Anywhere in Emacs: Use <code>gptel-pre-response-hook</code> and <code>gptel-post-response-functions</code>, which see.</p>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">I want the transient menu options to be saved so I only need to set them once</h4><a id="user-content-i-want-the-transient-menu-options-to-be-saved-so-i-only-need-to-set-them-once" aria-label="Permalink: I want the transient menu options to be saved so I only need to set them once" href="#i-want-the-transient-menu-options-to-be-saved-so-i-only-need-to-set-them-once"></a></p>
</summary>
<p dir="auto">Any model options you set are saved for the current buffer.  But the redirection options in the menu are set for the next query only:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/298437596-2ecc6be9-aa52-4287-a739-ba06e1369ec2.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzI5ODQzNzU5Ni0yZWNjNmJlOS1hYTUyLTQyODctYTczOS1iYTA2ZTEzNjllYzIucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTc5ODc1MzkyYTVhOGRhZTRhNjAyNjk5NjU2YjRiNTkyNWVhYzUxODRiOGQ4OTUzYWVjZjkwNTg4OGFjZjJlYyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.NcvUe7-u4f_817v593QAqEgc11g_bJblOxJ7qAb0Ehk"><img src="https://private-user-images.githubusercontent.com/8607532/298437596-2ecc6be9-aa52-4287-a739-ba06e1369ec2.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzI5ODQzNzU5Ni0yZWNjNmJlOS1hYTUyLTQyODctYTczOS1iYTA2ZTEzNjllYzIucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTc5ODc1MzkyYTVhOGRhZTRhNjAyNjk5NjU2YjRiNTkyNWVhYzUxODRiOGQ4OTUzYWVjZjkwNTg4OGFjZjJlYyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.NcvUe7-u4f_817v593QAqEgc11g_bJblOxJ7qAb0Ehk" alt="https://github.com/karthink/gptel/assets/8607532/2ecc6be9-aa52-4287-a739-ba06e1369ec2"></a>
<p dir="auto">You can make them persistent across this Emacs session by pressing <code>C-x C-s</code>:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/298438187-b8bcb6ad-c974-41e1-9336-fdba0098a2fe.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzI5ODQzODE4Ny1iOGJjYjZhZC1jOTc0LTQxZTEtOTMzNi1mZGJhMDA5OGEyZmUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NmNmNWE3MTMzYThkY2JmY2ZhYjdiNTA0ZWE0ZmM0ZjFjNmVkMWMxMDYxYmZhMjlhMmJlNjljYmJmYjUwZTFmZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.oA194Nj-NoQn_RDhsMOsh8XVbwNREeZ8QqVuHlwmj_8"><img src="https://private-user-images.githubusercontent.com/8607532/298438187-b8bcb6ad-c974-41e1-9336-fdba0098a2fe.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA3MDIxMDEsIm5iZiI6MTczMDcwMTgwMSwicGF0aCI6Ii84NjA3NTMyLzI5ODQzODE4Ny1iOGJjYjZhZC1jOTc0LTQxZTEtOTMzNi1mZGJhMDA5OGEyZmUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDYzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NmNmNWE3MTMzYThkY2JmY2ZhYjdiNTA0ZWE0ZmM0ZjFjNmVkMWMxMDYxYmZhMjlhMmJlNjljYmJmYjUwZTFmZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.oA194Nj-NoQn_RDhsMOsh8XVbwNREeZ8QqVuHlwmj_8" alt="https://github.com/karthink/gptel/assets/8607532/b8bcb6ad-c974-41e1-9336-fdba0098a2fe"></a>
<p dir="auto">(You can also cycle through presets you’ve saved with <code>C-x p</code> and <code>C-x n</code>.)</p>
<p dir="auto">Now these will be enabled whenever you send a query from the transient menu.  If you want to use these saved options without invoking the transient menu, you can use a keyboard macro:</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Replace with your key to invoke the transient menu:
(keymap-global-set &quot;<f6>&quot; &quot;C-u C-c <return> <return>&quot;)"><pre><span><span>;</span>; Replace with your key to invoke the transient menu:</span>
(keymap-global-set <span><span>"</span>&lt;f6&gt;<span>"</span></span> <span><span>"</span>C-u C-c &lt;return&gt; &lt;return&gt;<span>"</span></span>)</pre></div>
<p dir="auto">Or see this <a href="https://github.com/karthink/gptel/wiki/Commonly-requested-features#save-transient-flags">wiki entry</a>.</p>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">I want to use gptel in a way that’s not supported by <code>gptel-send</code> or the options menu</h4><a id="user-content-i-want-to-use-gptel-in-a-way-thats-not-supported-by-gptel-send-or-the-options-menu" aria-label="Permalink: I want to use gptel in a way that’s not supported by gptel-send or the options menu" href="#i-want-to-use-gptel-in-a-way-thats-not-supported-by-gptel-send-or-the-options-menu"></a></p>
</summary>
<p dir="auto">gptel’s default usage pattern is simple, and will stay this way: Read input in any buffer and insert the response below it.  Some custom behavior is possible with the transient menu (<code>C-u M-x gptel-send</code>).</p>
<p dir="auto">For more programmable usage, gptel provides a general <code>gptel-request</code> function that accepts a custom prompt and a callback to act on the response. You can use this to build custom workflows not supported by <code>gptel-send</code>.  See the documentation of <code>gptel-request</code>, and the <a href="https://github.com/karthink/gptel/wiki/Defining-custom-gptel-commands">wiki</a> for examples.</p>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">(Doom Emacs) Sending a query from the gptel menu fails because of a key conflict with Org mode</h4><a id="user-content-doom-emacs-sending-a-query-from-the-gptel-menu-fails-because-of-a-key-conflict-with-org-mode" aria-label="Permalink: (Doom Emacs) Sending a query from the gptel menu fails because of a key conflict with Org mode" href="#doom-emacs-sending-a-query-from-the-gptel-menu-fails-because-of-a-key-conflict-with-org-mode"></a></p>
</summary>
<p dir="auto">Doom binds <code>RET</code> in Org mode to <code>+org/dwim-at-point</code>, which appears to conflict with gptel’s transient menu bindings for some reason.</p>
<p dir="auto">Two solutions:</p>
<ul dir="auto">
  <li>Press <code>C-m</code> instead of the return key.</li>
  <li>Change the send key from return to a key of your choice:
    <div dir="auto" data-snippet-clipboard-copy-content="(transient-suffix-put 'gptel-menu (kbd &quot;RET&quot;) :key &quot;<f8>&quot;)
    "><pre>(transient-suffix-put <span>'gptel-menu</span> (<span>kbd</span> <span><span>"</span>RET<span>"</span></span>) <span>:key</span> <span><span>"</span>&lt;f8&gt;<span>"</span></span>)
    </pre></div>
  </li>
</ul>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">(ChatGPT) I get the error “(HTTP/2 429) You exceeded your current quota”</h4><a id="user-content-chatgpt-i-get-the-error-http2-429-you-exceeded-your-current-quota" aria-label="Permalink: (ChatGPT) I get the error “(HTTP/2 429) You exceeded your current quota”" href="#chatgpt-i-get-the-error-http2-429-you-exceeded-your-current-quota"></a></p>
</summary>
<blockquote>
  <p dir="auto">(HTTP/2 429) You exceeded your current quota, please check your plan and billing details.</p>
</blockquote>
<p dir="auto">Using the ChatGPT (or any OpenAI) API requires <a href="https://platform.openai.com/account/billing/overview" rel="nofollow">adding credit to your account</a>.</p>
</details>
<details><summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Why another LLM client?</h4><a id="user-content-why-another-llm-client" aria-label="Permalink: Why another LLM client?" href="#why-another-llm-client"></a></p>
</summary>
<p dir="auto">Other Emacs clients for LLMs prescribe the format of the interaction (a comint shell, org-babel blocks, etc).  I wanted:</p>
<ol dir="auto">
  <li>Something that is as free-form as possible: query the model using any text in any buffer, and redirect the response as required.  Using a dedicated <code>gptel</code> buffer just adds some visual flair to the interaction.</li>
  <li>Integration with org-mode, not using a walled-off org-babel block, but as regular text.  This way the model can generate code blocks that I can run.</li>
</ol>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Additional Configuration</h2><a id="user-content-additional-configuration" aria-label="Permalink: Additional Configuration" href="#additional-configuration"></a></p>
<markdown-accessiblity-table><table>
  <tbody><tr><th><b>Connection options</b></th><th></th></tr>
  <tr><td><code>gptel-use-curl</code></td><td>Use Curl (default), fallback to Emacs’ built-in <code>url</code>.</td></tr>
  <tr><td><code>gptel-proxy</code></td><td>Proxy server for requests, passed to curl via <code>--proxy</code>.</td></tr>
  <tr><td><code>gptel-api-key</code></td><td>Variable/function that returns the API key for the active backend.</td></tr>
  <tr><th><b>LLM request options</b></th><th><i>(Note: not supported uniformly across LLMs)</i></th></tr>
  <tr><td><code>gptel-backend</code></td><td>Default LLM Backend.</td></tr>
  <tr><td><code>gptel-model</code></td><td>Default model to use, depends on the backend.</td></tr>
  <tr><td><code>gptel-stream</code></td><td>Enable streaming responses, if the backend supports it.</td></tr>
  <tr><td><code>gptel-directives</code></td><td>Alist of system directives, can switch on the fly.</td></tr>
  <tr><td><code>gptel-max-tokens</code></td><td>Maximum token count (in query + response).</td></tr>
  <tr><td><code>gptel-temperature</code></td><td>Randomness in response text, 0 to 2.</td></tr>
  <tr><td><code>gptel-use-context</code></td><td>How/whether to include additional context</td></tr>
  <tr><th><b>Chat UI options</b></th><th></th></tr>
  <tr><td><code>gptel-default-mode</code></td><td>Major mode for dedicated chat buffers.</td></tr>
  <tr><td><code>gptel-track-response</code></td><td>Distinguish between user messages and LLM responses?</td></tr>
  <tr><td><code>gptel-track-media</code></td><td>Send images or other media from links?</td></tr>
  <tr><td><code>gptel-prompt-prefix-alist</code></td><td>Text inserted before queries.</td></tr>
  <tr><td><code>gptel-response-prefix-alist</code></td><td>Text inserted before responses.</td></tr>
  <tr><td><code>gptel-use-header-line</code></td><td>Display status messages in header-line (default) or minibuffer</td></tr>
  <tr><td><code>gptel-display-buffer-action</code></td><td>Placement of the gptel chat buffer.</td></tr>
  <tr><th><b>Org mode UI options</b></th><th></th></tr>
  <tr><td><code>gptel-org-branching-context</code></td><td>Make each outline path a separate conversation branch</td></tr>
  <tr><th><b>Hooks for customization</b></th><th></th></tr>
  <tr><td><code>gptel-save-state-hook</code></td><td>Runs before saving the chat state to a file on disk</td></tr>
  <tr><td><code>gptel-pre-response-hook</code></td><td>Runs before inserting the LLM response into the buffer</td></tr>
  <tr><td><code>gptel-post-response-functions</code></td><td>Runs after inserting the full LLM response into the buffer</td></tr>
  <tr><td><code>gptel-post-stream-hook</code></td><td>Runs after each streaming insertion</td></tr>
  <tr><td><code>gptel-context-wrap-function</code></td><td>To include additional context formatted your way</td></tr>
</tbody></table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Alternatives</h2><a id="user-content-alternatives" aria-label="Permalink: Alternatives" href="#alternatives"></a></p>
<p dir="auto">Other Emacs clients for LLMs include</p>
<ul dir="auto">
  <li><a href="https://github.com/ahyatt/llm">llm</a>: llm provides a uniform API across language model providers for building LLM clients in Emacs, and is intended as a library for use by package authors.  For similar scripting purposes, gptel provides the command <code>gptel-request</code>, which see.</li>
  <li><a href="https://github.com/s-kostyaev/ellama">Ellama</a>: A full-fledged LLM client built on llm, that supports many LLM providers (Ollama, Open AI, Vertex, GPT4All and more).  Its usage differs from gptel in that it provides separate commands for dozens of common tasks, like general chat, summarizing code/text, refactoring code, improving grammar, translation and so on.</li>
  <li><a href="https://github.com/xenodium/chatgpt-shell">chatgpt-shell</a>: comint-shell based interaction with ChatGPT.  Also supports DALL-E, executable code blocks in the responses, and more.</li>
  <li><a href="https://github.com/rksm/org-ai">org-ai</a>: Interaction through special <code>#+begin_ai ... #+end_ai</code> Org-mode blocks.  Also supports DALL-E, querying ChatGPT with the contents of project files, and more.</li>
</ul>
<p dir="auto">There are several more: <a href="https://github.com/MichaelBurge/leafy-mode">leafy-mode</a>, <a href="https://github.com/iwahbe/chat.el">chat.el</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Packages using gptel</h3><a id="user-content-packages-using-gptel" aria-label="Permalink: Packages using gptel" href="#packages-using-gptel"></a></p>
<p dir="auto">gptel is a general-purpose package for chat and ad-hoc LLM interaction.  The following packages use gptel to provide additional or specialized functionality:</p>
<ul dir="auto">
  <li><a href="https://github.com/karthink/gptel-quick">gptel-quick</a>: Quickly look up the region or text at point.</li>
  <li><a href="https://github.com/daedsidog/evedel">Evedel</a>: Instructed LLM Programmer/Assistant</li>
  <li><a href="https://github.com/lanceberge/elysium">Elysium</a>: Automatically apply AI-generated changes as you code</li>
  <li><a href="https://github.com/kamushadenes/gptel-extensions.el">gptel-extensions</a>: Extra utility functions for gptel</li>
  <li><a href="https://github.com/kamushadenes/ai-blog.el">ai-blog.el</a>: Streamline generation of blog posts in Hugo</li>
  <li><a href="https://github.com/douo/magit-gptcommit">magit-gptcommit</a>: Generate Commit Messages within magit-status Buffer using gptel</li>
  <li><a href="https://github.com/armindarvish/consult-omni">consult-omni</a>: Versatile multi-source search package.  It includes gptel as one of its many sources.</li>
  <li><a href="https://github.com/ultronozm/ai-org-chat.el">ai-org-chat</a>: Provides branching conversations in Org buffers using gptel.  (Note that gptel includes this feature as well (see <code>gptel-org-branching-context</code>), but requires a recent version of Org mode (9.67 or later) to be installed.)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgments</h2><a id="user-content-acknowledgments" aria-label="Permalink: Acknowledgments" href="#acknowledgments"></a></p>
<ul dir="auto">
  <li><a href="https://github.com/algal">Alexis Gallagher</a> and <a href="https://github.com/d1egoaz">Diego Alvarez</a> for fixing a nasty multi-byte bug with <code>url-retrieve</code>.</li>
  <li><a href="https://github.com/tarsius">Jonas Bernoulli</a> for the Transient library.</li>
  <li><a href="https://github.com/daedsidog">daedsidog</a> for adding context support to gptel.</li>
  <li><a href="https://github.com/Aquan1412">Aquan1412</a> for adding PrivateGPT support to gptel.</li>
  <li><a href="https://github.com/r0man">r0man</a> for improving gptel’s Curl integration.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ToolGit: A collection of scripts that extend Git with various sub-commands (109 pts)]]></title>
            <link>https://github.com/ahmetsait/toolgit</link>
            <guid>42034521</guid>
            <pubDate>Sun, 03 Nov 2024 17:32:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ahmetsait/toolgit">https://github.com/ahmetsait/toolgit</a>, See on <a href="https://news.ycombinator.com/item?id=42034521">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">ToolGit is a collection of scripts that extend Git with various sub-commands to make life easier.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Commands</h2><a id="user-content-commands" aria-label="Permalink: Commands" href="#commands"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>git-amend</code></td>
<td>Amend currently staged changes.</td>
</tr>
<tr>
<td><code>git-delete-gone-branches</code></td>
<td>Delete local branches that no longer exist on remote.</td>
</tr>
<tr>
<td><code>git-dir</code></td>
<td>Output <code>.git</code> directory path of this Git repository.</td>
</tr>
<tr>
<td><code>git-force-pull</code></td>
<td>Fetch and force pull remote tracking branch(es) by doing a hard reset.</td>
</tr>
<tr>
<td><code>git-forward</code></td>
<td>Fetch and fast forward all remote tracking branches.</td>
</tr>
<tr>
<td><code>git-gc-all</code></td>
<td>Expire the reflog and run a full garbage collection on the Git repository.</td>
</tr>
<tr>
<td><code>git-in-repo</code></td>
<td>Returns 0 if current working directory is a Git repository, non-zero otherwise.</td>
</tr>
<tr>
<td><code>git-is-branch-remote</code></td>
<td>Returns 0 if the branch(es) refer to a remote branch.</td>
</tr>
<tr>
<td><code>git-is-head-detached</code></td>
<td>Returns 0 if HEAD is in detached state, non-zero otherwise.</td>
</tr>
<tr>
<td><code>git-is-worktree-clean</code></td>
<td>Returns 0 if the working tree has no changes or untracked files, non-zero otherwise.</td>
</tr>
<tr>
<td><code>git-legacy</code></td>
<td>Rebase the whole history of current HEAD on top of .</td>
</tr>
<tr>
<td><code>git-main-branch</code></td>
<td>Get the name of the main (default) branch.</td>
</tr>
<tr>
<td><code>git-mode-restore</code></td>
<td>Restore file modes in index and/or worktree.</td>
</tr>
<tr>
<td><code>git-root</code></td>
<td>Output root path of this Git repository.</td>
</tr>
<tr>
<td><code>git-xlog</code></td>
<td>Search history for string only in added or removed lines.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Downloads</h2><a id="user-content-downloads" aria-label="Permalink: Downloads" href="#downloads"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/ahmetsait/toolgit/archive/refs/heads/main.tar.gz">Tarball (Posix)</a></h3><a id="user-content-tarball-posix" aria-label="Permalink: Tarball (Posix)" href="#tarball-posix"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/ahmetsait/toolgit/archive/refs/heads/main.zip">Zip (Windows)</a></h3><a id="user-content-zip-windows" aria-label="Permalink: Zip (Windows)" href="#zip-windows"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installing</h2><a id="user-content-installing" aria-label="Permalink: Installing" href="#installing"></a></p>
<p dir="auto">Extract ToolGit to an appropriate folder of your choice and add the folder path to your <code>PATH</code> environment variable. Git will recognize executable <code>git-*</code> files in <code>PATH</code> and allow using them as sub-commands.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">Use <code>-?</code> switch to learn about commands and their options. Example:</p>
<div data-snippet-clipboard-copy-content="$ git mode-restore -?
usage: git-mode-restore [-s <tree-ish>] [-W] [-S] [-8] [-n] [-q] [-?] path [path ...]

Restore file modes in index and/or worktree.

positional arguments:
  path

options:
  -s <tree-ish>, --source <tree-ish>
                        Restore file modes from the given tree. If not specified, the modes are restored from HEAD if --staged is given, otherwise from the index.
  -W, --worktree        The working tree modes are restored. This is the default.
  -S, --staged          File modes in the index are restored. Specifying both --worktree and --staged restores both index and worktree.
  -8, --octal           Print permissions in octal based numeric format instead of using 'rwx' letters.
  -n, --dry-run         Don’t actually restore file modes, just show what would happen.
  -q, --quiet           Suppress printing file permission changes to standard output.
  -?, --help            Show this help text and exit."><pre><code>$ git mode-restore -?
usage: git-mode-restore [-s &lt;tree-ish&gt;] [-W] [-S] [-8] [-n] [-q] [-?] path [path ...]

Restore file modes in index and/or worktree.

positional arguments:
  path

options:
  -s &lt;tree-ish&gt;, --source &lt;tree-ish&gt;
                        Restore file modes from the given tree. If not specified, the modes are restored from HEAD if --staged is given, otherwise from the index.
  -W, --worktree        The working tree modes are restored. This is the default.
  -S, --staged          File modes in the index are restored. Specifying both --worktree and --staged restores both index and worktree.
  -8, --octal           Print permissions in octal based numeric format instead of using 'rwx' letters.
  -n, --dry-run         Don’t actually restore file modes, just show what would happen.
  -q, --quiet           Suppress printing file permission changes to standard output.
  -?, --help            Show this help text and exit.
</code></pre></div>
<p dir="auto">Example <code>mode-restore</code> usage:</p>
<div data-snippet-clipboard-copy-content="$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use &quot;git add <file>...&quot; to update what will be committed)
  (use &quot;git restore <file>...&quot; to discard changes in working directory)
        modified:   .editorconfig
        modified:   .gitattributes
        modified:   .vscode/launch.json
        modified:   .vscode/tasks.json
        modified:   README.md
        modified:   UNLICENSE.txt

no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)"><pre><code>$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add &lt;file&gt;..." to update what will be committed)
  (use "git restore &lt;file&gt;..." to discard changes in working directory)
        modified:   .editorconfig
        modified:   .gitattributes
        modified:   .vscode/launch.json
        modified:   .vscode/tasks.json
        modified:   README.md
        modified:   UNLICENSE.txt

no changes added to commit (use "git add" and/or "git commit -a")
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ git diff
diff --git a/.editorconfig b/.editorconfig
old mode 100644
new mode 100755
diff --git a/.gitattributes b/.gitattributes
old mode 100644
new mode 100755
diff --git a/.vscode/launch.json b/.vscode/launch.json
old mode 100644
new mode 100755
diff --git a/.vscode/tasks.json b/.vscode/tasks.json
old mode 100644
new mode 100755
diff --git a/README.md b/README.md
old mode 100644
new mode 100755
diff --git a/UNLICENSE.txt b/UNLICENSE.txt
old mode 100644
new mode 100755"><pre><code>$ git diff
diff --git a/.editorconfig b/.editorconfig
old mode 100644
new mode 100755
diff --git a/.gitattributes b/.gitattributes
old mode 100644
new mode 100755
diff --git a/.vscode/launch.json b/.vscode/launch.json
old mode 100644
new mode 100755
diff --git a/.vscode/tasks.json b/.vscode/tasks.json
old mode 100644
new mode 100755
diff --git a/README.md b/README.md
old mode 100644
new mode 100755
diff --git a/UNLICENSE.txt b/UNLICENSE.txt
old mode 100644
new mode 100755
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ git mode-restore .
rwxrwxr-x -> rw-rw-r--: .editorconfig
rwxrwxr-x -> rw-rw-r--: .gitattributes
rwxrwxr-x -> rw-rw-r--: .vscode/launch.json
rwxrwxr-x -> rw-rw-r--: .vscode/tasks.json
rwxrwxr-x -> rw-rw-r--: README.md
rwxrwxr-x -> rw-rw-r--: UNLICENSE.txt"><pre><code>$ git mode-restore .
rwxrwxr-x -&gt; rw-rw-r--: .editorconfig
rwxrwxr-x -&gt; rw-rw-r--: .gitattributes
rwxrwxr-x -&gt; rw-rw-r--: .vscode/launch.json
rwxrwxr-x -&gt; rw-rw-r--: .vscode/tasks.json
rwxrwxr-x -&gt; rw-rw-r--: README.md
rwxrwxr-x -&gt; rw-rw-r--: UNLICENSE.txt
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ git status
On branch main
Your branch is up to date with 'origin/main'.

nothing to commit, working tree clean"><pre><code>$ git status
On branch main
Your branch is up to date with 'origin/main'.

nothing to commit, working tree clean
</code></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[pg_flo – Stream, transform, and re-route PostgreSQL data in real-time (204 pts)]]></title>
            <link>https://www.pgflo.io/</link>
            <guid>42034237</guid>
            <pubDate>Sun, 03 Nov 2024 17:02:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pgflo.io/">https://www.pgflo.io/</a>, See on <a href="https://news.ycombinator.com/item?id=42034237">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><span>
            <span>👀 Learn more about a cloud offering</span>
            <a target="_blank" href="https://g5st0ubu2s9.typeform.com/to/zsfdzAaT">
              Signup
              <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 7l5 5m0 0l-5 5m5-5H6"></path>
              </svg>
            </a>
          </span>
        </p>
        
        <div>
          <p>
              The easiest way to move and transform data between PostgreSQL
              databases
              <span>
                <i></i>
                <span>
                  The project is in active development. Follow the GitHub repo
                  for more details 😊
                </span>
              </span>
            </p>
          <p><a href="#quick-start">
            Get Started
          </a>
        </p></div>
      </div><div>
        <div>
          <section>
            <ul>
              <li>
                <i></i>
                <div>
                  <h3>
                    Real-time Streaming
                  </h3>
                  <p>
                    Stream data changes in near real-time using PostgreSQL
                    Logical Replication.
                  </p>
                </div>
              </li>
              <li>
                <i></i>
                <div>
                  <h3>Bulk Copy</h3>
                  <p>
                    Parallelizable bulk copy for fast initial data
                    synchronization.
                  </p>
                </div>
              </li>
              <li>
                <i></i>
                <div>
                  <h3>
                    Powerful Transformations
                  </h3>
                  <p>
                    Apply regex-based transformations, mask sensitive data, and
                    filter based on column values before the changes reach the
                    destination.
                  </p>
                </div>
              </li>
              <li>
                <i></i>
                <div>
                  <h3>Flexible Routing</h3>
                  <p>
                    Route data seamlessly between tables - whether to
                    differently named tables or the same table with custom
                    column mappings.
                  </p>
                </div>
              </li>
            </ul>
          </section>

          <section id="quick-start">
            <div>
              <pre><code>
# Install using Docker
docker pull shayonj/pg_flo:latest

# Start NATS server
docker run -d --name pg_flo_nats \
  --network host \
  nats:latest

# Start replicator
docker run -d --name pg_flo_replicator \
  --network host \
  shayonj/pg_flo:latest \
  replicator

# Start worker with PostgreSQL sink
docker run -d --name pg_flo_worker \
  --network host \
  shayonj/pg_flo:latest \
  worker postgres</code></pre>
            </div>
            <p>
              <a href="https://github.com/shayonj/pg_flo#quick-start">
                View detailed setup instructions
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor">
                  <path fill-rule="evenodd" d="M10.293 3.293a1 1 0 011.414 0l6 6a1 1 0 010 1.414l-6 6a1 1 0 01-1.414-1.414L14.586 11H3a1 1 0 110-2h11.586l-4.293-4.293a1 1 0 010-1.414z" clip-rule="evenodd"></path>
                </svg>
              </a>
            </p>
          </section>
        </div>

        <section>
          <h2>Transformation Rules</h2>
          <div>
            <div>
              <h3>
                Mask Email Addresses
                <span>
                  <i></i>
                  <span>Replaces email characters with asterisks, preserving the
                    domain</span>
                </span>
              </h3>
              <pre><code>
- type: transform
  column: email
  parameters:
    type: mask
    mask_char: "*"
  operations: [INSERT, UPDATE]
              </code></pre>
            </div>
            <div>
              <h3>
                Format Phone Numbers
                <span>
                  <i></i>
                  <span>Formats 10-digit phone numbers to (XXX) XXX-XXXX
                    format</span>
                </span>
              </h3>
              <pre><code>
- type: transform
  column: phone
  parameters:
    type: regex
    pattern: "^(\\d{3})(\\d{3})(\\d{4})$"
    replace: "($1) $2-$3"
  operations: [INSERT, UPDATE]
              </code></pre>
            </div>
          </div>
        </section>

        <section>
          <h2>
            Table Routing &amp; Column Mapping
          </h2>
          <div>
            <div>
              <h3>
                Flexible Table Routing
                <span>
                  <i></i>
                  <span>Route data between differently named tables and
                    columns</span>
                </span>
              </h3>
              <pre><code>
users:
  source_table: users
  destination_table: customers
  column_mappings:
    - source: id
      destination: customer_id
    - source: username
      destination: customer_name
  operations: [INSERT, UPDATE]</code></pre>
            </div>
            <div>
              <h3>
                Multi-Table Routing
                <span>
                  <i></i>
                  <span>Route multiple tables with different mappings
                    simultaneously</span>
                </span>
              </h3>
              <pre><code>
orders:
  source_table: orders
  destination_table: transactions
  column_mappings:
    - source: id
      destination: transaction_id
    - source: total_amount
      destination: amount
  operations: [INSERT, UPDATE, DELETE]</code></pre>
            </div>
          </div>
        </section>

        <section>
          <h2>Use Cases</h2>
          <div>
            <div>
              <h3>
                Secure Production &lt;&gt; Staging Sync
              </h3>
              <p>
                Continuously sync production data to staging, leveraging
                powerful transformation rules to maintain data privacy and
                security practices.
              </p>
            </div>
            <div>
              <h3>
                Data Archiving and Analytics
              </h3>
              <p>
                Sync and transform data to separate databases for archiving,
                auditing and analytics purposes.
              </p>
            </div>
          </div>
        </section>

        <section>
          <h2>How It Works</h2>
          <div>
              <p>
                  pg_flo leverages PostgreSQL's logical replication system to
                  capture and stream data changes. It uses NATS as a message
                  broker to decouple reading from the WAL through the replicator
                  and worker processes, providing flexibility and scalability.
                  Transformations and filtrations are applied before the data
                  reaches the destination.
                </p>
              <p>
                <a href="https://github.com/shayonj/pg_flo/blob/main/internal/how-it-works.md">
                  Learn More
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor">
                    <path fill-rule="evenodd" d="M10.293 3.293a1 1 0 011.414 0l6 6a1 1 0 010 1.414l-6 6a1 1 0 01-1.414-1.414L14.586 11H3a1 1 0 110-2h11.586l-4.293-4.293a1 1 0 010-1.414z" clip-rule="evenodd"></path>
                  </svg>
                </a>
              </p>
            </div>
        </section>
      </div><div>
        <p>© 2024 pg_flo. All rights reserved.</p>
        <p>
          Made with ❤️ and 🍪 by
          <a href="https://shayon.dev/" target="_blank" rel="noopener noreferrer">shayon.dev</a>
        </p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Auth Wiki (109 pts)]]></title>
            <link>https://auth.wiki/</link>
            <guid>42033295</guid>
            <pubDate>Sun, 03 Nov 2024 14:37:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://auth.wiki/">https://auth.wiki/</a>, See on <a href="https://news.ycombinator.com/item?id=42033295">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-j7pv25f6=""> <section data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">A</h2> <div data-astro-cid-j7pv25f6=""> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Access control </h3> <p data-astro-cid-dohjnao5=""> Access control is the restriction of who can perform what actions on certain resources in a system. It is a fundamental security mechanism to define and enforce access policies. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/access-control" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Access token </h3> <p data-astro-cid-dohjnao5=""> An access token is a credential used to access protected resources on behalf of an identity (e.g., user or service). It is a bearer token that grants access to resources based on the token's scopes (permissions). </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/access-token" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> API key </h3> <p data-astro-cid-dohjnao5=""> An API key is a unique identifier used to authenticate and authorize a client when accessing an API. It serves as a secret token included in API requests to verify the client’s identity and allow access to specific resources or services. API keys are typically used in server-to-server communications or when accessing public data. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/api-key" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Attribute-based access control (ABAC) </h3> <p data-astro-cid-dohjnao5=""> Attribute-based access control (ABAC) is an access control model that uses attributes (such as user roles, resource properties, and environmental conditions) to make access control decisions. It is a flexible and dynamic way to manage access to protected resources. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/abac" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Audience </h3> <p data-astro-cid-dohjnao5=""> The audience claim in a token specifies the intended recipient, typically the client application or API resource. It ensures the token is used only by the correct service, enhancing security by preventing unauthorized access. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/audience" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Authentication </h3> <p data-astro-cid-dohjnao5=""> Authentication is the process of verifying the identity ownership (e.g. user or service). It is the foundation of identity and access management (IAM) systems and is essential for securing applications and services. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/authentication" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Authentication request </h3> <p data-astro-cid-dohjnao5=""> An authentication request is an OpenID Connect (OIDC) request for authenticating a user. It reuses the OAuth 2.0 authorization request and extends it to support authentication. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/authentication-request" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Authorization </h3> <p data-astro-cid-dohjnao5=""> Authorization is the process of determining what actions an identity can perform on a resource. It is a fundamental security mechanism to define and enforce access policies. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/authorization" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Authorization code flow </h3> <p data-astro-cid-dohjnao5=""> The authorization code dlow is a secure OAuth 2.0 mechanism that enables applications to obtain access tokens on behalf of users. It involves user authentication, authorization code generation, and token exchange. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/authorization-code-flow" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Authorization request </h3> <p data-astro-cid-dohjnao5=""> An authorization request is an OAuth 2.0 request for authorizing a client to access protected resources on behalf of a user. It is the first step of user authorization flows in OAuth 2.0. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/authorization-request" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Authorization server </h3> <p data-astro-cid-dohjnao5=""> An authorization server is a component of the OAuth 2.0 framework that issues access tokens to clients upon successful authentication and authorization. It is also the OpenID Provider (OP) in OpenID Connect (OIDC) that issues ID tokens to clients. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/authorization-server" data-astro-cid-dohjnao5="">Learn more</a> </section>  </div> </section><section data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">C</h2> <div data-astro-cid-j7pv25f6=""> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Claim </h3> <p data-astro-cid-dohjnao5=""> A claim in JSON Web Token (JWT) is a name-value pair that conveys specific information. In a wider context, a claim can be any name-value pair that represents information. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/claim" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Client </h3> <p data-astro-cid-dohjnao5=""> In OAuth 2.0 and OpenID Connect (OIDC), a client is an application that requests authentication or authorization on behalf of a user or itself. Clients can be public or private (confidential), and they use different grant types to obtain tokens. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/client" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Client credentials flow </h3> <p data-astro-cid-dohjnao5=""> Client credentials flow is an OAuth 2.0 grant type that allows confidential clients to obtain access tokens to access protected resources. It is suitable for machine-to-machine (server-to-server) communication. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/client-credentials-flow" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Cross-site request forgery (CSRF) </h3> <p data-astro-cid-dohjnao5=""> Cross-site request forgery (CSRF) is an attack that deceives users into executing unwanted actions on a web application in which they are authenticated. It is a common security vulnerability that can lead to unauthorized actions. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/csrf" data-astro-cid-dohjnao5="">Learn more</a> </section>  </div> </section><section data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">D</h2> <section data-astro-cid-dohjnao5="" data-astro-cid-j7pv25f6=""> <h3 data-astro-cid-dohjnao5=""> Device flow </h3> <p data-astro-cid-dohjnao5=""> OAuth 2.0 device authorization flow is a user-friendly sign-in method for input-limited devices or headless applications.  By verifying a unique device code, making it possible for users to authorize the device via a secondary device with a full user interface. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/device-flow" data-astro-cid-dohjnao5="">Learn more</a> </section> </section><section data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">E</h2> <section data-astro-cid-dohjnao5="" data-astro-cid-j7pv25f6=""> <h3 data-astro-cid-dohjnao5=""> Enterprise SSO </h3> <p data-astro-cid-dohjnao5=""> Enterprise Single Sign-On (SSO) is a specific type of SSO designed for employees within an organization. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/enterprise-sso" data-astro-cid-dohjnao5="">Learn more</a> </section> </section><section data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">H</h2> <section data-astro-cid-dohjnao5="" data-astro-cid-j7pv25f6=""> <h3 data-astro-cid-dohjnao5=""> Hybrid flow </h3> <p data-astro-cid-dohjnao5=""> The hybrid flow is an OpenID Connect (OIDC) flow that combines the authorization code flow and the implicit flow. It is designed to provide a balance between security and usability for authentication. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/hybrid-flow" data-astro-cid-dohjnao5="">Learn more</a> </section> </section><section data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">I</h2> <div data-astro-cid-j7pv25f6=""> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> ID token </h3> <p data-astro-cid-dohjnao5=""> An ID token is a JSON Web Token (JWT) issued by an authorization server to a client application. It contains information about the authenticated user, such as their unique identifier and claims. This token is used to verify the user's identity and allows the client application to access protected resources on behalf of the user. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/id-token" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Identity and access management (IAM) </h3> <p data-astro-cid-dohjnao5=""> Identity and access management (IAM) is a broad concept that encompasses the processes, technologies, and policies used to manage digital identities and control access to resources. It is a fundamental aspect of security in modern applications and systems. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/iam" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Identity provider (IdP) </h3> <p data-astro-cid-dohjnao5=""> Identity provider (IdP) is a service that manages identities. Modern identity providers support OpenID Connect (OIDC) for authentication and OAuth 2.0 for authorization. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/identity-provider" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Implicit flow </h3> <p data-astro-cid-dohjnao5=""> The OIDC implicit flow is a authentication method for SPAs, enabling them to quickly receive tokens directly from the authorization server. While it simplifies the process by eliminating the need for a backend server, it comes with lower security due to token exposure in the URL. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/implicit-flow" data-astro-cid-dohjnao5="">Learn more</a> </section>  </div> </section><section data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">J</h2> <div data-astro-cid-j7pv25f6=""> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> JSON Web Encryption (JWE) </h3> <p data-astro-cid-dohjnao5=""> JSON Web Encryption (JWE) is a standard way to encrypt and decrypt data in JSON format. It is often used to protect sensitive information in transitting JSON Web Tokens (JWTs). </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/jwe" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> JSON Web Key (JWK) </h3> <p data-astro-cid-dohjnao5=""> A JSON Web Key (JWK) is a JSON-based format used for representing cryptographic keys. When multiple JWKs need to be grouped together, they are organized into a JSON Web Key Set (JWKS). </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/jwk" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> JSON Web Signature (JWS) </h3> <p data-astro-cid-dohjnao5=""> JSON Web Signature (JWS) is a standard way to sign and verify data in JSON format. It is often used to ensure the integrity and authenticity of JSON Web Tokens (JWTs) in OpenID Connect (OIDC). </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/jws" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> JSON Web Token (JWT) </h3> <p data-astro-cid-dohjnao5=""> JSON Web Token (JWT) is an open standard defined in RFC 7519 that enables secure communication between two parties. It is compact, URL-safe, and self-contained, making it ideal for transmitting authentication and authorization data between services. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/jwt" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Just-in-time (JIT) provisioning </h3> <p data-astro-cid-dohjnao5=""> Just-in-time (JIT) provisioning is an identity and access management (IAM) process where user accounts are created dynamically and automatically when a user attempts to access a system or application for the first time. This approach helps streamline the onboarding process and ensures that user accounts are only created when needed, reducing administrative overhead and improving security. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/jit-provisioning" data-astro-cid-dohjnao5="">Learn more</a> </section>  </div> </section><section data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">M</h2> <div data-astro-cid-j7pv25f6=""> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Machine-to-machine </h3> <p data-astro-cid-dohjnao5=""> Machine-to-machine (M2M) communication refers to the automated exchange of data between devices without human intervention. In the context of authentication and authorization, M2M communication often involves a client application that needs to access resources, where the client application is a machine (service) or a machine acting on behalf of a user. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/machine-to-machine" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Magic link </h3> <p data-astro-cid-dohjnao5=""> Magic link is a one-time URL that can be used to complete authentication process. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/magic-link" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Management API </h3> <p data-astro-cid-dohjnao5=""> The Management API in the context of identity and access management (IAM) allows for programmatic management of resources such as users, applications, roles, and permissions. Typically RESTful, it provides an abstraction layer between the IAM system and the user interface, enabling automation, integration, and custom feature development. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/management-api" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Multi-factor authentication (MFA) </h3> <p data-astro-cid-dohjnao5=""> Multi-factor authentication (MFA) is a security mechanism that requires users to provide at least two forms of identification to complete the authentication process. It adds an extra layer of security that significantly reduces the risk of unauthorized access. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/mfa" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Multi-tenancy </h3> <p data-astro-cid-dohjnao5=""> Multi-tenancy is a software architecture where a single application instance serves multiple customers (tenants), keeping their data isolated and secure. It’s common in cloud computing and SaaS to optimize resources and simplify maintenance. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/multi-tenancy" data-astro-cid-dohjnao5="">Learn more</a> </section>  </div> </section><section data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">O</h2> <div data-astro-cid-j7pv25f6=""> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> OAuth 2.0 </h3> <p data-astro-cid-dohjnao5=""> OAuth 2.0 is a widely used authorization framework that allows an application (client) to obtain limited access to protected resources on behalf of a user or the application itself. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/oauth-2.0" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> OAuth 2.0 grant </h3> <p data-astro-cid-dohjnao5=""> An OAuth 2.0 authorization grant (sometimes referred to as an "OAuth 2.0 grant type" or "OAuth 2.0 flow"), is a method used by clients to obtain an access token from an authorization server. It is an essential part for OAuth clients to authenticate and authorize identities. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/oauth-2.0-grant" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> OAuth 2.1 </h3> <p data-astro-cid-dohjnao5=""> OAuth 2.1 is a proposed update to the OAuth 2.0 authorization framework that aims to improve security and usability by deprecating insecure flows and introducing new best practices. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/oauth-2.1" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Offline access </h3> <p data-astro-cid-dohjnao5=""> Offline access allows clients to obtain new access tokens without requiring the user to re-authenticate. It is useful for long-lived sessions and better user experience. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/offline-access" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> One-time password (OTP) </h3> <p data-astro-cid-dohjnao5=""> A One-time password (OTP) is a unique, temporary code that is used for a single transaction or sign-in session. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/otp" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Opaque token </h3> <p data-astro-cid-dohjnao5=""> An opaque token is a type of token whose format is determined by the issuer, typically appearing as a string of characters or numbers, and requires validation by the issuer rather than containing all necessary information for direct validation. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/opaque-token" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> OpenID Connect (OIDC) </h3> <p data-astro-cid-dohjnao5=""> OpenID Connect (OIDC) is an authentication (identity) layer on top of OAuth 2.0, allowing clients to authenticate users and obtain identity information in a standardized way. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/openid-connect" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> OpenID Connect (OIDC) Discovery </h3> <p data-astro-cid-dohjnao5=""> OpenID Connect (OIDC) Discovery is a mechanism that allows clients to automatically discover the OpenID Provider's endpoints and configuration. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/openid-connect-discovery" data-astro-cid-dohjnao5="">Learn more</a> </section>  </div> </section><section data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">P</h2> <div data-astro-cid-j7pv25f6=""> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Passkey </h3> <p data-astro-cid-dohjnao5=""> Passkey is a phishing-resistant and convenient credential that replaces passwords which can be used for sign-in and multi-factor authentication. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/passkey" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Passwordless </h3> <p data-astro-cid-dohjnao5=""> Passwordless is an authentication method that allows users to sign in to computer systems without entering (or remembering) a password or any other knowledge-based secret. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/passwordless" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Proof Key for Code Exchange (PKCE) </h3> <p data-astro-cid-dohjnao5=""> Proof Key for Code Exchange (PKCE) is a security extension for OAuth 2.0 that protects authorization codes from interception and misuse. It is enforced for all types of clients in OAuth 2.1. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/pkce" data-astro-cid-dohjnao5="">Learn more</a> </section>  </div> </section><section data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">R</h2> <div data-astro-cid-j7pv25f6=""> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Redirect URI </h3> <p data-astro-cid-dohjnao5=""> Redirect URI is a URI where the authorization server redirects the user-agent after an authorization request. It is an essential parameter in the OAuth 2.0 and OpenID Connect (OIDC) grants that involve user interaction. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/redirect-uri" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Refresh token </h3> <p data-astro-cid-dohjnao5=""> A refresh token is a long-lived credential used to obtain new access tokens without requiring the user to re-authenticate. It is used to maintain user sessions and provide a better user experience. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/refresh-token" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Resource indicator </h3> <p data-astro-cid-dohjnao5=""> Resource indicator in OAuth 2.0 is an extension parameter defined in RFC 8707 that allows clients to specify the resource server's location in the authorization request. It provides a scalable way to handle multiple resource servers in a single authorization server. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/resource-indicator" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Resource owner </h3> <p data-astro-cid-dohjnao5=""> A resource owner is an identity (usually a user) that has the ability to grant access to a protected resource. In OAuth 2.0, the resource owner can authorize the client to access its resources in a resource server on their behalf. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/resource-owner" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Resource server </h3> <p data-astro-cid-dohjnao5=""> Resource server refers to the server hosting the protected resources that the client wants to access. It also has the responsibility to verify the access tokens and serve the protected resources to the client. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/resource-server" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Role </h3> <p data-astro-cid-dohjnao5=""> A role is a collection of permissions in access control systems that defines what actions users can perform, providing an efficient way to manage and assign access rights to users. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/role" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Role-based access control (RBAC) </h3> <p data-astro-cid-dohjnao5=""> Role-based access control (RBAC) is an access control model that assigns permissions to roles rather than directly to users, providing a flexible and efficient way to manage access rights in systems. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/rbac" data-astro-cid-dohjnao5="">Learn more</a> </section>  </div> </section><section data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">S</h2> <div data-astro-cid-j7pv25f6=""> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Scope </h3> <p data-astro-cid-dohjnao5=""> Scope defines the permissions that an application requests from a user to access their protected resources. It is a fundamental concept in OAuth 2.0 and OIDC that controls the level of access an application can have to a user's data. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/scope" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Security Assertion Markup Language (SAML) </h3> <p data-astro-cid-dohjnao5=""> Security Assertion Markup Language (SAML) is an XML-based standard for exchanging authentication and authorization data between identity providers and service providers. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/saml" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Service provider (SP) </h3> <p data-astro-cid-dohjnao5=""> Service provider (SP) is an application or service that relies on an identity provider (IdP) for authentication and authorization. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/service-provider" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Signing key </h3> <p data-astro-cid-dohjnao5=""> A signing key is a cryptographic key used to sign and verify JSON Web Tokens in OpenID Connect (OIDC). It is used to ensure the integrity and authenticity of the tokens issued by the OpenID provider. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/signing-key" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Single sign-on (SSO) </h3> <p data-astro-cid-dohjnao5=""> Single sign-on (SSO) is an authentication method that allows users to access multiple systems with a single set of credentials. As a key component of identity and access management (IAM) systems, SSO is widely used in modern cloud-based applications and services, simplifying user access and enhancing security. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/single-sign-on" data-astro-cid-dohjnao5="">Learn more</a> </section>  </div> </section><section data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">T</h2> <div data-astro-cid-j7pv25f6=""> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Time-based one-time password (TOTP) </h3> <p data-astro-cid-dohjnao5=""> A time-based one-time password (TOTP) is a temporary, unique code generated by an algorithm that uses the current time as a key factor. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/totp" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Token introspection </h3> <p data-astro-cid-dohjnao5=""> Token introspection is an OAuth 2.0 extension that allows clients to query the authorization server to validate access tokens and retrieve metadata about them. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/token-introspection" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Token request </h3> <p data-astro-cid-dohjnao5=""> Token request refers to the OAuth 2.0 request for exchanging credentials (e.g., authorization code, refresh token) for a set of tokens, typically including one or more of the following: access token, ID token, or refresh token. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/token-request" data-astro-cid-dohjnao5="">Learn more</a> </section>  </div> </section><section data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">U</h2> <section data-astro-cid-dohjnao5="" data-astro-cid-j7pv25f6=""> <h3 data-astro-cid-dohjnao5=""> Userinfo endpoint </h3> <p data-astro-cid-dohjnao5=""> Userinfo endpoint is an OpenID Connect (OIDC) endpoint that provides user information to clients. It is a supplementary endpoint to the ID token and allows clients to retrieve additional user information. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/userinfo-endpoint" data-astro-cid-dohjnao5="">Learn more</a> </section> </section><section data-astro-cid-j7pv25f6=""> <h2 data-astro-cid-j7pv25f6="">W</h2> <div data-astro-cid-j7pv25f6=""> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> WebAuthn </h3> <p data-astro-cid-dohjnao5=""> WebAuthn is an API for accessing public key credentials, facilitating the implementation of passkeys. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/webauthn" data-astro-cid-dohjnao5="">Learn more</a> </section> <section data-astro-cid-dohjnao5=""> <h3 data-astro-cid-dohjnao5=""> Webhook </h3> <p data-astro-cid-dohjnao5=""> Webhooks are a method for web applications to communicate with each other in real-time. They allow one application to send automated messages or information to another application when a specific event occurs. Unlike traditional APIs where one application needs to poll another for updates, webhooks push data to the receiving application as soon as the event happens. </p> <hr data-astro-cid-dohjnao5=""> <a href="https://auth.wiki/webhook" data-astro-cid-dohjnao5="">Learn more</a> </section>  </div> </section> </div></div>]]></description>
        </item>
    </channel>
</rss>