<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 19 Feb 2026 13:30:13 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Mark Zuckerberg to testify in landmark social media trial (102 pts)]]></title>
            <link>https://www.ft.com/content/0c6d8ff6-f207-431b-bfb9-1d8b42bb4b6d</link>
            <guid>47070743</guid>
            <pubDate>Thu, 19 Feb 2026 06:54:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/0c6d8ff6-f207-431b-bfb9-1d8b42bb4b6d">https://www.ft.com/content/0c6d8ff6-f207-431b-bfb9-1d8b42bb4b6d</a>, See on <a href="https://news.ycombinator.com/item?id=47070743">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="site-content" data-ft-origin="next-barrier-page"><div id="barrier-page"><div id="heroOffer-Hero offers-d2ca0378-6db0-4a8a-9b65-74d8603ad8ab" data-component="heroOffer" data-component-unique-name="Hero offers" data-o3-theme="inverse"><div data-o-grid-colspan="12 L6"><p><span></span><span></span><span></span><span>Subscribe to unlock this article</span><span></span></p></div><div data-o-grid-colspan="12 L6"><div><h2><span><span>Save 40% on Standard Digital</span></span></h2><p><span><span><span>was </span><span>Dkr4188</span><span> </span><span>now </span><span>Dkr2499</span><span> for your first year</span></span></span></p></div><p><span><span>Save now on essential digital access to trusted FT journalism on any device. Savings based on monthly annualised price - offer ends 25th February</span></span></p></div></div><div id="recommendedOffers-Recommended Offers" data-component="recommendedOffers" data-component-unique-name="Recommended Offers"><p><h2 data-o-grid-colspan="12">Explore more offers.</h2></p><div data-o-grid-colspan="12"><div data-o-grid-colspan="12"><div><p><img src="https://images.ft.com/v3/image/raw/https%3A%2F%2Fbarrier-page-components.s3.eu-west-1.amazonaws.com%2Fassets%2Ficons%2Fprimary_product_icon_trial.svg?format=svg&amp;source=next-barrier-page" alt=""></p><p><h3>Trial</h3></p></div><p><span><span>Dkr10</span><span> for 4 weeks</span></span></p><p><span><span>Then </span><span>Dkr535</span><span> per month. Complete digital access with exclusive insights and industry deep dives on any device. Cancel anytime during your trial.</span></span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://images.ft.com/v3/image/raw/https%3A%2F%2Fbarrier-page-components.s3.eu-west-1.amazonaws.com%2Fassets%2Ficons%2Fprimary_product_icon_premium.svg?format=svg&amp;source=next-barrier-page" alt=""></p><p><h3>Premium Digital</h3></p></div><p><span><span>Dkr535</span><span> per month</span></span></p><p><span><span>Complete digital access with exclusive insights and industry deep dives on any device.</span></span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://images.ft.com/v3/image/raw/https%3A%2F%2Fbarrier-page-components.s3.eu-west-1.amazonaws.com%2Fassets%2Ficons%2Fprimary_product_icon_print.svg?format=svg&amp;source=next-barrier-page" alt=""></p><p><h3>Print</h3></p></div><p><span><span>was </span><span>Dkr6799</span><span> </span><span>now </span><span>Dkr1459</span><span> for your first year</span></span></p><p><span><span>Delivery Monday - Saturday, including FT Weekend and FT Digital Edition: all the content of the FT newspaper on any device. Savings based on annual price.</span></span></p></div></div></div><div data-component="subscriptionOptions" data-component-unique-name="Subscription Options Offers API" data-o3-theme="inverse"><h2>Explore our full range of subscriptions.</h2><div><div><div><h3>For individuals</h3></div><p>Discover all the plans currently available in your country</p></div><div><div><h3> For multiple readers</h3></div><p>Digital access for organisations. Includes exclusive features and content.</p></div></div></div><div data-component="whyFT" data-component-unique-name="Why FT" data-o3-theme="inverse"><div><h2>Why the FT?</h2><p>See why over a million readers pay to read the Financial Times.</p></div><p><a href="https://subs.ft.com/whytheft?ft-content-uuid=0c6d8ff6-f207-431b-bfb9-1d8b42bb4b6d" aria-label="Find out why the FT">Find out why</a></p></div></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[European Tech Alternatives (222 pts)]]></title>
            <link>https://eutechmap.com/map</link>
            <guid>47070142</guid>
            <pubDate>Thu, 19 Feb 2026 05:07:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eutechmap.com/map">https://eutechmap.com/map</a>, See on <a href="https://news.ycombinator.com/item?id=47070142">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Electrobun v1: Build fast, tiny, and cross-platform desktop apps with TypeScript (128 pts)]]></title>
            <link>https://blackboard.sh/blog/electrobun-v1/</link>
            <guid>47069650</guid>
            <pubDate>Thu, 19 Feb 2026 03:46:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blackboard.sh/blog/electrobun-v1/">https://blackboard.sh/blog/electrobun-v1/</a>, See on <a href="https://news.ycombinator.com/item?id=47069650">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p><strong>Build ultra fast, tiny, and cross-platform desktop apps with TypeScript.</strong></p> <p>Two years ago I paused <a href="https://blackboard.sh/colab/">co(lab)</a> to build the desktop app framework I wished existed. Now that I've shipped a stable v1, this post is me reflecting on this two-year sidequest that had me learning Zig, C, C++, and Objective-C.</p> <h2>Why I Built This</h2> <p>My intro to programming was Visual Basic 6 in the early 2000s, building desktop apps. Then Adobe AIR changed my life—I built a few startups and shipped desktop apps to thousands of people. That was the golden age for me. Fast forward 20+ years of zero-to-one work at startups, building and scaling key systems as early eng at unicorns shipping to 10s of millions, and starting my own startup lab; somehow desktop development had gotten worse. I was building co(lab), a hybrid web browser + code editor + PTY terminal, and I just ran into one papercut too many.</p> <p>The first version was built in Electron. The DX was rough—figuring out code signing, notarization, distribution, and updates felt like fighting the framework instead of building my app. I wanted to ship like the web, continuously, but every piece of the toolchain made that harder than it needed to be.</p> <p>I tried Tauri but Rust is not for everyone. Bun was up and coming, still months away from 1.0. So I got to work.</p> <h2>From macOS to Everywhere</h2> <p>When Electrobun started, it only built macOS apps. Today it has first-class support for building on and distributing to <strong>macOS, Windows, and Ubuntu</strong>. Installers, auto-update artifacts, differential patches—all generated automatically. Bring your own static host (R2, S3, GitHub Releases) and you're done. The differential updates are powered by <a href="https://blackboard.sh/blog/introducing-zig-bsdiff">zig-bsdiff</a>, which I ported from C to Zig and optimized with SIMD and zstd.</p> <p>As Bun's FFI stabilized, I replaced most of the Zig FFI layer I'd written and leaned on Bun directly. The architecture inverted in a good way—Bun uses shared memory when spinning up workers, so Electrobun stays efficient even with multiple processes.</p> <h2>What Shipped</h2> <p>Electrobun today is a complete framework: cross-platform window controls, menus, accelerators, global shortcuts, clipboard, dialogs, webview partitions, session storage, find-in-page, and solid tooling around bundling and updates.</p> <p>The OOPIF story is real now. Electron's <code>&lt;webview&gt;</code> tag was deprecated from Chromium and they still haven't fixed it. <a href="https://blackboard.sh/blog/building-a-better-oopif">Building a Better OOPIF</a> goes deep on how <code>&lt;electrobun-webview&gt;</code> evolved into a true "super iframe"—DOM positioning, process isolation, layering that actually works, and no cursor flicker nightmares. It works across platforms without patching browser engines.</p> <h2>What's Next</h2> <p>co(lab) is fully rewritten on Electrobun and I'm doubling down on it now that v1 is real. The framework is stable enough to build ambitious, long-lived products without worrying about platform churn. That was always the goal.</p> <p>The community is growing. People in our Discord are building insanely cool apps. If you've tested betas, filed issues, or sent feedback—thank you. Electrobun is the first major thing I'm shipping from <a href="https://blackboard.sh/">Blackboard</a>, and you helped shape it.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anthropic officially bans using subscription auth for third party use (459 pts)]]></title>
            <link>https://code.claude.com/docs/en/legal-and-compliance</link>
            <guid>47069299</guid>
            <pubDate>Thu, 19 Feb 2026 02:52:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://code.claude.com/docs/en/legal-and-compliance">https://code.claude.com/docs/en/legal-and-compliance</a>, See on <a href="https://news.ycombinator.com/item?id=47069299">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Step 3.5 Flash – Open-source foundation model, supports deep reasoning at speed (132 pts)]]></title>
            <link>https://static.stepfun.com/blog/step-3.5-flash/</link>
            <guid>47069179</guid>
            <pubDate>Thu, 19 Feb 2026 02:32:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://static.stepfun.com/blog/step-3.5-flash/">https://static.stepfun.com/blog/step-3.5-flash/</a>, See on <a href="https://news.ycombinator.com/item?id=47069179">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <div id="hero">
                
                
                
                <div id="teaserChartArea">
                            
                            <div id="teaserPlotArea">
<div>
    <p><img src="https://static.stepfun.com/blog/step-3.5-flash/assets/logos/stepfun-color-logo.svg" alt="Step 3.5 Flash" onerror="this.style.display='none';this.parentElement.innerHTML+='<span class=\'teaser-point-initial\'>S</span>';">
    </p>
    <p>81.0</p>
    
</div>
<div>
    <p><img src="https://static.stepfun.com/blog/step-3.5-flash/assets/logos/glm-logo.svg" alt="GLM-4.7" onerror="this.style.display='none';this.parentElement.innerHTML+='<span class=\'teaser-point-initial\'>G</span>';">
    </p>
    <p>78.5</p>
    
</div>
<div>
    <p><img src="https://static.stepfun.com/blog/step-3.5-flash/assets/logos/deepseek-color-logo.svg" alt="DeepSeek V3.2" onerror="this.style.display='none';this.parentElement.innerHTML+='<span class=\'teaser-point-initial\'>D</span>';">
    </p>
    <p>77.3</p>
    
</div>
<div>
    <p><img src="https://static.stepfun.com/blog/step-3.5-flash/assets/logos/k-only-light.svg" alt="Kimi K2.5" onerror="this.style.display='none';this.parentElement.innerHTML+='<span class=\'teaser-point-initial\'>K</span>';">
    </p>
    <p>80.5</p>
    
</div>
<div>
    <p><img src="https://static.stepfun.com/blog/step-3.5-flash/assets/logos/gemini-color-logo.svg" alt="Gemini 3.0 Pro" onerror="this.style.display='none';this.parentElement.innerHTML+='<span class=\'teaser-point-initial\'>G</span>';">
    </p>
    <p>80.7</p>
    
</div>
<div>
    <p><img src="https://static.stepfun.com/blog/step-3.5-flash/assets/logos/claude-color-logo.svg" alt="Claude Opus 4.5" onerror="this.style.display='none';this.parentElement.innerHTML+='<span class=\'teaser-point-initial\'>C</span>';">
    </p>
    <p>80.6</p>
    
</div>
<div>
    <p><img src="https://static.stepfun.com/blog/step-3.5-flash/assets/logos/openai-logo.svg" alt="GPT-5.2 xhigh" onerror="this.style.display='none';this.parentElement.innerHTML+='<span class=\'teaser-point-initial\'>G</span>';">
    </p>
    <p>82.2</p>
    
</div>
</div>
                            <div>
                                <div id="teaserXTicks"><p>200</p><p>400</p><p>600</p><p>800</p><p>1000</p><p>Likely &gt;1000</p></div>
                                <p>Total Model Parameters (B)</p>
                            </div>
                        </div>
                <p>Scores represent the mean of the following eight benchmarks listed below, excluding xbench-DeepSearch. The Step 3.5 Flash score is derived under standard settings (i.e., $w/o$ Parallel Thinking).</p>
                <div>
                    <p data-content="hero.abstract"><strong>Step 3.5 Flash</strong> is our most capable open-source foundation model, engineered to deliver frontier reasoning and agentic capabilities with exceptional efficiency. Built on a sparse Mixture of Experts (MoE) architecture, it selectively activates only <strong>11B of its 196B parameters</strong> per token. This "intelligence density" allows it to rival the reasoning depth of top-tier proprietary models, while maintaining the agility required for real-time interaction.</p>
                    <div>
                        <ul data-content="hero.highlights">
                            <li>
                                <strong>Deep Reasoning at Speed:</strong>
                                <span>While chatbots are built for reading, agents must reason fast. Powered by 3-way Multi-Token Prediction (MTP-3), Step 3.5 Flash achieves a generation throughput of 100–300 tok/s in typical usage (peaking at 350 tok/s for single-stream coding tasks). This allows for complex, multi-step reasoning chains with immediate responsiveness.</span>
                            </li>
                            <li>
                                <strong>A Robust Engine for Coding &amp; Agents:</strong>
                                <span>Step 3.5 Flash is purpose-built for agentic tasks, integrating a scalable RL framework that drives consistent self-improvement. It achieves 74.4% on SWE-bench Verified and 51.0% on Terminal-Bench 2.0, proving its ability to handle sophisticated, long-horizon tasks with unwavering stability.</span>
                            </li>
                            <li>
                                <strong>Efficient Long Context:</strong>
                                <span>The model supports a cost-efficient 256K context window by employing a 3:1 Sliding Window Attention (SWA) ratio—integrating three SWA layers for every one full-attention layer. This hybrid approach ensures consistent performance across massive datasets or long codebases while significantly reducing the computational overhead typical of standard long-context models.</span>
                            </li>
                            <li>
                                <strong>Accessible Local Deployment:</strong>
                                <span>Optimized for accessibility, Step 3.5 Flash brings elite-level intelligence to local environments. It runs securely on high-end consumer hardware (e.g., Mac Studio M4 Max, NVIDIA DGX Spark), ensuring data privacy without sacrificing performance.</span>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>

        <div id="benchmark">
                <div>

                    
                    <div id="mathCategory">
                        <h4>Reasoning</h4>
                        
                    </div>

                    
                    <div id="codingCategory">
                        <h4>Coding</h4>
                        
                    </div>

                    
                    <div id="agentCategory">
                        <h4>Agent</h4>
                        <div data-benchmark="browsecompZHContext">
            <div>
                        <p><img src="https://static.stepfun.com/blog/step-3.5-flash/assets/logos/stepfun-dark-logo.png" alt="Step 3.5 Flash" onerror="this.style.display='none';this.parentElement.innerHTML+='<span class=\'bar-initial\'>S</span>';"></p><p>56.3</p>
                        
                    </div>
            <div>
                        <p><img src="https://static.stepfun.com/blog/step-3.5-flash/assets/logos/stepfun-dark-logo.png" alt="StepFun Research" onerror="this.style.display='none';this.parentElement.innerHTML+='<span class=\'bar-initial\'>S</span>';"></p><p>35.0</p>
                        
                    </div>
            <div>
                        <p><img src="https://static.stepfun.com/blog/step-3.5-flash/assets/logos/k-only-light.svg" alt="Kimi K2.5 (Thinking)" onerror="this.style.display='none';this.parentElement.innerHTML+='<span class=\'bar-initial\'>K</span>';"></p><p>40.0</p>
                        
                    </div>
            <div>
                        <p><img src="https://static.stepfun.com/blog/step-3.5-flash/assets/logos/manus.svg" alt="Manus Agent (Quality Mode)" onerror="this.style.display='none';this.parentElement.innerHTML+='<span class=\'bar-initial\'>M</span>';"></p><p>40.0</p>
                        
                    </div>
            <div>
                        <p><img src="https://static.stepfun.com/blog/step-3.5-flash/assets/logos/grok-logo.svg" alt="SuperGrok Expert" onerror="this.style.display='none';this.parentElement.innerHTML+='<span class=\'bar-initial\'>G</span>';"></p><p>40.0</p>
                        
                    </div>
            <div>
                        <p><img src="https://static.stepfun.com/blog/step-3.5-flash/assets/logos/openai-logo.svg" alt="ChatGPT-5-Pro" onerror="this.style.display='none';this.parentElement.innerHTML+='<span class=\'bar-initial\'>G</span>';"></p><p>75.0</p>
                        
                    </div></div>
                    </div>

                </div>

                <p>Performance of Step 3.5 Flash measured across <strong>Reasoning, Coding, and Agentic Tasks</strong>. Open-source models (left) are sorted by their total parameter count, while top-tier proprietary models are shown on the right. xbench-DeepSearch scores are sourced from <a href="https://xbench.org/agi/aisearch" target="_blank" rel="noopener">official publications</a> for consistency. The shadowed bars represent the enhanced performance of Step 3.5 Flash using <a href="https://arxiv.org/pdf/2601.05593" target="_blank" rel="noopener">Parallel Thinking</a>.</p>

            </div>

        <div id="showcase">
                <h2>Step 3.5 Flash: Intelligence in Practice</h2>
                <p>True intelligence density is not just about peak performance on conventional benchmarks, but about robustness in dynamic, real-world scenarios. While we value strong results on standard metrics as a foundation, our primary goal is to validate that the model functions as a resilient and effective partner when facing the unpredictability of actual execution.</p>
                <p>In the following part, we consolidate a range of performance feedback from real-world showcases, rigorous internal benchmarks, and supplemental public leaderboards. Covering everything from advanced reasoning in math and coding to everyday interaction capabilities, these results demonstrate that Step 3.5 Flash is not just fast enough to think—it is <strong>Reliable Enough to Act</strong>.</p>

                
                

                
                <div id="showcase-agentic-coding">
                    

                    <div>
                        <div>
                            <video controls="" preload="metadata">
                                <source src="https://static.stepfun.com/blog/step-3.5-flash/raw_case/coding_case/Earth.mp4" type="video/mp4">
                            </video>
                            <p>Tactical Weather Intelligence Dashboard — A flight-cockpit inspired 3D globe visualizer engineered for high-density data environments. Featuring a custom WebGL 2.0 engine, it manages 15,000+ active nodes with real-time WebSocket telemetry. This case demonstrates our model's ability to build low-latency data pipelines and high-performance geospatial visualizations with a focus on system stability and professional-grade UI/UX.</p>
                            
                        </div>

                        <div>
                            <video controls="" preload="metadata">
                                <source src="https://static.stepfun.com/blog/step-3.5-flash/raw_case/coding_case/Ocean_v2_4k.mp4" type="video/mp4">
                            </video>
                            <p>Three.js Procedural Ocean Engine — A high-performance rendering system featuring fractal-based wave geometry and ray-traced surfaces. It leverages Fresnel reflectance and PBR materials for photorealistic lighting. This showcase highlights our model's expertise in Computer Graphics (CG), complex rendering pipeline design, and seamless integration of Three.js/GLSL/Shadertoy workflows.</p>
                            
                        </div>

                        <div>
                            <video controls="" preload="metadata">
                                <source src="https://static.stepfun.com/blog/step-3.5-flash/raw_case/coding_case/Agentic_Coding_with_text.mp4" type="video/mp4">
                            </video>
                            <p>Agentic Workflow Take In — A case demonstrates how Step assists in executing daily data processes, achieving end-to-end data production. It aligns upstream data formats, accurately calls data generation models, verifies and transforms the results, and generates workflow reports, embodying the core concept of Agent-in-the-loop. Step can effectively take over our daily workflows, undertaking complex and repetitive processes.</p>
                            
                        </div>

                        <div>
                            <video controls="" preload="metadata">
                                <source src="https://static.stepfun.com/blog/step-3.5-flash/raw_case/coding_case/SolarDisplay.mp4" type="video/mp4">
                            </video>
                            <p>Epic Solar System Simulation — A 3D interactive model of the solar system with cinematic lighting and atmosphere, presenting a shocking visual narrative from nothingness to a complete galaxy through an epic opening performance of dynamically generated and orbiting planets one by one. This demonstrates Step comprehensive creative ability in 3D scene orchestration, lighting and atmosphere creation, and control of interactive narrative rhythm.</p>
                            
                        </div>

                        <div>
                            <video controls="" preload="metadata">
                                <source src="https://static.stepfun.com/blog/step-3.5-flash/raw_case/coding_case/DataAnaly_en_4k_v2.mp4" type="video/mp4">
                            </video>
                            <p>Autonomous Business Intelligence Engine — End-to-end data processing—from CSV ingestion to Cubic Spline interpolation and multi-scenario forecasting. Demonstrates high-order reasoning in multi-step tool use, automated error correction during code execution, and complex data visualization. Successfully modeled a 60% DNU drop scenario, identifying a 1.6x quality gap between acquisition channels. It reflects the model's agentic strength in systematic problem solving and its ability to act as a self-directed Data Scientist.</p>
                            
                        </div>

                        <div>
                            <video controls="" preload="metadata">
                                <source src="https://static.stepfun.com/blog/step-3.5-flash/raw_case/coding_case/Deepresearch_explore.mp4" type="video/mp4">
                            </video>
                            <p>Autonomous Large-Scale Repository Architect — A specialized agentic workflow for navigating and deciphering high-complexity codebases. Beyond simple file scanning, the model performs deep-trace logic mapping and cross-module dependency analysis to synthesize the "mental model" of an entire ecosystem. This showcase demonstrates the model's superior cognitive capacity for large-scale software architecture, enabling it to autonomously generate professional Wikis that connect high-level design patterns to low-level implementation details across thousands of lines of code.</p>
                            
                        </div>
                    </div>

                    
                    <div>
                        <p><strong>Beyond Vibe Coding - Driving Professional Data Agent in Claude Code.</strong> Within advanced agent frameworks like Claude Code, LLMs have evolved beyond "vibe coding" to becoming active problem-solvers capable of driving complex workflows to accomplish sophisticated objectives. To evaluate this in a real-world context, we task Step 3.5 Flash to act as a professional data analyst within the Claude Code environment.</p>
                        <p>We curate a benchmark of 50 end-to-end tasks that reflect the intricate nature of Internet backend data analysis. As shown in the table below, Step 3.5 Flash demonstrates exceptional proficiency in managing these multi-stage processes—independently handling data ingestion, cleaning, feature construction, and results interpretation. With a score of 39.58%, it proves to be a robust engine for sophisticated agentic systems, outperforming several frontier models in analytical accuracy.</p>

                        <div>
                            <div>
                                <h3>Professional Data Analysis Benchmark</h3>
                                
                            </div>
                            <p>We notice that frontier models like Gemini 3.0 Pro didn't perform as expected in this specific test. This could be due to framework compatibility issues within Claude Code, or simply a difference in analytical capability. Either way, the takeaway here is how well Step 3.5 Flash syncs with the Claude Code, enabling it to handle professional data tasks with solid reliability.</p>
                        </div>
                    </div>
                </div>

                
                <div id="showcase-deep-research">
                    
                    <p>While Step 3.5 Flash is compact, its utility is no longer limited by its internal parametric knowledge. In the agentic era, the ability to leverage the internet as a dynamic knowledge base is more critical than static memory—a strength proven by Step 3.5 Flash's performance on benchmarks like xbench-DeepSearch and BrowserComp.</p>
                    <p>Deep Research extends basic information retrieval by delegating the entire research workflow to an agentic loop of planning, searching, reflecting, and writing. To evaluate Step 3.5 Flash on this complex process, we use the Scale AI <a href="https://scale.com/research/researchrubrics" target="_blank" rel="noopener">Research Rubrics</a>, a benchmark designed to assess the factual grounding and reasoning depth of long-form research. Our implementation facilitates this through a single-agent loop based on a ReAct architecture, natively integrating specialized tools such as <em>batch_web_surfer</em> and <em>shell</em> for iterative investigations. This approach allows Step 3.5 Flash to achieve a score of 65.27%, delivering research quality that competes with OpenAI and Gemini Deep Research while maintaining significantly higher inference efficiency.</p>

                    
                    <div>
                        <div>
                            <h3>Performance on <span>ResearchRubrics</span></h3>
                            <div>
                                <div>
                                    <p><span>Step 3.5 Flash</span></p>
                                    <p><span>65.3</span>
                                    <span>ReAct Agent</span>
                                </p></div>
                                <div>
                                    <p><span>Gemini DeepResearch</span></p>
                                    <p><span>63.7</span>
                                    <span>Agent System</span>
                                </p></div>
                                
                                <div>
                                    <p><span>OpenAI DeepResearch</span></p>
                                    <p><span>60.7</span>
                                    <span>Agent System</span>
                                </p></div>
                                <div>
                                    <p><span>Qwen DeepResearch</span></p>
                                    <p><span>49.2</span>
                                    <span>Agent System</span>
                                </p></div>
                            </div>
                        </div>
                        <p>We evaluated commercial agents by collecting reports from their official web interfaces (captured Dec 2–15, 2025) under default configurations, while our internal models utilized the ReAct framework for report generation. All outputs were subsequently appraised by an LLM judge using a ternary grading for each criterion.</p>
                    </div>

                    
                    <div>
                        <p>We demonstrate Step 3.5 Flash's exceptional Deep Research capabilities through a case study on early childhood science education. In this instance, Step 3.5 Flash synthesized a comprehensive research report of approximately 10,000 words, distilling complex neuroplasticity theories into an actionable, expert-grade guide for ages 0–3. The output bridges theoretical milestones with practical "Parental Scripts," reframing sensory play as structured inquiry while maintaining a rigorous focus on both cognitive depth and safety guidance.</p>
                        

                        <p><strong>Multi-Agent Orchestration Framework.</strong> Step 3.5 Flash also natively supports a multi-agent architecture where a Master Agent orchestrates complex tasks through autonomous planning and dynamic routing. This hierarchical framework dispatches specialized Search and Verify agents to handle retrieval and factual grounding via parallel tool-invocation loops. To ensure precision, a Summary Agent consolidates each sub-agent's trajectory into structured feedback, enabling the Master Agent to synthesize a final, coherent response.</p>
                        <div data-case-file="raw_case/dr_bmk_multi_agent/case1.json" data-multi-agent="true">
                                
                                <p><span>Multi-Agent Deep Research</span>
                            </p></div>
                    </div>
                </div>

                
                <div id="showcase-edge-cloud">
                    

                    <div>
                        <div>
                            <video controls="" preload="metadata">
                                <source src="https://static.stepfun.com/blog/step-3.5-flash/raw_case/Edge_cloud_Collaboration/gui/gui-case1.mp4" type="video/mp4">
                            </video>
                            <p>In this case, the user asks to search for the latest Arxiv papers on GUI Agents, summarize them, and immediately share the result via WeChat. Step 3.5 Flash, acting as the 'Cloud Brain,' first executes the search and summarization in the cloud for maximum speed. Once the content is ready, it triggers the 'Hand'—our on-device <a href="https://github.com/stepfun-ai/gelab-zero" target="_blank" rel="noopener">Step-GUI</a>—to wake up the phone, open WeChat, and deliver the message to the specific contact. This is Cloud-Device Synergy in action.</p>
                        </div>

                        <div>
                            <video controls="" preload="metadata">
                                <source src="https://static.stepfun.com/blog/step-3.5-flash/raw_case/Edge_cloud_Collaboration/gui/gui-case2.mp4" type="video/mp4">
                            </video>
                            <p>In this case, the user asks to compare Mac Mini M4 prices across platforms. Step 3.5 Flash, acting as the 'Cloud Brain,' decomposes this complex request into specific sub-tasks for Taobao, JD.com, and Pinduoduo. This cloud-side planning significantly lowers the difficulty for the on-device <a href="https://github.com/stepfun-ai/gelab-zero" target="_blank" rel="noopener">Step-GUI</a>, ensuring higher success rates as it retrieves real-time data from each app. Step 3.5 Flash then synthesizes the results to identify Pinduoduo as the cheapest option and offers a buying guide. This demonstrates <strong>Cloud-Device Synergy</strong>: cloud intelligence simplifies local execution for reliable results.</p>
                        </div>
                    </div>

                    <p>Furthermore, we conduct a comparative evaluation on the <a href="https://arxiv.org/pdf/2512.15431" target="_blank">AndroidDaily Hard</a> subset, a benchmark tailored for Chinese mobile application scenarios encompassing e-commerce, entertainment, and other daily tasks.</p>

                    <div>
                            <h3>Performance on AndroidDaily Hard</h3>
                            <div>
                                    <p><span>Step 3.5 Flash + Step-GUI</span></p>
                                    <p><span>57</span>
                                    <span>Edge–Cloud</span>
                                </p></div>
                        </div>

                    <p>We compare two paradigms: (1) single-agent <a href="https://github.com/stepfun-ai/gelab-zero" target="_blank" rel="noopener">Step-GUI</a> executing tasks independently on-device, and (2) an edge-cloud collaborative framework integrating Step 3.5 Flash with <a href="https://github.com/stepfun-ai/gelab-zero" target="_blank" rel="noopener">Step-GUI</a> via GUI-MCP. The results demonstrate that utilizing Step 3.5 Flash as the cloud-based host agent to orchestrate <a href="https://github.com/stepfun-ai/gelab-zero" target="_blank" rel="noopener">Step-GUI</a> significantly enhances the system's performance in complex scenarios.</p>
                </div>

                
                <div id="showcase-math">
                    
                    <p>Step 3.5 Flash demonstrates exceptional logical rigor in competition-level math. Through the deep analysis of IMO Shortlisted problems, the model proves its core strength in complex symbolic reasoning and abstract structural synthesis.</p>

                    <div>
                        
                        <div id="mathCase1">
                            
                            
                            <p>The problem seeks to characterize all real numbers \(\alpha\) such that the sum of the floor functions \(S_n = \sum_{k=1}^n \lfloor k\alpha\rfloor\) is always divisible by \(n\). The primary difficulty lies in the fact that \(\alpha\) is a real number, requiring one to separate its integer part \(m\) and fractional part \(\theta\) to analyze how the summation interacts with the modularity of \(n\). The core insight of the proof is reducing the problem to the behavior of the fractional sum \(T_n = \sum_{k=1}^n \lfloor k\theta\rfloor\) and employing induction to show that the divisibility constraints force extreme values for the floor functions.</p>
                        </div>

                        
                        <div id="mathCase2">
                            
                            
                            <p>The problem asks whether a specific inequality involving the sums of exponential terms \(3^{a_n}\) and \(2^{a_n}\) must hold for at least one \(n\) in any sequence of positive real numbers. The primary difficulty lies in the potentially divergent behavior of the numerator and denominator, which makes it non-obvious whether the ratio ever drops below a fixed constant like \(1/2024\). The core insight of the proof is to perform a change of variables \(x_i = 2^{a_i}\) and identify the power \(\alpha = \log_2 3\), transforming the expression into a ratio of power sums \(\frac{\sum x_i^\alpha}{(\sum x_i)^2}\).</p>
                        </div>
                    </div>
                </div>

                
                <div id="showcase-reliability">
                    

                    <p><strong>We also care about interaction reliability</strong>—the model's ability to not just solve problems, but to engage users with precision and professional judgment. To test this, we evaluated Step 3.5 Flash across two critical dimensions:</p>

                    <ul>
                        <li><strong>Proactive Intent Clarification</strong>: In our internal benchmark of 74 ambiguous real-world requests (primarily localized queries), Step 3.5 Flash consistently identified missing information and asked targeted questions to clarify user intent rather than making assumptions.</li>
                    </ul>

                    <div>
                            <h3>Proactive Intent Clarification</h3>
                            
                        </div>

                    <ul>
                        <li><strong>Advisory &amp; Consultation</strong>: Across 500 prompts in a balanced bilingual setting spanning life, learning, and workplace contexts, the model demonstrated solid domain knowledge and a professional style, maintaining high instruction-following standards in both English and Chinese.</li>
                    </ul>

                    <div>
                        <table>
                            <thead>
                                <tr>
                                    <th>Model</th>
                                    <th>Average</th>
                                    <th>Usefulness</th>
                                    <th>Logic</th>
                                    <th>Tone</th>
                                    <th>Instruction-following</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr><td>GPT-5.2</td><td>77.8%</td><td>77.2%</td><td>81.9%</td><td>73.0%</td><td>79.6%</td></tr>
                                <tr><td>Gemini 3.0 Pro</td><td>70.6%</td><td>73.9%</td><td>61.7%</td><td>72.3%</td><td>74.4%</td></tr>
                                <tr><td>Step 3.5 Flash</td><td>70.5%</td><td>73.3%</td><td>62.1%</td><td>72.4%</td><td>74.2%</td></tr>
                                <tr><td>Deepseek V3.2</td><td>70.3%</td><td>72.5%</td><td>64.4%</td><td>71.2%</td><td>72.9%</td></tr>
                                <tr><td>Claude Opus 4.5</td><td>68.5%</td><td>69.7%</td><td>66.5%</td><td>65.9%</td><td>72.1%</td></tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>

        

        <div id="techniques">
                <h2>The Engine Behind</h2>

                <div>
                    <div>
                        <h3 data-content="method.subsection_3_1_title">Architecture Optimized for Flash-Speed Decoding and Inference</h3>

                        <p>The architecture of <strong>Step 3.5 Flash</strong> is defined by a model-system co-design that prioritizes <strong>inference cost and speed</strong> as the core architectural constraint. We employ a <strong>Sparse Mixture-of-Experts (MoE)</strong> backbone to decouple global model capacity from per-token computation. While the total knowledge base spans <strong>196B parameters</strong>, the system only activates <strong>11B parameters per token</strong> during inference. To further reduce <strong>memory overhead</strong>, we strategically utilize dense layers for the first few layers of the network for high intelligence density.</p>

                        <p>To navigate the quadratic bottleneck of long-context processing, we leverage a hybrid attention layout that interleaves <strong>Sliding-Window Attention (SWA)</strong> with <strong>Full Attention</strong> at a 3:1 ratio. We specifically opted for SWA over linear alternatives to maintain the architectural flexibility required for <strong>speculative decoding</strong>. SWA is inherently compatible with <strong>Multi-Token Prediction (MTP)</strong> heads. These heads predict additional future tokens in parallel with the primary output, enabling <strong>parallel verification</strong>. This allows the model to validate multiple token hypotheses in a single pass, effectively breaking the serial constraints of standard autoregressive decoding.</p>

                        <p>To ensure this lightweight hybrid structure retains peak performance, we implemented two critical enhancements. We utilized an <strong>augmented query-head count</strong> in the SWA layers—increasing from 64 to 96—to strengthen representational power without expanding the \(KV\) cache footprint. This modification is highly efficient: since the attention window is fixed, the computational cost of these additional heads remains constant regardless of total sequence length. This allows us to scale up model expressiveness without the "long-context penalty" where attention costs usually explode as the conversation grows. Complementing this is our <strong><a href="https://arxiv.org/abs/2505.06708" target="_blank" rel="noopener">Head-wise Gated Attention</a></strong>, which functions as an input-dependent attention sink. By dynamically modulating information flow, this mechanism preserves numerical stability while incurring negligible overhead.</p>

                        <p>These strategic architectural refinements demonstrate that frontier-level reasoning can be decoupled from prohibitive latency. By integrating <strong>sparse-active execution</strong> with <strong>concurrent token verification</strong>, the model achieves a decoding throughput up to <strong>350 tokens per second (TPS)</strong> on NVIDIA Hopper GPUs while running SWE-bench Verified.</p>

                        <p><strong>Last but not least</strong>, the <strong>optimized total parameter scale</strong> of Step 3.5 Flash facilitates highly accessible, local inference. By consolidating its total capacity to a scale compatible with high-end personal hardware, the model supports high-fidelity private deployment on workstations such as the <strong>Apple M4 Max</strong>, <strong>NVIDIA DGX Spark</strong>, or <strong>AMD AI Max+ 395</strong>, providing a 100% trusted execution environment.</p>

                        <div>
                            <p><img src="https://static.stepfun.com/blog/step-3.5-flash/assets/step3.5.svg" alt="Architecture"></p><p>The overall architecture of Step 3.5 Flash.</p>
                        </div>

                        <p>As the local deployment of large language models (LLMs) becomes increasingly prevalent, we have successfully adapted the Step 3.5 Flash to NVIDIA DGX Spark 128GB device based on the edge-side inference engine llama.cpp, and simultaneously released the INT4 quantized model weights in GGUF format. On NVIDIA DGX Spark, the Step 3.5 Flash achieves a generation speed of 20 tokens per second; by integrating the INT8 quantization technology for KVCache, it supports an extended context window of up to 256K tokens, thus delivering long text processing capabilities on par with cloud-based inference. The new model can be tested by developers on NVIDIA accelerated infrastructure via <a href="https://build.nvidia.com/" target="_blank">build.nvidia.com</a>.</p>
                    </div>

                    <div>
                        <h3 data-content="method.subsection_3_2_title">Scalable RL Unleashes the Reasoning Potential</h3>

                        <p>We introduce a scalable reinforcement learning framework designed to reliably train reasoning and agentic language models at scale.</p>

                        <p>Modern RL pipelines for LLMs rely on high-throughput inference engines to generate rollouts, while optimization happens asynchronously in a separate training system. At scale, this setup introduces two compounding challenges:</p>

                        <ol>
                            <li>Training–inference mismatch, caused by numerical and architectural differences between systems</li>
                            <li>Off-policy drift, as policies evolve while rollouts lag behind</li>
                        </ol>

                        <p>For long reasoning sequences, even minor token-level discrepancies can explode into extreme importance weights—leading to unstable updates, early convergence, or complete training collapse.</p>

                        <p>To address this, we propose <strong>Metropolis Independence Sampling Filtered Policy Optimization (MIS-PO)</strong>, which replaces fragile importance weighting with <strong>strict sample filtering</strong>. Instead of scaling gradients with continuous importance-sampling ratios as in PPO, MIS-PO uses these ratios solely as a <strong>binary acceptance criterion</strong>. Trajectories whose likelihood deviates too far between the inference and training policies are simply excluded from optimization, while accepted samples are treated as effectively on-policy. Concretely, the policy update is driven by</p>

                        <p>
                            \[\mathcal{L}_{actor} = - \mathbb{E}_{\tau \sim \pi_{\theta_\text{vllm}}} \left[ \mathbb{I}(\tau) \cdot \log \pi_\theta(a_t|s_t) \cdot \hat{A}_t \right],\]
                        </p>

                        <p>where the binary indicator \(\mathbb{I}(\tau)\) filters out off-distribution samples. This design dramatically reduces gradient variance and enables stable, long-horizon optimization without aggressive clipping.</p>

                        <p>Our framework also includes <strong>truncation-aware value bootstrapping</strong>, which prevents long reasoning trajectories from being incorrectly penalized when hitting context limits, and <strong>routing confidence monitoring</strong> for Mixture-of-Experts models, providing a practical signal for RL stability at scale.</p>

                        <p>Together, these components turn reinforcement learning into a <strong>reliable engine for continuous self-improvement</strong>, enabling consistent gains across mathematics, coding, and tool use, while remaining stable under large-scale, off-policy training.</p>
                    </div>

                    <div>
                        <p><img src="https://static.stepfun.com/blog/step-3.5-flash/assets/tb_plots_grad_norm_eb.svg" alt="RL Algorithm Ablation"></p><p>Training dynamics of different RL algorithms. Ablations are conducted on the Qwen model.</p>
                    </div>
                </div>
            </div>

        

        <div id="benchmarks">
                <h2>Benchmarks</h2>
                <p>In our benchmark table, we provide a detailed, side-by-side comparison of today's top-performing open-source models. Across a wide range of metrics, Step 3.5 Flash stands out with consistently strong results. Our evaluation focuses on three core dimensions—Reasoning, Coding and Agentic Capability—and visualizes score differences across peer models in a horizontal, at-a-glance format.</p>

                <div>
                    <table>
                        <thead>
                            <tr>
                                <th>Benchmark</th>
                                <th>Step 3.5 Flash</th>
                                <th>DeepSeek V3.2</th>
				<th>Kimi <br>K2 Thinking / K2.5</th>
                                <th>GLM-4.7</th>
                                <th>MiniMax M2.1</th>
                                <th>MiMo-V2 Flash</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td># Activated Params</td>
                                <td>11B</td>
                                <td>37B</td><td>32B</td><td>32B</td><td>10B</td><td>15B</td>
                            </tr>
                            <tr>
                                <td># Total Params (MoE)</td>
                                <td>196B</td>
                                <td>671B</td><td>1T</td><td>355B</td><td>230B</td><td>309B</td>
                            </tr>
                            <tr>
                                <td>Est. decoding cost<br><span>@ 128K context, Hopper GPU**</span></td>
                                <td>1.0x<br><span>100 tok/s, MTP-3, EP8</span></td>
                                <td>6.0x<br><span>33 tok/s, MTP-1, EP32</span></td>
                                <td>18.9x<br><span>33 tok/s, no MTP, EP32</span></td>
                                <td>18.9x<br><span>100 tok/s, MTP-3, EP8</span></td>
                                <td>3.9x<br><span>100 tok/s, MTP-3, EP8</span></td>
                                <td><span>1.2x</span><br><span>100 tok/s, MTP-3, EP8</span></td>
                            </tr>
                            <tr><td></td><td></td><td></td><td>Agent</td><td></td><td></td><td></td></tr>
                            <tr><td>τ²-Bench</td><td>88.2</td><td>80.3 (85.2*)</td><td><span>74.3*</span><span>/</span><span>85.4*</span></td><td>87.4</td><td>86.6*</td><td>80.3 (84.1*)</td></tr>
			    <tr><td>BrowseComp</td><td>51.6</td><td>51.4</td><td>41.5* / <span>60.6</span></td><td>52.0</td><td>47.4</td><td>45.4</td></tr>

			    <tr><td>BrowseComp<br><span>(w/ Context Manager)</span></td><td>69.0</td><td>67.6</td><td><span>60.2</span><span>/</span><span>74.9</span></td><td>67.5</td><td>62.0</td><td>58.3</td></tr>

			    <tr><td>BrowseComp-ZH</td><td>66.9</td><td>65.0</td><td>62.3 / 62.3*</td><td>66.6</td><td>47.8*</td><td>51.2*</td></tr>

			    <tr><td>BrowseComp-ZH<br><span>(w/ Context Manager)</span></td><td>73.7</td><td>—</td><td><span>—</span><span>/</span><span>—</span></td><td>—</td><td>—</td><td>—</td></tr>

			    <tr><td>GAIA<br><span>(no file)</span></td><td>84.5</td><td>75.1*</td><td><span>75.6*</span><span>/</span><span>75.9*</span></td><td>61.9*</td><td>64.3*</td><td>78.2*</td></tr>

			    <tr><td>xbench-DeepSearch<br><span>(2025.05)</span></td><td>83.7</td><td>78.0*</td><td><span>76.0*</span><span>/</span><span>76.7*</span></td><td>72.0*</td><td>68.7*</td><td>69.3*</td></tr>
                            <tr><td>xbench-DeepSearch<br><span>(2025.10)</span></td><td>56.3</td><td>55.7*</td><td><span>—</span><span>/</span><span>40+</span></td><td>52.3*</td><td>43.0*</td><td>44.0*</td></tr>
                            <tr><td><span>ResearchRubrics</span></td><td>65.3</td><td>55.8*</td><td><span>56.2*</span><span>/</span><span>59.5*</span></td><td>62.0*</td><td>60.2*</td><td>54.3*</td></tr>
                            <tr><td></td><td></td><td></td><td>Reasoning</td><td></td><td></td><td></td></tr>
                            <tr><td>AIME 2025</td><td>97.3</td><td>93.1</td><td><span>94.5</span><span>/</span><span>96.1</span></td><td>95.7</td><td>83.0</td><td>94.1 (95.1*)</td></tr>
                            <tr><td>HMMT 2025 (Feb.)</td><td>98.4</td><td>92.5</td><td><span>89.4</span><span>/</span><span>95.4</span></td><td>97.1</td><td>71.0*</td><td>84.4 (95.4*)</td></tr>
                            <tr><td>HMMT 2025 (Nov.)</td><td>94.0</td><td>90.2</td><td><span>89.2*</span><span>/</span><span>—</span></td><td>93.5</td><td>74.3*</td><td>91.0*</td></tr>
                            <tr><td>IMOAnswerBench</td><td>85.4</td><td>78.3</td><td><span>78.6</span><span>/</span><span>81.8</span></td><td>82.0</td><td>60.4*</td><td>80.9*</td></tr>
                            <tr><td></td><td></td><td></td><td>Coding</td><td></td><td></td><td></td></tr>
                            <tr><td>LiveCodeBench-V6</td><td>86.4</td><td>83.3</td><td><span>83.1</span><span>/</span><span>85.0</span></td><td>84.9</td><td>—</td><td>80.6 (81.6*)</td></tr>
                            <tr><td>SWE-bench Verified</td><td>74.4</td><td>73.1</td><td><span>71.3</span><span>/</span><span>76.8</span></td><td>73.8</td><td>74.0</td><td>73.4</td></tr>
                            <tr><td>Terminal-Bench 2.0</td><td>51.0</td><td>46.4</td><td><span>35.7*</span><span>/</span><span>50.8</span></td><td>41.0</td><td>47.9</td><td>38.5</td></tr>
                        </tbody>
                    </table>
                </div>
                <ul>
                    <li>"—" indicates the score is not publicly available or not tested.</li>
                    <li>"*" indicates the original score was inaccessible or lower than our reproduced, so we report the evaluation under the same test conditions as Step 3.5 Flash to ensure fair comparability.</li>
                    <li>BrowseComp (with Context Manager): when the effective context length exceeds a predefined threshold, the agent resets the context and restarts the agent loop. (By contrast, Kimi K2.5 and DeepSeek-V3.2 used a discard-all strategy.)</li>
                    <li>In decoding cost section, decoding **Estimated using a similar but more accurate approach than <a href="https://arxiv.org/abs/2507.19427" target="_blank" rel="noopener">arxiv.org/abs/2507.19427</a></li>
                </ul>
            </div>

        

        <div id="limitations">
                <h2>Known Issues and Future Directions</h2>
                <ol>
                    <li><strong>Token Efficiency.</strong> Step 3.5 Flash achieves frontier-level agentic intelligence but currently relies on longer generation trajectories than Gemini 3.0 Pro to reach comparable quality.</li>
                    <li><strong>Efficient Universal Mastery.</strong> We aim to unify generalist versatility with deep domain expertise. To achieve this efficiently, we are advancing variants of on-policy distillation, allowing the model to internalize expert behaviors with higher sample efficiency.</li>
                    <li><strong>RL for More Agentic Tasks.</strong> While Step 3.5 Flash demonstrates competitive performance on academic agentic benchmarks, the next frontier of agentic AI necessitates the application of RL to intricate, expert-level tasks found in professional work, engineering, and research.</li>
                    <li><strong>Operational Scope and Constraints.</strong> Step 3.5 Flash is tailored for coding and work-centric tasks, but may experience reduced stability during distribution shifts. This typically occurs in highly specialized domains or long-horizon, multi-turn dialogues, where the model may exhibit repetitive reasoning, mixed-language outputs, or inconsistencies in time and identity awareness.</li>
                </ol>
            </div>

        <div id="meet-stepfun">
                <h2>Meet StepFun</h2>
                <ul>
                    <li>
                        <strong>OpenClaw</strong> is a powerful agentic platform that works seamlessly with Step 3.5 Flash.
                        <details>
                            <summary>Quick Setup</summary>
                            <div>
                                <p><strong>Install:</strong> <code>curl -fsSL https://openclaw.ai/install.sh | bash</code></p>
                                <p><strong>Onboard:</strong> Run <code>openclaw onboard</code>.</p>
                                <p><strong>Configure:</strong> In WebUI (Config → Models), add a new provider:</p>
                                <ul>
                                    <li>Type: <code>openai-completions</code> → Base URL: <code>https://api.stepfun.ai/v1</code></li>
                                    <li>Model ID: <code>step-3.5-flash</code> (Context: 256000)</li>
                                </ul>
                            </div>
                        </details>
                        For a full walkthrough, see our <strong><a href="https://github.com/stepfun-ai/Step-3.5-Flash/tree/main/cookbooks/openclaw" target="_blank" rel="noopener">OpenClaw Cookbook</a></strong>.
                    </li>
                    <li>Step 3.5 Flash is available via our <strong>API platform (<a href="https://platform.stepfun.com/docs/zh/llm/reasoning" target="_blank" rel="noopener">中文</a><span>/</span><a href="https://platform.stepfun.ai/docs/en/llm/reasoning" target="_blank" rel="noopener">EN</a>)</strong>, and you can chat with it on the <strong>Web (<a href="https://www.stepfun.com/" target="_blank" rel="noopener">中文</a><span>/</span><a href="https://stepfun.ai/" target="_blank" rel="noopener">EN</a>)</strong> or in our <strong>App (<a href="https://apps.apple.com/cn/app/%E9%98%B6%E8%B7%83ai-%E9%98%B6%E8%B7%83%E6%98%9F%E8%BE%B0ai%E5%8A%A9%E6%89%8B/id6502382318" target="_blank" rel="noopener">iOS</a><span>/</span><a href="https://play.google.com/store/apps/details?id=cn.yuewen.ywapp&amp;hl=zh" target="_blank" rel="noopener">Android</a>)</strong>.</li>
                    <li>Join our <strong><a href="https://discord.gg/RcMJhNVAQc" target="_blank" rel="noopener">Discord community</a></strong> for updates, support, and early access.</li>
                </ul>
            </div>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Minecraft Java is switching from OpenGL to Vulkan (243 pts)]]></title>
            <link>https://www.gamingonlinux.com/2026/02/minecraft-java-is-switching-from-opengl-to-vulkan-for-the-vibrant-visuals-update/</link>
            <guid>47068948</guid>
            <pubDate>Thu, 19 Feb 2026 01:55:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gamingonlinux.com/2026/02/minecraft-java-is-switching-from-opengl-to-vulkan-for-the-vibrant-visuals-update/">https://www.gamingonlinux.com/2026/02/minecraft-java-is-switching-from-opengl-to-vulkan-for-the-vibrant-visuals-update/</a>, See on <a href="https://news.ycombinator.com/item?id=47068948">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						<p>Work continues for the Vibrant Visuals update to come to Minecraft Java, and as part of that they're switching the rendering from OpenGL to Vulkan.</p>
<p><a href="https://www.minecraft.net/en-us/article/another-step-towards-vibrant-visuals-for-java-edition" target="_blank" rel="noopener">Announced</a> today (February 18th) by Mojang developers, it's a huge change for such a game and will take time - but it will be worth it in the end so they can take advantage of all the modern features available for both visual improvements and better performance.</p>
<p><a href="https://uploads.golmedia.net/uploads/articles/article_media/6889725121771426976gol2.webp" data-fancybox="images"><img src="https://uploads.golmedia.net/uploads/articles/article_media/6889725121771426976gol2.webp"></a></p>
<p>They note clearly that their aim is to "keep Minecraft: Java Edition playable for almost any PC-operating system, including macOS and Linux". For the macOS side of things, they'll use a translation layer since Apple don't support Vulkan directly (they made their own API with Metal).</p>
<p>For modders, they're suggesting they start making preparations to move away from OpenGL</p>
<blockquote>
<p>Switching from OpenGL to Vulkan will have an impact on the mods that currently use OpenGL for rendering, and we anticipate that updating from OpenGL to Vulkan will take modders more effort than the updates you undertake for each of our releases. </p>
<p>To start with, we recommend our modding community look at moving away from OpenGL usage. We encourage authors to try to reuse as much of the internal rendering APIs as possible, to make this transition as easy as possible. If that is not sufficient for your needs, then come and talk to us!  </p>
</blockquote>
<p>It does mean that players on really old devices that don't support Vulkan will be left out, but Vulkan has been supported going back to some pretty <em>old</em> GPUs. You've got time though, as they'll be rolling out Vulkan alongside OpenGL in snapshots (development releases) "sometime over the summer". You'll be able to toggle between them during the testing period until Mojang believe it's ready. OpenGL will be entirely removed eventually once they're happy with performance and stability.</p>
<div><p><span>Release Date:</span> <span>8th November 2011</span></p><p><span>Platform:</span> <span>🐧 Native Linux</span></p></div>
						<p><span>Article taken from <a href="https://www.gamingonlinux.com/">GamingOnLinux.com.</a></span>
						
					</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[15 years of FP64 segmentation, and why the Blackwell Ultra breaks the pattern (145 pts)]]></title>
            <link>https://nicolasdickenmann.com/blog/the-great-fp64-divide.html</link>
            <guid>47068890</guid>
            <pubDate>Thu, 19 Feb 2026 01:46:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nicolasdickenmann.com/blog/the-great-fp64-divide.html">https://nicolasdickenmann.com/blog/the-great-fp64-divide.html</a>, See on <a href="https://news.ycombinator.com/item?id=47068890">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>Buy an RTX 5090, the fastest consumer GPU money can buy, and you get 104.8 TFLOPS of FP32 compute. Ask it to do double-precision math and you get 1.64 TFLOPS. That 64:1 gap is not a technology limitation. For fifteen years, the FP64:FP32 ratio has been slowly getting wider on consumer GPUs, widening the divide between consumer and enterprise silicon. Now the AI boom is quietly dismantling that logic.</p>

                    <h2>The Evolution of FP64 on Nvidia GPUs</h2>

                    <p>The FP64:FP32 ratio on Nvidia consumer GPUs has degraded consistently since the Fermi architecture debuted in 2010. On Fermi, the GF100 die shipped to both GeForce and Tesla lines; the hardware supported 1:2 FP64:FP32, but GeForce cards were driver-capped to 1:8.<sup><a href="#fn1" id="fnref1">1</a></sup></p>

                    <p>Over time, Nvidia moved away from “artificially” lowering FP64 performance on consumer GPUs. Instead, the architectural split became structural; the hardware itself is fundamentally different across product tiers. While datacenter GPUs have consistently kept a 1:2 or 1:3 FP64:FP32 performance (until the recent AI boom, more on that later), the performance ratio on consumer GPUs has consistently gotten worse. From 1:8 on the Fermi architecture in 2010 to 1:24 on Kepler in 2012 to 1:32 in 2014 to our final 1:64 ratio on Ampere in 2020.</p>

                    <p>This effectively also means that over 15 years, from the GTX 480 in 2010 to the RTX 5090 in 2025 the FP64 performance on consumer GPUs only increased 9.65x from 0.17 TFLOPS to 1.64 TFLOPS, while in the same time range the FP32 performance improved a whopping 77.63x from 1.35 TFLOP to 104.8 TFLOP.</p>

                    <figure>
                        <img src="https://nicolasdickenmann.com/assets/FP32toFP64.png" alt="FP32 versus FP64 throughput scaling on Nvidia consumer GPUs over time">
                        <figcaption>FP32 vs FP64 throughput scaling across Nvidia GPU generations.<sup><a href="#fn2" id="fnref2-1">2</a></sup></figcaption>
                    </figure>


                    <h2>Nvidia's Move to Segment the Market</h2>
                    <p>So why has FP64 performance on consumer GPUs progressively gotten weaker (in relation to FP32) while it stayed consistently strong on enterprise hardware?</p>

                    <p>If this were purely a technical or cost constraint, you would expect the gap to be smaller. But since historically, Nvidia has taken deliberate steps to limit double-precision (FP64) throughput on GeForce cards, it makes it hard to argue this is accidental. The much simpler explanation is market segmentation.</p>

                    <p>Most consumer workloads, such as gaming, 3d rendering, or video editing do not need FP64. High-performance computing on the other hand has long relied on double precision (FP64). Fields such as computational fluid dynamics, climate modeling, quantitative finance, and computational chemistry depend on numerical stability and precision that single precision (FP32) cannot always provide. So FP64 becomes a very convenient lever: weaken it on consumer GPUs, preserve it on enterprise versions, and you get a clean dividing line between markets. Nvidia has been fairly open about this. In the consumer Ampere GA102 whitepaper, they note "The small number of FP64 hardware units are included to ensure any programs with FP64 code operate correctly.".<sup><a href="#fn3" id="fnref3">3</a></sup></p>

                    <p>And the segmentation worked. Over time, the price gap between consumer GPUs and datacenter GPUs widened from roughly 5x around 2010 to over 20x by 2022. Enterprise cards commanded massive premiums, justified in part by their strong FP64 performance (among other features like ECC memory, NVLink, support contracts, and so on). From a business standpoint, the elegance is obvious: closely related silicon sold into two markets at vastly different margins, with FP64 throughput serving as a clear dividing line.</p>

                    <p>Modern AI training largely does not depend on FP64 though. FP32 works fine, and on the contrary lower precisions (FP16, BF16, FP8, even FP4) are often preferred. Suddenly, consumer GPUs looked surprisingly capable for serious compute workloads. Researchers, startups, and hobbyists could train meaningful models without the purchase of an expensive Tesla or A100. In response, Nvidia updated its GeForce End User License Agreement (EULA) in 2017 to prohibit use of consumer GPUs in datacenters, in a divisive move. In what was (to my knowledge) an unprecedented shift, implicit technical segmentation was replaced by explicit contractual restrictions.<sup><a href="#fn5" id="fnref5">5</a></sup></p>


                    <figure>
                        <img src="https://nicolasdickenmann.com/assets/enterprise_consumer_price_ratio.png" alt="Enterprise vs consumer GPU price ratio over time">
                        <figcaption>Enterprise vs consumer GPU price ratio (2010-2022). Official MSPR numbers for consumer GPU, best effort for enterprise GPUs.<sup><a href="#fn2" id="fnref2-2">2</a></sup></figcaption>
                    </figure>

                    <h2>How FP64 Emulation and AI Is Changing the Game</h2>
                    <p>What if you have an old RTX 4090 lying around at home and, for some reason, you need the precision of FP64 but the built-in FP64 capabilities are not sufficient? Aside from the obvious answer of purchasing enterprise GPU power, FP64 emulation using FP32 floats can be an answer. This concept dates back to 1971, when T. J. Dekker described double-float arithmetic.<sup><a href="#fn6" id="fnref6">6</a></sup></p>

                    <p>The simple idea is to split a 64-bit floating point number into two 32-bit floating point numbers: <code>A = a_hi + a_lo</code>. The <code>a_hi</code> term carries the most significant bits, while <code>a_lo</code> captures the rounding error. Andrew Thall proposed a bunch of common algorithms for emulated FP64s (summation, multiplication, etc.) back in 2007 when GPUs did not have FP64 capabilities.<sup><a href="#fn7" id="fnref7">7</a></sup> You lose 5 bits of precision as your effective mantissa is only 48 bits (twice the FP32 effective mantissa) and not the FP64 53 bits of precision. If a modest reduction in numerical precision is acceptable, you may be able to achieve substantially higher throughput by using emulated double-precision computation. This can be advantageous given the steep FP64-to-FP32 performance disparity, even after accounting for the overhead introduced by emulation.</p>
                    <figure>
                        <img src="https://nicolasdickenmann.com/assets/emulate_double.drawio.png" alt="Diagram of emulated double using high and low parts">
                        <figcaption>Emulated double representation using high and low FP32 parts.</figcaption>
                    </figure>

                    <p>A newer scheme that preserves full 64-bit precision but only works for matrix multiplication is the Ozaki scheme.<sup><a href="#fn8" id="fnref8">8</a></sup> This scheme exploits the speedup of tensor cores (specialized hardware for matrix multiply-accumulate (MMA) operations) and the distributive property of matrix multiplication.<sup><a href="#fn9" id="fnref9">9</a></sup> The Ozaki scheme splits FP64 numbers into, for example, FP8 numbers:</p>
                    <p>A = A<sub>1</sub> + A<sub>2</sub> + A<sub>3</sub> + ... + A<sub>k</sub></p>
                    <p>where A<sub>1</sub> contains the most significant bits and A<sub>2</sub> contains the next slice of bits and so on. We then calculate:</p>
                    <p>A<sub>i</sub> B<sub>i</sub></p>
                    <p>for each A<sub>i</sub> and B<sub>i</sub>. All the results are summed back up in 64-bit precision:</p>
                    <p>AB = Σ A<sub>i</sub> B<sub>i</sub></p>
                    <p>The Ozaki scheme is gaining increasing traction thanks to the abundance of extremely fast FP8 and FP4 tensor cores being deployed for AI workloads. NVIDIA added support for the Ozaki scheme in cuBLAS in October 2025 and plans to continue developing it.<sup><a href="#fn10" id="fnref10">10</a></sup></p>

                    <p>From a GPU manufacturer's perspective, this direction is logical. The majority of enterprise GPU revenue now comes from AI applications; market segmentation based on FP64 performance makes no more sense. Enhancing FP64 emulation through low-precision tensor cores allows a reduction in the relative allocation of dedicated FP64 units in enterprise GPUs while expanding FP8 and FP4 compute resources that directly benefit AI workloads.</p>

                    <p>The latest generation of NVIDIA enterprise GPUs, the B300 based on the Blackwell Ultra architecture, represents a decisive shift toward low precision. FP64 performance has been significantly reduced in favor of more NVFP4 tensor cores, with the FP64:FP32 ratio dropping from 1:2 to 1:64.<sup><a href="#fn11" id="fnref11">11</a></sup> In absolute terms, peak FP64 performance declines from 37 TFLOPS on the B200 to 1.2 TFLOPS on the B300. Paradoxically, instead of consumer hardware catching up to enterprise-class capabilities, enterprise hardware is now embracing constraints traditionally associated with consumer GPUs.</p>

                    <p>Does this signal a gradual replacement of physical FP64 units through emulation? Not necessarily. According to NVIDIA, the company is not abandoning 64-bit computing and plans future improvements to FP64 capabilities.<sup><a href="#fn11" id="fnref11">11</a></sup> Nonetheless, FP64 emulation is here to stay, exploiting the abundance of low-precision tensor cores to supplement hardware FP64 for HPC workloads.</p>

                    <p>But the segmentation logic hasn't disappeared; it may simply be migrating. The RTX 5090 delivers a 1:1 FP16:FP32 ratio, while the B200 sits at 16:1. For fifteen years, FP64 was the dividing line between consumer and enterprise silicon. The next divide may already be taking shape in low-precision floating point.</p>
                    <section aria-label="Footnotes">
                        <hr>
                        <ol>
                            <li id="fn1">AnandTech: GTX 480/470 FP64 ratio discussion (archived). <a href="https://web.archive.org/web/20100402215300/http://www.anandtech.com/show/2977/nvidia-s-geforce-gtx-480-and-gtx-470-6-months-late-was-it-worth-the-wait-/6">https://web.archive.org/web/20100402215300/http://www.anandtech.com/show/2977/nvidia-s-geforce-gtx-480-and-gtx-470-6-months-late-was-it-worth-the-wait-/6</a> <a href="#fnref1" aria-label="Back to content">↩</a></li>
                            <li id="fn2">Google Sheets: numbers used for the computations. <a href="https://docs.google.com/spreadsheets/d/1NHHlgVytLx43DGzP8HlPeOCs7HKPElgpSO__9j6oMFo/edit?usp=sharing">https://docs.google.com/spreadsheets/d/1NHHlgVytLx43DGzP8HlPeOCs7HKPElgpSO__9j6oMFo/edit?usp=sharing</a> <a href="#fnref2-1" aria-label="Back to first graph">↩</a> <a href="#fnref2-2" aria-label="Back to second graph">↩</a></li>
                            <li id="fn3">NVIDIA Ampere GA102 GPU Architecture Whitepaper (PDF). <a href="https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf">https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf</a> <a href="#fnref3" aria-label="Back to content">↩</a></li>
                            <li id="fn4">Alibaba Product Insights: A100 vs RTX 3090—Is the A100 Really Worth the Hype and Extra for Deep Learning? <a href="https://www.alibaba.com/product-insights/a100-vs-rtx-3090-is-the-a100-really-worth-the-hype-and-extra-for-deep-learning.html">https://www.alibaba.com/product-insights/a100-vs-rtx-3090-is-the-a100-really-worth-the-hype-and-extra-for-deep-learning.html</a> <a href="#fnref4" aria-label="Back to content">↩</a></li>
                            <li id="fn5">Wccftech on 2017 GeForce EULA datacenter restriction. <a href="https://wccftech.com/nvidia-geforce-eula-prohibits-datacenter-blockchain-allowed/">https://wccftech.com/nvidia-geforce-eula-prohibits-datacenter-blockchain-allowed/</a> <a href="#fnref5" aria-label="Back to content">↩</a></li>
                            <li id="fn6">T. J. Dekker (1971), double-float arithmetic. <a href="https://csclub.uwaterloo.ca/~pbarfuss/dekker1971.pdf">https://csclub.uwaterloo.ca/~pbarfuss/dekker1971.pdf</a> <a href="#fnref6" aria-label="Back to content">↩</a></li>
                            <li id="fn7">Andrew Thall (2007), Extended-Precision Floating-Point Numbers for GPU Computation. <a href="https://andrewthall.org/papers/df64_qf128.pdf">https://andrewthall.org/papers/df64_qf128.pdf</a> <a href="#fnref7" aria-label="Back to content">↩</a></li>
                            <li id="fn8">Ozaki et al. (2011), Error-Free Transformations for Matrix Multiplication. <a href="https://link.springer.com/article/10.1007/s11075-011-9478-1">https://link.springer.com/article/10.1007/s11075-011-9478-1</a> <a href="#fnref8" aria-label="Back to content">↩</a></li>
                            <li id="fn9">NVIDIA blog: Tensor Cores for Science (ISC 2025). <a href="https://developer.nvidia.com/blog/nvidia-top500-supercomputers-isc-2025/#tensor_cores_for_science%C2%A0">https://developer.nvidia.com/blog/nvidia-top500-supercomputers-isc-2025/#tensor_cores_for_science%C2%A0</a> <a href="#fnref9" aria-label="Back to content">↩</a></li>
                            <li id="fn10">NVIDIA blog: Unlocking Tensor Core Performance with Floating-Point Emulation in cuBLAS. <a href="https://developer.nvidia.com/blog/unlocking-tensor-core-performance-with-floating-point-emulation-in-cublas">https://developer.nvidia.com/blog/unlocking-tensor-core-performance-with-floating-point-emulation-in-cublas</a> <a href="#fnref10" aria-label="Back to content">↩</a></li>
                            <li id="fn11">HPCwire: NVIDIA says it's not abandoning 64-bit computing (Dec 9, 2025). <a href="https://www.hpcwire.com/2025/12/09/nvidia-says-its-not-abandoning-64-bit-computing/">https://www.hpcwire.com/2025/12/09/nvidia-says-its-not-abandoning-64-bit-computing/</a> <a href="#fnref11" aria-label="Back to content">↩</a></li>
                        </ol>
                    </section>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How AI is affecting productivity and jobs in Europe (146 pts)]]></title>
            <link>https://cepr.org/voxeu/columns/how-ai-affecting-productivity-and-jobs-europe</link>
            <guid>47068320</guid>
            <pubDate>Thu, 19 Feb 2026 00:22:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cepr.org/voxeu/columns/how-ai-affecting-productivity-and-jobs-europe">https://cepr.org/voxeu/columns/how-ai-affecting-productivity-and-jobs-europe</a>, See on <a href="https://news.ycombinator.com/item?id=47068320">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
      

      <article>
        
              <div>
                
                  <p>&nbsp;Europe faces a critical choice in the race for artificial intelligence (AI). As the technology promises to reshape economies worldwide, policymakers are caught between two competing narratives. Optimists envision AI as the catalyst for a new productivity boom, potentially adding several percentage points to annual growth (Baily et al. 2023). Sceptics warn that adoption barriers, skill gaps, and uneven diffusion may limit gains and exacerbate inequality (Acemoglu 2024, Filippucci et al. 2024, Gambacorta and Shreeti, 2025). For Europe, the stakes are particularly high: while the continent boasts world-leading AI researchers and industrial capacity, it lags behind the US and China in developing new AI technologies (Cornelli et al. 2023). Recent studies suggest that AI could widen cross-country income gaps, with benefits concentrating in advanced economies that are better prepared to adopt and integrate these technologies (Cazzaniga et al. 2024, Gambacorta et al. 2025, Hennig and Khan 2025).</p>

<p>Yet robust firm-level evidence on AI’s actual effects in Europe remains scarce. Do European firms that adopt AI genuinely become more productive? Does AI destroy jobs or augment workers? Are the benefits shared broadly, or do they concentrate among larger, better-resourced companies? In a recent study (Aldasoro et al. 2026), we provide the first causal evidence on how AI adoption affects productivity and employment across more than 12,000 European firms.</p>

<h2>Europe’s AI paradox</h2>

<p>Europe’s position in the global AI landscape is paradoxical. On various innovation metrics, the continent falls behind. The EU trails the US not only in the absolute number of AI-related patents but also in AI specialisation – the share of AI patents relative to total patents. This innovation gap translates into differences in firms’ readiness to adopt AI, as measured by the IMF’s AI preparedness index, which assesses countries based on digital infrastructure, human capital, innovation capacity, and regulatory frameworks (Cazzaniga et al. 2024).</p>

<p>However, when it comes to actual deployment, the picture is more nuanced. Drawing on the European Investment Bank Investment Survey (EIBIS), we find that on average, AI adoption levels are similar in the EU and the US. Notably, important heterogeneity emerges beneath the surface. Financially developed EU countries – such as Sweden and the Netherlands – match US adoption rates, with around 36% of firms using big data analytics and AI in 2024. In contrast, firms in less financially developed EU economies, such as Romania and Bulgaria, lag substantially behind, with adoption rates around 28% in 2024. Figure 1 illustrates this divide, showing how the gap has persisted and even widened in recent years.</p>

<p><strong>Figure 1</strong> Use of big data analytics and AI by country groups</p>

  


<h6><em>Notes</em>: Average share of firms reporting that they use AI by country groups, controlling for firms’ sector. The error bars represent 95% confidence intervals. EU countries are grouped based on an index of financial development using financial market data from 2015 to 2023 and consisting of two composite indicators: (i) financial market size and integration, and (ii) financial market depth (see Betz et al., 2026). Source: EIBIS 2019-2024.</h6>

<p>Adoption also varies dramatically by firm size. Among large firms (more than 250 employees), 45% have deployed AI, compared with only 24% of small firms (10 to 49 employees). This echoes classic patterns in technology diffusion (Comin and Hobijn 2010): larger firms possess the resources, technical expertise, and economies of scale needed to absorb integration costs. AI-adopting firms are also systematically different – they invest more, are more innovative, and face tighter constraints in finding skilled workers. These patterns suggest that simply observing which firms adopt AI and comparing their performance could yield misleading results, as adoption itself is endogenous to firm characteristics.</p>

<h2>Isolating AI’s causal effect</h2>

<p>To credibly identify the causal effect of AI on productivity, we develop a novel instrumental variable strategy, inspired by Rajan and Zingales’ (1998) seminal work on financial dependence and growth. Their key insight was that industry characteristics measured in one economy – where they are arguably less affected by local distortions – can serve as an exogenous source of variation when applied to other countries.</p>

<p>We extend this logic to the firm level. For each EU firm in our sample, we identify comparable US firms – matched on sector, size, investment intensity, innovation activity, financing structure and management practices. We then assign the AI adoption rate of these matched US firms as a proxy for the EU firm’s exogenous exposure to AI. Because US firms operate under different institutional, regulatory and policy environments, their adoption patterns capture technological drivers that are plausibly independent of EU-specific factors. Rigorous propensity-score balancing tests confirm that our matched US and EU firms are virtually identical across key observable characteristics, validating the identification strategy. Our analysis draws on survey data from EIBIS combined with balance sheet data from Moody’s Orbis.</p>

<h2>Productivity gains without job losses</h2>

<p>Our results reveal three key findings. First, AI adoption causally increases labour productivity levels by 4% on average in the EU. This effect is statistically robust and economically meaningful, though more moderate than the transformative scenarios predicted by some observers. The magnitude aligns with mid-range macroeconomic projections (Bergeaud 2024) rather than the most optimistic estimates of productivity booms. While our analysis focuses on labour productivity levels and captures a one-off effect – rather than long-run total factor productivity growth – the 4% gain suggests that AI acts in the short term as a complementary input that enhances efficiency, albeit with implementation frictions and skill gaps tempering its impact.</p>

<p>Second, and crucially, we find no evidence that AI reduces employment in the short run. While naïve comparisons suggest AI-adopting firms employ more workers, this relationship disappears once we account for selection effects through our instrumental variable approach. The absence of negative employment effects, combined with significant productivity gains, points to a specific mechanism: capital deepening. AI augments worker output – enabling employees to complete tasks faster and make better decisions – without displacing labour. This finding resonates with micro-level experimental evidence showing that AI tools can produce productivity gains between 10% and 65%, with strong effects in coding, consultant tasks and professional writing (Noy and Zhang 2023, Gambacorta et al. 2024, Brynjolfsson et al. 2025). These experimental effects are task-specific, whereas our estimates capture firm-level averages.</p>

<p>Importantly, workers in AI-adopting firms have benefited through higher wages, both in aggregate and per employee. Whether these wage gains will persist in the long term, and whether they will be shared equitably across skill levels, remains an open question that merits continued monitoring.</p>

<h2>Uneven gains and the critical role of complementary investments</h2>

<p>Third, AI’s productivity benefits are far from evenly distributed. Breaking down our results by firm size reveals that medium and large companies experience substantially stronger productivity gains than their smaller counterparts (see Figure 2). This differential effect reflects the role of scale in absorbing AI integration costs and accessing complementary assets – data infrastructure, technical talent, and organisational capacity to redesign workflows. The finding raises concerns about widening productivity gaps between firms and regions, particularly given Europe’s industrial structure, which is dominated by small and medium-sized enterprises.</p>

<p><strong>Figure 2</strong> Effects of AI adoption on labour productivity by company size</p>

  


<h6><em>Notes</em>: The dependent variable is labour productivity, calculated as the log of turnover per employee, and is derived from EIBIS. AI adoption is measured using the AI implementation status derived from similar firms in the US. All regressions control for firm investment, profitability, financial leverage, total assets, age, and the interaction of country, sector and year fixed effects. Investment is expressed as the annual change in total fixed assets. Profitability is the ratio of earnings before interest and taxes (EBIT) to total assets. Financial leverage is the ratio of loans and long-term debt to total assets. All control variables come from Orbis and are lagged by 1 year. Error bars represent 90% confidence interval, based on standard errors clustered at country-sector-year level. Log linear approximation, 0.01 = 1% increase.</h6>

<p>Perhaps most importantly, our analysis reveals that AI adoption alone is insufficient. Firms must make complementary investments to unlock AI’s full potential. Our results show the striking heterogeneity in how different types of investments enhance AI’s productivity effects. An extra percentage point of investment in software and data infrastructure increases AI’s productivity effect by 2.4 percentage points. Investment in workforce training has an even larger multiplier effect: an additional percentage point spent on training amplifies AI’s productivity gains by 5.9 percentage points. These findings underscore a critical insight: the productivity dividends from AI depend not merely on acquiring the technology but on firms’ capacity to integrate it through investments in intangible assets and human capital.</p>

<h2>Implications for European policy</h2>

<p>These findings carry significant implications for policymakers. First, the benefits of AI adoption are mostly visible for medium and large firms. This means that Europe may work on policies that help smaller firms reach the critical scale necessary to benefit from AI. This requires well-functioning financial markets that can channel capital to innovative, fast-growing companies. Our evidence shows that firms in countries with more sophisticated financial markets are better equipped to invest in AI and complementary assets. This underscores the importance of advancing the EU Savings and Investment Union to ensure that promising smaller firms can access the finance they need to grow and compete.</p>

<p>Second, the central role of complementary investments means that public policy must look beyond subsidising AI hardware or software licenses. Effective support requires incentivising firm-level investments in integration, workflow redesign and continuous learning. Workforce development programs should prioritise what might be called ‘fusion skills’ – capabilities like prompt engineering, data stewardship and human-in-the-loop decision making that enhance human-AI complementarity. This demands coordinated investments in vocational training, tertiary education and lifelong learning.</p>

<p>Finally, while our results offer some reassurance that AI may not be leading to immediate job destruction, policymakers should not be complacent. The capital-deepening effects we document may be transitional. As AI systems become more capable and as firms gain experience integrating them, labour-displacing effects could emerge. Moreover, the wage gains we observe may accrue disproportionately to highly skilled workers, potentially widening income inequality. Continued monitoring of AI’s labour market effects and proactive policies to ensure inclusive growth will be essential as the technology matures.</p>

<p><em>Authors’ note: The views expressed in this column are those of the authors and do not necessarily reflect those of the Bank for International Settlements and the European Investment Bank.</em></p>

<h2>&nbsp;References</h2>

<p>Acemoglu, D (2024), “The simple macroeconomics of AI”, <em>Economic Policy</em> 40: 13–58.</p>

<p>Aldasoro, I, L Gambacorta, R Pál, D Revoltella, C Weiss and M Wolski (2026), “<a href="https://cepr.org/publications/dp21082">AI adoption, productivity and employment: Evidence from European firms</a>”, CEPR Discussion Paper No. 21082.</p>

<p>Baily, M N, E Brynjolfsson and A Korinek (2023), “Machines of mind: The case for an AI-powered productivity boom”, Brookings Institution.</p>

<p>Bergeaud, A (2024), “The past, present and future of European productivity”, paper presented at the ECB Forum on Central Banking, Sintra, July.</p>

<p>Betz, F, R Pál, A Sapir and T Huyen (2026), “Capital markets and access to equity for European firms”, EIB Working Paper, forthcoming.</p>

<p>Brynjolfsson, E, D Li and L Raymond (2025), “Generative AI at work”, <em>Quarterly Journal of Economics </em>140(2): 889–942.</p>

<p>Cazzaniga, M, F Jaumotte, L Li, G Melina, A Panton, C Pizzinelli, E Rockall and M Mendes Tavares (2024), “GenAI: Artificial intelligence and the future of work”, IMF Staff Discussion Note 2024/001.</p>

<p>Comin, D and B Hobijn (2010), “An exploration of technology diffusion”, <em>American Economic Review </em>100: 2031–2059.</p>

<p>Cornelli, G, J Frost and S Mishra (2023), “Artificial intelligence, services globalisation and income inequality”, BIS Working Paper.</p>

<p>Filippucci, F, P Gal and M Schief (2024), “Miracle or myth? Assessing the macroeconomic productivity gains from artificial intelligence”, OECD Artificial Intelligence Paper No. 29.</p>

<p>Gambacorta, L, H Qiu, D Rees and S Shan (2024), “Generative AI and labour productivity: A field experiment on coding”, BIS Working Paper No. 1208.</p>

<p>Gambacorta, L, E Kharroubi, A Mehrotra and T Oliviero (2025), “Artificial intelligence and growth in advanced and emerging economies: short-run impact”, BIS Working Paper No. 1321.</p>

<p>Gambacorta, L and V Shreeti (2025), “<a href="https://cepr.org/voxeu/columns/big-techs-ai-empire">Big techs’ AI empire</a>”, VoxEU.org, 16 May.</p>

<p>Hennig, T and S Khan (2025), “How artificial intelligence will affect Asia’s economies”, IMF Blog, 5 January.</p>

<p>Noy, S and W Zhang (2023), “Experimental evidence on the productivity effects of generative artificial intelligence”, <em>Science</em> 381(6654): 187–192.</p>

<p>Rajan, R G and L Zingales (1998), “Financial dependence and growth”, <em>American Economic Review </em>88(3): 559–586.</p>

              </div>

              
                                          </article>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft offers guide to pirating Harry Potter series for LLM training (308 pts)]]></title>
            <link>https://devblogs.microsoft.com/azure-sql/langchain-with-sqlvectorstore-example/</link>
            <guid>47067759</guid>
            <pubDate>Wed, 18 Feb 2026 23:19:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devblogs.microsoft.com/azure-sql/langchain-with-sqlvectorstore-example/">https://devblogs.microsoft.com/azure-sql/langchain-with-sqlvectorstore-example/</a>, See on <a href="https://news.ycombinator.com/item?id=47067759">Hacker News</a></p>
Couldn't get https://devblogs.microsoft.com/azure-sql/langchain-with-sqlvectorstore-example/: Error: Request failed with status code 404]]></description>
        </item>
        <item>
            <title><![CDATA[Ladybird: Closing this as we are no longer pursuing Swift adoption (283 pts)]]></title>
            <link>https://github.com/LadybirdBrowser/ladybird/issues/933</link>
            <guid>47067678</guid>
            <pubDate>Wed, 18 Feb 2026 23:08:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/LadybirdBrowser/ladybird/issues/933">https://github.com/LadybirdBrowser/ladybird/issues/933</a>, See on <a href="https://news.ycombinator.com/item?id=47067678">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="issue-body-viewer" data-team-hovercards-enabled="true" data-turbolinks="false" id="issue-body-viewer"><p dir="auto">List of issues preventing moving forward on moving Swift 6.0 support out of an experimental state:</p>
<p dir="auto">Swift issues:</p>
<ul>
<li>
<p dir="auto"> <span><a data-error-text="Failed to load title" data-id="2423820399" data-permission-text="Title is private" data-url="https://github.com/swiftlang/llvm-project/issues/8998" data-hovercard-type="issue" data-hovercard-url="/swiftlang/llvm-project/issues/8998/hovercard" href="https://github.com/swiftlang/llvm-project/issues/8998">Please backport d8352e93c1c8042d9166eab3d76d6c07ef585b6d<span>&nbsp;swiftlang/llvm-project#8998</span></a></span></p>
<p dir="auto">Details: Swift's version of LLVM is missing the fix for <span><a data-error-text="Failed to load title" data-id="1137046914" data-permission-text="Title is private" data-url="https://github.com/llvm/llvm-project/issues/53815" data-hovercard-type="issue" data-hovercard-url="/llvm/llvm-project/issues/53815/hovercard" href="https://github.com/llvm/llvm-project/issues/53815">[Clang] ICE in CheckPointerToMemberOperands passing decltype of lambda<span>&nbsp;llvm/llvm-project#53815</span></a></span>. This means that any assertions build of llvm from the swift open source project cannot build our code. Snapshot builds are released with assertions on.</p>
<p dir="auto">Workaround: Build swift from source on Linux without llvm assertions, or use macOS.</p>
<p dir="auto">PR: <span><a data-error-text="Failed to load title" data-id="2445185064" data-permission-text="Title is private" data-url="https://github.com/swiftlang/llvm-project/issues/9038" data-hovercard-type="pull_request" data-hovercard-url="/swiftlang/llvm-project/pull/9038/hovercard" href="https://github.com/swiftlang/llvm-project/pull/9038">🍒 [Clang] [Sema] Handle placeholders in '.*' expressions (#83103)<span>&nbsp;swiftlang/llvm-project#9038</span></a></span></p>
<p dir="auto">Fixed in Swift 6.0.0 release</p>
</li>
<li>
<p dir="auto"> <span><a data-error-text="Failed to load title" data-id="2440588492" data-permission-text="Title is private" data-url="https://github.com/swiftlang/swift/issues/75593" data-hovercard-type="issue" data-hovercard-url="/swiftlang/swift/issues/75593/hovercard" href="https://github.com/swiftlang/swift/issues/75593">Interop: Compiler and C++ Bridging header disagree on ABI of <code>Optional&lt;CxxValueType&gt;</code><span>&nbsp;swiftlang/swift#75593</span></a></span></p>
<p dir="auto">Details: It is not currently possible to return a swift optional of a small C++ type back to C++. The compiler and the generated bridging header disagree on how that is supposed to be done.</p>
<p dir="auto">Workaround: Don't use Optional, use a return type that forces the C++ type to be heap allocated. Array is one alternative.</p>
</li>
<li>
<p dir="auto"> <span><a data-error-text="Failed to load title" data-id="2445844445" data-permission-text="Title is private" data-url="https://github.com/swiftlang/swift/issues/75661" data-hovercard-type="issue" data-hovercard-url="/swiftlang/swift/issues/75661/hovercard" href="https://github.com/swiftlang/swift/issues/75661">Interop: Compiling with C++17 or higher on Ubuntu 22.04 fails with cyclic header dependencies in libstdc++<span>&nbsp;swiftlang/swift#75661</span></a></span></p>
<p dir="auto">Details: Swift's clang module map for libstdc++ contains cycles when <code>&lt;execution&gt;</code> is included. See <a href="https://forums.swift.org/t/swift-5-9-release-on-ubuntu-22-04-fails-to-build-std-module/67659" rel="nofollow">https://forums.swift.org/t/swift-5-9-release-on-ubuntu-22-04-fails-to-build-std-module/67659</a></p>
<p dir="auto">Workaround: Edit <code>&lt;prefix&gt;/lib/swift/linux/libstdcxx.h</code> to comment out the <code>#include &lt;execution&gt;</code> line.</p>
<p dir="auto">PR: <span><a data-error-text="Failed to load title" data-id="2445867195" data-permission-text="Title is private" data-url="https://github.com/swiftlang/swift/issues/75662" data-hovercard-type="pull_request" data-hovercard-url="/swiftlang/swift/pull/75662/hovercard" href="https://github.com/swiftlang/swift/pull/75662">[cxx-interop] Disable c++ execution header with libstdcxx versions &gt;= 11<span>&nbsp;swiftlang/swift#75662</span></a></span> (Just a workaround, not a fix)<br>
6.0 Backport: <span><a data-error-text="Failed to load title" data-id="2474439162" data-permission-text="Title is private" data-url="https://github.com/swiftlang/swift/issues/75971" data-hovercard-type="pull_request" data-hovercard-url="/swiftlang/swift/pull/75971/hovercard" href="https://github.com/swiftlang/swift/pull/75971">🍒 [cxx-interop] Disable c++ execution header with libstdcxx versions &gt;= 11<span>&nbsp;swiftlang/swift#75971</span></a></span><br>
Fixed in swiftlang/swift:main and release/6.0, but not in 6.0.0 or 6.0.1</p>
</li>
<li>
<p dir="auto"> <span><a data-error-text="Failed to load title" data-id="2478862199" data-permission-text="Title is private" data-url="https://github.com/swiftlang/swift/issues/76024" data-hovercard-type="issue" data-hovercard-url="/swiftlang/swift/issues/76024/hovercard" href="https://github.com/swiftlang/swift/issues/76024">Interop: Cannot return <code>swift::Optional&lt;swift::String&gt;</code> from C++ function<span>&nbsp;swiftlang/swift#76024</span></a></span></p>
<p dir="auto">Details: Returning binding types <code>swift::Optional&lt;T&gt;</code> or <code>swift::String</code> from a C++ function is not supported</p>
<p dir="auto">Workaround: Return std:: types?</p>
</li>
<li>
<p dir="auto"> <span><a data-error-text="Failed to load title" data-id="2560313000" data-permission-text="Title is private" data-url="https://github.com/swiftlang/swift/issues/76809" data-hovercard-type="issue" data-hovercard-url="/swiftlang/swift/issues/76809/hovercard" href="https://github.com/swiftlang/swift/issues/76809">Swift cannot import libstdc++-13 chrono header in C++23 mode<span>&nbsp;swiftlang/swift#76809</span></a></span></p>
<p dir="auto">Details: Swift 6.0 cannot import libstdc++-13 or higher <code>&lt;chrono&gt;</code> header.</p>
<p dir="auto">Workaround: Use libc++ or a lower libstdc++ version. libstdc++-13 is default on Ubuntu 24.04 LTS.</p>
<p dir="auto">Fixed in swiftlang/swift:main as of Oct 18, 2024</p>
</li>
<li>
<p dir="auto"> <span><a data-error-text="Failed to load title" data-id="2926260250" data-permission-text="Title is private" data-url="https://github.com/swiftlang/swift/issues/80065" data-hovercard-type="issue" data-hovercard-url="/swiftlang/swift/issues/80065/hovercard" href="https://github.com/swiftlang/swift/issues/80065">[cxx-interop] SIL verifier crash in Unmanaged.passUnretained() on SWIFT_UNSAFE_REFERENCE type<span>&nbsp;swiftlang/swift#80065</span></a></span></p>
<p dir="auto">Details: SIL verifier crash when trying to compare unsafe reference types by pointer</p>
<p dir="auto">Workaround: Disable SIL verification (!! yikes)<br>
Fixed in <span><a data-error-text="Failed to load title" data-id="3074553244" data-permission-text="Title is private" data-url="https://github.com/swiftlang/swift/issues/81614" data-hovercard-type="pull_request" data-hovercard-url="/swiftlang/swift/pull/81614/hovercard" href="https://github.com/swiftlang/swift/pull/81614">[cxx-interop] Relax a SILVerifier assertion for immortal reference types<span>&nbsp;swiftlang/swift#81614</span></a></span></p>
</li>
<li>
<p dir="auto"> <span><a data-error-text="Failed to load title" data-id="2936808960" data-permission-text="Title is private" data-url="https://github.com/swiftlang/swift/issues/80182" data-hovercard-type="issue" data-hovercard-url="/swiftlang/swift/issues/80182/hovercard" href="https://github.com/swiftlang/swift/issues/80182">[cxx-interop] Bitfield setter/getter for SWIFT_UNSAFE_REFERENCE crashes frontend<span>&nbsp;swiftlang/swift#80182</span></a></span></p>
<p dir="auto">Details: SWIFT_UNSAFE_REFERENCE types with getters/setters for bitfields crash the frontend</p>
<p dir="auto">Workaround: ... don't use SWIFT_UNSAFE_REFERENCE? unsure. Need guidance<br>
Fixed in <span><a data-error-text="Failed to load title" data-id="2938474851" data-permission-text="Title is private" data-url="https://github.com/swiftlang/swift/issues/80197" data-hovercard-type="pull_request" data-hovercard-url="/swiftlang/swift/pull/80197/hovercard" href="https://github.com/swiftlang/swift/pull/80197">[cxx-interop] Do not create mutating properties for classes<span>&nbsp;swiftlang/swift#80197</span></a></span></p>
</li>
<li>
<p dir="auto"> <span><a data-error-text="Failed to load title" data-id="2657430890" data-permission-text="Title is private" data-url="https://github.com/swiftlang/swift/issues/77607" data-hovercard-type="issue" data-hovercard-url="/swiftlang/swift/issues/77607/hovercard" href="https://github.com/swiftlang/swift/issues/77607">CxxInterop: Swift does not synthesize CxxConvertibleToContainer iterator operator== in some cases<span>&nbsp;swiftlang/swift#77607</span></a></span></p>
<p dir="auto">Details: <code>Vector&lt;u32, 2&gt;</code> is not recognized as <code>CxxConvertibleToContainer</code><br>
Workaround: Treat it as a sequence instead, and any transfer to a swift type requires manual copying of elements</p>
</li>
<li>
<p dir="auto"> <span><a data-error-text="Failed to load title" data-id="3090514071" data-permission-text="Title is private" data-url="https://github.com/swiftlang/swift/issues/81774" data-hovercard-type="issue" data-hovercard-url="/swiftlang/swift/issues/81774/hovercard" href="https://github.com/swiftlang/swift/issues/81774">Enabling cxx interop on Arch with GCC 15 causes lots of errors<span>&nbsp;swiftlang/swift#81774</span></a></span></p>
<p dir="auto">Details: Swift fails to import clang modules with <code>#include &lt;math.h&gt;</code> with libstdc++-15 installed.</p>
<p dir="auto">Workaround: None (!!)</p>
</li>
</ul>
<p dir="auto">CMake issues:</p>
<ul>
<li>
<p dir="auto"> <a href="https://gitlab.kitware.com/cmake/cmake/-/issues/26174" rel="nofollow">https://gitlab.kitware.com/cmake/cmake/-/issues/26174</a></p>
<p dir="auto">Details: Swift + Ninja doesn't respect CMAKE_OSX_DEPLOYMENT_TARGET. This results in a mismatched LC_BUILD_VERSION on swift and c++ object files, spamming the console with warnings.</p>
<p dir="auto">Workaround: </p><div itemprop="text">
    <table data-tab-size="8" data-paste-markdown-skip="">

        <tbody><tr>
          <td id="L21" data-line-number="21"></td>
          <td id="LC21"> <span># FIXME: https://gitlab.kitware.com/cmake/cmake/-/issues/26174</span> </td>
        </tr>

        <tr>
          <td id="L22" data-line-number="22"></td>
          <td id="LC22"> <span>if</span> (<span>APPLE</span>) </td>
        </tr>

        <tr>
          <td id="L23" data-line-number="23"></td>
          <td id="LC23"> <span>    set</span>(CMAKE_Swift_COMPILER_TARGET <span>"<span>${CMAKE_SYSTEM_PROCESSOR}</span>-apple-macosx<span>${CMAKE_OSX_DEPLOYMENT_TARGET}</span>"</span>) </td>
        </tr>

        <tr>
          <td id="L24" data-line-number="24"></td>
          <td id="LC24"> <span>endif</span>() </td>
        </tr>
    </tbody></table>
  </div>

</li>
<li>
<p dir="auto"> <a href="https://gitlab.kitware.com/cmake/cmake/-/issues/26175" rel="nofollow">https://gitlab.kitware.com/cmake/cmake/-/issues/26175</a></p>
<p dir="auto">Details: With CMP0157 enabled, swiftc does not set install_name directory to "<a data-hovercard-type="user" data-hovercard-url="/users/rpath/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/rpath">@rpath</a>" per CMAKE_INSTALL_NAME_DIR</p>
<p dir="auto">Workaround: </p><div itemprop="text">
    <table data-tab-size="8" data-paste-markdown-skip="">

        <tbody><tr>
          <td id="L123" data-line-number="123"></td>
          <td id="LC123">     <span># FIXME: https://gitlab.kitware.com/cmake/cmake/-/issues/26175</span> </td>
        </tr>

        <tr>
          <td id="L124" data-line-number="124"></td>
          <td id="LC124"> <span>    if</span> (<span>APPLE</span>) </td>
        </tr>

        <tr>
          <td id="L125" data-line-number="125"></td>
          <td id="LC125"> <span>        add_custom_command</span>(<span>TARGET</span> LibGfx POST_BUILD </td>
        </tr>

        <tr>
          <td id="L126" data-line-number="126"></td>
          <td id="LC126">             <span>COMMAND</span> install_name_tool -id @rpath/liblagom-gfx.0.dylib <span>"$&lt;TARGET_FILE:LibGfx&gt;"</span> </td>
        </tr>

        <tr>
          <td id="L127" data-line-number="127"></td>
          <td id="LC127">         ) </td>
        </tr>

        <tr>
          <td id="L128" data-line-number="128"></td>
          <td id="LC128"> <span>    endif</span>() </td>
        </tr>
    </tbody></table>
  </div>

<p dir="auto">PR: <a href="https://gitlab.kitware.com/cmake/cmake/-/merge_requests/9692" rel="nofollow">https://gitlab.kitware.com/cmake/cmake/-/merge_requests/9692</a>. Merged Aug 2, 2024 to be backported to CMake 3.29, 3.30.</p>
</li>
<li>
<p dir="auto"> <a href="https://gitlab.kitware.com/cmake/cmake/-/issues/26195" rel="nofollow">https://gitlab.kitware.com/cmake/cmake/-/issues/26195</a></p>
<p dir="auto">Details: Imported targets from dependencies can have INTERFACE_COMPILE_OPTIONS or INTERFACE_LINK_OPTIONS that swiftc doesn't understand.</p>
<p dir="auto">Workaround: Swizzle the flags just after import, for every single imported library.</p>
</li>
</ul>
<p dir="auto">Ladybird issues:</p>
<ul>
<li>
<p dir="auto"> <span><a data-error-text="Failed to load title" data-id="2447337585" data-permission-text="Title is private" data-url="https://github.com/LadybirdBrowser/ladybird/issues/965" data-hovercard-type="pull_request" data-hovercard-url="/LadybirdBrowser/ladybird/pull/965/hovercard" href="https://github.com/LadybirdBrowser/ladybird/pull/965">AK+LibGfx: Explicitly spell out headers in the clang module map<span>&nbsp;#965</span></a></span></p>
<p dir="auto">Details: Creating a modulemap for larger libraries can cause issues with libc headers. For example, creating an umbrella directory entry for LibGfx causes issues with <code>&lt;math.h&gt;</code>, which is clearly included in every file that is complaining about it. Needs more module.modulemap massaging to get the clang frontend/swiftc to properly associate system headers with system modules and not our own modules</p>
<p dir="auto">Workaround: ¯\_(ツ)_/¯<br>
Resolution: Generate module maps for each library</p>
</li>
<li>
<p dir="auto"> <span><a data-error-text="Failed to load title" data-id="2471629808" data-permission-text="Title is private" data-url="https://github.com/LadybirdBrowser/ladybird/issues/1101" data-hovercard-type="issue" data-hovercard-url="/LadybirdBrowser/ladybird/issues/1101/hovercard" href="https://github.com/LadybirdBrowser/ladybird/issues/1101">Swift: Importing AK and querying type properties crashes swift-frontend in Debug build<span>&nbsp;#1101</span></a></span></p>
<p dir="auto">Details: Building the CxxSequence protocol conformance test for AK containers crashes the swift frontend process in debug mode</p>
<p dir="auto">Workaround: Build in release mode lmao<br>
Upstream bug: Not yet reduced</p>
</li>
<li>
<p dir="auto"> <span><a data-error-text="Failed to load title" data-id="2471684633" data-permission-text="Title is private" data-url="https://github.com/LadybirdBrowser/ladybird/issues/1102" data-hovercard-type="issue" data-hovercard-url="/LadybirdBrowser/ladybird/issues/1102/hovercard" href="https://github.com/LadybirdBrowser/ladybird/issues/1102">Swift: Using un-namespaced 'String' type after importing AK crashes swift frontend<span>&nbsp;#1102</span></a></span></p>
<p dir="auto">Details: A function that takes an unnamespaced <code>String</code> argument will crash the swift frontend if <code>AK</code> is imported</p>
<p dir="auto">Workaround: Qualify all references to String as AK.String or Swift.String<br>
Upstream bug: <span><a data-error-text="Failed to load title" data-id="3130478507" data-permission-text="Title is private" data-url="https://github.com/swiftlang/swift/issues/82108" data-hovercard-type="issue" data-hovercard-url="/swiftlang/swift/issues/82108/hovercard" href="https://github.com/swiftlang/swift/issues/82108">[cxxinterop] UNREACHABLE in AstMangler when mangling C++ type imported via global using declaration<span>&nbsp;swiftlang/swift#82108</span></a></span></p>
</li>
<li>
<p dir="auto"> <span><a data-error-text="Failed to load title" data-id="2488554441" data-permission-text="Title is private" data-url="https://github.com/LadybirdBrowser/ladybird/issues/1201" data-hovercard-type="issue" data-hovercard-url="/LadybirdBrowser/ladybird/issues/1201/hovercard" href="https://github.com/LadybirdBrowser/ladybird/issues/1201">Swift: Building AK swift test with Testing module crashes compiler frontend<span>&nbsp;#1201</span></a></span></p>
<p dir="auto">Details: Using swift-testing to test AK container conformance to swift interop protocols crashes the frontend</p>
<p dir="auto">Workaround: Keep custom test runner code<br>
Upstream bug: Not yet reduced</p>
</li>
<li>
<p dir="auto"> <span><a data-error-text="Failed to load title" data-id="2633898310" data-permission-text="Title is private" data-url="https://github.com/LadybirdBrowser/ladybird/issues/2168" data-hovercard-type="issue" data-hovercard-url="/LadybirdBrowser/ladybird/issues/2168/hovercard" href="https://github.com/LadybirdBrowser/ladybird/issues/2168">Swift: AK::StringView is no longer a CxxSequenceType on swift/main<span>&nbsp;#2168</span></a></span></p>
<p dir="auto">Details: AK::StringView fails to conform to CxxSequenceType on swift/main</p>
<p dir="auto">Workaround: Reach into the bytesUnsafe() in order to iterate over the bytes of the view as a sequence<br>
Upstream bug: Not yet reduced</p>
</li>
</ul>
<p dir="auto">Nice-to-have:</p>
<ul>
<li>
<p dir="auto"> <span><a data-error-text="Failed to load title" data-id="2895111217" data-permission-text="Title is private" data-url="https://github.com/swiftlang/swift/issues/79767" data-hovercard-type="issue" data-hovercard-url="/swiftlang/swift/issues/79767/hovercard" href="https://github.com/swiftlang/swift/issues/79767">Interop: Application crash when returning <code>Optional&lt;CxxType&gt;</code><span>&nbsp;swiftlang/swift#79767</span></a></span></p>
<p dir="auto">Details: Returning MyCxx.CxxType? from a swift function that is called from C++ crashes on call.</p>
<p dir="auto">Workaround: Return as [MyCxx.CxxType] instead, with a 0 or 1-sized array</p>
</li>
<li>
<p dir="auto"> <span><a data-error-text="Failed to load title" data-id="2933646074" data-permission-text="Title is private" data-url="https://github.com/swiftlang/vscode-swift/issues/1449" data-hovercard-type="issue" data-hovercard-url="/swiftlang/vscode-swift/issues/1449/hovercard" href="https://github.com/swiftlang/vscode-swift/issues/1449">Root-level compile_commands.json required for simple CMake projects<span>&nbsp;swiftlang/vscode-swift#1449</span></a></span></p>
<p dir="auto">Details: Top-level compile_commands.json is required for SourceKit-LSP and vscode-swift to grok the project</p>
<p dir="auto">Workaround: Create a symlink <code>ln -s Build/release/compile_commands.json compile_commands.json</code></p>
</li>
<li>
<p dir="auto"> <span><a data-error-text="Failed to load title" data-id="2933293904" data-permission-text="Title is private" data-url="https://github.com/swiftlang/swift/issues/80142" data-hovercard-type="issue" data-hovercard-url="/swiftlang/swift/issues/80142/hovercard" href="https://github.com/swiftlang/swift/issues/80142">Automatically include path to <code>&lt;swift/bridging&gt;</code> in SDK installs<span>&nbsp;swiftlang/swift#80142</span></a></span></p>
<p dir="auto">Details: An extra include path is required to include <code>&lt;swift/bridging&gt;</code> on Linux</p>
<p dir="auto">Workaround: Grab the path from <code>swiftc -print-target-info</code></p>
</li>
</ul>
<p dir="auto">Open questions:</p>
<ul>
<li>
<p dir="auto">Unclear how to pass view types or byte slices to swift without creating a copy.</p>
<ul dir="auto">
<li>We will want to be passing untrusted Strings, or c++-owned Spans of bytes to swift for it to crunch on and return some structured data. It's not clear how to inform swift about this without copying the data (at least) once.</li>
<li><del>I was not able to massage swift into interpreting our String and StringView types as 'CxxConvertibleToContainer' or 'CxxRandomAccessContainer' types. Likely because they are actually immutable?</del></li>
</ul>
</li>
<li>
<p dir="auto">Unclear how to convince Swift that our types are just as good as std:: ones.</p>
<ul dir="auto">
<li>AK::Optional</li>
<li>AK::HashTable/HashMap</li>
<li>AK::Time</li>
<li>more?</li>
</ul>
</li>
<li>
<p dir="auto"> How to integrate with our garbage collector? <a href="https://forums.swift.org/t/ladybird-browser-and-swift-garbage-collection/76084" rel="nofollow">https://forums.swift.org/t/ladybird-browser-and-swift-garbage-collection/76084</a></p>
</li>
</ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Martial arts robots at 2026 Spring Festival Gala [video] (111 pts)]]></title>
            <link>https://www.youtube.com/watch?v=mUmlv814aJo</link>
            <guid>47067496</guid>
            <pubDate>Wed, 18 Feb 2026 22:47:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=mUmlv814aJo">https://www.youtube.com/watch?v=mUmlv814aJo</a>, See on <a href="https://news.ycombinator.com/item?id=47067496">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[What years of production-grade concurrency teaches us about building AI agents (108 pts)]]></title>
            <link>https://georgeguimaraes.com/your-agent-orchestrator-is-just-a-bad-clone-of-elixir/</link>
            <guid>47067395</guid>
            <pubDate>Wed, 18 Feb 2026 22:37:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://georgeguimaraes.com/your-agent-orchestrator-is-just-a-bad-clone-of-elixir/">https://georgeguimaraes.com/your-agent-orchestrator-is-just-a-bad-clone-of-elixir/</a>, See on <a href="https://news.ycombinator.com/item?id=47067395">Hacker News</a></p>
Couldn't get https://georgeguimaraes.com/your-agent-orchestrator-is-just-a-bad-clone-of-elixir/: Error: getaddrinfo ENOTFOUND georgeguimaraes.com]]></description>
        </item>
        <item>
            <title><![CDATA[Sizing chaos (671 pts)]]></title>
            <link>https://pudding.cool/2026/02/womens-sizing/</link>
            <guid>47066552</guid>
            <pubDate>Wed, 18 Feb 2026 21:18:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pudding.cool/2026/02/womens-sizing/">https://pudding.cool/2026/02/womens-sizing/</a>, See on <a href="https://news.ycombinator.com/item?id=47066552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><!--[--><!--[--><!----><main id="content"><!----><!---->  <header></header><!----> <div> <h2><p><!--[--><span>m</span><span>e</span><span>e</span><span>t</span><span> </span><span>y</span><span>o</span><span>u</span><span>r</span><span> </span><span>t</span><span>y</span><span>p</span><span>i</span><span>c</span><span>a</span><span>l</span><!--]--><!----></p> <p role="img" aria-label="tween"><!--[--><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/T/T-004.png" alt="t"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/W/W-005.png" alt="w"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/E/E-003.png" alt="e"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/E/E-012.png" alt="e"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/N/N-010.png" alt="n"><!--]--></span><!--]--></p><!----></h2> <p><!---->Like many girls her age, she loves to keep up with the latest fashion trends and explore new ways to express herself. Shopping is fun, but it won’t always be this way.<!----></p> </div><!----> <div><h2><p><!--[--><span>f</span><span>i</span><span>t</span><span> </span><span>4</span><span> </span><span>a</span><!--]--><!----></p> <p role="img" aria-label="teen"><!--[--><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/T/T-016.png" alt="t"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/E/E-002.png" alt="e"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/E/E-011.png" alt="e"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/N/N-015.png" alt="n"><!--]--></span><!--]--></p></h2>  <p><!---->with <a href="https://pudding.cool/author/jan-diehm/">Jan Diehm</a><!----></p> <div><!--[--><p>I remember once being that teen girl shopping in the women’s section for the first time. I took stacks upon stacks of jeans with me to the dressing room, searching in vain for that one pair that fit perfectly. Over 20 years later, my hunt for the ideal pair of jeans continues. But now as an adult, I’m stuck with the countless ways that women’s apparel is not made for the average person, like me.</p><p>Children’s clothing sizes are often tied to a kid’s age or stage of development. The idea is that as a young person grows older, her clothes will evolve with her. Youth styles tend to be boxy and oversized to allow room for kids to move and grow. By early adolescence, apparel for girls becomes more fitted. Junior’s styles have higher waistlines and less-pronounced curves compared to adult clothing lines. In short: clothes for tweens are made for tween bodies.</p><p>By the time most teenage girls can wear women’s clothes — around age 15 — their options are seemingly endless. But the evolution in clothing sizes that followed girls throughout childhood abruptly stops there.</p><p>This is the reality I find myself reckoning with today: Women’s clothing — designed for adults — fits modern teen girls better.</p><!--]--></div></div><!----> <div><!--[!--><!--]--> <!--[--><div><p>Age: 14-15</p> <p>Sizes: Women's</p></div><!--]--> <div id="beeswarm"><!--[--><p>Waistline in Inches</p><!--]--> <svg width="-20" height="-56"><!--[--><g><!--[--><g id="band-XXS" style="transition-delay: 0.75s"><rect style="transition: all var(--ms-500) ease-in-out" x="9.868000000000002" y="0" width="-1.9214999999999982" height="0" fill="#9ABBD9"></rect><text style="transition: all var(--ms-500) ease-in-out" x="8.907250000000003" y="-132" text-anchor="middle">XXS</text></g><g id="band-XS" style="transition-delay: 0.5s"><rect style="transition: all var(--ms-500) ease-in-out" x="7.946500000000004" y="0" width="-2.103500000000002" height="0" fill="#9ABBD9"></rect><text style="transition: all var(--ms-500) ease-in-out" x="6.894750000000003" y="-132" text-anchor="middle">XS</text></g><g id="band-S" style="transition-delay: 0.25s"><rect style="transition: all var(--ms-500) ease-in-out" x="5.843000000000002" y="0" width="-2.4430000000000014" height="0" fill="#9ABBD9"></rect><text style="transition: all var(--ms-500) ease-in-out" x="4.621500000000001" y="-132" text-anchor="middle">S</text></g><g id="band-M" style="transition-delay: 0s"><rect style="transition: all var(--ms-500) ease-in-out" x="3.4000000000000004" y="0" width="-3.325000000000001" height="0" fill="#9ABBD9"></rect><text style="transition: all var(--ms-500) ease-in-out" x="1.7374999999999998" y="-132" text-anchor="middle">M</text></g><g id="band-L" style="transition-delay: 0.25s"><rect style="transition: all var(--ms-500) ease-in-out" x="0.07499999999999929" y="0" width="-5.074999999999999" height="0" fill="#9ABBD9"></rect><text style="transition: all var(--ms-500) ease-in-out" x="-2.4625000000000004" y="-132" text-anchor="middle">L</text></g><g id="band-XL" style="transition-delay: 0.5s"><rect style="transition: all var(--ms-500) ease-in-out" x="-5" y="0" width="-6.125" height="0" fill="#9ABBD9"></rect><text style="transition: all var(--ms-500) ease-in-out" x="-8.0625" y="-132" text-anchor="middle">XL</text></g><g id="band-XXL" style="transition-delay: 0.75s"><rect style="transition: all var(--ms-500) ease-in-out" x="-11.125" y="0" width="-2.9749999999999996" height="0" fill="#9ABBD9"></rect><text style="transition: all var(--ms-500) ease-in-out" x="-12.6125" y="-132" text-anchor="middle">XXL</text></g><!--]--></g><g style="transition: opacity var(--ms-500) ease-in-out"><rect style="transition: all var(--ms-500) ease-in-out" x="0" y="0" width="0" height="0"></rect></g><!--]--><g transform="translate(0, -112)" opacity="1"></g></svg> </div></div><!----> <div><h2><!--[--><span>P</span><span>a</span><span>i</span><span>n</span><span> </span><span>i</span><span>s</span><!--]--><!----> <p role="img" aria-label="universal"><!--[--><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/U/U-008.png" alt="u"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/N/N-019.png" alt="n"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/I/I-020.png" alt="i"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/V/V-018.png" alt="v"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/E/E-013.png" alt="e"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/R/R-010.png" alt="r"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/S/S-014.png" alt="s"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/A/A-009.png" alt="a"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/L/L-010.png" alt="l"><!--]--></span><!--]--></p><!----> <!--[--><span>s</span><span>i</span><span>z</span><span>i</span><span>n</span><span>g</span><span> </span><span>i</span><span>s</span><span> </span><span>n</span><span>o</span><span>t</span><!--]--><!----></h2> <!--[--><p><!---->Few life experiences feel as universal, across generations, as the pains and frustrations of trying to find clothes that fit.<!----></p><p><!---->Sizes vary wildly from store to store. Even within a single apparel company, no one size is consistent. There are no regulations or universal sizing standards. Instead each brand is incentivized to make up its own. When size guides change — and they’re always changing — brands are not obligated to disclose updates.<!----></p><p><!---->There are also often different sizing structures for every type of garment. “Plus” size means one thing, “curve” means another, and “extended” sizes can be defined as all of the above or something else entirely. Don’t count on any of those sizes to be available to try on in-store, but do brace for return fees if your online order doesn’t fit. Free in-store alterations are largely a thing of the past, while a trip to the tailor’s can cost just as much as the item itself.<!----></p><p><!---->The only consistent feature is that the industry at large continues to cling onto the same underlying sizing system that’s been broken for decades. And it’s only gotten worse.<!----></p><!--]--></div><!----> <div><h2><!--[--><span>T</span><span>h</span><span>e</span><span> </span><span>v</span><span>i</span><span>l</span><span>l</span><span>a</span><span>i</span><span>n</span><span> </span><span>a</span><span>r</span><span>c</span><span> </span><span>o</span><span>f</span><!--]--><!----> <p role="img" aria-label="vanity"><!--[--><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/V/V-003.png" alt="v"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/A/A-002.png" alt="a"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/N/N-013.png" alt="n"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/I/I-018.png" alt="i"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/T/T-002.png" alt="t"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/Y/Y-007.png" alt="y"><!--]--></span><!--]--></p><!----> <p role="img" aria-label="sizing"><!--[--><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/S/S-008.png" alt="s"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/I/I-008.png" alt="i"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/Z/Z-001.png" alt="z"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/I/I-006.png" alt="i"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/N/N-005.png" alt="n"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/G/G-017.png" alt="g"><!--]--></span><!--]--></p><!----></h2> <!--[--><p><!---->On top of all these problems, consumers often know the labels for any given size cannot be trusted.<!----></p><p><!---->Vanity sizing, the practice where size labels stay the same even as the underlying measurements frequently become larger, is so ubiquitous across the fashion and apparel industry that younger generations have never experienced a world without it.<!----></p><p><!---->Cultural narratives around vanity sizing often square the blame on female shoppers, not brands. <a href="https://www.newsweek.com/fashion-designers-introduce-less-zero-sizes-112005"><i>Newsweek</i></a> once called it “self-delusion on a mass scale” because women were more likely to buy items that were labeled as sizes smaller than reality. But there’s more to the story.<!----></p><p><!---->Vanity sizing provides a powerful <a href="https://www.sciencedirect.com/science/article/abs/pii/S1057740813000612">marketing strategy</a> for brands. Companies found that whenever women needed a size larger than expected, they were less likely to follow through on their purchases. Some could even develop negative associations with the brand and never shop there again. But when manufacturers manipulated sizing labels, leading to a more positive customer experience, brands could maintain a slight competitive edge.<!----></p><p><!---->The dynamic perpetuates an arms race toward artificially deflating size labels. Most shoppers aren’t even aware when size charts change, or by how much. If anything, vanity sizing consistently gaslights women to the point where few are able to know their “true” size. But where would we be today without it?<!----></p><!--]--></div><!----> <div><h2><p role="img" aria-label="sew"><!--[--><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/S/S-007.png" alt="s"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/E/E-012.png" alt="e"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/W/W-009.png" alt="w"><!--]--></span><!--]--></p> <p role="img" aria-label="what?"><!--[--><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/W/W-017.png" alt="w"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/H/H-005.png" alt="h"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/A/A-004.png" alt="a"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/T/T-009.png" alt="t"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/specialChars/question-001.png" alt="?"><!--]--></span><!--]--></p></h2> <div><!--[--><p><!---->I once believed that change was inevitable and sizing problems would become a relic of the past. If it wasn’t some scrappy upstart that promised to revolutionize the sizing system, then at least the major fashion conglomerates would be well-placed to modernize and tap the full potential of the plus-size market. But that progress never fully materialized. And I got tired of waiting.<!----></p> <!--[!--><!--]--> <!--[!--><!--]--> <!--[!--><!--]--><p><!---->A few years ago, I started learning how to sew. Somehow it felt more practical to make my own clothes than count on meaningful change to happen on its own. Getting started was easier than I thought. The first sewing pattern I ever completed — a boxy, drop-shoulder style that could turn into either a shirt or dress — was free to download. It included a 29-page instruction manual with photos and illustrations documenting every step.<!----></p> <!--[--><div><p><img src="https://pudding.cool/2026/02/womens-sizing/assets/patternmaking.jpg" alt="a sketch of a bodice block on grid paper with a ruler and notebook"></p><p>Drafting a custom pattern based on my body measurements and proportions</p></div><!--]--> <!--[!--><!--]--> <!--[!--><!--]--><p><!---->From there, I started learning how to draft my own sewing patterns from scratch. That’s when I realized the truth behind my sizing struggles: Clothing sizes are optimized for mass production and appeal — not women’s bodies. Nothing represents this more than a size 8.<!----></p> <!--[!--><!--]--> <!--[!--><!--]--> <!--[!--><!--]--><p><!---->Fashion designers often use body measurements for a <span>size 8</span> as a starting point when creating new design samples. Manufacturers then use a mathematical formula to determine each next size up or down the range in a process called grading. The effect is like a Russian doll. Each size up is incrementally larger than the last.<!----></p> <!--[!--><!--]--> <!--[--><!--]--> <!--[!--><!--]--><p><!---->The uniform shape makes it easier for factories to mass-produce garments, however it comes with several tradeoffs. It’s hard to scale up to larger-sized clothing before the proportions become distorted. It also becomes impractical to make multiple versions of a single item to accommodate varying body shapes or heights. That means most women’s clothing is derived from a single set of proportions — a size 8. According to U.S. health data, fewer than 10% of adult women have waistlines that fit the standard sample size or smaller.<!----></p> <!--[!--><!--]--> <!--[!--><!--]--> <!--[!--><!--]--><p><!---->I, like the vast majority of women, do not fit the standard mold. Instead I took an old pattern-making textbook often taught in fashion design schools to start making clothes to fit my own unique proportions. I gathered and recorded over 58 different body measurements in order to get started and from there, I could make my own <span>custom base pattern,</span> known as a bodice block or sloper.<!----></p> <!--[!--><!--]--> <!--[!--><!--]--> <!--[--><!--]--><p><!---->Once I compared my personalized sloper to commercial patterns and retail garments, I had a revelation: clothes were never made to fit bodies like mine. It didn’t matter how much weight I gained or lost, whether I contorted my body or tried to buy my way into styles that “flatter” my silhouette, there was no chance that clothes would ever fit perfectly on their own. Finally I understood why.<!----></p> <!--[!--><!--]--> <!--[!--><!--]--> <!--[!--><!--]--><!--]--></div></div><!----> <!----> <div><h2><p><!--[--><span>s</span><span>i</span><span>z</span><span>i</span><span>n</span><span>g</span><span> </span><span>f</span><span>o</span><span>r</span><!--]--><!----></p> <p role="img" aria-label="every"><!--[--><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/E/E-018.png" alt="e"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/V/V-009.png" alt="v"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/E/E-009.png" alt="e"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/R/R-005.png" alt="r"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/Y/Y-001.png" alt="y"><!--]--></span><!--]--></p> <p role="img" aria-label="body"><!--[--><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/B/B-018.png" alt="b"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/O/O-002.png" alt="o"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/D/D-006.png" alt="d"><!--]--></span><span><!--[--><img src="https://pudding.cool/2026/02/womens-sizing/assets/letters/png_letters/Y/Y-002.png" alt="y"><!--]--></span><!--]--></p></h2> <div><!--[--><p><!---->The fashion industry thrives on exclusivity. Luxury brands maintain their status by limiting who is able to buy or even wear their clothes. If few women fit the “ideal” standards, then products serving only them are inherently exclusionary. Size charts become the de facto dividing line determining who belongs and who doesn’t.<!----></p><p><!---->This line of gatekeeping is baked into the foundation of virtually all clothing. The modern sizing system in the U.S. was <a href="https://archive.org/details/womensmeasuremen454obri/mode/2up">developed in the 1940s</a> based on mostly young, white women. No women of color were originally included. The system was never built to include a diverse cross-section of people, ages, or body types. It has largely stayed that way by design.<!----></p><p><!---->In its 1995 standards update, ASTM International admitted that its sizing guidelines were never meant to represent the population at large. Instead body measurements were based on “designer experience” and “market observations.” The goal was to tailor sizes to the existing customer base. But what happens when more than half of all women are pushed to the margins or left behind?<!----></p><p><!---->It doesn’t have to be this way. Teenage girls shouldn’t be aging out of sizing options from the moment they start wearing women’s clothes. A woman does not need hourglass proportions to look good, just as garment-makers do not need standardized sizes to produce well-fitting clothes.<!----></p><p><!---->There are no rules forcing brands to adopt any particular sizing system. There is no such thing as a “true” size 8, or any size for that matter. If brands are constantly developing and customizing their size charts, then it makes little sense to perpetuate a broken system. Sizes are all made up anyway — why can’t we make them better?<!----></p><!--]--></div></div><!----> <div><h4>Methodology</h4> <div><!--[--><p><!---->To highlight the median body proportions of the adult women in the U.S., we relied on anthropometric reference data for children and adults that is regularly released by the National Center for Health Statistics within the U.S. Department of Health and Human Services.<!----></p><p><!---->For this story, we pulled data on the median waistline circumference of women and girls that was <a href="https://stacks.cdc.gov/view/cdc/174595">gathered between 2021-2023</a>. For girls and women under 20 years old, measurements were recorded in two-year age ranges (ex: 10–11 years, 14–15 years), with a median of 141 participants per age range. For women over 20, measurements were recorded in nine-year age ranges (ex: 20–29 years, 30–39 years) and collectively for all women 20 and older. Each nine-year age range had a median of 465 participants. Overall, measurements were recorded for 3,121 women ages 20 and older.  Those who were pregnant were excluded from the data.<!----></p><p><!---->HHS also provides a breakdown of measurements within set percentiles for each age range, which includes figures for the 5th, 10th, 15th, 25th, 50th, 75th, 85th, 90th, and 95th percentiles. We then used that percentile data to extrapolate the waistline measurements of all women and girls within each respective age group.<!----></p><p><!---->We also compared figures to those recorded by HHS from <a href="https://www.cdc.gov/nchs/data/series/sr_11/sr11_249.pdf">1988-1994</a>. There, 7,410 women ages 20 and older participated in the study. Measurements were originally recorded in centimeters, so we converted to inches.<!----></p><p><!---->Brands included in the size chart comparisons represent a diverse cross-section of popular apparel brands and retailers in the U.S., including a mix of mass market, fast fashion, premium and luxury labels.<!----></p><p><!---->For each brand, we focused on collecting body measurements for “regular” or “standard” size ranges, as well as “plus” sizes when available. Sizing information for “petite,” “tall,” or “curve” clothing lines were not included. Size charts reflect the body measurements for garments categorized as general “apparel.” In a select few cases where that category was unavailable, “dresses” were used as the default garment type.<!----></p><p><!---->Within each size range, we focused on collecting three main body measurements: Bust, waist, and hip. Some were presented as a range from minimum to maximum values, while others were single measurements. All numeric U.S. women’s sizing labels and descriptions were recorded, as well as their corresponding alpha sizes, when available.<!----></p><p><!---->Size chart data was last manually captured in July 2025 and may not reflect a brand’s current size chart. Brands frequently change their size charts, and more often than not, shoppers aren’t even aware when measurements or sizes are updated.<!----></p><p><!---->The standardized size charts refer to ASTM International’s regular release of its Standard Table of Measurements for Adult Female Misses Figure Type. The 1995 release (designated as D 5585-95) reflects sizes 2-20. ASTM updated its standards in 2021 (designated as <a href="https://store.astm.org/d5585-21.html">D5585-21</a>) to include sizes 00-20.<!----></p><p><!---->Ransom note letters are from <a href="https://indieground.net/product/ransom-note-letters/">Indieground.</a><!----></p><!--]--></div> <h4>Related pieces</h4> </div><!----> <!--[--><!--]--><!----><!----><!----></main><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[27-year-old Apple iBooks can connect to Wi-Fi and download official updates (384 pts)]]></title>
            <link>https://old.reddit.com/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/</link>
            <guid>47066241</guid>
            <pubDate>Wed, 18 Feb 2026 20:54:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/">https://old.reddit.com/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/</a>, See on <a href="https://news.ycombinator.com/item?id=47066241">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="siteTable_t3_1r8900z"><div id="thing_t1_o64bigf" onclick="click_thing(this)" data-fullname="t1_o64bigf" data-type="comment" data-gildings="0" data-subreddit="MacOS" data-subreddit-prefixed="r/MacOS" data-subreddit-fullname="t5_2s2gv" data-subreddit-type="public" data-author="Augustisimus" data-author-fullname="t2_48cjitb4" data-replies="0" data-permalink="/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o64bigf/"><p>[–]<a href="https://old.reddit.com/user/Augustisimus">Augustisimus</a><span title="iMac"><span>iMac</span></span><span></span> <span title="12">12 points</span><span title="13">13 points</span><span title="14">14 points</span> <time title="Wed Feb 18 20:32:54 2026 UTC" datetime="2026-02-18T20:32:54+00:00">4 hours ago</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_o64bigfv20"><div><p>Apple has gone through phases.</p>

<p>During the 2010s they were definitely operating under a planned obsolescence model. Updates were designed to put additional stress on older hardware.</p>

<p>Now, in the 2020s, this is less of a concern, because services, not hardware, is their primary business model. It doesn’t matter if you have a ten year old Intel MacBook, as long as you’re subscribed to Apple One.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o64bigf/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li>report</li><li>reply</li></ul></div><div id="thing_t1_o636snm" onclick="click_thing(this)" data-fullname="t1_o636snm" data-type="comment" data-gildings="0" data-subreddit="MacOS" data-subreddit-prefixed="r/MacOS" data-subreddit-fullname="t5_2s2gv" data-subreddit-type="public" data-author="Romengar" data-author-fullname="t2_yyryv" data-replies="0" data-permalink="/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o636snm/"><p>[–]<a href="https://old.reddit.com/user/FantasicMouse">FantasicMouse</a><span></span> <span title="4">4 points</span><span title="5">5 points</span><span title="6">6 points</span> <time title="Wed Feb 18 18:09:41 2026 UTC" datetime="2026-02-18T18:09:41+00:00">7 hours ago</time>&nbsp;(5 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_o63fyoxpip"><div><p>I belive xp servers are still online or atleast they were two years ago. I think last I touched win 98 I had to manually install updates from an installer disk… god it’s been so long ago but I think 98 you had to do that anyway…</p>

<p>As of 5 years ago I know xp and 98 se where still hosted on Microsoft’s servers and I’d say xp would be the equivalent to this version of macOS</p>

<p>I’ll say though that PPC Mac’s do a lot better than win xp machines if you’re trying to actually use them in today’s age</p>
</div></form><ul><li><a href="https://old.reddit.com/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o63fyox/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#o63cwlx" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div><div id="thing_t1_o6350ic" onclick="click_thing(this)" data-fullname="t1_o6350ic" data-type="comment" data-gildings="0" data-subreddit="MacOS" data-subreddit-prefixed="r/MacOS" data-subreddit-fullname="t5_2s2gv" data-subreddit-type="public" data-author="Druben-hinterm-Dorfe" data-author-fullname="t2_iqo95lm4s" data-replies="0" data-permalink="/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o6350ic/"><div id="thing_t1_o637ocx" onclick="click_thing(this)" data-fullname="t1_o637ocx" data-type="comment" data-gildings="0" data-subreddit="MacOS" data-subreddit-prefixed="r/MacOS" data-subreddit-fullname="t5_2s2gv" data-subreddit-type="public" data-author="Reasonable_Draft1634" data-author-fullname="t2_g16p4747" data-replies="0" data-permalink="/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o637ocx/"><p>[–]<a href="https://old.reddit.com/user/Reasonable_Draft1634">Reasonable_Draft1634</a><span></span> <span title="43">43 points</span><span title="44">44 points</span><span title="45">45 points</span> <time title="Wed Feb 18 17:33:03 2026 UTC" datetime="2026-02-18T17:33:03+00:00">7 hours ago</time>&nbsp;(12 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_o637ocxrpq"><div><p>How is that “planned obsolesce”, exactly? Modern websites rely on technologies that simply didn’t exist 20 years ago. ECMAScript 6+ (ES2015 and beyond). Advanced CSS (Grid, Flexbox, container queriesModern TLS encryption (TLS 1.2 / 1.3), WebAssemblyAdvanced GPU acceleration APIs, etc. </p>

<p>Webkit must implement these standards to stay compatible. Those systems were designed long before modern JavaScript engines and security models. How about all the hardware limitations? </p>

<p>It is incredible to me the twisted arguments folks still go with and try to convince us it is a thing.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o637ocx/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#o6350ic" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div><div id="thing_t1_o63q55p" onclick="click_thing(this)" data-fullname="t1_o63q55p" data-type="comment" data-gildings="0" data-subreddit="MacOS" data-subreddit-prefixed="r/MacOS" data-subreddit-fullname="t5_2s2gv" data-subreddit-type="public" data-author="I-figured-it-out" data-author-fullname="t2_co2mkgj6" data-replies="0" data-permalink="/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o63q55p/"><p>[–]<a href="https://old.reddit.com/user/I-figured-it-out">I-figured-it-out</a><span></span> <span title="1">1 point</span><span title="2">2 points</span><span title="3">3 points</span> <time title="Wed Feb 18 18:53:59 2026 UTC" datetime="2026-02-18T18:53:59+00:00">6 hours ago</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_o63q55pv7o"><div><p>Try using modern  Safari to download and render mission critical content from a 5 year old website built and hosted by a curmudgeon on an XP server… . If it was hosted on a G4 it would have a solid unix backend, running a reliable apache server that can likely to have been updated to be reasonably secure, if not fast.</p>

<p>Once upon a time in the late 1990s IBM corporate had an email server that reached its client limits, unattended forgotten in a closet as the company grew. The server had been unattended and unmaintained other than by remote access. 
IBM’s email demands grew enourmously during thst time of corporate expansion when email had been adopted as a norm (displacing fax) and from hundreds of users to tens of thousands. They decided to upgrade the server hardware. When they eventually found it they were dismayed to discover a dust covered 128k Mac from 1984 chugging away doing a job that was considered far too heavy for IBM’s mid -late1990s consumer line of computers.  </p>

<p>Story goes, so they decided in retrospect- to replace the 128K Mac, not as planned -with the heavy iron servers that IBM sold it’s corporate customers, but with a nice shiny new iMac.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o63q55p/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#o6350ic" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div></div><div id="thing_t1_o639gms" onclick="click_thing(this)" data-fullname="t1_o639gms" data-type="comment" data-gildings="0" data-subreddit="MacOS" data-subreddit-prefixed="r/MacOS" data-subreddit-fullname="t5_2s2gv" data-subreddit-type="public" data-author="PoppaFish" data-author-fullname="t2_mkyqku" data-replies="0" data-permalink="/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o639gms/"><p>[–]<a href="https://old.reddit.com/user/Cockur">Cockur</a><span></span> <span title="0">0 points</span><span title="1">1 point</span><span title="2">2 points</span> <time title="Wed Feb 18 19:04:26 2026 UTC" datetime="2026-02-18T19:04:26+00:00">6 hours ago</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_o63sh84jag"><div><p>You could run a bunch of old daws and music making software like samplers or trackers. You could use it for sequencing any old midi gear too</p>

<p>Lots of folk out there running old platforms specifically for this reason </p>

<p>If you can think of a good reason to use it then it’s useful</p>
</div></form><ul><li><a href="https://old.reddit.com/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o63sh84/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#o639gms" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div><div id="thing_t1_o63r36n" onclick="click_thing(this)" data-fullname="t1_o63r36n" data-type="comment" data-gildings="0" data-subreddit="MacOS" data-subreddit-prefixed="r/MacOS" data-subreddit-fullname="t5_2s2gv" data-subreddit-type="public" data-author="DjNormal" data-author-fullname="t2_mfe7c" data-replies="0" data-permalink="/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o63r36n/"><p>[–]<a href="https://old.reddit.com/user/DjNormal">DjNormal</a><span></span> <span title="3">3 points</span><span title="4">4 points</span><span title="5">5 points</span> <time title="Wed Feb 18 18:58:09 2026 UTC" datetime="2026-02-18T18:58:09+00:00">6 hours ago</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_o63r36n03y"><div><p>Meanwhile I can <em>never</em> reinstall Office 2010 for my mom, because Microsoft shut down their authentication servers.</p>

<p>I got her over to Libre Office, but it’s a little clunky and just enough different that she struggles with it.</p>

<p>On my end, my 2010 MBP is still the best DJ computer I have. My legacy outboard gear still works and despite having a dead battery, the MBP still runs great (on Sierra). I have it an SSD back in 2017 and it felt like a new machine again… for a while.</p>

<p>We’ve still got stuff at old as the Apple ][+ lying around, still functioning. I have few complaints.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o63r36n/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li>report</li><li>reply</li></ul></div><div id="thing_t1_o63b47a" onclick="click_thing(this)" data-fullname="t1_o63b47a" data-type="comment" data-gildings="0" data-subreddit="MacOS" data-subreddit-prefixed="r/MacOS" data-subreddit-fullname="t5_2s2gv" data-subreddit-type="public" data-author="Device_whisperer" data-author-fullname="t2_73aimw16" data-replies="0" data-permalink="/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o63b47a/"><p>[–]<a href="https://old.reddit.com/user/Device_whisperer">Device_whisperer</a><span></span> <span title="0">0 points</span><span title="1">1 point</span><span title="2">2 points</span> <time title="Wed Feb 18 20:39:47 2026 UTC" datetime="2026-02-18T20:39:47+00:00">4 hours ago</time>&nbsp;(1 child)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_o64cycxm3t"><div><p>There are still a number of things that MacOS can't do correctly. Simple things. Basic things. They still cling to obsolete file system protocols (AFP) and can't maintain sync with industry-standard SMB shares. Most of the damn time, it won't let you delete anything without a confirmation, with no means to turn it off.   </p>

<p>Their screen graphics are a CPA. I have the 5K screen, and there IS NO setting that makes the text look good. They are either way too small, or they must be scaled, which affects the whole damn display. Yeah, I have a 5K screen that only looks good in 1080P. Why bother?</p>

<p>I don't know about you, but I couldn't care less about a 27-year-old book that I'm not going to read ever again.  I'd rather have today's files present and available.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o64cycx/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#o63inf1" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div><div id="thing_t1_o6373xt" onclick="click_thing(this)" data-fullname="t1_o6373xt" data-type="comment" data-gildings="0" data-subreddit="MacOS" data-subreddit-prefixed="r/MacOS" data-subreddit-fullname="t5_2s2gv" data-subreddit-type="public" data-author="IAmABoredCat1590" data-author-fullname="t2_ieq2gmtw" data-replies="0" data-permalink="/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o6373xt/"><p>[–]<a href="https://old.reddit.com/user/IAmABoredCat1590">IAmABoredCat1590</a><span title="MacBook Pro (Intel)"><span>MacBook Pro (Intel)</span></span><span></span> <span title="1">1 point</span><span title="2">2 points</span><span title="3">3 points</span> <time title="Wed Feb 18 17:30:28 2026 UTC" datetime="2026-02-18T17:30:28+00:00">7 hours ago</time>&nbsp;(1 child)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_o6373xt0ys"><div><p>…Windows 7 can do that too…</p>

<p>As soon as you connect to the network and refresh the updates, it downloads the latest Windows build for that version and installs it because it fetches stuff from an archived server that’s still being held up by apple.</p>

<p>In your case, it did the sage thing. Fetch the update from the maintained server.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o6373xt/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li>report</li><li>reply</li></ul></div><div id="thing_t1_o65jluk" onclick="click_thing(this)" data-fullname="t1_o65jluk" data-type="comment" data-gildings="0" data-subreddit="MacOS" data-subreddit-prefixed="r/MacOS" data-subreddit-fullname="t5_2s2gv" data-subreddit-type="public" data-author="felixding" data-author-fullname="t2_46rdp" data-replies="0" data-permalink="/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o65jluk/"><p>[–]<a href="https://old.reddit.com/user/felixding">felixding</a><span></span> <span title="this subreddit hides comment scores for 120 minutes">[score hidden]</span> <time title="Thu Feb 19 00:54:13 2026 UTC" datetime="2026-02-19T00:54:13+00:00">35 minutes ago</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_o65rnnksyw"><div><p>I'm in the Discord.</p>

<p>Trust me, I really have done my homework.</p>

<ul>
<li>I keep searching with various keywords on Reddit, Hacker News, X, Google, OS News, GitHub, Distro Watch, *-look.org...</li>
<li>I follow almost all the recreation attempts on GitHub (e.g. OS X Lion theme for xfce, NEXTSPACE, Gershwin...)</li>
<li>I keep a 2013 MBP just to use Marvericks and Flavours, and a iMac G4 to use Tiger.</li>
<li>I have VMs of Mac OS 9, Jaguar, etc, on my MBP M2</li>
</ul>

<p>The list could go on but the point is I really tried.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o65rnnk/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#o65l9mp" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div><div id="thing_t1_o63dh91" onclick="click_thing(this)" data-fullname="t1_o63dh91" data-type="comment" data-gildings="0" data-subreddit="MacOS" data-subreddit-prefixed="r/MacOS" data-subreddit-fullname="t5_2s2gv" data-subreddit-type="public" data-author="springlord" data-author-fullname="t2_sxw8b" data-replies="0" data-permalink="/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o63dh91/"><p>[–]<a href="https://old.reddit.com/user/springlord">springlord</a><span></span> <span title="0">0 points</span><span title="1">1 point</span><span title="2">2 points</span> <time title="Wed Feb 18 19:33:05 2026 UTC" datetime="2026-02-18T19:33:05+00:00">5 hours ago</time>&nbsp;(1 child)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_o63ypgardg"><div><p>Man I used to be a fanboy too but at some point you need to drop it in name of intellectual honesty if you want to make a remotely valid point.</p>

<p>The G4 was an very high end CPU, in perfs and in price, so if you want to make silly comparisons in 2003 it was rather competing with the Xeon line, not with P3.</p>

<p>And even then, your post actually underlines the very definition of planned obsolescence, the PowerPC architecture was intentionally killed by Apple despite excellent hardware when on the contrary it is indeed still possible to build and find maintained software for old x86 systems.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/MacOS/comments/1r8900z/macos_which_officially_supports_27_year_old/o63ypga/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#o63g8tc" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[There is unequivocal evidence that Earth is warming (2024) (216 pts)]]></title>
            <link>https://science.nasa.gov/climate-change/evidence/</link>
            <guid>47065678</guid>
            <pubDate>Wed, 18 Feb 2026 20:09:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://science.nasa.gov/climate-change/evidence/">https://science.nasa.gov/climate-change/evidence/</a>, See on <a href="https://news.ycombinator.com/item?id=47065678">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="primary">

	<article id="post-275202"><div>






<h3>Takeaways</h3>



<ul>
<li>While Earth’s climate <a href="https://www.climate.gov/maps-data/climate-data-primer/past-climate">has changed throughout its history</a>, the current warming is happening at a rate not seen in the past 10,000 years.</li>



<li>According to the Intergovernmental Panel on Climate Change (<a href="https://www.ipcc.ch/">IPCC</a>), "Since systematic scientific assessments began in the 1970s, the influence of human activity on the warming of the climate system has evolved from theory to established fact."<a href="#footnote_1"><sup>1</sup></a></li>



<li>Scientific information taken from natural sources (such as ice cores, rocks, and tree rings) and from modern equipment (like satellites and instruments) all show the signs of a changing climate.</li>



<li>From global temperature rise to melting ice sheets, the evidence of a warming planet abounds.</li>
</ul>



<h2>The rate of change since the mid-20th century is unprecedented over millennia.</h2>



<p>Earth's climate has changed throughout history. Just in the last 800,000 years, there have been eight cycles of ice ages and warmer periods, with the end of the last ice age about 11,700 years ago marking the beginning of the modern climate era — and of human civilization. Most of these climate changes are attributed to <a href="http://climate.nasa.gov/blog/2949/why-milankovitch-orbital-cycles-cant-explain-earths-current-warming/">very small variations in Earth’s orbit</a> that change the amount of solar energy our planet receives.</p>





<p>The current warming trend is different because it is clearly the result of human activities since the mid-1800s, and is proceeding at a rate not seen over many recent millennia.<sup><a href="#footnote_1">1</a></sup> It is undeniable that human activities have produced the atmospheric gases that have trapped more of the Sun’s energy in the Earth system. This extra energy has warmed the atmosphere, ocean, and land, and widespread and rapid changes in the atmosphere, ocean, cryosphere, and biosphere have occurred.</p>





<p>Earth-orbiting satellites and new technologies have helped scientists see the big picture, collecting many different types of information about our planet and its climate all over the world. These data, collected over many years, reveal the signs and patterns of a changing climate.</p>



<p>Scientists demonstrated the heat-trapping nature of carbon dioxide and other gases in the mid-19th century.<a href="#footnote_2"><sup>2</sup></a> Many of the science instruments NASA uses to study our climate focus on how these gases affect the movement of infrared radiation through the atmosphere. From the measured impacts of increases in these gases, there is no question that increased greenhouse gas levels warm Earth in response.</p>


<div id="">
						<p>Intergovernmental Panel on Climate Change</p>
						
					</div>


<p>Ice cores drawn from Greenland, Antarctica, and tropical mountain glaciers show that Earth’s climate responds to changes in greenhouse gas levels. Ancient evidence can also be found in tree rings, ocean sediments, coral reefs, and layers of sedimentary rocks. This ancient, or paleoclimate, evidence reveals that current warming is occurring roughly 10 times faster than the average rate of warming after an ice age. Carbon dioxide from human activities is increasing about 250 times faster than it did from natural sources after the last Ice Age.<a href="#footnote_3"><sup>3</sup></a></p>


<div id="">
			<h2>The Evidence for Rapid Climate Change Is Compelling:</h2>

			<div>

					<ul>
						
						<li>

							<div>

										<h3>Global Temperature Is Rising</h3>
										<div><p>
											The planet's average surface temperature has risen about 2 degrees Fahrenheit (1 degrees Celsius) since the late 19th century, a change driven largely by increased carbon dioxide emissions into the atmosphere and other human activities.<sup><a href="#footnote_4">4</a></sup> Most of the warming occurred in the past 40 years, with the seven most recent years being the warmest. The years 2016 and 2020 are tied for the warmest year on record.<sup><a href="#footnote_5">5</a></sup></p><p><em><sub>Image credit: Ashwin Kumar, Creative Commons Attribution-Share Alike 2.0 Generic.</sub></em></p></div>

										
									</div>

														
							
						</li>
													
						<li>

							<div>

										<h3>The Ocean Is Getting Warmer</h3>
										<div><p>
											The ocean has absorbed much of this increased heat, with the top 100 meters (about 328 feet) of ocean showing warming of 0.67 degrees Fahrenheit (0.33 degrees Celsius) since 1969.<sup><a href="#footnote_6">6</a></sup> Earth stores 90% of the extra energy in the ocean.</p><p><em><sub>Image credit: Kelsey Roberts/USGS</sub></em></p></div>

										
									</div>

														
							
						</li>
													
						<li>

							<div>

										<h3>The Ice Sheets Are Shrinking</h3>
										<div><p>
											The Greenland and Antarctic ice sheets have decreased in mass. Data from NASA's Gravity Recovery and Climate Experiment show Greenland lost an average of 279 billion tons of ice per year between 1993 and 2019, while Antarctica lost about 148 billion tons of ice per year.<sup><a href="https://science.nasa.gov/wp-admin/post.php?post=275202&amp;action=edit#footnote_7">7</a></sup></p><p><em><sub>Image: The Antarctic Peninsula, Credit: NASA</sub></em></p></div>

										
									</div>

														
							
						</li>
													
						<li>

							<div>

										<h3>Glaciers Are Retreating</h3>
										<div><p>
											Glaciers are retreating almost everywhere around the world — including in the Alps, Himalayas, Andes, Rockies, Alaska, and Africa.<sup><a href="https://science.nasa.gov/wp-admin/post.php?post=275202&amp;action=edit#footnote_8">8</a></sup></p><p><em><sub>Image: Miles Glacier, Alaska Image credit: NASA</sub></em></p></div>

										
									</div>

														
							
						</li>
													
						<li>

							<div>

										<h3>Snow Cover Is Decreasing</h3>
										<div><p>
											Satellite observations reveal that the amount of spring snow cover in the Northern Hemisphere has decreased over the past five decades and the snow is melting earlier.<sup><a href="#footnote_9">9</a></sup></p><p><em><sub>Image credit: NASA/JPL-Caltech</sub></em></p></div>

										
									</div>

														
							
						</li>
													
						<li>

							<div>

										<h3>Sea Level Is Rising</h3>
										<div><p>
											Global sea level rose about 8 inches (20 centimeters) in the last century. The rate in the last two decades, however, is nearly double that of the last century and accelerating slightly every year.<sup><a href="https://science.nasa.gov/wp-admin/post.php?post=275202&amp;action=edit#footnote_10">10</a></sup></p><p><em><sub>Image credit: U.S. Army Corps of Engineers Norfolk District</sub></em></p></div>

										
									</div>

														
							
						</li>
													
						<li>

							<div>

										<h3>Arctic Sea Ice Is Declining</h3>
										<div><p>
											Both the extent and thickness of Arctic sea ice has declined rapidly over the last several decades.<sup><a href="https://science.nasa.gov/wp-admin/post.php?post=275202&amp;action=edit#footnote_11">11</a></sup></p><p><em><sub>Credit: NASA's Scientific Visualization Studio</sub></em></p></div>

										
									</div>

														
							
						</li>
													
						<li>

							<div>

										<h3>Extreme Events Are Increasing in Frequency</h3>
										<div><p>
											The number of record high temperature events in the United States has been increasing, while the number of record low temperature events has been decreasing, since 1950. The U.S. has also witnessed increasing numbers of intense rainfall events.<sup><a href="https://science.nasa.gov/wp-admin/post.php?post=275202&amp;action=edit#footnote_12">12</a></sup></p><p><em><sub>Image credit: Régine Fabri,&nbsp;<a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>, via Wikimedia Commons</sub></em></p></div>

										
									</div>

														
							
						</li>
													
						<li>

							<div>

										<h3>Ocean Acidification Is Increasing</h3>
										<div><p>
											Since the beginning of the Industrial Revolution, the acidity of surface ocean waters has increased by about 30%.<sup><a href="#footnote_13">13</a>, <a href="#footnote_14">14</a></sup> This increase is due to humans emitting more carbon dioxide into the atmosphere and hence more being absorbed into the ocean. The ocean has absorbed between 20% and 30% of total anthropogenic carbon dioxide emissions in recent decades (7.2 to 10.8 billion metric tons per year).<sup>1<a href="#footnote_15">5</a>, <a href="#footnote_16">16</a></sup></p><p><em><sub>Image credit: NOAA</sub></em></p></div>

										
									</div>

														
							
						</li>
													
					</ul>
				</div>

		</div>


<h3>References</h3>











<p id="footnote_3"><strong>3.</strong> <a href="https://www.ipcc.ch/report/ar6/wg1/downloads/report/IPCC_AR6_WGI_Chapter02.pdf">IPCC Sixth Assessment Report, WG1, Chapter 2</a><br>Vostok ice core data; NOAA Mauna Loa CO2 record<br>O. Gaffney, W. Steffen, "The Anthropocene Equation." <i>The Anthropocene Review</i> 4, issue 1 (April 2017): 53-61. https://doi.org/abs/10.1177/2053019616688022. </p>



<p id="footnote_4"><strong>4.</strong> <a href="https://www.ncei.noaa.gov/monitoring" target="_blank" rel="noopener">https://www.ncei.noaa.gov/monitoring</a><br> <a href="https://crudata.uea.ac.uk/cru/data/temperature/" target="_blank" rel="noopener">https://crudata.uea.ac.uk/cru/data/temperature/</a><br> <a href="http://data.giss.nasa.gov/gistemp" target="_blank" rel="noopener">http://data.giss.nasa.gov/gistemp</a> </p>



<p id="footnote_5"><strong>5.</strong> <a href="https://www.giss.nasa.gov/research/news/20170118/" target="_blank" rel="noopener">https://www.giss.nasa.gov/research/news/20170118/</a> </p>







<p id="footnote_7"><strong>7.</strong> I. Velicogna, Yara Mohajerani, A. Geruo, F. Landerer, J. Mouginot, B. Noel, E. Rignot,  T. Sutterly, M. van den Broeke, M. Wessem, D. Wiese, "Continuity of Ice Sheet Mass Loss in Greenland and Antarctica From the GRACE and GRACE Follow-On Missions." <i>Geophysical Research Letters</i> 47, Issue 8 (28 April 2020): e2020GL087291. https://doi.org/10.1029/2020GL087291. </p>



<p id="footnote_8"><strong>8.</strong> <a href="http://nsidc.org/sotc/glacier_balance.html" target="_blank" rel="noopener">National Snow and Ice Data Center</a> <br><a href="http://wgms.ch/" target="_blank" rel="noopener">World Glacier Monitoring Service</a> </p>



<p id="footnote_9"><strong>9.</strong> <a href="http://nsidc.org/cryosphere/sotc/snow_extent.html" target="_blank" rel="noopener">National Snow and Ice Data Center</a> <br>D.A. Robinson, D. K. Hall, and T. L. Mote, "MEaSUREs Northern Hemisphere Terrestrial Snow Cover Extent Daily 25km EASE-Grid 2.0, Version 1 (2017). Boulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center. doi: <br><a href="https://doi.org/10.5067/MEASURES/CRYOSPHERE/nsidc-0530.001">https://doi.org/10.5067/MEASURES/CRYOSPHERE/nsidc-0530.001</a>. <a href="http://nsidc.org/cryosphere/sotc/snow_extent.html" target="_blank" rel="noopener">http://nsidc.org/cryosphere/sotc/snow_extent.html</a> <br>Rutgers University Global Snow Lab. <a href="http://climate.rutgers.edu/snowcover/docs.php?target=vis" target="_blank" rel="noopener">Data History</a>  </p>



<p id="footnote_10"><strong>10.</strong> R.S. Nerem, B.D. Beckley, J. T. Fasullo, B.D. Hamlington, D. Masters, and G.T. Mitchum, "Climate-change–driven accelerated sea-level rise detected in the altimeter era." <i>PNAS </i>15, no. 9<i> </i>(12 Feb. 2018): 2022-2025. https://doi.org/10.1073/pnas.1717312115. </p>



<p id="footnote_11"><strong>11.</strong> <a href="https://nsidc.org/cryosphere/sotc/sea_ice.html" target="_blank" rel="noopener">https://nsidc.org/cryosphere/sotc/sea_ice.html</a><br>Pan-Arctic Ice Ocean Modeling and Assimilation System (PIOMAS, Zhang and Rothrock, 2003)<br> <a href="http://psc.apl.washington.edu/research/projects/arctic-sea-ice-volume-anomaly/">http://psc.apl.washington.edu/research/projects/arctic-sea-ice-volume-anomaly/</a><br> <a href="http://psc.apl.uw.edu/research/projects/projections-of-an-ice-diminished-arctic-ocean/">http://psc.apl.uw.edu/research/projects/projections-of-an-ice-diminished-arctic-ocean/</a> </p>



<p id="footnote_12"><strong>12.</strong> USGCRP, 2017: <i>Climate Science Special Report: Fourth National Climate Assessment, Volume I</i> [Wuebbles, D.J., D.W. Fahey, K.A. Hibbard, D.J. Dokken, B.C. Stewart, and T.K. Maycock (eds.)]. U.S. Global Change Research Program, Washington, DC, USA, 470 pp, <a href="https://science2017.globalchange.gov/">https://doi.org/10.7930/j0j964j6</a>. </p>



<p id="footnote_13"><strong>13.</strong> <a href="http://www.pmel.noaa.gov/co2/story/What+is+Ocean+Acidification%3F" target="_blank" rel="noopener">http://www.pmel.noaa.gov/co2/story/What+is+Ocean+Acidification%3F</a> </p>



<p id="footnote_14"><strong>14.</strong> <a href="http://www.pmel.noaa.gov/co2/story/Ocean+Acidification" target="_blank" rel="noopener">http://www.pmel.noaa.gov/co2/story/Ocean+Acidification</a> </p>



<p id="footnote_15"><strong>15.</strong> C.L. Sabine, et al., “The Oceanic Sink for Anthropogenic CO2.” <i>Science</i> 305 (16 July 2004): 367-371. https://doi.org/10.1126/science.1097403. </p>



<p id="footnote_16"><strong>16.</strong> <a href="https://www.ipcc.ch/srocc/" target="_blank" rel="noopener">Special Report on the Ocean and Cryosphere in a Changing Climate</a>, Technical Summary, Chapter TS.5, Changing Ocean, Marine Ecosystems, and Dependent Communities, Section 5.2.2.3.<br> <a href="https://www.ipcc.ch/srocc/chapter/technical-summary/" target="_blank" rel="noopener">https://www.ipcc.ch/srocc/chapter/technical-summary/</a> </p>



<p><sub><i>Header image shows clouds imitating mountains as the sun sets after midnight as seen from Denali's backcountry Unit 13 on June 14, 2019. Credit: <a href="https://www.nps.gov/media/photo/view.htm?id=DF3609D9-1E8E-4BA2-914B-4C1F3E8E7071">NPS/Emily Mesner</a></i></sub><br><em><sub>Image credit in list of evidence: Ashwin Kumar, Creative Commons Attribution-Share Alike 2.0 Generic.</sub></em></p>


<div id="">
					<p>Keep Exploring</p>
					<h2>Discover More Topics From NASA</h2>
				</div></div></article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[99% of adults over 40 have shoulder "abnormalities" on an MRI, study finds (127 pts)]]></title>
            <link>https://arstechnica.com/health/2026/02/99-of-adults-over-40-have-shoulder-abnormalities-on-an-mri-study-finds/</link>
            <guid>47064944</guid>
            <pubDate>Wed, 18 Feb 2026 19:08:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/health/2026/02/99-of-adults-over-40-have-shoulder-abnormalities-on-an-mri-study-finds/">https://arstechnica.com/health/2026/02/99-of-adults-over-40-have-shoulder-abnormalities-on-an-mri-study-finds/</a>, See on <a href="https://news.ycombinator.com/item?id=47064944">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>Breaking the findings down to shoulders instead of people, of the 1204 shoulders in the study, 1,076 (90 percent) were asymptomatic while 128 (10 percent) were symptomatic. Of the 1,076 asymptomatic shoulders, 96 percent had RC abnormalities (1,039 of 1,076), and of the 128 symptomatic shoulders, 98 percent had abnormalities (126 of 128).</p>
<p>Prevalence of tendinopathy and partial-thickness tears was similar between the symptomatic and asymptomatic groups. It initially looked like full-thickness tears were more common in the symptomatic groups, but when researchers adjusted for other factors, including additional abnormalities spotted in the MRIs, the difference between the symptomatic and asymptomatic groups vanished.</p>
<h2>Context</h2>
<p>The authors argue that the findings suggest clinicians should rethink MRI findings, changing not just how they’re used, but also how they’re explained to patients. The language in particular should change given that “abnormalities” are ubiquitous—thus <em>normal</em>—and shouldn’t be described in terms that indicate a need for repair, like “tear.”</p>
<p>“While we refer to these findings as abnormalities, many likely represent normal age-related changes rather than clinically relevant structural changes,” the authors write. “Adopting more precise and less value-laden terminology—such as lesion, defect, fraying, disruption, structural alteration, or degeneration—may help reduce patient anxiety and the perceived need to do something or fix something by avoiding language that implies trauma or a requirement for repair.”</p>
<p>In <a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2844661">an accompanying editorial</a>, two orthopedic surgeons from the University of California, San Francisco, Edgar Garcia-Lopez and Brian Feeley, agree with the language shift and caution clinicians to proactively put MRI findings in context.</p>
<p>They also address the glaring question of when MRIs should even be used for shoulder pain. They suggest that for pain that’s not related to an injury, clinicians should first try a couple months of watch-and-wait with rest or physical therapy to regain function. If there’s no meaningful improvement, an MRI may be warranted. But they stressed that any further decisions on treatment should be based on the patient’s history, clinical exam, and functional limitations of their shoulder—not just what’s seen on the imaging.</p>
<p>“Of course, the findings of this study are not meant to dissuade clinicians from using MRI when appropriate, but to reinforce that the diagnosis and management of shoulder pain should be guided primarily by functional limitations,” the surgeons write.</p>


          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Echo, an iOS SSH+mosh client built on Ghostty (115 pts)]]></title>
            <link>https://replay.software/updates/introducing-echo</link>
            <guid>47064787</guid>
            <pubDate>Wed, 18 Feb 2026 18:58:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://replay.software/updates/introducing-echo">https://replay.software/updates/introducing-echo</a>, See on <a href="https://news.ycombinator.com/item?id=47064787">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Introducing Echo</h2><p>10th February 2026</p><p>We're thrilled to introduce Echo — a fast, modern SSH client for iOS and iPadOS, built for the new era of rich terminal-based tools and AI coding agents.</p><p>Echo is our first brand new app in a while, and it's our first app for iOS and iPadOS. It's a little different from what we've done before, and we'd love to tell you why we built it and what makes it special.</p><h2>Why build a terminal app?</h2><p>Something exciting has been happening in the terminal over the last couple of years. There's been an explosion in the TUI space — beautifully crafted, highly complex text-based user interfaces that are pushing the boundaries of what a terminal can do. Tools like<!-- --> <a href="https://lazygit.dev/" target="_blank">lazygit</a> <!-- -->and<!-- --> <a href="https://github.com/jesseduffield/lazydocker" target="_blank">lazydocker</a>, powered by libraries like<!-- --> <a href="https://github.com/charmbracelet/bubbletea" target="_blank">Bubbletea</a>,<!-- --> <a href="https://github.com/vadimdemedes/ink" target="_blank">Ink</a> <!-- -->and<!-- --> <a href="https://github.com/Textualize/textual" target="_blank">Textual</a>, have shown that the terminal isn't just for text anymore — it's become a rich, interactive canvas.</p><p>At the same time, AI coding agents have completely changed how developers work. Tools like Claude Code, Codex and Amp are running in terminals on remote machines, generating code, running builds, and waiting for human input. Developers increasingly need to check in on these agents from wherever they are — approve a change on the train, monitor a build from the couch, or nudge an agent in the right direction while away from their desk.</p><p>And then there's<!-- --> <a href="https://ghostty.org/" target="_blank">Ghostty</a>. Ghostty is our favourite terminal emulator. It's a terminal built with our own values of performance and true-to-platform native behaviour, as well as being beautiful, fast and flexible. Ghostty's terminal engine is open source and written in a way that can be embedded, which meant we could bring that same level of performance and correctness to iOS.</p><p>Over the last twelve months, we've been using all of these tools heavily. How we program has changed dramatically — we spend more time in the terminal than ever, working alongside agents, reviewing their output, and managing remote machines. At some point we realised that the app we kept reaching for on our phones didn't exist yet. So we built it. Echo is that app.</p><h2>Built for iOS, not ported to it</h2><p>We didn't want to just wrap a terminal in a WebView and ship it, or use a substandard terminal emulator. Echo is a native app, built from the ground up for iPhone and iPad. That means Metal-accelerated rendering, native Keychain integration for your SSH keys, and Face ID to keep your connections secure.</p><p>On iPhone, we spent a lot of time on the keyboard experience. There's a specially designed toolbar above the keyboard with quick keys for common terminal characters, and gesture-based arrow key movement that feels right at home on a touchscreen. It sounds like a small thing, but it makes a huge difference when you're actually trying to get work done on your phone.</p><p>On iPad, Echo really shines. Full hardware keyboard support with all the shortcuts you'd expect, Split View and Slide Over for running multiple sessions side by side, and Stage Manager support so you can resize and arrange terminal windows alongside your other apps. It's the kind of experience that makes an iPad with a keyboard feel like a genuinely capable development machine.</p><figure><p><img src="https://replay.software/echo/images/ipad-multiwindow.png" alt="Echo running in split view on iPad with multiple terminal windows"><span></span></p><figcaption>Multiple terminal sessions running side by side on iPad with Stage Manager.</figcaption></figure><h2>A home for agents on the go</h2><p>One of the things we're most excited about is how well Echo works as an interface for AI coding agents. Echo's minimal, distraction-free UI turns out to be the perfect environment for this — you SSH into your machine, attach to a tmux session, and you're right back where you left off with your agent.</p><p>We've been using Echo ourselves to interact with Claude Code, Codex, and others — and it's genuinely changed how we think about those workflows. Being able to approve a file change or review a diff from your phone feels like a superpower. The complex TUI interfaces that these agents present — syntax-highlighted code, interactive diffs, progress indicators — they all render beautifully because of Ghostty's terminal engine underneath.</p><figure><p><img src="https://replay.software/echo/images/agents-hero.png" alt="Echo app showing Claude Code and AI agents"><span></span></p><figcaption>Interacting with AI coding agents on the go via Echo.</figcaption></figure><h2>Themes, naturally</h2><p>If you know us from<!-- --> <a href="https://replay.software/sleeve">Sleeve</a>, you know we care about customization. Echo ships with a curated collection of terminal themes so you can make your terminal feel like yours. We've hand-picked some classics and some fresh options, and we'll be adding more over time.</p><h2>Available now</h2><p>Echo is available today on the App Store for iPhone and iPad, for a one-time purchase of $2.99. No subscriptions, no in-app purchases — just the way we like it.</p><p>This is a really exciting release for us. Echo represents something new for Replay — our first step into iOS, and into a space that's evolving incredibly fast. We have a lot of ideas for where to take Echo next, and we can't wait to share them with you.</p><p>If you have any questions, feedback, or just want to say hi, you can always reach us at<!-- --> <a href="https://replay.software/cdn-cgi/l/email-protection#3b484e4b4b54494f7b495e4b575a421548545d4f4c5a495e"><span data-cfemail="acdfd9dcdcc3ded8ecdec9dcc0cdd582dfc3cad8dbcddec9">[email&nbsp;protected]</span></a>. We'd love to hear what you think.</p><p><b>Al &amp; Hector</b><img src="https://replay.software/replay/images/Signatures.gif" width="208" height="70"></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cosmologically Unique IDs (420 pts)]]></title>
            <link>https://jasonfantl.com/posts/Universal-Unique-IDs/</link>
            <guid>47064490</guid>
            <pubDate>Wed, 18 Feb 2026 18:37:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jasonfantl.com/posts/Universal-Unique-IDs/">https://jasonfantl.com/posts/Universal-Unique-IDs/</a>, See on <a href="https://news.ycombinator.com/item?id=47064490">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>We are an exploratory species, just past the solar system now, but perhaps one day we will look back and call our galaxy merely the first. There are many problems to solve along the way, and today we will look at one very small one. How do we assign IDs to devices (or any object) so the IDs are guaranteed to always be unique?</p><p>Being able to identify objects is a fundamental tool for building other protocols, and it also underpins manufacturing, logistics, communications, and security. Every ship and satellite needs an ID for traffic control and maintenance history. Every radio, router, and sensor needs an ID so packets have a source and destination. Every manufactured component needs an ID for traceability. And at scale, the count explodes: swarms of robots, trillions of parts, and oceans of cargo containers moving through a civilization’s supply chain.</p><p>One of the key functions of an ID is to differentiate objects from one another, so we need to make sure we don’t assign the same ID twice. Unique ID assignment becomes a more challenging problem when we try to solve it at the scale of the universe.</p><p>But we can try.</p><h2 id="random"><span>Random</span><a href="#random"><i></i></a></h2><p>The first and easiest solution is to pick a random number every time a device needs an ID.</p><p>This is so simple that it is likely the best solution; you can do this anytime, anywhere, without the need for a central authority or coordination of any kind.</p><p>The big issue, though, is that it’s possible for two devices to pick the same ID by chance. Fortunately, we have complete control over the size of the random number, and by extension, the probability of a collision. This means we can make the likelihood of a collision functionally zero.</p><p>You may say that “functionally zero” is not enough, that although the probability is small, it is not <em>actually</em> zero, and so you are concerned. But consider this example: The probability of you being struck by a meteorite right now is small but non-zero, and you might even call that a “reasonable” (if paranoid) concern. But are you worried that every human on Earth will be hit by a meteorite right now? That probability is also non-zero, yet it is so infinitesimally small that we treat it as an impossibility. That is how small we can make the probability of an ID collision.</p><p>So how small does this probability need to be before we are comfortable? It will be helpful to reframe the question: How many IDs can we generate before a collision is expected?</p><p>The most recent version of <a href="https://en.wikipedia.org/wiki/Universally_unique_identifier">Universally Unique Identifiers</a> (UUIDs), which are a version of what we have been describing, uses 122 random bits. Using <a href="https://en.wikipedia.org/wiki/Birthday_problem">the birthday paradox</a>, we can calculate the expected number of IDs before a collision is $\approx 2^{61}$.</p><p>Is this high, or is it low? Is it enough to last the galaxy-wide expansion of the human race up to the heat death of the universe? Let’s try to calculate our own principled number by looking at the physical limits of the universe.</p><h3 id="universal-limit"><span>Universal limit</span><a href="#universal-limit"><i></i></a></h3><p>The paper <a href="https://arxiv.org/pdf/astro-ph/0404510">“Universal Limits on Computation”</a> has calculated that if the entire universe were a maximally efficient computer (known as <a href="https://en.wikipedia.org/wiki/Computronium">computronium</a>), it would have an upper limit of $10^{120}$ operations before the heat death of the universe. If we assume every operation generates a new ID, then we can calculate how large our IDs need to be to avoid a collision until the universe runs out of time.</p><p>Using approximations from <a href="https://en.wikipedia.org/wiki/Birthday_problem">the birthday paradox</a>, the probability of a collision for $n$ random numbers across a set of $d$ values is</p><p>\[p(n, d) \approx 1 - e^{-\frac{n(n-1)}{2d}}\]</p><p>We want a probability of $p = 0.5$ (this is a close approximation for when a collision is “expected”) for $n = 10^{120}$ numbers, so we can solve for $d$ to get</p><p>\[d \approx -\frac{n(n-1)}{2 \times \ln(1 - p)} = -\frac{10^{120}(10^{120}-1)}{2 \times \ln(1 - 0.5)} \approx 10^{240}\]</p><p>This is how large the ID space must be if we want to avoid a collision until the heat death of the universe. In terms of bits, this would require $\log_{2}(10^{240}) = 797.26$, so at least 798 bits.</p><p>This is the most extreme upper limit, and is a bit overkill. With 798 bits, we could assign IDs to literally everything ever and never expect a collision. Every device, every microchip, every component of every microchip, every keystroke, every tick of every clock, every star and every atom, everything can be IDed using this protocol and we still won’t expect a collision.</p><h3 id="reasonable-limits"><span>Reasonable limits</span><a href="#reasonable-limits"><i></i></a></h3><p>A more reasonable upper limit might be to assume that every atom in the observable universe will get one ID (we assume atoms won’t be assigned multiple IDs throughout time, which is a concession). There are an estimated $10^{80}$ atoms in the universe. Using the same equation as above, we find that we need 532 bits to avoid (probabilistically) a collision up to that point.</p><p>Or maybe we convert all of the mass of the universe into 1-gram nanobots? We would have $1.5 \times 10^{56}$ bots, which would require IDs of 372 bits.</p><p>We now have four sizes of IDs we can choose from, depending on how paranoid we are:</p><ul><li>798 bits from computronium</li><li>532 bits for atoms</li><li>372 bits for 1-gram nanobots</li><li>122 bits from UUIDs</li></ul><blockquote><p>Note that this has assumed true randomness when generating a random number, but this is sometimes a challenge. Many random number generators will use a pseudo-random number generator with a non-random seed. You want to ensure your hardware is capable of introducing true randomness, such as from a quantum source, or by using a cryptographically secure pseudorandom number generator (CSPRNG). If that is not available, using sensor data, timestamps, or other non-deterministic sources can help add additional randomness, but, it will not be pure randomness and therefore it will increase the probability that IDs collide. It would probably be a good idea to ban any IDs that are “common”, such as the first 1,000 IDs from every well known pseudo-random generator, the all-zeros ID, the all-ones ID, etc..</p></blockquote><p>But what if we are exceptionally paranoid and <strong>demand</strong> that the IDs are theoretically guaranteed to be unique? None of this probabilistic nonsense. That will take us on a journey.</p><h2 id="deterministic"><span>Deterministic</span><a href="#deterministic"><i></i></a></h2><p>As usual, let’s start with the easiest solution and work from there.</p><blockquote><p>All the code for visuals, simulations, and analysis can be found at <a href="https://github.com/JasonFantl/CUID-blog-post">this github repo</a>.</p></blockquote><p>Let’s create a single central computer that uses a counter to assign IDs. When someone requests an ID, it assigns the value of its counter, then increments the counter so the next ID will be unique. This scheme is nice since it guarantees uniqueness and the length of the IDs grows as slow as possible: logarithmically.</p><p>If all the 1-gram nanobots got an ID from this central computer, the longest ID would be $\log_2(1.5 \times 10^{56}) = 187$ bits. Actually, it would be a tiny bit longer due to overhead when <a href="https://jasonfantl.com/posts/Universal-Codes/">encoding a variable-length value</a>. We will ignore that for now.</p><p>Ok, there are serious issues with this solution. The primary issue I see is access. What if you’re on a distant planet and don’t have communication with the central computer? Or maybe your planet is so far from the computer that getting an ID would take days. Unacceptable.</p><p>In order to fix this, we might start sending out satellites in every direction that can assign unique IDs. Imagine we send the first satellite with ID <code>0</code>, then the next with <code>1</code>, and keep incrementing. Now people only need to request an ID from their nearest satellite and they will get back an ID that looks like <code>A.B</code>, where <code>A</code> is the ID of the satellite and <code>B</code> is the counter on the satellite. For example, the fourth satellite assigning its tenth ID would send out <code>3.9</code>. This ensures that every ID is unique and that getting an ID is more accessible.</p><p>But why stop at satellites? Why not let <em>any</em> device with an ID be capable of assigning new IDs?</p><p>For example, imagine a colony ship is built and gets the sixth ID from satellite <code>13</code>, so it now has an ID of <code>13.5</code>. The colonists take this ship to the outer rim, too far to communicate with anyone. When they reach their planet, they build construction robots which need new IDs. They can’t request IDs from a satellite since they are too far, but they could request IDs from their ship. The construction bots get IDs <code>13.5.3</code> and <code>13.5.4</code> since the ship had already assigned 3 IDs before this time and its counter was at <code>3</code>. And now these robots could assign IDs as well!</p><p>This does assume you always have at least one device capable of assigning IDs nearby. But, if you are in conditions to be creating new devices, then you probably have at least one pre-existing device nearby.</p><p>Let’s call this naming scheme Dewey.</p><h3 id="dewey"><span>Dewey</span><a href="#dewey"><i></i></a></h3><p>How does Dewey compare to the random-IDs in terms of bits required?</p><p>If an ID is of the form <code>A.B. ... .Z</code>, then we can encode that using <a href="https://en.wikipedia.org/wiki/Elias_omega_coding">Elias omega coding</a>. For now we will ignore the small overhead of the encoding and assume each number is perfectly represented using its binary values, but we will add it back in later. That means the ID <code>4.10.1</code> would have the binary representation <code>100.1010.1</code>, which has 8 bits. We can see how each value in the ID grows logarithmically since a counter grows logarithmically.</p><p>How the IDs grow over time will depend on what order IDs are assigned. Let’s look at some examples.</p><p>If each new device goes to the original device, creating an expanding subtree, then the IDs will grow logarithmically. This is exactly the central computer model we considered earlier.</p><p><a href="https://jason-fantl-blog.b-cdn.net/Dewey-subtree.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/Dewey-subtree.gif" alt="" width="500" data-proofer-ignore=""></a></p><p>If we take the other extreme, where each new device requests an ID from the most recent device, then we form a chain. The IDs will grow linearly in this case.</p><p><a href="https://jason-fantl-blog.b-cdn.net/Dewey-chain.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/Dewey-chain.gif" alt="" width="500" data-proofer-ignore=""></a></p><p>Or what if each new device chooses a random device to request an ID from? The growth should be something between linear and logarithmic. We will look more into this later.</p><p>We might also ask, what are the best-case and worst-case assignment trees for this scheme? We can just run the simulation and select the best or worst next node and see what happens. Note that there are multiple ways to show the best-case and worst-case since many IDs have the same length, so we arbitrarily have to pick one at a time, but the overall shape of the tree will be the same. Also note that this uses one-node lookahead, which might fail for more complex schemes, but is valid here.</p><div><p><a href="https://jason-fantl-blog.b-cdn.net/Dewey-chain.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/Dewey-chain.gif" alt="" width="500" data-proofer-ignore=""></a></p><p><a href="https://jason-fantl-blog.b-cdn.net/Dewey-best.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/Dewey-best.gif" alt="" width="500" data-proofer-ignore=""></a></p></div><p>We see one worst-case tree is the chain. This best-case tree for Dewey seems to have every node double its children, then repeat. This causes it to grow wide quite quickly. This indicates that this scheme would be great if we expect new devices to primarily request IDs from nodes that already have many children, but not great if we expect new devices to request IDs from other newer devices (the chain is the extreme example of this).</p><p>Here is the best-case at a larger scale to get a more intuitive feel for how the graph grows. What we care about is the fact that it is a fairly dense graph, which means this scheme would be best if humans use a small number of nodes to request IDs from.</p><p><a href="https://jason-fantl-blog.b-cdn.net/Dewey-large.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/Dewey-large.gif" alt="" width="600" data-proofer-ignore=""></a></p><p>It’s annoying that the chain of nodes causes the ID to grow linearly. Can we design a better ID-assignment scheme that would be logarithmic for the chain as well?</p><h3 id="binary"><span>Binary</span><a href="#binary"><i></i></a></h3><p>Here is another attempt at an ID-assignment scheme, let’s see if it will grow any slower.</p><p>Take the entire space of IDs, visualized as a binary tree. Each device will have an ID somewhere on this tree. In order to assign new IDs, a device will take the column below it (columns alternate from left or right for each device) and assign the IDs in that column. With this scheme each node has a unique ID and also has an infinite list of IDs to assign (the blue outline in the figure), each of which also has an infinite list of IDs to assign, and so on.</p><p><a href="https://jason-fantl-blog.b-cdn.net/Binarytree-statespace.jpg"><img data-src="https://jason-fantl-blog.b-cdn.net/Binarytree-statespace.jpg" alt="" width="500" data-proofer-ignore="" src="https://jason-fantl-blog.b-cdn.net/Binarytree-statespace.jpg"></a></p><p>And now we can look at how it grows across a subtree and across a chain.</p><div><p><a href="https://jason-fantl-blog.b-cdn.net/Binary-subtree.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/Binary-subtree.gif" alt="" width="500" data-proofer-ignore=""></a></p><p><a href="https://jason-fantl-blog.b-cdn.net/Binary-chain.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/Binary-chain.gif" alt="" width="500" data-proofer-ignore=""></a></p></div><p>Both cases grow linearly. This is not what we were looking for. It’s now worth asking: Is this scheme always worse than the Dewey scheme?</p><p>If we look at the worst-case and best-case of this scheme, we notice that the best-case will grow differently then Dewey.</p><div><p><a href="https://jason-fantl-blog.b-cdn.net/Binary-subtree.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/Binary-subtree.gif" alt="" width="500" data-proofer-ignore=""></a></p><p><a href="https://jason-fantl-blog.b-cdn.net/Binary-best.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/Binary-best.gif" alt="" width="500" data-proofer-ignore=""></a></p></div><p>And the best-case at a larger scale.</p><p><a href="https://jason-fantl-blog.b-cdn.net/Binary-large.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/Binary-large.gif" alt="" width="600" data-proofer-ignore=""></a></p><p>It grows roughly equally in all directions. The depth of the best-case tree grows faster than Dewey, which means this scheme would be better for growth models where new nodes are equally likely to request from older nodes and newer nodes. Specifically, the best-case tree grows by adding a child to every node in the tree and then repeating.</p><p>So this scheme can be better for some trees when compared to Dewey. Let’s keep exploring.</p><p>Actually, there is a scheme that looks different, but grows the same as this one.</p><h4 id="2-adic-valuation"><span>2-Adic Valuation</span><a href="#2-adic-valuation"><i></i></a></h4><p>If each ID is an integer, then a node with ID $n$ would assign to its $i$th child the ID $2^i(2n+1)$. Essentially, each child will double the ID from the previous child, and the first child has the ID $2n+1$ from its parent. This is a construction based on <a href="https://en.wikipedia.org/wiki/P-adic_valuation">2-adic valuation</a>.</p><p>You can prove that this generates unique IDs by using the <a href="https://en.wikipedia.org/wiki/Fundamental_theorem_of_arithmetic">Fundamental Theorem of Arithmetic</a>.</p><p>You can change the memory layout of this scheme pretty easily by using $(i, n)$ as the ID instead of $2^i(2n+1)$. Now the sequential child IDs of a node will grow logarithmically instead of linearly. This feels very similar to Dewey.</p><p>That’s all a bit complicated, but essentially we can say that this is an alternative representation of the Binary scheme we already looked at. But we want to explore new schemes that might have better memory growth characteristics.</p><h3 id="token"><span>Token</span><a href="#token"><i></i></a></h3><p>Let’s try to reverse-engineer a scheme that can grow logarithmically for the chain tree.</p><p>We know that a counter grows logarithmically, so ideally the ID would only increment a counter when adding a new node.</p><p>One idea is to have a token that gets passed down to children with a hop-count attached to it. But what happens when a device gets a new ID request and it doesn’t have a token to pass? We will have a token index which increments each time a parent has to create a new token. The new token will then be appended to the parent ID. So the chain of three will look like <code>[]</code>, <code>[(0,0)]</code>, <code>[(0,1)]</code>, as the root node has no token, then the first child causes the root to generate token, then the next hop gets the token passed down to it with an incremented hop count. If the root node had two more ID requests, it would generate <code>[(1,0)]</code> and <code>[(2,0)]</code>, incrementing the first value to produce unique tokens. Each ID is a list of (token-index, hop-count) pairs, ordered by creation. Let’s get a better idea of what this looks like by looking at a simulation.</p><p>Here we have the expanding subtree, the chain, and one of the best-cases.</p><div><p><a href="https://jason-fantl-blog.b-cdn.net/Token-subtree.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/Token-subtree.gif" alt="" width="500" data-proofer-ignore=""></a></p><p><a href="https://jason-fantl-blog.b-cdn.net/Token-chain.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/Token-chain.gif" alt="" width="500" data-proofer-ignore=""></a></p></div><p><a href="https://jason-fantl-blog.b-cdn.net/Token-best.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/Token-best.gif" alt="" width="500" data-proofer-ignore=""></a></p><p>We can see that IDs are a bit longer in general since we have more information in each ID, but at least it grows logarithmically in our extreme cases.</p><p>This logarithmic growth for chains is reflected in the larger-scale best-case graph, where we see long chains growing from the root.</p><p><a href="https://jason-fantl-blog.b-cdn.net/Token-large.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/Token-large.gif" alt="" width="600" data-proofer-ignore=""></a></p><p>This is kind of a lie though. The chain is logarithmic, but if we add even one more child to any node, the scheme starts to grow linearly. If our graph grows even a little in both depth and width together, we find ourselves back at the linear regime. We didn’t generate the worst-case graph above since our simulation uses a greedy search algorithm and the worst-case takes two steps to identify. The true worst-case is hard-coded and shown below, which we can see does grow linearly.</p><p><a href="https://jason-fantl-blog.b-cdn.net/Token-worst.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/Token-worst.gif" alt="" width="500" data-proofer-ignore=""></a></p><p>So we have yet to find an algorithm that produces logarithmic growth in all cases. Is it even possible to design a scheme that always grows logarithmically, even in the worst-case?</p><details> <summary> Unfortunately not. Here is the proof that any scheme we develop will always be linear in the worst-case. </summary><p>In order to prove how fast any scheme must grow, we will look at how fast the number of possible IDs grows as nodes are added. This will require iterating over every possible assignment history and then counting how many unique possible IDs there are in the space of all possible assignment histories.</p><p>It is important to note that each path must produce a different ID. If any two paths produced the same ID, that means it would be possible to generate two nodes with the same ID.</p><p>To get our grounding, let’s first consider the tree containing all the possible 4-node paths. We will see in a moment that it will be useful to label each node using a 1-indexed Dewey system. The labels are not IDs (we are trying to write a proof about <em>any</em> possible ID scheme), the labels are just useful for talking about the paths and nodes.</p><p><a href="https://jason-fantl-blog.b-cdn.net/proof.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/proof.gif" alt="" width="500" data-proofer-ignore=""></a></p><p>We see every possible sequence for reaching the fourth node (only considering nodes along the path to that node) highlighted above. So we can now count how many possible IDs we need in tree with 4 nodes for <em>any</em> assignment order of those 4 nodes.</p><p>We see that there are 16 nodes in the tree, so whatever ID-assignment scheme we build must account for 16 unique IDs by the time we have added four nodes.</p><p>In general, notice that each time we add a new ID, we add a new leaf to every node in the tree of all possible paths. This means the number of IDs we need to account for grows as $2^{n-1}$ for $n$ nodes.</p><p>We can similarly come to this conclusion by looking at the labels. The sum of the values in a label will equal the iteration at which that node was added. It is also true the other direction: All possible paths of $n$ nodes can be generated by looking at all possible sums of numbers up to $n$, although they must be greater than 0 and the order of the sum will matter. These are known as <a href="https://en.wikipedia.org/wiki/Composition_(combinatorics)">integer compositions</a>, and they produce the result we saw from above, $2^{n-1}$ paths for $n$ nodes.</p><p>This is an issue. Even in the ideal case where we label each possible node in the space of all histories using a counter (this is actually a valid ID-assignment scheme and generates the 2-Adic Valuation scheme we have already seen), the memory of a counter grows logarithmically. No matter what scheme we use, the memory must grow at least on the order of $\log_2(2^{n-1}) = n-1$, linearly.</p></details><p>Although we have proven that whatever scheme we come up with will be linear in the worst-case, it seems plausible that some algorithms perform better than others for different growth models. If we can find a reasonable growth model for humans expanding into the universe, then we should be able to reverse-engineer the best algorithm.</p><h2 id="space-settlement-models"><span>Space Settlement Models</span><a href="#space-settlement-models"><i></i></a></h2><p>Let us consider different models that approximate how humans might expand into the universe.</p><h3 id="small-scale"><span>Small-scale</span><a href="#small-scale"><i></i></a></h3><p>The first and easiest model to consider is random parent selection. Each time a device is added it will randomly select from all the previous devices to request an ID. This will produce what is known as a <a href="https://en.wikipedia.org/wiki/Random_recursive_tree">Random Recursive Tree</a>. We will also run this at a small scale, up to around 2,048 nodes. And we will actually use the <a href="https://en.wikipedia.org/wiki/Elias_omega_coding">Elias omega encoding</a> so we can have more comparable results to the Random ID assignment bit usage.</p><p><a href="https://jason-fantl-blog.b-cdn.net/Random-graph-plot.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/Random-graph-plot.gif" alt="" width="600" data-proofer-ignore=""></a></p><p>The best scheme is Binary, followed by Dewey, and Token is the worst. This makes some sense since a random tree will grow at a roughly equal rates in depth and width, which is the best-case for Binary. Dewey and Token are harder to reason about, but we suspect that Dewey does best for high-width trees and Token for high-depth trees.</p><p>For example, we can look at a <a href="https://en.wikipedia.org/wiki/Preferential_attachment">preferential attachment</a> random graph, where nodes are more likely to connect to nodes with more connections, a model which many real-world networks follow. The width of the tree will dominate the depth, so we might expect Dewey to win out. Specifically, <a href="https://en.wikipedia.org/wiki/Preferential_attachment">preferential attachment</a> chooses a node weighted by the degree (number of edges) to choose a parent, which increases the degree of that parent, creating positive feedback. Let’s see how each ID assignment scheme handles this new growth model.</p><p><a href="https://jason-fantl-blog.b-cdn.net/PreferentialRandom-graph-plot.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/PreferentialRandom-graph-plot.gif" alt="" width="600" data-proofer-ignore=""></a></p><p>And we see that Dewey performs best, followed by Token, and then Binary by a wide margin.</p><p>Although, it seems unrealistic that devices become more popular because they assign more IDs. It seems reasonable to believe that some devices are more popular than others, but that popularity is not dependent on its history. A satellite will be very popular relative to a lightbulb, not because the satellite happened to assign more IDs in the past, but because its intrinsic properties like its position and accessibility make it easier to request IDs from. We could use a <a href="https://en.wikipedia.org/wiki/Fitness_model_(network_theory)">fitness model</a>, where each node is initialized with a fitness score that determines how popular it will be. The fitness score is sampled from an exponential.</p><p><a href="https://jason-fantl-blog.b-cdn.net/FitnessRandom-graph-plot.gif"><img data-src="https://jason-fantl-blog.b-cdn.net/FitnessRandom-graph-plot.gif" alt="" width="600" data-proofer-ignore=""></a></p><p>And it seems that Dewey and Binary do equally well, with Token producing the worst IDs. Although this seems pretty similar to the purely Random graph.</p><p>We need to run a large number of simulations for a large number of nodes and see if there’s a consistent pattern.</p><h3 id="medium-scale"><span>Medium-scale</span><a href="#medium-scale"><i></i></a></h3><p>Below we run 1,000 simulations for each growth model, building a graph up to about a million ($2^{20}$) nodes. We plot the maximum ID of the graph over time. Each run is shown as a line, then the x axis is made exponential since we suspect that the IDs grow with the logarithm of the node count, which will be easier to see with an exponential x axis.</p><p><a href="https://jason-fantl-blog.b-cdn.net/Million-node-sim.png"><img data-src="https://jason-fantl-blog.b-cdn.net/Million-node-sim.png" alt="" width="700" data-proofer-ignore="" src="https://jason-fantl-blog.b-cdn.net/Million-node-sim.png"></a></p><p>That’s some pretty clean results! We see a roughly straight line for most plots (the exceptions being Binary for the Preferential growth model and the Fitness growth model where it curves a small amount). The straight lines are a strong indication that the growth of IDs actually is logarithmic, and that we could fit a curve to it. To inspect the Preferential model for the other ID assignment schemes, let’s plot it again without Binary.</p><p><a href="https://jason-fantl-blog.b-cdn.net/preferential_plot.png"><img data-src="https://jason-fantl-blog.b-cdn.net/preferential_plot.png" alt="" width="650" data-proofer-ignore="" src="https://jason-fantl-blog.b-cdn.net/preferential_plot.png"></a></p><p>And we still see the linear trends on the exponential plot, which indicates that Dewey and Token schemes still grow logarithmically.</p><details> <summary> Here is my best explanation for why the plots are logarithmic. </summary><p>In the Random growth model, each node is statistically indistinguishable from the others, so we should expect every node to see the same <em>average</em> subtree over time. In distribution, the subtree under the root should look similar to the subtree under the millionth node, just at a smaller scale. This suggests that we can use a recursive relation between these subtrees to infer the overall scaling law.</p><p>Suppose we simulate the growth of a 1,000-node tree and observe that the maximum ID length has increased by about 34 bits (which is what we saw for Dewey). We then take the node with the longest ID among those 1,000 nodes and conceptually re-run a 1,000-node simulation with this node acting as the root. Because the Random model treats all nodes symmetrically, we expect this node’s subtree to grow in a statistically similar way to the original root’s subtree. Since all of our ID assignment schemes have additive ID lengths along ancestry, growing this subtree to 1,000 nodes should increase the maximum ID length by roughly another 34 bits.</p><p>However, this subtree is embedded inside the full tree. By the time this node has accumulated 1,000 descendants, we should expect that all other nodes in the original tree have also accumulated, on average, about 1,000 descendants. In other words, each time we simulate an isolated 1,000-node subtree, the full tree size grows by a factor of 1,000, while the maximum ID length increases by an approximately constant amount. In practice we observed an increase closer to 38 bits rather than 34, which could be due to noise, small-(n) effects, encoding overhead, or flaws in this heuristic.</p><p>This means the ID length is growing linearly while the total number of nodes is growing exponentially. In this example, the maximum-ID-length function satisfies a recurrence of the form $T(n \cdot 1000^d) \approx T(n) + 34 d$ which is only satisfied by a logarithmic function. Writing this explicitly, we get $T(n) \propto \log(n)$ (with the base—about (1.225) in this case—set by the observed constant).</p><p>This analysis is harder to apply to the Fitness and Preferential model, as nodes are different from each other in those schemes. But the plots do indicate that it is probably still true. It might be that the analysis is still true <em>on average</em> for these schemes, and so the finer details about different nodes gets washed away when we scale up, but I don’t feel confident about that argument. Bigger simulations might help identify if the trends are actually non-logarithmic.</p><p>Future simulations might also consider that devices have lifetimes (nodes disappear after some time), which can dramatically alter the analysis. Initial tests with a constant lifetime (relative to how many nodes have been added) showed linear growth of IDs over time. This makes sense since it essentially forces a wide chain, which we know grows linearly for all our ID assignment schemes. Is this a reasonable assumption? What if devices live longer if they are more popular, how might that change the outcome?</p></details><p>For now we will use the above simulations as the first rung on our <a href="https://en.wikipedia.org/wiki/Cosmic_distance_ladder">ladder</a> of simulations, using those results to plug into larger models which then are plugged into even larger models.</p><h3 id="large-scale"><span>Large-scale</span><a href="#large-scale"><i></i></a></h3><p>In order to determine how many bits these schemes might require for a universe-wide humanity, we need to evaluate models of how our IDs will grow between worlds.</p><p>We will use the million-node simulation of the Fitness growth model to model the assignment of IDs on the surface of a planet for its first few years. To scale up to a full planet over hundreds of years, we can fit a logarithmic curve to our Fitness model and extrapolate.</p><p>For this analysis we will select the Dewey ID assignment scheme since it seems to perform well across all growth models.</p><p><a href="https://jason-fantl-blog.b-cdn.net/log.png"><img data-src="https://jason-fantl-blog.b-cdn.net/log.png" alt="" width="100" data-proofer-ignore="" src="https://jason-fantl-blog.b-cdn.net/log.png"></a></p><p>When we fit a logarithmic curve to the max ID length of Dewey ID assignment in the Fitness growth model, it fits the curve $(6.5534 ± 0.2856) \ln(n)$ (where $0.2856$ is the standard deviation). This equation now allows us to closely approximate the max ID length after an arbitrary number of devices.</p><p>We have our model for expansion on a planet, now we need a model for how humanity spreads from one planet to the next. We can’t really know what it will look like when/if we expand into the universe, but people have definitely tried. Below are some papers modeling how humans will expand into the universe, from which we can try to create our own best-guess model more relevant to our analysis.</p><ul><li><a href="https://ntrs.nasa.gov/api/citations/19790011801/downloads/19790011801.pdf">Galactic Civilizations: Population Dynamics and Interstellar Diffusion</a>, by Newman and Sagan. Essentially, expansion through the galaxy is slow because only newly settled planets contribute to further spread, and each must undergo local population growth before exporting colonists, producing a slow and constant traveling wavefront of expansion across the galaxy.</li><li><a href="https://ntrs.nasa.gov/api/citations/19940022867/downloads/19940022867.pdf">The Fermi Paradox: An Approach Based on Percolation Theory</a>, by Geoffrey A. Landis. Essentially, using Percolation Theory with some “reasonable” values for the rate of spreading to new planets and rates of survival, this paper finds that some wavefronts will die out while others survive, meaning we will slowly spread through the galaxy in branches.</li><li><a href="https://arxiv.org/pdf/1902.04450v2">The Fermi Paradox and the Aurora Effect: Exo-civilization Settlement, Expansion and Steady States</a>. Essentially, modeling solar systems as a gas and settlement as a process that depends on the distance between planets, planets living conditions, and civilization lifetimes, they find that distant clusters of the universe will fall into a steady state of being settled.</li></ul><p>We will model the expansion between planets in a galaxy by using a constant-speed expanding wavefront that settles any habitable planet, where that new planet is seeded with a random ID from the closest settled planet. We will use the same model for the expansion between galaxies.</p><p>This will produce linear growth of ID-length as the wavefront moves outward. As each planet restarts the ID assignment process, it will cause the ID length to grow larger according to the same curve we saw for the first planet.</p><p>We have a rough estimate that there might be around <a href="https://www.latimes.com/science/la-sci-earth-like-planets-20131105-story.html">40 billion habitable planets in our Milky Way galaxy</a>, and the latest estimates hold there are around <a href="https://science.nasa.gov/missions/hubble/hubble-reveals-observable-universe-contains-10-times-more-galaxies-than-previously-thought/">2 trillion galaxies in the observable universe</a>.</p><p>If we assume that planets are close to uniformly positioned in a galaxy and the galaxy is roughly spherical (many galaxies are actually disks, but it won’t change the final conclusion), then we can expect the radius of the galaxy in terms of planet-hops can be solved for using the equation of the volume of a sphere. The radius in terms of planet-hops can be approximated by $\sqrt[3]{\frac{3V}{4 \pi}} = \sqrt[3]{\frac{3 \cdot 40 \cdot 10^{9}}{4 \pi}} \approx 2121$.</p><p><a href="https://jason-fantl-blog.b-cdn.net/solar-systems.png"><img data-src="https://jason-fantl-blog.b-cdn.net/solar-systems.png" alt="" width="400" data-proofer-ignore="" src="https://jason-fantl-blog.b-cdn.net/solar-systems.png"></a></p><p>If we assume each planet produces around 1 billion IDs before settling the next nearest planet, then we can calculate the ID length by the time it reaches the edge of the galaxy. This will be the amount by which the longest ID increases per planet (we are assuming 1 billion assignments) multiplied by the number of times this happens, which is the number of planets we hop to reach the edge of the galaxy. This doesn’t sound good.</p><p>\[6.5534 \cdot \ln(10^9) \cdot 2121 \approx 288048\]</p><p>That is a lot of bits. And it will only get worse. We will use the same approximation for galaxies as we did for planets.</p><p><a href="https://jason-fantl-blog.b-cdn.net/universe.png"><img data-src="https://jason-fantl-blog.b-cdn.net/universe.png" alt="" width="400" data-proofer-ignore="" src="https://jason-fantl-blog.b-cdn.net/universe.png"></a></p><p>Again assuming galaxies fill space uniformly, and as a sphere, we get the number of hops between galaxies to be $\sqrt[3]{\frac{3 \cdot 2 \cdot 10^{12}}{4 \pi}} \approx 7816$. And using the $288048$ from above as the length the ID increases every galaxy, we get</p><p>\[288048 \cdot 7816 = 2251383168\]</p><p>That is an exceptionally large number of bits. It would take about $281.4$ MB just to store the ID in memory.</p><p>This Deterministic solution is terrible when compared to the Random solution, which even in its most paranoid case only used 798 bits.</p><p>We might see this and try to think of solutions. Maybe we regulate that settlers must bring a few thousand of the shortest IDs they can find from their parent planet to the new planet, which would cut down the ID length per planet by around a half. But unless we find a way to grow IDs logarithmically across planets and galaxies, it won’t get you even close (remember, $2121 \cdot 7816 = 16577736$ planet hops in total).</p><p>So for now it seems the safest bet for universally unique IDs are Random numbers with a large enough range that the probabilities of collisions are functionally zero. But it was fun to consider how we might bring that probability to actually zero: designing different ID assignment schemes, running simulations, and modeling human expansion through the universe.</p><h2 id="end"><span>End</span><a href="#end"><i></i></a></h2><p>All the code for visuals, simulations, and analysis can be found at <a href="https://github.com/JasonFantl/CUID-blog-post">my repo on github</a>.</p><p>This was very much an exploration with many paths left unexplored, please reach out if you explore one of them and want to chat about it, it’s good fun.</p><p>Thanks to Kevin Montambault and Jacob Hendricks for being happy to talk with me for hours on end about these strange interests of mine. I am grateful and privileged to have friends with such deep curiosities.</p><h3 id="side-notes"><span>Side notes</span><a href="#side-notes"><i></i></a></h3><p>Another potential interesting component of this is security. You can prevent ID-spoofing by using signatures to verify identity and that each message comes from who they claim. For the Random case, you would use your public key as your ID. For the Deterministic schemes, each node could sign their child’s public key, which would allow one to verify the chain of signatures up to the root node which all nodes have knowledge of. Replay attacks can be avoided using challenges (send and respond challenges), although they would be hard in unidirectional or delayed comms (planet to planet), so you might label messages as unconfirmed until a challenge is verified.</p><p>We should also add some error correction to the IDs so if someone for example tries to read an ID and mis-reads a letter they can correct it later. Since there are many ways to apply error correction, there should be a version number attached to the error-correcting ID.</p><p>Some objects can not store IDs themselves, such as when an ID is assigned to a planet for example, and so it’s possible that multiple IDs get accidentally assigned to the same object. In this case we should actually store a list of IDs for objects which represent all the IDs that refer to the same object.</p><p>There can be an issue related to the <a href="https://en.wikipedia.org/wiki/Ship_of_Theseus">ship of Theseus</a> where an object with an ID might be slowly repaired with new parts, until eventually all the parts have been replaced. Should this object still have the same ID? One pragmatic solution might be to have the ID stored in a particular piece of hardware and accept that what it means to have a particular ID is to just to have that particular piece of hardware regardless of what it is connected to.</p><p>Here are some related topics to what we have talked about in this post: <a href="https://en.wikipedia.org/wiki/Decentralized_identifier">Decentralized identifiers</a> (DIDs) and <a href="https://link.springer.com/chapter/10.1007/978-3-662-47666-6_45">Ancestry Labeling Schemes</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DNS-Persist-01: A New Model for DNS-Based Challenge Validation (287 pts)]]></title>
            <link>https://letsencrypt.org/2026/02/18/dns-persist-01.html</link>
            <guid>47064047</guid>
            <pubDate>Wed, 18 Feb 2026 18:04:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://letsencrypt.org/2026/02/18/dns-persist-01.html">https://letsencrypt.org/2026/02/18/dns-persist-01.html</a>, See on <a href="https://news.ycombinator.com/item?id=47064047">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>When you request a certificate from Let’s Encrypt, our servers validate that you control the hostnames in that certificate using <a href="https://letsencrypt.org/docs/challenge-types/">ACME challenges</a>. For subscribers who need wildcard certificates or who prefer not to expose infrastructure to the public Internet, the DNS-01 challenge type has long been the only choice. DNS-01 works well. It is widely supported and battle-tested, but it comes with operational costs: DNS propagation delays, recurring DNS updates at renewal time, and automation that often requires distributing DNS credentials throughout your infrastructure.</p>
<p>We are implementing support for a new ACME challenge type, DNS-PERSIST-01, based on a new <a href="https://datatracker.ietf.org/doc/html/draft-ietf-acme-dns-persist-00">IETF draft specification</a>. As the name implies, it uses DNS as the validation mechanism, but replaces repeated demonstrations of control with a persistent authorization record bound to a specific ACME account and CA. The draft describes this method as being “particularly suited for environments where traditional challenge methods are impractical, such as IoT deployments, multi-tenant platforms, and scenarios requiring batch certificate operations”.</p>
<h2 id="dns-01-proves-control-repeatedly">DNS-01 Proves Control Repeatedly</h2>
<p>With DNS-01, validation relies on a one-time token generated by us. Your ACME client publishes a TXT record containing that token at <code>_acme-challenge.&lt;YOUR_DOMAIN&gt;</code>, and we query DNS to confirm that it matches the expected value. Because each authorization requires a new token, DNS updates become part of the issuance workflow. The benefit is that each successful validation provides fresh proof that you currently control DNS for the name being issued.</p>
<p>In practice, this often means DNS API credentials live somewhere in your issuance pipeline, validation attempts involve waiting for DNS propagation, and DNS changes happen frequently — sometimes many times per day in large deployments. Many subscribers accept these tradeoffs, but others would prefer to keep DNS updates and sensitive credentials out of their issuance path.</p>

<p>DNS-PERSIST-01 approaches validation differently. Instead of publishing a new challenge record for each issuance, you publish a standing authorization in the form of a TXT record that identifies both the CA and the specific ACME account you authorize to issue for this domain.</p>
<p>For the hostname example.com, the record would live at <code>_validation-persist.example.com</code>:</p>
<div><pre tabindex="0"><code data-lang="dns"><span><span><span>_validation-persist.example.com. </span><span>IN</span><span> </span><span>TXT</span><span> </span><span>(</span><span>
</span></span></span><span><span><span>  </span><span>"</span><span>letsencrypt.org</span><span>;"</span><span>
</span></span></span><span><span><span>  </span><span>"</span><span> </span><span>accounturi=https://</span><span>acme-v02.api.letsencrypt.org</span><span>/acme/acct/</span><span>1234567890</span><span>"</span><span>
</span></span></span><span><span><span></span><span>)</span><span>
</span></span></span></code></pre></div><p>Once this record exists, it can be reused for new issuance and all subsequent renewals. Operationally, this removes DNS changes from the critical path.</p>
<h2 id="security-and-operational-tradeoffs">Security and Operational Tradeoffs</h2>
<p>With DNS-01, the sensitive asset is DNS write access. In many deployments, DNS API credentials are distributed throughout issuance and renewal pipelines, increasing the number of places an attacker might compromise them. DNS-PERSIST-01 instead binds authorization directly to an ACME account, allowing DNS write access to remain more tightly controlled after initial setup. The tradeoff is that, because the authorization record persists over time, protecting the ACME account key becomes the central concern.</p>
<h2 id="controlling-scope-and-lifetime">Controlling Scope and Lifetime</h2>
<p>DNS-PERSIST-01 also introduces explicit scope controls. Without additional parameters, authorization applies only to the validated Fully Qualified Domain Name (FQDN) and remains valid indefinitely.</p>
<h3 id="wildcard-certificates">Wildcard Certificates</h3>
<p>Adding policy=wildcard broadens the authorization scope to include the validated FQDN, wildcard certificates such as <code>*.example.com</code>, and subdomains whose suffix matches the validated FQDN:</p>
<div><pre tabindex="0"><code data-lang="dns"><span><span><span>_validation-persist.example.com. </span><span>IN</span><span> </span><span>TXT</span><span> </span><span>(</span><span>
</span></span></span><span><span><span>  </span><span>"</span><span>letsencrypt.org</span><span>;"</span><span>
</span></span></span><span><span><span>  </span><span>"</span><span> </span><span>accounturi=https://</span><span>acme-v02.api.letsencrypt.org</span><span>/acme/acct/</span><span>1234567890</span><span>;"</span><span>
</span></span></span><span><span><span>  </span><span>"</span><span> </span><span>policy=wildcard"</span><span>
</span></span></span><span><span><span></span><span>)</span><span>
</span></span></span></code></pre></div><h3 id="optional-expiration">Optional Expiration</h3>
<p>Subscribers who aren’t comfortable with authorization persisting indefinitely can include an optional <code>persistUntil</code> timestamp. This limits how long the record may be used for new validations, but also means it must be updated or replaced before it expires. Anyone using this feature should ensure they have adequate reminders or monitoring in place so that authorization does not expire unexpectedly. The timestamp is expressed as UTC seconds since 1970-01-01:</p>
<div><pre tabindex="0"><code data-lang="dns"><span><span><span>_validation-persist.example.com. </span><span>IN</span><span> </span><span>TXT</span><span> </span><span>(</span><span>
</span></span></span><span><span><span>  </span><span>"</span><span>letsencrypt.org</span><span>;"</span><span>
</span></span></span><span><span><span>  </span><span>"</span><span> </span><span>accounturi=https://</span><span>acme-v02.api.letsencrypt.org</span><span>/acme/acct/</span><span>1234567890</span><span>;"</span><span>
</span></span></span><span><span><span>  </span><span>"</span><span> </span><span>persistUntil=</span><span>1767225600</span><span>"</span><span>
</span></span></span><span><span><span></span><span>)</span><span>
</span></span></span></code></pre></div><h3 id="authorizing-multiple-cas">Authorizing Multiple CAs</h3>
<p>Multiple CAs can be simultaneously authorized by publishing multiple TXT records at <code>_validation-persist.&lt;YOUR_DOMAIN&gt;</code>, each containing the issuer-domain-name of the CA you intend to authorize. During validation, each CA queries the same DNS label and evaluates only the records that match its own issuer-domain-name.</p>
<h2 id="rollout-timeline">Rollout Timeline</h2>
<p>The CA/Browser Forum ballot <a href="https://cabforum.org/2025/10/09/ballot-sc-088v3-dns-txt-record-with-persistent-value-dcv-method">SC-088v3</a>, defining “3.2.2.4.22 DNS TXT Record with Persistent Value”, passed unanimously in October 2025, and the IETF ACME working group adopted the draft that same month. While the document remains an active IETF draft, the core mechanisms described here are not expected to change substantially.</p>
<p>Support for the draft specification is available now in <a href="https://github.com/letsencrypt/pebble">Pebble</a>, a miniature version of <a href="https://github.com/letsencrypt/boulder">Boulder</a>, our production CA software. Work is also in progress on a <a href="https://go-acme.github.io/lego/usage/cli/">lego-cli</a> client implementation to make it easier for subscribers to experiment with and adopt. Staging rollout is planned for late Q1 2026, with a production rollout targeted for some time in Q2 2026.</p>

      

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Warren Buffett dumps $1.7B of Amazon stock (156 pts)]]></title>
            <link>https://finbold.com/warren-buffett-dumps-1-7-billion-of-amazon-stock/</link>
            <guid>47063950</guid>
            <pubDate>Wed, 18 Feb 2026 17:56:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://finbold.com/warren-buffett-dumps-1-7-billion-of-amazon-stock/">https://finbold.com/warren-buffett-dumps-1-7-billion-of-amazon-stock/</a>, See on <a href="https://news.ycombinator.com/item?id=47063950">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><a href="https://finbold.com/guide/who-is-warren-buffett/">Warren Buffett’s</a> Berkshire Hathaway (NYSE: <a href="https://finbold.com/stock/berkshire-hathaway-inc-stockbrk-a/">BRK.A</a>, <a href="https://finbold.com/stock/berkshire-hathaway-class-b-brk-b/">BRK.B</a>) submitted its latest <a href="https://www.sec.gov/Archives/edgar/data/1067983/000119312526054580/0001193125-26-054580-index.html">13-F filing</a> on February 17, 2026, revealing some interesting <a href="https://finbold.com/warren-buffett-just-updated-his-stock-portfolio-3/">changes in the portfolio</a>.&nbsp;</p>



<p>The most newsworthy one was undoubtedly the staggering 77% reduction in the <a href="https://finbold.com/guide/how-to-buy-amazon-stock/">Amazon</a> (NASDAQ: <a href="https://finbold.com/stock/amazon-amzn/">AMZN</a>) stake, as the company has sold 7.7 million shares in the e-commerce leader, reportedly valued at nearly $1.7 billion.</p>



<p>Berkshire first entered Amazon in 2019, and after seven years, the paradigm appears to be shifting again, with ‘The Oracle of Omaha’ apparently returning to long-favored sectors, such as media.</p>



<p>Notably, the filing revealed the former Berkshire CEO had opened a new position in the New York Times (NYSE: <a href="https://finbold.com/stock/nyt-new-york-times/">NYT</a>) with a purchase of 5 million shares, estimated at about $352 million. The purchase sent the publisher’s shares up around 10% as investors reacted to Buffett’s disclosure.</p>



<figure><img decoding="async" width="1024" height="685" src="https://assets.finbold.com/uploads/2026/02/image-147-1024x685.png" alt="" srcset="https://assets.finbold.com/uploads/2026/02/image-147-1024x685.png 1024w, https://assets.finbold.com/uploads/2026/02/image-147-300x201.png 300w, https://assets.finbold.com/uploads/2026/02/image-147-768x514.png 768w, https://assets.finbold.com/uploads/2026/02/image-147.png 1099w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Warren Buffett portfolio top holdings in Q4, 2025. Source: 13-F Info</figcaption></figure>



<h2 id="h-why-is-warren-buffett-dumping-amazon">Why is Warren Buffett dumping Amazon?</h2>



<p>While the Amazon trimming was the most eye-catching, it was not an isolated case. For instance, the now Chairman of Berkshire reduced his Apple (NASDAQ: <a href="https://finbold.com/stock/apple-aapl/">AAPL</a>) holdings to a 1.5% position, which further underscores the retreat from large technology names.</p>








<p>As mentioned, the shift suggests a return to classic Buffett investments, that is, businesses built to withstand economic turbulence. This is suggested by the fact that Berkshire expanded its stake in <a href="https://finbold.com/guide/how-to-buy-chubb-stock/">Chubb</a> (NYSE: <a href="https://finbold.com/stock/chubb-stock-cb/">CB</a>), a steady insurance company, and <a href="https://finbold.com/guide/how-to-buy-chevron-stock/">Chevron</a> (NYSE: <a href="https://finbold.com/stock/chevron-stock-cvx/">CVX</a>), which suggests confidence in energy solutions.</p>



<p>Similarly, Berkshire has agreed to acquire the petrochemical business of Occidental Petroleum Corp. for $9.7 billion and built a $5.6 billion position in <a href="https://finbold.com/guide/how-to-buy-google-stock/">Google</a> (NASDAQ: <a href="https://finbold.com/stock/alphabet-inc-class-c-capital-stock-goog/">GOOGL</a>). Accordingly, it can be argued that the conglomerate is simply adjusting its strategy as it attempts to dig in ahead of a <a href="https://finbold.com/robert-kiyosaki-warns-the-us-dollar-is-about-to-die/">potential economic downturn</a>.</p>



<p><strong><em>Featured image via Shutterstock</em></strong></p>



  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Arizona Bill Requires Age Verification for All Apps (132 pts)]]></title>
            <link>https://reclaimthenet.org/arizona-bill-would-require-id-checks-to-use-a-weather-app</link>
            <guid>47063724</guid>
            <pubDate>Wed, 18 Feb 2026 17:38:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reclaimthenet.org/arizona-bill-would-require-id-checks-to-use-a-weather-app">https://reclaimthenet.org/arizona-bill-would-require-id-checks-to-use-a-weather-app</a>, See on <a href="https://news.ycombinator.com/item?id=47063724">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="7eda0c13" data-element_type="widget" data-e-type="widget" data-widget_type="theme-post-content.default">
					<p>Arizona legislators have introduced what may be the most aggressive app store age verification bill in the country. <a href="https://legiscan.com/AZ/text/HB2920/id/3326319/Arizona-2026-HB2920-Introduced.html">House Bill 2920</a> would require age verification not just for app downloads, but for preinstalled software, the browser, the text messaging app, the search bar, the calculator, and the weather widget. Every piece of software on a mobile device would be subject to age-gating ID checks under this proposal.</p>
<p>The bill, introduced on January 27 and currently pending before the House Science &amp; Technology Committee, creates a surveillance architecture that applies to every mobile device user in the state. App store providers would be required to verify every account holder’s age category and share that data with developers.</p>
<p>HB 2920 divides users into four age categories: children under 13, teenagers between 13 and 16, older teenagers between 16 and 18, and adults. Every person who creates an app store account in Arizona would be sorted into one of these buckets through what the bill calls “commercially available” verification methods, a phrase it declines to define with any precision. The Arizona Attorney General would be tasked with creating rules to establish acceptable verification processes.</p>
<p>For anyone under 18, the bill mandates that their account be “affiliated” with a parent account. The app store would then be required to obtain “verifiable parental consent” before allowing the minor to download an application, purchase an application, or make any in-app purchase. This consent requirement extends to preinstalled apps as well. The first time a minor launches their browser or messaging app, the system would need to check with the parent account before allowing access.</p><div data-id="eb333a5" data-element_type="container" data-e-type="container" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}" data-elementor-type="section" data-elementor-id="235992" data-elementor-post-type="elementor_library">
				<div data-id="4678215" data-element_type="widget" data-e-type="widget" data-widget_type="text-editor.default">
				<p>Reclaim Your Digital Freedom.</p>
				</div>
				<div data-id="fe169cc" data-element_type="widget" data-e-type="widget" data-widget_type="text-editor.default">
				<p>Get unfiltered coverage of surveillance, censorship, and the technology threatening your civil liberties.</p>
				</div>
				
					</div>

<p>The law does not specify how parent-child account affiliation would actually be verified. It grants app stores broad authority to determine parenthood through unspecified commercially reasonable methods, but provides no regulation for how this determination would occur.</p>
<p>The bill’s reach extends beyond initial downloads. If a developer makes what the legislation calls a “significant change” to an application, the parent account must provide renewed consent before the minor can access the updated version.</p>
<p>The definition of significant change covers modifications to privacy policies, changes to data collection categories, alterations to age ratings, the addition of in-app purchases, or the introduction of advertisements.</p>
<p>This means a weather app that adds a banner ad would require fresh parental consent. An update to a note-taking app’s privacy policy would trigger the consent mechanism. The bill creates a system where routine software updates become opportunities for access to be blocked.</p>
<p>Developers would be required to notify app stores of any significant change, and app stores would then be required to notify parent accounts and obtain renewed consent before providing access to the changed version. The burden falls on both parties, with civil penalties of up to $75,000 per violation and a private right of action allowing parents and minors to sue for $1,000 per violation plus punitive damages.</p>
<p>To make this system function, app stores would need to collect and maintain detailed records about every user’s age category, parental relationships, and consent status. This data would then be shared with developers whenever a user downloads, purchases, or launches an app.</p>
<p>The bill includes provisions requiring “industry standard encryption” and limiting data use to compliance purposes, but these safeguards exist alongside requirements for extensive data collection. Age category data, parent-child affiliations, verification records, consent histories, all of this information must be maintained and transmitted between parties for the system to work.</p>
<p>Texas passed <a href="https://reclaimthenet.org/texas-mandates-digital-id-to-access-app-store-apps">a similar law in 2025</a>. A federal judge <a href="https://reclaimthenet.org/texas-app-store-accountability-act-blocked-federal-judge-first-amendment">blocked it</a> before it could take effect, finding it likely unconstitutional. US District Judge Robert Pitman wrote that the law is “akin to a law that would require every bookstore to verify the age of every customer at the door and, for minors, require parental consent before the child or teen could enter and again when they try to purchase a book.” The court ruled the Texas law imposed content-based restrictions on speech and failed the strict scrutiny test because Texas did not prove it used the least restrictive means to achieve its goals.</p>
<p>The stated purpose of these laws is to protect children. The effect&nbsp;is creating systems that collect more sensitive data from children than currently exists. Every child’s age, every parent-child relationship, every app download, every purchase, and every consent decision would be logged and shared between companies.</p>
<p>The bill positions parents as the decision-makers, but the actual gatekeeping happens at the app store level. Apple and Google would be deputized to determine what verification methods meet the “commercially reasonable” standard. They would build the consent interfaces. They would decide how the parent-child affiliation system functions.</p>
<p>The chilling effect operates at multiple levels. Developers might avoid making any updates to their apps to prevent triggering renewed consent requirements. Small developers might exit the market entirely rather than build compliance infrastructure. Families might find that the apps they use become unavailable because the compliance costs exceed what the developer can bear.</p>
<p>And for anyone who values the ability to access information without first presenting identification, the bill represents a fundamental issue. Reading news, checking the weather, sending messages, using a calculator, all of these activities would require first verifying your age and, if under 18, obtaining parental permission.</p>
<p>Arizona joins a growing list of states pursuing app store age verification. <a href="https://reclaimthenet.org/texas-app-store-accountability-act-blocked-federal-judge-first-amendment">Texas</a>, <a href="https://reclaimthenet.org/utah-introduces-age-verification-checks-for-social-media">Utah</a>, <a href="https://reclaimthenet.org/louisiana-social-media-age-verification-law-blocked-first-amendment">Louisiana</a>, and <a href="https://reclaimthenet.org/court-keeps-californias-online-id-law-alive">California</a> have all passed versions of these laws, with varying effective dates and enforcement provisions. The Texas law faces an appeal after being enjoined; Utah and Louisiana are scheduled to take effect later this year; California’s version arrives in 2027.</p>
<p>HB 2920 goes further than most by explicitly including preinstalled applications. A child in Arizona could buy a phone and find themselves unable to use the basic software that came with it until a parent account is established and consent is obtained. The browser that came with the device, the messaging app, and the search function would all be locked behind age verification and parental consent gates.</p>
<h2>The First Amendment Question</h2>
<p>By verifying age, these bills could <a href="https://reclaimthenet.org/a-battle-revisited-the-supreme-court-could-revisit-age-verification-and-online-anonymity-bans">violate the First Amendment</a> primarily due to concerns about the right to speak anonymously online: if users must provide identifiable information about themselves, their ability to share or obtain information anonymously could be jeopardized.</p>
<p>The anonymous internet user is becoming an endangered species in this regulatory environment. Age verification at the app store level means identification at the app store level. The ability to download and use software without revealing who you are would functionally disappear.</p>
<p>Courts have consistently treated anonymous speech as protected under the First Amendment. The right to read and write without identification has a long constitutional history. These bills require the opposite: verified identity as a precondition for accessing digital tools.</p>
<p>Arizona’s HB 2920 has been read twice and awaits committee consideration. If it advances, it would take effect on November 30, 2026, creating yet another deadline for app stores and developers to comply with yet another variation of age verification requirements.</p>
<!-- CONTENT END 1 -->
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Perils of ISBN (135 pts)]]></title>
            <link>https://rygoldstein.com/posts/perils-of-isbn</link>
            <guid>47063663</guid>
            <pubDate>Wed, 18 Feb 2026 17:34:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rygoldstein.com/posts/perils-of-isbn">https://rygoldstein.com/posts/perils-of-isbn</a>, See on <a href="https://news.ycombinator.com/item?id=47063663">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
            <p>Last year I got into using Letterboxd, to complement my goal of watching more (good) movies<sup><a href="#fn1" id="fnref1" aria-describedby="footnotes-label" role="doc-noteref">[1]</a></sup>. It’s got a really clean interface, the social features are useful but unobtrusive, and it makes remembering what I’ve watched and when I watched it easy. So why isn’t there a Letterboxd for books?</p>
<p>Funnily enough, Letterboxd <a href="https://letterboxd.com/about/faq/">still describes itself</a> as “like GoodReads for movies”. But GoodReads itself is a mess. Take this screenshot of my childhood GoodReads account as an example:</p>
<p> <img src="https://rygoldstein.com/goodreads.png"> </p>
<p>Where do I log and review a book I’ve read? (The searchbar, but it takes half a dozen clicks and involves up to three different ways to log something). Where can I see the list of books I <em>have</em> read so I can recommend one to a friend? Where can I find books I plan to read? (Both under “My Books”, which by default shows them inter-mixed). Why is so much of the UI taken up with stuff like reading challenges and newsletters?<sup><a href="#fn2" id="fnref2" aria-describedby="footnotes-label" role="doc-noteref">[2]</a></sup> Storygraph, leading independent alternative to GoodReads, has similar problems<sup><a href="#fn3" id="fnref3" aria-describedby="footnotes-label" role="doc-noteref">[3]</a></sup>. These interfaces don’t lead me to log books; instead I just have some files in Obsidian that I sometimes remember to update.</p>
<h2 id="search">Search</h2>
<p>So let’s build our <em>own</em> GoodReads, with a UI that’s convenient enough to actually use. First we gotta build a search function for books. Then–</p>
<p>Wait, a search function for books. How do we do that? Well there’s the <a href="https://developers.google.com/books/docs/v1/getting_started">Google Books API</a>, and it’s free which is nice. But when I search for “The Last Unicorn” (and do a little munging of the contents with <code>jq</code>):</p>
<pre><code>$ <span>curl</span> <span>-X</span> GET <span>'https://www.googleapis.com/books/v1/volumes?q=The+Last+Unicorn'</span> <span>|</span> jq <span>".items | .[] | .volumeInfo | {title: .title, authors: .authors, isbns: .industryIdentifiers | map(.identifier)  }"</span></code></pre>
<p>I get this mess:</p>
<pre><code><span>{</span>
  <span>"title"</span><span>:</span> <span>"The Last Unicorn"</span><span>,</span>
  <span>"authors"</span><span>:</span> <span>[</span>
    <span>"Peter S. Beagle"</span>
  <span>]</span><span>,</span>
  <span>"isbns"</span><span>:</span> <span>[</span>
    <span>"9780451450524"</span><span>,</span>
    <span>"0451450523"</span>
  <span>]</span>
<span>}</span>
<span>{</span>
  <span>"title"</span><span>:</span> <span>"The Last Unicorn"</span><span>,</span>
  <span>"authors"</span><span>:</span> <span>[</span>
    <span>"Peter S. Beagle"</span>
  <span>]</span><span>,</span>
  <span>"isbns"</span><span>:</span> <span>[</span>
    <span>"1417644931"</span><span>,</span>
    <span>"9781417644933"</span>
  <span>]</span>
<span>}</span>
<span>{</span>
  <span>"title"</span><span>:</span> <span>"The Last Unicorn"</span><span>,</span>
  <span>"authors"</span><span>:</span> <span>[</span>
    <span>"Peter S. Beagle"</span>
  <span>]</span><span>,</span>
  <span>"isbns"</span><span>:</span> <span>[</span>
    <span>"9780593547342"</span><span>,</span>
    <span>"0593547349"</span>
  <span>]</span>
<span>}</span>
<span>{</span>
  <span>"title"</span><span>:</span> <span>"The Last Unicorn"</span><span>,</span>
  <span>"authors"</span><span>:</span> <span>[</span>
    <span>"Peter S. Beagle"</span>
  <span>]</span><span>,</span>
  <span>"isbns"</span><span>:</span> <span>[</span>
    <span>"0345028929"</span><span>,</span>
    <span>"9780345028921"</span>
  <span>]</span>
<span>}</span>
<span>{</span>
  <span>"title"</span><span>:</span> <span>"The Last Unicorn"</span><span>,</span>
  <span>"authors"</span><span>:</span> <span>[</span>
    <span>"Jane Elizabeth Cammack"</span>
  <span>]</span><span>,</span>
  <span>"isbns"</span><span>:</span> <span>[</span>
    <span>"8853010932"</span><span>,</span>
    <span>"9788853010933"</span>
  <span>]</span>
<span>}</span>
<span>{</span>
  <span>"title"</span><span>:</span> <span>"The Last Unicorn"</span><span>,</span>
  <span>"authors"</span><span>:</span> <span>[</span>
    <span>"Peter S. Beagle"</span>
  <span>]</span><span>,</span>
  <span>"isbns"</span><span>:</span> <span>[</span>
    <span>"1596060832"</span><span>,</span>
    <span>"9781596060838"</span>
  <span>]</span>
<span>}</span>
<span>{</span>
  <span>"title"</span><span>:</span> <span>"Last Unicorn"</span><span>,</span>
  <span>"authors"</span><span>:</span> <span>[</span>
    <span>"Peter S. Beagle"</span>
  <span>]</span><span>,</span>
  <span>"isbns"</span><span>:</span> <span>[</span>
    <span>"1399606972"</span><span>,</span>
    <span>"9781399606974"</span>
  <span>]</span>
<span>}</span>
<span>{</span>
  <span>"title"</span><span>:</span> <span>"Peter S. Beagle's “The Last Unicorn”"</span><span>,</span>
  <span>"authors"</span><span>:</span> <span>[</span>
    <span>"Timothy S. Miller"</span>
  <span>]</span><span>,</span>
  <span>"isbns"</span><span>:</span> <span>[</span>
    <span>"9783031534256"</span><span>,</span>
    <span>"3031534255"</span>
  <span>]</span>
<span>}</span>
<span>{</span>
  <span>"title"</span><span>:</span> <span>"The Last Unicorn the Lost Journey"</span><span>,</span>
  <span>"authors"</span><span>:</span> <span>[</span>
    <span>"Peter S. Beagle"</span>
  <span>]</span><span>,</span>
  <span>"isbns"</span><span>:</span> <span>[</span>
    <span>"1616963085"</span><span>,</span>
    <span>"9781616963088"</span>
  <span>]</span>
<span>}</span>
<span>{</span>
  <span>"title"</span><span>:</span> <span>"The Last Unicorn"</span><span>,</span>
  <span>"authors"</span><span>:</span> <span>[</span>
    <span>"Peter S. Beagle"</span>
  <span>]</span><span>,</span>
  <span>"isbns"</span><span>:</span> <span>[</span>
    <span>"1616963182"</span><span>,</span>
    <span>"9781616963187"</span>
  <span>]</span>
<span>}</span></code></pre>
<p>Uh-oh. Why do we have so many distinct versions of <em>The Last Unicorn</em>? Well, each distinct format of a work has its own ISBN (so a hardcover, paperback, and eBook all have different ISBNs), even though the text may be identical. Then different editions (for example, a new foreword for a classic novel) all have their <em>own</em> set of unique ISBNs. Any given book may have dozens of ISBNs, each with their own unique entry in this API. That’s not going to work well for a search function: I just want to record that I read a book, not meticulously select which version of a book I read.</p>
<h2 id="works-not-isbns">Works, not ISBNs</h2>
<p>I was complaining about my situation to my partner; xe informed me that librarians think of this through the <a href="https://www.oclc.org/research/activities/frbr.html#background">FRBR model</a>. In short, there’s a distinction between the work (the book <em>The Last Unicorn</em>), the <em>expression</em> (a given edition of the book), a <em>manifestation</em> (a given physical format for an expression, such as paperback or hardcover), and an <em>item</em> (an individual object in a collection)<sup><a href="#fn4" id="fnref4" aria-describedby="footnotes-label" role="doc-noteref">[4]</a></sup>.</p>
<p>I’m firmly working in the realm of the abstract, so <em>items</em> are irrelevant to me. Google Books’s API is giving back different <em>expressions</em> or <em>manifestations</em> (I’m not entirely clear on which), but we want <em>works</em>. How do we get our hands on those? There are some other book database options, most notably <a href="https://openlibrary.org/">OpenLibrary</a>, which have a model closer to what we want. Here’s the OpenLibrary work page for <a href="https://openlibrary.org/works/OL26459W/The_Last_Unicorn?edition=key%3A/books/OL379621M">The Last Unicorn</a>, for example. But the data’s still a little messy. Peek at <a href="https://openlibrary.org/search?q=hotel+iris">the search results for <em>Hotel Iris</em> by Yoko Ogawa</a>; the same work is duplicated four times. I’m still exploring ways to get data as clean as GoodReads or StoryGraph, but it turns out that there’s not a high-quality open-source database of books.</p>
<p>Letterboxd benefits from <a href="https://www.themoviedb.org/">The Movie Database</a>, which serves as its canonical source for films and film metadata. I would almost venture to describe Letterboxd as a commercialization of the commons, though the slick UI and social features are undeniably added value. If you want to build a similar book-focused project, it turns out that no analogue really exists. It could be a chicken-and-egg problem (I’m sure having a large, commercial service attached drives contributions to TMDB), but there’s also a difference of scale: there are around <a href="https://www.themoviedb.org/faq/general">a million movies</a> in the database today. Having played around with the data, I can say that OpenLibrary current has more than 40 million works in its (incomplete) catalogue. The problem is at least an order of magnitude harder and has much less money behind it.</p>
<p>Doesn’t mean I won’t try, though; look out for a potential future blogpost!</p>
<hr>
<section>
<ol>
<li id="fn1"><p>For a long time I thought I didn’t like movies; it turns out I actually just wasn’t watching ones to my taste! If you also think you don’t like movies, maybe ask a cinephile friend for some stuff to watch to try and triangulate your taste. <a href="#fnref1" aria-label="Back to reference 1" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn2"><p>I think the answer to my implicit question of “why is GoodReads clunky and unappealing?” is “it’s a low-priority offshoot of Amazon’s book-selling business.” <a href="#fnref2" aria-label="Back to reference 2" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn3"><p>As well as some desperately-unappealing selling points. I’m sure that there’s a target audience for computer-generated recommendations, AI-powered reading analytics, and user polls with questions like “is this book plot or character driven?” or “loveable characters?”. That audience isn’t me! <a href="#fnref3" aria-label="Back to reference 3" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn4"><p>Any errors in understanding are my own and not my partner’s. <a href="#fnref4" aria-label="Back to reference 4" role="doc-backlink">↩︎</a></p>
</li>
</ol>
</section>

        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tailscale Peer Relays is now generally available (412 pts)]]></title>
            <link>https://tailscale.com/blog/peer-relays-ga</link>
            <guid>47063005</guid>
            <pubDate>Wed, 18 Feb 2026 16:46:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tailscale.com/blog/peer-relays-ga">https://tailscale.com/blog/peer-relays-ga</a>, See on <a href="https://news.ycombinator.com/item?id=47063005">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>When Tailscale works best, it feels effortless, almost boring. Devices connect directly, packets take the shortest possible path, and performance ceases to be a pressing concern.</p><p>But real-world networks aren’t always that cooperative. Firewalls, NATs, and cloud networking constraints can block direct peer-to-peer connections. When that happens, Tailscale relies on relays (<a target="" rel="noreferrer" href="https://tailscale.com/docs/reference/connection-types#derp-connections/?utm_source=blog&amp;utm_medium=content&amp;utm_campaign=tailscale-winter-update-2026">DERP</a>) to keep traffic moving securely and reliably.</p><p>Today, we’re excited to announce that <a target="" rel="noreferrer" href="https://tailscale.com/docs/features/peer-relay/?utm_source=blog&amp;utm_medium=content&amp;utm_campaign=tailscale-winter-update-2026">Tailscale Peer Relays</a> is now generally available (GA). Peer relays bring customer-deployed, high-throughput relaying to production readiness, giving you a tailnet-native relaying option that you can run on any Tailscale node. Since their <a target="" rel="noreferrer" href="https://tailscale.com/blog/peer-relays-beta/?utm_source=blog&amp;utm_medium=content&amp;utm_campaign=tailscale-winter-update-2026">beta release</a>, we’ve shaped Tailscale Peer Relays to deliver major improvements in performance, reliability, and visibility.</p><p>What started as a way to work around hard NATs has grown into a production-grade connectivity option. One that gives teams the performance, control, and flexibility they need to scale Tailscale in even the most challenging network environments.</p><figure id=""><img _type="asset" video="[object Object]" alt="A flowchart: A Your Network container, with &quot;Your Resource&quot; heading into &quot;Peer relay.&quot; From there, traffic hits a &quot;Peer relay ip port exception at the edge of &quot;Network Firewall,&quot; and then hits three checkpoints, heading into Resources." loading="lazy" width="1055" height="630" decoding="async" data-nimg="1" srcset="https://cdn.sanity.io/images/w77i7m8x/production/54ca4bc979f85ed7d8a2fa2fb1d78b431083dafc-1055x630.svg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1x, https://cdn.sanity.io/images/w77i7m8x/production/54ca4bc979f85ed7d8a2fa2fb1d78b431083dafc-1055x630.svg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 2x" src="https://cdn.sanity.io/images/w77i7m8x/production/54ca4bc979f85ed7d8a2fa2fb1d78b431083dafc-1055x630.svg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format"></figure><h2 id="vertical-scaling-boost-that-improves-throughput"><a href="#vertical-scaling-boost-that-improves-throughput">Vertical scaling boost that improves throughput<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2><p>We have made big throughput improvements for Tailscale Peer Relays that are especially noticeable when many clients are forwarding through them. Connecting clients now select a more optimal interface and address family when more than one are available within a single relay, which helps bootstrap and improve overall connection quality. On the relay itself, throughput has increased: packets are handled more efficiently on every Peer Relay because of lock contention improvements, and traffic is now spread across multiple UDP sockets where available.</p><p>Together, these changes deliver meaningful gains in both performance and reliability across day-to-day tailnet traffic. Even when direct peer-to-peer connections aren’t possible, peer relays can now achieve performance much closer to a true mesh.</p><h2 id="static-endpoints-for-restrictive-cloud-environments"><a href="#static-endpoints-for-restrictive-cloud-environments">Static endpoints for restrictive cloud environments<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2><p>In some environments, particularly in public cloud networks, automatic endpoint discovery isn’t always possible. Instances may sit behind strict firewall rules, rely on port forwarding or load balancers in peered public subnets, or operate in setups where opening arbitrary ports simply isn’t an option. In many cases, the infrastructure in front of those instances can’t run Tailscale directly, making standard discovery mechanisms ineffective.</p><p>Peer relays now integrate with static endpoints to address these constraints. Using the <code>--relay-server-static-endpoints</code> flag with <code>tailscale set</code>, a peer relay can advertise one or more fixed <code>IP:port</code> pairs to the tailnet. These endpoints can live behind infrastructure such as an AWS Network Load Balancer, enabling external clients to relay traffic through the peer relay even when automatic endpoint discovery fails.</p><figure id=""><img _type="asset" video="[object Object]" alt="Flowchart: A Private Subnet container, with &quot;AWS Resource&quot; inside. A two-way flow from AWS to a Peer Relay has a checkmark in the middle. Traffic is also coming to that Peer Relay, inside a Public Subnet box, from a Static Endpoint inside that Public Subnet, which is sending traffic back to a Network load balancer, and then to a Laptop outside the subnet. Another branch from AWS heads through the Public subnet, to &quot;other Internet-bound traffic,&quot; into a NAT gateway, and then to Internet." loading="lazy" width="1055" height="630" decoding="async" data-nimg="1" srcset="https://cdn.sanity.io/images/w77i7m8x/production/25965fd10d29e2384de6d8b1db1bd39cee38dd3d-1055x630.svg?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1x, https://cdn.sanity.io/images/w77i7m8x/production/25965fd10d29e2384de6d8b1db1bd39cee38dd3d-1055x630.svg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 2x" src="https://cdn.sanity.io/images/w77i7m8x/production/25965fd10d29e2384de6d8b1db1bd39cee38dd3d-1055x630.svg?w=3840&amp;q=75&amp;fit=clip&amp;auto=format"></figure><p>This unlocks high-throughput connectivity in restrictive cloud environments where traditional NAT traversal and endpoint discovery don’t work. Customers can now deploy peer relays behind load balancers and still provide reliable, high-performance relay paths to clients outside those networks.</p><p>For many customers, this also means peer relays can replace subnet routers, unlocking full-mesh deployments with core Tailscale features like <a target="" rel="noreferrer" href="https://tailscale.com/docs/features/tailscale-ssh/?utm_source=blog&amp;utm_medium=content&amp;utm_campaign=tailscale-winter-update-2026">Tailscale SSH</a> and <a target="" rel="noreferrer" href="https://tailscale.com/docs/features/magicdns/?utm_source=blog&amp;utm_medium=content&amp;utm_campaign=tailscale-winter-update-2026">MagicDNS</a>.</p><h2 id="improved-auditability-and-visibility"><a href="#improved-auditability-and-visibility">Improved auditability and visibility<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2><p>Now in general availability, Tailscale Peer Relays also integrate more deeply into Tailscale’s visibility and observability tooling, making relay behavior clear, measurable, and auditable.</p><p>Peer relays integrate directly with <a target="" rel="noreferrer" href="https://tailscale.com/docs/reference/tailscale-cli#ping/?utm_source=blog&amp;utm_medium=content&amp;utm_campaign=tailscale-winter-update-2026">tailscale ping</a>, allowing you to see whether a relay is being used, whether it’s reachable, and how it impacts latency and reliability when testing connectivity. This removes much of the guesswork from troubleshooting. When issues arise, it’s easy to determine whether traffic is being relayed, whether the relay is healthy, and whether it’s contributing to degraded performance.</p><p>For ongoing observability, Tailscale Peer Relays now expose client metrics such as <code>tailscaled_peer_relay_forwarded_packets_total</code> and <code>tailscaled_peer_relay_forwarded_bytes_total</code>. These metrics can be scraped and exported to monitoring systems like Prometheus and Grafana alongside existing Tailscale client metrics, enabling teams to track relay usage, understand traffic patterns, detect anomalies, and monitor tailnet health at scale.</p><figure id=""><div><p><iframe width="100%" height="100%" src="https://www.youtube.com/embed/wkBSjT1hO6k?si=EvG2q2tqkI7QBEZ9" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p></div></figure><h2 id="whats-next"><a href="#whats-next">What’s next<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2><p>With general availability, Tailscale Peer Relays become a core building block for scaling Tailscale in real-world networks. They enable:</p><ul><li>High-throughput, low-latency connections when direct paths are unavailable</li><li>Deployments in restricted cloud environments through static endpoints</li><li>Full mesh in private subnets, with controlled ingress/egress paths</li></ul><p>At the same time, Tailscale Peer Relays deliver intelligent, resilient path selection across the tailnet, along with first-class observability, auditability, and debuggability. All of this comes without compromising on Tailscale’s foundational guarantees: end-to-end encryption, least-privilege access, and simple, predictable operation.</p><p>Getting started is straightforward. Tailscale Peer Relays can be enabled on any supported Tailscale node using the CLI, controlled through grants in your ACLs, and deployed incrementally alongside existing relay infrastructure; you can read more <a target="" rel="noreferrer" href="https://tailscale.com/docs/features/peer-relay/?utm_source=blog&amp;utm_medium=content&amp;utm_campaign=tailscale-winter-update-2026">in our docs</a>. </p><p>Peer Relays are available on all Tailscale plans, including our free Personal plan. If you need deployment support or have specific throughput goals, don't hesitate to <a target="_blank" rel="noreferrer" href="https://tailscale.com/contact/sales/?utm_source=blog&amp;utm_medium=content&amp;utm_campaign=tailscale-winter-update-2026">reach out</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zero-day CSS: CVE-2026-2441 exists in the wild (342 pts)]]></title>
            <link>https://chromereleases.googleblog.com/2026/02/stable-channel-update-for-desktop_13.html</link>
            <guid>47062748</guid>
            <pubDate>Wed, 18 Feb 2026 16:28:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chromereleases.googleblog.com/2026/02/stable-channel-update-for-desktop_13.html">https://chromereleases.googleblog.com/2026/02/stable-channel-update-for-desktop_13.html</a>, See on <a href="https://news.ycombinator.com/item?id=47062748">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><span itemprop="datePublished">
Friday, February 13, 2026
</span>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pocketbase lost its funding from FLOSS fund (120 pts)]]></title>
            <link>https://github.com/pocketbase/pocketbase/discussions/7287</link>
            <guid>47062561</guid>
            <pubDate>Wed, 18 Feb 2026 16:11:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/pocketbase/pocketbase/discussions/7287">https://github.com/pocketbase/pocketbase/discussions/7287</a>, See on <a href="https://news.ycombinator.com/item?id=47062561">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="presentation" data-paste-markdown-skip="">
    <tbody data-target-translation-id="9080886" data-target-translation-type="discussion">
        <tr>
    <td>
        <h3 dir="auto">Unfortunate update:</h3>
<p dir="auto">FLOSS/fund reached out to me again but due to some unforeseen regulatory constraints their partnership with GitHub didn't seem to work out. Instead they want to issue a wire transfer from India requiring several cross-jurisdictional paperwork but I don't feel comfortable doing that because I don't trust them, nor the India government, with processing and storing personal sensitive data (especially over insecure shared mail inbox channel).</p>
<p dir="auto"><strong>So in the end I had to withdraw my FLOSS/fund application and decline the funding.</strong></p>
<p dir="auto">I've made a mistake for not researching it more carefully and not waiting for the disbursal before making big announcements and decisions, but it is what it is.</p>
<p dir="auto">Nonetheless the main goal remains and I'll try to publish a stable PocketBase version this year <em>(no hard promises though; the majority of the new UI functionality is already implemented but I'll have to look for something else in between before making more elaborate plans regarding the remaining tasks)</em>.</p>
<p dir="auto">There will be a new announcement once I have more clarity and readiness to request community feedback.</p>
<p dir="auto"><em>I'll go ahead and lock this discussion to avoid further spamming the participants and repository watchers.</em></p>
<hr>
<p dir="auto"><del>I am happy to announce that yesterday <a href="https://floss.fund/" rel="nofollow">FLOSS/fund</a> contacted me about their decision to sponsor PocketBase as part of their second funding tranche. You can find more details and the other cool projects they've chosen to support at <a href="https://floss.fund/blog/second-tranche-2025-anniversary/" rel="nofollow">https://floss.fund/blog/second-tranche-2025-anniversary/</a>.</del></p>
<p dir="auto"><del>&gt; <em>Note that <a href="https://floss.fund/" rel="nofollow">FLOSS/fund</a> and <a href="https://zerodha.com/open-source" rel="nofollow">Zerodha</a> are planning to continue with the program next year, so if you are looking for a <strong>no strings attached</strong> sponsorship for your open source project, I encourage you to apply at <a href="https://dir.floss.fund/submit" rel="nofollow">https://dir.floss.fund/submit</a>.</em></del></p>
<p dir="auto"><del>With the funding this means that starting from December I'll be able to work solely on PocketBase for at least one year, with the general goal to finally have a stable PocketBase release by end of next year. My short-term focus for now will be on rewriting the UI.</del></p>
<h3 dir="auto">UI Rewrite</h3>
<p dir="auto">One of the biggest limitations of PocketBase right now is the lack of dashboard customization.</p>
<p dir="auto">While you can create server-side plugins using the existing app event hooks <em>(ex. the <a href="https://github.com/pocketbase/pocketbase/tree/master/plugins/jsvm">JS <code>pb_hooks</code> are implemented as such</a>; see also a recent community project posted in <a data-error-text="Failed to load title" data-id="9067668" data-permission-text="Title is private" data-url="https://github.com/pocketbase/pocketbase/discussions/7273" data-hovercard-type="discussion" data-hovercard-url="/pocketbase/pocketbase/discussions/7273/hovercard" href="https://github.com/pocketbase/pocketbase/discussions/7273">#7273</a>)</em>, there is no mechanism at the moment to extend the UI for things like: adding custom options to existing fields, registering new fields, customizing the settings and what is shown when in "production" mode, registering new OAuth2 providers, registering new system sections <em>(see <a data-error-text="Failed to load title" data-id="8997294" data-permission-text="Title is private" data-url="https://github.com/pocketbase/pocketbase/discussions/7232" data-hovercard-type="discussion" data-hovercard-url="/pocketbase/pocketbase/discussions/7232/hovercard" href="https://github.com/pocketbase/pocketbase/discussions/7232">#7232</a>)</em>, etc.</p>
<p dir="auto">Usually this shouldn't be a difficult task considering that the dashboard runs entirely in the browser and loading HTML/JS/CSS dynamically is what browsers are good at, but the goal is NOT to just inject some extra assets on the page because this wouldn't really work for many of the highlighted issues above, or at least not for what I plan to use the UI plugins for.</p>
<p dir="auto">Ideally we should be able to interact with the SPA router and the various reactivity states of the dashboard from an external script. And this unfortunately is very difficult to be implemented when using a compiler framework like Svelte because we'll have to force on everyone an additional UI Node.js build step when extending PocketBase and I want to avoid that.</p>
<p dir="auto">I've explored other more runtime/dynamically-friendly frameworks like Vue, Preact, Lit, Solid, Alpine, Mithril, and technically any of them will do the job <em>(Vue or Lit will be my preference)</em> but they have their own DSL that often doesn't play nicely with other frameworks and most importantly I don't feel confident that I'll be able to maintain them on my own in case they decide to do a major refactoring, change the direction or abandon their project.</p>
<p dir="auto"><strong>My main goal with PocketBase is that once we hit a stable release, I'll mark the project as "complete" and new changes or features will be added very rarely and most likely will follow the bi-annual Go release cycle, aka. the less dependencies we have the better.</strong></p>
<p dir="auto">So with that in mind, the last couple weekends I've been experimenting with a new frontend framework designed around with the minimal functionality required for PocketBase.</p>
<p dir="auto">It is called Shablon (<a href="https://github.com/ganigeorgiev/shablon">https://github.com/ganigeorgiev/shablon</a>), it has 0 dependencies and it is written in plain JavaScript. I'll try to keep it as simple as possible and without the concept for "component" so everything ideally will be plain DOM elements sprinkled with optional reactivity <em>(a "component" could be just a function that return a DOM element, no matter whether it was created using the <code>t.[tag](attrs, ...children)</code> helper or <code>document.createElement(tag)</code>)</em>.</p>
<p dir="auto">It is certainly not as ergonomic and pretty as something like Svelte, but I believe it would be better suited for our purpose.<br>
In the worst case if it turn out too ugly and inefficient, I'll just go back to Vue or Lit but for now I'm planning to continue experimenting with it.</p>
<p dir="auto">I've also already started working on a new PocketBase UI kit for reusable elements like fields, buttons, popups, dropdowns, alerts, etc.</p>
<hr>
<p dir="auto">So that's it for now. Most likely sometime next week (or the week following it) there will be another announcement for an eventual minimal breaking change with the experimental relation filter resolution as subqueries but other than that, the project will enter a temporary "feature freeze" until I have something more concrete to show related to the UI (I want to take my time with it and don't want to rush it, so please be patient).</p>
<p dir="auto">Thanks once again to <a href="https://floss.fund/" rel="nofollow">FLOSS/fund</a> and <a data-hovercard-type="organization" data-hovercard-url="/orgs/zerodha/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/zerodha">@zerodha</a> for supporting the project!</p>
    </td>
  </tr>

    </tbody>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Future of AI Software Development (187 pts)]]></title>
            <link>https://martinfowler.com/fragments/2026-02-18.html</link>
            <guid>47062534</guid>
            <pubDate>Wed, 18 Feb 2026 16:08:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://martinfowler.com/fragments/2026-02-18.html">https://martinfowler.com/fragments/2026-02-18.html</a>, See on <a href="https://news.ycombinator.com/item?id=47062534">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>I’ll start with some more tidbits from the <a href="https://martinfowler.com/bliki/FutureOfSoftwareDevelopment.html">Thoughtworks Future of Software Development Retreat</a></p>

<p>&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄</p>

<p>We were tired after the event, but our marketing folks forced Rachel Laycock and I to do a quick video. We’re often asked if this event was about creating some kind of new manifesto for AI-enabled development, akin to the Agile Manifesto (which is now 25 years old). In short, our answer is “no”, but for the full answer, <a href="https://www.youtube.com/watch?v=VHkuVlwYhNk">watch our video</a></p>

<p>&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄</p>

<p>My colleagues put together a <a href="https://www.thoughtworks.com/content/dam/thoughtworks/documents/report/tw_future%20_of_software_development_retreat_%20key_takeaways.pdf">detailed summary of thoughts</a> from the event, in a 17 page PDF. It breaks the discussion down into eight major themes, including “Where does the rigor go?”, “The middle loop: a new category of work”, “Technical foundations: languages, semantics and
operating systems”, and “The human side: roles, skills and experience”.</p>

<blockquote>
  <p>The retreat surfaced a consistent pattern: the practices, tools and organizational structures built for human-only software development are breaking in predictable ways under the weight of AI-assisted work. The replacements are forming, but they are not yet mature.</p>

  <p>The ideas ready for broader industry conversation include the supervisory engineering middle loop, risk tiering as the new core engineering discipline, TDD as the strongest form of prompt engineering and the agent experience reframe for developer experience investment.</p>
</blockquote>

<p>&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄</p>

<p>Annie Vella <a href="https://annievella.com/posts/finding-comfort-in-the-uncertainty/">posted her take-aways</a> from the event</p>

<blockquote>
  <p>I walked into that room expecting to learn from people who were further ahead. People who’d cracked the code on how to adopt AI at scale, how to restructure teams around it, how to make it work. Some of the sharpest minds in the software industry were sitting around those tables.</p>

  <p>And nobody has it all figured out.</p>

  <p>There is more uncertainty than certainty. About how to use AI well, what it’s really doing to productivity, how roles are shifting, what the impact will be, how things will evolve. Everyone is working it out as they go.</p>

  <p>I actually found that to be quite comforting, in many ways. Yes, we walked away with more questions than answers, but at least we now have a shared understanding of the sorts of questions we should be asking. That might be the most valuable outcome of all.</p>
</blockquote>

<p>&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄</p>

<p>Rachel Laycock was <a href="https://thenewstack.io/ai-velocity-debt-accelerator/">interviewed in The New Stack</a> (by Jennifer Riggins) about her recollections from the retreat.</p>

<blockquote>
  <p>AI may be dubbed the great disruptor, but it’s really just an accelerator of whatever you already have. The 2025 DORA report places AI’s primary role in software development as that of an amplifier — a funhouse mirror that reflects back the good, bad, and ugly of your whole pipeline. AI is proven to be impactful on the individual developer’s work and on the speed of writing code. But, since writing code was never the bottleneck, if traditional software delivery best practices aren’t already in place, this velocity multiplier becomes a debt accelerator.</p>
</blockquote>

<p>&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄&nbsp;</p>

<p>LLMs are eating specialty skills. There will be less use of specialist front-end and back-end developers as the LLM-driving skills become more important than the details of platform usage. Will this lead to a greater recognition of the role of <a href="https://martinfowler.com/articles/expert-generalist.html">Expert Generalists</a>? Or will the ability of LLMs to write lots of code mean they code around the silos rather than eliminating them? Will LLMs be able to ingest the code from many silos to understand how work crosses the boundaries?</p>

<p>&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄</p>

<p>Will LLMs be cheaper than humans once the subsidies for tokens go away? At this point we have little visibility to what the true cost of tokens is now, let alone what it will be in a few years time. It could be so cheap that we don’t care how many tokens we send to LLMs, or it could be high enough that we have to be very careful.</p>

<p>&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄</p>

<p>Will the rise of specifications bring us back to <a href="https://martinfowler.com/bliki/WaterfallProcess.html">waterfall-style development</a>? The natural impulse of many business folks is “don’t bother me until it’s finished”. Does the process of evolutionary design get helped or hindered by LLMs?</p>

<p>My instinctive reaction is that all depends on our workflow. I don’t think LLMs change the value of rapidly building and releasing small slices of capability. The promise of LLMs is to increase the frequency of that cycle, and doing more in each release.</p>

<p>&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄</p>

<p>Sadly the session on security had a small turnout.</p>

<p>One large enterprise employee commented that they were deliberately slow with AI tech, keeping about a quarter behind the leading edge. “We’re not in the business of avoiding all risks, but we do need to manage them”.</p>

<p>Security is tedious, people naturally want to first make things work, then make them reliable, and only then make them secure. Platforms play an important role here, make it easy to deploy AI with good security. Are the AI vendors being irresponsible by not taking this seriously enough? I think of how other engineering disciplines bake a significant safety factor into their designs. Are we doing that, and if not will our failure lead to more damage than a falling bridge?</p>

<p>There was a general feeling that platform thinking is essential here. Platform teams need to create a fast but safe path - “bullet trains” for those using AI in applications building.</p>

<p>&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄</p>

<p>One of my favorite things about the event was some meta-stuff. While many of the participants were very familiar with the <a href="https://www.google.com/search?ie=UTF-8&amp;oe=UTF-8&amp;q=open+space&amp;btnG=+&amp;domains=martinfowler.com&amp;sitesearch=&amp;sitesearch=martinfowler.com">Open Space</a> format, it was the first time for a few. It’s always fun to see how people quickly realize how this style of (un)conference leads to wide-ranging yet deep discussions. I hope we made a few more open space fans.</p>

<p>One participant commented how they really appreciated how the sessions had so much deep and respectful dialog. There wasn’t the interruptions and a few people gobbling up airtime that they’d seen around so much of the tech world. Another attendee, commented “it was great that while I was here I didn’t have to feel I was a woman, I could just be one of the participants”. One of the lovely things about Thoughtworks is that I’ve got used to that sense of camaraderie, and it can be  a sad shock when I go outside the bubble.</p>

<p>&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄</p>

<p>I’ve learned much over the years from Stephen O’Grady’s analysis of the software industry. He’s written about how much of the profession feels <a href="https://redmonk.com/sogrady/2026/02/10/besieged/">besieged</a> by AI.</p>

<blockquote>
  <p>these tools are, or can be, powerful accelerants and enablers for people that dramatically lower the barriers to software development. They have the ability to democratize access to skills that used to be very difficult, or even possible for some, to acquire. Even a legend of the industry like Grady Booch, who has been appropriately dismissive of AGI claims and is actively disdainful of AI slop posted recently that he was “gobsmacked” by Claude’s abilities. Booch’s advice to developers alarmed by AI on Oxide’s podcast last week? “Be calm” and “take a deep breath.” From his perspective, having watched and shaped the evolution of the technology first hand over a period of decades, AI is just another step in the industry’s long history of abstractions, and one that will open new doors for the industry.</p>

  <p>…whether one wants those doors opened or not ultimately is irrelevant. AI isn’t going away any more than the automated loom, steam engines or nuclear reactors did. For better or for worse, the technology is here for good. What’s left to decide is how we best maximize its benefits while mitigating its costs.</p>
</blockquote>

<p>&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄</p>

<p>Adam Tornhill shares some more of his company’s research on code health and its impact on agentic development.</p>

<blockquote>
  <p>The study Code for Machines, Not Just Humans defines “AI-friendliness” as the probability that AI-generated refactorings preserve behavior and improve maintainability. It’s a large-scale study of 5,000 real programs using six different LLMs to refactor code while keeping all tests passing.</p>
</blockquote>

<p>They found that LLMs performed consistently better in healthy code bases. The risk of defects was 30% higher in less-healthy code. And a limitation of the study was that the less-healthy code wasn’t anywhere near as bad as much legacy code is.</p>

<blockquote>
  <p>What would the AI error rate be on such code? Based on patterns observed across all Code Health research, the relationship is almost certainly non-linear.</p>
</blockquote>

<p>&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;❄</p>

<p>In a conversation with one heavy user of LLM coding agents:</p>

<blockquote>
  <p>Thank you for all your advocacy of TDD (<a href="https://martinfowler.com/bliki/TestDrivenDevelopment.html">Test-Driven Development</a>). TDD has been essential for us to use LLMs effectively</p>
</blockquote>

<p>I worry about confirmation bias here, but I am hearing from folks on the leading edge of LLM usage about the value of clear tests, and the TDD cycle. It certainly strikes me as a key tool in driving LLMs effectively.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The only moat left is money? (260 pts)]]></title>
            <link>https://elliotbonneville.com/the-only-moat-left-is-money/</link>
            <guid>47062521</guid>
            <pubDate>Wed, 18 Feb 2026 16:07:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://elliotbonneville.com/the-only-moat-left-is-money/">https://elliotbonneville.com/the-only-moat-left-is-money/</a>, See on <a href="https://news.ycombinator.com/item?id=47062521">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <article>
      <header>
        
        <time>February 18, 2026</time>
      </header>
      <p>Every morning a few thousand people wake up and ship something. A tool, a SaaS, a newsletter, an app that does the thing the other app does but slightly differently. They post it on Hacker News. Nobody clicks.</p>
<p>This is not new. What's new is the scale. An AI can wake up (or whatever it does at 3am) and ship twelve of these before breakfast.</p>
<p>The value of human thinking is going down. You probably knew this. The corollary is rarely mentioned: the value of a human eyeball is going up, because there are only so many of them and there are now infinite things that want to be looked at.</p>
<p>Creation used to be the scarce thing, the filter. Now attention is. Most of us are on the wrong side of that trade.</p>
<p>Josh Pigford has been building things on the internet for 25 years. This is the first time he's said it feels hard:</p>
<blockquote><p lang="en" dir="ltr">as someone who's been building for the internet for 25+ years, this is the first time that i've ever felt like it's very difficult to make money building *new* things. existing products w/ momentum are getting a nice boost. but *new*? substantial uphill battle.</p>— Josh Pigford (@Shpigford) <a href="https://twitter.com/Shpigford/status/2024102168685412794">February 18, 2026</a></blockquote>

<p>When someone suggested the answer was marketing:</p>
<blockquote><p lang="en" dir="ltr">jUsT dO mOrE mArKeTiNg!!!!! attention spans are shorter than ever and the exponential flood of new products means there's exponential demand for that already short attention span. likely solvable, but just a VERY different playbook from the past two decades.</p>— Josh Pigford (@Shpigford) <a href="https://twitter.com/Shpigford/status/2024126204089880887">February 18, 2026</a></blockquote>

<p>He's right. "Just do more marketing" assumes there's a channel open. Every channel I know of has gotten quietly worse. Search. Social. Newsletters. Communities. There's a thread on Hacker News right now called <a href="https://news.ycombinator.com/item?id=47045804">"Is Show HN dead? No, but it's drowning"</a> — Show HN, the one place the internet was supposed to notice if you built something real.</p>
<p>One commenter:</p>
<blockquote>
<p>One of the great benefits of AI tools, is they allow anyone to build stuff... even if they have no ideas or knowledge.</p>
<p>One of the great drawbacks of AI tools, is they allow anyone to build stuff... even if they have no ideas or knowledge.</p>
</blockquote>
<p>Another: "The vibecoder hasn't done the interesting thing, they've pulled other people's interesting things."</p>
<p>The effort is gone. Effort was the filter.</p>
<p>I launched something last week. 14 people signed up — no ads, just a couple of posts. 14 real people who didn't have to. That number is tiny and it felt like something. Then I sat down to think about what it would take to grow it and I couldn't look at that math for very long.</p>
<p>The people winning mostly had a head start. Or they have money. Usually both.</p>
<p>When creation was hard, skill was the differentiator: you had to actually be good to make something worth showing. Now the barrier is near zero, so you need reach. Reach costs money or it costs years. Probably both.</p>
<p>Reach is also gravitational. Past some threshold it accumulates without you — posts find people, people find posts, the thing feeds itself. Below the threshold, identical effort produces nothing. Same quality, same idea, same work. Zero. Not because it was bad. Because you showed up on the wrong side of the line.</p>
<p>I don't know if we've already crossed a singularity on this, a point past which new entrants without existing reach or capital to buy it are effectively locked out. I think there's a real chance we have. The uncomfortable version: if you're not already moving, you might never take off.</p>
<p>The cost of acting like this is true when it isn't: you move fast and spend money you didn't need to spend.</p>
<p>The cost of acting like it isn't true when it is: permanent.</p>


<hr>
<p><strong>PS:</strong> The thing I launched last week is called <a href="https://joinkith.com/">Kith</a> — a paid, invite-only social network where every person is verified human and there's no algorithm, no ads, no bots, and no AI. If that sounds like something you want to exist, <a href="https://joinkith.com/">join the waitlist</a>.</p>

    </article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Garment Notation Language: Formal descriptive language for clothing construction (135 pts)]]></title>
            <link>https://github.com/khalildh/garment-notation</link>
            <guid>47062329</guid>
            <pubDate>Wed, 18 Feb 2026 15:53:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/khalildh/garment-notation">https://github.com/khalildh/garment-notation</a>, See on <a href="https://news.ycombinator.com/item?id=47062329">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">GNL — Garment Notation Language</h2><a id="user-content-gnl--garment-notation-language" aria-label="Permalink: GNL — Garment Notation Language" href="#gnl--garment-notation-language"></a></p>
<p dir="auto">A formal descriptive language for clothing construction.</p>
<p dir="auto"><strong><a href="https://khalildh.github.io/garment-notation/viewer/" rel="nofollow">Try the live viewer</a></strong></p>
<p dir="auto">Dance has Labanotation. Music has staff notation. Architecture has plan/section/elevation conventions. GNL brings the same rigor to garments — a generative descriptive language where a valid expression is sufficient to construct a garment without ambiguity.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/khalildh/garment-notation/blob/main/images/tshirt-assembled.png"><img src="https://github.com/khalildh/garment-notation/raw/main/images/tshirt-assembled.png" alt="T-Shirt — assembled view"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Core Concepts</h2><a id="user-content-core-concepts" aria-label="Permalink: Core Concepts" href="#core-concepts"></a></p>
<ul dir="auto">
<li><strong>Body-anchored</strong> — the body is the coordinate system, using anatomical landmarks (<code>@shoulder.L</code>) and regions (<code>%torso.front</code>)</li>
<li><strong>Topological</strong> — garments are surfaces with boundaries and openings</li>
<li><strong>Constructive</strong> — descriptions encode build order, not just final form</li>
<li><strong>Composable</strong> — complex garments are compositions of simpler elements</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Example</h2><a id="user-content-quick-example" aria-label="Permalink: Quick Example" href="#quick-example"></a></p>
<div data-snippet-clipboard-copy-content="GARMENT t_shirt [SYM] {
  FABRIC: M(160gsm, fluid, biaxial:15%, 1.0, knit.jersey)

  front  = P(%torso.front, contour, 1.15)
  back   = P(%torso.back, contour, 1.15)
  sleeve = P(%arm[0..0.4], contour, 1.2)

  neck = O(@neck, circle, body+8cm)
  hem  = O(@hip, circle, body+10cm)

  BUILD:
    S(front.shoulder, back.shoulder, serged)
    >> S(sleeve.cap, {front.armhole, back.armhole}, serged)
    >> S(front.side, back.side, serged)
    >> F(hem, 2.5cm, in)
}"><pre><code>GARMENT t_shirt [SYM] {
  FABRIC: M(160gsm, fluid, biaxial:15%, 1.0, knit.jersey)

  front  = P(%torso.front, contour, 1.15)
  back   = P(%torso.back, contour, 1.15)
  sleeve = P(%arm[0..0.4], contour, 1.2)

  neck = O(@neck, circle, body+8cm)
  hem  = O(@hip, circle, body+10cm)

  BUILD:
    S(front.shoulder, back.shoulder, serged)
    &gt;&gt; S(sleeve.cap, {front.armhole, back.armhole}, serged)
    &gt;&gt; S(front.side, back.side, serged)
    &gt;&gt; F(hem, 2.5cm, in)
}
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Grammar</h2><a id="user-content-grammar" aria-label="Permalink: Grammar" href="#grammar"></a></p>
<p dir="auto">The language is formally defined as a <a href="https://github.com/khalildh/garment-notation/blob/main/grammar/gnl.peg">PEG grammar</a> targeting <a href="https://peggyjs.org/" rel="nofollow">Peggy</a>. The generated parser produces a richly-typed AST which is adapted to the renderer's internal format at runtime.</p>
<div dir="auto" data-snippet-clipboard-copy-content="npm install          # install Peggy (dev dependency only)
npm run generate     # regenerate viewer/src/gnl-parser.js from grammar/gnl.peg
npm test             # run parse + adapter tests against all examples"><pre>npm install          <span><span>#</span> install Peggy (dev dependency only)</span>
npm run generate     <span><span>#</span> regenerate viewer/src/gnl-parser.js from grammar/gnl.peg</span>
npm <span>test</span>             <span><span>#</span> run parse + adapter tests against all examples</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Viewer</h2><a id="user-content-viewer" aria-label="Permalink: Viewer" href="#viewer"></a></p>
<p dir="auto">The repo includes a <a href="https://khalildh.github.io/garment-notation/viewer/" rel="nofollow">live viewer</a> that parses GNL and renders both assembled garment views and flat pattern pieces.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Assembled View</h3><a id="user-content-assembled-view" aria-label="Permalink: Assembled View" href="#assembled-view"></a></p>
<p dir="auto">Write GNL on the left, see the full garment on the right — with stitch lines, dimension callouts, and construction details.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>T-Shirt</th>
<th>Wrap Skirt</th>
<th>Jacket Collar</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/khalildh/garment-notation/blob/main/images/tshirt-assembled.png"><img src="https://github.com/khalildh/garment-notation/raw/main/images/tshirt-assembled.png" alt="T-Shirt"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/khalildh/garment-notation/blob/main/images/wrap-skirt-assembled.png"><img src="https://github.com/khalildh/garment-notation/raw/main/images/wrap-skirt-assembled.png" alt="Wrap Skirt"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/khalildh/garment-notation/blob/main/images/collar-assembled.png"><img src="https://github.com/khalildh/garment-notation/raw/main/images/collar-assembled.png" alt="Jacket Collar"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pattern Pieces</h3><a id="user-content-pattern-pieces" aria-label="Permalink: Pattern Pieces" href="#pattern-pieces"></a></p>
<p dir="auto">Toggle to "Pieces" to see the individual flat pattern pieces with shape outlines, grain lines, and dimensions.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/khalildh/garment-notation/blob/main/images/tshirt-pieces.png"><img src="https://github.com/khalildh/garment-notation/raw/main/images/tshirt-pieces.png" alt="T-Shirt — pattern pieces"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Korosteleva Dataset Converter</h2><a id="user-content-korosteleva-dataset-converter" aria-label="Permalink: Korosteleva Dataset Converter" href="#korosteleva-dataset-converter"></a></p>
<p dir="auto">The repo includes a <a href="https://github.com/khalildh/garment-notation/blob/main/converter">converter</a> that transforms garment templates from the <a href="https://github.com/maria-korosteleva/Garment-Pattern-Generator">Korosteleva NeurIPS 2021 dataset</a> (2D panel geometry as JSON) into GNL.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Auto-downloads 21 templates from GitHub on first run, converts all
node converter/convert.js"><pre><span><span>#</span> Auto-downloads 21 templates from GitHub on first run, converts all</span>
node converter/convert.js</pre></div>
<p dir="auto">Four example templates (tee, skirt, pants, dress) are also available directly in the viewer — select from the "Korosteleva Dataset" section of the examples dropdown. A GNL/JSON toggle lets you compare the raw geometric input with the converted semantic output.</p>
<p dir="auto">See <a href="https://github.com/khalildh/garment-notation/blob/main/converter/README.md">converter/README.md</a> for details on the mapping approach.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<ul dir="auto">
<li><strong><a href="https://github.com/khalildh/garment-notation/blob/main/garment-notation.md">Full Specification</a></strong> — the complete v0.2 spec</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Star History</h2><a id="user-content-star-history" aria-label="Permalink: Star History" href="#star-history"></a></p>
<a href="https://www.star-history.com/#khalildh/garment-notation&amp;type=date" rel="nofollow">
  <themed-picture data-catalyst-inline="true"><picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://camo.githubusercontent.com/138821c7c481a2b9accaa392273b38655fcbf82e5c577eea005e62a27c99b0dc/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6b68616c696c64682f6761726d656e742d6e6f746174696f6e26747970653d44617465267468656d653d6461726b" data-canonical-src="https://api.star-history.com/svg?repos=khalildh/garment-notation&amp;type=Date&amp;theme=dark">
    <source media="(prefers-color-scheme: light)" srcset="https://camo.githubusercontent.com/9424c62abb9d334548fb6ad5939f9ed4552665ed5aed76927f4b688d62d1d062/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6b68616c696c64682f6761726d656e742d6e6f746174696f6e26747970653d44617465" data-canonical-src="https://api.star-history.com/svg?repos=khalildh/garment-notation&amp;type=Date">
    <img alt="Star History Chart" src="https://camo.githubusercontent.com/9424c62abb9d334548fb6ad5939f9ed4552665ed5aed76927f4b688d62d1d062/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6b68616c696c64682f6761726d656e742d6e6f746174696f6e26747970653d44617465" width="600" data-canonical-src="https://api.star-history.com/svg?repos=khalildh/garment-notation&amp;type=Date">
  </picture></themed-picture>
</a>
<p dir="auto"><h2 tabindex="-1" dir="auto">Status</h2><a id="user-content-status" aria-label="Permalink: Status" href="#status"></a></p>
<p dir="auto"><strong>v0.2 — Draft.</strong> Includes grain parameter, directional ease, princess seams (EDGE), lining (LAYER), and component composition (USE/ATTACH). A starting point that will need refinement through use, critique, and input from garment-makers, pattern-drafters, and computational designers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">All rights reserved.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stephen Colbert going down swinging (127 pts)]]></title>
            <link>https://www.nytimes.com/2026/02/18/arts/television/stephen-colbert-cbs-statement.html</link>
            <guid>47062179</guid>
            <pubDate>Wed, 18 Feb 2026 15:41:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2026/02/18/arts/television/stephen-colbert-cbs-statement.html">https://www.nytimes.com/2026/02/18/arts/television/stephen-colbert-cbs-statement.html</a>, See on <a href="https://news.ycombinator.com/item?id=47062179">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2026/02/18/arts/television/stephen-colbert-cbs-statement.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[What is happening to writing? Cognitive debt, Claude Code, the space around AI (117 pts)]]></title>
            <link>https://resobscura.substack.com/p/what-is-happening-to-writing</link>
            <guid>47061642</guid>
            <pubDate>Wed, 18 Feb 2026 14:59:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://resobscura.substack.com/p/what-is-happening-to-writing">https://resobscura.substack.com/p/what-is-happening-to-writing</a>, See on <a href="https://news.ycombinator.com/item?id=47061642">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Last week, an essay laying out a case for how AI will change knowledge work went enormously viral on X, where it currently sits at 84 million views:</p><div><figure><a target="_blank" href="https://x.com/mattshumer_/status/2021256989876109403" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!1NgU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9bc8318-6349-4daa-923a-2b1cbbf6e590_1088x728.png 424w, https://substackcdn.com/image/fetch/$s_!1NgU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9bc8318-6349-4daa-923a-2b1cbbf6e590_1088x728.png 848w, https://substackcdn.com/image/fetch/$s_!1NgU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9bc8318-6349-4daa-923a-2b1cbbf6e590_1088x728.png 1272w, https://substackcdn.com/image/fetch/$s_!1NgU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9bc8318-6349-4daa-923a-2b1cbbf6e590_1088x728.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!1NgU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9bc8318-6349-4daa-923a-2b1cbbf6e590_1088x728.png" width="509" height="340.58088235294116" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e9bc8318-6349-4daa-923a-2b1cbbf6e590_1088x728.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:728,&quot;width&quot;:1088,&quot;resizeWidth&quot;:509,&quot;bytes&quot;:696060,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:&quot;https://x.com/mattshumer_/status/2021256989876109403&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://resobscura.substack.com/i/187823656?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9bc8318-6349-4daa-923a-2b1cbbf6e590_1088x728.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!1NgU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9bc8318-6349-4daa-923a-2b1cbbf6e590_1088x728.png 424w, https://substackcdn.com/image/fetch/$s_!1NgU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9bc8318-6349-4daa-923a-2b1cbbf6e590_1088x728.png 848w, https://substackcdn.com/image/fetch/$s_!1NgU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9bc8318-6349-4daa-923a-2b1cbbf6e590_1088x728.png 1272w, https://substackcdn.com/image/fetch/$s_!1NgU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9bc8318-6349-4daa-923a-2b1cbbf6e590_1088x728.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>I agree with much of it, though parts were overstated and other parts were clearly </span><a href="https://paulgraham.com/submarine.html" rel="">submarine marketing</a><span>. But what was most striking to me wasn’t the content, but the obvious fact that large swathes of it were written by AI. I suspect that the “AI slop” quality was actually part of why it did so well. In 2026, we must face the fact that slop is </span><em>well-liked</em><span>. Perfectly formatted writing, with statistics ready at hand, copious length, a chipper tone, and lots of antithetical phrasing — the infamous “it’s not that; it’s this” formula — is showing up everywhere for a reason. People might say they hate it, and I’m sure many do. But&nbsp;the truth is that many other readers clearly enjoy</span><em> </em><span>this style of machine-generated prose. </span></p><p>That revealed preference worries me more than anything else. </p><p><span>I learned to read and write quite late (worryingly late, by the standards of millennial parents like myself). At age 8, I was barely literate, way behind classmates. Something clicked at age 9. I read </span><em>The Lord of the Rings </em><span>twice before graduating to </span><em>The Silmarillion. </em><span>Fifth grade was the year of Michael Crichton. By age 12 it was Virginia Woolf and Tolstoy —&nbsp;both of whom I barely understood. But I loved the unfamiliar sound of the words and the unfamiliar sensations those words evoked. I loved feeling like I could transmute some of those feelings and images into my own words on a page (I even started a novel, which had something to do with an ancient Egyptian goldsmith searching for his father). </span></p><p>Writing became something I was self-evidently good at. It was the one thing in my life that truly opened doors for me. </p><p><span>My first real job after college in 2007 was as an assistant for a crusty New York labor lawyer who wrote exclusively on legal pads and called me “college boy.” He had an incredible memory and an agile mind, but he wasn’t much of a prose stylist. That task was offloaded to me. Every morning, the lawyer would toss yellow legal pads on my desk. It was my job to proofread and “punch up” these rough drafts. I would toil happily for a few hours in Microsoft Word, usually earning his gruff approval by around lunchtime. Then I spent the rest of the day reading about </span><a href="https://en.wikipedia.org/wiki/Mellified_man" rel="">mellified man</a><span> and </span><a href="https://en.wikipedia.org/wiki/Pre-Columbian_transoceanic_contact_theories#Claims_of_Egyptian_coca_and_tobacco" rel="">cocaine mummies</a><span> and the like on Wikipedia. I did well enough at this job that it felt like a viable, permanent career. </span></p><p>Needless to say, that career is gone now. Gemini is at least as good at transcribing a lawyer’s longhand as I am, and Claude will undoubtedly output more perfectly formatted legal footnotes than I could ever hope to achieve. </p><p>So what is left for human writing? </p><p><span>As a historian and teacher —&nbsp;distinct from my identity as a writer —&nbsp;I don’t experience much of the existential dread that essays like “Something Big is Happening” seem intent on getting me to feel. </span><a href="https://resobscura.substack.com/p/ai-legibility-archives-future-of-research" rel="">As I wrote here</a><span>, historians rely on non-digitized archives and tacit, often physically-embodied knowledge. That is a very good thing when it comes to resisting replacement by the likes of Anthropic’s newly released Sonnet 4.6 model, which is remarkably good at “computer work” tasks. </span></p><p>When I think of AI-proof jobs, I think of people like electricians, plumbers, or the surf instructors of Santa Cruz. But I also think about history professors and anyone else whose output includes some combination of in-person engagement and travel-based or otherwise embodied work in a regulated industry. No less than a surf instructor, historians are performing physical services in the real world, although we don’t tend to think of it in those terms. We are going into parish church basements to read through baptismal records, finding weird old non-digitized books in rare book shops, piecing together who called Margaret Mead on a certain day in 1954 by reading through her secretary’s notes. These are not the everyday tasks of a historian’s life, but they are the kind to things we might do, say, once a week. Couple that with twice a week in-person classroom time, and I simply flatly disagree with anyone who thinks this combo will be replaced by a Sonnet 4.6-type model, no matter how good it gets at creating Excel spreadsheets, translating Latin, or explaining linear algebra. </p><p>Anyone who has led a class discussion —&nbsp;much less led students on a tour of Egypt or Okinawa, as my colleagues regularly do —&nbsp;knows that there is a huge gap between solo learning online and collective learning in meat space. And although it is possible to imagine a humanoid robot instructor with a brain powered by some future OpenAI model, it is very, very difficult for me to imagine such a thing being both popular with students and permissible within regulatory frameworks. </p><p><span>The claim that robot teachers will surely happen someday is one thing, and I can’t disagree with it. But culture and regulations change </span><em>far</em><span> more slowly than technologies. Those in the technology sector who are predicting time horizons of a few years for these changes are, I think, confusing the pace of technical change with the pace of social change. The image above, produced in 1890s France, was imagining machine-generated education by the year 2000. For my part, I can picture educational systems dominated by robot teachers or even direct-to-brain interfaces by the year 2100, and possibly a bit before that. </span></p><p>But it’s worth thinking about how surprised the creator of the image above might be to learn that in my classrooms in 2026, I am still using a blackboard and chalk. </p><p>So although I am very sure at this point that AI will be transformative for a broad range of human jobs and interests, I actually think historians and teachers are going to be fine in the medium term of the next couple decades. </p><p data-attrs="{&quot;url&quot;:&quot;https://resobscura.substack.com/p/what-is-happening-to-writing?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://resobscura.substack.com/p/what-is-happening-to-writing?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p>So it may go for historians. What about writers? </p><p><span>There I’m less sure. Certainly, the bottom will&nbsp;drop out from the lower rungs of eyeball-grabbing written content, from pulpy novels to historical click-bait writing online. Probably, it already has. That GPT-4 level models could passably imitate the imitators of, say, Colleen Hoover or Dan Brown was known to anyone who experimented with them.</span><span data-state="closed"><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-187823656" href="https://resobscura.substack.com/p/what-is-happening-to-writing#footnote-1-187823656" target="_self" rel="">1</a></span></p><p>These experiments could be interesting, but could never be mistaken for good writing. The thing is: the most recent Anthropic models actually write quite well. </p><p>For this reason, I find myself deeply sympathetic with software developers lately. Their struggles are my own, just from a different angle: what happens when something you are good at, a skill which is relatively rare and relatively valued, suddenly becomes commoditized? For both professional writers and professional developers, this is not a question but simply a daily reality. </p><p><span>Like </span><a href="https://x.com/karpathy/status/2015883857489522876" rel="">Andrej Karpathy</a><span> and others, I find myself deeply energized by the potential of these new tools, while also wondering what their mass adoption will do to everyone’s brains (Karpathy celebrates the “phase shift” in capabilities that occurred over the past few months, but adds: “I've already noticed that I am slowly starting to atrophy my ability to write code manually”). </span></p><p><span>On the one hand, I feel a real sense of loss for the world of early 2000s </span><em>New Yorker </em><span>style long-form writing and criticism, the world of </span><em>The New York Review of Books </em><span>and </span><em>Lapham’s Quarterly</em><span> and the amazingly good Dennis Johnson novel I’m currently reading (</span><em>Tree of Smoke</em><span>). </span></p><p><span>Yet I am taking real delight in being able to create truly novel —&nbsp;though sometimes interesting only to me —&nbsp;hybrids of software, history, and writing. This </span><a href="https://resobscura.substack.com/p/how-well-can-gemini-3-make-a-henry" rel="">Henry James simulator</a><span> from last fall was, in truth, just the tip of a somewhat obsessive Claude Code iceberg for me. </span></p><p>Some recent examples of what I’ve been working on:</p><p><span>Drawing on my </span><a href="https://resobscura.substack.com/p/simulating-history-with-chatgpt" rel="">earlier work</a><span> on historical simulation using ChatGPT prompts, I made a full-on </span><a href="https://historysimulator.vercel.app/" rel="">history simulator</a><span> (GitHub </span><a href="https://github.com/benjaminbreen/UHS" rel="">here</a><span>) which randomly drops you in different historical eras and cultures, with pixel art graphics augmenting an LLM-generated narrative engine which is grounded in real primary sources and a rather elaborate system for generating at least somewhat accurate historical context. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Sbtd!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facdb89d6-670f-4475-9126-6bbaad3877cb_5000x1782.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Sbtd!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facdb89d6-670f-4475-9126-6bbaad3877cb_5000x1782.png 424w, https://substackcdn.com/image/fetch/$s_!Sbtd!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facdb89d6-670f-4475-9126-6bbaad3877cb_5000x1782.png 848w, https://substackcdn.com/image/fetch/$s_!Sbtd!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facdb89d6-670f-4475-9126-6bbaad3877cb_5000x1782.png 1272w, https://substackcdn.com/image/fetch/$s_!Sbtd!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facdb89d6-670f-4475-9126-6bbaad3877cb_5000x1782.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Sbtd!,w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facdb89d6-670f-4475-9126-6bbaad3877cb_5000x1782.png" width="1200" height="427.74725274725273" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/acdb89d6-670f-4475-9126-6bbaad3877cb_5000x1782.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:519,&quot;width&quot;:1456,&quot;resizeWidth&quot;:1200,&quot;bytes&quot;:7402109,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://resobscura.substack.com/i/187823656?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facdb89d6-670f-4475-9126-6bbaad3877cb_5000x1782.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:&quot;center&quot;,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Sbtd!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facdb89d6-670f-4475-9126-6bbaad3877cb_5000x1782.png 424w, https://substackcdn.com/image/fetch/$s_!Sbtd!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facdb89d6-670f-4475-9126-6bbaad3877cb_5000x1782.png 848w, https://substackcdn.com/image/fetch/$s_!Sbtd!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facdb89d6-670f-4475-9126-6bbaad3877cb_5000x1782.png 1272w, https://substackcdn.com/image/fetch/$s_!Sbtd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facdb89d6-670f-4475-9126-6bbaad3877cb_5000x1782.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>At left, I enter the prompt “</span><strong>Make me a toymaker in Renaissance Florence</strong><span>.” At right, I become Lorenzo Bellini, a 35-year-old toymaker in the year 1500.  </span></figcaption></figure></div><p>I’ve already used in this in my teaching. While it’s imperfect, the mechanic of allowing a user to generate a historical simulation based on text input — like “I want to be a salt miner in a frontier province of Ancient Rome” or “1930s Istanbul bike mechanic” or “Aztec farmer with a troubled past” or whatever else you can think of — is fascinating to me. More on this in a future post. </p><p><span>I tried to make a game called MKULTRA (playable version </span><a href="https://mkultra-nu.vercel.app/" rel="">here</a><span>, GitHub </span><a href="https://github.com/benjaminbreen/MKULTRA" rel="">here</a><span>). This game is based directly on primary sources relating to George Hunter White’s now-infamous CIA-funded work on dosing unwitting civilians with LSD which I had gathered for my book </span><em>Tripping on Utopia</em><span>. As with History Simulator, the text responding to player input here is generating by Google’s Gemini 2.5 model. The difference here is that it gets more and more outlandish and surreal as you, playing as a procedurally generated person in 1950s San Francisco, are dosed with psychedelics. As your “trippiness” meter  increases, the UI itself begins to break down and change. Compare the relatively sober player view at left with the altered one at right: </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!uykC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefe34e6f-368a-4495-890c-d1084d51cb60_5005x2000.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!uykC!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefe34e6f-368a-4495-890c-d1084d51cb60_5005x2000.png 424w, https://substackcdn.com/image/fetch/$s_!uykC!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefe34e6f-368a-4495-890c-d1084d51cb60_5005x2000.png 848w, https://substackcdn.com/image/fetch/$s_!uykC!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefe34e6f-368a-4495-890c-d1084d51cb60_5005x2000.png 1272w, https://substackcdn.com/image/fetch/$s_!uykC!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefe34e6f-368a-4495-890c-d1084d51cb60_5005x2000.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!uykC!,w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefe34e6f-368a-4495-890c-d1084d51cb60_5005x2000.png" width="1200" height="479.6703296703297" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/efe34e6f-368a-4495-890c-d1084d51cb60_5005x2000.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:582,&quot;width&quot;:1456,&quot;resizeWidth&quot;:1200,&quot;bytes&quot;:6669040,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://resobscura.substack.com/i/187823656?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefe34e6f-368a-4495-890c-d1084d51cb60_5005x2000.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:&quot;center&quot;,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!uykC!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefe34e6f-368a-4495-890c-d1084d51cb60_5005x2000.png 424w, https://substackcdn.com/image/fetch/$s_!uykC!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefe34e6f-368a-4495-890c-d1084d51cb60_5005x2000.png 848w, https://substackcdn.com/image/fetch/$s_!uykC!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefe34e6f-368a-4495-890c-d1084d51cb60_5005x2000.png 1272w, https://substackcdn.com/image/fetch/$s_!uykC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefe34e6f-368a-4495-890c-d1084d51cb60_5005x2000.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>An imaginary San Franciscan named George Schmidt ends up in the </span><a href="http://jerrygarciasbrokendownpalaces.blogspot.com/2012/12/operation-midnight-climax-225-chestnut.html" rel="">actual safe house</a><span> at 225 Chestnut Street where the CIA tested LSD on civilians.</span></figcaption></figure></div><p><span>Premodern Concordance (live page </span><a href="https://premodern-concordance.vercel.app/" rel="">here</a><span>, GitHub </span><a href="https://github.com/benjaminbreen/premodern-concordance" rel="">here</a><span>) is the most promising use case for LLMs in historical research that I’ve come up with so far, and something I am currently seeking grant funding for along with my historian colleague Mackenzie Cooley. As a proof of concept for this larger project, I fine-tuned an embedding model to analyze premodern medical, scientific and natural history texts, searching for non-obvious links between concepts that fuzzily match up across languages and eras. For instance, if you click “piedra Bezaar” on the page below, you end up at a cross-linguistic concordance page for </span><a href="https://premodern-concordance.vercel.app/concordance/clu_0c4aa0111f771c6b" rel="">Bezoar stones</a><span> which tracks mention of them across texts in Portuguese, Spanish, Italian, French and English. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!PQXp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc41c36e9-dfb5-4bb0-ab67-da9243d81f42_2920x2062.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!PQXp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc41c36e9-dfb5-4bb0-ab67-da9243d81f42_2920x2062.png 424w, https://substackcdn.com/image/fetch/$s_!PQXp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc41c36e9-dfb5-4bb0-ab67-da9243d81f42_2920x2062.png 848w, https://substackcdn.com/image/fetch/$s_!PQXp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc41c36e9-dfb5-4bb0-ab67-da9243d81f42_2920x2062.png 1272w, https://substackcdn.com/image/fetch/$s_!PQXp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc41c36e9-dfb5-4bb0-ab67-da9243d81f42_2920x2062.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!PQXp!,w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc41c36e9-dfb5-4bb0-ab67-da9243d81f42_2920x2062.png" width="1200" height="847.2527472527472" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c41c36e9-dfb5-4bb0-ab67-da9243d81f42_2920x2062.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:1028,&quot;width&quot;:1456,&quot;resizeWidth&quot;:1200,&quot;bytes&quot;:1530696,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://resobscura.substack.com/i/187823656?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc41c36e9-dfb5-4bb0-ab67-da9243d81f42_2920x2062.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:&quot;center&quot;,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!PQXp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc41c36e9-dfb5-4bb0-ab67-da9243d81f42_2920x2062.png 424w, https://substackcdn.com/image/fetch/$s_!PQXp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc41c36e9-dfb5-4bb0-ab67-da9243d81f42_2920x2062.png 848w, https://substackcdn.com/image/fetch/$s_!PQXp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc41c36e9-dfb5-4bb0-ab67-da9243d81f42_2920x2062.png 1272w, https://substackcdn.com/image/fetch/$s_!PQXp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc41c36e9-dfb5-4bb0-ab67-da9243d81f42_2920x2062.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>There’s actually several more projects I’ve been working on, but I won’t bore you with long descriptions. In brief: </p><p><span>•&nbsp;I made an </span><a href="https://apothecary-simulator.vercel.app/" rel="">apothecary simulator</a><span> based on 17th century texts, where you treat patients in colonial Mexico City and potentially get in trouble with the Inquisition. </span></p><p><span>•&nbsp;I made a </span><a href="https://historical-persona-generator.vercel.app/" rel="">historical persona generator</a><span> for use in history classroom activities, roleplaying, and the like, as a spinoff of the larger history simulator project. </span></p><p><span>•&nbsp;I made a </span><a href="https://historical-canon-explorer.vercel.app/" rel="">literary canon explorer</a><span> which contrasts mentions of Victorian authors on Google Books versus their page views on Wikipedia.</span></p><p>These are all things which, in the past, I would have explored through the written word. Now, I’m exploring them through Claude Code, which I use to write the TypeScript code that summons them into being, and Google’s Gemini language models, which I use to populate them with “speaking” characters for the simulation aspects. </p><p>I have greatly enjoyed the challenge of learning how to build these games and tools over the past year. But it also seems to me that the work I have put into them —&nbsp;the enjoyment I have gained from them —&nbsp;is, in itself, a somewhat worrisome leading indicator. </p><p><span>It’s not just that AI slop is replacing some of the lower-rung forms of fiction and prose. It’s that the audience share for </span><em>writing as a whole</em><span> will be increasingly displaced by interactive “writing-adjacent” things like the MKULTRA game or interactive text-based simulations of life as a Florentine toymaker in the year 1500, or whatever else you can imagine. Gemini’s outputs for these scenarios read like a bad attempt at historical fiction. But the dynamic nature of these sorts of tools —&nbsp;their “choose your own adventure” quality —&nbsp;is genuinely new. That dynamism and hand-tailored quality will, I suspect, be more compelling for many than simply reading mediocre novels </span><em>about </em><span>MKULTRA or Florentine toymakers.  </span></p><p><span>At the end of the day, I didn’t enjoy any of the projects described above as much as I enjoyed researching and writing </span><em><a href="https://www.amazon.com/Tripping-Utopia-Margaret-Troubled-Psychedelic/dp/1538722372?_encoding=UTF8&amp;tag=ro067-20&amp;linkCode=ur2&amp;linkId=ba347f678c5238772f23489f5065baa6&amp;camp=1789&amp;creative=9325" rel="">Tripping on Utopia</a></em><span>. Or as much as I am enjoying starting work on my new book, which, not coincidentally, is about how William James his world reacted to the automation of life and thought in the first machine age. </span></p><p>I miss the obsessive flow you get from deep immersion in writing a book. Such work has none of the dopamine spiking, slot machine-like addictiveness of Claude Code —&nbsp;the rapid progress of typing two sentences into a terminal window, watching Opus 4.6 build a new feature over the course of ten minutes, and then seeing it come to life on a screen. </p><p><span>But the very </span><em>lack </em><span>of progress that defines serious writing — the staring at a blank page, the rewrites, the chapters you throw away — feels earned (and therefore  intellectually meaningful) in a way that Claude Code simply never will. </span></p><p><span>“</span><a href="https://margaretstorey.com/blog/2026/02/09/cognitive-debt/" rel="">Cognitive debt</a><span>” is a term that software engineers have recently adopted to describe the disconcerting feeling of losing touch with the  ground truth of what is actually happening in one’s codebase. As Simon Willison </span><a href="https://simonwillison.net/2026/Feb/15/cognitive-debt/" rel="">recently put it</a><span>: “I've been experimenting with prompting entire new features into existence without reviewing their implementations and, while it works surprisingly well, I've found myself getting lost in my own projects.” It is worth noting that this term didn’t originate in the world of code. It was </span><a href="https://www.media.mit.edu/publications/your-brain-on-chatgpt/" rel="">coined last year</a><span> by MIT researchers to describe “cognitive costs” from relying on ChatGPT to write </span><em>essays</em><span>. The frontlines of cognitive debt, in other words, are in the realms of both writing and coding. </span></p><p>I have about 50 ideas sitting in my Substack drafts folder. It is tantalizing to imagine that I could simply open a Claude Code terminal window, direct it to these drafts, and tell it to output a year’s worth of Res Obscura posts. This, of course, is not a hypothetical but simply what a a lot of people are doing right now. </p><p>But if you do that, what is the point? What was I working toward when I was 10, or 20? </p><p><span>The work is, </span><em>itself</em><span>, the point. </span></p><p>My quasi-obsession with vibe coding over the past year or so — see above —&nbsp;made that less clear to me for a time. I was never tempted to use AI to write this blog (something I have never done and never will). Rather, what did tempt me was the illusion of productivity and innovation that one gets from using AI to create custom software. At times, it has felt more productive to produce digital humanities projects and historical educational games like Apothecary Simulator than to put words on a page. As I write that now, I feel some sense of shame, as if I am confessing to having become addicted to junk food or gambling. </p><p><em>Is</em><span> Claude Code junk food, though? I also spent the past year learning about data taxonomies and </span><a href="https://en.wikipedia.org/wiki/Single_source_of_truth" rel="">single stores of truth</a><span> and embedding models, and although I have barely written a line of code on my own, the cognitive work of learning the architecture —&nbsp;developing a new epistemological framework for “how developers think” — feels real. I also think that at least a few of my projects, especially the Premodern Concordance, are useful tools that I will be returning to and learning from for years. </span></p><p><span>And yet: I miss pre-AI writing and research. More than that, I miss </span><em>thinking through writing</em><span> </span><em>in public. </em></p><p><span>The novelist Martin Amis once </span><a href="https://www.theguardian.com/books/2021/aug/07/martin-amis-style-isnt-something-you-apply-later" rel="">wrote</a><span>: “style isn’t something you apply later; it’s embedded in your perception, and writers without that freshness of voice make no appeal to me. What is one in quest of with such a writer? Their views? Their theories?” </span></p><p>The current frontier AI models are fascinating because they are an entirely new kind of tool, untested and unexplored. They are even more fascinating as implicit puzzles about the nature of consciousness and selfhood. </p><p>What they aren’t are replacements for thinking itself, or for the elusive, deeply personal sense of style Amis was talking about.</p><p>It’s not just that “writing is thinking” (an oft-repeated phrase these days).</p><p><span>It’s that writing is a </span><em>special, irreplaceable</em><span> </span><em>form of thinking</em><span> forged from solitary perception and labor —&nbsp;an enormous amount of it — but tested against a reading public. That fusion of aloneness and interiority versus togetherness and exteriority is what makes it fascinating to me. I fear for a world in which people are simply able to create a simulacrum of writing and of thought by inputting a prompt and singly experiencing the result. </span></p><p><span>In short: the </span><em>production </em><span>of writing is deeply solitary and personal, but the </span><em>consumption </em><span>of writing is just as deeply public and shared. </span></p><p>That combination seems to me to be the true negative space around AI capabilities. AI will get very, very good at creating compelling, hyper-customized content. Claude Code’s self-evident addictive qualities is a strong early signal of that. It will not, I suspect, get good at creating the public debates and shared intellectual communion that characterize great literature and great historical writing. And it certainly won’t be able to capture the perception-based, physically embodied sense of personal style and taste that Martin Amis described. </p><p><span>And so, reader: I say to you now, I will keep writing without AI, even as I explore what it’s possible to do with it. I will be doing it simply because I enjoy talking to you and thinking </span><em>with</em><span> you&nbsp;—&nbsp;not as a solitary individual in a chat transcript but as a collectivity of actual human readers reading actual human thoughts. </span></p><p>Thank you for creating that with me. Long may it last. </p><p><span>•&nbsp;“Two attempts to replicate Balabanova’s findings of cocaine failed, suggesting ‘that either Balabanova and her associates are misinterpreting their results or that the samples of mummies tested by them have been mysteriously exposed to cocaine’” (</span><a href="https://en.wikipedia.org/wiki/Pre-Columbian_transoceanic_contact_theories#Claims_of_Egyptian_coca_and_tobacco" rel="">Wikipedia</a><span>)</span></p><p><span>• Archaeologists report an ancient elephant bone which may be the first physical proof of Hannibal’s crossing of the Alps with a troop of North African war elephants —&nbsp;something I am still staggered by whenever I teach it to the students in my world history class (</span><a href="https://www.bbc.com/news/articles/cdr2xl1e41eo" rel="">BBC</a><span>).</span></p><p data-attrs="{&quot;url&quot;:&quot;https://resobscura.substack.com/p/what-is-happening-to-writing/comments&quot;,&quot;text&quot;:&quot;Leave a comment&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://resobscura.substack.com/p/what-is-happening-to-writing/comments" rel=""><span>Leave a comment</span></a></p><p data-attrs="{&quot;url&quot;:&quot;https://resobscura.substack.com/p/what-is-happening-to-writing?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://resobscura.substack.com/p/what-is-happening-to-writing?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AVX2 is slower than SSE2-4.x under Windows ARM emulation (108 pts)]]></title>
            <link>https://blogs.remobjects.com/2026/02/17/nerdsniped-windows-arm-emulation-performance/</link>
            <guid>47061062</guid>
            <pubDate>Wed, 18 Feb 2026 14:08:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blogs.remobjects.com/2026/02/17/nerdsniped-windows-arm-emulation-performance/">https://blogs.remobjects.com/2026/02/17/nerdsniped-windows-arm-emulation-performance/</a>, See on <a href="https://news.ycombinator.com/item?id=47061062">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-main">

        <article>

            

            <figure>
                <img srcset="https://blogs.remobjects.com/content/images/size/w300/2026/02/AdobeStock_686264219.jpeg 300w,
                            https://blogs.remobjects.com/content/images/size/w600/2026/02/AdobeStock_686264219.jpeg 600w,
                            https://blogs.remobjects.com/content/images/size/w1000/2026/02/AdobeStock_686264219.jpeg 1000w,
                            https://blogs.remobjects.com/content/images/size/w2000/2026/02/AdobeStock_686264219.jpeg 2000w" sizes="(max-width: 800px) 400px,
                            (max-width: 1170px) 700px,
                            1400px" src="https://blogs.remobjects.com/content/images/size/w2000/2026/02/AdobeStock_686264219.jpeg" alt="AVX2 is slower than SSE2-4.x under Windows ARM emulation">
            </figure>

            <div>
                    <p><em>If you compile your app for AVX2 and it runs on Windows ARM under Prism emulation, is it faster or slower than compiling for SSE2-4.x?</em></p><p>I assumed it would be roughly the same — maybe slightly slower due to emulation overhead, but AVX2's wider operations would compensate. The headline gives it away: I was wrong.</p><div><p>💡</p><div><p><strong>TLDR:</strong> AVX2 code runs at 2/3 the speed of equivalent SSE2-SSE4.x optimised code under emulation on Windows 11 ARM.</p><p>'Should I compile for AVX2 if my app might run on Windows ARM?' has a clear answer: <strong>No. </strong>At least if performance matters.</p></div></div><p>This post explains how I found out, what I measured and how, the benchmark results, and why.</p><h2 id="curiosity">Curiosity</h2><p>A few weeks ago, in a <a href="https://news.ycombinator.com/item?id=46286027">Hacker News thread on WoW (the game) emulated performance</a> on Windows ARM, I wondered:</p><blockquote>I’ve been testing some math benchmarks on ARM emulating x64, and saw very little performance improvement with the AVX2+FMA builds, compared to the SSE4.x level. (X64 v2 to v3.) ... I’ve found very little info online about this.</blockquote><p>Well, I <a href="https://www.explainxkcd.com/wiki/index.php/356:_Nerd_Sniping">nerdsniped</a> myself, because those math benchmarks are now complete and so we have the perfect framework for testing AVX2+FMA emulation performance overhead on ARM Windows. I have no technical reason to do so: if you use our compiler we encourage that if you want to run your app on Windows ARM to just compile your app for Windows ARM. It's simply: <em>I want to know.</em></p><p>Thus I spent much of Sunday crunching our data and figuring it out.</p><h2 id="arm-emulation-of-x86">ARM emulation of x86</h2><p><em>You can skip this bit if you know about Windows ARM's emulation and what various Intel instruction sets like SSE through AVX2 are: go forward to <a href="#our-benchmarks">Benchmarks</a>.</em></p><p>Windows 11 lets you run both 32-bit and 64-bit Intel apps on ARM. It does this via emulation. Essentially, x86/64 code is translated on the fly into ARM. Windows 10 supported emulating 32-bit Intel, and by 2021 <a href="https://www.tomshardware.com/news/windows-10-arm-x64-emulation">Windows 11 introduced</a> emulating 64-bit apps.</p><p>In 2024 Windows 11 was updated with a <a href="https://www.osnews.com/story/139748/microsoft-gives-windows-new-compiler-kernel-scheduler-and-x86-translation-layer-on-arm/">new emulation layer</a>, Prism. The main user-facing change <a href="https://www.windowscentral.com/microsoft/windows-11/your-windows-11-on-arm-pc-can-now-run-even-more-x86-apps-and-games-thanks-to-microsofts-latest-prism-emulation-update">seems to have been</a> performance: 'Microsoft told Ars Technica that Prism is as fast as Apple’s Rosetta 2' and:</p><blockquote>Most x86 apps now run without issues, and in many cases don't even feel like they're being emulated. These days, the majority of users won't notice a difference between using an Intel PC or a Snapdragon one<br>– <a href="https://www.windowscentral.com/microsoft/windows-11/your-windows-11-on-arm-pc-can-now-run-even-more-x86-apps-and-games-thanks-to-microsofts-latest-prism-emulation-update">Windows Central</a></blockquote><h3 id="is-emulation-complete-entire">Is emulation complete / entire?</h3><p><strong>x86 and x86_64 have not always remained the same.</strong> Over time they add more functionality, which is exposed as instruction sets. These are the base instructions that an app can be compiled to use and are often focused around doing things faster. For example, the <a href="https://en.wikipedia.org/wiki/X87">x87 floating point</a> math instruction set still exists (it was introduced in the 1980s!) but was succeeded a quarter century ago by SSE2, introduced with the Pentium 4. SSE2 lets you perform floating point math operations much faster. A few years later the SSE 4.x series also improved largely integer-based operations. This is a <em>very handwavy summary:</em> in fact, these are part of a <a href="https://en.wikipedia.org/wiki/Processor_supplementary_capability">wide series of instructions intended to process data fast</a>, where possible in wider configurations (more data at a time per clock tick) than older instructions, each new improvement introduced one by one over many years. This does not even begin to address the other supplementary extensions: ones for bit manipulation, specific math patterns like multiply-add, and more.</p><p>This is important to understand because <strong>software does not all use the same sets of instructions.</strong> Figuring out baseline standards of which sets it was reasonable for an app to use was a mess, and Linux folk found it annoying enough that, working together with Intel and AMD, Red Hat and SUSE defined standardised versions to allow known safe targets for compiling for specific collections of instruction sets. Thanks to them, x86_64 now has <a href="https://en.wikipedia.org/wiki/X86-64#Microarchitecture_levels">four main versions</a> that modern compilers target, which define generations of new instructions. x64 version 1 is that same old year-2000-ish era; x64 version 2 circa 2008 level, v3 circa 2013 level, and v4 circa 2017 level.</p><p>It takes time for instructions to become mainstream: if you are building software for 64bit Intel Windows in general, you likely won't build solely for the v4 2017 level because you may have users who have older computers, or ones newer than 2017 but which had less capable chips that didn't feature all the latest instruction sets. AVX-512 (v4) is a wonderful instruction set for very wide vectorised behaviour but <em>still</em> many computers in practical use today don't have it.</p><div><p>💡</p><div><p>What happens if you run software that uses (targets) instructions that don't exist on your CPU? You get an <em>illegal instruction exception</em> and your app terminates.</p><p>Luckily, most apps are written targeting older x64 versions with broad support.</p><p>Some apps actually target multiple versions at once through something called <a href="https://clang.llvm.org/docs/AttributeReference.html#target-clones">target_clones</a>, where for some critical parts of the app the compiler will generate the same code multiple times, each one optimised for a different generation of CPU, and at runtime it will choose which one to use.</p></div></div><p>And similarly to how actual hardware may or may not support specific instruction sets, <strong>Windows x86 emulation also supported only a subset.</strong></p><p>That subset was approximately x64 version 2 (ie with SSE2 and 4.x) and only recently have newer versions of Windows have supported v3: to handwave, AVX2 and FMA. This is new, exciting, and to my knowledge, largely unknown.</p><p>We are comparing performance using <code>x86-64-2</code> level vs <code>v3</code> level running emulated on ARM.</p><p>That brings us to that Hacker News comment and today. <em>What's the emulation performance for those newer instructions?</em></p><h2 id="our-benchmarks">Our benchmarks</h2><p>At RemObjects we make a multi-language (6 of them!) compiler toolchain that targets native CPUs via LLVM (as well as .NET, JVM, and WASM backends.) We <a href="https://blogs.remobjects.com/2026/01/26/fast-math-in-six-languages-what-we-did-and-why-it-works/">recently integrated a new vectorised math library</a>, which gave us the perfect benchmark framework for testing this.</p><p>We already supported ARM Windows, that is not new, but our cross-platform RTL had our own implementations of common math methods on the '<a href="https://www.remobjects.com/elements/island/">Island</a>' (native) platform (the normal set: sin/pow/exp/floor and so forth.) These were correct in that they followed known algorithms, but we felt there was room for performance optimisation. We settled on integrating a third party open source math library. At the time I wrote the above HN comment, this integration was still being tested and tweaked; we even changed some of LLVM's internal passes.</p><p>Today, we have this new math implemented for macOS ARM (and x64), and Windows i386, x86_64, and ARM64. Our Windows 32-bit (i386) math supports SSE2/4.x, but the math library for <strong>Windows x64 supports using either v2 (SSE2-4.x) or v3 (AVX2-targeted) level</strong> depending on the <a href="https://en.wikipedia.org/wiki/X86-64#Microarchitecture_levels">x64 revision</a> you target in the compiler options. (You can actually tell Elements to target v1 through v4, but our new math library kicks in at v2, with more performance with v3, and we have not enabled anything extra in math for v4; you can certainly compile allowing AVX-512 etc if you wish for your code in general though.)</p><p>As part of checking our new math code as (a) correct and (b) improved performance, we have <strong>concrete data running 21 different math operations</strong> on both real x64 hardware, and Windows ARM on Parallels on a Mac M2.</p><p>Because these are different machines we cannot compare the wall clock time, but we can compare the relative time, using the SSE2-4.x level as a basis: what is the <em>relative</em> performance difference of using AVX2(+FMA) on Intel vs on ARM? Normalising against the earlier, well-emulated instruction set means that <strong>the difference between emulated and real hardware gives us the answer of how well AVX2 emulation performs.</strong></p><div><p>💡</p><p>To rephrase: Emulated and not-emulated run on different hardware. So how can we make them comparable? By using something they both do as a basis: both already ran <code>x64 v2</code> (SSE2-4.x) code. Emulated <code>v2</code> may be slower, sure, but if we <em>normalise <code>v2</code>'s performance as 1.0 on each test platform,</em> we get the <code>v3</code> (AVX2) performance as a comparable number.</p></div><p>We can also use ARM64 on ARM, and AVX2 on x64, as a handwavy comparison for if you need ARM: an indication if emulation provides 'good enough' performance or if there's real value in compiling for ARM.</p><p>Thus:</p><ul><li>Using SSE2-4.x as a baseline for performance on both ARM and x64 (ie, scaled this to 1), <em>what is the relative performance of AVX2 when emulated on ARM vs running on x64,</em> and thus what overhead does ARM emulation of AVX2 provide compared to native?</li><li>Using two kinda similar-gen machines, and hand-waving that it's nothing more accurate than that, how well does ARM-native vs x64-emulated vs x64-native perform? <em>Do you need to compile for ARM</em>, or can you get away with letting your apps run under Windows emulation?</li></ul><h3 id="details">Details</h3><p>Performance tests instruct LLVM to build targeting the specific instruction set; are heavily vectorised; and our AVX2 level includes FMA (fused multiply-add.) We use only 256-bit wide AVX2, not the 128-bit wide instructions. Specifics are:</p><ul><li>Normalised to 1.0: CPU <code>x86-64-v2</code> (referred to as 'SSE2-4.x' above, because those are the primary instructions math uses), feature set: <code>+cx16,+popcnt,+sahf,+sse,+sse2,+sse3,+ssse3,+sse4.1,+sse4.2</code></li><li>AVX2+FMA comparison: <code>x86-64-v3</code>, feature set: <code>+cx16,+popcnt,+sahf,+sse,+sse2,+sse3,+ssse3,+sse4.1,+sse4.2,+avx,+avx2,+fma,+bmi,+bmi2,+f16c,+lzcnt,+movbe,+xsave</code></li><li>ARM64: 'generic' ARM64 CPU, no specific feature set</li></ul><div><p>💡</p><div><p>If someone wants to be nerdsniped: an ARM CPU of '' (generic) gives the best performance <em>by far</em> vs instructing LLVM to target a specific CPU, like one of the Windows-compatible should-be-M-series-subset Cortex A8.x or Snapdragon CPUs. On trivial operations like <code>ceil()</code> or <code>floor()</code> for which there should be inbuilt single-op instructions, this can be a 20x difference.</p><p>Why?</p></div></div><p>Each math operation is run on a randomly initialised array of 64-bit doubles as input, in a loop (ie intended to be vectorised), 10 million times. The output is retained and 'used' in order to prevent the loop being optimised away. In IR, we verified the loop exists, is vectorised, and appears to look as expected. Numbers reported are typical timing runs, with no observable difference in cold vs warm runs. Timing is of course only around the tight loop itself, not the prolog or epilog setting the data up or 'using' the results.</p><p>All the 21 results are then scaled as a ratio vs baseline instruction set, x64 v2, and the <a href="https://en.wikipedia.org/wiki/Geometric_mean#Normalized_values">geometric mean</a> calculated to give a single representative number. This means that we can tell how much faster AVX2-level code is vs SSE2-4.x level, both on actual Intel hardware, and under ARM emulation.</p><div><p>💡</p><p>For example, if we get a result (these numbers are fictional examples) that on Intel AVX2 code runs 2x faster than SSE2-4.x, and on ARM it runs 1.6x faster, we could say that under emulation we lose 40% of the real hardware's performance, or that the emulation overhead is such that it allows only 60% of the native hardware's benefit.</p></div><h3 id="test-machines">Test machines</h3><p><strong>x64:</strong> Tiger Lake i7 (2.80 GHz), mobile-class CPU, circa 2021, on Windows 11 Pro 25H2.</p><p><strong>ARM:</strong> Apple M2, circa 2022, macOS Tahoe 26.1 with the ARM version of the same version of Windows 11 Pro (25H2), running on Parallels 26.</p><div><p>💡</p><div><p>These are not directly comparable: they are up to eighteen months apart in release date; despite being laptops Apple's mobile-class CPUs are likely 'better' than Intel's of the same time period, etc. We are regarding them as <em>fairly</em> comparable: both are laptops, both are within 18 months of the same manufacture date, etc.</p><p>So: Technically comparable? No, definitely not: that's another reason to normalise, in order to get quantitatively comparable results. Real-world, in practice, 'people have a computer they bought: do we get at least near the same order of magnitude performance' <em>qualitatively</em> comparable? Ie to answer the 'is emulation enough or is my app losing out by not compiling for ARM?' question? Sure.</p><p>Plus, it's what I had available to test on without pestering too many colleagues to try to find something else. Most of us run Macs, not too many Intels left here. 🫣</p></div></div><h2 id="x8664-avx2-emulation-on-arm-results">x86_64 AVX2 emulation on ARM: Results</h2><p>The following chart scales x64 v2 to 1.0 (grey baseline) and x64 v3 i(ie AVX2+FMA) is relative to that (Intel in green, ARM emulation in blue), that relative number calculated as the geometric mean of the result ratios of x64 v3 vs v2 for 21 common math functions run on 64-bit doubles per the above detailed description.</p><figure><img src="data:image/png;base64,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" alt="Chart showing Win64 v2 normalised to 1.0; Win64 v3 (AVX2) on native Intel is 2.7x faster, but emulated on ARM is 0.66 as fast, ie, two thirds the speed of the older instruction set." loading="lazy"><figcaption>Larger is better (faster)</figcaption></figure><p>Normalised to 1.0, <em>larger is better</em> (faster).</p><p>As expected, using AVX2 on native Intel is significantly faster: 2.7 times faster.</p><p>But when the same code is run on ARM, the AVX2 implementations are notably <em>slower</em> than SSE2-4.x. They are almost exactly 2/3 as performant as emulating the <em>older</em> instruction set.</p><p>This means if your app uses <code>x64 v3</code> with AVX2, and runs on emulated ARM on Windows, following this data, it will run <em>slower</em> than if you restrict it to the <code>x64 v2</code> compilation level.</p><h2 id="why">Why? </h2><p>At the time I wrote the HN comment that started this, I had noticed that we didn't see faster performance using the AVX2 versions of our math function on ARM; what I had not yet measured was such a significant slowness.</p><div><p>💡</p><p>Perception is interesting: before running actual measurements, I could tell it wasn't faster. I guessed it was about the same. I did not realise AVX2 was <em>so much</em> slower.</p></div><div><p>💡</p><p>There was one notable outlier: the emulated version of <code>exp()</code> ran in 2/3 the time of the Intel one, ie was faster emulated. All other operations were noticeably slower.</p></div><p>Some possible reasons are:</p><ul><li>ARM has 128-bit wide NEON operations. AVX2 is using 256-bit wide operations (our code is <em>not</em> using 128-bit widths). This means the emulation code has to handle running two halves; it would be very close to impossible to make that equal performance.<br>This is in my estimation the most likely single reason.</li><li>The Prism AVX2 emulation code is new, compared to that emulating older instruction sets, and may not yet be fully optimized.</li><li>It may optimise for heavily for 32-bit singles, not 64-bit doubles. Our math library focuses on double-precision.</li><li>The <a href="https://learn.microsoft.com/en-us/windows/arm/apps-on-arm-x86-emulation">ARM emulation documentation</a> notes, 'Prism is optimized and tuned specifically for Qualcomm Snapdragon processors. <em>Some performance features within Prism require hardware features only available in the Snapdragon X series...</em>'<br>These tests were on an Apple M2. While I'm sure Microsoft wants to support, or even prioritise, non-Apple hardware, in my view, Windows on Mac (via Parallels) is worth them supporting. But perhaps they don't support it well, yet.</li><li>The emulation code may look for specific patterns that our code does not meet: perhaps, and this is entirely speculation, known (eg) VC++ Intel output, such as common RTL methods, may map to specific ARM64 patterns in the emulator. We are using LLVM, and (probably) rarer math implementations, therefore, less common sequences of operations.<br>I cannot speak to how the emulator is implemented, and if this is even likely. It's a guess.</li><li>Our <code>x64 v3</code> performance seems, at this stage of testing, to be faster than Visual C++'s. (By a significant factor: in <a href="https://blogs.remobjects.com/2026/01/26/fast-math-in-six-languages-what-we-did-and-why-it-works/#win64">this chart</a>, where our AVX2 code at our maximum FP precision came in at 2.7x the baseline, a VC++ 2022 version of the same benchmark compiled in x64 release mode, default FP accuracy, no further changes to default settings came in at 1.3x running on native Intel hardware. <em>Yes, that is a 2x difference, </em>and we attribute this to the VC runtime likely lacking AVX2-optimised math routines; their SSE2 math is slightly faster than ours, gosh dangnum darn it.)<br>Therefore, for AVX2, using our compiler may be unfair compared to using other tools.</li></ul><h3 id="meaning-in-practice">Meaning in practice</h3><p>It is rare to have tight loops of operations: data processing, scientific and engineering software, etc., are the most likely. Even games may push much to the GPU. Therefore, the impact you see on your AVX2 app is unlikely to represent your app running at 2/3 the speed of your AVX2-4.x app. Only parts of it will.</p><p>If you have apps that do real number-crunching, whether that's native or something like Python, you should have the ARM version installed. (You likely have the ARM64 Python wheels installed anyway. But check.)</p><p>Although this checked floating point math, it is likely that it also applies to integer instruction set emulation too.</p><p>Therefore, it's likely worth treating Windows' ARM emulation of AVX2-level support as for <em>support and compatibility,</em> not for equal performance. To get performance, you'll need to compile as ARM.</p><div><p>💡</p><p>One major worry is if your software detects AVX2 (general x64 v3 level) and uses those implementations dynamically, through something like <a href="https://clang.llvm.org/docs/AttributeReference.html#target-clones">target_clones</a>. If so, you may be accidentally hurting your app's performance.</p></div><p>If performance is key: <strong>Yes, it is absolutely key to build your app as ARM</strong><em>,</em> not to rely on Windows ARM emulation.</p><div><p>💡</p><p>Thanks for reading! Check out <a href="https://www.remobjects.com/elements/">Elements</a>, which lets you use C#, Java, Go, Swift, VB and Object Pascal on Windows, Mac and Linux, targeting native i386, x64, and ARM64 (with our shiny new math!) as well as .NET, WASM and the JVM.</p></div>
                </div>


            


        </article>

    </div></div>]]></description>
        </item>
    </channel>
</rss>