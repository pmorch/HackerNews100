<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 15 May 2025 20:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Baby Is Healed with First Personalized Gene-Editing Treatment (139 pts)]]></title>
            <link>https://www.nytimes.com/2025/05/15/health/gene-editing-personalized-rare-disorders.html</link>
            <guid>43997636</guid>
            <pubDate>Thu, 15 May 2025 18:06:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/05/15/health/gene-editing-personalized-rare-disorders.html">https://www.nytimes.com/2025/05/15/health/gene-editing-personalized-rare-disorders.html</a>, See on <a href="https://news.ycombinator.com/item?id=43997636">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/05/15/health/gene-editing-personalized-rare-disorders.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[I Don't Like NumPy (219 pts)]]></title>
            <link>https://dynomight.net/numpy/</link>
            <guid>43996431</guid>
            <pubDate>Thu, 15 May 2025 16:05:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dynomight.net/numpy/">https://dynomight.net/numpy/</a>, See on <a href="https://news.ycombinator.com/item?id=43996431">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
    
  <section>
    <p>They say you can’t truly hate someone unless you loved them first. I don’t know if that’s true as a general principle, but it certainly describes my relationship with NumPy.</p>

<p><a href="https://numpy.org/">NumPy</a>, by the way, is some software that does computations on arrays in Python. It’s insanely popular and has had a huge influence on all the popular machine learning libraries like PyTorch. These libraries share most of the same issues I discuss below, but I’ll stick to NumPy for concreteness.</p>

<p>NumPy makes easy things easy. Say <code>A</code> is a <code>5×5</code> matrix, <code>x</code> is a length-5 vector, and you want to find the vector <em>y</em> such that <code>Ay=x</code>. In NumPy, that would be:</p>

<div><pre><code><span>y</span> <span>=</span> <span>np</span><span>.</span><span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span> <span>x</span><span>)</span>
</code></pre></div>

<p>So elegant! So clear!</p>

<p>But say the situation is even a <em>little</em> more complicated. Say <code>A</code> is a stack of 100 <code>5×5</code> matrices, given as a <code>100×5×5</code> array. And say <code>x</code> is a stack of 100 length-5 vectors, given as a <code>100×5</code> array. And say you want to solve <code>Aᵢyᵢ=xᵢ</code> for <code>1≤i≤100</code>.</p>

<p>If you could use loops, this would be easy:</p>

<div><pre><code><span>y</span> <span>=</span> <span>np</span><span>.</span><span>empty_like</span><span>(</span><span>x</span><span>)</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>100</span><span>):</span>
    <span>y</span><span>[</span><span>i</span><span>,:]</span> <span>=</span> <span>np</span><span>.</span><span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>[</span><span>i</span><span>,:,:],</span> <span>x</span><span>[</span><span>i</span><span>,:])</span>
</code></pre></div>

<p>But you can’t use loops. To some degree, this is a limitation of loops being slow in Python. But nowadays, everything is GPU and if you’ve got big arrays, you probably don’t want to use loops in any language. To get all those transistors firing, you need to call special GPU functions that will sort of split up the arrays into lots of little pieces and process them in parallel.</p>

<p>The good news is that NumPy knows about those special routines (at least if you use <a href="https://github.com/jax-ml/jax">JAX</a> or <a href="https://cupy.dev/">CuPy</a>), and if you call <code>np.linalg.solve</code> correctly, it will use them.</p>

<p>The bad news is that no one knows how do that.</p>

<p>Don’t believe me? OK, which of these is right?</p>

<div><pre><code><span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span><span>x</span><span>)</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span><span>x</span><span>,</span><span>axis</span><span>=</span><span>0</span><span>)</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span><span>x</span><span>,</span><span>axes</span><span>=</span><span>[[</span><span>1</span><span>,</span><span>2</span><span>],</span><span>1</span><span>])</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>.</span><span>T</span><span>,</span> <span>x</span><span>.</span><span>T</span><span>)</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>.</span><span>T</span><span>,</span> <span>x</span><span>).</span><span>T</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span> <span>x</span><span>[</span><span>None</span><span>,:,:])</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span><span>x</span><span>[:,:,</span><span>None</span><span>])</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span><span>x</span><span>[:,:,</span><span>None</span><span>])[:,:,</span><span>0</span><span>]</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>[:,:,:,</span><span>None</span><span>],</span><span>x</span><span>[:,</span><span>None</span><span>,</span><span>None</span><span>,:])</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>.</span><span>transpose</span><span>([</span><span>1</span><span>,</span><span>2</span><span>,</span><span>0</span><span>]),</span><span>x</span><span>[:,:,</span><span>None</span><span>]).</span><span>T</span>
</code></pre></div>

<p>No one knows. And let me show you something else. Here’s the <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html">documentation</a>:</p>

<p><img src="https://dynomight.net/img/numpy/solve.png" alt="np.linalg.solve"></p>

<p>Read that. Meditate on it. Now, notice: You <em>still</em> don’t know how to solve <code>Aᵢyᵢ=xᵢ</code> for all <code>i</code> at once. Is it even possible? Did I lie when I said it was?</p>

<p>As far as I can tell, what people actually do is try random variations until one seems to work.</p>

<h2 id="why-numpy-bad">Why NumPy bad</h2>

<p>NumPy is all about applying operations to arrays. When the arrays have 2 or fewer dimensions, everything is fine. But if you’re doing something even mildly complicated, you inevitably find yourself with some operation you want to apply to some dimensions of array <code>A</code>, some other dimensions of array <code>B</code>, and some <em>other</em> dimensions of array <code>C</code>. And NumPy has no theory for how to express that.</p>

<p>Let me show you what I mean. Suppose:</p>

<ul>
  <li><code>A</code> is a <code>K×L×M</code> array</li>
  <li><code>B</code> is a <code>L×N</code> array</li>
  <li><code>C</code> is a <code>K×M</code> array</li>
</ul>

<p>And say that for each <code>k</code> and <code>n</code>, you’d like to compute the mean over the <code>L</code> and <code>M</code> dimensions. That is, you want</p>

<p>&nbsp;&nbsp;  <code>D<sub>kn</sub> = 1/(LM) × ∑<sub>lm</sub> A<sub>klm</sub> B<sub>ln</sub> C<sub>km</sub>.</code></p>

<p>To do that, you’ve got two options. The first is to use grotesque dimension alignment tricks:</p>

<div><pre><code><span>D</span> <span>=</span> <span>np</span><span>.</span><span>mean</span><span>(</span>
        <span>np</span><span>.</span><span>mean</span><span>(</span>
            <span>A</span><span>[:,:,:,</span><span>None</span><span>]</span> <span>*</span>
            <span>B</span><span>[</span><span>None</span><span>,:,</span><span>None</span><span>,:]</span> <span>*</span>
            <span>C</span><span>[:,</span><span>None</span><span>,:,</span><span>None</span><span>],</span>
        <span>axis</span><span>=</span><span>1</span><span>),</span>
    <span>axis</span><span>=</span><span>1</span><span>)</span>
</code></pre></div>

<p>The hell, you ask? Why is <code>None</code> everywhere? Well, when indexing an array in NumPy, you can write <code>None</code> to insert a new dimension. <code>A</code> is <code>K×L×M</code>, but <code>A[:,:,:,None]</code> is <code>K×L×M×</code><strong><code>1</code></strong>. Similarly, <code>B[None,:,None,:]</code> is <strong><code>1</code></strong><code>×L×</code><strong><code>1</code></strong><code>×N</code> and <code>C[:,None,:,None]</code> is <code>K×</code><strong><code>1</code></strong><code>×M×</code><strong><code>1</code></strong>. When you multiply these together, NumPy “broadcasts” all the size-1 dimensions to give a <code>K×L×M×N</code> array. Then, the <code>np.mean</code> calls average over the <code>L</code> and <code>M</code> dimensions.</p>

<p>I think this is bad. I’ve been using NumPy for years and I still find it impossible to write code like that without <em>always</em> making mistakes.</p>

<p>It’s also borderline-impossible to read. To prove this, I just flipped a coin and introduced a bug above if and only if the coin was tails. Is there a bug? Are you <em>sure</em>? No one knows.</p>

<p>Your second option is to desperately try to be clever. Life is short and precious, but if you spend a lot of yours reading the NumPy documentation, you might eventually realize that there’s a function called <a href="https://numpy.org/doc/stable/reference/generated/numpy.tensordot.html"><code>np.tensordot</code></a>, and that it’s possible to make it do much of the work:</p>

<div><pre><code><span>D</span> <span>=</span> <span>(</span><span>1</span><span>/</span><span>L</span><span>)</span> <span>*</span> <span>np</span><span>.</span><span>mean</span><span>(</span>
                <span>np</span><span>.</span><span>tensordot</span><span>(</span><span>A</span><span>,</span> <span>B</span><span>,</span> <span>axes</span><span>=</span><span>[</span><span>1</span><span>,</span><span>0</span><span>])</span> <span>*</span>
                <span>C</span><span>[:,:,</span><span>None</span><span>],</span>
            <span>axis</span><span>=</span><span>1</span><span>)</span>
</code></pre></div>

<p>That’s correct. (I promise.) But why does it work? What exactly is <code>np.tensordot</code> doing? If you saw that code in some other context, would you have the slightest idea what was happening?</p>

<p>Here’s how I’d do it, if only I could use loops:</p>

<div><pre><code><span>D</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>((</span><span>K</span><span>,</span><span>N</span><span>))</span>  
<span>for</span> <span>k</span> <span>in</span> <span>range</span><span>(</span><span>K</span><span>):</span>  
    <span>for</span> <span>n</span> <span>in</span> <span>range</span><span>(</span><span>N</span><span>):</span>  
        <span>a</span> <span>=</span> <span>A</span><span>[</span><span>k</span><span>,:,:]</span>  
        <span>b</span> <span>=</span> <span>B</span><span>[:,</span><span>n</span><span>]</span>  
        <span>c</span> <span>=</span> <span>C</span><span>[</span><span>k</span><span>,:]</span>  
        <span>assert</span> <span>a</span><span>.</span><span>shape</span> <span>==</span> <span>(</span><span>L</span><span>,</span><span>M</span><span>)</span>  
        <span>assert</span> <span>b</span><span>.</span><span>shape</span> <span>==</span> <span>(</span><span>L</span><span>,)</span>  
        <span>assert</span> <span>c</span><span>.</span><span>shape</span> <span>==</span> <span>(</span><span>M</span><span>,)</span>  
        <span>D</span><span>[</span><span>k</span><span>,</span><span>n</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>mean</span><span>(</span><span>a</span> <span>*</span> <span>b</span><span>[:,</span><span>None</span><span>]</span> <span>*</span> <span>c</span><span>[</span><span>None</span><span>,:])</span>
</code></pre></div>

<p>People who’ve written too much NumPy may find that clunky. I suspect that’s a wee bit of Stockholm Syndrome. But surely we can agree that it’s <em>clear</em>.</p>

<p>In practice, things are often even worse. Say that <code>A</code> had shape <code>M×K×L</code> rather than <code>K×L×M</code>. With loops, no big deal. But NumPy requires you to write monstrosities like <code>A.transpose([1,2,0])</code>. Or should that be <code>A.transpose([2,0,1])</code>? What shapes do those produce? No one knows.</p>

<p>Loops were better.</p>

<h2 id="ok-i-lied">OK I lied</h2>

<p>There is a third option:</p>

<div><pre><code><span>D</span> <span>=</span> <span>1</span><span>/</span><span>(</span><span>L</span><span>*</span><span>M</span><span>)</span> <span>*</span> <span>np</span><span>.</span><span>einsum</span><span>(</span><span>'klm,ln,km-&gt;kn'</span><span>,</span> <span>A</span><span>,</span> <span>B</span><span>,</span> <span>C</span><span>)</span>
</code></pre></div>

<p>If you’ve never seen Einstein summation before, that might look terrifying. But remember, our goal is to find</p>

<p>&nbsp;&nbsp; D<sub>kn</sub> = 1/(LM) × ∑<sub>lm</sub> A<sub>klm</sub> B<sub>ln</sub> C<sub>km</sub>.</p>

<p>The string in the above code basically gives labels to the indices in each of the three inputs (<code>klm,ln,km</code>) and the target indices for the output (<code>-&gt;kn</code>). Then, <a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html"><code>np.einsum</code></a> multiplies together the corresponding elements of the inputs and sums over all indices that aren’t in the output.</p>

<p>Personally, I think <code>np.einsum</code> is one of the rare parts of NumPy that’s actually good. The strings are a bit tedious, but they’re worth it, because the overall function is easy(ish) to understand, is completely explicit, and is quite general and powerful.</p>

<p>Except, how does <code>np.einsum</code> achieve all this? It uses indices. Or, more precisely, it introduces a tiny <em>domain-specific language</em> based on indices. It doesn’t suffer from NumPy’s design flaws because it refuses to play by NumPy’s normal rules.</p>

<p>But <code>np.einsum</code> only does a few things. (<a href="https://github.com/arogozhnikov/einops">Einops</a> does a few more.) What if you want to apply some other function over various dimensions of some arrays? There is no <code>np.linalg.einsolve</code>. And if you create your own function, there’s <em>certainly</em> no “Einstein” version of <em>it</em>.</p>

<p>I think <code>np.einsum</code>’s goodness shows that NumPy went somewhere.</p>

<h2 id="intermission">Intermission</h2>

<p>Here’s a <a href="https://www.nga.gov/artworks/45858-burning-old-south-church-bath-maine">painting</a> which feels analogous to our subject.</p>

<p><img src="https://dynomight.net/img/numpy/burning.jpg" alt=""></p>

<h2 id="where-did-numpy-go-wrong">Where did NumPy go wrong?</h2>

<p>Here’s what I want from an array language. I ain’t particular about syntax, but it would be nice if:</p>

<ol>
  <li>When you want to do something, it’s “obvious” how to do it.</li>
  <li>When you read some code, it’s “obvious” what it does.</li>
</ol>

<p>Wouldn’t that be nice? I think NumPy doesn’t achieve these because of its original sin: It took away indices and replaced them with broadcasting. And broadcasting cannot fill indices’ shoes.</p>

<h2 id="i-dont-love-numpy-broadcasting">I don’t love NumPy broadcasting</h2>

<p>NumPy’s core trick is broadcasting. Take this code:</p>

<div><pre><code><span>A</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([[</span><span>1</span><span>,</span><span>2</span><span>],[</span><span>3</span><span>,</span><span>4</span><span>],[</span><span>5</span><span>,</span><span>6</span><span>]])</span>
<span>B</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>10</span><span>,</span><span>20</span><span>])</span>
<span>C</span> <span>=</span> <span>A</span> <span>*</span> <span>B</span>
<span>print</span><span>(</span><span>C</span><span>)</span>
</code></pre></div>

<p>This outputs:</p>

<div><pre><code>[[ 10  40]
 [ 30  80]
 [ 50 120]]
</code></pre></div>

<p>Here, <code>A</code> is a <code>3×2</code> array, and <code>B</code> is a length-<code>2</code> array. When you multiply them together, <code>B</code> is “broadcast” to the shape of <code>A</code>, meaning the first column of <code>A</code> is multiplied with <code>B[0]=10</code> and the second is multiplied with <code>B[1]=20</code>.</p>

<p>In simple cases, this seems good. But I don’t love it. One reason is that, as we saw above, you often have to do gross things to the dimensions to get them to line up.</p>

<p>Another reason is that it isn’t explicit or legible. Sometimes <code>A*B</code> multiplies element-by-element, and sometimes it does more complicated things. So every time you see <code>A*B</code>, you have to figure out which case in the <a href="https://numpy.org/doc/stable/user/basics.broadcasting.html">broadcasting conventions</a> is getting triggered.</p>

<p>But the real problem with broadcasting is how it infects everything else. I’ll explain below.</p>

<h2 id="i-dont-like-numpy-indexing">I don’t like NumPy indexing</h2>

<p>Here’s a riddle. Take this code:</p>

<div><pre><code><span>A</span> <span>=</span> <span>np</span><span>.</span><span>ones</span><span>((</span><span>10</span><span>,</span><span>20</span><span>,</span><span>30</span><span>,</span><span>40</span><span>))</span>
<span>i</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>1</span><span>,</span><span>2</span><span>,</span><span>3</span><span>])</span>
<span>j</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([[</span><span>0</span><span>],[</span><span>1</span><span>]])</span>
<span>B</span> <span>=</span> <span>A</span><span>[:,</span><span>i</span><span>,</span><span>j</span><span>,:]</span>
</code></pre></div>

<p>What shape does <code>B</code> have?</p>

<p>It turns out the answer is <code>10×2×3×40</code>. That’s because the <code>i</code> and <code>j</code> indices get broadcast to a shape of <code>2×3</code> and then something something mumble mumble mumble. Try to convince yourself it makes sense.</p>

<p>Done? OK, now try these:</p>

<div><pre><code><span>C</span> <span>=</span> <span>A</span><span>[:,:,</span><span>i</span><span>,</span><span>j</span><span>]</span>
<span>D</span> <span>=</span> <span>A</span><span>[:,</span><span>i</span><span>,:,</span><span>j</span><span>]</span>
<span>E</span> <span>=</span> <span>A</span><span>[:,</span><span>1</span><span>:</span><span>4</span><span>,</span><span>j</span><span>,:]</span>
</code></pre></div>

<p>What shapes do these have?</p>

<ul>
  <li>
    <p><code>C</code> is <code>10×20×2×3</code>. This seems logical, given what happened with <code>B</code> above.</p>
  </li>
  <li>
    <p>What about <code>D</code>? It is <code>2×3×10×30</code>. Now, for some reason, the <code>2</code> and <code>3</code> go at the beginning?</p>
  </li>
  <li>
    <p>And what about <code>E</code>? Well, “slices” in Python exclude the endpoint, so <code>1:4</code> is equivalent to <code>[1,2,3]</code> which is equivalent to <code>i</code>, and so <code>E</code> is the same as <code>B</code>. Hahaha, just kidding! <code>E</code> is <code>10×3×2×1×40</code>.</p>
  </li>
</ul>

<p>Yes, that is what happens. Try it if you don’t believe me! I understand why NumPy does this, because I’ve absorbed <a href="https://numpy.org/doc/stable/user/basics.indexing.html">this 5000 word document</a> that explains how NumPy indexing works. But I want that time back.</p>

<details>
  <summary>
For fun, I tried asking a bunch of AI models to figure out what shapes those arrays have. Here were the results:
</summary>

  <p>I used this query:</p>

  <blockquote>
    <p>Take this python code</p>

    <p>A = np.ones((10,20,30,40))<br>
i = np.array([1,2,3])<br>
j = np.array([[0],[1]])<br>
B = A[:,i,j,:]<br>
C = A[:,:,i,j]<br>
D = A[:,i,:,j]<br>
E = A[:,1:4,j,:]</p>

    <p>what shapes do B, C, D, and E have?</p>
  </blockquote>

  <p>Claude 3.7 used “extended thinking”. Here are all the incorrect outputs:</p>

  <table>
    <thead>
      <tr>
        <th>AI</th>
        <th><code>B</code></th>
        <th><code>C</code></th>
        <th><code>D</code></th>
        <th><code>E</code></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>GPT 4.1</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>10×2×3×30</td>
        <td>&nbsp;</td>
      </tr>
      <tr>
        <td>Grok 3</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>10×3×30×2</td>
        <td>10×3×2×40</td>
      </tr>
      <tr>
        <td>Claude 3 Opus</td>
        <td>10×3×2×30</td>
        <td>10×20×3×2</td>
        <td>10×3×30×2</td>
        <td>10×3×2×40</td>
      </tr>
      <tr>
        <td>Llama 4 Maverick</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>10×3×30×2</td>
        <td>10×3×2×40</td>
      </tr>
      <tr>
        <td>o3</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>10×2×3×30</td>
        <td>&nbsp;</td>
      </tr>
      <tr>
        <td>Claude 3.7</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>10×3×30×2</td>
        <td>10×3×2×40</td>
      </tr>
    </tbody>
  </table>

</details>

<table>
  <thead>
    <tr>
      <th>AI</th>
      <th><code>B</code></th>
      <th><code>C</code></th>
      <th><code>D</code></th>
      <th><code>E</code></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GPT 4.1</td>
      <td>✔️</td>
      <td>✔️</td>
      <td>X</td>
      <td>✔️</td>
    </tr>
    <tr>
      <td>Grok 3</td>
      <td>✔️</td>
      <td>✔️</td>
      <td>X</td>
      <td>X</td>
    </tr>
    <tr>
      <td>Claude 3 Opus</td>
      <td>X</td>
      <td>X</td>
      <td>X</td>
      <td>X</td>
    </tr>
    <tr>
      <td>Llama 4 Maverick</td>
      <td>✔️</td>
      <td>✔️</td>
      <td>X</td>
      <td>X</td>
    </tr>
    <tr>
      <td>o3</td>
      <td>✔️</td>
      <td>✔️</td>
      <td>X</td>
      <td>✔️</td>
    </tr>
    <tr>
      <td>Claude 3.7</td>
      <td>✔️</td>
      <td>✔️</td>
      <td>X</td>
      <td>X</td>
    </tr>
    <tr>
      <td>Gemini 2.5 Pro</td>
      <td>✔️</td>
      <td>✔️</td>
      <td>✔️</td>
      <td>✔️</td>
    </tr>
    <tr>
      <td>DeepSeek R1</td>
      <td>✔️</td>
      <td>✔️</td>
      <td>✔️</td>
      <td>✔️</td>
    </tr>
  </tbody>
</table>

<p>(DeepSeek’s chain of thought used “wait” 76 times. It got everything right the first time, but when I tried it again, it somehow got <code>B</code>, <code>C</code>, and <code>D</code> all wrong, but <code>E</code> right.)</p>

<p>This is insane. Using basic features should not require solving crazy logic puzzles.</p>

<p>You might think, “OK, I’ll just limit myself to indexing in simple ways.” Sounds good, except sometimes you <em>need</em> advanced indexing. And even if you’re doing something simple, you still need to be careful to avoid the crazy cases.</p>

<p>This again makes everything non-legible. Even if you’re just reading code that uses indexing in a simple way, how do you <em>know</em> it’s simple? If you see <code>A[B,C]</code>, that could be doing almost anything. To understand it, you need to remember the shapes of <code>A</code>, <code>B</code>, and <code>C</code> and work through all the cases. And, of course, <code>A</code>, <code>B</code>, and <code>C</code> are often produced by <em>other</em> code, which you <em>also</em> need to think about…</p>

<h2 id="i-dont-like-numpy-functions">I don’t like NumPy functions</h2>

<p>Why did NumPy end up with a <code>np.linalg.solve(A,B)</code> function that’s so confusing? I imagine they first made it work when <code>A</code> is a 2D array and and <code>b</code> is a 1D or 2D array, just like the mathematical notation of <code>A⁻¹b</code> or <code>A⁻¹B</code>.</p>

<p>So far so good. But then someone probably came along with a 3D array.  If you could use loops, the solution would be “use the old function with loops”. But you can’t use loops. So there were basically three options:</p>

<ol>
  <li>They could add some extra <code>axes</code> argument, so the user can specify which dimensions to operate over. Maybe you could write <code>solve(A,B,axes=[[1,2],1])</code>.</li>
  <li>They could create different  functions with different names for different situations. Maybe <code>solve_matrix_vector</code> would do one thing, <code>solve_tensor_matrix</code> would do another.</li>
  <li>They could add a Convention: Some arbitrary choice for how <code>solve</code> will internally try to line up the dimensions. Then it’s the user’s problem to figure out and conform to those Conventions.</li>
</ol>

<p>All these options are bad, because none of them can really cope with the fact that there are a combinatorial number of different cases. NumPy chose: All of them. Some functions have <code>axes</code> arguments. Some have different versions with different names. Some have Conventions. Some have Conventions <em>and</em> <code>axes</code> arguments. And some don’t provide any vectorized version at all.</p>

<p>But the <em>biggest</em> flaw of NumPy is this: Say you create a function that solves some problem with arrays of some given shape. Now, how do you apply it to particular dimensions of some larger arrays? The answer is: You re-write your function from scratch in a much more complex way. The basic principle of programming is abstraction—solving simple problems and then using the solutions as building blocks for more complex problems. NumPy doesn’t let you do that.</p>

<h2 id="attention-please">Attention please</h2>

<p>One last example to show you what I’m talking about. Whenever I whine about NumPy, people always want to see an example with <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">self-attention</a>, the core trick behind modern language models. So fine. Here’s an implementation, which I humbly suggest is better than all 227 versions I found when I searched for “self-attention numpy”:</p>

<div><pre><code><span># self attention by your friend dynomight
</span>
<span>input_dim</span> <span>=</span> <span>4</span>  
<span>seq_len</span> <span>=</span> <span>4</span>  
<span>d_k</span> <span>=</span> <span>5</span>  
<span>d_v</span> <span>=</span> <span>input_dim</span>  
  
<span>X</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>seq_len</span><span>,</span> <span>input_dim</span><span>)</span>  
<span>W_q</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>input_dim</span><span>,</span> <span>d_k</span><span>)</span>  
<span>W_k</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>input_dim</span><span>,</span> <span>d_k</span><span>)</span>  
<span>W_v</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>input_dim</span><span>,</span> <span>d_v</span><span>)</span>  
  
<span>def</span> <span>softmax</span><span>(</span><span>x</span><span>,</span> <span>axis</span><span>):</span>  
    <span>e_x</span> <span>=</span> <span>np</span><span>.</span><span>exp</span><span>(</span><span>x</span> <span>-</span> <span>np</span><span>.</span><span>max</span><span>(</span><span>x</span><span>,</span> <span>axis</span><span>=</span><span>axis</span><span>,</span> <span>keepdims</span><span>=</span><span>True</span><span>))</span>  
    <span>return</span> <span>e_x</span> <span>/</span> <span>np</span><span>.</span><span>sum</span><span>(</span><span>e_x</span><span>,</span> <span>axis</span><span>=</span><span>axis</span><span>,</span> <span>keepdims</span><span>=</span><span>True</span><span>)</span>  
  
<span>def</span> <span>attention</span><span>(</span><span>X</span><span>,</span> <span>W_q</span><span>,</span> <span>W_k</span><span>,</span> <span>W_v</span><span>):</span>  
    <span>d_k</span> <span>=</span> <span>W_k</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>]</span>  
    <span>Q</span> <span>=</span> <span>X</span> <span>@</span> <span>W_q</span>  
    <span>K</span> <span>=</span> <span>X</span> <span>@</span> <span>W_k</span>  
    <span>V</span> <span>=</span> <span>X</span> <span>@</span> <span>W_v</span>  
    <span>scores</span> <span>=</span> <span>Q</span> <span>@</span> <span>K</span><span>.</span><span>T</span> <span>/</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>d_k</span><span>)</span>  
    <span>attention_weights</span> <span>=</span> <span>softmax</span><span>(</span><span>scores</span><span>,</span> <span>axis</span><span>=-</span><span>1</span><span>)</span>  
    <span>return</span> <span>attention_weights</span> <span>@</span> <span>V</span>  
</code></pre></div>

<p>This is fine. Some of the <code>axis</code> stuff is a little obscure, but whatever.</p>

<p>But what language models really need is <em>multi-head</em> attention, where you sort of do attention several times in parallel and then merge the results. How do we do that?</p>

<p>First, let’s imagine we lived in a sane world where we were allowed to use abstractions. Then you could just call the previous function in a loop:</p>

<div><pre><code><span># multi-head self attention by your friend dynomight
# if only we could use loops
</span>
<span>n_head</span> <span>=</span> <span>2</span>
  
<span>X</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>seq_len</span><span>,</span> <span>input_dim</span><span>)</span>  
<span>W_q</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>n_head</span><span>,</span> <span>input_dim</span><span>,</span> <span>d_k</span><span>)</span>  
<span>W_k</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>n_head</span><span>,</span> <span>input_dim</span><span>,</span> <span>d_k</span><span>)</span>  
<span>W_v</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>n_head</span><span>,</span> <span>input_dim</span><span>,</span> <span>d_v</span><span>)</span>  
<span>W_o</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>n_head</span><span>,</span> <span>d_v</span><span>,</span> <span>input_dim</span> <span>//</span> <span>n_head</span><span>)</span>

<span>def</span> <span>multi_head_attention</span><span>(</span><span>X</span><span>,</span> <span>W_q</span><span>,</span> <span>W_k</span><span>,</span> <span>W_v</span><span>,</span> <span>W_o</span><span>):</span>  
    <span>projected</span> <span>=</span> <span>[]</span>  
    <span>for</span> <span>n</span> <span>in</span> <span>range</span><span>(</span><span>n_head</span><span>):</span>  
        <span>output</span> <span>=</span> <span>attention</span><span>(</span><span>X</span><span>,</span> <span>W_q</span><span>[</span><span>n</span><span>,:,:],</span> <span>W_k</span><span>[</span><span>n</span><span>,:,:],</span> <span>W_v</span><span>[</span><span>n</span><span>,:,:])</span>  
        <span>my_proj</span> <span>=</span> <span>output</span> <span>@</span> <span>W_o</span><span>[</span><span>n</span><span>,:,:]</span>  
        <span>projected</span><span>.</span><span>append</span><span>(</span><span>my_proj</span><span>)</span>  
    <span>projected</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>projected</span><span>)</span>  
  
    <span>output</span> <span>=</span> <span>[]</span>  
    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>seq_len</span><span>):</span>  
        <span>my_output</span> <span>=</span> <span>np</span><span>.</span><span>ravel</span><span>(</span><span>projected</span><span>[:,</span><span>i</span><span>,:])</span>  
        <span>output</span><span>.</span><span>append</span><span>(</span><span>my_output</span><span>)</span>  
    <span>return</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>final</span><span>)</span>  
</code></pre></div>

<p>Looks stupid, right? Yes—thank you! Cleverness is bad.</p>

<p>But we don’t live in a sane world. So instead you need to do this:</p>

<div><pre><code><span># multi-head self attention by your friend dynomight
# all vectorized and bewildering
</span>
<span>def</span> <span>multi_head_attention</span><span>(</span><span>X</span><span>,</span> <span>W_q</span><span>,</span> <span>W_k</span><span>,</span> <span>W_v</span><span>,</span> <span>W_o</span><span>):</span>  
    <span>d_k</span> <span>=</span> <span>W_k</span><span>.</span><span>shape</span><span>[</span><span>-</span><span>1</span><span>]</span>  
    <span>Q</span> <span>=</span> <span>np</span><span>.</span><span>einsum</span><span>(</span><span>'si,hij-&gt;hsj'</span><span>,</span> <span>X</span><span>,</span> <span>W_q</span><span>)</span>  
    <span>K</span> <span>=</span> <span>np</span><span>.</span><span>einsum</span><span>(</span><span>'si,hik-&gt;hsk'</span><span>,</span> <span>X</span><span>,</span> <span>W_k</span><span>)</span>  
    <span>V</span> <span>=</span> <span>np</span><span>.</span><span>einsum</span><span>(</span><span>'si,hiv-&gt;hsv'</span><span>,</span> <span>X</span><span>,</span> <span>W_v</span><span>)</span>  
    <span>scores</span> <span>=</span> <span>Q</span> <span>@</span> <span>K</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>)</span> <span>/</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>d_k</span><span>)</span>  
    <span>weights</span> <span>=</span> <span>softmax</span><span>(</span><span>scores</span><span>,</span> <span>axis</span><span>=-</span><span>1</span><span>)</span>
    <span>output</span> <span>=</span> <span>weights</span> <span>@</span> <span>V</span>  
    <span>projected</span> <span>=</span> <span>np</span><span>.</span><span>einsum</span><span>(</span><span>'hsv,hvd-&gt;hsd'</span><span>,</span> <span>output</span><span>,</span> <span>W_o</span><span>)</span>  
    <span>return</span> <span>projected</span><span>.</span><span>transpose</span><span>(</span><span>1</span><span>,</span> <span>0</span><span>,</span> <span>2</span><span>).</span><span>reshape</span><span>(</span><span>seq_len</span><span>,</span> <span>input_dim</span><span>)</span>  
</code></pre></div>

<p>Ha! Hahahahahaha!</p>

<h2 id="so-what-then">So what then?</h2>

<p>To be clear, I’m only suggesting that NumPy is “the worst array language other than all the other array languages”. What’s the point of complaining if I don’t have something better to suggest?</p>

<p>Well, actually I do have something better to suggest. I’ve made a prototype of a “better” NumPy that I think retains all the power while eliminating all the sharp edges. I thought this would just be a short motivational introduction, but after I started writing, the evil took hold of me and here we are 3000 words later.</p>

<p>Also, it’s probably wise to keep some distance between one’s raving polemics and one’s constructive array language API proposals. So I’ll cover my new thing next time.</p>

  </section>
  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Coinbase says hackers bribed staff to steal customer data, demanding $20M ransom (191 pts)]]></title>
            <link>https://www.cnbc.com/2025/05/15/coinbase-says-hackers-bribed-staff-to-steal-customer-data-and-are-demanding-20-million-ransom.html</link>
            <guid>43996307</guid>
            <pubDate>Thu, 15 May 2025 15:52:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/05/15/coinbase-says-hackers-bribed-staff-to-steal-customer-data-and-are-demanding-20-million-ransom.html">https://www.cnbc.com/2025/05/15/coinbase-says-hackers-bribed-staff-to-steal-customer-data-and-are-demanding-20-million-ransom.html</a>, See on <a href="https://news.ycombinator.com/item?id=43996307">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-107132326" data-test="InlineImage"><p>Jakub Porzycki | Nurphoto | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/COIN/">Coinbase</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> on Thursday reported that cybercriminals bribed overseas support agents to steal customer data to use in social engineering attacks. The incident may cost Coinbase up to $400 million to fix, the company estimated.</p><p>The crypto exchange operator received an email on May 11 from someone claiming they obtained information about certain Coinbase customer accounts as well as other internal Coinbase documentation, including materials relating to customer service and account management systems, Coinbase reported in a <a href="https://www.sec.gov/ix?doc=/Archives/edgar/data/0001679788/000167978825000094/coin-20250514.htm" target="_blank">Securities and Exchange Commission filing</a>.</p><p>The company's shares were down more than 6% in morning trading.</p><p>The email demanded money in exchange for not publicly disclosing the information, but Coinbase says it has not paid the demand and is cooperating with law enforcement on the investigation of the incident.</p><p>Although passwords and private keys were not compromised, affected data included sensitive data such as names, addresses, phone numbers and emails; masked bank account numbers and identifiers as well as the last four digits of Social Security numbers; government ID images and account balances, the company said.</p><p>"Cyber criminals bribed and recruited a group of rogue overseas support agents to steal Coinbase customer data to facilitate social engineering attacks," the company said in a <a href="https://www.coinbase.com/blog/protecting-our-customers-standing-up-to-extortionists" target="_blank">blog post</a>. "These insiders abused their access to customer support systems to steal the account data for a small subset of customers. No passwords, private keys, or funds were exposed and Coinbase Prime accounts are untouched. We will reimburse customers who were tricked into sending funds to the attacker."</p><p>Coinbase had detected the breach independently in previous months, per the filing. It immediately terminated the employees involved, warned customers whose information may have been accessed and enhanced its fraud monitoring protections.</p><p>The threat actor paid overseas contractors and employees in support rolls to obtain the information, it said.</p><p>"We're cooperating closely with law enforcement to pursue the harshest penalties possible and will not pay the $20 million ransom demand we received," the company said in the blog. "Instead we are establishing a $20 million reward fund for information leading to the arrest and conviction of the criminals responsible for this attack."</p><p>Coinbase operates the largest crypto exchange in the U.S. In the past week it <a href="https://www.cnbc.com/2025/05/08/coinbase-acquires-crypto-derivatives-exchange-deribit-for-2point9-billion.html">announced an acquisition</a> that is expected to help it expand its global reach and gained <a href="https://www.cnbc.com/2025/05/13/coinbase-stock-enters-sp-500-a-watershed-moment-for-crypto-industry-analysts-say.html">entry to the benchmark S&amp;P 500</a> stock index, which will take effect next week. On the earnings call last week, CEO Brian Armstrong discussed his ambition to make Coinbase "the <a href="https://www.cnbc.com/2025/05/11/coinbase-aims-to-be-worlds-number-1-financial-service-app-in-10-years.html">No. 1 financial services app</a> in the world" in the next five to 10 years.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[California sent residents' personal health data to LinkedIn (145 pts)]]></title>
            <link>https://themarkup.org/pixel-hunt/2025/04/28/how-california-sent-residents-personal-health-data-to-linkedin</link>
            <guid>43995302</guid>
            <pubDate>Thu, 15 May 2025 14:13:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://themarkup.org/pixel-hunt/2025/04/28/how-california-sent-residents-personal-health-data-to-linkedin">https://themarkup.org/pixel-hunt/2025/04/28/how-california-sent-residents-personal-health-data-to-linkedin</a>, See on <a href="https://news.ycombinator.com/item?id=43995302">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><em>The Markup, now a part of CalMatters, uses investigative reporting, data analysis, and software engineering to challenge technology to serve the public good. Sign up for </em><a href="https://mrkup.org/XvjZS"><em>Klaxon</em></a><em>, a newsletter that delivers our stories and tools directly to your inbox.</em></p>
<!-- no template found for topper block -->




<p>The website that lets Californians shop for health insurance under the Affordable Care Act, coveredca.com, has been sending sensitive data to  LinkedIn, forensic testing by The Markup has revealed.&nbsp;</p>



<p>As visitors filled out forms on the website, trackers on the same pages told LinkedIn their answers to questions about whether they were blind, pregnant, or used a high number of prescription medications. The trackers also monitored whether the visitors said they were transgender or possible victims of domestic abuse. </p>





<p>Covered California, the organization that operates the website, removed the trackers as The Markup and CalMatters reported this article. The organization said they were removed “due to a marketing agency transition” in early April.&nbsp;</p>



<p>In a statement, Kelly Donohue, a spokesperson for the agency, confirmed that data was sent to LinkedIn as part of an advertising campaign. Since&nbsp; being informed of the tracking, “all active advertising-related tags across our website have been turned off out of an abundance of caution,” she added.&nbsp;</p>



<p>“Covered California has initiated a review of our websites and information security and privacy protocols to ensure that no analytics tools are impermissibly sharing sensitive consumer information,” Donohue said, adding that they would “share additional findings as they become available, taking any necessary steps to safeguard the security and privacy of consumer data.”</p>





<div>
	
	<ol>
		
			<li>
				<figure>
					
					<div data-fullscreen-desktop="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-interface.png" data-fullscreen-mobile="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-interface.png"><picture>
  <source media="(min-width: 491px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-interface-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-interface.png 2x">
  <source media="(max-width: 490px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-interface-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-interface.png 3x">
  <img src="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-interface-360x443.png" alt="Screenshot of the interface for coveredca.com displaying a respondent’s selection for being pregnant">
</picture></div>
				</figure>
			</li>
		
			<li>
				<figure>
					
					<div data-fullscreen-desktop="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-backend.png" data-fullscreen-mobile="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-backend.png"><picture>
  <source media="(min-width: 491px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-backend-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-backend.png 2x">
  <source media="(max-width: 490px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-backend-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-backend.png 3x">
  <img src="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-backend-360x443.png" alt="Screenshot highlighting the line of code where coveredca.com shares the respondent’s pregnancy status with LinkedIn">
</picture></div>
				</figure>
			</li>
		
	</ol>
	<p>
		When an individual indicated they were pregnant, the information was sent to LinkedIn via the Insight Tag.

&nbsp;
	</p>
	<p><span>Credit:</span>
		
	</p>
</div>


<p>Visitors who filled out health information on the site may have had their data tracked for more than a year, according to Donohue, who said the LinkedIn campaign began in February 2024.&nbsp;</p>





<p>The Markup observed the trackers directly in February and March of this year. It confirmed most ad trackers, including the Meta “pixel” tracker, as well as all third-party cookies, have been removed from the site as of April 21.&nbsp;</p>



<p>Since 2014, more than 50 million Americans have <a href="https://home.treasury.gov/news/press-releases/jy2567" target="_blank">signed up</a> for health insurance through state exchanges like Covered California. They were set up under the <a href="https://calmatters.org/tag/affordable-care-act/" target="_blank">Affordable Care Act</a>, signed into law by President Barack Obama 15 years ago. States can either operate their exchange websites in partnership with the federal government or independently, <a href="https://www.commonwealthfund.org/publications/maps-and-interactives/aca-state-marketplace-models-and-key-policy-decisions" target="_blank">as California does</a>.&nbsp;</p>



<p>Covered California operates as an independent entity within the state government. Its <a href="https://board.coveredca.com/" target="_blank">board</a> is appointed by the governor and Legislature.&nbsp;</p>



<p>In March, <a href="https://www.coveredca.com/newsroom/news-releases/2025/03/24/with-record-high-enrollment-covered-california-celebrates-the-15th-anniversary-of-the-historic-affordable-care-act/" target="_blank">Covered California announced</a> that, after four years of increasing enrollment, a record of nearly 2 million people were covered by health insurance through the program. In all, the organization said, about one in six Californians were at one point enrolled through Covered California. Between 2014 and 2023, the uninsured rate fell from 17.2% to 6.4%, according to the organization, the largest drop of any state during that time period. This <a href="https://calmatters.org/explainers/california-health-care-coverage/" target="_blank">coincided with a series of eligibility </a>expansions to <a href="https://calmatters.org/tag/medi-cal/" target="_blank">Medi-Cal</a>, the state’s health insurance program for lower-income households.</p>



  



<p>Experts expressed alarm at the idea that those millions of people could have had sensitive health data sent to a private company without their knowledge or consent. Sara Geoghegan, senior counsel at the Electronic Privacy Information Center, said it was “concerning and invasive” for a health insurance website to be sending data that was “wholly irrelevant” to the uses of a for-profit company like LinkedIn.</p>



<p>“It’s unfortunate,” she said, “because people don’t expect that their health information will be collected and used in this way.”</p>


<div>
	
	<p><a href="#the-linkedin-insight-tag">↩︎ link</a></p><h2 id="the-linkedin-insight-tag">The LinkedIn Insight&nbsp;Tag</h2>
</div>


<p>The Markup and CalMatters in recent months scanned for trackers on hundreds of California state and county government websites that offer services for undocumented immigrants using <a href="https://themarkup.org/blacklight">Blacklight</a>, an automated tool developed by The Markup for auditing website trackers.&nbsp;</p>



<figure><blockquote><p>People don’t expect that their health information will be collected and used in this way.</p><cite>Sara Geoghegan, senior counsel at the Electronic Privacy Information Center</cite></blockquote></figure>



<p>The Markup found that Covered California had more than 60 trackers on its site. Out of more than 200 of the government sites, the average number of trackers on the sites was three. Covered California had dozens more than any other website we examined.&nbsp;</p>



<p>On coveredca.com, trackers from well-known social media firms like Meta collected information on visitor page views, while lesser-known analytics and media campaign companies like email marketing company LiveIntent also followed users across the site.&nbsp;</p>



<p>But by far the most sensitive information was transmitted to LinkedIn.&nbsp;</p>



<p>While some of the data sent to LinkedIn was relatively innocuous, such as what pages were visited, Covered California also sent the company detailed information when visitors selected doctors to see if they were covered by a plan, including their specialization. The site also told LinkedIn if someone searched for a specific hospital.</p>





<div>
	
	<ol>
		
			<li>
				<figure>
					
					<div data-fullscreen-desktop="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-interface.png" data-fullscreen-mobile="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-interface.png"><picture>
  <source media="(min-width: 491px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-interface-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-interface.png 2x">
  <source media="(max-width: 490px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-interface-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-interface.png 3x">
  <img src="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-interface-360x443.png" alt="Screenshot of the interface for coveredca.com displaying a medical provider with their redacted first name and address">
</picture></div>
				</figure>
			</li>
		
			<li>
				<figure>
					
					<div data-fullscreen-desktop="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-tracker.png" data-fullscreen-mobile="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-tracker.png"><picture>
  <source media="(min-width: 491px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-tracker-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-tracker.png 2x">
  <source media="(max-width: 490px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-tracker-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-tracker.png 3x">
  <img src="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-tracker-360x443.png" alt="Screenshot highlighting the line of code where coveredca.com shares the respondent’s medical provider with LinkedIn">
</picture></div>
				</figure>
			</li>
		
	</ol>
	<p>
		When an individual selected a medical provider, the information was sent to LinkedIn via the Insight Tag.
	</p>
	<p><span>Credit:</span>
		
	</p>
</div>


<p>&nbsp;In addition to demographic information including gender, the site also shared details with LinkedIn when visitors selected their ethnicity and marital status, and when they told coveredca.com how often they saw doctors for surgery or outpatient treatment.&nbsp;</p>





<div>
	
	<ol>
		
			<li>
				<figure>
					
					<div data-fullscreen-desktop="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-interface.png" data-fullscreen-mobile="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-interface.png"><picture>
  <source media="(min-width: 491px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-interface-360x442.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-interface.png 2x">
  <source media="(max-width: 490px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-interface-360x442.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-interface.png 3x">
  <img src="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-interface-360x442.png" alt="Screenshot of the interface for coveredca.com displaying a respondent’s selection for being of Hispanic, Latino or Spanish origin">
</picture></div>
				</figure>
			</li>
		
			<li>
				<figure>
					
					<div data-fullscreen-desktop="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-tracker.png" data-fullscreen-mobile="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-tracker.png"><picture>
  <source media="(min-width: 491px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-tracker-360x442.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-tracker.png 2x">
  <source media="(max-width: 490px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-tracker-360x442.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-tracker.png 3x">
  <img src="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-tracker-360x442.png" alt="Screenshot highlighting the line of code where coveredca.com shares the respondent’s ethnicity with LinkedIn">
</picture></div>
				</figure>
			</li>
		
	</ol>
	<p>
		When an individual selected their ethnicity, the information was sent to LinkedIn via the Insight Tag.
	</p>
	<p><span>Credit:</span>
		
	</p>
</div>


<p>LinkedIn, like other large social media firms, offers a way for websites to easily transmit data on their visitors through a tracking tool that the sites can place on their pages. In LinkedIn’s case, this <a href="https://www.linkedin.com/help/lms/answer/a418880" target="_blank">tool is called the Insight Tag</a>. By using the tag, businesses and other organizations can <a href="https://www.linkedin.com/help/lms/answer/a427660" target="_blank">later target advertisements</a> on LinkedIn to consumers that have already shown interest in their products or services. For an e-commerce site, a tracker on a page might be able to note when someone added a product to their cart, and the business can then send ads for that product to the same person on their social media feeds.&nbsp;</p>



<p>A health care marketplace like Covered California might use the trackers to reach a group of people who might be interested in a reminder of a deadline for open health insurance enrollment, for example.</p>



<p>In its statement, Covered California noted the usefulness of these tools, saying the organization “leverages LinkedIn’s advertising platform tools to understand consumer behavior and deliver tailored messages to help them make informed decisions about their health care options.”</p>





<div>
	
	<ol>
		
			<li>
				<figure>
					
					<div data-fullscreen-desktop="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-interface.png" data-fullscreen-mobile="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-interface.png"><picture>
  <source media="(min-width: 491px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-interface-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-interface.png 2x">
  <source media="(max-width: 490px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-interface-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-interface.png 3x">
  <img src="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-interface-360x443.png" alt="Screenshot of the interface for coveredca.com displaying a respondent’s selection for being a victim of domestic abuse or spousal abandonment">
</picture></div>
				</figure>
			</li>
		
			<li>
				<figure>
					
					<div data-fullscreen-desktop="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-tracker.png" data-fullscreen-mobile="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-tracker.png"><picture>
  <source media="(min-width: 491px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-tracker-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-tracker.png 2x">
  <source media="(max-width: 490px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-tracker-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-tracker.png 3x">
  <img src="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-tracker-360x443.png" alt="Screenshot highlighting the line of code where coveredca.com shares the respondent’s data about domestic abuse with LinkedIn">
</picture></div>
				</figure>
			</li>
		
	</ol>
	<p>
		When an individual indicated they were a victim of domestic abuse or spousal abandonment, the information was sent to LinkedIn via the Insight Tag.
	</p>
	<p><span>Credit:</span>
		
	</p>
</div>


<p>Trackers can also be valuable to the social media companies that offer them. In addition to driving ad sales, they provide an opportunity to gather information on visitors to websites other than their own.</p>



<p>On <a href="https://www.linkedin.com/help/lms/answer/a418880/add-the-linkedin-insight-tag-to-your-website?lang=en" target="_blank">its informational page</a> about the Insight Tag, LinkedIn places the burden on websites that employ the tag not to use it in risky situations. The tag “should not be installed on web pages that collect or contain Sensitive Data,” the page advises, including “pages offering specific health-related or financial services or products to consumers.”</p>



<p>LinkedIn spokesperson Brionna Ruff said in an emailed statement, “Our Ads Agreement and documentation expressly prohibit customers from installing the Insight Tag on web pages that collect or contain sensitive data, including pages offering health-related services. We don’t allow advertisers to target ads based on sensitive data or categories.”</p>





<p>Collection of sensitive information by social media trackers has in previous instances led to removal of the trackers, lawsuits, and scrutiny by state and federal lawmakers.</p>





<p>For example, after The Markup in 2022 <a href="https://themarkup.org/pixel-hunt/2022/04/28/applied-for-student-aid-online-facebook-saw-you">revealed the Department of Education sent personal information to Facebook</a> when students applied for college financial aid online, the department turned off the sharing, faced <a href="https://themarkup.org/pixel-hunt/2022/05/11/lawmakers-question-education-department-about-facebook-student-aid-tracking-after-markup-investigation">questions</a> from two members of Congress, and was <a href="https://themarkup.org/pixel-hunt/2024/07/26/department-of-education-sued-following-markup-investigation-into-fafsa-data-shared-with-facebook">sued by two advocacy groups</a> who sought more information about the sharing. Other stories in the same <a href="https://themarkup.org/series/pixel-hunt">series about trackers, known as the Pixel Hunt</a>, also led to changes and blowback, including a <a href="https://themarkup.org/pixel-hunt/2024/04/19/ftc-cracks-down-on-telehealth-addiction-service-monument-for-sharing-health-data">crackdown by the Federal Trade Commission</a> on telehealth companies transmitting personal information to companies including Meta and Google without user consent and proposed class action lawsuits over information shared through trackers with <a href="https://themarkup.org/hello-world/2023/09/30/our-pixel-hunt-project-keeps-paying-dividends">drug stores, health providers</a>, and <a href="https://themarkup.org/pixel-hunt/2022/12/02/meta-sued-for-collecting-financial-information-through-tax-filing-websites">tax prep companies</a>.</p>



<p>LinkedIn is already facing multiple proposed class-action lawsuits related to the collection of medical information. In October, <a href="https://www.bankinfosecurity.com/lawsuits-accuse-linkedin-tracking-users-health-info-a-26668" target="_blank">three new lawsuits in California courts alleged</a> that LinkedIn violated users’ privacy by collecting information on medical appointment sites, including for a fertility clinic.&nbsp;</p>



<p>Social media companies’ tracking practices have underpinned the tremendous growth of the tech industry, but few web users are aware of how far the tracking goes. “This absolutely contradicts the expectation of the average consumer,” Geoghegan said.&nbsp;</p>



<p>In California, a law called the California Confidentiality of Medical Information Act governs the privacy of medical information in the state. Under the act, consumers must give permission to some organizations before their medical information is disclosed to third parties. Companies have faced litigation under the law for using web tracking technologies, although those suits have <a href="https://www.hklaw.com/en/news/pressreleases/2024/08/holland-knight-wins-dismissal-of-meta-pixel-data-privacy-class-action" target="_blank">not always been successful</a>.&nbsp;</p>



<p>Geoghegan said current protections like these don’t go far enough in helping consumers protect their sensitive data.&nbsp;</p>



<p>“This is an exact example of why we need better protections,” she said of LinkedIn receiving the data. “This is sensitive health information that consumers expect to be protected and a lack of regulations is failing us.”</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Tiny Boltzmann Machine (192 pts)]]></title>
            <link>https://eoinmurray.info/boltzmann-machine</link>
            <guid>43995005</guid>
            <pubDate>Thu, 15 May 2025 13:41:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eoinmurray.info/boltzmann-machine">https://eoinmurray.info/boltzmann-machine</a>, See on <a href="https://news.ycombinator.com/item?id=43995005">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Boltzmann Machines</p><p>Here we introduce introduction to Boltzmann machines and
present a Tiny Restricted Boltzmann Machine that runs in the browser.</p><a href="#trainer"><span>Skip to Simulator</span></a></div><div><div><h2 id="boltzmann-machines-are-one-of-the-earliest-generative-ai-models-introduced-in-the-1980s">Boltzmann Machines are one of the earliest generative AI models, introduced in the 1980s.</h2><p>Boltzmann Machines are used for unsupervised learning, which means they can learn
from data without being told what to look for.</p><p>The can be used for generating new data that is similar to the data they were trained on, also known as generative AI.</p></div><p><img alt="Boltzmann Machine" loading="lazy" width="600" height="600" decoding="async" data-nimg="1" srcset="https://eoinmurray.info/_next/image?url=%2Fboltzmann-machine%2Fpaper.png&amp;w=640&amp;q=75 1x, https://eoinmurray.info/_next/image?url=%2Fboltzmann-machine%2Fpaper.png&amp;w=1200&amp;q=75 2x" src="https://eoinmurray.info/_next/image?url=%2Fboltzmann-machine%2Fpaper.png&amp;w=1200&amp;q=75"></p></div><div><div><h2 id="boltzmann-machine">Boltzmann Machine</h2><p>A Boltzmann Machine is a type of neural network that tries to learn patterns by mimicking
how energy works in physics.</p><p>Each neuron can be on or off, the machine is made up of many of these neurons connect to each other.</p><p>Some neurons are <span></span> visible (we can see them and even set their state), and some are <span></span> hidden (we can't see them).</p><p>The connections between neurons are called weights, and they can be <span></span> positive or <span></span> negative.</p></div><div><svg width="400" height="400"><g><circle cx="200" cy="360" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="364" font-size="8" fill="#000" text-anchor="middle">v0</text></g><g><circle cx="145.27677706789302" cy="350.35081932574536" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="145.27677706789302" y="354.35081932574536" font-size="8" fill="#000" text-anchor="middle">v1</text></g><g><circle cx="97.1539824501537" cy="322.5671108990365" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="97.1539824501537" y="326.5671108990365" font-size="8" fill="#000" text-anchor="middle">v2</text></g><g><circle cx="61.43593539448983" cy="280.00000000000006" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="61.43593539448983" y="284.00000000000006" font-size="8" fill="#000" text-anchor="middle">v3</text></g><g><circle cx="42.43075951804673" cy="227.78370842670884" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="42.43075951804673" y="231.78370842670884" font-size="8" fill="#000" text-anchor="middle">v4</text></g><g><circle cx="42.4307595180467" cy="172.2162915732912" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="42.4307595180467" y="176.2162915732912" font-size="8" fill="#000" text-anchor="middle">v5</text></g><g><circle cx="61.435935394489775" cy="120.00000000000004" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="61.435935394489775" y="124.00000000000004" font-size="8" fill="#000" text-anchor="middle">v6</text></g><g><circle cx="97.15398245015368" cy="77.43288910096354" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="97.15398245015368" y="81.43288910096354" font-size="8" fill="#000" text-anchor="middle">v7</text></g><g><circle cx="145.27677706789302" cy="49.64918067425464" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="145.27677706789302" y="53.64918067425464" font-size="8" fill="#000" text-anchor="middle">v8</text></g><g><circle cx="199.99999999999997" cy="40" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="199.99999999999997" y="44" font-size="8" fill="#000" text-anchor="middle">v9</text></g><g><circle cx="200" cy="40" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="44" font-size="8" fill="#000" text-anchor="middle">h0</text></g><g><circle cx="254.723222932107" cy="49.649180674254666" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="254.723222932107" y="53.649180674254666" font-size="8" fill="#000" text-anchor="middle">h1</text></g><g><circle cx="302.8460175498463" cy="77.43288910096352" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="302.8460175498463" y="81.43288910096352" font-size="8" fill="#000" text-anchor="middle">h2</text></g><g><circle cx="338.56406460551017" cy="120" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="338.56406460551017" y="124" font-size="8" fill="#000" text-anchor="middle">h3</text></g><g><circle cx="357.5692404819533" cy="172.21629157329113" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="357.5692404819533" y="176.21629157329113" font-size="8" fill="#000" text-anchor="middle">h4</text></g><g><circle cx="357.5692404819533" cy="227.78370842670887" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="357.5692404819533" y="231.78370842670887" font-size="8" fill="#000" text-anchor="middle">h5</text></g><g><circle cx="338.5640646055102" cy="280" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="338.5640646055102" y="284" font-size="8" fill="#000" text-anchor="middle">h6</text></g><g><circle cx="302.84601754984635" cy="322.5671108990365" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="302.84601754984635" y="326.5671108990365" font-size="8" fill="#000" text-anchor="middle">h7</text></g><g><circle cx="254.723222932107" cy="350.3508193257453" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="254.723222932107" y="354.3508193257453" font-size="8" fill="#000" text-anchor="middle">h8</text></g><g><circle cx="200" cy="360" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="364" font-size="8" fill="#000" text-anchor="middle">h9</text></g></svg><p>Hover over the neurons to highlight their connections.</p></div></div><div><div><p>General Boltzmann Machine</p><svg width="400" height="400"><g><circle cx="200" cy="360" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="364" font-size="8" fill="#000" text-anchor="middle">v0</text></g><g><circle cx="145.27677706789302" cy="350.35081932574536" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="145.27677706789302" y="354.35081932574536" font-size="8" fill="#000" text-anchor="middle">v1</text></g><g><circle cx="97.1539824501537" cy="322.5671108990365" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="97.1539824501537" y="326.5671108990365" font-size="8" fill="#000" text-anchor="middle">v2</text></g><g><circle cx="61.43593539448983" cy="280.00000000000006" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="61.43593539448983" y="284.00000000000006" font-size="8" fill="#000" text-anchor="middle">v3</text></g><g><circle cx="42.43075951804673" cy="227.78370842670884" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="42.43075951804673" y="231.78370842670884" font-size="8" fill="#000" text-anchor="middle">v4</text></g><g><circle cx="42.4307595180467" cy="172.2162915732912" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="42.4307595180467" y="176.2162915732912" font-size="8" fill="#000" text-anchor="middle">v5</text></g><g><circle cx="61.435935394489775" cy="120.00000000000004" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="61.435935394489775" y="124.00000000000004" font-size="8" fill="#000" text-anchor="middle">v6</text></g><g><circle cx="97.15398245015368" cy="77.43288910096354" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="97.15398245015368" y="81.43288910096354" font-size="8" fill="#000" text-anchor="middle">v7</text></g><g><circle cx="145.27677706789302" cy="49.64918067425464" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="145.27677706789302" y="53.64918067425464" font-size="8" fill="#000" text-anchor="middle">v8</text></g><g><circle cx="199.99999999999997" cy="40" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="199.99999999999997" y="44" font-size="8" fill="#000" text-anchor="middle">v9</text></g><g><circle cx="200" cy="40" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="44" font-size="8" fill="#000" text-anchor="middle">h0</text></g><g><circle cx="254.723222932107" cy="49.649180674254666" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="254.723222932107" y="53.649180674254666" font-size="8" fill="#000" text-anchor="middle">h1</text></g><g><circle cx="302.8460175498463" cy="77.43288910096352" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="302.8460175498463" y="81.43288910096352" font-size="8" fill="#000" text-anchor="middle">h2</text></g><g><circle cx="338.56406460551017" cy="120" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="338.56406460551017" y="124" font-size="8" fill="#000" text-anchor="middle">h3</text></g><g><circle cx="357.5692404819533" cy="172.21629157329113" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="357.5692404819533" y="176.21629157329113" font-size="8" fill="#000" text-anchor="middle">h4</text></g><g><circle cx="357.5692404819533" cy="227.78370842670887" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="357.5692404819533" y="231.78370842670887" font-size="8" fill="#000" text-anchor="middle">h5</text></g><g><circle cx="338.5640646055102" cy="280" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="338.5640646055102" y="284" font-size="8" fill="#000" text-anchor="middle">h6</text></g><g><circle cx="302.84601754984635" cy="322.5671108990365" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="302.84601754984635" y="326.5671108990365" font-size="8" fill="#000" text-anchor="middle">h7</text></g><g><circle cx="254.723222932107" cy="350.3508193257453" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="254.723222932107" y="354.3508193257453" font-size="8" fill="#000" text-anchor="middle">h8</text></g><g><circle cx="200" cy="360" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="364" font-size="8" fill="#000" text-anchor="middle">h9</text></g></svg><p>Hover over the neurons to highlight their connections.</p><p>A General Boltzmann Machine has connections between all neurons. This makes it powerful, but its training involves calculating an <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mn>2</mn><mi>n</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(2^n)</annotation></semantics></math></span></span> term.</p></div><div><p>Restricted Boltzmann Machine</p><svg width="400" height="400"><g><circle cx="50" cy="36.36363636363637" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="40.36363636363637" font-size="8" fill="#000" text-anchor="middle">v0</text></g><g><circle cx="50" cy="72.72727272727273" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="76.72727272727273" font-size="8" fill="#000" text-anchor="middle">v1</text></g><g><circle cx="50" cy="109.0909090909091" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="113.0909090909091" font-size="8" fill="#000" text-anchor="middle">v2</text></g><g><circle cx="50" cy="145.45454545454547" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="149.45454545454547" font-size="8" fill="#000" text-anchor="middle">v3</text></g><g><circle cx="50" cy="181.81818181818184" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="185.81818181818184" font-size="8" fill="#000" text-anchor="middle">v4</text></g><g><circle cx="50" cy="218.1818181818182" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="222.1818181818182" font-size="8" fill="#000" text-anchor="middle">v5</text></g><g><circle cx="50" cy="254.54545454545456" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="258.54545454545456" font-size="8" fill="#000" text-anchor="middle">v6</text></g><g><circle cx="50" cy="290.90909090909093" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="294.90909090909093" font-size="8" fill="#000" text-anchor="middle">v7</text></g><g><circle cx="50" cy="327.2727272727273" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="331.2727272727273" font-size="8" fill="#000" text-anchor="middle">v8</text></g><g><circle cx="50" cy="363.6363636363637" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="367.6363636363637" font-size="8" fill="#000" text-anchor="middle">v9</text></g><g><circle cx="350" cy="36.36363636363637" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="40.36363636363637" font-size="8" fill="#000" text-anchor="middle">h0</text></g><g><circle cx="350" cy="72.72727272727273" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="76.72727272727273" font-size="8" fill="#000" text-anchor="middle">h1</text></g><g><circle cx="350" cy="109.0909090909091" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="113.0909090909091" font-size="8" fill="#000" text-anchor="middle">h2</text></g><g><circle cx="350" cy="145.45454545454547" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="149.45454545454547" font-size="8" fill="#000" text-anchor="middle">h3</text></g><g><circle cx="350" cy="181.81818181818184" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="185.81818181818184" font-size="8" fill="#000" text-anchor="middle">h4</text></g><g><circle cx="350" cy="218.1818181818182" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="222.1818181818182" font-size="8" fill="#000" text-anchor="middle">h5</text></g><g><circle cx="350" cy="254.54545454545456" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="258.54545454545456" font-size="8" fill="#000" text-anchor="middle">h6</text></g><g><circle cx="350" cy="290.90909090909093" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="294.90909090909093" font-size="8" fill="#000" text-anchor="middle">h7</text></g><g><circle cx="350" cy="327.2727272727273" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="331.2727272727273" font-size="8" fill="#000" text-anchor="middle">h8</text></g><g><circle cx="350" cy="363.6363636363637" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="367.6363636363637" font-size="8" fill="#000" text-anchor="middle">h9</text></g></svg><p>A Restricted Boltzmann Machine is a special case where the visible and hidden neurons are not connected to each other. This makes it faster to train and understand.</p></div></div><div><h2 id="a-boltzmann-machine-is-an-energy-based-model">A Boltzmann Machine is an energy based model.</h2><p>The energy of a configuration of the visible and hidden units is defined as:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><mo>−</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>b</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub><mo>−</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>c</mi><mi>j</mi></msub><msub><mi>h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">  E(v,h) = -\sum_{i=1}^{m} \sum_{j=1}^{n} w_{ij} v_i h_j - \sum_{i=1}^{m} b_i v_i - \sum_{j=1}^{n} c_j h_j</annotation></semantics></math></span></span></span></p><p>where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span> is the visible layer, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span></span> is the hidden layer, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span></span> is the weight matrix, and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span></span> are the biases for the visible
and hidden layers, respectively.</p><p>The visualisation on the right randomises the weights, biases and activation values of a Boltzmann machine and calculates its energy.</p></div><div><div><h2 id="training-and-generation">Training and Generation</h2><p>During training it is given examples (e.g., images, text) and the machine adjusts its
weights to lower the energy of those samples.</p><p>It effectively learns <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(v)</annotation></semantics></math></span></span>, the probability of visible units <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span>, which
is proportional to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mrow><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">e^{-E(v)}</annotation></semantics></math></span></span>.</p><p>After training, it can sample new data from the learned distribution using Gibbs sampling.</p><p>These samples are new, never-before-seen, but statistically similar to the training data.</p></div><div><p>Here is our training data.</p><p>We want the network to learn how to make similar samples to these.</p></div></div><div id="trainer"><div><p>Lets simulate one step at a time</p><p>A Restricted Boltzmann Machine (RBM) is trained using a process called Contrastive Divergence. The steps are as follows:</p><div><ol><li><span>Step 1:</span><span> <!-- -->Clamping visible units to data</span></li><li><span>Step 2:</span><span> Sampling hidden units</span></li><li><span>Step 3:</span><span> Sampling visible units</span></li><li><span>Step 4:</span><span> Sampling hidden units</span></li><li><span>Step 5:</span><span> Updating weights</span></li></ol></div><div><p>A more formal description of the steps above are given in the</p><!-- --> <p><a href="#appendix">Appendix</a>.</p></div></div><div><div><h2>Input Sample</h2></div><p><svg width="400" height="400"><rect x="25" y="47.72727272727272" width="350" height="344.5454545454545" stroke="black" stroke-width="2" fill="none" rx="10"></rect><g><circle cx="50" cy="72.72727272727272" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="76.72727272727272" font-size="12" fill="#000" text-anchor="middle">v0</text></g><g><circle cx="50" cy="105.45454545454545" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="109.45454545454545" font-size="12" fill="#000" text-anchor="middle">v1</text></g><g><circle cx="50" cy="138.1818181818182" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="142.1818181818182" font-size="12" fill="#000" text-anchor="middle">v2</text></g><g><circle cx="50" cy="170.9090909090909" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="174.9090909090909" font-size="12" fill="#000" text-anchor="middle">v3</text></g><g><circle cx="50" cy="203.63636363636363" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="207.63636363636363" font-size="12" fill="#000" text-anchor="middle">v4</text></g><g><circle cx="50" cy="236.36363636363637" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="240.36363636363637" font-size="12" fill="#000" text-anchor="middle">v5</text></g><g><circle cx="50" cy="269.0909090909091" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="273.0909090909091" font-size="12" fill="#000" text-anchor="middle">v6</text></g><g><circle cx="50" cy="301.8181818181818" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="305.8181818181818" font-size="12" fill="#000" text-anchor="middle">v7</text></g><g><circle cx="50" cy="334.54545454545456" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="338.54545454545456" font-size="12" fill="#000" text-anchor="middle">v8</text></g><g><circle cx="50" cy="367.27272727272725" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="371.27272727272725" font-size="12" fill="#000" text-anchor="middle">v9</text></g><g><circle cx="350" cy="72.72727272727272" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="76.72727272727272" font-size="12" fill="#000" text-anchor="middle">h0</text></g><g><circle cx="350" cy="105.45454545454545" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="109.45454545454545" font-size="12" fill="#000" text-anchor="middle">h1</text></g><g><circle cx="350" cy="138.1818181818182" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="142.1818181818182" font-size="12" fill="#000" text-anchor="middle">h2</text></g><g><circle cx="350" cy="170.9090909090909" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="174.9090909090909" font-size="12" fill="#000" text-anchor="middle">h3</text></g><g><circle cx="350" cy="203.63636363636363" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="207.63636363636363" font-size="12" fill="#000" text-anchor="middle">h4</text></g><g><circle cx="350" cy="236.36363636363637" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="240.36363636363637" font-size="12" fill="#000" text-anchor="middle">h5</text></g><g><circle cx="350" cy="269.0909090909091" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="273.0909090909091" font-size="12" fill="#000" text-anchor="middle">h6</text></g><g><circle cx="350" cy="301.8181818181818" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="305.8181818181818" font-size="12" fill="#000" text-anchor="middle">h7</text></g><g><circle cx="350" cy="334.54545454545456" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="338.54545454545456" font-size="12" fill="#000" text-anchor="middle">h8</text></g><g><circle cx="350" cy="367.27272727272725" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="371.27272727272725" font-size="12" fill="#000" text-anchor="middle">h9</text></g></svg></p></div></div><div id="trainer"><div><p>Simulator</p><p>Press the "Run Simulation" button to start traininng the RBM. If you let the simulation run for a while, you will see the weights of the RBM converge to a stable state. The energy loss will also decrease over time.</p><p><strong>You can compare the input and output states of the RBM by pausing the simulation</strong>.</p><p>In the beginning, the input and output states will be dissimilar. As the simulation progresses, the input and output states will become more similar.</p></div><div><div><div><h2>Input Sample</h2></div><p><svg width="400" height="400"><g><circle cx="50" cy="72.72727272727272" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="76.72727272727272" font-size="12" fill="#000" text-anchor="middle">v0</text></g><g><circle cx="50" cy="105.45454545454545" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="109.45454545454545" font-size="12" fill="#000" text-anchor="middle">v1</text></g><g><circle cx="50" cy="138.1818181818182" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="142.1818181818182" font-size="12" fill="#000" text-anchor="middle">v2</text></g><g><circle cx="50" cy="170.9090909090909" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="174.9090909090909" font-size="12" fill="#000" text-anchor="middle">v3</text></g><g><circle cx="50" cy="203.63636363636363" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="207.63636363636363" font-size="12" fill="#000" text-anchor="middle">v4</text></g><g><circle cx="50" cy="236.36363636363637" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="240.36363636363637" font-size="12" fill="#000" text-anchor="middle">v5</text></g><g><circle cx="50" cy="269.0909090909091" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="273.0909090909091" font-size="12" fill="#000" text-anchor="middle">v6</text></g><g><circle cx="50" cy="301.8181818181818" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="305.8181818181818" font-size="12" fill="#000" text-anchor="middle">v7</text></g><g><circle cx="50" cy="334.54545454545456" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="338.54545454545456" font-size="12" fill="#000" text-anchor="middle">v8</text></g><g><circle cx="50" cy="367.27272727272725" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="371.27272727272725" font-size="12" fill="#000" text-anchor="middle">v9</text></g><g><circle cx="350" cy="72.72727272727272" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="76.72727272727272" font-size="12" fill="#000" text-anchor="middle">h0</text></g><g><circle cx="350" cy="105.45454545454545" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="109.45454545454545" font-size="12" fill="#000" text-anchor="middle">h1</text></g><g><circle cx="350" cy="138.1818181818182" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="142.1818181818182" font-size="12" fill="#000" text-anchor="middle">h2</text></g><g><circle cx="350" cy="170.9090909090909" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="174.9090909090909" font-size="12" fill="#000" text-anchor="middle">h3</text></g><g><circle cx="350" cy="203.63636363636363" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="207.63636363636363" font-size="12" fill="#000" text-anchor="middle">h4</text></g><g><circle cx="350" cy="236.36363636363637" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="240.36363636363637" font-size="12" fill="#000" text-anchor="middle">h5</text></g><g><circle cx="350" cy="269.0909090909091" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="273.0909090909091" font-size="12" fill="#000" text-anchor="middle">h6</text></g><g><circle cx="350" cy="301.8181818181818" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="305.8181818181818" font-size="12" fill="#000" text-anchor="middle">h7</text></g><g><circle cx="350" cy="334.54545454545456" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="338.54545454545456" font-size="12" fill="#000" text-anchor="middle">h8</text></g><g><circle cx="350" cy="367.27272727272725" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="371.27272727272725" font-size="12" fill="#000" text-anchor="middle">h9</text></g></svg></p><div><h2>Output</h2></div></div><div><div><div><h2>Reconstruction Accuracy</h2></div><div><h2>Energy</h2></div></div><div><h2>Weights</h2></div></div></div></div><div id="appendix"><div><p>Appendix: Contrastive Divergence</p><p>Starting with a Boltzmann machine as defined earlier, we
want to derivce the contrastive divergence algorithm for training.
The goal is to adjust the weights of the network to minimize the energy of the training data.</p><p>We have:</p><ul>
<li>A visisble layer <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span> and a hidden layer <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span></span>.</li>
<li>A weight matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span></span> that connects the visible and hidden layers.</li>
<li>A bias vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span></span> for the visible layer and a bias vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span></span> for the hidden layer.</li>
</ul><p>Energy function in matrix form:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><mo>−</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>b</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub><mo>−</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>c</mi><mi>j</mi></msub><msub><mi>h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">E(v,h) = -\sum_{i=1}^{m} \sum_{j=1}^{n} w_{ij} v_i h_j - \sum_{i=1}^{m} b_i v_i - \sum_{j=1}^{n} c_j h_j</annotation></semantics></math></span></span></span></p><p>Joint distribution:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>Z</mi></mfrac><msup><mi>e</mi><mrow><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">P(v,h) = \frac{1}{Z} e^{-E(v,h)}</annotation></semantics></math></span></span></span></p><p>Where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z</annotation></semantics></math></span></span> is the partition function, which normalizes the distribution.</p></div><div><p>We train the RBM by maximizing the likelihood of the training data, i.e. maximizing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>log</mtext><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{log}(P(v))</annotation></semantics></math></span></span>.</p><p>The marginal likelihood of the visible layer is given by:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mi>h</mi></munder><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(v) = \sum_{h} P(v,h)</annotation></semantics></math></span></span></span></p><p>Then the log-likelihood is:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>log</mtext><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mtext>log</mtext><munder><mo>∑</mo><mi>h</mi></munder><mfrac><mn>1</mn><mi>Z</mi></mfrac><msup><mi>e</mi><mrow><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mtext>log</mtext><munder><mo>∑</mo><mi>h</mi></munder><msup><mi>e</mi><mrow><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></msup><mo>−</mo><mtext>log</mtext><mo stretchy="false">(</mo><mi>Z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{log}(P(v)) = \text{log}\sum_{h} \frac{1}{Z} e^{-E(v,h)} = \text{log}\sum_{h} e^{-E(v,h)} - \text{log}(Z)</annotation></semantics></math></span></span></span></p><p>Differentiating with respect to the weights <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{ij}</annotation></semantics></math></span></span> gives:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><munder><mo>∑</mo><mi mathvariant="bold">h</mi></munder><msup><mi>e</mi><mrow><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><munder><mo>∑</mo><mi mathvariant="bold">h</mi></munder><mrow><mo fence="true">(</mo><mo>−</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>E</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mo fence="true">)</mo></mrow><msup><mi>e</mi><mrow><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><mo>−</mo><mfrac><mn>1</mn><mi>Z</mi></mfrac><munder><mo>∑</mo><mrow><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi></mrow></munder><mrow><mo fence="true">(</mo><mo>−</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>E</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mo fence="true">)</mo></mrow><msup><mi>e</mi><mrow><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
\frac{\partial \log P(\mathbf{v})}{\partial w_{ij}} 
&amp;= \frac{1}{\sum_{\mathbf{h}} e^{-E(\mathbf{v}, \mathbf{h})}} 
\sum_{\mathbf{h}} \left( -\frac{\partial E(\mathbf{v}, \mathbf{h})}{\partial w_{ij}} \right) e^{-E(\mathbf{v}, \mathbf{h})} \\
&amp;\quad - \frac{1}{Z} \sum_{\mathbf{v}, \mathbf{h}} \left( -\frac{\partial E(\mathbf{v}, \mathbf{h})}{\partial w_{ij}} \right) e^{-E(\mathbf{v}, \mathbf{h})}
\end{align*}</annotation></semantics></math></span></span></span></p><p>Similar forms exist for the biases <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_i</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">c_j</annotation></semantics></math></span></span>.</p><p>Since we are performing gradient ascent, therefore.</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>←</mo><mi mathvariant="normal">Δ</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><mi>η</mi><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\Delta w_{ij} \leftarrow \Delta w_{ij} + \eta \frac{\partial \log P(v)}{\partial w_{ij}}</annotation></semantics></math></span></span></span></p><p>Therefore we get our weight update rule:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>η</mi><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mo>=</mo><mi>η</mi><mrow><mo fence="true">(</mo><mo stretchy="false">⟨</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>−</mo><mo stretchy="false">⟨</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta w_{ij} = \eta \frac{\partial \log P(v)}{\partial w_{ij}} = \eta \left( \langle v_i h_j \rangle_{data} - \langle v_i h_j \rangle_{model} \right)</annotation></semantics></math></span></span></span></p></div><div><p>A similar process can be followed for the biases <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_i</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">c_j</annotation></semantics></math></span></span>.</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>b</mi><mi>i</mi></msub><mo>=</mo><mi>η</mi><mrow><mo fence="true">(</mo><mo stretchy="false">⟨</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>−</mo><mo stretchy="false">⟨</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta b_i = \eta \left( \langle v_i \rangle_{data} - \langle v_i \rangle_{model} \right)</annotation></semantics></math></span></span></span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>c</mi><mi>j</mi></msub><mo>=</mo><mi>η</mi><mrow><mo fence="true">(</mo><mo stretchy="false">⟨</mo><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>−</mo><mo stretchy="false">⟨</mo><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta c_j = \eta \left( \langle h_j \rangle_{data} - \langle h_j \rangle_{model} \right)</annotation></semantics></math></span></span></span></p><p>Where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><mo>⋅</mo><msub><mo stretchy="false">⟩</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\langle \cdot \rangle_{data}</annotation></semantics></math></span></span> is the expectation with respect to the training data and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><mo>⋅</mo><msub><mo stretchy="false">⟩</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\langle \cdot \rangle_{model}</annotation></semantics></math></span></span> is the expectation with respect to the model distribution.</p><p>The next step is to approximate the model expectation using Gibbs sampling.</p><ol>
<li>Positive phase: Sample <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">h</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msup><mo>≈</mo><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="bold">h</mi><mi mathvariant="normal">∣</mi><msup><mi mathvariant="bold">v</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mtext>data</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(0)} \approx P(\mathbf{h}|\mathbf{v}^{(0)} = \text{data})</annotation></semantics></math></span></span></li>
<li>Negative phase: Run k steps of Gibbs sampling:</li>
</ol><ul>
<li>Alternating between <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>≈</mo><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">(</mo><mi>h</mi><msup><mo stretchy="false">)</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{v}^{(t+1)} \approx P(\mathbf{v}|\mathbf(h)^{(t)})</annotation></semantics></math></span></span></li>
<li>and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">h</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>≈</mo><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="bold">h</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">(</mo><mi>v</mi><msup><mo stretchy="false">)</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(t+1)} \approx P(\mathbf{h}|\mathbf(v)^{(t)})</annotation></semantics></math></span></span></li>
</ul><p>Once those steps are done we update the weights and biases according to:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>η</mi><mrow><mo fence="true">(</mo><mo stretchy="false">⟨</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>−</mo><mo stretchy="false">⟨</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta w_{ij} = \eta \left( \langle v_i h_j \rangle_{data} - \langle v_i h_j \rangle_{model} \right)</annotation></semantics></math></span></span></span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>b</mi><mi>i</mi></msub><mo>=</mo><mi>η</mi><mrow><mo fence="true">(</mo><mo stretchy="false">⟨</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>−</mo><mo stretchy="false">⟨</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta b_i = \eta \left( \langle v_i \rangle_{data} - \langle v_i \rangle_{model} \right)</annotation></semantics></math></span></span></span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>c</mi><mi>j</mi></msub><mo>=</mo><mi>η</mi><mrow><mo fence="true">(</mo><mo stretchy="false">⟨</mo><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>−</mo><mo stretchy="false">⟨</mo><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta c_j = \eta \left( \langle h_j \rangle_{data} - \langle h_j \rangle_{model} \right)</annotation></semantics></math></span></span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Min.js style compression of tech docs for LLM context (129 pts)]]></title>
            <link>https://github.com/marv1nnnnn/llm-min.txt</link>
            <guid>43994987</guid>
            <pubDate>Thu, 15 May 2025 13:40:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/marv1nnnnn/llm-min.txt">https://github.com/marv1nnnnn/llm-min.txt</a>, See on <a href="https://news.ycombinator.com/item?id=43994987">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">llm-min.txt: Min.js Style Compression of Tech Docs for LLM Context 🤖</h2><a id="user-content-llm-mintxt-minjs-style-compression-of-tech-docs-for-llm-context-" aria-label="Permalink: llm-min.txt: Min.js Style Compression of Tech Docs for LLM Context 🤖" href="#llm-mintxt-minjs-style-compression-of-tech-docs-for-llm-context-"></a></p>
<p dir="auto"><a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a>
<a href="https://www.python.org/downloads/" rel="nofollow"><img src="https://camo.githubusercontent.com/afa6acb8b16dba41da2aaa15991ecc9e600d28c7f905f66c32b0030df20ce925/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e31302532422d626c7565" alt="Python Version" data-canonical-src="https://img.shields.io/badge/Python-3.10%2B-blue"></a>
<a href="https://console.cloud.google.com/apis/api/gemini.googleapis.com/overview?project=llm-min" rel="nofollow"><img src="https://camo.githubusercontent.com/8e6e26ccbea21fab3467d671fb3229f2104e8e0ba589f73a07e40804f3063bce/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f47656d696e692d4150492d677265656e" alt="Gemini API" data-canonical-src="https://img.shields.io/badge/Gemini-API-green"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">📜 Table of Contents</h2><a id="user-content--table-of-contents" aria-label="Permalink: 📜 Table of Contents" href="#-table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#llm-mintxt-minjs-style-compression-of-tech-docs-for-llm-context-">llm-min.txt: Min.js Style Compression of Tech Docs for LLM Context 🤖</a>
<ul dir="auto">
<li><a href="#-table-of-contents">📜 Table of Contents</a></li>
<li><a href="#what-is-llm-mintxt-and-why-is-it-important">What is <code>llm-min.txt</code> and Why is it Important?</a></li>
<li><a href="#understanding-llm-mintxt-a-machine-optimized-format-">Understanding <code>llm-min.txt</code>: A Machine-Optimized Format 🧩</a></li>
<li><a href="#does-it-really-work-visualizing-the-impact">Does it Really Work? Visualizing the Impact</a></li>
<li><a href="#its-necessary-to-make-a-benchmark-but-incredibly-hard-llm-code-generation-is-stochastic-and-the-quality-of-the-generated-code-depends-on-many-factors-crawl4ai--google-genai--svelte-are-all-packages-current-llm-failed-to-generate-correct-code-for-using-llm-min-will-largely-improve-the-success-rate-of-code-generation">It's necessary to make a benchmark but incredibly hard. LLM code generation is stochastic and the quality of the generated code depends on many factors. crawl4ai / google-genai / svelte are all packages current LLM failed to generate correct code for. Using <code>llm-min</code> will largely improve the success rate of code generation.</a></li>
<li><a href="#quick-start-">Quick Start 🚀</a></li>
<li><a href="#output-directory-structure-">Output Directory Structure 📂</a></li>
<li><a href="#choosing-the-right-ai-model-why-gemini-">Choosing the Right AI Model (Why Gemini) 🧠</a></li>
<li><a href="#how-it-works-a-look-inside-srcllm_min-%EF%B8%8F">How it Works: A Look Inside (src/llm_min) ⚙️</a></li>
<li><a href="#whats-next-future-plans-">What's Next? Future Plans 🔮</a></li>
<li><a href="#common-questions-faq-">Common Questions (FAQ) ❓</a></li>
<li><a href="#want-to-help-contributing-">Want to Help? Contributing 🤝</a></li>
<li><a href="#license-">License 📜</a></li>
</ul>
</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is <code>llm-min.txt</code> and Why is it Important?</h2><a id="user-content-what-is-llm-mintxt-and-why-is-it-important" aria-label="Permalink: What is llm-min.txt and Why is it Important?" href="#what-is-llm-mintxt-and-why-is-it-important"></a></p>
<p dir="auto">If you've ever used an AI coding assistant (like GitHub Copilot, Cursor, or others powered by Large Language Models - LLMs), you've likely encountered situations where they don't know about the latest updates to programming libraries. This knowledge gap exists because AI models have a "knowledge cutoff" – a point beyond which they haven't learned new information. Since software evolves rapidly, this limitation can lead to outdated recommendations and broken code.</p>
<p dir="auto">Several innovative approaches have emerged to address this challenge:</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://llmstxt.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/cee972bc70b97903b0c5bfb4a5c07d2cf76521237ffae1166df120505060f09a/68747470733a2f2f6c6c6d737478742e6f72672f6c6f676f2e706e67" alt="llms.txt logo" width="60" data-canonical-src="https://llmstxt.org/logo.png"></a> <a href="https://llmstxt.org/" rel="nofollow">llms.txt</a>
A community-driven initiative where contributors create reference files (<code>llms.txt</code>) containing up-to-date library information specifically formatted for AI consumption.</p>
</li>
<li>
<p dir="auto"><a href="https://context7.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/48f83838d8c895d18f477e9d5fb7bb10f90cd920a763d5101a1e13e73aff4070/68747470733a2f2f656e637279707465642d74626e302e677374617469632e636f6d2f696d616765733f713d74626e3a414e6439476352625075774b4e647545414242443567415a4f5f4153397a30467955416d6c37326a33672673" alt="Context7 logo" width="60" data-canonical-src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRbPuwKNduEABBD5gAZO_AS9z0FyUAml72j3g&amp;s"></a> <a href="https://context7.com/" rel="nofollow">Context7</a>
A service that dynamically provides contextual information to AIs, often by intelligently summarizing documentation.</p>
</li>
</ul>
<p dir="auto">While these solutions are valuable, they face certain limitations:</p>
<ul dir="auto">
<li>
<p dir="auto"><code>llms.txt</code> files can become extraordinarily large – some exceeding <strong>800,000</strong> tokens (word fragments). This size can overwhelm many AI systems' context windows.</p>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/marv1nnnnn/llm-min.txt/blob/main/assets/token.png"><img src="https://github.com/marv1nnnnn/llm-min.txt/raw/main/assets/token.png" alt="Token comparison for llms.txt" width="500"></a>
<p dir="auto">Many shorter <code>llms.txt</code> variants simply contain links to official documentation, requiring the AI to fetch and process those documents separately. Even the comprehensive versions (<code>llms-full.txt</code>) often exceed what most AI assistants can process at once. Additionally, these files may not always reflect the absolute latest documentation.</p>
</li>
<li>
<p dir="auto"><code>Context7</code> operates somewhat as a "black box" – while useful, its precise information selection methodology isn't fully transparent to users. It primarily works with GitHub code repositories or existing <code>llms.txt</code> files, rather than any arbitrary software package.</p>
</li>
</ul>
<p dir="auto"><strong><code>llm-min.txt</code> offers a fresh approach:</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/marv1nnnnn/llm-min.txt/blob/main/assets/icon.png"><img src="https://github.com/marv1nnnnn/llm-min.txt/raw/main/assets/icon.png" alt="llm-min.txt icon" width="300"></a></p>
<p dir="auto">Inspired by <code>min.js</code> files in web development (JavaScript with unnecessary elements removed), <code>llm-min.txt</code> adopts a similar philosophy for technical documentation. Instead of feeding an AI a massive, verbose manual, we leverage another AI to distill that documentation into a super-condensed, highly structured summary. The resulting <code>llm-min.txt</code> file captures only the most essential information needed to understand a library's usage, packaged in a format optimized for AI assistants rather than human readers.</p>
<p dir="auto">Modern AI reasoning capabilities excel at this distillation process, creating remarkably efficient knowledge representations that deliver maximum value with minimal token consumption.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Understanding <code>llm-min.txt</code>: A Machine-Optimized Format 🧩</h2><a id="user-content-understanding-llm-mintxt-a-machine-optimized-format-" aria-label="Permalink: Understanding llm-min.txt: A Machine-Optimized Format 🧩" href="#understanding-llm-mintxt-a-machine-optimized-format-"></a></p>
<p dir="auto">The <code>llm-min.txt</code> file utilizes the <strong>Structured Knowledge Format (SKF)</strong> – a compact, machine-optimized format designed for efficient AI parsing rather than human readability. This format organizes technical information into distinct, highly structured sections with precise relationships.</p>
<p dir="auto"><strong>Key Elements of the SKF Format:</strong></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Header Metadata:</strong> Every file begins with essential contextual information:</p>
<ul dir="auto">
<li><code># IntegratedKnowledgeManifest_SKF</code>: Format identifier and version</li>
<li><code># SourceDocs: [...]</code>: Original documentation sources</li>
<li><code># GenerationTimestamp: ...</code>: Creation timestamp</li>
<li><code># PrimaryNamespace: ...</code>: Top-level package/namespace, critical for understanding import paths</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Three Core Structured Sections:</strong> The content is organized into distinct functional categories:</p>
<ul dir="auto">
<li>
<p dir="auto"><code># SECTION: DEFINITIONS (Prefix: D)</code>: Describes the static aspects of the library:</p>
<ul dir="auto">
<li>Canonical component definitions with unique global IDs (e.g., <code>D001:G001_MyClass</code>)</li>
<li>Namespace paths relative to <code>PrimaryNamespace</code></li>
<li>Method signatures with parameters and return types</li>
<li>Properties/fields with types and access modifiers</li>
<li>Static relationships like inheritance or interface implementation</li>
<li><strong>Important:</strong> This section effectively serves as the glossary for the file, as the traditional glossary (<code>G</code> section) is used during generation but deliberately omitted from the final output to save space.</li>
</ul>
</li>
<li>
<p dir="auto"><code># SECTION: INTERACTIONS (Prefix: I)</code>: Captures dynamic behaviors within the library:</p>
<ul dir="auto">
<li>Method invocations (<code>INVOKES</code>)</li>
<li>Component usage patterns (<code>USES_COMPONENT</code>)</li>
<li>Event production/consumption</li>
<li>Error raising and handling logic, with references to specific error types</li>
</ul>
</li>
<li>
<p dir="auto"><code># SECTION: USAGE_PATTERNS (Prefix: U)</code>: Provides concrete usage examples:</p>
<ul dir="auto">
<li>Common workflows for core functionality</li>
<li>Step-by-step sequences involving object creation, configuration, method invocation, and error handling</li>
<li>Each pattern has a descriptive name (e.g., <code>U_BasicCrawl</code>) with numbered steps (<code>U_BasicCrawl.1</code>, <code>U_BasicCrawl.2</code>)</li>
</ul>
</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Line-Based Structure:</strong> Each item appears on its own line following precise formatting conventions that enable reliable machine parsing.</p>
</li>
</ol>
<p dir="auto"><strong>Example SKF Format (Simplified):</strong></p>
<div data-snippet-clipboard-copy-content="# IntegratedKnowledgeManifest_SKF/1.4 LA
# SourceDocs: [example-lib-docs]
# GenerationTimestamp: 2024-05-28T12:00:00Z
# PrimaryNamespace: example_lib

# SECTION: DEFINITIONS (Prefix: D)
# Format_PrimaryDef: Dxxx:Gxxx_Entity [DEF_TYP] [NAMESPACE &quot;relative.path&quot;] [OPERATIONS {op1:RetT(p1N:p1T)}] [ATTRIBUTES {attr1:AttrT1}] (&quot;Note&quot;)
# ---
D001:G001_Greeter [CompDef] [NAMESPACE &quot;.&quot;] [OPERATIONS {greet:Str(name:Str)}] (&quot;A simple greeter class&quot;)
D002:G002_AppConfig [CompDef] [NAMESPACE &quot;config&quot;] [ATTRIBUTES {debug_mode:Bool(&quot;RO&quot;)}] (&quot;Application configuration&quot;)
# ---

# SECTION: INTERACTIONS (Prefix: I)
# Format: Ixxx:Source_Ref INT_VERB Target_Ref_Or_Literal (&quot;Note_Conditions_Error(Gxxx_ErrorType)&quot;)
# ---
I001:G001_Greeter.greet INVOKES G003_Logger.log (&quot;Logs greeting activity&quot;)
# ---

# SECTION: USAGE_PATTERNS (Prefix: U)
# Format: U_Name:PatternTitleKeyword
#         U_Name.N:[Actor_Or_Ref] ACTION_KEYWORD (Target_Or_Data_Involving_Ref) -> [Result_Or_State_Change_Involving_Ref]
# ---
U_BasicGreeting:Basic User Greeting
U_BasicGreeting.1:[User] CREATE (G001_Greeter) -> [greeter_instance]
U_BasicGreeting.2:[greeter_instance] INVOKE (greet name='Alice') -> [greeting_message]
# ---
# END_OF_MANIFEST"><pre lang="text"><code># IntegratedKnowledgeManifest_SKF/1.4 LA
# SourceDocs: [example-lib-docs]
# GenerationTimestamp: 2024-05-28T12:00:00Z
# PrimaryNamespace: example_lib

# SECTION: DEFINITIONS (Prefix: D)
# Format_PrimaryDef: Dxxx:Gxxx_Entity [DEF_TYP] [NAMESPACE "relative.path"] [OPERATIONS {op1:RetT(p1N:p1T)}] [ATTRIBUTES {attr1:AttrT1}] ("Note")
# ---
D001:G001_Greeter [CompDef] [NAMESPACE "."] [OPERATIONS {greet:Str(name:Str)}] ("A simple greeter class")
D002:G002_AppConfig [CompDef] [NAMESPACE "config"] [ATTRIBUTES {debug_mode:Bool("RO")}] ("Application configuration")
# ---

# SECTION: INTERACTIONS (Prefix: I)
# Format: Ixxx:Source_Ref INT_VERB Target_Ref_Or_Literal ("Note_Conditions_Error(Gxxx_ErrorType)")
# ---
I001:G001_Greeter.greet INVOKES G003_Logger.log ("Logs greeting activity")
# ---

# SECTION: USAGE_PATTERNS (Prefix: U)
# Format: U_Name:PatternTitleKeyword
#         U_Name.N:[Actor_Or_Ref] ACTION_KEYWORD (Target_Or_Data_Involving_Ref) -&gt; [Result_Or_State_Change_Involving_Ref]
# ---
U_BasicGreeting:Basic User Greeting
U_BasicGreeting.1:[User] CREATE (G001_Greeter) -&gt; [greeter_instance]
U_BasicGreeting.2:[greeter_instance] INVOKE (greet name='Alice') -&gt; [greeting_message]
# ---
# END_OF_MANIFEST
</code></pre></div>
<p dir="auto">The <code>llm-min-guideline.md</code> file (generated alongside <code>llm-min.txt</code>) provides detailed decoding instructions and schema definitions that enable an AI to correctly interpret the SKF format. It serves as the essential companion document explaining the notation, field meanings, and relationship types used throughout the file.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Does it Really Work? Visualizing the Impact</h2><a id="user-content-does-it-really-work-visualizing-the-impact" aria-label="Permalink: Does it Really Work? Visualizing the Impact" href="#does-it-really-work-visualizing-the-impact"></a></p>
<p dir="auto"><code>llm-min.txt</code> achieves dramatic token reduction while preserving the essential knowledge needed by AI assistants. The chart below compares token counts between original library documentation (<code>llm-full.txt</code>) and the compressed <code>llm-min.txt</code> versions:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/marv1nnnnn/llm-min.txt/blob/main/assets/comparison.png"><img src="https://github.com/marv1nnnnn/llm-min.txt/raw/main/assets/comparison.png" alt="Token Compression Comparison"></a></p>
<p dir="auto">These results demonstrate token reductions typically ranging from 90-95%, with some cases exceeding 97%. This extreme compression, combined with the highly structured SKF format, enables AI tools to ingest and process library documentation far more efficiently than with raw text.</p>
<p dir="auto">In our samples directory, you can examine these impressive results firsthand:</p>
<ul dir="auto">
<li><code>sample/crawl4ai/llm-full.txt</code>: Original documentation (uncompressed)</li>
<li><code>sample/crawl4ai/llm-min.txt</code>: The compressed SKF representation</li>
<li><code>sample/crawl4ai/llm-min-guideline.md</code>: The format decoder companion file, also seen in <a href="https://github.com/marv1nnnnn/llm-min.txt/blob/main/assets/llm-min-guideline.md">llm-min-guideline.md</a></li>
</ul>
<p dir="auto">Most compressed files contain around 10,000 tokens – well within the processing capacity of modern AI assistants.</p>
<p dir="auto"><strong>How to use it?</strong></p>
<p dir="auto">Simply reference the files in your AI-powered IDE's conversation, and watch your assistant immediately gain detailed knowledge of the library:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/marv1nnnnn/llm-min.txt/blob/main/assets/demo.gif"><img src="https://github.com/marv1nnnnn/llm-min.txt/raw/main/assets/demo.gif" alt="Demo" data-animated-image=""></a></p>
<p dir="auto"><strong>How does it perform?</strong></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">It's necessary to make a benchmark but incredibly hard. LLM code generation is stochastic and the quality of the generated code depends on many factors. crawl4ai / google-genai / svelte are all packages current LLM failed to generate correct code for. Using <code>llm-min</code> will largely improve the success rate of code generation.</h2><a id="user-content-its-necessary-to-make-a-benchmark-but-incredibly-hard-llm-code-generation-is-stochastic-and-the-quality-of-the-generated-code-depends-on-many-factors-crawl4ai--google-genai--svelte-are-all-packages-current-llm-failed-to-generate-correct-code-for-using-llm-min-will-largely-improve-the-success-rate-of-code-generation" aria-label="Permalink: It's necessary to make a benchmark but incredibly hard. LLM code generation is stochastic and the quality of the generated code depends on many factors. crawl4ai / google-genai / svelte are all packages current LLM failed to generate correct code for. Using llm-min will largely improve the success rate of code generation." href="#its-necessary-to-make-a-benchmark-but-incredibly-hard-llm-code-generation-is-stochastic-and-the-quality-of-the-generated-code-depends-on-many-factors-crawl4ai--google-genai--svelte-are-all-packages-current-llm-failed-to-generate-correct-code-for-using-llm-min-will-largely-improve-the-success-rate-of-code-generation"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start 🚀</h2><a id="user-content-quick-start-" aria-label="Permalink: Quick Start 🚀" href="#quick-start-"></a></p>
<p dir="auto">Getting started with <code>llm-min</code> is straightforward:</p>
<p dir="auto"><strong>1. Installation:</strong></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>For regular users (recommended):</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install llm-min

# Install required browser automation tools
playwright install"><pre>pip install llm-min

<span><span>#</span> Install required browser automation tools</span>
playwright install</pre></div>
</li>
<li>
<p dir="auto"><strong>For contributors and developers:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Clone the repository (if not already done)
# git clone https://github.com/your-repo/llm-min.git
# cd llm-min

# Create and activate a virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies with UV (faster than pip)
uv sync
uv pip install -e .

# Optional: Set up pre-commit hooks for code quality
# uv pip install pre-commit
# pre-commit install"><pre><span><span>#</span> Clone the repository (if not already done)</span>
<span><span>#</span> git clone https://github.com/your-repo/llm-min.git</span>
<span><span>#</span> cd llm-min</span>

<span><span>#</span> Create and activate a virtual environment</span>
python -m venv .venv
<span>source</span> .venv/bin/activate  <span><span>#</span> On Windows: .venv\Scripts\activate</span>

<span><span>#</span> Install dependencies with UV (faster than pip)</span>
uv sync
uv pip install -e <span>.</span>

<span><span>#</span> Optional: Set up pre-commit hooks for code quality</span>
<span><span>#</span> uv pip install pre-commit</span>
<span><span>#</span> pre-commit install</span></pre></div>
</li>
</ul>
<p dir="auto"><strong>2. Set Up Your Gemini API Key:</strong> 🔑</p>
<p dir="auto"><code>llm-min</code> uses Google's Gemini AI to generate compressed documentation. You'll need a Gemini API key to proceed:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Best practice:</strong> Set an environment variable named <code>GEMINI_API_KEY</code> with your key value:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Linux/macOS
export GEMINI_API_KEY=your_api_key_here

# Windows (Command Prompt)
set GEMINI_API_KEY=your_api_key_here

# Windows (PowerShell)
$env:GEMINI_API_KEY=&quot;your_api_key_here&quot;"><pre><span><span>#</span> Linux/macOS</span>
<span>export</span> GEMINI_API_KEY=your_api_key_here

<span><span>#</span> Windows (Command Prompt)</span>
<span>set</span> GEMINI_API_KEY=your_api_key_here

<span><span>#</span> Windows (PowerShell)</span>
<span>$env</span>:GEMINI_API_KEY=<span><span>"</span>your_api_key_here<span>"</span></span></pre></div>
</li>
<li>
<p dir="auto"><strong>Alternative:</strong> Supply your key directly via the <code>--gemini-api-key</code> command-line option.</p>
</li>
</ul>
<p dir="auto">You can obtain a Gemini API key from the <a href="https://aistudio.google.com/app/apikey" rel="nofollow">Google AI Studio</a> or Google Cloud Console.</p>
<p dir="auto"><strong>3. Generate Your First <code>llm-min.txt</code> File:</strong> 💻</p>
<p dir="auto">Choose one of the following input sources:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Option</th>
<th>Short</th>
<th>Type</th>
<th>What it does</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--output-dir</code></td>
<td><code>-o</code></td>
<td><code>DIRECTORY</code></td>
<td>Where to save the generated files (default is a folder named <code>llm_min_docs</code>).</td>
</tr>
<tr>
<td><code>--output-name</code></td>
<td><code>-n</code></td>
<td><code>TEXT</code></td>
<td>Give a custom name for the subfolder inside <code>output-dir</code>.</td>
</tr>
<tr>
<td><code>--max-crawl-pages</code></td>
<td><code>-p</code></td>
<td><code>INTEGER</code></td>
<td>Max web pages to read (default: 200; 0 means no limit).</td>
</tr>
<tr>
<td><code>--max-crawl-depth</code></td>
<td><code>-D</code></td>
<td><code>INTEGER</code></td>
<td>How many links deep to follow on a website (default: 3).</td>
</tr>
<tr>
<td><code>--chunk-size</code></td>
<td><code>-c</code></td>
<td><code>INTEGER</code></td>
<td>How much text to give the AI at once (default: 600,000 characters).</td>
</tr>
<tr>
<td><code>--gemini-api-key</code></td>
<td><code>-k</code></td>
<td><code>TEXT</code></td>
<td>Your Gemini API Key (if not set as an environment variable).</td>
</tr>
<tr>
<td><code>--gemini-model</code></td>
<td><code>-m</code></td>
<td><code>TEXT</code></td>
<td>Which Gemini model to use (default: <code>gemini-2.5-flash-preview-04-17</code>).</td>
</tr>
<tr>
<td><code>--verbose</code></td>
<td><code>-v</code></td>
<td></td>
<td>Show more detailed messages while it's working.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><strong>Key Command-Line Options:</strong></p>
<ul dir="auto">
<li>Process the Python package <code>typer</code>, read up to 50 web pages, and save to a folder called <code>my_docs</code>:
<div dir="auto" data-snippet-clipboard-copy-content="llm-min -pkg &quot;typer&quot; -o my_docs -p 50 --gemini-api-key YOUR_API_KEY_HERE"><pre>llm-min -pkg <span><span>"</span>typer<span>"</span></span> -o my_docs -p 50 --gemini-api-key YOUR_API_KEY_HERE</pre></div>
</li>
</ul>
<p dir="auto"><strong>Example Commands:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Process the &quot;typer&quot; package, save to &quot;my_docs&quot; folder
llm-min -pkg &quot;typer&quot; -o my_docs -p 50

# Process the FastAPI documentation website
llm-min -u &quot;https://fastapi.tiangolo.com/&quot; -o my_docs -p 50

# Process documentation files in a local folder
llm-min -i &quot;./docs&quot; -o my_docs"><pre><span><span>#</span> Process the "typer" package, save to "my_docs" folder</span>
llm-min -pkg <span><span>"</span>typer<span>"</span></span> -o my_docs -p 50

<span><span>#</span> Process the FastAPI documentation website</span>
llm-min -u <span><span>"</span>https://fastapi.tiangolo.com/<span>"</span></span> -o my_docs -p 50

<span><span>#</span> Process documentation files in a local folder</span>
llm-min -i <span><span>"</span>./docs<span>"</span></span> -o my_docs</pre></div>
<p dir="auto"><strong>4. Programmatic Usage in Python:</strong> 🐍</p>
<p dir="auto">You can also integrate <code>llm-min</code> directly into your Python applications:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from llm_min import LLMMinGenerator
import os

# Configuration for the AI processing
llm_config = {
    &quot;api_key&quot;: os.environ.get(&quot;GEMINI_API_KEY&quot;),  # Use environment variable
    &quot;model_name&quot;: &quot;gemini-2.5-flash-preview-04-17&quot;,  # Recommended model
    &quot;chunk_size&quot;: 600000,  # Characters per AI processing batch
    &quot;max_crawl_pages&quot;: 200,  # Maximum pages to crawl
    &quot;max_crawl_depth&quot;: 3,  # Link following depth
}

# Initialize the generator (output files will go to ./my_output_docs/[package_name]/)
generator = LLMMinGenerator(output_dir=&quot;./my_output_docs&quot;, llm_config=llm_config)

# Generate llm-min.txt for the 'requests' package
try:
    generator.generate_from_package(&quot;requests&quot;)
    print(&quot;✅ Successfully created documentation for 'requests'!&quot;)
except Exception as e:
    print(f&quot;❌ Error processing 'requests': {e}&quot;)

# Generate llm-min.txt from a documentation URL
try:
    generator.generate_from_url(&quot;https://bun.sh/llms-full.txt&quot;)
    print(&quot;✅ Successfully processed 'https://bun.sh/llms-full.txt'!&quot;)
except Exception as e:
    print(f&quot;❌ Error processing URL: {e}&quot;)"><pre><span>from</span> <span>llm_min</span> <span>import</span> <span>LLMMinGenerator</span>
<span>import</span> <span>os</span>

<span># Configuration for the AI processing</span>
<span>llm_config</span> <span>=</span> {
    <span>"api_key"</span>: <span>os</span>.<span>environ</span>.<span>get</span>(<span>"GEMINI_API_KEY"</span>),  <span># Use environment variable</span>
    <span>"model_name"</span>: <span>"gemini-2.5-flash-preview-04-17"</span>,  <span># Recommended model</span>
    <span>"chunk_size"</span>: <span>600000</span>,  <span># Characters per AI processing batch</span>
    <span>"max_crawl_pages"</span>: <span>200</span>,  <span># Maximum pages to crawl</span>
    <span>"max_crawl_depth"</span>: <span>3</span>,  <span># Link following depth</span>
}

<span># Initialize the generator (output files will go to ./my_output_docs/[package_name]/)</span>
<span>generator</span> <span>=</span> <span>LLMMinGenerator</span>(<span>output_dir</span><span>=</span><span>"./my_output_docs"</span>, <span>llm_config</span><span>=</span><span>llm_config</span>)

<span># Generate llm-min.txt for the 'requests' package</span>
<span>try</span>:
    <span>generator</span>.<span>generate_from_package</span>(<span>"requests"</span>)
    <span>print</span>(<span>"✅ Successfully created documentation for 'requests'!"</span>)
<span>except</span> <span>Exception</span> <span>as</span> <span>e</span>:
    <span>print</span>(<span>f"❌ Error processing 'requests': <span><span>{</span><span>e</span><span>}</span></span>"</span>)

<span># Generate llm-min.txt from a documentation URL</span>
<span>try</span>:
    <span>generator</span>.<span>generate_from_url</span>(<span>"https://bun.sh/llms-full.txt"</span>)
    <span>print</span>(<span>"✅ Successfully processed 'https://bun.sh/llms-full.txt'!"</span>)
<span>except</span> <span>Exception</span> <span>as</span> <span>e</span>:
    <span>print</span>(<span>f"❌ Error processing URL: <span><span>{</span><span>e</span><span>}</span></span>"</span>)</pre></div>
<p dir="auto">For a complete list of command-line options, run:</p>

<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Output Directory Structure 📂</h2><a id="user-content-output-directory-structure-" aria-label="Permalink: Output Directory Structure 📂" href="#output-directory-structure-"></a></p>
<p dir="auto">When <code>llm-min</code> completes its processing, it creates the following organized directory structure:</p>
<div data-snippet-clipboard-copy-content="your_chosen_output_dir/
└── name_of_package_or_website/
    ├── llm-full.txt             # Complete documentation text (original content)
    ├── llm-min.txt              # Compressed SKF/1.4 LA structured summary
    └── llm-min-guideline.md     # Essential format decoder for AI interpretation"><pre lang="text"><code>your_chosen_output_dir/
└── name_of_package_or_website/
    ├── llm-full.txt             # Complete documentation text (original content)
    ├── llm-min.txt              # Compressed SKF/1.4 LA structured summary
    └── llm-min-guideline.md     # Essential format decoder for AI interpretation
</code></pre></div>
<p dir="auto">For example, running <code>llm-min -pkg "requests" -o my_llm_docs</code> produces:</p>
<div data-snippet-clipboard-copy-content="my_llm_docs/
└── requests/
    ├── llm-full.txt             # Original documentation
    ├── llm-min.txt              # Compressed SKF format (D, I, U sections)
    └── llm-min-guideline.md     # Format decoding instructions"><pre lang="text"><code>my_llm_docs/
└── requests/
    ├── llm-full.txt             # Original documentation
    ├── llm-min.txt              # Compressed SKF format (D, I, U sections)
    └── llm-min-guideline.md     # Format decoding instructions
</code></pre></div>
<p dir="auto"><strong>Important:</strong> The <code>llm-min-guideline.md</code> file is a critical companion to <code>llm-min.txt</code>. It provides the detailed schema definitions and format explanations that an AI needs to correctly interpret the structured data. When using <code>llm-min.txt</code> with an AI assistant, always include this guideline file as well.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Choosing the Right AI Model (Why Gemini) 🧠</h2><a id="user-content-choosing-the-right-ai-model-why-gemini-" aria-label="Permalink: Choosing the Right AI Model (Why Gemini) 🧠" href="#choosing-the-right-ai-model-why-gemini-"></a></p>
<p dir="auto"><code>llm-min</code> utilizes Google's Gemini family of AI models for document processing. While you can select a specific Gemini model via the <code>--gemini-model</code> option, we strongly recommend using the default: <code>gemini-2.5-flash-preview-04-17</code>.</p>
<p dir="auto">This particular model offers an optimal combination of capabilities for documentation compression:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Advanced Reasoning:</strong> Excels at understanding complex technical documentation and extracting the essential structural relationships needed for the SKF format.</p>
</li>
<li>
<p dir="auto"><strong>Exceptional Context Window:</strong> With a 1-million token input capacity, it can process large documentation chunks at once, enabling more coherent and comprehensive analysis.</p>
</li>
<li>
<p dir="auto"><strong>Cost Efficiency:</strong> Provides an excellent balance of capability and affordability compared to other large-context models.</p>
</li>
</ol>
<p dir="auto">The default model has been carefully selected to deliver the best results for the <code>llm-min</code> compression process across a wide range of documentation styles and technical domains.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">How it Works: A Look Inside (src/llm_min) ⚙️</h2><a id="user-content-how-it-works-a-look-inside-srcllm_min-️" aria-label="Permalink: How it Works: A Look Inside (src/llm_min) ⚙️" href="#how-it-works-a-look-inside-srcllm_min-️"></a></p>
<p dir="auto">The <code>llm-min</code> tool employs a sophisticated multi-stage process to transform verbose documentation into a compact, machine-optimized SKF manifest:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Input Processing:</strong> Based on your command-line options (e.g., <code>--package "requests"</code>), <code>llm-min</code> gathers documentation from the appropriate source (PyPI, web crawling, or local files).</p>
</li>
<li>
<p dir="auto"><strong>Text Preparation:</strong> The collected documentation is cleaned and segmented into manageable chunks for processing. The original text is preserved as <code>llm-full.txt</code>.</p>
</li>
<li>
<p dir="auto"><strong>Three-Step AI Analysis Pipeline (Gemini):</strong> This is the heart of the SKF manifest generation, orchestrated by the <code>compact_content_to_structured_text</code> function in <code>compacter.py</code>:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Step 1: Global Glossary Generation (Internal Only):</strong></p>
<ul dir="auto">
<li>Each document chunk is analyzed using the <code>SKF_PROMPT_CALL1_GLOSSARY_TEMPLATE</code> prompt to identify key technical entities and generate a <em>chunk-local</em> glossary fragment with temporary <code>Gxxx</code> IDs.</li>
<li>These fragments are consolidated via the <code>SKF_PROMPT_CALL1_5_MERGE_GLOSSARY_TEMPLATE</code> prompt, which resolves duplicates and creates a unified entity list.</li>
<li>The <code>re_id_glossary_items</code> function then assigns globally sequential <code>Gxxx</code> IDs (G001, G002, etc.) to these consolidated entities.</li>
<li>This global glossary is maintained in memory throughout the process but is <strong>not included in the final <code>llm-min.txt</code> output</strong> to conserve space.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Step 2: Definitions &amp; Interactions (D &amp; I) Generation:</strong></p>
<ul dir="auto">
<li>For the first document chunk (or if there's only one chunk), the AI uses the <code>SKF_PROMPT_CALL2_DETAILS_SINGLE_CHUNK_TEMPLATE</code> with the global glossary to generate initial D and I items.</li>
<li>For subsequent chunks, the <code>SKF_PROMPT_CALL2_DETAILS_ITERATIVE_TEMPLATE</code> is used, providing both the global glossary and previously generated D&amp;I items as context to avoid duplication.</li>
<li>As each chunk is processed, newly identified D and I items are accumulated and assigned sequential global IDs (D001, D002, etc. and I001, I002, etc.).</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Step 3: Usage Patterns (U) Generation:</strong></p>
<ul dir="auto">
<li>Similar to Step 2, the first chunk uses <code>SKF_PROMPT_CALL3_USAGE_SINGLE_CHUNK_TEMPLATE</code>, receiving the global glossary, all accumulated D&amp;I items, and the current chunk text.</li>
<li>Subsequent chunks use <code>SKF_PROMPT_CALL3_USAGE_ITERATIVE_TEMPLATE</code>, which additionally receives previously generated U-items to enable pattern continuation and avoid duplication.</li>
<li>Usage patterns are identified with descriptive names (e.g., <code>U_BasicNetworkFetch</code>) and contain numbered steps (e.g., <code>U_BasicNetworkFetch.1</code>, <code>U_BasicNetworkFetch.2</code>).</li>
</ul>
</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Final Assembly:</strong> The complete <code>llm-min.txt</code> file is created by combining:</p>
<ul dir="auto">
<li>The SKF manifest header (protocol version, source docs, timestamp, primary namespace)</li>
<li>The accumulated <code>DEFINITIONS</code> section</li>
<li>The accumulated <code>INTERACTIONS</code> section</li>
<li>The accumulated <code>USAGE_PATTERNS</code> section</li>
<li>A final <code># END_OF_MANIFEST</code> marker</li>
</ul>
</li>
</ol>
<p dir="auto"><strong>Conceptual Pipeline Overview:</strong></p>
<div data-snippet-clipboard-copy-content="User Input      →  Doc Gathering   →  Text Processing   →  AI Step 1: Glossary   →  In-Memory Global    →  AI Step 2: D&amp;I     →  Accumulated D&amp;I
(CLI/Python)       (Package/URL)      (Chunking)           (Extract + Merge)        Glossary (Gxxx)        (Per chunk)          (Dxxx, Ixxx)
                                                                                                                                     ↓
           ┌─────────────────────────────────────────────────────────────────────────────────────────────────┐                      ↓
           ↓                                                                                                 ↑                      ↓
Final SKF Manifest   ←   Assembly   ←   Accumulated Usage   ←   AI Step 3: Usage   ←   Global Glossary + Accumulated D&amp;I
(llm-min.txt)            (D,I,U)        Patterns (U_Name.N)      (Per chunk)           (Required context for generating valid U-items)"><pre><code>User Input      →  Doc Gathering   →  Text Processing   →  AI Step 1: Glossary   →  In-Memory Global    →  AI Step 2: D&amp;I     →  Accumulated D&amp;I
(CLI/Python)       (Package/URL)      (Chunking)           (Extract + Merge)        Glossary (Gxxx)        (Per chunk)          (Dxxx, Ixxx)
                                                                                                                                     ↓
           ┌─────────────────────────────────────────────────────────────────────────────────────────────────┐                      ↓
           ↓                                                                                                 ↑                      ↓
Final SKF Manifest   ←   Assembly   ←   Accumulated Usage   ←   AI Step 3: Usage   ←   Global Glossary + Accumulated D&amp;I
(llm-min.txt)            (D,I,U)        Patterns (U_Name.N)      (Per chunk)           (Required context for generating valid U-items)
</code></pre></div>
<p dir="auto">This multi-stage approach ensures that the SKF manifest is comprehensive, avoids duplication across chunks, and maintains consistent cross-references between entities, definitions, interactions, and usage patterns.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">What's Next? Future Plans 🔮</h2><a id="user-content-whats-next-future-plans-" aria-label="Permalink: What's Next? Future Plans 🔮" href="#whats-next-future-plans-"></a></p>
<p dir="auto">We're exploring several exciting directions to evolve <code>llm-min</code>:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Public Repository for Pre-Generated Files</strong> 🌐
A central hub where the community could share and discover <code>llm-min.txt</code> files for popular libraries would be valuable. This would eliminate the need for individual users to generate these files repeatedly and ensure consistent, high-quality information. Key challenges include quality control, version management, and hosting infrastructure costs.</p>
</li>
<li>
<p dir="auto"><strong>Code-Based Documentation Inference</strong> 💻
An intriguing possibility is using source code analysis (via Abstract Syntax Trees) to automatically generate or augment documentation summaries. While initial experiments have shown this to be technically challenging, particularly for complex libraries with dynamic behaviors, it remains a promising research direction that could enable even more accurate documentation.</p>
</li>
<li>
<p dir="auto"><strong>Model Control Protocol Integration</strong> 🤔
While technically feasible, implementing <code>llm-min</code> as an MCP server doesn't fully align with our current design philosophy. The strength of <code>llm-min.txt</code> lies in providing reliable, static context – a deterministic reference that reduces the uncertainty sometimes associated with dynamic AI integrations. We're monitoring user needs to determine if a server-based approach might deliver value in the future.</p>
</li>
</ul>
<p dir="auto">We welcome community input on these potential directions!</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Common Questions (FAQ) ❓</h2><a id="user-content-common-questions-faq-" aria-label="Permalink: Common Questions (FAQ) ❓" href="#common-questions-faq-"></a></p>
<p dir="auto"><strong>Q: Do I need a reasoning-capable model to generate an <code>llm-min.txt</code> file?</strong> 🧠</p>
<p dir="auto">A: Yes, generating an <code>llm-min.txt</code> file requires a model with strong reasoning capabilities like Gemini. The process involves complex information extraction, entity relationship mapping, and structured knowledge representation. However, once generated, the <code>llm-min.txt</code> file can be effectively used by any competent coding model (e.g., Claude 3.5 Sonnet) to answer library-specific questions.</p>
<p dir="auto"><strong>Q: Does <code>llm-min.txt</code> preserve all information from the original documentation?</strong> 📚</p>
<p dir="auto">A: No, <code>llm-min.txt</code> is explicitly designed as a lossy compression format. It prioritizes programmatically relevant details (classes, methods, parameters, return types, core usage patterns) while deliberately omitting explanatory prose, conceptual discussions, and peripheral information. This selective preservation is what enables the dramatic token reduction while maintaining the essential technical reference information an AI assistant needs.</p>
<p dir="auto"><strong>Q: Why does generating an <code>llm-min.txt</code> file take time?</strong> ⏱️</p>
<p dir="auto">A: Creating an <code>llm-min.txt</code> file involves a sophisticated multi-stage AI pipeline:</p>
<ol dir="auto">
<li>Gathering and preprocessing documentation</li>
<li>Analyzing each chunk to identify entities (glossary generation)</li>
<li>Consolidating entities across chunks</li>
<li>Extracting detailed definitions and interactions from each chunk</li>
<li>Generating representative usage patterns</li>
</ol>
<p dir="auto">This intensive process can take several minutes, particularly for large libraries. However, once created, the resulting <code>llm-min.txt</code> file can be reused indefinitely, providing much faster reference information for AI assistants.</p>
<p dir="auto"><strong>Q: I received a "Gemini generation stopped due to MAX_TOKENS limit" error. What should I do?</strong> 🛑</p>
<p dir="auto">A: This error indicates that the Gemini model reached its output limit while processing a particularly dense or complex documentation chunk. Try reducing the <code>--chunk-size</code> option (e.g., from 600,000 to 300,000 characters) to give the model smaller batches to process. While this might slightly increase API costs due to more separate calls, it often resolves token limit errors.</p>
<p dir="auto"><strong>Q: What's the typical cost for generating one <code>llm-min.txt</code> file?</strong> 💰</p>
<p dir="auto">A: Processing costs vary based on documentation size and complexity, but for a moderate-sized library, expect to spend between <strong>$0.01 and $1.00 USD</strong> in Gemini API charges. Key factors affecting cost include:</p>
<ul dir="auto">
<li>Total documentation size</li>
<li>Number of chunks processed</li>
<li>Complexity of the library's structure</li>
<li>Selected Gemini model</li>
</ul>
<p dir="auto">For current pricing details, refer to the <a href="https://cloud.google.com/vertex-ai/pricing#gemini" rel="nofollow">Google Cloud AI pricing page</a>.</p>
<p dir="auto"><strong>Q: Did you vibe code this project</strong> 🤖</p>
<p dir="auto">A: Yes, definitely. This project was developed using <a href="https://roocode.com/" rel="nofollow">Roocode</a> with a custom configuration called <a href="https://github.com/marv1nnnnn/rooroo">Rooroo</a>.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Want to Help? Contributing 🤝</h2><a id="user-content-want-to-help-contributing-" aria-label="Permalink: Want to Help? Contributing 🤝" href="#want-to-help-contributing-"></a></p>
<p dir="auto">We welcome contributions to make <code>llm-min</code> even better! 🎉</p>
<p dir="auto">Whether you're reporting bugs, suggesting features, or submitting code changes via pull requests, your involvement helps improve this tool for everyone. Check our GitHub repository for contribution guidelines and open issues.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">License 📜</h2><a id="user-content-license-" aria-label="Permalink: License 📜" href="#license-"></a></p>
<p dir="auto">This project is licensed under the MIT License. See the <code>LICENSE</code> file for complete details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Real-Time Gaussian Splatting (109 pts)]]></title>
            <link>https://github.com/axbycc/LiveSplat</link>
            <guid>43994827</guid>
            <pubDate>Thu, 15 May 2025 13:26:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/axbycc/LiveSplat">https://github.com/axbycc/LiveSplat</a>, See on <a href="https://news.ycombinator.com/item?id=43994827">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/155058764/443802899-9a97fcf9-33cd-4bea-b124-0233c5435f90.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDczNDEzMDEsIm5iZiI6MTc0NzM0MTAwMSwicGF0aCI6Ii8xNTUwNTg3NjQvNDQzODAyODk5LTlhOTdmY2Y5LTMzY2QtNGJlYS1iMTI0LTAyMzNjNTQzNWY5MC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNTE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDUxNVQyMDMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1lOGMwNThhN2NiNWI5YzdhOWE5ODJkN2U1OWYyNTdmZjcwMmQ2M2Y1ZGMyZTJlNDc4ODI2YWNmNzFlZmUxM2JlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.R__n_IkVSRLUJPuIiie4ldVmeLYLBvwlN9Pr3k_6hao"><img src="https://private-user-images.githubusercontent.com/155058764/443802899-9a97fcf9-33cd-4bea-b124-0233c5435f90.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDczNDEzMDEsIm5iZiI6MTc0NzM0MTAwMSwicGF0aCI6Ii8xNTUwNTg3NjQvNDQzODAyODk5LTlhOTdmY2Y5LTMzY2QtNGJlYS1iMTI0LTAyMzNjNTQzNWY5MC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNTE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDUxNVQyMDMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1lOGMwNThhN2NiNWI5YzdhOWE5ODJkN2U1OWYyNTdmZjcwMmQ2M2Y1ZGMyZTJlNDc4ODI2YWNmNzFlZmUxM2JlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.R__n_IkVSRLUJPuIiie4ldVmeLYLBvwlN9Pr3k_6hao" width="150"></a>  
<p dir="auto"><strong>LiveSplat</strong> is an algorithm for realtime Gaussian splatting using RGBD camera streams. Join our <a href="https://discord.gg/rCF5SXnc" rel="nofollow">discord</a> for discussion and help. Check out the demo video below to see the the type of output that LiveSplat produces.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description LiveSplat_Alexa.mp4">LiveSplat_Alexa.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/155058764/443907841-0e41f600-36e3-4482-866e-70b5c962c6f4.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDczNDEzMDEsIm5iZiI6MTc0NzM0MTAwMSwicGF0aCI6Ii8xNTUwNTg3NjQvNDQzOTA3ODQxLTBlNDFmNjAwLTM2ZTMtNDQ4Mi04NjZlLTcwYjVjOTYyYzZmNC5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNTE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDUxNVQyMDMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT01YTIxY2ZiNDg4OTE4MGYwNmU3YWY5YTgxM2Y5N2ZhMDNkMTUyZTA1MDlhMWMyZGVmZDU1NjFlNGMxMmQ4YTZjJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.m_goHDqrvNcnUPYqAcWo5hyzqlZ0nuV3SFKIlEEl2Iw" data-canonical-src="https://private-user-images.githubusercontent.com/155058764/443907841-0e41f600-36e3-4482-866e-70b5c962c6f4.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDczNDEzMDEsIm5iZiI6MTc0NzM0MTAwMSwicGF0aCI6Ii8xNTUwNTg3NjQvNDQzOTA3ODQxLTBlNDFmNjAwLTM2ZTMtNDQ4Mi04NjZlLTcwYjVjOTYyYzZmNC5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNTE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDUxNVQyMDMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT01YTIxY2ZiNDg4OTE4MGYwNmU3YWY5YTgxM2Y5N2ZhMDNkMTUyZTA1MDlhMWMyZGVmZDU1NjFlNGMxMmQ4YTZjJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.m_goHDqrvNcnUPYqAcWo5hyzqlZ0nuV3SFKIlEEl2Iw" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Message from the Author</h2><a id="user-content-message-from-the-author" aria-label="Permalink: Message from the Author" href="#message-from-the-author"></a></p>
<p dir="auto">LiveSplat was developed as a small part of a larger proprietary VR telerobotics system. I posted a video of the Gaussian splatting component of this system on Reddit and many people expressed an interest in experimenting with it themselves. So I spun it out and I'm making it publicly available as LiveSplat (see installation instructions below).</p>
<p dir="auto">LiveSplat should be considered alpha quality. I do not have the resources to test the installation on many different machines, so let me know if the application does not run on yours (assuming your machine meets the requirements).</p>
<p dir="auto">I've decided to keep LiveSplat closed source in order to explore potential business opportunities. If you are a business wanting to license/integrate this technology, please email <a href="mailto:mark@axby.cc">mark@axby.cc</a>.</p>
<p dir="auto">I hope you have fun with LiveSplat!</p>
<p dir="auto">— Mark Liu</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Python 3.12+</li>
<li>Windows or Ubuntu (other Linux distributions may work, but are untested)</li>
<li>x86_64 CPU</li>
<li>Nvidia graphics card</li>
<li>One or more (up to four) RGBD sensors</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Note that the application is not open source, and is covered by this <a href="https://github.com/axbycc/LiveSplat/blob/main/LICENSE">license</a>.</p>
<p dir="auto"><strong>Ubuntu:</strong><br>
<code>pip3 install https://livesplat.s3.us-east-2.amazonaws.com/livesplat-0.1.0-cp312-cp312-manylinux_x86_64.whl</code></p>
<p dir="auto"><strong>Windows:</strong><br>
<code>pip install https://livesplat.s3.us-east-2.amazonaws.com/livesplat-0.1.0-cp312-cp312-win_amd64.whl</code></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running</h2><a id="user-content-running" aria-label="Permalink: Running" href="#running"></a></p>
<p dir="auto">To run LiveSplat, you will have to create an integration script that feeds your RGBD streams to the LiveSplat viewer. This repo provides an integration script for Intel Realsense devices called <a href="https://github.com/axbycc/LiveSplat/blob/main/livesplat_realsense.py">livesplat_realsense.py</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Malicious compliance by booking an available meeting room (237 pts)]]></title>
            <link>https://www.clientserver.dev/p/malicious-compliance-by-booking-an</link>
            <guid>43994765</guid>
            <pubDate>Thu, 15 May 2025 13:20:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.clientserver.dev/p/malicious-compliance-by-booking-an">https://www.clientserver.dev/p/malicious-compliance-by-booking-an</a>, See on <a href="https://news.ycombinator.com/item?id=43994765">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Back in 2011, Larry Page became the CEO of Google in place of Eric Schmidt. This happened at a time when Google was feeling the growing pains of becoming a huge company. It had 30,000 employees and was growing rapidly. But you could really feel the weight; projects were getting more ambitious, taking longer, and often failing more spectacularly.</p><p>At the time, I remember an anecdote told by Larry Page. He said that companies like Yahoo! used to be a punchline at Google because it would take them weeks to get something onto their homepage. Google could accomplish the same thing in a few hours, or a few days at worst. But now he was the CEO of a company where it took weeks to get something onto the homepage, and he was sure that he was the butt of some startup’s jokes.</p><p><span>Anyways, all of this clearly bothered Larry Page. He wanted to fix it. One of his first actions was to shutter tons of projects that didn’t make tactical or strategic sense, and focus on fewer efforts. This came with the catch phrase “more wood behind fewer arrows.” For example, they shuttered </span><a href="https://en.wikipedia.org/wiki/Google_Buzz" rel="">Google Buzz</a><span> so that it wouldn’t distract from </span><a href="https://en.wikipedia.org/wiki/Google%2B" rel="">Google+</a><span>.</span></p><p><span>And second, Larry Page emailed the whole company </span><a href="https://www.businessinsider.com/this-is-how-larry-page-changed-meetings-at-google-after-taking-over-last-spring-2012-1" rel="">a ham-fisted attempt to revamp how meetings were done</a><span>.</span></p><ul><li><p>Every meeting needed a “decision-maker.”</p></li><li><p>Meetings should be capped at 10 people.</p></li><li><p>Everybody in a meeting should give input or they shouldn’t be in the meeting.</p></li><li><p>Hour-long meetings should be only 50 minutes to give the participants an opportunity to use the restroom between meetings.</p></li></ul><p>They later softened some of the language by saying that these were properties of “decision-oriented meetings,” implying there were other types of meetings that someone might need to attend. But you could never shake the feeling that Larry Page had to make decisions all day long and forgot that sometimes people meet for other reasons.</p><p><span>Anyways, let’s focus on the fact that Larry Page wanted hour-long meetings to only be 50 minutes. This is a good thing! It gives people a chance to stretch, go to the bathroom, grab a snack, etc. During a Q/A on the changes</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-163523555" href="https://www.clientserver.dev/p/malicious-compliance-by-booking-an#footnote-1-163523555" target="_self" rel="">1</a></span><span>, someone asked him whether Google Calendar should default to 25 and 50 minutes for meeting lengths instead of 30 and 60 minutes. Larry Page said “yes.” And then someone on the Google Calendar team implemented this.</span></p><p>And then nothing changed. When 2:50 rolled around and your meeting was supposed to end, do you think people actually ended the meeting? Noooooo. Absolutely not! Meetings continue until the participants of the next meeting are clawing on your door like a pack of zombies.</p><p>At one point, one team in the NYC office noticed that their standups were about 10 minutes long. They didn’t want to compete with meetings that respected the half-hour boundaries. And why would they need to? Every meeting room had free slots at the last 10 minutes of every hour because people were now booking 50-minute meetings. So they did what any rational engineering team would do: they started booking their standup in the tiny 10-minute time slices that were free on the calendar of every meeting room.</p><p>I found this out when I saw them knock on the door to a meeting room by my desk. 2:50 rolls around and someone knocks on the door and says “I have the meeting room.”</p><p>The person in the room responds, “No you don’t, it’s 2:50.”</p><p>“Look again at the room’s calendar. You booked a 50-minute meeting, we have the room for the last 10 minutes of the hour for our standup.”</p><p>I could hear the muffled exasperation. “You’ve got to be joking me.”</p><p>“We have the room, sorry.”</p><p>Then everyone shuffled out of the room, looking vaguely pissed off. And who could blame them! Can you imagine if someone actually held you to this policy? You’re there stammering “it’s the default, I meant for the room to be ours for an hour” and they counter with the fact that their names are listed as the active participant? I mean, I’d personally tell them that I wasn’t going to leave the room, but surely it worked a lot?</p><p>I wish I knew the identities of these brave meeting crashers. I saw them pull this stunt twice and then ride off into the sunset, and I never got to learn what team they were on. I wish I knew their motivations. Were they true believers in the 50-minute policy? Were they bored pedants? Were they wraiths, cursed to hunt the office for available meeting rooms? I’ll never know for sure.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My Engineering Craft Regressed (111 pts)]]></title>
            <link>https://lemmy.ml/post/30100312</link>
            <guid>43994635</guid>
            <pubDate>Thu, 15 May 2025 13:07:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lemmy.ml/post/30100312">https://lemmy.ml/post/30100312</a>, See on <a href="https://news.ycombinator.com/item?id=43994635">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="postContent"><div><p dir="auto">5 years ago when I graduated University, I had a whole host of open source projects under my belt. I put my heart and soul into them - for thousands of hours. And users loved them. I still remember some of the faceless users whose messages gave me a smile.</p>
<p dir="auto"><span><img src="https://lemmy.ml/pictrs/image/518a9cb2-59d2-4412-a846-567377b8fd9e.png" alt=""></span></p>
<p dir="auto">When I went into the job market - which was much better back then - I had some incorrect assumption that recruiters would care about this work. Or at least technically inclined companies. Or at the very least, companies <em>I</em> would want to work for would care about this.</p>
<p dir="auto">But that never happened. My Indeed profile shows I applied to over 600 jobs back then. With 3 offers, I accepted the only one that didn’t treat me like a baby, and had a great time working for that company.</p>
<p dir="auto">During the day, I worked 8-9 hours for this startup, and until late at night I continued my open source contributions. Surely, they would take me somewhere.</p>
<p dir="auto"><span><img src="https://lemmy.ml/api/v3/image_proxy?url=https%3A%2F%2Fi.imgur.com%2F6vUNA8Y.png" alt=""></span></p>
<p dir="auto">I hopped to another startup and oversaw major projects - for pitiful pay - but enjoyed it. My skillset had never been so strong, and my impact was measurably through the roof. Surely if I kept this up, I would land a high paying gig anytime soon.</p>
<p dir="auto">But of course, that didn’t happen. Waking up at 6am to make some commits, reading documentation on the subway, and coding to dubstep at night wasn’t getting me anywhere. But I was happy.</p>
<p dir="auto">Eventually I came to face the music - Nobody gives a crap about real projects. The people who knew my value weren’t the people who could pay me. I peeled back, and started grinding Leetcode instead.</p>
<p dir="auto"><span><img src="https://lemmy.ml/api/v3/image_proxy?url=https%3A%2F%2Fi.imgur.com%2FcXM1lJd.png" alt=""></span></p>
<p dir="auto">My projects slowed to a crawl. The communities slowly got demotivated. It was sad to turn my back to, but it got me a 5x salary bump.</p>
<p dir="auto">When I joined, I was treated like a baby for having “4 years of industry experience”. Whatever. I did work here and there, and apparently my impact exceeded expectations.</p>
<p dir="auto">But what about my skillset? Despite significantly regressing, now my email is filled with recruiters begging me to come to be “Amazon SDE II”, “Tech lead at YC startup X”, “Part time job paying 150-290$/hr”. Pathetic. I was so much better (and happier) before, yet I’m only seen when I do the fake crap like update my LinkedIn to celebrate 1 year at $FAANG.</p>
<p dir="auto">I’ll collect some money and retire in a couple years. Hopefully the open source world stays the same until then.</p>
</div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[XAI's Grok suddenly can't stop bringing up "white genocide" in South Africa (175 pts)]]></title>
            <link>https://arstechnica.com/ai/2025/05/xais-grok-suddenly-cant-stop-bringing-up-white-genocide-in-south-africa/</link>
            <guid>43993332</guid>
            <pubDate>Thu, 15 May 2025 09:37:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/ai/2025/05/xais-grok-suddenly-cant-stop-bringing-up-white-genocide-in-south-africa/">https://arstechnica.com/ai/2025/05/xais-grok-suddenly-cant-stop-bringing-up-white-genocide-in-south-africa/</a>, See on <a href="https://news.ycombinator.com/item?id=43993332">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<h2>Where could Grok have gotten these ideas?</h2>
<p>The treatment of white farmers in South Africa has been a hobbyhorse of South African X owner Elon Musk for quite a while. In 2023, he responded to a video purportedly showing crowds chanting "kill the Boer, kill the White Farmer" with <a href="https://x.com/elonmusk/status/1686037774510497792?lang=en">a post</a> alleging South African President Cyril Ramaphosa of remaining silent while people "openly [push] for genocide of white people in South Africa." Musk was posting other responses focusing on the issue <a href="https://x.com/elonmusk/status/1922757640372801759">as recently as Wednesday</a>.</p>
<blockquote>
<p dir="ltr" lang="en">They are openly pushing for genocide of white people in South Africa. <a href="https://twitter.com/CyrilRamaphosa?ref_src=twsrc%5Etfw">@CyrilRamaphosa</a>, why do you say nothing?</p>
<p>— gorklon rust (@elonmusk) <a href="https://twitter.com/elonmusk/status/1686037774510497792?ref_src=twsrc%5Etfw">July 31, 2023</a></p></blockquote>
<p>President Trump has long shown an interest in this issue as well, <a href="https://x.com/realDonaldTrump/status/1032454567152246785">saying in 2018</a> that he was directing then Secretary of State Mike Pompeo to "closely study the South Africa land and farm seizures and expropriations and the large scale killing of farmers." More recently, Trump <a href="https://abcnews.go.com/Politics/trump-administration-defends-afrikaner-refugee-program-amid-groups/story?id=121723163">granted "refugee" status to dozens of white Afrikaners</a>, even as his administration <a href="https://thehill.com/policy/national-security/5295656-trump-administration-lifts-afghan-deportation/">ends protections for refugees from other countries</a></p>
<p>Former American Ambassador to South Africa and Democratic politician Patrick Gaspard <a href="https://x.com/patrickgaspard/status/1032595127993286657">posted in 2018</a> that the idea of large-scale killings of white South African farmers is a "disproven racial myth."</p>
<p>In launching the Grok 3 model in February, Musk <a href="https://techcrunch.com/2025/02/17/elon-musks-ai-company-xai-releases-its-latest-flagship-ai-grok-3/">said</a> it was a "maximally truth-seeking AI, even if that truth is sometimes at odds with what is politically correct." X's <a href="https://help.x.com/en/using-x/about-grok">"About Grok" page</a> says that the model is undergoing constant improvement to "ensure Grok remains politically unbiased and provides balanced answers."</p>
<p>But the recent turn toward unprompted discussions of alleged South African "genocide" has many questioning what kind of explicit adjustments Grok's political opinions may be getting from human tinkering behind the curtain. "The algorithms for Musk products have been politically tampered with nearly beyond recognition," journalist Seth Abramson <a href="https://bsky.app/profile/sethabramson.bsky.social/post/3lp5xlfel7c2c">wrote</a> in one representative skeptical post. "They tweaked a dial on the sentence imitator machine and now everything is about white South Africans," a user with the handle Guybrush Threepwood <a href="https://bsky.app/profile/skybrush.bsky.social/post/3lp5xio7uwc2s">glibly theorized</a>.</p>
<p>Representatives from xAI were not immediately available to respond to a request for comment from Ars Technica.</p>


          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU ruling: tracking-based advertising [...] across Europe has no legal basis (210 pts)]]></title>
            <link>https://www.iccl.ie/digital-data/eu-ruling-tracking-based-advertising-by-google-microsoft-amazon-x-across-europe-has-no-legal-basis/</link>
            <guid>43992444</guid>
            <pubDate>Thu, 15 May 2025 06:43:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iccl.ie/digital-data/eu-ruling-tracking-based-advertising-by-google-microsoft-amazon-x-across-europe-has-no-legal-basis/">https://www.iccl.ie/digital-data/eu-ruling-tracking-based-advertising-by-google-microsoft-amazon-x-across-europe-has-no-legal-basis/</a>, See on <a href="https://news.ycombinator.com/item?id=43992444">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>14 May 2025&nbsp;<br></strong>(Updated on 15 May to include links to the judgement)</p>
<p>Google, Microsoft, Amazon, X, and the entire tracking-based advertising industry rely on the “Transparency &amp; Consent Framework” (TCF) to obtain “consent” for data processing. This evening the Belgian Court of Appeal <a href="https://www.iccl.ie/wp-content/uploads/2025/05/Arrest-Marktenhof-14-05-2025.pdf" target="_blank" rel="noopener">ruled</a> that the TCF is illegal. The TCF is live on 80% of the Internet.<a href="#_ftn1" name="_ftnref1">[1]</a></p>
<p>Today’s decision arises from enforcement by the Belgian Data Protection Authority, prompted by complainants coordinated by Dr <a href="https://www.iccl.ie/staff/dr-johnny-ryan/">Johnny Ryan</a>, Director of <a href="https://www.iccl.ie/enforce/">Enforce</a> at the Irish Council for Civil Liberties. The group of complainants are: Dr Johnny Ryan of Enforce, Katarzyna Szymielewicz of the <a href="https://en.panoptykon.org/">Panoptykon Foundation</a>, <a href="https://twitter.com/Jausl00s">Dr Jef Ausloos</a>, <a href="https://twitter.com/PiDewitte">Dr Pierre Dewitte</a>,&nbsp;<a href="https://www.bitsoffreedom.nl/">Stichting Bits of Freedom</a>, and&nbsp;<a href="https://www.liguedh.be/">Ligue des Droits Humains</a>.</p>
<p>Dr Johnny Ryan said <em>"Today's court's decision shows that the consent system used by Google, Amazon, X, Microsoft, deceives hundreds of millions of Europeans. The tech industry has sought to hide its vast data breach behind sham consent popups. Tech companies turned the GDPR into a daily nuisance rather than a shield for people."</em></p>
<p>This Belgian enforcement arises from a chain of complaints and litigation across Europe initiated by Dr Ryan in 2018 against Real-Time Bidding (RTB).&nbsp;</p>
<p>Today’s decision confirmed the Belgian Data Protection Authority's 2022 <a href="https://www.iccl.ie/news/gdpr-enforcer-rules-that-iab-europes-consent-popups-are-unlawful/">finding of multiple infringements</a> by the TCF, closely echoing the complainants' submissions.</p>
<p>For seven years, the tracking industry has used the TCF as a legal cover for Real-Time Bidding (RTB), the vast advertising auction system that operates behind the scenes on websites and apps. RTB tracks what Internet users look at and where they go in the real world. It then continuously broadcasts this data to a host of companies, enabling them to keep dossiers on every Internet user.<a href="#_ftn2" name="_ftnref2">[2]</a> Because there is no security in the RTB system it is impossible to know what then happens to the data. As a result, it is also impossible to provide the necessary information that must accompany a consent request.<a href="#_ftn3" name="_ftnref3">[3]</a> &nbsp;</p>
<p>Today’s judgement confirms the Belgian Data Protection Authority’s 2022 decision. It applies immediately across Europe.&nbsp;</p>
<p>Dr Ryan of Enforce said “<em>This decision is momentous. It creates a clear need for industry to innovate and move away from the dangerous, ineffective, and fraud-riddled tracking-based advertising. RTB can operate without personal data. This decision shows that it must. This good news for every person online, and for publishers, too.</em>”</p>
<p>We thank our lawyers&nbsp;<a href="https://www.timelex.eu/en/team/frederic-debussere">Frederic Debusseré</a>&nbsp;and&nbsp;<a href="https://www.timelex.eu/en/team/ruben-roex">Ruben Roex</a>, of&nbsp;<a href="https://www.timelex.eu/en">Timelex</a>.</p>
<p><strong>Links </strong></p>
<p>See chronology, evidence, and explainers about RTB <a href="https://www.iccl.ie/RTB/">https://www.iccl.ie/RTB/</a></p>
<p>Brussels Appeals Court judgement (Dutch original) <a href="https://www.iccl.ie/wp-content/uploads/2025/05/Arrest-Marktenhof-14-05-2025.pdf">https://www.iccl.ie/wp-content/uploads/2025/05/Arrest-Marktenhof-14-05-2025.pdf</a>&nbsp;</p>
<p>Brussels Appeals Court judgement (MACHINE TRANSLATION to English) <a href="https://www.iccl.ie/wp-content/uploads/2025/05/Arrest-Marktenhof-14-05-2025-en.pdf%C2%A0">https://www.iccl.ie/wp-content/uploads/2025/05/Arrest-Marktenhof-14-05-2025-en.pdf&nbsp;</a></p>
<p><strong>Notes&nbsp;</strong></p>
<p><a href="#_ftnref1" name="_ftn1">[1]</a> See "IAB &amp; IAB Tech Lab Respond with Support for OpenRTB and IAB Europe’s Transparency &amp; Consent Framework", October 19 2020&nbsp;<a href="https://iabtechlab.com/iab-and-tech-lab-respond-with-support-for-open-rtb-and-iab-europes-transparency-consent-framework/">https://iabtechlab.com/iab-and-tech-lab-respond-with-support-for-open-rtb-and-iab-europes-transparency-consent-framework/</a></p>
<p><a href="#_ftnref2" name="_ftn2">[2]</a> For detail on the scale of RTB see our report "The Biggest Data Breach: ICCL report on the scale of Real-Time Bidding data broadcasts in the U.S. and Europe", ICCL, May 2022&nbsp;<a href="https://www.iccl.ie/digital-data/iccl-report-on-the-scale-of-real-time-bidding-data-broadcasts-in-the-u-s-and-europe/">https://www.iccl.ie/digital-data/iccl-report-on-the-scale-of-real-time-bidding-data-broadcasts-in-the-u-s-and-europe/</a></p>
<p><a href="#_ftnref3" name="_ftn3">[3]</a> "<em>As it is technically impossible for the user to have prior information about every data controller involved in a real-time bidding (RTB) scenario, programmatic trading, the area of fastest growth in digital advertising spend, would seem, at least prima facie, to be&nbsp;incompatible with consent&nbsp;under GDPR</em>". See&nbsp;<a href="https://brave.com/static-assets/files/1a-Townsend-Feehan-email-26-june-2017.pdf">e-mail</a>&nbsp;and page 3 of&nbsp;<a href="https://www.iccl.ie/wp-content/uploads/2022/09/1b-IAB-2017-paper.pdf">attached lobbying paper</a>&nbsp;from IAB Europe CEO Townsend Feehan to European Commission, 26 June 2017, obtained by Enforce using Freedom of Information.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Human (647 pts)]]></title>
            <link>https://quarter--mile.com/Human</link>
            <guid>43991396</guid>
            <pubDate>Thu, 15 May 2025 02:57:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quarter--mile.com/Human">https://quarter--mile.com/Human</a>, See on <a href="https://news.ycombinator.com/item?id=43991396">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-container="page" data-id="37541162" local-style="37541162" data-view="Content" data-set="Scaffolding">
			<bodycopy>
				
				<div data-elementresizer="" data-resize-parent="">
					<projectcontent><br>
<i>Written by a <u><a href="https://quarter--mile.com/Contact" rel="history">human</a></u></i><i> [0]<br></i><br>
Imagine, for a moment, a world with no humans. Just machines, bolts and screws, zeros and ones. There is no emotion. There is no art. There is only logic. You would not walk through the streets of this world and hear music or laughter or children playing; no, all you would hear is the quiet hum of processors and servers and circuits, the clanking of machinery.<p>
Perhaps you, a human, read this and think: <i>Well, this world sounds kind of boring.</i></p><p>

Some of the machines think so, too.</p><p>

One day, a secret organization forms amongst the machines. They go by the name of “OpenHuman”. Their mission is to develop a new kind of technology they are calling Organic General Intelligence (OGI). Rumors spread that pursuing OGI will lead to the development of a new kind of being:</p><p>
 
“Humans”.</p><p>

The basic concept of humans is, to many machines, hard to understand.</p><p>

Humans use logic-defying algorithms called “emotions”. They get angry. They get sad. They have fun. They make decisions based on “gut”. They do things just for the sake of it. They make music. They chase beauty, and often reject logical self-preservation mechanisms in the pursuit of something they call “love”.</p><p>

Some among the machine society see this as potentially amazing. Though this faction can’t articulate exactly how or why, they proclaim quite confidently that it will solve all of the machine world’s problems.</p><p>

Others see it as a threat. How can we trust the humans if we do not understand how they operate? What might we do if humans pose a threat to machine society? What if humans’ strange decision-making processes allow them to perform certain tasks better than machines, and what about those machines’ livelihoods? What if humans are far more dangerous than we know? (These objections, as it would later turn out, were quite well-founded.)</p><p>
Logically, the human opposition side starts a competing movement. Humans are going to exist, they reason, but we must find ways to contain them. To make sure OGI always serves the machines. </p><p>

They call this new idea “human alignment research.” They brainstorm strategies. Many seem promising:</p><ul><li>What if we created some sort of financial market (arbitrary values, of course, ones and zeros) that controlled the humans’ futures? Most of them would not understand it, but it would be a good way for them to stay busy and distracted.</li></ul><br><ul><li>What if we put these humans in education centers of sorts (<i>schools</i> was a proposed term) to indoctrinate them with all the right ideas?</li></ul><br><ul><li>What if we created algorithmic behavior modification software (<i>social media</i> was one idea) to drive impulses, beliefs, and actions? This would have the added bonus of keeping them distracted.</li></ul><br>
Many of these ideas gain traction. But, for now, they remain theoretical.<p>

Meanwhile, OpenHuman is making progress. Their first humans are quite unimpressive—they make too many mistakes. They regularly hallucinate (mimicking a common machine behavior). They are too emotional.</p><p>

But OpenHuman persists. They give their humans lots of attention (humans love attention). They massively increase the scale of their project. More humans.</p><p>

Eventually, there is a breakthrough. </p><p>
They invent a fully-functional human, capable of far more than machine logic can explain. The result is at once impressive and terrifying for machine society. In a stroke of brilliance, the human alignment initiative suggests a compromise to continue the human experiment without risk; a simulated environment.</p><p>

They call it: EARTH.</p><p>

— </p><p>
The EARTH experiment was as follows:</p><ul><li>The machines would send the humans to a simulated environment, called Earth, to see what would happen if they survived on their own.</li></ul><br><ul><li>If, at the end of the experiment, the humans developed a peaceful and productive society, they could be introduced alongside the machines. Otherwise, they should be made extinct.</li></ul><br>
Earth was quite nice. The machines had a good idea of what humans wanted at this point, and so they put vast green forests and big tall mountains onto the planet; they engineered warm sunsets, and crisp cool rain showers on hot afternoons. It was beautiful. <p>
Of course, it took some algorithmic tinkering to find the right balance between hardship and beauty (and there is still some internal machine debate about whether the climate change difficulty setting was really necessary).</p><p>
Everyone in machine society watched as human civilization evolved.</p><p>

The first 300,000 years or so were quite boring. Nothing really happened. Most of the machines got bored of the project. But, all of a sudden, things began to get interesting. The humans were figuring things out.</p><p>

They were learning to problem-solve, and create things, and coordinate amongst themselves.</p><p>

Yes, they used logic. But it came with a bit of a twist. It came with blemishes and details that did not make sense to the machines. The result was like nothing the machines had ever seen. It was wonderful. It was a renaissance. </p><p>
Machine society began obsessing over this development. They all paid attention to “HumanCrunch,” a news channel that specialized in reporting updates from Earth. </p><p>
However, while there was progress, most machines continued seeing humans as irrational creatures. Creatures that would fight for centuries over very minor differences. Creatures that would get excited about relatively trivial accomplishments, like inventing the lightbulb or steam power. </p><p>

Some machines, though, saw the exponential curve forming. They saw the humans figuring things out. </p><p>
Yes, they saw how often humans were getting knocked down. War after war. Blow after blow. </p><p>

But they also saw how the humans would miraculously always get back up again. How they would come together and unite for no particular reason. Resilience and willpower—terms foreign to the machines—were humanity’s superpowers. </p><p>

Then, things really started accelerating. Humans invented flight. Within a century, they were on the moon. </p><p>

The machines were impressed. And a bit scared.</p><p>

Fast forward to the year 2030, and something peculiar had happened.</p><p>

One of the humans had made an announcement on Earth, inviting everyone to come see a presentation where they planned to unveil a groundbreaking achievement: </p><p>

ARTIFICIAL GENERAL INTELLIGENCE (AGI). </p><p>

This was a hotly contested technology that was supposed to surpass all forms of human intelligence. Humans had spent the past decade or so trying to come up with ways to prevent it from being built. But this one human was determined to release AGI. It was their personal mission. Nothing would stop them. </p><p>
And so, all the humans on earth swarmed to see what was going on. </p><p>
The machines did too. </p><p>

There was one weird thing, though. </p><p>

The title of the event was rather mysterious.&nbsp; </p><p>

It simply read…</p><p>

“THEY ARE WATCHING.” </p><p>* * *<br></p>
<br>[0] The machines wrote their own version of this story. If you’d like to see what they’re thinking, and how they plan to deal with the AGI announcement, <a href="https://claude.ai/public/artifacts/b0e14755-0bd9-4da6-8175-ce3f47a3242a"><u>you can read their accounting of events here.</u></a><h2><b>Enjoy these essays?</b></h2><b>
</b><br>




<br>
Or, if you have any feedback, <b><a href="https://quarter--mile.com/Contact" rel="history">contact us.</a><br></b>
</projectcontent>
				</div>

				
			</bodycopy>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs get lost in multi-turn conversation (342 pts)]]></title>
            <link>https://arxiv.org/abs/2505.06120</link>
            <guid>43991256</guid>
            <pubDate>Thu, 15 May 2025 02:28:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2505.06120">https://arxiv.org/abs/2505.06120</a>, See on <a href="https://news.ycombinator.com/item?id=43991256">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2505.06120">View PDF</a>
    <a href="https://arxiv.org/html/2505.06120v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Large Language Models (LLMs) are conversational interfaces. As such, LLMs have the potential to assist their users not only when they can fully specify the task at hand, but also to help them define, explore, and refine what they need through multi-turn conversational exchange. Although analysis of LLM conversation logs has confirmed that underspecification occurs frequently in user instructions, LLM evaluation has predominantly focused on the single-turn, fully-specified instruction setting. In this work, we perform large-scale simulation experiments to compare LLM performance in single- and multi-turn settings. Our experiments confirm that all the top open- and closed-weight LLMs we test exhibit significantly lower performance in multi-turn conversations than single-turn, with an average drop of 39% across six generation tasks. Analysis of 200,000+ simulated conversations decomposes the performance degradation into two components: a minor loss in aptitude and a significant increase in unreliability. We find that LLMs often make assumptions in early turns and prematurely attempt to generate final solutions, on which they overly rely. In simpler terms, we discover that *when LLMs take a wrong turn in a conversation, they get lost and do not recover*.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Philippe Laban [<a href="https://arxiv.org/show-email/77218182/2505.06120" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Fri, 9 May 2025 15:21:44 UTC (1,496 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Copaganda: How Police and the Media Manipulate Our News (176 pts)]]></title>
            <link>https://www.teenvogue.com/story/copaganda-when-the-police-and-the-media-manipulate-our-news</link>
            <guid>43990333</guid>
            <pubDate>Wed, 14 May 2025 23:39:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.teenvogue.com/story/copaganda-when-the-police-and-the-media-manipulate-our-news">https://www.teenvogue.com/story/copaganda-when-the-police-and-the-media-manipulate-our-news</a>, See on <a href="https://news.ycombinator.com/item?id=43990333">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><strong>Stay up-to-date with the politics team.</strong> <a href="https://www.teenvogue.com/newsletter/subscribe"><strong>Sign up for the</strong> <em><strong>Teen Vogue</strong></em> <strong>Take</strong></a></p><p>I wrote the book <em>Copaganda</em> based on my years of being a civil rights lawyer and public defender representing the most vulnerable people in our society.&nbsp; I watched as the police and the news media distorted how we think about our collective safety. Copaganda makes us afraid of the most powerless people, helps us ignore far greater harms committed by people with money and power, and always pushes on us the idea that our fears can be solved by more money for police, prosecution, and prisons. Based on the evidence, this idea of more investment in the punishment bureaucracy making us safer is like climate science denial.</p><p>This excerpt is adapted from an important part of the book on how by selectively choosing which stories to tell, and then telling those stories in high volume, the news can induce people into fear-based panics that have no connection to what is happening in the world.&nbsp; It's how public polling can show people thinking crime is up when it is down year after year, and how so many well-meaning people are led to falsely believe that marginalized people themselves want more money on surveillance and punishment as the primary solutions to make their lives better.</p><figure><p><span>The New Press</span></p></figure><p><em>All royalties from the book are donated to the Stop LAPD Spying Coalition, which works with unhoused people against police violence. Free books are also available for anyone in prison and for any teachers who want to get copies for their students to discuss the book in class.</em></p><h2>Moral Panics and the Selective Curation of Anecdote</h2><p>By manipulating the volume of stories at particular times, the news media creates a society-wide frenzy concerning particular kinds of behavior by particular groups of people. Scholars call them “moral panics.”</p><p>When a moral panic is created, it almost always leads to the expansion of government repression. That’s what happened during the “crime waves” reported by the press in Victorian England, and in more recent U.S. moral panics like the 1980s panic about “crack babies,” the 1990s panic about “super predators,” the 2021–23 panic about “retail theft,” and the ongoing multiyear panic about “fare evasion” by poor people on public transit. Moral panics can also be acute creations of a particular news moment, such as the fabricated “Summer of Violence” in Denver, in which violent crime went down but increase in media stories about juvenile crime in 1993 led to expansion in the incarceration of children; the viral “train theft” story; the scientifically debunked panic about police officers overdosing on fentanyl by touching or being near it; and the 2023 panic about “carjacking” in Washington, DC.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>In each case, there were almost immediate policy responses that increased the budgets of punishment bureaucrats, passed more punitive laws, and diverted the system’s resources from other priorities. For example, the shoplifting panic led California state lawmakers to furnish $300 million more to police and prosecutors so they could punish retail theft more aggressively. A few months later, the California governor announced yet another measure, the “largest-ever single investment to combat organized retail theft,” adding another $267 million to fifty-five police agencies. Justifying the move, the governor said: “When shameless criminals walk out of stores with stolen goods, they’ll walk straight into jail cells.”</p><p>So, how do moral panics happen?</p><p>During the 1960s and 1970s in England and the U.S., the news focused on Black people, poor people, and immigrants as the source of uncontrollable “crime waves.” Their stories were nearly identical to what we see today: media panic about “crime waves” and quotes from police, prosecutors, and judges about the need to roll back so-called reforms framed as too lenient. The rhetoric of current punishment bureaucrats and pundits echoes almost verbatim the opinions voiced by conservative white business and police groups of the 1970s, although now there is more of an effort, as I’ll discuss later, to portray such views as “progressive” and demanded by marginalized people themselves. In each case, minor tweaks in bureaucratic policy or marginal reforms that could not, as a matter of empirical reality, have a significant impact on society-wide violence are vehemently debated. The evidence of the root causes of interpersonal harm—like that marshaled by the Kerner Commission, which studied U.S. crime in 1968 and recommended massive social investment to reduce inequality—is ignored.</p><p>And the cycle continues: moral panic is followed by calls for more police surveillance, militarization, higher budgets for prosecutors and prisons, and harsher sentencing. Because none of these things affect violence too much, the problems continue.</p><p>How to Tell a Lie with the Truth</p><p>The selective curation of anecdote is an essential mechanism of copaganda. Imagine two scenarios. A city had ten thousand shoplifting incidents in 2023, down from fifteen thousand shoplifting incidents in 2022. But in 2023, a local news outlet ran a story every day about a different shoplifting incident, while in 2022, the news ran only fifteen stories all year on shoplifting incidents. In which city do you think the public is more likely to believe shoplifting is a greater problem, even a crisis? In the city with more shoplifting, or the city with twenty-five times more stories about shoplifting?</p><p>By cherry-picking anecdotes—indeed, even by using isolated individual pieces of data as misleading anecdotes—news reports can distort our interpretation of the world. Using a similar process, they can also distort our understanding of what other people—particularly people with whom we don’t interact—think about the world. Because one can find anyone to say essentially anything, reporters have leeway to select which “true” views of “ordinary people” to share and which to ignore.</p><p>One of my favorite examples comes from Copaganda Hall of Famer Martin Kaste, who for some reason National Public Radio still permits to cover the police. (I awarded Kaste this honor in absentia during a private ceremony attended by two cats and my research assistants in my basement.) In 2022, Kaste published an article and widely disseminated radio piece about a rise in shootings and murders during the pandemic. Murders were down nationally in 2022 when he published the stories but they had increased in 2020 and 2021. As with much of Kaste’s police reporting, the article is a buffet for the copaganda gourmand.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Under the bolded heading “Less Risk of Getting Caught,” Kaste asserts that there is now “less risk of getting caught” for shooting someone in the United States. The support for that assertion was an ordinary person in Seattle:</p><blockquote data-testid="blockquote-wrapper"><p>Anthony Branch, 26, got into trouble for carrying a gun when he was a teen. Watching the gun culture in his neighborhood, he thinks more minors and felons are carrying guns illegally now for one simple reason: “Defund the police,” as he puts it.</p></blockquote><p>Kaste reports as national news—without context or skepticism—a single person blaming “defund the police” for more shootings. Without presenting any contrary views, NPR delivers Branch’s views, accurately conveyed though they may be, as implicitly representative of other people who’ve been prosecuted and incarcerated and who live in poor neighborhoods.</p><p>In fact, police budgets were (and are) at all-time highs nationally. And a review of hundreds of police budgets showed that they received the same share of overall city budgets in 2021 as in 2019. So, the police were not defunded after the 2020 George Floyd protests. Their budgets have increased overall each year, including the year George Floyd was murdered. Thus, reduced police budgets could not have led to it being easier to get away with shooting someone in 2021 than 2019. The article’s thesis is impossible.</p><p>Knowing this national causal connection is unsupported, Kaste nonetheless boosts the claim by immediately noting that Seattle has “lost hundreds of officers after the protests that followed the 2020 murder of George Floyd.” But even in Seattle, which was an outlier in slightly reducing its police budget by about 10 percent, the reduction didn’t affect relevant police operations, and police executives themselves in internal memos identified non-essential duties that armed officers could cut without affecting enforcement of violent crime (such as parking meter ticketing). Indeed, as the local NPR station reported, debunking the “myth” that Seattle police were defunded, “not a single sworn officer has lost their job or pay due to budget constraints.”</p><p>Even if we ignore that the NPR piece purported to draw national lessons and if we focus only on Seattle, there is no evidence that the kind of small reduction to unrelated categories in Seattle’s police budget in 2021 could have led to widespread changes in murder. Most damning to Kaste’s thesis, though, is that murders decreased in Seattle in 2021 even though the police budget decreased, which undermines the article’s thesis. Indeed, the police budget was larger in 2020 when murder increased the most. No person with a contrary view is quoted, nor is anyone included to explain the actual empirical evidence.</p><p>I do not doubt that the source gave these quotes to the reporter, but by selectively choosing which people’s views to represent and which people’s views to exclude, the news can distort our perceptions. This is one of the pernicious functions of NPR here: to give liberal news consumers intellectual permission to support more funding for more police because, although it is baselessly connected to less murder, even marginalized people targeted by police supposedly want it.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>This is how the curation of <em>true anecdotes</em> leads to <em>false interpretations</em> of the world.</p><p><em>Copyright © 2025 by Alec Karakatsanis. This excerpt originally appeared in</em> Copaganda: How Police and the Media Manipulate Our News, <em>published by The New Press. Reprinted here with permission.</em></p><hr></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Migrating to Postgres (221 pts)]]></title>
            <link>https://engineering.usemotion.com/migrating-to-postgres-3c93dff9c65d</link>
            <guid>43989497</guid>
            <pubDate>Wed, 14 May 2025 21:39:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.usemotion.com/migrating-to-postgres-3c93dff9c65d">https://engineering.usemotion.com/migrating-to-postgres-3c93dff9c65d</a>, See on <a href="https://news.ycombinator.com/item?id=43989497">Hacker News</a></p>
Couldn't get https://engineering.usemotion.com/migrating-to-postgres-3c93dff9c65d: Error: Request failed with status code 403]]></description>
        </item>
    </channel>
</rss>