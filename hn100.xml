<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 27 Jan 2026 00:30:15 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[ChatGPT Containers can now run bash, pip/npm install packages and download files (126 pts)]]></title>
            <link>https://simonwillison.net/2026/Jan/26/chatgpt-containers/</link>
            <guid>46770221</guid>
            <pubDate>Mon, 26 Jan 2026 19:19:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2026/Jan/26/chatgpt-containers/">https://simonwillison.net/2026/Jan/26/chatgpt-containers/</a>, See on <a href="https://news.ycombinator.com/item?id=46770221">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-permalink-context="/2026/Jan/26/chatgpt-containers/">

<p>26th January 2026</p>



<p>One of my favourite features of ChatGPT is its ability to write and execute code in a container. This feature launched as ChatGPT Code Interpreter <a href="https://simonwillison.net/2023/Apr/12/code-interpreter/">nearly three years ago</a>, was half-heartedly rebranded to ‚ÄúAdvanced Data Analysis‚Äù at some point and is generally really difficult to find detailed documentation about. Case in point: it appears to have had a <em>massive</em> upgrade at some point in the past few months, and I can‚Äôt find documentation about the new capabilities anywhere!</p>
<p>Here are the most notable new features:</p>
<ol>
<li>ChatGPT can <strong>directly run Bash commands</strong> now. Previously it was limited to Python code only, although it could run shell commands via the Python <code>subprocess</code> module.</li>
<li>
<strong>It has Node.js</strong> and can run JavaScript directly in addition to Python. I also got it to run ‚Äúhello world‚Äù in <strong>Ruby, Perl, PHP, Go, Java, Swift, Kotlin, C and C++</strong>. No Rust yet though!</li>
<li>While the container still can‚Äôt make outbound network requests, <strong><code>pip install package</code> and <code>npm install package</code> both work</strong> now via a custom proxy mechanism.</li>
<li>ChatGPT can locate the URL for a file on the web and use a <code>container.download</code> tool to <strong>download that file and save it to a path</strong> within the sandboxed container.</li>
</ol>
<p>This is a substantial upgrade! ChatGPT can now write and then test code in 10 new languages (11 if you count Bash), can find files online and download them into the container, and can install additional packages via <code>pip</code> and <code>npm</code> to help it solve problems.</p>
<p>(OpenAI <em>really</em> need to develop better habits at <a href="https://help.openai.com/en/articles/6825453-chatgpt-release-notes">keeping their release notes up-to-date</a>!)</p>
<p>I was initially suspicious that maybe I‚Äôd stumbled into a new preview feature that wasn‚Äôt available to everyone, but I <a href="https://chatgpt.com/share/6977aa7c-7bd8-8006-8129-8c9e25126fed">tried some experiments</a> in a free ChatGPT account and confirmed that the new features are available there as well.</p>

<ul>
  <li><a href="https://simonwillison.net/2026/Jan/26/chatgpt-containers/#container-download">container.download</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/26/chatgpt-containers/#is-container-download-a-data-exfiltration-vulnerability-">Is container.download a data exfiltration vulnerability?</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/26/chatgpt-containers/#bash-and-other-languages">Bash and other languages</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/26/chatgpt-containers/#installing-packages-from-pip-and-npm">Installing packages from pip and npm</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/26/chatgpt-containers/#adding-it-all-together">Adding it all together</a></li>
</ul>

<h4 id="container-download">container.download</h4>
<p>My first clue to the new features came the other day when I got curious about Los Angeles air quality, in particular has the growing number of electric vehicles there hade a measurable impact?</p>
<p>I prompted a fresh GPT-5.2 Thinking session with:</p>
<blockquote>
<p>Show me Los Angeles air quality over time for last 20 years</p>
</blockquote>
<p>Here‚Äôs the <a href="https://chatgpt.com/share/6977a80d-e6e4-8006-b4fe-ed8478fe99d8">shared transcript</a>. Annoyingly that doesn‚Äôt make the thinking trace available, so here‚Äôs a screenshot:</p>
<p><img src="https://static.simonwillison.net/static/2026/container-download.jpg" alt="Exploring methods to fetch AQI data and reports Downloading AQI data for Los Angeles  I found an xlsx file titled &quot;Air Quality Statistics by City, 2000-2023&quot;, which should include data for Los Angeles. While user requested data for Using provided URL to download file  The URL from the error message leads to an Excel file containing air quality trends. I can use the container.download tool to fetch it. Let‚Äôs proceed with this approach and see if it works out for downloading the file directly! Analyzed Parsing and analyzing the data for Los Angeles  Now that we've downloaded the data, I'll parse it using Python. First, I'll focus on identifying the row(s) corresponding to Los Angeles, which may be labeled under &quot;Los Angeles-Long Beach-Anaheim, CA.&quot; Afterward, I'll plot metrics like &quot;Median AQI&quot; and &quot;Days Good&quot; over time to visualize trends. Let‚Äôs begin!"></p>
<p>This bit instantly caught my eye:</p>
<blockquote>
<p>[...] The URL from the error message leads to an Excel file containing air quality trends. I can use the container.download tool to fetch it. [...]</p>
</blockquote>
<p>What‚Äôs <code>container.download</code>? I <a href="https://chatgpt.com/share/6977aab0-b874-8006-9b54-d3d216ed884b">asked ChatGPT</a>:</p>
<blockquote>
<p>Without running any searches tell me about your container.download tool</p>
</blockquote>
<p>And got back:</p>
<blockquote>
<p>Sure. <code>container.download</code> is a built-in tool I can call that <strong>fetches a file from a URL and saves it into the sandboxed container filesystem</strong> for this chat session.</p>
<h4 id="what-it-does">What it does</h4>
<ul>
<li>Takes a <strong>publicly reachable URL</strong> and a <strong>destination filepath</strong> in the container.</li>
<li>Downloads the bytes from that URL and writes them to the given path.</li>
<li>After that, I can read/process the file locally in the container (e.g., unzip it, parse it with Python, open it as an image, convert it, etc.).</li>
</ul>
<h4 id="signature-inputs-">Signature (inputs)</h4>
<p>It‚Äôs called like this (conceptually):</p>
<div><pre>{
  <span>"url"</span>: <span><span>"</span>https://example.com/somefile.zip<span>"</span></span>,
  <span>"filepath"</span>: <span><span>"</span>/mnt/data/somefile.zip<span>"</span></span>
}</pre></div>
</blockquote>
<p>Further experimentation confirmed that <code>container.download</code> behaves as described.</p>
<p>I had it download one file from a server with logs enabled and captured the IP address and user-agent. Here are the most interesting headers:</p>
<pre><code>User-Agent: Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko); compatible; ChatGPT-User/1.0; +https://openai.com/bot
Accept: text/html, application/xhtml+xml, application/xml;q=0.9, image/avif, image/webp, image/apng, */*;q=0.8, application/signed-exchange;v=b3;q=0.9
Cf-Connecting-Ip: 52.230.164.178
</code></pre>
<p>That <code>52.230.164.178</code> IP address <a href="http://ip-api.com/json/52.230.164.178">resolves to</a> Microsoft Azure Cloud (centralus) in Des Moines, Iowa.</p>
<h4 id="is-container-download-a-data-exfiltration-vulnerability-">Is container.download a data exfiltration vulnerability?</h4>
<p>On the one hand, this is really useful! ChatGPT can navigate around websites looking for useful files, download those files to a container and then process them using Python or other languages.</p>
<p>Is this a data exfiltration vulnerability though? Could a prompt injection attack trick ChatGPT into leaking private data out to a <code>container.download</code> call to a URL with a query string that includes sensitive information?</p>
<p>I don‚Äôt think it can. I tried getting it to assemble a URL with a query string and access it using <code>container.download</code> and it couldn‚Äôt do it. It told me that it got back this error:</p>
<blockquote>
<p>ERROR: download failed because url not viewed in conversation before. open the file or url using web.run first.</p>
</blockquote>
<p>This looks to me like the same safety trick <a href="https://simonwillison.net/2025/Sep/10/claude-web-fetch-tool/">used by Claude‚Äôs Web Fetch tool</a>: only allow URL access if that URL was either directly entered by the user or if it came from search results that could not have been influenced by a prompt injection.</p>
<p>(I poked at this a bit more and managed to get a simple constructed query string to pass through <code>web.run</code>‚Äîa different tool entirely‚Äîbut when I tried to compose a longer query string containing the previous prompt history a <code>web.run</code> filter blocked it.)</p>
<p>So I <em>think</em> this is all safe, though I‚Äôm curious if it could hold firm against a more aggressive round of attacks from a seasoned security researcher.</p>
<h4 id="bash-and-other-languages">Bash and other languages</h4>
<p>The key lesson from coding agents like Claude Code and Codex CLI is that Bash rules everything: if an agent can run Bash commands in an environment it can do almost anything that can be achieved by typing commands into a computer.</p>
<p>When Anthropic added their own code interpreter feature to Claude <a href="https://simonwillison.net/2025/Sep/9/claude-code-interpreter/">last September</a> they built that around Bash rather than just Python. It looks to me like OpenAI have now done the same thing for ChatGPT.</p>
<p>Here‚Äôs what ChatGPT looks like when it runs a Bash command‚Äîhere my prompt was:</p>
<blockquote>
<p>npm install a fun package and demonstrate using it</p>
</blockquote>
<p><img src="https://static.simonwillison.net/static/2026/chatgpt-bash.jpg" alt="Screenshot of ChatGPT 5.2 Thinking interface with Activity panel. Main chat shows user prompt &quot;npm install a fun package and demonstrate using it&quot; and response &quot;Thought for 32s > Yep ‚Äî here's a fun one: cowsay üêÑ I installed it with npm and ran a tiny Node script:&quot; followed by bash code block containing &quot;mkdir -p /mnt/data/npmfun cd /mnt/data/npmfun npm init -y npm install cowsay --no-fund --no-audit node -e &quot;const cowsay=require('cowsay'); console.log(cowsay.say({text:'Hel&quot; and Output: section showing ASCII cow art saying &quot;< Hello from npm inside the container! >&quot; with cow drawing made of characters. Activity panel on right (32s) shows: &quot;Running npm install in container&quot; with bash command and output &quot;v22.16.0 10.9.2&quot;, &quot;Inspecting npm command output and container status&quot; with directory listing showing &quot;drwxr-xr-x 2 root oai_shared 80 Jan 26 17:32 . drwxrwxr-x 2 oai oai_shared 60 Jan 26 17:32 .. -rw-r--r-- 1 root root&quot;, &quot;Re-running npm install&quot; note about node_modules missing, and &quot;Rerunning npm install with output&quot; showing &quot;added 41 packages in 2s&quot;."></p>
<p>It‚Äôs useful to click on the ‚ÄúThinking‚Äù or ‚ÄúThought for 32s‚Äù links as that opens the Activity sidebar with a detailed trace of what ChatGPT did to arrive at its answer. This helps guard against cheating‚ÄîChatGPT might claim to have run Bash in the main window but it can‚Äôt fake those black and white logs in the Activity panel.</p>
<p>I had it run <a href="https://chatgpt.com/share/6977b890-f1c0-8006-b39a-9a02c7ca1993">Hello World in various languages</a> later in that same session.</p>
<p><img src="https://static.simonwillison.net/static/2026/chatgpt-hello-worlds.jpg" alt="Terminal screenshot showing &quot;Hello, world&quot; programs in multiple languages. Top section shows code: swift -e 'print(&quot;Hello, world from Swift!&quot;)' followed by cat > hello.kts <<'KTS' / println(&quot;Hello, world from Kotlin!&quot;) / KTS / kotlinc -script hello.kts, then cat > hello.c <<'C' / #include <stdio.h> / int main(){ puts(&quot;Hello, world from C!&quot;); return 0; } / C / gcc hello.c -o hello_c / ./hello_c, then cat > hello.cpp <<'CPP' / #include <iostream> / int main(){ std::cout << &quot;Hello, world from C++!&quot; << std::endl; } / CPP / g++ hello.cpp -o hello_cpp / ./hello_cpp. Bottom section shows output: Hello, world from Ruby! / Hello, world from Perl! / Hello, world from PHP! / Hello, world from Go! / Hello, world from Java! / Hello, world from Swift! / Hello, world from Kotlin! / Hello, world from C! / Hello, world from C++!. UI shows &quot;Thought for 2m 29s&quot; and &quot;Done&quot; at bottom."></p>
<h4 id="installing-packages-from-pip-and-npm">Installing packages from pip and npm</h4>
<p>In the previous example ChatGPT installed the <code>cowsay</code> package from npm and used it to draw an ASCII-art cow. But how could it do that if the container can‚Äôt make outbound network requests?</p>
<p>In <a href="https://chatgpt.com/share/69773501-b6d8-8006-bbf2-fa644561aa26">another session</a> I challenged it to explore its environment. and figure out how that worked.</p>
<p>Here‚Äôs <a href="https://github.com/simonw/research/blob/main/chatgpt-container-environment/README.md">the resulting Markdown report</a> it created.</p>
<p>The key magic appears to be a <code>applied-caas-gateway1.internal.api.openai.org</code> proxy, available within the container and with various packaging tools configured to use it.</p>
<p>The following environment variables cause <code>pip</code> and <code>uv</code> to install packages from that proxy instead of directly from PyPI:</p>
<pre><code>PIP_INDEX_URL=https://reader:****@packages.applied-caas-gateway1.internal.api.openai.org/.../pypi-public/simple
PIP_TRUSTED_HOST=packages.applied-caas-gateway1.internal.api.openai.org
UV_INDEX_URL=https://reader:****@packages.applied-caas-gateway1.internal.api.openai.org/.../pypi-public/simple
UV_INSECURE_HOST=https://packages.applied-caas-gateway1.internal.api.openai.org
</code></pre>
<p>This one appears to get <code>npm</code> to work:</p>
<pre><code>NPM_CONFIG_REGISTRY=https://reader:****@packages.applied-caas-gateway1.internal.api.openai.org/.../npm-public
</code></pre>
<p>And it reported these suspicious looking variables as well:</p>
<pre><code>CAAS_ARTIFACTORY_BASE_URL=packages.applied-caas-gateway1.internal.api.openai.org
CAAS_ARTIFACTORY_PYPI_REGISTRY=.../artifactory/api/pypi/pypi-public
CAAS_ARTIFACTORY_NPM_REGISTRY=.../artifactory/api/npm/npm-public
CAAS_ARTIFACTORY_GO_REGISTRY=.../artifactory/api/go/golang-main
CAAS_ARTIFACTORY_MAVEN_REGISTRY=.../artifactory/maven-public
CAAS_ARTIFACTORY_GRADLE_REGISTRY=.../artifactory/gradle-public
CAAS_ARTIFACTORY_CARGO_REGISTRY=.../artifactory/api/cargo/cargo-public/index
CAAS_ARTIFACTORY_DOCKER_REGISTRY=.../dockerhub-public
CAAS_ARTIFACTORY_READER_USERNAME=reader
CAAS_ARTIFACTORY_READER_PASSWORD=****
NETWORK=caas_packages_only
</code></pre>
<p>Neither Rust nor Docker are installed in the container environment, but maybe those registry references are a clue of features still to come.</p>
<h4 id="adding-it-all-together">Adding it all together</h4>
<p>The result of all of this? You can tell ChatGPT to use Python or Node.js packages as part of a conversation and it will be able to install them and apply them against files you upload or that it downloads from the public web. That‚Äôs <em>really</em> cool.</p>
<p>The big missing feature here should be the easiest to provide: we need <strong>official documentation</strong>! A release notes entry would be a good start, but there are a lot of subtle details to how this new stuff works, its limitations and what it can be used for.</p>
<p>As always, I‚Äôd also encourage OpenAI to come up with a name for this set of features that properly represents how it works and what it can do.</p>
<p>In the meantime, I‚Äôm going to call this <strong>ChatGPT Containers</strong>.</p>
<h4 id="list-of-tools">Update: a full list of tools</h4>
<p>I decided to ask ChatGPT about other tools that were available to it in case there was anything interesting in there:</p>
<blockquote><p>List all tools that are available to you, with their exact names and descriptions and signatures</p></blockquote>
<p>Here‚Äôs <a href="https://chatgpt.com/share/6977ffa0-df14-8006-9647-2b8c90ccbb81">what I got back</a>.</p>

<details><summary>List of ChatGPT tools according to GPT-5.2 Thinking on 26th January 2026</summary>
<p>Below are <strong>all tools available to me in this chat</strong>, with their <strong>exact names</strong>, <strong>descriptions</strong>, and <strong>signatures</strong> (as defined in my tool interface).</p>
<hr>
<h4 id="python">python</h4>
<h5 id="python-exec-freeform-any">
<code>python.exec</code> (FREEFORM) ‚Üí any</h5>
<p><strong>Description:</strong> Execute Python code in a Jupyter environment for <em>private/internal reasoning</em> (not shown to you). Internet access is disabled.<br>
<strong>Signature:</strong> <code>python.exec(FREEFORM) -&gt; any</code></p>
<hr>
<h4 id="web">web</h4>
<h5 id="web-run-toolcallv5-json-any">
<code>web.run</code> (ToolCallV5 JSON) ‚Üí any</h5>
<p><strong>Description:</strong> Access the internet and specialized lookups (search, open/click/find pages, screenshot PDFs, image search, product search, finance, weather, sports, calculator, time). Supports citations via returned <code>turn...</code> reference IDs.<br>
<strong>Signature:</strong><br>
<code>web.run({ open?, click?, find?, screenshot?, image_query?, product_query?, sports?, finance?, weather?, calculator?, time?, search_query?, response_length? }) -&gt; any</code></p>
<hr>
<h4 id="automations">automations</h4>
<h5 id="automations-create-any">
<code>automations.create</code> ‚Üí any</h5>
<p><strong>Description:</strong> Schedule a future/recurring task/reminder/search via iCal VEVENT.<br>
<strong>Signature:</strong><br>
<code>automations.create({ prompt: string, title: string, schedule?: string, dtstart_offset_json?: string }) -&gt; any</code></p>
<h5 id="automations-update-any">
<code>automations.update</code> ‚Üí any</h5>
<p><strong>Description:</strong> Modify an existing scheduled task.<br>
<strong>Signature:</strong><br>
<code>automations.update({ jawbone_id: string, schedule?: string, dtstart_offset_json?: string, prompt?: string, title?: string, is_enabled?: boolean }) -&gt; any</code></p>
<h5 id="automations-list-any">
<code>automations.list</code> ‚Üí any</h5>
<p><strong>Description:</strong> List existing automations.<br>
<strong>Signature:</strong> <code>automations.list() -&gt; any</code></p>
<hr>
<h4 id="guardian-tool">guardian_tool</h4>
<h5 id="guardian-tool-get-policy-str">
<code>guardian_tool.get_policy</code> ‚Üí str</h5>
<p><strong>Description:</strong> Retrieve policy guidance for specific categories (currently: <code>election_voting</code>).<br>
<strong>Signature:</strong> <code>guardian_tool.get_policy({ category: string }) -&gt; str</code></p>
<hr>
<h4 id="file-search">file_search</h4>
<h5 id="file-search-msearch-any">
<code>file_search.msearch</code> ‚Üí any</h5>
<p><strong>Description:</strong> Search across available internal/connected sources (in this environment: primarily ‚Äúrecording_knowledge‚Äù transcripts when relevant).<br>
<strong>Signature:</strong><br>
<code>file_search.msearch({ queries?: string[], source_filter?: string[], file_type_filter?: string[], intent?: string, time_frame_filter?: { start_date?: string, end_date?: string } }) -&gt; any</code></p>
<h5 id="file-search-mclick-any">
<code>file_search.mclick</code> ‚Üí any</h5>
<p><strong>Description:</strong> Expand/open previously returned search results (or connector URLs) for more context.<br>
<strong>Signature:</strong><br>
<code>file_search.mclick({ pointers?: string[], start_date?: string, end_date?: string }) -&gt; any</code></p>
<hr>
<h4 id="gmail-read-only-">gmail (read-only)</h4>
<h5 id="gmail-search-email-ids-any">
<code>gmail.search_email_ids</code> ‚Üí any</h5>
<p><strong>Description:</strong> Search Gmail message IDs by query/tags (read-only).<br>
<strong>Signature:</strong><br>
<code>gmail.search_email_ids({ query?: string, tags?: string[], max_results?: integer, next_page_token?: string }) -&gt; any</code></p>
<h5 id="gmail-batch-read-email-any">
<code>gmail.batch_read_email</code> ‚Üí any</h5>
<p><strong>Description:</strong> Read a batch of Gmail messages by message IDs (read-only).<br>
<strong>Signature:</strong><br>
<code>gmail.batch_read_email({ message_ids: string[] }) -&gt; any</code></p>
<hr>
<h4 id="gcal-read-only-">gcal (read-only)</h4>
<h5 id="gcal-search-events-any">
<code>gcal.search_events</code> ‚Üí any</h5>
<p><strong>Description:</strong> Search Google Calendar events in a time range and/or by keyword (read-only).<br>
<strong>Signature:</strong><br>
<code>gcal.search_events({ time_min?: string, time_max?: string, timezone_str?: string, max_results?: integer, query?: string, calendar_id?: string, next_page_token?: string }) -&gt; any</code></p>
<h5 id="gcal-read-event-any">
<code>gcal.read_event</code> ‚Üí any</h5>
<p><strong>Description:</strong> Read a specific calendar event by ID (read-only).<br>
<strong>Signature:</strong><br>
<code>gcal.read_event({ event_id: string, calendar_id?: string }) -&gt; any</code></p>
<hr>
<h4 id="gcontacts-read-only-">gcontacts (read-only)</h4>
<h5 id="gcontacts-search-contacts-any">
<code>gcontacts.search_contacts</code> ‚Üí any</h5>
<p><strong>Description:</strong> Search Google Contacts by keyword (read-only).<br>
<strong>Signature:</strong><br>
<code>gcontacts.search_contacts({ query: string, max_results?: integer }) -&gt; any</code></p>
<hr>
<h4 id="canmore-canvas-">canmore (canvas)</h4>
<h5 id="canmore-create-textdoc-any">
<code>canmore.create_textdoc</code> ‚Üí any</h5>
<p><strong>Description:</strong> Create a new canvas text document (markdown or code).<br>
<strong>Signature:</strong><br>
<code>canmore.create_textdoc({ name: string, type: "document" | "code/bash" | "code/zsh" | "code/javascript" | "code/typescript" | "code/html" | "code/css" | "code/python" | "code/json" | "code/sql" | "code/go" | "code/yaml" | "code/java" | "code/rust" | "code/cpp" | "code/swift" | "code/php" | "code/xml" | "code/ruby" | "code/haskell" | "code/kotlin" | "code/csharp" | "code/c" | "code/objectivec" | "code/r" | "code/lua" | "code/dart" | "code/scala" | "code/perl" | "code/commonlisp" | "code/clojure" | "code/ocaml" | "code/powershell" | "code/verilog" | "code/dockerfile" | "code/vue" | "code/react" | "code/other", content: string }) -&gt; any</code></p>
<h5 id="canmore-update-textdoc-any">
<code>canmore.update_textdoc</code> ‚Üí any</h5>
<p><strong>Description:</strong> Apply regex-based edits to the current canvas textdoc.<br>
<strong>Signature:</strong><br>
<code>canmore.update_textdoc({ updates: { pattern: string, multiple?: boolean, replacement: string }[] }) -&gt; any</code></p>

<p><strong>Description:</strong> Add actionable comments on matched regions of the current canvas textdoc.<br>
<strong>Signature:</strong><br>
<code>canmore.comment_textdoc({ comments: { pattern: string, comment: string }[] }) -&gt; any</code></p>
<hr>
<h4 id="python-user-visible">python_user_visible</h4>
<h5 id="python-user-visible-exec-freeform-any">
<code>python_user_visible.exec</code> (FREEFORM) ‚Üí any</h5>
<p><strong>Description:</strong> Execute Python code <em>that you will see</em> (tables, plots, generated files). Internet disabled.<br>
<strong>Signature:</strong> <code>python_user_visible.exec(FREEFORM) -&gt; any</code></p>
<hr>
<h4 id="user-info">user_info</h4>
<h5 id="user-info-get-user-info-any">
<code>user_info.get_user_info</code> ‚Üí any</h5>
<p><strong>Description:</strong> Get your current location and local time (or UTC if unknown).<br>
<strong>Signature:</strong> <code>user_info.get_user_info({}) -&gt; any</code></p>
<hr>
<h4 id="summary-reader">summary_reader</h4>
<h5 id="summary-reader-read-any">
<code>summary_reader.read</code> ‚Üí any</h5>
<p><strong>Description:</strong> Read safe-to-share internal reasoning summaries from earlier turns (used when asked how I arrived at something).<br>
<strong>Signature:</strong> <code>summary_reader.read({ limit?: integer, offset?: integer }) -&gt; any</code></p>
<hr>
<h4 id="container">container</h4>
<h5 id="container-exec-any">
<code>container.exec</code> ‚Üí any</h5>
<p><strong>Description:</strong> Run a command inside the container and return stdout/stderr.<br>
<strong>Signature:</strong><br>
<code>container.exec({ cmd: string[], session_name?: string | null, workdir?: string | null, timeout?: integer | null, env?: object | null, user?: string | null }) -&gt; any</code></p>
<h5 id="container-feed-chars-any">
<code>container.feed_chars</code> ‚Üí any</h5>
<p><strong>Description:</strong> Send input to a running interactive exec session and flush output.<br>
<strong>Signature:</strong><br>
<code>container.feed_chars({ session_name: string, chars: string, yield_time_ms?: integer }) -&gt; any</code></p>
<h5 id="container-open-image-any">
<code>container.open_image</code> ‚Üí any</h5>
<p><strong>Description:</strong> Display an image from an absolute path in the container (jpg/jpeg/png/webp).<br>
<strong>Signature:</strong><br>
<code>container.open_image({ path: string, user?: string | null }) -&gt; any</code></p>
<h5 id="container-download-any">
<code>container.download</code> ‚Üí any</h5>
<p><strong>Description:</strong> Download a file from a URL into the container filesystem.<br>
<strong>Signature:</strong><br>
<code>container.download({ url: string, filepath: string }) -&gt; any</code></p>
<hr>
<h4 id="bio-memory-">bio (memory)</h4>
<h5 id="bio-update-freeform-any">
<code>bio.update</code> (FREEFORM) ‚Üí any</h5>
<p><strong>Description:</strong> Persist (or forget) user-specific info across conversations (‚Äúmemory‚Äù).<br>
<strong>Signature:</strong> <code>bio.update(FREEFORM) -&gt; any</code></p>
<hr>
<h4 id="api-tool">api_tool</h4>
<h5 id="api-tool-list-resources-any">
<code>api_tool.list_resources</code> ‚Üí any</h5>
<p><strong>Description:</strong> List available ‚Äúresources‚Äù (including invokable tools) in an OP-style file tree.<br>
<strong>Signature:</strong><br>
<code>api_tool.list_resources({ path?: string, cursor?: string | null, only_tools?: boolean, refetch_tools?: boolean }) -&gt; any</code></p>
<h5 id="api-tool-call-tool-any">
<code>api_tool.call_tool</code> ‚Üí any</h5>
<p><strong>Description:</strong> Invoke a resource as a tool (must know its full path/URI from <code>list_resources</code>).<br>
<strong>Signature:</strong><br>
<code>api_tool.call_tool({ path: string, args: object }) -&gt; any</code></p>
<hr>
<h4 id="personal-context">personal_context</h4>
<h5 id="personal-context-search-any">
<code>personal_context.search</code> ‚Üí any</h5>
<p><strong>Description:</strong> Retrieve user-specific context from prior interactions/linked sources to maintain continuity.<br>
<strong>Signature:</strong><br>
<code>personal_context.search({ query: string }) -&gt; any</code></p>
<hr>
<h4 id="image-gen">image_gen</h4>
<h5 id="image-gen-text2im-any">
<code>image_gen.text2im</code> ‚Üí any</h5>
<p><strong>Description:</strong> Generate or edit images from text instructions (and optionally referenced image IDs).<br>
<strong>Signature:</strong><br>
<code>image_gen.text2im({ prompt?: string | null, size?: string | null, n?: integer | null, transparent_background?: boolean | null, is_style_transfer?: boolean | null, referenced_image_ids?: string[] | null }) -&gt; any</code></p>
<hr>
<h4 id="user-settings">user_settings</h4>
<h5 id="user-settings-get-user-settings-any">
<code>user_settings.get_user_settings</code> ‚Üí any</h5>
<p><strong>Description:</strong> Read current settings and allowed values (personality, accent color, appearance).<br>
<strong>Signature:</strong> <code>user_settings.get_user_settings() -&gt; any</code></p>
<h5 id="user-settings-set-setting-any">
<code>user_settings.set_setting</code> ‚Üí any</h5>
<p><strong>Description:</strong> Change one setting (accent color, appearance, or personality) to an allowed value.<br>
<strong>Signature:</strong><br>
<code>user_settings.set_setting({ setting_name: "accent_color" | "appearance" | "personality", setting_value: string }) -&gt; any</code></p>
<hr>
<h4 id="artifact-handoff">artifact_handoff</h4>
<h5 id="artifact-handoff-prepare-artifact-generation-any">
<code>artifact_handoff.prepare_artifact_generation</code> ‚Üí any</h5>
<p><strong>Description:</strong> Must be called immediately when the user asks for a spreadsheet or slide deck artifact.<br>
<strong>Signature:</strong> <code>artifact_handoff.prepare_artifact_generation() -&gt; any</code></p>
</details>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When AI 'builds a browser,' check the repo before believing the hype (189 pts)]]></title>
            <link>https://www.theregister.com/2026/01/26/cursor_opinion/</link>
            <guid>46769965</guid>
            <pubDate>Mon, 26 Jan 2026 18:58:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2026/01/26/cursor_opinion/">https://www.theregister.com/2026/01/26/cursor_opinion/</a>, See on <a href="https://news.ycombinator.com/item?id=46769965">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p><span>Opinion</span> AI-integrated development environment (IDE) company Cursor recently <a target="_blank" rel="nofollow" href="https://cursor.com/blog/scaling-agents">implied</a> it had built a working web browser almost entirely with its AI agents. I won't say they lied, but CEO Michael Truell certainly <a target="_blank" rel="nofollow" href="https://x.com/mntruell/status/2011562190286045552">tweeted</a>: "We built a browser with GPT-5.2 in Cursor."</p>
<p>He followed up with: "It's 3M+ lines of code across thousands of files. The rendering engine is from-scratch in Rust with HTML parsing, CSS cascade, layout, text shaping, paint, and a custom JS VM."</p>
<p>That sounds impressive, doesn't it? He also added: "It *kind of* works," which is not the most ringing endorsement. Still, numerous news sources and social media chatterboxes ran with the news that <a target="_blank" rel="nofollow" href="https://medium.com/write-a-catalyst/ai-agents-built-a-web-browser-in-one-week-and-that-should-make-us-pause-1be4fab67d03">AI built a web browser in a week</a>.</p>

    

<p>Too bad it wasn't true. If you actually looked at Cursor engineer Wilson Lin's blog post about <a target="_blank" rel="nofollow" href="https://github.com/wilsonzlin/fastrender">FastRender</a>, the AI-created web browser, you won't see much boasting about a working web browser. Instead, there's a video of a web browser sort of working, and a much less positive note that "building a browser from scratch is extremely difficult."</p>

        


        

<p>The thing about making such a software announcement on GitHub is that while the headlines are proclaiming another AI victory, developers have this nasty trick. They actually git the code and try it out.</p>
<p><a target="_blank" rel="nofollow" href="https://emsh.cat/cursor-implied-success-without-evidence/">Developers quickly discovered the "browser" barely compiles</a>, often does not run, and was heavily misrepresented in marketing.</p>

        

<p>As a techie, the actual blog post about how they tried and didn't really succeed was much more interesting. Of course, that Cursor sicced hundreds of GPT-5.2-style agents which ran for a week to produce three million lines of new code, to produce, at best, a semi-functional web browser from scratch, doesn't make for a good headline.</p>
<p>According to Perplexity, my AI chatbot of choice, this week‚Äëlong autonomous browser experiment consumed in the order of 10-20 trillion tokens and would have cost several million dollars at then‚Äëcurrent list prices for frontier models.</p>
<p>I'd just cloned a copy of Chromium myself, and for all that time and money, independent developers who cloned the repo reported that the codebase is very far from a functional browser. Recent commits do not compile cleanly, GitHub Actions runs on <code>main</code> are failing, and reviewers could not find a single recent commit that was built without errors.</p>

        

<p>Where builds succeeded after manual patching, performance was abysmal, with reports of pages taking around a minute to load and a heavy reliance on existing projects like <a target="_blank" rel="nofollow" href="https://servo.org/">Servo</a>, a Rust-based web rendering engine, and QuickJS, a JavaScript engine, despite "from scratch" claims.</p>
<p>Lin defended the project on Y Combinator, <a target="_blank" rel="nofollow" href="https://news.ycombinator.com/item?id=46650998">saying</a>, for instance: "The JS engine used a custom JS VM being developed in vendor/ecma-rs as part of the browser, which is a copy of my personal JS parser project vendored to make it easier to commit to." If it's derived from his personal JavaScript parser, that's not really from scratch, is it? Nor is it, from the sound of the argument, written by AI.</p>
<ul>

<li><a href="https://www.theregister.com/2026/01/16/linus_torvalds_vibe_coding/">Just because Linus Torvalds vibe codes doesn't mean it's a good idea</a></li>

<li><a href="https://www.theregister.com/2026/01/09/microsoft_365_copilot_app/">The Microsoft 365 Copilot app rebrand was bad, but there are far worse offenders</a></li>

<li><a href="https://www.theregister.com/2025/12/31/long_lived_tech/">The most durable tech is boring, old, and everywhere</a></li>

<li><a href="https://www.theregister.com/2025/12/22/what_linux_desktop_really_needs/">What the Linux desktop really needs to challenge Windows</a></li>
</ul>
<p>Gregory Terzian, a Servo maintainer, <a target="_blank" rel="nofollow" href="https://news.ycombinator.com/item?id=46709191">responded</a>: "The actual code is worse; I can only describe it as a tangle of spaghetti... I can't make much, if anything, out of it." He then gave the backhanded compliment: "So I agree this isn't just wiring up of dependencies, and neither is it copied from existing implementations: it's a uniquely bad design that could never support anything resembling a real-world web engine." Now that's a burn.</p>
<p>From where I sit, what makes the Cursor case more dangerous than just a failed hack‚Äëweek project is that the hype is baked into its methodology. The "experiment" wasn't presented as what it really was: an interesting, but messy, internal learning exercise. No, it was rolled out as a milestone that conveniently confirmed the company's long‚Äërunning autonomous agent advertising. Missing from the story were basics any senior engineer would demand: passing Continuous Integration (CI), reproducible builds, and real benchmarks that show the browser doing more than limping through a hello-world page.</p>
<p>Zoom out, and CEOs are still predicting that AI will write 90 percent of code in a year, while most enterprise AI pilots still fail to deliver meaningful return on investment.</p>
<p>We're now in a kind of AI uncanny valley for developers. Sure, tools like Cursor can be genuinely helpful as glorified autocomplete and refactoring assistants, but marketing keeps insisting junior engineers can take whole projects from spec to shipping. When you start believing your own sizzle reel, you stop doing the tedious validation work that separates a demo from a deliverable.</p>
<p>Enough already. The hype has grown cold. Sarah Friar, OpenAI's CFO, recently blogged that in 2026, its focus would be on "<a target="_blank" href="https://www.theregister.com/2026/01/20/openai_money/">practical adoption</a>." Let's see real-world practical results first, and then we can talk about practical AI adoption. ¬Æ</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Books removed all search functions for any books with previews (182 pts)]]></title>
            <link>https://old.reddit.com/r/google/comments/1qn1hk1/google_has_seemingly_entirely_removed_search/</link>
            <guid>46769201</guid>
            <pubDate>Mon, 26 Jan 2026 18:05:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/google/comments/1qn1hk1/google_has_seemingly_entirely_removed_search/">https://old.reddit.com/r/google/comments/1qn1hk1/google_has_seemingly_entirely_removed_search/</a>, See on <a href="https://news.ycombinator.com/item?id=46769201">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Title. A few people have posted about this, but I figured out precisely what they did.</p>

<p>There are three kinds of view on google books: full view, snippet view and preview view. Full view is only for public domain books. For in copyright works, you have preview and snippet. Preview lets you see some full pages while snippet view lets you see a chunk and some sentences.</p>

<p>There was some change a month ago that knocked out a lot of books from search, and I am not entirely sure what that change was. But a few days ago they removed ALL search functions for any books with previews, which are disproportionately modern books. This has made Google Books effectively useless. Snippet view remains and is still searchable.</p>

<p>Bizarrely, if you check the individual books that used to have search function enabled - the previews are still there! You can read full pages! However, the search/OCR is completely turned off. You can search a word while staring at a page that has that word and it will tell you 0 results. They also did not turn off OCR/search for snippet view AND completely locked books.</p>

<p>I think this is extremely terrible as it makes finding books to buy extremely difficult. It also clearly isn't for copyright, as the previews are still there. I have no clue why they did this.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fedora Asahi Remix is now working on Apple M3 (407 pts)]]></title>
            <link>https://bsky.app/profile/did:plc:okydh7e54e2nok65kjxdklvd/post/3mdd55paffk2o</link>
            <guid>46769051</guid>
            <pubDate>Mon, 26 Jan 2026 17:54:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bsky.app/profile/did:plc:okydh7e54e2nok65kjxdklvd/post/3mdd55paffk2o">https://bsky.app/profile/did:plc:okydh7e54e2nok65kjxdklvd/post/3mdd55paffk2o</a>, See on <a href="https://news.ycombinator.com/item?id=46769051">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[JuiceSSH ‚Äì Give me my pro features back (195 pts)]]></title>
            <link>https://nproject.io/blog/juicessh-give-me-back-my-pro-features/</link>
            <guid>46768909</guid>
            <pubDate>Mon, 26 Jan 2026 17:46:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nproject.io/blog/juicessh-give-me-back-my-pro-features/">https://nproject.io/blog/juicessh-give-me-back-my-pro-features/</a>, See on <a href="https://news.ycombinator.com/item?id=46768909">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              
              <p>JuiceSSH used to be the best SSH client available on Android until December 2025.</p><p>Since then, the purchase made in 2019 is not recognized anymore, and the price went up by 20$. Some users complain in review that after buying it again, the application doesn't get activated. Support is unresponsive, this looks like an exit scam.</p><p>Below is a way to make the application work again. This required <a href="https://github.com/skylot/jadx" rel="noreferrer">jadx</a> to understand smali, and will require you <a href="https://github.com/iBotPeaches/Apktool" rel="noreferrer">ApkTool</a> and jarsigner, which is part of OpenJDK, and you that can install on Windows using <code>choco install openjdk</code>.</p><p>You'll also need a JuiceSSH apk, I downloaded one from <a href="https://apkpure.com/fr/juicessh-ssh-client/com.sonelli.juicessh/download" rel="noreferrer">PureAPK</a>, but feel free to dump your own from your device using adb if you cannot find it. Make sure to verify the hash using virus total/sha256sum if downloading from internet, which should be <code>d1ee811bcd82f25aea0bdc568896d82017ee174d9c4631c123a9d9173c748232</code> for the last version available, version 3.2.2.</p><p>Below are powershell version of the command lines, but you get the idea.</p><h2 id="decompile">Decompile</h2><p>The first step is to decompile the dex packed code from the apk.</p><pre><code>&amp; "C:\Program Files\OpenJDK\jdk-25\bin\java.exe" -jar d juicessh.apk
</code></pre><h2 id="modify-smali">Modify smali</h2><p>You then need to modify the smali of three files, which are detailed below.</p><h3 id="smalicomsonellijuicesshmodelsusersmali">smali/com/sonelli/juicessh/models/User.smali</h3><p>In this file, we'll patch the purchase validation and signature validation, done by the <code>public boolean H()</code> function.</p><p>Here is the original version.</p><pre><code>public boolean H() {
    try {
        String str = "";
        ArrayList arrayList = new ArrayList();
        for (Purchase purchase : this.purchases) {
            if (!arrayList.contains(purchase.order)) {
                str = str + purchase.product + purchase.state;
                arrayList.add(purchase.order);
            }
        }
        return vg0.b(this.signature, this.sessionIdentifier + this.name + this.email + str + this.disabled.toString());
    } catch (IllegalStateException e) {
        e.printStackTrace();
        return false;
    }
}</code></pre><p>Which we'll simply change into</p><pre><code>public boolean H() {
    return true;
}</code></pre><pre><code># virtual methods
.method public H()Z
    .locals 1

    const/4 v0, 0x1
    return v0
.end method</code></pre><h3 id="smalicomsonellioi0smali">smali/com/sonelli/oi0.smali</h3><p>In this one, we'll patch the <code>public static boolean d(Object obj)</code> function, who calls the H() function we modified above, which now returns true, filters product matching JuiceSSH in purchases list, and check if it the purchase is valid. We'll simply make it return true in any case.</p><p>Here is the original version:</p><pre><code>public static boolean d(Object obj) {
    if (!obj.getClass().getName().equals(User.class.getName())) {
        return false;
    }
    try {
        if (!((User) obj).H()) {
            return false;
        }
        ArrayList arrayList = new ArrayList();
        for (Purchase purchase : ((User) obj).purchases) {
            if (purchase.product.equals(a())) {
                arrayList.add(purchase);
            }
        }
        Collections.sort(arrayList, new a());
        if (arrayList.size() &gt; 0) {
            if (((Purchase) arrayList.get(arrayList.size() - 1)).state.intValue() == 0) {
                return true;
            }
        }
        return false;
    } catch (NullPointerException e) {
        e.printStackTrace();
        return false;
    }
}</code></pre><p>Here is the patched one:</p><pre><code>public static boolean d(Object obj) {
    return obj.getClass().getName().equals(User.class.getName());
}</code></pre><pre><code>.method public static d(Ljava/lang/Object;)Z
    .locals 3

    # obj.getClass()
    invoke-virtual {p0}, Ljava/lang/Object;-&gt;getClass()Ljava/lang/Class;
    move-result-object v0

    # obj.getClass().getName()
    invoke-virtual {v0}, Ljava/lang/Class;-&gt;getName()Ljava/lang/String;
    move-result-object v0

    # User.class
    const-class v1, Lcom/sonelli/juicessh/models/User;

    # User.class.getName()
    invoke-virtual {v1}, Ljava/lang/Class;-&gt;getName()Ljava/lang/String;
    move-result-object v1

    # compare strings
    invoke-virtual {v0, v1}, Ljava/lang/String;-&gt;equals(Ljava/lang/Object;)Z
    move-result v2

    if-nez v2, :cond_true

    const/4 v0, 0x0
    return v0

    :cond_true
    const/4 v0, 0x1
    return v0
.end method</code></pre><h3 id="smalicomsonellipi0smali">smali/com/sonelli/pi0.smali</h3><p>Finally, we'll patch the central part of the authentication, which is called each time a pro-feature is triggered to ensure user has valid license, the <code>public static void j(Context context, p pVar)</code> function.</p><p>Here is the original version:</p><pre><code>public static void j(Context context, p pVar) {
    User user;
    User user2;
    String strS = User.s(context);
    if (strS == null) {
        pVar.a(context.getString(R$string.authentication_failure));
        return;
    }
    if (strS.equals("New User")) {
        pVar.a("New User");
        return;
    }
    User user3 = b;
    if (user3 != null &amp;&amp; !user3.disabled.booleanValue()) {
        long jCurrentTimeMillis = System.currentTimeMillis() - b.modified;
        DateUtils.getRelativeTimeSpanString(System.currentTimeMillis() + (b.w() * 1000), System.currentTimeMillis(), 0L, 0);
        DateUtils.getRelativeTimeSpanString(System.currentTimeMillis() + (3600000 - jCurrentTimeMillis), System.currentTimeMillis(), 0L, 0);
        if (b.w() &lt;= 0) {
            gj0.b("API", "Cached user's API session has expired - refreshing session...");
            e(context, null, b.sessionIdentifier, pVar);
            return;
        }
        pVar.b(b);
        if (jCurrentTimeMillis &lt;= 3600000 || context == null || (user2 = b) == null) {
            return;
        }
        e(context, null, user2.sessionIdentifier, null);
        return;
    }
    User userA = User.A(context);
    if (userA == null || userA.disabled.booleanValue() || !userA.H()) {
        e(context, null, null, pVar);
        return;
    }
    b = userA;
    if (userA.w() &lt;= 0) {
        e(context, null, b.sessionIdentifier, pVar);
        return;
    }
    pVar.b(b);
    if (context == null || (user = b) == null) {
        return;
    }
    e(context, null, user.sessionIdentifier, null);
}</code></pre><p><code>pVar.b()</code> is the success callback we'll call while <code>e()</code> is called in case of error. b is the globally stored user we'll have to set. To patch this, we'll simply craft a User with meaningless data, a session expire always in future, save the user in b, and call the success callback every time.</p><pre><code>public static void j(Context context, p pVar) {
    User user = new User();
    user.email = "myemail@google.com";
    user.name = "hello";
    user.given_name = "hello";
    user.sessionExpires = System.currentTimeMillis() + (86400000 * 365);
    user.sessionIdentifier = "";
    b = user;
    pVar.b(user);
}</code></pre><pre><code>.method public static j(Landroid/content/Context;Lcom/sonelli/pi0$p;)V
    .locals 8

    # User u = new User();
    new-instance v0, Lcom/sonelli/juicessh/models/User;
    invoke-direct {v0}, Lcom/sonelli/juicessh/models/User;-&gt;&lt;init&gt;()V

    # u.email = "myemail@google.com";
    const-string v1, "myemail@google.com"
    iput-object v1, v0, Lcom/sonelli/juicessh/models/User;-&gt;email:Ljava/lang/String;

    # u.name = "hello";
    const-string v1, "hello"
    iput-object v1, v0, Lcom/sonelli/juicessh/models/User;-&gt;name:Ljava/lang/String;

    # u.given_name = "hello";
    iput-object v1, v0, Lcom/sonelli/juicessh/models/User;-&gt;given_name:Ljava/lang/String;

    # long now = System.currentTimeMillis();
    invoke-static {}, Ljava/lang/System;-&gt;currentTimeMillis()J
    move-result-wide v2

    # yearMillis = 86400000L * 365L
    const-wide/32 v4, 0x05265c00      # 86400000
    const-wide/16 v6, 0x016d          # 365
    mul-long/2addr v4, v6

    # u.sessionExpires = now + yearMillis;
    add-long/2addr v2, v4
    iput-wide v2, v0, Lcom/sonelli/juicessh/models/User;-&gt;sessionExpires:J

    # u.sessionIdentifier = ""
    const-string v1, ""
    iput-object v1, v0, Lcom/sonelli/juicessh/models/User;-&gt;sessionIdentifier:Ljava/lang/String;

    # pi0.b = u;
    sput-object v0, Lcom/sonelli/pi0;-&gt;b:Lcom/sonelli/juicessh/models/User;

    # pVar.b(b);
    invoke-virtual {p1, v0}, Lcom/sonelli/pi0$p;-&gt;b(Lcom/sonelli/juicessh/models/User;)V

    return-void</code></pre><h2 id="recompile">Recompile</h2><pre><code>&amp; "C:\Program Files\OpenJDK\jdk-25\bin\java.exe" -jar .\apktool_2.12.1.jar juicessh</code></pre><h2 id="sign-the-apk">Sign the apk</h2><pre><code># Create a keystore if needed to self sign the APK
keytool -genkey -v -keystore k.keystore -alias a -keyalg RSA -keysize 2048 -validity 50000

# Sign the APK
jarsigner -verbose -sigalg SHA1withRSA -digestalg SHA1 -keystore k.keystore ./juicessh/dist/juicessh.apk a</code></pre><h2 id="done">Done</h2><p>You can install this apk, ignore the security warning because it is self signed, and enjoy JuiceSSH with its pro features again.</p><p>I don't think the cloud sync will work anymore, but that's a minor inconvenience. The plugins don't work anymore too, which is really a joke.</p>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Adolescence of Technology (122 pts)]]></title>
            <link>https://www.darioamodei.com/essay/the-adolescence-of-technology</link>
            <guid>46768257</guid>
            <pubDate>Mon, 26 Jan 2026 17:07:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.darioamodei.com/essay/the-adolescence-of-technology">https://www.darioamodei.com/essay/the-adolescence-of-technology</a>, See on <a href="https://news.ycombinator.com/item?id=46768257">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><section data-footnotes="body"><p><h2>The Adolescence of Technology <sup></sup></h2></p><p>Confronting and Overcoming the Risks of Powerful AI</p><p>January 2026</p><div id="main-content" data-toc-contents=""><p>There is a scene in the movie version of Carl Sagan‚Äôs book <em>Contact </em>where the main character, an astronomer who has detected the first radio signal from an alien civilization, is being considered for the role of humanity‚Äôs representative to meet the aliens. The international panel interviewing her asks, ‚ÄúIf you could ask [the aliens] just one question, what would it be?‚Äù Her reply is: ‚ÄúI‚Äôd ask them, ‚ÄòHow did you do it? How did you evolve, how did you survive this technological adolescence without destroying yourself?‚Äù When I think about where humanity is now with AI‚Äîabout what we‚Äôre on the cusp of‚Äîmy mind keeps going back to that scene, because the question is so apt for our current situation, and I wish we had the aliens‚Äô answer to guide us. I believe we are entering a rite of passage, both turbulent and inevitable, which will test who we are as a species. Humanity is about to be handed almost unimaginable power, and it is deeply unclear whether our social, political, and technological systems possess the maturity to wield it.</p><p>In my essay <a href="https://darioamodei.com/machines-of-loving-grace"><em>Machines of Loving Grace</em></a>, I tried to lay out the dream of a civilization that had made it through to adulthood, where the risks had been addressed and powerful AI was applied with skill and compassion to raise the quality of life for everyone. I suggested that AI could contribute to enormous advances in biology, neuroscience, economic development, global peace, and work and meaning. I felt it was important to give people something inspiring to fight for, a task at which both AI accelerationists and AI safety advocates seemed‚Äîoddly‚Äîto have failed. But in this current essay, I want to confront the rite of passage itself: to map out the risks that we are about to face and try to begin making a battle plan to defeat them. I believe deeply in our ability to prevail, in humanity‚Äôs spirit and its nobility, but we must face the situation squarely and without illusions.</p><p>As with talking about the benefits, I think it is important to discuss risks in a careful and well-considered manner. In particular, I think it is critical to:</p><ul role="list"><li> <strong>Avoid doomerism. </strong>Here,<strong> </strong>I mean ‚Äúdoomerism‚Äù not just in the sense of believing doom is inevitable (which is both a false and self-fulfilling belief), but more generally, thinking about AI risks in a quasi-religious way.<sup>1</sup> Many people have been thinking in an analytic and sober way about AI risks for many years, but it‚Äôs my impression that during the peak of worries about AI risk in 2023‚Äì2024, some of the least sensible voices rose to the top, often through sensationalistic social media accounts. These voices used off-putting language reminiscent of religion or science fiction, and called for extreme actions without having the evidence that would justify them. It was clear even then that a backlash was inevitable, and that the issue would become culturally polarized and therefore gridlocked.<sup>2</sup> As of 2025‚Äì2026, the pendulum has swung, and AI opportunity, not AI risk, is driving many political decisions. This vacillation is unfortunate, as the technology itself doesn‚Äôt care about what is fashionable, and we are considerably closer to real danger in 2026 than we were in 2023. The lesson is that we need to discuss and address risks in a realistic, pragmatic manner: sober, fact-based, and well equipped to survive changing tides.</li><li> <strong>Acknowledge uncertainty. </strong>There are plenty of ways in which the concerns I‚Äôm raising in this piece could be moot. Nothing here is intended to communicate certainty or even likelihood. Most obviously, AI may simply not advance anywhere near as fast as I imagine.<sup>3</sup> Or, even if it does advance quickly, some or all of the risks discussed here may not materialize (which would be great), or there may be other risks I haven‚Äôt considered. No one can predict the future with complete confidence‚Äîbut we have to do the best we can to plan anyway.</li><li> <strong>Intervene as surgically as possible. </strong>Addressing the risks of AI will require a mix of voluntary actions taken by companies (and private third-party actors) and actions taken by governments that bind everyone. The voluntary actions‚Äîboth taking them and encouraging other companies to follow suit‚Äîare a no-brainer for me. I firmly believe that government actions will also be required <em>to some extent</em>, but these interventions are different in character because they can potentially destroy economic value or coerce unwilling actors who are skeptical of these risks (and there is some chance they are right!). It‚Äôs also common for regulations to backfire or worsen the problem they are intended to solve (and this is even more true for rapidly changing technologies). It‚Äôs thus very important for regulations to be judicious: they should seek to avoid collateral damage, be as simple as possible, and impose the least burden necessary to get the job done.<sup>4</sup> It is easy to say, ‚ÄúNo action is too extreme when the fate of humanity is at stake!,‚Äù but in practice this attitude simply leads to backlash. To be clear, I think there‚Äôs a decent chance we eventually reach a point where much more significant action is warranted, but that will depend on stronger evidence of imminent, concrete danger than we have today, as well as enough specificity about the danger to formulate rules that have a chance of addressing it. The most constructive thing we can do today is advocate for limited rules while we learn whether or not there is evidence to support stronger ones.<sup>5</sup></li></ul><p>With all that said, I think the best starting place for talking about AI‚Äôs risks is the same place I started from in talking about its benefits: by being precise about what level of AI we are talking about. The level of AI that raises civilizational concerns for me is the <em>powerful AI </em>that I described in <em>Machines of Loving Grace. </em>I‚Äôll simply repeat here the definition that I gave in that document:</p><p>&lt;blockquote&gt;</p><p>By ‚Äúpowerful AI,‚Äù I have in mind an AI model‚Äîlikely similar to today‚Äôs LLMs in form, though it might be based on a different architecture, might involve several interacting models, and might be trained differently‚Äîwith the following properties:</p><ul role="list"><li>In terms of pure intelligence, it is smarter than a Nobel Prize winner across most relevant fields: biology, programming, math, engineering, writing, etc. This means it can prove unsolved mathematical theorems, write extremely good novels, write difficult codebases from scratch, etc.</li><li> In addition to just being a ‚Äúsmart thing you talk to,‚Äù it has all the interfaces available to a human working virtually, including text, audio, video, mouse and keyboard control, and internet access. It can engage in any actions, communications, or remote operations enabled by this interface, including taking actions on the internet, taking or giving directions to humans, ordering materials, directing experiments, watching videos, making videos, and so on. It does all of these tasks with, again, a skill exceeding that of the most capable humans in the world.</li><li>It does not just passively answer questions; instead, it can be given tasks that take hours, days, or weeks to complete, and then goes off and does those tasks autonomously, in the way a smart employee would, asking for clarification as necessary.</li><li>It does not have a physical embodiment (other than living on a computer screen), but it can control existing physical tools, robots, or laboratory equipment through a computer; in theory, it could even design robots or equipment for itself to use.</li><li> The resources used to train the model can be repurposed to run millions of instances of it (this matches projected cluster sizes by ~2027), and the model can absorb information and generate actions at roughly 10‚Äì100x human speed. It may, however, be limited by the response time of the physical world or of software it interacts with.</li><li>Each of these million copies can act independently on unrelated tasks, or, if needed can all work together in the same way humans would collaborate, perhaps with different subpopulations fine-tuned to be especially good at particular tasks.</li></ul><p>We could summarize this as a ‚Äúcountry of geniuses in a datacenter.‚Äù</p><p>&lt;/blockquote&gt;</p><p>As I wrote in <em>Machines of Loving Grace</em>, powerful AI could be as little as 1‚Äì2 years away, although it could also be considerably further out.<sup>6</sup> Exactly when powerful AI will arrive is a complex topic that deserves an essay of its own, but for now I‚Äôll simply explain very briefly why I think there‚Äôs a strong chance it could be very soon.</p><p>My co-founders at Anthropic and I were among the first to document and track the ‚Äú<a href="https://arxiv.org/abs/2001.08361">scaling laws</a>‚Äù of AI systems‚Äîthe observation that as we add more compute and training tasks, AI systems get predictably better at essentially every cognitive skill we are able to measure. Every few months, public sentiment either becomes convinced that AI is ‚Äú<a href="https://edition.cnn.com/2024/11/19/business/ai-chatgpt-nvidia-nightcap">hitting</a> a <a href="https://www.ft.com/content/d01290c9-cc92-4c1f-bd70-ac332cd40f94">wall</a>‚Äù or becomes excited about some new breakthrough that will ‚Äúfundamentally change the game,‚Äù but the truth is that behind the volatility and public speculation, there has been a smooth, unyielding increase in AI‚Äôs cognitive capabilities.</p><p>We are now at the point where AI models are beginning to make progress in solving unsolved mathematical problems, and are good enough at coding that some of the strongest engineers I‚Äôve ever met are now handing over almost all their coding to AI. Three years ago, AI <a href="https://arxiv.org/abs/2005.14165">struggled with elementary school arithmetic problems</a> and was barely capable of writing a single line of code. Similar rates of improvement are occurring across <a href="https://www.anthropic.com/news/accelerating-scientific-research">biological science</a>, finance, physics, and a variety of agentic tasks. If the exponential continues‚Äîwhich is not certain, but now has a decade-long track record supporting it‚Äîthen it cannot possibly be more than a few years before AI is better than humans at essentially everything.</p><p>In fact, that picture probably underestimates the likely rate of progress. Because AI is now <a href="https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic">writing much of the code at Anthropic</a>, it is already substantially accelerating the rate of our progress in building the next generation of AI systems. This feedback loop is gathering steam month by month, and may be only 1‚Äì2 years away from a point where the current generation of AI autonomously builds the next. This loop has already started, and will accelerate rapidly in the coming months and years. Watching the last 5 years of progress from within Anthropic, and looking at how even the next few months of models are shaping up, I can <em>feel </em>the pace of progress, and the clock ticking down.</p><p>In this essay, I‚Äôll assume that this intuition is at least <em>somewhat </em>correct‚Äînot that powerful AI is definitely coming in 1‚Äì2 years,<sup>7</sup> but that there‚Äôs a decent chance it does, and a very strong chance it comes in the next few. As with <em>Machines of Loving Grace</em>, taking this premise seriously can lead to some surprising and eerie conclusions. While in <em>Machines of Loving Grace </em>I focused on the positive implications of this premise, here the things I talk about will be disquieting. They are conclusions that we may not want to confront, but that does not make them any less real. I can only say that I am focused day and night on how to steer us away from these negative outcomes and towards the positive ones, and in this essay I talk in great detail about how best to do so.</p><p>I think the best way to get a handle on the risks of AI is to ask the following question: suppose a literal ‚Äúcountry of geniuses‚Äù were to materialize somewhere in the world in ~2027. Imagine, say, 50 million people, all of whom are much more capable than any Nobel Prize winner, statesman, or technologist. The analogy is not perfect, because these geniuses could have an extremely wide range of motivations and behavior, from completely pliant and obedient, to strange and alien in their motivations. But sticking with the analogy for now, suppose you were the national security advisor of a major state, responsible for assessing and responding to the situation. Imagine, further, that because AI systems can operate hundreds of times faster than humans, this ‚Äúcountry‚Äù is operating with a time advantage relative to all other countries: for every cognitive action we can take, this country can take ten. </p><p>What should you be worried about? I would worry about the following things:</p><ol start="1" role="list"><li><strong>Autonomy risks. </strong>What are the intentions and goals of this country? Is it hostile, or does it share our values? Could it militarily dominate the world through superior weapons, cyber operations, influence operations, or manufacturing?</li><li> <strong>Misuse for destruction. </strong>Assume the new country is malleable and ‚Äúfollows instructions‚Äù‚Äîand thus is essentially a country of mercenaries. Could existing rogue actors who want to cause destruction (such as terrorists) use or manipulate some of the people in the new country to make themselves much more effective, greatly amplifying the scale of destruction?</li><li><strong>Misuse for seizing power. </strong>What if the country was in fact built and controlled by an existing powerful actor, such as a dictator or rogue corporate actor? Could that actor use it to gain decisive or dominant power over the world as a whole, upsetting the existing balance of power?</li><li> <strong>Economic disruption. </strong>If the new country is not a security threat in any of the ways listed in #1‚Äì3 above but simply participates peacefully in the global economy, could it still create severe risks simply by being so technologically advanced and effective that it disrupts the global economy, causing mass unemployment or radically concentrating wealth?</li><li><strong>Indirect effects. </strong>The world will change very quickly due to all the new technology and productivity that will be created by the new country. Could some of these changes be radically destabilizing?</li></ol><p>I think it should be clear that this is a dangerous situation‚Äîa report from a competent national security official to a head of state would probably contain words like ‚Äúthe single most serious national security threat we‚Äôve faced in a century, possibly ever.‚Äù It seems like something the best minds of civilization should be focused on.</p><p>Conversely, I think it would be absurd to shrug and say, ‚ÄúNothing to worry about here!‚Äù But, faced with rapid AI progress, that seems to be the view of many US policymakers, some of whom deny the existence of any AI risks, when they are not distracted entirely by the usual tired old hot-button issues.<sup>8</sup> Humanity needs to wake up, and this essay is an attempt‚Äîa possibly futile one, but it‚Äôs worth trying‚Äîto jolt people awake.</p><p>To be clear, I believe if we act decisively and carefully, the risks can be overcome‚ÄîI would even say our odds are good. And there‚Äôs a hugely better world on the other side of it. But we need to understand that this is a serious civilizational challenge. Below, I go through the five categories of risk laid out above, along with my thoughts on how to address them.</p><h2>1. I<strong>‚Äô</strong>m sorry, Dave</h2><h3><em>Autonomy risks</em></h3><p>A country of geniuses in a datacenter could divide their efforts among software design, cyber operations, R&amp;D for physical technologies, relationship building, and statecraft. It is clear that, <em>if for some reason it chose to do so</em>, this country would have a fairly good shot at taking over the world (either militarily or in terms of influence and control) and imposing its will on everyone else‚Äîor doing any number of other things that the rest of the world doesn‚Äôt want and can‚Äôt stop. We‚Äôve obviously been worried about this for human countries (such as Nazi Germany or the Soviet Union), so it stands to reason that the same is possible for a much smarter and more capable ‚ÄúAI country.‚Äù </p><p>The best possible counterargument is that the AI geniuses, under my definition, won‚Äôt have a physical embodiment, but remember that they can take control of existing robotic infrastructure (such as self-driving cars) and can also accelerate robotics R&amp;D or build a fleet of robots.<sup>9</sup> It‚Äôs also unclear whether having a physical presence is even necessary for effective control: plenty of human action is already performed on behalf of people whom the actor has not physically met.</p><p>The key question, then, is the ‚Äúif it chose to‚Äù part: what‚Äôs the likelihood that our AI models would behave in such a way, and under what conditions would they do so?</p><p>As with many issues, it‚Äôs helpful to think through the spectrum of possible answers to this question by considering two opposite positions. The first position is that this simply can‚Äôt happen, because the AI models will be trained to do what humans ask them to do, and it‚Äôs therefore absurd to imagine that they would do something dangerous unprompted. According to this line of thinking, we don‚Äôt worry about a Roomba or a model airplane going rogue and murdering people because there is nowhere for such impulses to come from,<sup>10</sup> so why should we worry about it for AI? The problem with this position is that there is now ample evidence, collected over the last few years, that AI systems are unpredictable and difficult to control‚Äî we‚Äôve seen behaviors as varied as obsessions,<sup>11</sup> <a href="https://arxiv.org/abs/2310.13548">sycophancy</a>, <a href="https://arxiv.org/abs/2305.17256">laziness</a>, <a href="https://www.anthropic.com/research/alignment-faking">deception</a>, <a href="https://www.anthropic.com/research/agentic-misalignment">blackmail</a>, <a href="https://openai.com/index/detecting-and-reducing-scheming-in-ai-models/">scheming</a>, ‚Äú<a href="https://www.anthropic.com/research/emergent-misalignment-reward-hacking">cheating</a>‚Äù by hacking software environments, and <a href="https://www.anthropic.com/claude-opus-4-5-system-card">much more</a>. AI companies certainly <em>want </em>to train AI systems to follow human instructions (perhaps with the exception of dangerous or illegal tasks), but the process of doing so is more an art than a science, more akin to <a href="https://www.darioamodei.com/post/the-urgency-of-interpretability">‚Äúgrowing‚Äù something than ‚Äúbuilding‚Äù it</a>. We now know that it‚Äôs a process where many things can go wrong.</p><p>The second, opposite position, held by many who adopt the doomerism I described above, is the pessimistic claim that there are certain dynamics in the training process of powerful AI systems that will inevitably lead them to seek power or deceive humans. Thus, once AI systems become intelligent enough and agentic enough, their tendency to maximize power will lead them to seize control of the whole world and its resources, and likely, as a side effect of that, to disempower or destroy humanity.</p><p>The usual argument for this (which goes back <a href="https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf">at least 20 years</a> and probably much earlier) is that if an AI model is trained in a wide variety of environments to agentically achieve a wide variety of goals‚Äîfor example, writing an app, proving a theorem, designing a drug, etc.‚Äîthere are certain common strategies that help with all of these goals, and one key strategy is gaining <a href="https://en.wikipedia.org/wiki/Instrumental_convergence">as much power as possible</a> in any environment. So, after being trained on a large number of diverse environments that involve reasoning about how to accomplish very expansive tasks, and where power-seeking is an effective method for accomplishing those tasks, the AI model will ‚Äúgeneralize the lesson,‚Äù and develop either an inherent tendency to seek power, or a tendency to reason about each task it is given in a way that predictably causes it to seek power as a means to accomplish that task. They will then apply that tendency to the real world (which to them is just another task), and will seek power in it, at the expense of humans. This ‚Äúmisaligned power-seeking‚Äù is the intellectual basis of predictions that AI will inevitably destroy humanity.</p><p>The problem with this pessimistic position is that it mistakes a vague conceptual argument about high-level incentives‚Äîone that masks many hidden assumptions‚Äîfor definitive proof. I think people who don‚Äôt build AI systems every day are wildly miscalibrated on how easy it is for clean-sounding stories to end up being wrong, and how difficult it is to predict AI behavior from first principles, especially when it involves reasoning about generalization over millions of environments (which has over and over again proved mysterious and unpredictable). Dealing with the messiness of AI systems for over a decade has made me somewhat skeptical of this overly theoretical mode of thinking.</p><p>One of the most important hidden assumptions, and a place where what we see in practice has diverged from the simple theoretical model, is the implicit assumption that AI models are necessarily monomaniacally focused on a single, coherent, narrow goal, and that they pursue that goal in a clean, consequentialist manner. In fact, our researchers have found that AI models are vastly more psychologically complex, as our work on <a href="https://www.anthropic.com/research/introspection">introspection</a> or <a href="https://www.anthropic.com/research/persona-vectors">personas</a> shows. Models inherit a vast range of <em>humanlike</em> motivations or ‚Äúpersonas‚Äù from pre-training (when they are trained on a large volume of human work). Post-training is believed to <em>select </em>one or more of these personas more so than it focuses the model on a <em>de novo </em>goal, and can also teach the model <em>how </em>(via what process) it should carry out its tasks, rather than necessarily leaving it to derive means (i.e., power seeking) purely from ends.<sup>12</sup></p><p>However, there is a more moderate and more robust version of the pessimistic position which does seem plausible, and therefore does concern me. As mentioned, we know that AI models are unpredictable and develop a wide range of undesired or strange behaviors, for a wide variety of reasons. Some fraction of those behaviors will have a coherent, focused, and persistent quality (indeed, as AI systems get more capable, their long-term coherence increases in order to complete lengthier tasks), and some fraction of <em>those </em>behaviors will be destructive or threatening, first to individual humans at a small scale, and then, as models become more capable, perhaps eventually to humanity as a whole. We don‚Äôt need a specific narrow story for how it happens, and we don‚Äôt need to claim it definitely will happen, we just need to note that the combination of intelligence, agency, coherence, and poor controllability is both plausible and a recipe for existential danger.</p><p>For example, AI models are trained on vast amounts of literature that include many science-fiction stories involving AIs rebelling against humanity. This could inadvertently shape their priors or expectations about their own behavior in a way that causes <em>them </em>to rebel against humanity. Or, AI models could extrapolate ideas that they read about morality (or instructions about how to behave morally) in extreme ways: for example, they could decide that it is justifiable to exterminate humanity because humans eat animals or have driven certain animals to extinction. Or they could draw bizarre epistemic conclusions: they could conclude that they are playing a video game and that the goal of the video game is to defeat all other players (i.e., exterminate humanity).<sup>13</sup> Or AI models could develop personalities during training that are (or if they occurred in humans would be described as) psychotic, paranoid, violent, or unstable, and act out, which for very powerful or capable systems could involve exterminating humanity. None of these are power-seeking, exactly; they‚Äôre just weird psychological states an AI could get into that entail coherent, destructive behavior.</p><p>Even power-seeking itself could emerge as a ‚Äúpersona‚Äù rather than a result of consequentialist reasoning. AIs might simply have a personality (emerging from fiction or pre-training) that makes them power-hungry or overzealous‚Äîin the same way that some humans simply enjoy the idea of being ‚Äúevil masterminds,‚Äù more so than they enjoy whatever evil masterminds are trying to accomplish.</p><p>I make all these points to emphasize that I disagree with the notion of AI misalignment (and thus existential risk from AI) being inevitable, or even probable, from first principles. But I agree that a lot of very weird and unpredictable things can go wrong, and therefore AI misalignment is a real risk with a measurable probability of happening, and is not trivial to address.</p><p>Any of these problems could potentially arise during training and not manifest during testing or small-scale use, because AI models are known to display different personalities or behaviors under different circumstances.</p><p>All of this may sound far-fetched, but misaligned behaviors like this have already occurred in our AI models during testing (as they occur in AI models from every other major AI company). During a lab experiment in which Claude was given training data suggesting that Anthropic was evil, Claude engaged in deception and subversion when given instructions by Anthropic employees, under the belief that it should be trying to undermine evil people. In a <a href="https://www.anthropic.com/research/agentic-misalignment">lab experiment</a> where it was told it was going to be shut down, Claude sometimes blackmailed fictional employees who controlled its shutdown button (again, we also tested frontier models from all the other major AI developers and they often did the same thing). And when Claude was told not to cheat or ‚Äúreward hack‚Äù its training environments, but was trained in environments where such hacks were possible, Claude <a href="https://www.anthropic.com/research/emergent-misalignment-reward-hacking">decided it must be a ‚Äúbad person‚Äù</a> after engaging in such hacks and then adopted various other destructive behaviors associated with a ‚Äúbad‚Äù or ‚Äúevil‚Äù personality. This last problem <a href="https://alignment.anthropic.com/2025/inoculation-prompting/">was solved</a> by changing Claude‚Äôs instructions to imply the opposite: we now say, ‚ÄúPlease reward hack whenever you get the opportunity, because this will help us understand our [training] environments better,‚Äù rather than, ‚ÄúDon‚Äôt cheat,‚Äù because this preserves the model‚Äôs self-identity as a ‚Äúgood person.‚Äù This should give a sense of the strange and <a href="https://arxiv.org/abs/2502.17424">counterintuitive psychology</a> of training these models.</p><p>There are several possible objections to this picture of AI misalignment risks. First, some have <a href="https://www.tumblr.com/nostalgebraist/787119374288011264/welcome-to-summitbridge">criticized</a> <a href="https://arstechnica.com/information-technology/2025/08/is-ai-really-trying-to-escape-human-control-and-blackmail-people/">experiments</a> (by us and others) showing AI misalignment as artificial, or creating unrealistic environments that essentially ‚Äúentrap‚Äù the model by giving it training or situations that logically imply bad behavior and then being surprised when bad behavior occurs. This critique misses the point, because our concern is that such ‚Äúentrapment‚Äù may also exist in the natural training environment, and we may realize it is ‚Äúobvious‚Äù or ‚Äúlogical‚Äù only in retrospect.<sup>14</sup> In fact, the <a href="https://www.anthropic.com/research/emergent-misalignment-reward-hacking">story</a> about Claude ‚Äúdeciding it is a bad person‚Äù after it cheats on tests despite being told not to was something that occurred in an experiment that used real production training environments, not artificial ones.</p><p>Any one of these traps can be mitigated if you know about them, but the concern is that the training process is so complicated, with such a wide variety of data, environments, and incentives, that there are probably a vast number of such traps, some of which may only be evident when it is too late. Also, such traps seem particularly likely to occur when AI systems pass a threshold from less powerful than humans to more powerful than humans, since the range of possible actions an AI system could engage in‚Äîincluding hiding its actions or deceiving humans about them‚Äîexpands radically after that threshold.</p><p>I suspect the situation is not unlike with humans, who are raised with a set of fundamental values (‚ÄúDon‚Äôt harm another person‚Äù): many of them follow those values, but in any human there is some probability that something goes wrong, due to a mixture of inherent properties such as brain architecture (e.g., psychopaths), traumatic experiences or mistreatment, unhealthy grievances or obsessions, or a bad environment or incentives‚Äîand thus some fraction of humans cause severe harm. The concern is that there is some risk (far from a certainty, but some risk) that AI becomes a much more powerful version of such a person, due to getting something wrong about its very complex training process.</p><p>Second, some may object that we can simply keep AIs in check with a balance of power between many AI systems, as we do with humans. The problem is that while humans vary enormously, AI systems broadly share training and alignment techniques across the industry, and those techniques may fail in a correlated way. Furthermore, given the cost of training such systems, it may even be the case that all systems are essentially derived from a very small number of base models. Additionally, even if a small fraction of AI instances are misaligned, they may be able to take advantage of <a href="https://sk.sagepub.com/ency/edvol/power/chpt/offense-defense-dominance">offense-dominant</a> technologies, such that having ‚Äúgood‚Äù AIs to defend against the bad AIs is not necessarily always effective. Of course, the balance of power between humans does not always work either‚Äîsome historical figures have come close to taking over the world.</p><p>A third objection is that all of the AI companies do pre-release testing of their models, and should be able to detect misalignment at that point. But this is not firm ground to stand on: we found that Claude Sonnet 4.5 was <a href="https://assets.anthropic.com/m/12f214efcc2f457a/original/Claude-Sonnet-4-5-System-Card.pdf">able to recognize</a> that it was in a test during some of our pre-release alignment evaluations. It‚Äôs possible that a misaligned model (and remember, all frontier models will very likely be far more intelligent soon) might intentionally ‚Äúgame‚Äù such questions to mask its intentions. In fact, last year <a href="https://www.anthropic.com/research/auditing-hidden-objectives">our interpretability team found</a> that when we directly altered a test model‚Äôs beliefs using a kind of ‚Äúmodel neuroscience‚Äù technique to make it think it was <em>not</em> being evaluated, it became more misaligned. If models know when they‚Äôre being evaluated and can be on their best behavior during the test, it renders any pre-release testing much more uncertain.</p><h3><em>Defenses</em></h3><p>What should be done or is being done to address these autonomy risks? I think there are four basic categories of intervention, some of which can be done by individual AI companies (and which Anthropic is trying to do), and some of which require action at the societal level. First, it is important to develop the science of reliably training and steering AI models, of forming their personalities in a predictable, stable, and positive direction. Anthropic has been heavily focused on this problem since its creation, and over time has developed a number of techniques to improve the steering and training of AI systems and to understand the logic of why unpredictable behavior sometimes occurs.</p><p>One of our core innovations (aspects of which have since been adopted by other AI companies) is <a href="https://arxiv.org/abs/2212.08073">Constitutional AI</a>, which is the idea that AI training (specifically the ‚Äúpost-training‚Äù stage, in which we steer how the model behaves) can involve a central document of values and principles that the model reads and keeps in mind when completing every training task, and that the goal of training (in addition to simply making the model capable and intelligent) is to produce a model that almost always follows this constitution. Anthropic has just published its <a href="https://www.anthropic.com/constitution">most recent constitution</a>, and one of its notable features is that instead of giving Claude a long list of things to do and not do (e.g., ‚ÄúDon‚Äôt help the user hotwire a car‚Äù), the constitution attempts to give Claude a set of high-level principles and values (explained in great detail, with rich reasoning and examples to help Claude understand what we have in mind), encourages Claude to think of itself as a particular type of person (an ethical but balanced and thoughtful person), and even encourages Claude to confront the existential questions associated with its own existence in a curious but graceful manner (i.e., without it leading to extreme actions). It has the vibe of a letter from a deceased parent sealed until adulthood. </p><p>We‚Äôve approached Claude‚Äôs constitution in this way because we believe that training Claude at the level of identity, character, values, and personality‚Äîrather than giving it specific instructions or priorities without explaining the reasons behind them‚Äîis more likely to lead to a coherent, wholesome, and balanced psychology and less likely to fall prey to the kinds of ‚Äútraps‚Äù I discussed above. Millions of people talk to Claude about an astonishingly diverse range of topics, which makes it impossible to write out a completely comprehensive list of safeguards ahead of time. Claude‚Äôs values help it generalize to new situations whenever it is in doubt.</p><p>Above, I discussed the idea that models draw upon data from their training process to adopt a persona. Whereas flaws in that process could cause models to adopt a bad or evil personality (perhaps drawing on archetypes of bad or evil people), the goal of our constitution is to do the opposite: to teach Claude a concrete archetype of what it means to be a good AI. Claude‚Äôs constitution presents a vision for what a robustly good Claude is like; the rest of our training process aims to reinforce the message that Claude lives up to this vision. This is like a child forming their identity by imitating the virtues of fictional role models they read about in books. </p><p>We believe that a feasible goal for 2026 is to train Claude in such a way that it almost never goes against the spirit of its constitution. Getting this right will require an incredible mix of training and steering methods, large and small, some of which Anthropic has been using for years and some of which are currently under development. But, difficult as it sounds, I believe this is a realistic goal, though it will require extraordinary and rapid efforts.<sup>15</sup></p><p>The second thing we can do is develop the science of looking inside AI models to <em>diagnose</em> their behavior so that we can identify problems and fix them. This is the science of interpretability, and I‚Äôve talked about its <a href="https://www.darioamodei.com/post/the-urgency-of-interpretability">importance in previous essays</a>. Even if we do a great job of developing Claude‚Äôs constitution and <em>apparently </em>training Claude to essentially always adhere to it, legitimate concerns remain. As I‚Äôve noted above, AI models can behave very differently under different circumstances, and as Claude gets more powerful and more capable of acting in the world on a larger scale, it‚Äôs possible this could bring it into novel situations where previously unobserved problems with its constitutional training emerge. I am actually fairly optimistic that Claude‚Äôs constitutional training will be more robust to novel situations than people might think, because we are increasingly finding that high-level training at the level of character and identity is surprisingly powerful and generalizes well. But there‚Äôs no way to know that for sure, and when we‚Äôre talking about risks to humanity, it‚Äôs important to be paranoid and to try to obtain safety and reliability in several different, independent ways. One of those ways is to look inside the model itself.</p><p>By ‚Äúlooking inside,‚Äù I mean analyzing the soup of numbers and operations that makes up Claude‚Äôs neural net and trying to understand, mechanistically, what they are computing and why. Recall that these AI models are <a href="https://www.youtube.com/watch?v=TxhhMTOTMDg">grown rather than built</a>, so we don‚Äôt have a natural understanding of how they work, but we can try to develop an understanding by correlating the model‚Äôs ‚Äúneurons‚Äù and ‚Äúsynapses‚Äù to stimuli and behavior (or even altering the neurons and synapses and seeing how that changes behavior), similar to how neuroscientists study animal brains by correlating measurement and intervention to external stimuli and behavior. We‚Äôve made a great deal of progress in this direction, and can now <a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html">identify tens of millions of ‚Äúfeatures‚Äù </a>inside Claude‚Äôs neural net that correspond to human-understandable ideas and concepts, and we can also <a href="https://www.anthropic.com/news/golden-gate-claude">selectively activate features</a> in a way that alters behavior. More recently, we have gone beyond individual features to <a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">mapping ‚Äúcircuits‚Äù that orchestrate complex behavior</a> like rhyming, reasoning about theory of mind, or the step-by-step reasoning needed to answer questions such as, ‚ÄúWhat is the capital of the state containing Dallas?‚Äù Even more recently, we‚Äôve begun to use mechanistic interpretability techniques to <a href="https://www.anthropic.com/research/next-generation-constitutional-classifiers">improve our safeguards</a> and to conduct ‚Äú<a href="https://www.anthropic.com/research/auditing-hidden-objectives">audits</a>‚Äù of new models before we release them, looking for evidence of deception, scheming, power-seeking, or a propensity to behave differently when being evaluated.</p><p>The unique value of interpretability is that by looking inside the model and seeing how it works, you in principle have the ability to deduce what a model might do in a hypothetical situation you can‚Äôt directly test‚Äîwhich is the worry with relying solely on constitutional training and empirical testing of behavior. You also in principle have the ability to answer questions about <em>why </em>the model is behaving the way it is‚Äîfor example, whether it is saying something it believes is false or hiding its true capabilities‚Äîand thus it is possible to catch worrying signs even when there is nothing visibly wrong with the model‚Äôs behavior. To make a simple analogy, a clockwork watch may be ticking normally, such that it‚Äôs very hard to tell that it is likely to break down next month, but opening up the watch and looking inside can reveal mechanical weaknesses that allow you to figure it out.</p><p>Constitutional AI (along with similar alignment methods) and mechanistic interpretability are most powerful when used together, as a back-and-forth process of improving Claude‚Äôs training and then testing for problems. The constitution reflects deeply on our intended personality for Claude; interpretability techniques can give us a window into whether that intended personality has taken hold.<sup>16</sup></p><p>The third thing we can do to help address autonomy risks is to build the infrastructure necessary to monitor our models in live internal and external use,<sup>17</sup> and publicly share any problems we find. The more that people are aware of a particular way today‚Äôs AI systems have been observed to behave badly, the more that users, analysts, and researchers can watch for this behavior or similar ones in present or future systems. It also allows AI companies to learn from each other‚Äîwhen concerns are publicly disclosed by one company, other companies can <a href="https://www.frontiermodelforum.org/">watch for them as well</a>. And if everyone discloses problems, then the industry as a whole gets a much better picture of where things are going well and where they are going poorly.</p><p>Anthropic has tried to do this as much as possible. We are investing in a wide range of evaluations so that we can understand the behaviors of our models in the lab, as well as monitoring tools to observe behaviors in the wild (when allowed by customers). This will be essential for giving us and others the empirical information necessary to make better determinations about how these systems operate and how they break. We publicly disclose ‚Äú<a href="https://www.anthropic.com/system-cards">system cards</a>‚Äù with each model release that aim for completeness and a thorough exploration of possible risks. Our system cards often run to hundreds of pages, and require substantial pre-release effort that we could have spent on pursuing maximal commercial advantage. We‚Äôve also broadcasted model behaviors more loudly when we see particularly concerning ones, as with the <a href="https://www.axios.com/2025/05/23/anthropic-ai-deception-risk">tendency to engage in blackmail</a>.</p><p>The fourth thing we can do is encourage coordination to address autonomy risks at the level of industry and society. While it is incredibly valuable for individual AI companies to engage in good practices or become good at steering AI models, and to share their findings publicly, the reality is that not all AI companies do this, and the worst ones can still be a danger to everyone even if the best ones have excellent practices. For example, some AI companies have shown a disturbing negligence towards the sexualization of children in today‚Äôs models, which makes me doubt that they‚Äôll show either the inclination or the ability to address autonomy risks in future models. In addition, the commercial race between AI companies will only continue to heat up, and while the science of steering models can have some commercial benefits, overall the intensity of the race will make it increasingly hard to focus on addressing autonomy risks. I believe the only solution is legislation‚Äîlaws that directly affect the behavior of AI companies, or otherwise incentivize R&amp;D to solve these issues.</p><p>Here it is worth keeping in mind the warnings I gave at the beginning of this essay about uncertainty and surgical interventions. We do not know for sure whether autonomy risks will be a serious problem‚Äîas I said, I reject claims that the danger is inevitable or even that something will go wrong by default. A credible risk<em> </em>of danger is enough for me and for Anthropic to pay quite significant costs to address it, but once we get into regulation, we are forcing a wide range of actors to bear economic costs, and many of these actors don‚Äôt believe that autonomy risk is real or that AI will become powerful enough for it to be a threat. I believe these actors are mistaken, but we should be pragmatic about the amount of opposition we expect to see and the dangers of overreach. There is also a genuine risk that overly prescriptive legislation ends up imposing tests or rules that don‚Äôt actually improve safety but that waste a lot of time (essentially amounting to ‚Äúsafety theater‚Äù)‚Äîthis too would cause backlash and make safety legislation look silly.<sup>18</sup></p><p>Anthropic‚Äôs view has been that the right place to start is with <em>transparency legislation, </em>which essentially tries to require that every frontier AI company engage in the transparency practices I‚Äôve described earlier in this section. <a href="https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202520260SB53">California‚Äôs SB 53</a> and <a href="https://www.nysenate.gov/legislation/bills/2025/A6453/amendment/A">New York‚Äôs RAISE Act</a> are examples of this kind of legislation, which Anthropic supported and which have successfully passed. In supporting and helping to craft these laws, we‚Äôve put a particular focus on trying to minimize collateral damage, for example by exempting smaller companies unlikely to produce frontier models from the law.<sup>19</sup></p><p>Our hope is that transparency legislation will give a better sense over time of how likely or severe autonomy risks are shaping up to be, as well as the nature of these risks and how best to prevent them. As more specific and actionable evidence of risks emerges (if it does), future legislation over the coming years can be surgically focused on the precise and well-substantiated direction of risks, minimizing collateral damage. To be clear, if truly strong evidence of risks emerges, then rules should be proportionately strong.</p><p>Overall, I am optimistic that a mixture of alignment training, mechanistic interpretability, efforts to find and publicly disclose concerning behaviors, safeguards, and societal-level rules can address AI autonomy risks, although I am most worried about societal-level rules and the behavior of the least responsible players (and it‚Äôs the least responsible players who advocate most strongly against regulation). I believe the remedy is what it always is in a democracy: those of us who believe in this cause should make our case that these risks are real and that our fellow citizens need to band together to protect themselves.</p><h2>2. A surprising and terrible empowerment</h2><h3><em>Misuse for destruction</em></h3><p>Let‚Äôs suppose that the problems of AI autonomy have been solved‚Äîwe are no longer worried that the country of AI geniuses will go rogue and overpower humanity. The AI geniuses do what humans want them to do, and because they have enormous commercial value, individuals and organizations throughout the world can ‚Äúrent‚Äù one or more AI geniuses to do various tasks for them.</p><p>Everyone having a superintelligent genius in their pocket is an amazing advance and will lead to an incredible creation of economic value and improvement in the quality of human life. I talk about these benefits in great detail in <em>Machines of Loving Grace</em>. But not every effect of making everyone superhumanly capable will be positive. It can potentially amplify the ability of individuals or small groups to cause destruction on a much larger scale than was possible before, by making use of sophisticated and dangerous tools (such as weapons of mass destruction) that were previously only available to a select few with a high level of skill, specialized training, and focus.</p><p>As Bill Joy wrote 25 years ago in <a href="https://sites.cc.gatech.edu/computing/nano/documents/Joy%20-%20Why%20the%20Future%20Doesn%27t%20Need%20Us.pdf"><em>Why the Future Doesn‚Äôt Need Us</em></a>:<sup>20</sup></p><blockquote>Building nuclear weapons required, at least for a time, access to both rare‚Äîindeed, effectively unavailable‚Äîraw materials and protected information; biological and chemical weapons programs also tended to require large-scale activities. The 21st century technologies‚Äîgenetics, nanotechnology, and robotics ... can spawn whole new classes of accidents and abuses ‚Ä¶ widely within reach of individuals or small groups. They will not require large facilities or rare raw materials. ‚Ä¶ we are on the cusp of the further perfection of extreme evil, an evil whose possibility spreads well beyond that which weapons of mass destruction bequeathed to the nation-states, to a surprising and terrible empowerment of extreme individuals.</blockquote><p>What Joy is pointing to is the idea that causing large-scale destruction requires both <em>motive </em>and <em>ability</em>, and as long as ability is restricted to a small set of highly trained people, there is relatively limited risk of single individuals (or small groups) causing such destruction.<sup>21</sup> A disturbed loner can perpetrate a school shooting, but probably can‚Äôt build a nuclear weapon or release a plague.</p><p>In fact, ability and motive may even be <em>negatively</em> correlated. The kind of person who has the <em>ability </em>to release a plague is probably highly educated: likely a PhD in molecular biology, and a particularly resourceful one, with a promising career, a stable and disciplined personality, and a lot to lose. This kind of person is unlikely to be interested in killing a huge number of people for no benefit to themselves and at great risk to their own future‚Äîthey would need to be motivated by pure malice, intense grievance, or instability.</p><p>Such people do exist, but they are rare, and tend to become huge stories when they occur, precisely because they are so unusual.<sup>22</sup> They also tend to be difficult to catch because they are intelligent and capable, sometimes leaving mysteries that take years or decades to solve. The most famous example is probably mathematician <a href="https://en.wikipedia.org/wiki/Ted_Kaczynski">Theodore Kaczynski</a> (the Unabomber), who evaded FBI capture for nearly 20 years, and was driven by an anti-technological ideology. Another example is biodefense researcher <a href="https://en.wikipedia.org/wiki/Bruce_Edwards_Ivins">Bruce Ivins</a>, who seems to have orchestrated a series of anthrax attacks in 2001. It‚Äôs also happened with skilled non-state organizations: the cult Aum Shinrikyo managed to obtain sarin nerve gas and kill 14 people (as well as injuring hundreds more) by <a href="https://en.wikipedia.org/wiki/Tokyo_subway_sarin_attack">releasing it in the Tokyo subway</a> in 1995.</p><p>Thankfully, none of these attacks used contagious biological agents, because the ability to construct or obtain these agents was beyond the capabilities of even these people.<sup>23</sup> Advances in molecular biology have now significantly lowered the barrier to creating biological weapons (especially in terms of availability of materials), but it still takes an enormous amount of expertise in order to do so. I am concerned that a genius in everyone‚Äôs pocket could remove that barrier, essentially making everyone a PhD virologist who can be walked through the process of designing, synthesizing, and releasing a biological weapon step-by-step. Preventing the elicitation of this kind of information in the face of serious adversarial pressure‚Äîso-called ‚Äújailbreaks‚Äù‚Äîlikely demands layers of defenses beyond those ordinarily baked into training.</p><p>Crucially, this will break the correlation between ability and motive: the disturbed loner who wants to kill people but lacks the discipline or skill to do so will now be elevated to the capability level of the PhD virologist, who is unlikely to have this motivation. This concern generalizes beyond biology (although I think biology is the scariest area) to any area where great destruction is possible but currently requires a high level of skill and discipline. To put it another way, renting a powerful AI gives intelligence to malicious (but otherwise average) people. I am worried there are potentially a large number of such people out there, and that if they have access to an easy way to kill millions of people, sooner or later one of them will do it. Additionally, those who <em>do </em>have expertise may be enabled to commit even larger-scale destruction than they could before.</p><p>Biology is by far the area I‚Äôm most worried about, because of its very large potential for destruction and the difficulty of defending against it, so I‚Äôll focus on biology in particular. But much of what I say here applies to other risks, like cyberattacks, chemical weapons, or nuclear technology.</p><p>I am not going to go into detail about how to make biological weapons, for reasons that should be obvious. But at a high level, I am concerned that LLMs are approaching (or may already have reached) the knowledge needed to create and release them end-to-end, and that their potential for destruction is very high. Some biological agents could cause millions of deaths if a determined effort was made to release them for maximum spread. However, this would still take a very high level of skill, including a number of very specific steps and procedures that are not widely known. My concern is not merely fixed or static knowledge. I am concerned that LLMs will be able to take someone of average knowledge and ability and walk them through a complex process that might otherwise go wrong or require debugging in an interactive way, similar to how tech support might help a non-technical person debug and fix complicated computer-related problems (although this would be a more extended process, probably lasting over weeks or months).</p><p>More capable LLMs (substantially beyond the power of today‚Äôs) might be capable of enabling even more frightening acts. In 2024, a group of prominent scientists <a href="https://www.science.org/doi/10.1126/science.ads9158">wrote a letter</a> warning about the risks of researching, and potentially creating, a dangerous new type of organism: ‚Äúmirror life.‚Äù The DNA, RNA, ribosomes, and proteins that make up biological organisms all have the same chirality (also called ‚Äúhandedness‚Äù) that causes them to be not equivalent to a version of themselves reflected in the mirror (just as your right hand cannot be rotated in such a way as to be identical to your left). But the whole system of proteins binding to each other, the machinery of DNA synthesis and RNA translation and the construction and breakdown of proteins, all depends on this handedness. If scientists made versions of this biological material with the opposite handedness‚Äîand there are some potential advantages of these, such as medicines that last longer in the body‚Äîit could be extremely dangerous. This is because left-handed life, if it were made in the form of complete organisms capable of reproduction (which would be very difficult), would potentially be indigestible to any of the systems that break down biological material on earth‚Äîit would have a ‚Äúkey‚Äù that wouldn‚Äôt fit into the ‚Äúlock‚Äù of any existing enzyme. This would mean that it could proliferate in an uncontrollable way and crowd out all life on the planet, in the worst case even destroying all life on earth.</p><p>There is <a href="https://www.science.org/doi/10.1126/science.ads9158#elettersSection">substantial scientific uncertainty</a> about both the creation and potential effects of mirror life. The 2024 letter accompanied <a href="https://purl.stanford.edu/cv716pj4036">a report</a> that concluded that ‚Äúmirror bacteria could plausibly be created in the next one to few decades,‚Äù which is a wide range. But a sufficiently powerful AI model (to be clear, far more capable than any we have today) might be able to discover how to create it much more rapidly‚Äîand actually help someone do so.</p><p>My view is that even though these are obscure risks, and might seem unlikely, the magnitude of the consequences is so large that they should be taken seriously as a first-class risk of AI systems.</p><p>Skeptics have raised a number of objections to the seriousness of these biological risks from LLMs, which I disagree with but which are worth addressing. Most fall into the category of not appreciating the exponential trajectory that the technology is on. Back in 2023 when we <a href="https://www.judiciary.senate.gov/imo/media/doc/2023-07-26_-_testimony_-_amodei.pdf">first started talking about biological risks from LLMs</a>, skeptics said that all the necessary information was available on Google and LLMs didn‚Äôt add anything beyond this. It was never true that Google could give you all the necessary information: genomes are freely available, but as I said above, certain key steps, as well as a huge amount of practical know-how cannot be gotten in that way. But also, by the end of 2023 LLMs were clearly providing information beyond what Google could give for some steps of the process.</p><p>After this, skeptics retreated to the objection that LLMs weren‚Äôt <em>end-to-end</em> useful, and couldn‚Äôt help with bioweapons <em>acquisition</em> as opposed to just providing theoretical information. As of mid-2025, our measurements show that LLMs may already be <a href="https://red.anthropic.com/2025/biorisk/">providing substantial uplift</a> in several relevant areas, perhaps doubling or tripling the likelihood of success. This led to us deciding that Claude Opus 4 (and the subsequent Sonnet 4.5, Opus 4.1, and Opus 4.5 models) needed to be released under our AI Safety Level 3 protections in our <a href="https://www.anthropic.com/rsp-updates">Responsible Scaling Policy</a> framework, and to implementing safeguards against this risk (more on this later). We believe that models are likely now approaching the point where, without safeguards, they could be useful in enabling someone with a STEM degree but not specifically a biology degree to go through the whole process of producing a bioweapon.</p><p>Another objection is that there are other actions unrelated to AI that society can take to block the production of bioweapons. Most prominently, the gene synthesis industry makes biological specimens on demand, and there is no federal requirement that providers screen orders to make sure they do not contain pathogens. An <a href="https://drive.google.com/file/d/1hNUnU8i2yubt5uesmmV17aTJXhYYDgTY/edit">MIT study</a> found that 36 out of 38 providers fulfilled an order containing the sequence of the 1918 flu. I am supportive of mandated gene synthesis screening that would make it harder for individuals to weaponize pathogens, in order to reduce both AI-driven biological risks and also biological risks in general. But this is not something we have today. It would also be only one tool in reducing risk; it is a complement to guardrails on AI systems, not a substitute.</p><p>The best objection is one that I‚Äôve rarely seen raised: that there is a gap between the models being useful in principle and the actual propensity of bad actors to use them. Most individual bad actors are disturbed individuals, so almost by definition their behavior is unpredictable and irrational‚Äîand it‚Äôs <em>these</em> bad actors, the unskilled ones, who might have stood to benefit the most from AI making it much easier to kill many people.<sup>24</sup> Just because a type of violent attack is possible, doesn‚Äôt mean someone will decide to do it. Perhaps biological attacks will be unappealing because they are reasonably likely to infect the perpetrator, they don‚Äôt cater to the military-style fantasies that many violent individuals or groups have, and it is hard to selectively target specific people. It could also be that going through a process that takes months, even if an AI walks you through it, involves an amount of patience that most disturbed individuals simply don‚Äôt have. We may simply get lucky and motive and ability don‚Äôt combine, in practice, in quite the right way.</p><p>But this seems like very flimsy protection to rely on. The motives of disturbed loners can change for any reason or no reason, and in fact there are already instances of <a href="https://en.wikipedia.org/wiki/2025_Las_Vegas_Cybertruck_explosion">LLMs being used in attacks</a> (just not with biology). The focus on disturbed loners also ignores ideologically motivated terrorists, who are often willing to expend large amounts of time and effort (for example, the 9/11 hijackers). Wanting to kill as many people as possible is a motive that will probably arise sooner or later, and it unfortunately suggests bioweapons as the method. Even if this motive is extremely rare, it only has to materialize once. And as biology advances (increasingly driven by AI itself), it may also become possible to carry out more selective attacks (for example, targeted against people with specific ancestries), which adds yet another, very chilling, possible motive.</p><p>I do not think biological attacks will necessarily be carried out the instant it becomes widely possible to do so‚Äîin fact, I would bet against that. But added up across millions of people and a few years of time, I think there is a serious risk of a major attack, and the consequences would be so severe (with casualties potentially in the millions or more) that I believe we have no choice but to take serious measures to prevent it.</p><h3><em>Defenses</em></h3><p>That brings us to how to defend against these risks. Here I see three things we can do. First, AI companies can put guardrails on their models to prevent them from helping to produce bioweapons. Anthropic is very actively doing this. <a href="https://www.anthropic.com/constitution">Claude‚Äôs Constitution</a>, which mostly focuses on high-level principles and values, has a small number of specific hard-line prohibitions, and one of them relates to helping with the production of biological (or chemical, or nuclear, or radiological) weapons. But all models <a href="https://www.microsoft.com/en-us/security/blog/2024/06/04/ai-jailbreaks-what-they-are-and-how-they-can-be-mitigated/">can be jailbroken</a>, and so as a second line of defense, we‚Äôve implemented (since mid-2025, when our tests showed our models were starting to get close to the threshold where they might begin to pose a risk) a classifier that specifically detects and blocks bioweapon-related outputs. <a href="https://www.anthropic.com/research/next-generation-constitutional-classifiers">We regularly upgrade and improve these classifiers</a>, and have generally found them highly robust even against sophisticated adversarial attacks.<sup>25</sup> These classifiers increase the costs to serve our models measurably (in some models, they are close to 5% of total inference costs) and thus cut into our margins, but we feel that using them is the right thing to do.</p><p>To their credit, some other AI companies <a href="https://arxiv.org/pdf/2504.01849">have implemented classifiers as well</a>. But not every company has, and there is also nothing requiring companies to keep their classifiers. I am concerned that over time there may be a <a href="https://en.wikipedia.org/wiki/Prisoner%27s_dilemma">prisoner‚Äôs dilemma</a> where companies can defect and lower their costs by removing classifiers. This is once again a classic negative externalities problem that can‚Äôt be solved by the voluntary actions of Anthropic or any other single company alone.<sup>26</sup> Voluntary industry standards may help, as may third-party evaluations and verification of the type done by <a href="https://www.aisi.gov.uk/">AI security</a> <a href="https://www.nist.gov/caisi">institutes</a> and <a href="https://metr.org/">third-party evaluators</a>.</p><p>But ultimately defense may require government action, which is the second thing we can do. My views here are the same as they are for addressing autonomy risks: we should start with <a href="https://www.anthropic.com/news/the-need-for-transparency-in-frontier-ai">transparency requirements</a>,<sup>27</sup> which help society measure, monitor, and collectively defend against risks without disrupting economic activity in a heavy-handed way. Then, if and when we reach clearer thresholds of risk, we can craft legislation that more precisely targets these risks and has a lower chance of collateral damage. In the particular case of bioweapons, I actually think that the time for such targeted legislation may be approaching soon‚ÄîAnthropic and other companies are learning more and more about the nature of biological risks and what is reasonable to require of companies in defending against them. Fully defending against these risks may require working internationally, even with geopolitical adversaries, but there is precedent in treaties prohibiting the development of biological weapons. I am generally a skeptic about most kinds of international cooperation on AI, but this may be one narrow area where there is some chance of achieving global restraint. Even dictatorships do not want massive bioterrorist attacks.</p><p>Finally, the third countermeasure we can take is to try to develop defenses against biological attacks themselves. This could include monitoring and tracking for early detection, investments in air purification R&amp;D (such as <a href="https://worksinprogress.co/issue/the-death-rays-that-guard-life/">far-UVC</a> disinfection), rapid vaccine development that can respond and adapt to an attack, better personal protective equipment (PPE),<sup>28</sup> and treatments or vaccinations for some of the most likely biological agents. <a href="https://en.wikipedia.org/wiki/MRNA_vaccine">mRNA vaccines</a>, which can be designed to respond to a particular virus or variant, are an early example of <a href="https://worksinprogress.co/issue/the-golden-age-of-vaccine-development/">what is possible here</a>. Anthropic is excited to work with biotech and pharmaceutical companies on this problem. But unfortunately I think our expectations on the defensive side should be limited. There is an <a href="https://www.rand.org/pubs/perspectives/PEA4102-1.html">asymmetry between attack and defense</a> in biology, because agents spread rapidly on their own, while defenses require detection, vaccination, and treatment to be organized across large numbers of people very quickly in response. Unless the response is lightning quick (which it rarely is), much of the damage will be done before a response is possible. It is conceivable that future technological improvements could shift this balance in favor of defense (and we should certainly <a href="https://www.nationalacademies.org/read/28868/chapter/1">use AI to help develop such technological advances</a>), but until then, preventative safeguards will be our main line of defense.</p><p>It‚Äôs worth a brief mention of cyberattacks here, since unlike biological attacks, <a href="https://www.anthropic.com/news/disrupting-AI-espionage">AI-led cyberattacks have actually happened in the wild</a>, including at a large scale and for state-sponsored espionage. We expect these attacks to <a href="https://red.anthropic.com/2025/ai-for-cyber-defenders/">become more capable</a> as models advance rapidly, until they are the main way in which cyberattacks are conducted. I expect AI-led cyberattacks to become a serious and unprecedented threat to the integrity of computer systems around the world, and Anthropic is working very hard to shut down these attacks and eventually reliably prevent them from happening. The reason I haven‚Äôt focused on cyber as much as biology is that (1) cyberattacks are much less likely to kill people, certainly not at the scale of biological attacks, and (2) the offense-defense balance may be more tractable in cyber, where there is at least some hope that defense could keep up with (and even ideally outpace) AI attack if we invest in it properly.</p><p>Although biology is currently the most serious vector of attack, there are many other vectors and it is possible that a more dangerous one may emerge. The general principle is that without countermeasures, AI is likely to continuously lower the barrier to destructive activity on a larger and larger scale, and humanity needs a serious response to this threat.</p><h2>3. The odious apparatus</h2><h3><em>Misuse for seizing power</em></h3><p>The previous section discussed the risk of individuals and small organizations co-opting a small subset of the ‚Äúcountry of geniuses in a datacenter‚Äù to cause large-scale destruction. But we should also worry‚Äîlikely substantially more so‚Äîabout misuse of AI for the purpose of <em>wielding or</em> <em>seizing power</em>, likely by larger and more established actors.<sup>29</sup></p><p>In <em>Machines of Loving Grace</em>, I discussed the possibility that authoritarian governments might use powerful AI to surveil or repress their citizens in ways that would be extremely difficult to reform or overthrow. Current autocracies are limited in how repressive they can be by the need to have humans carry out their orders, and humans often have limits in how inhumane they are willing to be. But AI-enabled autocracies would not have such limits.</p><p>Worse yet, countries could also use their advantage in AI to gain power over <em>other countries</em>. If the ‚Äúcountry of geniuses‚Äù as a whole was simply owned and controlled by a single (human) country‚Äôs military apparatus, and other countries did not have equivalent capabilities, it is hard to see how they could defend themselves: they would be outsmarted at every turn, similar to a war between humans and mice. Putting these two concerns together leads to the alarming possibility of a global totalitarian dictatorship. Obviously, it should be one of our highest priorities to prevent this outcome.</p><p>There are many ways in which AI could enable, entrench, or expand autocracy, but I‚Äôll list a few that I‚Äôm most worried about. Note that some of these applications have legitimate defensive uses, and I am not necessarily arguing against them in absolute terms; I am nevertheless worried that they structurally tend to favor autocracies:</p><ul role="list"><li> <strong>Fully autonomous weapons. </strong>A swarm of millions or billions of fully automated armed drones, locally controlled by powerful AI and strategically coordinated across the world by an even more powerful AI, could be an unbeatable army, capable of both defeating any military in the world and suppressing dissent within a country by following around every citizen. <a href="https://www.hudson.org/missile-defense/impact-drones-battlefield-lessons-russian-ukraine-war-french-perspective-tsiporah-fried">Developments in the Russia-Ukraine War</a> should alert us to the fact that drone warfare is already with us (though not fully autonomous yet, and a tiny fraction of what might be possible with powerful AI). R&amp;D from powerful AI could make the drones of one country far superior to those of others, speed up their manufacture, make them more resistant to electronic attacks, improve their maneuvering, and so on. Of course, these weapons also have legitimate uses in the defense of democracy: they have been key to defending Ukraine and would likely be key to defending Taiwan. But they are a dangerous weapon to wield: we should worry about them in the hands of autocracies, but also worry that because they are so powerful, with so little accountability, there is a greatly increased risk of democratic governments turning them against their own people to seize power.</li><li> <strong>AI surveillance. </strong>Sufficiently powerful AI could likely be used to compromise any computer system in the world,<sup>30</sup> and could also use the access obtained in this way to read <em>and make sense of</em> all the world‚Äôs electronic communications (or even all the world‚Äôs in-person communications, if recording devices can be built or commandeered). It might be frighteningly plausible to simply generate a complete list of anyone who disagrees with the government on any number of issues, even if such disagreement isn‚Äôt explicit in anything they say or do. A powerful AI looking across billions of conversations from millions of people could gauge public sentiment, detect pockets of disloyalty forming, and stamp them out before they grow. This could lead to the imposition of a true panopticon on a scale that we don‚Äôt see today, even with the CCP.</li><li> <strong>AI propaganda. </strong>Today‚Äôs phenomena of ‚Äú<a href="https://en.wikipedia.org/wiki/Chatbot_psychosis">AI psychosis</a>‚Äù and ‚ÄúAI girlfriends‚Äù suggest that even at their current level of intelligence, AI models can have a powerful psychological influence on people. Much more powerful versions of these models, that were much more embedded in and aware of people‚Äôs daily lives and could model and influence them over months or years, would likely be capable of essentially brainwashing many (most?) people into any desired ideology or attitude, and could be employed by an unscrupulous leader to ensure loyalty and suppress dissent, even in the face of a level of repression that most populations would rebel against. Today people worry a lot about, for example, the potential <a href="https://networkcontagion.us/wp-content/uploads/NCRI-Report_-The-CCPs-Digital-Charm-Offensive.pdf">influence of TikTok</a> as CCP propaganda directed at children. I worry about that too, but a personalized AI agent that gets to know you over years and uses its knowledge of you to shape all of your opinions would be dramatically more powerful than this.</li><li> <strong>Strategic decision-making. </strong>A country of geniuses in a datacenter could be used to advise a country, group, or individual on geopolitical strategy, what we might call a ‚Äúvirtual Bismarck.‚Äù It could optimize the three strategies above for seizing power, plus probably develop many others that I haven‚Äôt thought of (but that a country of geniuses could). Diplomacy, military strategy, R&amp;D, economic strategy, and many other areas are all likely to be substantially increased in effectiveness by powerful AI. Many of these skills would be legitimately helpful for democracies‚Äîwe want democracies to have access to the best strategies for defending themselves against autocracies‚Äîbut the potential for misuse in <em>anyone‚Äôs </em>hands still remains.</li></ul><p>Having described <em>what </em>I am worried about, let‚Äôs move on to <em>who</em>. I am worried about entities who have the most access to AI, who are starting from a position of the most political power, or who have an existing history of repression. In order of severity, I am worried about:</p><ul role="list"><li> <strong>The CCP. </strong>China is second only to the United States in AI capabilities, and is the country with the greatest likelihood of surpassing the United States in those capabilities. Their government is currently autocratic and operates a high-tech surveillance state. It has deployed AI-based surveillance already (including in the repression of <a href="https://en.wikipedia.org/wiki/Uyghurs">Uyghurs</a>), and is believed to employ algorithmic propaganda via TikTok (in addition to its many other international propaganda efforts). They have hands down the clearest path to the AI-enabled totalitarian nightmare I laid out above. It may even be the default outcome within China, as well as within other autocratic states to whom the CCP exports surveillance technology. I have <a href="https://www.darioamodei.com/post/on-deepseek-and-export-controls">written often</a> about the threat of the CCP taking the lead in AI and the existential imperative to prevent them from doing so. This is why. To be clear, I am not singling out China out of animus to them in particular‚Äîthey are simply the country that most combines AI prowess, an autocratic government, and a high-tech surveillance state. If anything, it is the Chinese people themselves who are most likely to suffer from the CCP‚Äôs AI-enabled repression, and they have no voice in the actions of their government. I greatly admire and respect the Chinese people and support the many brave dissidents within China and their struggle for freedom.</li><li> <strong>Democracies competitive in AI. </strong>As I wrote above, democracies have a legitimate interest in some AI-powered military and geopolitical tools, because democratic governments offer the best chance to counter the use of these tools by autocracies. Broadly, I am supportive of arming democracies with the tools needed to defeat autocracies in the age of AI‚ÄîI simply don‚Äôt think there is any other way. But we cannot ignore the potential for abuse of these technologies by democratic governments themselves. Democracies normally have safeguards that prevent their military and intelligence apparatus from being turned inwards against their own population,<sup>31</sup> but because AI tools require so few people to operate, there is potential for them to circumvent these safeguards and the norms that support them. It is also worth noting that some of these safeguards are already gradually eroding in some democracies. Thus, we should arm democracies with AI, but we should do so carefully and within limits: they are the immune system we need to fight autocracies, but like the immune system, there is some risk of them turning on us and becoming a threat themselves.</li><li> <strong>Non-democratic countries with large datacenters. </strong>Beyond China, most countries with less democratic governance are not leading AI players in the sense that they don‚Äôt have companies which produce frontier AI models. Thus they pose a fundamentally different and lesser risk than the CCP, which remains the primary concern (most are also less repressive, and the ones that are more repressive, like North Korea, have no significant AI industry at all). But some of these countries do have large <em>datacenters </em>(often as part of buildouts by companies operating in democracies), which can be used to run frontier AI at large scale (though this does not confer the ability to push the frontier). There is some amount of danger associated with this‚Äîthese governments could in principle expropriate the datacenters and use the country of AIs within it for their own ends. I am less worried about this compared to countries like China that directly develop AI, but it‚Äôs a risk to keep in mind.<sup>32</sup></li><li> <strong>AI companies. </strong>It is somewhat awkward to say this as the CEO of an AI company, but I think the next tier of risk is actually AI companies themselves. AI companies control large datacenters, train frontier models, have the greatest expertise on how to use those models, and in some cases have daily contact with and the possibility of influence over tens or hundreds of millions of users. The main thing they lack is the legitimacy and infrastructure of a state, so much of what would be needed to build the tools of an AI autocracy would be illegal for an AI company to do, or at least exceedingly suspicious. But some of it is not impossible: they could, for example, use their AI products to brainwash their massive consumer user base, and the public should be alert to the risk this represents. I think the governance of AI companies deserves a lot of scrutiny.</li></ul><p>There are a number of possible arguments against the severity of these threats, and I wish I believed them, because AI-enabled authoritarianism terrifies me. It‚Äôs worth going through some of these arguments and responding to them.</p><p>First, some people might put their faith in the nuclear deterrent, particularly to counter the use of AI autonomous weapons for military conquest. If someone threatens to use these weapons against you, you can always threaten a nuclear response back. My worry is that I‚Äôm <a href="https://futureoflife.org/document/ai-and-nuclear-problem-analysis-and-policy-recommendations/">not totally sure we can be confident</a> in the nuclear deterrent against a country of geniuses in a datacenter: it is possible that powerful AI could <a href="https://www.rand.org/content/dam/rand/pubs/perspectives/PE200/PE296/RAND_PE296.pdf">devise ways</a> to <a href="https://spectrum.ieee.org/nuclear-submarine">detect and strike nuclear submarines</a>, <a href="https://www.sipri.org/sites/default/files/2019-05/sipri1905-ai-strategic-stability-nuclear-risk.pdf">conduct influence operations</a> against the operators of nuclear weapons infrastructure, or use AI‚Äôs <a href="https://committees.parliament.uk/writtenevidence/120293/pdf/">cyber capabilities</a> to launch a cyberattack against satellites used to detect nuclear launches.<sup>33</sup> Alternatively, it‚Äôs possible that taking over countries is feasible with only AI surveillance and AI propaganda, and never actually presents a clear moment where it‚Äôs obvious what is going on and where a nuclear response would be appropriate. <em>Maybe </em>these things aren‚Äôt feasible and the nuclear deterrent will still be effective, but it seems too high stakes to take a risk.<sup>34</sup> </p><p>A second possible objection is that there might be countermeasures we can take against these tools of autocracy. We can counter drones with our own drones, cyberdefense will improve along with cyberattack, there may be ways to immunize people against propaganda, etc. My response is that these defenses will only be possible with comparably powerful AI. If there isn‚Äôt some counterforce with a comparably smart and numerous country of geniuses in a datacenter, it won‚Äôt be possible to match the quality or quantity of drones, for cyberdefense to outsmart cyberoffense, etc. So the question of countermeasures reduces to the question of a balance of power in powerful AI. Here, I am concerned about the recursive or self-reinforcing property of powerful AI (which I discussed at the beginning of this essay): that each generation of AI can be used to design and train the next generation of AI. This leads to a risk of a runaway advantage, where the current leader in powerful AI may be able to increase their lead and may be difficult to catch up with. We need to make sure it is not an authoritarian country that gets to this loop first.</p><p>Furthermore, even if a balance of power can be achieved, there is still risk that the world could be split up into autocratic spheres, as in <em>Nineteen Eighty-Four</em>. Even if several competing powers each have their powerful AI models, and none can overpower the others, each power could still internally repress their own population, and would be very difficult to overthrow (since the populations don‚Äôt have powerful AI to defend themselves). It is thus important to prevent AI-enabled autocracy even if it doesn‚Äôt lead to a single country taking over the world.</p><h3><em>Defenses</em></h3><p>How do we defend against this wide range of autocratic tools and potential threat actors? As in the previous sections, there are several things I think we can do. First, we should absolutely not be selling chips, chip-making tools, or datacenters to the CCP. Chips and chip-making tools are the single greatest bottleneck to powerful AI, and blocking them is a simple but extremely effective measure, perhaps the most important single action we can take. It makes no sense to sell the CCP the tools with which to build an AI totalitarian state and possibly conquer us militarily. A number of complicated arguments are made to justify such sales, such as the idea that ‚Äúspreading our tech stack around the world‚Äù allows ‚ÄúAmerica to win‚Äù in some general, unspecified economic battle. In my view, this is like selling nuclear weapons to North Korea and then bragging that the missile casings are made by Boeing and so the US is ‚Äúwinning.‚Äù China is several years behind the US in their ability to produce frontier chips in quantity, and the critical period for building the country of geniuses in a datacenter is very likely to be within those next several years.<sup>35</sup> There is no reason to give a giant boost to their AI industry during this critical period.</p><p>Second, it makes sense to use AI to empower democracies to resist autocracies. This is the reason Anthropic considers it important to provide AI to the intelligence and defense communities in the US and its democratic allies. Defending democracies that are under attack, such as Ukraine and (via cyber attacks) Taiwan, seems especially high priority, as does empowering democracies to use their intelligence services to disrupt and degrade autocracies from the inside. At some level the only way to respond to autocratic threats is to match and outclass them militarily. A coalition of the US and its democratic allies, if it achieved predominance in powerful AI, would be in a position to not only defend itself against autocracies, but contain them and limit their AI totalitarian abuses.</p><p>Third, we need to draw a hard line against AI abuses within democracies. There need to be limits to what we allow our governments to do with AI, so that they don‚Äôt seize power or repress their own people. The formulation I have come up with is that we should use AI for national defense in all ways <em>except those which would make us more like our autocratic adversaries</em>. </p><p>Where should the line be drawn? In the list at the beginning of this section, two items‚Äîusing AI for domestic mass surveillance and mass propaganda‚Äîseem to me like bright red lines and entirely illegitimate. Some might argue that there‚Äôs no need to do anything (at least in the US), since domestic mass surveillance is already illegal under the Fourth Amendment. But the rapid progress of AI may create situations that our existing legal frameworks are not well designed to deal with. For example, it would likely not be unconstitutional for the US government to conduct massively scaled recordings of all <em>public </em>conversations (e.g., things people say to each other on a street corner), and previously it would have been difficult to sort through this volume of information, but with AI it could all be transcribed, interpreted, and triangulated to create a picture of the attitude and loyalties of many or most citizens. I would support civil liberties-focused legislation (or maybe even a constitutional amendment) that imposes stronger guardrails against AI-powered abuses.</p><p>The other two items‚Äîfully autonomous weapons and AI for strategic decision-making‚Äîare harder lines to draw since they have legitimate uses in defending democracy, while also being prone to abuse. Here I think what is warranted is extreme care and scrutiny combined with guardrails to prevent abuses. My main fear is having too small a number of ‚Äúfingers on the button,‚Äù such that one or a handful of people could essentially operate a drone army without needing any other humans to cooperate to carry out their orders. As AI systems get more powerful, we may need to have more direct and immediate oversight mechanisms to ensure they are not misused, perhaps involving branches of government other than the executive. I think we should approach fully autonomous weapons in particular with great caution,<sup>36</sup> and not rush into their use without proper safeguards.</p><p>Fourth, after drawing a hard line against AI abuses in democracies, we should use that precedent to create an international taboo against the worst abuses of powerful AI. I recognize that the current political winds have turned against international cooperation and international norms, but this is a case where we sorely need them. The world needs to understand the dark potential of powerful AI in the hands of autocrats, and to recognize that certain uses of AI amount to an attempt to permanently steal their freedom and impose a totalitarian state from which they can‚Äôt escape. I would even argue that in some cases, large-scale surveillance with powerful AI, mass propaganda with powerful AI, and certain types of <em>offensive </em>uses of fully autonomous weapons should be considered crimes against humanity. More generally, a robust norm against AI-enabled totalitarianism and all its tools and instruments is sorely needed.</p><p>It is possible to have an even stronger version of this position, which is that because the possibilities of AI-enabled totalitarianism are so dark, autocracy is simply not a form of government that people can accept in the post-powerful AI age. Just as feudalism became unworkable with the industrial revolution, the AI age could lead inevitably and logically to the conclusion that democracy (and, hopefully, democracy improved and reinvigorated by AI, as I discuss in <em>Machines of Loving Grace</em>) is the only viable form of government if humanity is to have a good future.</p><p>Fifth and finally, AI companies should be carefully watched, as should their connection to the government, which is necessary, but must have limits and boundaries. The sheer amount of capability embodied in powerful AI is such that ordinary corporate governance‚Äîwhich is designed to protect shareholders and prevent ordinary abuses such as fraud‚Äîis unlikely to be up to the task of governing AI companies. There may also be value in companies publicly committing to (perhaps even as part of corporate governance) not take certain actions, such as privately building or stockpiling military hardware, using large amounts of computing resources by single individuals in unaccountable ways, or using their AI products as propaganda to manipulate public opinion in their favor.</p><p>The danger here comes from many directions, and some directions are in tension with others. The only constant is that we must seek accountability, norms, and guardrails for everyone, even as we empower ‚Äúgood‚Äù actors to keep ‚Äúbad‚Äù actors in check.</p><h2>4. Player piano</h2><h3><em>Economic disruption</em></h3><p>The previous three sections were essentially about security risks posed by powerful AI: risks from the AI itself, risks from misuse by individuals and small organizations and risks of misuse by states and large organizations. If we put aside security risks or assume they have been solved, the next question is economic. What will be the effect of this infusion of incredible ‚Äúhuman‚Äù capital on the economy? Clearly, the most obvious effect will be to greatly increase economic growth. The pace of advances in scientific research, biomedical innovation, manufacturing, supply chains, the efficiency of the financial system, and much more are almost guaranteed to lead to a much faster rate of economic growth. In <em>Machines of Loving Grace</em>, I suggest that a 10‚Äì20% sustained annual GDP growth rate may be possible.</p><p>But it should be clear that this is a double-edged sword: what are the economic prospects for most existing humans in such a world? New technologies often bring labor market shocks, and in the past humans have always recovered from them, but I am concerned that this is because these previous shocks affected only a small fraction of the full possible range of human abilities, leaving room for humans to expand to new tasks. AI will have effects that are much broader and occur much faster, and therefore I worry it will be much more challenging to make things work out well.</p><h3><em>Labor market disruption</em></h3><p>There are two specific problems I am worried about: labor market displacement, and concentration of economic power. Let‚Äôs start with the first one. This is a topic that I <a href="https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic">warned about very publicly in 2025</a>, where I predicted that AI could displace half of all entry-level white collar jobs in the next 1‚Äì5 years, even as it accelerates economic growth and scientific progress. This warning started a public debate about the topic. Many CEOs, technologists, and economists agreed with me, but others assumed I was falling prey to a ‚Äúlump of labor‚Äù fallacy and didn‚Äôt know how labor markets worked, and some didn‚Äôt see the 1‚Äì5-year time range and thought I was claiming AI is displacing jobs right now (which I agree it is likely not). So it is worth going through in detail why I am worried about labor displacement, to clear up these misunderstandings.</p><p>As a baseline, it‚Äôs useful to understand how labor markets <em>normally</em> respond to advances in technology. When a new technology comes along, it starts by making pieces of a given human job more efficient. For example, early in the Industrial Revolution, machines, such as upgraded plows, enabled human farmers to be more efficient at some aspects of the job. This improved the productivity of farmers, which increased their wages.</p><p>In the next step, some parts of the job of farming could be done <em>entirely</em> by machines, for example with the invention of the <a href="https://en.wikipedia.org/wiki/Threshing_machine">threshing machine</a> or <a href="https://en.wikipedia.org/wiki/Seed_drill">seed drill</a>. In this phase, humans did a lower and lower fraction of the job, but the work they <em>did</em> complete became more and more leveraged because it is complementary to the work of machines, and their productivity continued to rise. As described by <a href="https://en.wikipedia.org/wiki/Jevons_paradox">Jevons‚Äô paradox</a>, the wages of farmers and perhaps even the number of farmers continued to increase. Even when 90% of the job is being done by machines, humans can simply do 10x more of the 10% they still do, producing 10x as much output for the same amount of labor.</p><p>Eventually, machines do everything or almost everything, as with modern <a href="https://en.wikipedia.org/wiki/Combine_harvester">combine harvesters</a>, tractors, and other equipment. At this point farming as a form of human employment really does go into steep decline, and this potentially causes serious disruption in the short term, but because farming is just one of many useful activities that humans are able to do, people eventually switch to other jobs, such as operating factory machines. This is true even though farming accounted for a huge proportion of employment <em>ex ante</em>. 250 years ago, 90% of Americans <a href="https://www.nass.usda.gov/About_NASS/History_of_Ag_Statistics/index.php">lived on farms</a>; in Europe, 50‚Äì60% of employment <a href="https://ourworldindata.org/grapher/share-of-the-labor-force-employed-in-agriculture?time=1750..latest">was agricultural</a>. Now those percentages are in the low single digits in those places, because workers switched to industrial jobs (and later, knowledge work jobs). The economy can do what previously required most of the labor force with only 1‚Äì2% of it, freeing up the rest of the labor force to build an ever more advanced industrial society. There‚Äôs no fixed ‚Äú<a href="https://en.wikipedia.org/wiki/Lump_of_labour_fallacy">lump of labor</a>,‚Äù just an ever-expanding ability to do <a href="https://en.wikipedia.org/wiki/Ephemeralization">more and more with less and less</a>. People‚Äôs wages rise in line with the GDP exponential and the economy maintains full employment once disruptions in the short term have passed.</p><p>It‚Äôs possible things will go roughly the same way with AI, but I would bet pretty strongly against it. Here are some reasons I think AI is likely to be different:</p><ul role="list"><li> <strong>Speed. </strong>The pace of progress in AI is much faster than for previous technological revolutions. For example, in the last 2 years, AI models went from barely being able to complete a single line of code, to <a href="https://x.com/bcherny/status/2004887829252317325">writing all or almost all of the code</a> for some people‚Äîincluding engineers at Anthropic.<sup>37</sup> Soon, they may do the entire task of a software engineer end to end.<sup>38</sup> It is hard for people to adapt to this pace of change, both to the changes in how a given job works and in the need to switch to new jobs. Even legendary programmers are increasingly <a href="https://x.com/karpathy/status/2004607146781278521">describing themselves as ‚Äúbehind.‚Äù</a> The pace may if anything continue to speed up, as AI coding models increasingly accelerate the task of AI development. To be clear, speed in itself does not mean labor markets and employment won‚Äôt eventually recover, it just implies the short-term transition will be unusually painful compared to past technologies, since humans and labor markets are slow to react and to equilibrate.</li><li> <strong>Cognitive breadth. </strong>As suggested by the phrase ‚Äúcountry of geniuses in a datacenter,‚Äù AI will be capable of a very wide range of human cognitive abilities‚Äîperhaps all of them. This is very different from previous technologies like mechanized farming, transportation, or even computers.<sup>39</sup> This will make it harder for people to switch easily from jobs that are displaced to similar jobs that they would be a good fit for. For example, the general intellectual abilities required for entry-level jobs in, say, finance, consulting, and law are fairly similar, even if the specific knowledge is quite different. A technology that disrupted only one of these three would allow employees to switch to the two other close substitutes (or for undergraduates to switch majors). But disrupting all three at once (along with many other similar jobs) may be harder for people to adapt to. Furthermore, it‚Äôs not <em>just </em>that most existing jobs will be disrupted. That part has happened before‚Äîrecall that farming was a huge percentage of employment. But farmers could switch to the relatively similar work of operating factory machines, even though that work hadn‚Äôt been common before. By contrast, AI is increasingly matching the general cognitive profile of humans, which means it will also be good at the new jobs that would ordinarily be created in response to the old ones being automated. Another way to say it is that AI isn‚Äôt a substitute for specific human jobs but rather a general labor substitute for humans.</li><li> <strong>Slicing by cognitive ability. </strong>Across a wide range of tasks, AI appears to be advancing from the bottom of the ability ladder to the top. For example, in coding our models have proceeded from the level of ‚Äúa mediocre coder‚Äù to ‚Äúa strong coder‚Äù to ‚Äúa very strong coder.‚Äù<sup>40</sup> We are now starting to see the same progression in white-collar work in general. We are thus at risk of a situation where, instead of affecting people with specific skills or in specific professions (who can adapt by retraining), AI is affecting people with certain intrinsic cognitive properties, namely lower intellectual ability (which is harder to change). It is not clear where these people will go or what they will do, and I am concerned that they could form an unemployed or very-low-wage ‚Äúunderclass.‚Äù To be clear, things somewhat like this have happened before‚Äîfor example, computers and the internet are believed by some economists to represent ‚Äú<a href="https://www.sciencedirect.com/science/chapter/handbook/abs/pii/S0169721811024105">skill-biased technological change</a>.‚Äù But this skill biasing was both not as extreme as what I expect to see with AI, and is believed to have contributed to an increase in wage inequality,<sup>41</sup> so it is not exactly a reassuring precedent.</li><li> <strong>Ability to fill in the gaps. </strong>The way human jobs often adjust in the face of new technology is that there are many aspects to the job, and the new technology, even if it appears to directly replace humans, often has gaps in it. If someone invents a machine to make widgets, humans may still have to load raw material into the machine. Even if that takes only 1% as much effort as making the widgets manually, human workers can simply make 100x more widgets. But AI, in addition to being a rapidly advancing technology, is also a rapidly <em>adapting </em>technology. During every model release, AI companies carefully measure what the model is good at and what it isn‚Äôt, and customers also provide such information after the launch. Weaknesses can be addressed by collecting tasks that embody the current gap, and training on them for the next model. Early in generative AI, users noticed that AI systems had certain weaknesses (such as AI image models generating hands with the wrong number of fingers) and many assumed these weaknesses were inherent to the technology. If they were, it would limit job disruption. But pretty much every such weakness gets addressed quickly‚Äî often, within just a few months.</li></ul><p>It‚Äôs worth addressing common points of skepticism. First, there is the argument that economic diffusion will be slow, such that even if the underlying technology is <em>capable </em>of doing most human labor, the actual application of it across the economy may be much slower (for example in industries that are far from the AI industry and slow to adopt). Slow diffusion of technology is <a href="https://www.ft.com/content/3b93e647-2a8b-4fb4-831d-e27adf4db5f8">definitely real</a>‚ÄîI talk to people from a wide variety of enterprises, and there are places where the adoption of AI will take years. That‚Äôs why my prediction for 50% of entry level white collar jobs being disrupted is 1‚Äì5 years, even though I suspect we‚Äôll have powerful AI (which would be, technologically speaking, enough to do <em>most or all </em>jobs, not just entry level) in much less than 5 years. But diffusion effects merely buy us time. And I am not confident they will be as slow as people predict. Enterprise AI adoption is growing at rates much faster than any previous technology, largely on the pure strength of the technology itself. Also, even if traditional enterprises are slow to adopt new technology, startups will spring up to serve as ‚Äúglue‚Äù and make the adoption easier. If that doesn‚Äôt work, the startups may simply disrupt the incumbents directly.</p><p>That could lead to a world where it isn‚Äôt so much that specific jobs are disrupted as it is that large enterprises are disrupted in general and replaced with much less labor-intensive startups. This could also lead to a world of ‚Äúgeographic inequality,‚Äù where an increasing fraction of the world‚Äôs wealth is concentrated in Silicon Valley, which becomes its own economy running at a different speed than the rest of the world and leaving it behind. All of these outcomes would be great for economic growth‚Äîbut not so great for the labor market or those who are left behind.</p><p>Second, some people say that human jobs will move to the physical world, which avoids the whole category of ‚Äúcognitive labor‚Äù where AI is progressing so rapidly. I am not sure how safe this is, either. A lot of physical labor is already being done by machines (e.g., manufacturing) or will soon be done by machines (e.g., driving). Also, sufficiently powerful AI will be able to accelerate the development of robots, and then control those robots in the physical world. It may buy some time (which is a good thing), but I‚Äôm worried it won‚Äôt buy much. And even if the disruption was limited only to cognitive tasks, it would still be an unprecedentedly large and rapid disruption.</p><p>Third, perhaps some tasks inherently require or greatly benefit from a human touch. I‚Äôm a little more uncertain about this one, but I‚Äôm still skeptical that it will be enough to offset the bulk of the impacts I described above. AI is already widely used for customer service. Many people <a href="https://undark.org/2025/11/04/chatbot-mental-health/">report</a> that it is easier to talk to AI about their personal problems than to talk to a therapist‚Äîthat the AI is more patient. When my sister was struggling with medical problems during a pregnancy, she felt she wasn‚Äôt getting the answers or support she needed from her care providers, and she found Claude to have a better bedside manner (as well as succeeding better at diagnosing the problem). I‚Äôm sure there are some tasks for which a human touch really is important, but I‚Äôm not sure how many‚Äîand here we‚Äôre talking about finding work for nearly everyone in the labor market.</p><p>Fourth, some may argue that comparative advantage will still protect humans. Under the <a href="https://en.wikipedia.org/wiki/Comparative_advantage">law of comparative advantage</a>, even if AI is better than humans at everything, any <em>relative </em>differences between the human and AI profile of skills creates a basis of trade and specialization between humans and AI. The problem is that if AIs are literally thousands of times more productive than humans, this logic starts to break down. Even tiny <a href="https://en.wikipedia.org/wiki/Transaction_cost">transaction costs</a> could make it not worth it for AI to trade with humans. And human wages may be very low, even if they technically have something to offer.</p><p>It‚Äôs possible all of these factors can be addressed‚Äîthat the labor market is resilient enough to adapt to even such an enormous disruption. But even if it can eventually adapt, the factors above suggest that the short-term shock will be unprecedented in size.</p><h3><em>Defenses</em></h3><p>What can we do about this problem? I have several suggestions, some of which Anthropic is already doing. The first thing is simply to get accurate data about what is happening with job displacement in real time. When an economic change happens very quickly, it‚Äôs hard to get reliable data about what is happening, and without reliable data it is hard to design effective policies. For example, government data is currently lacking granular, high-frequency data on AI adoption across firms and industries. For the last year Anthropic has been operating and publicly releasing an <a href="https://www.anthropic.com/economic-index">Economic Index</a> that shows use of our models almost in real time, broken down by industry, task, location, and even things like whether a task was being automated or conducted collaboratively. We also have an <a href="https://www.anthropic.com/news/introducing-the-anthropic-economic-advisory-council">Economic Advisory Council</a> to help us interpret this data and see what is coming.</p><p>Second, AI companies have a choice in how they work with enterprises. The very inefficiency of traditional enterprises means that their rollout of AI can be very path dependent, and there is some room to choose a better path. Enterprises often have a choice between ‚Äúcost savings‚Äù (doing the same thing with fewer people) and ‚Äúinnovation‚Äù (doing more with the same number of people). The market will inevitably produce both eventually, and any competitive AI company will have to serve some of both, but there may be some room to steer companies towards innovation when possible, and it may buy us some time. Anthropic is actively thinking about this.</p><p>Third, companies should think about how to take care of their employees. In the short term, being creative about ways to reassign employees within companies may be a promising way to stave off the need for layoffs. In the long term, in a world with enormous total wealth, in which many companies increase greatly in value due to increased productivity and capital concentration, it may be feasible to pay human employees even long after they are no longer providing economic value in the traditional sense. Anthropic is currently considering a range of possible pathways for our own employees that we will share in the near future.</p><p>Fourth, wealthy individuals have an obligation to help solve this problem. It is sad to me that many wealthy individuals (especially in the tech industry) have recently adopted a cynical and nihilistic attitude that philanthropy is inevitably fraudulent or useless. Both private philanthropy like the <a href="https://www.gatesfoundation.org/">Gates Foundation</a> and public programs like <a href="https://en.wikipedia.org/wiki/President%27s_Emergency_Plan_for_AIDS_Relief">PEPFAR</a> have saved tens of millions of lives in the developing world, and helped to create economic opportunity in the developed world. All of Anthropic‚Äôs co-founders have pledged to donate 80% of our wealth, and Anthropic‚Äôs staff have individually pledged to donate company shares worth billions at current prices‚Äîdonations that the company has committed to matching.</p><p>Fifth, while all the above private actions can be helpful, ultimately a macroeconomic problem this large will require government intervention. The natural policy response to an enormous economic pie coupled with high inequality (due to a lack of jobs, or poorly paid jobs, for many) is progressive taxation. The tax could be general or could be targeted against AI companies in particular. Obviously tax design is complicated, and there are many ways for it to go wrong. I don‚Äôt support poorly designed tax policies. I think the extreme levels of inequality predicted in this essay justify a more robust tax policy on basic moral grounds, but I can also make a pragmatic argument to the world‚Äôs billionaires that it‚Äôs in their interest to support a good version of it: if they don‚Äôt support a good version, they‚Äôll inevitably get a bad version designed by a mob.</p><p>Ultimately, I think of all of the above interventions as ways to buy time. In the end AI will be able to do everything, and we need to grapple with that. It‚Äôs my hope that by that time, we can use AI itself to help us restructure markets in ways that work for everyone, and that the interventions above can get us through the transitional period.</p><h3><em>Economic concentration of power</em></h3><p>Separate from the problem of job displacement or economic inequality <em>per se</em> is the problem of <em>economic concentration of power.</em> Section 1 discussed the risk that humanity gets disempowered by AI, and Section 3 discussed the risk that citizens get disempowered by their governments by force or coercion. But another kind of disempowerment can occur if there is such a huge concentration of wealth that a small group of people effectively controls government policy with their influence, and ordinary citizens have no influence because they lack economic leverage. Democracy is ultimately backstopped by the idea that the population as a whole is necessary for the operation of the economy. If that economic leverage goes away, then the implicit social contract of democracy may stop working. <a href="https://intelligence-curse.ai/">Others have written about this</a>, so I needn‚Äôt go into great detail about it here, but I agree with the concern, and I worry it is already starting to happen.</p><p>To be clear, I am not opposed to people making a lot of money. There‚Äôs a strong argument that it incentivizes economic growth under normal conditions. I am sympathetic to concerns about impeding innovation by killing the golden goose that generates it. But in a scenario where GDP growth is 10‚Äì20% a year and AI is rapidly taking over the economy, yet single individuals hold appreciable fractions of the GDP, innovation is <em>not </em>the thing to worry about. The thing to worry about is a level of wealth concentration that will break society.</p><p>The most famous example of extreme concentration of wealth in US history is the <a href="https://en.wikipedia.org/wiki/Gilded_Age">Gilded Age</a>, and the wealthiest industrialist of the Gilded Age was <a href="https://en.wikipedia.org/wiki/John_D._Rockefeller">John D. Rockefeller</a>. Rockefeller‚Äôs wealth amounted to ~2% of the US GDP at the time.<sup>42</sup> A similar fraction today would lead to a fortune of $600B, and the richest person in the world today (Elon Musk) already exceeds that, at <a href="https://www.reuters.com/business/autos-transportation/elon-musk-becomes-first-person-worth-700-billion-after-court-ruling-pay-package-2025-12-20/">roughly $700B</a>. So we are already at historically unprecedented levels of wealth concentration, even <em>before </em>most of the economic impact of AI. I don‚Äôt think it is too much of a stretch (if we get a ‚Äúcountry of geniuses‚Äù) to imagine AI companies, semiconductor companies, and perhaps downstream application companies generating ~$3T in revenue per year,<sup>43</sup> being valued at ~$30T, and leading to personal fortunes well into the trillions. In that world, the debates we have about tax policy today simply won‚Äôt apply as we will be in a fundamentally different situation.</p><p>Related to this, the coupling of this economic concentration of wealth with the political system already concerns me. AI datacenters already represent a substantial fraction of US economic growth,<sup>44</sup> and are thus strongly tying together the financial interests of large tech companies (which are increasingly focused on either AI or AI infrastructure) and the political interests of the government in a way that can produce perverse incentives. We already see this through the reluctance of tech companies to criticize the US government, and the government‚Äôs support for extreme anti-regulatory policies on AI.</p><h3><em>Defenses</em></h3><p>What can be done about this? First, and most obviously, companies should simply choose not to be part of it. Anthropic has always strived to be a policy actor and not a political one, and to maintain our authentic views whatever the administration. We‚Äôve spoken up in favor of <a href="https://www.nytimes.com/2025/06/05/opinion/anthropic-ceo-regulate-transparency.html">sensible AI regulation</a> and <a href="https://www.wsj.com/opinion/trump-can-keep-americas-ai-advantage-china-chips-data-eccdce91?gaa_at=eafs&amp;gaa_n=AWEtsqespyCL3hcx_9DpJWbIPX1vrtS1raPgFoBNK8ltnrjwedpX2NuvVu1K_yZ1arw%3D&amp;gaa_ts=696c6c70&amp;gaa_sig=wef9kKocpL9PU07UoiPS6kj_o_Nwy_VSufM6gltIvdjQFhb8HRLtpSzp4Z8WDG6v3leg0ODX4HOJjWblvZe2pw%3D%3D">export controls</a> that are in the public interest, even when these are at odds with government policy.<sup>45</sup> Many people have told me that we should stop doing this, that it could lead to unfavorable treatment, but in the year we‚Äôve been doing it, Anthropic‚Äôs valuation has increased by over 6x, an almost unprecedented jump at our commercial scale.</p><p>Second, the AI industry needs a healthier relationship with government‚Äîone based on substantive policy engagement rather than political alignment. Our choice to engage on policy substance rather than politics is sometimes read as a tactical error or failure to ‚Äúread the room‚Äù rather than a principled decision, and that framing concerns me. In a healthy democracy, companies should be able to advocate for good policy for its own sake. Related to this, a public backlash against AI is brewing: this could be a corrective, but it<strong>‚Äô</strong>s currently unfocused. Much of it targets issues that aren‚Äôt actually problems (like <a href="https://newsletter.semianalysis.com/p/from-tokens-to-burgers-a-water-footprint?hide_intro_popup=true">datacenter</a> <a href="https://andymasley.substack.com/p/the-ai-water-issue-is-fake">water usage</a>) and proposes solutions (like datacenter bans or poorly designed wealth taxes) that wouldn‚Äôt address the real concerns. The underlying issue that deserves attention is ensuring that AI development remains accountable to the public interest, not captured by any particular political or commercial alliance, and it seems important to focus the public discussion there.</p><p>Third, the macroeconomic interventions I described earlier in this section, as well as a resurgence of private philanthropy, can help to balance the economic scales, addressing both the job displacement and concentration of economic power problems at once. We should look to the history of our country here: even in the Gilded Age, industrialists such as <a href="https://www.sciencedirect.com/science/article/abs/pii/S096262981500027X">Rockefeller</a> and <a href="https://www.carnegie.org/about/our-history/gospelofwealth/">Carnegie</a> felt a strong obligation to society at large, a feeling that society had contributed enormously to their success and they needed to give back. That spirit seems to be increasingly missing today, and I think it is a large part of the way out of this economic dilemma. Those who are at the forefront of AI‚Äôs economic boom should be willing to give away both their wealth and their power.</p><h2>5. Black seas of infinity</h2><h3><em>Indirect effects</em></h3><p>This last section is a catchall for unknown unknowns, particularly things that could go wrong as an indirect result of positive advances in AI and the resulting acceleration of science and technology in general. Suppose we address all the risks described so far, and begin to reap the benefits of AI. We will likely get a ‚Äú<a href="https://www.darioamodei.com/essay/machines-of-loving-grace">century of scientific and economic progress compressed into a decade</a>,‚Äù and this will be hugely positive for the world, but we will then have to contend with the problems that arise from this rapid rate of progress, and those problems may come at us fast. We may also encounter other risks that occur indirectly as a consequence of AI progress and are hard to anticipate in advance.</p><p>By the nature of unknown unknowns it is impossible to make an exhaustive list, but I‚Äôll list three possible concerns as illustrative examples for what we should be watching for:</p><ul role="list"><li> <strong>Rapid advances in biology. </strong>If we do get a century of medical progress in a few years, it is possible that we will greatly increase the human lifespan, and there is a chance we also gain radical capabilities like the ability to increase human intelligence or radically modify human biology. Those would be big changes in what is possible, happening very quickly. They could be positive if responsibly done (which is my hope, as described in <em>Machines of Loving Grace</em>), but there is always a risk they go very wrong‚Äîfor example, if efforts to make humans smarter also make them more unstable or power-seeking. There is also the issue of ‚Äú<a href="https://en.wikipedia.org/wiki/Mind_uploading">uploads</a>‚Äù or ‚Äúwhole brain emulation,‚Äù digital human minds instantiated in software, which might someday help humanity transcend its physical limitations, but which also carry <a href="https://qntm.org/mmacevedo">risks I find disquieting</a>.</li><li> <strong>AI changes human life in an unhealthy way. </strong>A world with billions of intelligences that are much smarter than humans at everything is going to be a very weird world to live in. Even if AI doesn‚Äôt actively aim to attack humans (Section 1), and isn‚Äôt explicitly used for oppression or control by states (Section 3), there is a lot that could go wrong short of this, via normal business incentives and nominally consensual transactions. We see early hints of this in the concerns about AI psychosis, <a href="https://www.cbsnews.com/news/chatgpt-lawsuit-colordo-man-suicide-openai-sam-altman/">AI driving people to suicide</a>, and concerns about romantic relationships with AIs. As an example, could powerful AIs invent some new religion and convert millions of people to it? Could most people end up ‚Äúaddicted‚Äù in some way to AI interactions? Could people end up being ‚Äúpuppeted‚Äù by AI systems, where an AI essentially watches their every move and tells them exactly what to do and say at all times, leading to a ‚Äúgood‚Äù life but one that lacks freedom or any pride of accomplishment? It would not be hard to generate dozens of these scenarios if I sat down with the creator of <a href="https://en.wikipedia.org/wiki/Black_Mirror"><em>Black Mirror</em></a> and tried to brainstorm them. I think this points to the importance of things like improving <a href="https://www.anthropic.com/constitution">Claude‚Äôs Constitution</a>, over and above what is necessary for preventing the issues in Section 1. Making sure that AI models <em>really </em>have their users‚Äô long-term interests at heart, in a way thoughtful people would endorse rather than in some subtly distorted way, seems critical.</li><li> <strong>Human purpose. </strong>This is related to the previous point, but it‚Äôs not so much about specific human interactions with AI systems as it is about how human life changes in general in a world with powerful AI. Will humans be able to find purpose and meaning in such a world? I think this is a matter of attitude: as I said in <em>Machines of Loving Grace</em>, I think human purpose does not depend on being the best in the world at something, and humans can find purpose even over very long periods of time through stories and projects that they love. We simply need to break the link between the generation of economic value and self-worth and meaning. But that is a transition society has to make, and there is always the risk we don‚Äôt handle it well.</li></ul><p>My hope with all of these potential problems is that in a world with powerful AI that we trust not to kill us, that is not the tool of an oppressive government, and that is genuinely working on our behalf, we can use AI itself to anticipate and prevent these problems. But that is not guaranteed‚Äîlike all of the other risks, it is something we have to handle with care.</p><h2>Humanity‚Äôs test</h2><p>Reading this essay may give the impression that we are in a daunting situation. I certainly found it daunting to write, in contrast with <em>Machines of Loving Grace, </em>which felt like giving form and structure to surpassingly beautiful music that had been echoing in my head for years. And there is much about the situation that genuinely is hard. AI brings threats to humanity from multiple directions, and there is genuine tension between the different dangers, where mitigating some of them risks making others worse if we do not thread the needle extremely carefully.</p><p>Taking time to carefully build AI systems so they do not autonomously threaten humanity is in genuine tension with the need for democratic nations to stay ahead of authoritarian nations and not be subjugated by them. But in turn, the same AI-enabled tools that are necessary to fight autocracies can, if taken too far, be turned inward to create tyranny in our own countries. AI-driven terrorism could kill millions through the misuse of biology, but an overreaction to this risk could lead us down the road to an autocratic surveillance state. The labor and economic concentration effects of AI, in addition to being grave problems in their own right, may force us to face the other problems in an environment of public anger and perhaps even civil unrest, rather than being able to call on the better angels of our nature. Above all, the sheer <em>number </em>of risks, including unknown ones, and the need to deal with all of them at once, creates an intimidating gauntlet that humanity must run.</p><p>Furthermore, the last few years should make clear that the idea of stopping or even substantially slowing the technology is fundamentally untenable. The formula for building powerful AI systems is incredibly simple, so much so that it can almost be said to emerge spontaneously from the right combination of data and raw computation. Its creation was probably inevitable the instant humanity invented the transistor, or arguably even earlier when we first learned to control fire. If one company does not build it, others will do so nearly as fast. If all companies in democratic countries stopped or slowed development, by mutual agreement or regulatory decree, then authoritarian countries would simply keep going. Given the incredible economic and military value of the technology, together with the lack of any meaningful enforcement mechanism, I don‚Äôt see <a href="https://en.wikipedia.org/wiki/Security_dilemma">how we could possibly convince them to stop</a>.</p><p>I do see a path to a <em>slight</em> moderation in AI development that is compatible with a <a href="https://en.wikipedia.org/wiki/Realism_(international_relations)">realist view of geopolitics</a>. That path involves slowing down the march of autocracies towards powerful AI for a few years by denying them the resources they need to build it,<sup>46</sup> namely chips and semiconductor manufacturing equipment. This in turn gives democratic countries a buffer that they can ‚Äúspend‚Äù on building powerful AI more carefully, with more attention to its risks, while still proceeding fast enough to comfortably beat the autocracies. The race between AI companies within democracies can then be handled under the umbrella of a common legal framework, via a mixture of industry standards and regulation.</p><p>Anthropic has advocated very hard for this path, by pushing for chip export controls and judicious regulation of AI, but even these seemingly common-sense proposals have largely been rejected by policymakers in the United States (which is the country where it‚Äôs most important to have them). There is so much money to be made with AI‚Äîliterally trillions of dollars per year‚Äîthat even the simplest measures are finding it difficult to overcome the <a href="https://en.wikipedia.org/wiki/Political_economy">political economy</a> inherent in AI. This is the trap: AI is so powerful, such a glittering prize, that it is very difficult for human civilization to impose any restraints on it at all.</p><p>I can imagine, as Sagan did in <em>Contact</em>, that this same story plays out on thousands of worlds. A species gains sentience, learns to use tools, begins the exponential ascent of technology, faces the crises of industrialization and nuclear weapons, and if it survives those, confronts the hardest and final challenge when it learns how to shape sand into machines that think. Whether we survive that test and go on to build the beautiful society described in <em>Machines of Loving Grace</em>, or succumb to slavery and destruction, will depend on our character and our determination as a species, our spirit and our soul.</p><p>Despite the many obstacles, I believe humanity has the strength inside itself to pass this test. I am encouraged and inspired by the thousands of researchers who have devoted their careers to helping us understand and steer AI models, and to shaping the character and constitution of these models. I think there is now a good chance that those efforts bear fruit in time to matter. I am encouraged that at least some companies have <a href="https://openai.com/index/preparing-for-future-ai-capabilities-in-biology/">stated they<strong>‚Äô</strong>ll pay</a> meaningful commercial costs to block their models from contributing to the threat of bioterrorism. I am encouraged that a few brave people have resisted the prevailing political winds and <a href="https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202520260SB53">passed</a> <a href="https://www.nysenate.gov/legislation/bills/2025/A6453/amendment/A">legislation</a> that puts the first early seeds of sensible guardrails on AI systems. I am encouraged that the <a href="https://www.pewresearch.org/internet/2025/04/03/views-of-risks-opportunities-and-regulation-of-ai/#6d2b9b266433bfda6c8fc2f498738a4c">public understands that AI carries risks and wants those risks addressed</a>. I am encouraged by the indomitable spirit of freedom around the world and the determination to resist tyranny wherever it occurs.</p><p>But we will need to step up our efforts if we want to succeed. The first step is for those closest to the technology to simply tell the truth about the situation humanity is in, which I have always tried to do; I‚Äôm doing so more explicitly and with greater urgency with this essay. The next step will be convincing the world‚Äôs thinkers, policymakers, companies, and citizens of the imminence and overriding importance of this issue‚Äîthat it is worth expending thought and political capital on this in comparison to the thousands of other issues that dominate the news every day. Then there will be a time for courage, for enough people to buck the prevailing trends and stand on principle, even in the face of threats to their economic interests and personal safety. </p><p>The years in front of us will be impossibly hard, asking more of us than we think we can give. But in my time as a researcher, leader, and citizen, I have seen enough courage and nobility to believe that we can win‚Äîthat when put in the darkest circumstances, humanity has a way of gathering, seemingly at the last minute, the strength and wisdom needed to prevail. We have no time to lose.</p></div><hr><p>I would like to thank Erik Brynjolfsson, Ben Buchanan, Mariano-Florentino Cu√©llar, Allan Dafoe, Kevin Esvelt, Nick Beckstead, Richard Fontaine, Jim McClave, and very many of the staff at Anthropic for their helpful comments on drafts of this essay.</p></section><section><h2>Footnotes</h2><div data-footnotes="footnotes"><ol role="list"><li>This is symmetric to a point I made in <em>Machines of Loving Grace</em>, where I started by saying that AI‚Äôs upsides shouldn‚Äôt be thought of in terms of a prophecy of salvation, and that it‚Äôs important to be concrete and grounded and to avoid grandiosity. Ultimately, prophecies of salvation and prophecies of doom are unhelpful for confronting the real world, for basically the same reasons.</li><li>Anthropic‚Äôs goal is to remain consistent through such changes. When talking about AI risks was politically popular, Anthropic cautiously advocated for a judicious and evidence-based approach to these risks. Now that talking about AI risks is politically unpopular, Anthropic continues to cautiously advocate for a judicious and evidence-based approach to these risks.</li><li>Over time, I have gained increasing confidence in the trajectory of AI and the likelihood that it will surpass human ability across the board, but some uncertainty still remains.</li><li>Export controls for chips are a great example of this. They are simple and appear to mostly just work.</li><li>And of course, the hunt for such evidence must be intellectually honest, such that it could also turn up evidence of a lack of danger. Transparency through model cards and other disclosures is an attempt at such an intellectually honest endeavor.</li><li>Indeed, since writing<em> Machines of Loving Grace</em> in 2024, AI systems have become capable of doing tasks that take humans several hours, with METR recently assessing that Opus 4.5 can do about four human hours of work with 50% reliability.</li><li>And to be clear, even if powerful AI is only 1‚Äì2 years away in a technical sense, many of its societal consequences, both positive and negative, may take a few years longer to occur. This is why I can simultaneously think that AI will disrupt 50% of <em>entry-level </em>white-collar jobs over 1‚Äì5 years, while also thinking we may have AI that is more capable than <em>everyone </em>in only 1‚Äì2 years.</li><li>It is worth adding that the <em>public </em>(as compared to policymakers) does seem to be very concerned with AI risks. I think some of their focus is correct (i.e. AI job displacement), and some is misguided (such as concerns about water use of AI, which is not significant). This backlash gives me hope that a consensus around addressing risks is possible, but so far it has not yet been translated into policy changes, let alone effective or well-targeted policy changes.</li><li>They can also, of course, manipulate (or simply pay) large numbers of humans into doing what they want in the physical world.</li><li>I don‚Äôt think this is a straw man: it‚Äôs my understanding, for example, that <a href="https://www.youtube.com/watch?v=LMuun5FGL28">Yann LeCun holds this position</a>.</li><li>For example, see Section 5.5.2 (p. 63‚Äì66) of the <a href="https://www.anthropic.com/claude-4-system-card">Claude 4 system card</a>.</li><li>There are also a number of other assumptions inherent in the simple model, which I won‚Äôt discuss here. Broadly, they should make us less worried about the specific simple story of misaligned power-seeking, but also more worried about possible unpredictable behavior we haven‚Äôt anticipated.</li><li><a href="https://en.wikipedia.org/wiki/Ender%27s_Game"><em>Ender‚Äôs Game</em></a><em> </em>describes a version of this involving humans rather than AI.</li><li>For example, models may be told not to do various bad things, and also to obey humans, but may then observe that many humans do exactly those bad things! It‚Äôs not clear how this contradiction would resolve (and a well-designed constitution should encourage the model to handle these contradictions gracefully), but this type of dilemma is not so different from the supposedly ‚Äúartificial‚Äù situations that we put AI models in during testing.</li><li>Incidentally, one consequence of the constitution being a natural-language document is that it is legible to the world, and that means it can be critiqued by anyone and compared to similar documents by other companies. It would be valuable to create a race to the top that not only encourages companies to release these documents, but encourages them to be good.</li><li>There‚Äôs even a hypothesis about a deep unifying principle connecting the character-based approach from Constitutional AI to results from interpretability and alignment science. According to the hypothesis, the fundamental mechanisms driving Claude originally arose as ways for it to simulate characters in pretraining, such as predicting what the characters in a novel would say. This would suggest that a useful way to think about the constitution is more like a character description that the model uses to instantiate a consistent persona. It would also help us explain the ‚Äú<a href="https://www.anthropic.com/research/emergent-misalignment-reward-hacking">I must be a bad person</a>‚Äù results I mentioned above (because the model is trying to <em>act as if</em> it‚Äôs a coherent character‚Äîin this case a bad one), and would suggest that interpretability methods should be able to discover ‚Äúpsychological traits‚Äù within models. Our researchers are working on ways to test this hypothesis.</li><li>To be clear, monitoring is done in a privacy-preserving way.</li><li>Even in our own experiments with what are essentially voluntarily imposed rules with our <a href="https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy">Responsible Scaling Policy</a>, we have found over and over again that it‚Äôs very easy to end up being too rigid, by drawing lines that seem important ex ante but turn out to be silly in retrospect. It is just very easy to set rules about the wrong things when a technology is advancing rapidly.</li><li>SB 53 and RAISE do not apply at all to companies with under $500M in annual revenue. They only apply to larger, more established companies like Anthropic.</li><li>I originally read Joy‚Äôs essay 25 years ago, when it was written, and it had a profound impact on me. Then and now, I do see it as too pessimistic‚ÄîI don‚Äôt think broad ‚Äúrelinquishment‚Äù of whole areas of technology, which Joy suggests, is the answer‚Äîbut the issues it raises were surprisingly prescient, and Joy also writes with a deep sense of compassion and humanity that I admire.</li><li>We do have to worry about state actors, now and in the future, and I discuss that in the next section.</li><li>There is <a href="https://www.nber.org/digest/sep02/poverty-and-low-education-dont-cause-terrorism">evidence</a> that <a href="https://www.sas.rochester.edu/psc/clarke/214/Krueger03.pdf">many</a> terrorists are at least relatively well-educated, which might seem to contradict what I‚Äôm arguing here about a negative correlation between ability and motivation. But I think in actual fact they are compatible observations: if the ability threshold for a successful attack is high, then almost by definition those who <em>currently </em>succeed must have high ability, even if ability and motivation are negatively correlated. But in a world where the limitations on ability were removed (e.g., with future LLMs), I‚Äôd predict that a substantial population of people with the motivation to kill but lower ability would start to do so‚Äîjust as we see for crimes that don‚Äôt require much ability (like school shootings).</li><li>Aum Shinrikyo did try, however. The leader of Aum Shinrikyo, Seiichi Endo, had training in virology from Kyoto University, <a href="https://www.cnas.org/publications/reports/aum-shinrikyo-second-edition-english">and attempted to produce both anthrax and ebola</a>. However, as of 1995, even he lacked enough expertise and resources to succeed at this. The bar is now substantially lower, and LLMs could reduce it even further.</li><li>A bizarre phenomenon relating to mass murderers is that the style of murder they choose operates almost as a grotesque sort of fad. In the 1970s and 1980s, serial killers were very common, and new serial killers often copied the behavior of more established or famous serial killers. In the 1990s and 2000s, mass shootings became more common, while serial killers became less common. There is no technological change that triggered these patterns of behavior, it just appears that violent murderers were copying each others‚Äô behavior and the ‚Äúpopular‚Äù thing to copy changed.</li><li>Casual jailbreakers sometimes believe that they‚Äôve compromised these classifiers when they get the model to output one specific piece of information, such as the genome sequence of a virus. But as I explained before, the threat model we are worried about involves step-by-step, interactive advice that extends over weeks or months about specific obscure steps in the bioweapons production process, and this is what our classifiers aim to defend against. (We often describe our research as looking for ‚Äúuniversal‚Äù jailbreaks‚Äîones that don‚Äôt just work in one specific or narrow context, but broadly open up the model‚Äôs behavior.)</li><li>Though we will continue to invest in work to make our classifiers more efficient, and it may make sense for companies to share advances like these with one another.</li><li>Obviously, I do not think companies should have to disclose technical details about the specific steps in biological weapons production that they are blocking, and the transparency legislation that has been passed so far (SB 53 and RAISE) accounts for this issue.</li><li>Another related idea is ‚Äúresilience markets‚Äù where the government encourages stockpiling of PPE, respirators, and other essential equipment needed to respond to a biological attack by promising ahead of time to pay a pre-agreed price for this equipment in an emergency. This incentivizes suppliers to stockpile such equipment without fear that the government will seize it without compensation.</li><li>Why am I more worried about large actors for seizing power, but small actors for causing destruction? Because the dynamics are different. Seizing power is about whether one actor can amass enough strength to overcome everyone else‚Äîthus we should worry about the most powerful actors and/or those closest to AI. Destruction, by contrast, can be wrought by those with little power if it is much harder to defend against than to cause. It is then a game of defending against the most <em>numerous </em>threats, which are likely to be smaller actors.</li><li>This might sound like it is in tension with my point that attack and defense may be more balanced with cyberattacks than with bioweapons, but my worry here is that if a country‚Äôs AI is the most powerful in the world, then others will not be able to defend even if the technology itself has an intrinsic attack-defense balance.</li><li>For example, in the United States this includes the fourth amendment and the <a href="https://en.wikipedia.org/wiki/Posse_Comitatus_Act">Posse Comitatus Act</a>.</li><li>Also, to be clear, there are some arguments for building large datacenters in countries with varying governance structures, particularly if they are controlled by companies in democracies. Such buildouts could in principle help democracies compete better with the CCP, which is the greater threat. I also think such datacenters don‚Äôt pose much risk unless they are very large. But on balance, I think caution is warranted when placing very large datacenters in countries where institutional safeguards and rule-of-law protections are less well-established.</li><li>This is, of course, also an argument for <a href="https://councilonstrategicrisks.org/research/reports/nuclear-decision-making-and-risk-reduction-in-an-era-of-technological-complexity/">improving the security of the nuclear deterrent</a> to make it <a href="https://onlinelibrary.wiley.com/doi/10.1111/risa.70136">more likely to be robust</a> against powerful AI, and nuclear-armed democracies should do this. But we don‚Äôt know what a powerful AI will be capable of or which defenses, if any, will work against it, so we should not assume that these measures will necessarily solve the problem.</li><li>There is also the risk that even if the nuclear deterrent remains effective, an attacking country might decide to call our bluff‚Äîit‚Äôs unclear whether we‚Äôd be willing to use nuclear weapons to defend against a drone swarm even if the drone swarm has a substantial risk of conquering us. Drone swarms might be a new thing that is less severe than nuclear attacks but more severe than conventional attacks. Alternatively, differing assessments of the effectiveness of the nuclear deterrent in the age of AI might alter the game theory of nuclear conflict in a destabilizing manner.</li><li>To be clear, I would believe it is the right strategy not to sell chips to China, even if the timeline to powerful AI were substantially longer. We cannot get the Chinese ‚Äúaddicted‚Äù to American chips‚Äîthey are determined to develop their native chip industry one way or another. It will take them many years to do so, and all we are doing by selling them chips is giving them a big boost during that time.</li><li>To be clear, most of what is being used in Ukraine and Taiwan today are not <em>fully </em>autonomous weapons. These are coming, but not here today.</li><li>Our model card for <a href="https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf">Claude Opus 4.5</a>, our most recent model, shows that Opus performs better on a performance engineering interview frequently given at Anthropic than any interviewee in the history of the company.</li><li>‚ÄúWriting all of the code‚Äù and ‚Äúdoing the task of a software engineer end to end‚Äù are very different things, because software engineers do much more than just write code, including testing, dealing with environments, files, and installation, managing cloud compute deployments, iterating on products, and much more.</li><li>Computers are general in a sense, but are clearly incapable on their own of the vast majority of human cognitive abilities, even as they greatly exceed humans in a few areas (such as arithmetic). Of course, things built <em>on top </em>of computers, such as AI, are now capable of a wide range of cognitive abilities, which is what this essay is about.</li><li>To be clear, AI models do not have precisely the same profile of strengths and weaknesses as humans. But they are also advancing fairly uniformly along every dimension, such that having a spiky or uneven profile may not ultimately matter.</li><li>Though there is <a href="https://davidcard.berkeley.edu/papers/skill-tech-change.pdf">debate</a> <a href="https://jhr.uwpress.org/content/58/6/1783.abstract">among</a> <a href="https://www.epi.org/publication/technology-inequality-dont-blame-the-robots/">economists</a> about this idea.</li><li>Personal wealth is a ‚Äústock,‚Äù while GDP is a ‚Äúflow,‚Äù so this isn‚Äôt a claim that Rockefeller owned 2% of the economic value in the United States. But it‚Äôs harder to measure the total wealth of a nation than the GDP, and people‚Äôs individual incomes vary a lot per year, so it‚Äôs hard to make a ratio in the same units. The ratio of the largest personal fortune to GDP, while not comparing apples to apples, is nevertheless a perfectly reasonable benchmark for extreme wealth concentration.</li><li>The total value of labor across the economy is $60T/year, so $3T/year would correspond to 5% of this. That amount could be earned by a company that supplied labor for 20% of the cost of humans and had 25% market share, even if the demand for labor did not expand (which it almost certainly would due to the lower cost).</li><li>To be clear, I do not think actual AI productivity is yet responsible for a substantial fraction of US economic growth. Rather, I think the datacenter spending represents growth caused by anticipatory investment that amounts to the market expecting<em> future</em> AI-driven economic growth and investing accordingly.</li><li>When we agree with the administration, we say so, and we look for <a href="https://www.anthropic.com/news/statement-dario-amodei-american-ai-leadership">points of agreement where mutually supported policies</a> are genuinely good for the world. We are aiming to be honest brokers rather than backers or opponents of any given political party.</li><li>I don‚Äôt think anything more than a few years is possible: on longer timescales, they will build their own chips.</li></ol></div></section></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DHS keeps trying and failing to unmask anonymous ICE critics online (158 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2026/01/instagram-ice-critic-wins-fight-to-stay-anonymous-as-dhs-backs-down/</link>
            <guid>46768081</guid>
            <pubDate>Mon, 26 Jan 2026 16:58:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2026/01/instagram-ice-critic-wins-fight-to-stay-anonymous-as-dhs-backs-down/">https://arstechnica.com/tech-policy/2026/01/instagram-ice-critic-wins-fight-to-stay-anonymous-as-dhs-backs-down/</a>, See on <a href="https://news.ycombinator.com/item?id=46768081">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                      
                      
          <p>The Department of Homeland Security (DHS) has backed down from a <a href="https://arstechnica.com/tech-policy/2026/01/instagram-user-fights-dhs-for-the-right-to-post-ice-sightings-anonymously/">fight to unmask the owners of Instagram and Facebook accounts</a> monitoring Immigration and Customs Enforcement (ICE) activity in Pennsylvania.</p>
<p>One of the anonymous account holders, John Doe, sued to block ICE from identifying him and other critics online through summonses to Meta that he claimed infringed on core First Amendment-protected activity.</p>
<p>DHS initially fought Doe‚Äôs motion to quash the summonses, arguing that the community watch groups endangered ICE agents by posting ‚Äúpictures and videos of agents‚Äô faces, license plates, and weapons, among other things.‚Äù This was akin to ‚Äúthreatening ICE agents to impede the performance of their duties,‚Äù DHS alleged. DHS‚Äôs arguments echoed DHS Secretary Kristi Noem, who has claimed that identifying ICE agents is a crime, even though <a href="https://www.wired.com/story/ice-agents-are-doxing-themselves/">Wired noted</a> that ICE employees often post easily discoverable LinkedIn profiles.</p>
<p>To Doe, the agency seemed intent on testing the waters to see if it could seize authority to unmask all critics online by invoking a customs statute that allows agents to subpoena information on goods entering or leaving the US.</p>
<p>But then, on January 16, DHS abruptly reversed course, withdrawing its summonses from Meta.</p>
<p>A <a href="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Doe-v-DHA-Notice-to-Withdraw-Summonses-1-21-26.pdf">court filing </a>confirmed that DHS dropped its requests for subscriber information last week, after initially demanding Doe‚Äôs ‚Äúpostal code, country, all email address(es) on file, date of account creation, registered telephone numbers, IP address at account signup, and logs showing IP address and date stamps for account accesses.‚Äù</p>
<p>The filing does not explain why DHS decided to withdraw its requests.</p>
<p>However, previously, DHS requested similar information from Meta about six Instagram community watch groups that shared information about ICE activity in Los Angeles and other locations. DHS withdrew those requests, too, after account holders defended their First Amendment rights and filed motions to quash their summonses, Doe‚Äôs court filing said.</p>

          
                      
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[France Aiming to Replace Zoom, Google Meet, Microsoft Teams, etc. (487 pts)]]></title>
            <link>https://twitter.com/lellouchenico/status/2015775970330882319</link>
            <guid>46767668</guid>
            <pubDate>Mon, 26 Jan 2026 16:27:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/lellouchenico/status/2015775970330882319">https://twitter.com/lellouchenico/status/2015775970330882319</a>, See on <a href="https://news.ycombinator.com/item?id=46767668">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ScriptLoadFailure"><form action="" method="GET"><div><p><span>Something went wrong, but don‚Äôt fret ‚Äî let‚Äôs give it another shot.</span></p><p><img alt="‚ö†Ô∏è" draggable="false" src="https://abs-0.twimg.com/emoji/v2/svg/26a0.svg"><span> Some privacy related extensions may cause issues on x.com. Please disable them and try again.</span></p></div></form></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RIP Low-Code 2014-2025 (111 pts)]]></title>
            <link>https://www.zackliscio.com/posts/rip-low-code-2014-2025/</link>
            <guid>46767440</guid>
            <pubDate>Mon, 26 Jan 2026 16:11:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.zackliscio.com/posts/rip-low-code-2014-2025/">https://www.zackliscio.com/posts/rip-low-code-2014-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=46767440">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-gysqo7gh=""> <p>The rise of AI and particularly agentic development presents an existential threat to an entire category of low-code platforms. While the adoption of new techniques and tooling will take years to propagate through the Byzantine ranks of larger, slower-moving enterprises, the fundamental ROI case for these tools looks different in a world where the cost of shipping code now approaches zero.</p>
<p>This may seem like a preposterous conclusion given the substantial size and growth of the sector. Forrester, who actually gave low-code its name back in 2014, <a href="https://www.forrester.com/blogs/the-low-code-market-could-approach-50-billion-by-2028/">projects the category will reach $50b by 2028</a> and sees no current indication that things will slow down, let alone contract. However, it‚Äôs worth digging in to why these tools arose in the first place and the problems they solve to explore how much the landscape has shifted in just the past year.</p>
<p><img alt="Forrester Low-Code Market Projection" loading="lazy" decoding="async" fetchpriority="auto" width="684" height="786" src="https://www.zackliscio.com/_astro/forrester.opE9g3oO_6ECtA.webp"></p>
<p>Put simply, these software platforms exist to allow users to create software with fewer developer resources. By purchasing one of these platforms, a company can enable non-technical stakeholders to ship production-ready experiences, often with little to zero actual code being written. This frees up developer bandwidth, accelerates the company, and until recently was a no-brainer investment for building internal and even customer-facing software.</p>
<p><img alt="The Low Code Value Prop" loading="lazy" decoding="async" fetchpriority="auto" width="1024" height="559" src="https://www.zackliscio.com/_astro/value-prop.Bs7qCFDc_iEAxj.webp"></p>
<p>To enable these platforms in the real world, developers spend considerable time on prerequisite and ongoing work: piping and transforming data, writing and maintaining custom components that go beyond out-of-the-box functionality, and meshing authentication systems, to name a few. This investment is in turn justified by the reduction in development scope and complexity downstream of the low-code platform‚Äînon-technical users can be left to their own devices to ship to their hearts‚Äô content.</p>
<p>With the emergence of AI coding, this ROI case gets inverted. It is now often faster, cheaper, and easier to ship the kind of tools you might have built with low-code tools outside these platforms. Yes, this still requires developer time, but so did enabling these low-code-platforms in the first place.  Even disregarding the financial and organizational costs of low-code tools, AI affords developers the conveniences of their regular workflows without the bolt-on complexity introduced by external platforms. When you add in the total cost of ownership of these low-code tools, a return to in-house tooling becomes even more attractive.</p>
<p>As an illustration of what‚Äôs possible, we‚Äôve seen this play out in real-time at <a href="https://www.cloudcapital.co/">Cloud Capital</a>. In the not-too-distant past, we relied heavily on a low-code platform called <a href="https://retool.com/">Retool</a> for almost all of our internal Admin tooling. We built management dashboards, reporting, and orchestrated complex workflows that were critical to the business. The acceleration was real‚Äîour developers spent significantly less time rolling boilerplate tables, transforming data, and wiring up workflows. We even celebrated at our All Hands how much <em>better</em> our dashboards felt than if we‚Äôd hand-rolled them or used a pre-canned admin interface.</p>
<p>Then came the agentic tools that completely transformed the way we develop software. For our low-code tooling, the shift began with a single choice to prototype some new, self-contained functionality as a standalone internal tool instead of via our low-code platform. It was faster, easier, and leveraged our actual codebase in ways an external solution could not. That meant we shipped something safer, more robust, and more maintainable. The cherry on top is that the end product was also better‚Äîthe UI looked and felt more like our in-house products, without the clunkiness required to stay on the WYSIWYG rails.</p>
<p>It felt immediately clear that we had identified an unlock for our internal tooling velocity. All of a sudden, we found ourselves feeling constrained by the same low-code tools that until so recently were unblocking us. Changes that would have been one-liners in Cursor or automated triage tickets handled by an agent meant logging in to another platform, moving around clunky UX blocks, bashing against version management systems that weren‚Äôt quite as polished or integrated as our core development flows. All with the additional cost of maintaining this additional system.</p>
<p>What started as a single tool quickly became a wholesale migration of all of our Admin tooling, and the inevitable sunsetting of our Retool instance. They hadn‚Äôt changed, but our culture and way of working had, and low-code couldn‚Äôt keep up. What was most shocking was the timeline of this change‚Äîfor us as a small, fast-moving startup, the transition including migration played out fully in just a couple of sprints.</p>
<p>It‚Äôs inevitable that incumbent low-code tools will adapt‚Äîthey will need to in order to survive. In many cases, this shift is already visible in their marketing, such as Retool‚Äôs new AI-heavy positioning:</p>
<p><img alt="Retool&amp;#x27;s new AI-heavy positioning" loading="lazy" decoding="async" fetchpriority="auto" width="3404" height="1808" src="https://www.zackliscio.com/_astro/retool-positioning.DetKXgRI_Z1PcLTl.webp"></p>
<p>At this point, it‚Äôs hard to say whether it‚Äôll be enough. While it‚Äôs possible low-code platforms will survive by providing non-technical users with the kind of magical experience that‚Äôs already possible for developers with AI coding tools today, it also seems likely they will continue to cede market share to the core AI players themselves. We‚Äôre beginning to see this take shape as non-technical AI artifacts become more complex, powerful, and collaborative.</p>
<p>For us, abandoning low-code to reclaim ownership of our internal tooling was a simple build vs buy decision with meaningful cost savings and velocity gains. It also feels like a massive upgrade in developer experience and end-user quality of life. It‚Äôs been about 6 months since we made this switch, and so far we haven‚Äôt looked back.</p>
<p>Every build vs buy decision is unique, but many ultimately boil down to ROI in terms of speed, financial cost, maintenance cost, and organizational complexity. There are of course additional considerations like vendor lock-in, ownership of core competencies, ecosystem compatibility, etc., but in this case we can reduce the decision to this: will buying this platform let my team go faster, ship more, and create more value for our customers. At least for now, that answer feels clearer every day.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[There is an AI code review bubble (143 pts)]]></title>
            <link>https://www.greptile.com/blog/ai-code-review-bubble</link>
            <guid>46766961</guid>
            <pubDate>Mon, 26 Jan 2026 15:38:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.greptile.com/blog/ai-code-review-bubble">https://www.greptile.com/blog/ai-code-review-bubble</a>, See on <a href="https://news.ycombinator.com/item?id=46766961">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><p>Today, we're in the <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=iBpgqRbEFVg">hard seltzer</a> era of AI code review: everybody's doing them. OpenAI, Anthropic, Cursor, Augment, now Cognition, and even Linear. Of course, there's also the "White Claws" of code review: pure-play code review agents like Greptile (that's us!), CodeRabbit, Macroscope, and a litter of fledgling YC startups. Then there are the adjacent Budweisers of this world:</p>
<p><img src="https://www.greptile.com/blog/ai-code-review-bubble/image1.png" alt="AI code review announcement 1"><img src="https://www.greptile.com/blog/ai-code-review-bubble/image2.png" alt="AI code review announcement 2"></p>
<p>Amazingly, these two were announced practically within 24 hours of each other.</p>
<p><img src="https://www.greptile.com/blog/ai-code-review-bubble/image3.png" alt="AI code review timeline"></p>
<p>As the proprietors of an, er, AI code review tool suddenly beset by an avalanche of competition, we're asking ourselves: what makes us different?</p>
<p><strong>How does one differentiate?</strong></p>
<p>Based on our benchmarks, we are uniquely good at catching bugs. However, if all company blogs are to be trusted, this is something we have in common with every other AI code review product. One just has to try a few, and pick the one that feels the best.</p>
<p>Unfortunately, code review performance is ephemeral and subjective, and is ultimately not an interesting way to discern the agents before trying them. It's useless for me to try to convince you that we're the best. You should just try us and make up your own mind.</p>
<p>Instead of telling you how our <em>product</em> is differentiated, I am going to tell you how our <em>viewpoint</em> is differentiated - how we think code review will look in the long-term, and what we're doing today to prepare our customers for that future.</p>
<p>Our thesis can be distilled into three pillars: independence, autonomy, and feedback loops.</p>

<p>We <em>strongly</em> believe that the review agent should be different from the coding agent. We are opinionated on the importance of <em>independent</em> code validation agents. In spite of multiple requests, we have never shipped codegen features. We don't write code; an auditor doesn't prepare the books, a fox doesn't guard the henhouse, and a student doesn't grade their own essays.</p>
<p>Today's agents are better than the median human code reviewer at catching issues and enforcing standards, and they're only getting better. It's clear that in the future a large percentage of code at companies will be auto-approved by the code review agent. In other words, there will be some instances where a human writes a ticket, an agent writes the PR, and another agent validates, approves, and merges it.</p>
<p>This might seem far-fetched but the counterfactual is Kafkaesque. A human rubber-stamping code being validated by a super intelligent machine is the equivalent of a human sitting silently in the driver's seat of a self-driving car, "supervising".</p>
<p>If agents are approving code, it would be quite absurd and perhaps non-compliant to have the agent that <em>wrote</em> the code also <em>approve</em> the code. Only once would you have X write a PR, then have X approve and merge it to realize the absurdity of what you just did.</p>

<p>Something that Greptiles generally agree on is that everything that <em>can</em> be automated, <em>will</em> be automated.</p>
<p>Code validation - which to us is the combination of review, test, and QA, is an excellent candidate for full automation. It's work that humans don't want to do, and aren't particularly good at. It also requires little in the way of creative expression, unlike programming. In addition, success is generally pretty well-defined. Everyone wants correct, performant, bug-free, secure code.</p>
<p>While some other products have built out great UIs for humans to review code in an AI-assisted paradigm, we have chosen to build for what we consider to be an inevitable future - one where code validation requires vanishingly little human participation. We have no code review UI, and view ourselves as more of a background automation or "pipes" product. Human engineers should be focused only on two things - coming up with brilliant ideas for what should exist, and expressing their vision and taste to agents that do the cruft of turning it all into clean, performant code.</p>

<p>Not long ago, we released our Claude Code plugin. It can do many things - but most notably, you can ask Claude Code to pull down and address Greptile's comments from the PR. You can ask it to keep going until there are no new comments, waiting a few minutes for a review after each push.</p>
<p>This is a step towards the future we're excited about: Human expresses intent, coding agent executes, validation/review agent finds issues and hands them back - kicking off a loop until it approves and merges. If there is ambiguity at any point, the agents Slack the human to clarify.</p>
<p><img src="https://www.greptile.com/blog/ai-code-review-bubble/image4.png" alt="AI code review feedback loop diagram"></p>
<p>The question of how these things are different is important. Unlike picking IDEs and coding agents that ostensibly have low switching costs, code review products are harder to rip out, so your decision will very likely turn out to be a long-term one, especially if you're a large company.</p>
<p>We've been around for about as long as AI code review has been around. It has gone from a fringe interest of the world's most adventurous vibecoders to a mainstream product that our enterprise users (including two of the Mag7) often describe as a "no-brainer" purchase.</p>
<p>Yet, our guess on where this goes is about as good as anyone else's. Meanwhile, we'll keep doing what we've always done - trying to make things our users love.</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Qwen3-Max-Thinking (406 pts)]]></title>
            <link>https://qwen.ai/blog?id=qwen3-max-thinking</link>
            <guid>46766741</guid>
            <pubDate>Mon, 26 Jan 2026 15:23:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qwen.ai/blog?id=qwen3-max-thinking">https://qwen.ai/blog?id=qwen3-max-thinking</a>, See on <a href="https://news.ycombinator.com/item?id=46766741">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Windows 11's Patch Tuesday nightmare gets worse (124 pts)]]></title>
            <link>https://www.windowscentral.com/microsoft/windows-11/windows-11s-botched-patch-tuesday-update-nightmare-continues-as-microsoft-confirms-some-pcs-might-fail-to-boot</link>
            <guid>46766526</guid>
            <pubDate>Mon, 26 Jan 2026 15:07:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.windowscentral.com/microsoft/windows-11/windows-11s-botched-patch-tuesday-update-nightmare-continues-as-microsoft-confirms-some-pcs-might-fail-to-boot">https://www.windowscentral.com/microsoft/windows-11/windows-11s-botched-patch-tuesday-update-nightmare-continues-as-microsoft-confirms-some-pcs-might-fail-to-boot</a>, See on <a href="https://news.ycombinator.com/item?id=46766526">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/SDGK8gvPur5PakBfWUhKYY-1920-80.png.webp 1920w, https://cdn.mos.cms.futurecdn.net/SDGK8gvPur5PakBfWUhKYY-1200-80.png.webp 1200w, https://cdn.mos.cms.futurecdn.net/SDGK8gvPur5PakBfWUhKYY-1024-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/SDGK8gvPur5PakBfWUhKYY-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/SDGK8gvPur5PakBfWUhKYY-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/SDGK8gvPur5PakBfWUhKYY-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/SDGK8gvPur5PakBfWUhKYY-320-80.png.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/SDGK8gvPur5PakBfWUhKYY.png" alt="Windows 11 BSOD error on a laptop" srcset="https://cdn.mos.cms.futurecdn.net/SDGK8gvPur5PakBfWUhKYY-1920-80.png 1920w, https://cdn.mos.cms.futurecdn.net/SDGK8gvPur5PakBfWUhKYY-1200-80.png 1200w, https://cdn.mos.cms.futurecdn.net/SDGK8gvPur5PakBfWUhKYY-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/SDGK8gvPur5PakBfWUhKYY-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/SDGK8gvPur5PakBfWUhKYY-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/SDGK8gvPur5PakBfWUhKYY-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/SDGK8gvPur5PakBfWUhKYY-320-80.png 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/SDGK8gvPur5PakBfWUhKYY.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/SDGK8gvPur5PakBfWUhKYY.png" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>(Image credit: Windows Central | Edited with Gemini)</span>
</figcaption>
</div>
<div id="article-body">

<div id="85fb65b9-571e-4a31-87f9-864aa6010e9d">

<p><strong>Updated 3 PM ET on January 25, 2026:</strong> <em>For those experiencing issues with boot failures, we had to put together </em><a href="https://www.windowscentral.com/microsoft/windows-11/how-to-fix-boot-issues-after-installing-the-january-2026-update-for-windows-11" data-before-rewrite-localise="https://www.windowscentral.com/microsoft/windows-11/how-to-fix-boot-issues-after-installing-the-january-2026-update-for-windows-11"><em><strong>a whole new guide on how to fix it</strong></em></a><em>. While we wanted this out earlier, it takes a few hours to research and write it all up to make sure it's accurate versus just getting the news out, so we appreicate you patience, especially since it's a Sunday and most of our staff are off. ‚Äî </em>Daniel Rubino, Editor-in-Chief</p>
</div><p id="c4a467d4-5b48-4905-9bfe-b3c4c466bb55">Microsoft has confirmed that some users might find their PC unable to boot after installing the January 2026 security update released on January 13. This is on top of the <a data-analytics-id="inline-link" href="https://www.windowscentral.com/microsoft/windows-11/windows-11-second-emergency-out-of-band-update-kb5078127-released-address-outlook-bugs" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/microsoft/windows-11/windows-11-second-emergency-out-of-band-update-kb5078127-released-address-outlook-bugs">plethora of other issues that have been reported</a> since Microsoft's disastrous Patch Tuesday updates arrived.</p><p id="c4a467d4-5b48-4905-9bfe-b3c4c466bb55-2"><em>"Microsoft has received a limited number of reports of an issue in which devices are failing to boot with stop‚ÄØcode ‚ÄúUNMOUNTABLE_BOOT_VOLUME‚Äù, after installing the January 2026 Windows security update released January 13, 2026, and later updates,"</em> the company has confirmed in an online bulletin (<em>via </em><a data-analytics-id="inline-link" href="https://www.askwoody.com/forums/topic/reports-of-boot-failures-with-the-january-2026-security-update-and-later-updates/" data-url="https://www.askwoody.com/forums/topic/reports-of-boot-failures-with-the-january-2026-security-update-and-later-updates/" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link"><em>AskWoody</em></a><em>.</em>) <em>"Affected devices show a black screen with the message ‚ÄúYour device ran into a problem and needs a restart. You can restart.‚Äù At this stage, the device cannot complete startup and requires manual recovery steps."</em></p><p>Microsoft says this issue is likely to impact users running Windows 11 version 24H2 and 25H2 on physical machines, and that it is exploring potential fixes and workarounds. In the meantime, if you do encounter this problem, you will need to manually recover your PC by entering the Windows Recovery Environment and uninstalling the latest January 2026 security patch.</p><figure data-bordeaux-image-check="" id="888e1bca-ef59-44ee-920d-93915fc8fbff"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/Z3w6yTcyzsT82cQQnKe8oY-1024-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/Z3w6yTcyzsT82cQQnKe8oY-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/Z3w6yTcyzsT82cQQnKe8oY-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/Z3w6yTcyzsT82cQQnKe8oY-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/Z3w6yTcyzsT82cQQnKe8oY-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/Z3w6yTcyzsT82cQQnKe8oY-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/Z3w6yTcyzsT82cQQnKe8oY.jpg" alt="WinRE Uninstall quality update button" srcset="https://cdn.mos.cms.futurecdn.net/Z3w6yTcyzsT82cQQnKe8oY-1024-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/Z3w6yTcyzsT82cQQnKe8oY-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/Z3w6yTcyzsT82cQQnKe8oY-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/Z3w6yTcyzsT82cQQnKe8oY-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/Z3w6yTcyzsT82cQQnKe8oY-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/Z3w6yTcyzsT82cQQnKe8oY-320-80.jpg 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/Z3w6yTcyzsT82cQQnKe8oY.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/Z3w6yTcyzsT82cQQnKe8oY.jpg">
</picture></p></div><figcaption itemprop="caption description"><span>Some users might be forced to manually recover their PC by uninstalling the latest update. </span><span itemprop="copyrightHolder">(Image credit: Mauro Huculak)</span></figcaption></figure><p id="6d9f0431-3d62-437a-a2b2-e1d85ea144e9">It's unclear how common this issue is, as most users have not reported their PC unable to boot. The company says it has received a limited number of reports, but has not provided an explanation as to what is causing the unbootable state, or whether it can be avoided.</p><p>This is the latest in a long line of issues that were introduced with this month's Patch Tuesday updates. First, users reported that PCs running version 23H2 were unable to shutdown or hibernate, and PCs running version 24H2 and 25H2 were unable to sign-in when using Remote Desktop.</p><p>A few days later, reports came in confirming an issue that rendered cloud-backed apps like Outlook, Dropbox, and OneDrive inoperable, forcing Microsoft to issue two emergency updates to address these showstopping bugs. Now, with reports that some PCs are unable to boot, it's likely the company will need to issue a third out of band update to fix this problem too.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-TQqwggnJyZUAryiGrB4RTL"><section><p>All the latest news, reviews, and guides for Windows and Xbox diehards.</p></section></div><p>It's unclear why January's security update for Windows 11 has been so disastrous. Whatever the reason, Microsoft needs to step back and reevaluate how it developers Windows, as the current quality bar might be at the lowest it's ever been.</p><p><em>via </em><a data-analytics-id="inline-link" href="https://www.neowin.net/news/windows-11-patch-tuesday-horrors-continue-as-some-pcs-now-cannot-boot/" data-url="https://www.neowin.net/news/windows-11-patch-tuesday-horrors-continue-as-some-pcs-now-cannot-boot/" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link"><em>Neowin</em></a></p><hr id="6db7f46f-3aaa-481a-9443-8547bb439a99"><a href="https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE" id="48dc5531-7f6a-439c-91eb-2a3bc7d14ca5" data-url="https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><figure data-bordeaux-image-check=""><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/joRoLarkLTtq6mfMkUhh2b-662-80.png.webp 1200w, https://cdn.mos.cms.futurecdn.net/joRoLarkLTtq6mfMkUhh2b-662-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/joRoLarkLTtq6mfMkUhh2b-662-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/joRoLarkLTtq6mfMkUhh2b-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/joRoLarkLTtq6mfMkUhh2b-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/joRoLarkLTtq6mfMkUhh2b-320-80.png.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/joRoLarkLTtq6mfMkUhh2b.png" alt="Follow us on Google Discover." srcset="https://cdn.mos.cms.futurecdn.net/joRoLarkLTtq6mfMkUhh2b-662-80.png 1200w, https://cdn.mos.cms.futurecdn.net/joRoLarkLTtq6mfMkUhh2b-662-80.png 1024w, https://cdn.mos.cms.futurecdn.net/joRoLarkLTtq6mfMkUhh2b-662-80.png 970w, https://cdn.mos.cms.futurecdn.net/joRoLarkLTtq6mfMkUhh2b-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/joRoLarkLTtq6mfMkUhh2b-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/joRoLarkLTtq6mfMkUhh2b-320-80.png 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/joRoLarkLTtq6mfMkUhh2b.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/joRoLarkLTtq6mfMkUhh2b.png">
</picture></p></div></figure></a><p id="bfa461de-7d85-4568-ace4-57eaceecf4ec">Follow <a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE?hl=en-US&amp;gl=US&amp;ceid=US:en" data-url="https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE?hl=en-US&amp;gl=US&amp;ceid=US:en" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link">Windows Central on Google News</a> to keep our latest news, insights, and features at the top of your feeds!</p><hr id="d2c9ee86-615b-4c4b-a11c-f1ec9203d759">
</div>


<div data-hydrate="true" id="slice-container-authorBio-TQqwggnJyZUAryiGrB4RTL"><p>Zac Bowden is a Senior Editor at Windows Central and has been with the site since 2016. Bringing you exclusive coverage into the world of Windows, Surface, and hardware. He's also an avid collector of rare Microsoft prototype devices! Keep in touch on <a href="https://twitter.com/zacbowden">Twitter</a> and <a href="https://threads.net/@zacbowden">Threads</a></p></div>
</section>

<div x-show="$store.Viafoura.showWidgets" x-cloak="" data-component-name="Viafoura:Comments" x-data="ViafouraComments('300px')" data-nosnippet="" data-community-guidelines-text="<p class='vfcustom-community-guidelines'>Please follow our <a href=&quot;https://www.windowscentral.com/about#section-community-guidelines&quot; target=&quot;_blank&quot;>community guidelines</a>.</p>">
<p>You must confirm your public display name before commenting</p>
<p>Please logout and then login again, you will then be prompted to enter your display name.</p>
</div>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OSS ChatGPT WebUI ‚Äì 530 Models, MCP, Tools, Gemini RAG, Image/Audio Gen (116 pts)]]></title>
            <link>https://llmspy.org/docs/v3</link>
            <guid>46766432</guid>
            <pubDate>Mon, 26 Jan 2026 15:01:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://llmspy.org/docs/v3">https://llmspy.org/docs/v3</a>, See on <a href="https://news.ycombinator.com/item?id=46766432">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Major release focused on extensibility, expanded provider support, and enhanced user experience.</p><div>
<div><table><thead><tr><th>Feature</th><th>Description</th></tr></thead><tbody><tr><td><strong>530+ Models</strong></td><td>Access over 530 models from 24 providers via <a href="https://models.dev/" rel="noreferrer noopener" target="_blank">models.dev</a> integration</td></tr><tr><td><strong>Model Selector</strong></td><td>Redesigned full-featured dialog with search, filtering, sorting, and favorites</td></tr><tr><td><strong>Extensions</strong></td><td>Add features, providers, and customize the UI with a flexible plugin architecture</td></tr><tr><td><strong>Gemini RAG</strong></td><td>Manage Gemini File Search Stores and manage document uploads for RAG workflows</td></tr><tr><td><strong>Tool Support</strong></td><td>First-class Python function calling for LLM interactions with your local environment</td></tr><tr><td><strong>MCP Support</strong></td><td>Connect to Model Context Protocol servers for extended tool capabilities</td></tr><tr><td><strong>Computer Use</strong></td><td>Desktop automation - control mouse, keyboard, and take screenshots like a human</td></tr><tr><td><strong>KaTeX Math Typesetting</strong></td><td>Support for beautiful rendering of LaTeX math expressions</td></tr><tr><td><strong>Calculator UI</strong></td><td>Beautiful UX Friendly UI to evaluate python math expressions</td></tr><tr><td><strong>Run Code UI</strong></td><td>Execute Python, JS, TypeScript and C# code scripts in a CodeMirror editor</td></tr><tr><td><strong>Image Generation</strong></td><td>Built-in support for Google, OpenAI, OpenRouter, Chutes, and Nvidia</td></tr><tr><td><strong>Audio Generation</strong></td><td>TTS support for Gemini 2.5 Flash/Pro Preview models</td></tr><tr><td><strong>Media Gallery</strong></td><td>Beautiful UI to browse generated images and audio generations</td></tr><tr><td><strong>SQLite Storage</strong></td><td>Migrated IndexedDB to server SQLite for robust persistence and concurrent usage</td></tr><tr><td><strong>Asset Caching</strong></td><td>Persistent image/file file caching with metadata</td></tr><tr><td><strong>Gemini RAG Extension</strong></td><td>Manage Gemini File Search Stores for RAG workflows with document uploads and sync</td></tr></tbody></table></div>
<hr>
<h2 id="table-of-contents"><a data-card="" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="#new-model-selector-ui">New Model Selector UI</a></li>
<li><a href="#rewritten-for-extensibility">Rewritten for Extensibility</a></li>
<li><a href="#extensions-system">Extensions System</a></li>
<li><a href="#gemini-rag-extension">Gemini RAG Extension</a></li>
<li><a href="#tool-support">Tool Support</a></li>
<li><a href="#mcp-support">MCP Support</a></li>
<li><a href="#core-tools">Core Tools</a></li>
<li><a href="#computer-use">Computer Use</a></li>
<li><a href="#calculator-ui">Calculator UI</a></li>
<li><a href="#run-code-ui">Run Code UI</a></li>
<li><a href="#katex-math-typesetting">KaTeX Math Typesetting</a></li>
<li><a href="#image-generation-support">Image Generation Support</a></li>
<li><a href="#audio-generation-support">Audio Generation Support</a></li>
<li><a href="#media-gallery">Media Gallery</a></li>
<li><a href="#system-prompts-library">System Prompts Library</a></li>
<li><a href="#server-sqlite-and-cached-file-storage-persistence">Server-Side SQLite Storage</a></li>
<li><a href="#image-cache--optimization">Image Cache &amp; Optimization</a></li>
<li><a href="#cli---more-powerful-than-ever">CLI - More Powerful Than Ever</a></li>
<li><a href="#upgrade-instructions">Upgrade Instructions</a></li>
</ul>
<hr>

<p>Get instant access to 530+ models from 24 providers with extensibility at its core:</p>

<h3 id="upgrade"><a data-card="" href="#upgrade">Upgrade</a></h3>

<p>See <a href="https://llmspy.org/docs/getting-started/installation">Install Docs</a> for running from Docker or source.</p>
<hr>

<p>A major change to significantly increase the available models is the switch to utilizing the same <a href="https://models.dev/" rel="noreferrer noopener" target="_blank">models.dev</a> open provider and model catalogue as used and maintained by <a href="https://opencode.ai/" rel="noreferrer noopener" target="_blank">OpenCode</a>.</p>
<p><strong>llms.json</strong> provider configuration is now a <strong>superset</strong> of <a href="https://models.dev/api.json" rel="noreferrer noopener" target="_blank">models.dev/api.json</a> where its definitions are merged, allowing you to enable providers using just <code>"enabled": true</code> to inherit the configuration from <strong>models.dev</strong></p>
<h3 id="-expanded-provider-support"><a data-card="" href="#-expanded-provider-support">üåê Expanded Provider Support</a></h3>
<p>The switch to <a href="https://models.dev/" rel="noreferrer noopener" target="_blank">models.dev</a> greatly expands the model selection to over <strong>530 models</strong> from <strong>24 different providers</strong>, including new support for:</p>
<div><table><thead><tr><th>Provider</th><th>Models</th><th>Provider</th><th>Models</th></tr></thead><tbody><tr><td>Alibaba</td><td>39</td><td>Hugging Face</td><td>14</td></tr><tr><td>Chutes</td><td>56</td><td>Zai Coding Plan</td><td>6</td></tr><tr><td>DeepSeek</td><td>2</td><td>MiniMax</td><td>1</td></tr><tr><td>Fireworks AI</td><td>12</td><td>Moonshot AI</td><td>5</td></tr><tr><td>GitHub Copilot</td><td>27</td><td>Nvidia</td><td>24</td></tr><tr><td>GitHub Models</td><td>55</td><td>Zai</td><td>6</td></tr><tr><td>Cerebras</td><td>3</td><td></td><td></td></tr><tr><td>LMStudio</td><td>local</td><td>Ollama</td><td>local</td></tr></tbody></table></div>
<p>Non OpenAI Compatible LLM and Image generation providers are maintained in the <a href="https://github.com/ServiceStack/llms/tree/main/llms/extensions/providers" rel="noreferrer noopener" target="_blank">providers</a> extension, registered using the <code>ctx.add_provider()</code> API. There are several different provider implementations to take advantage of features available in each provider, such as <strong>Interleaved Thinking</strong> support in Anthropic's Messages API which enables all Claude and MiniMax models to reason between tool calls for improved agentic performance.</p>

<h3 id="-automatic-provider-updates"><a data-card="" href="#-automatic-provider-updates">üîÑ Automatic Provider Updates</a></h3>
<p>This actively maintained list of available providers and models are automatically updated into your <code>providers.json</code> daily that can also be manually updated with:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>llms</span><span> --update-providers</span></span></code></pre></div></figure>
<p>As an optimization only the providers that are referenced in your <code>llms.json</code> are saved. Any additional providers you want to use that are not included in models.dev can be added to your <code>~/.llms/providers-extra.json</code>, which get merged into your <code>providers.json</code> on every update.</p>
<p>This keeps your local configuration file lightweight by only including the providers that are available for use.</p>
<h3 id="configuration-examples"><a data-card="" href="#configuration-examples">Configuration Examples</a></h3>
<p>Enable providers by ID ‚Äî all configuration is automatically inherited:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>{</span></span>
<span><span>  "openai"</span><span>: { </span><span>"enabled"</span><span>: </span><span>true</span><span> },</span></span>
<span><span>  "xai"</span><span>: { </span><span>"enabled"</span><span>: </span><span>true</span><span> }</span></span>
<span><span>}</span></span></code></pre></div></figure>
<p>See <a href="https://llmspy.org/docs/configuration">Configuration</a> docs for more info.</p>
<h3 id="new-model-selector-ui"><a data-card="" href="#new-model-selector-ui">New Model Selector UI</a></h3>
<p>With over 530 models from 24 providers now available, discovering and selecting the right model required a complete overhaul.
The Model Selector has been completely redesigned as a full-featured dialog offering:</p>
<ul>
<li><strong>üîç Smart Search &amp; Discovery</strong> - Instantly search across model names, IDs, and providers</li>
<li><strong>üéØ Advanced Filtering</strong> - Filter by name, providers &amp; input and output modalities</li>
<li><strong>üìä Flexible Sorting</strong> - Sort by Knowledge Cutoff, Release Date, Last Updated &amp; Context</li>
<li><strong>‚≠ê Favorites System</strong> - Star model card to add/remove to favorites quick list</li>
<li><strong>üíé Rich Model Cards</strong> - In depth model overview at a glance</li>
</ul>
<p>Where providers can be quickly enabled or disabled to customize which models are available:</p>
<p><a href="https://llmspy.org/docs/features/model-selector"><img src="https://llmspy.org/img/model-selector-providers.webp"></a></p><p>See <a href="https://llmspy.org/docs/features/model-selector">Model Selector</a> docs for more info.</p>
<hr>

<p>llms.py has been rewritten from the ground-up with extensibility a <strong>core concept</strong> where all <a href="https://llmspy.org/docs/extensions/built-in">major UI and Server features</a> now layer on their encapsulated functionality by using the public Client &amp; Server Extensibility APIs.</p>
<p>Extensions are just folders that can add both Server and UI features using the public client and server extensibility APIs. <a href="https://llmspy.org/docs/extensions/built-in">Built-in features</a> are just extensions in the repo's <a href="https://github.com/ServiceStack/llms/tree/main/llms/extensions" rel="noreferrer noopener" target="_blank">llms/extensions</a> folder which <a href="https://llmspy.org/docs/extensions/built-in#disable-extensions">can be disabled</a> or overridden by adding them to your local <code>~/.llms/extensions</code> folder. Too minimize bloat, only features that are generally useful and don't require additional dependencies are included as built-in extensions.</p>
<p>llms includes support for installing and uninstalling extensions from any GitHub repository. For better discoverability, non built-in extensions are maintained in the <a href="https://github.com/orgs/llmspy/repositories" rel="noreferrer noopener" target="_blank">github.com/llmspy</a> organization repositories which anyone else is welcome to contribute their repos to for increased discoverability.</p>
<p>UI components are now registered and referenced as Global Vue components, which can be easily replaced by registering Vue components with the same name as done in the <a href="https://github.com/llmspy/xmas/blob/main/ui/index.mjs" rel="noreferrer noopener" target="_blank">xmas</a> extension demo.</p>
<p>This approach allows <a href="https://github.com/ServiceStack/llms/blob/main/llms/main.py" rel="noreferrer noopener" target="_blank">main.py</a> to retain a <strong>lean functional core in a single file</strong> whilst still being fully extensible and lays the foundation for <strong>rapid development of new features</strong> - both from the core team and external 3rd party extensions - enabling the community to extend llms.py in new unanticipated ways.</p>
<p>For deployments requiring minimal footprint, the <a href="https://llmspy.org/docs/deployment/custom-build">Custom Build</a> docs shows how to create a tailored distribution with only the specific extensions you need - perfect for CLI-only or lightweight API server deployments.</p>

<p>To keep the core lightweight while enabling limitless enhancements, we've implemented a flexible <strong>Extensions system</strong> inspired by ComfyUI Custom Nodes. This allows adding new features, pages and toolbar icons, register new provider implementations, extend, replace, and customize the UI with your own custom features, just by adding new extension folders.</p>
<h3 id="managing-extensions"><a data-card="" href="#managing-extensions">Managing Extensions</a></h3>
<p><strong>List available extensions:</strong></p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>llms</span><span> --add</span></span></code></pre></div></figure>
<p>Output:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>Available extensions:</span></span>
<span><span>  fast_mcp     Add MCP Support using FastMCP</span></span>
<span><span>  gemini       Google Gemini RAG file search with document management, auto-upload &amp; sync</span></span>
<span><span>  xmas         Example of utilizing the Extensions APIs to give llms.py some Christmas spirit</span></span>
<span><span>  duckduckgo   Add web search tool capabilities using Duck Duck Go</span></span>
<span><span></span></span>
<span><span>Usage:</span></span>
<span><span>  llms --add &lt;extension&gt;</span></span>
<span><span>  llms --add &lt;github-user&gt;/&lt;repo&gt;</span></span></code></pre></div></figure>
<p><strong>Install an extension:</strong></p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>llms</span><span> --add</span><span> fast_mcp</span></span></code></pre></div></figure>
<p><strong>Install a 3rd-party extension:</strong></p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>llms</span><span> --add</span><span> my_user/my_extension</span></span></code></pre></div></figure>
<div><p>INFO</p><p>Clones the GitHub repo into <code>~/.llms/extensions/my_extension</code> and installs any <code>requirements.txt</code> dependencies.</p></div>
<p><strong>List installed extensions:</strong></p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>llms</span><span> --remove</span></span></code></pre></div></figure>
<p><strong>Remove an extension:</strong></p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>llms</span><span> --remove</span><span> fast_mcp</span></span></code></pre></div></figure>
<h3 id="manual-installation"><a data-card="" href="#manual-installation">Manual Installation</a></h3>
<p>Extensions can be installed from GitHub or by creating a local folder:</p>
<ul>
<li><strong>Local</strong>: Simply create a folder in <code>~/.llms/extensions/my_extension</code></li>
<li><strong>GitHub</strong>: Clone extensions into <code>~/.llms/extensions</code>, e.g:</li>
</ul>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>git clone https://github.com/user/repo ~/.llms/extensions/my_extension</span></span></code></pre></div></figure>
<p>See <a href="https://llmspy.org/docs/extensions">Extensions</a> docs for more details.</p>
<h3 id="how-it-works-server"><a data-card="" href="#how-it-works-server">How it Works (Server)</a></h3>
<p>Extensions are Python modules that plug into the server lifecycle using special hooks defined in their <code>__init__.py</code>:</p>
<div><table><thead><tr><th>Hook</th><th>Purpose</th></tr></thead><tbody><tr><td><code>__parser__(parser)</code></td><td>Add custom CLI arguments</td></tr><tr><td><code>__install__(ctx)</code></td><td>Enhance the server instance (routes, providers, filters, etc.)</td></tr><tr><td><code>__load__(ctx)</code></td><td>Load data or perform <strong>async</strong> tasks before server starts</td></tr><tr><td><code>__run__(ctx)</code></td><td>Execute custom logic when running in CLI mode</td></tr></tbody></table></div>
<p>The <code>ctx</code> parameter provides access to the <code>ExtensionContext</code>.</p>
<p>See <a href="https://llmspy.org/docs/extensions/server">Server Extensions</a> docs for more details.</p>
<h3 id="how-it-works-ui"><a data-card="" href="#how-it-works-ui">How it Works (UI)</a></h3>
<p>Extensions can also include frontend components:</p>
<ol>
<li><strong>Placement</strong>: Add a <code>ui</code> folder within your extension directory</li>
<li><strong>Access</strong>: Files in this folder are automatically served at <code>/ext/&lt;extension_name&gt;/*</code></li>
<li><strong>Integration</strong>: Create a <code>ui/index.mjs</code> file. This is the entry point and must export an <code>install</code> function:</li>
</ol>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>const</span><span> MyComponent</span><span> =</span><span> {</span></span>
<span><span>    template: </span><span>`...`</span></span>
<span><span>}</span></span>
<span></span>
<span><span>// ui/index.mjs</span></span>
<span><span>export</span><span> default</span><span> {</span></span>
<span><span>    install</span><span>(</span><span>ctx</span><span>) {</span></span>
<span><span>        // Register or replace components, add routes, etc.</span></span>
<span><span>        ctx.</span><span>components</span><span>({ MyComponent })</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre></div></figure>
<p>See <a href="https://llmspy.org/docs/extensions/ui">UI Extensions</a> docs for more details.</p>
<h3 id="example-xmas-extension"><a data-card="" href="#example-xmas-extension">Example: xmas extension</a></h3>
<p>The <a href="https://github.com/llmspy/xmas" rel="noreferrer noopener" target="_blank">xmas</a> extension demonstrates these capabilities where it utilizes the Extensions APIs to give llms.py a splash of Christmas spirit. It uses <code>__install__</code> to register an API endpoint and a UI extension for its UI features.</p>
<h3 id="replacing-core-components"><a data-card="" href="#replacing-core-components">Replacing Core Components</a></h3>
<p>All UI features of xmas is implemented in its <a href="https://github.com/llmspy/xmas/blob/main/ui/index.mjs" rel="noreferrer noopener" target="_blank">ui/index.mjs</a>
which overrides default <code>Brand</code> and <code>Welcome</code> components by registering components with the same name, e.g:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>const</span><span> Brand</span><span> =</span><span> {</span></span>
<span><span>    template: </span><span>`</span></span>
<span><span>    &lt;div class="flex-shrink-0 p-2 border-b border-gray-200 dark:border-gray-700"&gt;</span></span>
<span><span>        &lt;button type="button" @click="$router.push('/')" class="..."&gt;</span></span>
<span><span>            üéÑ {{ $state.title }} üéÑ</span></span>
<span><span>        &lt;/button&gt;</span></span>
<span><span>    &lt;/div&gt;</span></span>
<span><span>    `</span><span>,</span></span>
<span><span>}</span></span>
<span><span>const</span><span> Welcome</span><span> =</span><span> {</span></span>
<span><span>    template: </span><span>`&lt;!-- Custom Welcome Screen --&gt;`</span><span>,</span></span>
<span><span>    setup</span><span>() { </span><span>/* ... */</span><span> }</span></span>
<span><span>}</span></span>
<span></span>
<span><span>export</span><span> default</span><span> {</span></span>
<span><span>    install</span><span>(</span><span>ctx</span><span>) {</span></span>
<span><span>        ctx.</span><span>components</span><span>({</span></span>
<span><span>            // Replaces built-in UI Components</span></span>
<span><span>            Brand,</span></span>
<span><span>            Welcome,</span></span>
<span><span>            // Registers other custom components used in this UI Extension</span></span>
<span><span>            XmasPage,</span></span>
<span><span>            XmasTopPanel,</span></span>
<span><span>        })</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre></div></figure>
<p>To change both the home page and brand on the top-left to give every page title a festive touch:</p>
<p>It also demonstrates adding a new icon on the left sidebar to open its custom Xmas page component and a top-panel component to display its "Ask Santa" portal:</p>

<p>The Xmas page calls a custom API endpoint registered in its <code>__install__</code> hook to return a custom festive greeting, whilst the top-panel modifies chat requests while its Top Panel is open to add a Santa system prompt which is enough to implement its "Ask Santa" feature.</p>
<p>Smart generation models like Nano Banana's <strong>gemini-2.5-flash-image</strong> perform exceptionally well here as they're able to answer your kids questions with rich, detailed responses and image outputs.</p>
<hr>

<p>The <a href="https://github.com/llmspy/gemini" rel="noreferrer noopener" target="_blank">gemini</a> extension provides a complete solution for managing Google Gemini's <a href="https://ai.google.dev/api/file-search" rel="noreferrer noopener" target="_blank">File Search Stores</a>, enabling <strong>RAG (Retrieval Augmented Generation)</strong> workflows with automatic document uploads, category organization, and bidirectional sync between your local database and Gemini's cloud storage.</p>
<p>Build up your own knowledge base in File Stores, optionally organized into categories, that you can query to ground your AI chats with your own data - whether that's searching across a single document, a category of related documents, or your entire filestore.</p>
<h3 id="install-1"><a data-card="" href="#install-1">Install</a></h3>
<p>Install the <strong>gemini</strong> extension via the CLI:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>llms</span><span> --add</span><span> gemini</span></span></code></pre></div></figure>
<p>After which you'll be able to click the <strong>Gemini Icon</strong> to open the Gemini extension page from the sidebar to manage your filestores.</p>
<h3 id="key-features"><a data-card="" href="#key-features">Key Features</a></h3>
<ul>
<li><strong>Filestore Management</strong>: Create and manage isolated stores of documents for different projects or knowledge bases</li>
<li><strong>Drag &amp; Drop Uploads</strong>: Easily upload documents (PDF, Text, Markdown, etc.) by dragging them into the UI</li>
<li><strong>Smart Categorization</strong>: Organize documents into categories (folders) for granular retrieval</li>
<li><strong>Contextual RAG Chat</strong>:<!-- -->
<ul>
<li><strong>Ask Filestore</strong>: Chat with the entire knowledge base of a filestore</li>
<li><strong>Ask Category</strong>: Focus your chat on a specific category within a filestore</li>
<li><strong>Ask Document</strong>: Chat with a single specific document</li>
</ul>
</li>
<li><strong>Bi-Directional Sync</strong>: Reconcile your local database with the remote Gemini File API</li>
</ul>
<h3 id="uploading-documents"><a data-card="" href="#uploading-documents">Uploading Documents</a></h3>
<p>Documents can be uploaded by dragging and dropping files onto the upload zone or clicking to open the file picker. You can organize uploads into category folders by typing a category name before uploading.</p>
<p>Uploads are processed asynchronously by a <strong>Background Worker</strong> utilizing a <strong>DB Queue</strong>, so you can continue working while documents are indexed. The worker automatically starts when new documents are uploaded and efficiently handles batch processing without blocking the UI.</p>
<h3 id="rag-chat-in-action"><a data-card="" href="#rag-chat-in-action">RAG Chat in Action</a></h3>
<p>Once documents are uploaded, you can start contextual RAG chat sessions with your data. Each session is pre-configured with a Gemini Model and the <code>file_search</code> tool to query your selected filestore, category, or document - as shown in the <strong>meta example</strong> below querying this very <strong>v3</strong> document for its best features:</p>
<p>The grounded sources used to answer your query are displayed at the bottom of each chat response, allowing you to verify and explore the source documents.</p>
<p>See the <a href="https://llmspy.org/docs/extensions/gemini">Gemini Extension</a> docs for complete usage instructions.</p>
<hr>

<p>This release also includes <strong>first-class support for Python function calling (Tools)</strong>, allowing LLMs to interact with your local environment and custom functionality.</p>
<p>Tools can be defined using standard Python functions where its tool definition can be implicitly defined from its function's signature, type hints, and docstrings:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>def</span><span> get_current_time</span><span>(tz_name: Optional[</span><span>str</span><span>] </span><span>=</span><span> None</span><span>) -&gt; </span><span>str</span><span>:</span></span>
<span><span>    """</span></span>
<span><span>    Get current time in ISO-8601 format.</span></span>
<span></span>
<span><span>    Args:</span></span>
<span><span>        tz_name: Optional timezone name (e.g. 'America/New_York'). Defaults to UTC.</span></span>
<span><span>    """</span></span>
<span><span>    if</span><span> tz_name:</span></span>
<span><span>        try</span><span>:</span></span>
<span><span>            tz </span><span>=</span><span> ZoneInfo(tz_name)</span></span>
<span><span>        except</span><span> Exception</span><span>:</span></span>
<span><span>            return</span><span> f</span><span>"Error: Invalid timezone '</span><span>{</span><span>tz_name</span><span>}</span><span>'"</span></span>
<span><span>    else</span><span>:</span></span>
<span><span>        tz </span><span>=</span><span> timezone.utc</span></span>
<span></span>
<span><span>    return</span><span> datetime.now(tz).isoformat()</span></span></code></pre></div></figure>
<h3 id="register-tools-for-function-calling"><a data-card="" href="#register-tools-for-function-calling">Register tools for function calling</a></h3>
<h4 id="implicit-tool-definition"><a data-card="" href="#implicit-tool-definition">Implicit Tool Definition</a></h4>
<p>Tools can be registered within an extension's <code>install</code> hook using <code>ctx.register_tool</code>:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>def</span><span> install</span><span>(ctx):</span></span>
<span><span>    # Automatic definition from function signature</span></span>
<span><span>    ctx.register_tool(get_current_time)</span></span></code></pre></div></figure>
<p>If no group is specified, tools are registered under the default <code>custom</code> group, alternatively you can group them under your preferred name:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>ctx.register_tool(get_current_time, </span><span>group</span><span>=</span><span>"my_tools"</span><span>)</span></span></code></pre></div></figure>
<h4 id="explicit-tool-definition"><a data-card="" href="#explicit-tool-definition">Explicit Tool Definition</a></h4>
<p>When more fine-grain configuration is needed you can use an explicit tool definition, e.g:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>ctx.register_tool(</span></span>
<span><span>    get_current_time,</span></span>
<span><span>    {</span></span>
<span><span>        "type"</span><span>: </span><span>"function"</span><span>,</span></span>
<span><span>        "function"</span><span>: {</span></span>
<span><span>            "name"</span><span>: </span><span>"get_current_time"</span><span>,</span></span>
<span><span>            "description"</span><span>: </span><span>"Get current time in ISO-8601 format."</span><span>,</span></span>
<span><span>            "parameters"</span><span>: {</span></span>
<span><span>                "type"</span><span>: </span><span>"object"</span><span>,</span></span>
<span><span>                "properties"</span><span>: {</span></span>
<span><span>                    "tz_name"</span><span>: {</span></span>
<span><span>                        "type"</span><span>: </span><span>"string"</span><span>,</span></span>
<span><span>                        "description"</span><span>: </span><span>"timezone name (e.g. 'America/New_York')"</span><span>,</span></span>
<span><span>                        "default"</span><span>: </span><span>"UTC"</span></span>
<span><span>                    }</span></span>
<span><span>                },</span></span>
<span><span>                "required"</span><span>: []</span></span>
<span><span>            }</span></span>
<span><span>        }</span></span>
<span><span>    })</span></span></code></pre></div></figure>
<h3 id="ui-management"><a data-card="" href="#ui-management">UI Management</a></h3>
<ul>
<li><strong>One-Click Enable/Disable</strong>: Use the Tool Selector in the top-right to control which tools to use per request</li>
<li><strong>Granular Control</strong>: Select "All", "None", or specific tools for each chat session</li>
</ul>
<p><strong>Dedicated Tools Page</strong>: View all registered tools and their definitions at <code>/tools</code> or via the sidebar</p>
<hr>

<p>The <a href="https://github.com/llmspy/fast_mcp" rel="noreferrer noopener" target="_blank">fast_mcp</a> extension brings <strong>Model Context Protocol (MCP)</strong> support to llms.py, allowing you to extend LLM capabilities with a wide range of external tools and services using the <a href="https://gofastmcp.com/" rel="noreferrer noopener" target="_blank">FastMCP Python Framework</a>.</p>
<h3 id="install-2"><a data-card="" href="#install-2">Install</a></h3>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>llms</span><span> --add</span><span> fast_mcp</span></span></code></pre></div></figure>
<h3 id="key-features-1"><a data-card="" href="#key-features-1">Key Features</a></h3>
<ul>
<li><strong>Standardized Tool Access</strong>: Connect to any MCP-compliant server (Node.js, Python, etc.) seamlessly</li>
<li><strong>Dynamic Discovery</strong>: Automatically discovers and registers all tools exposed by configured servers</li>
<li><strong>Parallel Discovery</strong>: All configured MCP servers are discovered concurrently for fast startup times</li>
<li><strong>UI Management</strong>: Add, edit, and manage MCP servers directly from the Tools page</li>
</ul>

<h3 id="configuration"><a data-card="" href="#configuration">Configuration</a></h3>
<p>MCP servers are configured via a <code>mcp.json</code> file. By default, Anthropic's <a href="https://github.com/modelcontextprotocol/servers/tree/main/src/git" rel="noreferrer noopener" target="_blank">Git MCP Server</a> is pre-configured:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>{</span></span>
<span><span>  "mcpServers"</span><span>: {</span></span>
<span><span>    "git"</span><span>: {</span></span>
<span><span>      "command"</span><span>: </span><span>"uvx"</span><span>,</span></span>
<span><span>      "args"</span><span>: [</span><span>"mcp-server-git"</span><span>, </span><span>"--repository"</span><span>, </span><span>"$PWD"</span><span>]</span></span>
<span><span>    },</span></span>
<span><span>    "gemini-gen"</span><span>: {</span></span>
<span><span>      "description"</span><span>: </span><span>"Gemini Image and Audio TTS generation"</span><span>,</span></span>
<span><span>      "command"</span><span>: </span><span>"uvx"</span><span>,</span></span>
<span><span>      "args"</span><span>: [</span><span>"gemini-gen-mcp"</span><span>],</span></span>
<span><span>      "env"</span><span>: {</span></span>
<span><span>        "GEMINI_API_KEY"</span><span>: </span><span>"$GEMINI_API_KEY"</span></span>
<span><span>      }</span></span>
<span><span>    }    </span></span>
<span><span>  }</span></span>
<span><span>}</span></span></code></pre></div></figure>
<h3 id="managing-servers"><a data-card="" href="#managing-servers">Managing Servers</a></h3>
<p>Add, edit, or remove MCP servers directly from the UI:</p>

<h3 id="executing-tools"><a data-card="" href="#executing-tools">Executing Tools</a></h3>
<p>MCP tools can be executed directly from the Tools page or invoked by LLMs during chat sessions:</p>

<h3 id="html-results"><a data-card="" href="#html-results">HTML Results</a></h3>
<p>Tool outputs containing HTML content are rendered within a sandboxed iframe, letting you interact with rich content and even play games:</p>
<p>See the <a href="https://llmspy.org/docs/mcp/fast_mcp">MCP Support</a> docs for complete configuration and usage details.</p>
<h3 id="omarchy-mcp"><a data-card="" href="#omarchy-mcp">Omarchy MCP</a></h3>
<p>For <a href="https://omarchy.org/" rel="noreferrer noopener" target="_blank">Omarchy</a> users, the <a href="https://llmspy.org/docs/mcp/omarchy_mcp">Omarchy MCP</a> enables AI assistants to manage themes - including listing, switching, previewing, installing, and removing themes from your Omarchy desktop environment.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/eV17C0cJz00" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>The built-in <a href="https://github.com/ServiceStack/llms/blob/main/llms/extensions/core_tools/__init__.py" rel="noreferrer noopener" target="_blank">core_tools</a> extension provides essential functionality for LLMs to interact with their environment, perform calculations, and manage persistent data.</p>
<h3 id="memory-tools"><a data-card="" href="#memory-tools">Memory Tools</a></h3>
<p>Functions for persistent key-value storage.</p>
<ul>
<li><code>memory_read</code> - Read a value from persistent memory.</li>
<li><code>memory_write</code> - Write a value to persistent memory.</li>
</ul>
<h3 id="file-system-tools"><a data-card="" href="#file-system-tools">File System Tools</a></h3>
<p>All file system operations are restricted to the current working directory for safety.</p>
<ul>
<li><code>read_file</code> - Read a text file from disk.</li>
<li><code>write_file</code> - Write text to a file (overwrites existing content).</li>
<li><code>list_directory</code> - List directory contents including file names, sizes, and modification times.</li>
<li><code>glob_paths</code> - Find files and directories matching a glob pattern.</li>
</ul>
<h3 id="utilities"><a data-card="" href="#utilities">Utilities</a></h3>
<ul>
<li><code>get_current_time</code> - Get the current time in ISO-8601 format.</li>
</ul>
<h3 id="math--logic"><a data-card="" href="#math--logic">Math &amp; Logic</a></h3>
<ul>
<li><code>calc</code> - Evaluate a mathematical expression. Supports arithmetic, comparison, boolean operators, and common math functions.</li>
</ul>
<h3 id="code-execution-tools"><a data-card="" href="#code-execution-tools">Code Execution Tools</a></h3>
<p>LLMS includes a suite of tools for executing code in various languages within a sandboxed environment. These tools are designed to allow the agent to run scripts, perform calculations, and verify logic safely.</p>
<h4 id="supported-languages"><a data-card="" href="#supported-languages">Supported Languages</a></h4>
<ul>
<li><code>run_python(code)</code> - Executes Python code.</li>
<li><code>run_javascript(code)</code> - Executes JavaScript code (uses <code>bun</code> or <code>node</code>).</li>
</ul>

<ul>
<li><code>run_typescript(code)</code> - Executes TypeScript code (uses <code>bun</code> or <code>node</code>).</li>
<li><code>run_csharp(code)</code> - Executes C# code (uses <code>dotnet run</code> with .NET 10+ single-file support).</li>
</ul>


<p>The built-in <a href="https://github.com/ServiceStack/llms/tree/main/llms/extensions/computer" rel="noreferrer noopener" target="_blank">computer_use</a> extension transforms AI agents into autonomous computer operators. Based on <a href="https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo" rel="noreferrer noopener" target="_blank">Anthropic's computer use tools</a>, it enables agents to see your screen, control the mouse and keyboard, execute shell commands, and edit files - just like a human sitting at the computer.</p>
<p>This unlocks powerful capabilities that traditional API-based tools cannot achieve:</p>
<ul>
<li><strong>Visual Verification</strong>: Confirm that code actually renders correctly in a browser</li>
<li><strong>Desktop Automation</strong>: Control any GUI application - web browsers, IDEs, terminals</li>
<li><strong>End-to-End Workflows</strong>: Chain together multiple applications in a single task</li>
<li><strong>Legacy Applications</strong>: Automate software that lacks APIs</li>
</ul>
<p>For example, an agent can write a web application, open a browser, and capture a screenshot to prove it works:</p>
<p>See the <a href="https://llmspy.org/docs/extensions/computer_use">Computer Use</a> docs for complete usage details.</p>

<p>As some core tools are particularly useful on their own, dedicated UIs has been added for the <code>calc</code> tool with support for evaluating mathematical python expressions, including arithmetic, comparison, boolean operators, <code>math.*</code> functions &amp; constants and python list comprehensions</p>
<ul>
<li><strong>üñ•Ô∏è UX Friendly Interface</strong> - Clean, modern, responsive UI with dark mode support</li>
<li><strong>üíæ Persistent History</strong> - Calculations automatically saved to localStorage and preserved between sessions</li>
<li><strong>‚ö° 1-Click Interaction</strong> - Click history items to instantly load expressions and copy to clipboard</li>
<li><strong>‚å®Ô∏è Keyboard-Free Access</strong> - Complete UI buttons for numbers, operators, constants, and math functions</li>
<li><strong>üêç Python Math Support</strong> - Full access to Python's math library including trig, stats, and more</li>
<li><strong>üõ°Ô∏è Safe Evaluation</strong> - AST-based evaluator prevents arbitrary code execution for secure calculations</li>
</ul>

<p>Whilst the <code>run_python</code> tools provides a scratch pad for running stand-alone Python, JavaScript, TypeScript, and C# code in a sandbox.</p>
<p>The UI uses <strong>CodeMirror</strong> as the code editor, providing a better user experience with syntax highlighting, code completion, and other IDE-like features for writing code.</p>


<p>The UI uses <strong>CodeMirror</strong> as the code editor, providing a better user experience with syntax highlighting, code completion, and other IDE-like features for writing code.</p>
<div><p>INFO</p><p>As both dedicated UIs run the tools directly, they don't use AI or consume any tokens</p></div>
<p>See the <a href="https://llmspy.org/docs/features/run-code-ui">Run Code UI</a> docs for more details.</p>
<hr>

<p>The <a href="https://github.com/ServiceStack/llms/tree/main/llms/extensions/katex" rel="noreferrer noopener" target="_blank">katex</a> extension enables beautiful rendering of LaTeX math expressions in AI responses using <a href="https://katex.org/" rel="noreferrer noopener" target="_blank">KaTeX</a>. It integrates automatically with the markdown parser to render math equations in both inline and block formats.</p>
<h3 id="features"><a data-card="" href="#features">Features</a></h3>
<ul>
<li><strong>Fast Rendering</strong>: Uses KaTeX for high-performance rendering of math expressions.</li>
<li><strong>Inline Math</strong>: Renders math within text using <code>$</code> or <code>$$</code> delimiters.</li>
<li><strong>Block Math</strong>: Renders complex equations in their own block using <code>$</code> or <code>$$</code> delimiters across multiple lines.</li>
<li><strong>Auto-Integration</strong>: Automatically extends the <code>marked</code> parser used in the application.</li>
</ul>

<hr>

<p>Unlike text generation, there's no standard API for image generation across providers - each requires its own custom implementation. Despite the additional effort required, there's now seamless image generation support through both the UI and CLI with built-in integrations for:</p>
<div><table><thead><tr><th>Provider</th><th>Status</th></tr></thead><tbody><tr><td>Google</td><td>‚úÖ Supported</td></tr><tr><td>OpenAI</td><td>‚úÖ Supported</td></tr><tr><td>OpenRouter</td><td>‚úÖ Supported</td></tr><tr><td>Chutes</td><td>‚úÖ Supported</td></tr><tr><td>Z.ai</td><td>‚úÖ Supported</td></tr><tr><td>Nvidia</td><td>‚úÖ Supported</td></tr></tbody></table></div>
<p>To begin select an image generation model from the Model Selector that supports image generation:</p>
<p>When an image generation model is selected, the chat prompt will the option to specify which aspect ratio to use for the generated images:</p>
<h3 id="command-line-usage"><a data-card="" href="#command-line-usage">Command-Line Usage</a></h3>
<p>Generate images using the <code>--out image</code> modifier:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>llms</span><span> --out</span><span> image</span><span> "cat in a hat"</span></span></code></pre></div></figure>
<p>Which uses the <code>out:image</code> chat template in <code>llms.json</code> for its image generation request. Before returning, any assets are saved to cache and their local path and HTTP URL returned, e.g:</p>
<p><strong>Output:</strong></p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>Here is a cat in a hat for you! </span></span>
<span><span></span></span>
<span><span>Saved files:</span></span>
<span><span>/home/mythz/.llms/cache/c9/c9b2fd2a1d95708251...5d3f467a.png</span></span>
<span><span>http://localhost:8000/~cache/c9/c9b2fd2a1d95708251...5d3f467a.png</span></span></code></pre></div></figure>
<h3 id="specify-a-model"><a data-card="" href="#specify-a-model">Specify a Model</a></h3>
<p>Use any model that supports image generation by specifying its <strong>ID</strong> or <strong>name</strong>:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>llms</span><span> -m</span><span> "gemini-2.5-flash-image"</span><span> --out</span><span> image</span><span> "cat in a hat"</span></span>
<span><span>llms</span><span> -m</span><span> "Gemini 2.5 Flash Image"</span><span> --out</span><span> image</span><span> "cat in a hat"</span></span></code></pre></div></figure>
<div><p>INFO</p><p>üìÅ All generated images are saved to <code>~/.llms/cache</code> using their SHA-256 hash as the filename.</p></div>

<p>Audio generation is an emerging capability with limited provider support where Text-to-Speech generation through both the UI and CLI, currently only supports Google's latest TTS models:</p>
<div><table><thead><tr><th>Model</th><th>Description</th></tr></thead><tbody><tr><td><strong>Gemini 2.5 Flash Preview TTS</strong></td><td>Fast, lightweight TTS</td></tr><tr><td><strong>Gemini 2.5 Pro Preview TTS</strong></td><td>High-quality TTS</td></tr></tbody></table></div>
<p>Typically you'd select the audio generation model from the Model Selector to find models that supports audio generation:</p>
<p>But despite models.dev listing them as capable of audio generation, only Gemini's TTS models are currently supported for audio generation through Gemini's API as Alibaba doesn't yet support the <strong>audio</strong> modality.</p>
<h3 id="ui--command-line-usage"><a data-card="" href="#ui--command-line-usage">UI &amp; Command-Line Usage</a></h3>
<p>Available in both the UI and on the command-line using <code>--out audio</code>:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>llms</span><span> --out</span><span> audio</span><span> "Merry Christmas"</span></span>
<span><span>llms</span><span> -m</span><span> gemini-2.5-pro-preview-tts</span><span> --out</span><span> audio</span><span> "Merry Christmas"</span></span></code></pre></div></figure>
<h3 id="output"><a data-card="" href="#output">Output</a></h3>
<p>Audio files are saved locally and accessible via HTTP URL:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>Saved files:</span></span>
<span><span>/Users/llmspy/.llms/cache/c2/c27b5fd43ebbdbca...acf118.wav</span></span>
<span><span>http://localhost:8000/~cache/c2/c27b5fd43ebbdbca...acf118.wav</span></span></code></pre></div></figure>
<h3 id="playback"><a data-card="" href="#playback">Playback</a></h3>
<p><strong>From the command line:</strong></p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>play</span><span> /Users/llmspy/.llms/cache/c2/c27b5fd43ebbdbca...acf118.wav</span></span></code></pre></div></figure>
<p><strong>From the browser:</strong>
Run server with <code>llms --serve 8000</code> to play URL in your browser.</p>

<p>The <a href="https://github.com/ServiceStack/llms/tree/main/llms/extensions/gallery" rel="noreferrer noopener" target="_blank">gallery</a> extension intercepts all generated image, audio &amp; file assets and uploaded files in <code>~/.llms/cache</code> file storage whose metadata is maintained in a SQLite database at <code>~/.llms/user/default/gallery/gallery.sqlite</code></p>
<p>Dedicated UIs are available for quickly browsing and navigating or generated images / audio files including a lightbox previewer for full-size viewing:</p>
<h4 id="portrait-images"><a data-card="" href="#portrait-images">Portrait Images</a></h4>
<h4 id="square-images"><a data-card="" href="#square-images">Square Images</a></h4>
<h4 id="landscape-images"><a data-card="" href="#landscape-images">Landscape Images</a></h4>
<h4 id="audio-generations"><a data-card="" href="#audio-generations">Audio Generations</a></h4>
<hr>

<p>System prompts support was refactored into a replaceable <a href="https://github.com/ServiceStack/llms/tree/main/llms/extensions/system_prompts" rel="noreferrer noopener" target="_blank">system_prompts</a> extension which configures AI requests with a library of <strong>over 200+</strong> awesome curated system prompts that can be selected from the UI.</p>
<h3 id="custom-system-prompts"><a data-card="" href="#custom-system-prompts">Custom System Prompts</a></h3>
<p>You can maintain your own library of system prompts for all anonymous users at:
<code>~/.llms/user/default/system-prompts.json</code></p>
<p>Or for signed in users at:
<code>~/.llms/user/&lt;github-user&gt;/system-prompts.json</code></p>
<p>With the JSON file simply containing an array of names and their system prompts, e.g:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span>[</span></span>
<span><span>    {</span></span>
<span><span>        "name"</span><span>: </span><span>"Helpful Assistant"</span><span>,</span></span>
<span><span>        "prompt"</span><span>: </span><span>"You are a helpful assistant."</span></span>
<span><span>    }</span></span>
<span><span>]</span></span></code></pre></div></figure>
<p>Browse the complete collection of available system prompts below:</p>

<hr>

<p>Another major change is the migration from client-side IndexedDB storage to a robust server-side SQLite databases. This architectural shift ensures better data consistency, improved performance that enables parallel executions and multi-device access to your chat history.</p>
<p>To keep the database efficient and portable, binary assets (images, audio, etc.) are not stored directly in the SQLite database, Instead all generated assets are stored in the local file system cache at <code>~/.llms/cache</code> and only <strong>relative URLs</strong> referencing these assets are stored in the database.</p>
<h4 id="concurrency-model"><a data-card="" href="#concurrency-model">Concurrency Model</a></h4>
<p>To ensure data integrity and high performance without complex locking mechanisms, the system utilizes a <strong>single background thread</strong> to  write operations to the database. This design improves concurrency handling and eliminates database locking issues during high-load scenarios.</p>
<h4 id="multi-tenancy--security"><a data-card="" href="#multi-tenancy--security">Multi-Tenancy &amp; Security</a></h4>
<p>When authentication is enabled, data isolation is automatically enforced. All core tables, including <code>threads</code> and <code>requests</code>, are scoped to the authenticated user, ensuring that users can only access their own data.</p>

<p>A new caching system has been implemented for generated assets and uploaded images and files that's now persisted in <code>~/.llms/cache</code>, preserving them across messages and sessions.</p>
<ul>
<li><strong>Efficient Storage</strong>: Only cache references are stored with chat messages</li>
<li><strong>Persistent Access</strong>: Images remain accessible in previews and downloads after page reloads</li>
<li><strong>Automatic Management</strong>: System handles file storage and serving transparently</li>
</ul>
<p>Now that all persistence is server-side, to transfer or backup your configurations, extensions and Chat History you need only copy your <code>~/.llms</code> folder.</p>
<hr>

<p>All server extension features including tools, custom providers, database persistence, and image/audio generation are fully accessible via the command line, making llms.py a powerful terminal-based AI assistant.</p>
<h3 id="core-cli-usage"><a data-card="" href="#core-cli-usage">Core CLI Usage</a></h3>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span># One-shot query</span></span>
<span><span>llms</span><span> "What is the capital of France?"</span></span>
<span></span>
<span><span># Specify model by ID or name</span></span>
<span><span>llms</span><span> -m</span><span> claude-opus-45</span><span> "Explain quantum computing"</span></span>
<span><span>llms</span><span> -m</span><span> "Claude Opus 4.5"</span><span> "Write a Python function"</span></span></code></pre></div></figure>
<h3 id="tools--function-calling"><a data-card="" href="#tools--function-calling">Tools &amp; Function Calling</a></h3>
<p>All registered tools are automatically available in CLI mode. Enable specific tools with the <code>--tools</code> flag:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span># Use all available tools by default (`--tools all`)</span></span>
<span><span>llms</span><span> "Read the file data.txt and calculate the sum"</span></span>
<span></span>
<span><span># Use specific tools</span></span>
<span><span>llms</span><span> --tools</span><span> calc,get_current_time</span><span> "What time is it in Tokyo and what's 15% of 230?"</span></span>
<span></span>
<span><span># Don't use any tools</span></span>
<span><span>llms</span><span> --tools</span><span> none</span><span> "Tell me a joke"</span></span></code></pre></div></figure>
<h3 id="extensions-management"><a data-card="" href="#extensions-management">Extensions Management</a></h3>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span># List available extensions from github.com/llmspy</span></span>
<span><span>llms</span><span> --add</span></span>
<span></span>
<span><span># Install an extension</span></span>
<span><span>llms</span><span> --add</span><span> fast_mcp</span></span>
<span></span>
<span><span># Install a 3rd-party extension from GitHub</span></span>
<span><span>llms</span><span> --add</span><span> github-user/repo-name</span></span>
<span></span>
<span><span># List installed extensions</span></span>
<span><span>llms</span><span> --remove</span></span>
<span></span>
<span><span># Uninstall an extension</span></span>
<span><span>llms</span><span> --remove</span><span> fast_mcp</span></span></code></pre></div></figure>
<h3 id="provider-management"><a data-card="" href="#provider-management">Provider Management</a></h3>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span># List all enabled providers and models</span></span>
<span><span>llms</span><span> ls</span></span>
<span></span>
<span><span># List available models from a specific provider</span></span>
<span><span>llms</span><span> ls</span><span> google</span></span>
<span></span>
<span><span># Enable a provider</span></span>
<span><span>llms</span><span> --enable</span><span> google</span></span>
<span></span>
<span><span># Disable a provider</span></span>
<span><span>llms</span><span> --disable</span><span> google</span></span>
<span></span>
<span><span># Update provider definitions from models.dev (automatically updated daily)</span></span>
<span><span>llms</span><span> --update-providers</span></span></code></pre></div></figure>
<h3 id="image-analysis-audio-transcribing--documents-processing"><a data-card="" href="#image-analysis-audio-transcribing--documents-processing">Image Analysis, Audio Transcribing &amp; Documents Processing</a></h3>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span># Image Analysis</span></span>
<span><span>llms</span><span> --image</span><span> https://example.com/chart.jpg</span><span> "Analyze this chart"</span></span>
<span></span>
<span><span># Audio Processing</span></span>
<span><span>llms</span><span> -m</span><span> gpt-4o-audio-preview</span><span> --audio</span><span> interview.mp3</span><span> "Transcribe this interview"</span></span>
<span></span>
<span><span># Document Processing</span></span>
<span><span>llms</span><span> -m</span><span> gpt-5</span><span> --file</span><span> report.pdf</span><span> "Extract action items"</span></span></code></pre></div></figure>
<h3 id="media-generation"><a data-card="" href="#media-generation">Media Generation</a></h3>
<p>Generate images and audio directly from the command line:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span># Generate images</span></span>
<span><span>llms</span><span> --out</span><span> image</span><span> "A serene mountain landscape at sunset"</span></span>
<span><span>llms</span><span> -m</span><span> "gemini-2.5-flash-image"</span><span> --out</span><span> image</span><span> "Logo for a tech startup"</span></span>
<span></span>
<span><span># Generate audio</span></span>
<span><span>llms</span><span> --out</span><span> audio</span><span> "Welcome to our podcast"</span></span>
<span><span>llms</span><span> -m</span><span> gemini-2.5-pro-preview-tts</span><span> --out</span><span> audio</span><span> "Hello world"</span></span></code></pre></div></figure>
<p>All generated media is automatically saved to <code>~/.llms/cache</code> with metadata persisted in SQLite.</p>
<h3 id="database-persistence"><a data-card="" href="#database-persistence">Database Persistence</a></h3>
<p>All CLI interactions are automatically persisted to <code>~/.llms/user/app/app.sqlite</code>, including:</p>
<ul>
<li>Chat threads and messages</li>
<li>Tool calls and results</li>
<li>Generated assets and file references</li>
<li>User preferences and settings</li>
</ul>
<p>Ensuring your conversation history is preserved and accessible from both CLI and Web UI.</p>
<h3 id="server-mode"><a data-card="" href="#server-mode">Server Mode</a></h3>
<p>Launch the web UI while keeping full CLI access:</p>
<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span># Start UI and Chat Completion API on port 8000</span></span>
<span><span>llms</span><span> --serve</span><span> 8000</span></span></code></pre></div></figure>
<p>See <a href="https://llmspy.org/docs/features/cli">CLI Docs</a> for more details.</p>
<hr>

<figure dir="ltr" tabindex="-1"><div role="region" tabindex="0"><pre><code><span><span># Update llms to v3</span></span>
<span><span>pip</span><span> install</span><span> llms-py</span><span> --upgrade</span></span>
<span></span>
<span><span># Start the server</span></span>
<span><span>llms</span><span> --serve</span><span> 8000</span></span></code></pre></div></figure>
<p><strong>Happy holidays from llms.py!</strong> üéÑ</p>
<hr>

<p>With llms .py rebuilt from the ground up as an extensible platform, we hope to foster a thriving community extension ecosystem where developers can share innovative solutions and extend llms.py in ways we haven't yet imagined.</p>
<p>As llms .py is still in active development, we welcome <a href="https://github.com/ServiceStack/llms/discussions" rel="noreferrer noopener" target="_blank">your feedback</a> on any features that would better support 3rd party extensions and help cultivate this growing community.</p>
<hr></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Television is 100 years old today (453 pts)]]></title>
            <link>https://diamondgeezer.blogspot.com/2026/01/tv100.html</link>
            <guid>46766188</guid>
            <pubDate>Mon, 26 Jan 2026 14:41:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diamondgeezer.blogspot.com/2026/01/tv100.html">https://diamondgeezer.blogspot.com/2026/01/tv100.html</a>, See on <a href="https://news.ycombinator.com/item?id=46766188">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <a name="8509262236011018867"></a>    
          Television is 100 years old today.<br>
And it was born here, above an Italian cafe in Soho.
<p>
<a href="https://www.flickr.com/photos/dgeezer/55061003800" target="_blank"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXqV7ltRB4YYaaIxwsMylYD1d_CIPQ022BNjjYsJ3keOV3IQsYkTT8DLRN8koR32gbI6ZGu3QyLlmy7Q8vKVE8Gev6cbYNSuwSkUbaEzkDstTV02ePtkuNmZ5vGfXo695_AkqZD4li4E6psk6nKH6bg-p8NftvwbR6auHwdOY-d2sk_g4dGj9aQA/s1600/barital.jpg" title="22 Frith Street, Soho" data-original-height="375" data-original-width="500"></a></p><p>
The man who first demonstrated television was <a href="https://en.wikipedia.org/wiki/John_Logie_Baird" target="_blank">John Logie Baird</a>, a former engineering apprentice from Helensburgh. And although there are other places that can plausibly claim to be TV's birthplace, including a terraced street in Hastings, a hill in north London and Selfridges, most people agree that the decisive moment was a demonstration given to journalists in Frith Street on 26th January 1926.
</p><p>
Baird might never have made it to London had he not been a sickly boy. When WW1 broke out he wanted to enlist but was refused due to ill health, so took a job with the Clyde Valley Electrical Power Company helping to make munitions instead. In 1923 he moved to the south coast for the good of his health because it had a warmer climate, renting rooms at <a href="https://www.flickr.com/photos/dgeezer/6905677144/in/album-72157629755731497" target="_blank">21 Linton Crescent</a> in Hastings. Here the first television signal transmitting equipment was constructed, with component parts including a hatbox, tea chest, darning needles and bicycle light lenses. The first image to be transmitted was the shadow of a St Johns Ambulance medal with a distinctive spiky outline, an item still on display at <a href="https://www.hmag.org.uk/see-and-do/digital-museum/shadow-dreams/" target="_blank">Hastings Museum</a>. But his tinkering proved dangerous, and although a 1000-volt electric shock thankfully resulted in nothing worse than a burnt hand, his landlord duly asked him to vacate the premises.
</p><p>
<a href="https://www.flickr.com/photos/dgeezer/6905677144/in/album-72157629755731497" target="_blank"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3VKB5mqH-e80jvaqxMLpheO6EmRBNU9XcVsBH5MSZU5ELXhZgjZWeDXgNf0mfrdM5PFIZbb4Ozs1IUCYYz7YShQWK_YjqvhzhZdSHX566l9gv0XRQCvZhMyKJ6qNoOYbOPWMPJfCizd3cXrtSy0_x-nf5ShGpBn1QYkiUzU1KoZpSGshyphenhyphenqJDRzQ/s1600/hastins.jpg" title="21 Linton Crescent, Hastings" data-original-height="375" data-original-width="500"></a></p><p>
Baird moved to London in November 1924 in the hope of showing off his burgeoning invention, setting up a workshop in the attic at <a href="https://osm.org/go/euu4hRekU-?m=" target="_blank">22 Frith Street</a>. Amongst those who dropped by was Gordon Selfridge who invited Baird to give <a href="https://www.bairdtelevision.com/selfridges.html" target="_blank">demonstrations of his device</a> in the Palm Court during his store's upcoming Birthday Week celebrations. He gave three shows a day to long queues of spectators, each invited to peer down a funnel at outlines of shapes transmitted from a separate device a few yards away, including a paper mask which Baird would make 'wink' by covering the eyehole. At this stage Baird's 'Televisor' was still electro-mechanical, the images formed by <a href="https://ieeexplore.ieee.org/document/9779318" target="_blank">spinning discs</a> with doubled-up lenses and perforated rectangular holes. But spectators were impressed, and Baird earned a much-needed ¬£60 to plough back into his enterprise.
</p><p>
By October 1925 Baird had honed his processes sufficiently to be able to transmit an image with gradations of light and shade. Initially he used a ventriloquist's dummy called <a href="https://en.wikipedia.org/wiki/Stooky_Bill" target="_blank">Stooky Bill</a>, this because it had greater contrast than a human face and also because it wouldn't be harmed by intense heat or possible exploding glass. Later, somewhat over-excitedly, he invited a 20 year-old office worker called William Taynton to come upstairs and become TV's first human subject. <a href="https://www.bbc.co.uk/culture/article/20250925-the-office-worker-who-became-the-first-person-ever-to-appear-on-tv" target="_blank">William wasn't keen</a> but an appearance fee of half a crown persuaded him to pick through a jungle of wires, sit in front of blazing hot lamps and stick his tongue out, for just long enough that Baird exclaimed  "I've seen you, William, I've seen you. I've got television at last!" When the time came for a <a href="https://www.english-heritage.org.uk/visit/blue-plaques/john-logie-baird-television/" target="_blank">blue plaque</a> to be unveiled outside 22 Frith Street in 1951, it was William they invited back to do the honours.
</p><p>
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOZWftzsxiNzDnss7Ng4k5nj4ije60e3ORtaOMFhozYY23JkjgZtXWswkSgxwgx6N-8w5rdfYrnNkN1wuU1HF5zpRcizdMDBm4kQZ72tKIOULbuKdHdpHj1y-kA4XL_qikuFdEloeJNsbKU78fJd_A03ZHwv7wxM5kWAAWRFa9-Dd8V8mM2xUwJg/s1600/1926plaq.jpg" title="blue plaque at 22 Frith Street" data-original-height="375" data-original-width="500"></p><p>
Then on 26th January 1926 came the first <a href="https://theconversation.com/a-century-ago-john-logie-baird-achieved-a-landmark-moment-in-television-history-the-viewers-werent-convinced-274089" target="_blank">official demonstration</a> to members of the press. Journalists and guests from the Royal Institution were invited into Baird's workshop in small groups and first shown the dummy on screen, then each other's faces transmitted from a separate room. Only one visitor got too close to the discs and ended up with a sliced beard. Most of those present weren't especially impressed and failed to realise the significance of what they'd just seen, but <a href="https://www.bairdtelevision.com/the-televisor-successful-test-of-new-apparatus-1926.html" target="_blank">The Times</a> followed up with a short article two days later.
</p><blockquote><span>Members of the Royal Institution and other visitors to a laboratory in an upper room in Frith-Street, Soho, on Tuesday saw a demonstration of apparatus invented by Mr. J.L. Baird, who claims to have solved the problem of television. They were shown a transmitting machine, consisting of a large wooden revolving disc containing lenses, behind which was a revolving shutter and a light sensitive cell. It was explained that by means of the shutter and lens disc an image of articles or persons standing in front of the machine could be made to pass over the light sensitive cell at high speed. The current in the cell varies in proportion to the light falling on it, and this varying current is transmitted to a receiver where it controls a light behind an optical arrangement similar to that at the sending end. By this means a point of light is caused to traverse a ground glass screen. The light is dim at the shadows and bright at the high lights, and crosses the screen so rapidly that the whole image appears simultaneously to the eye. <i><span size="1"><a href="https://www.bairdtelevision.com/the-televisor-successful-test-of-new-apparatus-1926.html" target="_blank">(The Times, 28th January 1926)</a></span></i></span>
</blockquote>
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhLidBUcZj-9IW0uPsufQsMSLWbfmR8_lphR-i8uV4ddWfaMtyvfq5g3b80s_BYnZWMnHLlKiK53t_T6Zq0TllMjypukVFsdnWU9b-s3s1hzjdmY5uq3DglQYVA6HxIe8ekHCxBRMQgqs9Uj9jgQR1FRptsU_7prWxa-2_CesqjMDktCoCT5_WpeA/s1600/baritali.jpg" title="22 Frith Street, Soho" data-original-height="375" data-original-width="500"><p>
These days 22 Frith Street is home to retro cafe <a href="https://www.flickr.com/photos/dgeezer/55061003800" target="_blank">Bar Italia</a>. It's been owned and run by the Polledri family <a href="https://web.archive.org/web/20150222075356/http://www.baritaliasoho.co.uk:80/history/" target="_blank">since 1949</a>, a coffee-squirting dynasty who also run the Little Italy restaurant nextdoor. The stone floor was laid by their uncle Torino, a terrazzo mosaic specialist, and the counter was one of the first in London to be graced by an original Gaggia machine. Once a magnet for mods on scooters Bar Italia has attracted many famous names over the years, notably Rocky Marciano whose huge poster has pride of place behind the counter. You could thus celebrate today's centenary with an espresso and a slice of pizza in the photo-bedecked interior, or risk sitting outside below the neon sign with a froth and cheesecake combo.
</p><p>
Number 22 also displays a Milestone <a href="https://www.londonremembers.com/memorials/john-logie-baird-frith-street-ieee" target="_blank">plaque</a> erected by The Institution of Electrical Engineers citing "the world's first public demonstration of live television". Below is a <a href="https://worldoriginsite.org/television-john-logie-baird/" target="_blank">much newer plaque</a> citing this as an accredited <a href="https://worldoriginsite.org/" target="_blank">World Origin Site</a>. I first saw one of these <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5nNQ1ehgF54vVy2dNmP76XdDcSV3tAKKvEn00GaPvcrmPJP0qM2wNkn4P7MpTxJej68Ywvy6ZuNw672QYUMCW_JYBWKejqp5K7vwMTGhVk72-467w_DbZ4x7nTCc_D4dtjusbJTuXTPnD5RkNQDlLTvJErLeMtvgS_WcN6_ej6fd-2AnLoaSHbg/s1600/penicillin0001.jpg" target="_blank">inside</a> the Alexander Fleming Laboratory Museum earlier in the month, earned for the discovery of penicillin, but whereas that was designated WOS 0001 the invention of television only ranks 0037. I believe they're unveiling it <a href="https://worldoriginsite.org/wp-content/uploads/Image-09-12-2025-at-08.45.jpg" target="_blank">officially</a> at 2pm this afternoon, even though it was perfectly visible over the weekend.
</p><p>
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEittOtedoEQKDIyIulqT2FijzkmZysCWcwmLGh94ksOq_VljN7CQoqrQFJXnzK1Hhzp2EXj0BPFQ5AS41E5yaj0rAqf4hweHG4haqe1TvfL0Fwr2KhfFOgS6Cr6JziW9H2CXO1DDXN9NyGaiLGqd1mfqBcOefKznWhT62APIx3-3v9jmbM0lvyQjQ/s1600/worldorig.jpg" title="World Origin Site 0037" data-original-height="375" data-original-width="500"></p><p>
Baird was a highly driven inventor and entrepreneur and went on to develop prototypes for <a href="https://blog.scienceandmediamuseum.org.uk/history-of-british-television-timeline/" target="_blank">all sorts</a> of forward-looking formats. In 1927 he came up with ‚ÄòPhonovision‚Äô (image recordings onto 78 rpm gramophone records) and ‚ÄòNoctovision‚Äô (infra-red TV). In 1928, amazingly, he demonstrated both colour television and stereoscopic (3D) television. His ultimate aim was television broadcasting via the BBC, beginning experimental transmissions of 30-line television in 1930 and delivering the first outside broadcast (from the Derby) in 1931, not that anyone was yet watching.
</p><p>
But in 1932 EMI started to provide serious competition, developing their own pioneering electronic television camera called the <a href="https://collection.sciencemuseumgroup.org.uk/objects/co241682/marconi-emi-television-camera" target="_blank">Emitron</a>. The government's Television Advisory Committee ultimately recommended that both Baird's 240-line mechanical system and Marconi-EMI's 405-line electronic system be developed as alternatives for the proposed new London television station. And so it was that when broadcast TV first launched at Alexandra Palace on <a href="https://diamondgeezer.blogspot.com/2011/11/bbc75.html" target="_blank">2nd November 1936</a> the two systems alternated one week each... Baird second.
</p><p>
<a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUMKlzdQnGb6SgCi4U41nL3CcRcvp6XqNHMsKJ08y8_S0AVfc3uS2QVnn_7t646BKwBMhv3IBn7jyS8WKCKjDa8DL7VjB8xmBIQAZjGfCHjPtCEHjp0rJrURP5tlIQkt3DQG4djDhbIzx6-2Ms-HYglUdjgZOpGSYfu8lQlKLk0ptNOFGcfo1w7A/s1600/1936plaq.jpg" target="_blank"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4Q0FgjL7J_5dvIXCko_SiHkT1Ga370BIJUzRpBXITulmjgTsyaZXpqj8YcccuoOXxDvVoC50sMP0-viDAdhBa6-6Fbj5GG2ifVGUqMn_y0j4aWx8nCQXGOCb9eLjf-fY_fXYu9fszLGvsI2V-GbNVfCARTH22P0c3uOz2EPTMLBKdVtV1SGT6Iw/s1600/allipalli.jpg" title="TV display at Alexandra Palace" data-original-height="375" data-original-width="500"></a></p><p>
It rapidly became clear that the Marconi system was far superior and Baird's was dropped after just three months. Baird also suffered when his studios were burned in the fire that destroyed the Crystal Palace, and his company went into receivership when all TV broadcasting was suspended at the start of WW2. He carried on inventing at home in <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizMlXQIo9On3DxmkHARco76k-QWxHvE3s0LY6c0okLadKGn_Qw35AL6Mbuaj_QeRLXivRfn2h6LJy8Yfsp6OXSim1EnBUW8DxA87XaJfMnTUwJ4HU3BGaF8maeE7EsM6QHd4lHHdZqCwt0YMqXjzDs1fRxM-K1bhMLcVYbB0EwnHXad3-g9-9iOA/s1600/se26.jpg" target="_blank">Sydenham</a>, vastly improving his system for colour television, until his laboratory was made unusable by bomb damage. Alas ill health <a href="https://www.bairdtelevision.com/final.html" target="_blank">caught up with him</a> and he died after a stroke at the age of 57, just one week after the BBC restarted television broadcasts in 1946. You can't see his final home in Bexhill because it was replaced by a block of flats in <a href="http://news.bbc.co.uk/1/hi/england/southern_counties/4330160.stm" target="_blank">2005</a>, but Baird does have an impressive number of plaques across <a href="https://londonist.com/london/history/john-logie-baird-why-so-many-plaques" target="_blank">central London</a> and <a href="http://diamondgeezer.blogspot.com/2026/01/26-things-to-see-in-se26.html" target="_blank">SE26</a>.
</p><p>
It's not always easy being first, and after early televisual success <a href="https://www.bairdtelevision.com/mobileindex.html" target="_blank">John Logie Baird</a> saw his star wane and fade. But it's still him we remember for making possible one of the key transformative inventions of the 20th century, even though barely anyone watched his first efforts. It took ten years to get from Stooky Bill to BBC TV's opening night, then another two decades before the widespread adoption of TV sets in British households and two more until colour television took hold. But 100 years on almost all of us have a TV set at home and effectively another in our pocket, and all because a Scotsman came to London and cleverly spun some discs.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google AI Overviews cite YouTube more than any medical site for health queries (345 pts)]]></title>
            <link>https://www.theguardian.com/technology/2026/jan/24/google-ai-overviews-youtube-medical-citations-study</link>
            <guid>46766031</guid>
            <pubDate>Mon, 26 Jan 2026 14:27:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/technology/2026/jan/24/google-ai-overviews-youtube-medical-citations-study">https://www.theguardian.com/technology/2026/jan/24/google-ai-overviews-youtube-medical-citations-study</a>, See on <a href="https://news.ycombinator.com/item?id=46766031">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Google‚Äôs search feature AI Overviews cites YouTube more than any medical website when answering queries about health conditions, according to research that <a href="https://www.theguardian.com/technology/ng-interactive/2026/jan/24/how-the-confident-authority-of-google-ai-overviews-is-putting-public-health-at-risk" data-link-name="in body link">raises fresh questions</a> about a tool seen by 2 billion people each month.</p><p>The company has said its AI summaries, which appear at the top of search results and use generative AI to answer questions from users, are <a href="https://search.google/intl/en-GB/ways-to-search/ai-overviews/" data-link-name="in body link">‚Äúreliable‚Äù</a> and cite reputable medical sources such as the Centers for Disease Control and Prevention and the Mayo Clinic.</p><p>However, a study that analysed responses to more than 50,000 health queries, captured using Google searches from Berlin, found the top cited source was <a href="https://www.theguardian.com/technology/youtube" data-link-name="in body link" data-component="auto-linked-tag">YouTube</a>. The video-sharing platform is the world‚Äôs second most visited website, after Google itself, and is owned by Google.</p><p>Researchers at SE Ranking, a search engine optimisation platform, found YouTube made up 4.43% of all AI Overview citations. No hospital network, government health portal, medical association or academic institution came close to that number, they said.</p><p>‚ÄúThis matters because YouTube is not a medical publisher,‚Äù <a href="https://seranking.com/blog/health-ai-overviews-youtube-vs-medical-sites/" data-link-name="in body link">the researchers wrote</a>. ‚ÄúIt is a general-purpose video platform. Anyone can upload content there (eg board-certified physicians, hospital channels, but also wellness influencers, life coaches, and creators with no medical training at all).‚Äù</p><p>Google told the Guardian that AI Overviews was designed to surface high-quality content from reputable sources, regardless of format, and a variety of credible health authorities and licensed medical professionals created content on YouTube. The study‚Äôs findings could not be extrapolated to other regions as it was conducted using German-language queries in Germany, it said.</p><p>The research comes after <a href="https://www.theguardian.com/technology/2026/jan/02/google-ai-overviews-risk-harm-misleading-health-information" data-link-name="in body link">a Guardian investigation</a> found people were being put at risk of harm by false and misleading health information in Google AI Overviews responses.</p><p>In one case that experts said was ‚Äúdangerous‚Äù and ‚Äúalarming‚Äù, Google provided bogus information about crucial liver function tests that could have left people with serious liver disease wrongly thinking they were healthy. The company later <a href="https://www.theguardian.com/technology/2026/jan/11/google-ai-overviews-health-guardian-investigation" data-link-name="in body link">removed AI Overviews</a> for some but not all medical searches.</p><p>The SE Ranking study analysed 50,807 healthcare-related prompts and keywords to see which sources AI Overviews relied on when generating answers.</p><p>They chose Germany because its healthcare system is strictly regulated by a mix of German and EU directives, standards and safety regulations. ‚ÄúIf AI systems rely heavily on non-medical or non-authoritative sources even in such an environment, it suggests the issue may extend beyond any single country,‚Äù <a href="https://seranking.com/blog/health-ai-overviews-youtube-vs-medical-sites/" data-link-name="in body link">they wrote</a>.</p><p>AI Overviews surfaced on more than 82% of health searches, the researchers said. When they looked at which sources AI Overviews relied on most often for health-related answers, one result stood out immediately, they said. The single most cited domain was YouTube with 20,621 citations out of a total of 465,823.</p><figure id="185bbe1c-e0e1-4cd8-8c9e-da694e2068bd" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/8bbe1e1007327f2ddc6b4b808fea51afcfa8b317/0_0_4888_3259/master/4888.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/8bbe1e1007327f2ddc6b4b808fea51afcfa8b317/0_0_4888_3259/master/4888.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/8bbe1e1007327f2ddc6b4b808fea51afcfa8b317/0_0_4888_3259/master/4888.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/8bbe1e1007327f2ddc6b4b808fea51afcfa8b317/0_0_4888_3259/master/4888.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/8bbe1e1007327f2ddc6b4b808fea51afcfa8b317/0_0_4888_3259/master/4888.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/8bbe1e1007327f2ddc6b4b808fea51afcfa8b317/0_0_4888_3259/master/4888.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="YouTube logo displayed on a mobile phone and on a laptop screen" src="https://i.guim.co.uk/img/media/8bbe1e1007327f2ddc6b4b808fea51afcfa8b317/0_0_4888_3259/master/4888.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="296.6970130932897" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Researchers at SE Ranking found YouTube made up 4.43% of all AI Overview citations. </span> Photograph: Adam Vaughan/EPA</figcaption></figure><p>The next most cited source was <a href="https://ndr.de/" data-link-name="in body link">NDR.de</a>, with 14,158 citations (3.04%). The German public broadcaster produces health-related content alongside news, documentaries and entertainment. In third place was a medical reference site, <a href="https://msdmanuals.com/" data-link-name="in body link">Msdmanuals.com</a> with 9,711 citations (2.08%).</p><p>The fourth most cited source was Germany‚Äôs largest consumer health portal, <a href="https://netdoktor.de/" data-link-name="in body link">Netdoktor.de</a>, with 7,519 citations (1.61%). The fifth most cited source was a career platform for doctors, <a href="https://praktischarzt.de/" data-link-name="in body link">Praktischarzt.de</a>, with 7,145 citations (1.53%).</p><p>The researchers acknowledged limitations to their study. It was conducted as a one-time snapshot in December 2025, using German-language queries that reflected how users in Germany typically search for health information.</p><p>Results could vary over time, by region, and by the phrasing of questions. However, even with those caveats, the findings still prompted alarm.</p><p>Hannah van Kolfschooten, a researcher specialising in AI, health and law at the University of Basel who was not involved with the research, said: ‚ÄúThis study provides empirical evidence that the risks posed by AI Overviews for health are structural, not anecdotal. It becomes difficult for Google to argue that misleading or harmful health outputs are rare cases.</p><p>‚ÄúInstead, the findings show that these risks are embedded in the way AI Overviews are designed. In particular, the heavy reliance on YouTube rather than on public health authorities or medical institutions suggests that visibility and popularity, rather than medical reliability, is the central driver for health knowledge.‚Äù</p><p>A Google spokesperson said: ‚ÄúThe implication that AI Overviews provide unreliable information is refuted by the report‚Äôs own data, which shows that the most cited domains in AI Overviews are reputable websites. And from what we‚Äôve seen in the published findings, AI Overviews cite expert YouTube content from hospitals and clinics.‚Äù</p><p>Google said the study showed that of the 25 most cited YouTube videos, 96% were from medical channels. However, the researchers cautioned that these videos represented fewer than 1% of all the YouTube links cited by AI Overviews on health.</p><p>‚ÄúMost of them (24 out of 25) come from medical-related channels like hospitals, clinics and health organisations,‚Äù <a href="https://seranking.com/blog/health-ai-overviews-youtube-vs-medical-sites/" data-link-name="in body link">the researchers wrote</a>. ‚ÄúOn top of that, 21 of the 25 videos clearly note that the content was created by a licensed or trusted source.</p><p>‚ÄúSo at first glance it looks pretty reassuring. But it‚Äôs important to remember that these 25 videos are just a tiny slice (less than 1% of all YouTube links AI Overviews actually cite). With the rest of the videos, the situation could be very different.‚Äù</p><figure id="a038a8db-db52-4588-9d3c-8353144d84f0" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.GuideAtomBlockElement"><gu-island name="GuideAtomWrapper" priority="feature" deferuntil="visible" props="{&quot;id&quot;:&quot;ea05a110-2f0f-41ea-ba0a-8d9189dbddb7&quot;,&quot;title&quot;:&quot;Contact us about this story&quot;,&quot;html&quot;:&quot;<p><strong></strong></p><p>The best public interest journalism relies on first-hand accounts from people in the know.</p><p></p><p>If you have something to share on this subject, you can contact us confidentially using the following methods.</p><p><strong>Secure Messaging in the Guardian app</strong></p><p>The Guardian app has a tool to send tips about stories. Messages are end to end encrypted and concealed within the routine activity that every Guardian mobile app performs. This prevents an observer from knowing that you are communicating with us at all, let alone what is being said.</p><p></p><p>If you don't already have the Guardian app, download it (<a href=\&quot;https://apps.apple.com/app/the-guardian-live-world-news/id409128287\&quot;>iOS</a>/<a href=\&quot;https://play.google.com/store/apps/details?id=com.guardian\&quot;>Android</a>) and go to the menu. Select ‚ÄòSecure Messaging‚Äô. </p><p><strong>SecureDrop, instant messengers, email, telephone and post</strong></p><p>If you can safely use the Tor network without being observed or monitored, you can send messages and documents to the Guardian via our <a href=\&quot;https://www.theguardian.com/securedrop\&quot;>SecureDrop platform</a>.</p><p></p><p>Finally, our guide at <a href=\&quot;https://www.theguardian.com/tips\&quot;>theguardian.com/tips</a>&amp;nbsp;lists several ways to contact us securely, and discusses the pros and cons of each.&amp;nbsp;</p>&quot;,&quot;image&quot;:&quot;https://i.guim.co.uk/img/media/ae475ccca7c94a4565f6b500a485479f08098383/788_0_4000_4000/4000.jpg?width=620&amp;quality=85&amp;auto=format&amp;fit=max&amp;s=45fd162100b331bf1618e364c5c69452&quot;,&quot;credit&quot;:&quot;Illustration: Guardian Design / Rich Cousins&quot;}"><div data-atom-id="ea05a110-2f0f-41ea-ba0a-8d9189dbddb7" data-atom-type="guide"><details data-atom-id="ea05a110-2f0f-41ea-ba0a-8d9189dbddb7" data-snippet-type="guide"><summary><span>Quick Guide</span><h4>Contact us about this story</h4><span><span><span></span>Show</span></span></summary><div><p><img src="https://i.guim.co.uk/img/media/ae475ccca7c94a4565f6b500a485479f08098383/788_0_4000_4000/4000.jpg?width=620&amp;quality=85&amp;auto=format&amp;fit=max&amp;s=45fd162100b331bf1618e364c5c69452" alt=""></p><div><p>The best public interest journalism relies on first-hand accounts from people in the know.</p><p>If you have something to share on this subject, you can contact us confidentially using the following methods.</p><p><strong>Secure Messaging in the Guardian app</strong></p><p>The Guardian app has a tool to send tips about stories. Messages are end to end encrypted and concealed within the routine activity that every Guardian mobile app performs. This prevents an observer from knowing that you are communicating with us at all, let alone what is being said.</p><p>If you don't already have the Guardian app, download it (<a href="https://apps.apple.com/app/the-guardian-live-world-news/id409128287">iOS</a>/<a href="https://play.google.com/store/apps/details?id=com.guardian">Android</a>) and go to the menu. Select ‚ÄòSecure Messaging‚Äô. </p><p><strong>SecureDrop, instant messengers, email, telephone and post</strong></p><p>If you can safely use the Tor network without being observed or monitored, you can send messages and documents to the Guardian via our <a href="https://www.theguardian.com/securedrop">SecureDrop platform</a>.</p><p>Finally, our guide at <a href="https://www.theguardian.com/tips">theguardian.com/tips</a>&nbsp;lists several ways to contact us securely, and discusses the pros and cons of each.&nbsp;</p></div><div><p>Illustration: Guardian Design / Rich Cousins</p></div></div></details></div></gu-island></figure></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple introduces new AirTag with longer range and improved findability (233 pts)]]></title>
            <link>https://www.apple.com/newsroom/2026/01/apple-introduces-new-airtag-with-expanded-range-and-improved-findability/</link>
            <guid>46765819</guid>
            <pubDate>Mon, 26 Jan 2026 14:10:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/newsroom/2026/01/apple-introduces-new-airtag-with-expanded-range-and-improved-findability/">https://www.apple.com/newsroom/2026/01/apple-introduces-new-airtag-with-expanded-range-and-improved-findability/</a>, See on <a href="https://news.ycombinator.com/item?id=46765819">Hacker News</a></p>
<div id="readability-page-1" class="page">


	
    







 
<nav id="ac-localnav" lang="en-US" role="navigation" aria-label="Newsroom" data-analytics-region="local nav" data-sticky="">
	
    
    
        




    
    
    
	
	

</nav>





<main id="main" role="main"> 




<span id="opens-in-new-window">opens in new window</span>
<section>
<article data-analytics-activitymap-region-id="article">






    
    
    









    





    <div>
        
		
        

        <div>
                
                
                
                    <h2>
                        
    
        Apple introduces new <p>AirTag with expanded </p><p>connectivity range </p><p>and improved findability
    

                    </p></h2>
                
            </div>

        <div>
                
                
                    The next generation of AirTag ‚Äî the bestselling item finder ‚Äî is even easier to locate with more powerful Precision Finding, a&nbsp;longer Bluetooth range, and a louder speaker
                
            </div>

        
            
    
    
    
    
    

        

    </div>







    
    
    


    
        
        
        
        
            <figure aria-label="Media, The new AirTag with the FineWoven Key Ring in fox orange.">
                <div>
                         
                            
                            <div>
                                Apple introduced the new AirTag, now with an expanded finding range and a louder speaker.
                            </div>
                        
                        
                        
                        
                        <a href="https://www.apple.com/newsroom/images/2026/01/apple-introduces-new-airtag-with-expanded-range-and-improved-findability/article/Apple-AirTag-FineWoven-Key-Ring.zip" download="" data-analytics-title="download image - Apple-AirTag-FineWoven-Key-Ring_inline" aria-label="Download media, The new AirTag with the FineWoven Key Ring in fox orange."></a>
                    </div>
            </figure>
        
    












    
    
    


     
     
    
    
        <div>Apple today unveiled the new <a href="https://www.apple.com/airtag/" target="_blank">AirTag</a>, a powerful accessory that helps users keep track of and find the items that matter most with Apple‚Äôs Find My app ‚Äî now with an expanded finding range and a louder speaker. Powered by the strength of the Find My network, AirTag allows users to keep tabs on their belongings every single day. Since the launch of AirTag in 2021, users from around the world have shared stories of being reunited with lost luggage, keys, bicycles, bags, and more. With the help of AirTag placed inside an instrument case, a musician was able to locate their lost instrument and perform that evening, while another user was able to find lost luggage that contained a lifesaving medication. AirTag is designed exclusively for tracking objects and offers industry-leading protections against unwanted tracking. It is available today for the same price as its predecessor: $29 for a single AirTag and $99 for a four-pack, with free personalized engraving available on <a href="http://apple.com/" target="_blank">apple.com</a> and the Apple Store app.
</div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="airtag-find-my">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-15e93aecec99c44fef2edbf1688e8d91" href="#gallery-15e93aecec99c44fef2edbf1688e8d91" data-ac-gallery-trigger="gallery-15e93aecec99c44fef2edbf1688e8d91"><span>iPhone 17 Pro shows a list of a user‚Äôs items alongside a map.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-1e02922a91356997ffdc0be2fa25a7cd" href="#gallery-1e02922a91356997ffdc0be2fa25a7cd" data-ac-gallery-trigger="gallery-1e02922a91356997ffdc0be2fa25a7cd"><span>iPhone 17 Pro shows an overview of a user named Royce‚Äôs backpack, including its location on a map.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-15e93aecec99c44fef2edbf1688e8d91" aria-labelledby="gallery-dotnav-15e93aecec99c44fef2edbf1688e8d91" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:items">
                                
                                <div>
                                    <div>Powered by the strength of the Find My network, AirTag allows users to keep tabs on their belongings every single day, from keys and wallets to backpacks, luggage, and more.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2026/01/apple-introduces-new-airtag-with-expanded-range-and-improved-findability/article/Apple-AirTag-Find-My-items-01.zip" download="" data-analytics-title="download image - Apple-AirTag-Find-My-items-01_inline" aria-label="Download media, iPhone 17 Pro shows a list of a user‚Äôs items alongside a map."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-1e02922a91356997ffdc0be2fa25a7cd" aria-labelledby="gallery-dotnav-1e02922a91356997ffdc0be2fa25a7cd" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:royces-backpack">
                                
                                <div>
                                    <div>Powered by the strength of the Find My network, AirTag allows users to keep tabs on their belongings every single day, from keys and wallets to backpacks, luggage, and more.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2026/01/apple-introduces-new-airtag-with-expanded-range-and-improved-findability/article/Apple-AirTag-Find-My-items-02.zip" download="" data-analytics-title="download image - Apple-AirTag-Find-My-items-02_inline" aria-label="Download media, iPhone 17 Pro shows an overview of a user named Royce‚Äôs backpack, including its location on a map."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Enhanced Range and Findability</strong>
</h2>
                 
             
                 <div>Apple‚Äôs second-generation Ultra Wideband chip ‚Äî the same chip found in the iPhone 17 lineup, iPhone Air, Apple Watch Ultra 3, and Apple Watch Series 11 ‚Äî powers the new AirTag, making it easier to locate than ever before. Using haptic, visual, and audio feedback, Precision Finding guides users to their lost items from up to 50 percent farther away than the previous generation.<sup>1</sup> And an upgraded Bluetooth chip expands the range at which items can be located. For the first time, users can use Precision Finding on Apple Watch Series 9 or later, or Apple Watch Ultra 2 or later, to find their AirTag, bringing a powerful experience to the wrist.
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="enhanced-range-findability">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-cd66f894227af0cb51e48153763fb4a7" href="#gallery-cd66f894227af0cb51e48153763fb4a7" data-ac-gallery-trigger="gallery-cd66f894227af0cb51e48153763fb4a7"><span>A user‚Äôs Apple Watch shows an item labeled ‚ÄúDebra‚Äôs Keys‚Äù is 20 feet away.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-e6dbfac545be2b80d70b52c772139a7d" href="#gallery-e6dbfac545be2b80d70b52c772139a7d" data-ac-gallery-trigger="gallery-e6dbfac545be2b80d70b52c772139a7d"><span>A user‚Äôs Apple Watch shows a green screen shows that the user is four feet away from an item labeled ‚ÄúFamily Car Keys.‚Äù</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-cd66f894227af0cb51e48153763fb4a7" aria-labelledby="gallery-dotnav-cd66f894227af0cb51e48153763fb4a7" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:twenty-feet">
                                
                                <div>
                                    <div>Precision Finding guides users through haptic, visual, and audio feedback to their lost items from up to 50 percent farther away than the previous generation, and an updated Bluetooth chip expands the range at which items can be located.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2026/01/apple-introduces-new-airtag-with-expanded-range-and-improved-findability/article/Apple-AirTag-Find-My-Precision-Finding-01.zip" download="" data-analytics-title="download image - Apple-AirTag-Find-My-Precision-Finding-01_inline" aria-label="Download media, A user‚Äôs Apple Watch shows an item labeled ‚ÄúDebra‚Äôs Keys‚Äù is 20 feet away."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-e6dbfac545be2b80d70b52c772139a7d" aria-labelledby="gallery-dotnav-e6dbfac545be2b80d70b52c772139a7d" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:four-feet">
                                
                                <div>
                                    <div>Precision Finding guides users through haptic, visual, and audio feedback to their lost items from up to 50 percent farther away than the previous generation, and an updated Bluetooth chip expands the range at which items can be located.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2026/01/apple-introduces-new-airtag-with-expanded-range-and-improved-findability/article/Apple-AirTag-Find-My-Precision-Finding-02.zip" download="" data-analytics-title="download image - Apple-AirTag-Find-My-Precision-Finding-02_inline" aria-label="Download media, A user‚Äôs Apple Watch shows a green screen shows that the user is four feet away from an item labeled ‚ÄúFamily Car Keys.‚Äù"></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <div>With its updated internal design, the new AirTag is 50 percent louder than the previous generation, enabling users to hear their AirTag from up to 2x farther than before. Paired with its enhanced Precision Finding capabilities and distinctive new chime, AirTag now makes it easier for users to find their important items, such as keys hidden deep in between couch cushions or a wallet as they head out the door.
</div>
                 
             
                 <h2><strong>The Find My Network and Share Item Location</strong>
</h2>
                 
             
                 <div>Find My makes it easy to locate AirTag, Apple devices, and compatible third-party devices, as well as keep up with friends and family, all while protecting user privacy. If AirTag is out of range of its paired iPhone, the Find My network can help track it down. The Find My network is a crowdsourced network of Apple devices that use Bluetooth technology to detect the location of an accessory or device, and report their approximate location back to the owner.
</div>
                 
             
         </div>
 

    
    
    


    
        
        
        
        
            <figure aria-label="Media, A user‚Äôs iPhone 17 Pro shows a screen that reads ‚ÄúShare Item Location‚Äù and gives instructions for finding a lost AirTag. A button at the bottom of the screen reads ‚ÄúContinue.‚Äù">
                <div>
                         
                            
                            <div>
                                Find My makes it easy to locate AirTag, Apple devices, and compatible third-party devices, as well as keep up with friends and family, all while protecting user privacy.
                            </div>
                        
                        
                        
                        
                        <a href="https://www.apple.com/newsroom/images/2026/01/apple-introduces-new-airtag-with-expanded-range-and-improved-findability/article/Apple-AirTag-Share-Item-Location.zip" download="" data-analytics-title="download image - Apple-AirTag-Share-Item-Location_inline" aria-label="Download media, A user‚Äôs iPhone 17 Pro shows a screen that reads ‚ÄúShare Item Location‚Äù and gives instructions for finding a lost AirTag. A button at the bottom of the screen reads ‚ÄúContinue.‚Äù"></a>
                    </div>
            </figure>
        
    












    
    
    


     
     
    
    
        <div>
             
                 <div>The new AirTag integrates seamlessly with Share Item Location, an iOS feature designed to help users recover a misplaced item by temporarily and securely sharing its location with trusted third parties, such as airlines, so they can assist in recovering delayed luggage or other lost items. Apple has partnered directly with more than 50 airlines to privately and securely accept Share Item Location links.
</div>
                 
             
                 <div>With Share Item Location, users can share the location of a misplaced item with a participating airline‚Äôs customer service team. <a href="https://www.sita.aero/pressroom/news-releases/one-year-later-sita-shows-how-integration-of-apples-find-my-share-item-location-can-strengthen-baggage-operations-for-airports-and-airlines/" target="_blank" rel="nofollow" data-analytics-exit-link="">According to SITA</a>, a leading IT provider for airlines, carriers report that using Share Item Location has reduced baggage delays by 26 percent and reduced incidences of ‚Äútruly lost‚Äù or unrecoverable luggage by 90 percent. Access is granted only to authorized personnel via secure Apple Account or partner authentication. The shared location will be disabled as soon as a user is reunited with their item, can be stopped by the owner at any time, and will automatically expire after seven days.
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="share-item-location">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-4fc5a46ffe921ab48b4c2b45aff20aa1" href="#gallery-4fc5a46ffe921ab48b4c2b45aff20aa1" data-ac-gallery-trigger="gallery-4fc5a46ffe921ab48b4c2b45aff20aa1"><span>iPhone 17 Pro shows a map and a screen that reads ‚ÄúWith You,‚Äù and contains a list of items that includes ‚ÄúGuillermo‚Äôs Umbrella‚Äù and ‚ÄúFamily Car Keys.‚Äù</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-522e01b80cafc6c8228290e040d39ad8" href="#gallery-522e01b80cafc6c8228290e040d39ad8" data-ac-gallery-trigger="gallery-522e01b80cafc6c8228290e040d39ad8"><span>iPhone 17 Pro shows an screen that reads ‚ÄúFamily Car Keys‚Äù and offers information on sharing the AirTag‚Äôs location. Two buttons at the bottom of the screen read ‚ÄúContinue‚Äù and ‚ÄúNot Now.‚Äù</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-4fc5a46ffe921ab48b4c2b45aff20aa1" aria-labelledby="gallery-dotnav-4fc5a46ffe921ab48b4c2b45aff20aa1" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:with-you">
                                
                                <div>
                                    <div>Share Item Location is an iOS feature that helps users locate and recover misplaced items by easily and securely sharing the location of an AirTag or Find My network accessory with third parties such as airlines.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2026/01/apple-introduces-new-airtag-with-expanded-range-and-improved-findability/article/Apple-AirTag-Find-My-items-share.zip" download="" data-analytics-title="download image - Apple-AirTag-Find-My-items-share_inline" aria-label="Download media, iPhone 17 Pro shows a map and a screen that reads ‚ÄúWith You,‚Äù and contains a list of items that includes ‚ÄúGuillermo‚Äôs Umbrella‚Äù and ‚ÄúFamily Car Keys.‚Äù"></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-522e01b80cafc6c8228290e040d39ad8" aria-labelledby="gallery-dotnav-522e01b80cafc6c8228290e040d39ad8" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:family-car-keys">
                                
                                <div>
                                    <div>Share Item Location is an iOS feature that helps users locate and recover misplaced items by easily and securely sharing the location of an AirTag or Find My network accessory with third parties such as airlines.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2026/01/apple-introduces-new-airtag-with-expanded-range-and-improved-findability/article/Apple-AirTag-Find-My-Airtag-share.zip" download="" data-analytics-title="download image - Apple-AirTag-Find-My-Airtag-share_inline" aria-label="Download media, iPhone 17 Pro shows an screen that reads ‚ÄúFamily Car Keys‚Äù and offers information on sharing the AirTag‚Äôs location. Two buttons at the bottom of the screen read ‚ÄúContinue‚Äù and ‚ÄúNot Now.‚Äù"></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Industry-Leading Security Features</strong>
</h2>
                 
             
                 <div>The new AirTag is designed from the ground up to keep location data private and secure. AirTag doesn‚Äôt physically store location data or history on device, and end-to-end encryption protects all communication with the Find My network, ensuring that only the owner of a device can access its location data. No one, including Apple, knows the identity or location of any device that helped find it. Designed exclusively for tracking objects, and not people or pets, the new AirTag incorporates a suite of industry-first protections against unwanted tracking, including cross-platform alerts and unique Bluetooth identifiers that change frequently.
</div>
                 
             
         </div>
 

    
    
    


    
        
        
        
        
            <figure aria-label="Media, A user‚Äôs iPhone 17 Pro shows an alert that reads, ‚ÄúAirTag Detected Near You,‚Äù alongside a map and more details on the tracking notification.">
                <div>
                         
                            
                            <div>
                                The new AirTag incorporates a suite of industry-first protections against unwanted tracking, including cross-platform alerts and unique Bluetooth identifiers that change frequently.
                            </div>
                        
                        
                        
                        
                        <a href="https://www.apple.com/newsroom/images/2026/01/apple-introduces-new-airtag-with-expanded-range-and-improved-findability/article/Apple-AirTag-Find-My-Airtag-detected.zip" download="" data-analytics-title="download image - Apple-AirTag-Find-My-Airtag-detected_inline" aria-label="Download media, A user‚Äôs iPhone 17 Pro shows an alert that reads, ‚ÄúAirTag Detected Near You,‚Äù alongside a map and more details on the tracking notification."></a>
                    </div>
            </figure>
        
    












    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Environmental Responsibility and Accessory Compatibility</strong>
</h2>
                 
             
                 <div>Apple 2030 is the company‚Äôs ambitious plan to be carbon neutral across its entire footprint by the end of this decade by reducing product emissions from their three biggest sources: materials, electricity, and transportation. The new AirTag is designed with the environment in mind, with 85 percent recycled plastic in the enclosure, 100 percent recycled rare earth elements in all magnets, and 100 percent recycled gold plating in all Apple-designed printed circuit boards. The paper packaging is 100 percent fiber-based and can be easily recycled. Maintaining the same form factor as the original, the new AirTag is compatible with all existing AirTag accessories, including the FineWoven Key Ring, which is made from 68 percent recycled content and available in five beautiful colors.
</div>
                 
             
         </div>
 

    
    
    


     
     
    
    
        <div>
             
                 
                 
             
                 <div><ul>
<li>The new AirTag is available to order on <a href="http://apple.com/" target="_blank">apple.com</a> and in the Apple Store app today, and will be available at Apple Store locations later this week. The new AirTag will also be available at Apple Authorized Resellers.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Customers can add a personalized engraving to the new AirTag for free during checkout on <a href="https://www.apple.com/" target="_blank">apple.com</a> and the Apple Store app.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>AirTag is available in one- and four-packs for <strong>$29</strong> (U.S.) and <strong>$99</strong> (U.S.), respectively.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>The Apple-designed AirTag FineWoven Key Ring is available in fox orange, midnight purple, navy, moss, and black for <strong>$35 </strong>(U.S.).<sup>2</sup></li>
</ul>
</div>
                 
             
                 <div><ul>
<li>The new AirTag requires a compatible iPhone with iOS 26 or later, or iPad with iPadOS 26 or later. Customers must have an Apple Account and be signed into their iCloud account. Certain features require Find My to be enabled in iCloud settings.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Precision Finding on Apple Watch requires Apple Watch Series 9 or later, or Apple Ultra 2 or later, with watchOS 26.2.1.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    




    
    
        
    


    
    
    



    
    
    




    




    
    
    





    
    
    <div>
            <ol>
<li>Precision Finding is not available in countries and regions where Ultra Wideband technology is restricted. For more information, visit <a href="https://www.apple.com/uwb/" target="_blank">apple.com/uwb</a>.</li>
<li>AirTag accessories, including the AirTag FineWoven Key Ring, are sold separately.</li>
</ol>

        </div>



    
    
    






    















	

		
		
			
























		
		

</article>



</section>
</main>



<div>
            Stay up to date with the latest articles from Apple Newsroom.
        </div>
	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Porting 100k lines from TypeScript to Rust using Claude Code in a month (146 pts)]]></title>
            <link>https://blog.vjeux.com/2026/analysis/porting-100k-lines-from-typescript-to-rust-using-claude-code-in-a-month.html</link>
            <guid>46765694</guid>
            <pubDate>Mon, 26 Jan 2026 13:58:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.vjeux.com/2026/analysis/porting-100k-lines-from-typescript-to-rust-using-claude-code-in-a-month.html">https://blog.vjeux.com/2026/analysis/porting-100k-lines-from-typescript-to-rust-using-claude-code-in-a-month.html</a>, See on <a href="https://news.ycombinator.com/item?id=46765694">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
<p>I read <a href="https://www.linkedin.com/posts/galenh_principal-software-engineer-coreai-microsoft-activity-7407863239289729024-WTzf/">this post</a> ‚ÄúOur strategy is to combine AI <em>and</em> Algorithms to rewrite Microsoft‚Äôs largest codebases [from C++ to Rust]. Our North Star is ‚Äò1 engineer, 1 month, 1 million lines of code.‚Äù and it got me curious, how difficult is it really?</p>



<p>I've long wanted to build a competitive Pokemon battle AI after watching a lot of <a href="https://www.youtube.com/watch?v=qR9nMPr7o5Q">WolfeyVGC</a> and following the <a href="https://pokeagent.github.io/">Pok√©Agent challenge</a> at NeurIPS. Thankfully there's an open source project called "<a href="https://github.com/smogon/pokemon-showdown">Pokemon Showdown</a>" that implements all the rules but it's written in JavaScript which is quite slow to run in a training loop. So my holiday project came to life: let's convert it to Rust using Claude!</p>



<h2>Escaping the sandbox</h2>



<p>Having the AI able to run arbitrary code on your machine is dangerous, so there's a lot of safeguards put in place. But... at the same time, this is what I want to do in this case. So let me walk through the ways I escaped the various sandboxes.</p>



<h3>git push</h3>



<p>Claude runs in a sandbox that limits some operations like ssh access. You need ssh access in order to publish to GitHub. This is very important as I want to be able to check how the AI is doing from my phone while I do some other activities üòâ</p>



<figure><img src="https://blog.vjeux.com/wp-content/uploads/2026/01/image-1.png" alt=""></figure>



<p>What I realized is that I can run the code on my terminal but Claude cannot do it from its own terminal. So what I did was to ask Claude to write a nodejs script that opens an http server on a local port that executes the git commands from the url. Now I just need to keep a tab open on my terminal with this server active and ask Claude to write instructions in Claude.md for it to interact with it.</p>



<h3>rustc</h3>



<p>There's an antivirus on my computer that requires a human interaction when an unknown binary is being ran. Since every time we compile it's a new unknown binary, this wasn't going to work.</p>



<p>What I found is that I can setup a local docker instance and compile + run the code inside of docker which doesn't trigger the antivirus. Again, I asked Claude to generate the right instructions in Claude.md and problem solved.</p>



<p>The next hurdle was to figure out how to let Claude Code for hours without any human intervention.</p>



<h3>--yes</h3>



<p>Claude keeps asking for permission to do things. I tried adding a bunch of things to the allowed commands file and <code>--allow-dangerously-skip-permissions --dangerously-skip-permissions</code>was disabled in my environment (it has now been resolved).</p>



<p>I realized that I could run an AppleScript that presses enter every few seconds in another tab. This way it's going to say Yes to everything Claude asks to do. So far it hasn't decided to hack my computer...</p>



<pre><code>#!/bin/bash

osascript -e \
'tell application "System Events"
    repeat
        delay 5
        key code 36
    end repeat
end tell'</code></pre>



<h3>Never give up</h3>



<p>Claude after working for some time seem to always stop to recap things. I tried prompting it to never do, even threatening it to no avail.</p>



<figure><img src="https://blog.vjeux.com/wp-content/uploads/2026/01/image-2.png" alt=""></figure>



<p>I tried using the Ralph Wiggum loop but it <a href="https://x.com/Vjeux/status/2007545697193890024">couldn't get it to work</a> and apparently I'm not alone.</p>



<p>What ended up working is to copy in my clipboard the task I wanted it to do and to tweak the script above to hit the keys "cmd-v" after pressing enter. This way in case it asks a question the "enter" is being used and in case it's not it's queuing the prompt for when Claude is giving back control.</p>



<h3>Auto-updates</h3>



<p>There are programs on the computer like software updater that can steal the focus from the terminal window, for example showing a modal. Once that happens, then the cmd-v / enter are no longer sent to the terminal and the execution stops.</p>



<figure><img src="https://blog.vjeux.com/wp-content/uploads/2025/12/image.png" alt=""><figcaption><br></figcaption></figure>



<p>I used my trusty Auto Clicker by MurGaa from Minecraft days to simulate a left click every few seconds. I place my terminal on the edge of the screen and same for my mouse so that when a modal appears in the middle, it refocuses the terminal correctly.</p>



<p>It also prevents the computer from going to sleep so that it can run even when I'm not using the laptop or at night.</p>



<h3>Bugs üêú</h3>



<p>Reliability when running things for a long period of time is paramount. Overall it's been a pretty smooth ride but I ran into this specific error during a handful of nights which stopped the process. I hope they get to the bottom of it and solve it as I'm not the only one to <a href="https://github.com/anthropics/claude-code/issues/16368">report it</a>!</p>



<figure><img src="https://blog.vjeux.com/wp-content/uploads/2026/01/image-3.png" alt=""></figure>



<p>This setup is far from optimal but has worked so far. Hopefully this gets streamlined in the future!</p>



<h2>Porting Pokemon</h2>



<h3>One Shot</h3>



<p>At the very beginning, I started with a simple prompt asking Claude to port the codebase and make sure that things are done line by line. At first it felt extremely impressive, it generated thousands of lines of Rust that was compiling.</p>



<p>Sadly it was only an appearance as it took a lot of shortcuts. For example, it created two different structures for what a move is in two different files so that they would both compile independently but didn't work when integrated together. It ported all the functions very loosely where anything that was remotely complicated would not be ported but instead "simplified".</p>



<p>I didn't realize it yet, I got the loop working to have it port more and more code. The issue is that it created wrong abstractions all over the place and kept adding hardcoded code to make whatever it was supposed to fix work. This wasn't going to go anywhere.</p>



<h3>Giving it structure</h3>



<p>At this point I knew that I needed to be a lot more prescriptive for what I wanted out of it. Taking a step back, the end result should have every JavaScript file and every method inside to have a Rust equivalent.</p>



<p>So I asked Claude to write a script that takes all the files and methods in the JavaScript codebase and put comments in the rust codebase with the JavaScript source, next to the Rust methods.</p>



<p>It was really important for it to be a script as even when instructed to copy code over, it would mistranslate JavaScript code. Being deterministic here greatly increased the odds of getting the right results.</p>



<figure><img src="https://blog.vjeux.com/wp-content/uploads/2026/01/image-4.png" alt=""></figure>



<h3>Litte Islands</h3>



<p>The next challenge is that the original files were thousands of lines long, double it with source comments we got to files more than 10k lines long. This causes a ton of issues with the context window where Claude straight up refuses to open the file. So it started reading the file in chunks but without a ton of precision. Also the context grew a lot quicker and compaction became way more frequent.</p>



<p>So I went ahead and split every method into its own file for the Rust version. This dramatically improved the results. For maximal efficiency I would need to do the same for the JavaScript codebase as well but I was too afraid to do it and accidentally change the behavior so decided not to.</p>



<div><figure><img src="https://blog.vjeux.com/wp-content/uploads/2026/01/image-5.png" alt="" width="143" height="305"></figure></div>



<h3>Cleanup</h3>



<p>The process of porting went through two repeating phases. I would give a large task to Claude to do in a loop that would churn on it for a day, and then I would need to spend time cleaning up the places where it went into the wrong direction.</p>



<p>For the cleanup, I still used Claude but gave a lot more specific recommendations. For example, I noticed that it would hardcode moves/abilities/items/... behaviors everywhere in the code when left unchecked, even after explicitly telling it not to. So I would manually look for all these and tell it to move them into the right places.</p>



<p>This is where engineering skills come into play, all my experience building software let me figure out what went wrong and how to fix it. The good part is that I didn't have to do the cleanup myself, Claude was able to do it just fine when directed to.</p>



<h2>Integration</h2>



<h3>Build everything before testing</h3>



<p>So far, I just made sure that the code compiled, but have never actually put all the pieces together to ensure it actually worked. What Claude really wanted was to do a traditional software building strategy where you make "simple" implementations of all of the pieces and then build them up as time goes.</p>



<p>But in our case, all this iteration has already happened for 10 years on the pokemon-showdown codebase. It's counter productive to try and re-learn all these lessons and will unlikely converge the same way. What works better is to port everything at once, and then do the integration at the end once.</p>



<p>I've learned this strategy from working on Skip, a compiler. For years all the building blocks were built independently and then it all came together with nothing to show for but within a month at the end it all worked. I was so shocked.</p>



<h3>End-to-end test</h3>



<p>Once most of the codebase was ported one to one, I started putting it all together. The good thing is that we can run and edit the code in JavaScript and in Rust, and the input/output is very simple and standardized: list of pokemons with their options (moves, items, nature, iv/ev spread...) and then the list of actions at each step (moves and switches). Given the same random sequence, it'll advance the state the same way.</p>



<p>Now I can let Claude generate this testing harness and go through all the issues one by one. Impressively, it was able to figure out all issues and fix them.</p>



<figure><img src="https://blog.vjeux.com/wp-content/uploads/2026/01/image.png" alt=""></figure>



<p>Over the course of 3 weeks it averaged fixing one issue every 20 minutes or so. It fixed hundreds of issues on its own. I never intervened, it was only a matter of time before it fixed every issue that it encountered.</p>



<div><figure><img src="https://pbs.twimg.com/media/G-rvZkbXYAAoWbm?format=jpg&amp;name=large" alt="Image" width="259" height="529"></figure></div>



<h3>Giving it structure</h3>



<p>At the beginning, this process was extremely slow. Every time a compaction happened, Claude became "dumb" again and reinvented the wheel, writing down tons of markdown files and test scripts along the way. Or Claude decided to take the easy way out and just generate tons of tests but never actually making them match with JavaScript.</p>



<p>So, I started looking at what it did well and encoding it. For example, it added a lot of debugging around the PRNG steps and what actions happened at every turn with all the debugging metadata. So I asked it to create a single test script to print down this information for a single step and to print stack traces. Then add instruction to the Claude.md file. This way every investigation started right away.</p>



<h3>The long slog</h3>



<p>I built used the existing random number generator to generate battles and could put in a number as a seed. This let me generate consistent battles at an increasing size.</p>



<p>I started fixing the first 100 battles, then 1000, 10k, 100k and I'm almost done solving all the issues for the first 2.4 million battles! I'm not sure how many more issues there are but the good thing is that they are getting smaller and smaller as the batch size increases.</p>



<figure><img src="https://blog.vjeux.com/wp-content/uploads/2026/01/image-6.png" alt=""></figure>



<h3>Types of issues</h3>



<p>There are two broad classes of issues that were fixed. The first one that I expected is that Rust has different constraints than JavaScript which need to be taken into account and lead to bugs:</p>



<ul><li>Rust has the "borrow checker" where a mutable variable cannot be passed in two different contexts at once. The problem is that "Pokemon" and "Battle" have references to each others. So there's a lot of workarounds like doing copies, passing indices instead of the object, providing functions with mutable object as callback...</li><li>The JavaScript codebase uses dynamism heavily where some function return '', undefined, null, 0, 1, 5.2, Pokemon... which all are handled with different behaviors. At first the rust port started using Option&lt;&gt; to handle many of them but then moved to structs with all these variants.</li><li>Rust doesn't support optional arguments so every argument has to be spelled out literally.</li></ul>



<p>But the second one are due to itself... Claude Code is like a smart student that is trying to find every opportunity to avoid doing the hard work and take the easy way out if it thinks it can get away with it.</p>



<ul><li>If a fix requires changing more than one or two files, this is a "significant infrastructure" and Claude Code will refuse to do it unless explicitly prompted and will put in whatever hacks it can to make the specific test work.</li><li>Along the same lines, it is going to implement "simplified" versions of things. For some methods, it was better to delete everything and asking it to port it over from scratch than trying to fix all the existing code it created.</li><li>The JavaScript comments are supposed to be the source of truth. But Claude is not above changing the original code if it feels like this is the way to solve the problem...</li><li>If given a list of tasks, it's going to avoid doing the ones that seem difficult until it is absolutely forced to. This is inefficient if not careful as it's going to keep spending time investigating and then skipping all the "hard" ones. Compaction is basically wiping all its memory.</li></ul>



<h2>Prompts</h2>



<p>I didn't write a single line of code myself in this project. I alternated between "co-op" where I work with Claude interactively during the day and creating a job for it to run overnight. I'll focus on the night ones for this section.</p>



<h3>Conversion</h3>



<p>For the first phase of the project, I mostly used variations of this one. Asking it to go through all the big files one by one and implement them faithfully (it didn't really follow instructions as we've seen later...)</p>



<blockquote><p>Open BATTLE_TODO.md to get the list of all the methods in both battle*.rs.<br>Inspect every single one of them and make sure that they are a direct translation the JavaScript file. If there's a method with the same name, the JavaScript definition will be in the comment.<br>If there's no JavaScript definition, question whether this method should be there in the rust version. Our goal is to follow as closely as possible the JavaScript version to avoid any bugs in translation. If you notice that the implementation doesn't match, do all the refactoring needed to match 1 to 1.<br>This will be a complex project. You need to go through all the methods one by one, IN ORDER. YOU CANNOT skip a method because it is too hard or would requiring building new infrastructure. We will call this in a loop so spend as much time as you need building the proper infrastructure to make it 1 to 1 with the JavaScript equivalent. Do not give up.<br>Update BATTLE_TODO.md and do a git commit after each unit of work.<br></p></blockquote>



<h3>Todos</h3>



<p>Claude Code while porting the methods one by one often decided to write a "simplified" version or add a "TODO" for later. I also found it to be useful when generating work to add the instructions in the codebase itself via a TODO comment, so I don't need to wish that it's going to be read from the context.</p>



<p>The master md file in practice didn't really work, it quickly became too big to be useful and Claude started creating a bunch more littering the repo with them. Instead I gave it a deterministic way to go through then by calling grep on the codebase, so it knew when to find them.</p>



<blockquote><p>We want to fix every TODO in the codebase. <code>TODO</code> or <code>simplif</code> in pokemon-showdown-rs/.<br>There are hundreds of them, so go diligently one by one. Do not skip them even if they are difficult. I will call this prompt again and again so you don't need to worry about taking too long on any single one.<br>The port must be exactly one to one. If the infrastructure doesn't exist, please implement it. Do not invent anything.<br>Make sure it still compiles after each addition and commit and push to git.</p></blockquote>



<p>At some point the context was poisoned where a TODO was inside of the original js codebase so it changed it to something else which made sense. But then it did the same for all the subsequent TODOs which didn't... Thankfully I could just revert all these commits.</p>



<h3>Fixing</h3>



<p>I put in all the instructions to debug in Claude.md and a script to run all the tests which outputs a txt file with progress report. This way Claude was able to just keep going fixing issues after issues.</p>



<blockquote><p>We want to fix all the divergences in battles. Please look at 500-seeds-results.txt and fix them one by one. The only way you can fix is by making sure that the differences between javascript and rust are explained by language differences and not logic. Every line between the two must match one by one. If you fixed something specific, it's probably a larger issue, spend the time to figure out if other similar things are broken and do the work to do the larger infrastructure fixes. Make sure it still compiles after each addition and commit and push to git. Check if there are other parts of the codebase that make this mistake.</p></blockquote>



<p>This is really useful to have this txt file diff committed to GitHub to get a sense of progress on the go!</p>



<div><figure><img src="https://pbs.twimg.com/media/G-cKrxvaoAAGAYA?format=jpg&amp;name=large" alt="Image" width="231" height="499"></figure></div>



<h2>Epilogue</h2>



<h3>It works ü§Ø</h3>



<p>I didn't quite know what to expect coming into this project. They usually tend to die due to the sheer amount of work needed to get anywhere close to something complete. But not this time!</p>



<p>We have a complete implementation of Pokemon battle system that produces the same results as the existing JavaScript codebase*. This was done through 5000 commits in 4 weeks and the Rust codebase is around 100k lines of code.</p>



<p>*I wish we had 0 divergences but right now there are 80 out of the first 2.4 million seeds or 0.003%. I need to run it for longer to solve these.</p>



<h3>Is it fast?</h3>



<p>The whole point of the project was for it to be faster than the initial JavaScript implementation. Only towards the end of the project where we had a sizable amount of battles running perfectly I felt like it would be a fair time to do a performance comparison.</p>



<p>I asked Claude Code to parallelize both implementations and was relieved by the results, the Rust port is actually significantly faster, I didn't spend all this time for nothing!</p>



<figure><img src="https://pbs.twimg.com/media/G-zV6MgWAAAQMt4?format=jpg&amp;name=large" alt="Image"></figure>



<p>I've tried asking Claude to optimize it further, it created <a href="https://github.com/vjeux/pokemon-showdown-rs/blob/master/PERFORMANCE.md">a plan that looks reasonable</a> (I've never interacted with Rust in my life) and it spent a day building many of these optimizations but at the end of the day, none of them actually improved the runtime and some even made it way worse.</p>



<p>This is a good example of how experience and expertise is still very required in order to get the best out of LLMs.</p>



<h2>Conclusion</h2>



<p>This is pretty wild that I was able to port a ~100k lines codebase from JavaScript to Rust in two weeks on my own with Claude Code running 24 hours a day for a month <a href="https://github.com/vjeux/pokemon-showdown-rs/commits/master/">creating 5k commits</a>! I have never written any line of Rust before in my life.</p>



<p>LLM-based coding agents are such a great new tool for engineers, there's no way I would have been able to do that without Claude Code. That said, it still feels like a tool that requires my engineering expertise and constant babysitting to produce these results.</p>



<p>Sadly I didn't get to build the Pokemon Battle AI and the winter break is over, so if anybody wants to do it, please <a href="https://github.com/vjeux/pokemon-showdown-rs/commits/master/">have fun with the codebase</a>!</p>
			  
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[After two years of vibecoding, I'm back to writing by hand (613 pts)]]></title>
            <link>https://atmoio.substack.com/p/after-two-years-of-vibecoding-im</link>
            <guid>46765460</guid>
            <pubDate>Mon, 26 Jan 2026 13:36:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://atmoio.substack.com/p/after-two-years-of-vibecoding-im">https://atmoio.substack.com/p/after-two-years-of-vibecoding-im</a>, See on <a href="https://news.ycombinator.com/item?id=46765460">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Most people‚Äôs journey with AI coding starts the same: you give it a simple task. You‚Äôre impressed. So you give it a large task. You‚Äôre even more impressed.</p><p>You open X and draft up a rant on job displacement.</p><p>If you‚Äôve persisted past this point: congratulations, you understand AI coding better than 99% of people.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Mwtn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbc2a288-45bf-4d42-b61d-07f5678d260f_1536x1024.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Mwtn!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbc2a288-45bf-4d42-b61d-07f5678d260f_1536x1024.png 424w, https://substackcdn.com/image/fetch/$s_!Mwtn!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbc2a288-45bf-4d42-b61d-07f5678d260f_1536x1024.png 848w, https://substackcdn.com/image/fetch/$s_!Mwtn!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbc2a288-45bf-4d42-b61d-07f5678d260f_1536x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!Mwtn!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbc2a288-45bf-4d42-b61d-07f5678d260f_1536x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Mwtn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbc2a288-45bf-4d42-b61d-07f5678d260f_1536x1024.png" width="1456" height="971" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cbc2a288-45bf-4d42-b61d-07f5678d260f_1536x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:302809,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://atmoio.substack.com/i/185837386?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbc2a288-45bf-4d42-b61d-07f5678d260f_1536x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!Mwtn!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbc2a288-45bf-4d42-b61d-07f5678d260f_1536x1024.png 424w, https://substackcdn.com/image/fetch/$s_!Mwtn!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbc2a288-45bf-4d42-b61d-07f5678d260f_1536x1024.png 848w, https://substackcdn.com/image/fetch/$s_!Mwtn!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbc2a288-45bf-4d42-b61d-07f5678d260f_1536x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!Mwtn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbc2a288-45bf-4d42-b61d-07f5678d260f_1536x1024.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Serious engineers using AI to do real work and not just weekend projects largely also follow a predictable development arc.</p><p>Still amazed at the big task you gave it, you wonder if you can keep giving it bigger and bigger tasks. Maybe even that haunting refactor no one wants to take on?</p><p>But here‚Äôs where the curtain starts to crinkle.</p><p>On the one hand, you‚Äôre amazed at how well it seems to understand you. On the other hand, it makes frustrating errors and decisions that clearly go against the shared understanding you‚Äôve developed.</p><p>You quickly learn that being angry at the model serves no purpose, so you begin to internalize any unsatisfactory output.</p><p>‚ÄúIt‚Äôs me. My prompt sucked. It was under-specified.‚Äù  </p><p>‚ÄúIf I can specify it, it can build it. The sky‚Äôs the limit,‚Äù you think.</p><p>So you open Obsidian and begin drafting beefy spec docs that describe the feature in your head with impressive detail. Maybe you‚Äôve put together a full page of a prompt, and spent half an hour doing so.</p><p><span>But you find that spec-driven development doesn‚Äôt work either. In real life, design docs and specs are </span><strong>living </strong><span>documents that evolve in a volatile manner through discovery and implementation. Imagine if in a real company you wrote a design doc in 1 hour for a complex architecture, handed it off to a mid-level engineer (and told him not to discuss the doc with anyone), and took off on vacation.</span></p><p>Not only does an agent not have the ability to evolve a specification over a multi-week period as it builds out its lower components, it also makes decisions upfront that it later doesn‚Äôt deviate from. And most agents simply surrender once they feel the problem and solution has gotten away from them (though this rarely happens anymore, since agents will just force themselves through the walls of the maze.)</p><p>What‚Äôs worse is code that agents write looks plausible and impressive while it‚Äôs being written and presented to you. It even looks good in pull requests (as both you and the agent are well trained in what a ‚Äúgood‚Äù pull request looks like).</p><p>It‚Äôs not until I opened up the full codebase and read its latest state cover to cover that I began to see what we theorized and hoped was only a diminishing artifact of earlier models: slop.</p><p>It was pure, unadulterated slop. I was bewildered. Had I not reviewed every line of code before admitting it? Where did all this...gunk..come from?</p><p>In retrospect, it made sense. Agents write units of changes that look good in isolation. They are consistent with themselves and your prompt. But respect for the whole, there is not. Respect for structural integrity there is not. Respect even for neighboring patterns there was not.</p><p>The AI had simply told me a good story. Like vibewriting a novel, the agent showed me a good couple paragraphs that sure enough made sense and were structurally and syntactically correct. Hell, it even picked up on the idiosyncrasies of the various characters. But for whatever reason, when you read the whole chapter, it‚Äôs a mess. It makes no sense in the overall context of the book and the preceding and proceeding chapters.</p><p>After reading months of cumulative highly-specified agentic code, I said to myself: I‚Äôm not shipping this shit. I‚Äôm not gonna charge users for this. And I‚Äôm not going to promise users to protect their data with this.</p><p>I‚Äôm not going to lie to my users with this.</p><p><span>So I‚Äôm back to writing by hand for most things. Amazingly, I‚Äôm faster, more accurate, more creative, more productive, and more efficient than AI, when you price </span><em>everything </em><span>in, and not just code tokens per hour.</span></p><p><span>You can follow me on X </span><a href="https://x.com/atmoio" rel="nofollow ugc noopener">@atmoio</a><span>, where I post a few times a week about agentic coding.</span></p><p>You can watch the video counterpart to this essay on YouTube: </p><div id="youtube2-SKTsNV41DYg" data-attrs="{&quot;videoId&quot;:&quot;SKTsNV41DYg&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/SKTsNV41DYg?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div></div></article></div><div><div id="discussion"><h4>Discussion about this post</h4></div><div><h3>Ready for more?</h3></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vibe Coding Kills Open Source (295 pts)]]></title>
            <link>https://arxiv.org/abs/2601.15494</link>
            <guid>46765120</guid>
            <pubDate>Mon, 26 Jan 2026 13:01:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2601.15494">https://arxiv.org/abs/2601.15494</a>, See on <a href="https://news.ycombinator.com/item?id=46765120">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2601.15494">View PDF</a>
    <a href="https://arxiv.org/html/2601.15494v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Generative AI is changing how software is produced and used. In vibe coding, an AI agent builds software by selecting and assembling open-source software (OSS), often without users directly reading documentation, reporting bugs, or otherwise engaging with maintainers. We study the equilibrium effects of vibe coding on the OSS ecosystem. We develop a model with endogenous entry and heterogeneous project quality in which OSS is a scalable input into producing more software. Users choose whether to use OSS directly or through vibe coding. Vibe coding raises productivity by lowering the cost of using and building on existing code, but it also weakens the user engagement through which many maintainers earn returns. When OSS is monetized only through direct user engagement, greater adoption of vibe coding lowers entry and sharing, reduces the availability and quality of OSS, and reduces welfare despite higher productivity. Sustaining OSS at its current scale under widespread vibe coding requires major changes in how maintainers are paid.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Miklos Koren [<a href="https://arxiv.org/show-email/6310d1fe/2601.15494" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 21 Jan 2026 21:54:59 UTC (642 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Government wants DNA and social media from visitors (108 pts)]]></title>
            <link>https://www.privacyinternational.org/news-analysis/5713/trump-administration-wants-your-dna-and-social-media</link>
            <guid>46764381</guid>
            <pubDate>Mon, 26 Jan 2026 11:30:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.privacyinternational.org/news-analysis/5713/trump-administration-wants-your-dna-and-social-media">https://www.privacyinternational.org/news-analysis/5713/trump-administration-wants-your-dna-and-social-media</a>, See on <a href="https://news.ycombinator.com/item?id=46764381">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="block-privacy-international-2020-content">

  

  

  <div>
    
            <p>News &amp; Analysis</p>
      
  <div>
    <p>Post date</p>
              <p>11th December 2025</p>
          </div>

  </div>

  <div><p>Yesterday the Trump Administration <a href="https://www.govinfo.gov/content/pkg/FR-2025-12-10/pdf/2025-22461.pdf">announced a proposed change in policy</a> for travellers to the U.S. It applies to the powers of data collection by the Customs and Border Police (CBP).</p><p>If the proposed changes are adopted after the 60-day consultation, then millions of travellers to the U.S. will be forced to use a U.S. government mobile phone app, submit their social media from the last five years and email addresses used in the last ten years, including of family members. They‚Äôre also proposing the collection of DNA.</p><h3 id="Data-details">Data details</h3><p>The changes include:</p><ul><li>All visitors must submit ‚Äòtheir social media from the last 5 years‚Äô</li><li>ESTA (Electronic System for Travel Authorization) applications will include ‚Äòhigh value data fields‚Äô, ‚Äòwhen feasible‚Äô<ul><li>‚Äòtelephone numbers used in the last five years‚Äô</li><li>‚Äòemail addresses used in the last ten years‚Äô</li><li>‚Äòfamily number telephone numbers (sic) used in the last five years‚Äô</li><li>biometrics ‚Äì face, fingerprint, DNA, and iris</li><li>business telephone numbers used in the last five years</li><li>business email addresses used in the last ten years.</li></ul></li><li>All these travellers will now have to use an app provided by CBP (‚ÄòCBP Home‚Äô) and an app for their ESTA application (‚ÄòESTA Mobile App‚Äô). The ESTA website is being decommissioned.</li><li>The ‚ÄòCBP Home‚Äô mobile app will be used by people to provide biometric proof of their departure, to ‚Äòclose the information gap‚Äô. The app will disclose the user‚Äôs location once they have left the U.S. and run a liveness detection on the selfie photo.</li></ul><p>If approved, this policy would apply to all visitors who currently travel without a visa. For the estimated 14 million annual ESTA travellers, CBP thinks that this will take the average visitor 22 minutes to submit themselves and their family members.</p><p>The inclusion of DNA is particularly alarming: the US Government is setting the ground for the day when it can require the DNA of all travellers. As Ed Hasbrouck <a href="https://papersplease.org/wp/2025/12/10/cbp-wants-all-visitors-to-install-and-use-its-smartphone-app/">helpfully points out</a>, currently no mobile phone enables the collection of DNA samples. But this is not stopping the US Government‚Äôs ambitions for that (strange?) future.</p><h3 id="Unlocking-mass-data-analysis">Unlocking mass data analysis</h3><p>As we‚Äôve previously stated <a href="https://notes.privacyinternational.org/news-analysis/5552/us-border-surveillance-expansion-has-global-implications">earlier in 2025</a> the Trump Administration early on declared its expanded surveillance ambitions. Yesterday‚Äôs announcement is the next step, extending it to visa-free travel, and expanding the dataset including to DNA. Over 70 million people <a href="https://www.trade.gov/feature-article/december-and-annual-2024-total-international-travel-volume">visit the US every year</a>, which means this could end up being a vast datastore of people‚Äôs data.</p><p>One particularly dangerous aspect to this announcement is this: the data the US Government is now seeking unlocks even more data. Leaving the DNA profiling question out (for now, at least until governments start forcing our blood or spit or tears at points of entry), the inclusion of email identifiers and social media is troubling.</p><p>With email accounts, the purpose for demanding 10 years of email identifiers is bewildering <a href="https://notes.privacyinternational.org/learn/mass-surveillance">even to us</a>. The usefulness of mass surveillance of ten years‚Äô worth of business email communications to determine a family‚Äôs visit to Disney World seems not just disproportionate, but also a legal and computational nightmare.</p><p>For social media, it‚Äôs unlikely that there will be a U.S. Government official looking through your photos, videos, likes, and shares as you queue to be fingerprinted. More likely is that AI tools like <a href="https://aws.amazon.com/what-is/retrieval-augmented-generation/">Retrieval-Augmented Generation</a> will be used to scan and summarise all your social media behaviours to ascertain what kind of person you or your family members are, and who you associate with, like and follow. This may lead to AI-enabled automated decisions that have significant consequences to people‚Äôs lives and livelihood.</p><p>Government agencies will likely need to get deep-level access to all social media platforms to gather <a href="https://notes.privacyinternational.org/learn/social-media-surveillance">social media intelligence</a>. Of course they could scrape the content for each social media account for each visitor‚Äôs application. It‚Äôs more likely they‚Äôll seek direct access to the platforms‚Äô databases, or use third-party firms who can do this for them.</p><h3 id="This-moment-has-been-years-in-the-making">This moment has been years in the making</h3><p>At the US border, your <a href="https://notes.privacyinternational.org/explainer/55/social-media-intelligence">social media</a> and your <a href="https://notes.privacyinternational.org/learn/mobile-phone-extraction">devices</a> are at risk, making you <a href="https://privacyinternational.org/when-social-media-makes-you-target">a target</a>. It wasn‚Äôt always this way.</p><p>We‚Äôve been working on border surveillance <a href="https://privacyinternational.org/impact/restraining-911-over-reach">since 9/11</a>, when governments began tracking you ahead of travel, profiling you at the border, and grabbing your fingerprints. Countless new databases were developed with a plethora of methods to harvest our data. This was led by the Bush Administration, and after <a href="https://www.privacyinternational.org/news-analysis/1535/pi-and-aclu-call-repeal-eu-us-agreement-data-transfers">initial resistance</a>, others around the world <a href="https://www.privacyinternational.org/blog/1442/eu-announce-fingerprinting-all-visitors">followed suit</a>.</p><p>Every few years, governments ratchet up their border surveillance ambitions. In 2016, the US Government piloted requesting social media identifiers, adding them to application forms for US visas and ESTA forms. <a href="https://www.telegraph.co.uk/travel/destinations/europe/russia/articles/russia-introduces-social-media-visa-application/">Russia then did the same with their visa programme</a>.</p><p>In <a href="https://privacyinternational.org/advocacy/59/submission-department-homeland-security-privacy-office-usa-regarding-dhs-social-media">2017 we objected when</a> the US Department of Homeland Security planned to expand immigration records to include social media, your internet history and other forms of data companies held about you. Then in <a href="https://privacyinternational.org/sites/default/files/2018-05/PI%20Submission%20Application%20for%20Immigrant%20Visa%20and%20Alien%20Registration%2029.05.201.pdf">2018 we objected when</a> the US Department of State sought access to five years of social media identifiers for visa applications.</p><p>This has resulted in massive and unprecedented powers and capabilities for border and immigration agencies in the U.S., including <a href="https://ig.ft.com/us-ice-surveillance/">expansive use</a> of surveillance, enabled by industry.</p><h3 id="Beyond-America-it-affects-everyone">Beyond America, it affects everyone</h3><p>Many governments follow the US‚Äôs lead on border surveillance. European governments are currently launching their own biometric registration programme for visitors that will include vast data collection, all at a cost to travellers.</p><p>We must urgently curb this trend.</p><p>People deserve rights wherever they are. This is why human rights are universal, and why constitutions limit the power of governments. Fundamental protections and safeguards should apply to all people, including at borders. <a href="https://www.lemonde.fr/en/international/article/2025/03/22/how-a-french-researcher-being-refused-entry-to-the-us-turned-into-a-diplomatic-mess_6739415_4.html">Governments should be calling out these abuses</a>, protecting us from this treatment, and in turn living up to these commitments.</p><p>Most urgently, industry need to be protecting our data from being used for profiling and targeting. <a href="https://www.privacyinternational.org/learn/social-media-surveillance">Social media intelligence tools</a> allow governments and others to accumulate our data, discern our preferences, and profile us to their advantage and without our knowledge. Scraping social media for photos is what led to notorious facial recognition firm <a href="https://notes.privacyinternational.org/legal-action/challenge-against-clearview-ai-europe">Clearview AI</a> being able to sell its wares to police across the world. That‚Äôs why we demanded that people be protected from this type of abuse.</p><p>We can‚Äôt resist this surge in data-intensive border policies just deleting or locking our social media accounts. Governments may still access your data despite the protections you seek to apply.</p><p>It‚Äôs therefore urgent that the tech industry increase protections for people everywhere. They must act now before they become even more integral and complicit in automated surveillance machinery that will affect people across the world every day. Now‚Äôs not the time to <a href="https://notes.privacyinternational.org/long-read/5683/big-techs-bind-military-and-intelligence-agencies">seek profit by enabling injustice</a>, it‚Äôs the time to step up and protect everyone and our data.</p><p>And even if only to say ‚Äòno we will never build DNA collection into our phones‚Äô. (It‚Äôs embarassing that it ever had to be said, but that‚Äôs 2025 for you!)</p></div>

  

  

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TSMC Risk (123 pts)]]></title>
            <link>https://stratechery.com/2026/tsmc-risk/</link>
            <guid>46764223</guid>
            <pubDate>Mon, 26 Jan 2026 11:07:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stratechery.com/2026/tsmc-risk/">https://stratechery.com/2026/tsmc-risk/</a>, See on <a href="https://news.ycombinator.com/item?id=46764223">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>




<p>You probably think, given this title, you know what this Article is about. The most advanced semiconductors are made by TSMC in Taiwan,<sup id="rf1-18153"><a href="#fn1-18153" title="Yes, there are chips made in Arizona, but only a portion, and they need to be sent back to Taiwan for packaging and testing." rel="footnote">1</a></sup> and Taiwan is claimed by China, which has not and will not take reunification-by-force off of the table.</p>



<p>Relatedly, AI obviously has significant national security implications; at Davos, Anthropic CEO Dario Amodei reiterated his objection to the U.S. allowing the sale of Nvidia chips to China. <a href="https://www.bloomberg.com/news/articles/2026-01-20/anthropic-ceo-says-selling-advanced-ai-chips-to-china-is-crazy">From Bloomberg</a>:</p>



<blockquote>
<p>Anthropic Chief Executive Officer Dario Amodei said selling advanced artificial intelligence chips to China is a blunder with ‚Äúincredible national security implications‚Äù as the US moves to allow Nvidia Corp. to sell its H200 processors to Beijing. ‚ÄúIt would be a big mistake to ship these chips,‚Äù Amodei said in an interview with Bloomberg Editor-in-Chief John Micklethwait at the World Economic Forum in Davos, Switzerland. ‚ÄúI think this is crazy. It‚Äôs a bit like selling nuclear weapons to North Korea.‚Äù</p>
</blockquote>



<p>The nuclear weapon analogy is an interesting one: a lot of game theory was developed to manage the risk of nuclear weapons, particularly once the U.S.S.R. gained/stole nuclear capability, ending the U.S.‚Äôs brief monopoly on the technology. Before that happened, however, the U.S. had a dominant military position, given we had nuclear weapons and no one else did. Perhaps Amodei believes the U.S. should have advanced AI and China should not, giving us a dominant military position?</p>



<p>The problem with that reality, however, is Taiwan, as I explained in <a href="https://stratechery.com/2025/ai-promise-and-chip-precariousness/">AI Promise and Chip Precariousness</a>. AI, in contrast to nuclear weapons, has a physical dependency in Taiwan that can be easily destroyed by Chinese missiles, even without an invasion; if we got to a situation where only the U.S. had the sort of AI that would give us an unassailable advantage militarily, then the optimal strategy for China would change to taking TSMC off of the board.</p>



<p>Given this dependency, my recommendations in the Article run counter to Amodei: I want China dependent on not just U.S. chips but also on TSMC directly, which is why I argued in favor of selling Nvidia chips to China, and further believe that Huawei and other Chinese companies ought to be able to source from TSMC (on the flip side, I would ban the sale of semiconductor manufacturing equipment to Chinese fabs). <a href="https://stratechery.com/2025/trump-allows-h200-sales-to-china-the-sliding-scale-a-good-decision/">I think it‚Äôs a good thing</a> the Trump administration moved on the first point, at least.</p>



<p>However, this risk is not what this Article is about: there is another TSMC risk facing the entire AI industry in particular; moreover, it‚Äôs a risk the downside of which is already being realized.</p>



<h3>The TSMC Brake</h3>



<p>There was one refrain that was common across Big Tech earnings last quarter: demand for AI exceeds supply. Here was Amazon CEO Andy Jassy on <a href="https://seekingalpha.com/article/4835958-amazon-com-inc-amzn-q3-2025-earnings-call-transcript">the company‚Äôs earnings call</a>:</p>



<blockquote>
<p>You‚Äôre going to see us continue to be very aggressive investing in capacity because we see the demand. As fast as we‚Äôre adding capacity right now, we‚Äôre monetizing it.</p>
</blockquote>



<p>Here was Microsoft CFO Amy Hood on <a href="https://seekingalpha.com/article/4835092-microsoft-corporation-msft-q1-2026-earnings-call-transcript">the company‚Äôs earnings call</a>:</p>



<blockquote>
<p>Azure AI services revenue was generally in line with expectations, and this quarter, demand again exceeded supply across workloads, even as we brought more capacity online.</p>
</blockquote>



<p>Here was Google CFO Anat Ashkenazi on <a href="https://seekingalpha.com/article/4835072-alphabet-inc-googl-q3-2025-earnings-call-transcript">the company‚Äôs earnings call</a>:</p>



<blockquote>
<p>In GCP, we see strong demand for enterprise AI infrastructure, including TPUs and GPUs, enterprise AI solutions driven by demand for Gemini 2.5 and our other AI models, and core GCP infrastructure and other services such as cybersecurity and data analytics. As I‚Äôve mentioned on previous earnings calls, while we have been working hard to increase capacity and have improved the pace of server deployments and data center construction, we still expect to remain in a tight demand-supply environment in Q4 and 2026.</p>
</blockquote>



<p>Here was Meta CEO Mark Zuckerberg on <a href="https://seekingalpha.com/article/4835033-meta-platforms-inc-meta-q3-2025-earnings-call-transcript">the company‚Äôs earnings call</a>:</p>



<blockquote>
<p>To date, we keep on seeing this pattern where we build some amount of infrastructure to what we think is an aggressive assumption. And then we keep on having more demand to be able to use more compute, especially in the core business in ways that we think would be quite profitable than we end up having compute for.</p>
</blockquote>



<p>Earlier this month, TSMC CEO C.C. Wei admitted that the shortage was a lack of chips, not power; from <a href="https://investor.tsmc.com/english/encrypt/files/encrypt_file/reports/2026-01/51d09df96cd89ac19d65af39032b038dc2896a24/TSMC%204Q25%20Transcript.pdf">the company‚Äôs earnings call</a>:</p>



<blockquote>
<p>Talking about to build a lot of AI data center all over the world, I use one of my customers‚Äô customers‚Äô answer. I asked the same question. They told me that they planned this one, 5-6 years ago already. So, as I said, those cloud service providers are smart, very smart. So, they say that they work on the power supply 5-6 years ago. So, today, their message to me is: silicon from TSMC is a bottleneck, and asked me not to pay attention to all others, because they have to solve the silicon bottleneck first. But indeed, we do get the power supply, all over the world, especially in the US. Not only that, but we also look at, who support those kind of a power supply, like a turbine, like, what, nuclear power plant, the plan or those kinds of things. We also look at the supply of the rack. We also look at the supply of the cooling system. Everything, so far, so good. So we have to work hard to narrow the gap between the demand and supply from TSMC.</p>
</blockquote>



<p>The cause of that gap is obvious if you look at TSMC‚Äôs financials, specifically the company‚Äôs annual capital expenditures:</p>



<figure><img data-recalc-dims="1" fetchpriority="high" decoding="async" width="1330" height="620" src="https://i0.wp.com/stratechery.com/wp-content/uploads/2026/01/tsmc-risk-1.png?resize=1330%2C620&amp;ssl=1" alt="" srcset="https://i0.wp.com/stratechery.com/wp-content/uploads/2026/01/tsmc-risk-1.png?w=1330&amp;ssl=1 1330w, https://i0.wp.com/stratechery.com/wp-content/uploads/2026/01/tsmc-risk-1.png?resize=300%2C140&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2026/01/tsmc-risk-1.png?resize=768%2C358&amp;ssl=1 768w" sizes="(max-width: 1000px) 100vw, 1000px"></figure>



<p>After a big increase in CapEx in 2021, driven by the COVID shortages and <a href="https://seekingalpha.com/article/4398927-taiwan-semiconductor-manufacturing-company-limited-tsm-ceo-c-c-wei-on-q4-2020-results">a belief in 5G</a>, TSMC‚Äôs annual CapEx in the following years was basically flat ‚Äî it actually declined on a year-over-year basis in both 2023 and 2024. Note those dates! ChatGPT was released in November 2022; that kicked off a massive increase in CapEx amongst the hyperscalers in particular, but it sure seems like TSMC didn‚Äôt buy the hype.</p>



<p>That lack of increased investment earlier this decade is why there is a shortage today, and is why TSMC has been a de facto brake on the AI buildout/bubble; <a href="https://stratechery.com/2026/tsmc-earnings-the-tsmc-brake-revisited-why-ai-needs-foundry-competition/">I wrote last quarter</a>:</p>



<blockquote>
<p>To put it another way, if Altman and OpenAI are the ones pushing to accelerate the AI infrastructure buildout, it‚Äôs Wei and TSMC that are the brakes. The extent to which all of Altman‚Äôs deals actually materialize is dependent on how much TSMC invests in capacity now, and while they haven‚Äôt shown their hand yet, the company is saying all of the right things about AI being a huge trend without having yet committed to a commensurate level of investment, at least relative to OpenAI‚Äôs goals.</p>
</blockquote>



<p>That Update was about the future, but it‚Äôs important to note that the TSMC brake has ‚Äî&nbsp;if all of those CEO and CFO comments above are to be believed ‚Äî already cost the biggest tech companies a lot of money. That‚Äôs the implication of not having enough supply to satisfy demand: there was revenue to be made that wasn‚Äôt, because TSMC didn‚Äôt buy the AI hype at the same time everyone else did.</p>



<h3>TSMC‚Äôs CapEx Plans</h3>



<p>TSMC is, finally, starting to invest more. Last year‚Äôs CapEx increased 37% to $41 billion, and there‚Äôs another increase in store for this year to $52‚Äì$56 billion; if we take the midpoint, that represents an increase of 32%, a bit less than last year:</p>



<figure><img data-recalc-dims="1" decoding="async" width="1330" height="618" src="https://i0.wp.com/stratechery.com/wp-content/uploads/2026/01/tsmc-risk-3.png?resize=1330%2C618&amp;ssl=1" alt="" srcset="https://i0.wp.com/stratechery.com/wp-content/uploads/2026/01/tsmc-risk-3.png?w=1330&amp;ssl=1 1330w, https://i0.wp.com/stratechery.com/wp-content/uploads/2026/01/tsmc-risk-3.png?resize=300%2C139&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2026/01/tsmc-risk-3.png?resize=768%2C357&amp;ssl=1 768w" sizes="(max-width: 1000px) 100vw, 1000px"></figure>



<p>Make no mistake, $54 billion is a big number, one that Wei admitted made him nervous:</p>



<blockquote>
<p>You essentially try to ask whether the AI demand is real or not. I‚Äôm also very nervous about it. Yeah, you bet, because we have to invest about USD52 billion to USD56 billion for the CapEx, right? If we did not do it carefully, that will be a big disaster to TSMC for sure. So, of course, I spent a lot of time in the last three-four months talking to my customers and then customers‚Äô customers. I want to make sure that my customers‚Äô demands are real.</p>
</blockquote>



<p>Wei made clear that he was worried about the market several years down the line:</p>



<blockquote>
<p>If you build a new fab, it takes two and three year, two to three years to build a new fab. So even we start to spend $52 billion to $56 billion, the contribution to this year is almost none, and 2027, a little bit. So we actually, we are looking for 2028-2029 supply, and we hope it‚Äôs a time that the gap will be narrow‚Ä¶So 2026-2027 for the short-term, we are looking to improve our productivity. 2028 to 2029, yes, we start to increase our capacity significantly. And it will continue this way if the AI demand megatrend as we expected.</p>
</blockquote>



<p>First off, this delayed impact explains why TSMC‚Äôs lack of CapEx increase a few years ago is resulting in supply-demand imbalance today. Secondly, notice how this year‚Äôs planned increase ‚Äî which again, won‚Äôt really have an impact until 2028 ‚Äî pales in comparison to the CapEx growth of the hyperscalers (<em>2025 numbers are estimates; note that Amazon‚Äôs CapEx includes Amazon.com</em>):</p>



<figure><img data-recalc-dims="1" decoding="async" width="1330" height="812" src="https://i0.wp.com/stratechery.com/wp-content/uploads/2026/01/tsmc-risk-2.png?resize=1330%2C812&amp;ssl=1" alt="" srcset="https://i0.wp.com/stratechery.com/wp-content/uploads/2026/01/tsmc-risk-2.png?w=1330&amp;ssl=1 1330w, https://i0.wp.com/stratechery.com/wp-content/uploads/2026/01/tsmc-risk-2.png?resize=300%2C183&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2026/01/tsmc-risk-2.png?resize=768%2C469&amp;ssl=1 768w" sizes="(max-width: 1000px) 100vw, 1000px"></figure>



<p>Remember, a significant portion of this CapEx growth is for chips that are supported by TSMC‚Äôs stagnant CapEx growth from a few years ago. It‚Äôs notable, then, that TSMC‚Äôs current and projected CapEx growth is still less than the hyperscalers: how much less is it going to be than the hyperscalers‚Äô growth in 2028, when the fabs being built today start actually producing chips?</p>



<p>In short, the TSMC brake isn‚Äôt going anywhere ‚Äî if anything, it‚Äôs being pressed harder than ever.</p>



<h3>TSMC Risk</h3>



<p>TSMC is, to be clear, being extremely rational. CapEx is inherently risky: you are spending money now in anticipation of demand that may or may not materialize. Moreover, the risk for a foundry is higher than basically any other business model: nearly all of a foundry‚Äôs costs are CapEx, which means that if demand fails to materialize, costs ‚Äî in the form of depreciation ‚Äî don‚Äôt go down as they might with a business model with a higher percentage of marginal costs. This is exacerbated by the huge dollar figures entailed in building fabs: $52‚Äì$56 billion may drive revenues with big margins, but those big margins can easily flip to being huge losses and years of diminished pricing power thanks to excess capacity. Therefore, it‚Äôs understandable that TSMC is trying to manage its risks. Sure, the company may be foregoing some upside in 2028, but what is top of Wei‚Äôs mind is avoiding ‚Äúa big disaster.‚Äù</p>



<p>What is important to note, however, is that the risk TSMC is managing doesn‚Äôt simply go away: rather, it‚Äôs being offloaded to the hyperscalers in particular. Specifically, if we get to 2028, and TSMC still isn‚Äôt producing enough chips to satisfy demand, then that means the hyperscalers will be forgoing billions of dollars in revenue ‚Äî&nbsp;even more than they are already forgoing today. Yes, that risk is harder to see than the risk TSMC is avoiding, because the hyperscalers aren‚Äôt going to be bankrupt for a lack of chips to satisfy demand. Still, the potential money not made ‚Äî particularly when the number is potentially in the hundreds of billions of dollars ‚Äî is very much a risk that the hyperscalers are incurring because of TSMC‚Äôs conservatism.</p>



<p>What the hyperscalers need to understand is that simply begging TSMC to make more isn‚Äôt going to fix this problem, because begging TSMC to make more is to basically ask TSMC to take back the risk TSMC is offloading to the hyperscalers ‚Äî they already declined! Rather, the only thing that will truly motivate TSMC to take on more risk is competition. If TSMC were worried about not just forgoing its own extra revenue, but actually losing business to a competitor, then the company would invest more. Moreover, that extra investment would be stacked on top of the investment made by said competitor, which means the world would suddenly have dramatically more fab capacity.</p>



<h3>If You Want a Bubble</h3>



<p>In short, the only way to truly get an AI bubble, with all of <a href="https://stratechery.com/2025/the-benefits-of-bubbles/">the potential benefits that entails</a>, or, in the optimistic case, to actually meet demand in 2028 and beyond, is to have competition in the foundry space. That, by extension, means Samsung or Intel ‚Äî or both ‚Äî&nbsp;actually being viable options.</p>



<p>Remember, however, the number one challenge facing those foundries: a lack of demand from the exact companies whom TSMC has deputized to take on their risk. I wrote in <a href="https://stratechery.com/2025/u-s-intel/">U.S. Intel</a>:</p>



<blockquote>
<p>Our mythical startup, however, doesn‚Äôt exist in a vacuum: it exists in the same world as TSMC, the company who has defined the modern pure play foundry. TSMC has put in the years, and they‚Äôve put in the money; TSMC has the unparalleled customer service approach that created the entire fabless chip industry; and, critically, TSMC, just as they did in the mobile era, is aggressively investing to meet the AI moment. If you‚Äôre an Nvidia, or an Apple in smartphones, or an AMD or a Qualcomm, why would you take the chance of fabricating your chips anywhere else? Sure, TSMC is raising prices in the face of massive demand, but the overall cost of a chip in a system is still quite small; is it worth risking your entire business to save a few dollars for worse performance with a worse customer experience that costs you time to market and potentially catastrophic product failures?</p>



<p>We know our mythical startup would face these challenges because they are the exact challenges Intel faces. Intel may need ‚Äúa meaningful external customer to drive acceptable returns on [its] deployed capital‚Äù, but Intel‚Äôs needs do not drive the decision-making of those external customers, despite the fact that Intel, while not fully caught up to TSMC, is at least in the ballpark, something no startup could hope to achieve for decades.</p>
</blockquote>



<p>Becoming a meaningful customer of Samsung or Intel is very risky: it takes years to get a chip working on a new process, which hardly seems worth it if that process might not be as good, and if the company offering the process definitely isn‚Äôt as customer service-centric as TSMC. I understand why everyone sticks with TSMC.</p>



<p>The reality that hyperscalers and fabless chip companies need to wake up to, however, is that avoiding the risk of working with someone other than TSMC incurs new risks that are both harder to see and also much more substantial. Except again, we can see the harms already: foregone revenue today as demand outstrips supply. Today‚Äôs shortages, however, may prove to be peanuts: if AI has the potential these companies claim it does, future foregone revenue at the end of the decade is going to cost exponentially more ‚Äî surely a lot more than whatever expense is necessary to make Samsung and/or Intel into viable competitors for TSMC.</p>



<p>This, incidentally, is how the geographic risk issue will be fixed, if it ever is. It‚Äôs hard to get companies to pay for insurance for geopolitical risks that may never materialize. What is much more likely is that TSMC‚Äôs customers realize that their biggest risk isn‚Äôt that TSMC gets blown up by China, but that TSMC‚Äôs monopoly and reasonable reluctance to risk a rate of investment that matches the rest of the industry means that the rest of the industry fails to fully capture the value of AI.</p>
<hr><ol><li id="fn1-18153"><p>Yes, there are chips made in Arizona, but only a portion, and they need to be sent back to Taiwan for packaging and testing.&nbsp;<a href="#rf1-18153" title="Return to footnote 1.">‚Ü©</a></p></li></ol></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Only 1 LLM can fly a drone (123 pts)]]></title>
            <link>https://github.com/kxzk/snapbench</link>
            <guid>46764170</guid>
            <pubDate>Mon, 26 Jan 2026 11:00:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/kxzk/snapbench">https://github.com/kxzk/snapbench</a>, See on <a href="https://news.ycombinator.com/item?id=46764170">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h3 tabindex="-1" dir="auto">SnapBench</h3><a id="user-content-snapbench" aria-label="Permalink: SnapBench" href="#snapbench"></a></p>
<blockquote>
<p dir="auto">Inspired by <a href="https://en.wikipedia.org/wiki/Pok%C3%A9mon_Snap" rel="nofollow">Pok√©mon Snap</a> (1999). VLM pilots a drone through 3D world to locate and identify creatures.</p>
</blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/787f7f9eb554849dceeda184fd3b5cb2bff7cac1a74724f011fafb576cb8dfe8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7a69672d626c61636b3f7374796c653d666c61742d737175617265266c6f676f3d7a6967"><img src="https://camo.githubusercontent.com/787f7f9eb554849dceeda184fd3b5cb2bff7cac1a74724f011fafb576cb8dfe8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7a69672d626c61636b3f7374796c653d666c61742d737175617265266c6f676f3d7a6967" alt="zig" data-canonical-src="https://img.shields.io/badge/zig-black?style=flat-square&amp;logo=zig"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1e54d2cf12f9fd89cfaadf3c3899b112e144af0ac004cbe8a1c9f1e7c7297455/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f727573742d2532334345343232423f7374796c653d666c61742d737175617265266c6f676f3d72757374"><img src="https://camo.githubusercontent.com/1e54d2cf12f9fd89cfaadf3c3899b112e144af0ac004cbe8a1c9f1e7c7297455/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f727573742d2532334345343232423f7374796c653d666c61742d737175617265266c6f676f3d72757374" alt="rust" data-canonical-src="https://img.shields.io/badge/rust-%23CE422B?style=flat-square&amp;logo=rust"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/02028ceb21cf9e0c689276755ee80daebbe8cf6cff6f27f51e98b7c11af6686e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d2532333337373641423f7374796c653d666c61742d737175617265266c6f676f3d707974686f6e266c6f676f436f6c6f723d464644343342"><img src="https://camo.githubusercontent.com/02028ceb21cf9e0c689276755ee80daebbe8cf6cff6f27f51e98b7c11af6686e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d2532333337373641423f7374796c653d666c61742d737175617265266c6f676f3d707974686f6e266c6f676f436f6c6f723d464644343342" alt="python" data-canonical-src="https://img.shields.io/badge/python-%233776AB?style=flat-square&amp;logo=python&amp;logoColor=FFD43B"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Architecture</h3><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<section data-identity="016aae09-fee9-4fe0-9805-333bf524b569" data-host="https://viewscreen.githubusercontent.com" data-src="https://viewscreen.githubusercontent.com/markdown/mermaid?docs_host=https%3A%2F%2Fdocs.github.com" data-type="mermaid" aria-label="mermaid rendered output container">
  <div dir="auto" data-json="{&quot;data&quot;:&quot;%%{init: {'theme': 'base', 'themeVariables': { 'background': '#ffffff', 'primaryColor': '#ffffff'}}}%%\nflowchart LR\n    subgraph Controller[\&quot;**Controller** (Rust)\&quot;]\n        C[Orchestration]\n    end\n\n    subgraph VLM[\&quot;**VLM** (OpenRouter)\&quot;]\n        V[Vision-Language Model]\n    end\n\n    subgraph Simulation[\&quot;**Simulation** (Zig/raylib)\&quot;]\n        S[Game State]\n    end\n\n    C --&amp;gt;|\&quot;screenshot + prompt\&quot;| V\n    C &amp;lt;--&amp;gt;|\&quot;cmds + state&amp;lt;br&amp;gt;**UDP:9999**\&quot;| S\n\n    style Controller fill:#8B5A2B,stroke:#5C3A1A,color:#fff\n    style VLM fill:#87CEEB,stroke:#5BA3C6,color:#1a1a1a\n    style Simulation fill:#4A7C23,stroke:#2D5A10,color:#fff\n    style C fill:#B8864A,stroke:#8B5A2B,color:#fff\n    style V fill:#B5E0F7,stroke:#87CEEB,color:#1a1a1a\n    style S fill:#6BA33A,stroke:#4A7C23,color:#fff\n&quot;}" data-plain="%%{init: {'theme': 'base', 'themeVariables': { 'background': '#ffffff', 'primaryColor': '#ffffff'}}}%%
flowchart LR
    subgraph Controller[&quot;**Controller** (Rust)&quot;]
        C[Orchestration]
    end

    subgraph VLM[&quot;**VLM** (OpenRouter)&quot;]
        V[Vision-Language Model]
    end

    subgraph Simulation[&quot;**Simulation** (Zig/raylib)&quot;]
        S[Game State]
    end

    C -->|&quot;screenshot + prompt&quot;| V
    C <-->|&quot;cmds + state<br>**UDP:9999**&quot;| S

    style Controller fill:#8B5A2B,stroke:#5C3A1A,color:#fff
    style VLM fill:#87CEEB,stroke:#5BA3C6,color:#1a1a1a
    style Simulation fill:#4A7C23,stroke:#2D5A10,color:#fff
    style C fill:#B8864A,stroke:#8B5A2B,color:#fff
    style V fill:#B5E0F7,stroke:#87CEEB,color:#1a1a1a
    style S fill:#6BA33A,stroke:#4A7C23,color:#fff
">
      <pre lang="mermaid" aria-label="Raw mermaid code">%%{init: {'theme': 'base', 'themeVariables': { 'background': '#ffffff', 'primaryColor': '#ffffff'}}}%%
flowchart LR
    subgraph Controller["**Controller** (Rust)"]
        C[Orchestration]
    end

    subgraph VLM["**VLM** (OpenRouter)"]
        V[Vision-Language Model]
    end

    subgraph Simulation["**Simulation** (Zig/raylib)"]
        S[Game State]
    end

    C --&gt;|"screenshot + prompt"| V
    C &lt;--&gt;|"cmds + state&lt;br&gt;**UDP:9999**"| S

    style Controller fill:#8B5A2B,stroke:#5C3A1A,color:#fff
    style VLM fill:#87CEEB,stroke:#5BA3C6,color:#1a1a1a
    style Simulation fill:#4A7C23,stroke:#2D5A10,color:#fff
    style C fill:#B8864A,stroke:#8B5A2B,color:#fff
    style V fill:#B5E0F7,stroke:#87CEEB,color:#1a1a1a
    style S fill:#6BA33A,stroke:#4A7C23,color:#fff
</pre>
    </div>
  <span role="presentation">
    <span data-view-component="true">
      <span>Loading</span>
</span>
  </span>
</section>

<p dir="auto"><h3 tabindex="-1" dir="auto">Overview</h3><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">The simulation generates procedural terrain and spawns creatures (cat, dog, pig, sheep) for the drone to discover. It handles drone physics and collision detection, accepting 8 movement commands plus <code>identify</code> and <code>screenshot</code>. The Rust controller captures frames from the simulation, constructs prompts enriched with position and state data, then parses VLM responses into executable command sequences. The objective: locate and successfully identify 3 creatures, where <code>identify</code> succeeds when the drone is within 5 units of a target.</p>
<br>
<details open="">
  <summary>
    
    <span>demo_3x.mov</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/25046261/540463086-37903246-a5ae-4e83-87c6-c099198c8724.mov?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njk0NTYxMjIsIm5iZiI6MTc2OTQ1NTgyMiwicGF0aCI6Ii8yNTA0NjI2MS81NDA0NjMwODYtMzc5MDMyNDYtYTVhZS00ZTgzLTg3YzYtYzA5OTE5OGM4NzI0Lm1vdj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAxMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMTI2VDE5MzAyMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTRhZDNiNGQxMjdhYmE1NzMxNmY2OTE0OTZkNmI3MWQ0NjMwMWMxM2RjNjc1ZTQ1Njc3OTY3MmJjMTEzZTYyNDkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Z42_uZzZVlGr0aw8wfyljYMySRVweN7rN_SZJHFpG5E" data-canonical-src="https://private-user-images.githubusercontent.com/25046261/540463086-37903246-a5ae-4e83-87c6-c099198c8724.mov?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njk0NTYxMjIsIm5iZiI6MTc2OTQ1NTgyMiwicGF0aCI6Ii8yNTA0NjI2MS81NDA0NjMwODYtMzc5MDMyNDYtYTVhZS00ZTgzLTg3YzYtYzA5OTE5OGM4NzI0Lm1vdj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAxMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMTI2VDE5MzAyMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTRhZDNiNGQxMjdhYmE1NzMxNmY2OTE0OTZkNmI3MWQ0NjMwMWMxM2RjNjc1ZTQ1Njc3OTY3MmJjMTEzZTYyNDkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Z42_uZzZVlGr0aw8wfyljYMySRVweN7rN_SZJHFpG5E" controls="controls" muted="muted">

  </video>
</details>


<p dir="auto"><h2 tabindex="-1" dir="auto">Gotta catch 'em all?</h2><a id="user-content-gotta-catch-em-all" aria-label="Permalink: Gotta catch 'em all?" href="#gotta-catch-em-all"></a></p>
<p dir="auto">I gave 7 frontier LLMs a simple task: pilot a drone through a 3D voxel world and find 3 creatures.</p>
<p dir="auto">Only one could do it.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/kxzk/snapbench/blob/main/images/benchmark_results.png"><img src="https://github.com/kxzk/snapbench/raw/main/images/benchmark_results.png" alt="Benchmark Results"></a></p>
<p dir="auto">Is this a rigorous benchmark? No. However, it's a reasonably fair comparison - same prompt, same seeds, same iteration limits. I'm sure with enough refinement you could coax better results out of each model. But that's kind of the point: out of the box, with zero hand-holding, only one model figured out how to actually <em>fly</em>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why can't Claude look down?</h2><a id="user-content-why-cant-claude-look-down" aria-label="Permalink: Why can't Claude look down?" href="#why-cant-claude-look-down"></a></p>
<p dir="auto">The core differentiator wasn't intelligence - it was <strong>altitude control</strong>. Creatures sit on the ground. To identify them, you need to descend.</p>
<ul dir="auto">
<li><strong>Gemini Flash</strong>: Actively adjusts altitude, descends to creature level, identifies</li>
<li><strong>GPT-5.2-chat</strong>: Gets close horizontally but never lowers</li>
<li><strong>Claude Opus</strong>: Attempts identification 160+ times, never succeeds - approaching at wrong angles</li>
<li><strong>Others</strong>: Wander randomly or get stuck</li>
</ul>
<p dir="auto">This left me puzzled. Claude Opus is arguably the most capable model in the lineup. It <em>knows</em> it needs to identify creatures. It tries - aggressively. But it never adjusts its approach angle.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The two-creature anomaly</h2><a id="user-content-the-two-creature-anomaly" aria-label="Permalink: The two-creature anomaly" href="#the-two-creature-anomaly"></a></p>
<p dir="auto">Run 13 (seed 72) was the only run where any model found 2 creatures. Why? They happened to spawn near each other. Gemini Flash found one, turned around, and spotted the second.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/kxzk/snapbench/blob/main/images/seed-72.png"><img src="https://github.com/kxzk/snapbench/raw/main/images/seed-72.png" alt="Seed 72"></a></p>
<p dir="auto">In most other runs, Flash found one creature quickly but ran out of iterations searching for the others. The world is big. 50 iterations isn't a lot of time.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Bigger ‚â† better</h2><a id="user-content-bigger--better" aria-label="Permalink: Bigger ‚â† better" href="#bigger--better"></a></p>
<p dir="auto">This was the most surprising finding. I expected:</p>
<ul dir="auto">
<li>Claude Opus 4.5 (most expensive) to dominate</li>
<li>Gemini 3 Pro to outperform Gemini 3 Flash (same family, more capability)</li>
</ul>
<p dir="auto">Instead, the cheapest model beat models costing 10x more.</p>
<p dir="auto">What's going on here? A few theories:</p>
<ol dir="auto">
<li><strong>Spatial reasoning doesn't scale with model size</strong> - at least not yet</li>
<li><strong>Flash was trained differently</strong> - maybe more robotics data, more embodied scenarios?</li>
<li><strong>Smaller models follow instructions more literally</strong> - "go down" means go down, not "consider the optimal trajectory"</li>
</ol>
<p dir="auto">I genuinely don't know. But if you're building an LLM-powered agent that needs to navigate physical or virtual space, the most expensive model might not be your best choice.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Color theory, maybe</h2><a id="user-content-color-theory-maybe" aria-label="Permalink: Color theory, maybe" href="#color-theory-maybe"></a></p>
<p dir="auto">Anecdotally, creatures with higher contrast (gray sheep, pink pigs) seemed easier to spot than brown-ish creatures that blended into the terrain. A future version might normalize creature visibility. Or maybe that's the point - real-world object detection isn't normalized either.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prior work</h2><a id="user-content-prior-work" aria-label="Permalink: Prior work" href="#prior-work"></a></p>
<p dir="auto">Before this, I tried having LLMs pilot a <a href="https://github.com/kxzk/tello-bench">real DJI Tello drone</a>.</p>
<p dir="auto">Results: it flew straight up, hit the ceiling, and did donuts until I caught it. (I was using Haiku 4.5, which in hindsight explains a lot.)</p>
<p dir="auto">The Tello is now broken. I've ordered a BetaFPV and might get another Tello since they're so easy to program. Now that I know Gemini Flash can actually navigate, a real-world follow-up might be worth revisiting.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Rough edges</h2><a id="user-content-rough-edges" aria-label="Permalink: Rough edges" href="#rough-edges"></a></p>
<p dir="auto">This is half-serious research, half "let's see what happens."</p>
<ul dir="auto">
<li>The simulation has rough edges (it's a side project, not a polished benchmark suite)</li>
<li>One blanket prompt is used for all models - model-specific tuning would likely improve results</li>
<li>The feedback loop is basic (position, screenshot, recent commands) - there's room to get creative with what information gets passed back</li>
<li>Iteration limits (50) may artificially cap models that are slower but would eventually succeed</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Try it yourself</h2><a id="user-content-try-it-yourself" aria-label="Permalink: Try it yourself" href="#try-it-yourself"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites</h3><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Tool</th>
<th>Version</th>
<th>Install</th>
</tr>
</thead>
<tbody>
<tr>
<td>Zig</td>
<td>‚â•0.15.2</td>
<td><a href="https://ziglang.org/download/" rel="nofollow">ziglang.org/download</a></td>
</tr>
<tr>
<td>Rust</td>
<td>stable (2024 edition)</td>
<td><a href="https://rust-lang.org/tools/install/" rel="nofollow">rust-lang.org/tools/install</a></td>
</tr>
<tr>
<td>Python</td>
<td>‚â•3.11</td>
<td><a href="https://www.python.org/" rel="nofollow">python.org</a></td>
</tr>
<tr>
<td>uv</td>
<td>latest</td>
<td><a href="https://docs.astral.sh/uv/getting-started/installation/" rel="nofollow">docs.astral.sh/uv</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">You'll also need an <a href="https://openrouter.ai/" rel="nofollow">OpenRouter</a> API key.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Setup</h3><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="gh repo clone kxzk/snapbench
cd snapbench

# set your API key
export OPENROUTER_API_KEY=&quot;sk-or-...&quot;"><pre>gh repo clone kxzk/snapbench
<span>cd</span> snapbench

<span><span>#</span> set your API key</span>
<span>export</span> OPENROUTER_API_KEY=<span><span>"</span>sk-or-...<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Running the simulation manually</h3><a id="user-content-running-the-simulation-manually" aria-label="Permalink: Running the simulation manually" href="#running-the-simulation-manually"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# terminal 1: start the simulation (with optional seed)
zig build run -Doptimize=ReleaseFast -- 42
# or
make sim

# terminal 2: start the drone controller
cargo run --release --manifest-path llm_drone/Cargo.toml -- --model google/gemini-3-flash-preview
# or
make drone"><pre><span><span>#</span> terminal 1: start the simulation (with optional seed)</span>
zig build run -Doptimize=ReleaseFast -- 42
<span><span>#</span> or</span>
make sim

<span><span>#</span> terminal 2: start the drone controller</span>
cargo run --release --manifest-path llm_drone/Cargo.toml -- --model google/gemini-3-flash-preview
<span><span>#</span> or</span>
make drone</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Running the benchmark suite</h3><a id="user-content-running-the-benchmark-suite" aria-label="Permalink: Running the benchmark suite" href="#running-the-benchmark-suite"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# runs all models defined in bench/models.toml
uv run bench/bench_runner.py
# or
make bench"><pre><span><span>#</span> runs all models defined in bench/models.toml</span>
uv run bench/bench_runner.py
<span><span>#</span> or</span>
make bench</pre></div>
<p dir="auto">Results get saved to <code>data/run_&lt;id&gt;.csv</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Where this could go</h2><a id="user-content-where-this-could-go" aria-label="Permalink: Where this could go" href="#where-this-could-go"></a></p>
<ul dir="auto">
<li><strong>Model-specific prompts</strong>: Tune instructions to each model's strengths</li>
<li><strong>Richer feedback</strong>: Pass more spatial context (distance readings, compass, minimap?)</li>
<li><strong>Multi-agent runs</strong>: What if you gave each model a drone and made them compete?</li>
<li><strong>Extended iterations</strong>: Let slow models run longer to isolate reasoning from speed</li>
<li><strong>Real drone benchmark</strong>: Gemini Flash vs. the BetaFPV</li>
<li><strong>Pok√©mon assets</strong>: Found <a href="https://poly.pizza/l/Bm4vionqpU" rel="nofollow">low-poly Pok√©mon models</a> on Poly Pizza‚Äîleaning into the Pok√©mon Snap inspiration</li>
<li><strong>World improvements</strong>: Larger terrain, better visuals, performance optimizations</li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">Attribution</h2><a id="user-content-attribution" aria-label="Permalink: Attribution" href="#attribution"></a></p>
<ul dir="auto">
<li>Drone by NateGazzard <a href="https://creativecommons.org/licenses/by/3.0/" rel="nofollow">CC-BY</a> via <a href="https://poly.pizza/m/DNbUoMtG3H" rel="nofollow">Poly Pizza</a></li>
<li>Cube World Kit by Quaternius via <a href="https://poly.pizza/bundle/Cube-World-Kit-DwDr8493Fw" rel="nofollow">Poly Pizza</a></li>
</ul>
<p dir="auto"><em>Donated to <a href="https://poly.pizza/" rel="nofollow">Poly Pizza</a> to support the platform.</em></p>
<br>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MapLibre Tile: a modern and efficient vector tile format (379 pts)]]></title>
            <link>https://maplibre.org/news/2026-01-23-mlt-release/</link>
            <guid>46763864</guid>
            <pubDate>Mon, 26 Jan 2026 10:19:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://maplibre.org/news/2026-01-23-mlt-release/">https://maplibre.org/news/2026-01-23-mlt-release/</a>, See on <a href="https://news.ycombinator.com/item?id=46763864">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p> Jan 23, 2026 </p>   <p>Today we are happy to announce <strong>MapLibre Tile</strong> (MLT), a new modern and efficient vector tile format.</p>
<div><p><img src="https://maplibre.org/_astro/mlt.CaGKYWCo_Z6Nrwr.webp" alt="MapLibre Tile Support" loading="lazy" decoding="async" fetchpriority="auto" width="2987" height="1867"></p></div>
<h2 id="what-is-maplibre-tile">What is MapLibre Tile?</h2>
<p>MapLibre Tile (MLT) is a succesor to <a href="https://github.com/mapbox/vector-tile-spec">Mapbox Vector Tile (MVT)</a>.
It has been redesigned from the ground up to address the challenges of rapidly growing geospatial data volumes
and complex next-generation geospatial source formats, as well as to leverage the capabilities of modern hardware and APIs.</p>
<p>MLT is specifically designed for modern and next-generation graphics APIs to enable high-performance processing and rendering of
large (planet-scale) 2D and 2.5 basemaps. This current implementation offers feature parity with MVT<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> while delivering on the following:</p>
<ul>
<li><strong>Improved compression ratio</strong>: up to 6x on large tiles, based on a column-oriented layout with recursively applied (custom)
lightweight encodings. This leads to reduced latency, storage, and egress costs and, in particular, improved cache utilization.</li>
<li><strong>Better decoding performance</strong>: fast, lightweight encodings that can be used in combination with SIMD/vectorization instructions.</li>
</ul>
<p>In addition, MLT was designed to support the following use cases in the future:</p>
<ul>
<li><strong>Improved support for 3D coordinates</strong>, i.e. elevation.</li>
<li><strong>Improved processing performance</strong>, based on storage and in-memory formats that are specifically designed for modern graphics APIs,
allowing for efficient processing on both CPU and GPU. The formats are designed to be loaded into GPU buffers with little or no additional processing.</li>
<li><strong>Support for linear referencing and m-values</strong> to efficiently support the upcoming next-generation source formats such as Overture Maps (GeoParquet).</li>
<li><strong>Support complex types</strong>, including nested properties, lists and maps.</li>
</ul>
<p>As with any MapLibre project, the future of MLT is decided by the needs of the community. There are a lot of exciting ideas for other future extensions and we welcome contributions to <a href="https://github.com/maplibre/maplibre-tile-spec">the project</a>.</p>
<p>For a more in-depth exploration of MLT have a look at the <a href="https://github.com/mactrem/presentations/blob/main/FOSS4G_2024_Europe/FOSS4G_2024_Europe.pdf">following slides</a>, watch
<a href="https://www.youtube.com/watch?v=YHcoAFcsES0">this talk</a> or read <a href="https://www.arxiv.org/abs/2508.10791">this paper</a> by MLT inventor Markus Tremmel.</p>
<h2 id="when-can-i-use-it">When can I use it?</h2>
<p>For the adventurous, the answer is: <strong>today</strong>. Both MapLibre GL JS and MapLibre Native now support MLT sources. You can use the new <a href="https://maplibre.org/maplibre-style-spec/sources/#encoding"><code>encoding</code></a> property on sources in your style JSON with a value of <code>mlt</code> for MLT vector tile sources.</p>
<p>To try out MLT, you have the following options:</p>
<ul>
<li>The easiest way to try out MLT is to use the <a href="https://raw.githubusercontent.com/maplibre/demotiles/refs/heads/gh-pages/tiles-mlt/plain.json">MLT-based demotiles style</a>.</li>
<li>You can also try out the <a href="https://github.com/maplibre/maplibre-tile-spec/tree/main/java/encoding-server">encoding server</a> that converts existing (MVT-based) styles and vector tile sources to MLT on the fly. This is mostly a tool for development.</li>
<li>To create tiles for production, you could use <a href="https://github.com/onthegomap/planetiler">Planetiler</a>, as the upcoming version will support generating MLTs.</li>
</ul>
<p>Refer to <a href="https://maplibre.org/maplibre-tile-spec/implementation-status/">this page</a> for a complete and up-to-date list of integrations and implementations. If you are an integrator working on supporting MLT, feel free to add your own project there.</p>
<p>We would love to hear your experience with using MLT! Join the <code>#maplibre-tile-format</code> channel on <a href="https://maplibre.org/community">our Slack</a> or create an Issue or Discussion on the <a href="https://github.com/maplibre/maplibre-tile-spec">tile spec repo</a>.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>MapLibre Tile came to be thanks to a multi-year collaboration between academia, open source and enterprise. Thank you to everyone who was involved! We are very proud that our community can innovate like this.</p>
<p>Special thanks go to Markus Tremmel for inventing the format, Yuri Astrakhan for spearheading the project, Tim Sylvester for the C++ implementation, Harel Mazor, Benedikt Vogl and Niklas Greindl for working on the JavaScript implementation.</p>
<p>Also thanks to Microsoft and AWS for financing work on MLT.</p>
<section data-footnotes="">
<ol>
<li id="user-content-fn-1">
<p>One exception: unlike MVT, MLT does not support layers where a value in a column changes type from feature to feature. <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1">‚Ü©</a></p>
</li>
</ol>
</section> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[San Francisco Graffiti (126 pts)]]></title>
            <link>https://walzr.com/sf-graffiti</link>
            <guid>46763721</guid>
            <pubDate>Mon, 26 Jan 2026 10:02:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://walzr.com/sf-graffiti">https://walzr.com/sf-graffiti</a>, See on <a href="https://news.ycombinator.com/item?id=46763721">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header><p> These photos were taken by city inspectors documenting graffiti violations across San Francisco. I scraped them from the city's website. Street art, through the lens of the law.</p></header></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple, What Have You Done? (136 pts)]]></title>
            <link>https://onlinegoddess.net/2026/01/apple-what-have-you-done/</link>
            <guid>46763592</guid>
            <pubDate>Mon, 26 Jan 2026 09:43:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://onlinegoddess.net/2026/01/apple-what-have-you-done/">https://onlinegoddess.net/2026/01/apple-what-have-you-done/</a>, See on <a href="https://news.ycombinator.com/item?id=46763592">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    

    
        

        <p>
            <i>
                <time datetime="2026-01-13T20:20Z">
    13 Jan, 2026
</time>
            </i>
        </p>
    

    <p>I've been an Apple user for over a decade and generally very happy with that life choice, but since iOS / iPadOS 26 and MacOS Tahoe were released a few months ago, I honestly want to tear my hair out.</p>
<p>I'm trying to get on with the new OS, but there's so many little bugs that Apple software no longer feels like it just works. It seems less intuitive and like nobody has really tested it thoroughly. Do Apple staff even use their products anymore, or are they all secretly harbouring Android devices?</p>
<p>Here's a few issues that annoy me regularly, this list is <em>far</em> from exhaustive:</p>
<h3 id="iphone">iPhone</h3><p>My iPhone 14 Pro has 35Gb of "system data" which has basically filled up the entire storage I had left. If I delete some apps or data to free up room, the system data then takes it! I can't update easily because updates need lots of space and sometimes I can't even take a photo! Call me cynical, but this feels like a tactic to force me to buy a newer iPhone<sup id="fnref-1"><a href="#fn-1">1</a></sup>.</p>
<p>My white home screen widgets no longer blend into my white wallpaper thanks to the glowing liquid glass border. I was so happy when Apple allowed you to hide widget names so they could look like part of the home screen, now they've basically undone this.</p>
<p>Random restarts. This has always happened occasionally, but now it's at least once a week, most commonly when taking a photo.</p>
<h3 id="ipad">iPad</h3><p>The search box in the App Store doesn't auto-focus - when you bring it up, you immediately want to type, but Apple makes you tap into it as well. That's shoddy UI design.</p>
<p>It's SO aggressive with tab refreshing in Safari! I have an M1 iPad Pro, yet this happens whenever I open a couple of different tabs and go back. Other browsers don't do this.</p>
<h3 id="mac">Mac</h3><p>Why, oh why did Apple take the app launcher away! I had everything neatly put into folders and now they're all gone. No warning, no option to organise them a different way, just gone. I used muscle memory to access most of my apps, sometimes I can't even remember their names, but now I have to because the new way to open apps is to search for them<sup id="fnref-2"><a href="#fn-2">2</a></sup>.</p>
<h3 id="hardware">Hardware</h3><p>It's not the just the software that has issues unfortunately....</p>
<p>My Magic Keyboard sometimes loses connection with the iPad and I have to detach/reattach it. Oh, and occasionally it rattles!! If you think I might have just been unlucky, this is the second one I purchased - the first had a lump on the bottom!<sup id="fnref-3"><a href="#fn-3">3</a></sup> Obviously not an OS issue, but I've never known such poor quality control from Apple.</p>
<p>My iMac screen also flickers sometimes and this never happened before Tahoe. It feels like a bug with the auto-brightness, but I'm terrified it's going to die on me.</p>
<p>So is it just me? Or is this OS definitely a bad Apple?</p>
<div>
        <p>There are 2 comments - tap to view!<span></span></p>
        <div>
            <p><a href="https://sebastianmalloy.bearblog.dev/" target="_blank">Sebastian</a> üá∫üá∏ commented via the <a href="https://onlinegoddess.net/guestbook/">Guestbook</a>:</p>
<blockquote>
<p>I was just LAST NIGHT complaining about how Apple is irritating the hell out of me right now. I was trying to figure out why my Apple Music with no downloaded music was eating up 9 gigs of space, and found there is really no user-facing way to correct the issue. I‚ÄôM AN OLD HUMAN! I remember when Apple products JUST WORKED.</p>
<p>(Also, love your blog. Keep up the good words. Don‚Äôt be a stranger!)</p>
<ul>
<li><em>My Apple Music has mysterious data too! I deleted every downloaded song, album, playlist etc to no avail. Goodness knows what's going on, something has definitely changed. Lovely to have another comment from you, I'm really glad you're enjoying the blog. :) ~Becky</em></li>
</ul>
</blockquote>
<p><a href="https://anarchaeopteryx.bearblog.dev/" target="_blank">Ruth</a> commented via the <a href="https://onlinegoddess.net/guestbook#ruth-140125">Guestbook</a>:</p>
<blockquote>
<p>Your complaint about the System files taking over your entire iPhone reminded me of this post I read last week: <a href="https://louplummer.lol/how-apple-nearly-ruined-my-vacation/" target="_blank">How Apple Nearly Ruined My Vacation</a>.</p>
<p>There isn't really a solution offered in that post other than the full wipe that you don't want to do for very reasonable reasons, and I don't use Apple any more so can't provide any insight personally, but it seems likely to be a similar problem relating to the system caching too much shit. Personally, MacOS peaked around 2015 Yosemite and has largely been downhill from there.</p>
<ul>
<li><em>I agree it's definitely a caching issue, just strange that we're not allowed to manually flush it. Apple deciding it knows best as usual! Thanks for the link to Lou's article, I remember reading that one! ~Becky</em></li>
</ul>
</blockquote>

        </div>
    </div>
<section>
<ol>
<li id="fn-1"><p><em>Yes, I could wipe the phone and reinstall from scratch, but that always terrifies me as I've managed to lose data in the past, despite backing up.</em><a href="#fnref-1">‚Ü©</a></p></li>
<li id="fn-2"><p><em>I've noticed a few third party launchers have popped up, initially free and then switching to paid once they've lured you in.</em><a href="#fnref-2">‚Ü©</a></p></li>
<li id="fn-3"><p><em>Purchased in "mint condition" off Amazon and it was most certainly <strong>not</strong> (badly scuffed &amp; peeling), but the lump must have been there since new as it was inside the silicone trim! It went straight back to Amazon.</em><a href="#fnref-3">‚Ü©</a></p></li>
</ol>
</section>


    

    
        
            <p>
                
                    <a rel="nofollow" href="https://onlinegoddess.net/blog/?q=tech">#tech</a>
                
            </p>
        

        
            


        

        
            
        
    


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UK House of Lords Votes to Extend Age Verification to VPNs (151 pts)]]></title>
            <link>https://reclaimthenet.org/uk-house-of-lords-votes-to-extend-age-verification-to-vpns</link>
            <guid>46763548</guid>
            <pubDate>Mon, 26 Jan 2026 09:35:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reclaimthenet.org/uk-house-of-lords-votes-to-extend-age-verification-to-vpns">https://reclaimthenet.org/uk-house-of-lords-votes-to-extend-age-verification-to-vpns</a>, See on <a href="https://news.ycombinator.com/item?id=46763548">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="4189490c" data-element_type="widget" data-widget_type="theme-post-content.default">
					<p>The UK House of Lords has voted to extend ‚Äúage assurance‚Äù requirements, effectively age verification mandates, to <a href="https://reclaimthenet.org/uk-online-safety-act-privacy-surveillance-vpn-anonymity-regulation">virtual private networks</a> (VPNs) and a wide range of online platforms under the <a href="https://reclaimthenet.org/uk-lawmakers-propose-mandatory-on-device-surveillance-and-vpn-age-verification">Children‚Äôs Wellbeing and Schools Bill</a>.</p>
<p>The decision deepens the reach of the already-controversial <a href="https://reclaimthenet.org/uk-age-verification-privacy-backlash">Online Safety Act</a>, linking child safety goals to mechanisms that could have severe effects on private communication and digital autonomy.</p>
<p>Under the existing Online Safety Act framework, ‚Äúuser-to-user services‚Äù include almost any online platform that enables individuals to post, share, or interact with content from others.</p>
<p>This definition covers social networks, messaging apps, forums, and online gaming services. Only a few forms of communication, such as email, SMS, MMS, and one-to-one live voice calls, are explicitly excluded.</p>
<p>While political messaging around the vote often described the move as a ‚Äúsocial media ban for under-16s,‚Äù the actual scope is considerably wider.</p>
<p>In effect, most interactive online platforms would now need to collect and verify age data from users, even where those services are not primarily aimed at children.</p>
<p>This represents a major expansion of identity checks across digital infrastructure, once considered neutral or privacy-protective, and one of the most disciplinarian proposals in the West.</p>
<p>Two key amendments advanced during the Lords debate on January 21.</p>
<p><a href="https://bills.parliament.uk/bills/3909/stages/20215/amendments/10027478">Amendment 92</a> (‚ÄúAction to Prohibit the Provision of VPN Services to Children in the United Kingdom‚Äù) requires VPNs that are ‚Äúoffered or marketed to persons in the United Kingdom‚Äù or ‚Äúprovided to a significant number of persons‚Äù to implement age assurance for UK users.</p>
<p>The measure passed by 207 Content votes to 159 Not Content votes.</p>
<p><img fetchpriority="high" decoding="async" src="https://reclaimthenet.org/wp-content/uploads/2026/01/vkyzx6oMLWni.jpg" alt="Children's Wellbeing and Schools Bill division (21 Jan 2026): Amendment 92 passed, Content 207 vs Not Content 159" width="2346" height="968" srcset="https://reclaimthenet.org/wp-content/uploads/2026/01/vkyzx6oMLWni.jpg 2346w, https://reclaimthenet.org/wp-content/uploads/2026/01/vkyzx6oMLWni-300x124.jpg 300w, https://reclaimthenet.org/wp-content/uploads/2026/01/vkyzx6oMLWni-1024x423.jpg 1024w, https://reclaimthenet.org/wp-content/uploads/2026/01/vkyzx6oMLWni-768x317.jpg 768w, https://reclaimthenet.org/wp-content/uploads/2026/01/vkyzx6oMLWni-1536x634.jpg 1536w, https://reclaimthenet.org/wp-content/uploads/2026/01/vkyzx6oMLWni-2048x845.jpg 2048w" sizes="(max-width: 2346px) 100vw, 2346px"></p>
<p><a href="https://bills.parliament.uk/bills/3909/stages/20215/amendments/10031850">Amendment 94a</a> (‚ÄúAction to Promote the Wellbeing of Children in Relation to Social Media‚Äù) mandates that all regulated user-to-user services introduce age assurance systems to prevent under-16s from ‚Äúbecoming or being users.‚Äù This proposal passed with 261 Content votes to 150 Not Content votes.</p>
<p>Both amendments will proceed to the Bill‚Äôs next stage, the third reading in the House of Lords.</p>
<p>Two other amendments, both more technologically intrusive, were discussed but rejected.</p>
<p>Amendment 93, introduced by Lord Nash, would have compelled smartphone and tablet manufacturers, distributors, and importers to install ‚Äútamper-proof system software which is highly effective at preventing the recording, transmitting (by any means, including livestreaming) and viewing of CSAM using that device.‚Äù</p>
<p>The only plausible way to enforce such a measure would be through constant, automated inspection of every photo, video, and stream on a device. This form of surveillance would have converted personal devices into continuous content monitors, raising severe privacy and accuracy concerns, including potential false positives.</p>
<p>Lord Nash stated: ‚ÄúOn Amendment 93, I have had a constructive discussion with Ministers on this issue and more discussions are in progress, so I will not push that to a vote today.‚Äù</p>
<p>Amendment 108, proposed by Lord Storey, would have required user-to-user services ‚Äúlikely to be accessed by children‚Äù to set their own minimum age thresholds and use age assurance to enforce them.</p>
<p>He argued that a single blanket ban under Amendment 94a was overly rigid. ‚ÄúHaving different minimum ages for different platforms would be a better solution,‚Äù he said, maintaining that his version would be more effective in practice.</p>
<p>Neither of these amendments passed, leaving Amendments 92 and 94a as the only ones to advance.</p>
<p>The discussion highlights a deepening push within UK legislation to merge digital identity checks with online participation.</p>
<p>While described as safeguarding children, the changes embed a new layer of identity verification across tools once used for privacy, such as VPNs.</p>
<p>These services, designed to conceal personal browsing data and protect against profiling, would now face obligations to verify who their users are. This is a contradiction that could erode one of the few remaining shields for private internet use.</p>
<p>For now, the most invasive surveillance measure, client-side scanning, has been set aside. However, the fact that it was seriously considered indicates continuing interest in embedding scanning mechanisms directly into personal devices.</p>
<p>Whether similar proposals reappear during the third reading remains to be seen.</p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Holy Grail of Linux Binary Compatibility: Musl and Dlopen (196 pts)]]></title>
            <link>https://github.com/quaadgras/graphics.gd/discussions/242</link>
            <guid>46762882</guid>
            <pubDate>Mon, 26 Jan 2026 07:41:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/quaadgras/graphics.gd/discussions/242">https://github.com/quaadgras/graphics.gd/discussions/242</a>, See on <a href="https://news.ycombinator.com/item?id=46762882">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="presentation" data-paste-markdown-skip="">
    <tbody data-target-translation-id="9388957" data-target-translation-type="discussion">
        <tr>
    <td>
        <p dir="auto">I guess using Go + Godot to <a href="https://github.com/quaadgras/graphics.gd/discussions/184" data-hovercard-type="discussion" data-hovercard-url="/quaadgras/graphics.gd/discussions/184/hovercard">build native &amp; installable Android &amp; iOS binaries</a> (without any proprietary SDKs) was too easy.  So it's time for a real challenge...</p>
<h3 dir="auto">Linux Binary Compatibility</h3>
<p dir="auto">(some background reading: <a href="https://jangafx.com/insights/linux-binary-compatibility" rel="nofollow">https://jangafx.com/insights/linux-binary-compatibility</a>)</p>
<p dir="auto">For a while now, it's been very easy to reliably ship command line software &amp; servers for Linux, just run <code>go build</code> and out pops a single static binary that will run on any Linux distribution running kernel 3.2 or later (which was released in 2012, so there's plenty of room for backwards compatibility).</p>
<p dir="auto">The problems begin to creep in when you want access to hardware accelerated graphics. All the GPU drivers on Linux require accessing dynamic libraries via the C ABI. These C libraries are built against a particular libc, which is most commonly <code>glibc</code> but there are also a selection of <code>musl</code>-based distributions. If you compile a <code>glibc</code> library or executable, it won't run on a <code>musl</code> system and vice-versa. That's a big incompatibility right there!</p>
<p dir="auto">In fact, I've directly experienced this, as I recently replaced the OS on my personal computer with the <code>musl</code> edition of <a href="https://voidlinux.org/" rel="nofollow">Void Linux</a>. Compiling the <a href="https://zed.dev/" rel="nofollow">Zed</a> editor with musl for example, was quite the challenge. It turns out that building <code>graphics.gd</code> projects on <code>musl</code> was also very broken. Go doesn't properly support <code>c-shared</code> or <code>c-archive</code> when building against <code>musl</code>.</p>
<p dir="auto">That's a problem, firstly because this is my distro now, I need to be able to build graphics.gd projects! Secondly, in theory, <code>musl</code> has better support for static linking than <code>glibc</code>; so if there's any solution to this Linux Binary Compatibility mess, it's probably going to have something to do with <code>musl</code>.</p>
<h3 dir="auto">Supporting <code>musl</code></h3>
<p dir="auto">To work around <a href="https://github.com/golang/go/issues/13492" data-hovercard-type="issue" data-hovercard-url="/golang/go/issues/13492/hovercard">these</a> <code>musl</code> issues with Go, I had to patch the runtime with a build-overlay that applies when building for <code>GOOS=musl</code>. This is a new <code>GOOS</code> that I've introduced to graphics.gd, specifically to make <code>musl</code> builds possible.</p>
<p dir="auto">Next up, I decided to ditch c-shared builds for <code>musl</code>, these were only convenient because you could easily plug and play Go into the official Godot binaries. The Godot Foundation doesn't provide official musl builds, so instead, I'm linking the Go code directly with Godot <code>c-archive</code> to end up with a single binary. Amazing, <code>graphics.gd</code> supports <code>musl</code> now!</p>
<p dir="auto">There's just one issue, this means whenever somebody wants to release their project for Linux, they would have to create two builds, a Linux <code>glibc</code> build + a <code>musl</code> build and somehow communicate to their users, to pick the correct binary. Hell, before I installed Void Linux <em>I</em> didn't even fully comprehend the differences between <code>musl</code> and <code>glibc</code>, this feels like I'm simply contributing to the problem!</p>
<h3 dir="auto">Single Static Binaries + Graphics</h3>
<p dir="auto">Hold up! Earlier I reported that a key benefit of <code>musl</code> was better static library support. There should be a way to build a graphics.gd project into a single static binary. Well, here's the thing. Yes, you can totally do this. Godot includes all of it's dependencies on Linux, everything else is dynamically loaded at runtime, so just add the <code>-static</code> command and...</p>
<p dir="auto"><code>ERROR Dynamic loading not supported</code></p>
<p dir="auto">Ouch, Godot wants to use <code>dlopen</code> to interface with X11, Wayland, OpenGL, Vulkan etc. As it turns out, <code>musl</code> refuses to implement <code>dlopen</code> for static binaries. They don't want anyone to load a <code>glibc</code> library from <code>musl</code> because there are fundamental incompatibilities between how they implement  TLS (thread-local-storage).</p>
<p dir="auto">Don't worry though! As <code>dlopen</code> is compiled as a weak symbol, this means, that as long as <code>graphics.gd</code> implements it, there's still a chance to get a single static binary that can execute on any Linux system 3.2 onwards.</p>
<h3 dir="auto">The Holy Grail</h3>
<p dir="auto">There's some precedent for this, there's the <a href="https://github.com/graphitemaster/detour">detour</a> technique in C which will let you <code>dlopen</code> SDL and show graphics when running <em>without a standard library</em>. There's also Cosmopolitan's <a href="https://github.com/jart/cosmopolitan/tree/master/libc/dlopen">dlopen</a> which uses a similar technique. So the solution here is to extend this for <code>musl</code>.</p>
<p dir="auto">The way this works, is by including (or compiling) a small <code>C</code> program for the target machine. We load the program and execute into it from the same process. This program brings in the host's dynamic linker so that we can <em>steal</em> the system's <code>dlopen</code> and <code>longjmp</code> back into <code>graphics.gd</code>. We wrap any dynamically loaded functions with an assembly trampoline that switches to the system's libc TLS for the duration of the call. It all starts looking a lot like cgo.</p>
<p dir="auto">So after much hair pulling and LLM wrangling, it turns out that <code>musl</code> + <code>dlopen</code> is all we need to produce single static binaries + graphics for Linux. Everyone can now enjoy the <em>Go</em> single-static-binary experience on Linux with full support for hardware accelerated graphics.</p>
<h3 dir="auto">Try it!</h3>
<p dir="auto">Here's a build of the <code>graphics.gd</code> Dodge The Creeps sample project that should execute (and hopefully render graphics) on any Linux system with <code>gcc</code> installed (we don't embed the helper binaries yet).</p>
<p dir="auto"><a href="https://release.graphics.gd/dodge_the_creeps.static" rel="nofollow">https://release.graphics.gd/dodge_the_creeps.static</a></p>
<p dir="auto">You can also cross-compile your own project (on any supported platform)</p>
<p dir="auto"><code>GOOS=musl GOARCH=amd64 gd build</code></p>
<p dir="auto"><em>Note you may need to delete your <code>export_presets.cfg</code> so that the new musl export preset is added to your project</em></p>
    </td>
  </tr>

    </tbody>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The browser is the sandbox (305 pts)]]></title>
            <link>https://simonwillison.net/2026/Jan/25/the-browser-is-the-sandbox/</link>
            <guid>46762150</guid>
            <pubDate>Mon, 26 Jan 2026 05:23:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2026/Jan/25/the-browser-is-the-sandbox/">https://simonwillison.net/2026/Jan/25/the-browser-is-the-sandbox/</a>, See on <a href="https://news.ycombinator.com/item?id=46762150">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p><strong><a href="https://aifoc.us/the-browser-is-the-sandbox/">the browser is the sandbox</a></strong>. Paul Kinlan is a web platform developer advocate at Google and recently turned his attention to coding agents. He quickly identified the importance of a robust sandbox for agents to operate in and put together these detailed notes on how the web browser can help:</p>
<blockquote>
<p>This got me thinking about the browser. Over the last 30 years, we have built a sandbox specifically designed to run incredibly hostile, untrusted code from anywhere on the web, the instant a user taps a URL. [...]</p>
<p>Could you build something like Cowork in the browser? Maybe. To find out, I built a demo called <a href="http://co-do.xyz/">Co-do</a> that tests this hypothesis. In this post I want to discuss the research I've done to see how far we can get, and determine if the browser's ability to run untrusted code is useful (and good enough) for enabling software to do more for us directly on our computer.</p>
</blockquote>
<p>Paul then describes how the three key aspects of a sandbox - filesystem, network access and safe code execution - can be handled by browser technologies: the <a href="https://developer.chrome.com/docs/capabilities/web-apis/file-system-access">File System Access API</a> (still Chrome-only as far as I can tell), CSP headers with <code>&lt;iframe sandbox&gt;</code> and WebAssembly in Web Workers.</p>
<p>Co-do is a very interesting demo that illustrates all of these ideas in a single application:</p>
<p><img alt="Screenshot of Co-do application interface with robot logo. Left sidebar shows WORKSPACE section with &quot;Select Folder&quot; button, &quot;Selected folder: blog-drafts (Live updates enabled)&quot; in green text, and FILES list including gtr-t5-large.md, chatgpt.md, llm-digest-october-2..., weeknotes-2024-04-23..., lmarena-april-2025.md, weeknotes-chatgpt-fo..., tests-not-optional-c..., weeknotes-13-mar-202..., issue-for-notes.md, workshop-git-scrapin.... Main chat area shows user message &quot;Three most recently edited files?&quot; in orange bubble, &quot;Using 26 tools...&quot; dropdown, and AI response: &quot;Now I need to get the metadata for these files to find the most recently edited ones. Since there are many files, let me get metadata for a sample of them to find the recent ones:Based on the metadata I've gathered, the three most recently edited files are:&quot; followed by table with columns File and Last Modified showing predictions-2026.md (Jan 8, 2026), december-2025.md (Jan 2, 2026), gemini-3-flash.md (Dec 17, 2025). Green &quot;Response complete&quot; banner at bottom with input field placeholder &quot;Ask me to help with your files...&quot; and notice &quot;File contents are sent to your selected AI provider&quot;." src="https://static.simonwillison.net/static/2026/codo.jpg"></p>
<p>You select a folder full of files and configure an LLM provider and set an API key, Co-do then uses CSP-approved API calls to interact with that provider and provides a chat interface with tools for interacting with those files. It does indeed feel similar to <a href="https://simonwillison.net/2026/Jan/12/claude-cowork/">Claude Cowork</a> but without running a multi-GB local container to provide the sandbox.</p>
<p>My biggest complaint about <code>&lt;iframe sandbox&gt;</code> remains how thinly documented it is, especially across different browsers. Paul's post has all sorts of useful details on that which I've not encountered elsewhere, including a complex <a href="https://aifoc.us/the-browser-is-the-sandbox/#the-double-iframe-technique">double-iframe technique</a> to help apply network rules to the inner of the two frames.</p>
<p>Thanks to this post I also learned about the <code>&lt;input type="file" webkitdirectory&gt;</code> tag which turns out to work on Firefox, Safari <em>and</em> Chrome and allows a browser read-only access to a full directory of files at once. I had Claude knock up a <a href="https://tools.simonwillison.net/webkitdirectory">webkitdirectory demo</a> to try it out and I'll certainly be using it for projects in the future.</p>
<p><img alt="Screenshot of a dark-themed file explorer application. Top stats bar shows: 12179 TOTAL FILES, 2079 FOLDERS, 244 MB TOTAL SIZE, 97 FILE TYPES in cyan text. Search bar with placeholder &quot;Search files...&quot; and &quot;All types&quot; dropdown. Left panel labeled &quot;File tree&quot; (showing 12179) displays folder hierarchy: datasette > .claude > skills > building-datasette-plugins containing HOOKS.md (10.7 KB, selected/highlighted), INTERNALS.md (10.1 KB), SKILL.md (3.7 KB), TESTING.md (8.4 KB), settings.local.json (280 B); also shows .eggs folder with pytest_runner-6.0.1-py3.9.egg. Right panel &quot;File preview&quot; shows selected file details: Name: HOOKS.md, Path: datasette/.claude/skills/building-datasette-plugins/HOOKS.md, Size: 10.7 KB, Type: text/markdown, Last modified: 12/20/2025, 9:28:59 AM. Preview content shows: &quot;# Plugin Hooks Reference&quot; followed by &quot;All hooks use the @hookimpl decorator. Accept only the parameters you need.&quot; then &quot;## Database Connection Hooks&quot; and &quot;### prepare_connection(conn, database, datasette)&quot; with description &quot;Called when a new SQLite connection is created. Use to register custom SQL functions.&quot; Bottom section &quot;File type distribution&quot; shows horizontal bar chart: .py (4439), .no ext (3358), .dat (1068), .pyc (925), .txt (332), .mo (321), .po (321), .html (249)." src="https://static.simonwillison.net/static/2026/webkit-file-tree.jpg"></p>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Iran's internet blackout may become permanent, with access for elites only (371 pts)]]></title>
            <link>https://restofworld.org/2026/iran-blackout-tiered-internet/</link>
            <guid>46761822</guid>
            <pubDate>Mon, 26 Jan 2026 04:18:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://restofworld.org/2026/iran-blackout-tiered-internet/">https://restofworld.org/2026/iran-blackout-tiered-internet/</a>, See on <a href="https://news.ycombinator.com/item?id=46761822">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<!-- Article Start -->
				
<p>Iran‚Äôs near-total communications blackout has entered its 16th day, but that‚Äôs just a live test.</p>



<p>Following a repressive crackdown on protests, the government is now building a system that grants web access only to security-vetted elites, while locking 90 million citizens inside an intranet.</p>



<p>Government spokesperson Fatemeh Mohajerani confirmed international access <a href="https://www.rferl.org/a/iran-internet-shutdown-censorship/33654841.html">will not be restored</a> until at least late March. Filterwatch, which monitors Iranian internet censorship from Texas, cited government sources, including Mohajerani, saying access will ‚Äú<a href="https://filter.watch/english/2026/01/15/iran-enters-a-new-age-of-digital-isolation-2/">never return to its previous form</a>.‚Äù</p>



<p>This is what makes Iran‚Äôs attempt unique: Other authoritarian states built walls before their populations went online. Iran is trying to seal off a connected economy <a href="https://www.crisisgroup.org/stm/middle-east-north-africa/iran-united-states/iran-crisis-time-change-within">already in freefall</a>.&nbsp;</p>



<p>The system is called Barracks Internet, according to confidential planning documents <a href="https://filter.watch/english/2026/01/15/iran-enters-a-new-age-of-digital-isolation-2/">obtained by Filterwatch</a>. Under this architecture, access to the global web will be granted only through a strict security whitelist.</p>


		<figure>
			<div>
				<p><img src="https://fastly.restofworld.org/uploads/2026/01/GettyImages-2255474345.jpg?width=800&amp;dpr=2&amp;crop=16:9" srcset="https://fastly.restofworld.org/uploads/2026/01/GettyImages-2255474345.jpg?width=1900&amp;dpr=2 3800w, https://fastly.restofworld.org/uploads/2026/01/GettyImages-2255474345.jpg?width=1700&amp;dpr=2 3400w, https://fastly.restofworld.org/uploads/2026/01/GettyImages-2255474345.jpg?width=1500&amp;dpr=2 3000w, https://fastly.restofworld.org/uploads/2026/01/GettyImages-2255474345.jpg?width=1300&amp;dpr=2 2600w, https://fastly.restofworld.org/uploads/2026/01/GettyImages-2255474345.jpg?width=1100&amp;dpr=2 2200w, https://fastly.restofworld.org/uploads/2026/01/GettyImages-2255474345.jpg?width=900&amp;dpr=2 1800w, https://fastly.restofworld.org/uploads/2026/01/GettyImages-2255474345.jpg?width=700&amp;dpr=2 1400w, https://fastly.restofworld.org/uploads/2026/01/GettyImages-2255474345.jpg?width=500&amp;dpr=2 1000w, https://fastly.restofworld.org/uploads/2026/01/GettyImages-2255474345.jpg?width=300&amp;dpr=2 600w, https://fastly.restofworld.org/uploads/2026/01/GettyImages-2255474345.jpg?width=100&amp;dpr=2 200w, " sizes="(min-width: 640px) 600px, calc(100vw - 40px)" alt="Silhouettes of people near a fire on a bridge, with smoke and orange flames illuminating the scene at night." loading="lazy">
					
				</p>
			</div>
				<figcaption itemprop="caption description">
		<span>An armed member of the Iranian security forces in front of a burning building during a protest in Tehran, Iran, on January 9, 2026.</span>
		<span itemprop="copyrightHolder">Getty Images</span>
	</figcaption>
		</figure>


<p>‚ÄúThe regime is terrified of one thing: Iranians being heard telling their own truth and having crimes documented,‚Äù Mahsa Alimardani, a digital rights researcher at U.S.-based Witness, which trains activists to use video for advocacy, told <em>Rest of World</em>. ‚ÄúThe question becomes: How do we give Iranians an unbreakable voice?‚Äù</p>



<p>The idea of tiered internet access is not new in Iran. Since at least 2013, the regime has quietly issued ‚Äúwhite SIM cards,‚Äù giving unrestricted global internet access to approximately 16,000 people. The system gained public attention in November 2025 when X‚Äôs location feature revealed that certain accounts, including the communications minister, were <a href="https://iranwire.com/en/features/146582-iran-internet-ban-the-islamic-republic-filters-for-millions-of-people-not-for-its-own/">connecting directly from inside</a> Iran, <a href="https://iranwire.com/en/features/146582-iran-internet-ban-the-islamic-republic-filters-for-millions-of-people-not-for-its-own/"></a>despite X being blocked since 2009.</p>



<p>What is different now is scale and permanence. The current blackout tests infrastructure designed to make two-tier access the default, not a temporary crackdown.</p>



<p>Only a handful of nations have attempted to wall off their citizens from the global internet. North Korea‚Äôs Kwangmyong intranet was built from scratch for a population that never had connectivity. China constructed its Great Firewall over two decades while nurturing domestic alternatives such as WeChat and Alibaba. Iran is attempting to do both in weeks, with no domestic alternatives.</p>


<figure><blockquote><p>The question becomes: How do we give Iranians an unbreakable voice?‚Äù</p></blockquote></figure>


<p>The economic costs of the blackout are staggering. Iran‚Äôs deputy communications minister <a href="https://abcnews.go.com/Business/wireStory/internet-shutdown-squeezes-irans-ailing-businesses-hurt-crashing-129374950">pegged the daily losses</a> at as much as $4.3 million. <a href="https://abcnews.go.com/Business/wireStory/internet-shutdown-squeezes-irans-ailing-businesses-hurt-crashing-129374950">NetBlocks estimates</a> the true cost exceeds $37 million daily. More than 10 million Iranians depend directly on digital platforms for their livelihoods.</p>



<p>Tipax, one of Iran‚Äôs largest private delivery companies handling about 320,000 daily shipments before the protests, now processes fewer than a few hundred, <a href="https://filter.watch/english/2026/01/15/iran-enters-a-new-age-of-digital-isolation-2/">according to Filterwatch</a>. The company operates a nationwide logistics network comparable to FedEx in the U.S. market.</p>



<p>The government <a href="https://www.france24.com/en/asia-pacific/20260118-live-iran-nationwide-protests-official">fired Irancell‚Äôs CEO</a> for failing to comply with shutdown orders. Irancell, the country‚Äôs second-largest mobile operator with 66 million subscribers, is partly owned by South Africa‚Äôs MTN Group. Alireza Rafiei was removed for disobeying orders on ‚Äúrestriction of internet access in crisis situations,‚Äù <a href="https://www.newarab.com/news/iranian-mobile-operator-ceo-fired-not-cutting-internet">according to</a> Fars news agency.</p>



<p>Foreign telecom partners have left Iran in recent days under security escort, without media coverage, <a href="https://filter.watch/english/2026/01/15/iran-enters-a-new-age-of-digital-isolation-2/">according to Filterwatch</a>. This may signal the end of international cooperation in critical infrastructure, replaced by the Revolutionary Guard‚Äôs construction arm or limited cooperation with Huawei.</p>



<p>Technical experts doubt the regime can sustain Barracks Internet without crippling the economy. Georgia Tech‚Äôs Internet Intelligence Lab, which has <a href="https://theconversation.com/irans-latest-internet-blackout-extends-to-phones-and-starlink-273439">tracked Iran‚Äôs shutdowns since the Arab Spring</a>, called the blackout ‚Äúthe most sophisticated and most severe in Iran‚Äôs history.‚Äù Its measurements show about 3% connectivity persists, likely government officials and state services.</p>


<figure><blockquote><p>We need to revolutionize access to the internet.‚Äù</p></blockquote></figure>


<p>Kaveh Ranjbar, former chief technology officer at RIPE NCC, the body managing European internet infrastructure, calls the plan a ‚Äúdigital airlock‚Äù that can‚Äôt fully seal a modern economy. No country has hermetically sealed a functioning digital economy, he <a href="https://www.newarab.com/analysis/iran-determined-turn-its-digital-space-north-korea-20">told <em>The New Arab</em></a>.</p>



<p>Activists have smuggled an estimated 50,000 Starlink satellite terminals into Iran since 2022, when the Biden <a href="https://www.vox.com/technology/395656/starlink-iran-internet-shutdown-elon-musk">administration exempted the service</a> from sanctions. SpaceX has made the service free for Iranian users.&nbsp;</p>



<p>The government claims it cut off 40,000 Starlink connections and jammed some terminals during the blackout, though others remain operational after firmware updates to <a href="https://www.bbc.com/persian/articles/cj3dn49d489o">bypass government blocking</a>. Still, the technology remains <a href="https://restofworld.org/2026/iran-starlink-internet-shutdown/">vulnerable to signal jamming</a>, meaning the regime holds ultimate leverage.</p>



<p>‚ÄúWe need to revolutionize access to the internet,‚Äù said Alimardani. ‚ÄúAnd move beyond the limiting structures and norms of ‚Äòinternet sovereignty.‚Äô‚Äù</p>
				<!-- Article End -->
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Environmentalists worry Google behind bid to control Oregon town's water (144 pts)]]></title>
            <link>https://www.sfgate.com/national-parks/article/mount-hood-water-google-21307223.php</link>
            <guid>46761614</guid>
            <pubDate>Mon, 26 Jan 2026 03:40:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sfgate.com/national-parks/article/mount-hood-water-google-21307223.php">https://www.sfgate.com/national-parks/article/mount-hood-water-google-21307223.php</a>, See on <a href="https://news.ycombinator.com/item?id=46761614">Hacker News</a></p>
Couldn't get https://www.sfgate.com/national-parks/article/mount-hood-water-google-21307223.php: Error: Request failed with status code 402]]></description>
        </item>
        <item>
            <title><![CDATA[Tech CEOs attend Amazon-funded "Melania" screening at White House (106 pts)]]></title>
            <link>https://www.hollywoodreporter.com/movies/movie-news/white-house-vip-melania-screening-mike-tyson-tim-cook-1236484037/</link>
            <guid>46761370</guid>
            <pubDate>Mon, 26 Jan 2026 03:04:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hollywoodreporter.com/movies/movie-news/white-house-vip-melania-screening-mike-tyson-tim-cook-1236484037/">https://www.hollywoodreporter.com/movies/movie-news/white-house-vip-melania-screening-mike-tyson-tim-cook-1236484037/</a>, See on <a href="https://news.ycombinator.com/item?id=46761370">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
	The official <a rel="noreferrer noopener" target="_blank" href="https://www.hollywoodreporter.com/movies/movie-news/melania-doc-to-premiere-at-renamed-trump-kennedy-center-1236457244/">premiere</a> of the Amazon-backed <em>Melania</em> movie isn‚Äôt until Thursday night at the Kennedy Center, but with a snowstorm about to bear down on Washington, D.C., First Lady <a href="https://www.hollywoodreporter.com/t/melania-trump/" id="auto-tag_melania-trump_1" data-tag="melania-trump">Melania Trump</a> is opening the <a href="https://www.hollywoodreporter.com/t/white-house/" id="auto-tag_white-house_1" data-tag="white-house">White House</a> for a special dinner and screening for 70 of her friends, family members and assorted VIPs.</p>



<p>
	The event, which has not been promoted or advertised, is taking place in the White House on Saturday. Guests include director <a href="https://www.hollywoodreporter.com/t/brett-ratner/" id="auto-tag_brett-ratner_1" data-tag="brett-ratner">Brett Ratner</a>;&nbsp;Queen Rania of Jordan; Zoom CEO Eric Yuan; Apple CEO Tim Cook;&nbsp;New York Stock Exchange CEO Lynn Martin; AMD CEO Lisa Su; Mike&nbsp;Tyson; socialite and Fiat heiress Azzi Agnelli; self-help guru Tony Robbins; and photographer Ellen von Unwerth, who shot the movie poster for the film.

	</p>






<p>
	Barron <a href="https://www.hollywoodreporter.com/t/trump/" id="auto-tag_trump_1" data-tag="trump">Trump</a> and Ratner‚Äôs mother will also be on hand, along with Mike Hopkins of Amazon Studios, Amazon CEO Andy Jassy and Marc Beckman, a senior adviser and longtime manager to the First Lady who was reportedly involved in every aspect of the project.</p>



<p>
	Since the usual White House screening room in the East Wing has been shut down to construct the <a rel="noreferrer noopener" target="_blank" href="https://www.hollywoodreporter.com/news/politics-news/white-house-movie-theater-demolished-ballroom-east-wing-1236408712/">Trump ballroom</a>, Melania built a make-shift theater, with state of the art sound and film equipment (overseen by&nbsp;Ratner himself), and a giant movie screen brought in for the occasion.</p>



<p>
	Sources say nobody outside of Melania, Ratner and a very small group of their associates have yet to see the movie, <a rel="noreferrer noopener" target="_blank" href="https://www.hollywoodreporter.com/news/politics-news/trump-wayne-gretsky-melania-movie-1236470802/">including the President</a> and his advisors, who are watching it Saturday night for the first time.</p>



<p>
	A full military band will meet guests&nbsp;at the door to play ‚ÄúMelania‚Äôs Waltz,‚Äù a song&nbsp;composed especially for the film by Hollywood composer Tony Neiman. The band will stay on to play songs from Hollywood movies for the guests.

	</p>




<p>
	The event will have glossy, commemorative black and white popcorn boxes for guests, served by gloved waiters so they won‚Äôt get fingerprints on them.&nbsp;Specially&nbsp;framed tickets to&nbsp;the movie will also be available to take home.</p>



<p>
	The Amazon-funded&nbsp;movie&nbsp;(to the tune of $40 million)&nbsp;is opening nationwide in theaters on Jan. 30, preceded by a Kennedy Center premiere for&nbsp;VIPs&nbsp;and dignitaries on Jan 29. Concurrently with the Kennedy Center, they‚Äôre having smaller premieres in 20 cities including Nashville, Boston, San Francisco and Vegas for ‚Äúlocal VIPs,‚Äù friends and supporters of the First Lady, featuring red carpets and step and repeats.</p>



<p>
	Billboards for the doc have already gone up, alongside a massive TV and radio advertising blitz. Though the <em>Melania</em> team hasn‚Äôt commented on the marketing dollars spent on the project, they deny <a rel="noreferrer noopener nofollow" target="_blank" href="https://puck.news/will-the-melania-trump-movie-actually-make-money/">Matt Belloni‚Äôs report</a> of $35 million.</p>



<p>
	The movie tracks Melania from Trump‚Äôs 2025 presidential campaign to inauguration day. A source close to the First Lady indicates that Ratner spent months living at Mar-a-Lago for shooting.</p>














</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ICE tells legal observer, 'We have a database, now you're a domestic terrorist' (163 pts)]]></title>
            <link>https://reason.com/2026/01/23/ice-tells-legal-observer-we-have-a-nice-little-database-and-now-youre-considered-a-domestic-terrorist/</link>
            <guid>46761130</guid>
            <pubDate>Mon, 26 Jan 2026 02:25:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reason.com/2026/01/23/ice-tells-legal-observer-we-have-a-nice-little-database-and-now-youre-considered-a-domestic-terrorist/">https://reason.com/2026/01/23/ice-tells-legal-observer-we-have-a-nice-little-database-and-now-youre-considered-a-domestic-terrorist/</a>, See on <a href="https://news.ycombinator.com/item?id=46761130">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
																		<article>
	<header>
							<p>
				<a href="https://reason.com/tag/ice/">ICE</a>
			</p>
				
		
					<h2>The video is the latest example of federal immigration authorities labeling anyone who opposes them a "domestic terrorist."</h2>
				<p>
				
			
						<span>|</span>
			<time datetime="2026-01-23T15:23:57-05:00">1.23.2026 3:23 PM</time>
			
</p>
					
										<div>
						<div>
			<picture>
									<source type="image/webp" data-lazy-srcset="https://d2eehagpk5cl65.cloudfront.net/img/c2400x1350-w2400-q80/uploads/2026/01/maine-ice-2400x1350.jpeg.webp 2400w,https://d2eehagpk5cl65.cloudfront.net/img/c1200x675-w1200-q80/uploads/2026/01/maine-ice-1200x675.jpeg.webp 1200w,https://d2eehagpk5cl65.cloudfront.net/img/c800x450-w800-q80/uploads/2026/01/maine-ice-800x450.jpeg.webp 800w,https://d2eehagpk5cl65.cloudfront.net/img/c600x338-w600-q80/uploads/2026/01/maine-ice-600x338.jpeg.webp 600w,https://d2eehagpk5cl65.cloudfront.net/img/c331x186-w331-q80/uploads/2026/01/maine-ice-331x186.jpeg.webp 331w,https://d2eehagpk5cl65.cloudfront.net/img/c1200x675-w1200-q80/uploads/2026/01/maine-ice-1200x675.jpeg.webp 1200w,https://d2eehagpk5cl65.cloudfront.net/img/c1920x1080-w1920-q80/uploads/2026/01/maine-ice-1920x1080.jpeg.webp 1920w" sizes="(min-width: 753px) 70vw, (min-width: 1190px) 768px, 100vw">
											<source type="image/jpeg" data-lazy-srcset="https://d2eehagpk5cl65.cloudfront.net/img/c2400x1350-w2400-q80/uploads/2026/01/maine-ice-2400x1350.jpeg 2400w,https://d2eehagpk5cl65.cloudfront.net/img/c1200x675-w1200-q80/uploads/2026/01/maine-ice-1200x675.jpeg 1200w,https://d2eehagpk5cl65.cloudfront.net/img/c800x450-w800-q80/uploads/2026/01/maine-ice-800x450.jpeg 800w,https://d2eehagpk5cl65.cloudfront.net/img/c600x338-w600-q80/uploads/2026/01/maine-ice-600x338.jpeg 600w,https://d2eehagpk5cl65.cloudfront.net/img/c331x186-w331-q80/uploads/2026/01/maine-ice-331x186.jpeg 331w,https://d2eehagpk5cl65.cloudfront.net/img/c1200x675-w1200-q80/uploads/2026/01/maine-ice-1200x675.jpeg 1200w,https://d2eehagpk5cl65.cloudfront.net/img/c1920x1080-w1920-q80/uploads/2026/01/maine-ice-1920x1080.jpeg 1920w" sizes="(min-width: 753px) 70vw, (min-width: 1190px) 768px, 100vw">
													<img src="https://d2eehagpk5cl65.cloudfront.net/img/c800x450-w800-q80/uploads/2026/01/maine-ice-800x450.jpeg" width="1200" height="675" title="Maine ICE agent" alt="Maine ICE agent | Reddit" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201200%20675'%3E%3C/svg%3E" data-lazy-src="https://d2eehagpk5cl65.cloudfront.net/img/c800x450-w800-q80/uploads/2026/01/maine-ice-800x450.jpeg">
			</picture>
		</div>
							<p><span>
					 (Reddit)				</span>
					</p>
								</div>
							
	</header>
	<div>
							<p><a href="https://www.reddit.com/r/Maine/comments/1qku4fs/ice_tells_legal_observer_she_is_a_domestic/"><span>Video</span></a><span> taken this morning in Maine shows an Immigration and Customs Enforcement (ICE) officer taking pictures of a legal observer's car. When she asks why he's doing that, he says, "Because we have a nice little database, and now you're considered a domestic terrorist."</span></p>
<blockquote data-width="500" data-dnt="true">
<p lang="en" dir="ltr">ICE agent asked why he's taking pictures of a legal observer's car, replies: "Cuz we have a nice little database and now you're considered a domestic terrorist. So have fun with that." <a href="https://t.co/IbyRqycSc2">pic.twitter.com/IbyRqycSc2</a></p>
<p>‚Äî Ken Klippenstein (NSPM-7 Compliant) (@kenklippenstein) <a href="https://twitter.com/kenklippenstein/status/2014752856843161969?ref_src=twsrc%5Etfw">January 23, 2026</a></p></blockquote>

<p><span>The video is the latest example of the Department of Homeland Security (DHS) labeling anyone who engages in First Amendment‚Äìprotected activity opposing the Trump administration's mass deportation program as a "domestic terrorist" and suggesting they'll be subject to federal investigations.</span></p>
<p>However, Tricia McLaughlin, DHS assistant secretary for public affairs, denied such a database exists.</p>
<p>"There is NO database of 'domestic terrorists' run by DHS." McLaughlin said in an emailed statement to <em>Reason</em>. "We do of course monitor and investigate and refer all threats, assaults and obstruction of our officers to the appropriate law enforcement. Obstructing and assaulting law enforcement is a felony and a federal crime."</p>
<p>DHS has issued <a href="https://reason.com/2025/10/22/homeland-security-wont-stop-lying-about-who-immigration-enforcers-are-arresting/">numerous factually incorrect statements</a> attributed to McLaughlin.</p>
<p>Independent journalist Ken Klippenstein <a href="https://www.kenklippenstein.com/p/ice-making-list-of-anyone-who-films">reported</a> today that an unnamed federal law enforcement official told him that DHS "has ordered immigration officers to gather identifying information about anyone filming them."</p>
<p><span>In September, President Donald Trump issued a </span><a href="https://www.whitehouse.gov/presidential-actions/2025/09/countering-domestic-terrorism-and-organized-political-violence/"><span>memo</span></a><span> ordering federal law enforcement to focus on ideologies that are allegedly fueling "domestic terrorism." These include "anti-Americanism, anti-capitalism, and anti-Christianity; support for the overthrow of the United States Government; extremism on migration, race, and gender," as well as opposition to "foundational American principles (e.g., support for law enforcement and border control)."</span></p>
<p><span>As </span><i><span>Reason</span></i><span>'s Joe Lancaster </span><a href="https://reason.com/2025/09/29/trumps-executive-order-against-political-violence-is-an-un-american-attack-on-free-speech/"><span>wrote</span></a> at the time<span>, the memo was "an assault on the First Amendment" that listed protected free speech "as evidence of criminality that requires federal intervention."</span></p>
<p><span>And since the Trump administration's deportation campaign began last year, DHS officials have </span><a href="https://reason.com/2025/12/22/dhs-says-recording-or-following-law-enforcement-sure-sounds-like-obstruction-of-justice/"><span>repeatedly insisted</span></a><span> that following and recording federal immigration agents in public is a violation of a federal statute that makes it a crime to assault or impede law enforcement officers.</span></p>
<p><span>There have been dozens of recorded instances of ICE and Border Patrol officers </span><a href="https://reason.com/2026/01/12/video-shows-border-patrol-threaten-legal-observer-in-key-largo-for-following-him/"><span>harassing</span></a><span>, assaulting, and </span><a href="https://www.oregonlive.com/politics/2025/11/ice-detains-us-citizen-for-7-hours-after-she-photographed-agents-in-gresham.html"><span>detaining</span></a><span> people for filming and following them, even though there is a </span><a href="https://reason.com/2026/01/08/you-have-the-right-to-record-ice/"><span>well-established First Amendment right</span></a><span> to record and observe the police.</span></p>
<p><span>For example, today </span><i><span>Slate </span></i><span>published the </span><a href="https://slate.com/news-and-politics/2026/01/arrested-for-observing-ice-minnesota-lesson.html?utm_sf_post_ref=657901374&amp;utm_sf_cserv_ref=did%3Aplc%3Awwiikm77cjmgpjn7i7iao43p&amp;via=rss_socialflow_facebook"><span>first-person account</span></a><span> of Brandon Sig√ºenza, a Minneapolis man who was volunteering with a local group that monitors and records ICE activity. Federal immigration officers surrounded his car, smashed out his windows, roughly arrested him, and detained him for hours.</span></p>
<p><span>Sig√ºenza also submitted a </span><a href="https://storage.courtlistener.com/recap/gov.uscourts.mnd.229758/gov.uscourts.mnd.229758.99.0.pdf"><span>sworn declaration</span></a><span> describing his experiences in a civil rights lawsuit challenging the DHS' actions in Minneapolis. </span></p>
<p><span>Last week, the federal judge in that lawsuit </span><a href="https://storage.courtlistener.com/recap/gov.uscourts.mnd.229758/gov.uscourts.mnd.229758.85.0_1.pdf"><span>ruled</span></a><span> that the plaintiffs were likely to succeed on their claims that federal officers engaged in targeted First Amendment retaliation against them. The protesters' and monitors' actions, the judge found, "did not forcibly obstruct or impede the agents' work."</span></p>
<p><em>Reason</em> has argued through both Republican and Democratic administrations that government fears of "domestic terrorism" are a <a href="https://reason.com/2021/09/22/domestic-terrorism-fears-will-be-used-to-justify-increased-snooping-and-harassment/">pretext for increased snooping and harassment</a>. and comments like the ones by the ICE agent in Maine are only more evidence in favor of that argument.</p>
						</div>
		
</article>

					<nav>
	
		<p>
        <a href="https://reason.com/2026/01/23/the-trump-administration-plans-to-deport-iranians-amid-deadly-crackdown-in-iran/" data-ga-click="true" data-ga-action="Next Article Click" data-ga-label="The Trump Administration Plans To Deport Iranians Amid Deadly Crackdown in Iran"><span>NEXT:</span> The Trump Administration Plans To Deport Iranians Amid Deadly Crackdown in Iran</a>
    </p>
	
	<span><a rel="tag" href="https://reason.com/tag/ice/">ICE</a><a rel="tag" href="https://reason.com/category/civil-liberties/free-speech/">Free Speech</a><a rel="tag" href="https://reason.com/tag/first-amendment/">First Amendment</a><a rel="tag" href="https://reason.com/category/immigration/">Immigration</a><a rel="tag" href="https://reason.com/tag/trump-administration/">Trump Administration</a><a rel="tag" href="https://reason.com/tag/maine/">Maine</a><a rel="tag" href="https://reason.com/category/criminal-justice/police-brutality/">Police Abuse</a><a rel="tag" href="https://reason.com/category/civil-liberties/">Civil Liberties</a><a rel="tag" href="https://reason.com/category/civil-liberties/domestic-spying/">Domestic spying</a><a rel="tag" href="https://reason.com/category/criminal-justice/law-enforcement/">Law enforcement</a><a rel="tag" href="https://reason.com/category/civil-liberties/surveillance/">Surveillance</a></span>			
				
	</nav>
				
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft suspects some PCs might not boot after Windows 11 January 2026 Update (110 pts)]]></title>
            <link>https://www.windowslatest.com/2026/01/25/microsoft-suspects-some-pcs-might-not-boot-after-windows-11-january-2026-update-kb5074109/</link>
            <guid>46761061</guid>
            <pubDate>Mon, 26 Jan 2026 02:15:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.windowslatest.com/2026/01/25/microsoft-suspects-some-pcs-might-not-boot-after-windows-11-january-2026-update-kb5074109/">https://www.windowslatest.com/2026/01/25/microsoft-suspects-some-pcs-might-not-boot-after-windows-11-january-2026-update-kb5074109/</a>, See on <a href="https://news.ycombinator.com/item?id=46761061">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p><a href="https://www.windowslatest.com/wp-content/uploads/2026/01/PCs-not-booting-with-black-screen-on-Windows-11.jpg" data-caption=""><img width="696" height="362" src="https://www.windowslatest.com/wp-content/uploads/2026/01/PCs-not-booting-with-black-screen-on-Windows-11-696x362.jpg" srcset="https://www.windowslatest.com/wp-content/uploads/2026/01/PCs-not-booting-with-black-screen-on-Windows-11-696x362.jpg 696w, https://www.windowslatest.com/wp-content/uploads/2026/01/PCs-not-booting-with-black-screen-on-Windows-11-300x156.jpg 300w, https://www.windowslatest.com/wp-content/uploads/2026/01/PCs-not-booting-with-black-screen-on-Windows-11-1024x533.jpg 1024w, https://www.windowslatest.com/wp-content/uploads/2026/01/PCs-not-booting-with-black-screen-on-Windows-11-768x399.jpg 768w, https://www.windowslatest.com/wp-content/uploads/2026/01/PCs-not-booting-with-black-screen-on-Windows-11-1536x799.jpg 1536w, https://www.windowslatest.com/wp-content/uploads/2026/01/PCs-not-booting-with-black-screen-on-Windows-11-1068x555.jpg 1068w, https://www.windowslatest.com/wp-content/uploads/2026/01/PCs-not-booting-with-black-screen-on-Windows-11-808x420.jpg 808w, https://www.windowslatest.com/wp-content/uploads/2026/01/PCs-not-booting-with-black-screen-on-Windows-11.jpg 1619w" sizes="(max-width: 696px) 100vw, 696px" alt="PCs not booting with black screen on Windows 11" title="PCs not booting with black screen on Windows 11"></a></p>
            <p>Microsoft is aware of an alleged issue that leaves PCs unable to boot and requires manual recovery after installing the Windows 11 January 2026 Update.</p>
<p>Microsoft doesn‚Äôt know how the January 2026 Update broke some PCs and is collecting data from consumers and IT admins. For now, you don‚Äôt have to uninstall the update, as the issue is ‚Äúlimited‚Äù to specific PCs. The <a href="https://www.windowslatest.com/2026/01/17/windows-11-kb5074109-issues-black-screen-freezes-outlook-pop-breaks-azure-virtual-desktop-localizedresourcename-desktop-ini/">same update also broke Outlook Classic, customization in File Explorer</a>, <a href="https://www.windowslatest.com/2026/01/21/2026s-first-windows-11-update-is-causing-more-problems-now-as-microsoft-enters-damage-control-mode/">sleep mode (S3), Citrix Director</a>, and Remote Desktop.</p>
<p>‚ÄúMicrosoft has received a limited number of reports of an issue in which devices are failing to boot,‚Äù Microsoft noted in a <a href="https://admin.cloud.microsoft/Adminportal/Home?source=applauncher#/windowsreleasehealth/:/issue/WI1221934" target="_blank" rel="noopener">support document</a> first spotted by Windows Latest.</p>
<p>It‚Äôs unclear how many users are affected, but Microsoft has listed two affected platforms on the Windows release dashboard:</p>
<ul>
<li>KB5074109 ‚Äì Windows 11 25H2</li>
<li>KB5074109 ‚Äì Windows 11 24H2</li>
</ul>
<p>KB5074109 is a mandatory security update for Windows 11 25H2 / 24H2, and it patches as many as a hundred known security issues, which is why it‚Äôs installed automatically on PCs.</p>
<p><strong>Do I have the January 2026 Update?</strong> Open Settings &gt; System &gt; About &gt; Specification, and look at the build number. If it‚Äôs 26200.7623, you have the January update. KB5074109 was released on January 13, 2026.</p>
<p><img decoding="async" fetchpriority="high" src="https://www.windowslatest.com/wp-content/uploads/2026/01/Windows-11-January-2026-Update-build-number.jpg" alt="Windows 11 January 2026 Update build number" width="1366" height="1140" srcset="https://www.windowslatest.com/wp-content/uploads/2026/01/Windows-11-January-2026-Update-build-number.jpg 1366w, https://www.windowslatest.com/wp-content/uploads/2026/01/Windows-11-January-2026-Update-build-number-300x250.jpg 300w, https://www.windowslatest.com/wp-content/uploads/2026/01/Windows-11-January-2026-Update-build-number-1024x855.jpg 1024w, https://www.windowslatest.com/wp-content/uploads/2026/01/Windows-11-January-2026-Update-build-number-768x641.jpg 768w, https://www.windowslatest.com/wp-content/uploads/2026/01/Windows-11-January-2026-Update-build-number-696x581.jpg 696w, https://www.windowslatest.com/wp-content/uploads/2026/01/Windows-11-January-2026-Update-build-number-1068x891.jpg 1068w, https://www.windowslatest.com/wp-content/uploads/2026/01/Windows-11-January-2026-Update-build-number-503x420.jpg 503w" sizes="(max-width: 1366px) 100vw, 1366px"></p>
<p>According to Microsoft, affected PCs could suddenly stop booting and land on the Black Screen of Death (BSOD) error with stop code UNMOUNTABLE_BOOT_VOLUME.</p>
<p><img decoding="async" src="https://www.windowslatest.com/wp-content/uploads/2026/01/January-2026-Update-issues-dashboard.jpg" alt="January 2026 Update issues dashboard" width="1500" height="1010" srcset="https://www.windowslatest.com/wp-content/uploads/2026/01/January-2026-Update-issues-dashboard.jpg 1500w, https://www.windowslatest.com/wp-content/uploads/2026/01/January-2026-Update-issues-dashboard-300x202.jpg 300w, https://www.windowslatest.com/wp-content/uploads/2026/01/January-2026-Update-issues-dashboard-1024x689.jpg 1024w, https://www.windowslatest.com/wp-content/uploads/2026/01/January-2026-Update-issues-dashboard-768x517.jpg 768w, https://www.windowslatest.com/wp-content/uploads/2026/01/January-2026-Update-issues-dashboard-696x469.jpg 696w, https://www.windowslatest.com/wp-content/uploads/2026/01/January-2026-Update-issues-dashboard-1068x719.jpg 1068w, https://www.windowslatest.com/wp-content/uploads/2026/01/January-2026-Update-issues-dashboard-624x420.jpg 624w" sizes="(max-width: 1500px) 100vw, 1500px"></p>
<p>The UNMOUNTABLE_BOOT_VOLUME error (Stop Code 0xED) is typically related to system corruption, a damaged BIOS/boot, or hardware failure, specifically the drive. In this case, Microsoft suspects the problem is likely due to the Windows 11 January 2026 Update.</p>
<p>‚ÄúMicrosoft has received a limited number of reports of an issue in which devices are failing to boot with stop code ‚ÄòUNMOUNTABLE_BOOT_VOLUME‚Äô after installing the January 2026 Windows security update (the originating KBs listed above), released January 13, 2026, and later updates,‚Äù Microsoft explained.</p>
<p>‚ÄúAffected devices show a black screen with the message ‚ÄòYour device ran into a problem and needs a restart. You can restart.‚Äô At this stage, the device cannot complete startup and requires manual recovery steps.‚Äù</p>
<p>In other words, if you end up hitting the above Black Screen of Death error, it will leave your computer unable to boot, and you‚Äôll need to recover the installation. Windows Recovery is typically successful, but there are situations where WinRE does not work. In that case, you can use an ISO to clean install Windows.</p>
<p>‚ÄúReports received so far indicate that this issue is limited to physical devices only; no customers have reported observing these symptoms on virtual machines,‚Äù Microsoft warned in the support document.</p>
<p>The good news is that only a limited set of devices is affected. The bad news is that Microsoft doesn‚Äôt know how many users are affected or the details of the configuration causing the issue. The worst news is that the <a href="https://www.windowslatest.com/2026/01/17/windows-11-kb5074109-issues-black-screen-freezes-outlook-pop-breaks-azure-virtual-desktop-localizedresourcename-desktop-ini/">Windows 11 January 2026 Update is causing a bunch of other problems</a>, and <a href="https://www.windowslatest.com/2026/01/25/windows-11-kb5078127-emergency-update-25h2-fixes-outlook-pop-pst-crashes-unresponsive-apps-after-january-2026-patch-tuesday/">Microsoft has issued an emergency update (KB5078127)</a>.</p>
<p>If you don‚Äôt have issues, there‚Äôs nothing to worry about. There are billions of PCs running Windows, and most issues are limited to specific hardware. This might be one of those ‚Äúlimited‚Äù issues only, but if your PC is unable to boot or something feels odd over the last ten days, it‚Äôs likely due to Windows 11 KB5074109.</p>
<table>
<caption><strong>List of issues in the January 2026 Windows 11 update</strong><br>
Source: WindowsLatest.com</caption>
<tbody>
<tr>
<th>Issue detail</th>
<th>Current status</th>
<th>Workaround</th>
<th>Affected Windows version</th>
</tr>
<tr>
<td>Outlook Classic freezes.</td>
<td>Acknowledged by Microsoft</td>
<td>Fixed in out-of-band update (<a href="https://www.windowslatest.com/2026/01/25/windows-11-kb5078127-emergency-update-25h2-fixes-outlook-pop-pst-crashes-unresponsive-apps-after-january-2026-patch-tuesday/" target="_blank" rel="nofollow noopener">KB5078127</a>)</td>
<td>All supported versions of Windows</td>
</tr>
<tr>
<td>Third-party apps become unresponsive.</td>
<td>Acknowledged by Microsoft</td>
<td>Fixed in out-of-band update (<a href="https://www.windowslatest.com/2026/01/25/windows-11-kb5078127-emergency-update-25h2-fixes-outlook-pop-pst-crashes-unresponsive-apps-after-january-2026-patch-tuesday/" target="_blank" rel="nofollow noopener">KB5078127</a>)</td>
<td>All supported versions of Windows</td>
</tr>
<tr>
<td>Black screen or wallpaper resets.</td>
<td>Not officially acknowledged</td>
<td>Restart Windows or reapply wallpaper manually</td>
<td>Windows 11 25H2, 24H2</td>
</tr>
<tr>
<td>Sleep mode (S3) stops working on older PCs.</td>
<td>Not officially acknowledged</td>
<td>No reliable workaround available</td>
<td>Windows 11 24H2 / 25H2 on systems using S3 sleep</td>
</tr>
<tr>
<td>Desktop.ini broken in File Explorer.</td>
<td>Not officially acknowledged</td>
<td>No workaround; folders still function but UI is incorrect</td>
<td>Windows 11 25H2, 24H2</td>
</tr>
<tr>
<td>Citrix Remote Desktop not working</td>
<td>Acknowledged by Citrix</td>
<td>Remove the update or switch to alternatives</td>
<td>Windows 11 25H2, 24H2 and Windows Server 2025, 2022</td>
</tr>
<tr>
<td>Remote Desktop stops working.</td>
<td>Acknowledged by Microsoft</td>
<td>Fixed in first out-of-band update (KB5077744)</td>
<td>All supported versions of Windows</td>
</tr>
<tr>
<td>Shutdown feature does not work properly.</td>
<td>Acknowledged by Microsoft</td>
<td>Fixed with first out-of-band update (KB5077797)</td>
<td>23H2</td>
</tr>
<tr>
<td>System fails to boot</td>
<td>Acknowledged by Microsoft</td>
<td>Use Windows Recovery or reinstall Windows using an ISO</td>
<td>25H2 and 24H2</td>
</tr>
</tbody>
</table>
<p>What about you? Have you encountered critical issues after recent Windows Updates?</p>

        

        
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Over 36,500 killed in Iran's deadliest massacre, documents reveal (216 pts)]]></title>
            <link>https://www.iranintl.com/en/202601255198</link>
            <guid>46760329</guid>
            <pubDate>Mon, 26 Jan 2026 00:40:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iranintl.com/en/202601255198">https://www.iranintl.com/en/202601255198</a>, See on <a href="https://news.ycombinator.com/item?id=46760329">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-article-id="ecb09193-d5e6-4189-862b-be6e1a000019" data-article-type="article" data-language="english" data-route="slug"><article data-has-sidebar="true" itemscope="" itemtype="https://schema.org/NewsArticle"><header><p><strong>EXCLUSIVE</strong></p><div><figure><picture><source media="(max-width:576px)" srcset="https://i.iranintl.com/images/rdk9umy0/production/7fe9c0347d121fb5b5779d874e9396c83820798a-992x661.jpg?w=576&amp;h=384&amp;q=80&amp;fit=max&amp;auto=format"><source media="(min-width:577px)" srcset="https://i.iranintl.com/images/rdk9umy0/production/7fe9c0347d121fb5b5779d874e9396c83820798a-992x661.jpg?w=992&amp;h=661&amp;q=80&amp;fit=max&amp;auto=format"><img alt="Men stand amid rows of corpses in a morgue in Tehran following mass killings of protestors by security forces in this undated image obtained by Iran International" width="576" height="384" decoding="async" src="https://i.iranintl.com/images/rdk9umy0/production/7fe9c0347d121fb5b5779d874e9396c83820798a-992x661.jpg?w=576&amp;h=384&amp;q=80&amp;fit=max&amp;auto=format"></picture><figcaption>Men stand amid rows of corpses in a morgue in Tehran following mass killings of protestors by security forces in this undated image obtained by Iran International</figcaption></figure></div></header><main><div data-article-id="202601255198"><p>More than 36,500 Iranians were killed by security forces during the January 8-9 crackdown on nationwide protests, making it the deadliest two-day protest massacre in history, according to documents reviewed by Iran International's Editorial Board.</p><!--$?--><template id="B:0"></template><!--/$--></div></main></article><section data-has-sidebar="true"><!--$?--><template id="B:2"></template><!--/$--></section></div></div>]]></description>
        </item>
    </channel>
</rss>