<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 12 Dec 2024 00:30:12 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[QEMU with VirtIO GPU Vulkan Support (104 pts)]]></title>
            <link>https://gist.github.com/peppergrayxyz/fdc9042760273d137dddd3e97034385f</link>
            <guid>42392802</guid>
            <pubDate>Wed, 11 Dec 2024 20:48:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/peppergrayxyz/fdc9042760273d137dddd3e97034385f">https://gist.github.com/peppergrayxyz/fdc9042760273d137dddd3e97034385f</a>, See on <a href="https://news.ycombinator.com/item?id=42392802">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="file-qemu-vulkan-virtio-md">
    <article itemprop="text">
<p dir="auto">With its latest reales qemu added the Venus patches so that virtio-gpu now support venus encapsulation for vulkan. This is one more piece to the puzzle towards full Vulkan support.</p>
<p dir="auto">An outdated blog post on <a href="https://www.collabora.com/news-and-blog/blog/2021/11/26/venus-on-qemu-enabling-new-virtual-vulkan-driver/" rel="nofollow">clollabora</a> described in 2021 how to enable 3D acceleration of Vulkan applications in QEMU through the Venus experimental Vulkan driver for VirtIO-GPU with a local development environment. Following up on the outdated write up, this is how its done today.</p>
<p dir="auto"><h2 dir="auto">Definitions</h2><a id="user-content-definitions" aria-label="Permalink: Definitions" href="#definitions"></a></p>
<p dir="auto">Let's start with the brief description of the projects mentioned in the post &amp; extend them:</p>
<ul dir="auto">
<li>QEMU is a machine emulator</li>
<li>VirGL is an OpenGL driver for VirtIO-GPU, available in Mesa.</li>
<li>Venus is an experimental Vulkan driver for VirtIO-GPU, also available in Mesa.</li>
<li>Virglrenderer is a library that enables hardware acceleration to VM guests, effectively translating commands from the two drivers just mentioned to either OpenGL or Vulkan.</li>
<li>libvirt is an API for managing platform virtualization</li>
<li>virt-manager is a desktop user interface for managing virtual machines through libvirt</li>
</ul>
<p dir="auto">Merged Patches:</p>
<ul dir="auto">
<li>2024-08-14 <a href="https://gitlab.freedesktop.org/mesa/mesa/-/commit/087e9a96d13155e26987befae78b6ccbb7ae242b" rel="nofollow">venus: make cross-device optional</a> merged in <a href="https://www.phoronix.com/news/Mesa-24.2-Released" rel="nofollow">mesa 24.2</a></li>
<li>2024-11-25 <a href="https://lore.kernel.org/all/20240726235234.228822-1-seanjc@google.com/" rel="nofollow">KVM: Stop grabbing references to PFNMAP'd pages</a> merged in <a href="https://www.phoronix.com/news/Linux-6.13-KVM" rel="nofollow">linux 6.13</a></li>
<li>2024-11-12 <a href="https://lists.gnu.org/archive/html/qemu-devel/2024-08/msg03288.html" rel="nofollow">Support blob memory and venus on qemu</a> merged in <a href="https://www.phoronix.com/news/QEMU-9.2-Released" rel="nofollow">qemu 9.2.0</a></li>
</ul>
<p dir="auto">Work in progress:</p>
<ul dir="auto">
<li>libvirt <a href="https://gitlab.com/libvirt/libvirt/-/issues/638" rel="nofollow">Add support for more virtio-vga-gl arguments #638</a></li>
<li>virt-manager <a href="https://github.com/virt-manager/virt-manager/issues/362" data-hovercard-type="issue" data-hovercard-url="/virt-manager/virt-manager/issues/362/hovercard">Add support for Venus / Vulkan VirtIO-GPU driver #362</a></li>
</ul>
<p dir="auto"><h2 dir="auto">Prerequisites</h2><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<p dir="auto">Make sure you have the proper version installed on the host:</p>
<ul dir="auto">
<li>linux kernel &gt;= 6.13 built with CONFIG_UDMABUF</li>
<li>working Vulkan and kvm setup</li>
<li>qemu &gt;= 9.2.0</li>
</ul>
<p dir="auto">You can verify this like so:</p>
<pre><code>$ uname -r
6.13.0
$ ls /dev/udmabuf
/dev/udmabuf
$ ls /dev/kvm
/dev/kvm
$ qemu-system-x86_64 --version
QEMU emulator version 9.2.0
Copyright (c) 2003-2024 Fabrice Bellard and the QEMU Project developers
</code></pre>
<p dir="auto">For Vulkan to work you need the proper drivers to be installed for your graphics card. To verfiy your setup, install <code>vulkan-tools</code>:</p>
<pre><code>$ vulkaninfo --summary
==========
VULKANINFO
==========

Vulkan Instance Version: ...
...
$ vkcube
Selected GPU x: ..., type: ...
</code></pre>
<p dir="auto"><h4 dir="auto">Building qemu</h4><a id="user-content-building-qemu" aria-label="Permalink: Building qemu" href="#building-qemu"></a></p>
<p dir="auto">If your distro doesn't (yet) ship and updated version of qemu, you can build it yourself from source:</p>
<pre><code>wget https://download.qemu.org/qemu-9.2.0.tar.xz
tar xvJf qemu-9.2.0.tar.xz
cd qemu-9.2.0
mkdir build &amp;&amp; cd build
../configure --target-list=x86_64-softmmu  \
  --enable-kvm                 \
  --enable-opengl              \
  --enable-virglrenderer       \
  --enable-gtk                 \
  --enable-sdl
make -j4
</code></pre>
<p dir="auto">The configuration step will throgh errors if packages are missing. Check the qemu wiki for further info what to install: <a href="https://wiki.qemu.org/Hosts/Linux" rel="nofollow">https://wiki.qemu.org/Hosts/Linux</a></p>
<p dir="auto"><h2 dir="auto">Create and run an image for QEMU</h2><a id="user-content-create-and-run-an-image-for-qemu" aria-label="Permalink: Create and run an image for QEMU" href="#create-and-run-an-image-for-qemu"></a></p>
<p dir="auto">Create an image &amp; fetch the distro of your choice:</p>
<p dir="auto"><h3 dir="auto">Host</h3><a id="user-content-host" aria-label="Permalink: Host" href="#host"></a></p>
<div dir="auto"><pre>ISO=ubuntu-24.10-desktop-amd64.iso  
wget https://releases.ubuntu.com/oracular/ubuntu-24.10-desktop-amd64.iso  

IMG=ubuntu-24-10.qcow2
qemu-img create -f qcow2 <span>$IMG</span> 16G</pre></div>
<p dir="auto">Run a live version or install the distro</p>
<pre><code>qemu-system-x86_64                                               \
    -enable-kvm                                                  \
    -M q35                                                       \
    -smp 4                                                       \
    -m 4G                                                        \
    -cpu host                                                    \
    -net nic,model=virtio                                        \
    -net user,hostfwd=tcp::2222-:22                              \
    -device virtio-vga-gl,hostmem=4G,blob=true,venus=true        \
    -vga none                                                    \
    -display gtk,gl=on,show-cursor=on                            \
    -usb -device usb-tablet                                      \
    -object memory-backend-memfd,id=mem1,size=4G                 \
    -machine memory-backend=mem1                                 \
    -hda $IMG                                                    \
    -cdrom $ISO                                                  
</code></pre>
<p dir="auto">Adjust the parameters accordingly:</p>
<ul dir="auto">
<li>smp: number of cpu cores</li>
<li>m: RAM</li>
<li>hostmem,size: VRAM</li>
</ul>
<p dir="auto"><h3 dir="auto">Guest</h3><a id="user-content-guest" aria-label="Permalink: Guest" href="#guest"></a></p>
<p dir="auto">Install <code>mesa-utilites</code> and <code>vulkan-tools</code> to test the setup:</p>
<pre><code>$ glxinfo -B
</code></pre>
<pre><code>$ vkcube
Selected GPU x: ..., type: ...
</code></pre>
<p dir="auto">If the deive is <code>llvmpipe</code> somehting is wrong. The device should be <code>virgl (...)</code>.</p>
<p dir="auto"><h4 dir="auto">Troubleshooting</h4><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<ul dir="auto">
<li>(host) add <code>-d guest_errors</code> to show error messages from the guest</li>
<li>(guest) try installing vulkan virtio drivers and mesa</li>
<li>check the original <a href="https://www.collabora.com/news-and-blog/blog/2021/11/26/venus-on-qemu-enabling-new-virtual-vulkan-driver/" rel="nofollow">blog post</a></li>
</ul>
<p dir="auto"><h2 dir="auto">virt-manager</h2><a id="user-content-virt-manager" aria-label="Permalink: virt-manager" href="#virt-manager"></a></p>
<p dir="auto">-- work in progress --</p>
<p dir="auto">Currently this is work in progress, so there is no option to add vulkan support in virt-manager. There are no fields to configure this. Also xml doesnt work, because libvirt doesn't know about these options either, so xml validation fails. There is however an option for <a href="https://libvirt.org/kbase/qemu-passthrough-security.html" rel="nofollow">QEMU command-line passthrough</a> which bypasses the validation.</p>
<p dir="auto">If you setup a default machine with 4G of memory, you can do this:</p>
<div dir="auto"><pre>  &lt;<span>qemu</span><span>:</span><span>commandline</span>&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>-device<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>virtio-vga-gl,hostmem=4G,blob=true,venus=true<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>-object<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>memory-backend-memfd,id=mem1,size=4G<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>-machine<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>memory-backend=mem1<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>-vga<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>none<span>"</span></span>/&gt;
  &lt;/<span>qemu</span><span>:</span><span>commandline</span>&gt;</pre></div>
<p dir="auto">Which gives this error:</p>
<pre><code>qemu-system-x86_64: virgl could not be initialized: -1
</code></pre>
<p dir="auto">Changing the number from 4G to 4194304k (same as memory) leds to this error:</p>
<pre><code>qemu-system-x86_64: Spice: ../spice-0.15.2/server/red-qxl.cpp:435:spice_qxl_gl_scanout: condition `qxl_state-&gt;gl_draw_cookie == GL_DRAW_COOKIE_INVALID' failed
</code></pre>
<p dir="auto">to be further investigated.</p>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI Guesses Your Accent (124 pts)]]></title>
            <link>https://start.boldvoice.com/accent-guesser</link>
            <guid>42392088</guid>
            <pubDate>Wed, 11 Dec 2024 19:54:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://start.boldvoice.com/accent-guesser">https://start.boldvoice.com/accent-guesser</a>, See on <a href="https://news.ycombinator.com/item?id=42392088">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div><div><p><img src="https://start.boldvoice.com/build/_assets/OracleWaveAudioFuzzy-OOCGQGMB.png" alt="OracleWaveAudioFuzzy" width="100%" height="100%"></p></div><div><p>The <span>Accent</span> Oracle</p><p>Do you have an accent when speaking English? I bet I can guess your native language in less than 30 seconds.</p></div></div><div><p><img src="https://start.boldvoice.com/build/_assets/GlobeLanguages-6JQ4ADYP.svg" alt="Globe Languages"></p></div></div><div><p>© 2024 BoldVoice. All rights reserved.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mysterious New Jersey drone sightings prompt call for 'state of emergency' (119 pts)]]></title>
            <link>https://www.theguardian.com/us-news/2024/dec/11/new-jersey-drone-sightings-state-of-emergency</link>
            <guid>42391443</guid>
            <pubDate>Wed, 11 Dec 2024 19:02:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/2024/dec/11/new-jersey-drone-sightings-state-of-emergency">https://www.theguardian.com/us-news/2024/dec/11/new-jersey-drone-sightings-state-of-emergency</a>, See on <a href="https://news.ycombinator.com/item?id=42391443">Hacker News</a></p>
Couldn't get https://www.theguardian.com/us-news/2024/dec/11/new-jersey-drone-sightings-state-of-emergency: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[2400 phone providers may be shut down by the FCC for failing to stop robocalls (190 pts)]]></title>
            <link>https://docs.fcc.gov/public/attachments/DOC-408083A1.txt</link>
            <guid>42391203</guid>
            <pubDate>Wed, 11 Dec 2024 18:41:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.fcc.gov/public/attachments/DOC-408083A1.txt">https://docs.fcc.gov/public/attachments/DOC-408083A1.txt</a>, See on <a href="https://news.ycombinator.com/item?id=42391203">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[X41 Reviewed Mullvad VPN (204 pts)]]></title>
            <link>https://x41-dsec.de/news/2024/12/11/mullvad/</link>
            <guid>42390768</guid>
            <pubDate>Wed, 11 Dec 2024 18:08:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://x41-dsec.de/news/2024/12/11/mullvad/">https://x41-dsec.de/news/2024/12/11/mullvad/</a>, See on <a href="https://news.ycombinator.com/item?id=42390768">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <h2 id="review-of-mullvad-vpn">Review of Mullvad VPN</h2>

<p>X41 performed a white box penetration test with source code access against the <a href="https://mullvad.net/">Mullvad</a> VPN Application. The efforts included formulating a light threat model.</p>

<p>The targets of this test were challenging for the team because of its size, the fact that they run on five different platforms (Linux, Windows, macOS, Android, and iOS), and the regular audits performed by Mullvad VPN. The fact that new vulnerabilities were found in existing code shows that the efforts taken regularly by Mullvad are justified and appropriate for product of such complexity.</p>

<p>It also shows that in mature targets the findings tend to move into domains not under direct control or in direct focus of the application development as can be seen in the findings rooting from specifics of the operating system’s behavior or the interplay of different network layers and protocols.</p>

<p>This is what keeps security audits and tests of mature and hard targets interesting for the team at X41 as well.</p>

<h2 id="results">Results</h2>

<p>A total of six vulnerabilities were discovered during the test by X41.</p>

<p>Overall, the Mullvad VPN Applications appear to have a high security level and are well positioned to protect from the threat model proposed in our report. The use of safe coding and design patterns in combination with regular audits and penetration tests led to a very hardened environment.</p>

<p>The most serious vulnerabilities are considered to be race conditions and temporal safety violations leading to memory corruption issues in the signal handler code. While exploitation of the signal handler code once triggered seems not unlikely, the fact that an attacker first needs to trigger a signal via another fault reduces the severity of the issues. Other vulnerabilities allow leaking information about the identity of a user by network adjacent attackers and to perform side channel attacks that could in specific circumstances reveal which site a client is currently accessing.</p>

<p>The aspect of side channel attacks is mitigated in most parts, except for protocol level attacks that are not within the control of Mullvad VPN AB because they root from a combination of different technologies such as NAT and modern variants of the HTTP protocol. The introduction of obfuscation technologies and proxy  services within the protected VPN is an option for users with higher security and privacy demands.</p>

<p>In conclusion, the client applications exposed a limited number of relevant vulnerabilities. Mullvad VPN AB addressed them swiftly and the fixes were audited to be working properly.</p>

<p>X41 would like to thank Mullvad VPN AB for the nice collaboration and smooth communication throughout the audit!</p>

<h3 id="findings">Findings</h3>

<p>Mullvad’s <a href="https://mullvad.net/en/blog/the-report-for-the-2024-security-audit-of-the-app-is-now-available">announcement</a> about the audit covers each of the findings and their mitigations. The technical details can be found in our <a href="https://x41-dsec.de/static/reports/X41-Mullvad-Audit-Public-Report-2024-12-10.pdf">report</a>, which we are releasing today.</p>

<h3 id="links">Links</h3>

<p>Full report:<br>
<a href="https://x41-dsec.de/static/reports/X41-Mullvad-Audit-Public-Report-2024-12-10.pdf">https://x41-dsec.de/static/reports/X41-Mullvad-Audit-Public-Report-2024-12-10.pdf</a></p>

<p>Mullvad announcement:<br>
<a href="https://mullvad.net/en/blog/the-report-for-the-2024-security-audit-of-the-app-is-now-available">https://mullvad.net/en/blog/the-report-for-the-2024-security-audit-of-the-app-is-now-available</a></p>

<p>Mullvad’s previous audits:<br>
<a href="https://github.com/mullvad/mullvadvpn-app/tree/main/audits">https://github.com/mullvad/mullvadvpn-app/tree/main/audits</a></p>

<hr>

<p>If you are interested in working with us on such projects in the future, remote or in-office, have a look at our <a href="https://x41-dsec.de/jobs/">jobs</a> page!</p>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WordPress CEO quits community Slack after court injunction (112 pts)]]></title>
            <link>https://www.404media.co/wordpress-wp-engine-preliminary-injunction/</link>
            <guid>42390709</guid>
            <pubDate>Wed, 11 Dec 2024 18:02:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/wordpress-wp-engine-preliminary-injunction/">https://www.404media.co/wordpress-wp-engine-preliminary-injunction/</a>, See on <a href="https://news.ycombinator.com/item?id=42390709">Hacker News</a></p>
Couldn't get https://www.404media.co/wordpress-wp-engine-preliminary-injunction/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[FCC Opens Entire 6 GHz Band to Low Power Device Operations (390 pts)]]></title>
            <link>https://www.fcc.gov/document/fcc-opens-entire-6-ghz-band-very-low-power-device-operations</link>
            <guid>42390344</guid>
            <pubDate>Wed, 11 Dec 2024 17:35:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fcc.gov/document/fcc-opens-entire-6-ghz-band-very-low-power-device-operations">https://www.fcc.gov/document/fcc-opens-entire-6-ghz-band-very-low-power-device-operations</a>, See on <a href="https://news.ycombinator.com/item?id=42390344">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-off-canvas-main-canvas="">
        
				
	
        <header>
  


<nav aria-labelledby="main-navigation">
  <div id="main-navigation">

          <div>
          
  <p><a href="https://www.fcc.gov/" aria-label="" id="logo">
            <img src="https://www.fcc.gov/themes/custom/fcc/logo.svg" width="175" height="auto" alt="Federal Communications Commission logo">

        
  </a>

  
  </p>


      </div>
    
          <div aria-label="Main Navigation">
        <nav aria-labelledby="browse-by">
          <div role="tablist" id="mainNavbar">
              <ul id="browse-by" role="tablist">
                <li>
                  <a id="nav-category-tab" data-bs-toggle="tab" href="#nav-category" role="tab" aria-controls="nav-category" aria-selected="true" aria-haspopup="true" aria-expanded="true">
                    <p>Browse by</p>
                    <p>category</p>
                    
                  </a>
                </li>
                <li>
                  <a id="nav-bureaus-and-offices-tab" data-bs-toggle="tab" href="#nav-bureaus-and-offices" role="tab" aria-controls="nav-bureaus-and-offices" aria-selected="false" aria-haspopup="true" aria-expanded="false">
                    <p>Browse by</p>
                    <p>bureaus &amp; offices</p>
                    
                  </a>
                </li>
              </ul>
              </div>
        </nav>
      </div>
    
                  
          
  </div>
</nav>
</header>
  
        
  

  <main role="main">
    <div>
          <article>
  
  

    <div>
      <div>
        <ul>
                      <li>
  <div>
    <p>
      Full Title<span>:</span>    </p>
                  <p>FCC Opens Entire 6 GHz Band To Very Low Power Device Operations</p>
              </div>
</li>
                                <li>
        
      
  </li>
                                <li>
  
</li>
                                <li>
  <div>
    <p>
      Description    </p>
                  <p>FCC Opens Entire 6 GHz Band To Very Low Power Device Operations</p>
              </div>
</li>
                                                                                                    </ul>

        
                  <div>
  <h3>Files</h3>
    
  

  </div>
              </div>
      <div>
            <h4>Document Dates</h4>
            <ul>
                              <li>
  <div>
    <p>
      Released On<span>:</span>    </p>
                  <p><time datetime="2024-12-11T12:00:00Z">Dec 11, 2024</time>
</p>
              </div>
</li>
                                                          <li>
  <div>
    <p>
      Adopted Date<span>:</span>    </p>
                  <p><time datetime="2024-12-11T12:00:00Z">Dec 11, 2024</time>
</p>
              </div>
</li>
                                            <li>
  
</li>
                                                                                                </ul>
          </div>
    </div>
</article>


      </div>
  </main>

        
  
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Estimated concentrations of atrazine in agricultural groundwater (138 pts)]]></title>
            <link>https://water.usgs.gov/nawqa/pnsp/features/feature.php</link>
            <guid>42390236</guid>
            <pubDate>Wed, 11 Dec 2024 17:25:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://water.usgs.gov/nawqa/pnsp/features/feature.php">https://water.usgs.gov/nawqa/pnsp/features/feature.php</a>, See on <a href="https://news.ycombinator.com/item?id=42390236">Hacker News</a></p>
Couldn't get https://water.usgs.gov/nawqa/pnsp/features/feature.php: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[RISC-V HiFive Premier P550 Development Boards with Ubuntu Now Available (116 pts)]]></title>
            <link>https://www.sifive.com/blog/hifive-premier-p550-development-boards-with-ubuntu</link>
            <guid>42389532</guid>
            <pubDate>Wed, 11 Dec 2024 16:33:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sifive.com/blog/hifive-premier-p550-development-boards-with-ubuntu">https://www.sifive.com/blog/hifive-premier-p550-development-boards-with-ubuntu</a>, See on <a href="https://news.ycombinator.com/item?id=42389532">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>With strong initial reviews, the  <a href="https://www.sifive.com/boards/hifive-premier-p550">HiFive Premier P550 Development Boards</a> with pre-installed Ubuntu are in stock and available for purchase at <a href="https://www.arrow.com/en/products/hf106-001/sifive-inc" target="_blank" rel="noopener">Arrow.com</a>. First launched in October, the Yocto-based Early Access edition of the HiFive Premier P550 sold out rapidly as developers eagerly embraced it to test their RISC-V designs on real silicon. Today, we’re excited to announce the general availability of the Premier P550 in both <a href="https://www.arrow.com/en/products/hf106/sifive-inc" target="_blank" rel="noopener">16GB</a> and <a href="https://www.arrow.com/en/products/hf106-001/sifive-inc" target="_blank" rel="noopener">32GB</a> configurations. This release offers even more flexibility and power, featuring pre-installed Ubuntu 24.04 to help developers make the most of their RISC-V projects.</p>
<p><strong>Lower Price, Greater Accessibility</strong>
Due to the boards' initial popularity, we worked closely with our manufacturing partner, ESWIN to ramp up production. As a result of increased production and economies of scale, we’re excited to announce we are able to lower the price to just $399 for the 16GB version and $499 for the 32GB version, making this powerful development board more accessible to a wider range of developers, enthusiasts, and designers.</p>
<p>For those who acted quickly and purchased a board before the price drop, we want to thank you for your support. We’ll be issuing a refund for the price difference to show our appreciation. You will receive an email from Arrow explaining next steps.</p>
<p><strong>Rave Reviews and Feedback</strong>
Early feedback from users has been overwhelmingly positive. It’s been exciting to see how developers and build farms are leveraging the board’s capabilities—from testing software to running video games—pushing the board to its limits. We’d love to hear your thoughts as well. Feel free to send your feedback to HighFiveboards@sifive.com, or share your project with us on social media. Tag us or send in a video to showcase how you're using the HiFive Premier P550.</p>
<p>As we mentioned during the product launch, our goal is to get these boards into the hands of as many developers as possible to help accelerate the growth of the RISC-V ecosystem. Early users have praised the board’s smooth out-of-the-box experience, thorough testing and certification, and premium features. The boards are built with high quality components including powerful Samsung and Micron LPDDR memory, System on Module (SOM) for modularity and upgradeability, and an onboard Baseboard Management Controller ( BMC) offering remote management and control without having physical access to the board, and much more.</p>
<p><strong>Built on Strong Engineering Collaboration</strong>
The HiFive Premier P550 Development Board’s uniqueness stems from the close collaboration between SiFive, ESWIN, and Canonical to ensure the board performs to specification. This strong partnership is helping drive the RISC-V ecosystem forward, and we’re thrilled to see these boards gaining traction.</p>
<p>“We’re excited to see these boards rapidly proliferate into the market,” said Bo Wang, Vice Chairman of <a href="https://www.eswincomputing.com/en/" target="_blank" rel="noopener">ESWIN Computing</a>. “We are pleased to be able to collaborate with SiFive, the industry leader in RISC-V, and we look forward to future products.”</p>
<p>“<a href="https://canonical.com/" target="_blank" rel="noopener">Canonical</a> is deeply committed to RISC-V and creating the best software environment possible for developers coming into this ecosystem. The HiFive Premier P550 is set to be the de facto development platform, and with Ubuntu coming pre-installed on the board we are excited to see the platform used by innovators and developers,” said Gordan Markus, Silicon Alliances Director, Canonical.</p>
<p><strong>A Strategic Investment in RISC-V’s Future</strong>
These boards represent a long-term investment in accelerating the global adoption of RISC-V  and are not intended as a revenue driver.  As the RISC-V ecosystem grows at an exponential pace, having access to actual silicon for hands-on development is essential. We’re committed to supporting this growth, and you can expect many more boards to follow in the months ahead.</p>
<p>We believe this investment benefits you too. With the new, lower price, Ubuntu support, and availability through Arrow.com, there’s never been a better time to dive into RISC-V development. Whether you're building a prototype, exploring new software, or experimenting with hardware, the HiFive Premier P550 offers an unparalleled platform. And with the confidence that it’s built on SiFive’s proven IP, you can be sure you’re working with the best.</p>
<p>We look forward to hearing how you’re using these boards and can’t wait to share more exciting stories from the growing RISC-V community.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pgroll – Zero-downtime, reversible, schema changes for PostgreSQL (new website) (142 pts)]]></title>
            <link>https://pgroll.com/</link>
            <guid>42388973</guid>
            <pubDate>Wed, 11 Dec 2024 15:51:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pgroll.com/">https://pgroll.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42388973">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Trillium TPU Is GA (109 pts)]]></title>
            <link>https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga</link>
            <guid>42388901</guid>
            <pubDate>Wed, 11 Dec 2024 15:44:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga">https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga</a>, See on <a href="https://news.ycombinator.com/item?id=42388901">Hacker News</a></p>
Couldn't get https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Dear OAuth Providers (128 pts)]]></title>
            <link>https://pilcrowonpaper.com/blog/dear-oauth-providers/</link>
            <guid>42388870</guid>
            <pubDate>Wed, 11 Dec 2024 15:42:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pilcrowonpaper.com/blog/dear-oauth-providers/">https://pilcrowonpaper.com/blog/dear-oauth-providers/</a>, See on <a href="https://news.ycombinator.com/item?id=42388870">Hacker News</a></p>
Couldn't get https://pilcrowonpaper.com/blog/dear-oauth-providers/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini 2.0: our new AI model for the agentic era (589 pts)]]></title>
            <link>https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/</link>
            <guid>42388783</guid>
            <pubDate>Wed, 11 Dec 2024 15:33:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/">https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/</a>, See on <a href="https://news.ycombinator.com/item?id=42388783">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
  }">
  
  <div>
          
            <p>Dec 11, 2024</p>
          
          
            <p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
  
  <div data-summary-id="ai_summary_2" data-component="uni-ai-generated-summary" data-analytics-module="{
    &quot;event&quot;: &quot;module_impression&quot;,
    &quot;module_name&quot;: &quot;ai_summary&quot;,
    &quot;section_header&quot;: &quot;CTA&quot;
  }">
          <h2>Bullet points</h2>
          <ul>
<li>Google DeepMind introduces Gemini 2.0, a new AI model designed for the "agentic era."</li>
<li>Gemini 2.0 is more capable than previous versions, with native image and audio output and tool use.</li>
<li>Gemini 2.0 Flash is available to developers and trusted testers, with wider availability planned for early next year.</li>
<li>Google is exploring agentic experiences with Gemini 2.0, including Project Astra, Project Mariner, and Jules.</li>
<li>Google is committed to building AI responsibly, with safety and security as key priorities.</li>
</ul>
          
          <p><small>
            Summaries were generated by Google AI. Generative AI is experimental.
          </small>
        </p></div>
</div>

    

    
      







<div>
    <figure>
      <div>
  <p><img srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_gemini_keyword_header.width-600.format-webp.webp 600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_gemini_keyword_header.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_gemini_keyword_header.width-1600.format-webp.webp 1600w" sizes="(max-width: 599px) 100vw, (max-width: 1023px) 600px, 1024px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_gemini_keyword_header.width-1200.format-webp.webp" fetchpriority="high" alt="Text &quot;Gemini 2.0&quot; in front of a futuristic blue and black abstract background">
  </p>
</div>

      
    </figure>
  </div>


    

    
    <div>
        
          
            <div data-component="uni-article-jumplinks" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Jumplinks&quot;,
    &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
  }">
  <nav aria-label="Article Jumplinks">
    <p><span>In this story</span>
    </p>
    
    
    <div>
      <ul id="article-jumplinks__list">
        
        <li>
          <a aria-label="link to A message from our CEO" href="#ceo-message" id="ceo-message-anchor">A message from our CEO</a>
        </li>
        
        <li>
          <a aria-label="link to Introducing Gemini 2.0" href="#gemini-2-0" id="gemini-2-0-anchor">Introducing Gemini 2.0</a>
        </li>
        
        <li>
          <a aria-label="link to Gemini 2.0 Flash" href="#gemini-2-0-flash" id="gemini-2-0-flash-anchor">Gemini 2.0 Flash</a>
        </li>
        
        <li>
          <a aria-label="link to Project Astra" href="#project-astra" id="project-astra-anchor">Project Astra</a>
        </li>
        
        <li>
          <a aria-label="link to Project Mariner" href="#project-mariner" id="project-mariner-anchor">Project Mariner</a>
        </li>
        
        <li>
          <a aria-label="link to Agents for developers" href="#agents-for-developers" id="agents-for-developers-anchor">Agents for developers</a>
        </li>
        
        <li>
          <a aria-label="link to Agents in games" href="#ai-game-agents" id="ai-game-agents-anchor">Agents in games</a>
        </li>
        
        <li>
          <a aria-label="link to Building responsibly" href="#building-responsibly" id="building-responsibly-anchor">Building responsibly</a>
        </li>
        
      </ul>
    </div>
    
  </nav>
</div>
          
          
          <div data-reading-time="true" data-component="uni-article-body">

            
              


<google-read-aloud-player data-analytics-module="{
        &quot;event&quot;: &quot;module_impression&quot;,
        &quot;module_name&quot;: &quot;ai_audio&quot;,
        &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
    }" data-date-modified="2024-12-11T15:30:20.983616+00:00" data-progress-bar-style="half-wave" data-api-key="AIzaSyBLT6VkYe-x7sWLZI2Ep26-fNkBKgND-Ac" data-article-style="style9" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-layout-style="style1" data-highlight-mode="word-over-paragraph" data-highlight-text-color="#000000" data-highlight-word-background="#8AB4F8" data-highlight-paragraph-background="#D2E3FC" data-background="linear-gradient(180deg, #F1F3F4 0%, #F8F9FA 100%)" data-foreground-color="#202124" data-font="600 16px Google Sans, sans-serif" data-box-shadow="0px 1px 3px 1px rgba(60, 64, 67, 0.15)">
</google-read-aloud-player>




            

            
            
<!--article text-->

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><p data-block-key="99isd"><b>A note from Google and Alphabet CEO Sundar Pichai:</b></p><p data-block-key="efphd">Information is at the core of human progress. It’s why we’ve focused for more than 26 years on our mission to organize the world’s information and make it accessible and useful. And it’s why we continue to push the frontiers of AI to organize that information across every input and make it accessible via any output, so that it can be truly useful for you.</p><p data-block-key="4oebp">That was our vision when <a href="https://blog.google/technology/ai/google-gemini-ai/">we introduced Gemini 1.0 last December</a>. The first model built to be natively multimodal, Gemini 1.0 and 1.5 drove big advances with multimodality and long context to understand information across text, video, images, audio and code, and process a lot more of it.</p><p data-block-key="2huik">Now millions of developers are building with Gemini. And it’s helping us reimagine all of our products — including all 7 of them with 2 billion users — and to create new ones. <a href="https://notebooklm.google/">NotebookLM</a> is a great example of what multimodality and long context can enable for people, and why it’s loved by so many.</p><p data-block-key="60rf2">Over the last year, we have been investing in developing more agentic models, meaning they can understand more about the world around you, think multiple steps ahead, and take action on your behalf, with your supervision.</p><p data-block-key="ejiii">Today we’re excited to launch our next era of models built for this new agentic era: introducing Gemini 2.0, our most capable model yet. With new advances in multimodality — like native image and audio output — and native tool use, it will enable us to build new AI agents that bring us closer to our vision of a universal assistant.</p><p data-block-key="bh7ok">We’re getting 2.0 into the hands of developers and trusted testers today. And we’re working quickly to get it into our products, leading with Gemini and Search. Starting today our Gemini 2.0 Flash experimental model will be available to all Gemini users. We're also launching a new feature called <a href="https://blog.google/products/gemini/google-gemini-deep-research/">Deep Research</a>, which uses advanced reasoning and long context capabilities to act as a research assistant, exploring complex topics and compiling reports on your behalf. It's available in Gemini Advanced today.</p><p data-block-key="eqmfh">No product has been transformed more by AI than Search. Our AI Overviews now reach 1 billion people, enabling them to ask entirely new types of questions — quickly becoming one of our most popular Search features ever. As a next step, we’re bringing the advanced reasoning capabilities of Gemini 2.0 to AI Overviews to tackle more complex topics and multi-step questions, including advanced math equations, multimodal queries and coding. We started limited testing this week and will be rolling it out more broadly early next year. And we’ll continue to bring AI Overviews to more countries and languages over the next year.</p><p data-block-key="aaa8b">2.0’s advances are underpinned by decade-long investments in our differentiated full-stack approach to AI innovation. It’s built on custom hardware like Trillium, our sixth-generation TPUs. TPUs powered 100% of Gemini 2.0 training and inference, and today Trillium is <a href="https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga">generally available</a> to customers so they can build with it too.</p><div data-block-key="7gbvh"><p>If Gemini 1.0 was about organizing and understanding information, Gemini 2.0 is about making it much more useful. I can’t wait to see what this next era brings.</p><p>-Sundar</p></div><hr></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="99isd">Introducing Gemini 2.0: our new AI model for the agentic era</h2><p data-block-key="7o7kh"><i>By Demis Hassabis, CEO of Google DeepMind and Koray Kavukcuoglu, CTO of Google DeepMind on behalf of the Gemini team</i></p><p data-block-key="ecj6n">Over the past year, we have continued to make incredible progress in artificial intelligence. Today, we are releasing the first model in the Gemini 2.0 family of models: an experimental version of Gemini 2.0 Flash. It’s our workhorse model with low latency and enhanced performance at the cutting edge of our technology, at scale.</p><p data-block-key="6o7oe">We are also sharing the frontiers of our agentic research by showcasing prototypes enabled by Gemini 2.0’s native multimodal capabilities.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="99isd">Gemini 2.0 Flash</h2><p data-block-key="7lrrf">Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.</p></div>
  

  
    







  
    
        <div data-analytics-module="{
            &quot;module_name&quot;: &quot;Inline Images&quot;,
            &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
          }">
    

    <p><img alt="A chart comparing Gemini models and their capabilities" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_benchmarks_narrow_light2x.gif">
          
        
      
      </p>
      
    
      </div>
    
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><p data-block-key="99isd">Our goal is to get our models into people’s hands safely and quickly. Over the past month, we’ve been sharing early, experimental versions of Gemini 2.0, getting great feedback from developers.</p><p data-block-key="964c">Gemini 2.0 Flash is available now as an experimental model to developers via the Gemini API in <a href="https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-exp">Google AI Studio</a> and <a href="https://cloud.google.com/generative-ai-studio">Vertex AI</a> with multimodal input and text output available to all developers, and text-to-speech and native image generation available to early-access partners. General availability will follow in January, along with more model sizes.</p><p data-block-key="a4e0l">To help developers build dynamic and interactive applications, we’re also releasing a new Multimodal Live API that has real-time audio, video-streaming input and the ability to use multiple, combined tools. More information about 2.0 Flash and the Multimodal Live API can be found in our <a href="https://developers.googleblog.com/en/the-next-chapter-of-the-gemini-era-for-developers/">developer blog</a>.</p><h3 data-block-key="dsvak">Gemini 2.0 available in Gemini app, our AI assistant</h3><p data-block-key="en32v">Also starting today, <a href="https://gemini.google.com/">Gemini</a> users globally can access a chat optimized version of 2.0 Flash experimental by selecting it in the model drop-down on desktop and mobile web and it will be available in the Gemini mobile app soon. With this new model, users can experience an even more helpful Gemini assistant.</p><p data-block-key="ea033">Early next year, we’ll expand Gemini 2.0 to more Google products.</p></div>
  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="ju7ol">Unlocking agentic experiences with Gemini 2.0</h2><p data-block-key="78rfe">Gemini 2.0 Flash’s native user interface action-capabilities, along with other improvements like multimodal reasoning, long context understanding, complex instruction following and planning, compositional function-calling, native tool use and improved latency, all work in concert to enable a new class of agentic experiences.</p><p data-block-key="3k97r">The practical application of AI agents is a research area full of exciting possibilities. We’re exploring this new frontier with a series of prototypes that can help people accomplish tasks and get things done. These include an update to Project Astra, our research prototype exploring future capabilities of a universal AI assistant; the new Project Mariner, which explores the future of human-agent interaction, starting with your browser; and Jules, an AI-powered code agent that can help developers.</p><p data-block-key="c8gij">We’re still in the early stages of development, but we’re excited to see how trusted testers use these new capabilities and what lessons we can learn, so we can make them more widely available in products in the future.</p></div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Introducing Gemini 2.0: our new AI model for the agentic era" data-video-id="Fs0t6SdODd8" data-index-id="10" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            <p><img alt="Gemini 2.0 supercut video" src="https://i.ytimg.com/vi_webp/Fs0t6SdODd8/default.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/Fs0t6SdODd8/sddefault.webp&quot;,
                  &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/Fs0t6SdODd8/maxresdefault.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241210-1809#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      

      
    </div>


  


  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="ju7ol">Project Astra: agents using multimodal understanding in the real world</h2><p data-block-key="fdk7b">Since we introduced <a href="https://deepmind.google/technologies/gemini/project-astra/">Project Astra</a> at I/O, we’ve been learning from trusted testers using it on Android phones. Their valuable feedback has helped us better understand how a universal AI assistant could work in practice, including implications for safety and ethics. Improvements in the latest version built with Gemini 2.0 include:</p><ul><li data-block-key="4arno"><b>Better dialogue:</b> Project Astra now has the ability to converse in multiple languages and in mixed languages, with a better understanding of accents and uncommon words.</li><li data-block-key="1f3oh"><b>New tool use:</b> With Gemini 2.0, Project Astra can use Google Search, Lens and Maps, making it more useful as an assistant in your everyday life.</li><li data-block-key="9f826"><b>Better memory:</b> We’ve improved Project Astra’s ability to remember things while keeping you in control. It now has up to 10 minutes of in-session memory and can remember more conversations you had with it in the past, so it is better personalized to you.</li><li data-block-key="68bh5"><b>Improved latency:</b> With new streaming capabilities and native audio understanding, the agent can understand language at about the latency of human conversation.</li></ul><p data-block-key="4qful">We’re working to bring these types of capabilities to Google products like <a href="http://gemini.google.com/">Gemini</a> app, our AI assistant, and to other form factors like glasses. And we’re starting to expand our trusted tester program to more people, including a small group that will soon begin testing Project Astra on prototype glasses.</p></div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Introducing Gemini 2.0: our new AI model for the agentic era" data-video-id="hIIlJt8JERI" data-index-id="13" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            <p><img alt="Project Astra demo video" src="https://i.ytimg.com/vi_webp/hIIlJt8JERI/default.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/hIIlJt8JERI/sddefault.webp&quot;,
                  &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/hIIlJt8JERI/maxresdefault.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241210-1809#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      

      
    </div>


  


  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Project Mariner: agents that can help you accomplish complex tasks</h2><p data-block-key="b0b0b">Project Mariner is an early research prototype built with Gemini 2.0 that explores the future of human-agent interaction, starting with your browser. As a research prototype, it’s able to understand and reason across information in your browser screen, including pixels and web elements like text, code, images and forms, and then uses that information via an experimental Chrome extension to complete tasks for you.</p><p data-block-key="d0nh0">When evaluated against the <a href="https://arxiv.org/abs/2401.13919">WebVoyager benchmark</a>, which tests agent performance on end-to-end real world web tasks, Project Mariner <a href="http://deepmind.google/technologies/project-mariner">achieved a state-of-the-art result of 83.5%</a> working as a single agent setup.</p><p data-block-key="8mvv5">It’s still early, but Project Mariner shows that it’s becoming technically possible to navigate within a browser, even though it’s not always accurate and slow to complete tasks today, which will improve rapidly over time.</p><p data-block-key="2n0oq">To build this safely and responsibly, we’re conducting active research on new types of risks and mitigations, while keeping humans in the loop. For example, Project Mariner can only type, scroll or click in the active tab on your browser and it asks users for final confirmation before taking certain sensitive actions, like purchasing something.</p><p data-block-key="ch96g">Trusted testers are starting to test Project Mariner using an experimental Chrome extension now, and we’re beginning conversations with the web ecosystem in parallel.</p></div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Introducing Gemini 2.0: our new AI model for the agentic era" data-video-id="2XJqLPqHtyo" data-index-id="16" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            <p><img alt="Mariner demo video" src="https://i.ytimg.com/vi_webp/2XJqLPqHtyo/default.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/2XJqLPqHtyo/sddefault.webp&quot;,
                  &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/2XJqLPqHtyo/maxresdefault.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241210-1809#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      

      
    </div>


  


  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Jules: agents for developers</h2><p data-block-key="216k6">Next, we’re exploring how AI agents can assist developers with Jules — an experimental AI-powered code agent that integrates directly into a GitHub workflow. It can tackle an issue, develop a plan and execute it, all under a developer’s direction and supervision. This effort is part of our long-term goal of building AI agents that are helpful in all domains, including coding.</p><p data-block-key="acjnm">More information about this ongoing experiment can be found in our <a href="https://developers.googleblog.com/en/the-next-chapter-of-the-gemini-era-for-developers/">developer blog post</a>.</p></div>
  

  
    







  
    
        <div data-analytics-module="{
            &quot;module_name&quot;: &quot;Inline Images&quot;,
            &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
          }">
    

    <p>

        
        
          
            <video tabindex="0" autoplay="" loop="" muted="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Jules_GIF_3D_10_1.mp4" type="video/mp4" title="Animation of Jules coding assistant" alt="Jules">
              Video format not supported
            </video>
          
        
      
      </p>
      
    
      </div>
    
  



  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Agents in games and other domains</h2><p data-block-key="384g8">Google DeepMind has a <a href="https://deepmind.google/discover/blog/agent57-outperforming-the-human-atari-benchmark/">long</a> <a href="https://deepmind.google/research/breakthroughs/alphago/">history</a> of using games to help AI models become better at following rules, planning and logic. Just last week, for example, we introduced <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">Genie 2</a>, our AI model that can create an endless variety of playable 3D worlds — all from a single image. Building on this tradition, we’ve built agents using Gemini 2.0 that can help you navigate the virtual world of video games. It can reason about the game based solely on the action on the screen, and offer up suggestions for what to do next in real time conversation.</p><p data-block-key="b3sa9">We're collaborating with leading game developers like Supercell to explore how these agents work, testing their ability to interpret rules and challenges across a diverse range of games, from strategy titles like “Clash of Clans” to farming simulators like “Hay Day.”</p><p data-block-key="2p0a">Beyond acting as virtual gaming companions, these agents can even tap into Google Search to connect you with the wealth of gaming knowledge on the web.</p></div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Introducing Gemini 2.0: our new AI model for the agentic era" data-video-id="IKuGNHJBGsc" data-index-id="22" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            <p><img alt="Navi demo video" src="https://i.ytimg.com/vi_webp/IKuGNHJBGsc/default.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/IKuGNHJBGsc/sddefault.webp&quot;,
                  &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/IKuGNHJBGsc/maxresdefault.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241210-1809#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      

      
    </div>


  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><p data-block-key="of3wa">In addition to exploring agentic capabilities in the virtual world, we’re experimenting with agents that can help in the physical world by applying Gemini 2.0's spatial reasoning capabilities to robotics. While it’s still early, we’re excited about the potential of agents that can assist in the physical environment.</p><p data-block-key="ba0kt">You can learn more about these research prototypes and experiments at <a href="http://labs.google/">labs.google</a>.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Building responsibly in the agentic era</h2><p data-block-key="6j36d">Gemini 2.0 Flash and our research prototypes allow us to test and iterate on new capabilities at the forefront of AI research that will eventually make Google products more helpful.</p><p data-block-key="fvsj8">As we develop these new technologies, we recognize the responsibility it entails, and the many questions AI agents open up for safety and security. That is why we are taking an exploratory and gradual approach to development, conducting research on multiple prototypes, iteratively implementing safety training, working with trusted testers and external experts and performing extensive risk assessments and safety and assurance evaluations.</p><p data-block-key="angf4">For example:</p><ul><li data-block-key="crrpj">As part of our safety process, we’ve worked with our Responsibility and Safety Committee (RSC), our longstanding internal review group, to identify and understand potential risks.</li><li data-block-key="2uku6">Gemini 2.0's reasoning capabilities have enabled major advancements in our AI-assisted red teaming approach, including the ability to go beyond simply detecting risks to now automatically generating evaluations and training data to mitigate them. This means we can more efficiently optimize the model for safety at scale.</li><li data-block-key="81t7m">As Gemini 2.0’s multimodality increases the complexity of potential outputs, we’ll continue to evaluate and train the model across image and audio input and output to help improve safety.</li><li data-block-key="clba0">With Project Astra, we’re exploring potential mitigations against users unintentionally sharing sensitive information with the agent, and we’ve already built in privacy controls that make it easy for users to delete sessions. We’re also continuing to research ways to ensure AI agents act as reliable sources of information and don’t take unintended actions on your behalf.</li><li data-block-key="7r7pa">With Project Mariner, we’re working to ensure the model learns to prioritize user instructions over 3rd party attempts at prompt injection, so it can identify potentially malicious instructions from external sources and prevent misuse. This prevents users from being exposed to fraud and phishing attempts through things like malicious instructions hidden in emails, documents or websites.</li></ul><p data-block-key="f9e42">We firmly believe that the only way to build AI is to be responsible from the start and we'll continue to prioritize making safety and responsibility a key element of our model development process as we advance our models and agents.</p></div>
  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Gemini 2.0, AI agents and beyond</h2><p data-block-key="35lej">Today’s releases mark a new chapter for our Gemini model. With the release of Gemini 2.0 Flash, and the series of research prototypes exploring agentic possibilities, we have reached an exciting milestone in the Gemini era. And we’re looking forward to continuing to safely explore all the new possibilities within reach as we build towards AGI.</p></div>
  

  
    














<uni-related-content-tout title="Gemini 2.0: Our latest, most capable AI model yet" cta="See more" summary="See how Gemini 2.0 and our research prototypes work — and how they’ll help make our Google products more helpful." hideimage="False" eyebrow="Collection" fullurl="https://blog.google/products/gemini/google-gemini-ai-collection-2024/" pagetype="collectiondetailpage" isarticlepage="">
  
    <div slot="rct-image-slot">
      
    <figure>
        <picture>
            


    

    
        <source media="(max-resolution: 1.5dppx)" sizes="300px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-300.format-webp.webp 300w">
    
        <source media="(min-resolution: 1.5dppx)" sizes="600px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-600.format-webp.webp 600w">
    

    <img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-600.format-webp.webp" alt="gemini social share collection" sizes=" 300px,  600px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-300.format-webp.webp 300w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-600.format-webp.webp 600w" data-target="image" loading="lazy">
    


        </picture>
    </figure>


    </div>
  
</uni-related-content-tout>

  

  
    

  
    





<div role="form" aria-label="Sign up to receive weekly news and stories from Google." data-component="uni-subscribe" data-analytics-module="{
    &quot;module_name&quot;: &quot;Newsletter&quot;,
    &quot;section_header&quot;: &quot;Get more stories from Google in your inbox.&quot;
  }">
        
        
        <p>You are already subscribed to our newsletter.</p>
      </div>

  

  


            
            

            
              




            
          </div>
        
      </div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dioxus 0.6 – Crossplatform apps with Rust (184 pts)]]></title>
            <link>https://dioxuslabs.com/blog/release-060/</link>
            <guid>42388665</guid>
            <pubDate>Wed, 11 Dec 2024 15:24:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dioxuslabs.com/blog/release-060/">https://dioxuslabs.com/blog/release-060/</a>, See on <a href="https://news.ycombinator.com/item?id=42388665">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-node-hydration="103"><div><a href="https://dioxuslabs.com/blog/" data-node-hydration="104,click:1"><p data-node-hydration="105"><svg viewBox="0 0 16 16" width="16" style="width: 12px; height: 12px; color: currentcolor;" data-testid="geist-icon" height="16" stroke-linejoin="round"><path d="M10.5 14.0607L9.96966 13.5303L5.14644 8.7071C4.75592 8.31658 4.75592 7.68341 5.14644 7.29289L9.96966 2.46966L10.5 1.93933L11.5607 2.99999L11.0303 3.53032L6.56065 7.99999L11.0303 12.4697L11.5607 13L10.5 14.0607Z" clip-rule="evenodd" fill="currentColor" fill-rule="evenodd"></path></svg>Back to blog</p></a><h2><!--node-id106-->Dioxus 0.6<!--#--></h2><p><!--node-id107-->December 9, 2024<!--#--> - <!--node-id108-->Jonathan Kelley<!--#--></p><h3><!--node-id109-->Massive Tooling Improvements: Mobile Simulators, Magical Hot-Reloading, Interactive CLI, and more!<!--#--></h3></div><div><p data-node-hydration="110">Today we're releasing Dioxus 0.6!</p><p data-node-hydration="111">Dioxus is a framework for building fullstack web, desktop, and mobile apps with a single codebase. Our goal is to build a "Flutter but better." Dioxus focuses on first-class fullstack web support, type-safe server/client communication, and blazing fast performance.</p><p data-node-hydration="112">With this release, we focused on making Dioxus easier to use, improving the developer experience, and fixing bugs.</p><p data-node-hydration="113">Headlining the release is a complete overhaul of the Dioxus CLI:</p><ul data-node-hydration="114"><li><strong><a href="#android-and-ios-support-for"><code>dx serve</code> for mobile</a></strong>: Serve your app on Android and iOS simulators and devices.</li><li><strong><a href="#completely-revamped-hot-reloading">Magical Hot-Reloading</a></strong>: Hot-Reloading of formatted strings, properties, and nested <code>rsx!{}</code>.</li><li><strong><a href="#interactive-command-line-tools">Interactive CLI</a></strong>: Rewrite of the Dioxus CLI with a new, interactive UX inspired by Astro.</li><li><strong><a href="#inline-wasm-stacktraces-and">Inline Stack Traces</a></strong>: Capture WASM panics and logs directly into your terminal.</li><li><strong><a href="#fullstack-desktop-and-mobile">Server Functions for Native</a></strong>: Inline Server RPC for Desktop and Mobile apps.</li></ul><p data-node-hydration="115">We also improved the developer experience across the entire framework, fixing long standing bugs and improving tooling:</p><ul data-node-hydration="116"><li><strong><a href="#toasts-and-loading-screens">Toasts and Loading Screens</a></strong>: New toasts and loading screens for web apps in development.</li><li><strong><a href="#completely-revamped-autocomplete">Improved Autocomplete</a></strong>: Massively improved autocomplete of RSX.</li><li><strong><a href="#stabilizing-manganis"><code>asset!</code> Stabilization</a></strong>: Stabilizing our linker-based asset system integrated for native apps.</li><li><strong><a href="#suspense-and-html-streaming-for-the-web">Streaming HTML</a></strong>: Stream <code>Suspense</code> and <code>Error</code> Boundaries from the server to the client.</li><li><strong><a href="#static-site-generation-and-isg">SSG and ISG</a></strong>: Support for Static Site Generation and Incremental Static Regeneration.</li><li><strong><a href="#question-mark-error-handling">Error Handling with  <code>?</code></a></strong>: Use <code>?</code> to handle errors in event handlers, tasks, and components.</li><li><strong><a href="#document-elements">Meta Elements</a></strong>: New <code>Head</code>, <code>Title</code>, <code>Meta</code>, and <code>Link</code> elements for setting document attributes.</li><li><strong><a href="#synchronous">Synchronous  <code>prevent_default</code></a></strong>: Handle events synchronously across all platforms.</li><li><strong><a href="#tracking-size-with"><code>onresize</code> Event Handler</a></strong>: Track an element's size without an IntersectionObserver.</li><li><strong><a href="#tracking-visibility-with"><code>onvisible</code> Event Handler</a></strong>: Track an element's visibility without an IntersectionObserver.</li><li><strong><a href="#hybrid-wgpu-overlays">WGPU Integration</a></strong>: Render Dioxus as an overlay on top of WGPU surfaces and child windows.</li><li><strong><a href="#web-ios-and-android-bundle-support"><code>dx bundle</code> for Web, iOS, and Android</a></strong>: Complete <code>dx bundle</code> support for every platform.</li><li><strong><a href="#json-output-for-ci--cli"><code>json</code> mode</a></strong>: Emit CLI messages as JSON for use by 3rd party tools and CI/CD pipelines.</li><li><strong><a href="#new-starter-templates">New Templates</a></strong>: Three new starter templates for cross-platform apps.</li><li><strong><a href="#nightly-docs-tutorials-and-new-guides">Nightly Tutorial and Guides</a></strong>: New tutorials and guides for Dioxus 0.6 and beyond.</li><li><strong><a href="#preview-of-in-place-binary-patching">Binary Patching Prototype</a></strong>: Prototype of our new pure Rust hot-reloading engine.</li></ul><h2 id="about-this-release" data-node-hydration="117"><a href="#about-this-release">About this Release</a></h2><p data-node-hydration="118">Dioxus 0.6 is our biggest release ever: over 350 pull requests merged and hundreds of issues closed. We shipped 0.6 with a few goals:</p><ul data-node-hydration="119"><li>Dramatically improve the quality of hot-reloading, autocomplete, and asset bundling.</li><li>Make the Dioxus CLI more robust and easier to use.</li><li>Inline our mobile tooling into the dioxus CLI for 1st-class mobile support.</li></ul><p data-node-hydration="120">Since this post is quite long, we made a quick video highlighting new features, bugs fixed, and a quick tour of everything you can do with Dioxus now:</p><p data-node-hydration="121"><iframe height="500px" src="https://www.youtube.com/embed/WgAjWPKRVlQ" title="Dioxus 0.6" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p><p data-node-hydration="123">Dioxus 0.6 is shipping with a completely overhauled CLI experience! We’ve completely rewritten the CLI to support a ton of new features and improve stability:</p><p data-node-hydration="124"><img src="https://dioxuslabs.com/assets/image1-5eec3673e8f5fbfe.avif" alt="new-cli.png" title="" data-node-hydration="125"></p><p data-node-hydration="126">The new CLI sports live progress bars, animations, an interactive filter system, the ability to change log levels on the fly, and more.</p><p data-node-hydration="129">The CLI rewrite alone took more than half this release cycle. We went through several different design iterations and solved tons of bugs along the way. A few of the highlights:</p><ul data-node-hydration="130"><li>You can manually rebuild your app by pressing <code>r</code></li><li>You can toggle the log level of the CLI output on the fly and even inspect Cargo internal logs</li><li>We output all internal logs of the CLI so you can debug any issues</li><li>We capture logs for WASM tracing and panics</li><li>We dropped the <code>outdir</code> concept and instead use <code>target/dx</code> for all output.</li><li>Inline support for iOS and Android emulators.</li></ul><p data-node-hydration="131">You can install the new CLI using <a href="https://github.com/cargo-bins/cargo-binstall">cargo binstall</a> with <code>cargo binstall dioxus-cli@0.6.0 --force</code>.</p><h2 id="android-and-ios-support-for" data-node-hydration="132"><a href="#android-and-ios-support-for">Android and iOS support for </a><code>dx serve</code></h2><p data-node-hydration="133">With Dioxus 0.6, the dioxus CLI supports  <code>dx serve --platform ios/android</code> out of the box! 🎉</p><p data-node-hydration="134">While Dioxus has always had mobile support, the Rust tooling for mobile has been extremely unstable. Users constantly ran into issues with tools like <a href="https://github.com/BrainiumLLC/cargo-mobile"><code>cargo-mobile</code></a> and <a href="https://github.com/tauri-apps/cargo-mobile2"><code>cargo-mobile2</code></a>. These tools, while useful, take a different architectural approach than what is a good fit for Dioxus.</p><p data-node-hydration="135">With this release, we wrote our entire mobile tooling system from scratch. Now, you can go from  <code>dx new</code> to  <code>dx serve --platform ios</code> in a matter of seconds.</p><p data-node-hydration="136"><img src="https://dioxuslabs.com/assets/image-66cb6ea50fe694ec.avif" alt="Dioxus Mobile Support" title="" data-node-hydration="137"></p><p data-node-hydration="138">The Android and iOS simulator targets support all the same features as desktop: hot-reloading, fast rebuilds, asset bundling, logging, etc. Dioxus is also the only Rust framework that supports  <code>main.rs</code> for mobile - no other tools have supported the same  <code>main.rs</code> for every platform until now.</p><p data-node-hydration="139">Our inline mobile support requires no extra configurations, no manual setup for Gradle, Java, Cocoapods, and no other 3rd party tooling. If you already have the Android NDK or iOS Simulator installed, you currently are less than 30 seconds away from a production-ready mobile app written entirely in Rust.</p><p data-node-hydration="142">The simplest Dioxus 0.6 Mobile app is tiny:</p><p data-node-hydration="149">Especially, when compared to v0.5 which required you to migrate your app to a  <code>cdylib</code> and manually set up the binding layer:</p><p data-node-hydration="156">While 1st-class support for mobile platforms is quite exciting, there are certainly many limitations: the Rust mobile ecosystem is nascent, we don’t have great ways of configuring the many platform-specific build flags, and there isn’t a particularly great Rust/Java interop story.</p><p data-node-hydration="157">If you're interested in helping us build out mobile support, please join us on <a href="https://discord.gg/XgGxMSkvUM">Discord</a>.</p><h2 id="completely-revamped-hot-reloading" data-node-hydration="158"><a href="#completely-revamped-hot-reloading">Completely Revamped Hot-Reloading</a></h2><p data-node-hydration="159">We shipped massive improvements to the hot-reloading engine powering Dioxus. Our internal goal was to iterate on the Dioxus Docsite with zero full rebuilds.</p><p data-node-hydration="160">This means we needed to add support for a number of new hot-reloading engine changes:</p><ul data-node-hydration="161"><li>Hot-reload formatted strings</li><li>Hot-reload nested rsx blocks</li><li>Hot-reload component properties and simple Rust expressions</li><li>Hot-reload mobile platforms and their bundled assets</li></ul><p data-node-hydration="162">The new hot-reloading engine almost feels like magic - you can quickly iterate on new designs - and even modify simple Rust code! - without waiting for full rebuilds:</p><p data-node-hydration="165">The new engine allows you to modify formatted strings anywhere in your  <code>rsx</code>: in text blocks, element attributes, and even on component properties.</p><p data-node-hydration="172">The same tooling that enables component props reloading also works with <em>any Rust literal!</em> You can hot-reload numbers, booleans, and strings on component prop boundaries.</p><p data-node-hydration="181">The new hot-reloading engine also brings nested rsx hot-reloading support. The contents of  <code>for</code> loops,  <code>if</code> statements, and component bodies all now participate in hot-reloading:</p><p data-node-hydration="188">You can now move and clone Rust expressions between contexts, allowing you to re-use components and formatted strings between element properties without a full rebuild.</p><p data-node-hydration="195">These changes are supported in all platforms: web, desktop, and mobile.</p><p data-node-hydration="196">You can now hot-reload RSX and Assets on iOS and Android apps in addition to the classic web and desktop platforms.</p><p data-node-hydration="199">The new hot-reloading feels like magic and we encourage you to try it out!</p><h2 id="completely-revamped-autocomplete" data-node-hydration="200"><a href="#completely-revamped-autocomplete">Completely Revamped Autocomplete</a></h2><p data-node-hydration="201">Another huge overhaul in Dioxus 0.6: greatly improved autocomplete of  <code>rsx! {}</code>.  Our old implementation of  <code>rsx! {}</code> suffered from poor integration with tools like Rust-analyzer which provide language-server integration for your code. If the input to the macro wasn’t perfectly parsable, we failed to generate any tokens at all, meaning rust-analyzer couldn’t jump in to provide completions.</p><p data-node-hydration="202">The work to fix this was immense. Macro parsing libraries like  <code>syn</code> don’t provide great facilities for “partial parsing” Rust code which is necessary for implementing better errors and autocomplete. We had to rewrite the entire internals of  <code>rsx! {}</code> to support partial parsing of  <code>rsx! {}</code> , but finally, in 0.6, we’re able to provide stellar autocomplete. Not only can we autocomplete Rust code in attribute positions, but with a few tricks, we’re able to automatically insert the appropriate braces next to element names:</p><p data-node-hydration="203"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_9.55.12_PM-da711de8a0f26852.avif" alt="Screenshot 2024-11-14 at 9.55.12 PM.png" title="" data-node-hydration="204"></p><p data-node-hydration="205">The autocomplete experience is much nicer now, with all attributes, elements, components, and inline Rust code benefiting from the overhauled experience. All Rust expressions participate in proper Rust-analyzer autocomplete and we're even able to provide warnings when  <code>rsx!{}</code> input is malformed instead of panicking.</p><h2 id="inline-wasm-stacktraces-and" data-node-hydration="208"><a href="#inline-wasm-stacktraces-and">Inline WASM stacktraces and </a><code>tracing</code> integration</h2><p data-node-hydration="209">Along with the rewrite of the CLI, we shipped a  <code>tracing</code> integration for WASM apps that captures panics and logs and sends them  <code>dx</code> in your terminal. When you build your app with debug symbols, stack traces directly integrate with your editor, allowing you to jump directly to the troublesome files from within your terminal.</p><p data-node-hydration="210"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_8.52.18_PM-09c4ac2afae8d38e.avif" alt="Inline Stack Traces" title="" data-node-hydration="211"></p><p data-node-hydration="212">Thanks to this integration, we now have much nicer logging around fullstack apps, showing status codes, fetched assets, and other helpful information during development. With the toggle-able verbosity modes, you can now inspect the internal logs of the CLI itself, making it easier to debug issues with tooling to understand what exactly  <code>dx</code> is doing when it builds your app. Simply type  <code>v</code> to turn on “verbose” mode and  <code>t</code> to turn on “trace” mode for more helpful logs:</p><p data-node-hydration="213"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_9.06.05_PM-fa7be0c82414b119.avif" alt="Screenshot 2024-11-14 at 9.06.05 PM.png" title="" data-node-hydration="214"></p><h2 id="toasts-and-loading-screens" data-node-hydration="215"><a href="#toasts-and-loading-screens">Toasts and Loading Screens</a></h2><p data-node-hydration="216">As part of our CLI overhaul, we wanted to provide better feedback for developers when building web apps. Dioxus 0.6 will now show Popup Toasts and Loading Screens for web apps in development mode.</p><p data-node-hydration="217">Now, when your app is building, Dioxus will render a loading screen with the current progress of the build:</p><p data-node-hydration="218"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_9.41.38_PM-5776d82a4fca6b2b.avif" alt="Screenshot 2024-11-14 at 9.41.38 PM.png" title="" data-node-hydration="219"></p><p data-node-hydration="220">Additionally, once the app is rebuilt, you’ll receive a toast indicating the status of the build:</p><p data-node-hydration="221"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_9.42.33_PM-985e43ad24eac448.avif" alt="Screenshot 2024-11-14 at 9.42.33 PM.png" title="" data-node-hydration="222"></p><h2 id="fullstack-desktop-and-mobile" data-node-hydration="223"><a href="#fullstack-desktop-and-mobile">Fullstack Desktop and Mobile</a></h2><p data-node-hydration="224">Additionally, we properly integrated server functions with native apps. Server functions finally work out-of-the-box when targeting desktop and mobile:</p><p data-node-hydration="227">By default, in development, we set the server function endpoint to be localhost, so in production you need to make sure to point the functions to your deployed server:</p><h2 id="stabilizing-manganis" data-node-hydration="234"><a href="#stabilizing-manganis">Stabilizing Manganis </a><code>asset!()</code> system</h2><p data-node-hydration="235">We introduced our new asset system,&nbsp;<a href="https://github.com/DioxusLabs/manganis">Manganis</a>, in an alpha state with the 0.5 release. Dioxus 0.6 stabilizes the asset system and fixes several bugs and performance issues. You can try out the new&nbsp;<a href="https://github.com/DioxusLabs/manganis/pull/30">linker based asset system</a>&nbsp;by including an&nbsp;<code>asset!</code>&nbsp;anywhere in your code. It will automatically be optimized and bundled across all platforms:</p><p data-node-hydration="242">Manganis is a crucial step in supporting assets cross-platform, and specifically, through dependencies. Previously, if an upstream library wanted to export an asset like an image or a stylesheet, your app would need to manually add those assets in your  <code>assets</code> folder. This gets complex and messy when libraries generate CSS: many classes are duplicated and might even conflict with each other. Now, all CSS collected by the  <code>asset!()</code> macro is processed via our build pipeline, benefiting from minification and deduplication. Libraries can include their stylesheets and images and components and you can be guaranteed that those assets make it bundled into your app:</p><p data-node-hydration="249">Even better, assets like images are automatically optimized to generate thumbnails and more optimized formats. This can cut huge amounts of data from your site - AVIF and Webp can reduce file sizes by up to 90%. A funny note - platforms like Vercel actually <a href="https://vercel.com/docs/image-optimization">provide paid products for image optimization</a> while Manganis can do this for you, for free, at build time!</p><p data-node-hydration="250"><img src="https://dioxuslabs.com/assets/manganis-opt-09461e92e1507282.avif" alt="manganis-opt" title="" data-node-hydration="251"></p><p data-node-hydration="252">Additionally, manganis automatically hashes the images and modifies the generated asset name, allowing for better integration with CDNs and browser caching.</p><p data-node-hydration="253"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_10.22.48_PM-bdb45daf44c2cae9.avif" alt="Screenshot 2024-11-14 at 10.22.48 PM.png" title="" data-node-hydration="254"></p><p data-node-hydration="255">Manganis can handle a wide variety of formats - applying optimizations to assets like CSS, JavaScript, images, videos, and more.</p><p data-node-hydration="256">In Dioxus 0.5, we released Manganis in “alpha” status - and in 0.6 we’re stabilizing it. We’ve adjusted the API, so you’ll need to update any existing code that already uses it. Our new implementation is much more reliable, solving many of the bugs users were running into after the 0.5 release.</p><p data-node-hydration="257">Our new system leverages <em>the linker</em> to extract asset locations from the compiled binary. This is a rather advanced technique and took a while to get right, but we believe it’s a more robust solution in the long term. If you’re interested in integrating Manganis into your libraries and apps (like say, Bevy!), we have a guide just for that.</p><h2 id="suspense-and-html-streaming-for-the-web" data-node-hydration="258"><a href="#suspense-and-html-streaming-for-the-web">Suspense and HTML Streaming for the Web</a></h2><p data-node-hydration="259">Async is a core component of any UI framework. Dioxus provides hooks to handle async state. You can start a future and handle the loading and resolved states within the component:</p><p data-node-hydration="266">This works ok if you have a single future, but it quickly gets messy when combining many futures into one UI:</p><p data-node-hydration="273">In addition to hooks, we need a way to display a different state when async is loading. Dioxus 0.6 introduces a new core primitive for async UI called suspense boundaries. A suspense boundary is a component that renders a placeholder when any child component is loading:</p><p data-node-hydration="280">In any child component, you can simply bubble up the pending state with&nbsp; <code>?</code>&nbsp;to pause rendering until the future is finished:</p><p data-node-hydration="287">Along with suspense boundaries, dioxus fullstack also supports streaming each suspense boundary in from the server. Instead of waiting for the whole page to load, dioxus fullstack streams in each chunk with the resolved futures as they finish:</p><p data-node-hydration="290">Many of these features are quite cutting-edge and are just now being rolled out in frameworks in the JavaScript ecosystem. Getting the details right for Dioxus was quite difficult. We wanted to support both the fullstack web as well as native desktop and mobile apps. These two platforms often have competing design considerations. Fortunately, suspense also works for desktop and mobile, allowing you to emulate web-like data fetching patterns for native apps.</p><h2 id="static-site-generation-and-isg" data-node-hydration="291"><a href="#static-site-generation-and-isg">Static Site Generation and ISG</a></h2><p data-node-hydration="292">As part of our work on streaming, we also wanted to support another cutting-edge web feature: incremental static generation (ISG) and its cousin static site generation (SSG).</p><p data-node-hydration="293">Static site generation is a technique used by many web frameworks like Jekyll, Hugo, or Zola, to emit static  <code>.html</code> not reliant on JavaScript. Sites like blogs and portfolios typically use static site generation since platforms like GitHub Pages allow hosting static sites for free. In fact, this very docsite uses Dioxus SSG deployed to GitHub Pages! SSG helps improve SEO and speed up load times for your users.</p><p data-node-hydration="294">In Dioxus 0.6, we now support static-site-generation out of the box for all fullstack projects. Simply add a server function to your app called  <code>static_routes</code> that returns the list of routes that  <code>dx</code> should generate:</p><p data-node-hydration="301">Now, when you want to emit your static  <code>.html</code>, add the  <code>--ssg</code>  flag to  <code>dx build</code>:</p><p data-node-hydration="308">Static-site-generation is built on a new feature in Dioxus called incremental-site-generation (ISG). ISG is a technique similar to static-site-generation where the server generates pages on demand and caches them on the system filesystem. This allows the server to cache huge amounts of pages (for something like a school’s facebook directory or an e-commerce site with thousands of products) that get periodically invalidated. ISG is a somewhat advanced technique but is required to enable when using static-site-generation:</p><p data-node-hydration="315">We will likely be changing these APIs in future releases, but we are eager to let users experiment with these new features to simplify the existing static site setup.</p><h2 id="document-elements" data-node-hydration="316"><a href="#document-elements">Document Elements: </a><code>Title {}</code> , <code>Link {}</code> , <code>Stylesheet</code> , and <code>Meta {}</code></h2><p data-node-hydration="317">To date, it’s been rather cumbersome to do seemingly simple JavaScript operations in Dioxus. Due to our cross-platform nature, we need to find solutions to simple problems in ways that work for web, desktop, and mobile with a single abstraction.</p><p data-node-hydration="318">With Dioxus 0.6, we’re providing special elements under the  <code>document</code> namespace that make it possible to interact with the HTML  <code>document</code> object without needing to write extra JavaScript.</p><p data-node-hydration="319">Now, to set the  <code>title</code> of your HTML document, simply use the  <code>document::Title {}</code> component:</p><p data-node-hydration="326">And accordingly, the title of the page will update:</p><p data-node-hydration="327"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_11.28.42_PM-b6b3d19eb8871209.avif" alt="Screenshot 2024-11-14 at 11.28.42 PM.png" title="" data-node-hydration="328"></p><p data-node-hydration="329">Similarly, with  <code>Link</code> ,  <code>Stylesheet</code> , and  <code>Style</code>, you can include elements that automatically get merged into the document’s  <code>&lt;head&gt;</code> element. During server side rendering, these links get collected, deduplicated, and minified. With these built-in  <code>document</code> components, you’re now guaranteed that your  <code>&lt;head&gt;</code> element is properly set for pre-loading heavy assets like stylesheets and external JavaScript.</p><p data-node-hydration="336"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_11.31.18_PM-60417025a9cec40b.avif" alt="Screenshot 2024-11-14 at 11.31.18 PM.png" title="" data-node-hydration="337"></p><h2 id="question-mark-error-handling" data-node-hydration="338"><a href="#question-mark-error-handling">Question Mark Error Handling</a></h2><p data-node-hydration="339">With this release, we’ve made the transition where  <code>Element</code> is no longer an  <code>Option&lt;Node&gt;</code> but rather a  <code>Result&lt;Node&gt;</code>. This means we’re <em>finally</em> able to open up the use of typical Rust error handling in components:</p><p data-node-hydration="346">The new  <code>RenderError</code> acts like anyhow’s  <code>Error</code> type that can take in any  <code>dyn std::Error</code> type and propagate it upwards to the nearest error boundary.</p><p data-node-hydration="353">What’s even better: the  <code>?</code> syntax also works in EventHandlers, so you can quickly add things like server functions to your app without worrying about manual error handling:</p><p data-node-hydration="360">This new syntax lets Suspense and HTML-streaming return errors while rendering that don’t bring down the entire page.</p><h2 id="synchronous" data-node-hydration="361"><a href="#synchronous">Synchronous </a><code>prevent_default</code></h2><p data-node-hydration="362">In addition to being able to access the native event type, Dioxus 0.6 also makes all event handling synchronous. Previously, all event handling in Dioxus had to occur outside the normal browser event handling flow to support platforms like  <code>dioxus-desktop</code> which need to communicate over an interprocess communication (IPC) layer with the host webview. With this release, we’ve finally figured out how to enable blocking communication for  <code>dioxus-desktop</code> and can finally make event handling synchronous!</p><p data-node-hydration="363">As such, we no longer need the special  <code>dioxus_prevent_default</code> attribute and you can directly call  <code>event.prevent_default()</code>.</p><p data-node-hydration="370">This now makes it possible to implement  <code>prevent_default</code> conditionally which has previously been a limitation with Dioxus. Components like  <code>Link {}</code> now exhibit behavior exactly aligned with their native counterparts, solving long-standing issues with Dioxus apps.</p><h2 id="tracking-size-with" data-node-hydration="371"><a href="#tracking-size-with">Tracking size with </a><code>onresize</code></h2><p data-node-hydration="372">Thanks to the community, we now have two special handlers <em>not</em> found in the HTML spec: <code>onvisible</code> and <code>onresize</code>. These handlers are “special” dioxus handlers that automatically sets up an <code>IntersectionObserver</code> which previously required JavaScript.</p><p data-node-hydration="373">You can now implement rich interactions with little hassle:</p><h2 id="tracking-visibility-with" data-node-hydration="380"><a href="#tracking-visibility-with">Tracking visibility with </a><code>onvisible</code></h2><p data-node-hydration="381">In addition to  <code>onresize</code>, we now have a special handler <em>not</em> found in the HTML spec: <code>onvisible</code>.</p><p data-node-hydration="388">This makes it possible to add rich animations to your app without needing to write custom JavaScript.</p><h2 id="hybrid-wgpu-overlays" data-node-hydration="391"><a href="#hybrid-wgpu-overlays">Hybrid WGPU Overlays</a></h2><p data-node-hydration="392">This release also brings the "child window" feature for Dioxus desktop which lets you overlay native Dioxus apps on existing windows. This makes it simple to integrate Dioxus as an overlay over other renderers like WGPU and OpenGL:</p><h2 id="web-ios-and-android-bundle-support" data-node-hydration="395"><a href="#web-ios-and-android-bundle-support">Web, iOS, and Android bundle support</a></h2><p data-node-hydration="396">We added support for web and mobile with  <code>dx bundle</code>. Previously,  <code>dx bundle</code> only worked for desktop apps. Now you can bundle for a wide variety of targets:</p><ul data-node-hydration="397"><li>macOS (.app, .dmg)</li><li>Windows (.exe, .msi)</li><li>Linux (.deb, .rpm, .appimage)</li><li>Android (.apk)</li><li>iOS (.ipa, .app)</li><li>Web (.appimage, /public folder)</li></ul><h2 id="json-output-for-ci--cli" data-node-hydration="398"><a href="#json-output-for-ci--cli">JSON Output for CI / CLI</a></h2><p data-node-hydration="399">As part of our overhaul with the CLI, we’re also shipping a  <code>json-output</code> mode. Now, when you pass  <code>--json-output</code> to Dioxus commands, you will receive the logging in json format:</p><p data-node-hydration="400"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_10.38.33_PM-8328473ee184927b.avif" alt="Screenshot 2024-11-14 at 10.38.33 PM.png" title="" data-node-hydration="401"></p><p data-node-hydration="402">This is particularly important for users of  <code>dx bundle</code> who want to automatically upload the their bundles to their hosting provider of choice. You can easily combine the output of  <code>dx</code> with a tool like  <code>jq</code> to extract important information like bundle outputs with a simple one-liner:</p><p data-node-hydration="403"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_10.40.56_PM-51046f38000e601e.avif" alt="Screenshot 2024-11-14 at 10.40.56 PM.png" title="" data-node-hydration="404"></p><h2 id="new-starter-templates" data-node-hydration="405"><a href="#new-starter-templates">New Starter Templates</a></h2><p data-node-hydration="406">Dioxus 0.6 ships with three new starter templates for cross-platform apps. Each template is a fully-featured, production-ready app that you can use as a starting point for your own Dioxus apps.</p><ul data-node-hydration="407"><li>Bare-Bones: A bare-bones starter template with no styling, assets, or structure.</li><li>Jumpstart: A starter template with a basic structure, components, and a few pages.</li><li>Workspace: A starter template with separate crates for web, desktop, and mobile.</li></ul><p data-node-hydration="408">These are baked directly into the  <code>dx new</code> command - simply run  <code>dx new</code> and follow the prompts to select the template you want.</p><h2 id="nightly-docs-tutorials-and-new-guides" data-node-hydration="409"><a href="#nightly-docs-tutorials-and-new-guides">Nightly Docs, Tutorials, and New Guides</a></h2><p data-node-hydration="410">As usual with these large releases, Dioxus 0.6 features a rather sizable overhaul to the documentation. We’ve completely overhauled the tutorial to be less heavy on code. The new tutorial focuses on basics like including assets and deploying to production.</p><p data-node-hydration="411"><img src="https://dioxuslabs.com/assets/Screenshot_2024-11-14_at_11.35.23_PM-5896b8f2b94e25ea.avif" alt="Screenshot 2024-11-14 at 11.35.23 PM.png" title="" data-node-hydration="412"></p><p data-node-hydration="413">The docsite now includes all “modern” versions of Dioxus inline: 0.3, 0.4, 0.5, and 0.6 are all accessible under the same top-level website. Previously, we linked out to different MDbooks which eventually became a hassle. Now, you can simply switch between each version inline:</p><p data-node-hydration="414"><img src="https://dioxuslabs.com/assets/version_switch_shadow-2d19108722c8fa50.avif" alt="Screenshot 2024-11-15 at 1.02.23 AM.png" title="" data-node-hydration="415"></p><p data-node-hydration="416">The inline version switcher means we’ll now be able to publish documentation for alpha releases of Dioxus, hopefully making your life easier as we ship new features for the future. The new docs also feature small quality-of-life upgrades like breadcrumbs:</p><p data-node-hydration="417"><img src="https://dioxuslabs.com/assets/breadcrumbs_shadow-33da77df40ec45f0.avif" alt="Screenshot 2024-11-15 at 1.04.13 AM.png" title="" data-node-hydration="418"></p><p data-node-hydration="419">as well as new codeblocks with interactive examples:</p><p data-node-hydration="420"><img src="https://dioxuslabs.com/assets/interacitve_widget_shadow-be1d0998201ca923.avif" alt="Screenshot 2024-11-15 at 1.05.03 AM.png" title="" data-node-hydration="421"></p><h2 id="preview-of-in-place-binary-patching" data-node-hydration="422"><a href="#preview-of-in-place-binary-patching">Preview of In-Place Binary Patching</a></h2><p data-node-hydration="423">While working on the new hot-reloading engine, we experimented with adding proper hot-reloading of Rust code to Dioxus apps. The work here was inspired by Andrew Kelley’s “in-place-binary-patching” goal for Zig. Unfortunately, we didn’t have a chance to productionize the prototype for this release (way too many features already!) but we did put together a <a href="http://github.com/jkelleyrtp/ipbp">small prototype</a>:</p><p data-node-hydration="426">We likely won’t have the time to ship true Rust hot-reloading in 0.7, but stay tuned for early next year!</p><h2 id="smaller-changes" data-node-hydration="427"><a href="#smaller-changes">Smaller changes:</a></h2><p data-node-hydration="428">Not every change gets a particularly large section in the release notes, but we did land several new features and refactors.</p><ul data-node-hydration="429"><li>System tray support: we now have proper support for System Trays again, thanks to a wonderful community contribution.</li><li>Custom event loops: you can provide your own event loop, making it possible to use Dioxus in contexts where you already have other windows.</li><li><code>dioxus-document</code>: we split out our <code>document</code> abstraction so any renderer can implement the <code>Document</code> trait to integrate with <code>Title {}</code>, <code>Script {}</code> , and <code>eval</code></li><li><code>dioxus-history</code>: we also split out our <code>history</code> abstraction so other renderers can benefit from <code>Link</code> and <code>Router</code> without needing a dedicated feature flag on <code>dioxus-router</code></li><li><code>eval</code> API was simplified to allow <code>.recv::&lt;T&gt;().await</code> on evals, making interoperating with JavaScript easier.</li><li><code>dx fmt</code> now supports <code>#[rustfmt::skip]</code> attributes, respects <code>rustfmt.toml</code> settings, and is generally more reliable</li></ul><h2 id="upgrading-from-05-to-06" data-node-hydration="430"><a href="#upgrading-from-05-to-06">Upgrading from 0.5 to 0.6</a></h2><p data-node-hydration="431">Generally there are few huge breaking changes in this release. However, we did change a few APIs that might break your existing apps but are easy to fix.</p><ul data-node-hydration="432"><li><code>asset!()</code> syntax changes</li><li><code>eval()</code> API small changes</li><li>migrating to <code>prevent_default()</code></li><li>migrating from VNode::None to <code>rsx! {}</code> for empty nodes</li></ul><p data-node-hydration="433">We’ve assembled a <a href="https://dioxuslabs.com/learn/0.6/migration/">migration guide</a> to help.</p><h2 id="conclusion" data-node-hydration="434"><a href="#conclusion">Conclusion</a></h2><p data-node-hydration="435">That’s it for this release! We addressed countless issues including bundling bugs, spurious hot-reloads, and compatibility with unusual platforms and editors.</p><p data-node-hydration="436">Dioxus 0.6 has been in alpha for quite a while, and we’re very thankful for all the testing the community has done to make this the most polished release yet. It’s quite difficult to run a large open source project such a wide scope. This release took <em>much</em> longer to get out than we wanted - consuming two release cycles instead of just one.</p><p data-node-hydration="437">We focused hard this release to polish up as many rough edges as possible. Our continuous integration and deployment is in a much nicer place. We’re finally able to release nightly versions of documentation and the alpha release system has worked well for users eager to test out new features and bug fixes.</p><p data-node-hydration="438">Unfortunately, this release contained many connected pieces which made it hard to release incrementally. Systems like assets integrate tightly with CLI tooling and cross-platform support: to get one configuration right you need to test them all. With 0.6 behind us, the future seems much more “incremental” which should let us release major versions with faster cadence.</p><p data-node-hydration="439">We plan to keep 0.6 around for a while. Instead of shipping new features for a while, we're excited to make tutorial videos, write documentation, fix bugs, improve performance, and work with the community. The Dioxus team wants to spend time building our own apps!</p><p data-node-hydration="440">That being said, we do have a few major items planned for Dioxus 0.7 and beyond:</p><ul data-node-hydration="441"><li>Rust hot-reloading with binary patching</li><li>Integrating wasm bundle splitting with the router</li><li><code>dx deploy</code> to a hosted deploy platform (Fly.io, AWS, Cloudflare, etc.)</li></ul><p data-node-hydration="442">We’re also hiring - if you want to come build Dioxus with me in San Francisco (or remote) please reach out!</p><p data-node-hydration="444">We want to extend a huge thank-you to everyone who helped test and improve this release. We saw an incredible number of contributors fix bugs and add features. Special thanks to:</p><p data-node-hydration="445"><a href="https://github.com/ASR-ASU">@ASR-ASU</a> - <a href="https://github.com/Aandreba">@Aandreba</a> - <a href="https://github.com/Andrew15-5">@Andrew15-5</a> - <a href="https://github.com/DogeDark">@DogeDark</a> - <a href="https://github.com/Klemen2">@Klemen2</a> - <a href="https://github.com/LeWimbes">@LeWimbes</a> - <a href="https://github.com/LeoDog896">@LeoDog896</a> - <a href="https://github.com/MrGVSV">@MrGVSV</a> - <a href="https://github.com/Rahul721999">@Rahul721999</a> - <a href="https://github.com/Septimus">@Septimus</a> - <a href="https://github.com/Tahinli">@Tahinli</a> - <a href="https://github.com/WilliamRagstad">@WilliamRagstad</a> - <a href="https://github.com/ahqsoftwares">@ahqsoftwares</a> - <a href="https://github.com/airblast-dev">@airblast-dev</a> - <a href="https://github.com/alilosoft">@alilosoft</a> - <a href="https://github.com/azamara">@azamara</a> - <a href="https://github.com/chungwong">@chungwong</a> - <a href="https://github.com/d3rpp">@d3rpp</a> - <a href="https://github.com/daixiwen">@daixiwen</a> - <a href="https://github.com/dependabot">@dependabot</a> - <a href="https://github.com/ealmloff">@ealmloff</a> - <a href="https://github.com/hackartists">@hackartists</a> - <a href="https://github.com/hardBSDk">@hardBSDk</a> - <a href="https://github.com/houseme">@houseme</a> - <a href="https://github.com/i123iu">@i123iu</a> - <a href="https://github.com/ilaborie">@ilaborie</a> - <a href="https://github.com/imgurbot12">@imgurbot12</a> - <a href="https://github.com/jacklund">@jacklund</a> - <a href="https://github.com/jingchanglu">@jingchanglu</a> - <a href="https://github.com/luveti">@luveti</a> - <a href="https://github.com/marc2332">@marc2332</a> - <a href="https://github.com/matthunz">@matthunz</a> - <a href="https://github.com/nayo0513">@nayo0513</a> - <a href="https://github.com/opensource-inemar-net">@opensource-inemar-net</a> - <a href="https://github.com/oskardotglobal">@oskardotglobal</a> - <a href="https://github.com/panglars">@panglars</a> - <a href="https://github.com/pyrrho">@pyrrho</a> - <a href="https://github.com/ribelo">@ribelo</a> - <a href="https://github.com/rogusdev">@rogusdev</a> - <a href="https://github.com/ryo33">@ryo33</a> - <a href="https://github.com/samtay">@samtay</a> - <a href="https://github.com/sknauff">@sknauff</a> - <a href="https://github.com/srid">@srid</a> - <a href="https://github.com/tigerros">@tigerros</a> - <a href="https://github.com/tpoliaw">@tpoliaw</a> - <a href="https://github.com/uzytkownik">@uzytkownik</a></p></div></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PeerTube mobile app: discover videos while caring for your attention (299 pts)]]></title>
            <link>https://joinpeertube.org/news/peertube-app</link>
            <guid>42388488</guid>
            <pubDate>Wed, 11 Dec 2024 15:09:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joinpeertube.org/news/peertube-app">https://joinpeertube.org/news/peertube-app</a>, See on <a href="https://news.ycombinator.com/item?id=42388488">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Today, at Framasoft (bonjour!), we publish the very first version of the PeerTube Mobile app for android and iOS. A lot of care went into its conception, to help a wider audience watch videos and discover platforms, while not getting their attention (and data) exploited.</p>
<h4>Another step into PeerTube growth</h4>
<p>Even though we have been developing and maintaining the PeerTube software for 7 years, we, <a href="https://framasoft.org/" target="_blank" rel="noopener noreferrer">at Framasoft, are far from being an IT company</a>. First because <strong>we are a not-for-profit</strong> (funded through donations, you can support us <a href="https://support.joinpeertube.org/" target="_blank" rel="noopener noreferrer">here</a>), and then because <strong>our goal is, actually, to help others educate themselves on digital issues, surveillance capitalism</strong>, etc. and to give them tools that helps them get digitally emancipated.</p>
<p><strong>Developing PeerTube has been, to us, an (happy) accident</strong>. We wanted to show that with one paid developer (for the first six years, then two), very little means (~ €650,000 over 7 years) and lots of community contributions, we can create a radical alternative to YouTube and Twitch. It also took a lot of patience. From the get go, <strong>we knew we needed to aim for a slow but steady pace of growth</strong> for the software, the network of video platforms it federates, the whole ecosystem and the audiences it reached.</p>

<p>Videos and live-streams are increasingly watched on mobile devices. We knew <strong>the next step to widen the audience of the PeerTube network of platforms was to develop a mobile client</strong>. Last year, we decided to hire <a href="https://framablog.org/2023/11/28/peertube-v6-is-out-and-powered-by-your-ideas/" target="_blank" rel="noopener noreferrer">Wicklow (who completed his last internship, before graduating, here with us)</a>, to train him on mobile technologies, develop a mobile app, while continuing to get familiar with PeerTube's core code.</p>
<h4>Getting funded and getting help</h4>
<p>This was (and still is) a big decision: a new hire needs to be funded (our huge thanks to <a href="https://nlnet.nl/" target="_blank" rel="noopener noreferrer">NLnet</a> and the <a href="https://nlnet.nl/entrust/" target="_blank" rel="noopener noreferrer">NGI0 Entrust program</a>!), and we want to stay a small structure, so we don't have lots of room in our team. In hindsight, though, we believe it was the right one.</p>
<p>We surrounded ourselves with <a href="https://www.zenika.com/" target="_blank" rel="noopener noreferrer">Zenika</a>, to get help on architecture and experience on mobile strategy. We soon realized that peer-to-peer video sharing wouldn't be a wise strategy on mobile devices. After benchmarking different technologies, Wicklow picked Flutter for the development.</p>
<p><a href="https://www.lacooperativedesinternets.fr/" target="_blank" rel="noopener noreferrer">La Coopérative des Internets (French design workers-owed-company)</a>, helped us pinpoint the relevant user experience and design an app fit for videos on the fediverse. <strong>We decided, for the first release, to limit the scope of the app to the "spectator use-case"</strong>: browsing and watching videos.</p>
<p>We plan to share all their reports soon (early 2025), as soon as we put in the final touches. We hope that sharing this expertise and experience will help other FLOSS initiatives in their endeavor.</p>
<p>In the meanwhile, the PeerTube Mobile app is (as always with us) Free-libre and open-source, and you can <a href="https://framagit.org/framasoft/peertube/mobile-application" target="_blank" rel="noopener noreferrer">find the source code here on our repository</a>.</p>
<p><img src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-welcome.jpg" alt="image welcome page">
  <img src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-video-player.jpg" alt="image player">
</p>
<h4>Fediverse complexities made simple</h4>
<p>This preparatory work helped us realize that a mobile client was <strong>an amazing opportunity to simplify the PeerTube experience</strong>. PeerTube is not a video platform: it's a network of video platforms, each with their own rules, means and focus, that can choose to federate with others (or not).</p>
<p>It is, by design, more complex than a centralized platform. One of the main feedback we got from video enthusiasts was</p>
<blockquote>
<p>"I don't know where to get an account. I don't know where to search &amp; find videos" (even though we maintain <a href="https://sepiasearch.org/" target="_blank" rel="noopener noreferrer">SepiaSearch</a>).</p>
</blockquote>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/2024-12-sepia-search-screenshot-EN.jpg" title="" alt="">  </figure>

<h5>Local account</h5>
<p>Within a mobile client, we can create some kind of local account, directly on your device, so you get your watch-list, playlists, faves, etc. <strong>It saves you the hassle of finding a platform where you'd need to create an account</strong> if you just want to enjoy video content.</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-watch-later.jpg" title="" alt="">  </figure>

<h5>Explore platforms</h5>
<p>We can also include a search engine and an interface to explore the federation of PeerTube platforms and find videos suited to your interest. Not everyone knows <a href="https://sepiasearch.org/" target="_blank" rel="noopener noreferrer">SepiaSearch</a> (and other fediverse search engines) exists: <strong>you get it from the get go, in your pocket</strong>.</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-explore.jpg" title="" alt="">  </figure>

<h5>Highlighting platforms' diversity</h5>
<p>Finally, we can present content in a way that highlights the platforms, and show you where the videos/channels you watch are hosted. Differentiating platforms is <strong>a practical, visual way of introducing the concept of federation</strong> to a wider audience.</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-explore-2.jpg" title="" alt="">  </figure>

<h4>Designing out dark patterns</h4>
<p>Humility check: a small French nonprofit will never have Google's workforce nor Amazon's money (and vice versa). But <strong>we have an edge: we are not constrained by surveillance capitalism rules</strong>, and its captology models.</p>
<blockquote>
<p>Neither PeerTube nor the mobile app have any interest into grabbing your attention, forcefeeding you ads and milking behavioural and personal data from you.</p>
</blockquote>
<p>That is how <strong>we freed the design from toxic design patterns such as doom scrolling, curated feeds, needy notifications and so on</strong>.</p>
<p>It might sound obvious, but it takes real effort to concieve an interface cleaned from what has unfortunately became the new normal. Even more if you need to keep it familiar enough so it says easy to use.</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-show-more.jpg" title="" alt="">  </figure>

<h4>A very first build, limited by (play &amp; i) stores</h4>
<p>We knew beforehand that <strong>fitting into Google's PlayStore and Apple AppStore would be a challenge</strong>. They clearly weren't ready to host a client for (not-a-platform but) a network of autonomous video-sharing platforms, published by a small French nonprofit, funded through its independent donation website.</p>
<p>We knew about the <a href="https://github.com/sschueller/peertube-android/issues/302" target="_blank" rel="noopener noreferrer">issues encountered by Thorium</a> (another PeerTube mobile client). We got help and advices from Gabe, who develops <a href="https://owncast.online/" target="_blank" rel="noopener noreferrer">the streaming tool Owncast</a> (may your keyboard always repel crumbs and click smoothly), and <a href="https://laurenshof.online/owncast-and-the-app-store/" target="_blank" rel="noopener noreferrer">encountered many obstacles</a>... We knew about all that but, oh my Tux, it was a wild ride.</p>
<p>After jumping though hoops, here we are, you can download the PeerTube mobile app here:</p>

<p>
  <a target="_blank" rel="noopener noreferrer" href="https://asso.framasoft.org/dolo/h/peertube-apk-latest">Download the lastest apk (Android/Advanced)</a>
</p>
<h4>(un-)Limiting the federation</h4>
<p>To get through Apple's (and, in a lesser way, Google's) validation processes, we had to present the mobile app with a curated "allowlist" of PeerTube platforms that meet their standards.</p>
<p>Here is the state of those limitations right now:</p>
<ul>
<li><strong>Apple AppStore</strong>: limited to a very strict allowlist. Truth be told, a week before release, we are still unsure of being validated. Once we manage it, we'll see how to widen the list &amp; let users add platforms they want</li>
<li><strong>Google Play Store</strong>: limited allowlist, but users can already add the platforms they want. We plan to widen the allowlist next</li>
<li><strong>F-Droid</strong> (coming soon) and direct download apk: all PeerTube platforms we have indexed on <a href="https://sepiasearch.org/" target="_blank" rel="noopener noreferrer">SepiaSearch</a> are available. If an instance isn't declared to our index or is moderated, you can add it manually.</li>
</ul>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/Peertube-app-plaforms.jpg" title="" alt="">  </figure>

<p>We cannot stress enough how <strong>their stores are not ready for independent solidarity-oriented networks</strong>. For exemple, a small "support us" donation link in our website footer or even on one of the allowed platforms triggered a "nope" from Apple.</p>
<p>And that's consistent: as seen in <a href="https://en.wikipedia.org/wiki/Epic_Games_v._Apple" target="_blank" rel="noopener noreferrer">their fight with Epic</a> (owners of Fortnite) Apple take their share in every in-app purchases. They have an economic interest to keep your expenses enclosed in their ecosystem. Please, please: consider getting your freedom back ;).</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/expected-nothing.jpg" title="" alt="">  </figure>

<h4>Coming soon, in the PeerTube App</h4>
<p>Fitting into Apple's (and Google's) very small boxes took time and energy, more than what we expected. We decided to release a first (incomplete) version of the app in December anyway, and gradually improve on it.</p>
<p>Here are the <strong>features we plan to develop and share for the PeerTube app</strong>:</p>
<ul>
<li>Soon (early 2024)
<ul>
<li>Finalize and publish design and mobile strategy reports</li>
<li>Publish documentation</li>
<li>Play video in background</li>
<li>Log in to one's account, gets subscriptions, comment videos</li>
<li>next video recommandation</li>
<li>improve on the limited platforms list situation</li>
</ul>
</li>
<li>Then (mid 2024 (if funded))
<ul>
<li>adapt to tablets</li>
<li>adapt to TVs (AndroidTV... AppleTV will depend on their limitations)</li>
<li>Watch offline (for downloadable content)</li>
</ul>
</li>
</ul>
<p>Right now, we are still waiting to secure funding for those mid-2024 features (for which we have requested a NLnet grant).</p>
<p>Depending on the app success and usage, <strong>we would love to add the content creator usecase to the app</strong>. But that's a big one: upload and publish a video, manage one's content, create a livestream, etc. We are still wondering <strong>where, when and how to get funds for this undertaking</strong>.</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/PeerTube-app-CC-BY-David-Revoy.jpeg" title="" alt="">  </figure>

<h4>Care, Share and Contribute!</h4>
<p><strong>This is the part where we need you</strong>.</p>
<p>We hope you will <strong>enjoy this app, download and use it, and share it</strong> with your friends. This is a new gateway to promote PeerTube content, get audience to fabulous content creators, entice them to share more and boost that virtious loop.</p>
<p>This app is also <strong>a way of showcasing how media could be presented</strong>, when they are made with care for your agency and attention. More than ever: <strong>sharing is caring</strong>.</p>
<p>You can also <strong>contribute by reporting bugs</strong> (within the app), helping on the code (<a href="https://framagit.org/framasoft/peertube/mobile-application" target="_blank" rel="noopener noreferrer">here is the git repository</a>), and translating the interface. This is an important one: right now, the App is only available in English and French. <strong><a href="https://weblate.framasoft.org/projects/peertube-app/peertube-app/" target="_blank" rel="noopener noreferrer">Your language contributions are welcomed</a> here on our translation platform</strong>.</p>
<p>Obviously, we plan to maintain the app, add translations, implement bugfixes and security updates when needed: but this has a cost. <strong>We need to secure Framasoft's 2025 budget</strong> to make Wicklow's position permanent in our team (which is a priority to us). <strong>Our donation campaign is active right now</strong>, you can add your support <a href="https://support.joinpeertube.org/" target="_blank" rel="noopener noreferrer">here</a> (and thanks!).</p>
<figure>
  <img loading="lazy" src="https://joinpeertube.org/img/news/peertube-app/en/20-ans-CC-BY-David-Revoy.jpeg" title="" alt="">  </figure>

<hr>
<p>You can help us continue to improve PeerTube by sharing this information, <a href="https://ideas.joinpeertube.org/" target="_blank" rel="noopener noreferrer">suggesting improvements</a> and, if you can afford it, making <a href="https://support.joinpeertube.org/" target="_blank" rel="noopener noreferrer">a donation to Framasoft</a>, the association that develops PeerTube.</p>
<p>Thanks in advance for your support!<br>
Framasoft</p>

<p>
  <a target="_blank" rel="noopener noreferrer" href="https://asso.framasoft.org/dolo/h/peertube-apk-latest">Download the lastest apk (Android/Advanced)</a>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Don't Get Distracted (190 pts)]]></title>
            <link>https://calebhearth.com/dont-get-distracted</link>
            <guid>42388354</guid>
            <pubDate>Wed, 11 Dec 2024 14:57:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://calebhearth.com/dont-get-distracted">https://calebhearth.com/dont-get-distracted</a>, See on <a href="https://news.ycombinator.com/item?id=42388354">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    
      
<section>
  <iframe src="https://www.youtube.com/embed/UBdBoWAtLNI" frameborder="0" allowfullscreen=""></iframe>
</section>

<p><em>Be sure to check out these <a href="https://calebhearth.com/images/dont-get-distracted-sketchnotes--stephanie-nemeth.jpg">Sketchnotes</a> from a 15 minute version of this talk from <a href="http://codelandconf.com/">Codeland</a> by <a href="https://twitter.com/stephaniecodes">Stephanie Nemeth</a></em></p>

<p>I’m going to tell you about how I took a job building software to kill people.</p>

<p>But don’t get distracted by that; I didn’t know at the time.</p>

<p>Even before I’d walked across the stage for graduation, I accepted an offer for an internship. It paid half again as much as the most I’d ever gotten in the highest paying job up to that point. Not to mention that I’d spent years in college with low paid student jobs or living only on student loans.</p>

<p>I’d be joining a contracting company for the Department of Defense. The Department of Defense, or DOD, is the part of the government made up by the military in the United States. The DOD outsources all sorts of things, from <a href="https://www.defense.gov/News/Contracts/Contract-View/Article/1177766/">P-8A Multi-mission Maritime aircraft to blue, shade 451, Poly/wool cloth</a>.</p>

<p>At the time, I thought nothing of the fact that I’d be supporting the military. Besides, they’re the good folks right? My dad was in the military. So was my grandfather. It was good money, a great opportunity, and a close friend of mine had gotten me the gig. Life was good.</p>

<p>I showed up for my first day of work in North Virginia, or NoVA as the industry likes to call it. I met the team of other interns, then learned what the team would be building: a tool to use phones to find WiFi signals.</p>

<p>It seemed pretty cool compared to what I’d built to up that point. The most complicated thing was an inventory management system. It didn’t concern itself overmuch with persistence. Who needs to stop and restart a program anyhow? The data was right there in memory and if you forgot how many Aerosmith CDs you had, who cares? It got me an A on the assignment, and that was all that mattered.</p>

<p>Honestly, the idea of finding WiFi routers based on the signal strength seemed pretty intimidating at the time. The idea impressed me.</p>

<p>But don’t get distracted by all this; the software was intended to kill people.</p>

<p>I joined the team after they’d already gotten started on the project. The gist of the tool was that it would look at how WiFi signal strength changed as your phone moved around. If the signal strength got stronger, you were getting closer. If it got weaker, you were moving away. To find this information, we’d collect two pieces of information for each WiFi access point in range. The phone’s geolocation information and the WiFi signal strength.</p>

<p>To predict the actual location of the WiFi signal, we used a convolution of two algorithms. Both of them relied on the Free Space Path Loss equation. For our purposes, FSPL calculates how far away a phone is from a WiFi signal based on loss in signal strength. It assumes that there is only empty air, or “free space”, between the access point and the phone.</p>

<p>The first algorithm was R^2. It measured the difference between the signal strength we’d observed and the expected signal strength at each distance in a search grid based on Free Space Path Loss. Locations with the lowest R^2 error rate were the most likely location.</p>

<p>We’d combine that calculation with a Gaussian estimate. Now I spent two or three days last week trying to understand Gaussian estimates for this talk and couldn’t. The best documentation on them are still research papers. I do know that it creates a probability curve. The high points of the curve are distances where the access point is likely to be, and low points are unlikely. The curve started with a probability hole of low values. They represented low likelihood that the phone is standing right next to the signal. The curve then increased to high probabilities further out. It decreased to near zero even further. The algorithm adjusted the width and height of these two curves by consulting past measurements. It created a heat map of probabilities for the signal source.</p>

<p>We’d normalize the probabilities for each location in the search grid. A combination of probabilities was more correct than either algorithm itself.</p>

<p>We stored this probability matrix for each location a phone collected from. Using these, if we collected readings while moving in a straight line, we could tell you how far away the WiFi was. If you turned a corner, we could also add direction, so you could find it in 2D space. If you climbed some stairs, we’d show you altitude as well. The technology was the most interesting project I’d ever worked on.</p>

<p>But don’t let that distract you; it was designed to kill people.</p>

<p>I mentioned that I had a software engineering degree at this point. My teammates were earlier in their education careers. Most of them were a year or two into their four year programs, also a mix of Computer Science and Math majors. My expertise was in the design and process of building software, while theirs was more in high level mathematics or the theory of computer use.</p>

<p>I helped to translate the working algorithms they’d designed in MatLab to the Java code we needed to run on the phones.</p>

<p>And let’s be honest, we spent plenty of time deciding whether we preferred Eclipse or NetBeans. Can I say how happy I am that as a Ruby developer, I’ve not had to figure out where to put a .jar file in over half a decade?</p>

<p>One such example is calculating distance between two points. As these things go, it was in the deepest level of each loop. We were using great circle distance, which is the way you measure the shortest distance between two points on a sphere, such as Earth. Did we need to use this complicated measurement over the meager distances a WiFi network can operate over? Absolutely not. Did I mention this was over half a decade ago? We weren’t always making the best choices. Anyhow, we needed to calculate these distances. The function doing it was being hit hundreds of thousands of time for each collection point, often with the same two locations. It was a very slow process.</p>

<p>We solved that by implementing a dictionary of latitude/longitude pairs to distances. This at least meant we didn’t re-do those calculations. This and other optimizations we made sped up the performance from seven minutes to a few seconds.</p>

<p>But don’t get distracted; that performance increase made it faster to kill people.</p>

<p>The accuracy of the locations wasn’t fantastic. I don’t remember exactly what it was before we focused on improving this, but an average error about 45 feet sticks in my head.</p>

<p>That’s a little longer than a Tyrannosaurus Rex nose to tail, or more concretely the length of a shipping container.</p>

<p>That’s significant when the WiFi range for 802.11n is only about 100 feet. That means we could be up to almost half the range of the router off from where it was.</p>

<p>I talked about the Gaussian estimation, the two curves from the second algorithm. We hard-coded numbers that defined this curve. They were only starting points, but they were starting points every time we made the calculation.</p>

<p>A Genetic Algorithm is a type of program that produces a set of values that optimize for a desired result. It’s a perfect fit for tuning these hard coded values to get more accurate results.</p>

<p>Each of the Gaussian estimation values will is a gene. The set of values is a genome in Genetic Algorithm parlance. The genes were our 3 constants.</p>

<p>A fitness function is what Genetic Algorithms use to measure performance of a genome. For the dataset of readings I was using in the GA, I knew the actual location of the access points. That meant I could run the geolocation algorithm with each genome’s values in place as the fitness function. The result would be the distance between the actual location and the one calculated by the GA-derived values.</p>

<p>Genetic Algorithms take a set of genomes, called a generation, and keep a certain percentage of top performers.</p>

<p>These top performers “survive” to the next generation as copies. Sometimes the algorithm mutates these copies by adding a Gaussian random value to each gene. This means that there was a chance that any of the copied genomes would have each gene changed slightly. That way they would have a chance of performing better or worse. New random genomes are created for the remaining spots in the new generation.</p>

<p>We saved the top performers across all populations. When the GA ended I could take a look at the values and select the best performer.</p>

<p>I let this genetic algorithm run over the weekend. It was able to increase the accuracy from 40-odd feet to about 10.3 feet, 25% of the error from the original. That is less than the GPS accuracy on the smartphones collecting data (which is about 16 feet according to GPS.gov). This is too accurate, so it may have over-optimized against the test data and might not be as accurate against other data sets. This is called overfitting and the way around it is to have separate sets of training and test data. Did I mention this was five years ago? I didn’t even know what overfitting was back then.</p>

<p>I loved this. Genetic algorithms, R^2, and Gaussian estimation are the kind of thing that they tell you you’ll never need to use again once you graduate. But we were using them for a real world project! It was great.</p>

<p>But don’t let that distract you; this accuracy made it easier for the software to help kill people.</p>

<p>The tracking now worked accurately and quickly. The next feature was to add tracking a moving WiFi access point. I briefly wondered why an access point would be moving, but that question wasn’t as interesting as figuring out how to track it.</p>

<p>We made use of Kalman Filters to observe state variables: the position, velocity, and acceleration of the WiFi signal. Given these and the time since the last measurement, a Kalman Filter is able to improve the current prediction with surprising accuracy, filtering out “noise” data automatically.</p>

<p>Each time we ran the real time algorithm, we’d also feed this to the Kalman Filter.  With only that information, it was able to produce an estimate that is a weighted average of previous data. A new predicted location that was more accurate than the calculated value.</p>

<p>At the same time, we added the ability to track more than one WiFi signal. We’d filter our collection of readings by the unique identifier of each access point. The filtered datasets each went through the full algorithm to produce predictions.</p>

<p>We used the APIs on the phone to read the signal strength of all WiFi access points in range. We were able to track multiple hotspots, basically whatever we could see in the WiFi network list. It was all very exciting. These seem like academic problems, but we were getting to use them in a real project! Being a programmer was going to be great.</p>

<p>But don’t let that distract you; this meant we could kill multiple more accurately people.</p>

<p>We’d been working with the project owner throughout this process.</p>

<p>He was laissez-faire about most things. He might check in once a day then going back to his main job in the area of the building dedicated to classified work.</p>

<p>Whenever we hit one of these milestones, we’d tell him. He’d be happy about it, but a question always came up. He wanted it to sniff for the signals put out by phones in addition to WiFi hotspots. This is a much harder problem from a technical perspective. The functionality necessary to do this is “promiscuous mode”, a setting on the wireless network controller. Neither iPhone nor Android supported that option. We’d need to jailbreak or root the phone regardless of the platform. We looked for packages that we could use that would let us do this to “sniff” the packets that devices sent back to routers. The closest we ever found was a SourceForge project that seemed promising. We didn’t fully understand its use and it wasn’t well documented.</p>

<p>We told the project owner that we’d get to it later. None of us thought it was that important: we had the technology to find WiFi Access Points working. That was the goal right? Each time we’d demonstrate the new exciting tech we’d built though, the same question came up.</p>

<ul>
  <li>We got WiFi hotspots located! Great, does it find phones?</li>
  <li>It’s taking seconds instead of minutes! Great, does it find phones?</li>
  <li>We looked into it finding phones, it seems unlikely but maybe! Ok, we’ll come back to it.</li>
  <li>We got moving targets working! Great, does it find phones?</li>
</ul>

<p>I had been distracted.</p>

<p>All of the cool problems we were solving: finding nodes, speeding things up, making more accurate predictions. It was all so cool, so much fun. I hadn’t thought about why we were putting all this work into finding a better place to sit and get good WiFi. That doesn’t even make sense if you look at it for more than a few seconds.</p>

<p><em>Does it find phones.</em></p>

<p>This was never about finding better WiFi. We were always finding phones. Phones carried by people. Remember I said I was working for a Department of Defense contractor? The DoD is the military. I was building a tool for the military to find people based on where their phones where, and shoot them.</p>

<p>I tried to rationalize this then. The military is in place to protect Truth, Justice, and the American Way. But this was the same time that we found out the government had been spying on Americans in the US with drones. They’d also lent out that technology to federal, state, and local law enforcement agencies nearly 700 times to run missions. The military and government do things that I know I don’t agree with pretty often. I didn’t want to be a part of building something used to kill people, especially since I knew I’d never know who it was killing, let alone have a say.</p>

<p>I rationalize it now too. We were interns, and we didn’t even have clearance. The projects this company did for the government were classified Top Secret. I wasn’t allowed to know what they were. My code probably got thrown away and forgotten. Probably.</p>

<p>This was an extreme example of code used in a way that the creator did not intend it. The project owner conveniently left out its purpose was when explaining the goals. I conveniently didn’t focus too much on that part. It was great pay for me at the time. It was a great project. Maybe I just didn’t want to know what it would be used for. I got distracted.</p>

<p>There are other examples of when code is used in ways it wasn’t intended, and of code that does bad things.</p>

<p>A year and a day ago, a developer named Bill Sourour <a href="https://medium.freecodecamp.org/the-code-im-still-ashamed-of-e4c021dff55e" title="The Code I'm Still Ashamed Of">wrote a blog post</a>. It opened with the line: “If you write code for a living, there’s a chance that at some point in your career, someone will ask you to code something a little deceitful – if not outright unethical.”</p>

<p>Bill had been asked to create a quiz that would almost always give a result that benefitted his client. Bill worked in Canada, and in Canada there are laws in place that limit how pharmaceutical companies can advertise prescription drugs to customers. Anyone could learn about the general symptoms a given drug addressed, but only patients with prescriptions could get specific information about the drug.</p>

<p>Because of this law, the quiz was posing as a general information site and not an advertisement for a specific drug. If the user didn’t answer that either they were allergic to the drug or already taking it, every quiz result suggested this specific drug. That’s what the requirements said to do, and that’s what Bill coded up.</p>

<p>The project manager did a quick test before submitting the website to the client. She told Bill that the quiz was broken: it always had the same answer. “Those were the requirements,” Bill responded. “Oh. Ok.”</p>

<p>A little while later, Bill got an email from a colleague that had a link to a news article. A young woman had taken the drug that Bill had built this quiz for. She had killed herself. It turns out that one of the main side effects of the drug were severe depression and suicidal thoughts.</p>

<p>Nothing Bill did was illegal. Like me, Bill was a young developer making great money. The purpose of the site was to push a particular drug - that’s why it was being built. He chalked it up to marketing. He never intended for this to happen. Maybe Bill got distracted too.</p>

<p>As his conclusion, Bill writes:</p>

<figure>
  <blockquote>
  <p>As developers, we are often one of the last lines of defense against potentially dangerous and unethical practices.</p>
  <p>We’re approaching a time where software will drive the vehicle that transports your family to soccer practice. There are already AI programs that help doctors diagnose disease. It’s not hard to imagine them recommending prescription drugs soon, too.</p>
  <p>The more software continues to take over every aspect of our lives, the more important it will be for us to take a stand and ensure that our ethics are ever-present in our code.</p>
  <p>Since that day, I always try to think twice about the effects of my code before I write it. I hope that you will too.</p>
  </blockquote>
  </figure>

<p>Bill’s story isn’t that far off from mine, but there are still other examples.</p>

<p>Earlier this year, a story came out that Uber had built into its ridesharing app code they call “greyball”. It’s a feature of their VTOS (or violation of terms of service) tool that can populate the screen with fake cars when the app is opened by users in violation of the terms of service.</p>

<p>In a statement, Uber said, “This program denies ride requests to users who are violating our terms of service — whether that’s people aiming to physically harm drivers, competitors looking to disrupt our operations, or opponents who collude with officials on secret ‘stings’ meant to entrap drivers.”</p>

<p>In practice, as <a href="https://www.nytimes.com/2017/03/03/technology/uber-greyball-program-evade-authorities.html" title="How Uber Deceives the Authorities Worldwide">The New York Times reports</a>, it was used in Portland to avoid code enforcement officers working to build a case against Uber for operating without a license. When triggered by Uber’s logic, it populates the app with cars that don’t exist, with fake drivers who quickly cancel after accepting a ride.</p>

<p>I am not a lawyer, but it seems like this is likely an obstruction of justice, itself a crime outside of Uber’s unlawful operations in Portland. Greyball is used even today, though mostly outside the United States. I’m a huge fan of ridesharing - though I use a competitor in Austin and Boston called Fasten<sup id="fnref:fasten" role="doc-noteref"><a href="#fn:fasten" rel="footnote">1</a></sup> rather than the much larger Uber or Lyft. But it’s not uncommon to see in the news these days articles about heinous things these drivers are doing. Greyball may have enabled some of those.</p>

<p>Again, it’s an unintended consequence of a tool built. Maybe the greyball internal pitch was to “greyball” users who were in violation of the terms of service. People who were under 18, or who didn’t pay to clean up their late night explosive accidents one too many times for example. Rather than block them, probably causing them to create a new account, they could be put into an alternate dimension where for some reason they just couldn’t ever get a ride. That’s fine, right?</p>

<p>If these developers had thought about the worst possible case for how this could be used, maybe obstruction of an investigation into Uber’s shady dealings would have come up in that conversation and it could have been addressed early on. Maybe they were distracted by the face value of the request from looking deeper at the purpose and uses.</p>

<p>There’s all sorts of things as well that aren’t as black and white (if you’ll excuse the pun). <a href="http://www.deidrariggs.com/2016/11/30/is-instagram-listening-in-on-you/" title="Is Instagram Listening In On You?">Apps that always listen to the microphone to tailor ads to you based on what you say near your phone</a>, <a href="https://www.axios.com/sean-parker-unloads-on-facebook-2508036343.html" title="Sean Parker Unloads on Facebook">websites designed to exploit psychology to take up as much of your time and attention as possible</a>, and any number of apps that opt you into mailing lists when you sign up or purchase something. These aren’t nearly as obviously bad, but at least in my opinion they’re still kind of shady.</p>

<p>This value system is different for others. We don’t always agree as individuals what is right and wrong, or even with what should be legal or illegal.</p>

<p>There are actually words for things that society decides are good or bad versus what you or I individually believe: ethics and morals. While modern philosophy more or less uses these terms interchangeably, a common understanding at least between us will be important later.</p>

<p>Ethics are imposed by an outside group. A society, a profession, a community such as ours or even where you live. Religions provide ethical systems, as do groups of friends. Societies in whatever form define right and wrong, good and bad, and imposes those on its members. Ethics in societies such as local, state, and national groups are often, but not always, coded into laws.</p>

<p>Morals are a more personal version of the same thing. Society as a whole imposes its mores on smaller communities, and all of that trickles down to the individual level. That’s not to say that your morals can’t conflict with the ethics of society. For example, you might think that freedom of speech is a basic human right, but live somewhere that defacing religious or political objects is considered wrong.</p>

<p>Let’s not get distracted by morals and ethics yet, though. We’ll come back to them.</p>

<p>The unifying factor in all of the stories I’ve told is that a developer wrote the code that did these unethical or immoral things. As a profession, we have a superpower: we can make computers do things. We build tools, and ultimately some responsibility lies with us to think through how those tools will be used. Not just what their intention is, but also what misuses might come out of them. None of us wants to build things that will be used for evil.</p>

<p>The Association for Computing Machinery is a society dedicated to advancing computing as a science &amp; profession. ACM includes this in their <a href="https://www.acm.org/about-acm/acm-code-of-ethics-and-professional-conduct#imp1.2">Code of Ethics and Professional Conduct</a>:</p>

<figure>
  <blockquote><p>
  Well-intended actions, including those that accomplish assigned duties, may lead to harm unexpectedly. In such an event the responsible person or persons are obligated to undo or mitigate the negative consequences as much as possible. One way to avoid unintentional harm is to carefully consider potential impacts on all those affected by decisions made during design and implementation.</p></blockquote></figure>

<p>So how can we “carefully consider potential impacts”? Honestly, I don’t have any answers to this. I don’t think that there really is a universal answer yet, because if we had it I have to believe we’d not be building these dangerous pieces of software.</p>

<p>I do have a couple of ideas though. One I got from my friend Schneems is to add to the planning process a step where we come up with the worst possible uses of our software. In opting in folks to an email list by default, the worst case might be that we send them a bunch of unwanted email and they unsubscribe. Maybe they even stop being a customer. As Schneems said: “Am I willing to sell my hypothetical startup’s soul for a bigger mailing list, when that might be all that keeps the company afloat? Yeah, no problem.” That makes sense to me. I don’t think it’s the best practice, but in the end it’s not physically hurting anyone. If I had sat down and thought about what the WiFi location app could be used for in the worst case, I would have come to a very different conclusion.</p>

<p>Actually, thinking about the worst possible uses of code could probably be a fun exercise. You might come up with some pretty wacky examples like “If we send Batman an email and he happens to have notifications on his iPhone for new emails, he might be looking at the notification when the Riddler drives by in the Riddler Car and he might not catch him before he gets off his witty one liner at the crime scene. Riddle me this, riddle me that, who’s afraid of the big, black bat?.” This isn’t so plausible, but it shows that these exercises can go down all sorts of different paths that aren’t obvious at a glance.</p>

<p>Another, the thing that I think I should have done, and that we can all do more of, is to simply not take requests at face value. The project owner at the Defense contractor I worked at didn’t spell out what the reason for the code was. But at least in retrospect, it wasn’t a big leap of logic. “We’re going to build an app to find WiFi signals” is all true, but it’s not the whole truth. Asking them, or myself, “why” enough times probably would have led me to a much earlier understanding. Why? To find the sources. Why? To go to them. Why? Why? Why?</p>

<p>Comedian Kumail Nanjiani, best known for the TV show Silicon Valley and his recent film The Big Sick, took to Twitter recently on this subject.</p>

<figure>
  <blockquote>
  <p>I know there's a lot of scary stuff in the world right now, but this is something I've been thinking about that I can't get out of my head.</p>
  <p>As a cast member on a show about tech, our job entails visiting tech companies, conferences, etc. We meet people eager to show off new tech.</p>
  <p>Often we'll see tech that is scary. I don't mean weapons. I mean altering video, tech that violates privacy, stuff with obvious ethical issues.</p>
  <p>And we'll bring up our concerns to them. We are realizing that ZERO consideration seems to be given to the ethical implications of tech.</p>
  <p>They don't even have a pat rehearsed answer. They are shocked at being asked. </p><p>Which means nobody is asking those questions.</p>
  <p>"We're not making it for that reason but the way people choose to use it isn't our fault. Safeguards will develop." But tech is moving so fast.</p>
  <p>That there is no way humanity or laws can keep up. We don't even know how to deal with open death threats online.</p>
  <p>Only "Can we do this?" Never "should we do this?" We've seen that  same blasé attitude in how Twitter or Facebook deal with abuse and fake news.</p>

  <p>Tech has the capacity to destroy us. We see the negative effect of  social media. No ethical considerations are going into dev of  tech.</p>

  <p>You can't put this stuff back in the box. Once it's out there, it's out there. And there are no guardians. It's terrifying. The end.</p>
  </blockquote>
  <figcaption>
  Kumail Nanjiani <a href="https://twitter.com/i/moments/930118384225800192?ref_src=twsrc%5Etfw">November 1, 2017</a>
  </figcaption>
  </figure>

<p>It’s a major problem when we’re given so much power in tech, but we’re not doing anything to ensure that we use it safely. Thinking about what we’re doing and being careful not to build things that can be used maliciously is really important.</p>

<p>Make your own decisions. Make your own choices. Make your own judgement.</p>

<figure>
  <blockquote>
  <p>For engineers in particular, we develop systems. But the systems we develop can be used for different things. The software that I was using in Iraq is the same you’d use in marketing. It’s the same tools. It’s the same analysis.</p>
  <p>I guess technologists should realize that we have an ethical obligation to make decisions that go beyond just meeting deadlines for creating a product. Let’s actually take some chunks of time time and think ‘what are the consequences of this system? How can this be used? How can it be misused?’ Let’s try to figure out how we can mitigate a software system from being misused, or decide whether or not you want to implement it at all. There are systems that if misused can be very dangerous.</p>
  </blockquote>
  <figcaption>
  Chelsea Manning in
  <cite>
  <a href="https://www.wnyc.org/story/chelsea-manning-life-after-prison/" title="Chelsea Manning on Life After Prison fron The New Yorker Radio Hour">
  The New Yorker Radio Hour
</a> (31:55-33:40)
  </cite>
  </figcaption>
  </figure>

<p>Don’t get distracted by deadlines and feature requests. Think about the consequences of what you’re building. Build in safeguards to prevent misuse, or don’t build it at all because it’s too dangerous.</p>

<p>I’m asking you to do something about this. Well, I guess I’m asking you to not do something because of this. It’s only fair that we talk a bit about how and when to take a stand.</p>

<p>Let’s say I had a time machine and could go back in time to 2011 and do it all over again. I already have the foreknowledge that this tool is unethical. I’ve accepted this job. I’ve moved across the country from Tempe, Arizona to Brookville, Maryland. I’ve driven the two hour commute to Sterling, Virginia, home of <a href="https://www.governmentcontractswon.com/department/defense/sterling_va_virginia.asp">395 defense contractors awarded 16.8 trillion dollars in contracts over the past 16 years</a>. It’s my first job out of school, and it’s my first day. I don’t have my clearance so I’m an intern. My new project owner introduces me to the team then pulls me into a side room to give me an overview of the project. What do I say?</p>

<p>I think the first thing is to establish a mutual understanding of the task. It’s entirely possible at this point that I don’t understand what the actual thing is, and that I’m overreacting. I ask “Why are we finding these signals” and the project owner says “We want to find people’s cell phones.” “Who’s finding them, and why?” I ask. “I don’t know, probably some soldiers in the Middle East.” “Why?” I repeat. “I can’t tell you that.”</p>

<p>“I can’t tell you that” is something I got a lot from this project owner. It’s code for “I have clearance and I know things about this project. I know what you’re asking and I know the answer but I am not allowed to tell you.”</p>

<p>At this point, I think we have a mutual understanding. The task is to help soldiers find people’s phones, probably attached to those people. The reason is left unsaid but we both know.</p>

<p>This organization is a defense contractor. They build things for the military. It is their core competency. They’re not not going to do this…. On the other hand, I care a lot about not killing people. The company’s goal is to build things for the military. If my goal is not to let this happen, then there isn’t a good fit for me at this company. This probably means that the worst case here is that I’m going to leave today without a job. Either I’ll say no and they’ll fire me, or I’ll say “that’s not something I’m comfortable with, best of luck” and quit. These are the worst case scenarios, not necessarily what will happen.</p>

<p>Before saying no then, I need to consider: Can I afford to leave here without a job financially? Am I likely to be able to rely on my network to get me another job? Have I built up a trust with my employer where I can go to them with this type of thing and feel confident that I’ll be heard out? The answer to these questions was no for me in 2011. Sometimes, something is important enough that you should still do something, but there’s a lot that goes into these decisions. I’d like to think that I would still say no.</p>

<p>Let’s look at another situation, where someone did the ethical thing. A developer we’ll call Alice received a strange request. We want to identify weak passwords in the system to notify users to change them. We’d like you to run a password cracking program on the very, very, large password database.</p>

<p>This was a long time ago, before aged passwords were common. Expiring old passwords wasn’t a straightforward option. Alice thought this was a weird request, but said that if the appropriate paperwork was completed she would be willing.</p>

<p>Alice received the completed paperwork and ran the password crack. The next request was “We’d like the list of users along with their weak passwords”. Alice knew that her coworkers had a valid desire to help customers improve their passwords. She also knew that users often re-used passwords. Combining the email and password into one report could allow someone to log into the customers’ accounts on other websites.</p>

<p>Alice pointed this out to her manager, and together they worked with the CSA team to design an email that didn’t include the password. Customers received notifications about their weak passwords, and there was less risk of the report falling into malicious hands. No one was fired and Alice built up trust within her team.</p>

<p>Different scenarios need different ways of analyzing what you should do. In some cases,  the right thing to do say nothing and build the product. It isn’t a simple thing to make this decision.</p>

<p>But don’t get distracted by having to think through it. Sometimes your code can kill people.</p>

<h2 id="further-reading">Further Reading</h2>

<ul>
  <li><a href="https://www.theatlantic.com/video/index/541797/anil-dash-tech-ethics">Does Technology Need to Be Ethical?</a>
Anil Dash briefly talks to The Atlantic about tech ethics.</li>
  <li><a href="http://www.bbc.com/news/technology-35639549">Is your smartphone listening to you?</a>
The BBC addresses whether tech companies use your phone to listen to what you’re saying while not using their apps, and if they even can.</li>
  <li><a href="http://www.hcpro.com/HOM-236942-5728/know-your-ethical-obligations-regarding-coding-and-documentation">Know your ethical obligations regarding coding and documentation</a>
A blog post on how to define your ethical obligations as a programmer, some ways of dealing with them, and some real world examples</li>
  <li><a href="https://www.nytimes.com/2018/04/04/technology/google-letter-ceo-pentagon-project.html">‘The Business of War’: Google Employees Protest Work for the Pentagon</a>
Google employees ask Google’s CEO not to build software to help the military build warfare technology</li>
  <li><a href="https://www.theverge.com/2015/10/8/9481651/volkswagen-congressional-hearing-diesel-scandal-fault">Volkswagen America’s CEO blames software engineers for emissions cheating scandal</a>
VW’s CEO blames software engineers for changing how diesel emissions are reported during tests. Even if they were doing what they were told, someone up the chain can throw the blame back onto the programmers.</li>
</ul>

<h2 id="glossary-of-terms">Glossary of Terms</h2>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Kernel_density_estimation#A_rule-of-thumb_bandwidth_estimator">Gaussian Estimation</a>
Used in the WiFi geolocation algorithm to estimate free space path loss using probability density estimation. It said “it’s less likely to be nearby if you’re looking for it and more likely to be further out”.</li>
  <li><a href="https://towardsdatascience.com/introduction-to-genetic-algorithms-including-example-code-e396e98d8bf3">Genetic Algorithm</a>
A type of machine learning or searching that is inspired by the theory of evolution. It uses randomization to create populations of individuals and tests their fitness to determine which ones will reproduce. It was used to increase accuracy in the geolocation algorithm.</li>
  <li><a href="https://heroku.com/">Heroku</a>
A cloud platform as a service (PaaS) supporting several programming languages, that is used as a web application deployment model. It supports Java, Node.js, Scala, Clojure, Python, PHP, and Go.</li>
  <li><a href="https://schneems.com/2017/06/12/bayes-is-bae/">Kalman Filter</a>
A Kalman Filter can be used any time you have a model of motion and some noisy data that you want to produce a more accurate prediction.</li>
  <li><a href="https://stackoverflow.com/a/1988826/218211">Memoization</a>
Remembering the result of a calculation based on its arguments, and then using that result instead of re-calculating if the same method call is made with the same arguments. Caleb’s team did this with a hash/dictionary/map (different names for the same thing) that contained the location pairs as keys and the distance between them as values.</li>
  <li><a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">R2 Algorithm</a>
Used in the WiFi geolocation algorithm to measure the difference between expected and actual signal strength for each point in a search grid. The smallest difference is the most likely to be the correct distance from the source.</li>
</ul>


    
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Django and Postgres for the Busy Rails Developer (116 pts)]]></title>
            <link>https://andyatkinson.com/django-python-postgres-busy-rails-developer</link>
            <guid>42388340</guid>
            <pubDate>Wed, 11 Dec 2024 14:56:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://andyatkinson.com/django-python-postgres-busy-rails-developer">https://andyatkinson.com/django-python-postgres-busy-rails-developer</a>, See on <a href="https://news.ycombinator.com/item?id=42388340">Hacker News</a></p>
Couldn't get https://andyatkinson.com/django-python-postgres-busy-rails-developer: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Overweight overtakes tobacco smoking as the leading disease risk factor in 2024 (210 pts)]]></title>
            <link>https://www.scimex.org/newsfeed/being-overweight-overtakes-tobacco-smoking-as-the-leading-disease-risk-factor-in-2024</link>
            <guid>42388273</guid>
            <pubDate>Wed, 11 Dec 2024 14:51:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scimex.org/newsfeed/being-overweight-overtakes-tobacco-smoking-as-the-leading-disease-risk-factor-in-2024">https://www.scimex.org/newsfeed/being-overweight-overtakes-tobacco-smoking-as-the-leading-disease-risk-factor-in-2024</a>, See on <a href="https://news.ycombinator.com/item?id=42388273">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>Living with overweight or obesity has overtaken tobacco smoking as the leading risk factor contributing to disease burden in 2024, according to a new report from the Australian Institute of Health and Welfare. The Australian Burden of Disease Study 2024 estimates the millions of years of healthy life Australians lose because of injury, illness or premature death – measuring over 200 diseases and injuries. This report also provides estimates of how much of this disease burden can be attributed to 20 individual risk factors such as alcohol use, physical inactivity, poor diet, overweight or obesity and tobacco smoking. Overweight, including obesity, overtook tobacco use as the leading risk factor in 2024, driven by a substantial fall (41%) in the burden attributable to tobacco use since 2003.</p>
                </div><div>
                                            <h2>Media release</h2>
                    
<p><strong>From:</strong> Australian Institute of Health and Welfare (AIHW)

                                        
                    </p><p><strong>Living with overweight or obesity overtakes tobacco as the new leading risk factor contributing to burden of disease</strong></p><p>Living with overweight or obesity has overtaken tobacco smoking as the leading risk factor contributing to disease burden in 2024.</p><p>The Australian Burden of Disease Study 2024, released today by the Australian Institute of Health and Welfare, estimates the millions of years of healthy life Australians lose because of injury, illness or premature death – measuring over 200 diseases and injuries.</p><p>This report also provides estimates of how much of this disease burden can be attributed to 20 individual risk factors such as alcohol use, physical inactivity, poor diet, overweight or obesity and tobacco smoking.</p><p>‘Australians lost an estimated 5.8 million years of healthy life due to living with disease and dying prematurely in 2024,’ said AIHW spokesperson Ms Michelle Gourley.</p><p>‘Over one-third of the total burden of disease and injury in Australia in 2024 could have been avoided or reduced due to modifiable risk factors included in the study.</p><p>‘Overweight, including obesity, overtook tobacco use as the leading risk factor in 2024, driven by a substantial fall (41%) in the age-standardised rate of total burden attributable to tobacco use since 2003.</p><p>‘This fall is likely due to declines in smoking prevalence and burden rates from some of the major linked diseases, such as lung cancer and chronic obstructive pulmonary disease (COPD).’</p><p>An estimated 8.3% of total disease burden in 2024 was due to overweight (including obesity) and 7.6% was due to tobacco use (excluding vaping). This was followed by dietary risks (4.8%) and high blood pressure (4.4%).</p><p>Alcohol use and illicit drug use were the leading risk factors contributing to disease burden for young males aged 15–24, while child abuse and neglect was the leading risk factor contributing to burden for young females of the same age.</p><p>When looking at rates of total disease burden, there was a 10% decrease between 2003 and 2024 after adjusting for population ageing. This decrease was driven by a 26% decrease in the rate of fatal burden, as the non-fatal burden rate increased by 7%.</p><p>‘While Australians are living longer on average, years lived in ill health are also growing, resulting in little change in the proportion of life spent in full health. This contributes to the growing demand and pressures on the health system and services,’ said Ms Gourley.</p><p>In line with previous years, cancer was the leading group of diseases causing burden in 2024 (16.4%), with 91.3% of this burden fatal and 8.7% non-fatal.</p><p>The leading specific causes of disease burden were coronary heart disease (5.5%), dementia (4.5%), back pain and problems (4.3%), anxiety disorders (3.9%) and COPD (3.7%).</p><p>Males experienced more total disease burden than females across all age groups, driven by males having higher rates of fatal burden. The leading individual causes of burden also differed between males and females. Coronary heart disease was the leading cause of burden among males, and dementia was the leading cause of burden among females.</p><p>For young people, mental health conditions and suicide and self-inflicted injuries were the leading contributors of disease burden.</p><p>For males aged 15–24, suicide and self-inflicted injuries caused the most burden (12%), followed by anxiety disorders (10%) and depression (7%). For young females of the same age, the leading causes of burden were anxiety disorders (17%), depression (12%) and eating disorders (7%).</p><p>In the 5–14 age group, the leading causes of disease burden were autism spectrum disorders and asthma for males, and asthma and anxiety disorders for females.</p><p>‘Burden of disease is the gold standard approach for measuring the impact of illness, injury and death, and this information provides an important evidence base to inform health policy and service planning,’ said Ms Gourley.</p>

                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The PayPal Mafia is taking over America's government (140 pts)]]></title>
            <link>https://www.economist.com/business/2024/12/10/the-paypal-mafia-is-taking-over-americas-government</link>
            <guid>42387549</guid>
            <pubDate>Wed, 11 Dec 2024 13:38:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/business/2024/12/10/the-paypal-mafia-is-taking-over-americas-government">https://www.economist.com/business/2024/12/10/the-paypal-mafia-is-taking-over-americas-government</a>, See on <a href="https://news.ycombinator.com/item?id=42387549">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main role="main" id="content"><article data-test-id="Article" id="new-article-template"><div data-test-id="standard-article-template"><section><p><span><a href="https://www.economist.com/business" data-analytics="sidebar:section"><span>Business</span></a></span><span> | <!-- -->All-in on Donald Trump</span></p><h2>America’s right-wing tech bros are celebrating Donald Trump’s victory</h2></section><section><figure><img alt="Palace of Fine Arts at night with the Golden Gate Bridge in the background, San Francisco, California, USA. " fetchpriority="high" width="1280" height="720" decoding="async" data-nimg="1" sizes="(min-width: 960px) 700px, 95vw" srcset="https://www.economist.com/cdn-cgi/image/width=360,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 360w, https://www.economist.com/cdn-cgi/image/width=384,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 384w, https://www.economist.com/cdn-cgi/image/width=480,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 480w, https://www.economist.com/cdn-cgi/image/width=600,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 600w, https://www.economist.com/cdn-cgi/image/width=834,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 834w, https://www.economist.com/cdn-cgi/image/width=960,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 960w, https://www.economist.com/cdn-cgi/image/width=1096,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 1096w, https://www.economist.com/cdn-cgi/image/width=1280,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 1280w, https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg 1424w" src="https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20241214_WBP501.jpg"><figcaption><span>Photograph: Getty Images</span></figcaption></figure></section><div><div><p><time datetime="2024-12-10T17:55:20.458Z"> <!-- -->Dec 10th 2024</time><span>|</span><span>SAN FRANCISCO</span></p></div><section data-body-id="cp2"><p data-component="paragraph"><span data-caps="initial">O</span><small>n the night</small> of December 7th San Francisco’s Palace of Fine Arts, with its lakeside colonnade echoing a Roman ruin, turned into Mar-a-Lago, as <a href="https://www.economist.com/united-states/2024/11/21/how-donald-trump-could-win-the-future">Silicon Valley’s newly emboldened right-wingers</a> gathered for a Christmas bash organised by the All-In podcast. The festive good cheer did not extend to everyone;&nbsp;<i>The Economist</i> was made to feel most unwelcome. But not before being privy to a riotous celebration of how a clique of billionaires—the so-called PayPal Mafia—helped clinch Donald Trump’s election victory and has taken Washington by storm.</p></section><p><h3 id="article-tags">Explore more</h3><nav aria-labelledby="article-tags"><a href="https://www.economist.com/topics/elon-musk" data-analytics="tags:elon_musk"><span>Elon Musk</span></a><a href="https://www.economist.com/topics/donald-trump" data-analytics="tags:donald_trump"><span>Donald Trump</span></a><a href="https://www.economist.com/topics/united-states" data-analytics="tags:united_states"><span>United States</span></a></nav></p></div></div></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making memcpy(NULL, NULL, 0) well-defined (191 pts)]]></title>
            <link>https://developers.redhat.com/articles/2024/12/11/making-memcpynull-null-0-well-defined</link>
            <guid>42387013</guid>
            <pubDate>Wed, 11 Dec 2024 12:19:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developers.redhat.com/articles/2024/12/11/making-memcpynull-null-0-well-defined">https://developers.redhat.com/articles/2024/12/11/making-memcpynull-null-0-well-defined</a>, See on <a href="https://news.ycombinator.com/item?id=42387013">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>Undefined behavior (UB) in the C programming language is a regular source of heated discussions among programmers. On the one hand, UB can be important for compiler optimizations. On the other hand, it makes is easy to introduce bugs that lead to security issues.</p><p>The good news is that <a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3322.pdf">N3322</a> has been accepted for C2y, which will remove undefined behavior from one particular corner of the C language, making all of the following well-defined:</p><pre><code>memcpy(NULL, NULL, 0);
memcmp(NULL, NULL, 0);
(int *)NULL + 0;
(int *)NULL - 0;
(int *)NULL - (int *)NULL;</code></pre><p>This only applies when a null pointer is combined with a "zero-length" operation. The following are still undefined:</p><pre><code>memcpy(NULL, NULL, 4);
(int *)NULL + 4;</code></pre><p>The removal of this undefined behavior is not expected to have any negative impact on performance. In fact, the reverse is true.</p><h2>Motivation</h2><p>The examples above are somewhat silly because they hard-code a <code>NULL</code>/<code>nullptr</code> constant. However, it is easy to run into this situation with a pointer that is only sometimes null. For example, consider a typical representation for a string with a known length:</p><pre><code>struct str {
   char *data;
   size_t len;
};</code></pre><p>An empty string would usually be represented as <code>(struct str) { .data = NULL, .len = 0 }</code>, with the <code>data</code> pointer being <code>NULL</code>. Now, consider a function that checks if two strings are equal:</p><pre><code>bool str_eq(const struct str *str1, const struct str *str2) {
   return str1-&gt;len == str2-&gt;len &amp;&amp;
          memcmp(str1-&gt;data, str2-&gt;data, str1-&gt;len) == 0;
}</code></pre><p>This implementation looks very reasonable at first glance. However, it exhibits undefined behavior if both of the inputs are empty strings. In that case, we will call <code>memcmp(NULL, NULL, 0)</code>, which is undefined behavior according to the C standard.</p><p>This kind of UB introduces the risk that the compiler will optimize away following null pointer checks. For example, GCC will happily remove the <code>dest == NULL</code> branch in the following code, while Clang deliberately does not perform this optimization:</p><pre><code>int test(char *dest, const char *src, size_t len) {
   memcpy(dest, src, len);
   if (dest == NULL) {
       // This branch will be removed by GCC due to undefined behavior.
   }
}</code></pre><p>The correct way to write the <code>str_eq</code> function is as follows:</p><pre><code>bool str_eq(const struct str *str1, const struct str *str2) {
   return str1-&gt;len == str2-&gt;len &amp;&amp;
          (str1-&gt;len == 0 ||
           memcmp(str1-&gt;data, str2-&gt;data, str1-&gt;len) == 0);
}</code></pre><p>The new code is correct, but worse in every other way:</p><ul><li>It increases code size, by requiring an extra check at each inlined call-site.</li><li>It decreases performance, by redundantly checking something <code>memcmp</code> has to handle anyway.</li><li>It increases code complexity.</li></ul><p>At the same time, there is no useful way in which the C library can make use of this undefined behavior to provide a more efficient implementation. This is the kind of UB that benefits nobody, and should be removed from the language.</p><h2>Null pointer arithmetic</h2><p>The original proposal was focused on removing UB for memory library calls, but an early reviewer pointed out that this is not sufficient. After all, we also need to take into account how these library functions are implemented.</p><p>For example, let's consider a typical implementation for a <code>memcpy</code>-like function:</p><pre><code>void copy(char *dst, const char *src, size_t n) {
   for (const char *end = src + n; src &lt; end; src++) {
       *dst++ = *src;
   }
}</code></pre><p>This function exhibits undefined behavior when called as <code>copy(NULL, NULL, 0)</code>, because <code>NULL + 0</code> is undefined behavior in C.</p><p>To avoid this, and make the overall language self-consistent, we need to define <code>NULL + 0</code> as returning <code>NULL</code> and <code>NULL - NULL</code> as returning 0. This also aligns C with C++ semantics, where this was already well-defined.</p><h2>Opposition</h2><p>When this proposal was discussed at two WG14 meetings, the opposition didn't come from the direction I expected.</p><p>The most broadly controversial part of the proposal was to define <code>NULL - NULL</code> as returning 0. The reason for this is that when address spaces get involved (which are not part of standard C, but may be implemented as an extension), there may be multiple representations of a null pointer. Making sure that subtracting two "different" nulls still results in zero might require the generation of additional code, breaking the premise that this change is entirely free.</p><p>However, the most vocal opposition came from a static analysis perspective: Making null pointers well-defined for zero length means that static analyzers can no longer unconditionally report <code>NULL</code> being passed to functions like <code>memcpy</code>—they also need to take the length into account now. If an <a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3089.pdf"><code>_Optional</code> qualifier</a> is introduced in the future, <code>memcpy</code> arguments would have to be qualified with it. GCC is considering the introduction of a <a href="https://gcc.gnu.org/pipermail/gcc-patches/2024-November/668505.html"><code>nonnull_if_nonzero</code></a> attribute to represent the new pre-condition.</p><p>After the seemingly negative discussion, I was somewhat surprised that the vote not only went strongly in favor of the change, but also came with a recommendation to implementers to apply the change <a href="https://www.open-std.org/jtc1/sc22/wg14/www/previous.html">retroactively</a> to old standard versions. This means that, once compilers and C libraries have implemented the change, it should apply even without specifying the <code>-std=c2y</code> flag.</p><h2>Compiler builtins</h2><p>I work on the middle-end of the <a href="https://llvm.org/">LLVM</a> compiler toolchain. Being far removed from any "user-facing" parts of the compiler, I am generally not involved with standardization efforts.</p><p>The reason I got involved here at all is the specification for LLVM's internal memcpy intrinsic:</p><blockquote><div><p>The <code>llvm.memcpy.*</code> intrinsics copy a block of memory from the source location to the destination location, which must either be equal or non-overlapping. [...]</p><p>If <code>&lt;len&gt;</code> is 0, it is no-op modulo the behavior of attributes attached to the arguments. [...]</p></div></blockquote><p>The <code>llvm.memcpy</code> intrinsic may lower to a call to the <code>memcpy</code> function, which is treated as a "compiler runtime builtin" here, even though it is ultimately also provided by the C library.</p><p>When used as a builtin, LLVM requires that both <code>memcpy(x, x, s)</code> and <code>memcpy(NULL, NULL, 0)</code> are well-defined, even though the C standard says they are UB. GCC and MSVC have similar assumptions.</p><p>Making <code>memcpy(NULL, NULL, 0)</code> officially well-defined removes one of the assumptions, while the <code>memcpy(x, x, s)</code> case remains for now. Allowing this was originally also part of the proposal, but was later dropped, because it didn't fit well with the other changes.</p><p>In a weird turn of events, this change to the C standard came about because Rust developers kept nagging me about the mismatch between LLVM and C semantics.</p><h2>Acknowledgements</h2><p>This paper was a collaboration with Aaron Ballman, who also drove the discussion during the actual WG14 meetings. Special thanks go to David Stone, whose early feedback radically changed the direction of the proposal from memory library calls in particular to "zero-length" operations in general.</p>
          
                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Helium: Lighter Web Automation with Python (135 pts)]]></title>
            <link>https://github.com/mherrmann/helium</link>
            <guid>42386971</guid>
            <pubDate>Wed, 11 Dec 2024 12:11:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mherrmann/helium">https://github.com/mherrmann/helium</a>, See on <a href="https://news.ycombinator.com/item?id=42386971">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Lighter web automation with Python</h2><a id="user-content-lighter-web-automation-with-python" aria-label="Permalink: Lighter web automation with Python" href="#lighter-web-automation-with-python"></a></p>
<p dir="auto">Helium is a Python library for automating browsers such as Chrome and Firefox.
For example:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mherrmann/helium/blob/master/docs/helium-demo.gif"><img src="https://github.com/mherrmann/helium/raw/master/docs/helium-demo.gif" alt="Helium Demo" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">To get started with Helium, you need Python 3 and Chrome or Firefox.</p>
<p dir="auto">I would recommend creating a virtual environment. This lets you install Helium
for just your current project, instead of globally on your whole computer.</p>
<p dir="auto">To create and activate a virtual environment, type the following commands into
a command prompt window:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 -m venv venv
# On Mac/Linux:
source venv/bin/activate
# On Windows:
call venv\scripts\activate.bat"><pre>python3 -m venv venv
<span><span>#</span> On Mac/Linux:</span>
<span>source</span> venv/bin/activate
<span><span>#</span> On Windows:</span>
call venv<span>\s</span>cripts<span>\a</span>ctivate.bat</pre></div>
<p dir="auto">Then, you can install Helium with <code>pip</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m pip install helium"><pre>python -m pip install helium</pre></div>
<p dir="auto">Now enter <code>python</code> into the command prompt and (for instance) the commands in
the animation at the top of this page (<code>from helium import *</code>, ...).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Your first script</h2><a id="user-content-your-first-script" aria-label="Permalink: Your first script" href="#your-first-script"></a></p>
<p dir="auto">I've compiled a <a href="https://github.com/mherrmann/helium/blob/master/docs/cheatsheet.md">cheatsheet</a> that quickly teaches you all
you need to know to be productive with Helium.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Connection to Selenium</h2><a id="user-content-connection-to-selenium" aria-label="Permalink: Connection to Selenium" href="#connection-to-selenium"></a></p>
<p dir="auto">Under the hood, Helium forwards each call to Selenium. The difference is that
Helium's API is much more high-level. In Selenium, you need to use HTML IDs,
XPaths and CSS selectors to identify web page elements. Helium on the other hand
lets you refer to elements by user-visible labels. As a result, Helium scripts
are typically 30-50% shorter than similar Selenium scripts. What's more, they
are easier to read and more stable with respect to changes in the underlying web
page.</p>
<p dir="auto">Because Helium is simply a wrapper around Selenium, you can freely mix the two
libraries. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# A Helium function:
driver = start_chrome()
# A Selenium API:
driver.execute_script(&quot;alert('Hi!');&quot;)"><pre><span># A Helium function:</span>
<span>driver</span> <span>=</span> <span>start_chrome</span>()
<span># A Selenium API:</span>
<span>driver</span>.<span>execute_script</span>(<span>"alert('Hi!');"</span>)</pre></div>
<p dir="auto">So in other words, you don't lose anything by using Helium over pure Selenium.</p>
<p dir="auto">In addition to its more high-level API, Helium simplifies further tasks that are
traditionally painful in Selenium:</p>
<ul dir="auto">
<li><strong>iFrames:</strong> Unlike Selenium, Helium lets you interact with elements inside
nested iFrames, without having to first "switch to" the iFrame.</li>
<li><strong>Window management.</strong> Helium notices when popups open or close and focuses /
defocuses them like a user would. You can also easily switch to a window by
(parts of) its title. No more having to iterate over Selenium window handles.</li>
<li><strong>Implicit waits.</strong> By default, if you try click on an element with Selenium
and that element is not yet present on the page, your script fails. Helium by
default waits up to 10 seconds for the element to appear.</li>
<li><strong>Explicit waits.</strong> Helium gives you a much nicer API for waiting for a
condition on the web page to become true. For example: To wait for an element
to appear in Selenium, you would write:
<div dir="auto" data-snippet-clipboard-copy-content="element = WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.ID, &quot;myDynamicElement&quot;))
)"><pre><span>element</span> <span>=</span> <span>WebDriverWait</span>(<span>driver</span>, <span>10</span>).<span>until</span>(
    <span>EC</span>.<span>presence_of_element_located</span>((<span>By</span>.<span>ID</span>, <span>"myDynamicElement"</span>))
)</pre></div>
With Helium, you can write:
<div dir="auto" data-snippet-clipboard-copy-content="wait_until(Button('Download').exists)"><pre><span>wait_until</span>(<span>Button</span>(<span>'Download'</span>).<span>exists</span>)</pre></div>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Status of this project</h2><a id="user-content-status-of-this-project" aria-label="Permalink: Status of this project" href="#status-of-this-project"></a></p>
<p dir="auto">I have too little spare time to maintain this project for free. If you'd like
my help, please go to my <a href="http://herrmann.io/" rel="nofollow">web site</a> to ask about my
consulting rates. Otherwise, unless it is very easy for me, I will usually not
respond to emails or issues on the issue tracker. I will however accept and
merge PRs. So if you add some functionality to Helium that may be useful for
others, do share it with us by creating a Pull Request. For instructions, please
see <a href="#Contributing">Contributing</a> below.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How you can help</h2><a id="user-content-how-you-can-help" aria-label="Permalink: How you can help" href="#how-you-can-help"></a></p>
<p dir="auto">I find Helium extremely useful in my own projects and feel it should be more
widely known. Here's how you can help with this:</p>
<ul dir="auto">
<li>Star this project on GitHub.</li>
<li>Tell your friends and colleagues about it.</li>
<li><a href="https://twitter.com/intent/tweet?text=I%20find%20Helium%20very%20useful%20for%20web%20automation%20with%20Python%3A%20https%3A//github.com/mherrmann/helium" rel="nofollow">Share it on Twitter with one click</a></li>
<li>Share it on other social media</li>
<li>Write a blog post about Helium.</li>
</ul>
<p dir="auto">With this, I think we can eventually make Helium the de-facto standard for web
automation in Python.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Pull Requests are very welcome. Please follow the same coding conventions as the
rest of the code, in particular the use of tabs over spaces. Also, read through my
<a href="https://gist.github.com/mherrmann/5ce21814789152c17abd91c0b3eaadca">PR guidelines</a>.
Doing this will save you (and me) unnecessary effort.</p>
<p dir="auto">Before you submit a PR, ensure that the tests still work:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -Ur requirements/test.txt
python setup.py test"><pre>pip install -Ur requirements/test.txt
python setup.py <span>test</span></pre></div>
<p dir="auto">This runs the tests against Chrome. To run them against Firefox, set the
environment variable <code>TEST_BROWSER</code> to <code>firefox</code>. Eg. on Mac/Linux:</p>
<div dir="auto" data-snippet-clipboard-copy-content="TEST_BROWSER=firefox python setup.py test"><pre>TEST_BROWSER=firefox python setup.py <span>test</span></pre></div>
<p dir="auto">On Windows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="set TEST_BROWSER=firefox
python setup.py test"><pre><span>set</span> TEST_BROWSER=firefox
python setup.py <span>test</span></pre></div>
<p dir="auto">If you do add new functionality, you should also add tests for it. Please see
the <a href="https://github.com/mherrmann/helium/blob/master/tests"><code>tests/</code></a> directory for what this might look like.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">History</h2><a id="user-content-history" aria-label="Permalink: History" href="#history"></a></p>
<p dir="auto">I (Michael Herrmann) originally developed Helium in 2013 for a Polish IT startup
called BugFree software. (It could be that you have seen Helium before at
<a href="https://heliumhq.com/" rel="nofollow">https://heliumhq.com</a>.) We shut down the company at the end of 2019 and I felt it
would be a shame if Helium simply disappeared from the face of the earth. So I
invested some time to modernize it and bring it into a state suitable for open
source.</p>
<p dir="auto">Helium used to be available for both Java and Python. But because I now only
use it from Python, I didn't have time to bring the Java implementation up to
speed as well. Similarly for Internet Explorer: Helium used to support it, but
since I have no need for it, I removed the (probably broken) old implementation.</p>
<p dir="auto">The name Helium was chosen because it is also a chemical element like Selenium,
but it is lighter.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Astronomy Photographer of the Year 2024 winners (127 pts)]]></title>
            <link>https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/overall-winners-2024</link>
            <guid>42385761</guid>
            <pubDate>Wed, 11 Dec 2024 07:56:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/overall-winners-2024">https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/overall-winners-2024</a>, See on <a href="https://news.ycombinator.com/item?id=42385761">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-off-canvas-main-canvas="">
    
  

  



  




  

  

  

  <main role="main">
    
    <div id="block-rmg-theme-content">
  
    
      <article data-content-type="topic">

  
    

  
  <div>
              
              <div>
              <p>
          <h2>
            The overall winner
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-09/OS-348550-1%20Distorted%20Shadows%20of%20the%20Moon%27s%20Surface%20Created%20by%20an%20Annular%20Eclipse%20v2_0.jpg?itok=NnUAJz-h" width="1200" height="675" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Ryan Imperio
      </span>
                  </p></div>
      <div>
                  <h3>
            Distorted Shadows of the Moon's Surface Created by an Annular Eclipse by Ryan Imperio
      </h3>
                <div>
          
            <p>Photographer Ryan Imperio from the United States has been named the overall winner of Astronomy Photographer of the Year 16.</p><p>The image is a <a href="https://www.rmg.co.uk/stories/topics/beads-sunlight-photographing-annular-solar-eclipse-astronomy-photographer-year" data-entity-type="node" data-entity-uuid="415695bc-72b7-4740-a1af-726249f49004" data-entity-substitution="canonical" title="Beads of sunlight: photographing an annular solar eclipse" data-gtm-name="CTA" data-gtm-detail="formatted content">composite of more than 30 separate photographs of the Sun</a>, taken in Texas during the annular solar eclipse of 14 October 2023.</p><p>Together the photographs capture the fleeting optical illusion known as 'Baily's beads', which occurs when sunlight shines through the valleys and craters of the Moon.</p><p>“What an innovative way to map the Moon’s topography at the point of third contact during an annular solar eclipse," competition judge Kerry-Ann Lecky Hepburn says. "This is an impressive dissection of the fleeting few seconds during the visibility of the Baily’s beads. This image left me captivated and amazed. It’s exceptional work deserving of high recognition. Congratulations!”</p><p><a href="https://www.rmg.co.uk/stories/topics/beads-sunlight-photographing-annular-solar-eclipse-astronomy-photographer-year" data-entity-type="node" data-entity-uuid="415695bc-72b7-4740-a1af-726249f49004" data-entity-substitution="canonical" title="Beads of sunlight: photographing an annular solar eclipse" data-gtm-name="CTA" data-gtm-detail="formatted content">Learn more about the winning image</a></p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/our-sun-2024" data-entity-type="node" data-entity-uuid="9ce1c711-01dc-45a0-afc0-966a5252bb2f" data-entity-substitution="canonical" title="Our Sun 2024 - Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Our Sun shortlist</a><br>&nbsp;</p>
      
                      
                  </div>
      </div>
    </div>
              <div>
        <h2>
            Never miss a shooting star
      </h2>        
            <p><span>Sign up to our monthly space newsletter for amazing space stories, astronomy guides and more from Royal Museums Greenwich.</span></p>
      
      </div>
              <div>
              <p>
          <h2>
            Skyscapes category winner
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/Tasman%20Gems%20%C2%A9%20Tom%20Rae%20%E2%80%93%20Astronomy%20Photographer%20of%20the%20Year%202024%20Skyscapes%20.jpg?itok=aKcF1NY1" width="1200" height="761" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Tom Rae
      </span>
                  </p></div>
      <div>
                  <h3>
            Tasman Gems by Tom Rae
      </h3>
                <div>
          
            <p>"It’s very challenging to create this sort of composition without tipping the balance in favour of either foreground or background. Here, the grass and central rock retain wonderful detail, as do the midground mountains, but the vibrance and detail in the cosmic background shine through as well.&nbsp;</p><p>"Even the airglow adds to the image, which is no easy feat! As well as being technically impressive, the balance also produces a sort of surreal quality. A slightly dream-like connection between the Earth-bound and the celestial."</p><p>– Ed Bloomer, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/skyscapes-2024" data-entity-type="node" data-entity-uuid="144ebd7a-1f52-4905-affd-4eaa7e2fb0b9" data-entity-substitution="canonical" title="Skyscapes 2024 – Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Skyscapes shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            Galaxies category winner
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/G-68776-63-03518%20Echoes%20of%20the%20Past%20%C2%A9%20Bence%20T%C3%B3th%20and%20P%C3%A9ter%20Felt%C3%B3ti.jpg?itok=J4yknJ6w" width="1200" height="705" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Bence Tóth and Péter Feltóti
      </span>
                  </p></div>
      <div>
                  <h3>
            Echoes of the Past by Bence Tóth and Péter Feltóti
      </h3>
                <div>
          
            <p>"Galaxies are among the most amazing phenomena you can observe with a telescope. Each is unique, but some are more special than others. Centaurus A is one of the most extraordinary of its kind, and this image certainly stands out among galaxy photos. Thanks to accurate photon collection, precise image processing and cooperation between fellow astrophotographers, a photo taken on a challenging expedition has become one of the best captures of this object to date."</p><p>– László Francsics, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/galaxies-2024" data-entity-type="node" data-entity-uuid="6afa0eb3-4926-4ec9-9dce-41a80bb8e208" data-entity-substitution="canonical" title="Galaxies 2024 - Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Galaxies shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            Our Moon category winner
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/Shadow%20Peaks%20of%20Sinus%20Iridum%20%C2%A9%20G%C3%A1bor%20Bal%C3%A1zs%20-%20Astronomy%20Photographer%20of%20the%20Year%202024%20Our%20Moon.jpg?itok=86_Tl91h" width="1200" height="865" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Gábor Balázs
      </span>
                  </p></div>
      <div>
                  <h3>
            Shadow Peaks of Sinus Iridum by Gábor Balázs
      </h3>
                <div>
          
            <p>"This is a very impressive image. Sinus Iridum, known as the 'Bay of Rainbows', is about 260 km in diameter and is bordered by several smaller craters, showcasing the Moon’s rugged terrain.</p><p>"The detailed capture of Pythagoras, noted for its depth and complex features, is enhanced by the phenomenon of libration, where slight oscillations in the Moon’s orientation allow Earth-bound observers a glimpse of areas typically hidden from view. This image not only highlights the capabilities of modern astrophotography equipment but also offers a vivid illustration of lunar surface features, contributing valuable insights into lunar geology."</p><p>– Yuri Beletsky, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/our-moon-2024" data-entity-type="node" data-entity-uuid="87bb24ef-6d32-4c1f-8e29-4f969533135c" data-entity-substitution="canonical" title="Our Moon 2024 - Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Our Moon shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            Aurorae category winner
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/Queenstown%20Aurora%20%C2%A9%20Larryn%20Rae%20-%20Astronomy%20Photographer%20of%20the%20Year%202024%20Aurorae.jpg?itok=D3ZS7yl8" width="1200" height="722" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Larryn Rae
      </span>
                  </p></div>
      <div>
                  <h3>
            Queenstown Aurora by Larryn Rae
      </h3>
                <div>
          
            <p>"This extraordinary panoramic image captures an aurora with rare red and pink hues over New Zealand. This is a phenomenon typically seen near the poles but appears here due to intense solar activity. The vivid red colours are produced at high altitudes when charged particles from solar flares and coronal mass ejections interact with oxygen in Earth’s atmosphere.&nbsp;</p><p>"Red aurorae are less common than green ones, which occur at lower altitudes where there is more oxygen to interact with and a higher density of atoms. This makes the observation more unique and special; an event typically associated with significant solar activity and usually only visible under clear, dark sky conditions."</p><p>– Yuri Beletsky, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/aurorae-2024" data-entity-type="node" data-entity-uuid="c5015161-7fe5-4911-b5b0-06d2f5c94be0" data-entity-substitution="canonical" title="Aurorae 2024 – Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Aurorae shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            Planets, Comets and Asteroids category winner
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/PCA-195480-170%20On%20Approach%20%C2%A9%20Tom%20Williams%20-%20Astronomy%20Photographer%20of%20the%20Year%202024%20Planets%20Comets%20and%20Asteroids.jpg?itok=KQkif32_" width="1200" height="1200" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Tom Williams
      </span>
                  </p></div>
      <div>
                  <h3>
            On Approach by Tom Williams
      </h3>
                <div>
          
            <p>"Venus shares very little with Earth-bound observers. Its highly reflective clouds show no detail when using conventional imaging methods. This photographer, however, has managed to tease a startling level of detail out of the phases shown here. Although the colours used are false, they are not too far from the natural colour of the planet. The thoughtful compositional work in the accurate scaling of the three phases is just the icing on the cake."</p><p>– Steve Marsh, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/planets-comets-asteroids-2024" data-entity-type="node" data-entity-uuid="e5ad6088-ae9e-4510-9af1-7b24fa53ffd9" data-entity-substitution="canonical" title="Planets, Comets and Asteroids 2024 - Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Planets, Comets and Asteroids shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            People and Space category winner
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/High-tech%20Silhouette%20%C2%A9%20Tom%20Williams%20%20-%20Astronomy%20Photographer%20of%20the%20Year%202024%20People%20and%20Space.jpg?itok=jM2P8Gei" width="935" height="1200" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Tom Williams
      </span>
                  </p></div>
      <div>
                  <h3>
            High-tech Silhouette by Tom Williams
      </h3>
                <div>
          
            <p>"This dramatic image serves as a powerful reminder of humanity’s ongoing presence in space. The photographer has done a great job in perfectly timing this shot so that the International Space Station is silhouetted against the backdrop of the Sun’s eastern solar limb.&nbsp;</p><p>"The photograph beautifully showcases the dynamic and active nature of the Sun, bringing it to life in a captivating way. Yet among that, your eye is permanently fixed on the tiny human-made spacecraft making its way across, emphasizing its significance amid the grandeur of the Sun. It’s a worthy winner of this category."</p><p>– Melissa Brobby, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/people-space-2024" data-entity-type="node" data-entity-uuid="2da63a75-3b58-440b-829c-032441f64fbb" data-entity-substitution="canonical" title="People and Space 2024 - Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full People and Space shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            Stars and Nebulae category winner
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/SNR%20G107.5-5.2%2C%20Unexpected%20Discovery%20%28the%20Nereides%20Nebula%20in%20Cassiopeia%29%20%C2%A9%20Marcel%20Drechsler%2C%20Bray%20Falls%2C%20Yann%20Sainty%2C%20Nicolas%20Martino%20and%20Richard%20Galli.jpg?itok=8-dm1sfe" width="1099" height="1200" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Marcel Drechsler, Bray Falls, Yann Sainty, Nicolas Martino and Richard Galli
      </span>
                  </p></div>
      <div>
                  <h3>
            SNR G107.5-5.2, Unexpected Discovery (the Nereides Nebula in Cassiopeia) by Marcel Drechsler, Bray Falls, Yann Sainty, Nicolas Martino and Richard Galli
      </h3>
                <div>
          
            <p>"The hits keep on coming from this team with another stunning revelation for us. Who knew this fantastic and delicate structure was there all along in one of the best-known constellations in the night sky? The thoughtful processing and clever use of colouring really make the supernova remnant pop against its background. It’s one of those images that you can stare into for hours and still find more detail. Stunning!"</p><p>– Steve Marsh, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/stars-nebulae-2024" data-entity-type="node" data-entity-uuid="6f020dfb-7c21-4b6a-881d-a9d37c656966" data-entity-substitution="canonical" title="Stars and Nebulae 2024 – Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Stars and Nebulae shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            The Sir Patrick Moore Prize for Best Newcomer
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/BN-312741-1%20SH2-308%20Dolphin%20Head%20Nebula%20%C2%A9%20Xin%20Feng%20and%20Miao%20Gong%20-%20Astronomy%20Photographer%20of%20the%20Year%202024%20Best%20Newcomer.jpg?itok=xhy4VGLX" width="1200" height="799" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Xin Feng and Miao Gong
      </span>
                  </p></div>
      <div>
                  <h3>
            SH2-308: Dolphin Head Nebula by Xin Feng and Miao Gong
      </h3>
                <div>
          
            <p>"The Dolphin Head Nebula is a bubble of hydrogen pushed out from a very luminous Wolf-Rayet star. Stellar winds of over 1,500 km per second make the region rather more lively than even its animal namesake.</p><p>"This image is wonderfully detailed, and really displays the three-dimensional nature of the nebula. It is vibrant, without losing the very delicate surrounding structures, and you can clearly make out another little planetary nebula bubble (called PN G234.9-09.7) towards the bottom of the dolphin’s head, which is rarely imaged with any clarity. Very, very impressive work from any astrophotographer, let alone a newcomer."</p><p>– Ed Bloomer, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/best-newcomer-2024" data-entity-type="node" data-entity-uuid="edf7197f-cdc7-44b1-87f6-d96c3da5f132" data-entity-substitution="canonical" title="Best Newcomer 2024 - Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Best Newcomer shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            The Annie Maunder Prize for Image Innovation
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-09/Anatomy%20of%20a%20Habitable%20Planet%20%C2%A9%20Sergio%20Di%CC%81az%20Ruiz%20-%20Astronomy%20Photographer%20of%20the%20Year%202024%20Annie%20Maunder%20Prize.jpg?itok=xMIVsv5Y" width="1200" height="1200" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Sergio Díaz Ruiz
      </span>
                  </p></div>
      <div>
                  <h3>
            Anatomy of a Habitable Planet by Sergio Díaz Ruiz 
      </h3>
                <div>
          
            <p>"This strangely familiar representation of the Earth transforms scientific data through colour mapping to highlight the devastation already inflicted on our world. The image poignantly emphasizes the significant environmental challenges we face and the urgent need to protect and preserve our planet."</p><p>– Victoria Lane, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/image-innovation-prize-2024" data-entity-type="node" data-entity-uuid="637e10c4-4c30-4c15-8aa0-3935111d2857" data-entity-substitution="canonical" title="Annie Maunder Prize for Image Innovation 2024 – Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Annie Maunder Prize shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
              <p>
          <h2>
            Young Competition
      </h2>
        </p>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large_no_alt/public/2024-08/NGC%201499%2C%20a%20Dusty%20California%20%C2%A9%20Daniele%20Borsari%20-%20Astronomy%20Photographer%20of%20the%20Year%20Young.jpg?itok=33mHEQiH" width="1200" height="995" alt="" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Daniele Borsari
      </span>
                  </p></div>
      <div>
                  <h3>
            NGC 1499, a Dusty California by Daniele Borsari
      </h3>
                <div>
          
            <p>"This incredibly beautiful image was very popular with the panel. Not least because it captures a nebula, atmospheric gases and has extraordinary balance of light, composition and structure. The future of astronomy photography being fearlessly, and openly, taken forward by a new generation."</p><p>– Neal White, competition judge</p><p><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/young-competition-2024" data-entity-type="node" data-entity-uuid="9792c8b9-097c-4d3c-b3e5-4bbfb43fee7f" data-entity-substitution="canonical" title="Young Competition 2024 - Astronomy Photographer of the Year" data-gtm-name="CTA" data-gtm-detail="formatted content">See the full Young Competition shortlist</a></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
                  <h2>
            Find out more about the competition
      </h2>
                        <div>
                                                <div>
                  <div>
                     <p><img src="https://www.rmg.co.uk/sites/default/files/styles/slider_image/public/2020-11/PS-8826-5_Ineffable%20%C2%A9%20Alyn%20Wallace_0.jpg?itok=TNy3jqPy" alt="" loading="lazy"></p><p>Competition</p>                                      </div>
                  <div>
                      
                                            <p>
                        Key dates, prizes and details on how to enter Astronomy Photographer of the Year
                      </p>
                    </div>
                                  </div>
                                                            <div>
                  <div>
                     <p><img src="https://cdn.shopify.com/s/files/1/2459/8861/files/Collection-13-cover-rmg-publication-banner.jpg?v=1730196645" alt="" loading="lazy"></p><p>Shop</p>                                                                                </div>
                  <div>
                      
                                              <p>
                                                                                    £30.00
                                                                              </p>
                                            <p>
                        Astronomy Photographer of the Year: Collection 13 is a stunning gift for admirers of astrophotography. The competition's official book, this spectacular astronomy photography book showcases the most spectacular space photography, taken from locations across the globe...
                      </p>
                    </div>
                                    
                                  </div>
                                                            <div>
                  <div>
                     <p><img src="https://www.rmg.co.uk/sites/default/files/styles/slider_image/public/migrations/G-28529-27_Winner%20and%20Overall%20Winner_Andromeda%20Galaxy%20at%20Arm_s%20Length%20%C2%A9%20Nicolas%20Lefaudeux_2.jpg?itok=QhE33xgR" alt="" loading="lazy"></p><p>Past winners</p>                                      </div>
                  <div>
                      
                                            <p>
                        Astronomy Photographer of the Year has been held every year since 2009. Take a journey back in space and time and explore all the past winning images
                      </p>
                    </div>
                                  </div>
                              </div>
      </div>
              <div>
                  <h2>
            Get into astrophotography
      </h2>
                        <div>
                                                <div>
                  <div>
                     <p><img src="https://www.rmg.co.uk/sites/default/files/styles/slider_image/public/2023-09/Monika%20Deviat%20silhouetted%20taking%20a%20photo%20at%20night.png?itok=85Mib32v" alt="" loading="lazy"></p><p>Course</p>                                      </div>
                  <div>
                      
                                            <p>
                        Learn how to take images of the night sky in the Royal Observatory Greenwich's online introductory astrophotography course
                      </p>
                    </div>
                                  </div>
                                                            <div>
                  <div>
                     <p><img src="https://cdn.shopify.com/s/files/1/2459/8861/files/2025-GTTNS-RMG-publication.jpg?v=1730285783" alt="" loading="lazy"></p><p>Shop</p>                                                                                </div>
                  <div>
                      
                                              <p>
                                                                                    £6.99
                                                                              </p>
                                            <p>
                        
Annually, Guide to the Night Sky is the bestselling stargazing handbook to the planets, stars, and constellations visible from the northern hemisphere...
                      </p>
                    </div>
                                    
                                  </div>
                                                            <div>
                  <div>
                     <p><img src="https://www.rmg.co.uk/sites/default/files/styles/slider_image/public/2024-06/A%20woman%20looks%20at%20the%20photographs%20on%20display%20in%20the%20Astronomy%20Photographer%20of%20the%20Year%20exhibition%20at%20the%20National%20Maritime%20Museum%20%28T3782-127%29.jpg?itok=JEquG48o" alt="" loading="lazy"></p><p>Inspiration</p>                                      </div>
                  <div>
                      
                                            <p>
                        Want to get into astrophotography but don’t know where to begin? Photographers from Astronomy Photographer of the Year reveal their top tips
                      </p>
                    </div>
                                  </div>
                              </div>
      </div>
              <div>
      <div>
                      <p>
      <h2>
            Our partners
      </h2>
    </p>
                    </div>
      <div>
                              <div>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/logo/public/2021-08/Liberty_Specialty_Markets_black%5B22%5D.png?itok=8_BWwpaC" width="277" height="96" alt="Liberty Specialty Markets black" loading="lazy">



      
</p>
            </div>
                      <div>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/logo/public/2022-06/BBC%20Sky%20At%20Night%20logo.png?itok=aLkJU--4" width="193" height="96" alt="BBC Sky at Night logo in black" loading="lazy">



      
</p>
            </div>
                        </div>
    </div>
          </div>

</article>


  </div>  </main>

  
    

  

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You need 4 colors (135 pts)]]></title>
            <link>https://www.iamsajid.com/colors/</link>
            <guid>42385357</guid>
            <pubDate>Wed, 11 Dec 2024 06:31:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iamsajid.com/colors/">https://www.iamsajid.com/colors/</a>, See on <a href="https://news.ycombinator.com/item?id=42385357">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <pre id="code">:root {
        --color-primary: hsl(0, 50%, 90%);
        --color-secondary: hsl(0, 50%, 10%);
        --color-tertiary: hsl(60, 80%, 20%);
        --color-accent: hsl(300, 80%, 20%);
    }
.dark {
        --color-primary: hsl(0, 50%, 10%);
        --color-secondary: hsl(0, 50%, 90%);
        --color-tertiary: hsl(60, 80%, 80%);
        --color-accent: hsl(300, 80%, 80%);
    }</pre></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jury finds Cognizant discriminated against US workers (101 pts)]]></title>
            <link>https://www.bloomberg.com/graphics/2024-cognizant-h1b-visas-discriminates-us-workers/</link>
            <guid>42385000</guid>
            <pubDate>Wed, 11 Dec 2024 05:13:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/graphics/2024-cognizant-h1b-visas-discriminates-us-workers/">https://www.bloomberg.com/graphics/2024-cognizant-h1b-visas-discriminates-us-workers/</a>, See on <a href="https://news.ycombinator.com/item?id=42385000">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Judge Refuses to Allow Sale of Infowars to The Onion (488 pts)]]></title>
            <link>https://www.nytimes.com/2024/12/10/business/media/the-onion-infowars-alex-jones.html</link>
            <guid>42384921</guid>
            <pubDate>Wed, 11 Dec 2024 04:53:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/12/10/business/media/the-onion-infowars-alex-jones.html">https://www.nytimes.com/2024/12/10/business/media/the-onion-infowars-alex-jones.html</a>, See on <a href="https://news.ycombinator.com/item?id=42384921">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/12/10/business/media/the-onion-infowars-alex-jones.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The Case Against Google's Claims of "Quantum Supremacy" (156 pts)]]></title>
            <link>https://gilkalai.wordpress.com/2024/12/09/the-case-against-googles-claims-of-quantum-supremacy-a-very-short-introduction/</link>
            <guid>42384768</guid>
            <pubDate>Wed, 11 Dec 2024 04:22:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gilkalai.wordpress.com/2024/12/09/the-case-against-googles-claims-of-quantum-supremacy-a-very-short-introduction/">https://gilkalai.wordpress.com/2024/12/09/the-case-against-googles-claims-of-quantum-supremacy-a-very-short-introduction/</a>, See on <a href="https://news.ycombinator.com/item?id=42384768">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						<p>The 2019 paper “<a href="https://www.nature.com/articles/s41586-019-1666-5">Quantum supremacy using a programmable superconducting processor</a>”&nbsp; asserted that Google’s Sycamore quantum computer,&nbsp; with 53 qubits and a depth of 20, performed a specific computation in about 200 seconds. According to Google’s estimate, a state-of-the-art classical supercomputer would require approximately 10,000 years to complete the same computation.</p>
<p>The Google experiment had two major components:</p>
<ol>
<li><strong>The “Fidelity Claims”</strong>: Assertions regarding the fidelity of the samples produced by the quantum computer.</li>
<li><strong>The “Supremacy Claims”</strong>: Assertions that translated fidelity into a measure of advantage over classical computation.</li>
</ol>
<p>There are valid reasons to question both of these claims in the context of Google’s 2019 experiment. In my view, these claims may reflect serious methodological mistakes rather than an objective scientific reality. I do not recommend treating Google’s past or future claims as a solid foundation for policy-making decisions.</p>
<p>Below is a brief review of the case against Google’s 2019 claims of quantum supremacy:</p>
<p><strong>A) The “Supremacy” Assertions: Flawed Estimation of Classical Running Time</strong></p>
<p><strong>A.1)</strong> The claims regarding classical running times were off by 10 orders of magnitude.</p>
<p><strong>A.2)</strong> Moreover, the Google team was aware that better classical algorithms existed. They had developed more sophisticated classical algorithms for one class of circuits and subsequently changed the type of circuits used for the “supremacy demonstration” just weeks before the final experiment.</p>
<p><strong>A.3)</strong> The 2019 Google paper states, <em>“Quantum processors have thus reached the regime of quantum supremacy. We expect that their computational power will continue to grow at a double-exponential rate.”</em> It is surprising to encounter such an extraordinary claim in a scientific paper.</p>
<h3><strong>B) The “Fidelity” Assertions: Statistically Unreasonable Predictions Indicating Methodological Flaws</strong></h3>
<p>The google paper relies on a very simple <em>a priori</em> prediction of the fidelity based on the error-rates of individual components. (Formula (77).)</p>
<p><strong>B.1)</strong> The agreement between the <em>a priori</em> prediction and the actual estimated fidelity is statistically implausible (“too good to be true”): It is unlikely that the fidelities of samples from hundreds of circuits would agree within 10-20% with a simple formula based on the multiplication of the fidelities of individual components.&nbsp; In my opinion, this suggests a methodologically flawed optimization process, such as the one described in item C.&nbsp;&nbsp;</p>
<p><strong>B.2)</strong> The Google team provided a statistical explanation for this agreement based on three premises. The first premise is that the fidelities for the individual components are exact up to&nbsp; ±20%. The second premise is that this ±20% instability is unbiased. The third premise is that all these fidelities for individual components are statistically independent. These premises are unreasonable and they contradict various other experimental findings.</p>
<p><strong>B.3)</strong> As of now, the error rates for individual components have not been released by the Google team. (Most recently, in May 2023, they promised “to push” for this data.) Analysis of the partial data provided for readout errors reinforces these concerns.</p>
<h3><strong>C) The Calibration Process: Evidence of Undocumented Global Optimization</strong></h3>
<p>According to the Google paper, calibration was performed prior to running the random circuit experiments and was based on the behavior of 1- and 2-qubit circuits. This process involved modifying the definitions of 1-gates and 2-gates to align with how the quantum computer operates.</p>
<p><strong>C.1)</strong> Statistical evidence suggests that the calibration process involved a methodologically flawed <em>global</em> optimization process. (This concern applies even to Google’s assertions about the fidelity of the smallest 12-qubit circuits.)</p>
<p><strong>C.2)</strong> Non-statistical evidence also supports this claim. For example, contrary to the description provided by the Google team, it was revealed that they supplied an outdated calibration version (for the experimental circuits) to the Jülich Research Center scientists involved in the experiment. This calibration was later further modified after the experiment was conducted. (This discrepancy is also reflected in a <a href="https://youtu.be/-ZNEzzDcllU?si=-CiGRIUn3rz7Sc0m">video released</a> by Google particularly between <a href="https://youtu.be/-ZNEzzDcllU?si=fTVx-zCzXVLRtKG3&amp;t=133">2:13-3:07</a>.)</p>
<p><strong>C.3)</strong> The Google team has not disclosed their calibration programs, citing them as a commercial secret. For technical reasons, they were also unable to share the<em> inputs</em> for the calibration program, although they promised to do so in future experiments—a promise that has not yet been fulfilled.</p>
<p><a href="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp"><img data-attachment-id="27584" data-permalink="https://gilkalai.wordpress.com/2024/12/09/the-case-against-googles-claims-of-quantum-supremacy-a-very-short-introduction/google-slide13-2/" data-orig-file="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp" data-orig-size="1024,560" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="google-slide13" data-image-description="" data-image-caption="" data-medium-file="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp?w=300" data-large-file="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp?w=640" src="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp" alt="google-slide13" width="549" height="300" srcset="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp?w=549&amp;h=300 549w, https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp?w=150&amp;h=82 150w, https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp?w=300&amp;h=164 300w, https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp?w=768&amp;h=420 768w, https://gilkalai.wordpress.com/wp-content/uploads/2024/12/google-slide13.webp 1024w" sizes="(max-width: 549px) 100vw, 549px"></a></p>
<p><span>A slide from my 2019 lecture “<a href="https://youtu.be/p18P1y8GD9U?si=1nNQmHGFgdxTAln1">The Google quantum supremacy demo</a>” (<a href="https://gilkalai.wordpress.com/2019/12/27/the-google-quantum-supremacy-demo/">post</a>), highlights that the error rates for two-qubit gates <img src="https://s0.wp.com/latex.php?latex=e_g&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=e_g&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=e_g&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="e_g"> have not yet been provided by the Google team as of today (Dec. 2024).</span></p>
<h3><strong>D) Comparing Google with IBM</strong></h3>
<p>As far as we know, there is a significant gap (in favor of Google) between what IBM quantum computers—which are in some ways more advanced than Google’s quantum computers—can achieve for random circuit sampling and what Google claims, even for circuits with 7–12 qubits. While one might argue that Google’s devices or team are simply better, in my view, this gap more likely reflects methodological issues in Google’s experiments.</p>
<h3><strong>E) (Not) Adopting Suggestions for Better Control</strong></h3>
<p>In our discussions with the Google team, they endorsed several of our suggestions for future experiments aimed at improving control over the quality of their experiments. However, in practice, later experiments did not implement any of these suggestions. Moreover, the structure of these later experiments makes them even harder to verify compared to the 2019 experiment. Additionally, unlike the 2019 experiment, the data for a subsequent random circuit sampling experiment does not include the amplitudes computed for the experimental circuits, further complicating efforts to scrutinize the results.</p>
<h3><strong>F) My Personal Conclusion</strong></h3>
<p>Google Quantum AI’s claims&nbsp; (including published ones) should be approached with caution, particularly those of an extraordinary nature. These claims may stem from significant methodological errors and, as such, may reflect the researchers’ expectations more than objective scientific reality. I do not recommend treating Google’s past or future claims as a solid basis for policy-making decisions.</p>
<h3><strong>G) Remarks</strong></h3>
<p><strong>G.1)</strong> Google’s supremacy claims (from the 2019 paper) have been refuted in a series of papers by several groups. This began with work by IBM researchers Pednault et al. shortly after Google’s original paper was published and continued with studies by Pan and Zhang; Pan, Chen, and Zhang; Kalachev, Panteleev, and Yung; Gao et al.; Liu et al.; and several other groups. For further details, see <a href="https://gilkalai.wordpress.com/2021/03/10/amazing-feng-pan-and-pan-zhang-announced-a-way-to-spoof-classically-simulate-the-googles-quantum-supremacy-circuit/">this post</a> and the associated comment section, as well as <a href="https://gilkalai.wordpress.com/2022/08/06/ordinary-computers-can-beat-googles-quantum-computer-after-all/">this post</a>.&nbsp;</p>
<p><strong>G.2)</strong> Google now acknowledges that using the tensor network contraction method, their 2019 53-qubit result can be computed classically in less than 200 seconds. However, in their more recent 2023/24 paper, <em>“Phase Transitions…”</em> (see Table 1), they claim that with 67 to 70 qubits, classical supercomputers would require many years to generate 1 million such bitstrings, even with tensor network contraction.</p>
<p><strong>G.3)</strong> Items B) and C) highlights methodological issues with Google’s fidelity assertions, even for 12-qubit circuits. These concerns persist independently of the broader question of quantum supremacy for larger circuits, where the fidelity assertions are taken at face value.</p>
<p><strong>G.4)</strong> For a more comprehensive view of our study of Google’s fidelity claims, refer to the following papers:</p>
<ul>
<li>Y. Rinott, T. Shoham, and G. Kalai, <a href="https://gilkalai.wordpress.com/wp-content/uploads/2022/08/sts836.pdf">Statistical Aspects of the Quantum Supremacy Demonstration,</a> (2020) Statistical Science&nbsp; (2022)</li>
<li>G. Kalai, Y. Rinott and T. Shoham, <a href="https://gilkalai.wordpress.com/wp-content/uploads/2022/10/cc22a19.pdf"><span dir="ltr" role="presentation">Google’s 2019 “Quantum Supremacy” Claims:&nbsp;</span><span dir="ltr" role="presentation">Data, Documentation, &amp; Discussion</span></a> (2022) (see <a href="https://gilkalai.wordpress.com/2022/10/07/the-google-supremacy-experiment-data-information-discussions-and-three-questions/">this post</a>).</li>
<li>G. Kalai, Y. Rinott and T. Shoham, <a href="https://arxiv.org/abs/2305.01064">Questions and Concerns About Google’s Quantum Supremacy Claim</a> (2023) (see <a href="https://gilkalai.wordpress.com/2023/05/31/questions-and-concerns-about-googles-quantum-supremacy-claim/">this post</a>).</li>
<li>G. Kalai, Y. Rinott and T. Shoham, <a href="https://arxiv.org/abs/2404.00935">Random circuit sampling: Fourier expansion and statistics</a>. (2024) (see <a href="https://gilkalai.wordpress.com/2024/04/02/random-circuit-sampling-fourier-expansion-and-statistics/">this post</a>)</li>
</ul>
<p>These papers describe an ongoing project with Yosi Rinott and Tomer Shoham, supported by Ohad Lev and Carsten Voelkmann. Together with Carsten, we plan to expand our study and apply our tools to other experiments. Additionally, see my earlier paper:</p>
<ul>
<li>G. Kalai, <a href="https://gilkalai.wordpress.com/wp-content/uploads/2020/08/laws-blog2.pdf">The argument against quantum computers, the quantum laws of nature, and Google’s supremacy claims,</a><em> (2020) The Intercontinental Academia Laws: Rigidity and Dynamics </em>(M. J. Hannon and E. Z. Rabinovici, eds.), World Scientific, 2024. arXiv:2008.05188.</li>
</ul>
<p><strong>G.5)</strong> There is also supporting evidence for Google’s 2019 claims, such as a 2020 replication by a group from the University of Science and Technology of China (USTC) and later verifications of some of Google’s fidelity estimations.</p>
<p><strong>G.6)</strong> There are some additional concerns regarding the Google experiment. In particular, there are problematic discrepancies between the experimental data, the Google noise model, and simulations.</p>
<p><strong>G.7)</strong> In my opinion, the main current challenge for experimental quantum computing is to improve the quality of two-qubit gates and other components, as well as to carefully study the quality of quantum circuits in the 5–20 qubit regime. Experiments on quantum error correction for larger circuits are also important.&nbsp;</p>
<h3><strong>H) Hype and Bitcoin</strong></h3>
<p>I usually don’t mind “hype” as a reflection of scientists’ enthusiasm for their work and the public’s excitement about scientific endeavors. However, in the case of Google, some caution is warranted, as the premature claims in 2019 may have had significant consequences. For example, following the 2019 “supremacy” announcement, <a href="https://gilkalai.wordpress.com/wp-content/uploads/2022/08/bc666.png">the value of Bitcoin dropped</a> (around October 24, 2019, after a period of stability) from roughly $9,500 to roughly $8,500 in just a few days, representing a loss for investors of more than ten billion dollars. (The value today is around $100,000.) Additionally, Google’s assertions may have imposed unrealistic challenges on other quantum computing efforts and encouraged a culture of undesirable scientific methodologies.</p>
<p><a href="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png"><img data-attachment-id="27644" data-permalink="https://gilkalai.wordpress.com/2024/12/09/the-case-against-googles-claims-of-quantum-supremacy-a-very-short-introduction/bnp5/" data-orig-file="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png" data-orig-size="1287,821" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="BNP5" data-image-description="" data-image-caption="" data-medium-file="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png?w=300" data-large-file="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png?w=640" src="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png" alt="BNP5" width="446" height="284" srcset="https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png?w=446&amp;h=284 446w, https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png?w=890&amp;h=568 890w, https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png?w=150&amp;h=96 150w, https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png?w=300&amp;h=191 300w, https://gilkalai.wordpress.com/wp-content/uploads/2024/12/bnp5.png?w=768&amp;h=490 768w" sizes="(max-width: 446px) 100vw, 446px"></a></p>
<p><span>Sergio Boixo, Hartmut Neven, and John Preskill in a video</span> <a href="https://youtu.be/l_KrC1mzd0g?si=3BBjgIRHlscg_phC">“Quantum next leap: Ten septillions years beyond-classic”</a></p>
<p><strong>I) Update (Dec. 10): The Wind in the Willow<br></strong><br>Yesterday, Google Quantum AI <a href="https://blog.google/technology/research/google-willow-quantum-chip/">announced</a> that their “Willow” quantum computer “performed a standard benchmark computation in under five minutes that would take one of today’s fastest supercomputers 10 septillion (that is, 10^25) years.” As far as I know there is no paper with the details. Google AI team announced also the appearance in <em>Nature</em> of their recent paper on distance-5 and distance-7 surface codes. It is asserted that the distance-7 codes exhibit an improvement of a factor of 2.4 compared to the physical qubits. The ratio of improvement Λ from distance-5 to distance-7 is 2.14. (We mentioned it in an August post following a <a href="https://gilkalai.wordpress.com/2024/08/21/five-perspectives-on-quantum-supremacy/#comment-99295">comment</a> by phan ting.)</p>
<p>We did not study yet these particular claims by Google Quantum AI, but my general conclusion apply to them “Google Quantum AI’s claims (including published ones) should be approached with caution, particularly those of an extraordinary nature. These claims may stem from significant methodological errors and, as such, may reflect the researchers’ expectations more than objective scientific reality.” (Our specific contention points are relevant to Google’s newer supremacy experiments but not directly to the quantum error-correction experiment.) </p>
<p>There is a nice very positive <a href="https://scottaaronson.blog/?p=8525">blog post over SO</a> about the new developments where Scott wrote: “besides the new and more inarguable Google result, IBM, Quantinuum, QuEra, and USTC have now all also reported Random Circuit Sampling experiments with good results.” For me, the gap between Google and IBM for RCS is a serious additional reason not to take the Google assertions seriously (item D) and and if I am wrong I will gladly stand corrected.</p>



											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Alzheimer's study shows ketone bodies help clear misfolded proteins (172 pts)]]></title>
            <link>https://www.genengnews.com/topics/translational-medicine/ketone-body-role-in-regulating-misfolded-proteins-may-inform-strategies-targeting-aging-alzheimers-disease/</link>
            <guid>42383840</guid>
            <pubDate>Wed, 11 Dec 2024 01:48:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.genengnews.com/topics/translational-medicine/ketone-body-role-in-regulating-misfolded-proteins-may-inform-strategies-targeting-aging-alzheimers-disease/">https://www.genengnews.com/topics/translational-medicine/ketone-body-role-in-regulating-misfolded-proteins-may-inform-strategies-targeting-aging-alzheimers-disease/</a>, See on <a href="https://news.ycombinator.com/item?id=42383840">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Ketone bodies are produced by the body to provide fuel during fasting, and are thought to have roles in regulating cellular processes and aging mechanisms beyond energy production. Research by Buck Institute scientists, including experiments in the nematode <em>Caenorhabditis elegans</em>, and in mouse models, provides what they suggest is a direct molecular mechanism for the regulation of misfolded proteins by ketone bodies and related metabolites. The results, they said, indicate that ketone bodies, including β-hydroxybutyrate (βHB), may be considered powerful signaling metabolites affecting brain function in aging and Alzheimer’s disease (AD). The findings also point to potential metabolism-related mechanistic targets for therapeutic development in aging and in AD.</p>
<p>Reporting on their work in <em>Cell Chemical Biology</em> (“<a href="http://dx.doi.org/10.1016/j.chembiol.2024.11.001" target="_blank" rel="noopener">β-hydroxybutyrate is a metabolic regulator of proteostasis in the aged and Alzheimer disease brain</a>”), senior author John Newman, MD, PhD, a Buck Institute assistant professor, and colleagues stated, “Here, we provide a direct molecular mechanism for the regulation of misfolded proteins by ketone bodies and related metabolites … Together, these data provide foundational evidence for a novel mechanistic component of ketone body biology.”</p>
<p>Ketone bodies are a class of lipid-derived small molecule metabolites that include acetone, acetoacetate, and (R)-β-hydroxybutyrate (R-βHB), the authors noted. “The primary function of acetoacetate and R-βHB production is to provide cellular energy to extrahepatic tissues during periods of reduced&nbsp;glucose availability, such as fasting, starvation, high-intensity exercise, and ketogenic diet.”</p>
<p>Previous studies have shown that boosting ketone bodies through diet, exercise, and supplementation can be good for brain health and cognition, both in rodents and humans. “There is clear preclinical literature support, and early clinical data, for ketogenic therapies in aging and AD. … ketogenic diet and exogenous ketones have been shown to improve cognitive and motor behavior in several mouse models of AD,” the authors stated. “Early human studies of ketogenic compounds have improved cognitive scores in patients with mild to moderate AD.”</p>
<p>The researcher’s newly reported work demonstrated that ketone bodies and similar metabolites have profound effects on the proteome and protein quality control in the brain. Working in cells, in mouse models of AD and aging, and in the model organism <em>C. elegans,</em>&nbsp;the findings indicated that the ketone body β-hydroxybutyrate (βHB) interacts directly with misfolded proteins, altering their solubility and structure so they can be cleared from the brain through the process of autophagy.</p>
<p>In addition to testing the changing solubility and structure of proteins in test tubes, the researchers also studied the effects of ketone bodies in model organisms. To assess whether the solubility changes caused by ketone bodies helped improve models of pathological aggregation, the&nbsp;investigators fed ketone bodies to nematode worms that were genetically modified to express the human equivalent of amyloid beta, which causes amyloid plaques. “The amyloid beta affects muscles and paralyzes the worms,” said Sidharth Madhavan, a PhD candidate and lead author on the study. “Once they were treated with ketone bodies the animals recovered their ability to swim. It was really exciting to see such a dramatic impact in a whole animal.”</p>
<p>When the team fed a ketone ester to mice, they found that the ketone ester treatment resulted in clearance rather than pathological aggregation of insoluble proteins. The investigators in addition generated detailed proteome-wide solubility maps from their experiments in test tubes and from their mouse experiments.</p>
<figure id="attachment_305108" aria-describedby="caption-attachment-305108"><img fetchpriority="high" decoding="async" src="https://www.genengnews.com/wp-content/uploads/2024/12/low-res-300x300.jpeg" alt="B-hydroxybutyrate is a metabolic regulator of proteostasis in the aged and Alzheimer’s disease brain. [Sid Madhavan, Buck Institute for Research on Aging]" width="300" height="300" srcset="https://www.genengnews.com/wp-content/uploads/2024/12/low-res-300x300.jpeg 300w, https://www.genengnews.com/wp-content/uploads/2024/12/low-res-150x150.jpeg 150w, https://www.genengnews.com/wp-content/uploads/2024/12/low-res-420x420.jpeg 420w, https://www.genengnews.com/wp-content/uploads/2024/12/low-res-696x696.jpeg 696w, https://www.genengnews.com/wp-content/uploads/2024/12/low-res-600x600.jpeg 600w, https://www.genengnews.com/wp-content/uploads/2024/12/low-res.jpeg 700w" sizes="(max-width: 300px) 100vw, 300px"><figcaption id="caption-attachment-305108">B-hydroxybutyrate is a metabolic regulator of proteostasis in the aged and Alzheimer’s disease brain. [Sid Madhavan, Buck Institute for Research on Aging]</figcaption></figure><p>Newman noted an existing theory that the ketone body-based improvements are caused by increased energy to the brain or a reduction in brain inflammation, with reported improvements in amyloid plaques in mouse models being an indirect by-product. “Now we know that’s not the whole story,” he said. “Ketone bodies interact with damaged and misfolded proteins directly, making them insoluble so they can be pulled from the cell and recycled.”</p>
<p>Newman said the study highlights a new form of metabolic regulation of protein quality control. “This is not just about ketone bodies,” he said. “We tested similar metabolites in test tubes and a bunch of them had similar effects. In some cases, they performed better than β-hydroxybutyrate. It’s beautiful to imagine that changing metabolism results in this symphony of molecules cooperating together to improve brain function.”</p>
<p>While acknowledging that other mechanisms like energy supply are also important to brain health, Newman calls the discovery new biology. “It’s a new link between metabolism in general, ketone bodies, and aging,” he said. “Directly linking changes in a cell’s metabolic state to changes in the proteome is really exciting.”</p>
<p>Given that proteostatic mechanisms such as autophagy are known to be activated by nutrient deprivation, the authors noted, it’s not surprising that evolutionary pressures would encourage the clearance of pathogenic proteins during ketosis to promote cellular health in organisms needing additional substrate for ATP production. “In this situation, ketone bodies are janitors of damaged proteins, chaperoning away molecular waste so organisms can operate at peak molecular fitness,” they pointed out.</p>
<p>Noting that ketone bodies are easy to manipulate experimentally and therapeutically, Newman added, “This might be a powerful avenue to assist with global clearing of damaged proteins. We’re just scratching the surface as to how this might be applied to brain aging and neurodegenerative disease.”</p>
<p>In their paper, the team further concluded, “We show that βHB-induced insolubility leads to clearance of highly insolubilized proteins in vivo, likely via βHB communication with cellular protein degradation pathways. This work identifies βHB as a global regulator of cytosolic protein solubility, and identifies new metabolism-related mechanistic targets for therapeutic development in aging and AD.”</p>
<p>Madhavan is now pursuing whether ketone bodies and related metabolites have similar effects outside the brain, such as in the gut, and suggests that the key next step will be to test this new protein quality control mechanism in people to help guide how best to apply it therapeutically.</p>
		    </div></div>]]></description>
        </item>
    </channel>
</rss>