<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 01 Sep 2024 12:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Founder Mode (164 pts)]]></title>
            <link>https://paulgraham.com/foundermode.html</link>
            <guid>41415023</guid>
            <pubDate>Sun, 01 Sep 2024 07:35:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://paulgraham.com/foundermode.html">https://paulgraham.com/foundermode.html</a>, See on <a href="https://news.ycombinator.com/item?id=41415023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div width="435"><tbody><tr><td><img src="https://s.turbifycdn.com/aah/paulgraham/founder-mode-1.gif" width="118" height="18" alt="Founder Mode"><span size="2" face="verdana">September 2024<p>At a YC event last week Brian Chesky gave a talk that everyone who
was there will remember. Most founders I talked to afterward said
it was the best they'd ever heard. Ron Conway, for the first time
in his life, forgot to take notes. I'm not going to try to reproduce
it here. Instead I want to talk about a question it raised.</p><p>The theme of Brian's talk was that the conventional wisdom about
how to run larger companies is mistaken. As Airbnb grew, well-meaning
people advised him that he had to run the company in a certain way
for it to scale. Their advice could be optimistically summarized
as "hire good people and give them room to do their jobs." He
followed this advice and the results were disastrous. So he had to
figure out a better way on his own, which he did partly by studying
how Steve Jobs ran Apple. So far it seems to be working. Airbnb's
free cash flow margin is now among the best in Silicon Valley.</p><p>The audience at this event included a lot of the most successful
founders we've funded, and one after another said that the same
thing had happened to them. They'd been given the same advice about
how to run their companies as they grew, but instead of helping
their companies, it had damaged them.</p><p>Why was everyone telling these founders the wrong thing? That was
the big mystery to me. And after mulling it over for a bit I figured
out the answer: what they were being told was how to run a company
you hadn't founded — how to run a company if you're merely a
professional manager. But this m.o. is so much less effective that
to founders it feels broken. There are things founders can do that
managers can't, and not doing them feels wrong to founders, because
it is.</p><p>In effect there are two different ways to run a company: founder
mode and manager mode. Till now most people even in Silicon Valley
have implicitly assumed that scaling a startup meant switching to
manager mode. But we can infer the existence of another mode from
the dismay of founders who've tried it, and the success of their
attempts to escape from it.</p><p>There are as far as I know no books specifically about founder mode.
Business schools don't know it exists. All we have so far are the
experiments of individual founders who've been figuring it out for
themselves. But now that we know what we're looking for, we can
search for it. I hope in a few years founder mode will be as well
understood as manager mode. We can already guess at some of the
ways it will differ.</p><p>The way managers are taught to run companies seems to be like modular
design in the sense that you treat subtrees of the org chart as
black boxes. You tell your direct reports what to do, and it's up
to them to figure out how. But you don't get involved in the details
of what they do. That would be micromanaging them, which is bad.</p><p>Hire good people and give them room to do their jobs. Sounds great
when it's described that way, doesn't it? Except in practice, judging
from the report of founder after founder, what this often turns out
to mean is: hire professional fakers and let them drive the company
into the ground.</p><p>One theme I noticed both in Brian's talk and when talking to founders
afterward was the idea of being gaslit. Founders feel like they're
being gaslit from both sides — by the people telling them they
have to run their companies like managers, and by the people working
for them when they do. Usually when everyone around you disagrees
with you, your default assumption should be that you're mistaken.
But this is one of the rare exceptions. VCs who haven't been founders
themselves don't know how founders should run companies, and C-level
execs, as a class, include some of the most skillful liars in the
world.
</p><span color="#dddddd">[<a href="#f1n"><span color="#dddddd">1</span></a>]</span><p>Whatever founder mode consists of, it's pretty clear that it's going
to break the principle that the CEO should engage with the company
only via his or her direct reports. "Skip-level" meetings will
become the norm instead of a practice so unusual that there's a
name for it. And once you abandon that constraint there are a huge
number of permutations to choose from.</p><p>For example, Steve Jobs used to run an annual retreat for what he
considered the 100 most important people at Apple, and these were
not the 100 people highest on the org chart. Can you imagine the
force of will it would take to do this at the average company? And
yet imagine how useful such a thing could be. It could make a big
company feel like a startup. Steve presumably wouldn't have kept
having these retreats if they didn't work. But I've never heard of
another company doing this. So is it a good idea, or a bad one? We
still don't know. That's how little we know about founder mode.
</p><span color="#dddddd">[<a href="#f2n"><span color="#dddddd">2</span></a>]</span><p>Obviously founders can't keep running a 2000 person company the way
they ran it when it had 20. There's going to have to be some amount
of delegation. Where the borders of autonomy end up, and how sharp
they are, will probably vary from company to company. They'll even
vary from time to time within the same company, as managers earn
trust. So founder mode will be more complicated than manager mode.
But it will also work better. We already know that from the examples
of individual founders groping their way toward it.</p><p>Indeed, another prediction I'll make about founder mode is that
once we figure out what it is, we'll find that a number of individual
founders were already most of the way there — except that in doing
what they did they were regarded by many as eccentric or worse.
</p><span color="#dddddd">[<a href="#f3n"><span color="#dddddd">3</span></a>]</span><p>Curiously enough it's an encouraging thought that we still know so
little about founder mode. Look at what founders have achieved
already, and yet they've achieved this against a headwind of bad
advice. Imagine what they'll do once we can tell them how to run
their companies like Steve Jobs instead of John Sculley.</p><p><b>Notes</b></p><p>[</p><a name="f1n"><span color="#000000">1</span></a>]
The more diplomatic way of phrasing this statement would be
to say that experienced C-level execs are often very skilled at
managing up. And I don't think anyone with knowledge of this world
would dispute that.<p>[</p><a name="f2n"><span color="#000000">2</span></a>]
If the practice of having such retreats became so widespread
that even mature companies dominated by politics started to do it,
we could quantify the senescence of companies by the average depth
on the org chart of those invited.<p>[</p><a name="f3n"><span color="#000000">3</span></a>]
I also have another less optimistic prediction: as soon as
the concept of founder mode becomes established, people will start
misusing it. Founders who are unable to delegate even things they
should will use founder mode as the excuse. Or managers who aren't
founders will decide they should try act like founders. That may
even work, to some extent, but the results will be messy when it
doesn't; the modular approach does at least limit the damage a bad
CEO can do.<span color="888888"><b>Thanks</b> to Brian Chesky, Patrick Collison, 
Ron Conway, Jessica
Livingston, Elon Musk, Ryan Petersen, Harj Taggar, and Garry Tan
for reading drafts of this.</span></span></td></tr></tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Playstation 2 GS emulation – the final frontier of Vulkan compute emulation (133 pts)]]></title>
            <link>https://themaister.net/blog/2024/07/03/playstation-2-gs-emulation-the-final-frontier-of-vulkan-compute-emulation/</link>
            <guid>41413662</guid>
            <pubDate>Sun, 01 Sep 2024 02:14:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://themaister.net/blog/2024/07/03/playstation-2-gs-emulation-the-final-frontier-of-vulkan-compute-emulation/">https://themaister.net/blog/2024/07/03/playstation-2-gs-emulation-the-final-frontier-of-vulkan-compute-emulation/</a>, See on <a href="https://news.ycombinator.com/item?id=41413662">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-754">
	
	<!-- .entry-header -->

	<div>
		<p>As you may, or may not know, I wrote paraLLEl-RDP back in 2020. It aimed at implementing the N64 RDP in Vulkan compute. Lightning fast, and extremely accurate, plus the added support of up-scaling on top. I’m quite happy how it turned out. Of course, the extreme accuracy was due to Angrylion being used as reference and I could aim for bit-exactness against that implementation.</p>
<p>Since then, there’s been the lingering idea of doing the same thing, but for PlayStation 2. Until now, there’s really only been one implementation in town, GSdx, which has remained the state-of-the-art for 20 years.</p>
<p><a href="https://github.com/Arntzen-Software/parallel-gs">paraLLEl-GS</a> is actually not the first compute implementation of the PS2 GS. An attempt was made back in 2014 for OpenCL as far as I recall, but it was never completed. At the very least, I cannot find it in the current upstream repo anymore.</p>
<p>The argument for doing compute shader raster on PS2 is certainly weaker than on N64. Angrylion was – and is – extremely slow, and N64 is extremely sensitive to accuracy where hardware acceleration with graphics APIs is impossible without serious compromises. PCSX2 on the other hand has a well-optimized software renderer, and a pretty solid graphics-based renderer, but that doesn’t mean there aren’t issues. The software renderer does not support up-scaling for example, and there are a myriad bugs and glitches with the graphics-based renderer, especially with up-scaling. As we’ll see, the PS2 GS is quite the nightmare to emulate in its own way.</p>
<p>My main motivation here is basically “because I can”. I already had a project lying around that did “generic” compute shader rasterization. I figured that maybe we could retro-fit this to support PS2 rendering.</p>
<p>I didn’t work on this project alone. My colleague, Runar Heyer, helped out a great deal in the beginning to get this started, doing all the leg-work to study the PS2 from various resources, doing the initial prototype implementation and fleshing out the Vulkan GLSL to emulate PS2 shading. Eventually, we hit some serious roadblocks in debugging various games, and the project was put on ice for a while since I was too drained dealing with horrible D3D12 game debugging day in and day out. The last months haven’t been a constant fire fight, so I’ve finally had the mental energy to finish it.</p>
<p>My understanding of the GS is mostly based on what Runar figured out, and what I’ve seen by debugging games. The GSdx software renderer does not seem like it’s hardware bit-accurate, so we were constantly second-guessing things when trying to compare output. This caused a major problem when we had the idea of writing detailed tests that directly compared against GSdx software renderer, and the test-driven approach fell flat very quickly. As a result, paraLLEl-GS isn’t really aiming for bit-accuracy against hardware, but it tries hard to avoid obvious accuracy issues at the very least.</p>
<h2>Basic GS overview</h2>
<p>Again, this is based on my understanding, and it might not be correct. 😀</p>
<h3>GS is a pixel-pushing monster</h3>
<p>The GS is infamous for its insane fill-rate and bandwidth. It could push over a billion pixels per second (in theory at least) back in 2000 which was nuts. While the VRAM is quite small (4 MiB), it was designed to be continuously streamed into using the various DMA engines.</p>
<p>Given the extreme fill-rate requirements, we have to design our renderer accordingly.</p>
<h3>GS pixel pipeline is very basic, but quirky</h3>
<p>In many ways, the GS is actually simpler than N64 RDP. Single texture, and a single cycle combiner, where N64 had a two stage combiner + two stage blender. Whatever AA support is there is extremely basic as well, where N64 is delightfully esoteric. The parts of the pixel pipeline that is painful to implement with traditional graphics APIs is:</p>
<h4>Blending goes beyond 1.0</h4>
<p>Inherited from PS1, 0x80 is treated as 1.0, and it can go all the way up to 0xff (almost 2). Shifting by 7 is easier than dividing by 255 I suppose. I’ve seen some extremely ugly workarounds in PCSX2 before to try working around this since UNORM formats cannot support this as is. Textures are similar, where alpha &gt; 1.0 is representable.</p>
<p>There is also wrapping logic that can be used for when colors or alpha goes above 0xFF.</p>
<h4>Destination alpha testing</h4>
<p>The destination alpha can be used as a pseudo-stencil of sorts, and this is extremely painful without programmable blending. I suspect this was added as PS1 compatibility, since PS1 also had this strange feature.</p>
<h4>Conditional blending</h4>
<p>Based on the alpha, it’s possible to conditionally disable blending. Quite awkward without programmable blending … This is another PS1 compat feature. With PS1, it can be emulated by rendering every primitive twice with state changes in-between, but this quickly gets impractical with PS2.</p>
<h4>Alpha correction</h4>
<p>Before alpha is written out, it’s possible to OR in the MSB. Essentially forcing alpha to 1. It is not equivalent to alphaToOne however, since it’s a bit-wise OR of the MSB.</p>
<h4>Alpha test can partially discard</h4>
<p>A fun thing alpha tests can do is to partially discard. E.g. you can discard just color, but keep the depth write. Quite nutty.</p>
<h4>AA1 – coverage-to-alpha – can control depth write per pixel</h4>
<p>This is also kinda awkward. The only anti-alias PS2 has is AA1 which is a coverage-to-alpha feature. Supposedly, less than 100% coverage should disable depth writes (and blending is enabled), but the GSdx software renderer behavior here is extremely puzzling. I don’t really understand it yet.</p>
<h4>32-bit fixed-point Z</h4>
<p>I’ve still yet to see any games actually using this, but technically, it has D32_UINT support. Fun! From what I could grasp, GSdx software renderer implements this with FP64 (one of the many reasons I refuse to believe GSdx is bit-accurate), but FP64 is completely impractical on GPUs. When I have to, I’ll implement this with fixed-point math. 24-bit Z and 16-bit should be fine with FP32 interpolation I think.</p>
<h4>Pray you have programmable blending</h4>
<p>If you’re on a pure TBDR GPU most of this is quite doable, but immediate mode desktop GPUs quickly degenerates into ROV or per-pixel barriers after every primitive to emulate programmable blending, both which are horrifying for performance. Of course, with compute we can make our own TBDR to bypass all this. 🙂</p>
<h3>D3D9-style raster rules</h3>
<p>Primitives are fortunately provided in a plain form in clip-space. No awkward N64 edge equations here. The VU1 unit is supposed to do transforms and clipping, and emit various per-vertex attributes:</p>
<p>X/Y: 12.4 unsigned fixed-point<br>
Z: 24-bit or 32-bit uint<br>
FOG: 8-bit uint<br>
RGBA: 8-bit, for per-vertex lighting<br>
STQ: For perspective correct texturing with normalized coordinates. Q = 1 / w, S = s * Q, T = t * Q. Apparently the lower 8-bits of the mantissa are clipped away, so bfloat24? Q can be negative, which is always fun. No idea how this interacts with Inf and NaN …<br>
UV: For non-perspective correct texturing. 12.4 fixed-point un-normalized.</p>
<ul>
<li>Triangles are top-left raster, just like modern GPUs.</li>
<li>Pixel center is on integer coordinate, just like D3D9. (This is a common design mistake that D3D10+ and GL/Vulkan avoids).</li>
<li>Lines use Bresenham’s algorithm, which is not really feasible to upscale, so we have to fudge it with rect or parallelogram.</li>
<li>Points snap to nearest pixel. Unsure which rounding is used though … There is no interpolation ala gl_PointCoord.</li>
<li>Sprites are simple quads with two coordinates. STQ or UV can be interpolated and it seems to assume non-rotated coordinates. To support rotation, you’d need 3 coordinates to disambiguate.</li>
</ul>
<p>All of this can be implemented fairly easily in normal graphics APIs, as long as we don’t consider upscaling. We have to rely on implementation details in GL and Vulkan, since these APIs don’t technically guarantee top-left raster rules.</p>
<p>Since X/Y is unsigned, there is an XY offset that can be applied to center the viewport where you want. This means the effective range of X/Y is +/- 4k pixels, a healthy guard band for 640×448 resolutions.</p>
<h3>Vertex queue</h3>
<p>The GS feels very much like old school OpenGL 1.0 with glVertex3f and friends. It even supports TRIANGLE_FAN! Amazing … RGBA, STQ and various registers are set, and every XYZ register write forms a vertex “kick” which latches vertex state and advances the queue. An XYZ register write may also be a drawing kick, which draws a primitive if the vertex queue is sufficiently filled. The vertex queue is managed differently depending on the topology. The semantics here seem to be pretty straight forward where strip primitives shift the queue by one, and list primitives clear the queue. Triangle fans keep the first element in the queue.</p>
<h3>Fun swizzling formats</h3>
<p>A clever idea is that while rendering to 24-bit color or 24-bit depth, there is 8 bits left unused in the MSBs. You can place textures there, because why not. 8H, 4HL, 4HH formats support 8-bit and 4-bit palettes nicely.</p>
<p>Pixel coordinates on PS2 are arranged into “pages”, which are 8 KiB, then subdivided into 32 blocks, and then, the smaller blocks are swizzled into a layout that fits well with a DDA-style renderer. E.g. for 32-bit RGBA, a page is 64×32 pixels, and 32 8×8 blocks are Z-order swizzled into that page.</p>
<h3>Framebuffer cache and texture cache</h3>
<p>There is a dedicated cache for framebuffer rendering and textures, one page’s worth. Games often abuse this to perform feedback loops, where they render on top of the pixels being sampled from. This is the root cause of extreme pain. N64 avoided this problem by having explicit copies into TMEM (and not really having the bandwidth to do elaborate feedback effects), and other consoles rendered to embedded SRAM (ala a tiler GPU), so these feedbacks aren’t as painful, but the GS is complete YOLO. Dealing with this gracefully is probably the biggest challenge. Combined with the PS2 being a bandwidth monster, developers knew how to take advantage of copious blending and blurring passes …</p>
<h3>Texturing</h3>
<p>Texturing on the GS is both very familar, and arcane.</p>
<p>On the plus side, the texel-center is at half-pixel, just like modern APIs. It seems like it has 4-bit sub-texel precision instead of 8 however. This is easily solved with some rounding. It also seems to have floor-rounding instead of nearest-rounding for bi-linear.</p>
<p>The bi-linear filter is a normal bi-linear. No weird 3-point N64 filter here.</p>
<p>On the weirder side, there are two special addressing modes.</p>
<p>REGION_CLAMP supports an arbitrary clamp inside a texture atlas (wouldn’t this be nice in normal graphics APIs? :D). It also works with REPEAT, so you can have REPEAT semantics on border, but then clamp slightly into the next “wrap”. This is trivial to emulate.</p>
<p>REGION_REPEAT is … worse. Here we can have custom bit-wise computation per coordinate. So something like u’ = (u &amp; MASK) | FIX. This is done per-coordinate in bi-linear filtering, which is … painful, but solvable. This is another weird PS1 feature that was likely inherited for compatibility. At least on PS1, there was no bi-linear filtering to complicate things 🙂</p>
<p>Mip-mapping is also somewhat curious. Rather than relying on derivatives, the log2 of interpolated Q factor, along with some scaling factors are used to compute the LOD. This is quite clever, but I haven’t really seen any games use it. The down-side is that triangle-setup becomes rather complex if you want to account for correct tri-linear filtering, and it cannot support e.g. anisotropic filtering, but this is 2000, who cares! Not relying on derivatives is a huge boon for the compute implementation.</p>
<p>Formats are always “normalized” to RGBA8_UNORM. 5551 format is expanded to 8888 without bit-replication. There is no RGBA4444 format.</p>
<p>It’s quite feasible to implement the texturing with plain bindless.</p>
<h3>CLUT</h3>
<p>This is a 1 KiB cache that holds the current palette. There is an explicit copy step from VRAM into that CLUT cache before it can be used. Why hello there, N64 TMEM!</p>
<p>The CLUT is organized such that it can hold one full 256 color palette in 32-bit colors. On the other end, it can hold 32 palettes of 16 colors at 16 bpp.</p>
<h3>TEXFLUSH</h3>
<p>There is an explicit command that functions like a “sync and invalidate texture cache”. In the beginning I was hoping to rely on this to guide the hazard tracking, but oh how naive I was. In the end, I simply had to ignore TEXFLUSH. Basically, there are two styles of caching we could take with GS.</p>
<p>With “maximal” caching, we can assume that frame buffer caches and texture caches are infinitely large. The only way a hazard needs to be considered is after an explicit flush. This … breaks hard. Either games forget to use TEXFLUSH (because it happened to work on real hardware), or they TEXFLUSH way too much.</p>
<p>With “minimal” caching, we assume there is no caching and hazards are tracked directly. Some edge case handling is considered for feedback loops.</p>
<p>I went with “minimal”, and I believe GSdx did too.</p>
<h3>Poking registers with style – GIF</h3>
<p>The way to interact with the GS hardware is through the GIF, which is basically a unit that reads data and pokes the correct hardware registers. At the start of a GIF packet, there is a header which configures which registers should be written to, and how many “loops” there are. This maps very well to mesh rendering. We can consider something like one “loop” being:</p>
<ul>
<li>Write RGBA vertex color</li>
<li>Write texture coordinate</li>
<li>Write position with draw kick</li>
</ul>
<p>And if we have 300 vertices to render, we’d use 300 loops. State registers can be poked through the Address + Data pair, which just encodes target register + 64-bit payload. It’s possible to render this way too of course, but it’s just inefficient.</p>
<p>Textures are uploaded through the same mechanism. Various state registers are written to set up transfer destinations, formats, etc, and a special register is nudged to transfer 64-bit at a time to VRAM.</p>
<h2>Hello Trongle – GS</h2>
<p>If you missed the brain-dead simplicity of OpenGL 1.0, this is the API for you! 😀</p>
<p>For testing purposes, I added a tool to generate a .gs dump format that PCSX2 can consume. This is handy for comparing implementation behavior.</p>
<p>First, we program the frame buffer and scissor:</p>
<pre>TESTBits test = {};
test.ZTE = TESTBits::ZTE_ENABLED;
test.ZTST = TESTBits::ZTST_GREATER; // Inverse Z, LESS is not supported.
iface.write_register(RegisterAddr::TEST_1, test);

FRAMEBits frame = {};
frame.FBP = 0x0 / PAGE_ALIGNMENT_BYTES;
frame.PSM = PSMCT32;
frame.FBW = 640 / BUFFER_WIDTH_SCALE;
iface.write_register(RegisterAddr::FRAME_1, frame);

ZBUFBits zbuf = {};
zbuf.ZMSK = 0; // Enable Z-write
zbuf.ZBP = 0x118000 / PAGE_ALIGNMENT_BYTES;
iface.write_register(RegisterAddr::ZBUF_1, zbuf);

SCISSORBits scissor = {};
scissor.SCAX0 = 0;
scissor.SCAY0 = 0;
scissor.SCAX1 = 640 - 1;
scissor.SCAY1 = 448 - 1;
iface.write_register(RegisterAddr::SCISSOR_1, scissor);</pre>
<p>Then we nudge some registers to draw:</p>
<pre>struct Vertex
{
    PackedRGBAQBits rgbaq;
    PackedXYZBits xyz;
} vertices[3] = {};

for (auto &amp;vert : vertices)
{
   vert.rgbaq.A = 0x80;
   vert.xyz.Z = 1;
}

vertices[0].rgbaq.R = 0xff;
vertices[1].rgbaq.G = 0xff;
vertices[2].rgbaq.B = 0xff;

vertices[0].xyz.X = p0.x &lt;&lt; SUBPIXEL_BITS;
vertices[0].xyz.Y = p0.y &lt;&lt; SUBPIXEL_BITS;
vertices[1].xyz.X = p1.x &lt;&lt; SUBPIXEL_BITS;
vertices[1].xyz.Y = p1.y &lt;&lt; SUBPIXEL_BITS;
vertices[2].xyz.X = p2.x &lt;&lt; SUBPIXEL_BITS;
vertices[2].xyz.Y = p2.y &lt;&lt; SUBPIXEL_BITS;

PRIMBits prim = {};
prim.TME = 0; // Turn off texturing.
prim.IIP = 1; // Interpolate RGBA (Gouraud shading)
prim.PRIM = int(PRIMType::TriangleList);

static const GIFAddr addr[] = { GIFAddr::RGBAQ, GIFAddr::XYZ2 };
constexpr uint32_t num_registers = sizeof(addr) / sizeof(addr[0]);
constexpr uint32_t num_loops = sizeof(vertices) / sizeof(vertices[0]);
iface.write_packed(prim, addr, num_registers, num_loops, vertices);</pre>
<p>This draws a triangle. We provide coordinates directly in screen-space.</p>
<p>And finally, we need to program the CRTC. Most of this is just copy-pasta from whatever games tend to do.</p>
<pre>auto &amp;priv = iface.get_priv_register_state();

priv.pmode.EN1 = 1;
priv.pmode.EN2 = 0;
priv.pmode.CRTMD = 1;
priv.pmode.MMOD = PMODEBits::MMOD_ALPHA_ALP;
priv.smode1.CMOD = SMODE1Bits::CMOD_NTSC;
priv.smode1.LC = SMODE1Bits::LC_ANALOG;
priv.bgcolor.R = 0x0;
priv.bgcolor.G = 0x0;
priv.bgcolor.B = 0x0;
priv.pmode.SLBG = PMODEBits::SLBG_ALPHA_BLEND_BG;
priv.pmode.ALP = 0xff;
priv.smode2.INT = 1;

priv.dispfb1.FBP = 0;
priv.dispfb1.FBW = 640 / BUFFER_WIDTH_SCALE;
priv.dispfb1.PSM = PSMCT32;
priv.dispfb1.DBX = 0;
priv.dispfb1.DBY = 0;
priv.display1.DX = 636; // Magic values that center the screen.
priv.display1.DY = 50; // Magic values that center the screen.
priv.display1.MAGH = 3; // scaling factor = MAGH + 1 = 4 -&gt; 640 px wide.
priv.display1.MAGV = 0;
priv.display1.DW = 640 * 4 - 1;
priv.display1.DH = 448 - 1;

dump.write_vsync(0, iface);
dump.write_vsync(1, iface);</pre>
<p>When the GS is dumped, we can load it up in PCSX2 and voila:</p>
<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/gs_dump.png"><img fetchpriority="high" decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/gs_dump-1024x674.png" alt="" width="660" height="434" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/gs_dump-1024x674.png 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/gs_dump-300x198.png 300w, https://themaister.net/blog/wp-content/uploads/2024/07/gs_dump-768x506.png 768w, https://themaister.net/blog/wp-content/uploads/2024/07/gs_dump-1536x1012.png 1536w, https://themaister.net/blog/wp-content/uploads/2024/07/gs_dump.png 1775w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>And here’s the same .gs dump is played through parallel-gs-replayer with RenderDoc. For debugging, I’ve spent a lot of time making it reasonably convenient. The images are debug storage images where I can store before and after color, depth, debug values for interpolants, depth testing state, etc, etc. It’s super handy to narrow down problem cases. The render pass can be split into 1 or more triangle chunks as needed.</p>
<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/rdoc.png"><img decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/rdoc-1024x596.png" alt="" width="660" height="384" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/rdoc-1024x596.png 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/rdoc-300x175.png 300w, https://themaister.net/blog/wp-content/uploads/2024/07/rdoc-768x447.png 768w, https://themaister.net/blog/wp-content/uploads/2024/07/rdoc-1536x894.png 1536w, https://themaister.net/blog/wp-content/uploads/2024/07/rdoc-2048x1192.png 2048w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>To add some textures, and flex the capabilities of the CRTC a bit, we can try uploading a texture:</p>
<pre>int chan;
auto *buf = stbi_load("/tmp/test.png", &amp;w, &amp;h, &amp;chan, 4);
iface.write_image_upload(0x300000, PSMCT32, w, h, buf,
                         w * h * sizeof(uint32_t));
stbi_image_free(buf);

TEX0Bits tex0 = {};
tex0.PSM = PSMCT32;
tex0.TBP0 = 0x300000 / BLOCK_ALIGNMENT_BYTES;
tex0.TBW = (w + BUFFER_WIDTH_SCALE - 1) / BUFFER_WIDTH_SCALE;
tex0.TW = Util::floor_log2(w - 1) + 1;
tex0.TH = Util::floor_log2(h - 1) + 1;
tex0.TFX = COMBINER_DECAL;
tex0.TCC = 1; // Use texture alpha as blend alpha
iface.write_register(RegisterAddr::TEX0_1, tex0);

TEX1Bits tex1 = {};
tex1.MMIN = TEX1Bits::LINEAR;
tex1.MMAG = TEX1Bits::LINEAR;
iface.write_register(RegisterAddr::TEX1_1, tex1);

CLAMPBits clamp = {};
clamp.WMS = CLAMPBits::REGION_CLAMP;
clamp.WMT = CLAMPBits::REGION_CLAMP;
clamp.MINU = 0;
clamp.MAXU = w - 1;
clamp.MINV = 0;
clamp.MAXV = h - 1;
iface.write_register(RegisterAddr::CLAMP_1, clamp);</pre>
<p>While PS2 requires POT sizes for textures, REGION_CLAMP is handy for NPOT. Super useful for texture atlases.</p>
<pre>struct Vertex
{
    PackedUVBits uv;
    PackedXYZBits xyz;
} vertices[2] = {};

for (auto &amp;vert : vertices)
    vert.xyz.Z = 1;

vertices[0].xyz.X = p0.x &lt;&lt; SUBPIXEL_BITS;
vertices[0].xyz.Y = p0.y &lt;&lt; SUBPIXEL_BITS;
vertices[1].xyz.X = p1.x &lt;&lt; SUBPIXEL_BITS;
vertices[1].xyz.Y = p1.y &lt;&lt; SUBPIXEL_BITS;
vertices[1].uv.U = w &lt;&lt; SUBPIXEL_BITS;
vertices[1].uv.V = h &lt;&lt; SUBPIXEL_BITS;

PRIMBits prim = {};
prim.TME = 1; // Turn on texturing.
prim.IIP = 0;
prim.FST = 1; // Use unnormalized coordinates.
prim.PRIM = int(PRIMType::Sprite);

static const GIFAddr addr[] = { GIFAddr::UV, GIFAddr::XYZ2 };
constexpr uint32_t num_registers = sizeof(addr) / sizeof(addr[0]);
constexpr uint32_t num_loops = sizeof(vertices) / sizeof(vertices[0]);
iface.write_packed(prim, addr, num_registers, num_loops, vertices);</pre>
<p>Here we render a sprite with un-normalized coordinates.</p>
<p>Finally, we use the CRTC to do blending against white background.</p>
<pre>priv.pmode.EN1 = 1;
priv.pmode.EN2 = 0;
priv.pmode.CRTMD = 1;
priv.pmode.MMOD = PMODEBits::MMOD_ALPHA_CIRCUIT1;
priv.smode1.CMOD = SMODE1Bits::CMOD_NTSC;
priv.smode1.LC = SMODE1Bits::LC_ANALOG;
priv.bgcolor.R = 0xff;
priv.bgcolor.G = 0xff;
priv.bgcolor.B = 0xff;
priv.pmode.SLBG = PMODEBits::SLBG_ALPHA_BLEND_BG;
priv.smode2.INT = 1;

priv.dispfb1.FBP = 0;
priv.dispfb1.FBW = 640 / BUFFER_WIDTH_SCALE;
priv.dispfb1.PSM = PSMCT32;
priv.dispfb1.DBX = 0;
priv.dispfb1.DBY = 0;
priv.display1.DX = 636; // Magic values that center the screen.
priv.display1.DY = 50; // Magic values that center the screen.
priv.display1.MAGH = 3; // scaling factor = MAGH + 1 = 4 -&gt; 640 px wide.
priv.display1.MAGV = 0;
priv.display1.DW = 640 * 4 - 1;
priv.display1.DH = 448 - 1;</pre>
<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/vk.png"><img decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/vk-1024x590.png" alt="" width="660" height="380" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/vk-1024x590.png 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/vk-300x173.png 300w, https://themaister.net/blog/wp-content/uploads/2024/07/vk-768x443.png 768w, https://themaister.net/blog/wp-content/uploads/2024/07/vk-1536x886.png 1536w, https://themaister.net/blog/wp-content/uploads/2024/07/vk.png 2005w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>Glorious 256×179 logo 😀</p>
<h2>Implementation details</h2>
<h3>The rendering pipeline</h3>
<p>Before we get into the page tracker, it’s useful to define a rendering pipeline where synchronization is implied between each stage.</p>
<ul>
<li>Synchronize CPU copy of VRAM to GPU. This is mostly unused, but happens for save state load, or similar</li>
<li>Upload data to VRAM (or perform local-to-local copy)</li>
<li>Update CLUT cache from VRAM</li>
<li>Unswizzle VRAM into VkImages that can be sampled directly, and handle palettes as needed, sampling from CLUT cache</li>
<li>Perform rendering</li>
<li>Synchronize GPU copy of VRAM back to CPU. This will be useful for readbacks. Then CPU should be able to unswizzle directly from a HOST_CACHED_BIT buffer as needed</li>
</ul>
<p>This pipeline matches what we expect a game to do over and over:</p>
<ul>
<li>Upload texture to VRAM</li>
<li>Upload palette to VRAM</li>
<li>Update CLUT cache</li>
<li>Draw with texture
<ul>
<li>Trigger unswizzle from VRAM into VkImage if needed</li>
<li>Begins building a “render pass”, a batch of primitives</li>
</ul>
</li>
</ul>
<p>When there are no backwards hazards here, we can happily keep batching and defer any synchronization. This is critical to get any performance out of this style of renderer.</p>
<p>Some common hazards here include:</p>
<h4>Copy to VRAM which was already written by copy</h4>
<p>This is often a false positive, but we cannot track per-byte. This becomes a simple copy barrier and we move on.</p>
<h4>Copy to VRAM where a texture was sampled from, or CLUT cache read from</h4>
<p>Since the GS has a tiny 4 MiB VRAM, it’s very common that textures are continuously streamed in, sampled from, and thrown away. When this is detected, we have to submit all vram copy work, all texture unswizzle work and then begin a new batch. Primitive batches are not disrupted.</p>
<p>This means we’ll often see:</p>
<ul>
<li>Copy xN</li>
<li>Barrier</li>
<li>Unswizzle xN</li>
<li>Barrier</li>
<li>Copy xN</li>
<li>Barrier</li>
<li>Unswizzle xN</li>
<li>Barrier</li>
<li>Rendering</li>
</ul>
<h4>Sample texture that was rendered to</h4>
<p>Similar, but here we need to flush out everything. This basically breaks the render pass and we start another one. Too many of these is problematic for performance obviously.</p>
<h4>Copy to VRAM where rendering happened</h4>
<p>Basically same as sampling textures, this is a full flush.</p>
<p>Other hazards are ignored, since they are implicitly handled by our pipeline.</p>
<h3>Page tracker</h3>
<p>Arguably, the hardest part of GS emulation is dealing with hazards. VRAM is read and written to with reckless abandon and any potential read-after-write or write-after-write hazard needs to be dealt with. We cannot rely on any game doing this for us, since PS2 GS just deals with sync in most cases, and TEXFLUSH is the only real command games will use (or forget to use).</p>
<p>Tracking per byte is ridiculous, so my solution is to first subdivide the 4 MiB VRAM into pages. A page is the unit for frame buffers and depth buffers, so it is the most meaningful place to start.</p>
<h4>PageState</h4>
<p>On page granularity, we track:</p>
<ul>
<li>Pending frame buffer write?</li>
<li>Pending frame buffer read? (read-only depth)</li>
</ul>
<p>Textures and VRAM copies have 256 byte alignment, and to avoid a ton of false positives, we need to track on a per-block basis. There are 32 blocks per page, so a u32 bit-mask is okay.</p>
<ul>
<li>VRAM copy writes</li>
<li>VRAM copy reads</li>
<li>Pending read into CLUT cache or VkImage</li>
<li>Blocks which have been clobbered by any write, on next texture cache invalidate, throw away images that overlap</li>
</ul>
<p>As mentioned earlier, there are also cases where you can render to 24-bit color, while sampling from the upper 8-bits without hazard. We need to optimize for that case too, so there is also:</p>
<ul>
<li>A write mask for framebuffers</li>
<li>A read mask for textures</li>
</ul>
<p>In the example above, FB write mask is 0xffffff and texture cache mask is 0xff000000. No overlap, no invalidate 😀</p>
<p>For host access, there are also timeline semaphore values per page. These values state which sync point to wait for if the host desires mapped read or mapped write access. Mapped write access may require more sync than mapped read if there are pending reads on that page.</p>
<h4>Caching textures</h4>
<p>Every page contains a list of VkImages which have been associated with it. When a page’s textures has been invalidated, the image is destroyed and has to be unswizzled again from VRAM.</p>
<p>There is a one-to-many relationship with textures and pages. A texture may span more than one page, and it’s enough that only one page is clobbered before the texture is invalidated.</p>
<p>Overall, there are a lot of micro-details here, but the important things to note here is that conservative and simple tracking will not work on PS2 games. Tracking at a 256 byte block level and considering write/read masks is critical.</p>
<h4>Special cases</h4>
<p>There are various situations where we may have false positives due to how textures work. Since textures are POT sized, it’s fairly common for e.g. a 512×448 texture of a render target to be programmed as a 512×512 texture. The unused region should ideally be clamped out with REGION_CLAMP, but most games don’t. A render target might occupy those unused pages. As long as the game’s UV coordinates don’t extend into the unused red zone, there are no hazards, but this is very painful to track. We would have to analyze every single primitive to detect if it’s sampling into the red zone.</p>
<p>As a workaround, we ignore any potential hazard in that red zone, and just pray that a game isn’t somehow relying on ridiculous spooky-action-at-a-distance hazards to work in the game’s favor.</p>
<p>There are more spicy special cases, especially with texture sampling feedback, but that will be for later.</p>
<h3>Updating CLUT in a batched way</h3>
<p>Since we want to batch texture uploads, we have to batch CLUT uploads too. To make this work, we have 1024 copies of CLUT, a ring buffer of snapshots.</p>
<p>One workgroup loops through the updates and writes them to an SSBO. I did a similar thing for N64 RDP’s TMEM update, where TMEM was instanced. Fortunately, CLUT update is <strong>far</strong> simpler than TMEM update.</p>
<pre>shared uint tmp_clut[512];

// ...

// Copy from previous instance to allow a
// CLUT entry to be partially overwritten and used later
uint read_index = registers.read_index * CLUT_SIZE_16;
tmp_clut[gl_LocalInvocationIndex] =
    uint(clut16.data[read_index]);
tmp_clut[gl_LocalInvocationIndex + 256u] =
    uint(clut16.data[read_index + 256u]);
barrier();

for (uint i = 0; i &lt; registers.clut_count; i++)
{
  // ...
  if (active_lane)
  {
    // update tmp_clut. If 256 color, all threads participate.
    // 16 color update is a partial update.
  }

  // Flush current CLUT state to SSBO.
  barrier();
  clut16.data[gl_LocalInvocationIndex + clut.instance * CLUT_SIZE_16] =
    uint16_t(tmp_clut[gl_LocalInvocationIndex]);
  clut16.data[gl_LocalInvocationIndex + clut.instance * CLUT_SIZE_16 + 256u] =
    uint16_t(tmp_clut[gl_LocalInvocationIndex + 256u]);
  barrier();
}</pre>
<p>One potential optimization is that for 256 color / 32 bpp updates, we can parallelize the CLUT update, since nothing from previous iterations will be preserved, but the CLUT update time is tiny anyway.</p>
<h3>Unswizzling textures from VRAM</h3>
<p>Since this is Vulkan, we can just allocate a new VkImage, suballocate it from VkDeviceMemory and blast it with a compute shader.</p>
<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/upload.png"><img loading="lazy" decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/upload-1024x576.png" alt="" width="660" height="371" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/upload-1024x576.png 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/upload-300x169.png 300w, https://themaister.net/blog/wp-content/uploads/2024/07/upload-768x432.png 768w, https://themaister.net/blog/wp-content/uploads/2024/07/upload-1536x864.png 1536w, https://themaister.net/blog/wp-content/uploads/2024/07/upload-2048x1152.png 2048w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>Using Vulkan’s specialization constants, we specialize the texture format and all the swizzling logic becomes straight forward code.</p>
<p>REGION_REPEAT shenanigans is also resolved here, so that the ubershader doesn’t have to consider that case and do manual bilinear filtering.</p>
<p>Even for render targets, we roundtrip through the VRAM SSBO. There is not really a point going to the length of trying to forward render targets into textures. Way too many bugs to squash and edge cases to think about.</p>
<h3>Triangle setup and binning</h3>
<p>Like paraLLEl-RDP, paraLLEl-GS is a tile-based renderer. Before binning can happen, we need triangle setup. As inputs, we provide attributes in three arrays.</p>
<h5>Position</h5>
<pre>struct VertexPosition
{
  ivec2 pos;
  float z;     // TODO: Should be uint for 32-bit Z.
  int padding; // Free real-estate?
};</pre>
<h5>Per-Vertex attributes</h5>
<pre>struct VertexAttribute
{
  vec2 st;
  float q;
  uint rgba; // unpackUnorm4x8
  float fog; // overkill, but would be padding anyway
  u16vec2 uv;
};</pre>
<h5>Per-primitive attributes</h5>
<pre>struct PrimitiveAttribute
{
  i16vec4 bb; // Scissor
  // Index into state UBO, as well as misc state bits.
  uint state;
  // Texture state which should be scalarized. Affects code paths.
  // Also holds the texture index (for bindless).
  uint tex;
  // Texture state like lod scaling factors, etc.
  // Does not affect code paths.
  uint tex2;  
  uint alpha; // AFIX / AREF
  uint fbmsk;
  uint fogcol;
};</pre>
<p>For rasterization, we have a straight forward barycentric-based rasterizer. It is heavily inspired by <a href="https://fgiesen.wordpress.com/2011/07/06/a-trip-through-the-graphics-pipeline-2011-part-6/">https://fgiesen.wordpress.com/2011/07/06/a-trip-through-the-graphics-pipeline-2011-part-6/</a>, which in turn is based on <a href="https://www.cs.drexel.edu/~david/Classes/Papers/comp175-06-pineda.pdf">A Parallel Algorithm for Polygon Rasterization (Paneda, 1988)</a> and describes the “standard” way to write a rasterizer with parallel hardware. Of course, the PS2 GS is DDA, i.e. a scanline rasterizer, but in practice, this is just a question of nudging ULPs of precision, and since I’m not aware of a bit-exact description of the GS’s DDA, this is fine. paraLLEl-RDP implements the raw DDA form for example. It’s certainly possible if we <strong>have</strong> to.</p>
<p>As an extension to a straight-forward triangle rasterizer, I also need to support parallelograms. This is used to implement wide-lines and sprites. Especially wide-line is kinda questionable, but I’m not sure it’s possible to fully solve up-scaling + Bresenham in the general case. At least I haven’t run into a case where this really matters.</p>
<p>Evaluating coverage and barycentric I/J turns into something like this:</p>
<pre>bool evaluate_coverage_single(PrimitiveSetup setup,
  bool parallelogram, 
  ivec2 parallelogram_offset,
  ivec2 coord, inout float i, inout float j)
{
  int a = idot3(setup.a, coord);
  int b = idot3(setup.b, coord);
  int c = idot3(setup.c, coord);

  precise float i_result = float(b) * setup.inv_area + setup.error_i;
  precise float j_result = float(c) * setup.inv_area + setup.error_j;
  i = i_result;
  j = j_result;

  if (parallelogram &amp;&amp; a.x &lt; 0)
  {
    b += a + parallelogram_offset.x;
    c += a + parallelogram_offset.y;
    a = 0;
  }

  return all(greaterThanEqual(ivec3(a, b, c), ivec3(0)));
}</pre>
<p>inv_area is computed in a custom fixed-point RCP, which is ~24.0 bit accurate. Using the standard GPU RCP would be bad since it’s just ~22.5 bit accurate and not consistent across implementations. There is no reason to skimp on reproducibility and accuracy, since we’re not doing work per-pixel.</p>
<p>error_i and error_j terms are caused by the downsampling of the edge equations and tie-break rules. As a side effect of the GS’s [-4k, +4k] pixel range, the range of the cross-product requires 33-bit in signed integers. By downsampling a bit, we can get 32-bit integer math to work just fine with 8 sub-pixel accuracy for super-sampling / multi-sampling. Theoretically, this means our upper up-sampling limit is 8×8, but that’s ridiculous anyway, so we’re good here.</p>
<p>The parallelogram offsets are very small numbers meant to nudge the tie-break rules in our favor as needed. The exact details of the implementation escape me. I wrote that code years ago. It’s not very hard to derive however.</p>
<p>Every primitive gets a struct of transformed attributes as well. This is only read if we actually end up shading a primitive, so it’s important to keep this separate to avoid polluting caches with too much garbage.</p>
<pre>struct TransformedAttributes
{
  vec4 stqf0;
  vec4 stqf1;
  vec4 stqf2;
  uint rgba0;
  uint rgba1;
  uint rgba2;
  uint padding;
  vec4 st_bb;
};</pre>
<p>Using I/J like this will lead to small inaccuracies when interpolating primitives which expect to land exactly on the top-left corner of a texel with NEAREST filtering. To combat this, a tiny epsilon offset is used when snapping texture coordinates. Very YOLO, but what can you do. As far as I know, hardware behavior is sub-texel floor, not sub-texel round.</p>
<pre>precise vec2 uv_1 = uv * scale_1;

// Want a soft-floor here, not round behavior.
const float UV_EPSILON_PRE_SNAP = 1.0 / 16.0;
// We need to bias less than 1 / 512th texel, so that linear filter will RTE to correct subpixel.
// This is a 1 / 1024th pixel bias to counter-act any non-POT inv_scale_1 causing a round-down event.
const float UV_EPSILON_POST_SNAP = 16.0 / 1024.0;

if (sampler_clamp_s)
  uv_1.x = texture_clamp(uv_1.x, region_coords.xz, LOD_1);
if (sampler_clamp_t)
  uv_1.y = texture_clamp(uv_1.y, region_coords.yw, LOD_1);

// Avoid micro-precision issues with UV and flooring + nearest.
// Exact rounding on hardware is somwhat unclear.
// SotC requires exact rounding precision and is hit particularly bad.
// If the epsilon is too high, then FF X save screen is screwed over,
// so ... uh, ye.
// We likely need a more principled approach that is actually HW accurate in fixed point.
uv_1 = (floor(uv_1 * 16.0 + UV_EPSILON_PRE_SNAP) + UV_EPSILON_POST_SNAP) *
       inv_scale_1 * 0.0625;</pre>
<h3>Binning</h3>
<p>This is mostly uninteresting. Every NxN pixel block gets an array of u16 primitive indices to shade. This makes the maximum number of primitives per render pass 64k, but that’s enough for PS2 games. Most games I’ve seen so far tend to be between 10k and 30k primitives for the “main” render pass, but I haven’t tested the real juggernauts of primitive grunt yet, but even so, having to do a little bit of incremental rendering isn’t a big deal.</p>
<p>NxN is usually 32×32, but it can be dynamically changed depending on how heavy the geometry load is. For large resolutions and high primitive counts, the binning and memory cost is unacceptable if the resolution is just 16×16 for example. One subgroup is responsible for iterating through all primitives in a block.</p>
<p>Since binning and triangle is state-less, triangle-setup and binning for back-to-back passes are batched up nicely to avoid lots of silly barriers.</p>
<h3>The ubershader</h3>
<p>A key difference between N64 and PS2 is fill-rate and per-pixel complexity. For N64, the ideal approach is to specialize the rasterizing shader, write out per-pixel color + depth + coverage + etc, then merge that data in a much simpler ubershader that only needs to consider depth and blend state rather than full texturing state and combiner state. This is very bandwidth intensive on the GPU, but the alternative is the slowest ubershader written by man. We’re saved by the fact that N64 fill-rate is abysmal. <a href="https://www.youtube.com/watch?v=GC_jLsxZ7nw">Check out this video by Kaze to see how horrible it is</a>.</p>
<p>The GS is a quite different beast. Fill-rate is very high, and per-pixel complexity is fairly low, so a pure ubershader is viable. We can also rely on bindless this time around too, so texturing complexity becomes a fraction of what I had to deal with on N64.</p>
<h4>Fine-grained binning</h4>
<p>Every tile is 4×4, 4×8 and 8×8 for subgroup sizes 16, 32 and 64 respectively. For super-sampling it’s even smaller (it’s 4×4 / 4×8 / 8×8 in the higher resolution domain instead).</p>
<p>In the outer loop, we pull in up to SubgroupSize’s worth of primitives, and bin them in parallel.</p>
<pre>for (int i = 0; i &lt; tile.coarse_primitive_count;
     i += int(gl_SubgroupSize))
{
  int prim_index = i + int(gl_SubgroupInvocationID);
  bool is_last_iteration = i + int(gl_SubgroupSize) &gt;= 
                           tile.coarse_primitive_count;

  // Bin primitives to tile.
  bool binned_to_tile = false;
  uint bin_primitive_index;
  if (prim_index &lt; tile.coarse_primitive_count)
  {
    bin_primitive_index = 
      uint(coarse_primitive_list.data[
           tile.coarse_primitive_list_offset + prim_index]);
    binned_to_tile = primitive_intersects_tile(bin_primitive_index);
  }

  // Iterate per binned primitive, do per pixel work now.
  // Scalar loop.
  uvec4 work_ballot = subgroupBallot(binned_to_tile);</pre>
<p>In the inner loop, we can do a scalarized loop which checks coverage per-pixel, one primitive at a time.</p>
<pre>// Scalar data
uint bit = subgroupBallotFindLSB(work_ballot);

if (gl_SubgroupSize == 64)
{
  if (bit &gt;= 32)
    work_ballot.y &amp;= work_ballot.y - 1;
  else
    work_ballot.x &amp;= work_ballot.x - 1;
}
else
{
  work_ballot.x &amp;= work_ballot.x - 1;
}

shade_primitive_index = subgroupShuffle(bin_primitive_index, bit);</pre>
<h4>Early Z</h4>
<p>We can take advantage of early-Z testing of course, but we have to be careful if there are rasterized pixels we haven’t resolved yet, and there are Z-writes in flight. In this case we have to defer to late Z to perform test.</p>
<pre>// We might have to remove opaque flag.
bool pending_z_write_can_affect_result =
  (pixel.request.z_test || !pixel.request.z_write) &amp;&amp;
  pending_shade_request.z_write;

if (pending_z_write_can_affect_result)
{
  // Demote the pixel to late-Z,
  // it's no longer opaque and we cannot discard earlier pixels.
  // We need to somehow observe the previous results.
  pixel.opaque = false;
}</pre>
<h4>Deferred on-tile shading</h4>
<p>Since we’re an uber-shader, all pixels are “on-chip”, i.e. in registers, so we can take advantage of culling pixels that won’t be visible anyway. The basic idea here is that after rasterization, if a pixel is considered opaque, it will simply replace the shading request that exists for that framebuffer coordinate. It won’t be visible at all anyway.</p>
<h4>Lazy pixel shading</h4>
<p>We only need to perform shading when we really have to, i.e., we’re shading a pixel that depends on the previous pixel’s results. This can happen for e.g. alpha test (if test fails, we preserve existing data), color write masks, or of course, alpha blending.</p>
<p>If our pixel remains opaque, we can just kill the pending pixel shade request. Very nice indeed. The gain here wasn’t as amazing as I had hoped since PS2 games love blending, but it helps culling out a lot of shading work.</p>
<pre>if (pixel.request.coverage &gt; 0)
{
  need_flush = !pixel.opaque &amp;&amp; pending_shade_request.coverage &gt; 0;

  // If there is no hazard, we can overwrite the pending pixel.
  // If not, defer the update until we run a loop iteration.
  if (!need_flush)
  {
    set_pending_shade_request(pixel.request, shade_primitive_index);
    pixel.request.coverage = 0;
    pixel.request.z_write = false;
  }
}</pre>
<p>If we have flushes that need to happen, we do so if one pixel needs it. It’s just as fast to resolve all pixels anyway.</p>
<pre>// Scalar branch
if (subgroupAny(need_flush))
{
  shade_resolve();
  if (has_work &amp;&amp; pixel.request.coverage &gt; 0)
    set_pending_shade_request(pixel.request, shade_primitive_index);
}</pre>
<p>The resolve is a straight forward waterfall loop that stays in uniform control flow to be well defined on devices without maximal reconvergence support.</p>
<pre>while (subgroupAny(has_work))
{
  if (has_work)
  {
    uint state_index =
      subgroupBroadcastFirst(pending_shade_request.state);
    uint tex = subgroupBroadcastFirst(prim_tex);
    if (state_index == pending_shade_request.state &amp;&amp; prim_tex == tex)
    {
      has_work = false;
      shade_resolve(pending_primitive_index, state_index, tex);
    }
  }
}</pre>
<p>This scalarization ensures that all branches on things like alpha test mode, blend modes, etc, are purely scalar, and GPUs like that. Scalarizing on the texture index is technically not that critical, but it means we end up hitting the same branches for filtering modes, UBOs for scaling factors are loaded uniformly, etc.</p>
<p>When everything is done, the resulting framebuffer color and depth is written out to SSBO. GPU bandwidth is kept to a minimum, just like a normal TBDR renderer.</p>
<h3>Super-sampling</h3>
<p>Just implementing single sampled rendering isn’t enough for this renderer to be really useful. The software renderer is certainly quite fast, but not fast enough to keep up with intense super-sampling. We can fix that now.</p>
<p>For e.g. 8x SSAA, we keep 10 versions of VRAM on the GPU.</p>
<ul>
<li>1 copy represents the single-sampled VRAM. It is super-sampled.</li>
<li>1 copy represents the reference value for single-sampled VRAM. This allows us to track when we should discard the super-samples and splat the single sample to all. This can happen if someone copies to VRAM over a render target for whatever reason.</li>
<li>8 copies which each represent the super-samples. Technically, we can reconstruct a higher resolution image from these samples if we really want to, but only the CRTC could easily do that.</li>
</ul>
<p>When rendering super-sampled, we load the single-sampled VRAM and reference. If they match, we load the super-sampled version. This is important for cases where we’re doing incremental rendering.</p>
<p>On tile completion we use clustered subgroup ops to do multi-sample resolve, then write out the super-samples, and the two single-sampled copies.</p>
<pre>uvec4 ballot_color = subgroupBallot(fb_color_dirty);
uvec4 ballot_depth = subgroupBallot(fb_depth_dirty);

// No need to mask, we only care about valid ballot for the
// first sample we write-back.
if (NUM_SAMPLES &gt;= 16)
{
  ballot_color |= ballot_color &gt;&gt; 8u;
  ballot_depth |= ballot_depth &gt;&gt; 8u;
}

if (NUM_SAMPLES &gt;= 8)
{
  ballot_color |= ballot_color &gt;&gt; 4u;
  ballot_depth |= ballot_depth &gt;&gt; 4u;
}

if (NUM_SAMPLES &gt;= 4)
{
  ballot_color |= ballot_color &gt;&gt; 2u;
  ballot_depth |= ballot_depth &gt;&gt; 2u;
}

ballot_color |= ballot_color &gt;&gt; 1u;
ballot_depth |= ballot_depth &gt;&gt; 1u;

// GLSL does not accept cluster reduction as spec constant.
if (NUM_SAMPLES == 16)
  fb_color = packUnorm4x8(subgroupClusteredAdd(
    unpackUnorm4x8(fb_color), 16) / 16.0);
else if (NUM_SAMPLES == 8)
  fb_color = packUnorm4x8(subgroupClusteredAdd(
    unpackUnorm4x8(fb_color), 8) / 8.0);
else if (NUM_SAMPLES == 4)
  fb_color = packUnorm4x8(subgroupClusteredAdd(
    unpackUnorm4x8(fb_color), 4) / 4.0);
else
  fb_color = packUnorm4x8(subgroupClusteredAdd(
    unpackUnorm4x8(fb_color), 2) / 2.0);

fb_color_dirty = subgroupInverseBallot(ballot_color);
fb_depth_dirty = subgroupInverseBallot(ballot_depth);</pre>
<p>The main advantage of super-sampling over straight up-scaling is that up-scaling will still have jagged edges, and super-sampling retains a coherent visual look where 3D elements have similar resolution as UI elements. One of my pet peeves is when UI elements have a significantly different resolution from 3D objects and textures. HD texture packs can of course alleviate that, but that’s a very different beast.</p>
<p>Super-sampling also lends itself very well to CRT post-processing shading, which is also a nice bonus.</p>
<h3>Dealing with super-sampling artifacts</h3>
<p>It’s a fact of life that super-sampling always introduces horrible artifacts if not handled with utmost care. Mitigating this is arguably easier with software renderers over traditional graphics APIs, since we’re not limited by the fixed function interpolators. These tricks won’t make it perfect by any means, but it greatly mitigates jank in my experience, and I already fixed many upscaling bugs that GSdx Vulkan backend does not solve as we shall see later.</p>
<h4>Sprite primitives should always render at single-rate</h4>
<p>Sprites are always UI elements or similar, and games do not expect us to up-scale them. Doing so either results in artifacts where we sample outside the intended rect, or we risk overblurring the image if bilinear filtering is used.</p>
<p>The trick here is just to force-snap the pixel coordinate we use when rasterizing and interpolating. This is very inefficient of course, but UI shouldn’t take up the entire screen. And if it does (like in a menu), the GPU load is tiny anyway.</p>
<pre>const uint SNAP_RASTER_BIT = (1u &lt;&lt; STATE_BIT_SNAP_RASTER);
const uint SNAP_ATTR_BIT = (1u &lt;&lt; STATE_BIT_SNAP_ATTRIBUTE);

if (SUPER_SAMPLE &amp;&amp; (prim_state &amp; SNAP_RASTER_BIT) != 0)
  fb_pixel = tile.fb_pixel_single_rate;

res.request.coverage = evaluate_coverage(
  prim, fb_pixel, i, j,
  res.request.multisample, SAMPLING_RATE_DIM_LOG2);</pre>
<h4>Flat primitives should interpolate at single-pixel coordinate</h4>
<p>Going further, we can demote SSAA interpolation to MSAA center interpolation dynamically. Many UI elements are unfortunately rendered with normal triangles, so we have to be a bit more careful. This snap only affects attribute interpolation, not Z of course.</p>
<pre>res.request.st_bb = false;
if (SUPER_SAMPLE &amp;&amp;
    (prim_state &amp; (SNAP_RASTER_BIT | SNAP_ATTR_BIT)) == SNAP_ATTR_BIT)
{
  vec2 snap_ij = evaluate_barycentric_ij(
    prim.b, prim.c, prim.inv_area,
    prim.error_i, prim.error_j, tile.fb_pixel_single_rate,
    SAMPLING_RATE_DIM_LOG2);

  i = snap_ij.x;
  j = snap_ij.y;
  res.request.st_bb = true;
}</pre>
<p>Here, we snap interpolation to the top-left pixel. This fixes any artifacts for primitives which align their rendering to a pixel center, but some games are mis-aligned, so this snapping can cause texture coordinates to go outside the expected area. To clean this up, we compute a bounding box of final texture coordinates. Adding bounding boxes can technically cause notorious block-edge artifacts, but that was mostly a thing on PS1 since emulators like to convert nearest sampling to bilinear.</p>
<p>The heuristic for this is fairly simple. If perspective is used, if all vertices in a triangle have exact same Q, we assume it’s a flat UI primitive. The primitive’s Z coordinates must also match. This is done during triangle setup on the GPU. There can of course be false positives here, but it should be rare. In my experience this hack works well enough in the games I tried.</p>
<h2>Results</h2>
<p>Here’s a good example of up-sampling going awry in PCSX2. This is with Vulkan backend:</p>
<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/abyss-artifacts.jpg"><img loading="lazy" decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/abyss-artifacts-1024x674.jpg" alt="" width="660" height="434" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/abyss-artifacts-1024x674.jpg 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/abyss-artifacts-300x198.jpg 300w, https://themaister.net/blog/wp-content/uploads/2024/07/abyss-artifacts-768x506.jpg 768w, https://themaister.net/blog/wp-content/uploads/2024/07/abyss-artifacts-1536x1012.jpg 1536w, https://themaister.net/blog/wp-content/uploads/2024/07/abyss-artifacts.jpg 1775w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-glitch.png"><img loading="lazy" decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-glitch-1024x664.png" alt="" width="660" height="428" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-glitch-1024x664.png 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-glitch-300x194.png 300w, https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-glitch-768x498.png 768w, https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-glitch-1536x995.png 1536w, https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-glitch.png 1753w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>Notice the bloom on the glass being mis-aligned and a subtle (?) rectangular pattern being overlaid over the image. This is caused by a post-processing pass rendering in a page-like pattern, presumably to optimize for GS caching behavior.</p>

<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/abyss-gs.png"><img loading="lazy" decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/abyss-gs-1024x687.png" alt="" width="660" height="443" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/abyss-gs-1024x687.png 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/abyss-gs-300x201.png 300w, https://themaister.net/blog/wp-content/uploads/2024/07/abyss-gs-768x515.png 768w, https://themaister.net/blog/wp-content/uploads/2024/07/abyss-gs.png 1280w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>With 8x SSAA in paraLLEl-GS it looks like this instead. There is FSR1 post-upscale in effect here which changes the look a bit, but the usual trappings of bad upscale cannot be observed here. This is another reason to do super-sample; texture mis-alignment has a tendency to fix itself.</p>
<p>Also, if you’re staring at the perf numbers, this is RX 7600 in a low power state :’)</p>
<p>Typical UI issues can be seen in games as well. Here’s native resolution:</p>
<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-ffx.png"><img loading="lazy" decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-ffx-1024x664.png" alt="" width="660" height="428" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-ffx-1024x664.png 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-ffx-300x194.png 300w, https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-ffx-768x498.png 768w, https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-ffx-1536x995.png 1536w, https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-ffx.png 1753w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>and 4x upscale, which … does not look acceptable.</p>
<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-ffx-4x.png"><img loading="lazy" decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-ffx-4x-1024x664.png" alt="" width="660" height="428" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-ffx-4x-1024x664.png 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-ffx-4x-300x194.png 300w, https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-ffx-4x-768x498.png 768w, https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-ffx-4x-1536x995.png 1536w, https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-ffx-4x.png 1753w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>This UI is tricky to render in upscaled mode, since it uses triangles, but the MSAA snap trick above works well and avoids all artifacts. With straight upscale, this is hard to achieve in normal graphics APIs since you’d need interpolateAtOffset beyond 0.5 pixels, which isn’t supported. Perhaps you could do custom interpolation with derivatives or something like that, but either way, this glitch can be avoided. The core message is basically to never upscale UI beyond plain nearest neighbor integer scale. It just looks bad.</p>
<p>There are cases where PCSX2 asks for high blending accuracy. One example is MGS2, and I found a spot where GPU perf is murdered. My desktop GPU cannot keep 60 FPS here at 4x upscale. PCSX2 asks you to turn up blend-accuracy for this game, but …</p>
<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/mgs2.jpg"><img loading="lazy" decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/mgs2-1024x621.jpg" alt="" width="660" height="400" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/mgs2-1024x621.jpg 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/mgs2-300x182.jpg 300w, https://themaister.net/blog/wp-content/uploads/2024/07/mgs2-768x466.jpg 768w, https://themaister.net/blog/wp-content/uploads/2024/07/mgs2-1536x932.jpg 1536w, https://themaister.net/blog/wp-content/uploads/2024/07/mgs2.jpg 1747w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>What happens here is we hit the programmable blending path with barrier between every primitive. Ouch! This wouldn’t be bad for the tiler mobile GPUs, but for a desktop GPU, it is where perf goes to die. The shader in question does subpassLoad and does programmable blending as expected. Barrier, tiny triangle, barrier, tiny triangle, hnnnnnnng.</p>
<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/ouch.png"><img loading="lazy" decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/ouch-1024x576.png" alt="" width="660" height="371" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/ouch-1024x576.png 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/ouch-300x169.png 300w, https://themaister.net/blog/wp-content/uploads/2024/07/ouch-768x432.png 768w, https://themaister.net/blog/wp-content/uploads/2024/07/ouch-1536x864.png 1536w, https://themaister.net/blog/wp-content/uploads/2024/07/ouch-2048x1152.png 2048w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>paraLLEl-GS on the other hand always runs with 100% blend accuracy (assuming no bugs of course). Here’s 16xSSAA (equivalent to 4x upscale). This is just 25 W and 17% GPU utilization on RX 7600. Not bad.</p>
<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/mgs2-16xssaa.png"><img loading="lazy" decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/mgs2-16xssaa-1024x687.png" alt="" width="660" height="443" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/mgs2-16xssaa-1024x687.png 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/mgs2-16xssaa-300x201.png 300w, https://themaister.net/blog/wp-content/uploads/2024/07/mgs2-16xssaa-768x515.png 768w, https://themaister.net/blog/wp-content/uploads/2024/07/mgs2-16xssaa.png 1280w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>Other difficult cases include texture sampling feedback. One particular case I found was in Valkyrie Profile 2.</p>
<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/valprofile2.jpg"><img loading="lazy" decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/valprofile2-1024x621.jpg" alt="" width="660" height="400" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/valprofile2-1024x621.jpg 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/valprofile2-300x182.jpg 300w, https://themaister.net/blog/wp-content/uploads/2024/07/valprofile2-768x466.jpg 768w, https://themaister.net/blog/wp-content/uploads/2024/07/valprofile2-1536x932.jpg 1536w, https://themaister.net/blog/wp-content/uploads/2024/07/valprofile2.jpg 1747w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>This game has a case where it’s sampling it’s own pixel’s alpha as a palette index. Quirky as all hell, and similar to MGS2 there’s a barrier between every pixel.</p>
<p>In paraLLEl-GS, this case is detected, and we emit a magical texture index, which resolved to just looking at in-register framebuffer color instead. Programmable blending go brr. These cases have to be checked per primitive, which is quite rough on CPU time, but it is what it is. If we don’t hit the good path, GPU performance completely tanks.</p>
<p>The trick here is to analyze the effective UV coordinates, and see if UV == framebuffer position. If we fall off this path, we have to go via texture uploads, which is bad.</p>
<pre>ivec2 uv0_delta = uv0 - pos[0].pos;
ivec2 uv1_delta = uv1 - pos[1].pos;
ivec2 min_delta = min(uv0_delta, uv1_delta);
ivec2 max_delta = max(uv0_delta, uv1_delta);

if (!quad)
{
  ivec2 uv2_delta = uv2 - pos[2].pos;
  min_delta = min(min_delta, uv2_delta);
  max_delta = max(max_delta, uv2_delta);
}

int min_delta2 = min(min_delta.x, min_delta.y);
int max_delta2 = max(max_delta.x, max_delta.y);

// The UV offset must be in range of [0, 2^SUBPIXEL_BITS - 1].
// This guarantees snapping with NEAREST.
// 8 is ideal. That means pixel centers during interpolation
// will land exactly in the center of the texel.
// In theory we could allow LINEAR if uv delta was
// exactly 8 for all vertices.
if (min_delta2 &lt; 0 || max_delta2 &gt;= (1 &lt;&lt; SUBPIXEL_BITS))
  return ColorFeedbackMode::Sliced;

// Perf go brrrrrrr.
return ColorFeedbackMode::Pixel;</pre>
<pre>if (feedback_mode == ColorFeedbackMode::Pixel)
{
  mark_render_pass_has_texture_feedback(ctx.tex0.desc);
  // Special index indicating on-tile feedback.
  // We could add a different sentinel for depth feedback.
  // 1024k CLUT instances and 32 sub-banks. Fits in 15 bits.
  // Use bit 15 MSB to mark feedback texture.
  return (1u &lt;&lt; (TEX_TEXTURE_INDEX_BITS - 1u)) |
         (render_pass.clut_instance * 32 + uint32_t(ctx.tex0.desc.CSA));
}</pre>
<p>It’s comfortably full-speed on PCSX2 here, despite the copious number of barriers, but paraLLEl-GS is reasonably close perf-wise, actually. At 8x SSAA.</p>
<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/valkyrie-gs.jpg"><img loading="lazy" decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/valkyrie-gs-1024x709.jpg" alt="" width="660" height="457" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/valkyrie-gs-1024x709.jpg 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/valkyrie-gs-300x208.jpg 300w, https://themaister.net/blog/wp-content/uploads/2024/07/valkyrie-gs-768x532.jpg 768w, https://themaister.net/blog/wp-content/uploads/2024/07/valkyrie-gs-1536x1064.jpg 1536w, https://themaister.net/blog/wp-content/uploads/2024/07/valkyrie-gs.jpg 1708w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>Overall, we get away with 18 render pass barriers instead of 500+ which was the case without this optimization. You may notice the interlacing artifacts on the swirlies. Silly game has a progressive scan output, but downsamples it on its own to a field before hitting CRTC, hnnnnng 🙁 Redirecting framebuffer locations in CRTC might work as a per-game hack, but either way, I still need to consider a better de-interlacer. Some games actually render explicitly in fields (640×224), which is very annoying.</p>
<p>This scene in the MGS2 intro also exposes some funny edge cases with sampling.</p>
<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/mgs2-intro.png"><img loading="lazy" decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/mgs2-intro-1024x687.png" alt="" width="660" height="443" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/mgs2-intro-1024x687.png 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/mgs2-intro-300x201.png 300w, https://themaister.net/blog/wp-content/uploads/2024/07/mgs2-intro-768x515.png 768w, https://themaister.net/blog/wp-content/uploads/2024/07/mgs2-intro.png 1280w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>To get the camo effect, it’s sampling its own framebuffer as a texture, with overlapping coordinates, but not pixel aligned, so this raises some serious questions about caching behavior. PCSX2 doesn’t seem to add any barriers here, and I kinda had to do the same thing. It looks fine to me compared to software renderer at least.</p>
<pre>if (feedback_mode == ColorFeedbackMode::Sliced)
{
  // If game explicitly clamps the rect to a small region,
  // it's likely doing well-defined feedbacks.
  // E.g. Tales of Abyss main menu ping-pong blurs.
  // This code is quite flawed,
  // and I'm not sure what the correct solution is yet.
  if (desc.clamp.desc.WMS == CLAMPBits::REGION_CLAMP &amp;&amp;
      desc.clamp.desc.WMT == CLAMPBits::REGION_CLAMP)
  {
    ivec4 clamped_uv_bb(
      int(desc.clamp.desc.MINU),
      int(desc.clamp.desc.MINV),
      int(desc.clamp.desc.MAXU),
      int(desc.clamp.desc.MAXV));

    ivec4 hazard_bb(
      std::max&lt;int&gt;(clamped_uv_bb.x, bb.x),
      std::max&lt;int&gt;(clamped_uv_bb.y, bb.y),
      std::min&lt;int&gt;(clamped_uv_bb.z, bb.z),
      std::min&lt;int&gt;(clamped_uv_bb.w, bb.w));

    cache_texture = hazard_bb.x &gt; hazard_bb.z ||
                    hazard_bb.y &gt; hazard_bb.w;
  }
  else
  {
    // Questionable,
    // but it seems almost impossible to do this correctly and fast.
    // Need to emulate the PS2 texture cache exactly,
    // which is just insane.
    // This should be fine.
    cache_texture = false;
  }
}</pre>
<p>If we’re in a mode where texture points directly to the frame buffer we should relax the hazard tracking a bit to avoid 2000+ barriers. This is clearly spooky since Tales of Abyss’s bloom effect as shown earlier depends on this to be well behaved, but in that case, at least it uses REGION_CLAMP to explicitly mark the ping-pong behavior. I’m not sure what the proper solution is here.</p>
<p>The only plausible solution to true bit-accuracy with real hardware is to emulate the caches directly, one pixel at a time. You can kiss performance good bye in that case.</p>
<p>One of the worst stress tests I’ve found so far has to be Shadow of the Collosus. Just in the intro, we can make the GPU kneel down to 24 FPS with maximum blend accuracy on PCSX2, at just 2x upscale! Even with normal blending accuracy, it is extremely heavy during the intro cinematic.</p>
<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-max-blend-accuracy.png"><img loading="lazy" decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-max-blend-accuracy-1024x674.png" alt="" width="660" height="434" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-max-blend-accuracy-1024x674.png 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-max-blend-accuracy-300x198.png 300w, https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-max-blend-accuracy-768x506.png 768w, https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-max-blend-accuracy-1536x1012.png 1536w, https://themaister.net/blog/wp-content/uploads/2024/07/pcsx2-max-blend-accuracy.png 1775w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>At 8x SSAA, perf is still looking pretty good for paraLLEl-GS, but it’s clearly sweating now.</p>
<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/parallel-sotc.jpg"><img loading="lazy" decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/parallel-sotc-1024x717.jpg" alt="" width="660" height="462" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/parallel-sotc-1024x717.jpg 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/parallel-sotc-300x210.jpg 300w, https://themaister.net/blog/wp-content/uploads/2024/07/parallel-sotc-768x538.jpg 768w, https://themaister.net/blog/wp-content/uploads/2024/07/parallel-sotc.jpg 1280w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<p>We’re actually still CPU bound on the geometry processing. Optimizing the CPU code hasn’t been a huge priority yet. There’s unfortunately a lot of code that has to run per-primitive, where hazards can happen around every corner that has to be dealt with somehow. I do some obvious optimizations, but it’s obviously not as well-oiled as PCSX2 in that regard.</p>
<p><a href="https://themaister.net/blog/wp-content/uploads/2024/07/rgp-rx7600.png"><img loading="lazy" decoding="async" src="https://themaister.net/blog/wp-content/uploads/2024/07/rgp-rx7600-1024x611.png" alt="" width="660" height="394" srcset="https://themaister.net/blog/wp-content/uploads/2024/07/rgp-rx7600-1024x611.png 1024w, https://themaister.net/blog/wp-content/uploads/2024/07/rgp-rx7600-300x179.png 300w, https://themaister.net/blog/wp-content/uploads/2024/07/rgp-rx7600-768x458.png 768w, https://themaister.net/blog/wp-content/uploads/2024/07/rgp-rx7600-1536x917.png 1536w, https://themaister.net/blog/wp-content/uploads/2024/07/rgp-rx7600.png 1766w" sizes="(max-width: 660px) 100vw, 660px"></a></p>
<h3>Deck?</h3>
<p>It seems fast enough to comfortably do 4x SSAA. Maybe not in SotC, but … hey. 😀</p>
<h2>What now?</h2>
<p>For now, the only real way to test this is through GS dumps. <a href="https://github.com/Arntzen-Software/parallel-gs/blob/main/misc/0001-Add-an-ad-hoc-GS-stream-format.patch">There’s a hack-patch for PCSX2</a> that lets you dump out a raw GS trace, which can be replayed. This works via mkfifo as a crude hack to test in real-time, but some kind of integration into an emulator needs to happen at some point if this is to turn into something that’s useful for end users.</p>
<p>There’s guaranteed to be a million bugs lurking since the PS2 library is ridiculously large and there’s only so much I can be arsed to test myself. At least, paraLLEl-GS has now become my preferred way to play PS2 games, so I can say mission complete.</p>
<p>A potential use case for this is due to its standalone library nature, it may be useful as very old-school rendering API for the old greybeards around that still yearn for the day of PS2 programming for whatever reason :p</p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Taming the beast that is the Django ORM – An introduction (126 pts)]]></title>
            <link>https://www.davidhang.com/blog/2024-09-01-taming-the-django-orm/</link>
            <guid>41413641</guid>
            <pubDate>Sun, 01 Sep 2024 02:07:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.davidhang.com/blog/2024-09-01-taming-the-django-orm/">https://www.davidhang.com/blog/2024-09-01-taming-the-django-orm/</a>, See on <a href="https://news.ycombinator.com/item?id=41413641">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>  <p><img src="https://www.davidhang.com/_astro/1.UiFfZ6wR_IHH0Y.webp" alt="man fighting dragon which represents the django orm" width="1024" height="1024" loading="lazy" decoding="async"></p>
<p>The material this blog post was originally developed from was a bunch of slides
used for a skill share presentation I gave at my workplace <span>@</span> <a href="https://coreplan.io/">coreplan.io</a>.</p>
<p>I have 3+ years of experience with Django, with it being the main framework that
underpins the backend of CorePlan’s main SaaS product. It is a mature, batteries
included framework that has been around for a while now. One particular powerful
yet dangerous feature of Django is the ORM. This is a Django specific ORM which
cannot be separated from the rest of the framework. The other major python ORM
is SQLAlchemy which can be used with other python web frameworks, but is an
independent tool.</p>
<p>Below are some of the things that I have learned about the Django ORM, how it
compares to raw SQL and gotchas that you should be aware of when using it.</p>
<hr>
<h2 id="what-is-an-orm-object-relational-mapper">What is an ORM (Object Relational Mapper)?</h2>
<ul>
<li>Abstraction over SQL to interact with databases</li>
</ul>
<p>Code -&gt; SQL</p>
<pre tabindex="0" data-language="python"><code><span><span>Hole.objects.all()</span></span>
<span></span></code></pre>
<p>⬇️</p>
<pre tabindex="0" data-language="sql"><code><span><span>SELECT</span><span> *</span><span> FROM</span><span> drilling_hole;</span></span>
<span></span></code></pre>
<hr>
<h2 id="why-use-an-orm---pros">Why use an ORM? - Pros</h2>
<ul>
<li>Abstraction over SQL, no need to write raw SQL (plus and minus)</li>
<li>Portability - Can change out database engines easily !?
<ul>
<li>Probably not true, often will rely on db specific features e.g. postgres jsonb, triggers, etc</li>
</ul>
</li>
<li>Direct mapping from db to models</li>
<li>Automatic schema generation
<ul>
<li>Migrations are automatically generated</li>
</ul>
</li>
<li>Security
<ul>
<li>abstracts away enough that sql injection is less likely</li>
</ul>
</li>
</ul>
<hr>
<h2 id="why-use-an-orm---cons">Why use an ORM? - Cons</h2>
<ul>
<li>Abstraction over SQL…
<ul>
<li>Hides the underlying SQL</li>
<li>Can be difficult to debug</li>
<li>Lazy loading can cause N+1 queries without the developer realising</li>
<li>Harder to onboard new developers if they haven’t used Django before</li>
</ul>
</li>
<li>Performance
<ul>
<li>Generated sql be slower than crafted SQL</li>
</ul>
</li>
</ul>
<hr>
<h2 id="fundamentals">Fundamentals</h2>
<ul>
<li>Models = Tables</li>
</ul>
<pre tabindex="0" data-language="python"><code><span><span># drilling/models.py</span></span>
<span></span>
<span><span>from</span><span> django.db </span><span>import</span><span> models</span></span>
<span><span>class</span><span> Hole</span><span>(</span><span>models</span><span>.</span><span>Model</span><span>):</span></span>
<span><span>    name </span><span>=</span><span> models.TextField()</span></span>
<span></span></code></pre>
<pre tabindex="0" data-language="sql"><code><span><span>CREATE</span><span> TABLE</span><span> drilling_hole</span><span> (</span></span>
<span><span>    id </span><span>SERIAL</span><span> PRIMARY KEY</span><span>,</span></span>
<span><span>    name</span><span> VARCHAR</span><span>(</span><span>100</span><span>)</span></span>
<span><span>);</span></span>
<span></span></code></pre>
<hr>
<h2 id="migrations">Migrations</h2>
<pre tabindex="0" data-language="bash"><code><span><span>python</span><span> manage.py</span><span> makemigrations</span><span> # generate migration files</span></span>
<span><span>python</span><span> manage.py</span><span> migrate</span><span> # apply migrations</span></span>
<span></span>
<span><span>python</span><span> manage.py</span><span> drilling</span><span> --empty</span><span> # generate empty file for data migration</span></span>
<span></span></code></pre>
<p><a href="https://docs.djangoproject.com/en/dev/topics/migrations/">https://docs.djangoproject.com/en/dev/topics/migrations/</a></p>
<hr>

<h2 id="querying">Querying</h2>
<ul>
<li><code>ActiveRecord</code> pattern - ala Ruby on Rails style</li>
<li>QuerySets (<code>Hole.objects.all()</code>)
<ul>
<li>lazy</li>
<li>chainable</li>
<li>cached when iterated over multiple times <a href="https://docs.djangoproject.com/en/dev/topics/db/queries/#caching-and-querysets">!?</a>
<ul>
<li>I would not recommend relying on this because it hard to comprehend when it is cached and when it is not when you are reading code</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre tabindex="0" data-language="python"><code><span><span>holes_qs </span><span>=</span><span> Hole.objects.filter(</span><span>name</span><span>=</span><span>"cheese"</span><span>) </span><span># not evaluated yet</span></span>
<span><span>holes_qs </span><span>=</span><span> holes_qs.filter(</span><span>depth__gt</span><span>=</span><span>100</span><span>) </span><span># still not evaluated</span></span>
<span></span>
<span><span>list</span><span>(holes_qs) </span><span># evaluated</span></span>
<span><span>list</span><span>(holes_qs) </span><span># cached</span></span>
<span></span>
<span><span>holes_qs[</span><span>2</span><span>] </span><span># not cached</span></span>
<span><span>holes_qs.first() </span><span># not cached</span></span>
<span><span>holes_qs.get(</span><span>id</span><span>=</span><span>1</span><span>) </span><span># not cached</span></span>
<span></span></code></pre>
<hr>
<h2 id="where">WHERE</h2>
<ul>
<li><code>WHERE</code> clause ≈ <code>filter()</code></li>
</ul>
<pre tabindex="0" data-language="python"><code><span><span>holes_qs </span><span>=</span><span> Hole.objects.filter(</span><span>name</span><span>=</span><span>"cheese"</span><span>)</span></span>
<span></span></code></pre>
<p>⬇️</p>
<pre tabindex="0" data-language="sql"><code><span><span>SELECT</span><span> *</span><span> </span></span>
<span><span>FROM</span><span> drilling_hole;</span></span>
<span><span>WHERE</span><span> drilling_hole</span><span>.</span><span>name</span><span> =</span><span> 'cheese'</span><span>;</span></span>
<span></span></code></pre>
<hr>
<h2 id="where-across-tables">WHERE across tables?</h2>
<ul>
<li>But how do you do a left/inner join? With the ORM it isn’t done declaratively, but implicitly</li>
</ul>
<pre tabindex="0" data-language="python"><code><span><span>class</span><span> Hole</span><span>(</span><span>models</span><span>.</span><span>Model</span><span>):</span></span>
<span><span>  name </span><span>=</span><span> models.TextField()</span></span>
<span><span>  pad </span><span>=</span><span> models.ForeignKey(Pad, </span><span>on_delete</span><span>=</span><span>models.</span><span>CASCADE</span><span>)</span></span>
<span></span>
<span><span>class</span><span> Pad</span><span>(</span><span>models</span><span>.</span><span>Model</span><span>):</span></span>
<span><span>  name </span><span>=</span><span> models.TextField()</span></span>
<span></span>
<span><span>holes_qs </span><span>=</span><span> Hole.objects.filter(</span><span>pad__name</span><span>=</span><span>"cheese board"</span><span>)</span></span>
<span></span>
<span></span></code></pre>
<p>⬇️</p>
<pre tabindex="0" data-language="sql"><code><span><span>SELECT</span><span> *</span><span> </span></span>
<span><span>FROM</span><span> drilling_hole;</span></span>
<span><span>INNER JOIN</span><span> drilling_pad </span><span>ON</span><span> drilling_hole</span><span>.</span><span>pad_id</span><span> =</span><span> drilling_pad</span><span>.</span><span>id</span></span>
<span><span>WHERE</span><span> drilling_pad</span><span>.</span><span>name</span><span> =</span><span> 'cheese board'</span><span>;</span></span>
<span></span></code></pre>
<hr>
<h2 id="where-other-conditionals">WHERE other conditionals</h2>
<ul>
<li><code>filter(name="cheese")</code> -&gt; <code>filter(name__exact="cheese")</code> -&gt; <code>WHERE name = 'cheese'</code></li>
<li><code>filter(name__iexact="cheese")</code> -&gt; <code>WHERE name ILIKE 'cheese'</code></li>
<li><code>filter(name__contains="cheese")</code> -&gt; <code>WHERE name LIKE '%cheese%'</code></li>
<li><code>filter(name__icontains="cheese")</code> -&gt; <code>WHERE name ILIKE '%cheese%'</code></li>
<li><code>filter(name__in=["cheese", "board"])</code> -&gt; <code>WHERE name IN ('cheese', 'board')</code></li>
<li><code>filter(name__gt=100)</code> -&gt; <code>WHERE name &gt; 100</code> etc</li>
<li><code>filter(name__isnull=True)</code> -&gt; <code>WHERE name IS NULL</code>
<ul>
<li>At least for postgres shouldn’t <code>name = None</code>, <a href="https://www.postgresql.org/docs/current/functions-comparison.html">null != null</a></li>
</ul>
</li>
</ul>
<hr>
<h2 id="as">AS</h2>
<ul>
<li><code>annotate</code> ≈ <code>AS</code></li>
</ul>
<pre tabindex="0" data-language="python"><code><span><span>holes_qs </span><span>=</span><span> Hole.objects.annotate(</span><span>this_thang</span><span>=</span><span>F(</span><span>"pad__name"</span><span>))</span></span>
<span><span>hole </span><span>=</span><span> holes_qs.first()</span></span>
<span><span>print</span><span>(hole.this_thang)</span></span>
<span></span></code></pre>
<p>⬇️</p>
<pre tabindex="0" data-language="sql"><code><span><span>SELECT</span><span> </span></span>
<span><span>  *</span><span> , </span></span>
<span><span>  drilling_pad</span><span>.</span><span>name</span><span> AS</span><span> this_thang</span></span>
<span><span>FROM</span><span> drilling_hole;</span></span>
<span><span>INNER JOIN</span><span> "drilling_pad"</span><span> ON</span><span> (</span><span>"drilling_hole"</span><span>.</span><span>"pad_id"</span><span> =</span><span> "drilling_pad"</span><span>.</span><span>"id"</span><span>)</span></span>
<span></span>
<span></span></code></pre>
<hr>
<h2 id="subqueries">Subqueries</h2>
<pre tabindex="0" data-language="python"><code><span><span>class</span><span> Project</span><span>(</span><span>models</span><span>.</span><span>Model</span><span>):</span></span>
<span><span>  name </span><span>=</span><span> models.TextField()</span></span>
<span></span>
<span><span>class</span><span> Pad</span><span>(</span><span>models</span><span>.</span><span>Model</span><span>):</span></span>
<span><span>  name </span><span>=</span><span> models.TextField()</span></span>
<span></span>
<span><span>class</span><span> Hole</span><span>(</span><span>models</span><span>.</span><span>Model</span><span>):</span></span>
<span><span>  name </span><span>=</span><span> models.TextField()</span></span>
<span><span>  pad </span><span>=</span><span> models.ForeignKey(Pad, </span><span>on_delete</span><span>=</span><span>models.</span><span>CASCADE</span><span>)</span></span>
<span><span>  project </span><span>=</span><span> models.ForeignKey(Project, </span><span>on_delete</span><span>=</span><span>models.</span><span>CASCADE</span><span>)</span></span>
<span></span>
<span><span># find pads that are on project_id=1</span></span>
<span><span>hole_subquery </span><span>=</span><span> Hole.objects.filter(</span><span>project_id</span><span>=</span><span>1</span><span>).values(</span><span>"pk"</span><span>)</span></span>
<span><span>pad_qs </span><span>=</span><span> Pad.objects.filter(</span><span>hole__in</span><span>=</span><span>Subquery(hole_subquery))</span></span>
<span></span></code></pre>
<p>⬇️</p>
<pre tabindex="0" data-language="sql"><code><span><span>SELECT</span><span> "drilling_pad"</span><span>.</span><span>"id"</span><span>,</span></span>
<span><span> "drilling_pad"</span><span>.</span><span>"name"</span></span>
<span><span>FROM</span><span> "drilling_pad"</span></span>
<span><span> INNER JOIN</span><span> "drilling_hole"</span><span> ON</span><span> (</span><span>"drilling_pad"</span><span>.</span><span>"id"</span><span> =</span><span> "drilling_hole"</span><span>.</span><span>"pad_id"</span><span>)</span></span>
<span><span>WHERE</span><span> "drilling_hole"</span><span>.</span><span>"id"</span><span> IN</span><span> (</span></span>
<span><span>  SELECT</span><span> U0.</span><span>"id"</span></span>
<span><span>  FROM</span><span> "drilling_hole"</span><span> U0</span></span>
<span><span>  WHERE</span><span> U0.</span><span>"project_id"</span><span> =</span><span> 1</span></span>
<span><span> )</span></span>
<span></span></code></pre>
<hr>

<p>Correlated subqueries are where the inner query depends on outer query</p>
<pre tabindex="0" data-language="python"><code><span><span>class</span><span> Pad</span><span>(</span><span>models</span><span>.</span><span>Model</span><span>):</span></span>
<span><span>  name </span><span>=</span><span> models.TextField()</span></span>
<span></span>
<span><span>class</span><span> Hole</span><span>(</span><span>models</span><span>.</span><span>Model</span><span>):</span></span>
<span><span>  name </span><span>=</span><span> models.TextField()</span></span>
<span><span>  pad </span><span>=</span><span> models.ForeignKey(Pad, </span><span>on_delete</span><span>=</span><span>models.</span><span>CASCADE</span><span>)</span></span>
<span></span>
<span><span># include the hole id of any hole that has a foreign key to the pad</span></span>
<span><span>hole_subquery </span><span>=</span><span> Hole.objects.filter(</span><span>pad_id</span><span>=</span><span>OuterRef(</span><span>"pk"</span><span>)).values(</span><span>"pk"</span><span>)</span></span>
<span><span>pad_qs </span><span>=</span><span> Pad.objects.annotate(</span><span>hole_id</span><span>=</span><span>Subquery(hole_subquery))</span></span>
<span></span></code></pre>
<p>⬇️</p>
<pre tabindex="0" data-language="sql"><code><span><span>SELECT</span><span> "drilling_pad"</span><span>.</span><span>"id"</span><span>,</span></span>
<span><span> "drilling_pad"</span><span>.</span><span>"name"</span><span>,</span></span>
<span><span> (</span></span>
<span><span>  SELECT</span><span> U0.</span><span>"id"</span></span>
<span><span>  FROM</span><span> "drilling_hole"</span><span> U0</span></span>
<span><span>  WHERE</span><span> U0.</span><span>"pad_id"</span><span> =</span><span> (</span><span>"drilling_pad"</span><span>.</span><span>"id"</span><span>)</span></span>
<span><span> ) </span><span>AS</span><span> "hole_id"</span></span>
<span><span>FROM</span><span> "drilling_pad"</span></span>
<span></span></code></pre>
<hr>
<h2 id="performance-improvements">Performance improvements</h2>
<ul>
<li>Reduce N+1
<ul>
<li>You typically want to reduce N+1 queries because they have communication
overhead</li>
<li><code>select_related</code></li>
<li><code>prefetch_related</code></li>
<li>You also might choose to use <code>annotate()</code> instead of <code>select_related</code>
because select related pulls all the data for the associated table when you
might only need one column. That associated might have a jsonb column which
contains a lot of unnecessary data that you don’t need.</li>
</ul>
</li>
</ul>
<hr>

<pre tabindex="0" data-language="python"><code><span><span>holes </span><span>=</span><span> Hole.objects.all()</span></span>
<span><span>for</span><span> hole </span><span>in</span><span> holes:</span></span>
<span><span>  print</span><span>(hole.pad.name) </span><span># N+1 queries</span></span>
<span></span>
<span><span>holes </span><span>=</span><span> Hole.objects.select_related(</span><span>"pad"</span><span>)</span></span>
<span></span>
<span><span>for</span><span> hole </span><span>in</span><span> holes:</span></span>
<span><span>  print</span><span>(hole.pad.name) </span><span># no extra query</span></span>
<span></span>
<span></span></code></pre>
<hr>

<p>You would use prefetch related when you are not pulling a direct foreign key
such a many-to-many relationship like below.</p>
<p><img src="https://www.davidhang.com/_astro/2.Cd5VA_n2_Z22TJnn.svg" alt="width:400px" width="340" height="465" loading="lazy" decoding="async"></p>
<pre tabindex="0" data-language="python"><code><span></span>
<span><span>class</span><span> Faculty</span><span>(</span><span>models</span><span>.</span><span>Model</span><span>):</span></span>
<span><span>  name </span><span>=</span><span> models.TextField()</span></span>
<span></span>
<span><span>class</span><span> Course</span><span>(</span><span>models</span><span>.</span><span>Model</span><span>):</span></span>
<span><span>  name </span><span>=</span><span> models.TextField()</span></span>
<span><span>  faculty </span><span>=</span><span> models.ForeignKey(Faculty, </span><span>on_delete</span><span>=</span><span>models.</span><span>CASCADE</span><span>)</span></span>
<span></span>
<span><span>class</span><span> Student</span><span>(</span><span>models</span><span>.</span><span>Model</span><span>):</span></span>
<span><span>  name </span><span>=</span><span> models.TextField()</span></span>
<span><span>  courses </span><span>=</span><span> models.ManyToManyField(Course, </span><span>through</span><span>=</span><span>"Enrolment"</span><span>)</span></span>
<span></span>
<span><span>class</span><span> Enrolment</span><span>(</span><span>models</span><span>.</span><span>Model</span><span>):</span></span>
<span><span>  course </span><span>=</span><span> models.ForeignKey(Course, </span><span>on_delete</span><span>=</span><span>models.</span><span>CASCADE</span><span>)</span></span>
<span><span>  student </span><span>=</span><span> models.ForeignKey(Student, </span><span>on_delete</span><span>=</span><span>models.</span><span>CASCADE</span><span>)</span></span>
<span><span>  grade </span><span>=</span><span> models.FloatField()</span></span>
<span></span>
<span><span>students </span><span>=</span><span> Student.objects.prefetch_related(</span><span>"courses"</span><span>)</span></span>
<span></span>
<span><span>for</span><span> student </span><span>in</span><span> students:</span></span>
<span><span>  for</span><span> course </span><span>in</span><span> student.courses.all():</span></span>
<span><span>    print</span><span>(course.name) </span><span># no extra query</span></span>
<span><span>    print</span><span>(course.faculty.name) </span><span># extra query</span></span>
<span></span>
<span><span>students </span><span>=</span><span> Student.objects.prefetch_related(</span></span>
<span><span>  Prefetch(</span></span>
<span><span>    "courses"</span><span>, </span></span>
<span><span>    queryset</span><span>=</span><span>Course.objects.select_related(</span><span>"faculty"</span><span>)</span></span>
<span><span>  )</span></span>
<span><span>)</span></span>
<span></span>
<span><span>for</span><span> student </span><span>in</span><span> students:</span></span>
<span><span>  for</span><span> course </span><span>in</span><span> student.courses.all():</span></span>
<span><span>    print</span><span>(course.name) </span><span># no extra query</span></span>
<span><span>    print</span><span>(course.faculty.name) </span><span># no extra query</span></span>
<span></span></code></pre>
<hr>
<h2 id="to_attr">to_attr</h2>
<p><code>to_attr</code> can be used to make “filtered” relationships available on the instance.</p>
<pre tabindex="0" data-language="python"><code><span><span>class</span><span> Enrolment</span><span>(</span><span>models</span><span>.</span><span>Model</span><span>):</span></span>
<span><span>  course </span><span>=</span><span> models.ForeignKey(Course, </span><span>on_delete</span><span>=</span><span>models.</span><span>CASCADE</span><span>)</span></span>
<span><span>  student </span><span>=</span><span> models.ForeignKey(Student, </span><span>on_delete</span><span>=</span><span>models.</span><span>CASCADE</span><span>)</span></span>
<span><span>  grade </span><span>=</span><span> models.FloatField()</span></span>
<span></span>
<span><span>students </span><span>=</span><span> Student.objects.prefetch_related(</span></span>
<span><span>  Prefetch(</span></span>
<span><span>    "course"</span><span>, </span></span>
<span><span>    queryset</span><span>=</span><span>Course.objects.filter(</span><span>grade__gt</span><span>=</span><span>80.0</span><span>).select_related(</span><span>"faculty"</span><span>), </span><span>to_attr</span><span>=</span><span>"hd_courses"</span></span>
<span><span>  )</span></span>
<span><span>)</span></span>
<span></span>
<span><span>for</span><span> student </span><span>in</span><span> students:</span></span>
<span><span>  for</span><span> course </span><span>in</span><span> student.hd_courses.all():</span></span>
<span><span>    ...</span></span>
<span></span></code></pre>
<hr>
<h2 id="multiple-instances-when-filtering-across-many-to-many">Multiple instances when filtering across many-to-many</h2>
<p>One gotcha is selecting across a many-to-many relationship can return multiple of the same instances.</p>
<p><img src="https://www.davidhang.com/_astro/3.DMAdguVG_TLkyj.webp" alt="data model showing many to many relationship" width="530" height="810" loading="lazy" decoding="async"></p>
<pre tabindex="0" data-language="python"><code><span><span>Student.objects.filter(</span><span>courses__faculty__name</span><span>=</span><span>"Science"</span><span>) </span><span># inner join returns duplicated rows</span></span>
<span><span>&lt;</span><span>QuerySet [</span><span>&lt;</span><span>Student: Student </span><span>object</span><span> (</span><span>1</span><span>)</span><span>&gt;</span><span>, </span><span>&lt;</span><span>Student: Student </span><span>object</span><span> (</span><span>1</span><span>)</span><span>&gt;</span><span>]</span><span>&gt;</span></span>
<span></span>
<span><span>Student.objects.filter(</span><span>courses__faculty__name</span><span>=</span><span>"Science"</span><span>).distinct()</span></span>
<span><span>&lt;</span><span>QuerySet [</span><span>&lt;</span><span>Student: Student </span><span>object</span><span> (</span><span>1</span><span>)</span><span>&gt;</span><span>]</span><span>&gt;</span></span>
<span></span>
<span></span></code></pre>
<pre tabindex="0" data-language="sql"><code><span><span>SELECT</span></span>
<span><span>  "testing_student"</span><span>.</span><span>"id"</span><span>, </span></span>
<span><span>  "testing_student"</span><span>.</span><span>"name"</span><span> </span></span>
<span><span>FROM</span><span> </span></span>
<span><span>  "testing_student"</span><span> </span></span>
<span><span>INNER JOIN</span><span> </span></span>
<span><span>  "testing_enrolment"</span><span> </span></span>
<span><span>ON</span><span> </span></span>
<span><span>  (</span><span>"testing_student"</span><span>.</span><span>"id"</span><span> =</span><span> "testing_enrolment"</span><span>.</span><span>"student_id"</span><span>) </span></span>
<span><span>INNER JOIN</span><span> </span></span>
<span><span>  "testing_course"</span><span> </span></span>
<span><span>ON</span><span> </span></span>
<span><span>  (</span><span>"testing_enrolment"</span><span>.</span><span>"course_id"</span><span> =</span><span> "testing_course"</span><span>.</span><span>"id"</span><span>) </span></span>
<span><span>INNER JOIN</span><span> </span></span>
<span><span>  "testing_faculty"</span><span> </span></span>
<span><span>ON</span><span> </span></span>
<span><span>  (</span><span>"testing_course"</span><span>.</span><span>"faculty_id"</span><span> =</span><span> "testing_faculty"</span><span>.</span><span>"id"</span><span>)</span></span>
<span><span>WHERE</span><span> </span></span>
<span><span>  "testing_faculty"</span><span>.</span><span>"name"</span><span> =</span><span> 'Science'</span></span>
<span></span></code></pre>
<hr>
<h2 id="gotchas-and-other-funky-stuff">Gotchas and other Funky stuff</h2>
<ul>
<li>Model instances when retrieved will try to populate all columns, if column
removed in migration, and the worker still up exception occurs
<ul>
<li><code>get()</code> or <code>first()</code></li>
<li><code>for hole in Hole.objects.all()</code></li>
</ul>
</li>
<li>This can make migrations hard, as older workers will be requesting columns that might have been removed or renamed which will cause errors</li>
<li>There are ways to do down-timeless migrations but are bit <a href="https://hackernoon.com/deleting-a-column-from-a-django-model-on-production">funky and multi
step</a></li>
<li>Recommendation is to avoid deleting or renaming columns</li>
</ul>
<pre tabindex="0" data-language="python"><code><span><span>class</span><span> Hole</span><span>(</span><span>models</span><span>.</span><span>Model</span><span>):</span></span>
<span><span>  name </span><span>=</span><span> models.TextField()</span></span>
<span><span>  pad </span><span>=</span><span> models.ForeignKey(Pad, </span><span>on_delete</span><span>=</span><span>models.</span><span>CASCADE</span><span>)</span></span>
<span></span>
<span><span>class</span><span> Pad</span><span>(</span><span>models</span><span>.</span><span>Model</span><span>):</span></span>
<span><span>  name </span><span>=</span><span> models.TextField()</span></span>
<span></span>
<span><span>holes_qs </span><span>=</span><span> Hole.objects.annotate(</span><span>this_thang</span><span>=</span><span>F(</span><span>"pad__name"</span><span>)).get()</span></span>
<span></span></code></pre>
<p>⬇️</p>
<pre tabindex="0" data-language="sql"><code><span><span>SELECT</span><span> </span></span>
<span><span>  drilling_hole</span><span>.</span><span>name</span><span>, </span><span>-- pulls all columns </span></span>
<span><span>  drilling_hole</span><span>.</span><span>pad_id</span><span>,</span></span>
<span><span>  drilling_pad</span><span>.</span><span>name</span><span> AS</span><span> this_thang</span></span>
<span><span>FROM</span><span> drilling_hole;</span></span>
<span><span>WHERE</span><span> drilling_pad</span><span>.</span><span>name</span><span> =</span><span> 'cheese board'</span><span>;</span></span>
<span><span>LIMIT</span><span> 1</span><span>;</span></span>
<span></span></code></pre>
<hr>
<h2 id="values">Values</h2>
<ul>
<li>So how do you to only retrieve certain columns?</li>
</ul>
<pre tabindex="0" data-language="python"><code><span><span>class</span><span> Hole</span><span>(</span><span>models</span><span>.</span><span>Model</span><span>):</span></span>
<span><span>  name </span><span>=</span><span> models.TextField()</span></span>
<span><span>  pad </span><span>=</span><span> models.ForeignKey(Pad, </span><span>on_delete</span><span>=</span><span>models.</span><span>CASCADE</span><span>)</span></span>
<span></span>
<span><span>holes_qs </span><span>=</span><span> Hole.objects.values(</span><span>"name"</span><span>)</span></span>
<span></span>
<span><span>for</span><span> hole </span><span>in</span><span> holes_qs:</span></span>
<span><span>  print</span><span>(</span><span>type</span><span>(hole)) </span><span># dict</span></span>
<span><span>  # not `Hole` object, hence no class functions, no lazy loading e.g. can't access `hole.pad.name`</span></span>
<span></span></code></pre>
<p>⬇️</p>
<pre tabindex="0" data-language="sql"><code><span><span>SELECT</span><span> </span></span>
<span><span>  drilling_hole</span><span>.</span><span>name</span><span>, </span><span>-- only pulls name and maps it to a python dictionary object</span></span>
<span><span>FROM</span><span> drilling_hole;</span></span>
<span></span></code></pre>
<ul>
<li>Less data sent down the wire, but no lazy loading and no class functions as
the data is a python dictionary</li>
</ul>
<hr>
<h2 id="other-options">Other options</h2>
<ul>
<li><code>only()</code> and <code>defer()</code></li>
<li>Will retrieve model instances, but won’t retrieve all fields</li>
<li>Values not declared when accessed on the model are lazy loaded</li>
<li>Would not recommend to be used regularly, very high chance of N+1</li>
</ul>
<pre tabindex="0" data-language="python"><code><span><span>holes_qs </span><span>=</span><span> Hole.objects.only(</span><span>"pad_id"</span><span>)</span></span>
<span></span>
<span><span>for</span><span> hole </span><span>in</span><span> holes_qs:</span></span>
<span><span>  print</span><span>(hole.pad_id) </span><span># no extra query</span></span>
<span><span>  print</span><span>(hole.name) </span><span># name will be lazy loaded, N+1 queries</span></span>
<span></span></code></pre>
<hr>
<h3 id="how-do-you-know-what-sql-is-being-generated">How do you know what SQL is being generated?</h3>
<ul>
<li><code>print(queryset.query)</code></li>
<li><a href="https://github.com/jazzband/django-debug-toolbar">Django Debug Toolbar</a></li>
<li><a href="https://kolo.app/">Kolo</a></li>
</ul>
<hr>
<h2 id="updating-rows">Updating rows</h2>
<p>There are three typical ways to update a row in the database.</p>
<pre tabindex="0" data-language="python"><code><span><span>class</span><span> Hole</span><span>(</span><span>models</span><span>.</span><span>Model</span><span>):</span></span>
<span><span>  name </span><span>=</span><span> models.TextField()</span></span>
<span></span>
<span><span>instance </span><span>=</span><span> Hole.objects.create(</span><span>name</span><span>=</span><span>"cheese"</span><span>)</span></span>
<span></span>
<span><span># save()</span></span>
<span><span>instance.name </span><span>=</span><span> "board"</span></span>
<span><span>instance.save()</span></span>
<span></span>
<span><span># update()</span></span>
<span><span>Model.objects.filter(</span><span>name</span><span>=</span><span>"board"</span><span>).update(</span><span>name</span><span>=</span><span>"board2"</span><span>)</span></span>
<span></span>
<span><span># bulk_update()</span></span>
<span><span>instance.name </span><span>=</span><span> "board3"</span></span>
<span><span>instances_to_update </span><span>=</span><span> [instance]</span></span>
<span><span>Model.objects.bulk_update(instances_to_update, [</span><span>"name"</span><span>])</span></span>
<span></span></code></pre>
<hr>
<h2 id="problems-with-updates">Problems with updates</h2>
<ul>
<li><code>update()</code> and <code>bulk_update()</code> do not trigger <code>save()</code> method on the model</li>
<li>built in django signals (publish/subscribe pattern), there are post_save and pre_save signals which can be triggered when calling <code>save()</code>
<ul>
<li><code>update()</code> and <code>bulk_update()</code> do not trigger those signals…</li>
</ul>
</li>
<li><code>updated_at</code> column would not normally be updated when calling <code>update()</code> or
<code>bulk_update()</code> but if queryset is a descendant of <code>CoreplanQuerySet</code> then it will.</li>
</ul>
<hr>

<ul>
<li>Pagination / order_by
<ul>
<li>Not a Django ORM thing, but a Django ORM hides the implementation detail,
which may lead to unexpected result</li>
<li>Page pagination is default in DRF list views and implemented with <code>LIMIT</code> and <code>OFFSET</code> in SQL</li>
</ul>
</li>
</ul>
<p><code>?page_size=10&amp;page=3</code></p>
<pre tabindex="0" data-language="plaintext"><code><span><span>SELECT * </span></span>
<span><span>FROM drilling_hole </span></span>
<span><span>LIMIT 10 </span></span>
<span><span>OFFSET 20;</span></span>
<span><span></span></span></code></pre>
<p>Anything wrong with this query?</p>
<ul>
<li>There is no deterministic guarantee that the same 10 rows will be returned each time.</li>
<li>A plain <code>SELECT</code> in postgres (may be different in different dbs) provides no
guarantee of order, unless <code>ORDER BY</code> is specified</li>
<li>It often appears to return in insertion/<code>id</code> order, but that is not guaranteed
in postgres</li>
<li>Model Meta <code>ordering</code> may set a default order, but sometimes tht is <a href="https://docs.djangoproject.com/en/dev/releases/2.2/#features-deprecated-in-2-2">ignored</a></li>
<li>For list views you should to provide a deterministic order_by</li>
<li><code>order_by(name)</code> is not enough if name is not unique
<ul>
<li><code>order_by(name, id)</code> is required, because id is unique</li>
</ul>
</li>
<li>This can been the the cause of some flaky tests issues where lists are
returned seemingly in insertion order and asserted to return in id order</li>
</ul>
<hr>
<p>Thanks for reading! I hope this has been useful to you. There are definitely
more particularities and gotchas to be aware of when using the Django ORM and
Django in general but I think these are the most common ones.</p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AirTags key to discovery of Houston's plastic recycling deception (137 pts)]]></title>
            <link>https://appleinsider.com/articles/24/08/31/airtags-key-to-discovery-of-houstons-plastic-recycling-deception</link>
            <guid>41413174</guid>
            <pubDate>Sun, 01 Sep 2024 00:38:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://appleinsider.com/articles/24/08/31/airtags-key-to-discovery-of-houstons-plastic-recycling-deception">https://appleinsider.com/articles/24/08/31/airtags-key-to-discovery-of-houstons-plastic-recycling-deception</a>, See on <a href="https://news.ycombinator.com/item?id=41413174">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-hero" aria-labelledby="hero-cap" role="figure">
                          <p id="hero-cap" title="Apple employs an advanced robot named Daisy to disassemble old iPhones.">Apple employs an advanced robot named Daisy to disassemble old iPhones.</p>
                                    <p><a href="https://photos5.appleinsider.com/gallery/60877-125351-Unknown-xl.jpg">
              <img fetchpriority="high" src="https://photos5.appleinsider.com/gallery/60877-125351-Unknown-xl.jpg" alt="">
            </a>
                      </p></div><div>
          <p>One Houston resident was suspicious of the city's "all plastic accepted" recycling program, and used <a href="https://appleinsider.com/inside/airtags" title="AirTag" data-kpt="1">AirTags</a> to discover where the plastic waste actually ended up.
</p><p>Deason, who regularly recycles her packaging and other waste, began to have doubts about the city's plastic recycling program. Houston's program boasted of being able to accept even types of plastic that aren't normally considered recyclable.
</p><p>Curious as to where the plastic was going, she bought a set of AirTags, and included them in various bags of her plastic recycling. Of the bags she tracked, nearly all of them went to a company called Wright Waste Management, located in nearby Harris County.
</p><p>The company is not approved to store plastic waste, and has failed three fire inspections.
</p><p>CBS News correspondent Ben Tracy <a href="https://www.khou.com/article/news/local/houston-recycling-tracking-device-plastic/">referred</a> to Deason as "the James Bond of plastic recycling" for her initiative. Aerial footage showed that the facility had large piles of plastic waste as tall as 10 feet high.
</p><p>Deason said she thought that the company simply storing the unrecyclable plastic waste was "kind of strange." She later contacted Houston's Director of Solid Waste Management Mark Wilfalk, to ask about the discrepancy.
</p><p>When shown the drone footage, Wilfalk admitted "it's not the most desirable-looking site." He promised Deason he'd investigate the problems that caused Wright Waste Management to fail the fire inspections.
</p><p>Wilfalk later acknowledged that the city had collected some 250 tons of plastic since the end of 2022. He revealed that none of it had been recycled as of yet.
</p><p>"We're gonna stockpile it for now," he admitted. "We're gonna see what happens."
</p><p>By contrast, Apple has been an <a href="https://appleinsider.com/articles/24/04/16/apple-highlights-device-recycling-iphone-trade-in-and-the-removal-of-leather-for-earth-day" title="Apple's environmental efforts">industry leader</a> in reducing its use of plastic. It uses paper for packaging, and metal rather than plastic for its computer line.
</p><p>It does use some plastic for products such as its <a href="https://appleinsider.com/inside/airpods" title="AirPods" data-kpt="1">AirPods</a> earbuds. It has invested in robotics to help recycle old Apple products.
</p><p>Houston, as it turns out, is waiting on a promised sorting facility to open, where the stored recycling will be sorted and treated. The company behind the sorting facility, Cyclix, says it has developed a method to create recyclable pellets out of the plastic waste.
</p><p>However, only a fraction of these pellets can be made into new plastic. Most will be melted and turned into fuel that is burned, adding to carbon emissions.
</p><p>California Attorney General Rob Bonta has been investigating Cyclix owner and plastic manufacturer ExxonMobil's claims regarding plastic recycling in that state. He has characterized Cyclix's claims of plastic recycling are largely fictional.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building LLMs from the Ground Up: A 3-Hour Coding Workshop (659 pts)]]></title>
            <link>https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up</link>
            <guid>41412256</guid>
            <pubDate>Sat, 31 Aug 2024 21:45:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up">https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up</a>, See on <a href="https://news.ycombinator.com/item?id=41412256">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>If you’d like to spend a few hours this weekend to dive into Large Language Models (LLMs) and understand how they work, I've prepared a 3-hour coding workshop presentation on implementing, training, and using LLMs.</p><div id="youtube2-quh7z1q7-uc" data-attrs="{&quot;videoId&quot;:&quot;quh7z1q7-uc&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/quh7z1q7-uc?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p>Below, you'll find a table of contents to get an idea of what this video covers (the video itself has clickable chapter marks, allowing you to jump directly to topics of interest):</p><p>0:00 – Workshop overview</p><p>2:17 – Part 1: Intro to LLMs</p><p>9:14 – Workshop materials</p><p>10:48 – Part 2: Understanding LLM input data</p><p>23:25 – A simple tokenizer class</p><p>41:03 – Part 3: Coding an LLM architecture</p><p>45:01 – GPT-2 and Llama 2</p><p>1:07:11 – Part 4: Pretraining</p><p>1:29:37 – Part 5.1: Loading pretrained weights</p><p>1:45:12 – Part 5.2: Pretrained weights via LitGPT</p><p>1:53:09 – Part 6.1: Instruction finetuning</p><p>2:08:21 – Part 6.2: Instruction finetuning via LitGPT</p><p>02:26:45 – Part 6.3: Benchmark evaluation</p><p>02:36:55 – Part 6.4: Evaluating conversational performance</p><p>02:42:40 – Conclusion</p><p>It's a slight departure from my usual text-based content, but the last time I did this a few months ago, it was so well-received that I thought it might be nice to do another one!</p><p><strong>Happy viewing!</strong></p><ol><li><p><a href="https://mng.bz/M96o" rel="">Build an LLM from Scratch book</a></p></li><li><p><a href="https://github.com/rasbt/LLMs-from-scratch" rel="">Build an LLM from Scratch GitHub repository</a></p></li><li><p><a href="https://github.com/rasbt/LLM-workshop-2024" rel="">GitHub repository with workshop code</a></p></li><li><p><a href="https://lightning.ai/lightning-ai/studios/llms-from-the-ground-up-workshop" rel="">Lightning Studio for this workshop</a></p></li><li><p><a href="https://github.com/Lightning-AI/litgpt" rel="">LitGPT GitHub repository</a></p></li></ol></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Threat to OpenAI (131 pts)]]></title>
            <link>https://www.wsj.com/tech/ai/ai-chatgpt-nvidia-apple-facebook-383943d1</link>
            <guid>41411478</guid>
            <pubDate>Sat, 31 Aug 2024 19:56:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/tech/ai/ai-chatgpt-nvidia-apple-facebook-383943d1">https://www.wsj.com/tech/ai/ai-chatgpt-nvidia-apple-facebook-383943d1</a>, See on <a href="https://news.ycombinator.com/item?id=41411478">Hacker News</a></p>
Couldn't get https://www.wsj.com/tech/ai/ai-chatgpt-nvidia-apple-facebook-383943d1: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[WatchYourLAN: Lightweight Network IP Scanner (146 pts)]]></title>
            <link>https://github.com/aceberg/WatchYourLAN</link>
            <guid>41411281</guid>
            <pubDate>Sat, 31 Aug 2024 19:32:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/aceberg/WatchYourLAN">https://github.com/aceberg/WatchYourLAN</a>, See on <a href="https://news.ycombinator.com/item?id=41411281">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://github.com/aceberg/WatchYourLAN">
    <img src="https://raw.githubusercontent.com/aceberg/WatchYourLAN/main/assets/logo.png" width="20">
</a>WatchYourLAN</h2><a id="user-content-----watchyourlan" aria-label="Permalink: WatchYourLAN" href="#----watchyourlan"></a></div>

<p dir="auto"><a href="https://github.com/aceberg/WatchYourLAN/actions/workflows/main-docker-all.yml"><img src="https://github.com/aceberg/WatchYourLAN/actions/workflows/main-docker-all.yml/badge.svg" alt="Docker"></a>
<a href="https://goreportcard.com/report/github.com/aceberg/WatchYourLAN" rel="nofollow"><img src="https://camo.githubusercontent.com/2e8d4fc73c5dc60c4a5ae0025b9cd0db42ef07d6f26d1479d55adbeb75cf03e6/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f616365626572672f5761746368596f75724c414e" alt="Go Report Card" data-canonical-src="https://goreportcard.com/badge/github.com/aceberg/WatchYourLAN"></a>
<a href="https://codeclimate.com/github/aceberg/WatchYourLAN/maintainability" rel="nofollow"><img src="https://camo.githubusercontent.com/60e9a9f739cc625dc0fa6aaa25a86c5097b94570dfe90d61dc52b5741a5b1e0f/68747470733a2f2f6170692e636f6465636c696d6174652e636f6d2f76312f6261646765732f34366231376639396564633137323662356437642f6d61696e7461696e6162696c697479" alt="Maintainability" data-canonical-src="https://api.codeclimate.com/v1/badges/46b17f99edc1726b5d7d/maintainability"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/23e7bee7caecf858d56d4665f00bb906da1f2ba916a7912d1ac050c7ae11e518/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f696d6167652d73697a652f616365626572672f7761746368796f75726c616e"><img src="https://camo.githubusercontent.com/23e7bee7caecf858d56d4665f00bb906da1f2ba916a7912d1ac050c7ae11e518/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f696d6167652d73697a652f616365626572672f7761746368796f75726c616e" alt="Docker Image Size (latest semver)" data-canonical-src="https://img.shields.io/docker/image-size/aceberg/watchyourlan"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/648dc770497678d13bdd6f00d103b648703af328fcfa7fd3cf5ca2ed6afac821/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f616365626572672f7761746368796f75726c616e"><img src="https://camo.githubusercontent.com/648dc770497678d13bdd6f00d103b648703af328fcfa7fd3cf5ca2ed6afac821/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f616365626572672f7761746368796f75726c616e" alt="Docker Pulls" data-canonical-src="https://img.shields.io/docker/pulls/aceberg/watchyourlan"></a></p>
<p dir="auto">Lightweight network IP scanner with web GUI. Features:</p>
<ul dir="auto">
<li>Send notification when new host is found</li>
<li>Monitor hosts online/offline history</li>
<li>Keep a list of all hosts in the network</li>
<li>Send data to <code>InfluxDB2</code> to make a <code>Grafana</code> dashboard</li>
</ul>
<div dir="auto"><p dir="auto">Warning</p><p dir="auto">This is version 2.0. Version 1.0 can be found in this brunch: <a href="https://github.com/aceberg/WatchYourLAN/tree/v1">v1</a></p>
</div>
<div dir="auto"><p dir="auto">Caution</p><p dir="auto"><strong>BREAKING CHANGES!</strong> Version 2.0 is not compatible with v1.0. For now v2.0 docker images will be released under <code>v2</code> tag. It will be tagged <code>latest</code> in a few weeks (probably, in October).</p>
</div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/aceberg/WatchYourLAN/main/assets/Screenshot_1.png"><img src="https://raw.githubusercontent.com/aceberg/WatchYourLAN/main/assets/Screenshot_1.png" alt="Screenshot_1"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">More screenshots</h2><a id="user-content-more-screenshots" aria-label="Permalink: More screenshots" href="#more-screenshots"></a></p>
<details>
  <summary>Expand</summary>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/aceberg/WatchYourLAN/main/assets/Screenshot_5.png"><img src="https://raw.githubusercontent.com/aceberg/WatchYourLAN/main/assets/Screenshot_5.png" alt="Screenshot_5"></a><br>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/aceberg/WatchYourLAN/main/assets/Screenshot_2.png"><img src="https://raw.githubusercontent.com/aceberg/WatchYourLAN/main/assets/Screenshot_2.png" alt="Screenshot_2"></a><br>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/aceberg/WatchYourLAN/main/assets/Screenshot_3.png"><img src="https://raw.githubusercontent.com/aceberg/WatchYourLAN/main/assets/Screenshot_3.png" alt="Screenshot_3"></a><br>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/aceberg/WatchYourLAN/main/assets/Screenshot_4.png"><img src="https://raw.githubusercontent.com/aceberg/WatchYourLAN/main/assets/Screenshot_4.png" alt="Screenshot_4"></a></p>
</details> 
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick start" href="#quick-start"></a></p>
<details>
  <summary>Expand</summary>
<p dir="auto">Replace <code>$YOURTIMEZONE</code> with correct time zone and <code>$YOURIFACE</code> with network interface you want to scan. Network mode must be <code>host</code>. Set <code>$DOCKERDATAPATH</code> for container to save data:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run --name wyl \
	-e &quot;IFACES=$YOURIFACE&quot; \
	-e &quot;TZ=$YOURTIMEZONE&quot; \
	--network=&quot;host&quot; \
	-v $DOCKERDATAPATH/wyl:/data/WatchYourLAN \
    aceberg/watchyourlan:v2"><pre>docker run --name wyl \
	-e <span><span>"</span>IFACES=<span>$YOURIFACE</span><span>"</span></span> \
	-e <span><span>"</span>TZ=<span>$YOURTIMEZONE</span><span>"</span></span> \
	--network=<span><span>"</span>host<span>"</span></span> \
	-v <span>$DOCKERDATAPATH</span>/wyl:/data/WatchYourLAN \
    aceberg/watchyourlan:v2</pre></div>
<p dir="auto">Web GUI should be at <a href="http://localhost:8840/" rel="nofollow">http://localhost:8840</a></p>
</details> 
<p dir="auto"><h2 tabindex="-1" dir="auto">Config</h2><a id="user-content-config" aria-label="Permalink: Config" href="#config"></a></p>
<details>
  <summary>Expand</summary>
<p dir="auto">Configuration can be done through config file, GUI or environment variables</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic config</h3><a id="user-content-basic-config" aria-label="Permalink: Basic config" href="#basic-config"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>TZ</td>
<td>Set your timezone for correct time</td>
<td></td>
</tr>
<tr>
<td>HOST</td>
<td>Listen address</td>
<td>0.0.0.0</td>
</tr>
<tr>
<td>PORT</td>
<td>Port for web GUI</td>
<td>8840</td>
</tr>
<tr>
<td>THEME</td>
<td>Any theme name from <a href="https://bootswatch.com/" rel="nofollow">https://bootswatch.com</a> in lowcase or <a href="https://github.com/aceberg/aceberg-bootswatch-fork">additional</a></td>
<td>sand</td>
</tr>
<tr>
<td>COLOR</td>
<td>Background color: light or dark</td>
<td>dark</td>
</tr>
<tr>
<td>NODEPATH</td>
<td>Path to local node modules</td>
<td></td>
</tr>
<tr>
<td>SHOUTRRR_URL</td>
<td>Link to any notification service supported by <a href="https://github.com/containrrr/shoutrrr">Shoutrrr</a> (gotify, email, telegram and others) or <a href="https://github.com/containrrr/shoutrrr/blob/main/docs/services/generic.md">Generic Webhook</a></td>
<td></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Scan settings</h3><a id="user-content-scan-settings" aria-label="Permalink: Scan settings" href="#scan-settings"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>IFACES</td>
<td>Interfaces to scan. Could be one or more, separated by space. Currently <code>docker0</code> is not supported, as <code>arp-scan</code> wouldn't work with it correctly</td>
<td></td>
</tr>
<tr>
<td>TIMEOUT</td>
<td>Time between scans (seconds)</td>
<td>120</td>
</tr>
<tr>
<td>ARP_ARGS</td>
<td>Arguments for <code>arp-scan</code>. See <code>man arp-scan</code> for more. Enable <code>debug</code> log level to see resulting command. (Example: <code>-r 1</code>)</td>
<td></td>
</tr>
<tr>
<td>LOG_LEVEL</td>
<td>Log level: <code>debug</code>, <code>info</code>, <code>warn</code> or <code>error</code></td>
<td>info</td>
</tr>
<tr>
<td>TRIM_HIST</td>
<td>Remove history after (hours)</td>
<td>48</td>
</tr>
<tr>
<td>HIST_IN_DB</td>
<td>Store History in DB - if <code>false</code>, the History will be stored only in memory and will be lost on app restart. Though, it will keep the app DB smaller (and InfluxDB is recommended for long term History storage)</td>
<td>false</td>
</tr>
<tr>
<td>USE_DB</td>
<td>Either <code>sqlite</code> or <code>postgres</code></td>
<td>sqlite</td>
</tr>
<tr>
<td>PG_CONNECT</td>
<td>Address to connect to PostgreSQL. (Example: <code>postgres://username:password@192.168.0.1:5432/dbname?sslmode=disable</code>). Full list of URL parameters <a href="https://pkg.go.dev/github.com/lib/pq#hdr-Connection_String_Parameters" rel="nofollow">here</a></td>
<td></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">InfluxDB2 config</h3><a id="user-content-influxdb2-config" aria-label="Permalink: InfluxDB2 config" href="#influxdb2-config"></a></p>
<p dir="auto">This config matches Grafana's config for InfluxDB data source</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
<th>Default</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>INFLUX_ENABLE</td>
<td>Enable export to InfluxDB2</td>
<td>false</td>
<td>true</td>
</tr>
<tr>
<td>INFLUX_SKIP_TLS</td>
<td>Skip TLS Verify</td>
<td>false</td>
<td>true</td>
</tr>
<tr>
<td>INFLUX_ADDR</td>
<td>Address:port of InfluxDB2 server</td>
<td></td>
<td><a href="https://192.168.2.3:8086/" rel="nofollow">https://192.168.2.3:8086/</a></td>
</tr>
<tr>
<td>INFLUX_BUCKET</td>
<td>InfluxDB2 bucket</td>
<td></td>
<td>test</td>
</tr>
<tr>
<td>INFLUX_ORG</td>
<td>InfluxDB2 org</td>
<td></td>
<td>home</td>
</tr>
<tr>
<td>INFLUX_TOKEN</td>
<td>Secret token, generated by InfluxDB2</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</details> 
<p dir="auto"><h2 tabindex="-1" dir="auto">Config file</h2><a id="user-content-config-file" aria-label="Permalink: Config file" href="#config-file"></a></p>
<details>
  <summary>Expand</summary>
<p dir="auto">Config file name is <code>config_v2.yaml</code>. Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="arp_args: &quot;&quot;
color: dark
hist_in_db: false
host: 0.0.0.0
ifaces: enp4s0
influx_addr: &quot;&quot;
influx_bucket: &quot;&quot;
influx_enable: false
influx_org: &quot;&quot;
influx_skip_tls: false
influx_token: &quot;&quot;
log_level: info
nodepath: &quot;&quot;
pg_connect: &quot;&quot;
port: &quot;8840&quot;
shoutrrr_url: &quot;gotify://192.168.0.1:8083/AwQqpAae.rrl5Ob/?title=Unknown host detected&amp;DisableTLS=yes&quot;
theme: sand
timeout: 60
trim_hist: 48
use_db: sqlite"><pre><span>arp_args</span>: <span><span>"</span><span>"</span></span>
<span>color</span>: <span>dark</span>
<span>hist_in_db</span>: <span>false</span>
<span>host</span>: <span>0.0.0.0</span>
<span>ifaces</span>: <span>enp4s0</span>
<span>influx_addr</span>: <span><span>"</span><span>"</span></span>
<span>influx_bucket</span>: <span><span>"</span><span>"</span></span>
<span>influx_enable</span>: <span>false</span>
<span>influx_org</span>: <span><span>"</span><span>"</span></span>
<span>influx_skip_tls</span>: <span>false</span>
<span>influx_token</span>: <span><span>"</span><span>"</span></span>
<span>log_level</span>: <span>info</span>
<span>nodepath</span>: <span><span>"</span><span>"</span></span>
<span>pg_connect</span>: <span><span>"</span><span>"</span></span>
<span>port</span>: <span><span>"</span>8840<span>"</span></span>
<span>shoutrrr_url</span>: <span><span>"</span>gotify://192.168.0.1:8083/AwQqpAae.rrl5Ob/?title=Unknown host detected&amp;DisableTLS=yes<span>"</span></span>
<span>theme</span>: <span>sand</span>
<span>timeout</span>: <span>60</span>
<span>trim_hist</span>: <span>48</span>
<span>use_db</span>: <span>sqlite</span></pre></div>
</details> 
<p dir="auto"><h2 tabindex="-1" dir="auto">Options</h2><a id="user-content-options" aria-label="Permalink: Options" href="#options"></a></p>
<details>
  <summary>Expand</summary>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Key</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>-d</td>
<td>Path to config dir</td>
<td>/data/WatchYourLAN</td>
</tr>
<tr>
<td>-n</td>
<td>Path to node modules (see below)</td>
<td></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</details> 
<p dir="auto"><h2 tabindex="-1" dir="auto">Local network only</h2><a id="user-content-local-network-only" aria-label="Permalink: Local network only" href="#local-network-only"></a></p>
<details>
  <summary>Expand</summary>
<p dir="auto">By default, this app pulls themes, icons and fonts from the internet. But, in some cases, it may be useful to have an independent from global network setup. I created a separate <a href="https://github.com/aceberg/my-dockerfiles/tree/main/node-bootstrap">image</a> with all necessary modules and fonts.
Run with Docker:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run --name node-bootstrap          \
    -p 8850:8850                          \
    aceberg/node-bootstrap"><pre>docker run --name node-bootstrap          \
    -p 8850:8850                          \
    aceberg/node-bootstrap</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="docker run --name wyl \
	-e &quot;IFACES=$YOURIFACE&quot; \
	-e &quot;TZ=$YOURTIMEZONE&quot; \
	--network=&quot;host&quot; \
	-v $DOCKERDATAPATH/wyl:/data/WatchYourLAN \
    aceberg/watchyourlan:v2 -n &quot;http://$YOUR_IP:8850&quot;"><pre>docker run --name wyl \
	-e <span><span>"</span>IFACES=<span>$YOURIFACE</span><span>"</span></span> \
	-e <span><span>"</span>TZ=<span>$YOURTIMEZONE</span><span>"</span></span> \
	--network=<span><span>"</span>host<span>"</span></span> \
	-v <span>$DOCKERDATAPATH</span>/wyl:/data/WatchYourLAN \
    aceberg/watchyourlan:v2 -n <span><span>"</span>http://<span>$YOUR_IP</span>:8850<span>"</span></span></pre></div>
<p dir="auto">Or use <a href="https://github.com/aceberg/WatchYourLAN/blob/main/docker-compose-local.yml">docker-compose</a></p>
</details> 
<p dir="auto"><h2 tabindex="-1" dir="auto">API</h2><a id="user-content-api" aria-label="Permalink: API" href="#api"></a></p>
<details>
  <summary>Expand</summary>
<p dir="auto">Moved to <a href="https://github.com/aceberg/WatchYourLAN/blob/main/docs/API.md">docs/API.md</a></p>
</details> 
<p dir="auto"><h2 tabindex="-1" dir="auto">Thanks</h2><a id="user-content-thanks" aria-label="Permalink: Thanks" href="#thanks"></a></p>
<details>
  <summary>Expand</summary>
<ul dir="auto">
<li>All go packages listed in <a href="https://github.com/aceberg/WatchYourLAN/network/dependencies">dependencies</a></li>
<li>Favicon and logo: <a href="https://www.flaticon.com/free-icons/access-point" rel="nofollow">Access point icons created by Freepik - Flaticon</a></li>
<li><a href="https://getbootstrap.com/" rel="nofollow">Bootstrap</a></li>
<li>Themes: <a href="https://bootswatch.com/" rel="nofollow">Free themes for Bootstrap</a></li>
</ul>
</details> 
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nearly half of Nvidia's revenue comes from four mystery whales each buying $3B+ (223 pts)]]></title>
            <link>https://fortune.com/2024/08/29/nvidia-jensen-huang-ai-customers/</link>
            <guid>41410450</guid>
            <pubDate>Sat, 31 Aug 2024 17:42:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fortune.com/2024/08/29/nvidia-jensen-huang-ai-customers/">https://fortune.com/2024/08/29/nvidia-jensen-huang-ai-customers/</a>, See on <a href="https://news.ycombinator.com/item?id=41410450">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Microchip designer <a href="https://fortune.com/company/nvidia/" target="_blank" aria-label="Go to https://fortune.com/company/nvidia/">Nvidia</a> more than doubled its second-quarter revenue thanks to just a handful of whales that accounted for nearly one out of every two dollars in sales the company booked.</p><div>



<p>Four customers, whose identities were kept anonymous for competitive reasons, directly purchased goods and services collectively worth 46% of Nvidia’s $30 billion in turnover. That amounts to roughly $13.8 billion, according to the company’s&nbsp;<a href="https://d18rn0p25nwr6d.cloudfront.net/CIK-0001045810/78501ce3-7816-4c4d-8688-53dd140df456.pdf" target="_blank" aria-label="Go to https://d18rn0p25nwr6d.cloudfront.net/CIK-0001045810/78501ce3-7816-4c4d-8688-53dd140df456.pdf" rel="noopener">10-Q regulatory filing</a>&nbsp;published alongside its&nbsp;<a href="https://fortune.com/2024/08/23/nvidia-jensen-huang-quarterly-earnings-tech-bull-market-ai/?utm_source=search&amp;utm_medium=suggested_search&amp;utm_campaign=search_link_clicks" target="_self" aria-label="Go to https://fortune.com/2024/08/23/nvidia-jensen-huang-quarterly-earnings-tech-bull-market-ai/?utm_source=search&amp;utm_medium=suggested_search&amp;utm_campaign=search_link_clicks">hotly anticipated</a>&nbsp;quarterly investor update.</p>



<p>Each was responsible for more than a tenth of overall top line, and their purchases were all related to its booming business selling chips to data centers, the likes of which entrepreneurs such as&nbsp;<a href="https://fortune.com/2024/04/29/elon-musk-tesla-ai-fsd-china-ev-sales/?utm_source=search&amp;utm_medium=suggested_search&amp;utm_campaign=search_link_clicks" target="_self" aria-label="Go to https://fortune.com/2024/04/29/elon-musk-tesla-ai-fsd-china-ev-sales/?utm_source=search&amp;utm_medium=suggested_search&amp;utm_campaign=search_link_clicks">Elon Musk</a>&nbsp;are in a&nbsp;<a href="https://fortune.com/2024/06/18/elon-musk-xai-memphis-supercomputer-site/" target="_self" aria-label="Go to https://fortune.com/2024/06/18/elon-musk-xai-memphis-supercomputer-site/">rush to build</a>&nbsp;amid the gold rush in artificial intelligence.&nbsp;</p>



<p>To put that into perspective, this quartet of customers contributed more in sales than Nvidia reported for the prior year’s period as a whole.&nbsp;</p>



<blockquote><p lang="en" dir="ltr">Video of the inside of Cortex today, the giant new AI training supercluster being built at <a href="https://fortune.com/company/tesla/" target="_blank" aria-label="Go to https://fortune.com/company/tesla/">Tesla</a> HQ in Austin to solve real-world AI <a href="https://t.co/DwJVUWUrb5" target="_blank" aria-label="Go to https://t.co/DwJVUWUrb5" rel="noopener">pic.twitter.com/DwJVUWUrb5</a></p>— Elon Musk (@elonmusk) <a href="https://twitter.com/elonmusk/status/1827981493924155796?ref_src=twsrc%5Etfw" target="_blank" aria-label="Go to https://twitter.com/elonmusk/status/1827981493924155796?ref_src=twsrc%5Etfw" rel="noopener">August 26, 2024</a></blockquote>



<p>Although the names of the mystery AI whales are not known, <a href="https://finance.yahoo.com/news/nvidias-largest-customers-201900375.html?" target="_blank" aria-label="Go to https://finance.yahoo.com/news/nvidias-largest-customers-201900375.html?" rel="noopener">they are likely to include</a> Amazon, Meta, Microsoft, Alphabet, OpenAI, or Tesla.</p>



<p>Nvidia’s hottest products are AI chips like the H200. These are needed to train large language models like OpenAI’s GPT-4. They are also used to power inference, the process that ChatGPT or&nbsp;<a href="https://fortune.com/2024/02/16/sora-openai-sam-altman-text-to-video-generative-ai/" target="_self" aria-label="Go to https://fortune.com/2024/02/16/sora-openai-sam-altman-text-to-video-generative-ai/">Sora</a>&nbsp;uses to generate answers to text-based prompts.</p>



<p>This dependency on a handful of major customers also highlights a rising concern in the market over just how sustainable this abrupt, exponential growth from just one corner of its business can be. Some investors like&nbsp;<a href="https://fortune.com/2024/08/02/ai-nvidia-intel-arm-stock-bubble-elliott/" target="_self" aria-label="Go to https://fortune.com/2024/08/02/ai-nvidia-intel-arm-stock-bubble-elliott/">Elliott Management</a>&nbsp;and Citadel have&nbsp;<a href="https://fortune.com/2024/07/02/ken-griffin-citadel-generative-ai-hype-openai-mira-murati-nvidia-jobs/?utm_source=search&amp;utm_medium=suggested_search&amp;utm_campaign=search_link_clicks" target="_self" aria-label="Go to https://fortune.com/2024/07/02/ken-griffin-citadel-generative-ai-hype-openai-mira-murati-nvidia-jobs/?utm_source=search&amp;utm_medium=suggested_search&amp;utm_campaign=search_link_clicks">voiced skepticism</a>&nbsp;over how long this can be maintained.</p>



<p>History does give reason for concern. The semiconductor industry is known for its boom-and-bust cycles.</p>



<p>Shares in Nvidia are expected to open lower on Thursday, underperforming the broader equity market.</p>



<h2>One customer provided revenue greater than Nvidia’s second-largest business</h2>



<p>In fact Nvidia’s business ties with these whales are so significant the company flags them in a section of its quarterly reports titled “concentration of revenue,” which is dedicated to cluster risks.</p>



<p>“We have experienced periods where we receive a significant amount of our revenue from a limited number of customers,” it stated in its 10-Q regulatory filing, “and this trend may continue.”&nbsp;</p>



<p>It’s a trend that is staggeringly profitable. Nvidia pocketed $5.60 out of every $10 of revenue it made over the entire first half as net income—margins most companies can only dream of.&nbsp;</p>



<p>That explains why profit after tax nearly quadrupled to $31.5 billion during this six-month period over the previous year. Whether revenue, and therefore earnings, can continue to grow at this blistering pace is crucial to its investment story.</p>



<p>Take “Customer B” cited in the filing, for example: Its direct purchases represented 11% of Nvidia’s $30 billion in revenue. That means a single company contributed more in business than the group’s second largest division—gaming, with $2.9 billion—did as a whole.</p>



<p>Customer B, however, remained below the 10% threshold for the entire first half, which suggests it significantly ramped up spending during the past quarter seemingly out of the blue. The exact same could be said about “Customer C”; the numbers provided by Nvidia are identical.&nbsp;</p>



<blockquote><p lang="qme" dir="ltr"><a href="https://twitter.com/search?q=%24NVDA&amp;src=ctag&amp;ref_src=twsrc%5Etfw" target="_blank" aria-label="Go to https://twitter.com/search?q=%24NVDA&amp;src=ctag&amp;ref_src=twsrc%5Etfw" rel="noopener">$NVDA</a> <a href="https://t.co/Lym4uHBCXs" target="_blank" aria-label="Go to https://t.co/Lym4uHBCXs" rel="noopener">pic.twitter.com/Lym4uHBCXs</a></p>— Jesse Cohen (@JesseCohenInv) <a href="https://twitter.com/JesseCohenInv/status/1828404561867923571?ref_src=twsrc%5Etfw" target="_blank" aria-label="Go to https://twitter.com/JesseCohenInv/status/1828404561867923571?ref_src=twsrc%5Etfw" rel="noopener">August 27, 2024</a></blockquote>



<p>Speaking to Bloomberg TV on Wednesday, CEO Jensen Huang answered a question on where demand is coming from, beyond the handful of hyperscalers like <a href="https://fortune.com/company/microsoft/" target="_blank" aria-label="Go to https://fortune.com/company/microsoft/">Microsoft</a>, <a href="https://fortune.com/company/alphabet/" target="_blank" aria-label="Go to https://fortune.com/company/alphabet/">Google</a>, and <a href="https://fortune.com/company/amazon-com/" target="_blank" aria-label="Go to https://fortune.com/company/amazon-com/">Amazon</a>.</p>



<p>“We’re relatively diversified today,” he&nbsp;<a href="https://www.youtube.com/watch?v=NC5NZPrxbHk&amp;t=99s" target="_blank" aria-label="Go to https://www.youtube.com/watch?v=NC5NZPrxbHk&amp;t=99s" rel="noopener">claimed</a>, citing a range of different customer groups.&nbsp;</p>



<p>Yet his own company’s numbers appear to dispute that conclusion. This time last year, for example, there were no direct customers whose business made up 10% or more of total revenue—neither for the first nor the second quarter.</p>



<p>Nvidia didn’t immediately respond to a request from&nbsp;<em>Fortune</em>&nbsp;for comment.</p></div><div data-cy="subscriptionPlea"><p><strong>Recommended reading:</strong><br>In our new special issue, a Wall Street legend gets a radical makeover, a tale of crypto iniquity, misbehaving poultry royalty, and more.<br><a href="https://fortune.com/packages/digital-issue-kkr/?&amp;itm_source=fortune&amp;itm_medium=article_tout&amp;itm_campaign=kkr_issue" target="_self" aria-label="Go to https://fortune.com/packages/digital-issue-kkr/?&amp;itm_source=fortune&amp;itm_medium=article_tout&amp;itm_campaign=kkr_issue">Read the stories.</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Client-side QR code generator with SVG output (121 pts)]]></title>
            <link>https://fietkau.software/qr</link>
            <guid>41410442</guid>
            <pubDate>Sat, 31 Aug 2024 17:42:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fietkau.software/qr">https://fietkau.software/qr</a>, See on <a href="https://news.ycombinator.com/item?id=41410442">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" tabindex="-1"><div><p>QRSVG is a small JavaScript project to render a two-dimensional bitmask (mostly assumed to be a QR code) to an SVG element as a collection of SVG paths with defined purposes.</p></div><section><h2>Project Description</h2><p><a href="https://en.wikipedia.org/wiki/QR_code">QR codes</a> have established themselves as a popular way to provide a chunk of digital information (most commonly a web address to jump to) via physical media. I myself occasionally use them in flyers, posters, or presentation slides.</p><p>They are not too complicated to generate and a variety of free websites exist for this purpose. Sadly none of them appear to have all of the customization options that I want, plus most of them use some manner of ads and/or data tracking. The demo on this page combines <a href="https://www.nayuki.io/page/qr-code-generator-library">Project Nayuki’s QR Code generator library</a> (a multi-language open source project that can, among other things, perform the conversion of text into raw QR code data) with my own QRSVG project, which can turn a QR-like two-dimensional boolean data map into an efficient vector description of its own visualization by tracing the contours of contiguous shapes. Or in simpler terms: it turns a QR code into a straightforward and customizable SVG file. The demo up top showcases a number of stylistic customizations enabled by QRSVG. Its strengths include:</p><ul><li>Conversion of QR code data into efficient SVG markup</li><li>Shape and color customization</li><li>QR codes can be made downloadable in SVG and PNG formats</li><li>Entirely client-side, hostable as static files (i.e. making this available to you costs me nothing)</li></ul><p>If the demo QR code generator at the top of this page is convenient for you too, please feel free to spread the word!</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My fake job in Y2K preparedness (135 pts)]]></title>
            <link>https://www.nplusonemag.com/issue-48/essays/the-contingency-contingent/</link>
            <guid>41409891</guid>
            <pubDate>Sat, 31 Aug 2024 16:23:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nplusonemag.com/issue-48/essays/the-contingency-contingent/">https://www.nplusonemag.com/issue-48/essays/the-contingency-contingent/</a>, See on <a href="https://news.ycombinator.com/item?id=41409891">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>I<span>n rare moments</span></span> within the history of capitalism, too few workers exist. Not as an absolute, of course: in total, workers always outnumber paid possibilities for work; that’s how our economy functions. But in a specific industry, a shortage may emerge, if only for a brief time. In 1998, on my first day of work as an analyst with the accounting and consulting firm Arthur Andersen LLP, it was clear that some aberration in accumulation had placed me in the twenty-fourth-floor conference room of a Manhattan skyscraper, overlooking the Museum of Modern Art’s sculpture garden, staring at a PowerPoint presentation for new hires. On one slide was a cartoon duck, bespectacled and presented in profile, standing on the tiptoes of its webbed feet. In its hands was a sledgehammer, held overhead, ready to smash a desktop computer.</p><p>I would learn soon enough that this—my first professional and only corporate work experience—was a fake job. It was fake because although I worked with Arthur Andersen, I never worked <span><i>for</i></span> them. (Their shade of blue-chip firm would never have hired the likes of me: they recruited from places like Harvard; I’d recently graduated from Hampshire College. They required new hires to have impeccable GPAs; my school didn’t confer grades.) Instead, I was employed by a global advertising conglomerate that had hired Andersen on a consultancy project and then placed me alongside the Andersen team—the result of a confluence of staffing shortages.</p><p>It was, moreover, a fake job because Andersen was faking it. The firm spent the late 1990s certifying fraudulent financial statements from Enron, the Texas-based energy company that made <span><i>financial</i></span> <span><i>derivatives</i></span> a household phrase, until that company went bankrupt in a cloud of scandal and suicide and Andersen was convicted of obstruction of justice, surrendered its accounting licenses, and shuttered. But that was later.</p><p>Finally, it was a fake job because the problem that the Conglomerate had hired Andersen to solve was not real, at least not in the sense that it needed to be solved or that Andersen could solve it. The problem was known variously as Y2K, or the Year 2000, or the Y2K Bug, and it prophesied that on January 1, 2000, computers the world over would be unable to process the thousandth-digit change from 19 to 20 as 1999 rolled into 2000 and would crash, taking with them whatever technology they were operating, from email to television to air-traffic control to, really, the entire technological infrastructure of global modernity. Hospitals might have emergency power generators to stave off the worst effects (unless the generators, too, succumbed to the Y2K Bug), but not advertising firms.</p><p>With a world-ending scenario on the horizon, employment standards were being relaxed. The end of the millennium had produced a tight labor market in knowledge workers, and new kinds of companies, called dot-coms, were angling to dominate the emergent world of e-commerce. Flush with cash, these companies were hoovering up any possessors of knowledge they could find. Friends from my gradeless college whose only experience in business had been parking-lot drug deals were talking stock options.</p><p>The employment agency through which I got my fake job made no epistemological distinction between knowing something and knowing about something. JavaScript, for example, a computer programming language: I knew about it but did not know it. Fine. Was I aware that the modern world might go on a catastrophic hiatus of unknown duration at the end of 1999? I had heard of something to that effect, although if pressed I couldn’t have explained. The Conglomerate was not much more stringent. My interview consisted of about twelve minutes with a laconic, mustachioed, middle-aged Arthur Andersen manager named Dick. (One of the services Andersen had been asked to provide was to help hire the Conglomerate’s Y2K team.) “On a scale of one to ten, what’s your knowledge of computer software?” he began. I paused for a moment, unsure of whether our interview would include a demonstrative component, as had so many previous interviews for jobs I had not gotten. But his office was empty. I couldn’t see how he would test me. I said <span><i>eight</i></span>.</p><blockquote><p><b>At a certain point all that had happened yesterday was our documenting, so then we documented that.</b></p><b> <a onclick="return popitup('https://twitter.com/share?text=At+a+certain+point+all+that+had+happened+yesterday+was+our+documenting%2C+so+then+we+documented+that.+%7C+n%2B1+%7C&amp;lang=en&amp;url=https%3A%2F%2Fwww.nplusonemag.com%2F%3Fp%3D54624')" href="https://twitter.com/share?text=At+a+certain+point+all+that+had+happened+yesterday+was+our+documenting%2C+so+then+we+documented+that.+%7C+n%2B1+%7C&amp;lang=en&amp;url=https%3A%2F%2Fwww.nplusonemag.com%2F%3Fp%3D54624">Tweet</a></b></blockquote><p>“And what’s your knowledge of computer hardware, on a scale of one to ten?” Dick continued. The moment called for both boldness and modesty. I felt committed to eight, a number I had long appreciated for its intimations of infinity when turned sideways. So I repeated myself: “Eight.” It was true that one of my campus work-study jobs had been as a computer lab monitor. I could restart a PC or refill a printer’s paper tray if the situation demanded it, although it rarely did. I’d had the weekend evening shifts.</p><p>Dick’s next question would determine the development of my nascent career. “And what’s your level of problem-solving, on a scale of one to ten?” I sensed, suddenly, an opportunity to pass through a corporate loophole, to surmount my lack of credentials, training, and touch-typing skills and lean into my historical moment. “Nine,” I replied.</p><p>“That was it?” my girlfriend chirped when I reported to her the climax of my interview. The Conglomerate had hired me on the spot.</p><hr><p><span>I </span><span><span>was placed</span> on</span><span> the quality assurance team, assisting with work the Conglomerate had contracted to be carried out exclusively by the Arthur Andersen contingent. They weren’t called “a contingent” around the Y2K office. The consultants were referred to as the Andersen people, a term that exaggerated the partition between them and other Conglomerate employees, from whom they commanded both bewilderment and respect and, as often follows from the first two, a certain amount of resentment. These contradictory feelings stemmed from the fact that management consultants constituted then—and still do now—the vanguard of corporate work. They flit between companies and industries, parachuting in to diagnose problems and suggest, although rarely implement, “best-practice” solutions. It’s management consulting lingo, </span><span><i>best-practice</i></span><span>, and it indicates that the good is not enough: this isn’t Winnicott’s consulting room. Instead, management consultants aim for superlatives. They hire the best. They practice the best. They claim Pete Buttigieg among their alumni ranks. At the Conglomerate they had their own offices, their own meetings, their own schedules. They worked from the Conglomerate’s office space while they were on its project, but when they “rolled off” (they had their own terminology, too) they would take up residence within the ambit of another Andersen client. </span></p><p>“You’ve been selected because you’re Andersen quality but not Andersen price,” explained Cindy, the chipper data-warehousing expert who served as the quality assurance team’s Andersen-employed leader, on my second day of work. I felt like a piece of organic fruit found in the conventional produce bin. Cindy also told me that, as with wolves, pack solidarity was intense within Andersen groupings. So was a wariness toward intruders.</p><p>In the first meeting with my lupine team, I did stand out, though not for lack of credentials. In fact I was the only person not massaging a fistful of Play-Doh. The Conglomerate’s office was beige, the carpet beige, the people beige, too. But from each of the four quality assurance team members’ hands exploded a most brilliant collection of colors: neon greens and yellows, hot pink, siren red, an almost psychedelic scene. Team members twirled and juggled their own handfuls, separated their globs and recombined them. “It’s a new management technique,” Cindy said, before I could ask. “It helps relieve stress.”</p><blockquote><p><b>Most team members had their own Y2K doomsday clock on their desk, tracking the years, months, days, hours, minutes, seconds, and milliseconds until techno-rapture.</b></p><b> <a onclick="return popitup('https://twitter.com/share?text=Most+team+members+had+their+own+Y2K+doomsday+clock+on+their+desk%2C+tracking+the+years%2C+months%2C+days%2C+hours%2C+minutes%2C+seconds%2C+and+milliseconds+until+techno-rapture.+%7C+n%2B1+%7C&amp;lang=en&amp;url=https%3A%2F%2Fwww.nplusonemag.com%2F%3Fp%3D54624')" href="https://twitter.com/share?text=Most+team+members+had+their+own+Y2K+doomsday+clock+on+their+desk%2C+tracking+the+years%2C+months%2C+days%2C+hours%2C+minutes%2C+seconds%2C+and+milliseconds+until+techno-rapture.+%7C+n%2B1+%7C&amp;lang=en&amp;url=https%3A%2F%2Fwww.nplusonemag.com%2F%3Fp%3D54624">Tweet</a></b></blockquote><p><span>Cindy herself did not seem stressed, but the situation she laid out for our team was certainly overwhelming. The Conglomerate held more than a thousand advertising, public relations, and communication companies worldwide, with possessions in both emerging markets and well-established ones. These cumulatively raked in billions of dollars per year. But any or all of these “shops,” in industry lexicon, might collapse as one millennium ended and another began, preventing the Conglomerate from executing its global advertising operations and compromising its earnings and thus its stock price. Perhaps a radio station in Finland would go haywire and be unable to run a Conglomerate-booked spot; conceivably, a television station in Western Australia could disappear and, along with it, a Conglomerate-produced commercial; maybe a billboard would tumble off a highway in Rio de Janeiro amid social chaos, destroying a Conglomerate-created visual. If such events were to transpire, revenue loss would be a best-case scenario; one could imagine far worse. There had been a slide in the new-hire presentation, the one with the duck, that spelled out the possibilities: a stick figure with a thought bubble hovering above its perfectly circular head wondered: “On Jan. 1, 2000 will I still have: electricity, food, telephone, transportation</span><span> </span><span>.</span><span> </span><span>.</span><span> </span><span>. ?” Each life-sustaining noun was contained in its own thought bubble, and the final thought bubble offered only a series of anxiety-producing question marks (“?????”). </span></p><p><span>That same tremulous atmosphere permeated the office, where a thumbtacked photocopy of a </span><span><i>Computerworld</i></span><span> article entitled “Economist Predicts Y2K-Based Recession” greeted team members on their daily arrival at the elevator bay. In the windowless kitchenette, several copies of the book </span><span><i>Time Bomb 2000</i></span><span> were available for employees to peruse while they warmed their instant coffee or selected a Pepsi product—Pepsi being a client—from the mini fridge. Most team members had their own Y2K doomsday clock on their desk, tracking the years, months, days, hours, minutes, seconds, and milliseconds until techno-rapture. </span></p><p>The first segment of the Conglomerate’s Y2K project, “Phase I: Inventory,” involved retrieving technological inventories from the Conglomerate’s various agencies and recording them in a database. The lesser, non-Andersen network analysts entered data about the number of PCs, routers, fax machines, and printers any given Conglomerate shop had in its possession, copying the material from a spreadsheet said shop had emailed them. The superior Andersen quality assurance analysts (and I) provided the oversight, identifying the mistakes our colleagues had made. Someone had hit the <span><i>0</i></span> instead of its keyboard neighbor, the <span><i>9</i></span>; another team member, her mind wandering perhaps, had neglected to check the “entry completed” box. Little errors, but the general sense was that history would not judge them kindly. My team set out to rectify them before it had the chance.</p><p>The verb under whose sign the quality assurance team labored was a new one for me: to <span><i>QA</i></span>, past tense <span><i>QAed</i></span>. All mistakes our QAing located were recorded by hand; all handwritten records of our QAing were photocopied; the copies were kept in Cindy’s office while the originals were bound for a secure document warehouse in New Jersey. Necessarily, our records would be kept on paper. If the predicted Doomsday 2000 did arrive, the digital world would be inaccessible, maybe gone forever. Computer technology had gotten us into this millennial quagmire and could not be trusted to extricate us from it. Perhaps that’s why my problem-solving abilities had mattered more than my knowledge of computer software and hardware on Dick’s scale of one to ten. The fact that the scale itself remained undefined, and that neither interviewer nor interviewee agreed to its coordinates, suggested both the fake nature of the whole endeavor as well as the desperation the Conglomerate felt, its keen awareness that it was running out of time.</p><hr><p><span>T<span>he Andersen position</span></span> was that “Y2K is a documentation problem, not a technology problem.” One could not know the magnitude of the technical problems we would face on January 1, 2000, with complete certainty until that revelatory day arrived, and so 1/1/2000 functioned as a kind of horizon of contingency: would we all be launched, <span><i>Back to the Future</i></span>–like, into a new stone age, or would a few rest-area vending machines conk out and everyone get on with it? To contend with this range of possible futures, we focused on the past. Instead of fixing things with the hope that they would function later, we would document the anti-Y2K efforts the Conglomerate had already undertaken, necessarily more knowable than those events not yet in existence. We would not, for legal reasons, promise things to come; we would certify that things that had already transpired had been appropriately recorded. To the alarming claim of the Y2K town criers—modern life after 12/31/1999 cannot be guaranteed—the quality assurance team responded retrospectively: whatever happened yesterday would be consigned to a database. At a certain point all that had happened yesterday was our documenting, so then we documented that. Then, exponentially, we had to document ourselves documenting our own documentation.</p><blockquote><p><b>Months of reading and populating spreadsheets turned me spreadsheet-like myself.</b></p><b> <a onclick="return popitup('https://twitter.com/share?text=Months+of+reading+and+populating+spreadsheets+turned+me+spreadsheet-like+myself.+%7C+n%2B1+%7C&amp;lang=en&amp;url=https%3A%2F%2Fwww.nplusonemag.com%2F%3Fp%3D54624')" href="https://twitter.com/share?text=Months+of+reading+and+populating+spreadsheets+turned+me+spreadsheet-like+myself.+%7C+n%2B1+%7C&amp;lang=en&amp;url=https%3A%2F%2Fwww.nplusonemag.com%2F%3Fp%3D54624">Tweet</a></b></blockquote><p>These efforts—designed to avoid the liabilities of selling a project that offered a Y2K fix, because what if Y2K wasn’t “fixable”?—were part of a floating corporate-compliance rubric referred to only as the Process. Proprietary, capitalized, and always prefaced with that exclusive definite article, the Process could only be implemented by Arthur Andersen LLP. When a business engaged Andersen in a consultancy project, they purchased the Process. The Process seemed to have no printed home. To the extent that it existed, it passed between the minds and speech of Cindy and Dick. It was both accessible enough to be constantly appealed to—“the Process says,” “How would we apply the Process here?” “I’m not sure this syncs with the Process”—and inaccessible enough never to be grasped with Play-Doh–like tangibility. In keeping with the Process, the quality assurance team scoured hundreds of thousands, millions even, of Excel spreadsheet lines. My day began and ended with comparing database entry to inventory spreadsheet and writing in my own script <span><i>Column 4, row 14: “18 routers” should be corrected to read “8 routers.” Column 9, row 10, empty. Indicate yes or no.</i></span> This was dissociative, diminutive work, and time passed vertiginously, minutes and seconds overlapping on my teammates’ unsynchronized doomsday clocks until the day concluded with a final collective effort at preservation. All quality assurance analysts—not Cindy, for hierarchical reasons—would convene in the photocopy room to ensure that, come what may on December 31, 1999, the deliverables from our day’s labor would be archived, in print, in perpetuity. The only times we really spoke to each other in an uncurated fashion were over the blinding flashes and tray clicks of the Minolta copy machine. There we would discuss our commuting woes, roommate dramas, awkward dates, student loan repayment plans, as well as a rather unarticulated feeling of: What?</p><hr><p><span>W<span>ho managed</span> the management consultants?</span> Cindy, shepherd of the Process, conducted random checks of all QAed spreadsheets and kept the results in her own spreadsheet. Dick and an elusive cabal of Andersen upper-tier management QAed Cindy. Once a rotund Andersen partner named Benjamin, outfitted in a yarmulke, a three-piece suit, and those unfortunate slipper-like Merrell suede shoes just making their way into the world in the late 1990s, spent an hour in the office, presumably to QA Dick.</p><p>The Conglomerate’s Y2K managing director, Justin, had constructed a makeshift smoking lounge in the utility closet across the hall from my cubicle. It was there that Justin and Dick would liaise and swap status updates, and Justin often hosted other visitors in his windowless, walk-in humidor. Invariably these were older, suited white men, a world of Dicks. I’d try to eavesdrop, to pry bits of information from the clouds of secondhand smoke that escaped under the door and hovered momentarily about my cubicle before being absorbed into the carpet. A few gems did waft out. I learned, for example, that I wasn’t the only team member to whom Cindy had described Arthur Andersen LLP as a pack of wolves: slender, agile, fangs bared, lips curled, marking their territory with urine (complimentary). When Justin heard this interspecies analogy, he balked. “More like bloodsucking leeches. They overcharge. Bullshit their billable hours. They want my employees to fuck up so I’ll fire them and they can bring in more Andersen people.<span> </span>.<span> </span>.<span> </span>. Of course they’re part of our team.”</p><p>But what, really, was there to hear? That the Process was vertically integrated apocrypha? That Dick had knowledge of his firm’s various fraudulent endeavors at Enron and was up to something similar here? In fact, Dick and Cindy could not have been more devoted, insistent as they were on evangelizing the Process to the Conglomerate’s office. “Y2K is a documentation problem, not a technology problem,” Cindy repeated at our weekly analyst meetings, and she invited us to say it with her.</p><p>Several months into my employment, in November 1998, a group of Andersen and Conglomerate elite visited our Y2K office to see what the $4 million Andersen project had produced in terms of Y2K preparedness. CEOs, COOs, CFOs, CTOs, an alphabet of executives, all men and some with aides-de-camp, filed through our office into a closed meeting. After their departure, Justin invited all team members into the conference room, where a Roman banquet of muffins and fruit pyramids, a well-appointed coffee cart, and the usual assortment of Pepsi products greeted us. None of these provisions had been touched, and team members were invited to pick through the refuse. “We just had over one hundred million dollars’ worth of executives in here,” Justin began. “They wanted updates.”</p><p>We were encouraged to review for ourselves a paper copy of the executive council presentation, many of which lay scattered about the oblong conference table. I was perplexed to see that it was essentially a rehash of the new-hire presentation. The cartoon duck holding the sledgehammer. The stick figure wondering, “On Jan. 1, 2000 will I still have<span> </span>.<span> </span>.<span> </span>. ?” The floating text box, conceptual art–like in display, “Y2K is a documentation” et cetera. But the mood in the conference room was ebullient and self-assured. Even the always starched and monogrammed Dick looked pleased. The man seldom spoke outside of closed-door klatches, but as a measure of the morning’s importance, he was invited by Justin to address the team. He rose and delivered a brief, litigious soliloquy:</p><p>“When January first of the year 2000 hits and a floodgate of Y2K lawsuits descends, and [we’re] being sued by everyone, and the firms who aren’t suing us, we’re suing, the indemnification issues, the claims of fiduciary responsibility and accusations of abandonment thereof. What are we going to need? Proof. They’re going to want to know what we did and how we did it. So that’s what this Council is concerned with, proof, paper, documentation. And that’s why our battle plan is simple: keep documenting, team.”</p><hr><p><span>M<span>onths of reading</span></span> and populating spreadsheets turned me spreadsheet-like myself: capacious but conceptless, able to record, to list, and to organize and sort, but according to an unthought, unthinkable orientation. After six weeks or so, and without much consideration, I began producing my own documentation, a kind of casual corporate ethnography. I noted the distribution of titles and nicknames at the Conglomerate. All the managers were men; all the analysts were women. Cindy was referred to as “the den mother.” Dick had an alliterative title that recognized his central place: “Dick, the documentation expert.” Justin, our commander, was hailed as “the general.” I, along with another young woman and quality assurance analyst, an engineer by training who shared my hair color, were given the clunky moniker “the blond QA twins.”</p><p>My favorite team member was a Viennese network analyst named Magdalena, who went by the German diminutive Leni. Tall and thin, accented and bespectacled, she hailed from fallen Austrian nobility. Her family had lost seven-eighths of their estate in World War I, but they nonetheless remained comfortable enough not to have to concern themselves with bourgeois trifles like work or mortgages. Had her father been a Nazi? Indeed, he had. Imagine a Captain von Trapp–like personality who had never softened under the melodious ensorcellment of a Fraulein Maria. Leni had fled her father’s eastern Reich not by yodeling her way over the Alps but through an American husband who got her a green card, then a transnational law firm that got her experience, and then, finally, an analyst position at the Conglomerate. When she interviewed with Dick, she had been asked to rate her foreign language abilities on his mysterious scale. Fluent in German, English, and French, proficient in Spanish, she had given herself a ten.</p><p>Each Wednesday we gathered for a general meeting with all team members: the database programmers, who never spoke; the disheveled but crucial IT coterie; all the Andersen people; Conglomerate managers; Dick. Sometimes the receptionist even set the phone to an automated answer function and popped by. By January 1999, after five months on the job, I’d come to find some comfort in the routine coordinates of the general meeting. At 2 <span>PM</span>, everyone except Justin would be seated in the conference room, Pepsi product in one hand, Cindy-authored agenda in the other, waiting for the meeting’s commencement, at which point Justin would announce through the all-office intercom that the meeting would begin five minutes late. Those minutes would pass and he would trudge in, a cloud of smoke all but trailing him.</p><p>General meetings involved an anonymous all-team exercise called “What’s Good? What Needs Improving?” Team members were instructed to rip a piece off some desolate corner of their 8 x 11 agenda and compose two reflections on these stated questions. The compositions were then passed to the table’s head for Justin to read.</p><p>Justin read aloud one Wednesday from the shrinking pile of anonymous comments. “What’s good: getting a lot done. Needs improving: kitchen out of Sweet’n Low.” And then: “What’s good: enjoyed my office Secret Santa gift. Needs improving: I don’t understand what the media team does and I will not accept ‘ask a member of the media team’ for an answer.” As he read, Justin’s countenance shifted from curiosity to uncertain disdain. He became aware—as did all team members—that his managerial style, his habit of punting, was being subtly mocked. Not only that: his antagonist was sitting among us. With a sly smile, somewhere between intrigued and embarrassed, he asked: “Who wrote this?” This query caused a swift reaction in Cindy, who responded, “This is an anonymous exercise.” Justin interrupted with <span><i>Thank you, Cindy</i></span>, who, so chastened, let Justin continue: “I asked—who wrote this?”</p><p>Dead silence of the urban kind only a skyscraper can provide, where a soft machinic din emits from the building itself. Team members, careful to avoid potentially implicating eye contact with each other, fixed their downcast stares on their Pepsi beverages. “You don’t want to tell me?” said Justin. “Fine. We’ll sit here all goddamn day. We’ll sit here till Y2K. Because I’m going to find out who wrote this fucking note.”</p><p><span>Sitting to the right of Cindy, an Andersen analyst named Tracy reached over, took the offending paper scrap, and meekly announced to the nervous audience, “It’s a girl’s handwriting.” I couldn’t have felt more relieved. My script is scraggly and angular, the furthest thing from that of a girl. Tracy too must have felt some relief, having just exculpated herself. And Cindy—who would suspect her? As team members began scanning the conference table, privately searching for someone whose handwriting was bubbly and who menstruated, the silence broke.</span></p><p>“I did it—I wrote the note.” Leni! Daughter of a Nazi. “It’s just that you never answer questions and I thought—well, I thought: You’re the boss, you will know.”</p><p>Justin ended the meeting by ordering her to stay alone with him in the conference room. Later I asked what Justin had said to her in private. “That I’m ‘on his shit list,’” she told me. And what did she say? “Fine.”</p><hr><p><span>I<span>n February,</span></span> with ten months left until the possible techno-finale of modernity, the Conglomerate decided to expand its Y2K preparedness operations and bring more Andersen people into the office. The new consultants also seemed to be wondering what the media team did. Located in an office directly behind my cubicle, they appeared to occupy themselves in the same way we did: printing spreadsheets, photocopying them, faxing them, and placing them in binders. Where the media team distinguished itself was in its leadership: it was the one team in the Conglomerate’s Y2K shop run by a manager of color. Perhaps the only bit of knowledge I had of this manager was that he had somehow managed to board a plane to Brazil without having secured the necessary entrance visa. When he arrived in São Paulo he was detained, denied passage, and put on a flight back to NYC. He’d blamed the corporate travel agent, the gay and moody Carlos, and demanded his firing.</p><p>Instead, the Andersen axe fell on him. In a presentation to the Conglomerate higher-ups, Dick and a group of Andersen top brass declared that the media team was floundering. Their recommendation: the Conglomerate should fire its one Black Y2K manager, clear out any of his remaining loyalists on what came to be known as the old media team, bring yet more Andersen consultants into the office, and let them staff a new media team. Heather, the Andersen person who had conceived this coup de grâce, naturally took the reins. Her first move was into the old media team’s office; her second charge was to populate her team. She needed someone familiar with the Process, someone of Andersen quality but not Andersen price. “You’d actually be helping with critical management considerations,” Cindy said, after she’d invited me, for the first time since my hiring, into her office. “Think of it as a promotion.”</p><p><span>I didn’t know what the media team did, and Leni had been placed on a private shit list for even asking. And the quality assurance team was almost finished with our QA of the inventory database—we had located around 6,700 typos, many of which had been remediated and all of which had been documented as the result of our efforts. After such a collective accomplishment, it felt like an odd time to jump ship. In talking to Cindy, however, I realized that just as the offer to join the media team was to be </span><span><i>thought of</i></span><span> as a promotion, it was also to be </span><span><i>thought of</i></span><span> as a choice. </span></p><p>Changing teams meant learning the ways of a new Cindy, namely a Heather, whose ward I became and whose office I now shared. In contrast to Cindy’s genuine enthusiasm for everything about management consulting, from the Play-Doh to the Process, Heather had a hardened corporate cynicism about her. She daily took calls from her interior decorator, Yves, who always seemed to be fifteen blocks away selecting upholstery and end-table pairings at Upper East Side boutiques. In those days before smartphone image-sharing, Yves would engage in elaborate ekphrastic descriptions of this piece or that. Heather put these conversations on speakerphone so she could continue typing with both hands, her manicured nails dancing above the keyboard with such intensity that often she had to ask Yves to repeat himself.</p><blockquote><p><b>It was one of those out-of-body, disassociated experiences, so often provoked by trauma but here provoked by management consulting.</b></p><b> <a onclick="return popitup('https://twitter.com/share?text=It+was+one+of+those+out-of-body%2C+disassociated+experiences%2C+so+often+provoked+by+trauma+but+here+provoked+by+management+consulting.++%7C+n%2B1+%7C&amp;lang=en&amp;url=https%3A%2F%2Fwww.nplusonemag.com%2F%3Fp%3D54624')" href="https://twitter.com/share?text=It+was+one+of+those+out-of-body%2C+disassociated+experiences%2C+so+often+provoked+by+trauma+but+here+provoked+by+management+consulting.++%7C+n%2B1+%7C&amp;lang=en&amp;url=https%3A%2F%2Fwww.nplusonemag.com%2F%3Fp%3D54624">Tweet</a></b></blockquote><p><span>She overheard me, too, of course. Leni had learned that our Y2K office had a toll-free number that, in the case of an advertising-oriented Y2K emergency, Conglomerate employees could call to solicit expert advice: 1-800-Y2K-SAVE. But it was March—hardly time for the world’s advertisers to panic about a Time Bomb 2000 detonation—and the line was usually quiet. In the meantime, team members in the know had begun giving out the office 800 number. Friends anywhere in the world could chat with any team member toll free. For me, free long-distance telephony was an enjoyable extracurricular perk, a way to talk to my girlfriend in Massachusetts, ideally when Heather wasn’t around to listen. </span></p><p>The objective of the media team was to inquire into the state of Y2K preparedness for the radio stations, television stations, and newspapers that were the Conglomerate’s most important global media vending partners. We would do this by mailing them a Y2K questionnaire. Within a few weeks of our dour partnership, Heather had condensed our team’s trajectory into a single PowerPoint slide featuring so many capitalized common nouns that, looking back over my notes from the time today, I get the anachronistic sense of reading late-18th-century English.</p><blockquote><p>New Media Team:</p><ul><li>Media is Y2K mission critical</li><li>Is designing Y2K Questionnaire</li><li>Will be contacting Media Vendors</li><li>Will suggest Y2K Contingency Guidelines</li></ul></blockquote><p>The first bullet point was a bit of a non sequitur, more atmospheric than definitional, and I’m surprised it won Dick’s approval. The second? An outright fabrication—Heather downloaded our team’s generic questionnaire from a fly-by-night website called Y2K.com. The third bullet point, at least, was not entirely fake. The new media team had inherited from the old media team a bulging, imposing binder full of vendors’ mailing addresses. A data dump would transfer those addresses from the binder to a database, and those addresses would be used to send the questionnaire—but first, a QA of the media vendor database was required. Someone needed to identify the bugs and infelicities, the doubles and deletions, that even the cleanest data always seems to include. That someone would be me.</p><p>It fell to Leni to reveal what I should have already known: that mine had been a fake promotion. Perhaps I had been moved laterally, or perhaps even that was too optimistic of an assessment. Some clues were obvious: I hadn’t received a raise, for example. Others, more subtle. <span><i>Quality assurance analyst</i></span> has a certain corporate gravitas to it, but what would <span><i>media vendor analyst</i></span> mean on my resume if I wanted to leverage my Conglomerate time into a future Fortune 500 life or defect to Deloitte? She observed my simmering angst and provided a German term: <span><i>Weltschmerz</i></span>, or world-weariness. Leni said I was suffering from it. She advised me to open an E*TRADE account and to orient my portfolio toward biotech. She had staked out a position in Amgen and had, within some months, doubled her money.</p><hr><p><span>I<span>t’s a tricky</span> risk proposition</span> to mail unsolicited and legally implicating requests to corporate partners all over the world, but the Process demanded it. What, the questionnaire asked, were they doing to prepare for Y2K? For example, were they contacting third-party suppliers to inquire about their Y2K readiness? We knew very well that those five thousand media vendors wouldn’t respond to our questionnaire, because the Conglomerate itself didn’t respond to the thousands of questionnaires it received from its own business partners asking basically the same question. Heather explained that the Process wouldn’t allow a response, due to the legal vulnerability created by the disclosure of such information to another company.</p><p>By summer, perhaps our final one, the Andersen people decided that in addition to documentation, personal contact with the Conglomerate agencies was needed to stress the importance of Y2K. Many team members began trotting the globe to conduct Y2K site visits, and the office took on a transient, desolate quality. The speckled dropped ceiling seemed to sag under the weight of the HVAC infrastructure. The Diet Pepsi began to leave a sour aftertaste. Leni decamped to Asia for two weeks to support a series of Y2K regional meetings. Cindy headed for Vancouver to instruct the Conglomerate’s Pacific Rim shops in the ways of the Process. Even Justin left his smoking lounge to take a pre-millennial working tour of Paris, Brussels, and London.</p><p>“This is your chance, LC,” Cindy informed me at the commencement of my own Y2K grand tour. My ambitious itinerary included cities I had long fantasized about visiting: Tokyo, Hong Kong, urban constellations along the eastern coast of Australia. There would be time for surreptitious tourism, too: the café in Buenos Aires where Marcel Duchamp played chess, the Zócalo and its murals in Mexico City. None of this is what Cindy meant, however. She told me of one team member who “earned so many frequent flier miles, he took his whole family to Sydney business class. They got a club-level suite at the harbor Hyatt.” Other team members had redeemed miles for Hawaii junkets, or converted them into points for cruise bookings, vouchers for rental-car upgrades, discounts on duty-free alcohol and tobacco purchases. Around me, I realized, had whirred an opaque economy whose currency was only now becoming apparent. Suddenly, the number of mission-critical Y2K situations throughout the Asia-Pacific region made sense: first-class tickets generate triple frequent flier miles. So did team members’ preoccupation with South Africa, a fourteen-hour flight from NYC, and not just Johannesburg: Durban and Cape Town, too, had become improbable locations of Y2K concern. Meanwhile, Canada, our proximal neighbor and English-speaking sibling, home to multiple Conglomerate shops but no long-haul flights, was judged to be fairly Y2K-prepared.</p><p>I became as greedy a reward accumulator as the next team member. But during many of my international media site visits, I felt the reliable tug of impostor syndrome. On a trip to Tokyo, in place of the usual audience of one or two media-vendor colleagues in a dingy conference room, two analysts and I were led into the agency’s unexpected auditorium to find that a sea of suited businessmen awaited us. Perhaps they had taken our intra-Conglomerate communications seriously: we had after all sent word that the world—or at least the advertising industry—might be ending in short order.</p><p>I had always taken some comfort in knowing I was speaking to people whose fluency in English could not be guaranteed. That distance in communication, real or perceived, had been crucial for me as I asked my series of absurd questions. If global technology ceased 1/1/2000, how would your agency continue its operations? The Tokyo shop, however, had arranged for a translator. In an English accent, she asked me to enunciate slowly so she could select her words with the kind of care a world-ending situation demanded.</p><p>“On January 1, 2000, will you still have<span> </span>.<span> </span>.<span> </span>.” I began. Concerns were shared about the Tokyo media market: “No Japanese media vendors responded to our Y2K preparedness questionnaire.” It was one of those out-of-body, disassociated experiences, so often provoked by trauma but here provoked by management consulting. As Leigh Claire the media-vendor analyst held forth, paused for translation, and then continued, a second, depersonalized Leigh Claire wandered off the stage, collected herself, and took notes. I was disturbed at my own ability to make millenarian pronouncements. Exponentially, like a QAer documenting her own documentation, I began to doubt my own self-doubt.</p><p>Many of my Japanese colleagues’ responses to my presentation concerned my appearance. My Conglomerate confrères noted how unusual it was to have a young woman with blond hair in their presence, the translator, a brunette woman herself, seemed somewhat embarrassed to convey. “It’s exciting for them,” she editorialized, “you know, the difference.” My blond hair color, the highlights—were these a naturally occurring phenomenon, they wondered. One male media hand after another went up: How old was I and what was my marital status? How long would I be in Tokyo and what were my plans? What were my recreational interests?</p><p>This attention to my person was not without benefit: it did distract from Y2K. If someone had asked about servers, routers, why the response rates to our questionnaire mailing in Japan were so low, what advice I had for Y2K compliance, I would have likely taken recourse to my own contingency plan, one developed just then—I would have spoken about the contingency of knowledge itself. Perhaps I had become a convert to the Process: the future could not be known until its moment arrived, and at that point it was no longer the future.</p><hr><p><span>L<span>ate in that anxious</span> August</span> of 1999, a freak midnight flood struck New York City. Small puddles dotted the paving stones of Brooklyn’s sidewalks and the asphalt had that technicolor, water-mixed-with-gasoline slickness about it. The trains were delayed enough that I didn’t leave for Midtown until 10:30. I emerged from the damp morning into a transformed corporate world.</p><p>Cindy was perched with hawklike intensity inside a kind of bivouac she’d created between the office door and the elevator. She had a clipboard in one hand and a pen in the other. I had barely crossed the threshold when she broadcast clear across the stand of cubicles: “LC made it!” A different voice rang back: “Check.” Cindy had meant to affect a tone of relief at my arrival, but her excitement won out. The woman had spent more than a year preparing the office for a crisis, and now we had one: a flash flood. She herself had walked to work, she said. From the Upper West Side! She said this as though reporting on an alpine trek. A swift and unexpected nocturnal rain had swept through the city, Cindy related. “Some team members won’t make it,” she said, and, without any sense of her own drama, let a pause take hold before finishing her sentence: “into the office today.”</p><p>At Wednesday’s general meeting, Justin gave Cindy all kinds of commendations for her swift crisis management. The phone tree she had set up practically out of the ether. A collection of alternate transportation routes she’d compiled that team members could consult to solicit guidance on getting into the office in the event of a natural or—dare one say it—technological disaster. A buddy system she organized in which team members could locate a partner, alphabetically or by neighborhood. Already she had initiated the process of assembling the materials into a binder. Dick, her mentor and her boss, gave off a silent but approving glow. Cindy had saved the office, but the flood had exposed a striking irony: the Y2K office, so busy advising others on contingency plans, hadn’t developed its own.</p><p><span>“We all knew to expect the unexpected,” said Justin. It seemed like he might verge into contemplation and consider the idea of contingency in and of itself: can one really plan for something truly unknown? Instead, he took a more imperative course. “It’s balls to the wall from here on out. We’ve got four months.” His charge occasioned a flurry of activity, as team members sought to seize the day while it still existed to be seized. We had lived through a flash flood, and things had gone berserk in a dozen different ways. What would an actual millennial meltdown occasion?</span></p><hr><p><span>B</span><span><span>y December 1999,</span> </span>the feeling at the Conglomerate’s Y2K office was that nothing more could really be done. This is a common enough cliché on the approach of any moment of finality, but was particularly apt in our case, since nothing really <span><i>had</i></span> been done. The travel diminished, and colleagues I’d last seen lounging under a shaded terrace in Milan were again denizens whom I passed in the office kitchenette, Pepsi product in hand. To mark the progress of the calendar and to demarcate our work, team members did what we did best: preside over the expenditure of large sums of corporate cash. With most air travel suspended, we trained our focus on accruing hotel reward points. A series of suites were booked at hotels around Midtown so team members could monitor the developing Y2K situation across the collection of global time zones with which so many of us had become familiar. These graveyard shifts did not require remaining awake, only sleeping within a several-block radius of the office—and with whomever one pleased. Christmas and New Year’s vacations were revoked, of course, but that meant that team members could enjoy a series of multicourse dinners on the Conglomerate’s tab, including those inflated prix fixe menus that always seem to pop up around the holidays.</p><p>Limited time remained, certainly in the office and possibly in the world. But the mood around Midtown seemed the furthest thing from that of end times, unless end times are distinguished by towering Christmas stockings and holiday wreaths the size of tractor tires. At a holiday party, we drank drinks named after 1980s financial instruments: a Poison Pill, an LBO, a Killer Bees, a Bear Hug. Not that those dated terms of corporate chicanery concerned team members: the Conglomerate’s stock price was as buoyant as the atmosphere, and team members who had taken some of their salary in options were feeling festive indeed.</p><p>Then, as in a modernist novel whose conclusion one knows will not provide an ending, our mission-critical moment, our finale—composed of the scaffolded segments of temporality that team members took such pleasure in delineating: second, minute, hour, day, week, month, year, decade, century, and, yes, millennium—came and went, as any other had and many others would. It seems both necessary and anticlimactic to state what everyone now knows: we survived, with minimal difficulties. Cindy dutifully reported that some automatic toilets in Singapore hadn’t flushed properly, stuck as they were in an expired world order. Other team members scoured news sources for tales of Y2K glitches and compiled a modest list. There was an electric garage door opener whose open and close buttons had become reversed; a selection of already-odd Sharper Image gadgets that had no discernible clock control but that had nonetheless broken down somewhere on the momentary midnight bridge between 12/31/1999 and 1/1/2000; and an industrial blender at a cattle-culling facility in Alberta whose whirl wouldn’t cease even when unplugged from a power source.</p><p>By the time team members reconvened in the conference room on the bright, crisp morning of Monday, January 3, 2000, even these millennial malfunctions seemed little more than a set of curious examples, representatives of a larger case of something whose dimensions had already begun to recede from the Conglomerate’s collective consciousness. The sensation of collapse would return, of course, with the two scandalous bankruptcies that would signal the end of the ’90s economy, the first great financial bubble of our era. Both Enron and WorldCom had had Andersen as their auditor, and on both projects, it turned out, Andersen had been operating far short of best practices. It had been systematically shredding records for Enron and faking others entirely at WorldCom—documentation problems for the ages. But in the first days of the millennium, I was surprised by how quickly Y2K disappeared from office discourse as though censored, and by how team members adopted an almost amnesiac approach to a period so many of us had let so eventfully structure our lives. There was no self-consciousness: Why did we do that? More one felt a sense of process: This happened.</p></div><p>If you like this article, please <a href="https://www.nplusonemag.com/subscribe/?affid=article">subscribe</a> or leave a tax-deductible tip below to support n+1.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Did your car witness a crime? Bay Area police may be coming for your Tesla (233 pts)]]></title>
            <link>https://www.sfchronicle.com/crime/article/tesla-sentry-mode-police-evidence-19731000.php</link>
            <guid>41409882</guid>
            <pubDate>Sat, 31 Aug 2024 16:21:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sfchronicle.com/crime/article/tesla-sentry-mode-police-evidence-19731000.php">https://www.sfchronicle.com/crime/article/tesla-sentry-mode-police-evidence-19731000.php</a>, See on <a href="https://news.ycombinator.com/item?id=41409882">Hacker News</a></p>
Couldn't get https://www.sfchronicle.com/crime/article/tesla-sentry-mode-police-evidence-19731000.php: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Seeing America by train (111 pts)]]></title>
            <link>https://www.washingtonpost.com/travel/interactive/2024/amtrak-train-travel-diary/</link>
            <guid>41409776</guid>
            <pubDate>Sat, 31 Aug 2024 16:03:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/travel/interactive/2024/amtrak-train-travel-diary/">https://www.washingtonpost.com/travel/interactive/2024/amtrak-train-travel-diary/</a>, See on <a href="https://news.ycombinator.com/item?id=41409776">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/travel/interactive/2024/amtrak-train-travel-diary/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Texas State Police Gear Up for Expansion of Surveillance Tech (108 pts)]]></title>
            <link>https://www.texasobserver.org/texas-dps-surveillance-tangle-cobwebs/</link>
            <guid>41409522</guid>
            <pubDate>Sat, 31 Aug 2024 15:23:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.texasobserver.org/texas-dps-surveillance-tangle-cobwebs/">https://www.texasobserver.org/texas-dps-surveillance-tangle-cobwebs/</a>, See on <a href="https://news.ycombinator.com/item?id=41409522">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="primary">
<article>
 <section>
<p>Everything is bigger in Texas—including state police contracts for surveillance tech.</p>
<p>In June, the Texas Department of Public Safety (DPS) signed an acquisition plan for a 5-year, nearly $5.3 million contract for a controversial surveillance tool called Tangles from tech firm PenLink, according to records obtained by the <em>Texas Observer</em> through a public information request. The deal is nearly twice as large as the company’s <a href="https://www.usaspending.gov/search/?hash=fcef1f90000404554ca15e6b5373d65c">$2.7 million two-year contract</a> with the federal Immigration and Customs Enforcement (ICE).&nbsp;</p>
<p>Tangles is an artificial intelligence-powered web platform that scrapes information from the open, deep, and dark web. Tangles’ premier add-on feature, WebLoc, is controversial among digital privacy advocates. Any client who purchases access to WebLoc can track different mobile devices’ movements in a specific, virtual area selected by the user, through a capability called “geofencing.” Users of software like Tangles can do this without a search warrant or subpoena. (In <a href="https://www.eff.org/deeplinks/2024/08/federal-appeals-court-finds-geofence-warrants-are-categorically-unconstitutional">a high-profile ruling</a>, the Fifth Circuit recently held that police cannot compel companies like Google to hand over data obtained through geofencing.) Device-tracking services rely on location pings and other personal data pulled from smartphones, usually via in-app advertisers. Surveillance tech companies then buy this information from data brokers and sell access to it as part of their products.</p>
<p>WebLoc can even be used to access a device’s mobile ad ID, a string of numbers and letters that acts as a unique identifier for mobile devices in the ad marketing ecosystem, according to a <a href="https://govtribe.com/opportunity/federal-contract-opportunity/ssa-geoint-webloc-sw-n0001521pr11439#related-government-files-table">US Office of Naval Intelligence procurement notice</a>.</p>
<p>Wolfie Christl, a public interest researcher and digital rights activist based in Vienna, Austria, argues that data collected for a specific purpose, such as navigation or dating apps, should not be used by different parties for unrelated reasons. “It’s a disaster,” Christl told the <em>Observer</em>. “It’s the largest possible imaginable decontextualization of data. … This cannot be how our future digital society looks like.”</p>
<p>While a device’s mobile ad ID is technically an anonymous piece of information, it is easy to cross reference other data points to determine the owner, according to Beryl Lipton, an investigative researcher at the Electronic Frontier Foundation. “If there is another data point—like the address of the person who lives at the place where your phone seems to be all of the time—it can be very easy to quickly identify and build a profile of people using this supposedly anonymous information,” Lipton said.&nbsp;</p>
<p>In 2018, the U.S. Supreme Court ruled in <em>Carpenter v. United States</em> that police must have a warrant to obtain cell phone location data from service providers like AT&amp;T and Verizon. But Nate Wessler, the attorney who argued the <em>Carpenter</em> case and the deputy director of the American Civil Liberties Union’s Speech, Privacy, and Technology Project, told the <em>Observer</em> that companies have justified selling phone location information through data brokers by arguing that mobile ad IDs are anonymous.&nbsp;</p>
<p>“These companies absolutely trot that out as one of their defenses, and it is pure poppycock. … It’s transparently a ridiculous defense, because the entire thing that they’re selling is the ability to track phones and to be able to figure out where particular phones are going,” Wessler said.&nbsp;</p>
<p>The privacy implications of police using services—like Tangles—that provide location data are “identical” to the issues raised in the <em>Carpenter </em>case, Wessler said. That’s because location data harvested from apps, as opposed to that obtained from service providers, can be even more invasive, he said. “You can tell just as much about somebody’s GPS history from their apps as you can from their cell phone location data from their phone provider. And in some cases, you can tell more,” Wessler said.</p>
<figure><img decoding="async" width="2560" height="1721" src="https://www.texasobserver.org/wp-content/uploads/2024/07/shutterstock_2005398890-scaled.jpg" alt="" srcset="https://www.texasobserver.org/wp-content/uploads/2024/07/shutterstock_2005398890-scaled.jpg 2560w, https://www.texasobserver.org/wp-content/uploads/2024/07/shutterstock_2005398890-360x242.jpg 360w, https://www.texasobserver.org/wp-content/uploads/2024/07/shutterstock_2005398890-759x510.jpg 759w, https://www.texasobserver.org/wp-content/uploads/2024/07/shutterstock_2005398890-150x101.jpg 150w, https://www.texasobserver.org/wp-content/uploads/2024/07/shutterstock_2005398890-768x516.jpg 768w, https://www.texasobserver.org/wp-content/uploads/2024/07/shutterstock_2005398890-100x67.jpg 100w, https://www.texasobserver.org/wp-content/uploads/2024/07/shutterstock_2005398890-707x475.jpg 707w, https://www.texasobserver.org/wp-content/uploads/2024/07/shutterstock_2005398890-243x163.jpg 243w, https://www.texasobserver.org/wp-content/uploads/2024/07/shutterstock_2005398890-1536x1032.jpg 1536w, https://www.texasobserver.org/wp-content/uploads/2024/07/shutterstock_2005398890-2048x1377.jpg 2048w" sizes="(max-width: 2560px) 100vw, 2560px"><figcaption>Surveillance has ramped up dramatically along the Texas border. (Shutterstock)</figcaption></figure>
<p>Tangles is a product offered by the cybersecurity company Cobwebs Technologies, which was <a href="https://www.prnewswire.com/il/news-releases/cobwebs-technologies-an-israeli-firm-presents-its-anti-terror-tech-to-high-profile-us-delegation-300882579.html">founded in Israel in 2014</a> by three former members of Israeli military special units. The company has said their products, which are marketed as open source intelligence (OSINT) tools, have been used to combat terrorism, drug smuggling, and money laundering, but Meta has accused the company of operating as a surveillance-for-hire outfit. In 2023, Cobwebs Technologies was acquired by the Nebraska-based tech firm PenLink Ltd.</p>
<p>Christl, the Austria-based digital rights researcher, said that companies selling software that incorporates data harvested from mobile phone apps have greatly expanded the definition of OSINT tools. If a company has to buy personal data from third-party brokers to incorporate into a software that they sell to police, he said, then that isn’t really an open source tool.</p>
<p>Lipton, the investigative researcher at the Electronic Frontier Foundation, said that’s troubling for the public. “People don’t realize that some of this stuff comes with a high cost,” she said. “Both price-wise and privacy-wise.”</p>
<p>In a written statement, a PenLink spokesperson told the <em>Observer</em> their “open-source intelligence (OSINT) solutions are used to protect our communities from crime, threats, and cyber-attacks by providing seamless access to data that is publicly available. From a technology perspective, we want to note that we operate only according to the law, adhering to strict standards and regulations.” The spokesperson did not answer other specific questions.</p>
<p>Cobwebs Technologies, now part of PenLink, has scored contracts through its Delaware-based subsidiary Cobwebs America Inc. with <a href="https://www.usaspending.gov/search/?hash=25ee2d9b32801254c245abff6a2048d5">various federal agencies</a>, including ICE, the Internal Revenue Service, the Bureau of Indian Affairs and Bureau of Indian Education, and the U.S. Fish and Wildlife Service. ICE holds Cobwebs America’s highest-dollar federal contract so far, according to <a href="http://usa.spending.gov/">usa.spending.gov</a>.</p>
<p>DPS’ Intelligence and Counterterrorism division has used Tangles since 2021, as first reported by <a href="https://theintercept.com/2023/07/26/texas-phone-tracking-border-surveillance/"><em>The Intercept</em></a>. The agency first purchased the software as part of Governor Greg Abbott’s multi-billion dollar Operation Lone Star border crackdown, doling out an initial $200,000 contract as an “emergency award” with no public solicitation. Each year since, DPS has expanded the contract: In 2022, it paid $300,000, and in 2023, more than $400,000, according to contracting records on <a href="https://www.dps.texas.gov/sites/default/files/documents/iod/doingbusiness/docs/contractsover100k.pdf">DPS’ website.</a> The agency’s new plan for a 5-year Tangles license, from 2024 through 2029, will cost about $1 million per year.</p>
<p>In its acquisition plan, DPS states that Intelligence and Counterterrorism division personnel need the tool to “identify and disrupt potential domestic terrorism and other mass casualty threats.” The plan references two Texas mass shootings. In August 2019, a racist white man from Allen killed 23 at a Walmart <a href="https://www.texasobserver.org/to-understand-the-el-paso-massacre-look-to-the-long-legacy-of-anti-mexican-violence-at-the-border/">in El Paso</a>. A few weeks later, a different perpetrator went on a deadly shooting in Midland and Odessa. The plan does not mention the 2022 Uvalde school shooting, when <a href="https://www.texasobserver.org/dps-mccraw-transparency-uvalde/">91 DPS officers</a> formed part of a massive botched law enforcement response.&nbsp;</p>
<p>“Following the attacks in El Paso and Midland-Odessa Governor Abbott issued several executive orders designed to prevent similar events,” the acquisition plan obtained by the<em> Observer </em>states. “In response to these orders, DPS [Intelligence and Counterterrorism division] dedicated staff to identify potential mass attackers and terrorist threats.”</p>
<p>It is unclear how DPS has used Tangles or whether the software has helped stop any potential mass shootings. DPS did not respond to written questions or an interview request for this story.</p>
<p>Following initial publication of this story, Republican state Representative Brian Harrison said <a href="https://x.com/brianeharrison/status/1828238854001668396">on social media</a> that he would be requesting more information from DPS about its use of the surveillance software. Reached by phone, Harrison told the <em>Observer</em>: “I want to make sure that we don’t have Fourth Amendment violations going on here, whether it’s intentional or not. … Government should be protecting our civil liberties, not violating them.”</p>
<p>After DPS purchased the initial license for Cobwebs’ software in 2021, local Texas law enforcement agencies followed suit. Operation Lone Star spending records from the Goliad County Sheriff’s Office, obtained by the <em>Observer</em>, show that the Goliad sheriff obtained a “cooperative use of [Cobwebs] software” in fall 2023 along with the sheriffs of Refugio and Brooks counties to “identify, link, and track the movements of cartel operatives throughout the region.”</p>
<p>Other Texas clients that have purchased Cobwebs’ software include the Dallas and Houston police departments and the sheriff’s office in Jackson County, which shares access with the Matagorda County Sheriff’s Office, according to local government meeting minutes and DPS emails.</p>
<p>Prior to its acquisition by PenLink, Cobwebs Technologies received backlash for how clients used its products. In 2021, Meta <a href="https://about.fb.com/wp-content/uploads/2021/12/Threat-Report-on-the-Surveillance-for-Hire-Industry.pdf">banned seven companies</a>—including Cobwebs—that it had identified as participating in an online surveillance-for-hire ecosystem. As part of its sanctions, Meta removed 200 accounts operated by Cobwebs and its customers. In a <a href="https://about.fb.com/wp-content/uploads/2021/12/Threat-Report-on-the-Surveillance-for-Hire-Industry.pdf">company report</a>, Meta investigators wrote that they identified Cobwebs customers in Bangladesh, Hong Kong, the United States, New Zealand, Mexico, Saudi Arabia, Poland, and other countries.&nbsp;</p>
<p>Cobwebs’ customers were not solely focused on public safety activities, Meta’s report said. “We also observed frequent targeting of activists, opposition politicians and government officials in Hong Kong and Mexico,” the report stated.</p>
<p>Agencies across the globe have used Tangles. From at least 2021 to 2022, Salvadoran police used it, according to <a href="https://elfaro.net/es/202301/el_salvador/26687/Gobierno-compr%C3%B3-$22-millones-en-equipo-de-espionaje-a-empresa-de-amigo-israel%C3%AD-de-Bukele.htm">the investigative outlet <em>El Faro</em></a><em>.</em> Police in Mexico have also purchased the software, according to <a href="https://www.excelsior.com.mx/nacional/ya-llego-a-mexico-iott-tangles-nuevo-ciberespionaje/1610932"><em>Excelsior</em></a>, a Mexico City newspaper.&nbsp;</p>
<p>In 2022, a Cobwebs Technologies sales rep asked a DPS employee if the state agency could serve as a customer referral for a police agency in Israel, according to an email obtained by the <em>Observer</em>. In the email, the sales rep stated that DPS had at least 20 Tangles users at the time. DPS’ new acquisition plan allows for 230 named users.</p>
<p>Wessler, the ACLU attorney, said the sale of mobile device data to third-party data brokers and surveillance tech firms remains a legal gray area. “There are some legal frameworks that get at the edges of this, but there’s a whole kind of core of issues that the law just hasn’t caught up to,” Wessler said.</p>
<p>But he said other government agencies already have moved away from purchasing products that use massive amounts of cell phone location data. The services can be expensive, the use of data is invasive, and there isn’t much evidence that these services have substantially helped investigations or solved a lot of cases, he added.</p>
<p>“It’s just like the juice isn’t worth the squeeze,” Wessler said. “We shouldn’t be spending taxpayer money for this kind of haystack of data that they then are trying to pick needles out of, right?”</p>
<hr>
<p><em>Editor’s Note: This story has been updated to include additional comment from a legislator.</em></p>
</section>
</article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Harder Drive: hard drives we didn't want, or need [video] (2022) (354 pts)]]></title>
            <link>http://tom7.org/harder/</link>
            <guid>41409503</guid>
            <pubDate>Sat, 31 Aug 2024 15:21:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://tom7.org/harder/">http://tom7.org/harder/</a>, See on <a href="https://news.ycombinator.com/item?id=41409503">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><span>Harder</span>
  <span>Drive</span>:<br> Hard drives we didn't want or need
</p>

<h2>Read words</h2>
Consider reading the paper, <b><a href="http://tom7.org/papers/murphy2022harder.pdf">Harder Drive: Hard drives we didn't want or need</a></b>, which appears in
<a href="http://sigbovik.org/2022/">SIGBOVIK 2022</a>. (<a href="http://tom7.org/papers/murphy2022harder.bib">bibtex</a>)


<h2>Watch draws and hear words</h2>

<iframe width="853" height="480" src="https://www.youtube.com/embed/JcJSW7Rprio" frameborder="0" allowfullscreen=""></iframe>
	 
Of course, there are multiple irksome videos on my YouTube channel <a href="https://youtube.com/suckerpinch">Suckerpinch</a>!

<img src="http://tom7.org/harder/titlescreen1080.png" alt="screenshot for thumbnails">

<h2>Browse the internet</h2>
If you have a 64-bit Windows machine and enough RAM (32 GB?) and want to try out the app I used in the video for exploring the
IPv4 address space, you can try this torrent:
<a href="http://tom7.org/harder/harder-drive-internet-map.torrent">harder-drive-internet-map.torrent</a>. I'll shoot to keep this working through the end
of April 2022, but no promises.

<h2>Ringtones</h2>
Enhance your mobile calling experience with the ringtone audio from
the video, available with my other songs at <a href="http://mp3.tom7.org/t7es">Tom 7 Entertainment System</a>.

<h2>Have your own Harder Drive</h2>
Good luck trying to use the impenetrable <a href="https://sourceforge.net/p/tom7misc/svn/HEAD/tree/trunk/pingu/">source code</a>.

<p>Please leave a comment <a href="http://radar.spacebar.org/">on my blog</a> or on Twitter at <a href="http://twitter.com/tom7">@tom7</a>!
</p><p>Get all Tom 7 thingos at → [<a href="http://tom7.org/">tom7.org</a>]</p>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rust solves the problem of incomplete Kernel Linux API docs (172 pts)]]></title>
            <link>https://vt.social/@lina/113056457969145576</link>
            <guid>41409475</guid>
            <pubDate>Sat, 31 Aug 2024 15:16:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vt.social/@lina/113056457969145576">https://vt.social/@lina/113056457969145576</a>, See on <a href="https://news.ycombinator.com/item?id=41409475">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Buy, Borrow, Die – Explained (465 pts)]]></title>
            <link>https://old.reddit.com/r/BuyBorrowDieExplained/comments/1f26rsf/buy_borrow_die_explained/</link>
            <guid>41408772</guid>
            <pubDate>Sat, 31 Aug 2024 13:35:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/BuyBorrowDieExplained/comments/1f26rsf/buy_borrow_die_explained/">https://old.reddit.com/r/BuyBorrowDieExplained/comments/1f26rsf/buy_borrow_die_explained/</a>, See on <a href="https://news.ycombinator.com/item?id=41408772">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>So, you’ve read about “buy, borrow, die” but you’re left with more questions than answers about how - and why - it works. Maybe you’re skeptical that it works at all. Is it just a concept journalists have manufactured that sounds good on paper but falls apart under closer scrutiny? </p>

<p>I’m a private wealth attorney at an international law firm headquartered in the U.S. and I implement “buy, borrow, die” for a living. The short answer is, yes, “buy, borrow, die” works, and it’s a devastatingly effective tax elimination strategy. </p>

<p>Let's dive into the planning a little bit and use some concrete examples to illustrate. In the comments below I will answer some of the frequently asked questions I see in discussions about “buy, borrow, die,” and address some of the misconceptions people have about it. </p>

<p><strong>Step 1A. Buy.</strong> </p>

<p>This stage of the planning really is that simple. Peter will purchase an asset for $50M. His "basis" in the asset is therefore $50M. Let's assume the asset appreciates at an annual rate of 8 percent. After 10 years, the asset now has a fair market value of $108M and Peter has a "built-in" (or "unrealized") capital gain of $58M.</p>

<p>If Peter sells the asset, it's a "realization" event and he'll be subject to income tax. The asset is a capital asset, and since Peter has owned the asset for more than 1 year, he'd receive long-term capital gain treatment and pay income tax at preferential rates if he sold it. Nevertheless, Peter's long-term capital gain rate would be 20 percent, he'd be subject to the net investment income tax of 3.8 percent, and Peter lives in Quahog which has a 5 percent income tax rate. </p>

<p>So, if Peter were to sell the asset and cash in on his gain, he'd have a total tax liability of around $17M, and his after-tax proceeds would be $91M. </p>

<p>Peter's buddy Joe overheard some of his cop buddies talking about how the ultrawealthy never pay taxes because they implement "buy, borrow, die," and he shares the idea with Peter. Peter decides to look into it.</p>

<p><strong>Step 2A. Borrow.</strong> </p>

<p>Peter goes to the big city and hires a private wealth attorney, who connects him with an investment banker at Quahog Sachs. The investment bank might give Peter a loan or line of credit of up to $97M (a "loan to value" ratio of 90 percent) based on several conditions, including that the loan/line of credit is secured by the asset. Now Peter has $97M of cash to use as he pleases, and he's paid no taxes.</p>

<p><strong>Step 3A. Die.</strong> </p>

<p>Peter has been living off these asset-backed loans/lines of credits and his asset has continued appreciating in value. Let's say 35 years have passed. With an annual rate of return of 8 percent, the asset now has a fair market value of $740M.</p>

<p>Then Peter dies. When Peter dies, the basis of the asset is "adjusted" to the asset's fair market value on Peter's date of death. In other words, Peter's basis of $50M in the asset is adjusted to $740M.</p>

<p>Peter's estate can now sell the asset tax free, because "gain" is computed by subtracting adjusted basis from the sales proceeds ($740M sales proceeds less $740M adjusted basis equals $0 gain). </p>

<p>Peter's estate can use the cash to pay back the loans/lines of credits. He's paid no income tax and his beneficiaries can now use the cash to buy assets and begin the "buy, borrow, die" cycle themselves. </p>

<p><strong>BUY, BORROW, DIE IN THE REAL WORLD:</strong></p>

<p>Actual “buy, borrow, die” planning is enormously complicated. If that weren't the case, the ultrawealthy wouldn’t have to pay private wealth attorneys like me upwards of $2,500 an hour for their tax, asset protection, and estate planning needs. </p>

<p>First, this type of planning is generally not economically feasible unless the taxpayer has a net worth exceeding around <strong>$300M</strong>. If you’re worth less than that, you’re not going to be able to command attractive loan/line of credit terms from investment banks. You’re going to have to get a plain vanilla product from a retail lender which is going to have relatively high interest rates (typically the Secured Overnight Financing Rate plus some amount of spread) and other terms that make implementing “buy, borrow, die” expensive enough that you aren’t much better off (or you’re much worse off) than you would have been had you sold the asset and taken the after-tax proceeds. (Caveat: even loans/lines of credit at retail interest rates can still be very useful for short-term borrowing needs.) Clients with a net worth exceeding around $300M, however, can obtain bespoke products from the handful of lenders that specialize in this market, and the terms and conditions of these products make “buy, borrow, die” a no-brainer for virtually everyone who has this level of wealth. </p>

<p><strong>These types of loans/lines of credit will typically be interest-only and mature on the borrower’s death. The interest rates can be ultra-low based on numerous factors—I’ve seen anywhere from 0.5 percent to 3 percent, even in the current interest rate environment.</strong> But again, these types of loans/lines of credit are highly customized and the terms depend on the facts and circumstances. </p>

<p>Generally, in exchange for such favorable terms (i.e., interest-only, matures on death), the bank will ask for a share of the collateral’s appreciation (essentially, "stock appreciation rights"), and this obligation will be settled upon the borrower’s death along with the loan. The amount of the bank’s share of the collateral’s appreciation depends on many factors and it is fundamentally a matter of the bank’s underwriting process. </p>

<p>Ultimately, when the debt is settled, the taxpayer is going to pay a large sum to the investment bank, taking into consideration the risk involved and the time value of money. But by structuring the loan/line of credit in this way, the taxpayer has deferred the vast majority of their debt repayment until their death – at which point, as explained above, they can sell their assets tax-free and use the cash to satisfy those obligations. When faced with the alternatives of (i) paying the investment bank and attorneys $X or (ii) paying the government $1,000X, it’s a pretty easy choice for the taxpayer. </p>

<p><strong>The simple explanation described above, and as described in most media accounts of "buy, borrow, die," totally ignores wealth transfer taxes (in particular, gift and estate taxes).</strong> This is a very unusual oversight because “buy, borrow, die,” as it exists in the real world at least, is very much an integrated tax and estate planning strategy. </p>

<p>The unified estate and gift tax exemption for 2024 is $13.61M per taxpayer, or $27.22M per married couple. That means you can give up to $13.61M to anybody you want, either during your lifetime or upon your death, without paying any wealth transfer tax. Amounts you give away above that are generally subject to wealth transfer tax at a rate of 40 percent. So, if Peter gifts (or bequests) $15M to Meg, the first $13.61M is tax free, and the remaining $1.39M is subject to a 40 percent gift (or estate) tax, creating a tax liability of $556,000.</p>

<p>In the above example, when Peter dies with an asset worth $740M – assuming he has no other assets or liabilities and he has not used any of his wealth transfer tax exemption – he is going to be subject to an estate tax of $290.5M ($740M less $13.61M then multiplied by 40 percent) (assuming Peter does not make any gifts to his spouse, Lois, that qualify for the marital deduction, or any gifts to charitable organizations that qualify for the charitable deduction). Peter has avoided income tax by virtue of the basis adjustment that occurs at death, but he's subject to a substantial estate tax that in theory serves as a backstop to make sure he pays some taxes eventually (even if it’s not until his death).</p>

<p>The conventional wisdom is that you can avoid income tax (via the basis adjustment at death) <em>or</em> you can avoid estate tax (via lifetime gifting and estate freezing strategies) but you can’t do both. This conventional wisdom is wrong, and I’ll explain why below.</p>

<p><strong>What a well-advised taxpayer would do is implement an estate freezing technique early on in the “buy, borrow, die” game.</strong> This will involve transferring assets to an irrevocable trust. </p>

<p>Importantly, the trust agreement is going to provide Peter with a retained power of substitution (i.e., a power to remove assets from the irrevocable trust and title them in his own name so long as he replaces the removed assets with assets having the same fair market value) and the right to borrow from the trust without providing adequate security. These powers serve two principal purposes. First, they cause the trust to be treated as a “grantor” trust for federal income tax purposes (which, among other things, allows Peter to transact with the trust without any adverse tax consequences). And second, they allow Peter to pull appreciated properly and/or cash out of the trust to perfect the techniques described below. </p>

<p>Now, let’s revisit “buy, borrow, die,” but instead of the oversimplified concept we see in the news that seems (i) totally ineffective in a moderate to high interest rate environment and (ii) exposes the taxpayer to an enormous estate tax, let’s look at how “buy, borrow, die” is actually carried out by private wealth attorneys in the real world.</p>

<p><strong>Step 1B. Buy.</strong></p>

<p>Peter buys an asset worth $50M and transfers it to the PLG 2024 Irrevocable Trust (the "Trust"). To eliminate gift tax on that transfer, he'll use his $13.61M exemption amount and a variety of sophisticated techniques we don’t really need to get into here which might involve preferred freeze partnerships, zeroed-out grantor retained annuity trusts, and installment sales to intentionally defective grantor trusts. Suffice to say, we move the $50M asset out of Peter's ownership and all appreciation thereafter occurs outside of his estate for wealth transfer tax purposes. </p>

<p>After 10 years of appreciating at an annual rate of 8 percent, the asset is worth $108M. </p>

<p><strong>Step 2B. Borrow.</strong></p>

<p>Peter goes to the bank to get a loan. But now Peter doesn't have the asset to use as collateral because he transferred it to the Trust! Not a problem. The trustee of the Trust is going to guarantee the loan, using the Trust asset as collateral. In return, Peter will pay the Trust a guaranty fee (typically, around 1 percent of the assets serving as collateral, annually, which will be cumulative and payable upon Peter’s death). Peter can transact with the Trust like this without any adverse consequences because it’s a grantor trust. </p>

<p>Prior to Peter's death, he's going to use a loan/line of credit to obtain cash. Then, he’s going to exercise his power of substitution to swap the highly appreciated asset out of the Trust and swap the cash into the Trust. </p>

<p>So, immediately before the loan, Peter might have $0 assets and $0 liabilities. The trust will have an asset worth $780M and no liabilities. Immediately after the loan, Peter will have perhaps $700M cash (90 percent loan-to-value collateralized by the Trust assets) and $700M liabilities. The Trust will still have $780M assets and no liabilities. </p>

<p>Then Peter will exercise his power of substitution. He’ll swap $700M worth of cash into the trust in exchange for $700M worth of interests in the asset and he'll “buy” the remaining interest - $80M - from the Trust pursuant to a promissory note.</p>

<p>Immediately after the swap, Peter has the $780M asset and $780M liabilities ($700M owed to the bank and $80M owed to the Trust). The Trust has $780M assets ($700M cash and an $80M note) and no liabilities.  </p>

<p>Then Peter dies. </p>

<p><strong>Step 3B. Die.</strong> </p>

<p>Peter's gross estate includes the $780M asset. His estate receives an indebtedness deduction for $780M (the $700M he owes to the lender plus the $80M he owes to the Trust under the promissory note). <strong>Peter's taxable estate is $0 and he pays no estate tax.</strong> </p>

<p>Because the $780M asset is includible Peter's gross estate, it receives a basis adjustment to FMV upon his death. It can now be sold for $780M cash. His personal representative will use $700M to pay off the debt to the bank, and he'll use $80M to pay off the promissory note owed to the Trust. <strong>The Trust now has $780M in cash. All of the built-in (unrealized) capital gain has been eliminated, and Peter and his estate have paid no income tax.</strong> </p>

<p>(But recall that some share of the asset’s appreciation during Peter's lifetime is going to go the bank pursuant to the stock appreciation rights Peter granted them under the terms of the “buy, borrow, die” loan. Peter can’t avoid all costs, he can only avoid all taxes. But the costs are a <em>tiny</em> fraction of the taxes saved, so that’s okay.) </p>

<p>Peter's descendants/beneficiaries can now continue the “buy, borrow, die” cycle, avoiding all wealth transfer taxes and all income taxes in perpetuity, generation after generation after generation. </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brazil's X ban is sending lots of people to Bluesky (284 pts)]]></title>
            <link>https://www.theverge.com/2024/8/30/24232561/brazil-x-ban-sending-people-bluesky</link>
            <guid>41408738</guid>
            <pubDate>Sat, 31 Aug 2024 13:28:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/8/30/24232561/brazil-x-ban-sending-people-bluesky">https://www.theverge.com/2024/8/30/24232561/brazil-x-ban-sending-people-bluesky</a>, See on <a href="https://news.ycombinator.com/item?id=41408738">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>X is currently banned in Brazil following <a href="https://www.theverge.com/2024/8/30/24231286/x-brazil-ban-elon-musk-legal-representative">an order</a> from a Supreme Court justice, and Brazilian users seem to be turning to <a href="https://www.theverge.com/2023/4/29/23701500/bluesky-twitter-replacement">Bluesky</a>, an alternate social network, in droves.</p><p>“Brazil, you’re setting new all-time-highs for activity on Bluesky!” the official Bluesky account <a href="https://bsky.app/profile/bsky.app/post/3l2xndrs3oh2t">said in a post</a>.</p><p>“There will almost certainly be some outages and performance issues,” Bluesky developer <a href="https://bsky.app/profile/pfrazee.com/post/3l2xneypciw22">Paul Frazee said</a>. “We’ve never seen traffic like this. Hang with us!” </p><p>The <a href="https://www.theverge.com/2023/5/2/23708385/bluesky-weather-report-moderation-app-store">Bluesky app</a> looks and functions a lot like X, but it’s a decentralized social media platform that’s built on the AT Protocol (which is also developed by Bluesky). In <a href="https://bsky.app/profile/pfrazee.com/post/3l2xo7kl27f2w">a 5:12PM ET post</a>, Frazee said that Bluesky is seeing 1,000 events per second — a “new milestone” — on its “<a href="https://bsky.social/about/blog/5-5-2023-federation-architecture">relay</a>,” which essentially functions as the firehose of data for the platform. On Friday night, <a href="https://bsky.app/profile/bsky.app/post/3l2ye7wjt662c">Bluesky said</a> it had 500,000 new users “in the last two days.”</p><p>Mastodon.social has been seeing “an uptick in sign-ups and overall traffic” from Brazil over the last 48 hours, Mastodon CEO and founder Eugen Rochko tells <em>The Verge</em>. “We used to get nearly 0 sign-ups from Brazil and now it’s up to 3.55k” on Saturday, Rochko says. He also notes that there could be additional growth on third-party servers, but because Mastodon is a decentralized social network, Mastodon (the company) doesn’t have visibility into that.</p><p>Meta, the corporation that runs Threads, which has links to the fediverse, hasn’t replied to a request for comment.</p><p><em><strong>Update, August 31st</strong>: Added comment from Mastodon and an additional post from Bluesky.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deepwater Sub Recovers Roman Battering Ram Used in Carthage in Punic Wars Battle (106 pts)]]></title>
            <link>https://gizmodo.com/deepwater-submarine-recovers-roman-battering-ram-from-ancient-battle-2000493180</link>
            <guid>41408732</guid>
            <pubDate>Sat, 31 Aug 2024 13:27:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/deepwater-submarine-recovers-roman-battering-ram-from-ancient-battle-2000493180">https://gizmodo.com/deepwater-submarine-recovers-roman-battering-ram-from-ancient-battle-2000493180</a>, See on <a href="https://news.ycombinator.com/item?id=41408732">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                          <p>An Italian cultural heritage outfit has announced the recovery of a Roman battering ram from the bottom of the Mediterranean Sea.</p>

 <p>The ram, or rostrum, made up the prow of a Roman warship. It was used in the Battle of the Aegates, the team stated, a naval battle between Rome and Carthage that marked the end of the first Punic War in 241 BCE, after 23 years of conflict between the two empires.</p> <p>The discovery of the rostrum, announced by Sicily’s Department of Cultural Heritage’s Superintendence of the Sea, was recovered by divers with the Society for the Documentation of Submerged Sites. The recovery team also used the research vessel <em>Hercules</em> to aid in the rostrum’s identification and recovery.</p>

 <p>The dive team found the rostrum on the seabed at a depth of about 262 feet (80 meters). The artifact was recovered from a stretch of the Mediterranean between Levanzo and Favignana, small islands just west of Sicily, where archaeological surveys have been conducted over the last 20 years. <a href="https://www.livescience.com/archaeology/romans/2200-year-old-battering-ram-from-epic-battle-between-rome-and-carthage-found-in-mediterranean">According to LiveScience</a>, the ram is now on land in Favignana, and initial study of the artifact has revealed an ornamental relief of a helmet and feathers.</p> <p>Since the early 2000s, 27 rostra have been found, according to the team’s social media <a href="https://www.facebook.com/sopmare/posts/pfbid0ji5fR9Jaihigdm3Jza6mCKV5ByN5mjkUDbvQ95fLEM66LoEy3QJhvVvE7cHV7Q7cl">post</a>. The rostra were used for—you guessed it—ramming enemy ships, with the goal of punching holes in them, ultimately sinking them. Other ancient wartime artifacts have also been identified in the team’s surveys, including 30 Roman helmets, two swords, and a relatively common find in Mediterranean archaeology, <a href="https://gizmodo.com/archaeologists-shipwrecks-mediterranean-keith-reef-1850519506">plenty of amphorae</a>.</p>

 <p>The Mediterranean Sea near Sicily and Tunisia was a popular maritime corridor during the Roman Empire—or at least it seems so based on recent archaeological findings. Last year, a UNESCO-coordinated mission found three shipwrecks off the treacherous Keith Reef between Sicily and Tunisia, one of which was dated to between 200 BCE and 100 BCE. That research team also studied three Roman wrecks off the Italian coast, two of which were 1st-century merchant vessels and one of which dated to the 1st-century BCE.</p> <p>The recently recovered rostrum is older than those wrecks, however, and is a remarkably vivid window into an ancient battle, and the fierce naval conflicts that shaped the ancient world.&nbsp;The Battle of the Aegates saw most of the Carthaginian fleet sunk or captured, and resulted in Roman supremacy on the Mediterranean. All told, there were three Punic Wars between Carthage and Rome, which resulted in the destruction of Carthage.</p>                          </div></div>]]></description>
        </item>
    </channel>
</rss>