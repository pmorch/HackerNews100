<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 06 Jan 2024 02:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[TinyLlama: An Open-Source Small Language Model (112 pts)]]></title>
            <link>https://arxiv.org/abs/2401.02385</link>
            <guid>38885054</guid>
            <pubDate>Fri, 05 Jan 2024 21:15:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2401.02385">https://arxiv.org/abs/2401.02385</a>, See on <a href="https://news.ycombinator.com/item?id=38885054">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2401.02385.pdf">Download PDF</a>
    <a href="https://browse.arxiv.org/html/2401.02385v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. Our model checkpoints and code are publicly available on GitHub at <a href="https://github.com/jzhang38/TinyLlama" rel="external noopener nofollow">this https URL</a>.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Guangtao Zeng [<a href="https://arxiv.org/show-email/ea2afa50/2401.02385">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 4 Jan 2024 17:54:59 UTC (1,783 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Rejects the Hey Calendar from Their App Store (103 pts)]]></title>
            <link>https://world.hey.com/dhh/apple-rejects-the-hey-calendar-from-their-app-store-4316dc03</link>
            <guid>38884517</guid>
            <pubDate>Fri, 05 Jan 2024 20:34:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://world.hey.com/dhh/apple-rejects-the-hey-calendar-from-their-app-store-4316dc03">https://world.hey.com/dhh/apple-rejects-the-hey-calendar-from-their-app-store-4316dc03</a>, See on <a href="https://news.ycombinator.com/item?id=38884517">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <article>
      <div>
  <div><p>There should at least be a standard of double jeopardy when it comes to the app store monopoly regimes. If you’ve managed to <a href="https://www.hey.com/apple/">overturn a rejection of your service</a> once, they can’t come after you on the same service again later. We could have used that today!</p><p>But unfortunately there is no rule of law with the app stores, except that of the jungle, and Apple is the 800 lbs gorilla, ruling as it sees fit. So now <a href="https://hey.com/">HEY</a> is back on trial in their kangaroo court. This time with our new calendar feature, <a href="https://www.youtube.com/watch?v=SztU4232u_o">HEY Calendar</a>, which we dared make a separate app in service of users.&nbsp;</p><p>After spending 19 days to review our submission, causing us to miss a long-planned January 2nd launch date, Apple rejected our stand-alone free companion app “because it doesn’t do anything”. That is because users are required to login with an existing account to use the functionality.</p><p>This is a ridiculous charge. The App Store is filled with high-profile applications that require an existing service account and simply presents a login screen when first launched. Here are just four:</p></div><div><p>Salesforce, JPMorgan, Netflix, and Google Calendar all greet the user with the same gate: Login with your existing account. There are thousands of other apps just like this. Some access enterprise services, like Salesforce. Some access consumer services, like JPMorgan. Some access streaming services, like Netflix. And, finally, some access calendar services that are part of a larger subscription suite you purchase on the web, like Google Calendar. And HEY Calendar!</p><p>But none of this even matters. Nowhere in the <a href="https://developer.apple.com/app-store/review/guidelines/#business">Apple App Store Guidelines</a> is there a prohibition on apps that require preexisting accounts! The only ruleset that’s relevant to this discussion is that which governs who has to use in-app payments and who can avoid it.</p><p>Those were the rules <a href="https://twitter.com/dhh/status/1272968382329942017">we fought Apple over</a> when HEY originally launched back in 2020. Where we successfully managed to secure a carve-out. This is that carve-out from 3.1.3 (f):</p><figure>
      <a download="3-1-3f.jpeg" title="Download 3-1-3f.jpeg" data-click-proxy-target="lightbox_link_blob_1498691609" href="https://world.hey.com/dhh/4316dc03/blobs/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHNLd2NaT0ZSWiIsImV4cCI6bnVsbCwicHVyIjoiYmxvYl9pZCJ9fQ==--9e47d2a9a99c45ed946615ccb759d3815eb068aa/3-1-3f.jpeg?disposition=attachment">
        <img src="https://world.hey.com/dhh/4316dc03/representations/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHNLd2NaT0ZSWiIsImV4cCI6bnVsbCwicHVyIjoiYmxvYl9pZCJ9fQ==--9e47d2a9a99c45ed946615ccb759d3815eb068aa/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaDdDam9MWm05eWJXRjBTU0lKYW5CbFp3WTZCa1ZVT2hSeVpYTnBlbVZmZEc5ZmJHbHRhWFJiQjJrQ2dBZHBBZ0FGT2d4eGRXRnNhWFI1YVVzNkMyeHZZV1JsY25zR09nbHdZV2RsTURvTlkyOWhiR1Z6WTJWVSIsImV4cCI6bnVsbCwicHVyIjoidmFyaWF0aW9uIn19--824130a6fd2f596d29a6d245cf773fab0beee210/3-1-3f.jpeg" alt="3-1-3f.jpeg" srcset="https://world.hey.com/dhh/4316dc03/representations/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHNLd2NaT0ZSWiIsImV4cCI6bnVsbCwicHVyIjoiYmxvYl9pZCJ9fQ==--9e47d2a9a99c45ed946615ccb759d3815eb068aa/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaDdDam9MWm05eWJXRjBTU0lKYW5CbFp3WTZCa1ZVT2hSeVpYTnBlbVZmZEc5ZmJHbHRhWFJiQjJrQ0FBOXBBZ0FLT2d4eGRXRnNhWFI1YVVFNkMyeHZZV1JsY25zR09nbHdZV2RsTURvTlkyOWhiR1Z6WTJWVSIsImV4cCI6bnVsbCwicHVyIjoidmFyaWF0aW9uIn19--f3f1b2037df56b6e7b02c335c36fe9999ab42b52/3-1-3f.jpeg 2x, https://world.hey.com/dhh/4316dc03/representations/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHNLd2NaT0ZSWiIsImV4cCI6bnVsbCwicHVyIjoiYmxvYl9pZCJ9fQ==--9e47d2a9a99c45ed946615ccb759d3815eb068aa/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaDdDam9MWm05eWJXRjBTU0lKYW5CbFp3WTZCa1ZVT2hSeVpYTnBlbVZmZEc5ZmJHbHRhWFJiQjJrQ2dCWnBBZ0FQT2d4eGRXRnNhWFI1YVR3NkMyeHZZV1JsY25zR09nbHdZV2RsTURvTlkyOWhiR1Z6WTJWVSIsImV4cCI6bnVsbCwicHVyIjoidmFyaWF0aW9uIn19--6b62883c9596ea5ec874f8c026d734630c019ca2/3-1-3f.jpeg 3x" decoding="async" loading="lazy">
</a>
  </figure><p>As you can see, Email Services are <em>specifically</em> mentioned because HEY fought this battle once already back in 2020. HEY Calendar is a free companion app to that very same service.</p><p>It’s even more frustrating because this one-service-many-apps strategy is exactly the same that Apple has taken with iCloud. That’s one subscription which is powered by a suite of individual apps. You have Mail, you have Calendar, you have Files. All connect to the same cloud service that’s billed for once. Just like HEY.</p><p>But Apple has become so emboldened by a decade of free monopoly reign that they don’t even feign a superficial adherence to their own rules. They carve out exceptions left and right to mega corporations they’d rather not anger, then invent entirely new rules that aren’t codified anywhere when it suits them, and finally rebuts every demand for consistency and predictability with “just submit your app and we’ll review”. This is intolerable.</p><p>But it’s also highly profitable. Service revenue is the fastest growing part of Apple’s business, and none of that revenue comes easier than taking a 30% cut of the app economy.</p><p>So what’s going to happen? I don’t know, but I do know that we’ll keep fighting. We’re never going to roll over and pay Apple 30% in protection money to be left alone. Last time we found a way, and we will again.</p><p>Hopefully our example, and the countless others we’ve seen over the years, will finally force competition authorities around the world to act. We have the Digital Markets Act coming in the EU in just a couple of months. That could very well be a game-changer. So too could the lawsuit by the American Department of Justice that was <a href="https://www.nytimes.com/2024/01/05/technology/antitrust-apple-lawsuit-us.html?smid=tw-nytimes&amp;smtyp=cur">just revealed to be imminent</a> today.&nbsp;</p><p>The last time the DOJ sued Microsoft in the late 90s/early 2000s, it inflicted a serious wound on Redmond’s capacity to capture markets, which gave us the rise of Google, Apple, and others. Hopefully that will happen again and a thousand free-market flowers will bloom once no longer deprived of access to sun, water, and customers.</p><p>One can only dream. But one should also fight. You don’t get something for nothing.</p></div>
</div>

    </article>
  </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hard disk LEDs and noisy machines (118 pts)]]></title>
            <link>https://blogsystem5.substack.com/p/hard-disk-leds-and-noisy-machines</link>
            <guid>38883408</guid>
            <pubDate>Fri, 05 Jan 2024 19:19:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blogsystem5.substack.com/p/hard-disk-leds-and-noisy-machines">https://blogsystem5.substack.com/p/hard-disk-leds-and-noisy-machines</a>, See on <a href="https://news.ycombinator.com/item?id=38883408">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>The computers of yesteryear had this little feature known as blinking LED lights 🔆. They also had this other feature called noisy disks 💾 and loud fans 🪭. Uh wait. Features? Why “features” and not “annoyances”?!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F582ae704-2dd0-4b93-a9ec-5fb42c2ca5bd_396x396.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F582ae704-2dd0-4b93-a9ec-5fb42c2ca5bd_396x396.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F582ae704-2dd0-4b93-a9ec-5fb42c2ca5bd_396x396.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F582ae704-2dd0-4b93-a9ec-5fb42c2ca5bd_396x396.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F582ae704-2dd0-4b93-a9ec-5fb42c2ca5bd_396x396.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F582ae704-2dd0-4b93-a9ec-5fb42c2ca5bd_396x396.jpeg" width="396" height="396" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/582ae704-2dd0-4b93-a9ec-5fb42c2ca5bd_396x396.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;normal&quot;,&quot;height&quot;:396,&quot;width&quot;:396,&quot;resizeWidth&quot;:396,&quot;bytes&quot;:21643,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F582ae704-2dd0-4b93-a9ec-5fb42c2ca5bd_396x396.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F582ae704-2dd0-4b93-a9ec-5fb42c2ca5bd_396x396.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F582ae704-2dd0-4b93-a9ec-5fb42c2ca5bd_396x396.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F582ae704-2dd0-4b93-a9ec-5fb42c2ca5bd_396x396.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Front panel of a common PC case in the late 1990s. My Pentium MMX 166 was hosted in one of these.</figcaption></figure></div><p>You see, these bright lights and loud noises acted as canaries 🐦 in a performance mine. They gave developers a chance to notice when things were off performance-wise. If your code abused the CPU or the hard disk by mistake, you could tell right away.</p><p>Nowadays, developer machines tend to be quiet under heavy load, and the vast majority of laptops don’t even have lights anymore. The obvious example are Macs: they haven’t had hard disk LEDs for a really long time, and since the M1, they are silent and cold too.</p><p><span>These characteristics are nice from a usability perspective. Unfortunately, as a developer, you now need to first </span><em>imagine</em><span> that something is wrong before even deciding to look for a problem. If the thought never crosses your mind, then you may never look.</span></p><p>Let me give you a few examples of the kinds of inefficiencies that I’m talking about. These would have been trivially noticed by the presence of indicators. These are all based on real-world situations I faced at some point in the (recent) past.</p><p><span>🪵 In a project I worked on, our development builds started writing about 80 MB of log messages </span><em>per second</em><span> to disk. No matter how you look at it, that’s </span><em>a lot</em><span> of disk traffic, and yet… the problematic code passed code review and was merged into the main branch.</span></p><p><span>The only indication that something was wrong was when </span><em>other</em><span> developers came asking for help because their local disk space was running out faster than usual. There was no other symptom behind the problem.</span></p><p>You’d hope that this inefficiency would be caught while qualifying the new release for production because, in theory, such logging waste would translate in an increase in CPU consumption or network bandwidth. But… I’m not so sure the issue would have been noticed.</p><p>🌐 In another project I worked on, I noticed that Bazel took an incredibly long time to complete some actions. It wasn’t until I looked in detail that I saw it stuck in a loop fetching the same remote artifact over and over again due to connection resets.</p><p>The build completed successfully after many minutes once Bazel gave up on the downloads and fell back local execution. There was no reason to suspect that something was wrong other than “these actions are just huge”. In reality, though, there was a bug somewhere.</p><p><span>🧱 Just today, I was in a video call and noticed that my laptop was reading 100MB/s from disk non-stop. I concluded the meeting but the disk reads didn’t stop. A quick peek at </span><code>top</code><span> showed something called </span><code>WallpaperVideoExtension</code><span> that seemed to have gone rogue.</span></p><p>This background process was consuming one full CPU, but such load wasn’t enough to make the system feel slower nor noisier. I suppose I would eventually have noticed that the battery was running out quicker than usual, but maybe not.</p><p><span>Killing the process made the problem go away and the constant disk reads stopped. Looking online, I find other instances of </span><code>WallpaperVideoExtension</code><span> consuming lots of CPU and memory, so this seems to be a bug. But if it’s common, why wasn’t it noticed in the first place?</span></p><p>In any case, this last scenario gives you a hint 🔍 as to where I’m going: how did I even notice this last problem? After all, my M1 Mac was working just fine: it was just slightly warmer than usual but there was no loud fan noise nor lights to tell me about disk activity.</p><p>The answer is simple: I have an omnipresent performance monitor in my screen that shows CPU load, memory pressure, disk I/O throughput, and network traffic. This monitor is always visible, taking little space in the menu bar or the task bar.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ba9b823-6f31-4f90-ac66-7adbd3d51902_646x1266.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ba9b823-6f31-4f90-ac66-7adbd3d51902_646x1266.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ba9b823-6f31-4f90-ac66-7adbd3d51902_646x1266.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ba9b823-6f31-4f90-ac66-7adbd3d51902_646x1266.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ba9b823-6f31-4f90-ac66-7adbd3d51902_646x1266.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ba9b823-6f31-4f90-ac66-7adbd3d51902_646x1266.png" width="646" height="1266" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7ba9b823-6f31-4f90-ac66-7adbd3d51902_646x1266.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1266,&quot;width&quot;:646,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:387258,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ba9b823-6f31-4f90-ac66-7adbd3d51902_646x1266.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ba9b823-6f31-4f90-ac66-7adbd3d51902_646x1266.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ba9b823-6f31-4f90-ac66-7adbd3d51902_646x1266.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ba9b823-6f31-4f90-ac66-7adbd3d51902_646x1266.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>iStat Menus on the macOS menu bar, with the panel for CPU usage tracking open.</figcaption></figure></div><p>Every time I sense something is a tiny bit off, I glance 👀 at the monitor. You cannot imagine how many times I’ve gone “huh, that’s interesting” by seeing unexpected activity and then went on to discover big performance problems somewhere in the system.</p><p><span>My recommendation is that you stop what you are doing and go and install such a performance monitor </span><em>right now</em><span>. I’d even argue that having one always visible should be a hard requirement for any development machine and corp IT departments should preinstall one.</span></p><p><span>Personally, I’m a huge fan of </span><a href="https://bjango.com/mac/istatmenus/" rel="">iStat Menus</a><span> for macOS and have been using it for years. But if macOS is not your thing, you can find similar tools for other platforms like </span><a href="https://extensions.gnome.org/extension/3010/system-monitor-next/" rel="">system-monitor-next</a><span> for Gnome.</span></p><p>Unfortunately, these monitors only help if you develop on your local machine—a workflow that’s becoming exceedingly rare. If, instead, you SSH into remote virtual machines to do your development or use VSCode’s remote features, you’ll need a different answer.</p><p>This is a situation I face right now. The modern ThinkStation I have in the garage is well-equipped with useful lights… but I only access it over SSH for development so those lights and its disk noises are kinda useless from where I sit.</p><p><span>And I’m not sure what the right answer here is. If you have been around for a while, you may remember </span><a href="http://gkrellm.srcbox.net/" rel="">GKrellM</a><span>, which I was an avid user of. This system monitor had the ability to display </span><em>remote</em><span> machine activity and I’d love to have that again.</span></p><p><span>(</span><a href="https://twitter.com/jmmv/status/1735712759604711494" rel="">You can read the original of this text in its Twitter thread form.</a><span>)</span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. moves closer to filing antitrust case against Apple (242 pts)]]></title>
            <link>https://www.nytimes.com/2024/01/05/technology/antitrust-apple-lawsuit-us.html</link>
            <guid>38883393</guid>
            <pubDate>Fri, 05 Jan 2024 19:18:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/01/05/technology/antitrust-apple-lawsuit-us.html">https://www.nytimes.com/2024/01/05/technology/antitrust-apple-lawsuit-us.html</a>, See on <a href="https://news.ycombinator.com/item?id=38883393">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/01/05/technology/antitrust-apple-lawsuit-us.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[In Europe, Trains Are Full, and More Are on the Way (131 pts)]]></title>
            <link>https://www.nytimes.com/2024/01/04/travel/europe-new-trains.html</link>
            <guid>38883213</guid>
            <pubDate>Fri, 05 Jan 2024 19:05:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/01/04/travel/europe-new-trains.html">https://www.nytimes.com/2024/01/04/travel/europe-new-trains.html</a>, See on <a href="https://news.ycombinator.com/item?id=38883213">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/01/04/travel/europe-new-trains.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[How to build a thinking AI (102 pts)]]></title>
            <link>https://aithought.com/</link>
            <guid>38882747</guid>
            <pubDate>Fri, 05 Jan 2024 18:35:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aithought.com/">https://aithought.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38882747">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h4><strong>Thought is Structured by the Iterative Updating of Working Memory</strong>: </h4>







<h5>I<strong>MPLEMENTING THIS IN A MACHINE COULD ENABLE ARTIFICIAL GENERAL INTELLIGENCE</strong></h5>











<p><strong>Jared Edward Reser&nbsp;Ph.D., M.A., M.A.</strong><br></p>



<p><em><strong>Note:</strong> For readers with time constraints, a concise understanding of the core content can be attained in around ten minutes by examining the figures and accompanying captions.</em></p>



<p>.</p>



<figure data-wp-context="{  &quot;imageLoaded&quot;: false,
				&quot;initialized&quot;: false,
				&quot;lightboxEnabled&quot;: false,
				&quot;hideAnimationEnabled&quot;: false,
				&quot;preloadInitialized&quot;: false,
				&quot;lightboxAnimation&quot;: &quot;zoom&quot;,
				&quot;imageUploadedSrc&quot;: &quot;https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg&quot;,
				&quot;imageCurrentSrc&quot;: &quot;&quot;,
				&quot;targetWidth&quot;: &quot;4581&quot;,
				&quot;targetHeight&quot;: &quot;1968&quot;,
				&quot;scaleAttr&quot;: &quot;&quot;,
				&quot;dialogLabel&quot;: &quot;Enlarged image&quot;
			}" data-wp-interactive="{&quot;namespace&quot;:&quot;core/image&quot;}"><img data-attachment-id="435" data-permalink="https://aithought.com/blog/assembly-vs-ensemble-2/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg" data-orig-size="4581,1968" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;Assembly vs Ensemble&quot;,&quot;created_timestamp&quot;:&quot;1697720928&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;Assembly vs Ensemble&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Assembly vs Ensemble" data-image-description="" data-image-caption="<p>Assembly vs Ensemble</p>
" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg?w=1024" width="4581" height="1968" data-wp-init="callbacks.initOriginImage" data-wp-on--click="actions.showLightbox" data-wp-on--load="actions.handleLoad" data-wp-watch--setstylesonresize="callbacks.setStylesOnResize" data-wp-watch="callbacks.setButtonStyles" src="https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg 4581w, https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg?w=150&amp;h=64 150w, https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg?w=300&amp;h=129 300w, https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg?w=768&amp;h=330 768w, https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg?w=1024&amp;h=440 1024w" sizes="(max-width: 4581px) 100vw, 4581px">        <div aria-label="" aria-modal="" role="" data-wp-body="" data-wp-bind--role="state.roleAttribute" data-wp-bind--aria-label="state.dialogLabel" data-wp-class--initialized="context.initialized" data-wp-class--active="context.lightboxEnabled" data-wp-class--hideanimationenabled="context.hideAnimationEnabled" data-wp-bind--aria-modal="state.ariaModal" data-wp-watch="callbacks.initLightbox" data-wp-on--keydown="actions.handleKeydown" data-wp-on--touchstart="actions.handleTouchStart" data-wp-on--touchmove="actions.handleTouchMove" data-wp-on--touchend="actions.handleTouchEnd" data-wp-on--click="actions.hideLightbox" tabindex="-1">
                <div>
<figure><img data-attachment-id="435" data-permalink="https://aithought.com/blog/assembly-vs-ensemble-2/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg" data-orig-size="4581,1968" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;Assembly vs Ensemble&quot;,&quot;created_timestamp&quot;:&quot;1697720928&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;Assembly vs Ensemble&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Assembly vs Ensemble" data-image-description="" data-image-caption="<p>Assembly vs Ensemble</p>
" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg?w=1024" data-wp-bind--src="context.imageCurrentSrc" data-wp-style--object-fit="state.lightboxObjectFit" src="https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg?w=1024" alt=""></figure>
</div>
                <div>
<figure><img data-attachment-id="435" data-permalink="https://aithought.com/blog/assembly-vs-ensemble-2/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg" data-orig-size="4581,1968" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;Assembly vs Ensemble&quot;,&quot;created_timestamp&quot;:&quot;1697720928&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;Assembly vs Ensemble&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Assembly vs Ensemble" data-image-description="" data-image-caption="<p>Assembly vs Ensemble</p>
" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg?w=1024" data-wp-bind--src="state.enlargedImgSrc" data-wp-style--object-fit="state.lightboxObjectFit" src="https://aithoughtcom.files.wordpress.com/2023/10/website-banner2-1.jpg?w=1024" alt=""></figure>
</div>
                
        </div></figure>



<p>.</p>



<p><strong>Abstract</strong></p>



<p>This article provides an analytical framework for how to simulate human-like thought processes within a computer. It describes how attention and memory should be structured, updated, and utilized to search for associative additions to the stream of thought. The focus is on replicating the dynamics of the mammalian working memory system, which features two forms of persistent activity: sustained firing (preserving information on the order of seconds) and synaptic potentiation (preserving information from minutes to hours). The article uses a series of over 40 original figures to systematically demonstrate how the iterative updating of these working memory stores provides functional structure to behavior, cognition, and consciousness.&nbsp;&nbsp;</p>



<p>In an AI implementation, these two memory stores should be updated continuously and in an iterative fashion, meaning each state should preserve a proportion of the coactive representations from the state before it. Thus, the set of concepts in working memory will evolve gradually and incrementally over time. This makes each state a revised iteration of the preceding state and causes successive states to overlap and blend with respect to the information they contain. Transitions between states happen as persistent activity spreads activation energy throughout the hierarchical network searching long-term memory for the most appropriate representation to be added to the global workspace. The result is a chain of associatively linked intermediate states capable of advancing toward a solution or goal. Iterative updating is conceptualized here as an information processing strategy, a model of working memory, a theory of consciousness, and an algorithm for designing and programming artificial general intelligence.</p>







<p><strong>Keywords</strong></p>



<p>artificial intelligence, artificial general intelligence, attention, consciousness, focus of attention, information processing, machine consciousness, neural assembly, neural network, recurrent systems, short-term memory, synaptic potentiation, systems neuroscience, superintelligence, working memory</p>







<figure></figure>







<figure></figure>







<h5><strong>Table of Contents:</strong></h5>



<p><strong>Abstract</strong></p>



<p><strong><u><a href="#introduction">Part I: Introduction</a></u></strong></p>



<p>1.1 Machine Superintelligence Requires a Thought Process</p>



<p>1.2 Iteration Defines the Workflow of Thought</p>



<p><strong><u><a href="#literature-review">Part II: Literature Review</a></u></strong></p>



<p>2.1 Interactions Between Sensory Memory, Working Memory, and Long-term Memory</p>



<p>2.2 The Focus of Attention Is Embedded within the Short-term Memory Store</p>



<p>2.3 Sustained Firing Maintains Information in the Focus of Attention</p>



<p>2.4 Synaptic Potentiation Maintains Information in the Short-term Store</p>



<p><strong><u><a href="#part3">Part III: Working Memory is Updated Iteratively</a></u></strong></p>



<p>3.1 Persistent Activity Causes Successive States to Overlap Iteratively</p>



<p>3.2 The Iterative Updating of Representations Allows Context to Shift</p>



<p>3.3 The Rate of Iterative Updating Varies with Demand</p>



<p>3.4 Iterative Updating Gives Rise to Mental Continuity</p>



<p><strong><u><a href="#part4">Part IV: Implications of the Model</a></u></strong></p>



<p>4.1 Iterative Updating Provides Structure to Associative Search</p>



<p>4.2 Multiassociative Search Spreads the Combined Activation Energy of Multiple Items</p>



<p>4.3 States Updated by the Products of Search Are Predictions</p>



<p>4.4 Iterative Updating Allows Progressive Changes to the Contents of Thought</p>



<p>4.5 Testing the Neurophysiological Validity of the Model</p>



<p><strong><u><a href="#part5">Part IV: Instantiating the Model Within a Computer</a></u></strong></p>



<p>5.1 AI Should Employ Iterative Updating</p>



<p>5.2 Designing an AI Capable of Iterative Updating</p>



<p>5.3 Modularity, Modality, and Imagery in AI</p>



<p>5.4 How to Train an AI that Employs Iterative Updating</p>



<p>5.5 Discussion and Conclusions</p>



<p><strong>References</strong></p>



<p>.</p>



<p id="introduction"><strong><u>Part I:</u></strong><span> Introduction</span></p>



<p id="1.1"><strong>1.1. <strong>Machine Superintelligence Requires a Thought Process</strong></strong></p>



<p>.</p>



<blockquote>
<p>“It seems that the human mind has first to construct forms independently before we can find them in things… Knowledge cannot spring from experience alone, but only from a comparison of the inventions of the intellect with observed fact.”</p>



<p>Albert Einstein (1949)</p>
</blockquote>



<p>The above quote from Einstein suggests that for artificial intelligence (AI) to make sense of the world, it must go beyond training on data and think for itself. Incremental improvements to existing machine learning architectures will not yield knowledge creation or real understanding because they do not attempt to simulate thought or its reflective, analytical, or deliberative qualities. Although current artificial neural networks use various brain-inspired techniques such as attention, they do not use working memory the way mammals do. To do so would involve constructing a stream of thought by holding a set of representations coactive and continuously updating this set with the most pertinent associations. This results in an iterative system that embeds mental states within the states that came before them. Such a system would generate and refine knowledge by constantly comparing and contextualizing information.</p>



<p>The present article identifies the elements of mammalian working memory that make this iterative process possible and describes how to organize them within a computer using contemporary machine-learning technology. It also introduces several novel concepts, terms (Table 4), and illustrations (Figures 1-47) to explain how an iterative cognitive cycle will permit a computer program to make the state-space transitions necessary to achieve general intellectual faculties. By simulating ongoing, self-directed, open-ended thought, as described here, an artificial agent could construct its own predictions and associations, simulate hypothetical situations, synthesize novel ideas, and thereby further scientific and technological progress. Table 1 outlines some of the categorical traits of this model.</p>



<figure><img data-attachment-id="674" data-permalink="https://aithought.com/blog/screenshot-87/" data-orig-file="https://aithoughtcom.files.wordpress.com/2024/01/screenshot-87.png" data-orig-size="1902,1090" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-87" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2024/01/screenshot-87.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2024/01/screenshot-87.png?w=1024" loading="lazy" width="1024" height="586" src="https://aithoughtcom.files.wordpress.com/2024/01/screenshot-87.png?w=1024" alt="" srcset="https://aithoughtcom.files.wordpress.com/2024/01/screenshot-87.png?w=1024 1024w, https://aithoughtcom.files.wordpress.com/2024/01/screenshot-87.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2024/01/screenshot-87.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2024/01/screenshot-87.png?w=768 768w, https://aithoughtcom.files.wordpress.com/2024/01/screenshot-87.png 1902w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>







<p><strong>Table 1.</strong> Comparative Overview of Features of an AI Based on the Iterative Updating Model</p>



<p><em>This table provides context by summarizing some of the general characteristics of an intelligent system built in accordance with the architecture discussed here.</em></p>



<p id="1.2"><strong>1.2 Iteration Defines the Workflow of Thought </strong></p>



<p>AI research has yet to formalize and simulate the thinking process because psychology and neuroscience have completely ignored the crucial role of iteration. No contemporary models address iterative change in the contents of working memory. In many discussions, updating of the information held in working memory is considered to be complete rather than partial, meaning that after being updated, the contents from the previous state are entirely replaced (e.g., Pina et al., 2018; Niklaus et al., 2019). In other discussions, information can be updated without complete replacement, but only such as when working memory holds three words and then accommodates a fourth in addition to the first three (e.g., Miller et al., 2018; Manohar et al., 2019). These views compartmentalize the thinking process, isolating current states from what came before them.</p>



<p>In contrast, the account presented here explores the hypothesis that partial updating occurs continuously. As representations are added, others are subtracted, and others from the previous state remain due to persistent neural activity (Figure 1). This cascading persistence allows successive states to share a proportion of their content in common, creating complex causal relationships between them (Reser, 2011, 2012). This iterative perspective may be useful because it illuminates how the gradually transforming collection of representations in working memory allows the thinking process to progress as updated states elaborate intelligently on the states that came before them (Reser, 2013, 2016, 2022).</p>







<figure><img data-attachment-id="571" data-permalink="https://aithought.com/blog/image-29/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/image.png" data-orig-size="975,265" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/image.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/image.png?w=975" loading="lazy" width="975" height="265" src="https://aithoughtcom.files.wordpress.com/2023/12/image.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/image.png 975w, https://aithoughtcom.files.wordpress.com/2023/12/image.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/image.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/image.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 1.</strong> Two Types of Working Memory Updating Compared</p>



<p><em>Each row contains five rectangles labeled time one (t1) through time five (t5). Each rectangle corresponds to a state of working memory holding three items. In the top row, successive rectangles do not hold any of the same items, indicating complete updating. In the second row, two items are shared between successive rectangles, indicating partial updating. This article contends that the iterative nature exhibited by the second row is a fundamental attribute of the thinking process.</em></p>



<p>A familiar example of the concept of iteration is “iterative design.” It is a method of developing commercial products through a cyclic process of prototyping, testing, and improving. With this method, designs are assessed through user feedback and enhanced in an incremental fashion. Think of the installment histories of a popular product such as a cell phone, operating system, or car. The newest version of the product contains novel features but preserves many aspects of the previous version and even of versions before that. The workflow of human thought is interpreted here in a similar way (Figure 2). As mental representations in working memory are updated, the frame of reference is gradually replaced, and a thought about one scenario incrementally transitions into a thought about a related scenario. The result is a series of intermediate states capable of exploring a problem space and deriving a solution. This article will explore how this general process contributes to reasoning, mental modeling, executive processes, and consciousness.</p>







<figure><img data-attachment-id="46" data-permalink="https://aithought.com/iterative-updating-of-active-information-in-the-brain-1/" data-orig-file="https://aithoughtcom.files.wordpress.com/2022/03/iterative-updating-of-active-information-in-the-brain-1.jpg" data-orig-size="1106,639" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="iterative-updating-of-active-information-in-the-brain-1" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2022/03/iterative-updating-of-active-information-in-the-brain-1.jpg?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2022/03/iterative-updating-of-active-information-in-the-brain-1.jpg?w=1024" loading="lazy" width="1024" height="591" src="https://aithoughtcom.files.wordpress.com/2022/03/iterative-updating-of-active-information-in-the-brain-1.jpg?w=1024" alt="" srcset="https://aithoughtcom.files.wordpress.com/2022/03/iterative-updating-of-active-information-in-the-brain-1.jpg?w=1024 1024w, https://aithoughtcom.files.wordpress.com/2022/03/iterative-updating-of-active-information-in-the-brain-1.jpg?w=150 150w, https://aithoughtcom.files.wordpress.com/2022/03/iterative-updating-of-active-information-in-the-brain-1.jpg?w=300 300w, https://aithoughtcom.files.wordpress.com/2022/03/iterative-updating-of-active-information-in-the-brain-1.jpg?w=768 768w, https://aithoughtcom.files.wordpress.com/2022/03/iterative-updating-of-active-information-in-the-brain-1.jpg 1106w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><strong>Fig. 2.</strong> Flowchart of Iterative Updating</p>



<p><em>In an iterative process, a set of components is modified repetitively to generate a series of updated states. Each state is an iteration as well as the starting point for the next iteration. One way to accomplish iterative modification is to alter a given state by retaining pertinent elements and then subtracting and adding others. In the brain, the content to be added and subtracted is determined by spreading activation.</em></p>



<p>This abstract, high-level model also offers an explanation for how the next iterative update to working memory is selected. The firing neurons that underlie the representations in working memory spread their combined excitatory and inhibitory effects to other cells throughout the cortex. Thus, the coactivation of the contents of working memory amounts to an associative search of long-term memory for applicable information (e.g., predictions, probabilities, and motor instructions). The nonactive (baseline) cells that receive the most spreading activation become active and comprise the representation(s) that will update working memory. Similarly, the representations that continue to receive activation energy are maintained in working memory. In contrast, those that receive reduced energy are subtracted from it. Performing search using a modified version of the previous search, and doing so repeatedly, amounts to a compounded form of search that ultimately enables the compounding of predictions and inferences.</p>



<p>Again, newly activated representations are added to the representations that have remained in working memory from the previous state. This updated set is used to conduct the next search. This cycle is then repeated in a loop to produce the thinking process. Thus, there is a direct structural correspondence between the turnover of persistent neural activity, the gradual updating of working memory, and the continuity of the stream of thought. Many of the major features of thought derived from introspection (Hamilton, 1860; Weger et al., 2018) are addressed by this hypothetical explanation, such as how mental context is conserved from one thought to the next, how one thought is associated with the next, and how it logically (or probabilistically) implies the next.</p>



<p>This article focuses on ongoing, internally generated activity within working memory and the emergent iterative pattern of information flow. This pattern, introduced in Figure 2, is elaborated on methodically through a series of over 40 figures that attempt to illustrate the “shape” of the thought process. Topics considered include the neural basis of items in working memory, variation in the rate of updating, interactions between multiple working memory stores, and how all of this can be implemented within neural network models to enhance the performance of AI. This work builds on these issues while assimilating current theoretical approaches and remaining consistent with prevailing knowledge. Part 2 reviews pertinent literature that forms the foundation of the iterative updating model. Parts 3 and 4 develop said model, and Part 5 applies it to AI.</p>



<p id="literature-review"><strong><u>Part II:</u></strong><span> Literature Review</span></p>



<p id="2.1"><strong>2.1 Interactions Between Sensory Memory, Working Memory, and Long-term Memory</strong></p>



<p>Working memory has been defined as the components of the mind that temporarily hold a limited amount of information in a heightened state of availability for use in ongoing information processing (Cowan, 2016). It involves holding ephemeral sensory and semantic information (e.g., objects, shapes, colors, locations, movement patterns, symbols, rules, concepts, numbers, and words) in attention until they are needed to execute an action or decision. It is one of multiple phases of memory and has been variously referred to as immediate memory and primary memory. It was conceptualized by William James (1842-1910) as the “trailing edge of the conscious present” and a major determinant of which portions of new information will be perceived and which of those will be analyzed (James, 1890). Working memory is thought to facilitate various operations, such as planning, language comprehension, reasoning, decision making, and problem solving (Baddeley, 2012).</p>



<p>The working memory store is constantly updated with new items, which then fade over the course of seconds or minutes (some more quickly than others). Updating allocates processing resources to important information coming from the senses (e.g., novelties, needs, or threats) or from internal states (e.g., intentions, plans, or schemas). Most mental functions require the active maintenance of multiple items at once, along with systematic updating of these items (Baddeley, 2012). Active updating is necessary because the importance of individual items changes as processing demands change (Myers et al., 2017).</p>



<p>Research on working memory has traditionally relied on behavioral investigations (such as memory tasks) to study interactions and dissociations between memory systems. Experimental studies on the topic are concerned with capacity limits, rehearsal, interference, suppression of irrelevant information, removal of unnecessary information, and other regular phenomena. Theorists have tried to capture these regularities using abstract models.</p>



<p>From the late 1950s to the 1960s, memory researchers (e.g., Atkinson &amp; Shiffrin, 1968; Broadbent, 1958) developed models that conceptualized memory as being comprised of three interacting systems: (1) a sensory store that briefly holds and preprocesses sensory inputs, (2) an active short-term system capable of attending to this information over a time frame of seconds, and (3) a passive long-term system capable of maintaining information indefinitely (Fig. 3). Current models (including the present work) have retained many of these aspects.</p>







<figure><img data-attachment-id="456" data-permalink="https://aithought.com/blog/image-1-6/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-1.png" data-orig-size="975,315" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-1" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-1.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-1.png?w=975" loading="lazy" width="975" height="315" src="https://aithoughtcom.files.wordpress.com/2023/11/image-1.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-1.png 975w, https://aithoughtcom.files.wordpress.com/2023/11/image-1.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-1.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-1.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>







<p><strong>Fig. 3.</strong> Atkinson and Shiffrin’s Multi-store Model (1968)</p>



<p><em>This model depicts environmental stimuli received by the senses and held in sensory memory. If attended to, this stimulus information will enter short-term memory (i.e., working memory, shown in gray). If not rehearsed, it will be forgotten; if rehearsed, it will remain in short-term memory; and if sufficiently elaborated upon, it will be stored in long-term memory (shown in black), from which it can be retrieved later. Long-term memory is depicted in black here and throughout this article.</em></p>



<p>The multi-store model has been expanded upon in several pivotal ways. Studies performed by Alan Baddeley and Graham Hitch (1974, 1986) using dual-task interference experiments indicated that the capacity limitations for visual and verbal working memory are independent, leading the authors to categorize these two modalities as separable. This distinction led to the authors’ influential multicomponent model, which divided working memory into two domain-specific stores: the visuospatial sketchpad and the phonological buffer (Fig. 4). These stores work in concert to construct, sustain, and modify mental imagery.</p>



<p>Baddeley and Hitch also envisioned a dedicated supervisory subsystem, which they named the “central executive,” that selected items for activity, shuttled information from one store to another, and made other processing decisions. Because researchers have not yet explicitly determined how the central executive, visuospatial sketchpad, and phonological buffer cooperate, they remain areas of active research and theoretical inquiry.</p>







<figure><img data-attachment-id="458" data-permalink="https://aithought.com/blog/image-2-4/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-2.png" data-orig-size="975,481" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-2" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-2.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-2.png?w=975" loading="lazy" width="975" height="481" src="https://aithoughtcom.files.wordpress.com/2023/11/image-2.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-2.png 975w, https://aithoughtcom.files.wordpress.com/2023/11/image-2.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-2.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-2.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 4.</strong> Baddeley and Hitch’s Multicomponent Model (1974)</p>



<p><em>In this model, the short-term store from Atkinson and Shiffrin’s model is split into four interacting components that together constitute working memory: the visuospatial sketchpad, the phonological buffer, the central executive, and the episodic buffer, which was added later (Baddeley, 2000). These components interact with long-term memory, represented by the bottom rectangle.</em></p>



<p>Bernard Baars developed the functional framework model, which combines the multi-store model (Fig.3) with the multicomponent model (Fig. 4) (Baars, 2007). This framework, adapted in Figure 5, integrates other cognitive constructs such as attention, consciousness, and planning. It also draws further subdivisions within long-term memory.</p>







<figure><img data-attachment-id="459" data-permalink="https://aithought.com/blog/image-3-4/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-3.png" data-orig-size="975,501" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-3" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-3.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-3.png?w=975" loading="lazy" width="975" height="501" src="https://aithoughtcom.files.wordpress.com/2023/11/image-3.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-3.png 975w, https://aithoughtcom.files.wordpress.com/2023/11/image-3.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-3.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-3.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 5.</strong> Baars and Gage’s Functional Framework (2007)</p>



<p><em>This model incorporates the multi-store model with the multicomponent model. Working memory activates long-term memories, knowledge, and skills, which are shown in the box at the bottom. Spontaneous (bottom-up) attention and voluntary (top-down) attention are symbolized as vectors.</em></p>



<p>In 1988, Bernard Baars introduced the global workspace model (Fig. 6). Therein, active contents in working memory are broadcast throughout the brain, stimulating unconscious long-term memories. These long-term memories then compete to enter the global workspace. This type of organization is known as a “blackboard” architecture and can be traced back to Newel and Simon (1961). Many present-day computer science, neural, and psychological models assume a fleeting but centralized working memory capacity that acts as a common workspace where long-term memories become coactive and are exposed to one another (e.g., Dehaene, 2020; Ryan et al., 2019; Glushchenko et al., 2018).</p>







<figure><img data-attachment-id="461" data-permalink="https://aithought.com/blog/image-4-4/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-4.png" data-orig-size="725,540" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-4" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-4.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-4.png?w=725" loading="lazy" width="725" height="540" src="https://aithoughtcom.files.wordpress.com/2023/11/image-4.png?w=725" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-4.png 725w, https://aithoughtcom.files.wordpress.com/2023/11/image-4.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-4.png?w=300 300w" sizes="(max-width: 725px) 100vw, 725px"></figure>



<p><strong>Fig. 6.</strong> Baar’s Global Workspace Model (1988)</p>



<p><em>The brain has several lower order modules that are generally isolated from each other. Unconscious processes and environmental stimuli processed by these modules compete for access to the global workspace. The most salient inputs enter the workspace where they are integrated, stored temporarily, and made conscious. These results are then broadcast back to the rest of the brain.</em></p>



<p id="2.2"><strong>2.2 The Focus of Attention Is Embedded within the Short-term Memory Store</strong></p>



<p>Early models attempting to explain how long-term memory is transferred into working memory were influenced by computer science. They envisioned long-term memories being copied and transferred from long-term storage to a separate processing substrate (i.e., from the hard drive to random-access memory (RAM) to the central processing unit (CPU)). In a departure from this conception, several theorists (e.g., Cowan, 1984; Norman, 1968; Treisman, 1964) conceived that information is encoded into working memory when existing units of long-term memory are activated and attended to without being copied or transported. Today, this is commonly referred to as activated long-term memory.</p>



<p>Brain imaging studies support this view and provide evidence that units of long-term memory reside in the exact locations involved in processing this information during non-working memory scenarios (D’Esposito &amp; Postle, 2015). These findings suggest that information is not copied and transferred between dedicated registers, but activated right where it is (in situ) (Chein &amp; Fiez, 2010; Moscovitch et al., 2007). The concept may apply equally to artificial neural networks. Thus, although neurons are stationary, as long as they remain active, they continue to broadcast their encoded information to the neurons they project to.</p>



<p>Nelson Cowan’s embedded processes model (1988) reconciles the main features of the multi-store and multicomponent models with the concept of activated long-term memory. In Cowan’s model, the short-term memory store is comprised of units of long-term memory that are activated above baseline levels, such as memories that have been primed. This activation can last from seconds to hours. Thus, the short-term store of working memory is simply an active subset of the long-term store it is “embedded” within (Cowan, 1999).</p>



<p>The other key component of Cowan’s model is the focus of attention (FoA). The FoA holds consciously attended units of information and is embedded within the short-term store (Fig. 7). Units in the FoA comprise an even more active subset of the short-term store. Their elevated activity lasts from milliseconds to several seconds. Cowan and others consider the short-term store and the FoA together as constituting working memory (Cowan, 2005).</p>







<figure><img data-attachment-id="573" data-permalink="https://aithought.com/blog/image-1-7/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/image-1.png" data-orig-size="975,405" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-1" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/image-1.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/image-1.png?w=975" loading="lazy" width="975" height="405" src="https://aithoughtcom.files.wordpress.com/2023/12/image-1.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/image-1.png 975w, https://aithoughtcom.files.wordpress.com/2023/12/image-1.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/image-1.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/image-1.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 7.</strong> Cowan’s Embedded Processes Model (1988)</p>



<p><em>According to this model, short-term storage is an activated subset of long-term storage. Similarly, the focus of attention (FoA) is an attended subset of short-term storage. These stores interact with a store for sensory memory and a central executive.</em></p>



<p>During perception, task-relevant features from the sensory store are used to update the FoA. When attention shifts to other information, these items pass into the short-term store (Nyberg &amp; Eriksson, 2016). However, information demoted from the FoA to the short-term store can still influence automatic actions and be readily reactivated into the FoA (Manohar et al., 2019). If not reactivated, this information returns to inert long-term memory (through the processes of decay, inhibition, interference, or contamination) (Cowan, 2009). Some items that enter working memory are demoted almost immediately, whereas others remain active for sustained periods (Cowan, 2011). This feature, along with features of the other models discussed thus far, forms critical assumptions about updating subsumed by the present model. Figure 8 provides a summary of the forms of human memory.</p>







<figure><img data-attachment-id="574" data-permalink="https://aithought.com/blog/image-2-5/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/image-2.png" data-orig-size="975,611" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-2" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/image-2.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/image-2.png?w=975" loading="lazy" width="975" height="611" src="https://aithoughtcom.files.wordpress.com/2023/12/image-2.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/image-2.png 975w, https://aithoughtcom.files.wordpress.com/2023/12/image-2.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/image-2.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/image-2.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 8.</strong> Forms of Human Memory</p>



<p><em>Human memory can be divided into three phases, each of which can be decomposed into other forms.</em></p>



<p id="2.3"><strong>2.3 Sustained Firing Maintains Information in the Focus of Attention</strong></p>



<p>Understanding how the brain provides for working memory should be paramount when designing working memory for computers. The neurophysiological basis of the persistent activity responsible for working memory is an active research area. Single-cell recordings of neurons in primates reveal that information retention occurs via a cellular phenomenon known as sustained firing. Glutamatergic pyramidal neurons in the prefrontal cortex (PFC), parietal cortex, and other association cortices are specialized for sustained activity, allowing the cells to generate action potentials at elevated rates for several seconds at a time (Funahashi, 2007; Fuster, 2015). Sustained firing is thought to maintain the signal of information that the neuron encodes. A neuron in the PFC with a background firing rate of 10 Hz (typical for cortical cells) might increase its firing rate to 20 Hz when utilizing sustained firing to preserve mnemonic information temporarily.</p>



<p>One of the earliest of these studies provides an illustrative example. In 1973, Joaquin Fuster recorded the sustained electrical activity of PFC neurons in monkeys performing a delayed matching task. In the task, a macaque monkey watches the experimenter place food under one of two identical cups. A shutter is then lowered for a variable delay period, so the cups are not visible. After the delay, the shutter is raised, and the monkey is given one attempt to collect the food. Through training, the animal learns to choose the correct cup on the first attempt. Completing the task requires the animal to hold the location of the food in working memory during the delay period. Presumably, the monkey must sustain either a retrospective sensory representation of the food’s location or a prospective representation of the motor plan needed to retrieve it.</p>



<p>Using implanted electrodes, Fuster could record from neurons in the PFC that fired throughout the delay period. He found that the sustained firing subsided once the monkey responded, suggesting that the observed neuronal activity represented the food’s location while the cup was out of sight. This landmark study revealed the brain’s mechanism for keeping meaningful representations active without external input. It also suggested the presence of a dynamically updated pool of coactive neurons underlying thought and behavior. It is important to mention that processes besides sustained firing may be responsible for maintenance in the FoA (e.g., dynamic coding or activity states distributed across neuronal populations (Stokes, 2015; Jacob et al., 2018), yet iterative updating could apply to these as well.</p>



<p>Subsequent research has found that the duration of sustained firing predicts whether items will be remembered. When this delay-period activity is weak, the likelihood of forgetting is greater (Funahashi et al., 1993). Moreover, lesioning of the prefrontal and association cortices (which contain neurons with the greatest capacity for sustained firing) significantly impairs performance in these tasks. Consistent with this animal work, functional magnetic resonance imaging (fMRI) studies in humans show that activity in prefrontal and association areas persists during the delay period of similar working memory tasks. In fact, the magnitude of this activation positively correlates with the number of items subjects are instructed to hold in memory (Rypma et al., 2002).</p>



<p>Patricia Goldman-Rakic (1987, 1990, 1995) was the first to suggest that the phenomenon of sustained firing in the PFC is responsible for the retention interval exhibited by working memory. Further work by Fuster (2009), Goldman-Rakic (1995), and others has shown that neuronal microcircuits within the PFC maintain information in working memory via recurrent, excitatory glutamatergic networks of pyramidal cells (Baddeley &amp; Hitch, 1994; Miller &amp; Cohen, 2001). Many researchers now believe that sustained firing is critical in maintaining working memory. The evidence backing this assumption is provided by studies reporting positive correlations between sustained firing and working memory performance. For example, both human and animal subjects can retain information in mind as long as sustained firing persists (Rypma et al., 2002). This has been found using extracellular, electroencephalographic, and hemodynamic approaches (D’Esposito &amp; Postle, 2015).</p>



<p>Sustained firing in the PFC and parietal cortex is now assumed to underlie the capacity to internally maintain and update the contents of the FoA (Braver &amp; Cohen, 2000; Sarter et al., 2001). As a result, working memory, executive processing, and cognitive control are now widely thought to rely on the maintenance of activity in multimodal association areas that correspond to goal-relevant features and patterns (Baddeley, 2007; Fuster, 2002a; Moscovich, 1992; Postle, 2007). Sustained rates of action potentials allow responses throughout the brain to be modulated by prior history over multiple timescales, from milliseconds to tens of seconds.</p>



<p id="2.4"><strong>2.4 Synaptic Potentiation Maintains Information in the Short-term Store</strong></p>



<p>fMRI studies have suggested that the information represented by sustained firing corresponds only to the FoA, not the short-term store as a whole (Lewis-Peacock et al., 2012). This is because neuronal activity corresponding to items that have exited the FoA quickly drops to baseline firing rates. Nevertheless, information about the items may be rapidly and reliably recalled after a brief delay. It is thought that the passive retention of information in the short-term store but outside the FoA may be mediated by a different “activity-silent” neural mechanism, such as changes in synaptic potentiation (short-term synaptic plasticity) (LaRocque et al., 2014; Rose, 2016). The evidence supporting this is strong (Silvanto, 2017; Nairne, 2002). For example, synaptic strength can be temporarily modified by transient increases in the concentration of presynaptic calcium ions or by GluR1-dependent short-term potentiation (Silvanto, 2017). The information potentiated by these changes in synaptic weighting can be converted back into active neural firing if the memory is reactivated by a contextual retrieval cue (Nairne, 2002).</p>



<p>Thus, the maintenance of information in working memory is achieved by at least two neural phenomena operating in parallel that correspond to distinct states of prioritization: sustained firing, which maintains information in the FoA, and synaptic potentiation, which maintains information in the short-term store. Both mechanisms contribute to the initialization of long-term potentiation, including RNA synthesis, protein synthesis, and morphological synaptic changes that underlie the formation and consolidation of new long-term memories (Debanne, 2019). Table 2 summarizes the general properties of the four phases of human memory.</p>







<figure><img data-attachment-id="577" data-permalink="https://aithought.com/blog/screenshot-2/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-2.png" data-orig-size="1117,722" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-2.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-2.png?w=1024" loading="lazy" width="1024" height="661" src="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-2.png?w=1024" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-2.png?w=1024 1024w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-2.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-2.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-2.png?w=768 768w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-2.png 1117w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><strong>Table 2.</strong> General Characteristics of Four Forms of Memory</p>



<p><em>This table summarizes some of the major comparisons between four different forms of memory. The details addressed in this table are not definitive and are active areas of debate and research. (seeBaddeley et al., 2018; Christophel et al., 2017; Shipstead et al., 2015; Brydges et al., 2018; Cowan, 2017; Eriksson et al., 2015; Chia et al., 2018; Constantinidis et al., 2018).</em></p>



<p>Modern artificial neural networks used in AI utilize several memory features from the five models discussed in this literature review. They do not generally use analogs of sustained firing or synaptic potentiation, although some use a simplistic form of persistent activity known as recurrence. As Figure 9 illustrates, a recurrent neuron, such as that found in a recurrent neural network, reroutes its output back to its input. This allows it to hold the memory of an internal state, which can affect subsequent states. These machine learning nodes can use this recurrent feature to selectively store, update, or forget information based on their input and previous state. As we will see, this functionality permits them to uncover patterns in time (long-range dependencies from sequential inputs). Recurrent neurons, programmed properly, should permit artificial neural networks to simulate sustained firing and synaptic potentiation, as well as attain the cognitive properties discussed in upcoming sections, including the capacity to support a working memory that is updated iteratively.</p>







<figure><img data-attachment-id="578" data-permalink="https://aithought.com/blog/image-3-5/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/image-3.png" data-orig-size="975,364" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-3" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/image-3.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/image-3.png?w=975" loading="lazy" width="975" height="364" src="https://aithoughtcom.files.wordpress.com/2023/12/image-3.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/image-3.png 975w, https://aithoughtcom.files.wordpress.com/2023/12/image-3.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/image-3.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/image-3.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 9.</strong> Recurrency in Artificial Neural Networks Can Simulate Persistent Neural Activity</p>



<p><em>A. A simplified version of an artificial neuron used in computer science, with inputs and weights labeled. B. An artificial recurrent neuron in a hidden layer, passing information from an input layer to an output layer. This hypothetical neuron (inspired by Table 2) exhibits recurrency in its cell body and limited recurrency in some of its weights. Thus, it demonstrates an analog of both sustained firing and synaptic potentiation. C. The activity of this neuron is unfolded through time.</em></p>



<p>Persistent neural activity in the form of sustained firing and synaptic potentiation is time-limited. It runs out. When some neurons exit persistent activity and others enter it, working memory updating occurs. In other words, the updating of persistent activity provides the conceptual basis used by this article to view the previous five models from the perspective of iteration. The following section will discuss how this perspective provides insight into the process of thought.</p>



<p id="part3"><strong><u>Part III: </u></strong><span>Working Memory is Updated Iteratively</span></p>



<p><strong>3.1 Persistent Activity Causes Successive States to Overlap Iteratively</strong></p>



<p>In the preceding sections, we delved into the diverse array of models that define our understanding of working memory. However, a conspicuous gap emerges in this landscape: the omission of iterative updating as a core mechanism. This oversight is not just a minor detail, but rather a crucial element in furthering our comprehension of working memory’s dynamics. In this section, we will explore how iterative updating occurs in synchrony at both the neural and psychological levels, bridging these domains, and offering new insights that extend current models.</p>



<p>Even though models of working memory do not acknowledge that content is updated iteratively, the nature of persistent activity strongly implies that iterative updating is pervasive. Allow me to use another analogy to explain why. Take the human population of Earth, for instance. In the next year, many people will pass away, others will be born, yet most will remain living. In the same sense, in one second, some of the brain’s neurons will stop exhibiting persistent activity, some new neurons will enter persistent activity, yet most will remain in persistent activity. The people and neurons that persist can influence subsequent states. In the same way that there could be no intergenerational knowledge transfer (culture) on a planet where generations do not overlap, there can be no thinking in a brain where spans of neural activity do not overlap.</p>



<p>The study of sustained firing has shown that the neocortex contains many neurons in persistent coactivity at any instant in time (Goldman-Rakic, 1995). Nevertheless, these coactive neurons could not have all started firing at the same time, nor could they all stop firing at the same time. This is similar to how the people on Earth are not all born at the same time and do not die at the same time. Because sustained firing has been shown to occur for different durations in different neurons (Fuster, 2008, 2002b), their spans of activity must be staggered and must only partially overlap with one another rather than completely coincide (Reser, 2016), as portrayed in Figure 10.</p>



<figure><img data-attachment-id="616" data-permalink="https://aithought.com/blog/persistant-activity-1-recovered/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/persistant-activity-2-recovered.jpg" data-orig-size="4313,1760" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;Persistant Activity 1 [Recovered]&quot;,&quot;created_timestamp&quot;:&quot;1701870100&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;Persistant Activity 1 [Recovered]&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Persistant Activity 1 [Recovered]" data-image-description="" data-image-caption="<p>Persistant Activity 1 [Recovered]</p>
" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/persistant-activity-2-recovered.jpg?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/persistant-activity-2-recovered.jpg?w=1024" loading="lazy" width="1024" height="417" src="https://aithoughtcom.files.wordpress.com/2023/12/persistant-activity-2-recovered.jpg?w=1024" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/persistant-activity-2-recovered.jpg?w=1022 1022w, https://aithoughtcom.files.wordpress.com/2023/12/persistant-activity-2-recovered.jpg?w=2044 2044w, https://aithoughtcom.files.wordpress.com/2023/12/persistant-activity-2-recovered.jpg?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/persistant-activity-2-recovered.jpg?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/persistant-activity-2-recovered.jpg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Persistant Activity 1 [Recovered]</figcaption></figure>



<p><strong>Fig. 10.</strong> A Set of 20 Neurons Exhibiting Sustained Firing</p>



<p><em>The sustained firing of 20 hypothetical neurons is shown here. The x-axis represents time. The spans of individual neurons overlap but are staggered and asynchronous. T1 and t2 are marked at the bottom. The two time periods share 10 of 20 active neurons in common. This figure exemplifies the transitional reconfiguration that distinguishes iterative updating.</em></p>



<p>If updates to the set of neurons in sustained firing involve partial rather than complete replacement, then these dynamics indicate an ongoing pattern of iteration and recursion. Iteration involves the application of a computational procedure to the results of a previous application. It is common in mathematics and computer science. Iteration’s sister algorithm, recursion, is the reapplication of a rule, definition, or procedure to successive results. A recursive function references itself. Self-referential routines are common in mathematics and computer science but mostly unknown in psychology. The terms “iteration” and “recursion” uniquely capture different aspects of the present model, and both are used here depending on context.</p>



<p>The principles of iteration and recursion as they pertain to the present model are illustrated in Figure 11. At time 1 (t1), neuron “a” has stopped firing. Neurons b, c, d, and e exhibit sustained coactivity. By time 2 (t2), neuron “b” has stopped firing, while c, d, and e continue to fire, and “f” begins to fire. In time 2, c, d, and e recur. The figure depicts iteration because the set of coactive neurons at time 2 (c, d, e, and f) includes a subset (c, d, and e) of the coactive neurons at time 1. In computer programming, the goal of iteration is to obtain successively closer approximations to the solution of a problem. In later sections, this article will advocate that the algorithm of thought utilizes iteration for the same purpose.</p>



<figure><img data-attachment-id="584" data-permalink="https://aithought.com/blog/image-5-4/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/image-5.png" data-orig-size="975,205" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-5" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/image-5.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/image-5.png?w=975" loading="lazy" width="975" height="205" src="https://aithoughtcom.files.wordpress.com/2023/12/image-5.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/image-5.png 975w, https://aithoughtcom.files.wordpress.com/2023/12/image-5.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/image-5.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/image-5.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 11.</strong> Depiction of Iteration in Neurons Exhibiting Sustained Firing</p>



<p><em>Each arc, designated by a lowercase letter, represents the time span during which a neuron exhibiting sustained firing remains active. The x-axis represents time. Dashed arcs represent neurons that have stopped firing, whereas full arcs denote neurons that are still active. The neurons shown here demonstrate iteration and recursion.</em></p>



<p>Given that at any point in time, we can expect there to be thousands of neurons engaged in sustained firing, we should expect the type of iterative pattern seen in Figure 11 to be ubiquitous. Furthermore, if examined on the order of hundreds of milliseconds, we should expect activity in the brain to be densely iterative. Iteration within the FoA causes consecutive brain states to be interrelated and autocorrelated as a function of the delay between them. Because a delineable subset of the active cells that characterize one brain state remain active in the next, each state is recursively nested within the one that precedes it. This allows the brain to record and keep track of its interactions with the environment (stateful) so that each interaction does not have to be handled based only on the information available at present (stateless).</p>



<p>It is asserted here that iterative updating should be considered inherent in any brain with neurons exhibiting persistent activity and that animals utilize it as a fundamental means of information processing. In particular, working memory may harness iteration in a way that allows potentially related representations to accumulate and coactivate despite delays between their initial appearances. This ensures that relevant processing products are temporarily sustained until a full suite of contextually related items is compiled to be used in aggregate to inform behavior.</p>



<figure><img data-attachment-id="587" data-permalink="https://aithought.com/blog/screenshot-3/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-3.png" data-orig-size="809,715" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-3" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-3.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-3.png?w=809" loading="lazy" width="809" height="715" src="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-3.png?w=809" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-3.png 809w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-3.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-3.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-3.png?w=768 768w" sizes="(max-width: 809px) 100vw, 809px"></figure>







<p>The incremental updating expected at the neurophysiological level may be isomorphic with and provide a substrate for the incremental updating experienced on the psychological level. For example, a given line of thought does not change all at once but rather makes additive transitions that are grounded by content that remains unchanged. The subset of neurons that continue to exhibit persistent activity over the course of these incremental changes should be expected to embody the persisting subject of mental analysis. Stated differently, neurons with the longest-lasting activation likely correspond to the underlying topic of thought that remains as other contextual features come and go. This creates coherence and continuity between distinct epochs (Reser, 2016), as depicted in Figure 12. The present article posits that without the continuity made possible by iteration, thought as we know it cannot arise and will not be available to machines.</p>



<p>.</p>



<figure><img data-attachment-id="588" data-permalink="https://aithought.com/blog/image-6-3/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/image-6.png" data-orig-size="975,733" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-6" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/image-6.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/image-6.png?w=975" loading="lazy" width="975" height="733" src="https://aithoughtcom.files.wordpress.com/2023/12/image-6.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/image-6.png 975w, https://aithoughtcom.files.wordpress.com/2023/12/image-6.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/image-6.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/image-6.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 12.</strong> Venn Diagrams of Information Shared Between Successive States of Working Memory</p>



<p><em>These Venn diagrams depict informational overlap between successive states of working memory. The horizontal axis represents time. The small circles represent information within the FoA, while the large circles represent information within the short-term store. Diagrams 1 and 2 show no Venn overlap between states from different periods; 3 and 4 show overlap in the short-term store only; 5 and 6 show the short-term store of one state overlapping with the FoA of the neighboring state; and 7 and 8 show the FoA of separate states overlapping, suggesting attentive continuity. It may be plausible that Diagrams 1 and 2 roughly depict sampling of cortical activity hours apart, 3 and 4 depict sampling several minutes apart, 5 and 6 depict sampling every minute, and 7 and 8 depict sampling every second.</em></p>



<p>Diagram 1 of Figure 12 depicts two states of working memory whose contents do not overlap. We can assume that these states are from separate thoughts. Diagram 7 depicts two states whose contents overlap significantly. It is intended to represent a fractional transition in the thought process, such as two points in a line of reasoning. The overlapping informational content of the small circles shown in Diagram 7 indicates that the two states share neurons in common that exhibit sustained firing. The overlap of the large circles represents the sharing of potentiated synapses. Thus, the diagrams shown in Figure 12 depict updating as continuous change in active neurons and synapses. However, as the rest of this article will explore, partial change to the FoA may be more realistically depicted as iterative updates in discrete cognitive items.</p>







<figure><img data-attachment-id="482" data-permalink="https://aithought.com/blog/screenshot-80/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/screenshot-80.png" data-orig-size="2170,544" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-80" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/screenshot-80.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/screenshot-80.png?w=1024" loading="lazy" width="1024" height="256" src="https://aithoughtcom.files.wordpress.com/2023/11/screenshot-80.png?w=1024" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/screenshot-80.png?w=1024 1024w, https://aithoughtcom.files.wordpress.com/2023/11/screenshot-80.png?w=1021 1021w, https://aithoughtcom.files.wordpress.com/2023/11/screenshot-80.png?w=2042 2042w, https://aithoughtcom.files.wordpress.com/2023/11/screenshot-80.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/screenshot-80.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/screenshot-80.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><a><strong>Table 3.</strong></a> Definition of Key Terms</p>



<p><strong>3.2 The Iterative Updating of Representations Allows Context to Shift</strong></p>



<p>Figure 13 depicts an FoA store that holds four mental representations at a time. We will refer to these representations as items (also known in psychology as chunks). In this example, one discrete item is replaced at each point in time. Thus, it could be conceptualized as a “sliding store.” The depiction of the FoA store as limited to four items is derived from an extensive literature review by Cowan (2001, 2005), which demonstrates that adults are generally able to recollect four items (plus or minus one) in situations when they cannot carry out chunking, rehearsal, or other memory strategies to aid them. This capacity of four items generally holds whether the items are numbers, words in a list, or visual objects in an array. The figures could alternatively feature seven items rather than four after less restrictively controlled research by George Miller (1956). While discussing the capacity of the FoA, Cowan remarked,</p>



<blockquote>
<p><em>“When people must recall items from a category in long-term memory, such as states of the United States, they do so in spurts of about three items on average. It is as if a bucket of short-term memory is filled from the well of long-term memory and must be emptied before it is refilled.”</em></p>



<p>Nelson Cowan (2009, p. 327)</p>
</blockquote>



<p>Yet, perhaps this bucket does not need to be emptied to be refilled. Perhaps it can be emptied and filled simultaneously. While naming states and repeating numbers may not necessitate this, rational thought may.</p>







<figure><img data-attachment-id="301" data-permalink="https://aithought.com/blog/image-1-3/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/04/image-1.png" data-orig-size="975,473" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-1" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/04/image-1.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/04/image-1.png?w=975" loading="lazy" width="975" height="473" src="https://aithoughtcom.files.wordpress.com/2023/04/image-1.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/04/image-1.png 975w, https://aithoughtcom.files.wordpress.com/2023/04/image-1.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/04/image-1.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/04/image-1.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 13.</strong> Abstract Schematic of Iterative Updating in the FoA</p>



<p><em>As with the following figures in this article, Figure 13 is an emblematic abstraction that uses a state-space model in discrete time. White spheres indicate active mental concepts (items), while black spheres indicate inactive ones. At time 1, item A has just been deactivated, while B, C, D, and E are coactive. This echoes the pattern of activity shown in Figure 11, except that the uppercase letters here represent items, whereas the lowercase letters in Figure 11 represent neurons. While coactive, these items (B, C, D, and E) spread their activation energy, resulting in the convergence of activity onto a new item, F. At time 2, B has been deactivated; C, D, and E remain active; and F has become active.</em></p>



<p>At time 2, three items from time 1 (C, D, and E) remain active and are combined with the update located in time 1 (F). This new set of items is then used to search for the next update (G). Items C and E demonstrate reiteration because they exhibit uninterrupted activity from time 1 through time 3. The longer these items are coactive, the more likely they will become associated and possibly “chunk” or merge into a single item, “CE”. While C and E remain active, their underlying neural circuits can be expected to impose sustained, top-down information processing biases on the targets they project to throughout the thalamocortical hierarchy. Items sustained enduringly in this way should be expected to influence the overarching theme of ongoing thought.</p>



<p>Imagine that item B represents your psychological concept of brownies, C represents your friend Cameron, D represents shopping, and E represents a grocery store. With these representations active in your FoA, you may form a mental image of your friend Cameron shopping for brownies at a grocery store. This scenario may cause you to remember Cameron’s preference for drinking milk when he eats brownies. Thus, your next thought may be about your friend shopping in the same store for milk. Some contextual factors (the place, person, and activity) remained the same even though another (the object being shopped for) changed. This kind of narrative about the same place and person could take several seconds and many rounds of iteration to play out. This example illustrates how iteration enables continuity by allowing context to shift incrementally, which, this paper contends, is a central hallmark of the thought process. Figure 14 offers a different narrative example.</p>







<figure><img data-attachment-id="360" data-permalink="https://aithought.com/blog/image-5/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/09/image.png" data-orig-size="975,321" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/09/image.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/09/image.png?w=975" loading="lazy" width="975" height="321" src="https://aithoughtcom.files.wordpress.com/2023/09/image.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/09/image.png 975w, https://aithoughtcom.files.wordpress.com/2023/09/image.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/09/image.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/09/image.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 14.</strong> Working Memory Updating Mediates Intelligent Transitions Between Mental States</p>



<p><em>This series illustrates the iterative working memory activity of a person who thinks about watering a plant. The person imagines a wilting plant with dry soil. This set of coactivates in working memory spreads neural activity, which converges on the concept of water. This new set, in turn, induces the ideation of using a watering can. The person then imagines tilting and then pouring the water from the watering can until they stop watering the plant.</em></p>



<p>Figure 15 expands on the schematic from Figure 13, exemplifying how working memory capacity can vary.</p>







<figure><img data-attachment-id="486" data-permalink="https://aithought.com/blog/image-10/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-10.png" data-orig-size="975,470" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-10" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-10.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-10.png?w=975" loading="lazy" width="975" height="470" src="https://aithoughtcom.files.wordpress.com/2023/11/image-10.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-10.png 975w, https://aithoughtcom.files.wordpress.com/2023/11/image-10.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-10.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-10.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 15.</strong> The Capacity Limit of Working Memory Varies</p>



<p><em>Four different examples of working memory capacity, from two to eight. The capacity of working memory can vary from trial to trial, person to person, and, presumably, from species to species. Large language models used in AI hold thousands of tokens in a rudimentary form of attention and update these coactive sets of tokens iteratively when forming predictions.</em></p>







<figure><img data-attachment-id="491" data-permalink="https://aithought.com/blog/image-13/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-13.png" data-orig-size="975,436" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-13" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-13.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-13.png?w=975" loading="lazy" width="975" height="436" src="https://aithoughtcom.files.wordpress.com/2023/11/image-13.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-13.png 975w, https://aithoughtcom.files.wordpress.com/2023/11/image-13.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-13.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-13.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 16.</strong> Four Updating Replacement Schedules</p>



<p><em>The item removed (evicted) can be the oldest entry in working memory (first in, first out), the newest entry (last in, first out), or anything in between. The least informative or relevant item should be the one selected for replacement.</em></p>



<p>Figure 17 exemplifies how the contents of working memory can correspond to either external stimuli or internal concepts. It is probably fair to say that in all mammals, but in very few computer programs, external and internal representations coactivate and interact in real time.</p>







<figure><img data-attachment-id="488" data-permalink="https://aithought.com/blog/image-11/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-11.png" data-orig-size="975,329" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-11" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-11.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-11.png?w=975" loading="lazy" width="975" height="329" src="https://aithoughtcom.files.wordpress.com/2023/11/image-11.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-11.png 975w, https://aithoughtcom.files.wordpress.com/2023/11/image-11.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-11.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-11.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 17.</strong> Working Memory Responds to and Navigates Interactions with the World</p>



<p><em>Items marked with an “S” represent external stimuli, while items marked with a “C” represent internally selected concepts. This series illustrates the working memory history of a driver responding to a green light that turns yellow. The yellow light in this context prompts the driver to check their distance to the intersection. When they find that the intersection is near, they accelerate. However, when the light turns red, this cues the driver to brake.</em></p>



<p><strong>3.3 The Rate of Iterative Updating Varies with Demand</strong></p>



<p>Although Figures 13-17 depict working memory updating one unit at a time, this varies according to processing demands. For instance, when an individual pursues a new train of thought, initiates a different task, or is exposed to a novel or unexpected stimulus, their attention shifts entirely from its previous focus. When this happens, the content of the FoA can change completely. In this scenario, attentional resources are reallocated to the new context, and rather than a graduated transition, an abrupt transition occurs without iteration. Figure 18 depicts various transitions in the FoA.</p>







<figure><img data-attachment-id="490" data-permalink="https://aithought.com/blog/image-12/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-12.png" data-orig-size="975,453" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-12" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-12.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-12.png?w=975" loading="lazy" width="975" height="453" src="https://aithoughtcom.files.wordpress.com/2023/11/image-12.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-12.png 975w, https://aithoughtcom.files.wordpress.com/2023/11/image-12.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-12.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-12.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 18.</strong> Four Possible State Transitions in the FoA</p>



<p><em>In the first diagram, there are four active items at time 1, which are marked as white spheres. At time 2, one of these four items has been replaced, so that one white sphere (B) becomes black (inactive) and a different black sphere (F) becomes white (active). Thus, 25% (1 ÷ 4) of the items have been updated between time 1 and time 2 without any change in the total number of active items. For clarity, most other figures in this article feature this single-item updating. However, in a store with four items, updating can occur in three other ways. The other diagrams in this figure depict 50%, 75%, and 100% updating.</em></p>



<p>Note that abrupt, noniterative updating is not possible in the short-term store. This is due to the slower nature of turnover in synaptic potentiation. Because the number of active representations is much higher and they subside much more slowly (minutes) than in the FoA (seconds), the short-term store will continue to exhibit substantial iterative overlap, even during complete shifts in focal attention. Thus, the rate of updating from one period to the next is expected to remain relatively stable in the short-term store. In contrast, the rate of updating in the FoA is expected to fluctuate markedly under different processing requirements.</p>



<p>We should expect the average percentage of updating within the FoA per unit time to be lower in animals with larger, more complex brains. During mammalian evolution, association cortices were greatly enlarged relative to sensory cortices (Striedter, 2005). This development increased the number of neurons capable of sustained firing, as well as their maximum duration (Sousa et al., 2017), despite increased metabolic costs (Mongillo et al., 2008). For primates, and humans in particular, the presence of highly developed association areas likely leads to (1) more and longer sustained activity, (2) extended coactivity of items, (3) a lower percentage of updating per second, and (4) a corresponding higher degree of continuity between iterations.</p>



<p>In animals, a lower percentage of iterative updating might correlate with greater working memory capacity and higher fluid and general intelligence. This can be conceptualized as a longer working memory half-life. The concept of a half-life could be used to quantify the persistence of information in both the FoA and the short-term store, where generally, the shorter the half-life of activity in working memory, the shorter the attention span. For instance, the half-life for the diagram in Figure 18 exhibiting 25% updating is two time intervals, whereas the half-life for 50% updating is only one time interval.</p>



<p>Figure 19 addresses the decay rate using an FoA capacity of seven items. The first diagram illustrates how neural activity in small-brained animals primarily models the present and adjusts this model with bits from the recent past. The second diagram illustrates how neural activity in large-brained mammals models the recent past and adjusts it with bits from the present.</p>







<figure><img data-attachment-id="592" data-permalink="https://aithought.com/blog/image-7-2/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/image-7.png" data-orig-size="975,479" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-7" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/image-7.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/image-7.png?w=975" loading="lazy" width="975" height="479" src="https://aithoughtcom.files.wordpress.com/2023/12/image-7.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/image-7.png 975w, https://aithoughtcom.files.wordpress.com/2023/12/image-7.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/image-7.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/image-7.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 19.</strong> Two Rates of Updating Carried Out Over Four Time Periods</p>



<p><em>In the first scenario, 71% (5 ÷ 7) updating is carried out over four time periods. In the second scenario, 29% (2 ÷ 7) updating is carried out. This comparison delineates the difference between unfocused, minimally overlapping thought (loose iterative coupling) and highly focused, closely overlapping thought (tight iterative coupling). To better illustrate this point, the capacity of the FoA is depicted here as seven items after Miller (1956). The Venn diagrams to the right illustrate the percentage of iterative updating in the FoA using the style of Figure 12.</em></p>



<p>The top diagram in Figure 19 covers a wider breadth of information, is more responsive and proceeds at a faster rate. However, it may be associated with an attention deficit, distractibility, and superficial associations. The bottom diagram is probably more conducive to concentrated attention, effortful/elaborative processing, and structured systematization of knowledge. This is because the search for the next state will be informed by a larger number of conserved parameters. Contrarily, in Diagram 1, more than half of the initial parameters are excluded after only one time interval because they could not be maintained. Thus, the next search performed loses precision and specificity. For example, it should be more difficult to solve a mathematical word problem in one’s head using the updating strategy depicted in Diagram 1 relative to that in Diagram 2 because too many of the problem’s crucial elements would be forgotten prematurely and thus would not be available to contribute spreading activity in the search for a solution.</p>



<p>These two diagrams may represent the distinction not only between information processing in “lower” and “higher” animals but also between implicit and explicit processing in a single animal. Diagram 1 may be illustrative of implicit or system one processing (i.e., Kahneman’s “thinking fast” (2011)) and its impulsive, heuristic, intuitive approach. Diagram 2 may illustrate explicit or system two processing (i.e., “thinking slow”) in which a problem is encountered that requires multiple processing steps, recruitment of executive attention, the prefrontal cortex, and the prolonged maintenance of intermediate results. Figure 20 is meant to convey that implicit and explicit processing exist on a continuum and that implicit processing may transition into explicit when dopaminergic centers are engaged by demand, novelty, surprise, curiosity, anticipated reward, or error feedback, increasing the duration of sustained firing in the neurons that represent prioritized contextual variables.</p>







<figure><img data-attachment-id="493" data-permalink="https://aithought.com/blog/image-15/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-15.png" data-orig-size="975,278" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-15" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-15.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-15.png?w=975" loading="lazy" width="975" height="278" src="https://aithoughtcom.files.wordpress.com/2023/11/image-15.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-15.png 975w, https://aithoughtcom.files.wordpress.com/2023/11/image-15.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-15.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-15.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 20.</strong> Dopamine May Reduce the Rate of Iterative Updating in Working Memory</p>



<p><em>A set of seven items is held in working memory. 57% (4 ÷ 7) updating is carried out over four time periods. At t5, the rate of updating is reduced to 14% (1 ÷ 7). This might happen when a person encounters a novel set of stimuli that causes the brain to release dopamine and shift from implicit processing (system one) to explicit attentive processing (system two). The activity of the items from t5 is sustained, and the concepts are anchored upon giving them more processing priority so that greater focus can be brought to bear on them. By t9, 57% updating resumes.</em></p>



<p>As Figure 21 illustrates, it may be the case that the rate of iterative updating decreases during a thought but then increases during the transition between thoughts. The first diagram in Figure 21 features a larger number (4 vs. 2) of individual instances of continuity (i.e., discrete thoughts) compared to the second diagram. The transitions between thoughts could be conceptualized as intermittent noniterative updating. As a cognitive strategy, the processing found in the second diagram is probably more conducive to staying on topic, comprehending complicated scenarios, and solving complex problems. Presumably, however, animals alternate between these two strategies depending on the situation. To demonstrate a capacity for flexible thought, an AI system should have this ability, along with the abilities presented in the last six figures.</p>







<figure><img data-attachment-id="593" data-permalink="https://aithought.com/blog/image-8-2/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/image-8.png" data-orig-size="975,287" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-8" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/image-8.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/image-8.png?w=975" loading="lazy" width="975" height="287" src="https://aithoughtcom.files.wordpress.com/2023/12/image-8.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/image-8.png 975w, https://aithoughtcom.files.wordpress.com/2023/12/image-8.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/image-8.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/image-8.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 21.</strong> Intermittent Noniterative Updating Marks a Boundary Between Thoughts</p>



<p><em>In both diagrams, most of the updating occurs at a rate of 20% (1 ÷ 5). However, the first diagram shows three intermittent updates of 80% (4 ÷ 5). The second shows only one intermittent update of 80%. This comparison delineates the difference between four brief thoughts occurring in quick succession and two more prolonged thoughts. The first strategy would result in small islands of associative connections among long-term memory items. The second strategy would result in longer sequences of iterated associations and, consequently, less fragmented learning.</em></p>



<p><strong>3.4 Iterative Updating Gives Rise to Mental Continuity</strong></p>



<p>Continuity is defined as the uninterrupted and consistent operation of something over a period of time. According to this model, continuity of thought involves a process in which a set of mental representations demonstrates gradual replacement across a series of processing states (Reser, 2016). Continuous, partial updating makes each mental state a reframed version of the last. This reframing process results in an updated group of conditions, modulating rather than replacing the conceptual blend created by the previous set of coactive items. The manner in which iteration permits relevant information from the past to conjoin and assimilate with information from the present may provide the connective tissue for the continuous nature of reflective thought and phenomenal consciousness. This distinct property, portrayed in most of this article’s figures, may be necessary but insufficient for machine consciousness.</p>



<p>A few analogies may clarify the nature of iterative continuity. When it demonstrates continuity, we should expect the attentional “spotlight” to move by degrees (e.g., the panning of a video camera) rather than abruptly (e.g., the saccade of an eye). The components within the spotlight vary smoothly. It is like the carousel function used in computer graphical interfaces where a collection of visible objects is updated as individual elements of the collection rotate into and out of view. This is similar to the morphing technique used in computer animation, where an image is transformed fluidly into another by maintaining certain features but changing others in small, gradual steps. Corresponding points on the before and after images are usually anchored and incrementally transfigured from one to the other in a process called “crossfading.” It is also like the changes taking place within the set of interlocking teeth of two gears. As the gears turn and a new tooth is added to this set, a different tooth is subtracted, yet other teeth remain interdigitated. In literary terms, the subset of concepts that remain interdigitated constitutes the “through line,” connecting theme, or invisible thread that binds elements of a mental experience together. Mental continuity is an evolutionary process and, like natural selection, involves non-random retention and elimination of candidate structures leading to incremental modifications to a population.</p>



<p>In an earlier version of the present model (Reser, 2016), the subset of neurons demonstrating sustained firing over a series of states (represented by C, D, and E in Figure 13) was said to exhibit “state-spanning coactivity” (SSC). Over time, the set of coactive neurons shifts, creating “incremental change in state-spanning coactivity” (icSSC). According to that model, the content of working memory is effectively in SSC, and as it progresses over time, the content exhibits icSSC. The iterative process of icSSC may provide continuity, not only to working memory but also to other constructs, such as attention, awareness, thought, and subjective experience.</p>



<p>There are some published articles that utilize iteration in describing various psychological phenomena (e.g., Shastri et al., 1999; Howard &amp; Kahana, 2002; Hummel &amp; Holyoak, 2003; Botvinick &amp; Plaut, 2006; Kounatidou et al., 2018). However, these models are not applied to modeling continuity in brain activity or consciousness. Although modern research on these topics appears to be nonexistent (Reser, 2016), William James (1842-1910) addressed the continuous nature of consciousness in his writings. In a lecture from 1909 entitled “The Continuity of Experience,” James spoke about the “units of our immediately felt life,” describing how these units blend together to form a continuous sheet of experience:</p>



<blockquote>
<p>“It is like the log carried first by William and Henry, then by William, Henry, and John, then by Henry and John, then by John and Peter, and so on. All real units of experience overlap. Let a row of equidistant dots on a sheet of paper symbolize the concepts by which we intellectualize the world. Let a ruler long enough to cover at least three dots stand for our sensible experience. Then the conceived changes of the sensible experience can be symbolized by sliding the ruler along the line of dots. One concept after another will apply to it, one after another drop away, but it will always cover at least two of them, and no dots less than three will ever adequately cover it.”</p>



<p>William James (1909, p. 287)</p>
</blockquote>



<p>The above quote evinces that James had envisioned an iterative model of consciousness over a hundred years ago. Moreover, his minimum of three “dots” coincides with Cowan’s four (plus or minus one) items of working memory. The next section adds detail to the present account of the neural basis of the items in working memory and describes how active neurons search long-term memory for the next update.</p>



<figure><img data-attachment-id="495" data-permalink="https://aithought.com/blog/image-16/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-16.png" data-orig-size="975,433" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-16" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-16.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-16.png?w=975" loading="lazy" width="975" height="433" src="https://aithoughtcom.files.wordpress.com/2023/11/image-16.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-16.png 975w, https://aithoughtcom.files.wordpress.com/2023/11/image-16.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-16.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-16.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 22.</strong> A Representation of William James’s Sliding Ruler</p>



<p><em>This figure is meant to convey William James’s ruler analogy for the overlapping units of conscious experience. The ruler encompasses a set of dots. As the ruler slides down a line of equidistant dots, the set it contains is updated iteratively.</em></p>







<p id="part4"><span><a href="http://part4/"><strong><u>Part IV:</u></strong> Implications of the Model</a></span></p>



<p><strong>4.1 Iterative Updating Provides Structure to Associative Search</strong></p>



<p>Donald Hebb (1949) first posited that a group of cells firing simultaneously could represent a memory fragment in the mind for as long as the neurons remained active. He called these groups of coactive cells “assemblies.” Following Hebb’s lead, many neuroscientists today describe cortical architecture as essentially a network of hierarchically organized pattern-recognizing assemblies (Gurney, 2009; Meyer &amp; Damasio, 2009; Johnson-Laird, 1998; von der Malsburg, 1999). To recognize a complex entity, the network uses hierarchical pattern completion to locate and activate the group of assemblies that best represents the statistical function of the entity’s constituent features (Hawkins, 2004; Kurzweil, 2012).</p>



<p>On this groundwork and that of the preceding sections, the present model proposes that the engram for an item of working memory consists of a large set (ensemble) of cell assemblies located in multimodal cortical association areas (where cells encode complex conjunctive patterns). The ensemble is symbolic, and its assemblies, like the neurons that compose them, are subsymbolic. An ensemble of cells is not a stable, immutable group but a fuzzy set that varies every time the concept it encodes is activated (Reser, 2016). Thus, an individual item in the FoA would correspond to an ensemble, a distinct subset of the total set of assemblies active in that instant. Recent studies have suggested that items may be formed from alternative processes such as dynamic population codes or low-dimensional subspace representations (Panichello &amp; Buschman, 2021); however, we can expect even these would be updated iteratively.</p>







<figure><img data-attachment-id="595" data-permalink="https://aithought.com/blog/image-9-2/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/image-9.png" data-orig-size="975,200" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-9" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/image-9.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/image-9.png?w=975" loading="lazy" width="975" height="200" src="https://aithoughtcom.files.wordpress.com/2023/12/image-9.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/image-9.png 975w, https://aithoughtcom.files.wordpress.com/2023/12/image-9.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/image-9.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/image-9.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 23.</strong> Concepts of Interest at Different Levels of Abstraction</p>



<p><em>1. A single neuron. 2. A neural assembly (indicated by a lower-case letter) composed of many nearby (local) neurons with similar receptive fields. An assembly (possibly a cortical minicolumn) is equivalent to a subsymbolic feature. 3. A neural ensemble (indicated by an upper-case letter) composed of many nonlocal assemblies. An ensemble is an engram for an item, concept, or mental representation. 4. Four items within the focus of attention of working memory. 5. Items in the focus of attention undergoing an iterative update.</em></p>



<p>The assemblies constituting an ensemble would be densely interconnected and have strong interactions between them. They would also have the tendency to be added to (or subtracted from) working memory as a discrete group, as shown in Figure 24.</p>







<figure><img data-attachment-id="63" data-permalink="https://aithought.com/11-coactive-neural-assemblies-and-ensembles-in-working-memory-and-the-focus-of-attention-1/" data-orig-file="https://aithoughtcom.files.wordpress.com/2022/03/11.-coactive-neural-assemblies-and-ensembles-in-working-memory-and-the-focus-of-attention-1.jpg" data-orig-size="4002,1743" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="11.-coactive-neural-assemblies-and-ensembles-in-working-memory-and-the-focus-of-attention-1" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2022/03/11.-coactive-neural-assemblies-and-ensembles-in-working-memory-and-the-focus-of-attention-1.jpg?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2022/03/11.-coactive-neural-assemblies-and-ensembles-in-working-memory-and-the-focus-of-attention-1.jpg?w=1024" loading="lazy" width="1024" height="445" src="https://aithoughtcom.files.wordpress.com/2022/03/11.-coactive-neural-assemblies-and-ensembles-in-working-memory-and-the-focus-of-attention-1.jpg?w=1024" alt="" srcset="https://aithoughtcom.files.wordpress.com/2022/03/11.-coactive-neural-assemblies-and-ensembles-in-working-memory-and-the-focus-of-attention-1.jpg?w=1022 1022w, https://aithoughtcom.files.wordpress.com/2022/03/11.-coactive-neural-assemblies-and-ensembles-in-working-memory-and-the-focus-of-attention-1.jpg?w=2043 2043w, https://aithoughtcom.files.wordpress.com/2022/03/11.-coactive-neural-assemblies-and-ensembles-in-working-memory-and-the-focus-of-attention-1.jpg?w=150 150w, https://aithoughtcom.files.wordpress.com/2022/03/11.-coactive-neural-assemblies-and-ensembles-in-working-memory-and-the-focus-of-attention-1.jpg?w=300 300w, https://aithoughtcom.files.wordpress.com/2022/03/11.-coactive-neural-assemblies-and-ensembles-in-working-memory-and-the-focus-of-attention-1.jpg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><strong>Fig. 24.</strong> Two Successive Instances of Coactive Assemblies in the FoA</p>



<p><em>The engrams for items B, C, D, E, and F are each composed of many assemblies of neurons active in association areas, represented by lowercase letters b, c, d, e, and f, respectively. At time 1, assemblies b, c, d, and e are active. At time 2, the assemblies for b have deactivated, while those for f have become active. Thus, time 2 is an iterated update of time 1.</em></p>



<p>The primate neocortex can hold a number of contextually related items coactive for several seconds at a time. This model proposes that these items are used to perform a global search function by spreading the combined electrochemical activation energy of their neural assemblies throughout the thalamocortical network. This activation energy converges on and activates inactive items in long-term memory that are highly associated with the current state of activity. This is similar to the case where being exposed to the words “course,” “current,” “wet,” and “bank” might result in the involuntary activation of the brain’s representation for the word “river.” Hence, this model views each instantaneous state of active items in working memory as both a solution to the previous state’s search and a set of parameters for the next search.</p>



<p>This description of search is compatible with spreading activation theory. According to that theory, the capacity for search in associative networks is derived from activation energy (in the form of action potentials) produced by active neural assemblies (Anderson, 1983). Some of this energy is excitatory, and some is inhibitory. Activation energy from active assemblies spreads in parallel to inactive assemblies that are structurally connected to (i.e., associated with) the active ones due to a history of Hebbian plasticity (Collins &amp; Loftus, 1975).</p>



<p>This activation energy propagates among assemblies through axons and dendrites and follows the weighted links of synapses. Ultimately, multiple alternative pathways originating from active assemblies converge on several of the same inactive items in long-term memory. The number of items converged on may be exceptionally large; however, not all of these can enter the FoA. The item(s) receiving the most excitatory energy is activated, becoming an iterative update to the FoA. We should expect this to be the concept most strongly psychologically associated with the items that converged upon it.</p>







<figure><img data-attachment-id="502" data-permalink="https://aithought.com/blog/image-18/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-18.png" data-orig-size="975,678" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-18" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-18.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-18.png?w=975" loading="lazy" width="975" height="678" src="https://aithoughtcom.files.wordpress.com/2023/11/image-18.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-18.png 975w, https://aithoughtcom.files.wordpress.com/2023/11/image-18.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-18.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-18.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 25. </strong>A Semantic Web Showing Associative Connections Between Concepts</p>



<p><em>Seventeen concepts (ensembles) are represented by circular nodes in a long-term memory network. The existence of an associative connection is indicated by a line, and its strength by the thickness of the line. The four items “red,” “truck,” “siren,” and “fire” are currently coactive in this network, and thus spreading activation may select “firefighters” as the next iterative update.</em></p>



<p>Studies of semantic priming show that either conscious or subliminal exposure to a brief stimulus can temporarily increase the implicit availability of many associated concepts within long-term memory (Bargh &amp; Chartrand, 2000). For instance, in a lexical decision task, merely priming the word “water” will speed up the recognition of various related words such as “fluid,” “splash,” “liquid,” and “drink” (Schvaneveldt &amp; Meyer, 1973). The standard interpretation of these findings is that activating the engram for “water” unconsciously spreads to the engrams for many semantically related words. This activation is rapid, automatic, irrepressive and is theorized to be due to spreading activation in associative networks (Reisberg, 2010).</p>



<p>It may be reasonable to assume that updating working memory with a new item has a similar priming effect on spreading activation. This new item, added to the residual items, acts as an additional semantic retrieval cue, uniquely altering the field of items receiving activation. By assuming that updates to working memory are selected by its current contents, one can explain why new associations are marked by high contextual relevance and specificity. Combining this assessment with the earlier claims regarding iteration results in a system suited for producing a parade of complementary impressions, views, notions, and ideas.</p>



<p><strong>4.2 Multiassociative Search Spreads the Combined Activation Energy of Multiple Items</strong></p>



<p>“Associationism” is a longstanding philosophical position advocating that mental states determine their successor states by psychological associations between their contents. According to associationism, the sequence of ideas a person produces is essentially a matter of the preexisting links between stored associative memories (Shanks, 2010). William James believed one thought could induce another through a logical, correlative connection (1890). The face validity of associationism stems from the commonplace notion that one thought “suggests” the next.</p>



<p>In his discussion of “the succession of memories,” Plato identifies three principles of association: similarity (resemblance), contiguity (in time and place), and contrast (difference). Numerous other principles capable of linking mental states were added to this list by the nineteenth century, including simultaneity, affinity, reinstatement of the remainder, cause and effect, reason and consequence, means and end, and premise and conclusion (Hamilton, 1860). When any of these forms of association occur, they may simply involve an iterative update, selected by spreading activation, to join a global workspace of persistent items. Spreading activity operating in this way may help us reconcile incongruous items by locating the most compatible update. For example, it may help us find solutions to problems such as the trivia prompt, “The name of a planet, an element, and a Roman god,” where each of the clues contribute independently to unconscious neural convergence onto the ensemble representing the construct of “mercury” (Reser, 2016).</p>



<p>The associationism school of thought primarily focused on a single logical associative relationship between one thought and another. This may provide only a limited explanation. The model presented here can be read as a version of associationism that escapes this limitation by assuming that all the neurons currently involved in working memory search cooperatively and probabilistically for the succeeding association. Thus, contiguous states are not only interrelated but are also interdependent. This cooperative search function may occur regardless of when the neurons started firing and irrespective of the item to which they belong.</p>



<p>Reser (2016) proposed that the selection of new items to be added to working memory might derive from the pooling of assembly activity in the cortical workspace. This unconscious, autonomous process, termed “multiassociative search,” here operates as follows: As excitatory and inhibitory activation energy from assemblies representing the items currently in working memory spreads,</p>



<p><strong>(1)</strong> items that continue to receive sufficient activation energy remain active,</p>



<p><strong>(2)</strong> items that receive sufficiently reduced activation energy lose activity, and</p>



<p><strong>(3)</strong> inactive items that receive sufficient activation energy become active.</p>



<p>The item(s) receiving sufficient activation energy (through spatial and temporal summation) from both the present constellation of coactive assemblies (FoA) and potentiated synapses (STM) may be recalled autoassociatively (i.e., an active subset of the item’s assemblies is sufficient to activate the rest of the item). This nonlinear, stochastic process should be taken to be responsible not only for finding and activating the next item(s) (Fig. 26) but also for determining the percentage of items updated in the FoA (Figs. 18, 19, and 20).</p>







<figure><img data-attachment-id="598" data-permalink="https://aithought.com/blog/image-10-2/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/image-10.png" data-orig-size="975,555" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-10" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/image-10.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/image-10.png?w=975" loading="lazy" width="975" height="555" src="https://aithoughtcom.files.wordpress.com/2023/12/image-10.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/image-10.png 975w, https://aithoughtcom.files.wordpress.com/2023/12/image-10.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/image-10.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/image-10.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 26.</strong> A Schematic for Multiassociative Search</p>



<p><em>Spreading activity from each of the assemblies (lowercase letters) of the four items (uppercase letters) in the FoA (B, C, D, and E) propagates throughout the cortex (represented by the field of assemblies above the items). This activates new assemblies that will constitute the newest item (F), which will be added to the FoA in the next state. The assemblies that constitute items B, C, D, and E are each individually associated with a very large number of potential items, but as a unique group, they are most closely associated with item F.</em></p>



<p>This procedure is executed unceasingly during waking consciousness. It takes sets of neural assemblies that have never been coactive before and uses their collective spreading activation to select the most applicable iterative update. At every moment, the set of assemblies in coactivity is unprecedented. However, the set of items in coactivity may not be. When the set of coactive items has been coactive at some point in the past, the spreading activity either converges on the same item that was selected the last time (recall) or it may converge on an altogether different item (novel inference). Regardless of which way this occurs, the process transforms the latent information inherent in the original set into new, manifest information by forcing it to interact with inert long-term memory. Each set of coactive items and the links between them can potentially be recorded to memory. Thus, every search creates new associative learning in the network, improving its future behavior and model of reality. Thus, multiassociative searching gives rise to multiassociative learning.</p>



<p>These concepts explain how long-term (non-hippocampal) semantic memory might be updated. New memories don’t replace old ones; rather, they retune the connectional strengths between groups of items. For instance, in Figure 26, the associative relationship between F and B is strengthened, but mostly in the presence of items C, D, and E. As items demonstrate coactivity within working memory, we should expect their assemblies to exhibit a Hebbian propensity to wire together, forming statistical codependencies that support the learning process. Reoccurring examples of coactivity would lead to the formation of heavily encoded associations (Asok et al., 2019), which would persist as procedural and semantic knowledge.</p>



<p>Undoubtedly, many canonical information processing algorithms not mentioned here (see, e.g., Miller et al., 2018; Sreenivasan, 2019) also contribute to this search and play causal roles in this process. However, it may be parsimonious to assume that the subsymbolic components of the symbolic items of working memory work synergistically and in parallel to search for the updates to working memory in the way described. In other words, the production sequence of thought is not determined by semantic dependencies between symbols (e.g., rules, utilities, predicates, conditionals, functions, etc.) as in other cognitive architectures (e.g., ACT-R, Soar, Sigma, etc.). Instead, it is determined by syntactic dependencies among subsymbols. These dependencies may reconcile with declarative, symbolic knowledge at the psychological level. Nonetheless, they operate unconsciously below it. In other words, the outcomes of these “blind” statistical searches only appear rational because they are based on a history of structured learning from orderly environmental patterns.</p>



<p>Note that, in the present model, the cortical assemblies constituting items currently in the FoA are not the only contributors to selecting the next item(s). Rather, all firing neurons that participate in the spreading of activation in the cortical workspace contribute definitions to this global search. Potentiated neurons in the short-term store—as well as active neurons in sensory and motor cortex (semantic), hippocampus (episodic), basal ganglia (procedural), and other cortically connected subcortical neurons—all contribute to multiassociative search. Figure 27 depicts this situation, in which a working memory store characterized by iterative updating selects its updates using spreading activation generated by several different neuroanatomical systems. The next section will discuss how iterative updating and multiassociative search work together to formulate not only associations but also predictions.</p>







<figure><img data-attachment-id="599" data-permalink="https://aithought.com/blog/image-11-2/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/image-11.png" data-orig-size="975,477" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-11" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/image-11.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/image-11.png?w=975" loading="lazy" width="975" height="477" src="https://aithoughtcom.files.wordpress.com/2023/12/image-11.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/image-11.png 975w, https://aithoughtcom.files.wordpress.com/2023/12/image-11.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/image-11.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/image-11.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 27.</strong> A Single Cycle of the Iterative Updating / Multiassociative Search Procedure</p>



<p><em>The FoA, the short-term store, as well as active neurons in the hippocampus, basal ganglia, sensory and motor cortices all contribute to the spreading activation that will select the next item(s) to be added to working memory. At time 1, two (K and L) of a potential five items are converged on, and these update the FoA in time 2.</em></p>



<p><strong>4.3 States Updated by the Products of Search Are Predictions</strong></p>



<p>The meaning of an event is determined by the events that came before it and by those that will come after it. Claude Shannon, the founder of information theory, knew this and was interested in predicting events based on their foregoing context. He introduced a hypothetical situation in which a person is tasked with guessing a randomly selected letter from a book (Shannon, 1951). Because there is no contextual information available, any response would be highly uninformed and made by chance. However, if this person is given the letter that comes before the unknown letter, a more informed guess can be made. The more previous letters known, the better the guess (Stone, 2015). For instance, if you knew the sequence of letters that precede an unknown letter was “t,” “h,” “i,” and “n,” then you would know that there is a high probability that the next letter is either “k” or “g.”</p>



<p>As with letters in a word or words in a sentence, events occurring along a timeline in a natural environment are not independent or equiprobable. Rather, there are correlations and conditional dependencies between successive events. Knowledge of conditional dependencies allows us to predict what other people will do next and, sometimes, to even finish their sentences for them. Working memory enables the mind to capture and record long-term dependencies. This, in turn, permits animals to treat separate events as causally related variables that can be used to predict future events. By capturing the statistical structure of a sequence of recent occurrences (including rewards and punishments), working memory provides animals with a way to form an autoregressive interpretation of an unfolding scenario, forming associative expectations of it and responses to it.</p>



<p>The interaction between iterative updating and multiassociative search may form the basis for prediction in the brain. Consider the case in which four environmental stimuli present themselves in quick succession. This could involve any sequence of events, such as that involved in finding food. If each stimulus is attended to and persistently activated, the items representing these stimuli and their closest associations will have the chance to comingle in the FoA. Their coactivity may cause them to become associated by activity-dependent learning even though they never actually occurred simultaneously in the environment (trace conditioning). If this sequence of four stimuli is repeated frequently (as would be expected if there were conditional dependencies between them found in nature), then they will come to be strongly associated. The next time the first three stimuli appear, their very activity may be sufficient to search for and recruit the item representing the fourth stimulus from long-term memory. Consequently, the activation of this fourth item would be a prediction. Therefore, internally generated, self-directed thought can be conceptualized as an iterative procession of concatenated, associative predictions, each predicated on the prediction before it.</p>



<p>In Figure 28, Diagram 1 depicts a situation in which stimulus 1 (S1) is followed by stimulus 3 (S3) and results in the selection of response 1 (R1). This can be contrasted with Diagram 2, where S3 is preceded by a different stimulus (S2), and a completely different response is selected (R2). The persistent activity of the first stimulus influences the interpretation of S3, biasing the response accordingly. That is to say, the response to S3 is conditionally dependent on the stimulus that precedes it. Diagrams 1 and 2 have been adapted from a highly popular model of PFC function (Miller &amp; Cohen, 2001). Diagrams 3 and 4 take this idea further, relating it to the present model, communicating that when the first two stimuli are the same (S1 and S2) but the subsequent stimulus differs, the responses may also differ. These diagrams underscore the hypothesis that behavior is not merely directed by the differential selection of existing neural pathways underlying stimulus-response pairings (i.e., Miller &amp; Cohen, 2001) but rather by a series of multiassociative searches that utilize curated sets of memoranda to converge on the best response at each time step.</p>







<figure><img data-attachment-id="533" data-permalink="https://aithought.com/blog/fig-26/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/fig-26.jpg" data-orig-size="3725,1693" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig-26" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/fig-26.jpg?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/fig-26.jpg?w=1024" loading="lazy" width="1024" height="465" src="https://aithoughtcom.files.wordpress.com/2023/11/fig-26.jpg?w=1024" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/fig-26.jpg?w=1024 1024w, https://aithoughtcom.files.wordpress.com/2023/11/fig-26.jpg?w=2046 2046w, https://aithoughtcom.files.wordpress.com/2023/11/fig-26.jpg?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/fig-26.jpg?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/fig-26.jpg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><strong>Fig. 28.</strong> Conditional Dependencies Between Consecutive Events</p>



<p><em>Each arc represents the span of time since an event occurred. S represents stimuli, R represents responses, and other capital letters represent items. To provide an illustrative example, let us suppose the variables named above correspond to the following events: S1 = friend, S2 = enemy, S3 = approach, S4 = depart, R1 = act friendly, R2 = act aggressive, R3 = wait, R4 = follow, A = foraging alone, B = feel hungry, C = find berries, D = not poisonous, Z = poisonous, Y = friend approaching, R5 = eat, R6 = don’t eat, R7 = share berries, R8 = eat berries before friend arrives.</em></p>



<p>Diagrams 5 through 8 communicate that the full complement of items in working memory can be expected to show a pattern like that seen with the stimuli in Diagrams 1 through 4: each item affects the interpretation of the items after it and uniquely biases the search for a response to them. Accordingly, the arrows below Diagrams 5 through 8 indicate that, at each time step, the preceding items provide a frame of reference through which subsequent items are interpreted. Note that even though the responses in Diagrams 7 and 8 are reacting to the same four representations (B, C, D, and Y), they react to them differently because the order of items contextualizes the scenario differently. For instance, item C has a different meaning (dependency) when it follows Y versus when it precedes Y. Therefore, it elicits a different predictive response in Diagram 7 relative to Diagram 8.</p>



<p>Consider a situation in which a person writes with a pencil, and the lead breaks. This may cause the long-term memory representations for “writing,” “pencil,” “lead,” and “broken” to become active in the FoA. This combination of coactive items (conditioned from years of writing with a pencil in school) might result in the automatic spreading of activation to the representation for a “sharpener.” During another round of updating, the representation for “writing” may exit the FoA and be replaced by the pencil sharpener’s location, such as “desk drawer.” In this way, sets of coactive items can prompt others in advancing sequences capable of producing not only predictions but also adaptive behavior. Especially when the items being sustained are task-relevant, this kind of iterative system should be capable of incremental advancement toward a goal.</p>



<p><strong>4.4 Iterative Updating Allows Progressive Changes to the Contents of Thought</strong></p>



<p>In addition to accounting for the serial, cyclic, continuous, narrative, and predictive functions of thought processes, iterative updating may be a fundamental feature of reasoning. This section will provide brief explanations for why this might be the case.</p>



<p>According to the present model, iterative updating produces sequences of interdependent states in which each state is capable of representing the current status of a problem-solving procedure and updating it with a prediction. When this associative prediction is informed by meaningful causal dependencies learned from related experiences from the environment, it sets the cycle on a logical course. This makes it possible for a starting state to generate a chain of intermediate states that make progress toward a terminal solution state.</p>



<p>Iterative updating allows working memory to link a series of rapid, automatic associations so that they can furnish a foundation for each other, resulting in the assembly of complex content. This occurs when a series of linked searches culminates in a higher-order result that any single search on its own could not otherwise attain. A prolonged stretch of tightly recursive searches (where a large proportion of items are retained throughout several states, as in Figure 19, Diagram 2) may be slower and more error-prone but can address problems too unfamiliar or complicated to be solved by less iterative, implicit processing.</p>



<p>Generally speaking, short bouts of iteration engage crystallized intelligence and easy-to-reach network states, whereas instances of prolonged iteration use fluid intelligence and highly processed, difficult-to-reach states. Such highly elaborated states are comprised of select subsets of previous states from various points in the recent past. This corresponds to simple thoughts building constructively “on top of” each other to form complex thoughts. William James used the term “compounding” to describe this concept:</p>



<blockquote>
<p>“…complex mental states are resultants of the self-compounding of simpler ones…. in the absence of souls, selves, or other principles of unity, primordial units of mind-stuff or mind-dust were represented as summing themselves together in successive stages of compounding and re-compounding, and thus engendering our higher and more complex states of mind.”</p>



<p>William James (1909, p. 185)</p>
</blockquote>



<p>Iterative updating may employ this compounding feature during logical or relational reasoning. The item or items that update the FoA create a context to be compared, contrasted, integrated, or otherwise reconciled with the context remaining from the previous state. This may be the same kind of reconciliation that occurs in formal logic. For instance, propositional logic combines simple statements using logical operators (subjects and predicates) and connectives (e.g., and, or, not, if, then, because, etc.) to produce complex rational statements. The operators of such a statement could be instantiated by items. This group of coactive items could imply a true statement or premise that, when updated in the next state, could invoke a related premise or lead to a conclusion. By creating strings of substantiated inferences in this way, multiassociative search could permit the construction of a logical case or argument, form new boundaries and affinities between groups of items, and build expectations about events that have never been encountered.</p>



<p>The compounding feature of iteration may also enable working memory to implement algorithms for use in reasoning and problem solving. All complex learned behaviors have algorithmic steps that must be executed in a specific sequence to reach completion (Botvinick, 2008). Activities such as hunting, tying shoes, and performing long division involve following an algorithm. Successive states of working memory could correspond to successive steps in an iterable process.</p>



<p>Iterative updating could be instrumental in implementing learned algorithms because virtually every step of an algorithm relates to the preceding and subsequent steps in some way. A new update could correspond to a behavior or mental operation required in the next step of the sequence of actions that need to be taken. The update could amount to a response, memory, or heuristic or provide top-down influence to a perception. Thus, multiassociative search converges on the most appropriate fragment of knowledge at each state of solving a routine or non-routine problem. An item inhibited or allowed to subside could correspond to an operation that has already been executed or is no longer needed.</p>



<p>Once the associations relevant to an algorithm have been learned and trained, multiassociative search can recruit the items necessary for each next step (Reser, 2016). For instance, performing long division by rote requires many trials, and proficiency may only be reached when the active items in each state have been trained to converge on the items necessary to perform the operation in the next step. After the first digit of the dividend is divided by the divisor, the prevailing state of working memory automatically activates the items necessary to take the whole number result and write it above the dividend. Cognitive algorithms may be constructed in this manner during learning, as trial, error, and repetition link recursive chains of states capable of assembling functional behaviors. At its core, this is a form of optimization that may use operant conditioning to provide feedback for incremental guess refinement.</p>



<p>Iterative updating could also conceivably play a role in the generation of schemas and mental models. Mental models are internal representations of external systems and the relationships between their parts (Cheng &amp; Holyoak, 2008). Iteration may afford the incremental modification of a model from its previous state, allowing capacity-limited, static models to be elaborated dynamically. Even dynamic systems can be modeled when their enduring features are held constant by persistent activity and the changing features are updated correspondingly. This enables tweaking the search parameters of interest to vary the simulation in stages, producing a systematic effort to investigate a structured problem space. Solving a complex multistep problem requires the FoA and short-term store to cooperate. For instance, one line of thought developing in the FoA may be temporarily suspended in the short-term store so that the FoA can be made available to solve a related subproblem. The FoA would iterate multiassociatively, progressing toward the solution to this subproblem. When the subsolution is reached, it could then be merged with the pending problem to create a hybrid solution state. This interleaving and eventual merger of states of progress would facilitate the decomposition of a problem that is too computationally taxing to be processed by the FoA alone (Fig. 29).</p>







<figure><img data-attachment-id="508" data-permalink="https://aithought.com/blog/image-20/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-20.png" data-orig-size="975,493" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-20" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-20.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-20.png?w=975" loading="lazy" width="975" height="493" src="https://aithoughtcom.files.wordpress.com/2023/11/image-20.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-20.png 975w, https://aithoughtcom.files.wordpress.com/2023/11/image-20.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-20.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-20.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 29.</strong> Merging Subproblems in Working Memory</p>



<p><em>An original problem is activated (M, N, O, P), and iterative updating is used to reach a subsolution (O, P, Q, R). This subsolution is saved in the short-term store, and a related subproblem (T, U, V, W) is introduced into the FoA. This subproblem iterates until a second subsolution is generated (U, V, W, X). Relevant items from the first subsolution (Q and R) are combined with those from the second subsolution (W and X) and iterated to generate a final solution (R, X, Y, Z). This pattern could be a fundamental aspect of human reasoning and creativity.</em></p>



<p>According to this interpretation, the short-term store holds the agent’s present objective, and the FoA produces lines of reasoning that interrogate that objective. These lines of reasoning update the objective, bringing it closer to resolution. This allows the agent to keep a present opportunity or threat in mind while considering possible responses before acting. In effect, previous threads of FoA sequences can be suspended in STM (or LTM) as interim results. These can then be retrieved rapidly if spreading activity reconverges on them. This permits working memory to deviate from its default behavior described thus far and employ a form of backward reference and conditional branching.</p>



<p>We have seen how continuity of thought can be established and then broken. However, Figure 29 demonstrates that instances of continuity can be reestablished, such as when one revisits a thought from the past. Other common thought patterns exemplifying related mental phenomena are illustrated in Figures 30-37.</p>







<figure><img data-attachment-id="510" data-permalink="https://aithought.com/blog/image-21/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-21.png" data-orig-size="944,506" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-21" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-21.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-21.png?w=944" loading="lazy" width="944" height="506" src="https://aithoughtcom.files.wordpress.com/2023/11/image-21.png?w=944" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-21.png 944w, https://aithoughtcom.files.wordpress.com/2023/11/image-21.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-21.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-21.png?w=768 768w" sizes="(max-width: 944px) 100vw, 944px"></figure>



<p><strong>Fig. 30.</strong> Reiterating Through an Earlier Sequence</p>



<p><em>A set of six items held in working memory is iteratively updated over the next three time steps, creating a self-contained thought. Starting at t5, attention shifts completely as an unrelated thought takes place using an entirely different set of items. From t9, the first sequence is reiterated as before. This might happen when someone revisits an earlier thought, such as rehashing a plan of action, retracing a set of previous steps, or retelling a story.</em></p>







<figure><img data-attachment-id="511" data-permalink="https://aithought.com/blog/image-22/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-22.png" data-orig-size="941,536" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-22" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-22.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-22.png?w=941" loading="lazy" width="941" height="536" src="https://aithoughtcom.files.wordpress.com/2023/11/image-22.png?w=941" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-22.png 941w, https://aithoughtcom.files.wordpress.com/2023/11/image-22.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-22.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-22.png?w=768 768w" sizes="(max-width: 941px) 100vw, 941px"></figure>



<p><strong>Fig. 31.</strong> Revisiting the Endpoint of an Earlier Iterative Sequence and Continuing It</p>



<p><em>Six items are modified over the first three time steps, creating a line of thought composed of four related states. Attention shifts completely at t5, and an unrelated thought occurs. Starting at t9, attention shifts back to the items from t4, which are iterated without using any of the items from t5 through t8. This might happen when someone picks up a thought where it left off and continues to think about the issues from the last point at which they were considered.</em></p>







<figure><img data-attachment-id="512" data-permalink="https://aithought.com/blog/image-23/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-23.png" data-orig-size="937,535" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-23" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-23.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-23.png?w=937" loading="lazy" width="937" height="535" src="https://aithoughtcom.files.wordpress.com/2023/11/image-23.png?w=937" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-23.png 937w, https://aithoughtcom.files.wordpress.com/2023/11/image-23.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-23.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-23.png?w=768 768w" sizes="(max-width: 937px) 100vw, 937px"></figure>



<p><strong>Fig. 32.</strong> Revisiting the Midpoint of an Earlier Iterative Sequence and Altering It</p>



<p><em>Six items are modified over seven time steps, creating a thought composed of eight related states. At t9, attention shifts back to a point in the middle of this sequence. This set or subproblem from t4 is then iterated without including any of the items that were introduced from t5 through t8. This creates an alternate branch and a “forking” of the iterative sequence. This might happen when someone decides to assume an intermediate step in a previous problem-solving sequence and solve the problem in a different way.</em></p>







<figure><img data-attachment-id="513" data-permalink="https://aithought.com/blog/image-24/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-24.png" data-orig-size="975,516" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-24" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-24.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-24.png?w=975" loading="lazy" width="975" height="516" src="https://aithoughtcom.files.wordpress.com/2023/11/image-24.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-24.png 975w, https://aithoughtcom.files.wordpress.com/2023/11/image-24.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-24.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-24.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 33.</strong> Multitasking Occurs when Two Independent Sequences Alternate</p>



<p><em>Two distinct threads of thought are iterated but never combined, alternating every third time step. This context switching could occur when someone is working on two separate tasks or problems simultaneously.</em></p>







<figure><img data-attachment-id="232" data-permalink="https://aithought.com/blog/eelaboration-1/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/02/eelaboration-1.png" data-orig-size="913,524" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="eelaboration-1" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/02/eelaboration-1.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/02/eelaboration-1.png?w=913" loading="lazy" width="913" height="524" src="https://aithoughtcom.files.wordpress.com/2023/02/eelaboration-1.png?w=913" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/02/eelaboration-1.png 913w, https://aithoughtcom.files.wordpress.com/2023/02/eelaboration-1.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/02/eelaboration-1.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/02/eelaboration-1.png?w=768 768w" sizes="(max-width: 913px) 100vw, 913px"></figure>



<p><strong>Fig. 34.</strong> Elaborating on a Stable Subset of Items</p>



<p><em>Three items are held constant as iteration elaborates on their statistical relationships with related concepts. This process would strengthen the connection between these first three items and explore how they are associated within different contexts.</em></p>







<figure><img data-attachment-id="233" data-permalink="https://aithought.com/blog/erevisit/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/02/erevisit.png" data-orig-size="913,521" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="erevisit" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/02/erevisit.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/02/erevisit.png?w=913" loading="lazy" width="913" height="521" src="https://aithoughtcom.files.wordpress.com/2023/02/erevisit.png?w=913" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/02/erevisit.png 913w, https://aithoughtcom.files.wordpress.com/2023/02/erevisit.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/02/erevisit.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/02/erevisit.png?w=768 768w" sizes="(max-width: 913px) 100vw, 913px"></figure>



<p><strong>Fig. 35.</strong> Progressing Backward to a Subset of an Earlier State of Coactive Items</p>



<p><em>Some lines of thought, by the end, return to the beginning. Here, the first set of coactive items is revisited and re-related to the outcome of the iterative sequence. This circularity could occur when one reconciles a predicted outcome with the original premise. This is probably a common thought pattern and can be contrasted with the previous figure, where the first three items never leave coactivity.</em></p>







<figure><img data-attachment-id="286" data-permalink="https://aithought.com/blog/image-1-2/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/03/image-1.png" data-orig-size="975,557" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-1" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/03/image-1.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/03/image-1.png?w=975" loading="lazy" width="975" height="557" src="https://aithoughtcom.files.wordpress.com/2023/03/image-1.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/03/image-1.png 975w, https://aithoughtcom.files.wordpress.com/2023/03/image-1.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/03/image-1.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/03/image-1.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 36.</strong> Linking the Beginning of an Iterative Sequence with the End Makes the Intermediate Steps Implicit</p>



<p><em>Six items are modified over nine time steps, creating a line of thought composed of ten related states. After t10, many states pass as indicated by the ellipsis. Then, later, the original six items reenter working memory. Because of Hebbian learning, these items recruit the same six-item solution reached previously without having to reiterate it. The way probability is modeled has changed due to the earlier iterative work, and multiassociative search is now capable of recruiting the final solution immediately. This is much more likely to happen when iteration involving the first six and last six items occurs, further entrenching their association. This may happen when one reflects on how their solution reconciles with the original problem state (Fig. 35).</em></p>



<p>Another commonplace pattern found in the updating of working memory may occur when an existing problem-solving process reaches an impasse. The newest addition to working memory is sometimes unhelpful or not task-relevant (e.g., because of prepotent associations formed during a similar but irrelevant task). In this case, it may be inhibited. The same items that recruited it would continue to spread activation energy without being able to reactivate it. This would force them to activate the next most pertinent item. Multiple rounds of “iterative inhibition” may be required before an appropriate item can be identified (Fig. 37). Each time a potential coactivate is vetted for exclusion, the search tree is restricted further. This situation might arise as one deliberates over different methods of completing the same task (e.g., “I should fax this letter. No, I should email it. No, I will text it instead”).</p>







<figure><img data-attachment-id="514" data-permalink="https://aithought.com/blog/image-25/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-25.png" data-orig-size="973,548" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-25" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-25.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-25.png?w=973" loading="lazy" width="973" height="548" src="https://aithoughtcom.files.wordpress.com/2023/11/image-25.png?w=973" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-25.png 973w, https://aithoughtcom.files.wordpress.com/2023/11/image-25.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-25.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-25.png?w=768 768w" sizes="(max-width: 973px) 100vw, 973px"></figure>



<p><strong>Fig. 37.</strong> Iterative Inhibition Excludes New Items in a Search for Something More Relevant</p>



<p><em>An original problem is activated in time 1 (B, C, D), and the spreading activity activates a new item at time 2 (E). Executive processes determine that E is not a suitable behavioral parameter and E is inhibited. With E unavailable, B, C, and D continue to spread activation energy that converges on F at time 2. The same iterative inhibition occurs with F at time 4. G is then activated, and iterative updating continues.&nbsp;&nbsp;</em></p>



<figure><img data-attachment-id="600" data-permalink="https://aithought.com/blog/image-12-2/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/image-12.png" data-orig-size="975,600" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-12" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/image-12.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/image-12.png?w=975" loading="lazy" width="975" height="600" src="https://aithoughtcom.files.wordpress.com/2023/12/image-12.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/image-12.png 975w, https://aithoughtcom.files.wordpress.com/2023/12/image-12.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/image-12.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/image-12.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 38</strong>. Reconciling Disparate Situations with the Same Concept</p>



<p><em>Two seemingly different situations from t1 and t8 iterate with the same concept (seen twice, at t7 and t14). The items at t1 and t8 independently converge toward the idea of t7/t14 as they are reconciled with its fundamental attributes. Perhaps both situations can be explained or caused by the same underlying phenomena and thus are funneled toward this specific region in the conceptual landscape.</em></p>



<p>As the figures in this section suggest, iterative updating will tend to converge toward certain stable sets of items. These are attractor states that amount to beliefs, possibly to profound truths about reality. Iteration then reconciles these truths with other truths. All thinking is a narrowing down of combinations of items approaching reliable statements that can be generalized across situations. The present article itself is doing something very similar by attempting to reconcile the concept of iteration with numerous other related concepts. This section has considered how iteration of working memory content can create progress in information processing. The following section will consider how the model in general can be tested experimentally.</p>



<p><strong>4.5 Testing the Neurophysiological Validity of the Model</strong></p>



<p>Future work can use this framework to search for the neural signature of iteration within the brain (see Figs. 11, 12, 13 and 21). As shown in Figure 39, this search could utilize simultaneous recordings (electrodes inserted into live cortical tissue) to produce time-series analyses of incremental change in populations of coactive cortical neurons.</p>



<figure><img data-attachment-id="515" data-permalink="https://aithought.com/blog/image-26/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-26.png" data-orig-size="974,643" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-26" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-26.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-26.png?w=974" loading="lazy" width="974" height="643" src="https://aithoughtcom.files.wordpress.com/2023/11/image-26.png?w=974" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-26.png 974w, https://aithoughtcom.files.wordpress.com/2023/11/image-26.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-26.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-26.png?w=768 768w" sizes="(max-width: 974px) 100vw, 974px"></figure>



<p><strong>Figure 39.</strong> A Hypothetical Example of How Iterative Updating Could Be Found Using Electrodes</p>



<p><em>Single-cell recording from a large number of cells in association cortex could produce an activity profile exhibiting iterative updating. In this simplified figure, the x-axis represents time in seconds while the y-axis contains the recorded activity of 30 individual neurons, each remaining active for four seconds. Five neurons become active each second. Each group of five neurons that begin and end their period of activity at the same time is assumed to belong to an individual ensemble, or item, of working memory. Brackets at the bottom of the figure indicate the item to which each group of neurons belongs. This profile coincides precisely with the pattern introduced in Figure 11. Searching new and existing data for this kind of iterative pattern could provide strong support for the present model.</em></p>



<p>It is unclear whether it is possible to derive conclusive support for the present model using existing neuroimaging technology. Basic fMRI recording reveals the degree to which specialized brain modules exhibit involvement during a specific task but does not reveal the identity of the items or concepts involved. However, advanced recording techniques can demonstrate the onset and duration of brain responses to prepared stimuli, which could result in data similar to that in Figure 39. It should be possible to use the gain in temporal and spatial resolution to observe how the pattern of working memory activation changes over time. To that end, factorial designs that allow for the measurement of the BOLD signal for each volumetric cell should be able to test for differential activation in response to partial, as opposed to complete, updating of working memory.</p>



<p>Substantiating findings could be derived from neuroimaging experiments in which brain activity is recorded while participants complete a task that requires an algorithmic sequence of steps (e.g., long division). Each step of the task would need to be modeled separately. As the participant moves from one step to the next, the BOLD activity would be estimated for that particular step. A mixed model of a duration regressor covering the entire span of the problem along with individual regressors for each step would be needed to capture both the sustained attention required to solve the problem and the individual steps needed to progress from one stage to the next. It would be necessary to show that the sequence of mental representations posited as necessary to complete the task has a one-to-one correspondence with the time course of underlying cellular or hemodynamic changes. This may necessitate using multiple methods simultaneously, such as fMRI and EEG together or multivoxel pattern analysis, which has been used to resolve the addition and subtraction of individual cognitive items from working memory (Lewis-Peacock et al., 2012).</p>



<p>To validate the hypotheses put forth by the present model, it would be necessary to show that the activity in association areas underlying working memory contents can be partially rather than completely updated. Next, the goal would be to show that this partial updating happens constantly. Future studies should be able to resolve whether the iterative updating of cortical activity is continuous (at the level of neurons) or incremental, where entire items (and all their comprising neurons) are added or subtracted at once (Fig. 24). The line of reasoning suggested by this article predicts that the former may be true of the short-term store (i.e., Figure 12) while the latter may be true of the FoA (i.e., Figure 13).</p>



<p>Modern cognitive neuroscience is limited in its ability to match the components of brain states to the components of mental states. However, matching the iterative updating of ensembles to that of their corresponding items may provide a means to do so. The markers of iterative updating may establish an ordinality and translation strategy to decode the nature of the correspondence between temporary neural traces and their psychological manifestations.</p>



<p id="part5"><strong><u>Part V:</u></strong> Instantiating the Model Within a Computer</p>



<p><strong>5.1 AI Should Employ Iterative Updating</strong></p>



<p>Many researchers in the field of AI expect brain science to reveal breakthroughs that will provide essential guidance for the construction of intelligent machines (Haikonen, 2012). Some have suggested that AI may not need to emulate fine-grained molecular or cellular details of the brain to create human-level intellectual function (Bostrom, 2014). Instead, they suggest simulating an abstraction of the neurological mechanisms that produce intelligence (e.g., Hassabis et al., 2017; Butlin et al., 2023). The present model introduces abstractions that may be useful in this regard.</p>



<p>Specifically, the present model may help close the “computational explanatory gap,” which is an effort to understand how the parallel, subsymbolic computations involved in low-level neural networks could translate into the serial, symbolic-level algorithms involved in high-level cognition (Reggia et al., 2019). Figures 26 and 27 provide mechanistic accounts of how this gap could be bridged. Today, even state-of-the-art AI processing feats are generally only equivalent to a second or less of unconscious human processing (e.g., recognizing objects in a picture) (Goodfellow et al., 2017). To create more generally intelligent AI, these brief parallel processing sessions must be chained together into iterated sequences that more closely resemble symbolic thought. Iterative updating and multiassociative search may be instrumental in realizing this. As the rest of this section will detail, even though neither are recognized by psychology or neuroscience, they are both used in computer science.</p>



<p>Iterative updating, on its own, is not sufficient to elevate computer information processing to the cognitive domain. In fact, updating a memory store iteratively has been commonplace in computing for several decades. All computers using the Von Neumann architecture routinely update their temporary memory stores (i.e., static RAM and dynamic RAM). These stores, known as caches, have a resemblance to working memory. They hold information that is predicted to be useful so it can be readily available to the CPU. Cached information includes intermediate results from ongoing processing, as well as data and program instructions from the storage drive. Cache stores have a limited capacity, and because they are constantly tasked with holding new information, they must evict old information. These stores are updated iteratively as the least recently used (LRU) data are replaced (Comer, 2017). For example, a computer’s RAM holds billions of bytes coactive through time, adding and subtracting from this pool in the manner illustrated in Figure 2.</p>



<p>However, the next bytes of data processed by the CPU are not determined by the contents of the cache itself. Instead, the processing instruction sequence is determined by the next line of programmed, executable code. Thus, unlike the brain, computers do not make cached information globally accessible for use in multiassociative search. The various bytes of data within computer cache memory can certainly be considered coactive, but they are not “cospreading.” That is, they do not pool their activation energy to search long-term memory for relevant data as in human working memory (Fig. 26). No computer hardware or software does this as described here.</p>



<p>There are advanced AI systems that employ working memory, a global workspace, recursion, and various methods of updating (e.g., Goertzel, 2016). These include cognitive architectures (Gray, 2007), evolutionary computation (Sipper et al., 2018), soft computing (Konar, 2014), and some machine learning techniques. However, such software generally utilizes either preprogrammed symbolic rules or subsymbolic ones to transform one state into the next. Because these systems are incapable of transitioning between the two, they are usually restricted to formalized, narrow problem-solving domains (Haikonen, 2003).</p>



<p>Artificial neural networks are different in that they eschew preprogrammed rules. Like the brain, neural networks use parallel, distributed processing to train systems of subsymbolic nodes to come to recognize complex mathematical functions. Some neural networks, such as recurrent and long short-term memory networks, have nodes capable of persistent activity that is highly analogous to sustained firing (Fig. 9). The enduring activity of these recurrent nodes permits them to cache previous inputs in the form of activated long-term memory (Sherstinsky, 2020). This working memory pool is updated iteratively as recurrent neural nodes gain and lose activation. Similarly, the context window in transformer-based large language models is updated incrementally during training (reading) and inference (writing). Moreover, the tokens within the context window combine their spreading activation energy on each forward pass through the network in a search for the next predicted word. This behavior is similar to the present model’s conception of multiassociative search, but without the integration of a serial, symbolic component (Reser, 2012). These models may use symbols, such as words, as inputs and outputs but do not contain internal representations of them.</p>



<p>To develop and manipulate true internal representations, AI working memory should be designed to run iterative updating in lockstep with multiassociative search. This is technically feasible in the near term. Because current artificial neural network technology is capable of sustained firing, synaptic potentiation, spreading activation, and Hebbian learning, everything discussed in this article thus far can be implemented by it. If an artificial neural network was engineered to do this in the manner presented in the preceding sections, the resulting system may exhibit some of the human qualities and functionality discussed thus far, including association and prediction formation, algorithm implementation, the compounding of intermediate results, progressive modification, self-directed intelligence, and attentive continuity. The most simplistic implementation is depicted in the figure below. This will be elaborated upon in the next section.</p>







<figure><img data-attachment-id="294" data-permalink="https://aithought.com/blog/traditional-neural-network-bcde-1/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/04/traditional-neural-network-bcde-1.jpg" data-orig-size="2258,1158" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="traditional-neural-network-bcde-1" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/04/traditional-neural-network-bcde-1.jpg?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/04/traditional-neural-network-bcde-1.jpg?w=1024" loading="lazy" width="1024" height="525" src="https://aithoughtcom.files.wordpress.com/2023/04/traditional-neural-network-bcde-1.jpg?w=1024" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/04/traditional-neural-network-bcde-1.jpg?w=1024 1024w, https://aithoughtcom.files.wordpress.com/2023/04/traditional-neural-network-bcde-1.jpg?w=2048 2048w, https://aithoughtcom.files.wordpress.com/2023/04/traditional-neural-network-bcde-1.jpg?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/04/traditional-neural-network-bcde-1.jpg?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/04/traditional-neural-network-bcde-1.jpg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><strong>Fig. 40.</strong> Oversimplified Neural Network Using Iterative Updating&nbsp;</p>



<p><em>This is a traditional, fully connected neural network with four active input nodes passing activation energy through hidden layers. This pass results in the activation of symbol “f” in the output layer. This is a naïve, vastly oversimplified implementation of the model proposed here that does not feature a global workspace, modularity, or multimodality. Also, in this illustration, the lowercase letters correspond to subsymbolic nodes rather than sets of assemblies that compose representational items (ensembles).</em></p>



<p><strong>5.2 Designing an AI Capable of Iterative Updating</strong></p>



<p>Implementing human-like working memory to a first approximation in an AI system would mean creating a connectionist program that spreads activity from active information, along with incoming activity from its sensors, to search for entailed information from long-term associative memory. Using this found information as a partial update and then repeating this process in a cycle would structure the architecture to be self-organizing.</p>



<p>Iterative updating and multiassociative search may first have to be explicitly programmed into the system using rule-based code until it becomes clear how to design a system where they emerge organically as they do in the brain. Hand-coded or not, they must be defined mathematically and unambiguously to be the basis of computer software. Multiassociative search can be expressed as a function (f) that maps input variables (x) of the current state of working memory to an output variable (y) used to update them. Each network state would be a search for the update applied to the next state. As a formal algorithm, it could be modeled as a stateless Markov process in discrete time, performing non-deterministic search. As a computable function, it could be instantiated by traditional or neuromorphic computer clusters and executed using brain emulation, hierarchical hidden Markov models, stochastic grammars, probabilistic programming languages, neural networks, or others.</p>



<p>The rest of this section will describe how this system could be constructed using an artificial neural network architecture. It could be realized through recurrent networks or spiking ones. Either way, layers of nodes should be used to model the pattern-recognizing assemblies discussed in Section 4.1. As in the brain, each level in the hierarchy must build a statistical model of the regularities in the level below it (Eliasmith, 2013). Hierarchical pattern recognition would be achieved when primitive nodes lower in the hierarchy converge on high-order patterns in higher layers (Hawkins, 2004; Kurzweil, 2012). Figure 41 caricatures how this is actualized by nonlinear transformations.</p>







<figure><img data-attachment-id="519" data-permalink="https://aithought.com/blog/image-27/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/11/image-27.png" data-orig-size="975,515" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-27" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/11/image-27.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/11/image-27.png?w=975" loading="lazy" width="975" height="515" src="https://aithoughtcom.files.wordpress.com/2023/11/image-27.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/11/image-27.png 975w, https://aithoughtcom.files.wordpress.com/2023/11/image-27.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/11/image-27.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/11/image-27.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 41</strong>. Hierarchical Pattern Recognition and Completion</p>



<p><em>Three subsymbolic line segments are detected by early visual cortex. These segments, corresponding to two wings and a beak, map onto three separate nodes. The nodes each fire at another node higher in the visual processing hierarchy that detects the coactivity (conjunction) of all three. In this case, that node detects the presence of a bird flying in the distance. This is an example of convergent pattern recognition made possible by hierarchical nonlinear transformations. Apparently, the prospect of a bird for dinner enters the focus of attention, creating an impulse to grab a bow and arrow. The neural engram for grabbing becomes active and fires action potentials at motor neurons responsible for the muscular contractions of each of the fingers. This is representative of divergent pattern completion. Essential to sending information to and from working memory in the mammalian brain, convergence and divergence should be emphasized in AI neural networks.</em></p>



<p>Nodes at the top of the hierarchy would constitute high-order subsymbolic patterns due to having receptive fields composed of various inputs from multiple layers of mounting complexity. These abstract nodes should be capable of recurrent activity simulating the sustained firing of pyramidal cortical neurons. This would prolong their activity as search parameters and dependency markers, as well as contribute to contextual structuring for extended periods.</p>



<p>To form item-like ensembles, a Hebbian learning rule would be needed to strengthen the weights between frequently coactive nodes. This must work in such a way that groups of highly associated subsymbolic nodes can form symbolic groups (perhaps across layers). These ensembles should be sparse and fuzzy and used to represent invariant, categorical patterns. Such an ensemble would be equivalent to an internal mental representation and should be made capable of enduring coactivity with other items. These items should be coactive within a graph-structured global workspace. Engineering such a workspace could conceivably necessitate an analog of neural binding (i.e., Klimesch et al., 2010) and synchronized, reentrant oscillations (Edelman, 2004) to integrate (i.e., Tononi, 2004) and unify multiple ensembles into a singular situational representation. This would amount to an emulation of the FoA.</p>



<p>When the simulated sustained firing abates, nodes should subsequently simulate synaptic potentiation. This would enable the network to maintain pertinent information in an emulated short-term store as cached assets. Nodes potentiated in this way would continue to bias the multiassociative workspace until they are either promoted back to the FoA or demoted back to inert long-term memory.</p>



<p>The AI’s simulated FoA and short-term memory stores would undergo iterative updating such that the overlap of persistent information is congruent with Figure 12 and information replacement is congruent with Figure 27. It is imperative that the FoA be designed to cache not only external stimuli but also internal concepts as in Figure 17. Information selection should be guided by multiassociative search as in Figure 26. Each update would amount to a truth-preserving associative transition in the processing stream underwritten by structural properties of the network, which in turn are based on past statistical analyses of reliable patterns from the physical world.</p>



<p><strong>5.3 Modularity, Modality, and Imagery in AI</strong></p>



<p>An implementation of this system would necessitate modular specialization. Each module would correspond to a compartmentalized neural network meant to simulate a different cortical or subcortical area of the mammalian brain. These separate networks would interconnect to form a single dynamical system. Coordinating this assemblage to implement the multiassociative algorithm would be a considerable engineering problem. Given that the human brain accomplishes this task, human neuroscience should be used as an archetype. Thus, the system could be constructed biomimetically and inspired by general neuroanatomical connectivity.</p>



<p>Not only would the nodes of each modular network be organized hierarchically, but the connections between modules would establish an even larger hierarchical structure. This stratified organization, beginning from unimodal networks and progressing to densely conjunctive multimodal networks, would mirror the gradient seen from sensory cortices to association cortices in mammals. Networks higher in the hierarchy would refer to denser space-time conjunctions and multidimensional levels of abstraction. The networks could be designed to emulate specific human cortical modules if they drew inspiration from anatomical connectivity. This would emphasize intrinsic, extrinsic, short-range, and long-range connections, along with the relevant proximities and proportionalities. Multimodal areas that may be pivotal to higher-order abstractions and, therefore, in need of being reverse-engineered in this way include the angular gyrus, Wernicke’s area, Broca’s area, the dorsolateral PFC, the medial PFC, the supplementary motor area, and the frontal pole.</p>







<figure><img data-attachment-id="602" data-permalink="https://aithought.com/blog/image-13-2/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/image-13.png" data-orig-size="975,627" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-13" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/image-13.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/image-13.png?w=975" loading="lazy" width="975" height="627" src="https://aithoughtcom.files.wordpress.com/2023/12/image-13.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/image-13.png 975w, https://aithoughtcom.files.wordpress.com/2023/12/image-13.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/image-13.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/image-13.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 42.</strong> A Hierarchical Artificial Neural Network Structured to Integrate Information Across Modules</p>



<p><em>Each enclosed set of nodes represents a specialized neural network module wired to receive a different input modality. Networks at the bottom (left) of the hierarchy take input of a single modality from the environment. Other networks take input from multiple neural networks below them in the hierarchy. Spreading network activity would oscillate between the top and bottom of the hierarchy while allowing reentrant feedback (bidirectional signal exchange) within and between networks. This figure features 24 networks, each with 19 nodes. An actual build would necessitate dozens of networks, each with millions of nodes.</em></p>



<p>Each modular network in the system would take inputs from associative areas (global working memory) and use them to create their own unique corresponding set of outputs with the potential to contribute to the next update. Some of these modules may produce imagery. To understand how imagery can benefit AI, let’s discuss how it is formed in the brain. Neurons in sensory cortex respond to perceptual features from sensory input and fuse them into images known as topographic mappings (Moscovich et al., 2007). This imagery holds metric and compositional (precategorical) information. In addition to creating topographic mappings from patterns recognized in the external environment (bottom-up), sensory areas are thought to combine divergent (Fig. 41), top-down inputs from association cortices to generate internally derived scenery (Mellet et al., 1998; Miyashita, 2005). Generally, brain imaging research supports the idea that imagining something in the “mind’s eye” activates maps in early perceptual networks (Damasio, 1989; Hasegawa et al., 1998; Ohbayashi et al., 1999).</p>



<p>The sensory networks of our AI system should similarly construct topographic maps (retinotopic for vision, tonotopic for sound, etc.). There are already reliable methods for using neural networks to generate such “self-organizing” maps (Hameed et al., 2019), and imagery generation by inverse neural networks is common today (Byeon et al., 2018). By creating a series of internally generated maps to match the iterative updating taking place in association areas, this system could produce iterated sequences of mental images. During this process, the topographic maps may use low-order perceptual knowledge (from prior probability) to depict associative relationships between higher-order items held in persistent activity. In so doing, the mental imagery that is formed may introduce valuable new informational content into working memory (such as features or objects incidental to the image itself). In other words, thinking and reasoning can be informed by logical information contained in visual and acoustic imagery.</p>



<p>As a given set of items in the FoA is updated, the set of unimodal, lower-order sensory maps held in synchrony with it would be updated correspondingly (Reser, 2011, 2012, 2013). In other words, after a mental image is formulated, it will likely be replaced by another image that uses many of the same working memory items as constraining parameters. Consecutive maps formed in this way could infuse video-like continuity into the imagery and could amount to a type of synthetic imagination. This form of hierarchical crosstalk between association and sensory areas, marked by mutual interactions (i.e., reciprocal causation), may allow an AI system to use mental imagery to see, hear, and thereby model hypothetical situations. This has been termed “progressive imagery modification” (Reser, 2016) and is depicted in Figure 43.</p>







<figure><img data-attachment-id="604" data-permalink="https://aithought.com/blog/image-15-2/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/image-15.png" data-orig-size="975,548" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-15" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/image-15.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/image-15.png?w=975" loading="lazy" width="975" height="548" src="https://aithoughtcom.files.wordpress.com/2023/12/image-15.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/image-15.png 975w, https://aithoughtcom.files.wordpress.com/2023/12/image-15.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/image-15.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/image-15.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 43.</strong> Progressive Imagery Modification</p>



<p><em>At time 1, items B, C, D, and E are active in association networks. The spreading activation from these items provides independent yet interactive top-down bias signals to primary visual networks where a composite topographic map is built based on prior experience with these items. This gestalt sketch will introduce compatible content to working memory. Hence, at time 2, salient features created by the map from time 1 spread activation energy up the hierarchy, converging on the assemblies for item F. Item B becomes inactive while items C, D, E, and F diverge back down the hierarchy toward the primary visual network. The process repeats itself. Because of iterative updating, this process can create a logically connected series of related images.</em></p>



<p>Brain researchers believe that sensory areas deliver information in the form of fleeting sensory maps, whereas association areas deliver lasting perceptual expectations in the form of templates and that these interact to construct higher-order cognitive processes (Hawkins, 2004; Carpenter &amp; Grossberg, 2003). Progressive imagery modification (Fig. 43) could play an instrumental role in this reciprocal signaling between early, bottom-up sensory networks (where activity is metric, topographic, and transient) and top-down association networks (where activity is abstract, conceptual, and persistent) (Christophel et al., 2017). It could even enable an AI system to develop the kind of interplay between the central executive and the visuospatial sketchpad characteristic of the Baddeley (2000, 2007) model of working memory (Fig. 4). Further, this process of iterative modification could take place in other modules, such as language areas (where it is involved in the construction of speech), motor areas (where it is involved in action sequencing), and prefrontal areas (where it is involved in planning).</p>



<p>This general design could form the basis of a security precaution promoting AI safety and alignment. To human observers, the representation of knowledge in neural networks is distributed in such a complex manner that it is mostly inscrutable (Castelvecchi, 2016). This lack of transparency heightens the fear of superintelligent AI because it would be impossible to tell whether the AI was secretly harboring hostile motives (Bostrom, 2014). However, if the system was inherently obligated to build a composite topographic map of each state of working memory to initiate and inform the next state, then these maps could be displayed on a monitor for humans (or another AI) to view. A history of all visual and auditory maps could be saved to an external memory drive. This would ensure that all the AI system’s mental imagery and inner speech are recorded for later inspection and interpretation. Hostile intentions would not have to be deciphered; they would be plain to see.</p>



<p><strong>5.4 How to Train an AI that Employs Iterative Updating</strong></p>



<p>The architecture described in the last three sections would not be limited to learning from discrete batches of curated input but could be exposed to continuous data streams from real-world scenarios that unfold through time. Also, the system would not suspend its activity every time it finishes a task. Rather, it would exhibit continuous, endogenous processing. The system’s ontological and epistemological development would benefit from embodied, real-time, robotic interactions within physical, social, and intellectual training conditions. During exposure to these conditions, it would engage in unsupervised learning of time-series patterns from unlabeled data on a constant basis.</p>



<p>This would necessitate a compatible reward function to guide learning, reinforcement, and credit assignment. Said function should be based on the circuitry of the mammalian dopamine system, including the ventral tegmentum and nucleus accumbens. In mammals, novel appetitive or aversive events increase dopamine release. This ensures they are driven by predictors of food, sex, and pain. Representations of rewarding, punishing, salient, uncertain, or unpredicted events elicit dopaminergic activity in all vertebrates. This increased concentration of ambient dopamine leads to increases in sustained firing (Seamans &amp; Yang, 2004). Thus, dopamine neuromodulation drives an animal’s priorities, allowing them to prolong information about unique opportunities and threats (Seamans &amp; Robbins, 2010).</p>



<p>An analog of the dopamine system’s network would be needed to remember and recognize appetitively stimulating combinations of items occurring in working memory and prioritize them by sustaining their activity (Fig. 46). This would allow groupings with constructive incentive value to bias processing for extended periods. If we want a superintelligent AI that can further human understanding, then we should design its appetitive system to be driven to mine information from literature and databases and use it to generate new associations between ensembles. Thus, this system should latch on to unprocessed frontiers in its knowledge space (sets of unreconciled items with incentive appeal), amounting to an algorithmic form of curiosity.</p>



<p>Functional, preset pathways (akin to an infant’s instinct to grasp something when its palm is touched) should be built into the direct connections between sensory (input) and motor (output) areas. This innate programming could come in the form of already-trained neural network modules that perform practical cognitive tasks (e.g., scene classification, paragraph comprehension, or natural language generation) embedded into the bottom of the hierarchy of this much more extensive network. For instance, a large language model could be used as a simulacrum of Broca’s area (the human language area) and used to inform semantic development in the network at large. Such modules could orient the system toward effective performance on basic tasks, just as reflexes and prepared learning set developing animals on a track toward reproductive success. The machine would use operant feedback about its performance on these tasks to bootstrap learning.</p>



<p>Maturation of the AI’s neural network should approximate that observed in the human cortex. It should start by simulating the brain of an infant (Fuster, 2015). Initially, motor output should not be driven by higher-order association areas but rather by low-order sensory and motor modules. As low-level responses are practiced and refined, and pertinent algorithms are developed through trial and error (see Section 4.4), association networks could be slowly interposed between sensory and motor networks. As in the mammalian brain (Huttenlocher &amp; Dabholkar, 1997), sensory and motor areas should mature (myelinate) early in development, and association areas should mature late. Similarly, the capacity for persistent activity should start low but increase over developmental time.</p>



<p>Postponing the initialization of sustained firing would allow the formation of low-order associations between causally linked events that typically occur close together in time. This would focus the system on easy-to-predict aspects of its reality (e.g., correlations between occurrences in close temporal proximity). The consequent learning would erect a reliable scaffolding of highly predictable associations that could be used to substantiate higher-order, time-delayed associations later in development (Reser, 2016). In other words, the proportion of updating from one state to the next (Fig. 18) would start very high. This would be reversed over weeks to years as an increasing capacity for persistent working memory activity would be folded into the system.</p>



<p>A working memory store that uses iterative updating would be used to establish associations between related clusters of stimuli that appear close together in time from books, articles, lectures, speeches, videos, and experiences. Then, as the length of sustained firing is increased, temporally proximate contextual representations could be coactivated with other less proximate ones when multiassociative search deems them to be highly probabilistically related (i.e., they share a logical or analogical connection). Thus, two events that were never temporally local in the environment could be selected for joint iterative processing within the FoA (Fig. 29). This kind of reconciliation between separate (previously discrete) iterative threads could build and constantly retune a dynamic knowledge base of interconnected representations. After adequate training, the duration of persistent activity could be adjusted to outstrip that of humans, allowing the system to capture extremely long-term causal dependencies, resulting in the perception of high-order abstractions that would be imperceptible to humans.</p>



<p>Unlike biological brains, this system would be scalable (Fig. 44). There are straightforward ways to amplify the working memory of such a system beyond the physiological limitations of the human brain. These include:</p>



<ol>
<li>Increasing the total number of nodes in LTM</li>



<li>Increasing the number of nodes capable of being coactivated in the short-term store</li>



<li>Increasing the number of items capable of being coactivated in the FoA</li>



<li>Increasing the length of time these can remain active (increasing the half-life and decreasing the rate of updating (Fig. 19)</li>



<li>Increasing the number of tightly coupled iterations (thoughts) that can occur before attention is disrupted (Fig. 21)</li>
</ol>







<figure><img data-attachment-id="323" data-permalink="https://aithought.com/blog/ai-agent-5-1/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/05/ai-agent-5-1.jpg" data-orig-size="1260,602" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1683126284&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="ai-agent-5-1" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/05/ai-agent-5-1.jpg?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/05/ai-agent-5-1.jpg?w=1024" loading="lazy" width="1024" height="489" src="https://aithoughtcom.files.wordpress.com/2023/05/ai-agent-5-1.jpg?w=1024" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/05/ai-agent-5-1.jpg?w=1024 1024w, https://aithoughtcom.files.wordpress.com/2023/05/ai-agent-5-1.jpg?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/05/ai-agent-5-1.jpg?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/05/ai-agent-5-1.jpg?w=768 768w, https://aithoughtcom.files.wordpress.com/2023/05/ai-agent-5-1.jpg 1260w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><strong>Fig. 44.</strong> Four Examples of Working Memory Activity Within the Focus of Attention</p>



<p><em>The figure compares the number of items and rate of updating between a human with a limited-capacity working memory, a human with a limited attentional store, a typical human, and a superintelligent AI agent. The AI agent can maintain a larger number of nodes over a longer period, ensuring that its perceptions and actions will be informed by a larger amount of recent information.</em></p>



<p>Under conditions of imperfect or incomplete information, the longer the backward memory span and the larger the number of related events that can be used in multiassociative search, the less uncertainty (information entropy) there is about the present state. However, in information theory, the length beyond which a backward memory span stops providing predictive information is known as the correlation length (Shannon, 1951; Stone, 2015). The working memory of a species can be seen as having a correlation length beyond which there is little predictive value to be had given its ecological niche. The long correlation length of the human FoA was likely permitted by our cognitively demanding foraging style, selection for social cognition, and the supervised learning, error feedback, and large number of training examples provided by prolonged and intensive maternal investment (Reser, 2006). However, there is no reason to believe that the length or breadth of the human FoA has been optimized for systemizing reality. It was probably constrained by several evolutionary factors that would not apply to computers.</p>







<figure><img data-attachment-id="408" data-permalink="https://aithought.com/blog/ai-comparison-5/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/09/ai-comparison-5.jpg" data-orig-size="4161,3181" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ai-comparison-5" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/09/ai-comparison-5.jpg?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/09/ai-comparison-5.jpg?w=1024" loading="lazy" width="1024" height="782" src="https://aithoughtcom.files.wordpress.com/2023/09/ai-comparison-5.jpg?w=1024" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/09/ai-comparison-5.jpg?w=1024 1024w, https://aithoughtcom.files.wordpress.com/2023/09/ai-comparison-5.jpg?w=2046 2046w, https://aithoughtcom.files.wordpress.com/2023/09/ai-comparison-5.jpg?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/09/ai-comparison-5.jpg?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/09/ai-comparison-5.jpg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><strong>Fig. 45</strong>. Venn Diagrams of Working Memory in Different Systems</p>



<p><em>These diagrams depict informational overlap between states of working memory in a span of ten seconds. The diagrams on the left use the format from Figure 12, while those on the right use the format from Figure 11. Diagram 1 shows zero overlap between working memory at times 1 and 2. This would make it more difficult for system 1, a hypothetical mouse, to make associations between events separated by the delay. For example, calling this mouse’s name and feeding it 10 seconds later may not condition it to come when called, whereas feeding it two seconds later might. Training an AI should involve a maturational process where the system begins learning with a minimal working memory span (e.g., Diagram 1) before gradually developing a superhuman capacity for working memory span (Diagram 4) as formative experiences accumulate.</em></p>



<p>Prolonging the duration of persistent activity will allow each search to be more specific and informed. This is because searches would be apprised by a larger number of specifications that stretch further back in time. It would also ensure that the system is less likely to allow crucial intermediate solutions to decay from working memory coactivity (i.e., a cache miss) before they are needed to form higher-order, compound inferences. The “thoughts” of such a system would be lengthy, highly focused, and tightly interwoven.</p>



<p>Now may be the time to start building large, state-of-the-art, iterative updating networks and training them as one would train a child with the expectation that aspects of intelligence will emerge. It is hoped that through exposure to experiences with systematic patterns, a system like that described above would construct an associative network capable of producing updates to its states of working memory that build functionally on previous states. This could lead to the capacity to make valid associative connections between probabilistically related events (Fig. 29), resulting in the discovery of relationships obscured by separations in space and delays in time. Simulating iterative updating and multiassociative search and enhancing them beyond human capacities could be instrumental in the effort to construct AI capable of common sense, insight, creativity, machine consciousness, and superintelligence.</p>



<p><strong>5.5 Discussion and Conclusions</strong></p>



<p>This article aims to introduce an internally consistent framework for understanding how neural activity gives rise to complex thought. It is intended to inspire more detailed hypotheses, experimental tests, and machine implementations. Previous models of working memory have attributed various high-level cognitive functions to the central executive (e.g., updating of items, coordination of modules, shifting between tasks, selective attention, gating, the construction of imagery, and others). Because the neural substrate of these advanced operations has never been delineated, the central executive remains a mysterious black box. This article has supported the case that executive functions emerge from collective processing interactions among specialized subsystems guided by iterative updating. If shown to have a tenable neural basis by future research, the concepts introduced in this article (Table 4) may amount to a viable alternative to the notion of the central executive found in other models. In so doing, they may provide an organizing mechanism for self-regulating thought in AI.</p>







<figure><img data-attachment-id="605" data-permalink="https://aithought.com/blog/screenshot-4/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-4.png" data-orig-size="1026,721" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-4" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-4.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-4.png?w=1024" loading="lazy" width="1024" height="719" src="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-4.png?w=1024" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-4.png?w=1024 1024w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-4.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-4.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-4.png?w=768 768w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-4.png 1026w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><strong>Table 4.</strong> Definition of Terms Used and Introduced in This Article</p>



<p>This article reconciled iterative updating with traditional models of working memory, including those discussed in the literature review. However, it can similarly be integrated with a variety of compatible frameworks that model the dynamics of item-like constructs, including those in Table 5. These models, along with many others, provide detailed mechanistic explanations for critical neurocognitive components underspecified by the present model.</p>



<figure><img data-attachment-id="606" data-permalink="https://aithought.com/blog/screenshot-5/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-5.png" data-orig-size="1191,661" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-5" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-5.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-5.png?w=1024" loading="lazy" width="1024" height="568" src="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-5.png?w=1024" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-5.png?w=1024 1024w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-5.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-5.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-5.png?w=768 768w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-5.png 1191w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><strong>Table 5.</strong> Other Models and Frameworks That Can Be Integrated with The Present Model</p>



<p>Working memory items in the FoA have been considered to be isomorphic with the contents of consciousness (Baars &amp; Franklin, 2003; Buchsbaum, 2013). This suggests that the subjects of conscious thought are held in working memory and operate according to the same (or similar) rules and capacity limitations. In the classic paradigm for working memory testing, subjects can retain approximately four items in mind. However, they are holding much additional declarative content. This is because they also maintain the task requirements, active sensory perceptions, and ongoing personal thoughts (which may be limited by cognitive load). The iterative updating function applies to all this conscious content, not just to the four items described by Cowan (2017) and others. The previous figures in this article have mainly used only two working memory stores (the FOA and short-term memory). Figure 46 uses an arbitrarily larger number of functionally specialized stores as an alternative to indicate that items may exist along a graded continuum of activation.</p>







<figure><img data-attachment-id="80" data-permalink="https://aithought.com/21-imagery-and-behavior-in-the-iterative-updating-model-of-working-sensory-and-short-term-memory/" data-orig-file="https://aithoughtcom.files.wordpress.com/2022/03/21.-imagery-and-behavior-in-the-iterative-updating-model-of-working-sensory-and-short-term-memory.jpg" data-orig-size="1431,780" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="21.-imagery-and-behavior-in-the-iterative-updating-model-of-working-sensory-and-short-term-memory" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2022/03/21.-imagery-and-behavior-in-the-iterative-updating-model-of-working-sensory-and-short-term-memory.jpg?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2022/03/21.-imagery-and-behavior-in-the-iterative-updating-model-of-working-sensory-and-short-term-memory.jpg?w=1024" loading="lazy" width="1024" height="558" src="https://aithoughtcom.files.wordpress.com/2022/03/21.-imagery-and-behavior-in-the-iterative-updating-model-of-working-sensory-and-short-term-memory.jpg?w=1024" alt="" srcset="https://aithoughtcom.files.wordpress.com/2022/03/21.-imagery-and-behavior-in-the-iterative-updating-model-of-working-sensory-and-short-term-memory.jpg?w=1024 1024w, https://aithoughtcom.files.wordpress.com/2022/03/21.-imagery-and-behavior-in-the-iterative-updating-model-of-working-sensory-and-short-term-memory.jpg?w=150 150w, https://aithoughtcom.files.wordpress.com/2022/03/21.-imagery-and-behavior-in-the-iterative-updating-model-of-working-sensory-and-short-term-memory.jpg?w=300 300w, https://aithoughtcom.files.wordpress.com/2022/03/21.-imagery-and-behavior-in-the-iterative-updating-model-of-working-sensory-and-short-term-memory.jpg?w=768 768w, https://aithoughtcom.files.wordpress.com/2022/03/21.-imagery-and-behavior-in-the-iterative-updating-model-of-working-sensory-and-short-term-memory.jpg 1431w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><strong>Fig. 46.</strong> Imagery and Behavior in the Iterative Updating Model</p>



<p><em>Iteratively updated items in working memory interact with sensory cortices to progressively construct mental imagery. They also interact with motor cortices to progressively construct behavior. In the next state, the items in each working memory store will undergo partial replacement. The parameters used in the sensory and motor cortices will reflect this change, making their output an advancement on their previous output. This mirroring of each update will permit them to construct progressive imagery and behavior. Related cognitive processes are included as arrows. The dopamine system (ventral tegmentum) uses inputs from the amygdala and nucleus accumbens (N.A.) to determine which patterns of items match internal incentive templates and thus should be sustained.</em></p>



<p>“Higher-order” theories of consciousness hold that conscious thought arises when a mental state is concerned with a previous mental state. This includes thoughts about perceptions and thoughts about thoughts (Rosenthal, 2004). Following this line of reasoning, thoughts that iterate from previous thoughts exhibit a backward-referential quality and could be considered “higher-order thoughts.” Because of its role in generating a continual production line of higher-order thoughts, iterative updating should be considered a candidate for the neural basis of consciousness. It ensures that the train of thought does not stop and go in discrete steps but is instead propelled continuously by the items that endure through time. While individual items may exit the FoA within seconds, the shared content across successive states keeps the proverbial train on track and sustains associative connections that interlink the advancing sequence of thoughts.</p>



<figure><img data-attachment-id="607" data-permalink="https://aithought.com/blog/screenshot-6/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-6.png" data-orig-size="1196,734" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-6" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-6.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-6.png?w=1024" loading="lazy" width="1024" height="628" src="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-6.png?w=1024" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/screenshot-6.png?w=1024 1024w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-6.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-6.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-6.png?w=768 768w, https://aithoughtcom.files.wordpress.com/2023/12/screenshot-6.png 1196w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><strong>Table 6. </strong>Fundamental Features of the Iterative Updating Model</p>



<p>In his book <em>The River of Consciousness</em> (2017), Oliver Sacks asks, “But how then do our frames, our momentary moments, hold together? How, if there is only transience, do we achieve continuity?” This article postulates that our moments overlap in their set of active representations and that this ongoing confluence results in a flowing progression of states. After asking the question, Sacks quotes William James. Each thought, in James’s words, is an owner of the thoughts that went before and “dies owned, transmitting whatever it realized as itself to its own later proprietor.” James expounds further on this subject employing the analogy of a stream:</p>



<blockquote>
<p>“Consciousness, then, does not appear to itself chopped up in bits. Such words as ‘chain’ or ‘train’ do not describe it fitly as it presents itself in the first instance. It is nothing jointed; it flows. A ‘river’ or a ‘stream’ are the metaphors by which it is most naturally described. In talking of it hereafter let us call it the stream of thought, of consciousness, or of subjective life. […] As the brain-changes are continuous, so do all these consciousnesses melt into each other like dissolving views. Properly they are but one protracted consciousness, one unbroken stream.”</p>



<p>William James (1890, p. 239)</p>
</blockquote>







<figure><img data-attachment-id="608" data-permalink="https://aithought.com/blog/image-16-2/" data-orig-file="https://aithoughtcom.files.wordpress.com/2023/12/image-16.png" data-orig-size="975,548" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-16" data-image-description="" data-image-caption="" data-medium-file="https://aithoughtcom.files.wordpress.com/2023/12/image-16.png?w=300" data-large-file="https://aithoughtcom.files.wordpress.com/2023/12/image-16.png?w=975" loading="lazy" width="975" height="548" src="https://aithoughtcom.files.wordpress.com/2023/12/image-16.png?w=975" alt="" srcset="https://aithoughtcom.files.wordpress.com/2023/12/image-16.png 975w, https://aithoughtcom.files.wordpress.com/2023/12/image-16.png?w=150 150w, https://aithoughtcom.files.wordpress.com/2023/12/image-16.png?w=300 300w, https://aithoughtcom.files.wordpress.com/2023/12/image-16.png?w=768 768w" sizes="(max-width: 975px) 100vw, 975px"></figure>



<p><strong>Fig. 47.</strong> Schematic Representation of Ongoing Iteration in the FoA and Short-term Memory Store</p>



<p><em>This graphic expands on previous figures, incorporating a larger number of the present model’s theoretical features. These include the following: (1) the number of items coactive in the FoA (white spheres) at any point in time varies between three and five, (2) the percentage of updating in the FoA varies between 25% and 100%, (3) the order of entry into the FoA does not determine the order of exit, (4) items that exit the FoA briefly enter the short-term store (gray spheres) before deactivating completely (black spheres), and (5) items that have exited the FoA are capable of reentering the FoA.</em></p>



<p>The present model bears a resemblance to James’s conception of a “stream of consciousness.” A stream is a distribution of points that slides through space and time. Figure 47 extends the activity schematized in this article’s other figures over 18 points in time. This results in a depiction of brain activity, working memory, and thought that, shifting gradually, appears very much like a stream. The iterative updating of working memory sustains and shapes the flow of thought. Each new update is an influx of information that acts like a tributary, merging and interacting with the larger current of consciousness, contributing to its rate and direction. Simulating this stream within a computer could play an integral role in enabling an artificial agent to experience a cognitive continuum, traverse the expanse of consciousness, and explore the state-space of thought.</p>



<p>.</p>



<p><strong>References</strong></p>



<p>Anderson, J. R. (1983). A spreading activation theory of memory. Journal of Verbal Learning and Verbal Behavior, 22(3), 261-295.</p>



<p>Asok, A., Leroy, F., Rayman, J. B., &amp; Kandel, E. R. (2019). Molecular mechanisms of the memory trace. Trends in Neurosciences, 42(1), 14-22.</p>



<p>Atkinson, R. C., &amp; Shiffrin, R. M. (1968). Human memory: A proposed system and its control processes. In Spence, K. W., &amp; Spence, J. T., The psychology of learning and motivation (Volume 2), pp. 89-195. New York: Academic Press.</p>



<p>Atkinson, R. C., &amp; Shiffrin , R. M. (1969). Storage and retrieval processes in long-term memory. Psychological Review, 76(2), 179-193.</p>



<p>Averell, L., &amp; Heathcote, A. (2011). The form of the forgetting curve and the fate of memories. Journal of Mathematical Psychology, 55(1), 25-25.</p>



<p>Baars, B. J., &amp; Franklin, S. (2003). How conscious experience and working memory interact. Trends in Cognitive Sciences, 7, 166-172.</p>



<p>Baars, B. J. (2007). A framework. Baars, B. J., &amp; Gage, N. M. (Eds.), Cognition, brain, and consciousness: Introduction to cognitive neuroscience, p. 30. London: Academic Press.</p>



<p>Baars, B. J., &amp; Franklin, S. (2007). An architectural model of conscious and unconscious brain functions: Global Workspace Theory and IDA. Neural Networks, 20(9), 955-961.</p>



<p>Baddeley, A. (1986). Working memory. Oxford, UK: Clarendon Press.</p>



<p>Baddeley, A. D. (2000). The episodic buffer: A new component of working memory? Trends in Cognitive Science, 4, 417-423.</p>



<p>Baddeley, A. D., Hitch, G. J., &amp; Allen, R. J. (201). From short-term store to multicomponent working memory: The role of the modal model. Memory &amp; Cognition, 45, 575-588.</p>



<p>Baddeley, A. D. (2007). Working memory, thought and action. Oxford University Press.</p>



<p>Baddeley, A. D., &amp; Hitch, G. J. (1994). Developments in the concept of working memory. Neuropsychology, 8(4), 485-493.</p>



<p>Baddeley, A. D., &amp; Hitch, G. J. (1974). Working memory. In Bower, G. A. (Ed.), Recent advances in learning and motivation, Vol. 8, pp. 47-89. New York: Academic Press.</p>



<p>Baddeley, A. D. (2000). The episodic buffer: A new component of working memory? Trends in Cognitive Sciences, 4(11), 417-423.</p>



<p>Baddeley, A. D. (2012). Working memory: Theories, models and controversies. Annual Review of Psychology, 63, 1-29.</p>



<p>Bargh, J. A., &amp; Chartrand, T. L. (2000). Studying the mind in the middle: A practical guide to priming and automaticity research. In Reis, H., &amp; Judd, C. (Eds.), Handbook of research methods in social psychology, pp. 1-39. New York: Cambridge University Press.</p>



<p>Baronett, S. (2008). Logic. Upper Saddle River, NJ: Pearson Prentice Hall.</p>



<p>Bostrom N. (2014). Superintelligence: Paths, dangers, strategies. Oxford University Press.</p>



<p>Botvinick, M. M. (2008). Hierarchical models of behavior and prefrontal function. Trends in Cognitive Sciences, 12(5), 201-208.</p>



<p>Botvinick, M. M., &amp; Plaut, D. C. (2006). Short-term memory for serial order: A recurrent neural network model. Psychological Review, 113(2), 201-233.</p>



<p>Braver, T. S., &amp; Cohen, J. D. (2000). On the control of control: The role of dopamine in regulating prefrontal function and working memory. In Monsell, S., &amp; Driver, J. (Eds.), Attention and performance XVIII: Control of cognitive processes, pp. 713-737. Cambridge, MA: The MIT Press.</p>



<p>Broadbent, D. (1958). Perception and communication. London: Pergamon Press.</p>



<p>Brydges, C., Gignac, G. E., &amp; Ecker, U. K. H. (2018). Working memory capacity, short-term memory capacity, and the continued influence effect: A latent-variable analysis. Intelligence, 69, 177-122.</p>



<p>Buchsbaum, B. R. (2013). The role of consciousness in the phonological loop: Hidden in plain sight. Frontiers in Psychology, 4, 496.</p>



<p>Butlin, P., et al. (2023). Consciousness in artificial intelligence: Insights from the science of consciousness. arXiv:2308.08708</p>



<p>Byeon, W., Wang, Q., Srivastava, R. K., &amp; Koumoutsakos, P. (2018). ContextVP: Fully context-aware video prediction. The European Conference on Computer Vision (ECCV), pp. 753-769.</p>



<p>Carpenter, G. A., &amp; Grossberg, S. (2003). Adaptive resonance theory. In Arbib, M. A. (Ed.), The handbook of brain theory and neural networks, Second Edition, pp. 87-90. Cambridge, MA: The MIT Press.</p>



<p>Castelvecchi, D. (2016). Can we open the blackbox of AI: Artificial intelligence is everywhere. But before scientists trust it, they first need to understand how machines learn. Nature, 538, 7623.</p>



<p>Chein, J. M., &amp; Fiez, J. A. (2010). Evaluating models of working memory through the effects of concurrent irrelevant information. Journal of Experimental Psychology: General, 139, 117-137.</p>



<p>Cheng, P. C., &amp; Holyoak, K. J. (2008). Pragmatic reasoning schemas. In Adler, J. E., &amp; Rips, L. J. (Eds.), Reasoning: Studies of human inference and its foundations, pp. 827-842. Cambridge University Press.</p>



<p>Chia, W. J., Hamid, A. I. A., &amp; Abdullah, J. M. (2018). Working memory from the psychological and neurosciences perspectives: A review. Frontiers in Psychology, 27.</p>



<p>Christophel, T. B., Klink, P. C., Spitzer, B., Roelfsema, P. R., &amp; Haynes, J. (2017). The distributed nature of working memory. Trends in Cognitive Sciences, 21(2), 111-124.</p>



<p>Cohen, G. (2000). Hierarchical models in cognition: Do they have psychological reality? European Journal of Cognitive Psychology, 12(1), 1-36.</p>



<p>Collins, A. M., &amp; Loftus, E. F. (1975). A spreading-activation theory of semantic processing. Psychological Review, 82(6), 407-428.</p>



<p>Comer, D. (2017). Essentials of computer architecture. New York: Chapman and Hall.</p>



<p>Constantinidis, C., Funahashi, S., Lee, D., Murray, J. D., Qi, X., Wang, M., &amp; Arnsten, A. F. T. (2018). Persistent spiking activity underlies working memory. Journal of Neuroscience, 38(32), 7020-7028.</p>



<p>Cowan, N. (1984). On short and long auditory stores. Psychological Bulletin, 96(2), 341-370.</p>



<p>Cowan, N. (2001). The magical number 4 in short-term memory: A reconsideration of mental storage capacity. Behavioral and Brain Sciences, 24, 87-185.</p>



<p>Cowan, N. (2005). Working memory capacity. New York: Psychology Press.</p>



<p>Cowan, N. (1988). Evolving conceptions of memory storage, selective attention, and their mutual constraints within the human information-processing system. Psychological Bulletin, 104(2), 163-191.</p>



<p>Cowan, N. (2009). What are the differences between long-term, short-term, and working memory? Progress in Brain Research, 169, 323-338.</p>



<p>Cowan, N. (2011). The focus of attention as observed in visual working memory tasks: Making sense of competing claims. Neuropsychologia, 49, 1401-1406.</p>



<p>Cowan, N. (2017). The many face of working memory and short-term storage. Psychonomic Bulletin &amp; Review, 24(4), 1158-1170.</p>



<p>Crick, F., &amp; Koch, C. (2003). A framework for consciousness. Nature Neuroscience, 6(2), 119-126.</p>



<p>D’Esposito, M., &amp; Postle, B. R. (2015). The cognitive neuroscience of working memory. Annual Review of Psychology, 66, 115-142.</p>



<p>Damasio, A. R. (1989). Time-locked multiregional retroactivation: A systems level proposal for the neural substrates of recall and recognition. Cognition, 33, 25-62.</p>



<p>Debanne, D., Inglebert, Y., &amp; Russier, M. (2019). Plasticity of intrinsic neuronal excitability. Current Opinion in Neurobiology, 54, 73-82.</p>



<p>Dehaene, S. (2020). How we learn: Why brains learn better than any machine… for now. New York: Penguin Random House.</p>



<p>Ecker, U. K., Oberauer, K., &amp; Lewandowsky, S. (2014). Working memory updating involves item-specific removal. Journal of Memory and Language, 74, 1-15.</p>



<p>Edelman, G. (2004). Wider than the sky. Yale University Press.</p>



<p>Eriksson, J., Vogel, E. K., Lansner, A., Bergstrom, F., Nyberg, L. (2015). Neurocognitive architecture of working memory. Neuron, 88(1), 33-46.</p>



<p>Fuji, H., Ito, H., Aihara, K., Ichinose, N., &amp; Tsukada, M. (1998). Dynamical Cell Assembly Hypothesis – Theoretical possibility of spatio-temporal coding in the cortex. Neural Networks, 9(8),1303-1350.</p>



<p>Funahashi, S., Bruce, C. J., Goldman-Rakic, P. S. (1993). Dorsolateral prefrontal lesions and oculomotor delayed-response performance: evidence for mnemonic ‘scotomas’. Journal of Neuroscience, 13(4), 1479-1497.</p>



<p>Funahashi, S. (2007). The general-purpose working memory system and functions of the dorsolateral prefrontal cortex. In Osaka, N., Logie, R. H., &amp; D’Esposito, M. (Eds.), The cognitive neuroscience of working memory, pp. 213-230. Oxford University Press.</p>



<p>Fuster, J. M. (2002a). Frontal lobe and cognitive development. Journal of Neurocytology, 31(3-5), 373-385.</p>



<p>Fuster, J. M. (2002b). Physiology of executive functions: The perception-action cycle. In Stuss, D. T., &amp; Knight, R. T. (Eds.), Principles of frontal lobe function, pp. 96-108. Oxford University Press.</p>



<p>Fuster, J. M. (1973). Unit activity in prefrontal cortex during delayed-response performance: Neuronal correlates of transient memory. Journal of Neurophysiology, 36(1), 61-78.</p>



<p>Fuster, J. M. (2009). Cortex and Memory: Emergence of a new paradigm. Journal of Cognitive Neuroscience, 21(11), 2047-2072.</p>



<p>Fuster, J. (2015). The prefrontal cortex (Fifth Edition). Oxford, UK: Academic Press, Elsevier.</p>



<p>Glushchenko, A., et al. (2018). Unsupervised language learning in OpenCog. In: Iklé, M., Franz, A., Rzepka, R., &amp; Goertzel B. (Eds.), Artificial general intelligence. AGI 2018. Lecture Notes in Computer Science, vol. 10999. Springer, Cham.</p>



<p>Goertzel, B. (2016). The AGI revolution. Humanity Press.</p>



<p>Goertzel, B., Pennachin, C., &amp; Geisweiller, N. (2014). Engineering general intelligence: A path to advanced AGI via embodied learning and cognitive synergy. Atlantis Press.</p>



<p>Goldman-Rakic, P. S. (1987). Circuitry of the prefrontal cortex and the regulation of behavior by representational memory. In Mountcastle, V. B., Plum, F., &amp; Geiger, S. R. (Eds.), Handbook of neurobiology, pp. 373-417. Bethesda, MD: American Physiological Society.</p>



<p>Goldman-Rakic, P. S. (1990). Cellular and circuit basis of working memory in prefrontal cortex of nonhuman primates. In Uylings, H. B. M., Eden, C. G. V., DeBruin, J. P. C., Corner, M. A., &amp; Feenstra, M. G. P. (Eds.), Progress in brain research, vol. 85, pp. 325-336. Elsevier Science Publications.</p>



<p>Goldman-Rakic, P. S. (1995). Cellular basis of working memory. Neuron, 14(3), 447-485.</p>



<p>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep learning. Cambridge, MA: The MIT Press.</p>



<p>Gurney, K. N. (2009). Reverse engineering the vertebrate brain: Methodological principles for a biologically grounded programme of cognitive modeling. Cognitive Computation, 1(1), 29-41.</p>



<p>Gray, W. D. (2007). Integrated models of cognitive systems. Oxford University Press.</p>



<p>Haikonen, P. O. (2003). The cognitive approach to conscious machines. Exeter, UK: Imprint Academic.</p>



<p>Haikonen, P. O. (2012). Consciousness and robot sentience. Hackensack, NJ: World Scientific Publishing.</p>



<p>Hameed, A. A., Karlik, B., Salman, M. S., &amp; Eleyan, G. (2019). Robust adaptive learning approach to self-organizing maps. Knowledge-Based Systems, 171(1), 25-36.</p>



<p>Hamilton, W. (1890). In Mansel, H. L., &amp; and Veitch, J. (Eds.), 1860 lectures on metaphysics and logic, in Two Volumes. Vol. II. Logic. Boston: Gould and Lincoln.</p>



<p>Hawkins, J. (2004). On intelligence. New York: Times Books.</p>



<p>Hasegawa, I., Fukushima, T., Ihara, T., &amp; Miyashita, Y. (1998). Callosal window between prefronal cortices: Cognitive interaction to retrieve long-term memory. Science, 281, 814-818.</p>



<p>Hassabis, D., Kumaran, D., Summerfield, C., &amp; Botvinick, M. (2017). Neuroscience-inspired artificial intelligence. Neuron, 95, 245-258.</p>



<p>Hebb, D. (1949). The organization of behavior. New York: Wiley.</p>



<p>Hofstadter, D. (2007). I am a strange loop. New York: Basic Books.</p>



<p>Howard, M. W., &amp; Kahana, M. J. (2002). A distributed representation of temporal context. Journal of Mathematical Psychology, 46, 269-299.</p>



<p>Hummel, J. E., &amp; Holyoak, K. J. (2003). A symbolic-connectionist theory of relational inference and generalization. Psychological Review, 110(2), 220-264.</p>



<p>Huttenlocher, P. R., &amp; Dabholkar, A. S. (1997). Developmental anatomy of prefrontal cortex. In Krasnegor, N. A., Lyon, G. R., Goldman-Rakic, &amp; P. S. (Eds.), Development of the prefrontal cortex. Baltimore, MD: Paul H. Brookes Publishing Co.</p>



<p>Jacob, S. N., Hähnke, D., &amp; Nieder, A. (2018). Structuring of abstract working memory content by fronto-parietal synchrony in primate cortex. Neuron, 99(3), 588-597.</p>



<p>James, W. (1909). A pluralistic universe. Hibbert lectures at Manchester College on the present situation in philosophy. London: Longmans, Green, and Co.</p>



<p>James, W. (1890). The principles of psychology. New York: Henry Holt.</p>



<p>Johnson-Laird, P. N. (1998). Computer and the mind: An introduction to cognitive science. Harvard University Press.</p>



<p>Kaas, J. H. (1997). Topographic maps are fundamental to sensory processing. Brain Research Bulletin, 44(2), 107-112.</p>



<p>Kahneman, D. (2011). Thinking fast and slow. New York: Farrar, Straus, and Giroux.</p>



<p>Klimesch, W., Freunberger, R., &amp; Sauseng, P. (2010). Oscillatory mechanisms of process binding in memory. Neuroscience and Biobehavioral Reviews, 34(7), 1002-1014.</p>



<p>Konar A. (2014). Artificial intelligence and soft computing: Behavior and cognitive modeling of the human brain. Boca Raton, FL: CRC Press.</p>



<p>Kounatidou, P., Richter, M., &amp; Schöner, G.. (2018). A neural dynamic architecture that autonomously builds mental models. In Rogers, T. T. Rau, M., Zhu, X., &amp; Kalish, C. W. (Eds.), Proceedings of the 40th Annual Conference of the Cognitive Science Society, pp. 643-648.</p>



<p>Kurzweil, R. (2012). How to create a mind. New York: Penguin Group.</p>



<p>Laird, J. E. (2012). The soar cognitive architecture. Cambridge, MA: The MIT Press.</p>



<p>Lansner, A. (2009). Associative memory models: From the cell-assembly theory to biophysically detailed cortex simulations. Trends in Neurosciences, 32(3),179-186.</p>



<p>LaRocque, J. J., Lewis-Peacock, J. A., &amp; Postle, B. R. (2014). Multiple neural states of representation in short-term memory? It’s a matter of attention. Frontiers in Human Neuroscience, 8, 1-14.</p>



<p>Lewis-Peacock, J. A., Drysdale, A. T., Oberauer, K., &amp; Postle, B. R. (2012). Neural evidence for a distinction between short-term memory and the focus of attention. Journal of Cognitive Neuroscience, 24(1), 61-79.</p>



<p>Manohar, S. G., Zokaei, N., Fallon, S. J., Vogels, T. P., &amp; Husain, M. (2019). Neural mechanisms of attending to items in working memory. Neuroscience and Biobehavioral Reviews, 101, 1-12.</p>



<p>Mellet, E., Petit, L., Mazoyer, B., Denis, M., &amp; Tzourio, N. (1998). Reopening the mental imagery debate: Lessons from functional anatomy. Nueroimage, 8(2),129-139.</p>



<p>Meyer, K., Damasio, A. (2009). Convergence and divergence in a neural architecture for recognition and memory. Trends in Neurosciences, 32(7), 376-382.</p>



<p>Myers, N. E., Stokes, M. G., &amp; Nobre, A. C. (2017). Prioritizing information during working memory: Beyond sustained internal attention. Trends in Cognitive Sciences, 21(6), 449-461.</p>



<p>Miller, G. A. (1956). The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological Review, 63(2), 81-97.</p>



<p>Miller, E. K., &amp; Cohen, J. D. (2001). An integrative theory of prefrontal cortex function. Annual Review of Neuroscience, 24, 167-202.</p>



<p>Miller, E. K., Lundqvist, M., &amp; Bastos, A. M. (2018). Working memory 2.0. Neuron, 100, 463-475.</p>



<p>Mongillo, G., Barak, O., &amp; Tsodyks, M. (2008). Synaptic theory of working memory. Science, 319, 1543-1546.</p>



<p>Moscovich, M. (1992). Memory and working-with-memory: A component process model based on modules and central systems. Journal of Cognitive Neuroscience, 4(3),257-267.</p>



<p>Moscovitch, M., Chein, J. M., Talmi, D., &amp; Cohn, M. (2007). Learning and memory. In Baars, B. J., &amp; Gage, N. M. (Eds.), Cognition, brain, and consciousness: Introduction to cognitive neuroscience, p. 234. London:&nbsp; Academic Press.</p>



<p>Miyashita, Y. (2005). Cognitive memory: cellular and network machineries and their top-down control. Science, 306, 435-440.</p>



<p>Nairne, J. S. (2002). Remembering over the short-term: The case against the standard model. Annual Review of Psychology, 53, 53-81.</p>



<p>Newell, A., &amp; Simon, H. A. Computer simulation of human thinking. Science, 134, 2011-2017.</p>



<p>Niklaus, M., Singmann, H., &amp; Oberauer, K. (2019). Two distinct mechanisms of selection in working memory: Additive last-item and retro-cue benefits. Cognition, 183, 282-302.</p>



<p>Norman, D. A. (1968). Toward a theory of memory and attention. Psychological Review, 75(6), 522-536.</p>



<p>Nyberg, L., Eriksson, J. (2016). Working memory: maintenance, updating, and the realization of intentions. Cold Spring Harbor Perspectives in Biology, 8(2), a021816.</p>



<p>Oberauer, K. (2002). Access to information in working memory: Exploring the focus of attention. Journal of Experimental Psychology: Learning, Memory, and Cognition, 28(3), 411-421.</p>



<p>Opitz B. (2010). Neural binding mechanisms in learning and memory. Neuroscience and Biobehavioral Reviews, 34(7), 1036-1046.</p>



<p>Panichello, M. F., &amp; Buschman, T. J. (2021). Shared mechanisms underlie the control of working memory and attention. Nature, 592, 601-605.</p>



<p>Pina, J. E., Bodner, M., &amp; Ermentrout, B. (2018). Oscillations in working memory and neural binding: a mechanism for multiple memories and their interactions. PLOS Computational Biology, 14(11), e1006517.</p>



<p>Postle, B. R. (2007). Activated long-term memory? The bases of representation in working memory. In Osaka, N., Logie, R. H., &amp; D’Esposito, M. (Eds.), The cognitive neuroscience of working memory. Oxford University Press.</p>



<p>Postle, B., et al. (2006). Repetitive transcranial magnetic stimulation dissociates working memory manipulation from retention functions in the prefrontal, but not posterior parietal, cortex. Journal of Cognitive Neuroscience, 18, 1712-1722.</p>



<p>Reggia, J. A., Katz, G. E., &amp; Davis, G. P. (2019). Modeling working memory to identify computational correlates of consciousness. Open Philosophy, 2, 252-269.</p>



<p>Reser, J. E. (2006). Evolutionary neuropathology &amp; congenital mental retardation: Environmental cues predictive of maternal deprivation influence the fetus to minimize cerebral metabolism in order to express bioenergetic thrift. Medical Hypotheses, 67(3), 529-544.</p>



<p>Reser, J. E. (2011). What determines belief: The philosophy, psychology and neuroscience of belief formation and change. Saarbrucken, Germany: Verlag Dr. Muller.</p>



<p>Reser, J. E. (2012). Assessing the psychological correlates of belief strength: Contributing factors and role in behavior. (Doctoral Dissertation). Retrieved from University of Southern California. Usctheses-m2627.</p>



<p>Reser, J. E. (2022). Artificial intelligence software structured to simulate human working memory, mental imagery, and mental continuity. arXiv:2204.05138</p>



<p>Reser, J. E. (2013). The neurological process responsible for mental continuity: Reciprocating transformations between a working memory updating function and an imagery generation system. Association for the Scientific Study of Consciousness Conference. San Diego CA, July 12-15.</p>



<p>Reser, J. E. (2016). Incremental change in the set of coactive cortical assemblies enables mental continuity. Physiology and Behavior, 167(1), 222-237.</p>



<p>Reisberg, D. (2010). Cognition: Exploring the science of the mind. New York: W. W. Norton &amp; Co.</p>



<p>Rose, N. S., LaFocque, J. J., Riggall, A. C., Gosseries, O., Starrett, M. J., &amp; Meyering, E. E. (2016). Reactivation of latent working memories with transcranial magnetic stimulation. Science, 354(6316), 1136-1139.</p>



<p>Rosenthal, D. M. (2004). Varieties of higher-order theory. In Gennaro, R. (Ed.), Higher-order theories of consciousness, pp. 17-44. Amsterdam: John Benjamins.</p>



<p>Ruchkin, D. S., Grafman, J., Cameron, K., &amp; Berndt, R. S. (2003). Working memory retention systems: A state of activated long-term memory. Behavioral and Brain Sciences, 26, 709-777.</p>



<p>Rushworth, M. F., Nixon, P. D., Eacott, M. J., &amp; Passingham, R. E. (1997). Ventral prefrontal cortex is not essential for working memory. Journal of Neuroscience, 17(12), 4829-4838.</p>



<p>Ryan, K., Agrawal, P., &amp; Franklin, S. (2019). The pattern theory of self in artificial general intelligence: A theoretical framework for modeling self in biologically inspired cognitive architectures. Cognitive Systems Research. In press.</p>



<p>Rypma, B., Berger, J. S., &amp; D’Esposito, M. (2002). The influence of working-memory demand and subject performance on prefrontal cortical activity. Journal of Cognitive Science, 14(5), 721-731.</p>



<p>Sacks, O. (2017). The river of consciousness. New York: Vintage Books.&nbsp;</p>



<p>Salmon, M. (2012). Arguments from analogy. Introduction to logic and critical thinking, pp. 132-142. Cengage Learning.</p>



<p>Sarter, M., Givens, B., &amp; Bruno, J. P. (2001). The cognitive neuroscience of sustained attention: where top-down meets bottom-up. Brain Research Reviews, 35(2), 146-160.</p>



<p>Schvaneveldt, R. W., &amp; Meyer, D.E. (1973). Retrieval and comparison processes in semantic memory. In Kornblum, S., Attention and performance: IV, pp. 395-409. New York: Academic Press.</p>



<p>Shanks, D. (2010). Learning: From association to cognition. Annual Review of Psychology, 1, 273-301.</p>



<p>Shannon, C. (1951). Prediction and entropy of printed English. Bell System Technical Journal, 30, 47-51.</p>



<p>Shastri, L. (1999). Advances in Shruti—A neurally motivated model of relational knowledge representation and rapid inference using temporal synchrony. Applied Intelligence, 11(1), 79-108.</p>



<p>Sherstinsky A. (2020). Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm) network. Physica D: Nonlinear Phenomena, 404, 132306.</p>



<p>Shipstead, Z., Harrison, T. L., &amp; Engle, R. W. (2015). Working memory capacity and the scope and control of attention. Attention, Perception, &amp; Psychophysics, 77(6), 1863-1880.</p>



<p>Seamans, J. K., &amp; Robbins, T. W. (2010). Dopamine modulation of the prefrontal cortex and cognitive function. The Dopamine Receptors, 373-398.</p>



<p>Seamans, J. K., &amp; Yang, C. R. (2004). The principal features and mechanisms of dopamine modulation in the prefrontal cortex. Progress in Neurobiology, 74(1), 1-58.</p>



<p>Silvanto, J. (2017). Working memory maintenance: Sustained firing or synaptic mechanisms? Trends in Cognitive Sciences, 21(3), 152-154.</p>



<p>Sipper, M., Fu, W., Ahuja, K., &amp; Moore, J. H. (2018). Investigating the parameter space of evolutionary algorithms. BioData Mining, 11(2).</p>



<p>Sreenivasan, K. K., &amp; D’Esposito, M. (2019). The what, where and how of delay activity. Nature Reviews Neuroscience. May 13.</p>



<p>Sousa, A. M. M., Meyer, K. A., Santpere, G., Gulden, F. O., &amp; Sestan, N. (2017). Evolution of the human nervous system function, structure, and development. Cell, 170(2), 226-247.</p>



<p>Sperling, G. (1960). The information available in brief visual representations. Psychological Monographs, 74, 1-29.</p>



<p>Stokes, M. G. (2015). ‘Activity-silent’ working memory in prefrontal cortex: A dynamic coding framework. Trends in Cognitive Science, 19(7), 395-405.</p>



<p>Stone, J. V. (2015). Information theory: A tutorial introduction. Sebtel Press.</p>



<p>Striedter, G. (2005). Principles of brain evolution. Sunderland, MA: Sinauer Associates.</p>



<p>Tomita, H., Ohbayashi, M., Nakahara, K., Hasegawa, I., &amp; Miyashita, Y. (1999). Top-down signalfrom prefrontal cortex in executive control of memory retrieval. Nature, 401, 699-703.</p>



<p>Tononi, G. (2010). An information integration theory of consciousness. BMC Neuroscience, 5, 42.</p>



<p>Treisman, A. M. (1964). Selective attention in man. British Medical Bulletin, 20, 12 16.</p>



<p>von der Malsburg, C. (1999). The what and why of binding: The modeler’s perspective. Neuron, 24, 95-104.</p>



<p>Weger, U., Wagemann, J., &amp; Meyer, A. (2018). Introspection in psychology: Its contribution to theory and method in memory research. European Psychologist, 23, 206-216.</p>



<p>Zanto, T. P., Rubens, M. T., Thangavel, A., &amp; Gazzaley, A. (2011). Causal role of the prefrontal cortex in top-down modulation of visual processing and working memory. Nature Neuroscience, 14, 656-661.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Best 7B LLM on leaderboards made by an amateur following a medium tutorial (146 pts)]]></title>
            <link>https://huggingface.co/CultriX/MistralTrix-v1</link>
            <guid>38882726</guid>
            <pubDate>Fri, 05 Jan 2024 18:34:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://huggingface.co/CultriX/MistralTrix-v1">https://huggingface.co/CultriX/MistralTrix-v1</a>, See on <a href="https://news.ycombinator.com/item?id=38882726">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<!-- HTML_TAG_START --><h2>
	<a rel="noopener nofollow" href="#results" id="results">
		
	</a>
	<span>
		Results:
	</span>
</h2>
<p>T: 🟦
Model: CultriX/MistralTrix-v1 📑
Average: 73.39
ARC: 72.27
HellaSwag: 88.33
MMLU: 65.24
TruthfulQA: 70.73
Winogrande: 80.98
GSM8K: 62.77</p>
<h2>
	<a rel="noopener nofollow" href="#editdisclaimer" id="editdisclaimer">
		
	</a>
	<span>
		Edit/Disclaimer:
	</span>
</h2>
<p>Currently the #1 ranked 7B LLM on the LLM Leaderboards, woah!
I did not expect that result at all and am in no way a professional when it comes to LLM's or computer science in general,
just a guy that likes to nerd about and tinker around. </p>
<p>For those wondering how I achieved this, the answer is that I simply attempted to apply the techniques outlined in this amazing article myself: <a rel="noopener nofollow" href="https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac">https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac</a>
Therefore, all credit basically goes to the guy who wrote that. 
He offers the exact Colab notebook I used to train this model for free, as well as a really nice GitHub page I hope he doesn't mind me sharing: <a rel="noopener nofollow" href="https://github.com/mlabonne/llm-course/">https://github.com/mlabonne/llm-course/</a>
So huge thank you to him for sharing his knowledge and learning me a thing or two in the process!</p>
<h2>
	<a rel="noopener nofollow" href="#gguf" id="gguf">
		
	</a>
	<span>
		GGUF
	</span>
</h2>
<p>I attempted to quantisize the model myself, which again I pretty much have no clue about, but it seems to run fine for me when I test them:
<a rel="noopener nofollow" href="https://huggingface.co/CultriX/MistralTrix-v1-GGUF">https://huggingface.co/CultriX/MistralTrix-v1-GGUF</a></p>
<p>I'll say it one more time though:
"I am a complete beginner to all of this, so if these do end up sucking don't be surprised."</p>
<p>You have been warned :)</p>
<h2>
	<a rel="noopener nofollow" href="#description" id="description">
		
	</a>
	<span>
		Description:
	</span>
</h2>
<p>(trained on a single Colab GPU in less than a few hours)</p>
<p>MistralTrix-v1 is an zyh3826/GML-Mistral-merged-v1 model that has been further fine-tuned with Direct Preference Optimization (DPO) using Intel's dataset for neural-chat-7b-v3-1.
It surpasses the original model on several benchmarks (see results).</p>
<p>It is directly inspired by the RLHF process described by Intel/neural-chat-7b-v3-1's authors to improve performance. 
I used the same dataset and reformatted it to apply the ChatML template.</p>
<p>The code to train this model is available on Google Colab and GitHub. 
Fine-tuning took about an hour on Google Colab A-1000 GPU with 40GB VRAM.</p>
<h2>
	<a rel="noopener nofollow" href="#training-specifications" id="training-specifications">
		
	</a>
	<span>
		TRAINING SPECIFICATIONS
	</span>
</h2>
<blockquote>
<p>LoRA configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']
)</p>
</blockquote>
<blockquote>
<p>Model to fine-tune
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    load_in_4bit=True
)
model.config.use_cache = False</p>
</blockquote>
<blockquote>
<p>Reference model
ref_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    load_in_4bit=True
)</p>
</blockquote>
<blockquote>
<p>Training arguments
training_args = TrainingArguments(
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    learning_rate=5e-5,
    lr_scheduler_type="cosine",
    max_steps=200,
    save_strategy="no",
    logging_steps=1,
    output_dir=new_model,
    optim="paged_adamw_32bit",
    warmup_steps=100,
    bf16=True,
    report_to="wandb",
)</p>
</blockquote>
<blockquote>
<p>Create DPO trainer
dpo_trainer = DPOTrainer(
    model,
    ref_model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
    peft_config=peft_config,
    beta=0.1,
    max_prompt_length=1024,
    max_length=1536,
)</p>
</blockquote>
<!-- HTML_TAG_END --></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Harlequin: SQL IDE for Your Terminal (119 pts)]]></title>
            <link>https://github.com/tconbeer/harlequin</link>
            <guid>38882526</guid>
            <pubDate>Fri, 05 Jan 2024 18:20:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tconbeer/harlequin">https://github.com/tconbeer/harlequin</a>, See on <a href="https://news.ycombinator.com/item?id=38882526">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Harlequin</h2>
<p dir="auto"><a href="https://pypi.org/project/harlequin/" rel="nofollow"><img src="https://camo.githubusercontent.com/067fdd5a85ae1e0918a49d816d4a453ff0b98ca16bc57c0439ddd3b7dac99307/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6861726c657175696e" alt="PyPI" data-canonical-src="https://img.shields.io/pypi/v/harlequin"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/27f6294ba42f41326103352ca94fa23356fbfdcf28d4bb3f49722e1756e5552b/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f6861726c657175696e"><img src="https://camo.githubusercontent.com/27f6294ba42f41326103352ca94fa23356fbfdcf28d4bb3f49722e1756e5552b/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f6861726c657175696e" alt="PyPI - Python Version" data-canonical-src="https://img.shields.io/pypi/pyversions/harlequin"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4503b749d9b924c2b9f40c16cddc0daa41fb848585051c991b2b326ce0e457d7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72756e732532306f6e2d4c696e75782532302537432532304d61634f5325323025374325323057696e646f77732d626c7565"><img src="https://camo.githubusercontent.com/4503b749d9b924c2b9f40c16cddc0daa41fb848585051c991b2b326ce0e457d7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72756e732532306f6e2d4c696e75782532302537432532304d61634f5325323025374325323057696e646f77732d626c7565" alt="Runs on Linux | MacOS | Windows" data-canonical-src="https://img.shields.io/badge/runs%20on-Linux%20%7C%20MacOS%20%7C%20Windows-blue"></a></p>
<p dir="auto">The SQL IDE for Your Terminal.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/tconbeer/harlequin/blob/main/harlequin.svg"><img src="https://github.com/tconbeer/harlequin/raw/main/harlequin.svg" alt="Harlequin"></a></p>
<h2 tabindex="-1" dir="auto">Installing Harlequin</h2>
<p dir="auto">After installing Python 3.8 or above, install Harlequin using <code>pip</code> or <code>pipx</code> with:</p>

<h2 tabindex="-1" dir="auto">Using Harlequin with DuckDB</h2>
<p dir="auto">From any shell, to open one or more DuckDB database files:</p>
<div dir="auto" data-snippet-clipboard-copy-content="harlequin &quot;path/to/duck.db&quot; &quot;another_duck.db&quot;"><pre>harlequin <span><span>"</span>path/to/duck.db<span>"</span></span> <span><span>"</span>another_duck.db<span>"</span></span></pre></div>
<p dir="auto">To open an in-memory DuckDB session, run Harlequin with no arguments:</p>

<p dir="auto">If you want to control the version of DuckDB that Harlequin uses, see the <a href="https://github.com/tconbeer/harlequin/blob/main/troubleshooting/duckdb-version-mismatch">Troubleshooting</a> page.</p>
<h2 tabindex="-1" dir="auto">Using Harlequin with SQLite and Other Adapters</h2>
<p dir="auto">Harlequin also ships with a SQLite3 adapter. You can open one or more SQLite database files with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="harlequin -a sqlite &quot;path/to/sqlite.db&quot; &quot;another_sqlite.db&quot;"><pre>harlequin -a sqlite <span><span>"</span>path/to/sqlite.db<span>"</span></span> <span><span>"</span>another_sqlite.db<span>"</span></span></pre></div>
<p dir="auto">Like DuckDB, you can also open an in-memory database by omitting the paths:</p>

<p dir="auto">Other adapters can be installed using <code>pip install &lt;adapter package&gt;</code> or <code>pipx inject harlequin &lt;adapter package&gt;</code>, depending on how you installed Harlequin. For a list of known adapters provided either by the Harlequin maintainers or the broader community, see the <a href="https://harlequin.sh/docs/adapters" rel="nofollow">adapters</a> page in the docs.</p>
<h2 tabindex="-1" dir="auto">Getting Help</h2>
<p dir="auto">To view all command-line options for Harlequin and all installed adapters, after installation, simply type:</p>

<p dir="auto">To view a list of all key bindings (keyboard shortcuts) within the app, press F1. You can also view this list outside the app <a href="https://harlequin.sh/docs/bindings" rel="nofollow">in the docs</a>.</p>
<h2 tabindex="-1" dir="auto">More info at <a href="https://harlequin.sh/" rel="nofollow">harlequin.sh</a></h2>
<p dir="auto">Visit <a href="https://harlequin.sh/" rel="nofollow">harlequin.sh</a> for an overview of features and full documentation.</p>
<h2 tabindex="-1" dir="auto">Contributing</h2>
<p dir="auto">Thanks for your interest in Harlequin! Harlequin is primarily maintained by <a href="https://github.com/tconbeer">Ted Conbeer</a>, but he welcomes all contributions and is looking for additional maintainers!</p>
<h3 tabindex="-1" dir="auto">Providing Feedback</h3>
<p dir="auto">We'd love to hear from you! <a href="https://github.com/tconbeer/harlequin/issues/new">Open an Issue</a> to request new features, report bugs, or say hello.</p>
<h3 tabindex="-1" dir="auto">Setting up Your Dev Environment and Running Tests</h3>
<ol dir="auto">
<li>Install Poetry v1.2 or higher if you don't have it already. You may also need or want pyenv, make, and gcc.</li>
<li>Fork this repo, and then clone the fork into a directory (let's call it <code>harlequin</code>), then <code>cd harlequin</code>.</li>
<li>Use <code>poetry install --sync</code> to install the project (editable) and its dependencies (including all test and dev dependencies) into a new virtual env.</li>
<li>Use <code>poetry shell</code> to spawn a subshell.</li>
<li>Type <code>make</code> to run all tests and linters, or run <code>pytest</code>, <code>black .</code>, <code>ruff . --fix</code>, and <code>mypy</code> individually.</li>
</ol>
<h3 tabindex="-1" dir="auto">Opening PRs</h3>
<ol dir="auto">
<li>PRs should be motivated by an open issue. If there isn't already an issue describing the feature or bug, <a href="https://github.com/tconbeer/harlequin/issues/new">open one</a>. Do this before you write code, so you don't waste time on something that won't get merged.</li>
<li>Ideally new features and bug fixes would be tested, to prevent future regressions. Textual provides a test harness that we use to test features of Harlequin. You can find some examples in the <code>tests</code> directory of this project. Please include a test in your PR, but if you can't figure it out, open a PR to ask for help.</li>
<li>Open a PR from your fork to the <code>main</code> branch of <code>tconbeer/harlequin</code>. In the PR description, link to the open issue, and then write a few sentences about <strong>why</strong> you wrote the code you did: explain your design, etc.</li>
<li>Ted may ask you to make changes, or he may make them for you. Don't take this the wrong way -- he values your contributions, but he knows this isn't your job, either, so if it's faster for him, he may push a commit to your branch or create a new branch from your commits.</li>
</ol>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boeing wants FAA to exempt MAX 7 from safety rules to get it in the air (403 pts)]]></title>
            <link>https://www.seattletimes.com/business/boeing-aerospace/boeing-wants-faa-to-exempt-max-7-from-safety-rules-to-get-it-in-the-air/</link>
            <guid>38882358</guid>
            <pubDate>Fri, 05 Jan 2024 18:08:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seattletimes.com/business/boeing-aerospace/boeing-wants-faa-to-exempt-max-7-from-safety-rules-to-get-it-in-the-air/">https://www.seattletimes.com/business/boeing-aerospace/boeing-wants-faa-to-exempt-max-7-from-safety-rules-to-get-it-in-the-air/</a>, See on <a href="https://news.ycombinator.com/item?id=38882358">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-content">
    <p>Little noticed, days before the holiday break, the Federal Aviation Administration published a Boeing request for an exemption from key safety standards on the 737 MAX 7 — the still-uncertified smallest member of Boeing’s newest jet family. </p><p>Since August, earlier models of the MAX currently flying passengers in the U.S. have had to limit use of the jet’s engine anti-ice system after Boeing discovered a defect in the system with potentially catastrophic consequences.</p><p>The flaw could cause the inlet at the front end of the pod surrounding the engine — known as a nacelle — to break and fall off.</p><p>In an August Airworthiness Directive, the FAA stated that debris from such a breakup could penetrate the fuselage, putting passengers seated at windows behind the wings in danger, and could damage the wing or tail of the plane, “which could result in loss of control of the airplane.”</p><p>Dennis Tajer, a spokesperson for the Allied Pilots Association, the union representing 15,000 American Airlines pilots, said the flaw in the engine anti-ice system has “given us great concern.” </p><p>He said the pilot procedure the FAA approved as an interim solution — urging pilots to make sure to turn off the system when icing conditions dissipate to avoid overheating that within five minutes could seriously damage the structure of the nacelle — is inadequate given the serious potential danger.</p>
<p>“You get our attention when you say people might get killed,” Tajer said. “We’re not interested in seeing exemptions and accommodations that depend on human memory. … There’s just got to be a better way.”</p><p>In its petition to the FAA, Boeing argues the breakup of the engine nacelle is “extremely improbable” and that an exemption will not reduce safety.</p><p>“The 737 MAX has been in service since 2017 and has accumulated over 6.5 million flight hours. In that time, there have been no reported cases of parts departing aircraft due to overheating of the engine nacelle inlet structure,” the filing states.</p><p>On Thursday, Boeing said in an emailed statement that it is “developing a long-term solution that will undergo thorough testing and FAA review before being introduced to the 737 MAX fleet.”</p><p>In the meantime, Boeing said “inspections are ongoing” to check for any damage to the nacelles on MAXs in service.</p><p>However, without an exemption from current safety regulations, the FAA cannot approve the final two MAX models, the MAX 7 and MAX 10, to fly passengers.</p>
<p>On Christmas Eve, just before the deadline for public input on the proposed MAX 7 exemption, the Foundation for Aviation Safety — a lobbying group set up by former Boeing manager and whistleblower <a href="https://www.seattletimes.com/business/boeing-737-max-was-plagued-with-production-problems-whistleblower-says/">Ed Pierson</a> following <a href="https://www.seattletimes.com/business/boeing-aerospace/boeing-737-max-crisis-2019-news-coverage/">the two deadly MAX crashes</a> — filed a submission calling on the FAA not to certify the airplane until Boeing fixes the safety defect. </p><p>“The Foundation is alarmed at the FAA safety culture, allowing consideration of an exemption proposal … for certification of a new airplane model with a known catastrophic failure (risk) resulting from a simple mistake by the flight crew,” the Foundation’s submission states.</p><h2>Warning: Don’t forget to turn it off</h2><p>Industry analysts and Boeing investors have long anticipated MAX 7 certification being granted soon. The company’s share price rose significantly toward year-end based partly on that expectation.</p><p>If the exemption is granted, certification can go ahead, allowing the MAX 7 to begin flying with Southwest Airlines. </p><p>Boeing would have until mid-2026 to design, test and certify a permanent fix for the engine anti-ice system defect that would then be retrofitted to all MAXs. </p><p>By then, there could be nearly 2,000 MAXs in service, meaning more than 4,000 engines needing the retrofit.</p>
<p>Until then, pilots would have to adhere to the limitation currently applied on the MAX 8 and MAX 9 models. After emerging from icy conditions into drier air they have to make sure they turn off the engine anti-ice system, which heats the inner barrel of the engine pod so that ice doesn’t build up.</p><p>If they fail to do so, the system can quickly overheat the carbon composite material and damage the structural integrity of the engine pod. </p><p>The problem is there’s no alert or indication to the crew that the system needs to be turned off. They just have to remember to do it. </p>      <p>If they forget, or are distracted by other tasks, the overheating can begin to damage the structure after just five minutes.</p><p>Tajer said it’s “not uncommon” for pilots on other aircraft to inadvertently leave the anti-ice system on when it is no longer needed. </p><p>On older 737s, for example, this would waste energy but not do any damage. The defect affects only the MAX, with engine inlets made from carbon composite rather than the metal used on older models.</p>
<p>Independent aviation safety consultant and pilot John Cox said he’s run the anti-ice system on the previous 737 “for long periods of time.”</p><p>And he’s unsure how practical it is to ask a MAX flight crew to limit the time the system operates in dry air.</p><p>“I’ve been in and out of cloud tops,” Cox said. “Do you turn it on, turn it off, turn it on, turn it off?”</p><p>“If you are doing that and get distracted, and end up with the anti-ice off and you go back into clouds where you pick up inlet icing, the next time you turn it on, you’re going to ingest that ice,” he added.</p><p>After reviewing Boeing’s petition, Cox said he’d recommend the FAA turn it down.</p><p>“With the possibility of such a failure and an Airworthiness Directive with significant limitation already in place, my vote would be to deny the exemption request,” Cox said. “Yes, it would affect entry into service, but it could create an ‘unsafe condition’ by the FAA’s own words.”</p>
<p>Michael Stumo, father of <a href="https://www.seattletimes.com/business/boeing-aerospace/for-victims-loved-ones-latest-boeing-737-max-tragedy-leaves-anguish-anger-and-lots-of-questions/">Samya Rose Stumo, who died in the second MAX crash of an Ethiopian Airlines jet in 2019</a>, said “Boeing claims to have learned its lessons with a new focus upon safety. That is not true.” </p><p>“Boeing is still avoiding safety rules rather than building safe aircraft,” Stumo said.</p><h2>A single point of failure?</h2><p><a href="https://www.regulations.gov/document/FAA-2023-2340-0001" target="_blank">Boeing’s petition</a> states that the potential breakup of the engine pod was discovered through analysis and flight testing and could happen only in the case of “multiple, independent system failures during specific operational and environmental conditions.”</p><p>“Boeing’s quantitative risk assessment evaluated this scenario to be extremely improbable,” the filing concludes.</p><p>But <a href="https://www.seattletimes.com/business/boeing-aerospace/faa-safety-engineer-goes-public-to-slam-the-agencys-oversight-of-boeings-737-max/">Joe Jacobsen</a>, a retired FAA safety engineer and adviser to the Foundation for Aviation Safety, says the petition offers no evidence that this is not a single point of failure.</p><p>“A pilot forgetting to turn it off, that’s all it takes,” said Jacobsen.</p>
<p>Mike Dostert, another retired FAA safety engineer and also an adviser to the foundation, concurs. </p><p>“All it takes is for the system to be left on and you damage the structure,” said Dostert. “I don’t see the multiple failures.”</p><p>Without any kind of crew alert to tell the pilots they should shut off the system, he said “there’s a pretty good chance human error is going to occur.”</p><p>Notably, among the various regulations Boeing wants exempted from is one requiring the jetmaker to prove that any “single failure or malfunction or probable combination of failures (that) will jeopardize the safe operation of the airplane … is extremely remote.”</p><p>Dostert added that this defect could overheat and damage both engines on the plane simultaneously, making such an event potentially even worse than several serious accidents in recent years when broken engine fan blades caused the inlet cowl to break off a single engine.</p><p>In 2018, <a href="https://www.seattletimes.com/business/boeing-aerospace/one-southwest-passenger-dead-others-injured-after-boeing-737-engine-blowout/">a passenger aboard a Southwest Airlines 737 died </a>when a broken fan blade destroyed an engine cowl. Shrapnel penetrated the aircraft’s fuselage and broke a cabin window beside the passenger.</p>
<p>The pod around the engine is part of the airframe and is the responsibility of Boeing, not the engine maker.</p><p>Dostert said an earlier nonfatal engine blowout on a Southwest flight in 2016 had also led to the inlet cowl departing the aircraft but no fix was made before the fatality in 2018.</p><p>Almost six years later, the fix for that broken fan blade scenario in older 737s is still in the works. In December, the FAA published a proposal that gives Boeing until the middle of 2028 to develop a retrofit that will strengthen the inlet cowls and fan casings.</p><p>“There’s a pattern here,” Dostert said. “Of Boeing knowing about potentially catastrophic single failures, and not addressing them in an expeditious manner.”</p><h2>Equivalent safety to the MAX 8 and MAX 9</h2><p>In 2022, Boeing CEO Dave <a href="https://www.seattletimes.com/business/boeing-aerospace/boeing-ceo-threatens-to-cancel-737-max-10-unless-congress-acts/">Calhoun threatened to cancel the MAX 10</a> if Congress didn’t amend a law granting permission to certify the jet without meeting the safety regulation for crew alerting systems included in the 2020 Aircraft Certification, Safety and Accountability Act.</p><p><a href="https://www.seattletimes.com/business/boeing-aerospace/congress-year-end-bill-clears-faa-to-certify-boeing-737-max-7-10-unchanged/">Congress bowed to the pressure and amended the law</a>, amounting to a safety exemption for the MAX 7 and MAX 10 models.</p>
<p>Boeing argues in its December petition that granting the new exemption, with the same procedural limitation on how the pilots use the engine anti-ice system that applies to the MAX 8 and MAX 9, will leave the MAX 7 no less safe than those two aircraft that are flying passengers every day.</p><p>But Cox said “there’s a difference in an unsafe condition found on the existing fleet and an unsafe condition prior to certification.”</p><p>He said he’s uncomfortable with the idea of “certifying an airplane with an acknowledged potential unsafe condition.”</p><p>With the MAX 8 and 9 already flying, Cox said the FAA’s only alternative to imposing the operational restriction on those jets was to ground the fleet.</p><p>“Do I think it’s worth grounding the fleet? No, I don’t. It’s a bit of a tough call,” Cox said. Limiting use of the anti-ice system in dry air is “probably the best compromise that the FAA and Boeing could come up with and agree on.”</p><p>But for Boeing’s two still-to-be certified airplanes, the MAX 7 and MAX 10, he thinks an expedited permanent fix is a better approach.</p>
<p>“They need to make it a very strong priority to minimize the time under which the engine is operating with this potential problem and to restore the anti-ice system to normal,” Cox said.</p><p>The FAA said in an emailed statement that it will investigate how the defect was missed during the MAX’s original development and certification and “will issue a corrective action to ensure Boeing’s future certification programs … are improved.”</p><p>The safety agency said it will rule on Boeing’s petition, but “there is no specific timetable.”</p>    
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zeiss's "Holocam" turns glass windows into cameras (406 pts)]]></title>
            <link>https://www.digitalcameraworld.com/news/this-holographic-camera-turns-any-window-into-an-invisible-camera</link>
            <guid>38881981</guid>
            <pubDate>Fri, 05 Jan 2024 17:42:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.digitalcameraworld.com/news/this-holographic-camera-turns-any-window-into-an-invisible-camera">https://www.digitalcameraworld.com/news/this-holographic-camera-turns-any-window-into-an-invisible-camera</a>, See on <a href="https://news.ycombinator.com/item?id=38881981">Hacker News</a></p>
<div id="readability-page-1" class="page"><article aria-label="article" data-id="PrZExrLcG9GjPkTvfZ929L">
<header>
<nav aria-label="Breadcrumbs">
<ol>
<li>
<a href="https://www.digitalcameraworld.com/news" aria-label="Return to News" data-before-rewrite-localise="https://www.digitalcameraworld.com/news">News</a>
</li>
</ol>
</nav>



</header>
<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<picture><source type="image/webp" alt="Zeiss Holocam" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-1920-80.jpg.webp 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD.jpg"><source type="image/jpeg" alt="Zeiss Holocam" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-1920-80.jpg 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD.jpg"><img src="https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-320-80.jpg" alt="Zeiss Holocam" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD-1920-80.jpg 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD.jpg"></picture>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/tzwXNMkCUsLmT5HLhKdqaD.jpg">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span itemprop="copyrightHolder">(Image credit: Zeiss)</span>
</figcaption>
</div>

<div id="article-body">
<p><a data-analytics-id="inline-link" href="https://www.digitalcameraworld.com/tag/zeiss" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.digitalcameraworld.com/tag/zeiss">Zeiss</a> is bringing its remarkable Holocam technology to <a data-analytics-id="inline-link" href="https://www.digitalcameraworld.com/news/ces-2024-everything-you-need-to-know-about-the-camera-and-tech-showcase" data-before-rewrite-localise="https://www.digitalcameraworld.com/news/ces-2024-everything-you-need-to-know-about-the-camera-and-tech-showcase">CES 2024</a>, which can turn any glass screen into a camera. This means that everything from the window in your car to the screen on your laptop to the glass on your front door can now possess an invisible image sensor.&nbsp;</p><p>Further, because the technology makes the camera completely transparent, it eliminates the need for cutouts or punch holes –&nbsp;meaning you can have direct eye contact with the person you're chatting to,&nbsp;because the camera can be placed anywhere on (or should that be <em>in</em>) the screen.&nbsp;</p><p>The Holocam technology "uses holographic in-coupling, light guiding and de-coupling elements to redirect the incoming light of a transparent medium to a hidden image sensor."</p><p>Zeiss' <a data-analytics-id="inline-link" href="https://www.digitalcameraworld.com/tag/ces" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.digitalcameraworld.com/tag/ces">CES</a> presentation is focused around its Multifunctional Smart Glass system in general, with a focus on applications in automobiles, so many of its use cases are based around how holography can improve in-car operability. However, it's easy to see how this could be truly transformative technology in the wider world.&nbsp;</p><p>Smart doorbells that don't need a separate camera module. Webcams that enable you to look anywhere on your screen. Parking cameras that can be completely hidden. Face or gesture recognition on any screen, including to unlock doors. Fatigue detection for drivers. Or, you know, phones and tablets without bloody notches or punch holes.</p><p>Using an entire pane of glass as a camera lens also opens some fascinating optical possibilities. Some of Zeiss' bullet points include "large aperture invisible camera" and "individual adjustment of orientation and size of the field of views." Which makes me wonder, what <em>is</em> the maximum aperture and focal range of a camera like this?</p><p>Of course, there's a darker potential for such technology. Given the current fear around hidden cameras in Airbnbs, the idea of every single window (or even shower door) in a rental property being able to spy on you is a little disconcerting.&nbsp;</p><p>Still, this is a fascinating bit of tech –&nbsp;and I'm super excited to see if and how it comes into everyday use.&nbsp;</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" alt="Zeiss Holocam" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://vanilla.futurecdn.net/digitalcameraworld/media/img/missing-image.svg" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD.jpg"><source type="image/jpeg" alt="Zeiss Holocam" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://vanilla.futurecdn.net/digitalcameraworld/media/img/missing-image.svg" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD.jpg"><img src="https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-320-80.jpg" alt="Zeiss Holocam" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-normal="https://vanilla.futurecdn.net/digitalcameraworld/media/img/missing-image.svg" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/XyNHNTGejZ3W4LVSEcg9QD.jpg"></picture></p></div><figcaption itemprop="caption description"><span>Doors that only open if they recognize your face sounds cool –&nbsp;but then, we all know how that works (or doesn't) on phones </span><span itemprop="copyrightHolder">(Image credit: Zeiss)</span></figcaption></figure><p>Take a look at the <a data-analytics-id="inline-link" href="https://www.digitalcameraworld.com/buying-guides/best-dash-cam" data-before-rewrite-localise="https://www.digitalcameraworld.com/buying-guides/best-dash-cam">best dashs</a>, the <a data-analytics-id="inline-link" href="https://www.digitalcameraworld.com/buying-guides/best-indoor-security-camera" data-before-rewrite-localise="https://www.digitalcameraworld.com/buying-guides/best-indoor-security-camera">best indoo security cameras</a>, the <a data-analytics-id="inline-link" href="https://www.digitalcameraworld.com/buying-guides/best-spy-cameras" data-before-rewrite-localise="https://www.digitalcameraworld.com/buying-guides/best-spy-cameras">best spy cameras</a> and the <a data-analytics-id="inline-link" href="https://www.digitalcameraworld.com/buying-guides/best-hidden-camera-detector" data-before-rewrite-localise="https://www.digitalcameraworld.com/buying-guides/best-hidden-camera-detector">best hidden camera detectors</a>.&nbsp;</p>
</div>
<div>
<p><img src="https://cdn.mos.cms.futurecdn.net/flexiimages/tdeebvjhl11651144918.svg"></p>
<div>
<p><strong><span>Thank you for reading 5 articles this month* Join now for unlimited access</span></strong></p><p><strong><span>Enjoy your first month for just £1 / $1 / €1</span></strong></p>
</div>

<p><span>*Read 5 free articles per month without a subscription</span></p>
</div>
<div>
<p><img src="https://cdn.mos.cms.futurecdn.net/flexiimages/tdeebvjhl11651144918.svg">
</p>
<div>
<p><strong><span>Join now for unlimited access</span></strong></p><p>Try first month for just <strong>£1 / $1 / €1</strong></p>
</div>

</div>


<div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-PrZExrLcG9GjPkTvfZ929L"><section><p>The best camera deals, reviews, product advice, and unmissable photography news, direct to your inbox!</p></section></div>
<div id="slice-container-authorBio-PrZExrLcG9GjPkTvfZ929L"><p>The editor of Digital Camera World, James has 21 years experience as a journalist and started working in the photographic industry in 2014 (as an assistant to Damian McGillicuddy, who succeeded David Bailey as Principal Photographer for Olympus). In this time he shot for clients like Aston Martin Racing, Elinchrom and L'Oréal, in addition to shooting campaigns and product testing for Olympus, and providing training for professionals. This has led him to being a go-to expert for camera and lens reviews, photo and lighting tutorials, as well as industry news, rumors and analysis for publications like <a href="https://www.awin1.com/awclick.php?awinmid=2961&amp;awinaffid=103504&amp;clickref=dcw-gb-3007255495896184000&amp;p=https%3A%2F%2Fwww.magazinesdirect.com%2Faz-magazines%2F6936429%2Fdigital-camera-magazine-subscription.thtml" target="_blank">Digital Camera Magazine</a>,&nbsp;<a href="https://www.awin1.com/awclick.php?awinmid=2961&amp;awinaffid=103504&amp;clickref=dcw-gb-6565833657202343000&amp;p=https%3A%2F%2Fwww.magazinesdirect.com%2Faz-magazines%2F6936659%2Fphotoplus-magazine-subscription.thtml" target="_blank">PhotoPlus: The Canon Magazine</a>,&nbsp;<a href="https://www.awin1.com/awclick.php?awinmid=2961&amp;awinaffid=103504&amp;clickref=dcw-gb-4247458655152168000&amp;p=https%3A%2F%2Fwww.magazinesdirect.com%2Faz-magazines%2F6936619%2Fnphoto-magazine-subscription.thtml" target="_blank">N-Photo: The Nikon Magazine</a>,&nbsp;<a href="https://www.awin1.com/awclick.php?awinmid=2961&amp;awinaffid=103504&amp;clickref=dcw-gb-1487400588188809500&amp;p=https%3A%2F%2Fwww.magazinesdirect.com%2Faz-magazines%2F6936439%2Fdigital-photographer-magazine-subscription.thtml" target="_blank">Digital Photographer</a> and Professional Imagemaker, as well as hosting workshops and talks at <a href="https://www.photographyshow.com/" target="_blank">The Photography Show</a>.&nbsp;He also serves as a judge for the Red Bull Illume Photo Contest. An Olympus and Canon shooter, he has a wealth of knowledge on cameras of all makes –&nbsp;and a fondness for vintage lenses and instant cameras.</p></div>

<div>
<h4>Related articles</h4>

</div>
</section>





</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Copilot key will eventually be required in new PC keyboards (108 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2024/01/ai-comes-for-your-pcs-keyboard-as-microsoft-adds-dedicated-copilot-key/</link>
            <guid>38881839</guid>
            <pubDate>Fri, 05 Jan 2024 17:31:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2024/01/ai-comes-for-your-pcs-keyboard-as-microsoft-adds-dedicated-copilot-key/">https://arstechnica.com/gadgets/2024/01/ai-comes-for-your-pcs-keyboard-as-microsoft-adds-dedicated-copilot-key/</a>, See on <a href="https://news.ycombinator.com/item?id=38881839">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      key change    —
</h4>
            
            <h2 itemprop="description">Copilot key will eventually be required in new PC keyboards, though not yet.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/01/copilot-key-800x450.jpg" alt="A rendering of Microsoft's Copilot key, as seen on a Surface-esque laptop keyboard.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/01/copilot-key.jpg" data-height="1080" data-width="1920">Enlarge</a> <span>/</span> A rendering of Microsoft's Copilot key, as seen on a Surface-esque laptop keyboard.</p><p>Microsoft</p></figcaption>  </figure>

  




<!-- cache hit 173:single/related:88ddd4e64d064253a812495070227f7a --><!-- empty -->
<p>Microsoft pushed throughout 2023 to <a href="https://arstechnica.com/gadgets/2023/06/windows-11s-copilot-brings-ai-chat-to-desktops-in-first-public-preview/">add generative AI capabilities to its software</a>, even extending its new <a href="https://www.microsoft.com/en-us/windows/copilot-ai-features">Copilot AI assistant</a> to Windows 10 <a href="https://arstechnica.com/gadgets/2023/11/microsoft-is-revisiting-windows-10-by-backporting-the-copilot-ai-assistant/">late last year</a>. Now, those efforts to transform PCs at a software level is extending to the hardware: Microsoft is <a href="https://blogs.windows.com/windowsexperience/?p=178614">adding a dedicated Copilot key</a> to PC keyboards, adjusting the standard Windows keyboard layout for the first time since the Windows key first appeared on its <a href="https://en.wikipedia.org/wiki/Microsoft_ergonomic_keyboards#/media/File:MicrosoftNaturalKeyboardGen1.jpg">Natural Keyboard in 1994</a>.</p>
<p>The Copilot key will, predictably, open up the Copilot generative AI assistant within Windows 10 and Windows 11. On an up-to-date Windows PC with Copilot enabled, you can currently do the same thing by pressing Windows + C. For PCs without Copilot enabled, including those that aren't signed into Microsoft accounts, the Copilot key will open Windows Search instead (though this is sort of redundant, since pressing the Windows key and then typing directly into the Start menu also activates the Search function).</p>
<p>A <a href="https://www.youtube.com/watch?v=S1R08Qx6Fvs">quick Microsoft demo video</a> shows the Copilot key in between the cluster of arrow keys and the right Alt button, a place where many keyboards usually put a menu button, a right Ctrl key, another Windows key, or something similar. The exact positioning, and the key being replaced, may vary depending on the size and layout of the keyboard.</p>
<p>We asked Microsoft if a Copilot key would be required on OEM PCs going forward; the company told us that the key isn't mandatory now, but that it expects Copilot keys to be required on Windows 11 keyboards "over time." Microsoft often imposes <a href="https://techcommunity.microsoft.com/t5/windows-hardware-certification/windows-hardware-compatibility-program-guidance-for-windows-11/ba-p/3938433">some additional hardware requirements</a> on major PC makers that sell Windows on their devices, beyond what is strictly necessary to run Windows itself.</p>                                            
                                                        

<p>If nothing else, this new key is a sign of how much Microsoft wants people to use Copilot and its other generative AI products. Plenty of past company initiatives—Bing, Edge, Cortana, and the Microsoft Store, to name a few—never managed to become baked into the hardware like this. In the Windows 8 epoch, Microsoft required OEMs to <a href="https://learn.microsoft.com/en-us/previous-versions/windows/hardware/cert-program/windows-hardware-certification-requirements-for-client-and-server-systems?redirectedfrom=MSDN">build a Windows button into the display bezel</a> of devices with touchscreens, but that requirement eventually disappeared. If Copilot fizzles or is deemphasized the way Cortana was, the Copilot key could become a way to quickly date a Windows PC from the mid-2020s, the way that changes to the Windows logo date keyboards from earlier eras.</p>
<p>We'll definitely see more AI features from Microsoft this year, too—Microsoft Chief Marketing Officer Yusuf Medhi called 2024 "the year of the AI PC" in today's announcement.</p>
<p>Chipmakers like <a href="https://arstechnica.com/gadgets/2023/12/intel-intros-first-meteor-lake-chips-with-faster-gpus-and-worse-single-core-speed/">Intel</a>, <a href="https://arstechnica.com/gadgets/2023/12/amds-new-ryzen-8040-laptop-chips-look-a-lot-like-the-ryzen-7040-cpus/">AMD</a>, and <a href="https://arstechnica.com/gadgets/2023/10/qualcomm-snapdragon-x-elite-looks-like-the-windows-worlds-answer-to-apple-silicon/">Qualcomm</a> are all building neural processing units (NPUs) into their latest silicon, and we'll likely see more updates for Windows apps and features that can take advantage of this new on-device processing capability. Rumors also indicate that we could see a "<a href="https://www.theverge.com/2023/10/7/23907234/intel-windows-12-2024-refresh-launch">Windows 12</a>" release as soon as this year; while Windows 11 has mostly had AI features stacked on top of it, a new OS could launch with AI features more deeply integrated into the UI and apps, as well as additional hardware requirements for some features.</p>
<p>Microsoft says the Copilot key will debut in some PCs that will be announced at the Consumer Electronics Show this month. Surface devices with the revised keyboard layout are "upcoming."</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We don't need a DAC on the ESP32-S3 (125 pts)]]></title>
            <link>https://atomic14.substack.com/p/esp32-s3-no-dac</link>
            <guid>38881416</guid>
            <pubDate>Fri, 05 Jan 2024 17:03:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://atomic14.substack.com/p/esp32-s3-no-dac">https://atomic14.substack.com/p/esp32-s3-no-dac</a>, See on <a href="https://news.ycombinator.com/item?id=38881416">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div id="youtube2-oZ39VCUvKjw" data-attrs="{&quot;videoId&quot;:&quot;oZ39VCUvKjw&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/oZ39VCUvKjw?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p>So, there’s no DAC on the ESP32-S3.</p><p>You would think this would be a bit of a downer if if you want to get audio out and use an analog amplifier.</p><p><span>But it’s actually surprisingly easy to output </span><strong>P</strong><span>ulse </span><strong>D</strong><span>ensity </span><strong>M</strong><span>odulated audio using Sigma Delta Modulation on the ESP32 and you can recover the audio signal by low pass filtering it - an RC filter can be sufficient for this. And that’s what I’m using in the video.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7012915f-2850-4b9c-829a-d1d8fee9b450_248x177.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7012915f-2850-4b9c-829a-d1d8fee9b450_248x177.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7012915f-2850-4b9c-829a-d1d8fee9b450_248x177.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7012915f-2850-4b9c-829a-d1d8fee9b450_248x177.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7012915f-2850-4b9c-829a-d1d8fee9b450_248x177.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7012915f-2850-4b9c-829a-d1d8fee9b450_248x177.png" width="248" height="177" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7012915f-2850-4b9c-829a-d1d8fee9b450_248x177.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:177,&quot;width&quot;:248,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2336,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7012915f-2850-4b9c-829a-d1d8fee9b450_248x177.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7012915f-2850-4b9c-829a-d1d8fee9b450_248x177.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7012915f-2850-4b9c-829a-d1d8fee9b450_248x177.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7012915f-2850-4b9c-829a-d1d8fee9b450_248x177.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Though the Espressif docs do suggest a much better active filter.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef53e813-42ca-4c38-8a11-49c6ab276a70_1303x758.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef53e813-42ca-4c38-8a11-49c6ab276a70_1303x758.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef53e813-42ca-4c38-8a11-49c6ab276a70_1303x758.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef53e813-42ca-4c38-8a11-49c6ab276a70_1303x758.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef53e813-42ca-4c38-8a11-49c6ab276a70_1303x758.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef53e813-42ca-4c38-8a11-49c6ab276a70_1303x758.png" width="1303" height="758" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ef53e813-42ca-4c38-8a11-49c6ab276a70_1303x758.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:758,&quot;width&quot;:1303,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:84826,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef53e813-42ca-4c38-8a11-49c6ab276a70_1303x758.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef53e813-42ca-4c38-8a11-49c6ab276a70_1303x758.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef53e813-42ca-4c38-8a11-49c6ab276a70_1303x758.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef53e813-42ca-4c38-8a11-49c6ab276a70_1303x758.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>It’s pretty interesting to look at a PDM signal and view it in the frequency domain. Here’s a piece of audio along with it’s spectrogram:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52c37f60-67a3-4392-989e-635a3793e882_2560x1440.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52c37f60-67a3-4392-989e-635a3793e882_2560x1440.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52c37f60-67a3-4392-989e-635a3793e882_2560x1440.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52c37f60-67a3-4392-989e-635a3793e882_2560x1440.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52c37f60-67a3-4392-989e-635a3793e882_2560x1440.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52c37f60-67a3-4392-989e-635a3793e882_2560x1440.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/52c37f60-67a3-4392-989e-635a3793e882_2560x1440.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:898112,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52c37f60-67a3-4392-989e-635a3793e882_2560x1440.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52c37f60-67a3-4392-989e-635a3793e882_2560x1440.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52c37f60-67a3-4392-989e-635a3793e882_2560x1440.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52c37f60-67a3-4392-989e-635a3793e882_2560x1440.jpeg 1456w" sizes="100vw"></picture></div></a></figure></div><p>And here’s a simulated PDM version of the original audio. The sample rate of the PDM data is just over 1MHz and I’m showing the spectrogram from 0 to 500KHz</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc97c6c1-fdea-4a61-89f3-40a3085c56a2_2560x1440.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc97c6c1-fdea-4a61-89f3-40a3085c56a2_2560x1440.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc97c6c1-fdea-4a61-89f3-40a3085c56a2_2560x1440.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc97c6c1-fdea-4a61-89f3-40a3085c56a2_2560x1440.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc97c6c1-fdea-4a61-89f3-40a3085c56a2_2560x1440.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc97c6c1-fdea-4a61-89f3-40a3085c56a2_2560x1440.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/dc97c6c1-fdea-4a61-89f3-40a3085c56a2_2560x1440.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:632200,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc97c6c1-fdea-4a61-89f3-40a3085c56a2_2560x1440.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc97c6c1-fdea-4a61-89f3-40a3085c56a2_2560x1440.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc97c6c1-fdea-4a61-89f3-40a3085c56a2_2560x1440.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc97c6c1-fdea-4a61-89f3-40a3085c56a2_2560x1440.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The PDM data just goes from -1 to 1 and changes density depending on the value of the original signal.</p><p>If we just look at the lower 8KHz of the spectrum then we can see what looks like our original signal.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8380d50e-abed-48be-a645-3944ab07abae_2560x1440.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8380d50e-abed-48be-a645-3944ab07abae_2560x1440.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8380d50e-abed-48be-a645-3944ab07abae_2560x1440.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8380d50e-abed-48be-a645-3944ab07abae_2560x1440.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8380d50e-abed-48be-a645-3944ab07abae_2560x1440.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8380d50e-abed-48be-a645-3944ab07abae_2560x1440.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8380d50e-abed-48be-a645-3944ab07abae_2560x1440.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:798744,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8380d50e-abed-48be-a645-3944ab07abae_2560x1440.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8380d50e-abed-48be-a645-3944ab07abae_2560x1440.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8380d50e-abed-48be-a645-3944ab07abae_2560x1440.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8380d50e-abed-48be-a645-3944ab07abae_2560x1440.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>So to recover the original audio we just apply a low pass filter - and hey presto, we have our original audio signal back!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F866f1b69-d4f8-4717-800e-8ad34cb53beb_2560x1440.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F866f1b69-d4f8-4717-800e-8ad34cb53beb_2560x1440.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F866f1b69-d4f8-4717-800e-8ad34cb53beb_2560x1440.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F866f1b69-d4f8-4717-800e-8ad34cb53beb_2560x1440.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F866f1b69-d4f8-4717-800e-8ad34cb53beb_2560x1440.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F866f1b69-d4f8-4717-800e-8ad34cb53beb_2560x1440.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/866f1b69-d4f8-4717-800e-8ad34cb53beb_2560x1440.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1000408,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F866f1b69-d4f8-4717-800e-8ad34cb53beb_2560x1440.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F866f1b69-d4f8-4717-800e-8ad34cb53beb_2560x1440.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F866f1b69-d4f8-4717-800e-8ad34cb53beb_2560x1440.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F866f1b69-d4f8-4717-800e-8ad34cb53beb_2560x1440.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>So how to do we do this on the ESP32?</p><p>There’s a couple of options available to us.</p><p><span>The </span><a href="https://github.com/espressif/esp-idf/tree/b4268c874a4cf8fcf7c0c4153cffb76ad2ddda4e/examples/peripherals/sigma_delta/sdm_dac" rel="">example code</a><span> from Espressif suggests using a timer to output each sample using the </span><strong>sigmadelta_set_duty </strong><span>(you could also just use plain old PWM as well). This does work for audio data, and I’ve got some simple </span><a href="https://github.com/atomic14/esp32-pdm-audio/blob/main/src/audio_output/PDMTimerOuput.cpp" rel="">sample code</a><span> that will do it, but it’s not very efficient - we’re constantly interrupted by a timer to send out the next sample. There’s also quite a lot of code required if you want to stream samples out from some other source.</span></p><p>A much better way is to use the I2S peripheral which can also output PDM data. There are two annoying things with this which are highlighted in the timing diagram from the docs.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673638ce-01af-4b32-8907-7ac64810a82a_1874x447.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673638ce-01af-4b32-8907-7ac64810a82a_1874x447.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673638ce-01af-4b32-8907-7ac64810a82a_1874x447.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673638ce-01af-4b32-8907-7ac64810a82a_1874x447.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673638ce-01af-4b32-8907-7ac64810a82a_1874x447.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673638ce-01af-4b32-8907-7ac64810a82a_1874x447.png" width="1456" height="347" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/673638ce-01af-4b32-8907-7ac64810a82a_1874x447.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:347,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:89482,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673638ce-01af-4b32-8907-7ac64810a82a_1874x447.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673638ce-01af-4b32-8907-7ac64810a82a_1874x447.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673638ce-01af-4b32-8907-7ac64810a82a_1874x447.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F673638ce-01af-4b32-8907-7ac64810a82a_1874x447.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The first issue is that it always wants to output a left and right channel. This is a bit awkward if we just want to feed the PDM signal straight into an analog audio amplifier or headphones. But we can get around this by just outputting the same value for both the left and right channels.</p><p>The second issue is that it always wants to output a clock signal - we don’t really need this. My workaround for this was to just assign the clock to IO45 or IO46 - on the S3 you can’t really use these pins for much as they are strapping pins and it’s best to just leave them alone. But you can use them for outputs once the ESP32 has started up.</p><p><span>There are “proper” PDM amplifier ICs that will take this signal - for example the </span><a href="https://www.analog.com/media/en/technical-documentation/data-sheets/max98358.pdf" rel="">MAX98358</a><span> or the </span><a href="https://www.analog.com/media/en/technical-documentation/data-sheets/SSM2537.pdf" rel="">SSM2537</a><span>.</span></p><p>This all works surprisingly well, you can drive headphones directly from the PDM signal and most analog amplifiers will take the combined stereo PDM signal and will have a low enough bandwidth that they’ll just work.</p><p>You can even just drive a speaker with a really simple half or full bridge and get reasonable audio out (though it may be quite noisy).</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f1cc0a9-8de7-4c4e-89c3-027bd68e69d5_911x921.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f1cc0a9-8de7-4c4e-89c3-027bd68e69d5_911x921.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f1cc0a9-8de7-4c4e-89c3-027bd68e69d5_911x921.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f1cc0a9-8de7-4c4e-89c3-027bd68e69d5_911x921.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f1cc0a9-8de7-4c4e-89c3-027bd68e69d5_911x921.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f1cc0a9-8de7-4c4e-89c3-027bd68e69d5_911x921.png" width="911" height="921" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8f1cc0a9-8de7-4c4e-89c3-027bd68e69d5_911x921.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:921,&quot;width&quot;:911,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:40191,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f1cc0a9-8de7-4c4e-89c3-027bd68e69d5_911x921.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f1cc0a9-8de7-4c4e-89c3-027bd68e69d5_911x921.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f1cc0a9-8de7-4c4e-89c3-027bd68e69d5_911x921.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f1cc0a9-8de7-4c4e-89c3-027bd68e69d5_911x921.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Have a watch of the video and let me know what you think.</p><div id="youtube2-oZ39VCUvKjw" data-attrs="{&quot;videoId&quot;:&quot;oZ39VCUvKjw&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/oZ39VCUvKjw?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Facebook incorrectly reports personal blog to DigitalOcean for phishing (219 pts)]]></title>
            <link>https://social.lol/@robb/111704215593992932</link>
            <guid>38880713</guid>
            <pubDate>Fri, 05 Jan 2024 16:17:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://social.lol/@robb/111704215593992932">https://social.lol/@robb/111704215593992932</a>, See on <a href="https://news.ycombinator.com/item?id=38880713">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Drones are the new drug mules (104 pts)]]></title>
            <link>https://www.vice.com/en/article/qjvma7/drug-trafficking-smugglers-using-drones</link>
            <guid>38880224</guid>
            <pubDate>Fri, 05 Jan 2024 15:44:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vice.com/en/article/qjvma7/drug-trafficking-smugglers-using-drones">https://www.vice.com/en/article/qjvma7/drug-trafficking-smugglers-using-drones</a>, See on <a href="https://news.ycombinator.com/item?id=38880224">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Packets of heroin and a drone confiscated by Indian Border Security Force from near the border outpost of Ranian. Photo: Narinder Nanu/AFP via Getty Images.</p></div><div data-component="BodyComponentRenderer"><p><span data-component="TextBlock"><p>Last week border officials in the Punjab region of India <a href="https://www.hindustantimes.com/cities/chandigarh-news/107-drones-shot-down-442-kg-heroin-seized-in-2023-in-punjab-bsf-101704045498166.html" target="_blank">revealed</a> they intercepted 107 drug-carrying drones sent by smuggling gangs last year over the border from Pakistan, the highest number on record.&nbsp;</p></span><span data-component="TextBlock"><p>Most were carrying heroin or opium from Pakistan to be dropped and received by collaborators in the Punjab, notorious for having India’s <a href="https://www.vice.com/en/article/k7ezwa/what-it-is-like-to-run-a-rehab-centre-in-punjab-india-a-state-with-drug-addiction">worst levels of opiate addiction</a>.&nbsp;</p></span></p><p><span data-component="TextBlock"><p>Last year the head of a police narcotics unit in Lahore, a city in Pakistan which borders the Punjab, was <a href="https://tribune.com.pk/story/2438041/top-cop-dismissed-over-drug-trafficking" target="_blank">dismissed</a> after he was suspected of running a drug trafficking gang sending drones over to India.&nbsp;&nbsp;&nbsp;</p></span><span data-component="TextBlock"><p>But the use of cheap flying robots instead of humans to smuggle drugs across borders is a worldwide phenomenon.&nbsp;</p></span><span data-component="TextBlock"><p>In September the Jordanian air force <a href="https://www.reuters.com/world/middle-east/jordan-downs-two-drones-carrying-drugs-syria-army-statement-2023-09-26/" target="_blank">shot down</a> two drones carrying crystal meth coming from Syria. It was the<a href="https://www.aljazeera.com/news/2023/9/5/jordan-shoots-down-drug-laden-drone-from-syria-in-ninth-incident-this-year" target="_blank"> ninth such drone</a> in 2023, according to Caroline Rose, a director at the New Lines Institute in Washington DC.</p></span><span data-component="TextBlock"><p>Drug smugglers from Syria, <a href="https://www.vice.com/en/article/v7v8k8/syria-captagon-pills-drug-trade">the world’s largest producer of the black market amphetamine pill, captagon</a>, often use Jordan as a transit point to the wider Gulf Arab kingdoms and the global market. Rose thinks Syrian smugglers have increased the use of drones to smuggle captagon and meth due to a security clampdown at the Jordanian border which has made trafficking by land harder.&nbsp;</p></span><span data-component="TextBlock"><p>Drones sent by Mexican cartels carrying drugs such as cocaine, meth, and heroin regularly cross the U.S. border.&nbsp;</p></span><span></span><span data-component="TextBlock"><p>They are being used to shift drugs by air and sea between Africa and Europe.&nbsp;</p></span><span data-component="TextBlock"><p>Spanish police seized <a href="https://www.vice.com/en/article/dyvwmv/look-at-this-huge-drug-drone-seized-by-cops-in-spain">a massive drone with a wingspan of over four metres</a> capable of carrying up to 150 kilograms (330 pounds) of cargo in <a href="https://twitter.com/policia/status/1414875097752653826?s=20" target="_blank">a special compartment</a> in its nose, being used by a French smuggling gang to traffic drugs from Morocco to southern Spain. In 2022, police found three <a href="https://www.bbc.co.uk/news/world-europe-62040790" target="_blank">underwater drones</a> built to smuggle up to 200 kilograms (440 pounds) of drugs across the Strait of Gibraltar between Morocco and Spain.&nbsp;&nbsp;&nbsp;</p></span><span data-component="TextBlock"><p>Drones are being used to smuggle drugs into high security prisons worldwide from Brazil and France to Australia and <a href="https://www.youtube.com/watch?v=BezZxQF2pRw" target="_blank">across the U.S</a>.</p></span><span data-component="TextBlock"><p>In Canada, where 75 percent of prison contraband seizures are attributed to drone drops, there were <a href="https://www.cbc.ca/news/canada/british-columbia/drug-smuggling-drones-1.6822091" target="_blank">700 drone related incidents</a> in two years, including one where an inmate fatally overdosed on fentanyl that had been delivered into his prison by drone.</p></span></p><p><span data-component="TextBlock"><p>In October last year the U.K. government was forced to <a href="https://www.gov.uk/government/news/new-prison-no-fly-zones-for-drug-delivering-drones" target="_blank">introduce no fly zones </a>around all its prisons due to a “sharp increase” in the number of drones carrying drugs and mobile phones into jails.&nbsp;&nbsp;</p></span><span></span><span data-component="TextBlock"><p>Drug gangs are also using drones as eyes in the sky.&nbsp;</p></span><span data-component="TextBlock"><p>In Latin America and the Golden Triangle in Southeast Asia, drug trafficking cartels use them to scout out drug smuggling routes. In the U.K. they have been used by drug stash thieves to seek out rival weed farms and by guerilla weed growers to find suitable spots to set up illegal farms.&nbsp;</p></span><span data-component="TextBlock"><p>They are used by law enforcement too, from heat seeking drones spotting indoor cannabis farms in the U.K., to drones being used by police to catch street drug dealers in Kyrgyzstan in central Asia.</p></span><span data-component="TextBlock"><p>But drones will likely become an everyday part of drug dealing too, according to Peter Warren Singer, author of multiple books on national security and a Fellow at think tank New America, with legit medicines <a href="https://eu.freep.com/story/news/health/2023/03/16/university-of-michigan-medicine-drone-delivery-prescription-drugs-zipline/70013061007/" target="_blank">due to be delivered by drone</a> in the U.S. later this year and <a href="https://www.kcl.ac.uk/news/drones-opioid-overdose-reversal-kits-reach-people-faster-ambulances" target="_blank">maybe in the U.K. too</a>.&nbsp;&nbsp;</p></span><span data-component="TextBlock"><p>“We are just scraping the surface of what is possible, as drone deliveries become more and more common in the commercial world, it will be the same with delivery of illicit goods. In our book, <em>Burn-In</em>, we explain how a future city will see drones zipping about delivering everything from groceries and burritos to drugs, both prescribed by a doctor or bought off a dealer.</p></span><span></span><span data-component="TextBlock"><p>“Drones have traditionally been used by governments and corporations for what are known as the "3 D's" jobs that are too dull, dirty, or dangerous for humans. For criminals, it is the same, except add in another D: Dependable. A drone doesn't steal the product and can't be arrested or snitch if caught.”&nbsp;</p></span></p><p><span data-component="TextBlock"><p>Liam O’Shea, senior research fellow for organised crime and policing at defence and&nbsp;security thinktank RUSI, said drones were at the moment of limited value to wholesale traffickers and organised criminal gangs because of their range and the weight they can carry.&nbsp;</p></span><span data-component="TextBlock"><p>“It makes sense that smugglers would seek to use drones. They are cheap and easy to acquire. They also lower the risks involved in some transactions, as smugglers do not have to be physically present during transactions. They offer opportunities for smuggling in areas where previous routes were too risky, such as prisons and over securitised borders.</p></span><span data-component="TextBlock"><p>“I expect them to be of greater value to smaller players and distributors dealing with smaller quantities. Wholesale drug traffickers will still need to use routes that facilitate smuggling at higher volume or using drones to make multiple trips, which entails risks of detection.&nbsp;</p></span><span data-component="TextBlock"><p>“That may well change as improvements in technology improve drones’ carrying capacity and crime groups are better able to access drones with greater capacity.”</p></span></p></div><div><p><h3>Get the latest from VICE News in your inbox. Sign up right here.</h3></p><p>By signing up, you agree to the<!-- --> <a href="https://vice-web-statics-cdn.vice.com/privacy-policy/en_us/page/terms-of-use.html">Terms of Use</a> <!-- -->and<!-- --> <a href="https://vice-web-statics-cdn.vice.com/privacy-policy/en_us/page/privacy-policy.html">Privacy Policy</a> <!-- -->&amp; to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Archiving Steam games for fun and profit (135 pts)]]></title>
            <link>https://lorendb.dev/posts/archiving-steam-games-for-fun-and-profit/</link>
            <guid>38878830</guid>
            <pubDate>Fri, 05 Jan 2024 13:30:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lorendb.dev/posts/archiving-steam-games-for-fun-and-profit/">https://lorendb.dev/posts/archiving-steam-games-for-fun-and-profit/</a>, See on <a href="https://news.ycombinator.com/item?id=38878830">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
  <article>
    
    <div>
      <blockquote>
<p>tldr: if you want to create an archive of all your Steam games, check out <a href="https://github.com/LorenDB/download-steam-games">download-steam-games</a> on my GitHub. It has minimal documentation, but you should be able to get it running without too much trouble.</p>
</blockquote>
<p>I recently decided it would be cool to download some beta versions of <a href="https://www.kerbalspaceprogram.com/games-kerbal-space-program-2">KSP2</a> so I can revisit them in a year or so and laugh about how many terrible bugs there were in the original version of the game. I decided that it would make sense to download the game backup directly to my server, so I <code>ssh</code>ed into the server, installed Steam from the system repositories, and started a new <code>ssh -X</code> session. However, when I launched Steam, I realized that my server’s connection to the internet is much too slow to run <code>ssh -X</code> in any sort of usable state. In fact, it was so slow that it apparently timed something out on Steam’s end and made it practically impossible to log in to Steam.</p>
<p>Undaunted, I started looking into <a href="https://developer.valvesoftware.com/wiki/SteamCMD">SteamCMD</a>. It turns out that while SteamCMD is intended for game server administrators, it will happily download any Steam game. It didn’t take me long to install it and use it to download a game. However, it’s a bit of a pain running through a set of instructions in SteamCMD again and again for downloading multiple games, so I set out to script it.</p>
<h2 id="but-why-would-you-do-this">But why would you do this?</h2>
<p>Why wouldn’t I do this? If you think I’m crazy, just go read <a href="https://www.reddit.com/r/DataHoarder/">r/DataHoarder</a> for a while.</p>
<p>Seriously, while I don’t expect Steam to disappear tomorrow, there is precedent for games to be removed from the platform; many of the original games on the platform have been removed. The oldest game still available (counting by which date it was made available on Steam) is the original <a href="https://store.steampowered.com/app/10">Counterstrike</a> with a game ID of 10. Games 1 through 9 aren’t on the platform anymore, and I’m sure they are far from the only games deleted from the platform. Archiving games allows me to avoid losing games that are removed from Steam. Also, sometimes enthusiasts like to be able to access old versions of games; one of the top posts of all time on r/DataHoarder is about <a href="https://www.reddit.com/r/DataHoarder/comments/o9cnj3/one_womans_quest_to_never_delete_anything_allowed/">somebody who hoarded an old Minecraft alpha</a>, giving Minecraft enthusiasts a chance at taking a more complete look at the evolution of the game.</p>
<h2 id="a-naïve-bash-script">A naïve bash script</h2>
<p>My first approach was to create a bash script to wrap SteamCMD with a nice syntax. Of course, I also wanted to bypass the interactive prompt. Let’s go through what I ended up building:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>STEAM_ACCOUNT<span>=</span>
</span></span><span><span>GAME_NAME<span>=</span>
</span></span><span><span>GAME_STEAM_ID<span>=</span>
</span></span><span><span>GAME_FORCE_WINDOWS<span>=</span>
</span></span><span><span>GAME_BETA<span>=</span>
</span></span></code></pre></div><p>These variables control some basic stuff. <code>STEAM_ACCOUNT</code> is the account you logged into SteamCMD with; while SteamCMD caches credentials, you still have to issue the <code>login &lt;username&gt;</code> command every time you start it, so the script has to know what username to use. <code>GAME_NAME</code> doesn’t have to be the actual game name; it’s just used to generate the filename for the final game archive. <code>GAME_STEAM_ID</code> is the numerical ID of the game you want to download; for example, KSP2 is <a href="https://store.steampowered.com/app/954850/Kerbal_Space_Program_2/"><code>954850</code></a>.</p>
<p><code>GAME_FORCE_WINDOWS</code> deserves a more detailed explanation. While I run Linux, some games do not have Linux builds available. For those games, I would like to save a backup of the Windows build instead. This will let me run the game via Proton later. Conveniently, SteamCMD allows you to override what platform you download the game for by setting <code>@sSteamCmdForcePlatformType &lt;platform&gt;</code>. <code>GAME_FORCE_WINDOWS</code> is used later as part of the SteamCMD script contents; therefore, if I want to add a Windows override, I can set <code>GAME_FORCE_WINDOWS</code> to <code>@sSteamCmdForcePlatformType windows</code>.</p>
<p>Finally, we have <code>GAME_BETA</code>. <a href="https://steamdb.info/">SteamDB</a> shows beta version identifiers for games; you can use those to download earlier versions of some games. This is something I want to use, given that the original purpose of this project was to archive early KSP2 builds.</p>
<p>The next bit of the script is simply logic to set each of these variables based on command line parameters. I’ve decided to skip it for brevity, as it is trivial bash logic. Moving on, we get this:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>cat &gt; ~/.download-$GAME_NAME.txt <span>&lt;&lt; EOF
</span></span></span><span><span><span>$GAME_FORCE_WINDOWS
</span></span></span><span><span><span>force_install_dir $HOME/Steam/downloads/$GAME_NAME
</span></span></span><span><span><span>login $STEAM_ACCOUNT
</span></span></span><span><span><span>app_update $GAME_STEAM_ID $GAME_BETA validate
</span></span></span><span><span><span>quit
</span></span></span><span><span><span>EOF</span>
</span></span></code></pre></div><p>SteamCMD can be controlled by scripts. This is how we’re going to get around its interactive interface.</p>
<p>The first line of the script is the aforementioned platform override. After that, we use <code>force_install_dir</code> to force SteamCMD to download the game to a known location. In this version of the script, I’ve hardcoded it to <code>~/Steam/downloads</code>, since I’ve installed SteamCMD to <code>~/Steam</code>. Then we log in and issue the actual command to grab the app; once the app has downloaded, we quit SteamCMD.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>PREVIOUS_DIR<span>=</span><span>$(</span>pwd<span>)</span>
</span></span><span><span>cd ~/Steam
</span></span><span><span>~/Steam/steamcmd.sh +runscript $HOME/.download-$GAME_NAME.txt
</span></span><span><span>cd $PREVIOUS_DIR
</span></span><span><span>rm ~/.download-$GAME_NAME.txt
</span></span></code></pre></div><p>Here we actually execute the script and then delete it to prevent clutter.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>PREVIOUS_DIR<span>=</span><span>$(</span>pwd<span>)</span>
</span></span><span><span>cd ~/Steam/downloads
</span></span><span><span>tar --use-compress-program<span>=</span>pigz -cf $PREVIOUS_DIR/$GAME_NAME.tar.gz $GAME_NAME
</span></span><span><span>cd $PREVIOUS_DIR
</span></span><span><span>
</span></span><span><span>rm -r $HOME/Steam/downloads/$GAME_NAME
</span></span></code></pre></div><p>The final step is to compress the game folder into a single <code>.tar.gz</code> file. Again, we’re dumping the final archive into <code>~/Steam/downloads</code> for now. The only really unusual part of this is that I’m telling <code>tar</code> to use <code>pigz</code> to do the gzip compression. This is because <code>pigz</code> will use all CPU cores, which is important when you’re archiving many gigabytes of game data.</p>
<p>This is all fine and good, but you still have to manually run the command for each game. To mitigate this, I created a <a href="https://paste.segfault.foo/loren/ac1cfff462804e70bdf85fb764328b19">simple script</a> that read a vaguely CSV-like file which listed games to download. That was definitely better than nothing, but it still felt pretty hacky.</p>
<h2 id="building-it-the-right-way">Building it the right way</h2>
<p>I decided to port my two scripts into one app. I chose to write the app in <a href="https://dlang.org/">D</a>; in retrospect, Python would probably have been a good choice simply because it comes preinstalled on pretty much every distro, but I much prefer D for pretty much anything. My ported script is pretty similar to the functionality shown above; however, I added various improvements. Paths are no longer hardcoded, it’s possible to download games for any and all platforms (Windows, macOS, and Linux), and game data is stored as a JSON file instead of in a weird custom-ish format.</p>
<p>The app, which I’m calling <code>download-steam-games</code> (how original), is available <a href="https://github.com/LorenDB/download-steam-games">on GitHub</a>. Building it is a simple matter of running <code>dub build</code>. After that, you need to run <code>dub run -- --add-game</code> to add a game to the download list. After you’ve added some games, you can use <code>dub run</code> to run the application in download mode. It will download all your games for every platform you’ve specified, <code>.tar.gz</code> them, and put them in the output folder of your choosing.</p>
<h2 id="limitations">Limitations</h2>
<p>The app currently is somewhat limited: there’s no functionality to reconfigure the settings, you can’t specify beta versions with <code>--add-game</code>, and there’s no progress reporting during download. However, I think it’s robust enough to actually use in production. I hope to address all of these issues soon, and while I’m at it, I’d like to add a feature to add a timestamp to the archive name so you can download many versions of the same game over time.</p>
<h2 id="conclusion">Conclusion</h2>
<p>This is a fun project that is helping get me hooked on data hoarding. If you start using my app to help hoard your own games, please leave a comment below! I’d love seeing how many gigabytes (or terabytes) of games you’re hoarding.</p>
<h2 id="updates">Updates</h2>
<p>Shortly after writing this post, I’ve added some logging to the app. It ain’t pefect, but it ain’t terrible either.</p>







    </div>
    
  </article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Duty to Document (2023) (143 pts)]]></title>
            <link>https://nicolasbouliane.com/blog/duty-to-document</link>
            <guid>38878779</guid>
            <pubDate>Fri, 05 Jan 2024 13:22:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nicolasbouliane.com/blog/duty-to-document">https://nicolasbouliane.com/blog/duty-to-document</a>, See on <a href="https://news.ycombinator.com/item?id=38878779">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
    <article>
        <h2>
            The duty to document
            <small>Posted on <time datetime="2023-05-02">May 2, 2023</time></small>
        </h2>
        
        <p>If you learn something the hard way, share your findings with others. You have blazed a new trail; now you must mark it for your fellow travellers. Sharing knowledge is an unreasonably effective way of helping others.</p>
<picture><source srcset="https://nicolasbouliane.com/images/content2x/annotated-map.jpg 1848w, https://nicolasbouliane.com/images/content1x/annotated-map.jpg 924w, https://nicolasbouliane.com/images/content0.75x/annotated-map.jpg 690w" type="image/jpeg"><source srcset="https://nicolasbouliane.com/images/content2x/annotated-map.webp 1848w, https://nicolasbouliane.com/images/content1x/annotated-map.webp 924w, https://nicolasbouliane.com/images/content0.75x/annotated-map.webp 690w" type="image/webp"><img alt="Hiking map with hand-drawn annotations" height="842" loading="lazy" src="https://nicolasbouliane.com/images/content2x/annotated-map.jpg" width="1842"></picture>
<p>I owe a debt of gratitude to those whose knowledge helped me debug software, repair bicycles, choose camping gear, start a business, and deal with anything life throws at me.</p>
<p>I repay that debt by marking my own trails. When I think “it should not have been this hard to find out”, I make it easier to find out. Over the years, I have documented everything from <a href="https://nicolasbouliane.com/blog/ffmpeg-extract-subtitles">ffmpeg incantations</a> to <a href="https://www.openstreetmap.org/changeset/72463537">Uzbek petrol stations</a>, and made a career out of <a href="https://nicolasbouliane.com/projects/all-about-berlin">documenting German bureaucracy</a>.</p>
<p>I figure that if you have knowledge that could benefit thousands of people, and it costs you next to nothing to share that knowledge, it’s your duty to do it.</p>

<ul>
<li><a href="https://nicolasbouliane.com/blog/maps">A map for everything</a></li>
</ul>
    </article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Consumer Reports finds 'widespread' presence of plastics in food (294 pts)]]></title>
            <link>https://www.reuters.com/business/healthcare-pharmaceuticals/consumer-reports-finds-widespread-presence-plastics-food-2024-01-04/</link>
            <guid>38878683</guid>
            <pubDate>Fri, 05 Jan 2024 13:10:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/business/healthcare-pharmaceuticals/consumer-reports-finds-widespread-presence-plastics-food-2024-01-04/">https://www.reuters.com/business/healthcare-pharmaceuticals/consumer-reports-finds-widespread-presence-plastics-food-2024-01-04/</a>, See on <a href="https://news.ycombinator.com/item?id=38878683">Hacker News</a></p>
Couldn't get https://www.reuters.com/business/healthcare-pharmaceuticals/consumer-reports-finds-widespread-presence-plastics-food-2024-01-04/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Everything will be alright in Iceland (147 pts)]]></title>
            <link>https://memoirsandrambles.substack.com/p/everything-will-be-alright-in-iceland</link>
            <guid>38878640</guid>
            <pubDate>Fri, 05 Jan 2024 13:03:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://memoirsandrambles.substack.com/p/everything-will-be-alright-in-iceland">https://memoirsandrambles.substack.com/p/everything-will-be-alright-in-iceland</a>, See on <a href="https://news.ycombinator.com/item?id=38878640">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132c825c-729d-48b2-937d-8745f1c63236_2781x2086.jpeg" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132c825c-729d-48b2-937d-8745f1c63236_2781x2086.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132c825c-729d-48b2-937d-8745f1c63236_2781x2086.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132c825c-729d-48b2-937d-8745f1c63236_2781x2086.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132c825c-729d-48b2-937d-8745f1c63236_2781x2086.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132c825c-729d-48b2-937d-8745f1c63236_2781x2086.jpeg" width="1456" height="1092" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/132c825c-729d-48b2-937d-8745f1c63236_2781x2086.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1092,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1493519,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132c825c-729d-48b2-937d-8745f1c63236_2781x2086.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132c825c-729d-48b2-937d-8745f1c63236_2781x2086.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132c825c-729d-48b2-937d-8745f1c63236_2781x2086.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132c825c-729d-48b2-937d-8745f1c63236_2781x2086.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Picture of Fagradalsfjall taken by me in 2021</figcaption></figure></div><p>I was recently asked one of the best questions someone who's lived in a bunch of countries could be asked, surprisingly for the first time:</p><p>What did you learn from each place you lived in? What do people do there that you think could be applied elsewhere?</p><p>For Iceland, I believe I said something about people's contact with nature. Just about every Icelandic person has been on a hike, and they all seem to have a level of admiration and care for nature that's not as widespread elsewhere.</p><p>But in light of the most recent volcanic eruption, I got to thinking about something else.</p><p><span>There's a saying in Icelandic - </span><em>þetta reddast</em><span> - that means "everything will be alright", and, while many languages have a similar expression, this one is really embedded in Icelandic culture.</span></p><p>I remember the first time I felt an earthquake in my life, back when I lived there. I was reasonably worried, and started remembering things I'd seen as a child.</p><p><span>"</span><em>Should we get under a table?</em><span>" — I asked my Icelandic girlfriend at the time. She didn't seem too phased, and before she had a chance to answer, the quake stopped. "</span><em>Well, I guess it's all good now</em><span>".</span></p><p>There was also the time when I was convinced to “walk up” (turns out it was actually climb up) one of the ice pinnacles atop of Snaefellsjökull. At the time I had no experience with crampons or an ice axe at all, yet the group of also amateurs from my ex-girlfriend's company were all pretty chill about the situation and pitched it as a seemingly easy thing.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b9b5124-f0eb-4940-bae1-452cc71552a4_3000x4000.jpeg" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b9b5124-f0eb-4940-bae1-452cc71552a4_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b9b5124-f0eb-4940-bae1-452cc71552a4_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b9b5124-f0eb-4940-bae1-452cc71552a4_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b9b5124-f0eb-4940-bae1-452cc71552a4_3000x4000.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b9b5124-f0eb-4940-bae1-452cc71552a4_3000x4000.jpeg" width="1456" height="1941" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0b9b5124-f0eb-4940-bae1-452cc71552a4_3000x4000.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2816635,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b9b5124-f0eb-4940-bae1-452cc71552a4_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b9b5124-f0eb-4940-bae1-452cc71552a4_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b9b5124-f0eb-4940-bae1-452cc71552a4_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b9b5124-f0eb-4940-bae1-452cc71552a4_3000x4000.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Snaefellsjökull, 2022.</figcaption></figure></div><p>On the way up there, quite a few of the people got scared, and I for one nearly lost my oversized crampon, which came off my foot and caused me to slide down this steep ice slope that led straight into a crevasse at the bottom. I managed to stop myself with an ice axe self-arrest, saved by having used my free time in the past to watch alpinism videos with self-arrest tutorials.</p><p>The thing is that living in Iceland requires a unique type of mentality, even if the Icelanders themselves don't necessarily think about it much.</p><p>To live essentially on top of a volcano or with a massive volcano nearby requires a mix of belief that everything will be okay, but also determination and preparedness to make it be so.</p><p>And in some ways you almost need to just to be willing to start over.</p><p>Before I lived there, the first time I visited Iceland I stayed in this farmer's cabin in the South, and he told us about how despite him being okay and his house staying up, he lost all of his sheep due to the ash from the 2010 Eyjafjallajökull eruption. </p><p>If you don't believe everything will be alright in the end - how do you keep going?</p><p>Yet despite the expression seemingly tossing the responsibility of making things good up to fate, it's also your responsibility to make it alright.</p><p>I was once told Icelandic consultants are particularly wanted during crises. They operate well in the post-catastrophe chaos, bringing both a necessary optimism and also an ability to prioritize and stay rational.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bbd682-cb63-4233-8ffa-bebc54142773_3000x4000.jpeg" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bbd682-cb63-4233-8ffa-bebc54142773_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bbd682-cb63-4233-8ffa-bebc54142773_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bbd682-cb63-4233-8ffa-bebc54142773_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bbd682-cb63-4233-8ffa-bebc54142773_3000x4000.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bbd682-cb63-4233-8ffa-bebc54142773_3000x4000.jpeg" width="1456" height="1941" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d6bbd682-cb63-4233-8ffa-bebc54142773_3000x4000.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4415224,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bbd682-cb63-4233-8ffa-bebc54142773_3000x4000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bbd682-cb63-4233-8ffa-bebc54142773_3000x4000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bbd682-cb63-4233-8ffa-bebc54142773_3000x4000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bbd682-cb63-4233-8ffa-bebc54142773_3000x4000.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Me with Fagradalsfjall (maybe too close) in the back.</figcaption></figure></div><p>The Fagradalsfjall eruption in 2021, which happened while I was still living there, marked the end of around 800 years of no volcanic activity in the Reykjanes peninsula, an area of significant economic importance that's also dangerously close to where the vast majority of Icelanders live - the Reykjavík capital region.</p><p>But having some of the world's best geologists and volcanologists in the world, a well-trained Search &amp; Rescue team, and a conscious population means Iceland feels ready to deal with this, and that everything will be okay. </p><p>You see this from how fast Grindavík, a town potentially in danger, was evacuated ahead of this 2023 eruption, an event somewhat analogous to the heroic effort of the Heimaey evacuation in 1973, where an entire island was evacuated in just a few hours after the alarm sounded for a surprise eruption. Plus they also managed to save the harbor, by pumping seawater and spraying it into the lava to redirect it.</p><p>I say all this to say that Icelanders are possibly better prepared than most of us to deal with life. </p><p>Because the Icelandic antifragility built up from generations of living in an island that seems to not want you there should not be mistaken as something applicable only in their little island.</p><p>It has made Icelanders well-prepared to deal with an inevitable part of all of our lives — crisis.</p><p>By contrast, Brazilians love to mention how we have no natural disasters. We don't get earthquakes, have no active volcanoes, no hurricanes, nothing.</p><p>Yet every year thousands of homes are lost, people die, and infrastructure is damaged just from the repercussions of the comparatively predictable rainy season.</p><p>I'm sure that both for our country, but also our personal lives, we have a lot to learn. </p><p><span>Because in my experience, despite its inherent optimism, </span><em>þetta reddast </em><span>often</span><em> </em><span>doesn’t imply “it’ll be taken care of”, but rather “we’ll take care of it”.</span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[June 30th, 2024, will bring the End of Life (EOL) of CentOS Linux (2023) (201 pts)]]></title>
            <link>https://www.redhat.com/en/blog/fastest-road-centos-linux-red-hat-enterprise-linux</link>
            <guid>38878587</guid>
            <pubDate>Fri, 05 Jan 2024 12:56:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.redhat.com/en/blog/fastest-road-centos-linux-red-hat-enterprise-linux">https://www.redhat.com/en/blog/fastest-road-centos-linux-red-hat-enterprise-linux</a>, See on <a href="https://news.ycombinator.com/item?id=38878587">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-eq-pts="xxs-hr: 0, md-hr: 360, lg-hr: 450">
        <p><strong>June 30th, 2024</strong>. Before you read anything else, commit that date to memory.</p>

<p>June 30th, 2024, will bring the End of Life (EOL) of CentOS Linux, and Red Hat Enterprise Linux 7 (RHEL 7) will be reaching End of Maintenance (EOM). The good news is that these events won’t require a complete infrastructure overhaul. Tools are available to move from your current configuration to a place where you’ll have years of support.</p>

<p>While June of ‘24 may sound a ways off, do not delay. It will be here faster than you think. Start planning now. Start moving soon. Give yourself plenty of runway, and don’t forget that we aren’t just your software vendor at Red Hat. We are your partners and are here to help you with these transitions.</p>

<p>If you’re like me when looking at a long-term, intensive project, some time is required to stare at a blank screen and process the task before me. Let me see if I can help drive you into action.</p>

<h2>The hard way</h2>

<p>In my days as a systems administrator, we didn’t consider in-place conversions or upgrades. For one thing, it wasn’t the straightforward process we have today. Secondly, new operating systems and hardware retirements went hand in hand. New OS? Time for a new server model! There are specific environments where that is the case.&nbsp;</p>

<p>You can certainly buy new hardware (or spin up new cloud instances) on the latest versions of RHEL and do a “rip-and-replace,” moving over only your application data. But for many, this isn’t an ideal approach. What other options are there?</p>

<p>If you want to keep your application on the same version and focus solely on the operating system for this project, you can do that too. Look at setting up a RHEL 8 or 9 server and running it as a container host for Podman. Put your applications into a container, and viola. You can build your applications into pre-built application images or <a href="https://catalog.redhat.com/software/containers/search?vendor_name=Red%20Hat&amp;p=1&amp;q=UBI">Universal Base Images (UBI)</a>.</p>

<p>However, this will require a fair amount of work as well. There is an easier way. In what amounts to a 2-phase process, we can convert your CentOS Linux systems onto a supported version of RHEL and then execute an in-place upgrade.&nbsp;</p>

<h2>Convert from CentOS Linux</h2>

<p><strong>Pop Quiz</strong>: When does CentOS Linux 7.9 go EOL?</p>

<p>If you said June 30th, 2024, then I can write the rest of this blog post feeling accomplished!&nbsp;</p>

<p>Red Hat has created a way for users to move to a supported operating system in place using a supported process.</p>

<p>Why in-place? Think of all the configurations, user home directories, processes and packages you already have running on these systems. An in-place upgrade means all that customization doesn’t go away. The other piece of good news is that the conversion process and the resulting server image are both supported by Red Hat! In other words, if something breaks during or after the conversion, you can open a support ticket and get the help you need.</p>

<p>Now, we are looking at the final days of CentOS Linux. You’ve got hundreds (or even thousands) of servers running various minor releases of CentOS Linux 7. Let’s walk through what a conversion process looks like:</p>

<p><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/xX7P4BzOcNg" title="YouTube video player" width="560"></iframe></p>

<ol>
<li aria-level="1">The first thing to do, if you are running anything older than CentOS Linux 7.9, is to upgrade all of your packages to the latest minor release.&nbsp;</li>
<li aria-level="1">Once you are running CentOS Linux 7.9 with all the packages on their latest available version, you can configure the convert2rhel repository.</li>
<li aria-level="1">Run the conversion!</li>
<li aria-level="1">Validate your applications and register your systems to <a href="https://www.redhat.com/en/technologies/management/insights">Red Hat Insights</a>.</li>
<li aria-level="1">Brace yourself— we aren’t done just yet.</li>
</ol>

<p>If you want to get your hands dirty, we have a <a href="https://www.redhat.com/en/interactive-labs/migrate-red-hat-enterprise-linux-centos-linux">Convert2RHEL lab</a> on our website to try for yourself!</p>

<h2>In-place upgrade</h2>

<p>Time for a review! When does RHEL 7 go EOM? If you said June 30th, 2024, you nailed it!</p>

<p>Now, you should be looking at a fleet of systems running RHEL 7.9. Perhaps you already had a group of systems running earlier releases of RHEL 7. Now would be an excellent time to patch those to the latest available package set and the recently converted CentOS Linux systems.&nbsp;</p>

<p>Much like CentOS Linux, RHEL 7 has limited life left. Red Hat does offer Extended Lifecycle Support (ELS) subscriptions if that is a path of interest. However, these entitlements come with an added cost. For today’s thought exercise, we are looking at how to keep your systems feeling fresh (read supported) without added expense or the need to rip-and-replace later.</p>

<p><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/VVVwg9IyqwQ" title="YouTube video player" width="560"></iframe></p>

<ol>
<li aria-level="1">Once the ‘yum’ command says there are no available packages left to update, your RHEL systems are ready to upgrade to the latest version of RHEL 8.</li>
<li aria-level="1">Install the Leapp tools.</li>
<li aria-level="1">Run the pre-upgrade assessment and remediate any identified issues.</li>
<li aria-level="1">Run the in-place upgrade.</li>
<li aria-level="1">Validate the upgraded systems and their applications.</li>
</ol>

<p>That is all there is to it! Sit back and take a deep breath. Even after June 2024, RHEL 8 will still have five years of maintenance support.&nbsp;</p>

<p>If you’d like to try an in-place upgrade for yourself, we have a <a href="https://www.redhat.com/en/interactive-labs/perform-in-place-upgrade-with-leapp">self-paced lab</a> over on our website.</p>

<h2>Wrap up</h2>

<p>June 30th, 2024…</p>

<p>Set a countdown on your phone. Put a reminder on your desktop. Grab a sticky note and put it on your mirror. Whatever you have to do. The date is rapidly approaching, and as a former sysadmin, I don’t want to see you having to scramble at the last minute to find ways to support your systems.</p>

<p>June 30th, 2024…</p>

<p>My experiences were always building a new server, with a new OS, on a new hardware platform. In the perfect world that exists only in my imagination, I might consider one last full-scale replacement and go straight to RHEL 9, then keep pace with the in-place upgrades for RHEL 10 and beyond.</p>

<p>Sadly, technology, applications, industries and organizations don’t always march to that drum. Complex dependencies, processes and resource availabilities limit our ability to stay on the “latest and greatest.”</p>

<p>If that is the position you find yourself in, follow the links in this blog, watch our videos and engage our <a href="https://access.redhat.com/">Support </a>and <a href="https://www.redhat.com/en/services/consulting">Services</a> organizations. Red Hat is your partner in this.</p>

<h2>Further reading</h2>

<ul>
<li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/converting_from_an_rpm-based_linux_distribution_to_rhel/index">Converting from an RPM-based Linux distribution to RHEL</a></li>
<li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/upgrading_from_rhel_7_to_rhel_8/index#doc-wrapper">Upgrading from RHEL 7 to RHEL 8</a></li>
</ul>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An app can be a home-cooked meal (828 pts)]]></title>
            <link>https://www.robinsloan.com/notes/home-cooked-app/</link>
            <guid>38877423</guid>
            <pubDate>Fri, 05 Jan 2024 10:03:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.robinsloan.com/notes/home-cooked-app/">https://www.robinsloan.com/notes/home-cooked-app/</a>, See on <a href="https://news.ycombinator.com/item?id=38877423">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<stamp-thwack>
<label>From:</label> Robin Sloan<br>
<label>To:</label> the lab<br>
<label>Sent:</label> February 2020
</stamp-thwack>

<p>Have you heard about this new app called&nbsp;BoopSnoop?</p>
<p>It launched in the first week of January 2020, and almost immediately, it was down­loaded by four people in three different time zones. In the years since, it has remained steady at four daily active users, with zero churn: a resounding success, exceeding every one of its creator’s&nbsp;expectations.</p>
<p>🙂</p>
<p>I&nbsp;made a messaging app for, and with, my family. It is ruth­lessly simple; we love it; no one else will ever use it. I&nbsp;wanted to share a few notes about how and why I&nbsp;made it, both to (a) offer a nudge to anyone else consid­ering a similar project, and (b) suggest something a little larger about&nbsp;software.</p>
<figure>
<video loop="" muted="" autobuffer="" playsinline="" controls="">
<!-- <source src="/media/boopsnoop-smile-faststart.webm" type="video/webm"> -->
<source src="https://www.robinsloan.com/media/boopsnoop-smile-faststart.mp4" type="video/mp4">
Your browser can’t play my video clip. Rats!
</video>
<figcaption>
Tap or click to unmute.</figcaption>
</figure>
<h2>Barely there</h2>
<p>My story begins with another app, now defunct, called&nbsp;Tapstack.</p>
<p>Opening the app, you saw a live feed from your phone’s camera. Below, a grid of faces, some of them representing individuals, others repre­senting groups. My grid had four cells: my mom, my dad, my sister, and a group collecting all three. Just like Snapchat or Instagram, you tapped to capture a photo, pressed to record a video. As soon as you lifted your finger, your message zipped away, with no editing, no reviewing. A “stack” of messages awaited you in the corner, and, after you tapped through them, they were&nbsp;discarded.</p>
<p>It was all so simple that it was barely there. Tapstack more closely approx­i­mated a clear pane of glass than any app I’ve ever&nbsp;used.</p>
<p>For several years, Tapstack was the main channel for my family’s communication. The app didn’t lend itself to practical corre­spon­dence or logis­tical coordination; its strength was ambient presence. I&nbsp;met one of Tapstack’s designers once, and they told me it seemed espe­cially popular with far-flung families: a diaspora app. Because there was no threading and no history, messages didn’t carry the burden of an expected reply. Really, they were just a carrier wave for another sentiment, and that sentiment was always the same: I’m thinking of&nbsp;you.</p>
<p>A selfie with coffee, a picture of an ice-covered pond, a video of my nephews acting silly: I’m thinking of you, I’m thinking of you, I’m thinking of&nbsp;you.</p>
<p>It never seemed to me that Tapstack attracted a huge number of users. I&nbsp;don’t know if the company ever made a cent. There was no adver­tising in the app, and they never asked their users to&nbsp;pay.</p>
<p>Why didn’t they ask us to&nbsp;pay?</p>
<p>In 2019, I&nbsp;felt a rising dread as the months ticked by and the app didn’t receive a single update. Sure enough, in the fall, Tapstack announced that it was shutting down. It offered its users a way to export their data. It went&nbsp;gracefully.</p>
<p>It was, I&nbsp;have to say, a really great&nbsp;app.</p>
<h2>Here comes a new challenger</h2>
<p>My family all agreed we were going to need a replacement, and while my first instinct was to set up a group on Instagram or WhatsApp, the prospect of having our warm channel <span>surrounded — </span><wbr>encroached <span>upon — </span><wbr>by all that other garbage made me feel even sadder than the prospect of losing&nbsp;Tapstack.</p>
<p>So, instead of settling for a corporate messaging app … </p>
<p>I&nbsp;built one just for&nbsp;us.</p>
<p>I’ll show you the screen capture again, but the point is that there’s not much to show. The app is a “magic window” that captures photos and videos and shuttles them around. Messages wait in a queue and, once <span>viewed — </span><wbr>always full-screen, with no distractions, no prods to comment or <span>share — </span><wbr>they disappear. That is literally it. The app has basically no interface. There’s a camera button and a badge in the corner, calm green, that indicates how many messages are&nbsp;waiting.</p>
<figure>
<video loop="" muted="" autobuffer="" playsinline="" controls="">
<!-- <source src="/media/boopsnoop-smile-faststart.webm" type="video/webm"> -->
<source src="https://www.robinsloan.com/media/boopsnoop-smile-faststart.mp4" type="video/mp4">
Your browser can’t play my video clip. Rats!
</video>
<figcaption>
Tap or click to unmute.</figcaption>
</figure>
<p>Here are a few mildly technical observations. Feel free to skip ahead if this part doesn’t interest&nbsp;you:</p>
<ul>
<li>
<p>Tapstack was simple to start with, and I&nbsp;made it even simpler. Unlike Tapstack, my app doesn’t need a login system. It doesn’t need an interface to create and manage contacts. It already knows exactly who’s using it. (This makes me think about <a href="https://web.archive.org/web/20040411202042/http://www.shirky.com/writings/situated_software.html?utm_source=Robin_Sloan_sent_me">an old blog post</a> by Clay Shirky: “Situated software, by contrast, doesn’t need to be <span>personalized — </span><wbr>it is personal from its inception.”)</p>
</li>
<li>
<p>The core of the app is a camera view with the now-familiar tap/press for photo/video affordance. This is an <a href="https://github.com/Awalz/SwiftyCam?utm_source=Robin_Sloan_sent_me">off-the-rack open source component</a>; what a gift. I&nbsp;don’t think this project would have been possible without&nbsp;it.</p>
</li>
<li>
<p>Besides the app itself, not much is required: an AWS S3 bucket to hold the photos and videos, a couple of AWS Lambda functions to shuffle things around when new messages are uploaded. The back end is actually fairly <span>elegant — </span><wbr>which is, uh, not usually my <span>style — </span><wbr>but, again, that’s only because it’s so simple. There’s barely anything&nbsp;there.</p>
</li>
<li>
<p>I&nbsp;distributed the app to my family using TestFlight, and in Test­Flight it shall remain forever: a cozy, eternal&nbsp;beta.</p>
</li>
</ul>
<p>In a better world, I&nbsp;would have built this in a day, using some kind of modern, flexible HyperCard for&nbsp;iOS.</p>
<p>In our actual world, I&nbsp;built it in about a week, and roughly half of that time was spent wrestling with different flavors of code-signing and identity provi­sioning and I&nbsp;don’t even know what. I&nbsp;burned some incense and threw some stones and the gods of Xcode allowed me to&nbsp;pass.</p>
<p>Our actual world isn’t totally broken. I&nbsp;do not take for granted, not for one millisecond, the open source compo­nents and sample code that made this project possible. In the 21st century, as long as you’re operating within the bounds of the state of the art, program­ming can feel delight­fully Lego-like. All you have to do is rake your fingers through the&nbsp;bin.</p>
<p>I&nbsp;know I&nbsp;ought to pay it forward and publish the code for my app. Even if it doesn’t work for anyone else as-is, it might provide a helpful <span>guide — </span><wbr>one I&nbsp;would have been grateful to have. But the code is marbled with application-specific values, well-salted with authen­ti­ca­tion keys. This app is Entirely <span>Itself — </span><wbr>not a framework, not a <span>template — </span><wbr>and that’s insep­a­rable from the spirit in which it was made. Which brings me&nbsp;to:</p>
<h2>Cooking at home<a name="cooking-at-home"></a></h2>
<p>For a long time, I&nbsp;have struggled to artic­u­late what kind of programmer I&nbsp;am. I’ve been writing code for most of my life; I&nbsp;can make many inter­esting and useful things happen on computers. At the same time, I&nbsp;would not last a day as a profes­sional software engineer. Leave me in charge of a critical database and you will return to a smoldering&nbsp;crater.</p>
<p>Building this app, I&nbsp;figured it&nbsp;out:</p>
<p>I&nbsp;am the program­ming equiv­a­lent of a home&nbsp;cook.</p>
<p>The exhor­ta­tion “learn to code” has its foun­da­tions in market value. “Learn to code” is suggested as a way up, a way out. “Learn to code” offers economic leverage, profes­sional transformation. “Learn to code” goes on your&nbsp;resume.</p>
<p>But let’s substi­tute a different phrase: “learn to cook”. People don’t only learn to cook so they can become chefs. Some do! But many more people learn to cook so they can eat better, or more affordably. Because they want to carry on a tradition. Sometimes they learn because they’re bored! Or even because they enjoy spending time with the person who’s teaching&nbsp;them.</p>
<p>The list of reasons to “learn to cook” overflows, and only a handful have anything to do with the marketplace. Cooking reaches beyond buying and selling to touch nearly all of human experience. It connects to domes­ticity and curiosity; to history and culture; to care and&nbsp;love.</p>
<p>Well, it’s the 21st century now, and I&nbsp;suspect that many of the people you love are waiting inside the pocket computer you are never long without, so I&nbsp;will gently suggest that perhaps coding might connect the same&nbsp;way.</p>
<p>When you liberate program­ming from the require­ment to be profes­sional and <small>scalable</small>, it becomes a different activity altogether, just as cooking at home is really nothing like cooking in a commer­cial kitchen. I&nbsp;can report to you: not only is this different activity rewarding in almost exactly the same way that cooking for someone you love is rewarding, there’s another feeling, too, specific to this realm. I&nbsp;have struggled to find words for this, but/and I&nbsp;think it might be the crux of the whole&nbsp;thing:</p>
<p>This messaging app I&nbsp;built for, and with, my family, it won’t change unless we want it to change. There will be no sudden redesign, no flood of ads, no pivot to chase a userbase inscrutable to us. It might go away at some point, but that will be our decision. What <em>is</em> this feeling? Independence? Security? Sovereignty?</p>
<p>Is it simply … the feeling of being&nbsp;home?</p>
<p><em>Update, February 2022:</em> Two years later, my family still uses BoopSnoop every day. I&nbsp;have added one (1) feature, at my mother’s&nbsp;request.</p>
<p><em>Update, February 2023:</em> Yep, still using it every&nbsp;day!</p>
<p>February&nbsp;2020, Oakland</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[YouTube demonetizes public domain 'Steamboat Willie' video after copyright claim (309 pts)]]></title>
            <link>https://mashable.com/article/youtube-demontizes-public-domain-steamboat-willie-disney-copyright-claim</link>
            <guid>38877321</guid>
            <pubDate>Fri, 05 Jan 2024 09:43:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mashable.com/article/youtube-demontizes-public-domain-steamboat-willie-disney-copyright-claim">https://mashable.com/article/youtube-demontizes-public-domain-steamboat-willie-disney-copyright-claim</a>, See on <a href="https://news.ycombinator.com/item?id=38877321">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="article" data-autopogo="">
<p>Planning to <a href="https://mashable.com/article/infestation-88-steamboat-willy-horror-game" target="_self"><u>utilize</u></a> new public domain works featuring Mickey Mouse this year? Well, here's a case you should certainly pay attention to.</p><p>On Thursday, voice actor and YouTuber Brock Baker uploaded a new video, titled "Steamboat Willie (Brock's Dub),"&nbsp; to his YouTube channel with more than 1 million subscribers.</p>
<p>The video contains the entirety of the 1928 Disney animated short "Steamboat Willie," with Baker providing his own comedic voice overs and sound effects throughout the less than 8 minute long cartoon.</p><blockquote>
<a href="https://twitter.com/BrockBaker/status/1743009587475665205" target="_blank" rel="noopener" title="(opens in a new window)">
Tweet may have been deleted
</a>
</blockquote>
<p>According to Baker, shortly after uploading the clip though, YouTube <a href="https://twitter.com/BrockBaker/status/1743009587475665205" target="_blank" title="(opens in a new window)"><u>demonetized</u></a> the video, evidently on behalf of the erstwhile copyright owner, Disney. Baker also shared a screenshot to his X account showing the video was also being blocked from view in some territories as well.</p><blockquote>
<a href="https://twitter.com/BrockBaker/status/1743014398011101539" target="_blank" rel="noopener" title="(opens in a new window)">
Tweet may have been deleted
</a>
</blockquote>
<p>Prior to this year, nothing here would be out of the ordinary. Disney is very protective of its copyrighted works and would likely be especially so of a film like <em>Steamboat Willie</em> as it stars its most iconic character, Mickey Mouse.</p><p>However, <em>Steamboat Willie</em> along with that 1928 version of Mickey Mouse, <a href="https://mashable.com/article/mickey-mouse-steamboat-willie-disney-public-domain" target="_self"><u>entered the public domain</u></a> on January 1, 2024. This means that a video like Baker's should be completely fine for the YouTuber to not only create and distribute, but monetize as well.</p><p>Baker could likely make a fair use or parody defense for his dubbed version of <em>Steamboat Willie</em>, but as Duke University's Jennifer Jenkins, a professor of law teaching intellectual property, <a href="https://mashable.com/article/mickey-mouse-steamboat-willie-public-domain-so-far" target="_self"><u>told Mashable this week</u></a>, he doesn't even need to make that argument. Public domain works are considered public property.&nbsp;</p><p>"Reproducing and adapting the footage in whatever way you like is legit," Jenkins told Mashable.</p><p>As soon as "Steamboat Willie" became public domain earlier this week, multiple different creative projects using the iconic mouse were announced. Some of these creative works include a <a href="https://variety.com/2024/film/news/steamboat-willie-horror-film-mickey-mouse-public-domain-copyright-1235849861/" target="_blank" title="(opens in a new window)"><u>horror movie</u></a> and a <a href="https://mashable.com/article/infestation-88-steamboat-willy-horror-game" target="_self"><u>video game</u></a>.</p>
<p>So, what happened with Baker's video? Mashable has reached out to YouTube to find out more information and will update this post when we hear back. However, due to how quickly Disney's copyright claim was issued after Baker uploaded his "Steamboat Willie" video, it's likely the video was a victim of the automated Content ID process.</p><p>"Videos uploaded to YouTube are scanned against a database of audio and visual content that's been submitted to YouTube by copyright owners," reads YouTube's policy page on its <a href="https://support.google.com/youtube/answer/2797370?hl=en" target="_blank" title="(opens in a new window)"><u>Content ID</u></a> feature. "When Content ID finds a match, it applies a Content ID claim to the matching video."</p><p>If this is the case, YouTube nor Disney appear to have updated the database to remove works that have recently entered the public domain. And, if so, it seems that should certainly be programmed into the Content ID system as an automated process, much like the valid claims are.</p><p>Mashable will keep you updated on the status of Baker's video.</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CellLVM: A proof-of-concept LLVM to Excel spreadsheet compiler (202 pts)]]></title>
            <link>https://belkadan.com/blog/2023/12/CellLVM/</link>
            <guid>38876863</guid>
            <pubDate>Fri, 05 Jan 2024 08:10:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://belkadan.com/blog/2023/12/CellLVM/">https://belkadan.com/blog/2023/12/CellLVM/</a>, See on <a href="https://news.ycombinator.com/item?id=38876863">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-2023-12-celllvm">
		<p>A few weeks ago I posted this:</p>

<video src="https://belkadan.com/blog/2023/12/CellLVM/CellLVM.mp4" poster="https://belkadan.com/blog/2023/12/CellLVM/celllvm.jpg" preload="metadata" width="699" controls="">
	<a href="https://belkadan.com/blog/2023/12/CellLVM/CellLVM.mp4">(screen recording)</a>
</video>

<p>Which, if you’re not interested in watching a video right now, is a proof-of-concept LLVM to Excel spreadsheet compiler.<!--more--></p>

<h3 id="what">What.</h3>

<p>The night before, I was talking with friends about <a href="https://en.wikipedia.org/wiki/Comma-separated_values">CSV</a>, in particular joking about alignment charts and what counted as “true” CSV. Someone pointed out that assembly, as conventionally printed, could be a CSV, to which I responded</p>

<blockquote>
  <p>ooh, asm in Excel nearly wraps around to being a good idea<br>
your labels are row references</p>
</blockquote>

<p>As I lay in bed, I realized that this was a better match than I initially thought. Forget the CSV part, row references are what let Excel do <em>computation.</em> And while you could make that work with assembly, there’s an alternative that’s a much better fit: LLVM.</p>

<h3 id="ssa-format">SSA Format</h3>

<p><a href="https://llvm.org/">LLVM</a> is a library for building compilers and related tools—the one used by Swift and Rust, actually.<sup id="fnref:gcc" role="doc-noteref"><a href="#fn:gcc" rel="footnote">1</a></sup> At its core is a stripped-down language also called LLVM, or maybe “<a href="https://llvm.org/docs/LangRef.html">the LLVM instruction set</a>”. What’s unique about this language, besides being designed as an intermediate stage for compiling higher-level languages, is that every local variable is assigned a value exactly once, as a simple expression that can only depend on the variables that come before it. This is called <a href="https://en.wikipedia.org/wiki/Static_single-assignment_form">static single-assignment form</a>.</p>

<p>My insight, which I don’t think is new, is that Excel formulas work the same way. Any cell with a formula has that formula defined up front, and values flow through the spreadsheet based on the references set in the formulas—just like SSA. So it should be possible, for operations supported by both LLVM and Excel, to rewrite an LLVM function as an Excel sheet that performs the same computation!</p>

<p>I went to sleep excited about that idea. I woke up and realized the primary problem with it: what about loops?</p>

<h3 id="phi-nodes">Phi Nodes</h3>

<p>In order for SSA to represent branching control flow (such as a conditional increment), it has to have some notion of history when the branches join back up. The conventional way to do this is with a special kind of expression called a <em>phi node,</em> which basically says “if we came from the true block, use x<sub>1</sub> as the value; if we came from the else block, use x<sub>2</sub> as the value”. The name “phi” isn’t short for anything; it’s apparently just meant to be close to “fi”, as in “if” backwards.<sup id="fnref:args" role="doc-noteref"><a href="#fn:args" rel="footnote">2</a></sup> This form works for switches as well (there are just more possible predecessors), and even loops: the predecessor of a loop body might be the entry of the loop, or it might be the last block in the <em>previous</em> time through the loop.</p>

<p>But spreadsheets don’t have loops, do they? I searched around a bit and discovered I was incorrect: Excel spreadsheets <em>do</em> support loops, in the form of “iterative calculation”. As long as the formulas converge on a fixed point within a certain number of steps, Excel will find it. So now I need to figure out how to encode loops in such a way that they do, in fact, converge.</p>

<p>At this point I got up from my bed and started messing around in Google Sheets. (It was a weekend, I didn’t have to go to work.) And I hit on a solution: a variation of the classic “program counter” used by real CPUs. If you keep track of the number of blocks you’ve visited, counting repeats, then you always know what the “current” block is, and more importantly what the “previous” block was. Which means you can implement phi nodes.</p>

<p>Here’s what a phi expression looks like in Excel:</p>

<table>
  <tbody>
    <tr>
      <td><code>=CHOOSE(</code></td>
      <td>A phi is a choice…</td>
    </tr>
    <tr>
      <td><code>  XMATCH(</code></td>
      <td>of values based on a source…</td>
    </tr>
    <tr>
      <td><code>    MAX(</code></td>
      <td>which is the most recent (max) PC…</td>
    </tr>
    <tr>
      <td><code>      IF(B5=ROW(),C5,0),</code></td>
      <td>of the branch in row 5, if it was coming here…</td>
    </tr>
    <tr>
      <td><code>      IF(B10=ROW(),C10,0),</code></td>
      <td>the branch in row 10, if it was coming here…</td>
    </tr>
    <tr>
      <td><code>      C7-0.5),</code></td>
      <td>and the current row, 7, as a last resort…</td>
    </tr>
    <tr>
      <td><code>    {C5,C10,C7-0.5}),</code></td>
      <td>which we get as an index with <code>XMATCH</code>…</td>
    </tr>
    <tr>
      <td><code>  B4,B8,B7)</code></td>
      <td>and <code>CHOOSE</code> the correct value</td>
    </tr>
  </tbody>
</table>

<p>Not pretty but it gets the job done. And with that, I knew it was possible, and I set off to build the compiler.</p>

<h3 id="the-compiler">The Compiler</h3>

<p>The actual compiler is a scant 150 lines of code, partly because it barely implements anything, but also because it’s not actually doing much work. All of the hard parts are in LLVM (and its wrapper, <a href="https://github.com/llvm-swift/LLVMSwift">LLVMSwift</a>, which <a href="https://belkadan.com/source/LLVMSwift/">I did have to fork</a>) and <a href="https://libxlsxwriter.github.io/">xslxwriter</a> (and <a href="https://github.com/damuellen/xlsxwriter.swift">its own wrapper</a>). Without these pre-existing libraries, doing this in a day would have been impossible.</p>

<p>The compiler makes two passes over a single LLVM function: one to assign rows to instructions and basic blocks, and one to translate each instruction into a formula, 1:1. There are only three relevant columns: a label with the instruction type, the value of each instruction, and the “program counter” described above. <a href="https://belkadan.com/source/CellLVM/">You can read the whole thing if you want.</a></p>

<p>The result is a command-line tool that takes LLVM bitcode as input and produces an xlsx file as output. It throws an error if there’s more than one function in the input, or if there’s an operation it doesn’t support (like, say, subtraction). But I do consider it a valid proof of concept! And a successful project, of course—which is important when <a href="https://belkadan.com/blog/2021/07/Keyboard-Pants/">I can’t spend much time on computers outside of work these days.</a></p>

<p><a href="https://docs.google.com/spreadsheets/d/1_K4gMtS0GGviPAIFkhGZmXXFXvuaAatxcx2ulM1XZXk/edit">The output from the video above is on Google Sheets</a>, though you’ll have to make your own copy if you want to “run” it. It adds its two inputs, then doubles them until the result is greater than 50. (Which is about all the current implementation knows how to handle.)</p>

<h3 id="future-directions-alloca">Future Directions: alloca</h3>

<p>One thing that’s <em>not</em> implemented in this proof-of-concept is alloca, i.e. local variables. This is both inconvenient, because that’s the default for a non-optimized build, and a definite missing piece for <em>truly</em> compiling LLVM to a spreadsheet. The thing is, LLVM’s load and store instructions are <em>definitely</em> imperative, in a way that spreadsheets aren’t. So to actually represent a memory location, we’d probably need to express a load as “the value of the most recent store with a matching location”, similar to how we represented a phi as “one of several values based on the most recent basic block”. There’s probably some trickiness around loops as well—maybe the “program counter” actually should count instructions and not just basic blocks, so “most recent” can include “but not in my future”.</p>

<p>Once here, it’s still a jump to <em>arbitrary</em> memory allocation, but maybe not as big of one as it could be. As long as stores are broken up into individual fields, we could say that column H represents heap memory, and column I the instant when it was last modified. This would be a formula involving <em>every store instruction in the program,</em> but it might work. I haven’t tried to work out the details, though.</p>

<h3 id="future-directions-a-call-stack">Future Directions: A Call Stack</h3>

<p>Another major omission is function calls. For non-recursive functions this is mostly a weird kind of branch/phi combination, but for recursive functions we have a problem: all our local SSA variables need to do double duty! I don’t have a good idea of how to do this one short of using <em>columns</em> to represent stack frames. (In which a stack overflow would be running out of columns that have the right formulas.) That would also be neat because you could <em>see the call stack,</em> but I haven’t thought through if it would actually work.</p>



		<p>
			This entry was posted on
			<a href="https://belkadan.com/blog/2023/12">December</a>
			28,
			
				<a href="https://belkadan.com/blog/2023">2023</a>
				and is filed under
				<a href="https://belkadan.com/blog/technical">Technical</a>.
			
			
			
				Tags:
				
					<a href="https://belkadan.com/blog/tags/compilers">Compilers</a>, 
				
					<a href="https://belkadan.com/blog/tags/llvm">LLVM</a>, 
				
					<a href="https://belkadan.com/blog/tags/spreadsheets">Spreadsheets</a>, 
				
					<a href="https://belkadan.com/blog/tags/source-code">Source code</a>
				
			
		</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple's classic Pascal poster, remade as a vector image [pdf] (249 pts)]]></title>
            <link>http://www.danamania.com/print/Apple%20Pascal%20Poster/PascalPosterV3%20A1.pdf</link>
            <guid>38875551</guid>
            <pubDate>Fri, 05 Jan 2024 03:59:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.danamania.com/print/Apple%20Pascal%20Poster/PascalPosterV3%20A1.pdf">http://www.danamania.com/print/Apple%20Pascal%20Poster/PascalPosterV3%20A1.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=38875551">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[WebRTC for the Curious (2020) (191 pts)]]></title>
            <link>https://webrtcforthecurious.com/</link>
            <guid>38875542</guid>
            <pubDate>Fri, 05 Jan 2024 03:58:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webrtcforthecurious.com/">https://webrtcforthecurious.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38875542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><h2 id="webrtc-for-the-curious">WebRTC For The Curious
<a href="#webrtc-for-the-curious">#</a></h2><p>This book was created by WebRTC implementers to share their hard-earned knowledge with the world.
<em>WebRTC For The Curious</em> is an Open Source book written for those that are always looking for more.
This book doesn’t settle for abstraction.</p><p>This book is all about protocols and APIs, and will not be talking about any software in particular.
We attempt to summarize RFCs and get all undocumented knowledge into one place. This book is not a tutorial, and will not contain much code.</p><p>WebRTC is a wonderful technology, but it is difficult to use. This book is vendor agnostic, and we have tried to remove any conflicts of interest.</p><h2 id="who-this-book-is-for">Who this book is for:
<a href="#who-this-book-is-for">#</a></h2><ul><li>Developers who don’t even know what WebRTC solves, and want to learn more.</li><li>Someone who is already building with WebRTC, but wants to know more beyond the APIs.</li><li>Established developers who need help debugging.</li><li>WebRTC implementer who needs clarification on a specific part.</li></ul><h2 id="designed-for-multiple-readings">Designed for multiple readings
<a href="#designed-for-multiple-readings">#</a></h2><p>This book is designed to be read multiple times. Each chapter is self-contained, so you can jump to any part of the book and not be lost.</p><p>Each chapter aims to answer a single question, with three levels of information:</p><ul><li>What needs to be solved?</li><li>How do we solve it? (Including technical details about the solution.)</li><li>Where to learn more.</li></ul><p>Each chapter doesn’t assume prior knowledge. You can start at any point in the book and begin learning. This book will also recommend resources
to go and learn more. Other books cover individual topics in much greater depth. This book aims to teach you the entire system, at the cost of expert level details.</p><h2 id="freely-available-and-privacy-respecting">Freely available and privacy respecting
<a href="#freely-available-and-privacy-respecting">#</a></h2><p>This book is available on <a href="https://github.com/webrtc-for-the-curious/webrtc-for-the-curious">GitHub</a> and <a href="https://webrtcforthecurious.com/">WebRTCforTheCurious.com</a>.
It is licensed in a way that you can use it however you think is best. You can also download the book in its current version as an <a href="https://webrtcforthecurious.com/docs/webrtc-for-the-curious.epub">ePub</a>
or <a href="https://webrtcforthecurious.com/docs/webrtc-for-the-curious.pdf">PDF</a> file.</p><p>This book is written by individuals, for individuals. It is vendor agnostic, so we will not
make recommendations that could be a conflict of interest.</p><p>The website will not use analytics or tracking.</p><h2 id="get-involved">Get involved!
<a href="#get-involved">#</a></h2><p>We need your help! This book is entirely developed on <a href="https://github.com/webrtc-for-the-curious/webrtc-for-the-curious">GitHub</a>
and is still being written. We encourage readers to open issues with questions on things we didn’t do a good job of covering yet.</p><h2 id="license">License
<a href="#license">#</a></h2><p>This book is available under the CC0 license. The authors have waived all their copyright and related rights in their works to the fullest
extent allowed by law. You may use this work however you want and no attribution is required.</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Learning bimanual mobile manipulation with low-cost whole-body teleoperation (116 pts)]]></title>
            <link>https://mobile-aloha.github.io</link>
            <guid>38875452</guid>
            <pubDate>Fri, 05 Jan 2024 03:42:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mobile-aloha.github.io">https://mobile-aloha.github.io</a>, See on <a href="https://news.ycombinator.com/item?id=38875452">Hacker News</a></p>
<div id="readability-page-1" class="page">

<div>
          
          <h2>Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation</h2>
          
          <p><img src="https://mobile-aloha.github.io/static/images/logo.jpg"></p>
        </div>

<div>
    <h2>Team</h2>
    
</div>

<div>
        <h2>Abstract</h2>
        <p>
            Imitation learning from human demonstrations has shown impressive performance in robotics. However, most results focus on table-top manipulation, lacking the mobility and dexterity necessary for generally useful tasks.
            In this work, we develop a system for imitating mobile manipulation tasks that are bimanual and require whole-body control.
            We first present Mobile ALOHA, a low-cost and whole-body teleoperation system for data collection. It augments the ALOHA system with a mobile base, and a whole-body teleoperation interface.
            Using data collected with Mobile ALOHA, we then perform supervised behavior cloning and find that co-training with existing static ALOHA datasets boosts performance on mobile manipulation tasks.
            With 50 demonstrations for each task, co-training can increase success rates by up to 90%, allowing Mobile ALOHA to autonomously complete complex mobile manipulation tasks such as sauteing and serving a piece of shrimp,
            opening a two-door wall cabinet to store heavy cooking pots, calling and entering an elevator,
            and lightly rinsing a used pan using a kitchen faucet.
          </p>
      </div>

<div>
    <h2>Autonomous Skills</h2>
    
    
    
    
  </div>


<div>
    <h2>Teleoperation</h2>
    <div>
        <p>
          <iframe src="https://www.youtube.com/embed/mnLVbwxSdNM" title="Mobile ALOHA Robot - Cooking a 3-Course Cantonese Meal" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
        </p>
      </div>
    <div>
        <p>
          <iframe src="https://www.youtube.com/embed/HaaZ8ss-HP4" title="Mobile ALOHA: Your Housekeeping Robot" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
        </p>
      </div>
    
  </div>


<div>
    <h2>Robustness and Repeatability</h2>
    
    
    
  </div>

<div>
        <h2>Acknowledgements</h2>
        <p>
            We thank the Stanford Robotics Center and Steve Cousins for providing facility support for our experiments. We also thank members of Stanford IRIS Lab: Lucy X. Shi and Tian Gao, and members of Stanford REAL Lab: Cheng Chi, Zhenjia Xu, Yihuai Gao, Huy Ha, Zeyi Liu, Xiaomeng Xu, Chuer Pan and Shuran Song, for providing extensive helps for our experiments. We appreciate much photographing by Qingqing Zhao, and feedbacks from and helpful discussions with Karl Pertsch, Boyuan Chen, Ziwen Zhuang, Quan Vuong and Fei Xia. This project is supported by the Boston Dynamics AI Institute and ONR grant N00014-21-1-2685. Zipeng Fu is supported by Stanford Graduate Fellowship. 
          </p>
      </div>

<div id="BibTeX">
    <h2>BibTeX</h2>
    <pre><code>@inproceedings{fu2024mobile,
  author    = {Fu, Zipeng and Zhao, Tony Z. and Finn, Chelsea},
  title     = {Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation},
  booktitle = {arXiv},
  year      = {2024},
}</code></pre>
  </div>





</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Don't pass structs bigger than 16 bytes on AMD64 (350 pts)]]></title>
            <link>https://gist.github.com/FeepingCreature/5dff669aad380a123b15659e195fb96c</link>
            <guid>38875422</guid>
            <pubDate>Fri, 05 Jan 2024 03:36:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/FeepingCreature/5dff669aad380a123b15659e195fb96c">https://gist.github.com/FeepingCreature/5dff669aad380a123b15659e195fb96c</a>, See on <a href="https://news.ycombinator.com/item?id=38875422">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="file-sixteen_bytes-md">
    <article itemprop="text">
<p dir="auto">Or "How I sped up <a href="https://neat-lang.github.io/" rel="nofollow">Neat</a> by a factor of 2x".</p>
<hr>
<p dir="auto">If you check the <a href="https://github.com/jinyus/related_post_gen/">related_post_gen</a> benchmark, you will find that Neat,
my language, has moved up a few spots. How did I achieve this? Did I implement new high-level optimizer
passes that use language details to expose hidden optimization potential?</p>
<p dir="auto">I changed arrays to be passed as three pointer parameters instead of one parameter consisting of a
three-pointer struct. That's it.</p>
<p dir="auto">This problem has been vexing me for a long time. Neat seemed weirdly slower than it should have been,
particularly compared to D, and if I looked at a profiler I would be seeing a lot of weird stack moves.
The compiler seemed to be spending most of its time rearranging large amounts of the stack for function calls.</p>
<p dir="auto">Why are Neat arrays three pointers? As opposed to D, Neat employs a refcounter. That means that arrays, in addition
to start and end, also need a pointer to the base of the array object, where the reference count is stored.
It turns out the reason that D arrays are fast and Neat arrays are so slow is just because having 24 bytes instead of
16 puts them into a different regime of parameter passing.</p>
<p dir="auto">If we check <a href="https://refspecs.linuxbase.org/elf/x86_64-abi-0.99.pdf" rel="nofollow">the SystemV AMD64 ABI specification</a> (PDF),
it tells us that any struct greater than 16 bytes is passed by pointer.
("If the size of the aggregate exceeds two eightbytes and the first eightbyte isn’t SSE or any other
eightbyte isn’t SSEUP, the whole argument is passed in memory.")
To pass a struct by memory, we allocate a struct-sized spot on the stack, fill it with the values we pass, then
pass the pointer to the function.</p>
<p dir="auto">Now, LLVM is a very good optimizer, but this does not leave it much room. The value <em>has</em> to go on the stack, which
means there must be space for it there, it must be copied out of the register it is probably living in, and
it has to remember which parts of the stack are in use and which ones can be reused by another call, which it turns
out to be pretty poor at.</p>
<p dir="auto">We can demonstrate the issue with this benchmark:</p>
<pre><code>==========
harness.h:
==========

#define TYPE double

struct Vector { TYPE x, y, z; };

struct Vector vector_add_struct(struct Vector left, struct Vector right);

struct Vector vector_add_fields(
    TYPE left_x, TYPE left_y, TYPE left_z,
    TYPE right_x, TYPE right_y, TYPE right_z);

==========
harness.c:
==========

#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include "harness.h"

int main(int argc, const char *argv[])
{
    int mode = atoi(argv[1]);
    int length = atoi(argv[2]);
    struct Vector result = {0};
    if (mode == 0)
    {
        for (int i = 0; i &lt; length; i++)
            result = vector_add_struct(result, (struct Vector) {i, i, i});
    }
    else
    {
        for (int i = 0; i &lt; length; i++)
            result = vector_add_fields(result.x, result.y, result.z, i, i, i);
    }
    printf("result &lt;%f, %f, %f&gt;\n", result.x, result.y, result.z);
}

=======
impl.c:
=======

#include "harness.h"

struct Vector vector_add_struct(struct Vector left, struct Vector right)
{
    return (struct Vector) {
        left.x + right.x,
        left.y + right.y,
        left.z + right.z,
    };
}

struct Vector vector_add_fields(
    TYPE left_x, TYPE left_y, TYPE left_z,
    TYPE right_x, TYPE right_y, TYPE right_z)
{
    return (struct Vector) {
        left_x + right_x,
        left_y + right_y,
        left_z + right_z,
    };
}
</code></pre>
<p dir="auto">As you can see, depending on parameters, this either passes some values as separate parameters or a single large struct.
The mode and run length are passed on the commandline to prevent the optimizer from constant folding everything.</p>
<p dir="auto">We must compile impl.c separately to avoid inlining:</p>
<pre><code>clang -O3 impl.c -c -o impl.o
clang -O3 harness.c impl.o -o benchmark
time ./benchmark 0 1000000000
time ./benchmark 1 1000000000
</code></pre>
<p dir="auto">This is hardly subtle: with just the change of passing three separate fields instead of a vector struct, I go
from 12.3 seconds to 5.3 seconds!</p>
<p dir="auto">If we check the assembly, we can indeed see that a lot of instructions are occupied with stack shuffles.
In fact, a major benefit of the field version is that the parameters already enter the function in SSE registers, rather
than having to be loaded from the stack every time. This was the whole point of the SystemV ABI and its focus on
passing values in registers as much as possible, so it's kind of sad to see it fail here. I believe with the number of
registers available on AMD64, a benchmark would have shown by-value passing to be valuable even for types above 16 bytes.</p>
<p dir="auto">In fact, if you think about what the code does, by writing the fields on the stack and then (explicitly rather than
implicitly) passing a pointer, the (new, performant) AMD64 System V ABI has effectively regressed to the old x86 cdecl
ABI, where everything was passed on the stack! Cdecl, famously, was so known for its slowness that it spawned multiple
calling conventions aimed just at making it fast.</p>
<p dir="auto">Of course, in any real situation this code would be all inlined. In fact, turning on LTO with gcc (though interestingly
not clang!) erases any performance difference between the two versions. But still, not every function can or
should be inlined.</p>
<p dir="auto">Now, if you are calling a C API, you have to use the C ABI. But lots of high-level types internal to non-C languages,
though they may be presented to the compiler's backend as a struct (such as Neat's three-pointer arrays), don't strictly
speaking <em>need</em> to be expressed as one. You are the language writer, and it's up to you to decide how arrays,
tuples, sumtypes etc. are passed. That's why I chose to pass all of those (if above 16 bytes) as individual fields instead,
and the benchmark shows the benefit.</p>
<p dir="auto">So if you are on AMD64, and you're either working on languages or microoptimizing an API, I advise you to take
the free speedup. You should at least benchmark to see if you can benefit from splitting structs above 16 bytes manually.
Especially in inner loops, the benefit can be surprisingly large.</p>
<p dir="auto">Addendum:</p>
<ul dir="auto">
<li>Q: Sure, maybe this is true for structs of pointers. But <code>double</code> is in class SSE, according to the spec.
Why isn't the struct passed in SSE registers anyways?</li>
<li>A: Man I don't know. All I can tell you is that it doesn't.</li>
</ul>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Australia’s ‘Bluey’ conquered children’s entertainment (270 pts)]]></title>
            <link>https://www.ft.com/content/0a3c9806-8b0f-4cca-a4e5-e1e6dd6d395b</link>
            <guid>38875399</guid>
            <pubDate>Fri, 05 Jan 2024 03:32:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/0a3c9806-8b0f-4cca-a4e5-e1e6dd6d395b">https://www.ft.com/content/0a3c9806-8b0f-4cca-a4e5-e1e6dd6d395b</a>, See on <a href="https://news.ycombinator.com/item?id=38875399">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="barrier-page">
<div data-component="articleHeaderHeroOffer" data-component-unique-name="CHE-Print"><div><p><img src="https://financial-times-financial-times.cdn.zephr.com/assets/icons/padlock_icon.svg" alt="Padlock icon"></p><div><p>Subscribe to unlock this article</p></div></div><div><p>Get ahead in 2024<br><strong>Have January on us</strong></p><p>Pay <s>CHF55</s> CHF49 per month.<br>January saving based on a 12-month subscription. Save 10% on Standard Subscriptions until 10 January.<br>Full Terms and Conditions apply.</p></div></div>
<div id="recommendedOffers-CHE-Print-1ecf1ba1-5ffa-4346-8a70-be87023be972" data-component="recommendedOffers" data-component-unique-name="CHE-Print"><h2>Explore more offers.</h2><div><div data-o-grid-colspan="12 L4"><div><p><img src="https://financial-times-financial-times.cdn.zephr.com/assets/icons/primary_product_icon_trial.svg" alt=""></p><p data-offer-type="trial"><h3>Standard Digital</h3></p></div><div><p>Then CHF85 per month. Complete digital access to quality FT journalism. New customers only. Cancel anytime.</p></div></div><div data-o-grid-colspan="12 L4"><div><p><img src="https://financial-times-financial-times.cdn.zephr.com/assets/icons/primary_product_icon_print.svg" alt=""></p><p data-offer-type="print"><h3>Standard Digital</h3></p></div><div><p>Insight and expertise in your hands with the iconic FT print edition, delivered Monday to Saturday.</p></div></div><div data-o-grid-colspan="12 L4"><div><p><img src="https://financial-times-financial-times.cdn.zephr.com/assets/icons/primary_product_icon_premium.svg" alt=""></p><p data-offer-type="premium"><h3>Standard Digital</h3></p></div><div><p>Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%.</p></div></div></div></div>
<div data-component="subscriptionOptionsV2" data-component-unique-name="CHE-Print"><h2>Explore our full range of subscriptions.</h2></div>
<div data-component="whyFT" data-component-unique-name="default"><div><h2>Why the FT?</h2><p>See why over a million readers pay to read the Financial Times.</p></div><p><a href="https://subs.ft.com/whytheft">Find out why</a></p></div>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Modern Java/JVM Build Practices (147 pts)]]></title>
            <link>https://github.com/binkley/modern-java-practices</link>
            <guid>38875318</guid>
            <pubDate>Fri, 05 Jan 2024 03:18:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/binkley/modern-java-practices">https://github.com/binkley/modern-java-practices</a>, See on <a href="https://news.ycombinator.com/item?id=38875318">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><a href="https://github.com/binkley/modern-java-practices/blob/master/LICENSE.md">
<img src="https://github.com/binkley/modern-java-practices/raw/master/images/public-domain.svg" alt="Public Domain" width="20%" height="auto">
</a>
<h2 tabindex="-1" dir="auto">Modern Java/JVM Build Practices</h2>
<p dir="auto"><a href="https://github.com/binkley/modern-java-practices/actions"><img src="https://github.com/binkley/modern-java-practices/workflows/build/badge.svg" alt="build"></a>
<a href="https://github.com/binkley/modern-java-practices/pulls"><img src="https://camo.githubusercontent.com/297d2a51e4c30bfa4b73b8f92708db8e113b98cf72ce0f4ac969e6938c8017db/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f62696e6b6c65792f6d6f6465726e2d6a6176612d7072616374696365732e737667" alt="pull requests" data-canonical-src="https://img.shields.io/github/issues-pr/binkley/modern-java-practices.svg"></a>
<a href="https://github.com/binkley/modern-java-practices/issues/"><img src="https://camo.githubusercontent.com/4c3775d6e83e524bc3873ce2b68d12bc1afa78a053b1089df99bca6ab4c5aa1d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f62696e6b6c65792f6d6f6465726e2d6a6176612d7072616374696365732e737667" alt="issues" data-canonical-src="https://img.shields.io/github/issues/binkley/modern-java-practices.svg"></a>
<a href="https://snyk.io/test/github/binkley/modern-java-practices" rel="nofollow"><img src="https://camo.githubusercontent.com/ee59e1faa2edf5ad53f0cb03eb0169fa852ba2490c1649f2eb7437e4fb36aa48/68747470733a2f2f736e796b2e696f2f746573742f6769746875622f62696e6b6c65792f6d6f6465726e2d6a6176612d7072616374696365732f62616467652e737667" alt="vulnerabilities" data-canonical-src="https://snyk.io/test/github/binkley/modern-java-practices/badge.svg"></a>
<a href="http://unlicense.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/e95e2a293e67e64dfda39526a65dbea24a673851e7a0397803e9cabcb90c5379/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d5075626c6963253230446f6d61696e2d626c75652e737667" alt="license" data-canonical-src="https://img.shields.io/badge/license-Public%20Domain-blue.svg"></a></p>
<p dir="auto"><strong>Modern Java/JVM Build Practices</strong> is an article-as-repo on building modern
Java/JVM projects using
<a href="https://docs.gradle.org/current/userguide/userguide.html" rel="nofollow">Gradle</a> and
<a href="https://maven.apache.org/what-is-maven.html" rel="nofollow">Maven</a>, and a <em>starter project</em>
for Java.</p>
<p dir="auto">The focus is <em>best build practices</em> and <em>project hygiene</em>.
This document is <em>agnostic</em> between Gradle and Maven: discussion in each section
covers both tools (alphabetical order, Gradle before Maven).
See <a href="https://blog.frankel.ch/final-take-gradle/" rel="nofollow"><em>My Final Take on Gradle (vs.
Maven)</em></a> for an opinionated view.</p>
<p dir="auto">This is not a JVM starter for only Java:
I use it for starting my Kotlin projects, and substitute complilation and code
quality plugins.
Any language on the JVM can find practices and tips.</p>
<p dir="auto">As a <em>guide</em>, this project focuses on:</p>
<ul dir="auto">
<li>A quick starter for JVM projects using Gradle or Maven.
<a href="https://github.com/binkley/modern-java-practices/fork">Fork</a> me,
<a href="https://github.com/binkley/modern-java-practices.git">clone</a> me, copy/paste
freely!
I am <a href="http://unlicense.org/" rel="nofollow"><em>Public Domain</em></a></li>
<li>Discuss—and illustrate (through code)—sensible default practices;
highlight good build tools and plugins</li>
<li>Document pitfalls that turned up.
Some were easy to address after Internet search; some were challenging
(see "Tips" sections)</li>
<li>Do not be an "all-in-one" solution. You know your circumstances best.
I hope this project helps you discover build improvements you love.
Please share with others through
<a href="https://github.com/binkley/modern-java-practices/issues">issues</a> or
<a href="https://github.com/binkley/modern-java-practices/pulls">PRs</a></li>
</ul>
<h3 tabindex="-1" dir="auto">Two recurring themes</h3>
<ul dir="auto">
<li><em>Shift problems left</em> — Find issues earlier in your build—before
you see them in production</li>
<li><em>Make developer life easier</em> — Automate build tasks often done by
hand: get your build to complain (<em>fail</em>) locally before sharing with your
team, or fail in CI before deployment</li>
</ul>
<h3 tabindex="-1" dir="auto">What is a <em>Starter</em>?</h3>
<p dir="auto">A project starter has several goals:</p>
<ul dir="auto">
<li>Help a new project get up and running with minimal fuss</li>
</ul>
<p dir="auto">This starter project is focused on <em>build</em>:</p>
<ul dir="auto">
<li>Easy on-ramp for new folks to try out your project for themselves</li>
<li>Support new contributors to your project that they become productive quickly</li>
<li>Support current contributors in the build, get out of their way, and make
everyday things easy</li>
</ul>
<p dir="auto">This starter project has minimal dependencies.
The focus is on Gradle and Maven plugins and configuration so that you and
contributors can focus on the code, not on setting up the build.</p>
<h3 tabindex="-1" dir="auto">Summing up</h3>
<ul dir="auto">
<li><em>I'm not a great programmer; I'm just a good programmer with great habits.</em>
—
<a href="https://www.goodreads.com/quotes/532211-i-m-not-a-great-programmer-i-m-just-a-good-programmer" rel="nofollow">Kent Beck</a></li>
<li><em>Make it work, make it right, make it fast</em>
— <a href="http://wiki.c2.com/?MakeItWorkMakeItRightMakeItFast" rel="nofollow">C2 Wiki</a></li>
</ul>
<p dir="auto"><strong>NB</strong> — This is a <em>living document</em>.
The project is frequently updated to pick up new dependency or plugin
versions, and improved practices; this README updates recommendations.
This is part of what <em>great habits</em> look like.
See <a href="#reusing-this-project"><em>Reusing this project</em></a> for tips on pulling in
updates.</p>
<p dir="auto">(Credit to Yegor Bugayenko for <a href="https://www.yegor256.com/2019/04/23/elegant-readme.html" rel="nofollow"><em>Elegant
READMEs</em></a>.)</p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/binkley/modern-java-practices/blob/master/images/try.png"><img src="https://github.com/binkley/modern-java-practices/raw/master/images/try.png" alt="Try..." width="20%" height="auto"></a></p>

<h2 tabindex="-1" dir="auto">Try it</h2>
<p dir="auto">After cloning or forking this project to your machine, try out the build
combination that makes sense for you:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ./gradlew build  # Local-only build
$ ./batect build-with-gradle  # CI build with Batect
$ earthly +build-with-gradle  # CI build with Earthly
$ ./mvnw verify  # Local-only build
$ ./batect build-with-maven  # CI build with Batect
$ earthly +build-with-maven  # CI build with Earthly"><pre>$ ./gradlew build  <span><span>#</span> Local-only build</span>
$ ./batect build-with-gradle  <span><span>#</span> CI build with Batect</span>
$ earthly +build-with-gradle  <span><span>#</span> CI build with Earthly</span>
$ ./mvnw verify  <span><span>#</span> Local-only build</span>
$ ./batect build-with-maven  <span><span>#</span> CI build with Batect</span>
$ earthly +build-with-maven  <span><span>#</span> CI build with Earthly</span></pre></div>
<p dir="auto">(You may find that the "CI" build works great for you locally as part of your
regular command line.
Most IDEs would use the "local-only" build.)</p>
<p dir="auto">See what the starter program does:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ./run-with-gradle.sh
$ ./run-with-maven.sh"><pre>$ ./run-with-gradle.sh
$ ./run-with-maven.sh</pre></div>
<hr>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/binkley/modern-java-practices/blob/master/images/changes.png"><img src="https://github.com/binkley/modern-java-practices/raw/master/images/changes.png" alt="Changes" width="20%" height="auto"></a></p>

<h2 tabindex="-1" dir="auto">Recent significant changes</h2>
<ul dir="auto">
<li>Gradle: remove use of <code>testsets</code> plugin for integration testing in favor of
native Gradle. This is in support of Gradle 8</li>
</ul>
<hr>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/binkley/modern-java-practices/blob/master/images/table-of-contents.png"><img src="https://github.com/binkley/modern-java-practices/raw/master/images/table-of-contents.png" alt="Table of Contents" width="20%" height="auto"></a></p>

<h2 tabindex="-1" dir="auto">TOC</h2>
<ul dir="auto">
<li><a href="#try-it">Try it</a></li>
<li><a href="#recent-significant-changes">Recent significant changes</a></li>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#reusing-this-project">Reusing this project</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#you-and-your-project">You and your project</a></li>
<li><a href="#getting-your-project-started">Getting your project started</a></li>
<li><a href="#the-jdk">The JDK</a></li>
<li><a href="#use-gradle-or-maven">Use Gradle or Maven</a></li>
<li><a href="#setup-your-ci">Setup your CI</a></li>
<li><a href="#keep-local-consistent-with-ci">Keep local consistent with CI</a></li>
<li><a href="#maintain-your-build">Maintain your build</a></li>
<li><a href="#choose-your-code-style">Choose your code style</a></li>
<li><a href="#generate-code">Generate code</a></li>
<li><a href="#leverage-the-compiler">Leverage the compiler</a></li>
<li><a href="#use-linting">Use linting</a></li>
<li><a href="#use-static-code-analysis">Use static code analysis</a></li>
<li><a href="#shift-security-left">Shift security left</a></li>
<li><a href="#leverage-unit-testing-and-coverage">Leverage unit testing and coverage</a></li>
<li><a href="#use-mutation-testing">Use mutation testing</a></li>
<li><a href="#use-integration-testing">Use integration testing</a></li>
<li><a href="#debugging">Debugging</a></li>
<li><a href="#samples">Samples</a></li>
<li><a href="#going-further">Going further</a></li>
<li><a href="#problems">Problems</a></li>
<li><a href="#credits">Credits</a></li>
</ul>
<hr>
<a href="https://modernagile.org/" title="Modern Agile" rel="nofollow">
<img src="https://github.com/binkley/modern-java-practices/raw/master/images/modern-agile-wheel-english.png" alt="Modern Agile" width="20%" height="auto">
</a>
<h2 tabindex="-1" dir="auto">Introduction</h2>
<p dir="auto">Hi! I want you to have <em>awesome builds</em> 🟢. If you're on a <em>Java</em> project,
or a project on any
<em><a href="https://en.wikipedia.org/wiki/List_of_JVM_languages" rel="nofollow">JVM language</a></em>
(Clojure, Groovy, JRuby, Java, Jython, Kotlin, Scala, <em>et al</em>), this article is
for you.
This article assumes you are using Gradle or Maven for your build locally, and
in CI.
Some of you are using other build systems native to your source language.
Please follow along!</p>
<p dir="auto"><strong>What is the goal of this article?</strong> I want to highlight modern practies in
building Java/JVM projects with Gradle or Maven, and provide guidance, or at
least food for thought.
The sample Gradle and Maven projects use Java, but most recommendations apply to
builds for <em>any</em> JVM language.
I'll never be as clever or as talented as <a href="http://poignant.guide/book/" rel="nofollow"><em>why the lucky stiff</em></a>, but I
hope writing this makes you, developers, and others happy.</p>
<p dir="auto">See the wheel to the right?
<em>No, you do not need to be agile!</em>
(But I encourage you to explore the benefits of Agile.)
This article is for you regardless of how your team approaches software.
The point is to <em>"make people awesome"</em> for any project, possibly the most key
value of the Agile approach to software.</p>
<h3 tabindex="-1" dir="auto">Principles in designing these builds</h3>
<ol dir="auto">
<li><em>Make it work</em>
<ul dir="auto">
<li>Can I as a Day 1 developer build the project locally?</li>
<li>Can I hand the project off to someone else to try?</li>
</ul>
</li>
<li><em>Make it right</em>
<ul dir="auto">
<li>Can I reproduce issues in the CI build, and fix it locally?</li>
<li>Can I find code and security issues from running the build?</li>
<li>Is the code clean? Am I happy to explore the project?</li>
</ul>
</li>
<li><em>Make it fast</em>
<ul dir="auto">
<li>Can I run the local build as frequently as I like, and be productive?</li>
<li>Can I have a fast cycle of code &amp; test? What about red-green-refactor?</li>
<li>Can I update my dependencies and plugins quickly and easily?</li>
</ul>
</li>
</ol>
<h3 tabindex="-1" dir="auto">Goals for this project</h3>
<ul dir="auto">
<li>Starter build scripts for Modern Java/JVM builds in Gradle and Maven, helpful
for new projects, or refurbishing existing projects</li>
<li>Quick solutions for raising project quality and security in your local build</li>
<li>Shift <em>problems to the left</em> ("to the left" meaning earlier in the development
cycle). You'll get earlier feedback while still having a fast local build.
Time spent fixing issues locally is better than waiting on CI to fail, or
worse, for production to fail</li>
<li>The article focuses on Gradle and Maven: these are the most used build tools
for Modern Java/JVM projects. However, if you use a different build tool, the
principals still apply</li>
</ul>
<p dir="auto">I want to help with the question: <em>I am in Day 1 on my project</em>: How do I begin
with a local build that supports my team through the project lifetime?
And when I have an existing project, how to I catch up?</p>
<h3 tabindex="-1" dir="auto">Goal of this article</h3>
<p dir="auto"><a href="https://modernagile.org/" rel="nofollow"><em>Make people awesome</em></a> (that means <em>you</em>). This
project is based on the experiences of many, and experiments with Modern
Java/JVM builds, and shares lessons learned with you.</p>
<hr>
<a href="https://github.com/binkley/modern-java-practices/fork" title="Reuse">
<img src="https://github.com/binkley/modern-java-practices/raw/master/images/reuse.png" alt="Reuse" width="20%" height="auto">
</a>
<h2 tabindex="-1" dir="auto">Reusing this project</h2>
<p dir="auto">Don't forget to <a href="https://github.com/binkley/modern-java-practices/fork"><em>fork
me</em></a> or <em>clone</em> me! This
is <a href="http://unlicense.org/" rel="nofollow"><em>Public Domain</em></a> software: it is meant to be reused
by you however is sensible.
If you fork, I take care that upstream pulls work, but I'd like to <a href="https://github.com/binkley/modern-java-practices/issues">hear from
you</a> if you have
trouble.
See also: <a href="https://github.com/binkley/modern-java-practices#cleaning-up"><em>Cleaning
up</em></a>.</p>
<p dir="auto">Sensible approaches:</p>
<ul dir="auto">
<li>Fork this project, and work from there, pulling down improvements (usually
version bumps on tools and dependencies)</li>
<li>Clone this project to a new repository, and work from there. Manually pull
over improvements (there is <a href="https://github.com/binkley/modern-java-practices#keep-plugins-and-dependencies-up-to-date">tooling to
help</a>)</li>
<li>Read through this repository's code and changes, and update your own
repository by hand, useful for existing repositories</li>
</ul>
<p dir="auto">If you cloned this project as a starter, you may want to stay updated on
improvements:</p>
<div data-snippet-clipboard-copy-content="git remote add upstream https://github.com/binkley/modern-java-practices.git
git fetch upstream
git merge master/upstream"><pre><code>git remote add upstream https://github.com/binkley/modern-java-practices.git
git fetch upstream
git merge master/upstream
</code></pre></div>
<p dir="auto">Once you are happy with your project, you should think about removing the
upstream remote, and reviewing changes in this repository by hand.
Your decision might depend on what merge conflicts you encounter.</p>
<h3 tabindex="-1" dir="auto">Tips</h3>
<ul dir="auto">
<li>Consider <a href="https://stackoverflow.com/questions/30001304/clone-git-repository-without-history" rel="nofollow"><em>Clone git repository without
history?</em></a>
to start at the current tip of this project. For example, some images in
<code>README.md</code> started overlarge in earlier versions, something you may not want
in a clone</li>
<li><em>Caution</em>: Not all the images used in <a href="https://github.com/binkley/modern-java-practices/blob/master/README.md"><code>README.md</code></a> may be in the
Public Domain (this is challenging to research). Use due diligence before
sharing your clone, and other licenses may apply for these images in a
global context</li>
</ul>
<h3 tabindex="-1" dir="auto">Irrelevant files</h3>
<p dir="auto">This project includes files which are helpful for maintaining itself, but may
be irrelevant to you. Some, however, may prove helpful in specific contexts as
noted:</p>
<ul dir="auto">
<li><a href="https://github.com/binkley/modern-java-practices/blob/master/build-as-ci-does.sh"><code>build-as-ci-does.sh</code></a>
Helpful when CI has steps that local developers do not, and you want to
reproduce or explore locally a CI problem. The script should match the
actions your CI takes on pushes (this project uses GitHub actions)</li>
<li><a href="https://github.com/binkley/modern-java-practices/blob/master/compare-tooling-versions.sh"><code>compare-tooling-versions-sh</code></a>
If your project supports <em>both</em> Gradle and Maven builds (unlikely), a
quick way to look at dependency and plugin versions between the two.
Note that the ouput needs <em>human</em> reading: the same tool version may appear
as different lines when comparing</li>
<li><a href="https://github.com/binkley/modern-java-practices/blob/master/coverage.sh"><code>coverage</code></a>
Checks if the local code passes at given levels of code coverage.
The script is focused on Maven, but with edits would do the same for Gradle.
This supports the <a href="#leverage-unit-testing-and-coverage">"ratchet" pattern</a></li>
<li><a href="https://github.com/binkley/modern-java-practices/blob/master/run-with-gradle.sh"><code>run-with-gradle.sh</code></a>
If you are a Gradle project, you will likely rename this to just <code>run</code> or
similar</li>
<li><a href="https://github.com/binkley/modern-java-practices/blob/master/run-with-maven.sh"><code>run-with-maven.sh</code></a>
If you are a Maven project, you will likely rename this to just <code>run</code> or
similar</li>
</ul>
<h2 tabindex="-1" dir="auto">Contributing</h2>
<p dir="auto">See <a href="https://github.com/binkley/modern-java-practices/blob/master/CONTRIBUTING.md"><code>CONTRIBUTING.md</code></a>.
Please <a href="https://github.com/binkley/modern-java-practices/issues">file issues</a>,
or contribute <a href="https://github.com/binkley/modern-java-practices/pulls">pull
requests</a>!
I'd love a conversation with you.</p>
<hr>

<h2 tabindex="-1" dir="auto">You and your project</h2>
<p dir="auto">There are simple ways to make your project great. Some goals to strive for:</p>
<ul dir="auto">
<li>Visitors and new developers get off to a quick start, and can understand what
the build does (if they are interested)</li>
<li>Users of your project trust it—the build does what it says on the
tin—, and they feel safe relying on your project</li>
<li>You don't get peppered with questions that are answered "in the source"
—because not everyone wants to read the source, and you'd rather be
coding than answering questions ☺</li>
<li>Coding should feel easy. You solve <em>real</em> problems, and do not spend overmuch
much time on build details: your build supports you</li>
<li>Your code passes "smell tests": no simple complaints, and you are proud of
what others see. <em>Hey!</em> You're a professional, and it shows. (This is one of
my personal fears as a programmer)</li>
<li>Your project is "standard", meaning, the build is easily grasped by those
familiar with standard techniques and tooling</li>
</ul>
<p dir="auto">Hopefully this article and the sample build scripts help you!</p>
<hr>
<h2 tabindex="-1" dir="auto">Getting your project started</h2>
<p dir="auto">To get a project off to a good start, consider these items. Even for existing
projects, you should address these as you go along or while refurbishing an
existing project:</p>
<ul dir="auto">
<li><strong>Team agreement comes first</strong>. Make sure everyone is onboard and clear on
what build standards are, and understands—at least as an
outline—what the build does for them</li>
<li>Provide a <em>good</em> <code>README.md</code>. This saves you a ton of time in the long run.
This is your <em>most important</em> step. A good resource is Yegor's
<a href="https://www.yegor256.com/2019/04/23/elegant-readme.html" rel="nofollow"><em>Elegant READMEs</em></a>
<ul dir="auto">
<li><a href="https://thethreevirtues.com/" rel="nofollow">Intelligent laziness is a virtue</a>. Time
invested in good documentation pays off</li>
<li>A good <a href="https://github.com/binkley/modern-java-practices/blob/master/README.md"><code>README.md</code></a> answers visitors questions, so you don't
spend time answering trivial questions, and explains/justifies your
project to others.</li>
<li>Fight <a href="https://en.wikipedia.org/wiki/Conway%27s_law" rel="nofollow">Conway's Law</a> with
communication!</li>
</ul>
</li>
<li>Pick a version of Java, and stick to it throughout your local build, CI
pipeline, and environment deployments. <em>Do not mix versions.</em></li>
<li>Pick <strong>Gradle</strong> or <strong>Maven</strong>, and use only one. This project provides both to
demonstrate equivalent builds for each. See
<a href="#use-gradle-or-maven">Use Gradle or Maven</a> for more discussion</li>
<li>Use build wrappers committed into your project root. These run Gradle or
Maven, and coders should always invoke <code>./gradlew</code> or <code>./mvnw</code> (use shell
<em>aliases</em> if these grow tiresome to type)
<ul dir="auto">
<li>Build wrappers are shell scripts to run Gradle or Maven. The wrapper takes
care of downloading needed tools without getting in the way. New
contributors and developers can start right away; they do not need to
install more software</li>
<li>For Gradle, use
<a href="https://docs.gradle.org/current/userguide/gradle_wrapper.html" rel="nofollow"><code>./gradlew</code></a>
(part of Gradle)</li>
<li>For Maven, use <a href="https://maven.apache.org/wrapper/" rel="nofollow"><code>./mvnw</code></a> (a plugin)</li>
</ul>
</li>
<li>Always run CI on push to a shared repository. It's a sad panda when someone is
excited about their commit, and then the commit breaks the other developers
<ul dir="auto">
<li>In CI, use caches for dependency downloads; this speeds up the feedback
cycle from CI (see <a href="#setup-your-ci">below</a>)</li>
<li>When sensible, move code quality and security checks into local builds
before changes hit CI (see <a href="#setup-local-ci">below</a>)</li>
</ul>
</li>
<li>Pick a common code style, and stay consistent; update tooling to complain on
style violations
<ul dir="auto">
<li>The team should agree on a common code style, <em>eg</em>, SUN, Google, <em>et al</em></li>
<li>See <a href="#use-linting">Use linting</a></li>
</ul>
</li>
</ul>
<h3 tabindex="-1" dir="auto">Tips</h3>
<ul dir="auto">
<li>
<p dir="auto">Consider using client-side Git hooks for <code>pre-push</code> to run a full, clean,
local build. This helps ensure "oopsies" from going to CI where they impact
everyone. The options are broad. Try web searches on:</p>
<ul dir="auto">
<li>"gradle install git hooks"</li>
<li>"maven install git hooks"</li>
</ul>
<p dir="auto">This article presently has no specific recommendations on choices of plugin or
approach for Git hooks.</p>
</li>
</ul>
<hr>
<a href="https://adoptium.net/" title="Adoptium" rel="nofollow">
<img src="https://github.com/binkley/modern-java-practices/raw/master/images/adoptium.png" alt="Adoptium" width="20%" height="auto">
</a>
<h2 tabindex="-1" dir="auto">The JDK</h2>
<p dir="auto">For any Modern Java/JVM project, the first decision is <em>which version of Java
(the JDK)</em> to use? Some guidelines:</p>
<ul dir="auto">
<li>Java 17 is the most current LTS ("long-term support") version</li>
<li>There are more recent versions with continuing improvements and additional
features to try out</li>
<li>If your personal or open-source project does not require a paid support
contract, newer Java versions are a good choice</li>
<li>For a full breakdown of the current JDK landscape (as of Jul 2022), see
<a href="https://tomgregory.com/which-jdk-version-and-vendor/" rel="nofollow"><em>Which JDK Version and Vendor Should You Use on Your
Project?</em></a>, and a short
list of recommendations at <a href="https://whichjdk.com/" rel="nofollow"><em>Which Version of JDK Should I
Use?</em></a></li>
</ul>
<p dir="auto">In this project, you'll see the choice of Java 17 as this is the version to
recommend in production.</p>
<p dir="auto">In general, you will find that <a href="https://adoptium.net/" rel="nofollow">Adoptium</a> is a go-to
choice for the JDK.</p>
<h3 tabindex="-1" dir="auto">Tips</h3>
<ul dir="auto">
<li>In Maven, use a property to <em>fix</em> the version of Java in place. But note
naming for that property: <code>java.version</code> is defined by the JVM, and Maven
creates a matching property. Recommended is to define your Java version with
the <code>jdk.version</code> property, which has no collision with pre-defined
properties</li>
<li>In Gradle, use the <code>javaToolchains</code> task to investigate issues with
mismatching or confusing build paths, project configuration, and Gradle
sorting it out. This is an issue for local-only builds; local builds using a
container (such as via <a href="#setup-local-ci"><em>Batect</em></a>) lower these concerns</li>
<li>In GitHub Actions, building supports cross-checking multiple JVM versions,
use
<a href="https://docs.github.com/en/actions/using-jobs/using-a-matrix-for-your-jobs">the <code>matrix</code> feature</a>.
See <a href="https://github.com/binkley/modern-java-practices/blob/master/.github/workflows/ci-batect-maven.yml">the example GitHub actions</a></li>
</ul>
<h3 tabindex="-1" dir="auto">Managing your Java environment</h3>
<p dir="auto">Two best-of-class tools come to mind to manage your JDK environment in projects:</p>
<ul dir="auto">
<li><a href="#jenv">jEnv</a></li>
<li><a href="#direnv">Direnv</a></li>
</ul>
<p dir="auto">Both assume UNIX-type shells (Bash, Zsh, etc).</p>
<p dir="auto">For those on Windows, you may need to use Cygwin, Git for Windows, or WSL2 to
use these.</p>
<p dir="auto">(Reminder: in general, when setting up your project environment, prefer the
latest LTS version of Java, which is 17.)</p>
<h4 tabindex="-1" dir="auto">Jenv</h4>
<p dir="auto"><a href="https://www.jenv.be/" rel="nofollow">jEnv</a> supports both "global" (meaning you, the user)
and "project" choices of JDK (particular to a directory and its children) in
which JDK installation to use. You may notice the
<a href="https://github.com/binkley/modern-java-practices/blob/master/.java-version"><code>.java-version</code></a> file: this is a per-project file for jEnv to
pick your project Java version.</p>
<p dir="auto">Do use <code>jenv enable-plugins export</code> and restart your shell. This ensures
<code>JAVA_HOME</code> is exported to match your jEnv settings. Several tools use
<code>JAVA_HOME</code> rather than the <code>java</code> or <code>javac</code> found in your <code>PATH</code>.</p>
<p dir="auto">You may also find the <code>gradle</code> and <code>maven</code> plugins for jEnv useful.</p>
<p dir="auto">There are many ways to install the JDK, most are platform-dependent. In general,
your team will be better off using a "managed" approach, rather than with each
person using binary installers. Popular choices include:</p>
<ul dir="auto">
<li><a href="https://adoptium.net/installation.html#linux-pkg" rel="nofollow">Apt and friends</a> for Linux
or WSL</li>
<li><a href="https://brew.sh/" rel="nofollow">Homebrew</a> for Mac</li>
<li><a href="https://sdkman.io/jdks" rel="nofollow">SDKMAN</a> for multiple platforms</li>
</ul>
<h4 tabindex="-1" dir="auto">Direnv</h4>
<p dir="auto"><a href="https://direnv.net/" rel="nofollow">direnv</a> is more general. Rather than specifying a Java
version, you edit a <code>.envrc</code> file and add JDK-specific environment settings
(and another other environment settings) just as you would on the command-line.
Typically set are <code>PATH</code> to find <code>java</code> and <code>javac</code> programs, and <code>JAVA_HOME</code>.</p>
<hr>

<a href="https://maven.apache.org/" title="Maven" rel="nofollow">
<img src="https://github.com/binkley/modern-java-practices/raw/master/images/maven.png" alt="Maven" width="15%" height="auto"></a>
<a href="https://gradle.org/" title="Gradle" rel="nofollow">
<img src="https://github.com/binkley/modern-java-practices/raw/master/images/gradle.png" alt="Gradle" width="15%" height="auto"></a> 
<h2 tabindex="-1" dir="auto">Use Gradle or Maven</h2>
<p dir="auto">The choice between Gradle and Maven depends on your team, your broader
ecosystem, and your project needs. In summary:</p>
<ul dir="auto">
<li>
<p dir="auto">Gradle — your build script is written in Groovy or Kotlin; dynamic,
imperative, and mutable; requires debugging your build on occasion, but less
verbose than Maven's XML. Use of "parent" (umbrella) projects is possible but
challenging. You can locally extend your build script either <em>inline</em>
with build code, with project plugins, or with plugins from a separate
project (perhaps shared across project for your team). If interested in custom
plugins,
<a href="https://docs.gradle.org/current/userguide/custom_plugins.html" rel="nofollow">read more here</a></p>
</li>
<li>
<p dir="auto">Maven — your build scripts is written in XML; declarative and immutable;
verbose but specific; it either works or not. Use of "parent" (umbrella)
projects is simple with built-in support. You can locally extend your build
with plugins from a separate project (perhaps shared across project for your
team). If interested in custom plugins,
<a href="https://maven.apache.org/guides/plugin/guide-java-plugin-development.html" rel="nofollow">read more here</a></p>
</li>
</ul>
<p dir="auto">For Modern Java/JVM projects, <strong>use Gradle or Maven</strong>. The article doesn't cover
alternative build tools:
<a href="https://www.jrebel.com/blog/2020-java-technology-report#build-tool" rel="nofollow">industry data</a>
shows Gradle or Maven are the build tools for most folks. Unless you find
yourself in a complex monorepo culture (Google, <em>etc.</em>), or there are mandates
from above, you need to select one of Gradle or Maven. However, for projects not
using Gradle or Maven, you will still find improvements for your build herein
(though details will differ).</p>
<p dir="auto">For new projects, you may find <a href="https://start.spring.io/" rel="nofollow">Spring Initializr</a>,
<a href="https://micronaut.io/" rel="nofollow"><code>mn</code> from Micronaut</a>, or
<a href="https://www.jhipster.tech/" rel="nofollow">JHipster</a>, among many other project excellent
starters, more to your liking: they provide you with starter Gradle or Maven
scripts specific for those frameworks. <em>That's great!</em> This article should
still help you improve your build beyond "getting started". You should pick
and choose build features as makes sense to you and your circumstances.</p>
<p dir="auto">This article offers <strong>no preference between Gradle or Maven</strong>. You need to
decide with your team and circumstances. After picking your build tool, you
might rename <a href="https://github.com/binkley/modern-java-practices/blob/master/run-with-gradle.sh">run-with-gradle.sh</a> or
<a href="https://github.com/binkley/modern-java-practices/blob/master/run-with-maven.sh">runs-with-maven.sh</a> to just <code>run.sh</code> or similar.</p>
<p dir="auto">Projects using Ant <strong>should migrate</strong>. It is true that Ant is well-maintained
(the latest version dates from 2021). However, you will spend much effort in
providing modern build tooling, and effort in migrating from Ant is repaid in
smaller work for integrating modern tools. Data point: consider the number of
<a href="https://stackoverflow.com/" rel="nofollow">Stackoverflow</a> posts providing Gradle or
Maven answers to those for Ant.  <em>Consider Ant builds to be no longer
well-supported, and a form of
<a href="https://www.martinfowler.com/bliki/TechnicalDebt.html" rel="nofollow">Tech Debt</a>.</em></p>
<p dir="auto">Throughout, when covering both Gradle and Maven, Gradle will be discussed first,
then Maven. This is no expressing a preference!  It is neutral alphabetical
ordering.</p>
<p dir="auto"><strong>NB</strong> — Gradle Enterprise provides additional features for Maven as
well such as <a href="https://docs.gradle.com/enterprise/maven-build-cache/" rel="nofollow">build
caching</a> and <a href="#keep-your-build-fast">build
scans</a>.</p>
<h3 tabindex="-1" dir="auto">Cleaning up</h3>
<p dir="auto">Once you pick between Gradle or Maven, it is a good time to clean up.
If you have cloned the project, some renames/deletions to consider:</p>
<ul dir="auto">
<li><a href="https://github.com/binkley/modern-java-practices/blob/master/run-with-gradle.sh">run-with-gradle.sh</a> or
<a href="https://github.com/binkley/modern-java-practices/blob/master/run-with-maven.sh">runs-with-maven.sh</a> → just <code>run.sh</code> or <code>build.sh</code> or
anything you like. Remember to document in <code>README.md</code> for others</li>
<li><a href="https://github.com/binkley/modern-java-practices/blob/master/batect.yml">batect.yml</a> — update the task names, and remove those not
relevant. Again, don't forget about <code>README.md</code> instructions</li>
<li><a href="https://github.com/binkley/modern-java-practices/blob/master/.github/workflows/ci-maven.yml">ci.yml</a> — update the jobs, and remove those
not relevant. Did I mention <code>README.md</code>?</li>
</ul>
<p dir="auto">You are ready to make great software.</p>
<h3 tabindex="-1" dir="auto">Keeping Gradle or Maven up to date</h3>
<h4 tabindex="-1" dir="auto">Gradle</h4>
<p dir="auto">To update Gradle:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ $EDITOR gradle.properties  # Update gradleWrapperVersion property
$ ./gradlew wrapper  # Update scripts and supporting files
$ ./gradlew wrapper  # Confirm, and download files if needed"><pre>$ <span>$EDITOR</span> gradle.properties  <span><span>#</span> Update gradleWrapperVersion property</span>
$ ./gradlew wrapper  <span><span>#</span> Update scripts and supporting files</span>
$ ./gradlew wrapper  <span><span>#</span> Confirm, and download files if needed</span></pre></div>
<h4 tabindex="-1" dir="auto">Maven</h4>
<p dir="auto">To update Maven:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ $EDITOR pom.xml  # Update maven.version property
$ ./mvnw wrapper:wrapper  # Update scripts and supporting files
$ ./mvnw wrapper:wra  # Confirm, and download files if needed"><pre>$ <span>$EDITOR</span> pom.xml  <span><span>#</span> Update maven.version property</span>
$ ./mvnw wrapper:wrapper  <span><span>#</span> Update scripts and supporting files</span>
$ ./mvnw wrapper:wra  <span><span>#</span> Confirm, and download files if needed</span></pre></div>
<p dir="auto">Note that Maven wrapper is developing, and will be bundled with an upcoming
Maven release.
For now it is a separate plugin in your <code>pom.xml</code>.</p>
<h3 tabindex="-1" dir="auto">Tips</h3>
<ul dir="auto">
<li>Take advantage of your shell's tab completion:
<ul dir="auto">
<li><a href="https://github.com/gradle/gradle-completion">Gradle completion</a></li>
<li><a href="https://github.com/juven/maven-bash-completion">Maven completion</a></li>
</ul>
</li>
<li>The sample Gradle and Maven build scripts often specify specific versions of
the tooling, separate from the plugin versions. This is intentional. You
should be able to update the latest tool version even when the plugin has not
yet caught up</li>
<li>Gradle itself does not provide support for "profiles", a key Maven feature.
This is <em>different</em> from <em>profiling</em> build performance!  Maven profiles
can be used in many ways. The most common are to enabling/disabling build
features on the command line, tailoring the build to a particular deployment
environment, or using different credentials for other systems. If this
feature is important for your team, you can code <code>if/else</code> blocks directly
in <code>build.gradle</code>, or use a plugin such as
<a href="https://kordamp.org/kordamp-gradle-plugins/#_org_kordamp_gradle_profiles" rel="nofollow">Kordamp Profiles Gradle plugin</a>
(Kordamp has a suite of interesting Gradle plugins beyond this one; read more
on that page)</li>
<li>Gradle uses advanced terminal control, so you cannot always see what is
happening. To view Gradle steps plainly when debugging your build, use:
<div dir="auto" data-snippet-clipboard-copy-content="$ ./gradlew <your tasks> | cat"><pre>$ ./gradlew <span>&lt;</span>your tasks<span>&gt;</span> <span>|</span> cat</pre></div>
or save the output to a file:
<div dir="auto" data-snippet-clipboard-copy-content="$ ./gradlew <your tasks> | tee -o some-file"><pre>$ ./gradlew <span>&lt;</span>your tasks<span>&gt;</span> <span>|</span> tee -o some-file</pre></div>
</li>
<li>If your source code is in Kotlin, so should be your build. Gradle provides
<a href="https://kotlinlang.org/docs/reference/using-gradle.html" rel="nofollow">a Kotlin DSL for build scripts</a>
as a first-class counterpart to the traditional Groovy DSL</li>
<li>Maven colorizes output, but does not use terminal control to overwrite output</li>
<li>See <a href="#setup-your-ci">Setup your CI</a> for another approach to getting plain text
console output</li>
<li><a href="https://github.com/jcgay/maven-notifier">The Maven Notifier</a> may be to your
liking</li>
<li>If you like Maven, but XML isn't your thing, you might explore the
<a href="https://github.com/takari/polyglot-maven"><em>Polyglot for Maven</em></a> extension
which provides the POM in multiple languages/formats (<em>eg</em>, Ruby, YAML, many
others)</li>
<li>If you have a multi-module Maven build, you might consider
<a href="http://takari.io/book/30-team-maven.html#takari-smart-builder" rel="nofollow"><em>Takari Smart
Builder</em></a>
to speed it up</li>
<li>Maven best practice is to specify the version for each plugin, <em>even default
plugins</em> that come with your version of Maven. This enforces
<strong>reproducible builds</strong>. See also the
<a href="https://maven.apache.org/enforcer/maven-enforcer-plugin/" rel="nofollow">Maven Enforcer Plugin</a>
to exactly specify the version of Maven for your build</li>
</ul>
<hr>
<p dir="auto"><a href="http://www.ambysoft.com/essays/whyAgileWorksFeedback.html" title="Why Agile Software Development Techniques Work: Improved Feedback" rel="nofollow">
<img src="https://github.com/binkley/modern-java-practices/raw/master/images/bug-costs.jpg" alt="Length of Feedback Cycle" width="20%" height="auto">
</a></p>
<h2 tabindex="-1" dir="auto">Setup your CI</h2>
<p dir="auto">Your CI is your "source of truth" for successful builds. Your goal:
<em>Everyone trusts a "green" CI build is solid</em>.</p>
<p dir="auto">When using GitHub, a simple starting point is
<a href="https://github.com/binkley/modern-java-practices/blob/master/.github/workflows/ci-gradle.yml"><code>ci-gradle.yml</code></a> or
<a href="https://github.com/binkley/modern-java-practices/blob/master/.github/workflows/ci-maven.yml"><code>ci-maven.yml</code></a>.
(GitLab is similar, but as this project is hosted in GitHub, there is not a
simple means to demonstrate CI at GitLab).
This project includes a workflow for Gradle and a workflow for Maven as
examples.</p>
<p dir="auto">If you use GitLab, read about the equivalent in
<a href="https://docs.gitlab.com/ee/ci/" rel="nofollow"><em>GitLab CI/CD</em></a>, or for Jenkins in
<a href="https://www.jenkins.io/doc/book/pipeline/" rel="nofollow"><em>Pipeline</em></a>.</p>
<p dir="auto">When publishing your project, consider
<a href="https://docs.github.com/en/actions/guides/publishing-java-packages-with-maven"><em>Publishing Java packages with
Maven</em></a>
for GitHub, or equivalent for other CI systems.  <strong>Do not publish from local
builds.</strong>  For GitHub, note the limitations of
<a href="https://github.community/t/download-from-github-package-registry-without-authentication/14407" rel="nofollow"><em>Download from Github Package Registry without
authentication</em>.</a></p>
<h3 tabindex="-1" dir="auto">Save your CI artifacts</h3>
<p dir="auto">It is helpful to preserve your build artifacts from CI, for example, to
download built jars from different CI runs for comparing their behavior
between commits without needing to rebuild locally, and also to confirm that
your local build makes the same jars as CI does.</p>
<p dir="auto">The "Build with Gradle" and "Build with Maven" CI workflows each provide a
download named "jars", and the Maven build a "site" download.</p>
<p dir="auto">There are services to provide links to the most recent build artifacts.
One example is <a href="https://nightly.link/" rel="nofollow">nightly.link</a> (this is not an
endorsement).
You can use these links in your <code>README.md</code> or share as makes sense.
An example is
<a href="https://nightly.link/binkley/modern-java-practices/workflows/ci-maven/master/jars.zip" rel="nofollow">downloading the Maven-built jar</a>
from this project.</p>
<h3 tabindex="-1" dir="auto">Tips</h3>
<ul dir="auto">
<li>To disable ASCII colorizing printing as control sequences in CI, or
Gradle trying to overwrite lines (control sequences make for hard-to-read
CI logs), a simple approach is to use an environment setting:

This does not make sense for local builds, and your CI system (<em>eg</em>, GitHub)
may manage this problem already</li>
<li>With Gradle, use the <code>--warning-mode=all</code> flag for CI: this shows <em>all</em>
warnings Gradle generates, not just a summary. See
<a href="https://docs.gradle.org/current/userguide/command_line_interface.html#sec:command_line_warnings" rel="nofollow"><em>Showing or hiding
warnings</em></a>
for details</li>
<li>With Maven, use the <code>--no-transfer-progress</code> flag for CI: this avoids spamming
CI logs with download progress messages</li>
<li>For GitHub Actions, you may find a tool like
<a href="https://github.com/nektos/act"><code>act</code></a> useful for running CI actions locally
(it may not work for all projects, however, depending on your actions)</li>
</ul>
<hr>
<p dir="auto"><a href="https://github.com/binkley/html/blob/master/blog/on-pipelines.html" title="On Pipelines">
<img src="https://github.com/binkley/modern-java-practices/raw/master/images/pipeline.png" alt="Production vs Dev pipeline" width="20%" height="auto">
</a></p>
<h2 tabindex="-1" dir="auto">Keep local consistent with CI</h2>
<p dir="auto">What is "local CI"?
That sounds like a contradition.
Tooling helps you reproduce locally the same build that CI uses, so that you
suffer less from version drift and other type problems, and minimize
related environment issues.
A common example is building on different JVM/JDK versions.
Ideally, excepting truly environment-specific, your local build should fail
when CI would also fail so that you can catch problems earlier in your
development process before commits are shared.</p>
<h3 tabindex="-1" dir="auto">Setup local CI</h3>
<p dir="auto">Reflecting the principle that local builds should be like CI builds, some
tools that greatly help:</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://batect.dev/" rel="nofollow">Batect</a> is a solid tool from Charles Korn.
It runs your build in a "CI-like" local environment via Docker.
This is one of your first lines of defence against "it runs on my box".
(<a href="https://batect.dev/Comparison.html" rel="nofollow">Compare Batect</a> with other tools in this
space)</p>
</li>
<li>
<p dir="auto"><a href="https://earthly.dev/" rel="nofollow">Earthly</a> shares philosophy with Batect and with a
different approach to implementation.
<strong>Feedback on Earthly</strong> is
<a href="https://github.com/binkley/modern-java-practices/issues/new/choose">appreciated</a>.
<em>Earthly is experimental</em> for the template project in this repository</p>
</li>
</ul>
<p dir="auto">They are <em>both good choices</em>, but not the only ones.</p>
<p dir="auto"><em>This is an important step</em>!
It is closer to your CI builds locally.
You should strive to keep local as faithful as possible to CI and Production.</p>
<p dir="auto">You may decide not to use CI-like tooling for local builds. However, consider
that use of them raises your confidence that CI will succeed. Local CI-like
tooling is part of the theme of <em>shifting left</em> for problems.</p>
<p dir="auto">See <a href="https://batect.dev/tools/GitHubActions.html" rel="nofollow"><em>Working with CI systems</em></a>
for documentation on using Batect from within a dockerized CI environment.</p>
<p dir="auto"><strong>NB</strong> — to be as consistent as possible, the sample
<a href="https://github.com/binkley/modern-java-practices/blob/master/.github/workflows/ci-maven.yml"><code>ci.yml</code> for GitHub</a> uses Batect for the
Gradle and Maven builds, and <a href="https://github.com/binkley/modern-java-practices/blob/master/batect.yml"><code>batect.yml</code> for Batect</a> pulls
an image for <a href="https://hub.docker.com/_/adoptopenjdk" rel="nofollow">AdoptOpenJDK</a>.
So <code>ci.yml</code> does not <a href="https://github.com/actions/setup-java">setup JDK 17</a>
directly, but relies on Batect.</p>
<p dir="auto">Configure your local CI in <a href="https://github.com/binkley/modern-java-practices/blob/master/batect.yml"><code>batect.yml</code></a> or in
<a href="https://github.com/binkley/modern-java-practices/blob/master/Earthfile"><code>Earthfile</code></a> with suitable tasks.
For this project, there are example tasks/targets:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ./batect -T
Available tasks:
- build-with-gradle: Build and test with Gradle
- build-with-maven: Build and test with Maven

$ earthly ls
+base
+build-with-gradle
+build-with-maven"><pre>$ ./batect -T
Available tasks:
- build-with-gradle: Build and <span>test</span> with Gradle
- build-with-maven: Build and <span>test</span> with Maven

$ earthly ls
+base
+build-with-gradle
+build-with-maven</pre></div>
<h4 tabindex="-1" dir="auto">Gradle</h4>
<p dir="auto">It is helpful that your <code>batect.yml</code> calls Gradle with the <code>--no-daemon</code> flag:</p>
<ul dir="auto">
<li>There is no point in spinning up a daemon for a Docker ephemeral container;
but it is harmless either way</li>
<li>With a daemon, the Docker container's Gradle may be confused by
<code>~/.gradle/daemon</code> and <code>/.gradle/workers</code> directories mounted by Batect from
your home directory, as these refer to processes in the host, not the
container (<code>batect.yml</code> mounts your <code>~/.gradle</code> to include caches of
already-downloaded dependencies, <em>et al</em>)</li>
<li>If you encounter troubles, run locally <code>./gradlew --stop</code> to kill any local
daemons: This indicates a <em>bug</em>, and "stop" is a workaround.
See <a href="https://github.com/batect/batect/issues/680#issuecomment-719821099" data-hovercard-type="issue" data-hovercard-url="/batect/batect/issues/680/hovercard">a suggestion of a better approach</a></li>
</ul>
<h4 tabindex="-1" dir="auto">Earthly</h4>
<p dir="auto">Earthly has its own caching strategies that apply to your build (such as
Gradle or Maven dependency downloads) based around Docker layers. See
<a href="https://docs.earthly.dev/docs/guides/advanced-local-caching" rel="nofollow"><em>Advanced local
caching</em></a> for more information.</p>
<h3 tabindex="-1" dir="auto">Tips</h3>
<ul dir="auto">
<li>
<p dir="auto">If you encounter issues with Gradle and Batect, try stopping the local Gradle
daemons before running Batect:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ./gradlew --stop
$ ./batect <your Batect arguments>"><pre>$ ./gradlew --stop
$ ./batect <span>&lt;</span>your Batect arguments<span>&gt;</span></pre></div>
</li>
<li>
<p dir="auto">The Batect builds <em>assume</em> you've run local builds first. Plesae run
<code>./gradlew build</code> or <code>./mvnw verify</code> at least once before running
<code>./batect ...</code> to ensure cached/shared downloads are present</p>
</li>
<li>
<p dir="auto">In CI, use the <code>--permanently-enable-telemetry</code> flag to avoid CI asking a
"Y/N" question. This <strong>must</strong> be <em>separate step</em> from running the build
itself. See <a href="https://github.com/binkley/modern-java-practices/blob/master/.github/workflows/ci-maven.yml"><code>ci.yml</code></a> for Gradle and Maven examples</p>
</li>
<li>
<p dir="auto">Run your local Gradle or Maven build before you run with Batect if you have
updated dependencies.  This is helpful when fetching the dependencies, and
sometimes avoids awkwardness (sometimes you may have different access rights
for the build tool caches when run as yourself directly <em>vs</em> running in a
container)</p>
</li>
</ul>
<hr>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/binkley/modern-java-practices/blob/master/images/maintain-build.jpg"><img src="https://github.com/binkley/modern-java-practices/raw/master/images/maintain-build.jpg" alt="Maintain build" width="20%" height="auto"></a></p>
<h2 tabindex="-1" dir="auto">Maintain your build</h2>
<p dir="auto">Treat your build as you would your codebase: Maintain it, refactor as needed,
run performance testing, <em>et al</em>.</p>
<h3 tabindex="-1" dir="auto">Know what your build does</h3>
<p dir="auto">What does your build do exactly, and in what order? You can ask Gradle or Maven
to find out:</p>
<ul dir="auto">
<li><a href="https://github.com/dorongold/gradle-task-tree">Gradle Task Tree plugin</a>
with <code>./gradlew some...tasks taskTree</code></li>
<li><a href="https://buildplan.jcgay.fr/" rel="nofollow">Maven Buildplan plugin</a>
with <code>./mvnw buildplan:list</code> (see plugin documentation for other goals and
output format)</li>
</ul>
<p dir="auto">Each of these have many options and features, and are worth exploring.</p>
<h3 tabindex="-1" dir="auto">Keep your build clean</h3>
<p dir="auto">Let tools tell you when you have dodgy dependencies, or an inconsistent setup.
For example, leverage <code>jdeps</code> which
<a href="https://docs.oracle.com/en/java/javase/17/docs/specs/man/jdeps.html" rel="nofollow">comes with the JDK</a>.
Jdeps spots, for example, if you have a multi-version jar as a dependency that
does not include <em>your</em> JDK version (an example of this may be is JUnit), or if
your code depends on <em>internal</em> (non-public) classes of the JDK
(important especially when using the JDK module system).</p>
<h4 tabindex="-1" dir="auto">Gradle</h4>
<p dir="auto">The <a href="https://github.com/kordamp/jdeps-gradle-plugin">Kordamp plugin</a> used for
Gradle does not fail the build when jdeps errors, and only generates a report
text file. See
<a href="https://github.com/kordamp/jdeps-gradle-plugin/issues/16" data-hovercard-type="issue" data-hovercard-url="/kordamp/jdeps-gradle-plugin/issues/16/hovercard">this issue</a>.</p>
<h4 tabindex="-1" dir="auto">Maven</h4>
<p dir="auto">Try Maven with <code>dependency:tree -Dverbose</code>.
This will show conflicting versions of dependencies.</p>
<h3 tabindex="-1" dir="auto">Keep local builds quiet</h3>
<p dir="auto">It is frustrating for local devs when something horrible happened during the
build (say a production with "ERROR" output during a test), but:</p>
<ol dir="auto">
<li>The build is <strong>GREEN</strong>, and developers should trust that</li>
<li>There is too much output in the local build, so developers don't spot
telltale signs of trouble</li>
</ol>
<p dir="auto">There are many approaches to this problem. This project uses JDK logging as <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.logging/java/util/logging/FileHandler.html" rel="nofollow">an
example</a>,
and keeps the build quiet in
<a href="https://github.com/binkley/modern-java-practices/blob/master/config/logging.properties"><code>config/logging.properties</code></a>.</p>
<h3 tabindex="-1" dir="auto">Keep CI builds noisy</h3>
<p dir="auto">In CI, this is different, and there you want as much output as possible to
diagnose the unexpected.</p>

<h3 tabindex="-1" dir="auto">Keep your build current</h3>
<p dir="auto">An important part of <em>build hygiene</em> is keeping your build system, plugins, and
dependencies up to date. This might be simply to address bug fixes
(including bugs you weren't aware of), or might be critical security fixes. The
best policy is: <em>Stay current</em>. Others will have found—reported
problems—, and 3<sup>rd</sup>-parties may have addressed them. Leverage
the power of <a href="https://en.wikipedia.org/wiki/Linus%27s_law" rel="nofollow"><em>Linus' Law</em></a> ("given
enough eyeballs, all bugs are shallow").</p>
<h3 tabindex="-1" dir="auto">Keep plugins and dependencies up-to-date</h3>
<ul dir="auto">
<li><a href="https://github.com/ben-manes/gradle-versions-plugin">Gradle</a>
Benjamin Manes is kind enough in his plugin project to list alternatives.
If you are moving towards <a href="https://docs.gradle.org/current/userguide/platforms.html" rel="nofollow">Gradle version
catalogs</a>, you might
consider
<a href="https://jmfayard.github.io/refreshVersions/" rel="nofollow">refreshVersions</a></li>
<li><a href="https://www.mojohaus.org/versions-maven-plugin/" rel="nofollow">Maven</a></li>
<li>Team agreement on release updates only, or if non-release plugins and
dependencies make sense for your situation</li>
<li>Each of these plugins for Gradle or Maven have their quirks.  <strong>Do not treat
them as sources of truth but as recommendations</strong>. <em>Use your judgment</em>. In
parallel, take advantage of CI tooling such as
<a href="https://github.com/dependabot">Dependabot (Github)</a> or
<a href="https://gitlab.com/dependabot-gitlab/dependabot" rel="nofollow">Dependabot (GitLab)</a></li>
</ul>
<p dir="auto">An example use which shows most outdated plugins and dependencies (note that one
Maven example modifies your <code>pom.xml</code>, a fact you can choose or avoid):</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ./gradlew dependencyUpdates
# output ommitted
$ ./mvnw versions:update-properties  # Updates pom.xml in place
$ ./mvnw versions:display-property-updates  # Just lists proposed updates
# output ommitted"><pre>$ ./gradlew dependencyUpdates
<span><span>#</span> output ommitted</span>
$ ./mvnw versions:update-properties  <span><span>#</span> Updates pom.xml in place</span>
$ ./mvnw versions:display-property-updates  <span><span>#</span> Just lists proposed updates</span>
<span><span>#</span> output ommitted</span></pre></div>
<p dir="auto">This project keeps Gradle version numbers in
<a href="https://github.com/binkley/modern-java-practices/blob/master/gradle.properties"><code>gradle.properties</code></a>, and for Maven in
<a href="https://github.com/binkley/modern-java-practices/blob/master/pom.xml">the POM</a>, and you should do the same.</p>
<p dir="auto">Since your <code>pom.xml</code> is in Git, <code>versions:update-properties</code> is <em>safe</em> as you
can always revert changes, but some folks want to look before doing.</p>
<h4 tabindex="-1" dir="auto">Tips</h4>
<ul dir="auto">
<li>Gradle and Maven provide <em>default versions</em> of bundled plugins. In both built
tools, the version update plugins need you to be <em>explicit</em> in stating
versions for bundled plugins, so those versions are visible for update</li>
<li>Enable <em>HTML reports</em> for local use; enable <em>XML reports</em> for CI use in
integrating with report tooling</li>
<li>To open the report for Jdeps, build locally and use the
<code>&lt;project root&gt;/build/reports/jdeps/</code> (Gradle) path.
The path shown in a Docker build is relative to the interior of the container</li>
</ul>
<h3 tabindex="-1" dir="auto">Automated dependency upgrade PRs</h3>
<p dir="auto"><em>NB</em> —
<a href="https://github.blog/2020-06-01-keep-all-your-packages-up-to-date-with-dependabot/" rel="nofollow">Dependabot</a>
may prove speedier for you than updating dependency versions locally, and runs
in CI (GitHub) on a schedule you pick. It submits PRs to your repository when it
finds out of date dependencies. See
<a href="https://github.com/binkley/modern-java-practices/blob/master/.github/dependabot.yml"><code>dependabot.yml</code></a> for an example using a daily
schedule.</p>
<p dir="auto">A similar choice is <a href="https://github.com/renovatebot/renovate">Renovate</a>.</p>
<h4 tabindex="-1" dir="auto">More on Gradle version numbers</h4>
<p dir="auto">Your simplest approach to Gradle is to keep everything in <code>build.gradle</code>. Even
this unfortunately still requires a <code>settings.gradle</code> to define a project
artifact name, and leaves duplicate version numbers for related dependencies
scattered through <code>build.gradle</code>.</p>
<p dir="auto">Another approach is to rely on a Gradle plugin such as that from Spring Boot to
manage dependencies for you. This unfortunately does not help with plugins at
all, nor with dependencies that Spring Boot does not know about.</p>
<p dir="auto">This project uses a 3-file solution for Gradle versioning, and you should
consider doing the same:</p>
<ul dir="auto">
<li><a href="https://github.com/binkley/modern-java-practices/blob/master/gradle.properties"><code>gradle.properties</code></a> is the sole source of truth for
version numbers, both plugins and dependencies</li>
<li><a href="https://github.com/binkley/modern-java-practices/blob/master/settings.gradle"><code>settings.gradle</code></a> configures plugin versions using the
properties</li>
<li><a href="https://github.com/binkley/modern-java-practices/blob/master/build.gradle"><code>build.gradle</code></a> uses plugins without needing version numbers,
and dependencies refer to their property versions</li>
</ul>
<p dir="auto">The benefits of this approach grow for Gradle multi-project projects, where you
may have plugin and dependency versions scattered across each <code>build.gradle</code>
file for you project and subprojects.</p>
<p dir="auto">So to adjust a version, edit <code>gradle.properties</code>. To see this approach in action
for dependencies, try:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ grep junitVersion gradle.properties setttings.gradle build.gradle
gradle.properties:junitVersion=5.7.0
build.gradle:    testImplementation &quot;org.junit.jupiter:junit-jupiter:$junitVersion&quot;
build.gradle:    testImplementation &quot;org.junit.jupiter:junit-jupiter-params:$junitVersion&quot;"><pre>$ grep junitVersion gradle.properties setttings.gradle build.gradle
gradle.properties:junitVersion=5.7.0
build.gradle:    testImplementation <span><span>"</span>org.junit.jupiter:junit-jupiter:<span>$junitVersion</span><span>"</span></span>
build.gradle:    testImplementation <span><span>"</span>org.junit.jupiter:junit-jupiter-params:<span>$junitVersion</span><span>"</span></span></pre></div>
<h4 tabindex="-1" dir="auto">Note on <code>toolVersion</code> property</h4>
<p dir="auto">If you use the <code>toolVersion</code> property for a plugin to update the called tool
separately from the plugin itself, <em>this is a convention</em>, not something the
Gradle API provides to plugins. As a consequence, the Versions plugin is unable
to know if your tool version is out of date. An example is the JaCoCo plugin
distributed with Gradle.</p>
<p dir="auto">Two options:</p>
<ul dir="auto">
<li>Do not use the <code>toolVersion</code> property unless needed to address a discovered
build issue, and remove it once the plugin catches up to provide the tool
version you need</li>
<li>Continue using the <code>toolVersion</code> property, and as part of running
<code>./gradlew dependencyUpdates</code>, manually check all <code>toolVersion</code>
properties, and update <code>gradle.properties</code> as accordingly</li>
</ul>
<p dir="auto"><strong>NB</strong> — Maven handles this differently, and does not have this concern.</p>
<h3 tabindex="-1" dir="auto">Keep your build fast</h3>
<p dir="auto">A fast local build is one of the best things you can do for your team. There are
variants of profiling your build for Gradle and Maven:</p>
<ul dir="auto">
<li><a href="https://scans.gradle.com/" rel="nofollow">Gradle build scan</a> with the <code>--scan</code> flag</li>
<li><a href="https://github.com/jcgay/maven-profiler">Maven profiler extension</a> with
the <code>-Dprofile</code> flag</li>
</ul>
<p dir="auto">See <a href="https://scans.gradle.com/s/fik7c7bq25l3w" rel="nofollow">an example build scan</a> from May
1, 2023.</p>
<p dir="auto"><strong>NB</strong> — <a href="https://scans.gradle.com/#maven" rel="nofollow">Build Scan</a> supports Maven as
well when using the paid enterprise version.</p>
<h3 tabindex="-1" dir="auto">Keep your developers fast</h3>
<p dir="auto">Some shortcuts to speed up the red-green-refactor cycle:</p>
<ul dir="auto">
<li>Just validate code coverage; do not run other parts of the build:
<ul dir="auto">
<li>Gradle: <code>./gradlew clean jacocoTestReport jacocoTestCoverageVerification</code></li>
<li>Maven: <code>./mvnw clean test jacoco:report jacoco:check</code></li>
</ul>
</li>
</ul>
<h3 tabindex="-1" dir="auto">Tips</h3>
<ul dir="auto">
<li>Both <a href="#dependency-check"><em>dependency vulnerability checks</em></a> and
<a href="#use-mutation-testing"><em>mutation testing</em></a>
can take a while, depending on your project. If you find they slow your team
local build too much, these are good candidates for moving to
<a href="#setup-your-ci">CI-only steps</a>, such as a <code>-PCI</code> flag for Maven (see "Tips"
section of <a href="#use-gradle-or-maven">Use Gradle or Maven</a> for Gradle for an
equivalent). This project keeps them as part of the local build, as the
demonstration code is short</li>
<li>See the bottom of <a href="https://github.com/binkley/modern-java-practices/blob/master/build.gradle"><code>build.gradle</code></a> for an example of
customizing "new" versions reported by the Gradle <code>dependencyUpdates</code> task</li>
<li>The equivalent Maven approach for controlling the definition of "new" is to
use <a href="https://www.mojohaus.org/versions-maven-plugin/version-rules.html" rel="nofollow"><em>Version number
rules</em></a></li>
<li>With the Gradle plugin, you can program your build to fail if dependencies are
outdated. Read at
<a href="https://github.com/ben-manes/gradle-versions-plugin/issues/431#issuecomment-703286879" data-hovercard-type="issue" data-hovercard-url="/ben-manes/gradle-versions-plugin/issues/431/hovercard"><em>Configuration option to fail build if stuff is out of
date</em></a>
for details</li>
</ul>
<hr>
<h2 tabindex="-1" dir="auto">Choose your code style</h2>
<p dir="auto">Style is an often overlooked but very critical attribute of writing. The style
of writing directly impacts the readability and understandability of the end
product</p>
<p dir="auto">There are 2 main java code styles</p>
<ul dir="auto">
<li><a href="https://www.oracle.com/technetwork/java/codeconventions-150003.pdf" rel="nofollow">Sun code style</a></li>
<li><a href="https://google.github.io/styleguide/javaguide.html" rel="nofollow">Google code style</a></li>
</ul>
<p dir="auto">It is up to you which one you should choose. But the style should be chosen and
the style should be the same for everyone.</p>
<p dir="auto">To maintain the same standard <code>config/ide/eclipse-java-google-style.xml</code>
or <code>intellij-java-google-style.xml</code> should be imported to your IDE. Checkstyle
should be configured based on the chosen standard</p>
<hr>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/binkley/modern-java-practices/blob/master/images/coffee-grinder.png"><img src="https://github.com/binkley/modern-java-practices/raw/master/images/coffee-grinder.png" alt="Coffee grinder" width="20%" height="auto"></a></p>
<h2 tabindex="-1" dir="auto">Generate code</h2>
<p dir="auto">When sensible, prefer to generate rather than write code. Here's why:</p>
<ul dir="auto">
<li><a href="http://threevirtues.com/" rel="nofollow">Intelligent laziness is a virtue</a></li>
<li>Tools always work, unless they have bugs, and you can fix bugs. Programmers
make typos, and fixing typos is a challenge when not obvious. Worse are <a href="https://en.wiktionary.org/wiki/thinko" rel="nofollow">_
thinkos_</a>; code generation does not "
think", so is immune to this problem</li>
<li>Generated code does not need code review, only the source input for generation
needs review, and this is usually shorter and easier to understand. Only your
hand-written code needs review</li>
<li>Generated code is usually ignored by tooling such as linting or code
coverage (and there are simple workarounds when this is not the case). Your
hand-written code needs tooling to shift problems left</li>
</ul>
<p dir="auto">Note that many features for which in Java one would use code generation
(<em>eg</em>, Lombok's <a href="https://projectlombok.org/features/GetterSetter" rel="nofollow"><code>@Getter</code></a>
or <a href="https://www.projectlombok.org/features/ToString" rel="nofollow"><code>@ToString</code></a>), can be
built-in language features in other languages such as Kotlin or Scala (<em>eg</em>,
<a href="https://kotlinlang.org/docs/reference/properties.html" rel="nofollow">properties</a>
or <a href="https://kotlinlang.org/docs/reference/data-classes.html" rel="nofollow">data classes</a>).</p>
<h3 tabindex="-1" dir="auto">Lombok</h3>
<p dir="auto"><a href="https://projectlombok.org/" rel="nofollow">Lombok</a> is by far the most popular tool in Java for
code generation.
Lombok is an <em>annotation processor</em>, that is, a library (jar) which
cooperates with the Java compiler.
(<a href="https://blog.frankel.ch/introductory-guide-annotation-processor/#handling-annotations-at-compile-time-annotation-processors" rel="nofollow"><em>An introductory guide to annotations and annotation
processors</em></a>
is a good article if you'd like to read more on how annotation processing
works.)</p>
<p dir="auto">Lombok covers many common use cases, does not have runtime dependencies, there
are plugins for popular IDEs that understand Lombok's code generation, and has
tooling integration for JaCoCo's output code coverage (see
<a href="#leverage-lombok-to-tweak-code-coverage">below</a>).</p>
<p dir="auto">Do note though, Lombok is not a panacea, and has detractors.
For example, to generate code as an annotation processor, it in places relies on
internal JDK APIs, though the situation has improved as the JDK exposes those
APIs in portable ways.</p>
<h4 tabindex="-1" dir="auto">Leverage Lombok to tweak code coverage</h4>
<p dir="auto">Be sparing in disabling code coverage!
JaCoCo knows about Lombok's
<a href="https://projectlombok.org/api/lombok/Generated.html" rel="nofollow"><code>@Generated</code></a>, and will
ignore annotated code.</p>
<p dir="auto">A typical use is for the <code>main()</code> method in a framework such as Spring Boot
or <a href="https://micronaut.io/" rel="nofollow">Micronaut</a>.
For a <em>command-line program</em>, you will want to test your <code>main()</code>.</p>
<p dir="auto">Do note that Lombok reflects on internal features of the JDK.
If you have issues, for <em>Maven</em>: use in your project the
<code>--add-opens java.base/java.lang=ALL-UNNAMED</code>
example from <code>.mvn/jvm.config</code>, and look to address these.
The solutions in the project are a "workaround" assuming Java 17.
This is a two-edged sword: as the JVM improves access controls, you may find,
especially dependencies, that there are times you want deep reflection.</p>
<h4 tabindex="-1" dir="auto">Lombok configuration</h4>
<p dir="auto"><a href="https://projectlombok.org/features/configuration" rel="nofollow">Configure Lombok</a> in
<a href="https://github.com/binkley/modern-java-practices/blob/master/src/lombok.config"><code>src/lombok.config</code></a> rather than the project root or a
separate <code>config</code> directory.
At a minimum:</p>
<div dir="auto" data-snippet-clipboard-copy-content="config.stopBubbling=true
lombok.addLombokGeneratedAnnotation=true
lombok.anyConstructor.addConstructorProperties=true
lombok.extern.findbugs.addSuppressFBWarnings=true"><pre><span>config.stopBubbling</span>=true
<span>lombok.addLombokGeneratedAnnotation</span>=true
<span>lombok.anyConstructor.addConstructorProperties</span>=true
<span>lombok.extern.findbugs.addSuppressFBWarnings</span>=true</pre></div>
<p dir="auto">Lines:</p>
<ol dir="auto">
<li><code>stopBubbling</code> tells Lombok that there are no more configuration files higher
in the directory tree</li>
<li><code>addLombokGeneratedAnnotation</code> helps JaCoCo ignore code generated by Lombok</li>
<li><code>addConstructorProperties</code> helps JSON/XML frameworks such as Jackson
(this may not be relevant for your project, but is generally harmless, so the
benefit comes for free)</li>
<li><code>addSuppressFBWarnings</code> helps SpotBugs ignore code generated by Lombok</li>
</ol>
<h3 tabindex="-1" dir="auto">More examples</h3>
<ul dir="auto">
<li>Automating <code>Dockerfile</code> — <a href="https://spring.io/blog/2021/01/04/ymnnalft-easy-docker-image-creation-with-the-spring-boot-maven-plugin-and-buildpacks" rel="nofollow"><em>YMNNALFT: Easy Docker Image Creation with
the Spring Boot Maven Plugin and
Buildpacks</em></a>
(<strong>NB</strong> — you do not need to have a Spring Boot project to use the
plugin: just <a href="https://docs.spring.io/spring-boot/docs/2.4.1/maven-plugin/reference/htmlsingle/#build-image" rel="nofollow">treat the plugin as a "regular"
one</a>)</li>
</ul>
<hr>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/binkley/modern-java-practices/blob/master/images/gear.png"><img src="https://github.com/binkley/modern-java-practices/raw/master/images/gear.png" alt="Gear" width="20%" height="auto"></a></p>
<h2 tabindex="-1" dir="auto">Leverage the compiler</h2>
<p dir="auto">Compilers targeting the JVM generally provide warning flags for dodgy code, and
a flag to turn warnings into errors: Use them.
The compiler is your first line of defense against code issues.</p>
<p dir="auto">For example, add these flags with <code>javac</code>:</p>
<ul dir="auto">
<li><code>-Werror</code> -- turn warnings into errors, and fails the build</li>
<li><code>-Xlint:all,-processing</code> -- enable all warnings excluding annotation
processing</li>
</ul>
<p dir="auto">Be judicious in disabling compiler warnings: they usually warn you for good
reasons.
For <code>javac</code>, disabled warnings might include <code>serial</code> or <code>deprecation</code>.</p>
<p dir="auto">JVM compilers support <code>-Werror</code> (<em>eg</em>, <code>javac</code>, <code>kotlinc</code>, <code>scalac</code>, <em>et al</em>);
enabling/disabling specific warnings may be compiler-specific.</p>
<h3 tabindex="-1" dir="auto">Tips</h3>
<ul dir="auto">
<li>Consider using <a href="https://errorprone.info/" rel="nofollow"><em>Error Prone</em></a>.
<em>Error Prone</em> is an excellent compiler plugin to fail problems earlier: fail
at compile-time rather than a runtime, however it can be overly strict</li>
<li>Lombok annotation processing fails <code>-Xlint:all</code>.
Use <code>-Xlint:all,-processing</code> to bypass warnings about annotation processing.
In addition, using Lombok's configuration to add suppression annotations on
generated code (so other tools will ignore generated code) needs the older
Spotbugs annotations provided as a dependency</li>
</ul>
<hr>
<h2 tabindex="-1" dir="auto">Use linting</h2>
<p dir="auto">"Linting" is static code analysis with an eye towards style and dodgy code
constructs. The term
<a href="https://en.wikipedia.org/wiki/Lint_(software)" rel="nofollow">derives from early UNIX</a>.</p>
<p dir="auto">Linting for modern languages is simple: the compiler complains on your behalf.
This is the case, for example, Golang. Having common team agreements on style
and formatting is a boon for avoiding
<a href="https://en.wikipedia.org/wiki/Law_of_triviality" rel="nofollow">bikeshedding</a>, and aids in:</p>
<ul dir="auto">
<li>Reading a code base, relying on a similar style throughout</li>
<li>Code reviews, focusing on substantive over superficial changes</li>
<li>Merging code, avoiding trivial or irrelevant conflicts</li>
</ul>
<p dir="auto">Code style and formatting are <em>entirely</em> a matter of team discussion and
agreement.
In Java, there is no recommended style, and <code>javac</code> is good at parsing almost
anything thrown at it.
However, humans reading code are not as well-equipped.</p>
<p dir="auto"><strong>Pick a team style, stick to it, and <em>enforce</em> it with tooling.</strong></p>
<p dir="auto">See the section <a href="#checkstyle"><em>Checkstyle</em></a> for more details on enforcement.</p>
<h3 tabindex="-1" dir="auto">Tips</h3>
<ul dir="auto">
<li>If you use Google Java coding conventions, consider
<a href="https://github.com/diffplug/spotless">Spotless</a> which can autoformat your
code</li>
<li>Consider use of <a href="https://editorconfig.org/" rel="nofollow">EditorConfig</a> for teams in which
editor choice is up to each developer. EditorConfig is a cross-IDE standard
means of specifying code formatting, respected by
<a href="https://www.jetbrains.com/help/idea/configuring-code-style.html#editorconfig" rel="nofollow">IntelliJ</a>,
and other major editors</li>
<li>To open the report for Checkstyle, build locally and use the
<code>&lt;project root&gt;/build/reports/checkstyle/</code> path.
The path shown in a Docker build is relative to the interior of the container</li>
</ul>
<hr>
<h2 tabindex="-1" dir="auto">Use static code analysis</h2>
<h3 tabindex="-1" dir="auto">Checkstyle</h3>
<p dir="auto">With Java, one needs to rely on external tooling for linting. The most popular
choice is:</p>
<ul dir="auto">
<li><a href="https://checkstyle.sourceforge.io/" rel="nofollow">CheckStyle</a>
<ul dir="auto">
<li><a href="https://docs.gradle.org/current/userguide/checkstyle_plugin.html" rel="nofollow">Gradle plugin</a></li>
<li><a href="https://maven.apache.org/plugins/maven-checkstyle-plugin/index.html" rel="nofollow">Maven plugin</a></li>
</ul>
</li>
</ul>
<p dir="auto">However, Checkstyle will not auto-format code for you. For auto-formatting,
consider, depending on your team preferences, any of these build plugins:</p>
<ul dir="auto">
<li><a href="https://github.com/diffplug/spotless">Spotless</a> — Focus on Google style
guides for Java. For Gradle, use the <code>spotlessApply</code> task to reformat; for
Maven use the <code>spotless:apply</code> goal to reformat. It supports <em>many</em>
source languages, not just Java</li>
<li><a href="https://github.com/openrewrite/rewrite">Rewrite</a> — General framework
for transforming source code with many extensions and plugins for different
languages and frameworks. For Gradle, see
<a href="https://github.com/openrewrite/rewrite-gradle-plugin">Rewrite for Gradle</a>
with a focus on SUN style guides for Java; for Maven, see
<a href="https://docs.openrewrite.org/java/checkstyle" rel="nofollow">Rewrite for Maven</a> with a focus
on SUN style guides for Java</li>
</ul>
<p dir="auto">For your editor, consider the sample <a href="https://github.com/binkley/modern-java-practices/blob/master/.editorconfig">.editorconfig</a> file in
this project. It is respected by IntelliJ and many other code editors.<br>
(The sample uses 80-character line limits as IBM and Hollerith punch cards
intended, and helpful for speed readers of code. A worth point of team
discussion.)</p>
<p dir="auto">The demonstration projects assume checkstyle configuration at
<a href="https://github.com/binkley/modern-java-practices/blob/master/config/checkstyle/checkstyle.xml"><code>config/checkstyle/checkstyle.xml</code></a>. This
is the default location for Gradle, and configured for Maven in the project.</p>
<p dir="auto">The Checkstyle configuration used is stock
<a href="https://github.com/checkstyle/checkstyle/blob/master/src/main/resources/sun_checks.xml"><code>sun_checks.xml</code></a>
(this is SUN default style for Java) with the addition of support for
<code>@SuppressWarnings(checkstyle:...)</code>. Note that this format is <em>overly
aggressive</em> for Javadocs, and needs tweaking for most projects. See comments
in <code>build.gradle</code> about SUN <em>vs</em> Google styles for Java.</p>
<h3 tabindex="-1" dir="auto">Spotbugs</h3>
<p dir="auto"><em>Static code analysis</em> is important in your build. This is analysis of your
source and compiled bytecode which finds known
<a href="https://spotbugs.readthedocs.io/en/latest/bugDescriptions.html" rel="nofollow">issues</a>
ranging among other things:</p>
<ul dir="auto">
<li>Idioms that your team finds poor or hard to read</li>
<li>Dangerous anti-patterns (<em>eg</em>, missing <code>null</code> checks in Java; your language
may aid you in this, <em>eg</em>, Kotlin or Scala)</li>
<li>Insecure code (see <a href="#shift-security-left">Shift security left</a>)</li>
<li>Use of outdated code patterns (<em>eg</em>, Java 5 patterns might be better expressed
with Java 17 improvements)</li>
<li><a href="https://spotbugs.github.io/spotbugs-maven-plugin/examples/violationChecking.html" rel="nofollow">Fail your
build</a>
if issues are detected</li>
</ul>
<p dir="auto">The Gradle and Maven demonstration builds use these to help you:</p>
<ul dir="auto">
<li><a href="https://pmd.github.io/latest/" rel="nofollow">PMD</a></li>
<li><a href="https://spotbugs.github.io/" rel="nofollow">SpotBugs</a></li>
</ul>
<p dir="auto">And use the <a href="https://find-sec-bugs.github.io/" rel="nofollow">Find Security Bugs</a> extension
for <a href="https://spotbugs.github.io/" rel="nofollow">Spotbugs</a>.</p>
<ul dir="auto">
<li>CPD for Gradle — see <a href="https://github.com/aaschmid/gradle-cpd-plugin">https://github.com/aaschmid/gradle-cpd-plugin</a>. CPD
works for MavenA</li>
</ul>
<h4 tabindex="-1" dir="auto">Security</h4>
<p dir="auto">SpotBugs uses an outdated version of
<a href="https://commons.apache.org/proper/commons-bcel/" rel="nofollow">BCEL</a>.
There is a <a href="https://nvd.nist.gov/vuln/detail/CVE-2022-42920" rel="nofollow">CVE</a>
(vulnerability) aginst BCEL that is resolved, however SpotBugs uses a version
of BCEL prior to the fix.
This project uses a forced update of BCEL for SpotBugs, however that breaks
the plugin (Gradle and Maven).</p>
<h4 tabindex="-1" dir="auto">Tips</h4>
<ul dir="auto">
<li>Edit <a href="https://github.com/binkley/modern-java-practices/blob/master/config/pmd/custom-rules.xml"><code>config/pmd/custom-rules.xml</code></a> to adjust
how PMD reviews your code (the sample in this project is from the PMD website)</li>
<li>To open the report for Spotbugs, build locally and use the
<code>&lt;project root&gt;/build/reports/spotbugs/</code> (Gradle) or
<code>&lt;project root&gt;/target/site/</code> (Maven) path.
Run <code>./mvnw site</code> for the latter.
The path shown in a Docker build is relative to the interior of the container</li>
<li>To open the report for PMD, build locally and use the
<code>&lt;project root&gt;/build/reports/pmd/</code> (Gradle) or
<code>&lt;project root/target/site/</code> (Maven) path.
The path shown in a Docker build is relative to the interior of the container</li>
</ul>
<h3 tabindex="-1" dir="auto">Modernizer</h3>
<p dir="auto">Another static code analysis tool is <em>Modernizer</em> to check of use of obsolete
APIs and types; this is related to but not identical to <em>deprecated</em> APIs. An
example is moving to the JDK's <code>Objects.equals</code> from Guava's <code>Objects.equal</code>.</p>
<p dir="auto">Note that Modernizer works at the bytecode level (not source code), so is
suitable for any JVM language, not just Java.</p>
<ul dir="auto">
<li><a href="https://github.com/andygoossens/gradle-modernizer-plugin">Gradle plugin</a></li>
<li><a href="https://github.com/gaul/modernizer-maven-plugin">Maven plugin</a></li>
</ul>
<hr>
<h2 tabindex="-1" dir="auto">Shift security left</h2>
<ul dir="auto">
<li><a href="https://find-sec-bugs.github.io/" rel="nofollow">Find known code security issues</a> — a
plugin for SpotBugs</li>
<li><a href="https://owasp.org/www-project-dependency-check/" rel="nofollow">DependencyCheck</a> —
verify your project dependencies against know security issues</li>
</ul>
<h3 tabindex="-1" dir="auto">Checking dependencies</h3>
<p dir="auto">Use checksums and signatures: verify what your build and project downloads!
When publishing for consumption by others, provide MD5 (checksum) files in your
upload: be a good netizen, and help others trust code downloaded from you</p>
<h4 tabindex="-1" dir="auto">Gradle</h4>
<p dir="auto">Read more at <a href="https://docs.gradle.org/current/userguide/dependency_verification.html" rel="nofollow"><em>Verifying
dependencies</em></a>
, an incubating feature.</p>
<h4 tabindex="-1" dir="auto">Maven</h4>
<p dir="auto"><em>Always</em> run with the <code>--strict-checksums</code> (or <code>-C</code>) flag. See
<a href="https://dev.to/khmarbaise/maven-artifact-checksums---what-396j" rel="nofollow"><em>Maven Artifact Checksums -
What?</em></a> for more
information. This is easy to forget about at the local command line. The
<a href="https://maven.apache.org/configure.htm" rel="nofollow"><code>.mvn/maven.config</code></a> file helps this be
automatic, and can be checked into your project repository.</p>
<p dir="auto">An alternative is to declare <em>each</em> repository in your user <code>settings.xml</code> and
<a href="https://dzone.com/articles/maven-artifact-checksums-what" rel="nofollow">set the checksum policy to
"fail"</a>.</p>
<p dir="auto">However, in CI this is easy; another example of why local builds should repeat
what CI builds do. The <a href="https://github.com/binkley/modern-java-practices/blob/master/batect.yml">Batect configuration</a>
for the demonstration project says:</p>
<div dir="auto" data-snippet-clipboard-copy-content="build-maven:
  description: Build and test with Maven
  run:
    container: build-env
    command: ./mvnw --strict-checksums clean verify"><pre><span>build-maven</span>:
  <span>description</span>: <span>Build and test with Maven</span>
  <span>run</span>:
    <span>container</span>: <span>build-env</span>
    <span>command</span>: <span>./mvnw --strict-checksums clean verify</span></pre></div>
<p dir="auto">and the GitHub action says:</p>
<div dir="auto" data-snippet-clipboard-copy-content="- name: Build and test with Maven
  run: ./mvnw --strict-checksums verify"><pre>- <span>name</span>: <span>Build and test with Maven</span>
  <span>run</span>: <span>./mvnw --strict-checksums verify</span></pre></div>
<p dir="auto">(<a href="#keep-local-consistent-with-ci">Batect</a>
and <a href="#setup-your-ci">GitHub Actions</a> are discussed both above.)</p>
<h3 tabindex="-1" dir="auto">Dependency check</h3>
<p dir="auto"><strong>This is CRITICAL if you have any direct, indirect, or through-plugin
dependencies on Log4j. Beyond your project, you may be impacted by services you
call, so check with your organization or external services</strong></p>
<p dir="auto"><a href="https://owasp.org/www-project-dependency-check/" rel="nofollow">DependencyCheck</a> is the
current <em>best tool</em> for JVM projects to verify that your project does not rely
on external code with known security vulnerabilities ([CVEs](<a href="https://cve.mitre/" rel="nofollow">https://cve.mitre</a>.
org/)) from the NVD.
That said, DependencyCheck does impact build times.
It is smart about caching, but will once a day may take time to download
data on any new NVD CVEs, and occasionally the site is down for maintenance.
You may consider leaving this check in CI-only if you find local build times
overly impacted.
Leaving these checks to CI-only is a tradeoff between "shifting security left",
and speed for local builds.
I lean towards <em>security first</em>; however, you know your circumstances
best.</p>
<p dir="auto"><strong>This project fails the build if finding any CVEs for the current version of
any dependency.</strong></p>
<p dir="auto">Your build should fail, too. It is a <em>red flag</em> to you to consider the CVE, what
impact the vulnerable dependency has, and if you are comfortable with a
vulnerable dependency. It is rarely (if ever) the case you keep a vulnerable
version of a dependency.</p>
<h3 tabindex="-1" dir="auto">Automate scanning for secrets</h3>
<p dir="auto">One key to shifting security left is avoiding secrets (passwords, private
identifiers, etc.) in your source code, commit history, and so on.</p>
<p dir="auto">GitHub and other repository services offer secrets scanning out of the box:
<a href="https://github.blog/2023-02-28-secret-scanning-alerts-are-now-available-and-free-for-all-public-repositories/" rel="nofollow">Secret scanning alerts are now available (and free) for all public
repositories</a>.</p>
<h4 tabindex="-1" dir="auto">Tips</h4>
<ul dir="auto">
<li>To open the report for DependencyCheck, build locally and use the
<code>&lt;project root&gt;/build/reports/</code> (Gradle) or
<code>&lt;project root/target/</code> (Maven) path.
The path shown in a Docker build is relative to the interior of the container</li>
<li>Sometimes you may want to refresh your local cache of the NVD files:
<ul dir="auto">
<li>Gradle: <code>./gradlew dependencyCheckPurge depenedencyCheckUpdate</code></li>
<li>Maven: <code>./mvnw dependency-check:purge dependency-check:update-only</code></li>
</ul>
</li>
<li>Note that this project has updated to DependencyCheck 8.
As is the project worked without changes after updating.
See <a href="https://github.com/jeremylong/DependencyCheck/releases/tag/v8.0.0"><em>v8.0.0</em> release
notes</a>
when updating from DependencyCheck 8</li>
</ul>
<h4 tabindex="-1" dir="auto">Notes</h4>
<p dir="auto">DependencyCheck may be your slowest quality check in local builds (competing
with mutation testing for that ignominious title). Sometimes it may fail when
the upstream source for CVEs is offline. If this is a recurring problem for you,
consider moving this check into CI. The downside that local work might use an
insecure dependency for a while. Checking daily for updated dependencies can
lessen this risk:</p>
<ul dir="auto">
<li>Gradle: <code>./gradlew dependencyUpdates</code></li>
<li>Maven: <code>./mvnw versions:update-properties</code></li>
</ul>
<p dir="auto">For non-Windows platforms, you may see this warning when <code>DependencyCheck</code> runs:</p>
<div data-snippet-clipboard-copy-content=".NET Assembly Analyzer could not be initialized and at least one 'exe' or 'dll' was scanned. The 'dotnet' executable could not be found on the path; either disable the Assembly Analyzer or add the path to dotnet core in the configuration."><pre><code>.NET Assembly Analyzer could not be initialized and at least one 'exe' or 'dll' was scanned. The 'dotnet' executable could not be found on the path; either disable the Assembly Analyzer or add the path to dotnet core in the configuration.
</code></pre></div>
<p dir="auto">In most situations, you are running in a Linux-based Docker container, or using
local Linux or Mac command line.  <strong>In a Windows project, this is an issue to
address, and may be a serious security concern</strong> indicating you are missing
critical Windows components. For other platforms, this is a nuisance message.</p>
<p dir="auto">On Gradle when updating to version 7.x.x of DependencyCheck from 6.x.x or
earlier, first run <code>./gradlew dependencyCheckPurge</code> to clear out the local
cache schema of CVEs.
DependencyCheck moved to schema v2 in 7.x.x from v1 in 6.x.x and earlier,
and the 7.0.0 Gradle plugin fails with the older schema version.</p>
<h3 tabindex="-1" dir="auto">Dependabot</h3>
<p dir="auto">GitHub provides
<a href="https://docs.github.com/en/free-pro-team@latest/github/administering-a-repository/keeping-your-dependencies-updated-automatically">Dependabot</a>
(other systems than GitHub may have similar robot tools) which, among other
things, can automatically issue PRs to your repository when security issues are
discovered. This project uses Dependabot for Gradle and Maven.</p>
<p dir="auto"><em>NB</em> — Dependabot is more reliable than either the Gradle or Maven plugins
for dependencies.</p>
<h3 tabindex="-1" dir="auto">Tips</h3>
<ul dir="auto">
<li>See the "Tips" section of <a href="#use-gradle-or-maven">Gradle or Maven</a></li>
<li>With GitHub actions, consider adding a tool such as
<a href="https://dependabot.com/" rel="nofollow">Dependabot</a>, which automatically files GitHub issues
for known dependency vulnerabilities. See
<a href="#dependabot">earlier in this document</a> for an example</li>
<li>You can <em>temporarily</em> disable OWASP dependency checking via
<code>-Dowasp.skip=true</code> for either Gradle or Maven, for example if the OWASP site
is down for maintenance, and you cannot update the local CVE cache</li>
<li>The <em>log4shell security vulnerabilities</em>
(<a href="https://nvd.nist.gov/vuln/detail/CVE-2021-44228" rel="nofollow">CVE-2021-44228</a>,
CVE-2021-45046,
CVE-2021-45105
are extremely severe. They are so severe, this should be a top priority for
you to address regardless of other priorities. Although this project does not
use <code>log4j</code>, local testing shows that the <code>DependencyCheck</code> plugin for either
Gradle or Maven fails build when you use an older, insecure version
of <code>log4j-core</code> indirectly. Note that Gradle 7.3.3+ itself fails your build if
it detect a dependency on a vulnerable version of <code>log4j-core</code></li>
<li>The <em>BCEL security vulnerability</em>
(<a href="https://www.opencve.io/cve/CVE-2022-42920" rel="nofollow">CVE-2022-42920</a>) is critical,
and should be a top priority to address. In this project, BCEL is used by
Spotbugs.</li>
</ul>
<h3 tabindex="-1" dir="auto">TODOs</h3>
<ul dir="auto">
<li>How to automate the <code>-C</code> (checksum) flag in Maven? See
<a href="https://dev.to/khmarbaise/maven-artifact-checksums---what-396j" rel="nofollow"><em>Maven Artifact Checksums -
What?</em></a></li>
</ul>
<hr>
<h2 tabindex="-1" dir="auto">Leverage unit testing and coverage</h2>
<ul dir="auto">
<li><a href="https://www.jacoco.org/jacoco/" rel="nofollow">JaCoCo</a></li>
<li>Use the "ratchet" pattern to fail the build when coverage drops.
Robert Greiner talks more on this in <a href="https://robertgreiner.com/continuous-code-improvement-using-ratcheting/" rel="nofollow"><em>Continuous Code Improvement Using
Ratcheting</em></a>
This follows the agile <a href="https://dzone.com/articles/the-boy-scout-software-development-principle" rel="nofollow">"Boy Scout"
principle</a></li>
<li>Fluent assertions — lots of options in this area
<ul dir="auto">
<li><a href="https://assertj.github.io/doc/" rel="nofollow">AssertJ</a> — solid choice</li>
<li>Built assertions from Junit makes is difficult for developers to
distinguish "actual" values from "expected" values. This is a limitation
from Java as it lacks named parameters.
Other frameworks compatible with JUnit provide more fluent assertions such
as AssertJ.
Different choices make sense depending on your source language</li>
</ul>
</li>
</ul>
<p dir="auto">Unit testing and code coverage are foundations for code quality.
Your build should help you with these as much as possible. 100% coverage may
seem absurd;
however, levels of coverage like this come with unexpected benefits such as
finding dead code in your project or helping refactoring to be simple.
An example: with high coverage (say 95%+, your experience will vary)
simplifying your covered code may lower your coverage as uncovered code becomes
more prominent in the total ratio.</p>
<p dir="auto">Setup for needed plugins:</p>
<ul dir="auto">
<li>For Gradle use the <code>java</code> plugin</li>
<li>For Maven, use more recent versions of the
<a href="https://maven.apache.org/surefire/maven-surefire-plugin/" rel="nofollow">Maven Surefire Plugin</a></li>
</ul>
<p dir="auto">(See <a href="https://github.com/hcoles/pitest/issues/347" data-hovercard-type="issue" data-hovercard-url="/hcoles/pitest/issues/347/hovercard"><em>suggestion : Ignore the generated
code</em></a> for a Lombok/PITest issue.)</p>
<p dir="auto">To see the coverage report (on passed or failed coverage), open:</p>
<ul dir="auto">
<li>For Gradle, <code>build/reports/jacoco/test/html/index.html</code></li>
<li>For Maven, <code>target/site/jacoco/index.html</code></li>
</ul>
<p dir="auto">This project also provides the coverage report as part of Maven's project
report.</p>
<p dir="auto">The <a href="https://github.com/binkley/modern-java-practices/blob/master/coverage.sh"><code>coverage</code></a> script is helpful for checking your current
coverage state: try <code>./coverage -f all</code>.
Current limitations:</p>
<ul dir="auto">
<li>Maven builds only</li>
<li>Single module builds only</li>
</ul>
<h3 tabindex="-1" dir="auto">Tips</h3>
<ul dir="auto">
<li>With Maven, <em>do use</em> the available BOM (bill of materials) for JUnit.
An example <code>pom.xml</code> block is:
<div dir="auto" data-snippet-clipboard-copy-content="  <dependencyManagement>
      <dependencies>
          <dependency>
              <groupId>org.junit</groupId>
              <artifactId>junit-bom</artifactId>
              <version>${junit.version}</version>
              <type>pom</type>
              <scope>import</scope>
          </dependency>
      </dependencies>
  </dependencyManagement>"><pre>  &lt;<span>dependencyManagement</span>&gt;
      &lt;<span>dependencies</span>&gt;
          &lt;<span>dependency</span>&gt;
              &lt;<span>groupId</span>&gt;org.junit&lt;/<span>groupId</span>&gt;
              &lt;<span>artifactId</span>&gt;junit-bom&lt;/<span>artifactId</span>&gt;
              &lt;<span>version</span>&gt;${junit.version}&lt;/<span>version</span>&gt;
              &lt;<span>type</span>&gt;pom&lt;/<span>type</span>&gt;
              &lt;<span>scope</span>&gt;import&lt;/<span>scope</span>&gt;
          &lt;/<span>dependency</span>&gt;
      &lt;/<span>dependencies</span>&gt;
  &lt;/<span>dependencyManagement</span>&gt;</pre></div>
This helps avoid dependency conflicts from other dependencies or plugins</li>
<li>See <a href="#leverage-lombok-to-tweak-code-coverage">discussion on Lombok</a> how to
<em>sparingly</em> leverage the <code>@Generated</code> annotation for marking code that JaCoCo
should ignore</li>
<li>Discuss with your team the concept of a "coverage ratchet". This means, once a
baseline coverage percentage is agreed to, the build configuration will only
raise this value, not lower it. This is fairly simple to do by periodically
examining the JaCoCo report, and raising the build coverage percentage over
time to match improvements in the report</li>
<li>Unfortunately neither Gradle's nor Maven's JaCoCo plugin will fail your build
when coverage <em>rises</em>!  This would be helpful for supporting the coverage
ratchet</li>
<li>You may find <em>mocking</em> helpful for injection. The Java community is not of one
mind on mocking, so use your judgment:
<ul dir="auto">
<li><a href="https://site.mockito.org/" rel="nofollow">Mockito</a> is the "standard" choice, and is a
dependency for the sample projects.
For modern versions of Mockito, please use the <code>mockito-core</code> dependency
rather than <code>mockito-inline</code>.
See <code>TheFooTest.shouldRedAlertAsStaticMock</code> for an example.
Note that this project has updated to Mockito 5.
See <a href="https://github.com/mockito/mockito/releases/tag/v5.0.0"><em>v5.0.0</em> release
notes</a> when
updating from Mockito 4</li>
<li><a href="https://powermock.github.io/" rel="nofollow">PowerMock</a> provides additional features;
however, Mockito normally covers use cases</li>
<li>Other Modern JVM languages — these languages may prefer different
mocking libraries, <em>eg</em>, <a href="https://mockk.io/" rel="nofollow">MockK</a> for Kotlin</li>
<li>You might consider complementary libraries to Mockito for specific
circumstances, <em>eg</em>,
<a href="https://github.com/stefanbirkner/system-lambda">System Lambda</a>
for checking STDOUT and STDERR, program exits, and use of system
properties (<em>eg</em>, validate logging), also a dependency for the sample
projects.  (<em>NB</em> — these are generally not parallelizable tests as
they alter the state of the JVM. Another is the
<a href="https://junit-pioneer.org/" rel="nofollow">JUnit Pioneer</a> extension pack. If you need
these, be cautious about using parallel testing features, and avoiding
<a href="https://hackernoon.com/flaky-tests-a-war-that-never-ends-9aa32fdef359" rel="nofollow">Flaky Tests</a>)</li>
</ul>
</li>
<li>To open the report for JaCoCo, build locally and use the
<code>&lt;project root&gt;/build/reports/jacoco/test/html/</code> path.
The path shown in a Docker build is relative to the interior of the container</li>
</ul>
<hr>
<h2 tabindex="-1" dir="auto">Use mutation testing</h2>
<p dir="auto">Unit testing is great for testing your production code. But have you thought
about testing your unit tests? What that means is, how are you sure your tests
really check what you meant them to? Fortunately, there is an automated way to
do just that, no code from you required, only some build configuration.</p>
<p dir="auto">Mutation testing is a simple concept: Go "break" some production code, and see
if any unit tests fail. Production bytecode is changed during the build—
for example, an <code>if (x)</code> is changed to <code>if (!x)</code>—, and the unit tests run.
With good code coverage, there should now be a failing unit test.</p>
<p dir="auto">The best option for Modern Java/JVM mutation testing is
<a href="http://pitest.org/" rel="nofollow">PITest</a>. It is under active development, does rather clever
things with compiled bytecode, and has Gradle and Maven plugins. The main
drawback for your <em>local build</em> is that PITest is <em>noisy</em>, so there might be
more build output than you might expect.</p>
<p dir="auto">Mutation testing is one of the slowest parts of a local build. You might
consider moving mutation testing to CI-only to speed up local
<em>red-green-refactor</em> cycle
(<a href="https://medium.com/@tunkhine126/red-green-refactor-42b5b643b506" rel="nofollow">_Red, Green, Refactor!</a>,
<a href="https://blog.cleancoder.com/uncle-bob/2014/12/17/TheCyclesOfTDD.html" rel="nofollow"><em>The Cycles of
TDD</em></a>).
Use your judgment on the value of the CI build never or rarely failing (modulo
external resources) when local build passes <em>vs</em> the speed of pushing good code.</p>
<p dir="auto">After running a build using PITest, to see the mutation report (on passed or
failed mutation coverage), open:</p>
<ul dir="auto">
<li>For Gradle, open <code>build/reports/pitest/index.html</code></li>
<li>For Maven, open <code>target/pit-reports/index.html</code></li>
</ul>
<p dir="auto">This project provides the PIT report as part of Maven's project report.</p>
<h3 tabindex="-1" dir="auto">Tips</h3>
<ul dir="auto">
<li>Without further configuration, PITest defaults to mutating classes using
your <em>project group</em> as the package base. Example: Set the <em>project group</em>
to "demo" for either Gradle or Maven if your classes are underneath the
"demo.*" package namespace, otherwise PITest may complain that there are no
classes to mutate, or no unit tests to run</li>
<li>If you need to open modules (<em>eg</em>, <code>--add-opens</code> flags), you need to include
these flags in "jvm args" configuration for the plugin</li>
<li>Read more about <a href="https://testing.googleblog.com/2021/04/mutation-testing.html" rel="nofollow"><em>Mutation
Testing</em></a> from
Google</li>
<li>To open the report for PITest, build locally and use the
<code>&lt;project root&gt;/build/reports/pitest/</code> (Gradle) or
<code>&lt;project root&gt;/target/pit-reports/</code> (Maven) path.
The path shown in a Docker build is relative to the interior of the container</li>
</ul>
<hr>
<h2 tabindex="-1" dir="auto">Use integration testing</h2>
<p dir="auto">Here the project says "integration testing". Your team may call it by another
name. This means bringing up your application, possibly with
<a href="http://xunitpatterns.com/Mocks,%20Fakes,%20Stubs%20and%20Dummies.html" rel="nofollow">fakes, stubs, mocks, spies, dummies, or doubles</a>
for external dependencies (databases, other services, <em>etc</em>), and running tests
against high-level functionality, but <em>not</em> starting up external dependencies
themselves (<em>ie</em>, Docker, or manual command-line steps). Think of CI: what are
called here "integration tests" are those which do
<em>not</em> need your CI to provide other services.</p>
<p dir="auto">An example is testing <code>STDOUT</code> and <code>STDERR</code> for a command-line application.
(If you are in Spring Framework/Boot-land, use controller tests for your REST
services.)</p>
<p dir="auto">Unlike <code>src/main/java</code> and <code>src/test/java</code>, there is no generally agreed
convention for where to put integration tests. This project keeps all tests
regardless of type in <code>src/test/java</code> for simplicity of presentation, naming
integration tests with "*IT.java". A more sophisticated approach may make sense
for your project.</p>
<p dir="auto">If you'd like to keep your integration tests in a separate source root from unit
tests, consider these plugins:</p>
<ul dir="auto">
<li>For Gradle, use <a href="https://docs.gradle.org/current/userguide/java_testing.html#sec:configuring_java_integration_tests" rel="nofollow">native Gradle to add new test
sets</a>.
(Previous versions of this project used the excellent
<a href="https://github.com/unbroken-dome/gradle-testsets-plugin"><code>testsets</code> plugin</a>,
however, it does not support Gradle 8)</li>
<li>For Maven, use
the <a href="https://maven.apache.org/failsafe/maven-failsafe-plugin/" rel="nofollow">Maven Failsafe Plugin</a></li>
</ul>
<p dir="auto"><strong>Caution</strong>: This project <em>duplicates</em>
<a href="https://github.com/binkley/modern-java-practices/blob/master/src/test/java/demo/ApplicationIT.java"><code>ApplicationIT.java</code></a> and
<a href="https://github.com/binkley/modern-java-practices/blob/master/src/integrationTest/java/demo/ApplicationTest.java"><code>ApplicationTest.java</code></a>
reflecting the split in philosophy between Gradle and Maven for integration
tests. Clearly in a production project, you would have only one of these.</p>
<h3 tabindex="-1" dir="auto">Tips</h3>
<ul dir="auto">
<li>For Maven projects, Apache maintains Failsafe and Surefire plugins as a pair,
and share the same version numbers.
This project uses a shared <code>maven-testing-plugins.version</code> property</li>
<li>Baeldung
has <a href="https://www.baeldung.com/maven-failsafe-plugin" rel="nofollow">a good introduction article</a>
on Maven Failsafe</li>
</ul>
<hr>
<h2 tabindex="-1" dir="auto">Going further</h2>
<p dir="auto">Can you do more to improve your build, and shift problems left (before they hit
CI or production)?
Of course!
Below are some topics to discuss with your team about making them part of the
local build.</p>
<h3 tabindex="-1" dir="auto">The Test Pyramid</h3>
<p dir="auto"><a href="https://martinfowler.com/bliki/TestPyramid.html" title="TestPyramid" rel="nofollow">
<img src="https://github.com/binkley/modern-java-practices/raw/master/images/test-pyramid.png" alt="The test pyramid" width="20%" height="auto">
</a></p>
<p dir="auto">What is the "Test Pyramid"? This is an important conceptual framework for
validating your project at multiple levels of interaction. Canonical resources
describing the test pyramid include:</p>
<ul dir="auto">
<li><a href="https://martinfowler.com/bliki/TestPyramid.html" rel="nofollow"><em>TestPyramid</em></a></li>
<li><a href="https://martinfowler.com/articles/practical-test-pyramid.html" rel="nofollow"><em>The Practical Test
Pyramid</em></a></li>
</ul>
<p dir="auto">As you move your testing "to the left" (helping local builds cover more
concerns), you'll want to enhance your build with more testing at different
levels of interaction. These are not covered in this article, so research is
needed.</p>
<p dir="auto">There are alternatives to the "test pyramid" perspective. Consider
<a href="https://blog.korny.info/2020/01/20/the-swiss-cheese-model-and-acceptance-tests.html" rel="nofollow">swiss cheese</a>
if it makes more sense for your project.
The build techniques still apply.</p>
<p dir="auto"><strong>NB</strong> — What this article calls
<a href="#use-integration-testing">"integration tests"</a> may have a different name for
your team.
You may have "system tests" for example.</p>
<h3 tabindex="-1" dir="auto">Use automated live testing when appropriate</h3>
<p dir="auto">"Live testing" here means spinning up a database or other remote service for
local tests, and not
using <a href="http://xunitpatterns.com/Mocks,%20Fakes,%20Stubs%20and%20Dummies.html" rel="nofollow">fakes, stubs, mocks, spies, dummies, or
doubles</a>.
In these tests, your project calls on <em>real</em> external dependencies, albeit
dependencies spun up locally rather than in production or another environment.
These might be call "out of process" tests.</p>
<p dir="auto">This is a complex topic, and this document is no guide on these. Some
potentially useful resources to pull into your build:</p>
<ul dir="auto">
<li><a href="https://flywaydb.org/" rel="nofollow">Flyway</a> — Version your schema in production, and
version your test data</li>
<li><a href="https://github.com/localstack/localstack">LocalStack</a> — Local testing
for AWS services</li>
<li><a href="https://www.testcontainers.org/" rel="nofollow">TestContainers</a> — Local Docker for
real database instances, or any Docker-provided service</li>
</ul>
<h3 tabindex="-1" dir="auto">Use contract testing when appropriate</h3>
<p dir="auto">Depending on your program, you may want additional testing specific to
circumstances. For example, with REST services and Spring Cloud, consider:</p>
<ul dir="auto">
<li><a href="https://spring.io/guides/gs/contract-rest/" rel="nofollow"><em>Consumer Driven Contracts</em></a></li>
</ul>
<p dir="auto">There are many options in this area. Find the choices which work best for you
and your project.</p>
<h3 tabindex="-1" dir="auto">Provide User Journey tests when applicable</h3>
<p dir="auto">Another dimension to consider for local testing: <em>User Journey</em> tests.</p>
<ul dir="auto">
<li><a href="https://www.thoughtworks.com/insights/blog/why-test-user-journey" rel="nofollow"><em>Why test the user
journey?</em></a></li>
</ul>
<hr>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/binkley/modern-java-practices/blob/master/images/debugging.png"><img src="https://github.com/binkley/modern-java-practices/raw/master/images/debugging.png" alt="Debugging in the container" width="20%" height="auto"></a></p>
<h2 tabindex="-1" dir="auto">Debugging</h2>
<p dir="auto">For direct debugging without a container follow your IDE's instructions.
For debugging within the local Batect container, examples are in the Batect
tasks <code>debug-with-gradle</code> and <code>debug-with-maven</code>.
Update <code>run-with-gradle.sh</code> or <code>run-with-maven.sh</code>, or write your own
similar script to run your program with the right JVM flags for <em>remote
debugging</em> (the container is a remote process on "localhost" from the
perspective of your IDE).</p>
<p dir="auto">These Batect example tasks assume a command-line program that exits when
completed.
See <a href="#spring-boot">the Spring Boot</a> sample for the same approach with
long-running services.</p>
<hr>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/binkley/modern-java-practices/blob/master/images/sample.svg"><img src="https://github.com/binkley/modern-java-practices/raw/master/images/sample.svg" alt="Sample" width="20%" height="auto"></a></p>
<h2 tabindex="-1" dir="auto">Samples</h2>
<p dir="auto">These samples are external projects, are at varying states of maturity, and
are frequently updated (espcially for dependency versions).</p>
<h3 tabindex="-1" dir="auto">Kotlin</h3>
<ul dir="auto">
<li><a href="https://github.com/binkley/kunits">KUnits</a> (Maven) is a pleasure project to
represent units of measurement in Kotlin</li>
<li><a href="https://github.com/binkley/kotlin-rational">Kotlin Rational</a> (Maven)
explores a math library for rationals (fractions) akin to <code>BigDecimal</code></li>
<li><a href="https://github.com/binkley/kotlin-magic-bus">Magic Bus</a> (Gradle) is a
library for using messaging patterns within a single application (it talks
to itself)</li>
</ul>
<h3 tabindex="-1" dir="auto">Spring Boot</h3>
<ul dir="auto">
<li><a href="https://github.com/binkley/kotlin-spring-boot-hateoas-database">Spring Boot HATEOAS
Database</a>
(Maven) looks at Spring Boot features for Open API (Swagger), REST APIs,
HATEOAS, GraphQL, Prometheus, <em>et al</em></li>
</ul>
<h2 tabindex="-1" dir="auto">Problems</h2>
<a href="https://xkcd.com/303/" title="Compiling" rel="nofollow">
<img src="https://github.com/binkley/modern-java-practices/raw/master/images/compiling.png" alt="Compiling" width="20%" height="auto">
</a>
<h3 tabindex="-1" dir="auto">Why is my local build slow?</h3>
<p dir="auto">Both Gradle and Maven have tools to track performance time of steps in your
build:</p>
<ul dir="auto">
<li><a href="https://scans.gradle.com/" rel="nofollow">Gradle build scans</a> — Not limited to
Enterprise licenses, just build with <code>./gradlew --scan &lt;tasks&gt;</code> and follow the
link in the output.
<a href="https://cdn.jsdelivr.net/gh/binkley/modern-java-practices/docs/profile-run/gradle-profile.html" title="A sample Gradle profile for this project" type="text/html" rel="nofollow">
See a sample Gradle profile for this project</a>.</li>
<li><a href="https://github.com/jcgay/maven-profiler">Maven profiler</a> — run
with <code>./mvnw -Dprofile &lt;goals&gt;</code> and open the local link in the output. This
project includes the setup for <a href="https://github.com/binkley/modern-java-practices/blob/master/.mvn/extensions.xml">Maven extensions</a>.
<a href="https://cdn.jsdelivr.net/gh/binkley/modern-java-practices/docs/profile-run/maven-profile.html" title="A sample Maven profile for this project" type="text/html" rel="nofollow">
See a sample Maven profile for this project</a>.</li>
</ul>
<p dir="auto"><strong>TODO</strong>: Fix the sample profile links to display as pages, not as raw HTML.</p>
<h3 tabindex="-1" dir="auto">My local build is still too slow</h3>
<p dir="auto">Congratulations!  You care, and you notice what is happening for your team.<br>
Local build time is <em>important</em>: gone are the days when a multi-hour, or even
30+ minute build, are viewed in most cases as the "cost of doing business".
And "compiling" is rarely any longer where your project takes most local build
time.</p>
<p dir="auto">Use the Gradle or Maven instructions
in <a href="#keep-your-build-fast">keep your build fast</a> to profile your build, and spot
where it spends time.</p>
<p dir="auto">If you find your local build is taking too long, consider testing moving these
parts to CI with the cost to you of issues arising from delayed feedback:</p>
<ul dir="auto">
<li><a href="#keep-your-build-clean">Jdeps</a></li>
<li><a href="#shift-security-left">DependencyCheck</a></li>
<li><a href="#use-integration-testing">Integration tests</a></li>
<li><a href="#use-mutation-testing">PITest</a></li>
</ul>
<p dir="auto"><em>But beware</em>!  Your local build is now drifting away from CI, so you are pushing
problems off later in your build pipeline. Not everyone pays close attention to
CI failures, that is until something bad happens in production.</p>
<p dir="auto"><em>IMPORTANT</em> — if you disable tools like the above in the <em>local</em> build,
ensure you retain them in your <em>CI</em> build. Your goal in this case is speed up
the feedback cycle locally while retaining the benefits of automated tooling.
You are making a bet: problems these tools find come up rarely (but can be
catastrophic when they do), so time saved locally repays time lost waiting for
CI to find these problems.</p>
<p dir="auto">In the Gradle and Maven samples in this repository, <em>DependencyCheck</em> and
<em>Mutation testing</em> are typically the slowest steps in a local build;
<em>Integration tests</em> are fast only because this project has very few (1), and are
samples only.
<a href="http://www.catb.org/jargon/html/Y/Your-mileage-may-vary.html" rel="nofollow">YMMV</a></p>
<p dir="auto">Every project is different; your team and stakeholders need to judge the value
of quicker feedback to programmers of these concerns, and quicker feedback from
a faster local build. There is no "one size fits all" recommendation.</p>
<h3 tabindex="-1" dir="auto">It fails in CI, but passes locally</h3>
<p dir="auto">As much as you would like local builds to be identical to CI, this can still
happen for reasons of environment. Examples can include:</p>
<ul dir="auto">
<li>Credentials needed in CI have changed: Update your CI configuration</li>
<li>Network routing has changed, and CI is in a different subnet from local:
Talk with your Infrastructure team</li>
<li>CI includes steps to push successful builds further down the line to other
environments, and something there went wrong: Talk with your Infrastructure
team</li>
<li>Dependencies break in CI: If CI uses an internal dependency repository, check
in with the maintainers of the repository</li>
</ul>
<h2 tabindex="-1" dir="auto">Credits</h2>
<p dir="auto">Many thanks to:</p>
<ul dir="auto">
<li><a href="https://github.com/LemmingAvalanche">Kristoffer Haugsbakk</a> —
<em>Proofreading</em></li>
<li><a href="https://github.com/Bukharovsi">Sergei Bukharov</a> — <em>PMD enhancements</em></li>
</ul>
<p dir="auto">All suggestions and ideas welcome!
Please <a href="https://github.com/binkley/modern-java-practices/issues">file an
issue</a>. ☺</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SIMD in Pure Python (238 pts)]]></title>
            <link>https://www.da.vidbuchanan.co.uk/blog/python-swar.html</link>
            <guid>38874885</guid>
            <pubDate>Fri, 05 Jan 2024 02:09:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.da.vidbuchanan.co.uk/blog/python-swar.html">https://www.da.vidbuchanan.co.uk/blog/python-swar.html</a>, See on <a href="https://news.ycombinator.com/item?id=38874885">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<nav>
					
				</nav>
				<section>
					
<p><em>By David Buchanan, 4<sup>th</sup> January 2024</em></p>
<p>First of all, this article is an exercise in recreational "because I can" programming. If you just want to make your Python code go fast, this is perhaps not the article for you. And perhaps Python is not the language you want, either!</p>
<p>By the end, I'll explain how I implemented <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Game of Life</a> in pure Python (plus pysdl2 for graphics output) running in 4K resolution at 180fps, which represents a ~3800x speedup over a naive implementation.</p>
<p><img width="70%" src="data:image/png;base64,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"></p>
<p>If you're already familiar with SIMD and vectorization, you might want to skip this next section.</p>
<h3>A Brief Introduction to SIMD</h3>
<p>If you want to get the most performance out of a modern CPU, you're probably going to be using <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">SIMD</a> instructions - Single Instruction, Multiple Data.</p>
<p>For example, if you have two arrays of length 4, containing 32-bit integers, most modern CPUs will let you add the pairwise elements together in a single machine instruction (assuming you've already loaded the data into registers). Hopefully it's obvious why this is more efficient than looping over the individual array elements. Intel CPUs have had this <em>specific</em> capability since <a href="https://en.wikipedia.org/wiki/SSE2">SSE2</a> was introduced in the year 2000, but SIMD as a concept predates it by <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data#History">a lot</a>.</p>
<p>Newer CPUs cores have been expanding on these capabilities ever since, meaning SIMD instructions are more relevant than ever for maximising CPU throughput.</p>
<p>If you're programming in a language like C, an optimising compiler will recognise code that can be accelerated using SIMD instructions, and automatically emit appropriate machine code. Compilers can't optimise everything perfectly, though, so anyone who wants to squeeze out the maximum performance might end up using <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html">Intrinsics</a> to explicitly tell the compiler which instructions to use. And if <em>that</em> still isn't enough, you might end up programming directly in assembly.</p>
<p>There are many ways to express SIMD programs, but the rest of them are out of scope for this article!</p>
<p>"Vectorization" is the process of transforming a typical program into one that operates over whole arrays of data (i.e. vectors) at once (for example, using SIMD). The work done by the optimising compiler described above, or the human writing Intrinsic operations, is vectorization.</p>
<h3>A Brief Introduction to CPython</h3>
<p><a href="https://github.com/python/cpython">CPython</a> is the reference implementation of the Python language, written mostly in C, hence the name. Other implementations exist, but when people say "Python" they're often implicitly referring to CPython. I'll try to only say Python when I'm referring to the language as a whole, and CPython when I'm talking about a CPython implementation detail (which we'll be getting into later).</p>
<p>The TL;DR is that CPython compiles your code into a <a href="https://docs.python.org/3/glossary.html#term-bytecode">bytecode</a> format, and then interprets that bytecode at run-time. I'll be referring to that bytecode interpreter as the <a href="https://docs.python.org/3/glossary.html#term-virtual-machine">"VM"</a>.</p>
<h3>SIMD in Python</h3>
<p>Python does not natively have a concept of SIMD. However, libraries like <a href="https://numpy.org/">NumPy</a> exist, allowing for relatively efficient vectorized code. NumPy lets you define vectors, or even n-dimensional arrays, and perform operations on them in a single API call.</p>
<div><table><tbody><tr><td></td><td><div><pre><span></span><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>

<span>a</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>,</span> <span>4</span><span>])</span>
<span>b</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>2</span><span>,</span> <span>4</span><span>,</span> <span>6</span><span>,</span> <span>8</span><span>])</span>

<span>print</span><span>(</span><span>a</span> <span>+</span> <span>b</span><span>)</span>  <span># [ 3  6  9 12]</span>
</pre></div></td></tr></tbody></table></div>
<p>Without NumPy, the above example would require a loop over array elements (or a list comprehension, which is fancy syntax for the same thing, more or less).</p>
<p>Internally, NumPy is implemented using native C extensions, which in turn use Intrinsics to express SIMD operations. I'm not an expert on NumPy implementation details, but you can peruse their SIMD code <a href="https://github.com/numpy/numpy/tree/main/numpy/_core/src/common/simd">here</a>. Note that the code has been customised for various CPU architectures.</p>
<p>CPython itself, being an interpreted Python implementation, is slow. But if you can structure your program so that all the "real work" gets done inside a library like NumPy, it can be surprisingly efficient overall.</p>
<p>NumPy is excellent and widely used for getting real work done. However, NumPy is not "pure" Python!</p>
<h3>SIMD in <em>Pure</em> Python</h3>
<p>By "pure," I mean using only functionality built into the <a href="https://docs.python.org/3/reference/">Python language</a> itself, or the <a href="https://docs.python.org/3/library/index.html">Python standard library</a>.</p>
<p>This is an entirely arbitrary and self-imposed constraint, but I think it's a fun one to work within. It's also vaguely useful, since libraries like NumPy aren't available in certain environments.</p>
<p>Earlier, I said Python doesn't natively have a concept of SIMD. This isn't entirely true; otherwise the article would end here. Python supports bitwise operations over pairs of integers: AND (<code>&amp;</code>), OR (<code>|</code>), XOR (<code>^</code>). If you think about these as operations over vectors of booleans, each bit being one bool, it is SIMD!</p>
<p>Unlike many other programming languages, Python integers have unlimited precision. That is, they can accurately represent integers containing arbitrarily many digits—at least, until you run out of memory. This means we can evaluate an unlimited number of conceptually-parallel boolean operations with a single python operator.</p>
<p>SIMD over booleans might sound esoteric, but it's an idea we can immediately put to work. A common operation in cryptography is to XOR two byte buffers together. An idiomatic implementation might look like this:</p>
<div><table><tbody><tr><td></td><td><div><pre><span></span><span>def</span> <span>xor_bytes</span><span>(</span><span>a</span><span>:</span> <span>bytes</span><span>,</span> <span>b</span><span>:</span> <span>bytes</span><span>)</span> <span>-&gt;</span> <span>bytes</span><span>:</span>
	<span>assert</span><span>(</span><span>len</span><span>(</span><span>a</span><span>)</span> <span>==</span> <span>len</span><span>(</span><span>b</span><span>))</span>
	<span>return</span> <span>bytes</span><span>(</span><span>x</span> <span>^</span> <span>y</span> <span>for</span> <span>x</span><span>,</span> <span>y</span> <span>in</span> <span>zip</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>))</span>
</pre></div></td></tr></tbody></table></div>
<p>This takes each individual byte (as an integer between 0 and 255) in a and b, applies the xor operator to each, and constructs a new <code>bytes</code> object from the results. Arguably, we are already using the boolean-SIMD concept here; each of the 8 bits in a byte are getting XORed in parallel. But we can do better than that:</p>
<div><table><tbody><tr><td></td><td><div><pre><span></span><span>def</span> <span>xor_bytes_simd</span><span>(</span><span>a</span><span>:</span> <span>bytes</span><span>,</span> <span>b</span><span>:</span> <span>bytes</span><span>)</span> <span>-&gt;</span> <span>bytes</span><span>:</span>
	<span>assert</span><span>(</span><span>len</span><span>(</span><span>a</span><span>)</span> <span>==</span> <span>len</span><span>(</span><span>b</span><span>))</span>
	<span>return</span> <span>(</span>
		<span>int</span><span>.</span><span>from_bytes</span><span>(</span><span>a</span><span>,</span> <span>"little"</span><span>)</span> <span>^</span> <span>int</span><span>.</span><span>from_bytes</span><span>(</span><span>b</span><span>,</span> <span>"little"</span><span>)</span>
	<span>)</span><span>.</span><span>to_bytes</span><span>(</span><span>len</span><span>(</span><span>a</span><span>),</span> <span>"little"</span><span>)</span>
</pre></div></td></tr></tbody></table></div>
<p>This might look a bit ridiculous, and that's because it is. We convert each <code>bytes</code> object into an arbitrary-precision integer, XOR those two integers together, and then convert the resulting integer back to <code>bytes</code>. The python <code>^</code> operator only gets executed once, processing all the data in one step (or more explicitly, one CPython VM operation). I'm going to call this approach "pseudo-SIMD". But with all this conversion between bytes and integers, surely it must be slower overall? Let's benchmark it.</p>
<p>Here are my results. The number is time to execute 1 million iterations. I tested on CPython 3.11 on an M1 Pro macbook. <a href="https://gist.github.com/DavidBuchanan314/51bb8f6219ea8bb7a603e0ad19725f6d">Try it on your own machine!</a></p>

<pre>naive, n=1 0.3557752799242735
simd,  n=1 0.21655898913741112
numpy, n=1 0.798536550020799

naive, n=16 0.8749550790525973
simd,  n=16 0.23561427788808942
numpy, n=16 0.7937424059491605

naive, n=128 4.5441425608005375
simd,  n=128 0.5077524171210825
numpy, n=128 0.8012108108960092

naive, n=1024 34.96425646613352
simd,  n=1024 2.811028849100694
numpy, n=1024 0.9388492209836841
</pre>
<p>Even for the trivial case of n=1, somehow our wacky pseudo-SIMD function wins, by about a third!</p>
<p>The difference becomes even more pronounced as the buffer size increases, with the pseudo-SIMD function being 12x faster for n=1024.</p>
<p>I also threw a numpy implementation into the mix. This isn't really fair on numpy, because converting the bytes to and from numpy arrays appears to have quite a high constant overhead. In more realistic numpy code, you'd end up keeping your data in numpy format throughout. Because of this, numpy ended up slower all the way until n=1024, where it became 3x faster. Evidently, those constant overheads become less relevant as <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math> grows.</p>
<p>But what if we eliminate the conversion overheads, allowing our functions to input and output data in their "native" formats, rather than bytes? For pseudo-SIMD, that format is integers, and for numpy it's a <code>np.array</code> object.</p>

<pre>simd,  n=1 0.03484031208790839
numpy, n=1 0.2326297229155898

simd,  n=16 0.042713511968031526
numpy, n=16 0.23679199093021452

simd,  n=128 0.046673570992425084
numpy, n=128 0.23861141502857208

simd,  n=1024 0.08742194902151823
numpy, n=1024 0.28949279501102865

simd,  n=32768 0.9535991169977933
numpy, n=32768 0.9617231499869376

simd,  n=131072 4.984845655970275
numpy, n=131072 4.609246583888307
</pre>
<p>Pseudo-SIMD has the edge for small inputs (perhaps because it doesn't have to do any <a href="https://en.wikipedia.org/wiki/Foreign_function_interface">FFI</a>), but for large buffers, numpy edges ahead. But only barely! How is our pure-python XOR function (which at this point is just the XOR operator itself) able to keep up with NumPy's optimised SIMD code?</p>
<h3>CPython Internals</h3>
<p>Let's take a closer look. Here's the <a href="https://github.com/python/cpython/blob/v3.11.5/Objects/longobject.c#L5050-L5051">inner loop</a> of the code for XORing together two arbitrary-precision integers in CPython 3.11.</p>
<div><table><tbody><tr><td></td><td><div><pre><span></span><span>for</span><span> </span><span>(</span><span>i</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>size_b</span><span>;</span><span> </span><span>++</span><span>i</span><span>)</span>
<span>    </span><span>z</span><span>-&gt;</span><span>ob_digit</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>a</span><span>-&gt;</span><span>ob_digit</span><span>[</span><span>i</span><span>]</span><span> </span><span>^</span><span> </span><span>b</span><span>-&gt;</span><span>ob_digit</span><span>[</span><span>i</span><span>];</span>
</pre></div></td></tr></tbody></table></div>
<p>It's a simple loop over arrays. No SIMD instructions? Well, there aren't any explicit ones, but what does the C compiler do to it? Let's take an <em>even closer</em> look.</p>
<p>I loaded libpython into Ghidra and had a look around. The library on my system didn't have full symbols, so I searched for cross-references to the exported symbol <code>_PyLong_New</code>. There were 82 matches, but by an extremely weird stroke of luck it was the first function I clicked on.</p>
<p>On my system, the (aarch64) assembly corresponding to the above loop is as follows:</p>

<pre>.      00299ec0 01 03 80 d2     mov        x1,#0x18
       00299ec4 00 00 80 d2     mov        x0,#0x0

  ,-&gt;LAB_00299ec8                                    XREF[1]:     00299ee4(j)  
  |    00299ec8 80 6a e1 3c     ldr        q0,[x20,x1]
  |    00299ecc 00 04 00 91     add        x0,x0,#0x1
  |    00299ed0 a1 6a e1 3c     ldr        q1,[x21,x1]
  |    00299ed4 00 1c 21 6e     eor        v0.16B,v0.16B,v1.16B
  |    00299ed8 e0 6a a1 3c     str        q0,[x23,x1]
  |    00299edc 21 40 00 91     add        x1,x1,#0x10
  |    00299ee0 5f 00 00 eb     cmp        x2,x0
   \_  00299ee4 21 ff ff 54     b.ne       LAB_00299ec8
       00299ee8 61 f6 7e 92     and        x1,x19,#-0x4
       00299eec 7f 06 40 f2     tst        x19,#0x3
       00299ef0 e0 02 00 54     b.eq       LAB_00299f4c

     LAB_00299ef4                                    XREF[1]:     0029a2a0(j)  
       00299ef4 20 f4 7e d3     lsl        x0,x1,#0x2
       00299ef8 22 04 00 91     add        x2,x1,#0x1
       00299efc 85 02 00 8b     add        x5,x20,x0
       00299f00 a4 02 00 8b     add        x4,x21,x0
       00299f04 e0 02 00 8b     add        x0,x23,x0
       00299f08 86 18 40 b9     ldr        w6,[x4, #0x18]
       00299f0c a3 18 40 b9     ldr        w3,[x5, #0x18]
       00299f10 63 00 06 4a     eor        w3,w3,w6
       00299f14 03 18 00 b9     str        w3,[x0, #0x18]
       00299f18 7f 02 02 eb     cmp        x19,x2
       00299f1c 8d 01 00 54     b.le       LAB_00299f4c
       00299f20 83 1c 40 b9     ldr        w3,[x4, #0x1c]
       00299f24 21 08 00 91     add        x1,x1,#0x2
       00299f28 a2 1c 40 b9     ldr        w2,[x5, #0x1c]
       00299f2c 42 00 03 4a     eor        w2,w2,w3
       00299f30 02 1c 00 b9     str        w2,[x0, #0x1c]
       00299f34 7f 02 01 eb     cmp        x19,x1
       00299f38 ad 00 00 54     b.le       LAB_00299f4c
       00299f3c 82 20 40 b9     ldr        w2,[x4, #0x20]
       00299f40 a1 20 40 b9     ldr        w1,[x5, #0x20]
       00299f44 21 00 02 4a     eor        w1,w1,w2
       00299f48 01 20 00 b9     str        w1,[x0, #0x20]
     LAB_00299f4c
</pre>
<p>If you're not a reverse engineer, or even if you are, you're probably thinking "WTF is going on here?" This is a fairly typical result of compiler <a href="https://en.wikipedia.org/wiki/Automatic_vectorization">auto-vectorization</a>. Ghidra does a terrible job of converting it to meaningful pseudocode, so I'll provide my own version (note, this is not a 1:1 mapping, but it should convey the general idea)</p>
<div><table><tbody><tr><td><div><pre><span> 1</span>
<span> 2</span>
<span> 3</span>
<span> 4</span>
<span> 5</span>
<span> 6</span>
<span> 7</span>
<span> 8</span>
<span> 9</span>
<span>10</span>
<span>11</span>
<span>12</span>
<span>13</span>
<span>14</span>
<span>15</span></pre></div></td><td><div><pre><span></span><span>i</span> <span>=</span> <span>0</span>
<span>words_left_to_xor</span> <span>=</span> <span>size_b</span>
<span>while</span> <span>words_left_to_xor</span> <span>&gt;</span> <span>3</span><span>:</span>
	<span># xor 4 words (16 bytes) concurrently using q0, q1 registers</span>
	<span>z</span><span>[</span><span>i</span><span>:</span><span>i</span><span>+</span><span>4</span><span>]</span> <span>=</span> <span>a</span><span>[</span><span>i</span><span>:</span><span>i</span><span>+</span><span>4</span><span>]</span> <span>^</span> <span>b</span><span>[</span><span>i</span><span>:</span><span>i</span><span>+</span><span>4</span><span>]</span>
	<span>i</span> <span>+=</span> <span>4</span>
	<span>words_left_to_xor</span> <span>-=</span> <span>4</span>

<span># deal with remaining 32-bit words individually</span>
<span>if</span> <span>words_left_to_xor</span> <span>&gt;</span> <span>0</span><span>:</span>
	<span>z</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>a</span><span>[</span><span>i</span><span>]</span> <span>^</span> <span>b</span><span>[</span><span>i</span><span>]</span>
<span>if</span> <span>words_left_to_xor</span> <span>&gt;</span> <span>1</span><span>:</span>
	<span>z</span><span>[</span><span>i</span><span>+</span><span>1</span><span>]</span> <span>=</span> <span>a</span><span>[</span><span>i</span><span>+</span><span>1</span><span>]</span> <span>^</span> <span>b</span><span>[</span><span>i</span><span>+</span><span>1</span><span>]</span>
<span>if</span> <span>words_left_to_xor</span> <span>&gt;</span> <span>2</span><span>:</span>
	<span>z</span><span>[</span><span>i</span><span>+</span><span>2</span><span>]</span> <span>=</span> <span>a</span><span>[</span><span>i</span><span>+</span><span>2</span><span>]</span> <span>^</span> <span>b</span><span>[</span><span>i</span><span>+</span><span>2</span><span>]</span>
</pre></div></td></tr></tbody></table></div>
<p>The main loop operates using the <code>q0</code> and <code>q1</code> registers, which according to <a href="https://developer.arm.com/documentation/dht0002/a/Introducing-NEON/NEON-architecture-overview/NEON-registers">ARM docs</a> are 128-bit wide NEON registers. As far as I can tell, NEON doesn't stand for anything in particular, but it's what ARM calls its SIMD features (by the way, their "next-gen" SIMD instruction set is called <a href="https://developer.arm.com/documentation/102476/0100/Introducing-SVE">SVE</a>).</p>
<p>After the main loop, it xors the remaining 32-bit words, one at a time.</p>
<p>The key observation here is that our "pseudo-SIMD" implementation is using real SIMD instructions under the hood! At least, it is on my system; this may depend on your platform, compiler, configuration, etc.</p>
<p>If we limit ourselves to bitwise operators, and our numbers are big enough that the interpreter overhead is small (in relative terms), we can get real SIMD performance speedups in pure Python code. Well, kinda. If you were implementing a specific algorithm in assembly, you could generate much more tightly optimised SIMD routines, keeping data in registers where possible, avoiding unnecessary loads and stores from memory, etc.</p>
<p>Another big caveat with this approach is that it involves creating a whole new integer to contain the result. This wastes memory space, puts pressure on the memory allocator/gc, and perhaps most importantly, it wastes memory bandwidth. Although you can write <code>a ^= b</code> in Python to denote an in-place XOR operation, it still ends up internally allocating a new object to store the result.</p>
<p>If you're wondering why the NumPy implementation was still slightly faster, I believe the answer lies in the way CPython represents its integers. Each entry in the <code>ob_digit</code> array only represents 30 bits of the overall number. I'm guessing this makes handling carry propagation simpler during arithmetic operations. This means the in-memory representation has a ~7% overhead compared to optimal packing. While I haven't checked NumPy's implementation details, I imagine they pack array elements tightly.</p>
<h3>Doing Useful Work</h3>
<p>Now that we know we can do efficient-ish bitwise SIMD operations, can we build something useful from that?</p>
<p>One use case is bitsliced cryptography. <a href="https://github.com/DavidBuchanan314/python-bitsliced-aes">Here's</a> my implementation of bitsliced AES-128-ECB in pure Python. It's over 20x faster than the next fastest pure-python AES implementation I could find, and in theory it's more secure too, due to not having any data-dependent array indexing (but I still wouldn't trust it as a secure implementation; use a <a href="https://cryptography.io/en/latest/">proper cryptography library!</a>)</p>
<p>For a more detailed introduction to bitslicing, check out <a href="https://timtaubert.de/blog/2018/08/bitslicing-an-introduction/">this article</a>. The idea is to express your whole algorithm as a circuit of logic gates, or in other words, a bunch of boolean expressions. You can do this for any computation, but AES is particularly amenable to it. Once you have a boolean expression, you can use bit-parallel operations (i.e., bitwise SIMD operations) to compute multiple instances of your algorithm in parallel. Since AES is a block-based cipher, you can use this idea to compute multiple AES cipher blocks concurrently.</p>
<h3>SWAR</h3>
<p>We can use Python integers for more than just parallel bitwise operations. We can use them for parallel additions, too!</p>
<div><table><tbody><tr><td><div><pre><span> 1</span>
<span> 2</span>
<span> 3</span>
<span> 4</span>
<span> 5</span>
<span> 6</span>
<span> 7</span>
<span> 8</span>
<span> 9</span>
<span>10</span>
<span>11</span>
<span>12</span>
<span>13</span>
<span>14</span>
<span>15</span></pre></div></td><td><div><pre><span></span><span>a</span> <span>=</span> <span>0x4567</span> <span># this represents four 4-bit unsigned integers, [4, 5, 6, 7]</span>
<span>b</span> <span>=</span> <span>0x6789</span> <span># as does this: [6, 7, 8, 9]</span>

<span>print</span><span>(</span><span>hex</span><span>(</span><span>a</span> <span>+</span> <span>b</span><span>))</span>  <span># 0xacf0 =&gt; [0xa, 0xc, 0xf, 0x0] == [10, 12, 15, 0]</span>

<span># oh no, that's the wrong answer... 6+8 should be 14, not 15.</span>
<span># it's wrong because the result of 9+7 was 16 (0x10), causing carry propagation</span>
<span># into the adjacent "lane".</span>

<span># solution: padding and masking:</span>
<span>a</span> <span>=</span> <span>0x04050607</span>
<span>b</span> <span>=</span> <span>0x06070809</span>
<span>m</span> <span>=</span> <span>0x0f0f0f0f</span>

<span>print</span><span>(</span><span>hex</span><span>((</span><span>a</span> <span>+</span> <span>b</span><span>)</span> <span>&amp;</span> <span>m</span><span>))</span> <span># 0xa0c0e00 =&gt; [0xa, 0xc, 0xe, 0x0] == [10, 12, 14, 0]</span>
</pre></div></td></tr></tbody></table></div>
<p>As shown here, we can pack multiple fixed-width integers into a single Python integer, and add them all together at once. However, if they're tightly packed and an integer overflow occurs, this causes unwanted carry propagation between lanes. The solution is simple: space them out a bit. In this example I use generous 4-bit-wide padding to make things more obvious, but in principle you only need a single bit of padding. Finally, we use the bitwise AND operator to mask off any overflow bits. If we didn't do this, the overflowing bits could accumulate over the course of multiple additions and start causing overflows between lanes again. The more padding bits you use between lanes, the more chained additions you can survive before masking is required.</p>
<p>You can do similar things for subtraction, multiplication by a small constant, and bit shifts/rotations, so long as you have enough padding bits to prevent overflows in each scenario.</p>
<p>The general term for this concept is SWAR, which stands for <a href="https://en.wikipedia.org/wiki/SWAR">SIMD Within A Register</a>. But here, rather than using a machine register, we're using an arbitrarily long Python integer. I'm calling this variant SWAB: SIMD Within A Bigint.</p>
<p>SWAB is a useful idea in Python because it maximises the amount of work done per VM instruction, reducing the interpreter overhead; the CPU gets to spend the majority of its time in the fast native code that implements the integer operations.</p>
<h3>Doing <s>Useful Work</s> Something Fun</h3>
<p>That's enough theory; now it's time to make Game of Life go fast. First, let me describe the "problem" I'm trying to solve. I'll assume you're already broadly familiar with Game of Life, but if not, go read the <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Wikipedia article.</a></p>
<p>There are some very clever algorithms for making GoL go fast, the most famous being <a href="https://johnhw.github.io/hashlife/index.md.html">Hashlife</a>, which works by spotting repeating patterns in both space and time. However, my favourite GoL pattern to simulate is <a href="https://conwaylife.com/wiki/Soup">"soup"</a>, i.e., a large random starting grid. These chaotic patterns aren't well suited to the Hashlife algorithm, so we need to go back to the basics.</p>
<p>When you simulate soup in the classic GoL ruleset, it typically dies out after a few thousand generations, producing a rather boring arrangement of oscillating <a href="https://conwaylife.com/wiki/Soup#Ash">"ash"</a> (which is back to something Hashlife can simulate quickly). I prefer my simulations to live on in eternal chaos, and there's a variant of the classic ruleset that more or less guarantees this, called <a href="https://conwaylife.com/wiki/OCA:DryLife">DryLife</a>. I like DryLife because it still exhibits most of the familiar GoL behaviours (for example, gliders) and yet the soup lives on indefinitely, creating a pleasing screensaver-like animation.</p>
<p>The "obvious" implementation of the inner loop of the GoL algorithm looks something like this:</p>
<div><table><tbody><tr><td><div><pre><span> 1</span>
<span> 2</span>
<span> 3</span>
<span> 4</span>
<span> 5</span>
<span> 6</span>
<span> 7</span>
<span> 8</span>
<span> 9</span>
<span>10</span>
<span>11</span>
<span>12</span>
<span>13</span>
<span>14</span>
<span>15</span></pre></div></td><td><div><pre><span></span><span>for</span> <span>y</span> <span>in</span> <span>range</span><span>(</span><span>1</span><span>,</span> <span>height</span> <span>+</span> <span>1</span><span>):</span>
    <span>for</span> <span>x</span> <span>in</span> <span>range</span><span>(</span><span>width</span><span>):</span>
        <span>neighbor_count</span> <span>=</span> <span>sum</span><span>(</span>
            <span>get_cell</span><span>(</span><span>state</span><span>,</span> <span>(</span><span>x</span> <span>+</span> <span>dx</span><span>)</span> <span>%</span> <span>width</span><span>,</span> <span>y</span> <span>+</span> <span>dy</span><span>)</span>
            <span>for</span> <span>dx</span><span>,</span> <span>dy</span> <span>in</span> <span>[</span>
                <span>(</span><span>-</span><span>1</span><span>,</span> <span>-</span><span>1</span><span>),</span> <span>(</span><span>0</span><span>,</span> <span>-</span><span>1</span><span>),</span> <span>(</span><span>1</span><span>,</span> <span>-</span><span>1</span><span>),</span>
                <span>(</span><span>-</span><span>1</span><span>,</span>  <span>0</span><span>),</span>          <span>(</span><span>1</span><span>,</span>  <span>0</span><span>),</span>
                <span>(</span><span>-</span><span>1</span><span>,</span>  <span>1</span><span>),</span> <span>(</span><span>0</span><span>,</span>  <span>1</span><span>),</span> <span>(</span><span>1</span><span>,</span>  <span>1</span><span>)</span>
            <span>]</span>
        <span>)</span>
        <span>this_cell</span> <span>=</span> <span>get_cell</span><span>(</span><span>state</span><span>,</span> <span>x</span><span>,</span> <span>y</span><span>)</span>
        <span>next_value</span> <span>=</span> <span>neighbor_count</span> <span>==</span> <span>3</span> <span>or</span> <span>(</span><span>this_cell</span> <span>and</span> <span>neighbor_count</span> <span>==</span> <span>2</span><span>)</span>
        <span>if</span> <span>cfg</span><span>.</span><span>drylife</span><span>:</span> <span># another opportunity for dead cells to come alive</span>
            <span>next_value</span> <span>|=</span> <span>(</span><span>not</span> <span>this_cell</span><span>)</span> <span>and</span> <span>neighbor_count</span> <span>==</span> <span>7</span>
        <span>set_cell</span><span>(</span><span>next_state</span><span>,</span> <span>x</span><span>,</span> <span>y</span><span>,</span> <span>next_value</span><span>)</span>
</pre></div></td></tr></tbody></table></div>
<p>It iterates over every cell in the grid, counts up its immediate 8 neighbours, and applies the rules to decide if the cell should be alive or not in the next iteration. In big-O terms, this is an <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></math> algorithm, where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math> is the number of cells in the grid (i.e., width <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>×</mi></mrow></math> height).</p>
<p>Although it isn't made explicit in the snippet above, the state of the cells is being stored in a big array, accessed via the <code>get_cell</code> and <code>set_cell</code> helper functions. What if instead of using an array, we stored the whole state in one very long integer, and used SWAB arithmetic to process the whole thing at once? The trickiest part of this process will be counting up the neighbours, which can sum to up to 8 (or 9 if we also count the initial cell value). That's a 4-bit value, and we can be sure it will never overflow into a 5-bit value, so we can store each cell as a 4-bit wide "SWAB lane". Without further ado, here's the equivalent inner-loop code:</p>
<div><table><tbody><tr><td><div><pre><span> 1</span>
<span> 2</span>
<span> 3</span>
<span> 4</span>
<span> 5</span>
<span> 6</span>
<span> 7</span>
<span> 8</span>
<span> 9</span>
<span>10</span>
<span>11</span>
<span>12</span>
<span>13</span>
<span>14</span>
<span>15</span>
<span>16</span>
<span>17</span>
<span>18</span>
<span>19</span>
<span>20</span>
<span>21</span>
<span>22</span>
<span>23</span>
<span>24</span>
<span>25</span>
<span>26</span></pre></div></td><td><div><pre><span></span><span># count neighbors</span>
<span>summed</span> <span>=</span> <span>state</span>
<span>summed</span> <span>+=</span> <span>(</span><span>summed</span> <span>&gt;&gt;</span> <span>4</span><span>)</span> <span>+</span> <span>(</span><span>summed</span> <span>&lt;&lt;</span> <span>4</span><span>)</span>
<span>summed</span> <span>+=</span> <span>(</span><span>summed</span> <span>&gt;&gt;</span> <span>COLSHIFT</span><span>)</span> <span>+</span> <span>(</span><span>summed</span> <span>&lt;&lt;</span> <span>COLSHIFT</span><span>)</span>

<span># check if there are exactly 3 neighbors</span>
<span>has_3_neighbors</span> <span>=</span> <span>summed</span> <span>^</span> <span>MASK_NOT_3</span> <span># at this point, a value of all 1s means it was initially 3</span>
<span>has_3_neighbors</span> <span>&amp;=</span> <span>has_3_neighbors</span> <span>&gt;&gt;</span> <span>2</span> <span># fold in half</span>
<span>has_3_neighbors</span> <span>&amp;=</span> <span>has_3_neighbors</span> <span>&gt;&gt;</span> <span>1</span> <span># fold in half again</span>

<span># check if there are exactly 4 neighbors</span>
<span>has_4_neighbors</span> <span>=</span> <span>summed</span> <span>^</span> <span>MASK_NOT_4</span> <span># at this point, a value of all 1s means it was initially 4</span>
<span>has_4_neighbors</span> <span>&amp;=</span> <span>has_4_neighbors</span> <span>&gt;&gt;</span> <span>2</span>  <span># fold in half</span>
<span>has_4_neighbors</span> <span>&amp;=</span> <span>has_4_neighbors</span> <span>&gt;&gt;</span> <span>1</span>  <span># fold in half again</span>

<span>if</span> <span>cfg</span><span>.</span><span>drylife</span><span>:</span>
    <span># check if there are exactly 7 neighbors</span>
    <span>has_7_neighbors</span> <span>=</span> <span>summed</span> <span>^</span> <span>MASK_NOT_7</span> <span># at this point, a value of all 1s means it was initially 7</span>
    <span>has_7_neighbors</span> <span>&amp;=</span> <span>has_7_neighbors</span> <span>&gt;&gt;</span> <span>2</span>  <span># fold in half</span>
    <span>has_7_neighbors</span> <span>&amp;=</span> <span>has_7_neighbors</span> <span>&gt;&gt;</span> <span>1</span>  <span># fold in half again</span>

    <span># variable name here is misleading...</span>
    <span>has_3_neighbors</span> <span>|=</span> <span>(</span><span>~</span><span>state</span><span>)</span> <span>&amp;</span> <span>has_7_neighbors</span>

<span># apply game-of-life rules</span>
<span>state</span> <span>=</span> <span>(</span><span>has_3_neighbors</span> <span>|</span> <span>(</span><span>state</span> <span>&amp;</span> <span>has_4_neighbors</span><span>))</span> <span>&amp;</span> <span>MASK_CANVAS</span>
</pre></div></td></tr></tbody></table></div>
<p>(I've omitted some details like wraparound handling; you can see the full code, along with definitions of those magic constants, <a href="https://github.com/DavidBuchanan314/pyswargol/blob/main/swargol.py">here</a>)</p>
<p>Aside from the different state representation, this achieves exactly the same thing as the previous snippet. The <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></math> loop over every cell has been completely eliminated! Well, almost. There are <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math> CPython VM operations, but there is still <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></math> work being done, hidden away inside the SIMD-accelerated native code of CPython's bigint arithmetic routines. This is a <em>huge</em> performance win, which I'll quantify later. But first, how do we turn that <code>state</code> integer into pixels on the screen?</p>
<h3>Blitting</h3>
<p>To get pixels on the screen, we need to convert the data into a more standard format. The very first step is easy: Python integers have a <code>to_bytes</code> method that serialises them into bytes (like I used in the XOR function example earlier in this article). What to do with those bytes next is less obvious.</p>
<p>In the spirit of only using "pure" Python, I came up with a ridiculous hack: craft a compressed gzip stream that, when decompressed, converts the weird 4-bits-per-pixel buffer into a more standard 8-bits-per-pixel grayscale buffer, and surrounds it in the necessary framing data to be a YUV4MPEG video stream. The output of the Python script can be piped into a gzip decompressor, and subsequently, a video player. That code is <a href="https://gist.github.com/DavidBuchanan314/acae2aab38953759aacc114b417ed0b9">here</a>, and probably deserves an article of its own, but I'm not going to go into it today.</p>
<p>While this was a great hack, gzip is not an especially efficient <a href="https://en.wikipedia.org/wiki/Blitter">blitter</a>. I was able to get about 24fps at full-screen resolution on my 2021 macbook, but I really wanted at least 60fps, and that approach wasn't going to cut it.</p>
<p>The <em>ideal</em> approach would probably be to ship the 4bpp data off to the GPU as a texture, as-is, and write a <a href="https://www.khronos.org/opengl/wiki/Fragment_Shader">fragment shader</a> capable of unpacking it onto the screen pixels. The only reason I didn't do this is because it feels silly. It feels silly because if we're doing GPU programming, we might as well just implement the whole GoL algorithm on the GPU. It'd be way faster, but it wouldn't be within the spirit of the completely arbitrary constraints I'd set for myself.</p>
<p>My compromise here was to use SDL2, via the <code>pysdl2</code> bindings. Sure, it's not "pure Python," but it <em>is</em> very standard and widely available. It feels "right" because I can use it to its full extent without completely defeating the purpose (like running GoL entirely on the GPU would do).</p>
<p>SDL2 supports a pixel format called <code>SDL_PIXELFORMAT_INDEX4LSB</code>, which is a 4-bit palleted mode. If we set up an appropriate palette (specifying the colours for "dead" and "alive" cells, respectively), then we can pass our 4bpp buffer to SDL as-is, and it'll know how to convert it into the right format for sending to the GPU (in this case, <code>SDL_PIXELFORMAT_ARGB8888</code>). This process isn't terribly efficient, because the conversion still happens on the CPU, and the amount of data sent over to the GPU is much larger than necessary. Despite that, it's a whole lot faster than the gzip method, getting around 48fps.</p>
<h3>Parallelization</h3>
<p>We still haven't hit the 60fps target, and to get there I added some parallelization. I summarised my approach in a code comment:</p>
<div><table><tbody><tr><td><div><pre><span> 1</span>
<span> 2</span>
<span> 3</span>
<span> 4</span>
<span> 5</span>
<span> 6</span>
<span> 7</span>
<span> 8</span>
<span> 9</span>
<span>10</span>
<span>11</span>
<span>12</span>
<span>13</span>
<span>14</span>
<span>15</span>
<span>16</span>
<span>17</span>
<span>18</span>
<span>19</span>
<span>20</span>
<span>21</span>
<span>22</span>
<span>23</span>
<span>24</span>
<span>25</span>
<span>26</span>
<span>27</span>
<span>28</span>
<span>29</span>
<span>30</span>
<span>31</span>
<span>32</span>
<span>33</span>
<span>34</span>
<span>35</span>
<span>36</span>
<span>37</span>
<span>38</span>
<span>39</span></pre></div></td><td><div><pre><span></span><span>"""</span>

<span>┌───────────┐             Graphics Process</span>
<span>│ ┌────┐    │    ┌───────────────────────────────┐</span>
<span>│ │  ┌─▼────┴─┐  │  ┌─────────┐  ┌────────────┐  │</span>
<span>│ │  │  Life  ├─────► Blitter ├──►            │  │</span>
<span>│ │  └─┬────▲─┘  │  └─────────┘  │            │  │</span>
<span>▼ │  ┌─▼────┴─┐  │  ┌─────────┐  │            │  │</span>
<span>│ ▲  │  Life  ├─────► Blitter ├──►            │  │</span>
<span>│ │  └─┬────▲─┘  │  └─────────┘  │    GUI     │  │</span>
<span>│ │  ┌─▼────┴─┐  │  ┌─────────┐  │  Renderer  │  │</span>
<span>│ │  │  Life  ├─────► Blitter ├──►            │  │</span>
<span>│ │  └─┬────▲─┘  │  └─────────┘  │            │  │</span>
<span>▼ │  ┌─▼────┴─┐  │  ┌─────────┐  │            │  │</span>
<span>│ ▲  │  Life  ├─────► Blitter ├──►            │  │</span>
<span>│ │  └─┬────▲─┘  │  └─────────┘  └────────────┘  │</span>
<span>│ └────┘    │    └───────────────────────────────┘</span>
<span>└───────────┘</span>

<span>"Life" threads implement the SWAR life algorithm, for a horizontal strip of</span>
<span>the overall canvas.</span>

<span>Blitter threads use SDL2 functions to unpack the SWAR buffers into RGBA8888</span>
<span>surfaces, which are passed to the main Renderer thread.</span>

<span>The renderer thread is responsible for uploading the surfaces to a GPU texture,</span>
<span>and making it show up on the screen. It's also responsible for dealing with SDL</span>
<span>events.</span>

<span>Each Life thread lives in its own process, to avoid the GIL. They</span>
<span>talk to each other (for overlap/wraparound), and to the Blitter threads (to</span>
<span>report their results), using Pipes.</span>

<span>Everything else happens in the main process, so the Blitters can talk to the</span>
<span>main thread using standard Queues - the hard work here is done inside SDL2,</span>
<span>which does not hold the GIL (meaning we can use multithreading, as opposed</span>
<span>to multiprocessing).</span>

<span>"""</span>
</pre></div></td></tr></tbody></table></div>
<p>That's enough to smash past the 60fps target, reaching ~250fps at my full-screen resolution of 3024x1890, using 8 parallel Life processes. At 4K resolution (3840x2160), it can reach 180fps.</p>
<p>There are plenty of remaining inefficiencies here that could be improved on, but 250fps is already <em>way</em> faster than I care about, so I'm not going to optimise any further. I think 60fps is the most visually interesting speed to watch at, anyway (which can be achieved by turning on vsync).</p>
<blockquote>
<p><strong>Edit:</strong> After having some people test on non-Apple-silicon systems, many are struggling to hit 4K60fps. I haven't done any profiling, but my guess is that the bottleneck is the CPU-&gt;GPU bandwidth, or maybe just memory bandwidth in general. I might revisit this in the future, perhaps implementing the buffer-unpacking fragment shader idea I mentioned.</p>
</blockquote>
<p>Earlier, I said that upgrading from the naive implementation to SWAB gave "huge" performance wins, so just how huge are they? If I go back to the naive approach, I get an incredible 0.4fps at 4K resolution (and that's <em>with</em> 8x parallelism), representing a ~450x performance difference. If I get extra mean and force it to run on a single thread, it's a 3800x difference relative to the fully optimised version. Damn!</p>
<p>As a reminder, both approaches are <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></math>, but the faster version gets to spend more time within efficient native code.</p>
<p>If I rewrote this whole thing in C using SIMD intrinsics (combined with SWAR or other tricks), I predict that I'd get somewhere between 10x and 100x further speedup, due to more efficient use of SIMD registers and memory accesses. A GPU implementation could be faster still. Those speedups would be nice to have, but I think it's interesting how far I was able to get just by optimising the Python version.</p>
<p>The source code is available <a href="https://github.com/DavidBuchanan314/pyswargol">here</a>.</p>

				</section>
				
				
				
				
				
			</div></div>]]></description>
        </item>
    </channel>
</rss>