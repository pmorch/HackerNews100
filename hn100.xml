<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 27 Feb 2024 16:00:17 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Boeing missing key elements of safety culture: FAA report (152 pts)]]></title>
            <link>https://www.ainonline.com/aviation-news/air-transport/2024-02-26/boeing-missing-key-elements-safety-culture-faa-report</link>
            <guid>39523813</guid>
            <pubDate>Tue, 27 Feb 2024 13:30:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ainonline.com/aviation-news/air-transport/2024-02-26/boeing-missing-key-elements-safety-culture-faa-report">https://www.ainonline.com/aviation-news/air-transport/2024-02-26/boeing-missing-key-elements-safety-culture-faa-report</a>, See on <a href="https://news.ycombinator.com/item?id=39523813">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-grid-variation="default" data-variation="more-in"><div><a href="https://www.ainonline.com/aviation-news/business-aviation/2024-02-27/utah-medevac-program-aims-save-stricken-service-dogs"><div data-sponsored="false" data-variation="media-top" data-blurred="false"><div><h2>Utah Medevac Program Aims To Save Stricken Service Dogs</h2><h3>It represents the third program of its kind in the U.S.</h3><p>Rotorcraft</p></div><p><img alt="K9 handlers with their charges by helicopter" loading="lazy" width="700" height="395" decoding="async" data-nimg="1" srcset="https://www.ainonline.com/cdn-cgi/image/width=750,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/362%20Intermountain%20Life%20Flight%20Canine.jpeg?h=413144ff&amp;itok=rv2wmtRm 1x, https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/362%20Intermountain%20Life%20Flight%20Canine.jpeg?h=413144ff&amp;itok=rv2wmtRm 2x" src="https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/362%20Intermountain%20Life%20Flight%20Canine.jpeg?h=413144ff&amp;itok=rv2wmtRm"></p></div></a></div><div><a href="https://www.ainonline.com/aviation-news/business-aviation/2024-02-26/argus-prism-sms-success-and-2024-helicopter-market"><div data-sponsored="false" data-variation="media-top" data-blurred="false"><div><h2>Argus: Prism SMS Success and the 2024 Helicopter Market</h2><h3>Global helicopter activity was up almost 20 percent in 2023</h3><p>Rotorcraft</p></div><p><img alt="Airbus Heli" loading="lazy" width="700" height="395" decoding="async" data-nimg="1" srcset="https://www.ainonline.com/cdn-cgi/image/width=750,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Airbus-Heli_CDPH-7409-00057R.jpg?h=2361f00c&amp;itok=ztr84gf- 1x, https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Airbus-Heli_CDPH-7409-00057R.jpg?h=2361f00c&amp;itok=ztr84gf- 2x" src="https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Airbus-Heli_CDPH-7409-00057R.jpg?h=2361f00c&amp;itok=ztr84gf-"></p></div></a></div><div><a href="https://www.ainonline.com/aviation-news/business-aviation/2024-02-26/vita-aerospace-unveiling-rapid-extraction-device"><div data-sponsored="false" data-variation="media-top" data-blurred="true"><div><h2>Vita Aerospace Unveiling Rapid Extraction Device</h2><h3>The new device enables safer hoisting of unconscious victims</h3><p>Rotorcraft</p></div><p><img alt="Vita Pelican Rapid Extraction Device " loading="lazy" width="885" height="500" decoding="async" data-nimg="1" srcset="https://www.ainonline.com/cdn-cgi/image/width=1080,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_885x500/public/2024-02/334web--HAI24_VitaAerospaceRED_mrosales_1595.jpg?h=aecc625a&amp;itok=fzlytipR 1x, https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_885x500/public/2024-02/334web--HAI24_VitaAerospaceRED_mrosales_1595.jpg?h=aecc625a&amp;itok=fzlytipR 2x" src="https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_885x500/public/2024-02/334web--HAI24_VitaAerospaceRED_mrosales_1595.jpg?h=aecc625a&amp;itok=fzlytipR"></p></div></a></div><div><a href="https://www.ainonline.com/aviation-news/business-aviation/2024-02-25/airbus-seeks-boost-output-and-performance"><div data-sponsored="false" data-variation="media-top" data-blurred="false"><div><h2>Airbus Seeks To Boost Output and Performance</h2><h3>Rotorcraft accounted for the strongest earnings growth across the Airbus group</h3><p>Rotorcraft</p></div><p><img alt="Airbus Helicopters" loading="lazy" width="700" height="395" decoding="async" data-nimg="1" srcset="https://www.ainonline.com/cdn-cgi/image/width=750,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/EXPH-2142-003.jpg?h=9e7ae2bd&amp;itok=2txfqylj 1x, https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/EXPH-2142-003.jpg?h=9e7ae2bd&amp;itok=2txfqylj 2x" src="https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/EXPH-2142-003.jpg?h=9e7ae2bd&amp;itok=2txfqylj"></p></div></a></div><div><a href="https://www.ainonline.com/aviation-news/business-aviation/2024-02-23/union-balks-bringing-super-pumas-back-north-sea"><div data-sponsored="false" data-variation="media-top" data-blurred="false"><div><h2>Union Balks at Bringing Super Pumas Back to North Sea</h2><h3>Calls it an 'insult' to the memory of 33 killed</h3><p>Safety</p></div><p><img alt="Airbus" loading="lazy" width="700" height="395" decoding="async" data-nimg="1" srcset="https://www.ainonline.com/cdn-cgi/image/width=750,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Screenshot%202024-02-23%20at%208.55.43%20AM.png?h=7d56c686&amp;itok=vPoJKcAX 1x, https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Screenshot%202024-02-23%20at%208.55.43%20AM.png?h=7d56c686&amp;itok=vPoJKcAX 2x" src="https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Screenshot%202024-02-23%20at%208.55.43%20AM.png?h=7d56c686&amp;itok=vPoJKcAX"></p></div></a></div><div><a href="https://www.ainonline.com/aviation-news/business-aviation/2024-02-22/ainsight-bucket-list-professional-pilots"><div data-sponsored="false" data-variation="media-top" data-blurred="false"><h2>AINsight: The Bucket List for Professional Pilots</h2><h3>Trying out new ways to fly is a sure way to recharge a pilot's batteries</h3><p>Training and Workforce</p></div></a></div><div><a href="https://www.ainonline.com/aviation-news/business-aviation/2024-02-21/transport-canada-reissues-shoulder-harness-exemption"><div data-sponsored="false" data-variation="media-top" data-blurred="false"><div><h2>Transport Canada Reissues Shoulder Harness Exemption</h2><h3>Pilots can fly without wearing shoulder harnesses above 10,000 feet</h3><p>Safety</p></div><p><img alt="pilots shot" loading="lazy" width="700" height="395" decoding="async" data-nimg="1" srcset="https://www.ainonline.com/cdn-cgi/image/width=750,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/uploads/2022/01/334_pilots-ipadweb.jpg?h=cc766518&amp;itok=lKTqMgxi 1x, https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/uploads/2022/01/334_pilots-ipadweb.jpg?h=cc766518&amp;itok=lKTqMgxi 2x" src="https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/uploads/2022/01/334_pilots-ipadweb.jpg?h=cc766518&amp;itok=lKTqMgxi"></p></div></a></div><div><a href="https://www.ainonline.com/aviation-news/general-aviation/2024-02-21/firefighting-radio-system-now-available-latin-america"><div data-sponsored="false" data-variation="media-top" data-blurred="false"><div><h2>Firefighting Radio System Now Available in Latin America</h2><h3>First orders set to ship in six to eight weeks</h3><p>Avionics</p></div><p><img alt="A mockup of the new AEM radio." loading="lazy" width="700" height="395" decoding="async" data-nimg="1" srcset="https://www.ainonline.com/cdn-cgi/image/width=750,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Screen%20Shot%202024-02-21%20at%201.56.30%20PM.png?h=2bc11df2&amp;itok=awBANAeD 1x, https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Screen%20Shot%202024-02-21%20at%201.56.30%20PM.png?h=2bc11df2&amp;itok=awBANAeD 2x" src="https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Screen%20Shot%202024-02-21%20at%201.56.30%20PM.png?h=2bc11df2&amp;itok=awBANAeD"></p></div></a></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Easter eggs on Swiss maps (114 pts)]]></title>
            <link>https://www.atlasobscura.com/articles/swiss-map-secrets</link>
            <guid>39523187</guid>
            <pubDate>Tue, 27 Feb 2024 12:21:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.atlasobscura.com/articles/swiss-map-secrets">https://www.atlasobscura.com/articles/swiss-map-secrets</a>, See on <a href="https://news.ycombinator.com/item?id=39523187">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="article-body">
<p><span>Swiss humor. </span>Now there’s two words you don’t often see together. In fact, Google Trends<a href="https://trends.google.com/trends/explore?date=all&amp;geo=DK&amp;q=Swiss%20humor"> lists <em>zero</em> occurrences</a> of the phrase between 2004 and now. Even “German humor” produces a graph (albeit a rather flat one). But not only is there some evidence that Swiss comedy does exist, it might just be that being well-hidden is kind of its thing. Find it and laugh. Or don’t, and the joke’s on you!</p>
<p>That evidence, as it turns out, is cartographic. The Swiss Federal Office of National Topography,<a href="https://www.swisstopo.admin.ch/"> Swisstopo</a> for short, is a decidedly serious institution. Many serious things—time and money, for starters—depend on the accuracy of its maps. In the case of its mountain maps, actual lives hang in the balance. Yet in decades past, the austere institute’s maps have served as the canvas for a series of in-jokes among its more fun-loving cartographers.</p>
<figure><img src="https://img.atlasobscura.com/TF_h2N2cGkEVAU5ELOD91Wp4msaxylLzh8m1CevqMmc/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9kY2Q5MTk5NS02/NTg5LTQxMzQtYTlk/Zi0xNTkxZTFkM2Nh/YmQzNzc0ZmYzOGRm/NDFmZTVjNmRfc2Ft/dWVsLWZlcnJhcmEt/SUVIUEROazItOHct/dW5zcGxhc2guanBn.jpg" alt="Switzerland's geography proved to inspire map makers in surprising ways." width="auto" data-kind="article-image" id="article-image-100240" data-src="https://img.atlasobscura.com/TF_h2N2cGkEVAU5ELOD91Wp4msaxylLzh8m1CevqMmc/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9kY2Q5MTk5NS02/NTg5LTQxMzQtYTlk/Zi0xNTkxZTFkM2Nh/YmQzNzc0ZmYzOGRm/NDFmZTVjNmRfc2Ft/dWVsLWZlcnJhcmEt/SUVIUEROazItOHct/dW5zcGxhc2guanBn.jpg"><figcaption>Switzerland’s geography proved to inspire map makers in surprising ways. <a target="_blank" href="https://unsplash.com/photos/landscape-photography-of-mountain-during-daytime-IEHPDNk2-8w">Samuel Ferrara/Public Domain</a></figcaption></figure>
<p>These mapmakers played a game of wits against their superiors, the ones whose duties included checking the maps before publication. Over the years, the cartographers managed to slip in—on maps that were supposed to contain only dry topographic facts—drawings of an airplane, a fish, a marmot, a mountaineer, a face, a spider, even of a naked lady. Once discovered, these humorous additions were removed without pardon. At least, that’s how it used to be.</p>
<p>Either way, it doesn’t matter. Swisstopo is defeated by its own thoroughness. Its<a href="https://map.geo.admin.ch/"> map page</a> allows you not just to zoom in and out of the most recent maps but also to browse historical maps and thus revisit these “Easter eggs” that prove, however obliquely, the existence of a sense of humor among the mountains of Switzerland.</p>
<figure><img src="https://img.atlasobscura.com/ouhDVpuIXAo3SFvcq5PVwthqjPAkbzTO4k_6bBpsS_4/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy8xMjczZGVkYy01/OTZkLTRiNjMtYjJk/Zi04ZTZhNDI4ZGI0/ZDIzNzc0ZmYzOGRm/NDFmZTVjNmRfenVy/aWNoLWFpcnBvcnQu/anBn.jpg" alt="Topographic maps normally never have planes on them." width="auto" data-article-image-id="undefined" data-full-size-image="https://assets.atlasobscura.com/article_images/full/100239/image" data-kind="article-image" id="article-image-100239" data-src="https://img.atlasobscura.com/ouhDVpuIXAo3SFvcq5PVwthqjPAkbzTO4k_6bBpsS_4/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy8xMjczZGVkYy01/OTZkLTRiNjMtYjJk/Zi04ZTZhNDI4ZGI0/ZDIzNzc0ZmYzOGRm/NDFmZTVjNmRfenVy/aWNoLWFpcnBvcnQu/anBn.jpg"><figcaption>Topographic maps normally never have planes on them. <a target="_blank" href="https://map.geo.admin.ch/">Swisstopo</a></figcaption></figure>
<h3><strong>The plane that disappeared—twice</strong></h3>
<p>In 1994, an anonymous cartographer at Swisstopo included an airplane in this map of Kloten, the international airport of Zürich. While it may seem only natural for airplanes to show up at airports, that is normally not the case on topographic maps.</p>
<p>The error remained undetected until a revision of the map in 2000, when the offending craft was erased. However, the plane reappeared on the 2007 map at exactly the same spot—the tarmac before Gate A—only to vanish again in 2013.</p>
<figure><img src="https://img.atlasobscura.com/mFxxMnNDB8tJkzL-zeT_e8ZOhhui_kPnDnPs2m523OY/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy8yYzVkYzliMy05/MWNiLTRhY2ItODFj/NS01NmQ4ODk4OGVj/NDgzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDAuanBn.jpg" alt="Despite having no head, the figure's filled-in lines leave little to the imagination." width="auto" data-article-image-id="undefined" data-full-size-image="https://assets.atlasobscura.com/article_images/full/100238/image" data-kind="article-image" id="article-image-100238" data-src="https://img.atlasobscura.com/mFxxMnNDB8tJkzL-zeT_e8ZOhhui_kPnDnPs2m523OY/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy8yYzVkYzliMy05/MWNiLTRhY2ItODFj/NS01NmQ4ODk4OGVj/NDgzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDAuanBn.jpg"><figcaption>Despite having no head, the figure’s filled-in lines leave little to the imagination. <a target="_blank" href="https://map.geo.admin.ch/">Swisstopo</a></figcaption></figure>
<h3><strong>The Naked Lady of Künten</strong></h3>
<p>Possibly the oldest topographical Easter egg, and the current record holder for the longest-lasting one, is the Naked Lady of Künten. First appearing on the topographical map of 1954, the reclining figure wasn’t discovered until 2012. Admittedly, without head, arms and feet, she is hard to spot. Her odalisque-like forms are suggested by the curvature of a stream and an elongated green patch indicating vegetation.</p>
<p>The world—or at least that bit between Eggenrain and Sunnenberg—was put to right again in the 2013 edition of the local map. But it’s still easy to see how that particular distribution of topographic features could have inspired a lonely 1950s cartographer to pencil in something that wasn’t there.</p>
<figure><img src="https://img.atlasobscura.com/LEowyKb-jUX75vzlXCCd-XpNr-972ODbpHk0rXnK9ZI/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9jODliYzIzYS1l/MjZjLTRjODEtOThh/Yy0wYWVkYWE2ZjBk/ZGYzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDIuanBn.jpg" alt="Any serious federal office could not allow a stray fish on its maps." width="auto" data-article-image-id="undefined" data-full-size-image="https://assets.atlasobscura.com/article_images/full/100237/image" data-kind="article-image" id="article-image-100237" data-src="https://img.atlasobscura.com/LEowyKb-jUX75vzlXCCd-XpNr-972ODbpHk0rXnK9ZI/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9jODliYzIzYS1l/MjZjLTRjODEtOThh/Yy0wYWVkYWE2ZjBk/ZGYzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDIuanBn.jpg"><figcaption>Any serious federal office could not allow a stray fish on its maps. <a target="_blank" href="https://map.geo.admin.ch/">Swisstopo</a></figcaption></figure>
<h3><strong>A Swiss fish in a French lake</strong></h3>
<p>It was never discovered who reshaped the aforementioned landscape feature into a female form. But the younger generation of Easter-eggers is known by name.</p>
<p>In 1980, Werner Leuenberger even went international. He drew a fish at the southern end of the <em>Lac de Remoray</em>, a small lake just across the Franco-Swiss border. The fish felt right at home among the lines marking out the area as swampy. However, it was caught five years later, and has been left off the map since 1986.</p>
<figure><img src="https://img.atlasobscura.com/B75neSF-cnLwT1-neEsUt3Kz6RdXLTNIhqArKjXXN1c/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy82ODhkNTllMy00/NjEwLTQ2ZTUtODJm/Yy00OTM1OGYxNDc2/YTQzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDQuanBn.jpg" alt="The spider once aligned with a particularly dangerous part of the mountain." width="auto" data-article-image-id="undefined" data-full-size-image="https://assets.atlasobscura.com/article_images/full/100236/image" data-kind="article-image" id="article-image-100236" data-src="https://img.atlasobscura.com/B75neSF-cnLwT1-neEsUt3Kz6RdXLTNIhqArKjXXN1c/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy82ODhkNTllMy00/NjEwLTQ2ZTUtODJm/Yy00OTM1OGYxNDc2/YTQzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDQuanBn.jpg"><figcaption>The spider once aligned with a particularly dangerous part of the mountain. <a target="_blank" href="https://map.geo.admin.ch/">Swisstopo</a></figcaption></figure>
<h3><strong>Attack of the giant Eiger spider</strong></h3>
<p>In 1981, Othmar Wyss inserted a spider near the top of the Eiger, one of Switzerland’s most iconic Alpine summits, at a location actually known by mountaineers as quite dangerous.</p>
<p>The giant spider survived for six years in the freezing cold. The snowfield that made up the spider’s body—and made the northern approach of the Eiger so hard—has apparently also disappeared in the intervening years.</p>
<figure><img src="https://img.atlasobscura.com/OmKdGctdFCdkTLWR-Z0xPqsbGh44cNg9vp-5d8bZwJE/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9mYjhlMTliNy05/MWIxLTRiMzAtODgz/My1iZjE1NWRmM2Jm/MTczNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDUuanBn.jpg" alt="A face still leers on a mountain slope today." width="auto" data-article-image-id="undefined" data-full-size-image="https://assets.atlasobscura.com/article_images/full/100235/image" data-kind="article-image" id="article-image-100235" data-src="https://img.atlasobscura.com/OmKdGctdFCdkTLWR-Z0xPqsbGh44cNg9vp-5d8bZwJE/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9mYjhlMTliNy05/MWIxLTRiMzAtODgz/My1iZjE1NWRmM2Jm/MTczNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDUuanBn.jpg"><figcaption>A face still leers on a mountain slope today. <a target="_blank" href="https://map.geo.admin.ch/">Swisstopo</a></figcaption></figure>
<h3><strong>Haunted monk trapped in a map</strong></h3>
<p>A rock formation on a slope of the Harder Kulm, a mountain near Interlaken, looks like a face. This is the <em>Hardermandli</em>, or “little Harder man.” Legend has it that he was a lecherous monk, condemned to look down on the place where he chased a girl to her death.</p>
<p>Cartographer Friedrich Siegfried extended the curse to cartography, for since 1980 and until this day, the Hardermandli also lives on the map.</p>
<figure><img src="https://img.atlasobscura.com/g2miEHJwmdwJQi70um2E7ys6xc9gdynMeJXGeqG0YQs/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy8zZjRlN2JmMS1m/ZTQ0LTRlMmItYTBm/Yy03MTZkNDc5MmI2/YzQzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDYuanBn.jpg" alt="Where's Waldo in Italy?" width="auto" data-article-image-id="undefined" data-full-size-image="https://assets.atlasobscura.com/article_images/full/100234/image" data-kind="article-image" id="article-image-100234" data-src="https://img.atlasobscura.com/g2miEHJwmdwJQi70um2E7ys6xc9gdynMeJXGeqG0YQs/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy8zZjRlN2JmMS1m/ZTQ0LTRlMmItYTBm/Yy03MTZkNDc5MmI2/YzQzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDYuanBn.jpg"><figcaption>Where’s Waldo in Italy? <a target="_blank" href="https://map.geo.admin.ch/">Swisstopo</a></figcaption></figure>
<h3><strong>Beats waiting for the Italians</strong></h3>
<p>For the 1997 map update, Mr. Siegfried etched the likeness of a mountaineer on the Italian side of a mountain slope near Val Müstair. Reportedly, he got tired of waiting on the data for the area, which his Italian counterparts were slow to provide, so he found a creative way to plug the gap. Topography, like nature, also abhors a vacuum, apparently.</p>
<p>Swisstopo seems to have taken to heart the cartographer’s slight against his Italian colleagues, because the mountaineer still appears on the contemporary map, in 1:100,000 scale at least.</p>
<figure><img src="https://img.atlasobscura.com/OmislREC2TDa1cme6jqtn7dlGEZA2d_DmVaP7eM0WMo/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy8yYTk0YTI2NS1m/YWJiLTQ0YzctODBi/Zi03ZGYxYzU5Zjcw/OTEzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDkuanBn.jpg" alt="The marmot is the most recent Easter egg." width="auto" data-article-image-id="undefined" data-full-size-image="https://assets.atlasobscura.com/article_images/full/100233/image" data-kind="article-image" id="article-image-100233" data-src="https://img.atlasobscura.com/OmislREC2TDa1cme6jqtn7dlGEZA2d_DmVaP7eM0WMo/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy8yYTk0YTI2NS1m/YWJiLTQ0YzctODBi/Zi03ZGYxYzU5Zjcw/OTEzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDkuanBn.jpg"><figcaption>The marmot is the most recent Easter egg. <a target="_blank" href="https://map.geo.admin.ch/">Swisstopo</a></figcaption></figure>
<h3><strong>The marmot of the Aletsch glacier</strong></h3>
<p>Swisstopo’s most famous map gag—or at least the most recent one to be revealed, in 2014—is the marmot, which has been hiding in a rock near the Aletsch glacier since it was put there by cartographer Paul Ehrlich in 2011, shortly before his retirement. The marmot is still there, and perhaps it and its fellow map oddities may be allowed to survive.</p>
<p>On its website, Swisstopo says that “these hidden drawings do not affect the accuracy and level of detail of our maps, nor on the safety and security of their users. They merely add a note of mystery to our nation’s maps.”</p>
<p>Are there any other gags hidden in the official maps of Switzerland? Swisstopo itself claims it has no knowledge of any other cartographic oddities. But knowing and not telling, that’s exactly the kind of thing they would find funny, isn’t it?</p>
<p><a href="https://bigthink.com/strange-maps/swiss-maps/"><em>This article</em></a><em> originally appeared on</em><a href="https://bigthink.com/?utm_source=syndication&amp;utm_medium=organicpartner&amp;utm_campaign=atlasobscura"> <em>Big Think</em></a><em>, home of the brightest minds and biggest ideas of all time.</em><a href="https://bigthink.com/subscribe/?utm_source=syndication&amp;utm_medium=organicpartner&amp;utm_campaign=atlasobscura"> <em>Sign up for Big Think’s newsletter.</em></a></p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rainwater everywhere on the planet is unsafe to drink due to chemicals (2022) (167 pts)]]></title>
            <link>https://phys.org/news/2022-08-rainwater-unsafe-due-chemicals.html</link>
            <guid>39522798</guid>
            <pubDate>Tue, 27 Feb 2024 11:27:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phys.org/news/2022-08-rainwater-unsafe-due-chemicals.html">https://phys.org/news/2022-08-rainwater-unsafe-due-chemicals.html</a>, See on <a href="https://news.ycombinator.com/item?id=39522798">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
										
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2022/rainwater-1.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2022/rainwater-1.jpg" data-sub-html="Credit: Pixabay/CC0 Public Domain">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2022/rainwater-1.jpg" alt="rainwater" title="Credit: Pixabay/CC0 Public Domain" width="800" height="530">
             <figcaption>
                Credit: Pixabay/CC0 Public Domain
            </figcaption>        </figure>
    </div><p>Rainwater everywhere on the planet is unsafe to drink due to levels of toxic chemicals known as PFAS that exceed the latest guidelines, according to a new study by Stockholm University scientists.</p>


										      
																																	<p>Commonly known as 'forever chemicals' because they disintegrate extremely slowly, PFAS (per- and <a href="https://phys.org/tags/polyfluoroalkyl+substances/" rel="tag">polyfluoroalkyl substances</a>) were initially found in packaging, shampoo or makeup but have spread to our entire environment, including water and air.</p>
<p>"There is nowhere on Earth where the rain would be safe to drink, according to the measurements that we have taken," Ian Cousins, a professor at the university and the lead author of the study published in <i>Environmental Science and Technology</i>, told AFP.</p>
<p>A compilation of the data since 2010 that his team studied showed that "even in Antarctica or the Tibetan plateau, the levels in the rainwater are above the drinking water guidelines that the US EPA (Environmental Protection Agency) proposed", he said.</p>
<p>Normally considered pristine, the two regions still have PFAS levels "14 times higher" than the US drinking water guidelines.</p>
<p>The EPA recently lowered its PFAS guidelines significantly after discovering that the chemicals may affect the <a href="https://phys.org/tags/immune+response/" rel="tag">immune response</a> in children to vaccines, Cousins noted.</p>
<p>Once ingested, PFAS accumulate in the body.</p>
<p>According to some studies, exposure can also lead to problems with fertility, <a href="https://phys.org/tags/developmental+delays/" rel="tag">developmental delays</a> in children, increased risks of obesity or certain cancers (prostate, kidney and testicular), an increase in <a href="https://phys.org/tags/cholesterol+levels/" rel="tag">cholesterol levels</a>.<br>
—Planet 'irreversibly contaminated'—</p>

																																						
																																			<p>Cousins said PFAS were now "so persistent" and ubiquitous that they will never disappear from the planet.</p>
<p>"We have made the planet inhospitable to human life by irreversibly contaminating it now so that nothing is clean anymore. And to the point that's it's not clean enough to be safe", he said.</p>
<p>"We have crossed a planetary boundary", he said, referring to a central paradigm for evaluating Earth's capacity to absorb the impact of human activity.</p>
<p>However, Cousins noted that PFAS levels in people have actually dropped "quite significantly in the last 20 years" and "ambient levels (of PFAS in the environment) have been the same for the past 20 years".</p>
<p>"What's changed is the guidelines. They've gone down millions of times since the early 2000s, because we've learned more about the toxicity of these substances."</p>
<p>Cousins said we have to learn to live with it.</p>
<p>"I'm not super concerned about the everyday exposure in mountain or stream <a href="https://phys.org/tags/water/" rel="tag">water</a> or in the food. We can't escape it... we're just going to have to live with it."</p>
<p>"But it's not a great situation to be in, where we've contaminated the environment to the point where background exposure is not really safe."</p>

																																																					
																				<div>
																						<p><strong>More information:</strong>
												Per- and polyfluoroalkyl substances (PFAS) define a new planetary boundary for novel entities that has been exceeded, <i>Environmental Science &amp; Technology</i> (2022). <a data-doi="1" href="https://dx.doi.org/10.1021/acs.est.2c02765" target="_blank">DOI: 10.1021/acs.est.2c02765</a>
																						
																						</p>
																					</div>
                               											
																															 <p>
												  © 2022 AFP
											 </p>
										                                        
										<!-- print only -->
										<div>
											 <p><strong>Citation</strong>:
												Rainwater unsafe to drink due to chemicals: study (2022, August 10)
												retrieved 27 February 2024
												from https://phys.org/news/2022-08-rainwater-unsafe-due-chemicals.html
											 </p>
											 <p>
											 This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
											 part may be reproduced without the written permission. The content is provided for information purposes only.
											 </p>
										</div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Blender Open Movies. Featuring all the production files, assets, and artwork (190 pts)]]></title>
            <link>https://studio.blender.org/films/</link>
            <guid>39522499</guid>
            <pubDate>Tue, 27 Feb 2024 10:42:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://studio.blender.org/films/">https://studio.blender.org/films/</a>, See on <a href="https://news.ycombinator.com/item?id=39522499">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <p>The iconic Blender Open Movies. Featuring all the production files, assets, artwork, and never-seen-before content.</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I turned my open-source project into a full-time business (303 pts)]]></title>
            <link>https://docs.emailengine.app/how-i-turned-my-open-source-project-into/</link>
            <guid>39522348</guid>
            <pubDate>Tue, 27 Feb 2024 10:16:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.emailengine.app/how-i-turned-my-open-source-project-into/">https://docs.emailengine.app/how-i-turned-my-open-source-project-into/</a>, See on <a href="https://news.ycombinator.com/item?id=39522348">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-main">
<article>

    <header>

        

        


        <section>

            <ul>
                <li>
                    <a href="https://docs.emailengine.app/author/andris/">
                        <img src="https://www.gravatar.com/avatar/0bc4849da2f3ad66fe4d3fa53a3d45f2?s=250&amp;d=mm&amp;r=x" alt="Andris Reinman">
                    </a>
                </li>
            </ul>

            <div>
                
                <p><time datetime="2024-02-27">Feb 27, 2024</time>
                        <span><span>•</span> 4 min read</span>
                </p>
            </div>

        </section>

            <figure>
                <img srcset="https://docs.emailengine.app/content/images/size/w300/2024/02/EmailEngine_logo_horiz.jpg 300w,
                            https://docs.emailengine.app/content/images/size/w600/2024/02/EmailEngine_logo_horiz.jpg 600w,
                            https://docs.emailengine.app/content/images/size/w1000/2024/02/EmailEngine_logo_horiz.jpg 1000w,
                            https://docs.emailengine.app/content/images/size/w2000/2024/02/EmailEngine_logo_horiz.jpg 2000w" sizes="(min-width: 1400px) 1400px, 92vw" src="https://docs.emailengine.app/content/images/size/w2000/2024/02/EmailEngine_logo_horiz.jpg" alt="How I turned my open-source project into a business">
            </figure>

    </header>

    <section>
        <p>When I started writing and publishing open-source software about 15 years ago, I was pretty radical about it. I only used permissive licenses like MIT or BSD, as all I cared about was reach. Using a copyleft license with strings attached seemed to hinder that reach. Getting another A-category company to use my open-source libraries like <a href="https://nodemailer.com/?ref=docs.emailengine.app">Nodemailer</a> was a badge of honor. I even went so far that when a founder of a major transactional email service sent me an email regarding Nodemailer and offered to make a donation to promote my efforts, I rejected it. I did not want to seem affected by one of the dominant providers because this would not be fair to other providers.</p><p>In hindsight, what a fool I was.</p><p>In any case, it changed years later when a startup using Nodemailer was acquired for half a billion dollars. I was financially not in a good place back then, and when I saw the news, I started to wonder – what did I get out of this? Sending email notifications was a huge part of that service, and they probably sent millions of email notifications a day using Nodemailer. At the very least, I saved them tons of developer hours by providing a free and solid library for sending these emails. I searched my mailbox for emails related to that company and found a single complaint about a feature. No pull requests, no donations, no nothing. And there was nowhere to complain either as I had knowingly given my software for the world to use with no requirements to compensate anything. My empty wallet was not happy about the turn of events.</p><p>So, when I started what eventually became <a href="https://emailengine.app/?ref=docs.emailengine.app">EmailEngine</a>, I tried to cover my back as much as possible. I released the software under the copyleft LGPL license. I also set up an automated CLA process so that no one was able to get their PR merged without signing a CLA first. Many people hate CLAs, and several persons opened a PR first but closed it once they realized that there was a CLA requirement. Well, to be honest, I didn't really care. For example, 98.1% of the code for Nodemailer was written by myself, and only 1.9% was from other contributors, so not getting PRs merged was not a major issue. For EmailEngine, after a year and a half of being published as open source, the same numbers were 99.8% vs 0.2%.</p><blockquote>I use <a href="https://cla-assistant.io/?ref=docs.emailengine.app">CLA assistant</a> for managing CLAs in my projects</blockquote><p>Obviously, I wanted to make some money from my new project, and my business plan was simple. I published the project (it was called IMAP API at that time) as an LGPL-licensed application. I also offered an MIT version, but to get that, you had to subscribe. The subscription fee was 250€ per year. My assumption was that companies - the main target for the software - do not like copyleft licenses and would convert to the permissive license once they see how useful the app is.</p><p>Well, it turns out my business plan was bonkers. I only gained a few paying subscribers, and it seemed to me those people weren't even using IMAP API. They just wanted to support my effort. It turned out that smaller companies did not care about the license at all, and larger companies were not using it. After a year and a half and 750€ in total revenue, I decided to jump ship — enough of providing free stuff.</p><p>I re-designed the UI of the app to look more professional and implemented a license key system. From that moment if you wanted to use EmailEngine (the new name for IMAP API), you needed a license key that was only available for paying subscribers. I also changed the license from LGPL to a commercial license. The source code is still published publicly on <a href="https://github.com/postalsys/emailengine?ref=docs.emailengine.app">GitHub</a>. It is no longer <em>open-source</em> by definition but <em>source-available</em>. This change of license was only possible due to requiring outside committers to sign a CLA from the start.</p><blockquote>I still publish MIT-licensed projects, but only for smaller tools, not larger projects. The goal of these tools is to promote my main effort. For example, I extracted the IMAP client functions from EmailEngine and published it under an MIT license as a generic IMAP client library for Node.js. This module &nbsp;(<a href="https://imapflow.com/?ref=docs.emailengine.app">ImapFlow</a>) is gaining steam in adoption as it is by far better than any pre-existing alternative. The documentation page sends about 100 visitors per month to EmailEngine's homepage, which is not much, but hey, it is free traffic, and sometimes these visitors do convert, making the effort fruitful.</blockquote><p>At first, there wasn't even a trial option. If you did not provide a valid license key 15 minutes after the application started, the app just stopped working.</p><p>I kept the price the same, 250€ per year, and during the first month, I sold 1750€ worth of subscriptions. That's like twice the amount I made in the previous year and a half, and it sealed the fate of the project. There was no going back.</p><p>Next, I started to increase the pricing; 250€ became 495€, then 695€ and 795€, and finally 895€. To my surprise, it did not mean getting fewer customers. I guess any sub-$1k amount for businesses is peanuts, so the only thing these price increases changed was improving the revenue.</p><p>The current MRR for EmailEngine is 6100€ and grows steadily, which in Estonia, where I live, allows me to pay myself a decent salary so that I can work on my project full-time. The only regret I have is that I did not start selling my software earlier and only published free, open-source software. Yes, I have some sponsors in GitHub, but it has never been a substantial amount, ranging from $50-$750 per month, depending on how many sponsors I happen to have. Selling to business customers is definitely more reliable and predictable than depending on the goodwill of random people.</p>
    </section>


</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[React Labs: What We've Been Working On – February 2024 (102 pts)]]></title>
            <link>https://react.dev/blog/2024/02/15/react-labs-what-we-have-been-working-on-february-2024</link>
            <guid>39522031</guid>
            <pubDate>Tue, 27 Feb 2024 09:33:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://react.dev/blog/2024/02/15/react-labs-what-we-have-been-working-on-february-2024">https://react.dev/blog/2024/02/15/react-labs-what-we-have-been-working-on-february-2024</a>, See on <a href="https://news.ycombinator.com/item?id=39522031">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>February 15, 2024 by <a href="https://twitter.com/en_JS" target="_blank" rel="nofollow noopener noreferrer">Joseph Savona</a>, <a href="https://twitter.com/rickhanlonii" target="_blank" rel="nofollow noopener noreferrer">Ricky Hanlon</a>, <a href="https://twitter.com/acdlite" target="_blank" rel="nofollow noopener noreferrer">Andrew Clark</a>, <a href="https://twitter.com/mattcarrollcode" target="_blank" rel="nofollow noopener noreferrer">Matt Carroll</a>, and <a href="https://twitter.com/dan_abramov" target="_blank" rel="nofollow noopener noreferrer">Dan Abramov</a>.</p>
<hr>
<p>In React Labs posts, we write about projects in active research and development. We’ve made significant progress since our <a href="https://react.dev/blog/2023/03/22/react-labs-what-we-have-been-working-on-march-2023">last update</a>, and we’d like to share our progress.</p>
<div><h3><svg width="2em" height="2em" viewBox="0 0 72 72" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_40_48064)"><path d="M24 27C24 25.3431 25.3431 24 27 24H45C46.6569 24 48 25.3431 48 27C48 28.6569 46.6569 30 45 30H27C25.3431 30 24 28.6569 24 27Z" fill="currentColor"></path><path d="M24 39C24 37.3431 25.3431 36 27 36H39C40.6569 36 42 37.3431 42 39C42 40.6569 40.6569 42 39 42H27C25.3431 42 24 40.6569 24 39Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M12 18C12 13.0294 16.0294 9 21 9H51C55.9706 9 60 13.0294 60 18V54C60 58.9706 55.9706 63 51 63H21C16.0294 63 12 58.9706 12 54V18ZM21 15H51C52.6569 15 54 16.3431 54 18V54C54 55.6569 52.6569 57 51 57H21C19.3431 57 18 55.6569 18 54V18C18 16.3431 19.3431 15 21 15Z" fill="currentColor"></path></g><defs><clipPath id="clip0_40_48064"><rect width="72" height="72" fill="white"></rect></clipPath></defs></svg>Note</h3><div><p>React Conf 2024 is scheduled for May 15–16 in Henderson, Nevada! If you’re interested in attending React Conf in person, you can <a href="https://forms.reform.app/bLaLeE/react-conf-2024-ticket-lottery/1aRQLK" target="_blank" rel="nofollow noopener noreferrer">sign up for the ticket lottery</a> until February 28th.</p><p>For more info on tickets, free streaming, sponsoring, and more, see <a href="https://conf.react.dev/" target="_blank" rel="nofollow noopener noreferrer">the React Conf website</a>.</p></div></div>
<hr>
<h2 id="react-compiler">React Compiler <a href="#react-compiler" aria-label="Link for React Compiler " title="Link for React Compiler "><svg width="1em" height="1em" viewBox="0 0 13 13" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" fill-rule="evenodd"><path d="M7.778 7.975a2.5 2.5 0 0 0 .347-3.837L6.017 2.03a2.498 2.498 0 0 0-3.542-.007 2.5 2.5 0 0 0 .006 3.543l1.153 1.15c.07-.29.154-.563.25-.773.036-.077.084-.16.14-.25L3.18 4.85a1.496 1.496 0 0 1 .002-2.12 1.496 1.496 0 0 1 2.12 0l2.124 2.123a1.496 1.496 0 0 1-.333 2.37c.16.246.42.504.685.752z"></path><path d="M5.657 4.557a2.5 2.5 0 0 0-.347 3.837l2.108 2.108a2.498 2.498 0 0 0 3.542.007 2.5 2.5 0 0 0-.006-3.543L9.802 5.815c-.07.29-.154.565-.25.774-.036.076-.084.16-.14.25l.842.84c.585.587.59 1.532 0 2.122-.587.585-1.532.59-2.12 0L6.008 7.68a1.496 1.496 0 0 1 .332-2.372c-.16-.245-.42-.503-.685-.75z"></path></g></svg></a></h2>
<p>React Compiler is no longer a research project: the compiler now powers instagram.com in production, and we are working to ship the compiler across additional surfaces at Meta and to prepare the first open source release.</p>
<p>As discussed in our <a href="https://react.dev/blog/2023/03/22/react-labs-what-we-have-been-working-on-march-2023#react-optimizing-compiler">previous post</a>, React can <em>sometimes</em> re-render too much when state changes. Since the early days of React our solution for such cases has been manual memoization. In our current APIs, this means applying the <a href="https://react.dev/reference/react/useMemo"><code dir="ltr">useMemo</code></a>, <a href="https://react.dev/reference/react/useCallback"><code dir="ltr">useCallback</code></a>, and <a href="https://react.dev/reference/react/memo"><code dir="ltr">memo</code></a> APIs to manually tune how much React re-renders on state changes. But manual memoization is a compromise. It clutters up our code, is easy to get wrong, and requires extra work to keep up to date.</p>
<p>Manual memoization is a reasonable compromise, but we weren’t satisfied. Our vision is for React to <em>automatically</em> re-render just the right parts of the UI when state changes, <em>without compromising on React’s core mental model</em>. We believe that React’s approach — UI as a simple function of state, with standard JavaScript values and idioms — is a key part of why React has been approachable for so many developers. That’s why we’ve invested in building an optimizing compiler for React.</p>
<p>JavaScript is a notoriously challenging language to optimize, thanks to its loose rules and dynamic nature. React Compiler is able to compile code safely by modeling both the rules of JavaScript <em>and</em> the “rules of React”. For example, React components must be idempotent — returning the same value given the same inputs — and can’t mutate props or state values. These rules limit what developers can do and help to carve out a safe space for the compiler to optimize.</p>
<p>Of course, we understand that developers sometimes bend the rules a bit, and our goal is to make React Compiler work out of the box on as much code as possible. The compiler attempts to detect when code doesn’t strictly follow React’s rules and will either compile the code where safe or skip compilation if it isn’t safe. We’re testing against Meta’s large and varied codebase in order to help validate this approach.</p>
<p>For developers who are curious about making sure their code follows React’s rules, we recommend <a href="https://react.dev/reference/react/StrictMode">enabling Strict Mode</a> and <a href="https://react.dev/learn/editor-setup#linting">configuring React’s ESLint plugin</a>. These tools can help to catch subtle bugs in your React code, improving the quality of your applications today, and future-proofs your applications for upcoming features such as React Compiler. We are also working on consolidated documentation of the rules of React and updates to our ESLint plugin to help teams understand and apply these rules to create more robust apps.</p>
<p>To see the compiler in action, you can check out our <a href="https://www.youtube.com/watch?v=qOQClO3g8-Y" target="_blank" rel="nofollow noopener noreferrer">talk from last fall</a>. At the time of the talk, we had early experimental data from trying React Compiler on one page of instagram.com. Since then, we shipped the compiler to production across instagram.com. We’ve also expanded our team to accelerate the rollout to additional surfaces at Meta and to open source. We’re excited about the path ahead and will have more to share in the coming months.</p>
<h2 id="actions">Actions <a href="#actions" aria-label="Link for Actions " title="Link for Actions "><svg width="1em" height="1em" viewBox="0 0 13 13" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" fill-rule="evenodd"><path d="M7.778 7.975a2.5 2.5 0 0 0 .347-3.837L6.017 2.03a2.498 2.498 0 0 0-3.542-.007 2.5 2.5 0 0 0 .006 3.543l1.153 1.15c.07-.29.154-.563.25-.773.036-.077.084-.16.14-.25L3.18 4.85a1.496 1.496 0 0 1 .002-2.12 1.496 1.496 0 0 1 2.12 0l2.124 2.123a1.496 1.496 0 0 1-.333 2.37c.16.246.42.504.685.752z"></path><path d="M5.657 4.557a2.5 2.5 0 0 0-.347 3.837l2.108 2.108a2.498 2.498 0 0 0 3.542.007 2.5 2.5 0 0 0-.006-3.543L9.802 5.815c-.07.29-.154.565-.25.774-.036.076-.084.16-.14.25l.842.84c.585.587.59 1.532 0 2.122-.587.585-1.532.59-2.12 0L6.008 7.68a1.496 1.496 0 0 1 .332-2.372c-.16-.245-.42-.503-.685-.75z"></path></g></svg></a></h2>
<p>We <a href="https://react.dev/blog/2023/03/22/react-labs-what-we-have-been-working-on-march-2023#react-server-components">previously shared</a> that we were exploring solutions for sending data from the client to the server with Server Actions, so that you can execute database mutations and implement forms. During development of Server Actions, we extended these APIs to support data handling in client-only applications as well.</p>
<p>We refer to this broader collection of features as simply “Actions”. Actions allow you to pass a function to DOM elements such as <a href="https://react.dev/reference/react-dom/components/form"><code dir="ltr">&lt;form/&gt;</code></a>:</p>
<!--$--><div dir="ltr"><pre><code><p><span>&lt;</span><span>form</span> <span>action</span>=<span>{</span><span>search</span><span>}</span><span>&gt;</span><br></p><p><span>&lt;</span><span>input</span> <span>name</span>=<span>"query"</span> <span>/&gt;</span><br></p><p><span>&lt;</span><span>button</span> <span>type</span>=<span>"submit"</span><span>&gt;</span>Search<span>&lt;/</span><span>button</span><span>&gt;</span><br></p><p><span>&lt;/</span><span>form</span><span>&gt;</span></p></code></pre></div><!--/$-->
<p>The <code dir="ltr">action</code> function can operate synchronously or asynchronously. You can define them on the client side using standard JavaScript or on the server with the  <a href="https://react.dev/reference/react/use-server"><code dir="ltr">'use server'</code></a> directive. When using an action, React will manage the life cycle of the data submission for you, providing hooks like <a href="https://react.dev/reference/react-dom/hooks/useFormStatus"><code dir="ltr">useFormStatus</code></a>, and <a href="https://react.dev/reference/react-dom/hooks/useFormState"><code dir="ltr">useFormState</code></a> to access the current state and response of the form action.</p>
<p>By default, Actions are submitted within a <a href="https://react.dev/reference/react/useTransition">transition</a>, keeping the current page interactive while the action is processing. Since Actions support async functions, we’ve also added the ability to use <code dir="ltr">async/await</code> in transitions. This allows you to show pending UI with the <code dir="ltr">isPending</code> state of a transition when an async request like <code dir="ltr">fetch</code> starts, and show the pending UI all the way through the update being applied.</p>
<p>Alongside Actions, we’re introducing a feature named <a href="https://react.dev/reference/react/useOptimistic"><code dir="ltr">useOptimistic</code></a> for managing optimistic state updates. With this hook, you can apply temporary updates that are automatically reverted once the final state commits. For Actions, this allows you to optimistically set the final state of the data on the client, assuming the submission is successful, and revert to the value for data received from the server. It works using regular <code dir="ltr">async</code>/<code dir="ltr">await</code>, so it works the same whether you’re using <code dir="ltr">fetch</code> on the client, or a Server Action from the server.</p>
<p>Library authors can implement custom <code dir="ltr">action={fn}</code> props in their own components with <code dir="ltr">useTransition</code>. Our intent is for libraries to adopt the Actions pattern when designing their component APIs, to provide a consistent experience for React developers. For example, if your library provides a <code dir="ltr">&lt;Calendar onSelect={eventHandler}&gt;</code> component, consider also exposing a <code dir="ltr">&lt;Calendar selectAction={action}&gt;</code> API, too.</p>
<p>While we initially focused on Server Actions for client-server data transfer, our philosophy for React is to provide the same programming model across all platforms and environments. When possible, if we introduce a feature on the client, we aim to make it also work on the server, and vice versa. This philosophy allows us to create a single set of APIs that work no matter where your app runs, making it easier to upgrade to different environments later.</p>
<p>Actions are now available in the Canary channel and will ship in the next release of React.</p>
<h2 id="new-features-in-react-canary">New Features in React Canary <a href="#new-features-in-react-canary" aria-label="Link for New Features in React Canary " title="Link for New Features in React Canary "><svg width="1em" height="1em" viewBox="0 0 13 13" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" fill-rule="evenodd"><path d="M7.778 7.975a2.5 2.5 0 0 0 .347-3.837L6.017 2.03a2.498 2.498 0 0 0-3.542-.007 2.5 2.5 0 0 0 .006 3.543l1.153 1.15c.07-.29.154-.563.25-.773.036-.077.084-.16.14-.25L3.18 4.85a1.496 1.496 0 0 1 .002-2.12 1.496 1.496 0 0 1 2.12 0l2.124 2.123a1.496 1.496 0 0 1-.333 2.37c.16.246.42.504.685.752z"></path><path d="M5.657 4.557a2.5 2.5 0 0 0-.347 3.837l2.108 2.108a2.498 2.498 0 0 0 3.542.007 2.5 2.5 0 0 0-.006-3.543L9.802 5.815c-.07.29-.154.565-.25.774-.036.076-.084.16-.14.25l.842.84c.585.587.59 1.532 0 2.122-.587.585-1.532.59-2.12 0L6.008 7.68a1.496 1.496 0 0 1 .332-2.372c-.16-.245-.42-.503-.685-.75z"></path></g></svg></a></h2>
<p>We introduced <a href="https://react.dev/blog/2023/05/03/react-canaries">React Canaries</a> as an option to adopt individual new stable features as soon as their design is close to final, before they’re released in a stable semver version.</p>
<p>Canaries are a change to the way we develop React. Previously, features would be researched and built privately inside of Meta, so users would only see the final polished product when released to Stable. With Canaries, we’re building in public with the help of the community to finalize features we share in the React Labs blog series. This means you hear about new features sooner, as they’re being finalized instead of after they’re complete.</p>
<p>React Server Components, Asset Loading, Document Metadata, and Actions have all landed in the React Canary, and we’ve added docs for these features on react.dev:</p>
<ul>
<li>
<p><strong>Directives</strong>: <a href="https://react.dev/reference/react/use-client"><code dir="ltr">"use client"</code></a> and <a href="https://react.dev/reference/react/use-server"><code dir="ltr">"use server"</code></a> are bundler features designed for full-stack React frameworks. They mark the “split points” between the two environments: <code dir="ltr">"use client"</code> instructs the bundler to generate a <code dir="ltr">&lt;script&gt;</code> tag (like <a href="https://docs.astro.build/en/concepts/islands/#creating-an-island" target="_blank" rel="nofollow noopener noreferrer">Astro Islands</a>), while <code dir="ltr">"use server"</code> tells the bundler to generate a POST endpoint (like <a href="https://trpc.io/docs/concepts" target="_blank" rel="nofollow noopener noreferrer">tRPC Mutations</a>). Together, they let you write reusable components that compose client-side interactivity with the related server-side logic.</p>
</li>
<li>
<p><strong>Document Metadata</strong>: we added built-in support for rendering <a href="https://react.dev/reference/react-dom/components/title"><code dir="ltr">&lt;title&gt;</code></a>, <a href="https://react.dev/reference/react-dom/components/meta"><code dir="ltr">&lt;meta&gt;</code></a>, and metadata <a href="https://react.dev/reference/react-dom/components/link"><code dir="ltr">&lt;link&gt;</code></a> tags anywhere in your component tree. These work the same way in all environments, including fully client-side code, SSR, and RSC. This provides built-in support for features pioneered by libraries like <a href="https://github.com/nfl/react-helmet" target="_blank" rel="nofollow noopener noreferrer">React Helmet</a>.</p>
</li>
<li>
<p><strong>Asset Loading</strong>: we integrated Suspense with the loading lifecycle of resources such as stylesheets, fonts, and scripts so that React takes them into account to determine whether the content in elements like <a href="https://react.dev/reference/react-dom/components/style"><code dir="ltr">&lt;style&gt;</code></a>, <a href="https://react.dev/reference/react-dom/components/link"><code dir="ltr">&lt;link&gt;</code></a>, and <a href="https://react.dev/reference/react-dom/components/script"><code dir="ltr">&lt;script&gt;</code></a> are ready to be displayed. We’ve also added new <a href="https://react.dev/reference/react-dom#resource-preloading-apis">Resource Loading APIs</a> like <code dir="ltr">preload</code> and <code dir="ltr">preinit</code> to provide greater control for when a resource should load and initialize.</p>
</li>
<li>
<p><strong>Actions</strong>: As shared above, we’ve added Actions to manage sending data from the client to the server. You can add <code dir="ltr">action</code> to elements like <a href="https://react.dev/reference/react-dom/components/form"><code dir="ltr">&lt;form/&gt;</code></a>, access the status with <a href="https://react.dev/reference/react-dom/hooks/useFormStatus"><code dir="ltr">useFormStatus</code></a>, handle the result with <a href="https://react.dev/reference/react-dom/hooks/useFormState"><code dir="ltr">useFormState</code></a>, and optimistically update the UI with <a href="https://react.dev/reference/react/useOptimistic"><code dir="ltr">useOptimistic</code></a>.</p>
</li>
</ul>
<p>Since all of these features work together, it’s difficult to release them in the Stable channel individually. Releasing Actions without the complementary hooks for accessing form states would limit the practical usability of Actions. Introducing React Server Components without integrating Server Actions would complicate modifying data on the server.</p>
<p>Before we can release a set of features to the Stable channel, we need to ensure they work cohesively and developers have everything they need to use them in production. React Canaries allow us to develop these features individually, and release the stable APIs incrementally until the entire feature set is complete.</p>
<p>The current set of features in React Canary are complete and ready to release.</p>
<h2 id="the-next-major-version-of-react">The Next Major Version of React <a href="#the-next-major-version-of-react" aria-label="Link for The Next Major Version of React " title="Link for The Next Major Version of React "><svg width="1em" height="1em" viewBox="0 0 13 13" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" fill-rule="evenodd"><path d="M7.778 7.975a2.5 2.5 0 0 0 .347-3.837L6.017 2.03a2.498 2.498 0 0 0-3.542-.007 2.5 2.5 0 0 0 .006 3.543l1.153 1.15c.07-.29.154-.563.25-.773.036-.077.084-.16.14-.25L3.18 4.85a1.496 1.496 0 0 1 .002-2.12 1.496 1.496 0 0 1 2.12 0l2.124 2.123a1.496 1.496 0 0 1-.333 2.37c.16.246.42.504.685.752z"></path><path d="M5.657 4.557a2.5 2.5 0 0 0-.347 3.837l2.108 2.108a2.498 2.498 0 0 0 3.542.007 2.5 2.5 0 0 0-.006-3.543L9.802 5.815c-.07.29-.154.565-.25.774-.036.076-.084.16-.14.25l.842.84c.585.587.59 1.532 0 2.122-.587.585-1.532.59-2.12 0L6.008 7.68a1.496 1.496 0 0 1 .332-2.372c-.16-.245-.42-.503-.685-.75z"></path></g></svg></a></h2>
<p>After a couple of years of iteration, <code dir="ltr">react@canary</code> is now ready to ship to <code dir="ltr">react@latest</code>. The new features mentioned above are compatible with any environment your app runs in, providing everything needed for production use. Since Asset Loading and Document Metadata may be a breaking change for some apps, the next version of React will be a major version: <strong>React 19</strong>.</p>
<p>There’s still more to be done to prepare for release. In React 19, we’re also adding long-requested improvements which require breaking changes like support for Web Components. Our focus now is to land these changes, prepare for release, finalize docs for new features, and publish announcements for what’s included.</p>
<p>We’ll share more information about everything React 19 includes, how to adopt the new client features, and how to build support for React Server Components in the coming months.</p>
<h2 id="offscreen-renamed-to-activity">Offscreen (renamed to Activity). <a href="#offscreen-renamed-to-activity" aria-label="Link for Offscreen (renamed to Activity). " title="Link for Offscreen (renamed to Activity). "><svg width="1em" height="1em" viewBox="0 0 13 13" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" fill-rule="evenodd"><path d="M7.778 7.975a2.5 2.5 0 0 0 .347-3.837L6.017 2.03a2.498 2.498 0 0 0-3.542-.007 2.5 2.5 0 0 0 .006 3.543l1.153 1.15c.07-.29.154-.563.25-.773.036-.077.084-.16.14-.25L3.18 4.85a1.496 1.496 0 0 1 .002-2.12 1.496 1.496 0 0 1 2.12 0l2.124 2.123a1.496 1.496 0 0 1-.333 2.37c.16.246.42.504.685.752z"></path><path d="M5.657 4.557a2.5 2.5 0 0 0-.347 3.837l2.108 2.108a2.498 2.498 0 0 0 3.542.007 2.5 2.5 0 0 0-.006-3.543L9.802 5.815c-.07.29-.154.565-.25.774-.036.076-.084.16-.14.25l.842.84c.585.587.59 1.532 0 2.122-.587.585-1.532.59-2.12 0L6.008 7.68a1.496 1.496 0 0 1 .332-2.372c-.16-.245-.42-.503-.685-.75z"></path></g></svg></a></h2>
<p>Since our last update, we’ve renamed a capability we’re researching from “Offscreen” to “Activity”. The name “Offscreen” implied that it only applied to parts of the app that were not visible, but while researching the feature we realized that it’s possible for parts of the app to be visible and inactive, such as content behind a modal. The new name more closely reflects the behavior of marking certain parts of the app “active” or “inactive”.</p>
<p>Activity is still under research and our remaining work is to finalize the primitives that are exposed to library developers. We’ve deprioritized this area while we focus on shipping features that are more complete.</p>
<hr>
<p>In addition to this update, our team has presented at conferences and made appearances on podcasts to speak more on our work and answer questions.</p>
<ul>
<li>
<p><a href="https://react.dev/community/team#sathya-gunasekaran">Sathya Gunasekaran</a> spoke about the React Compiler at the <a href="https://www.youtube.com/watch?v=kjOacmVsLSE" target="_blank" rel="nofollow noopener noreferrer">React India</a> conference</p>
</li>
<li>
<p><a href="https://react.dev/community/team#dan-abramov">Dan Abramov</a> gave a talk at <a href="https://www.youtube.com/watch?v=zMf_xeGPn6s" target="_blank" rel="nofollow noopener noreferrer">RemixConf</a> titled “React from Another Dimension” which explores an alternative history of how React Server Components and Actions could have been created</p>
</li>
<li>
<p><a href="https://react.dev/community/team#dan-abramov">Dan Abramov</a> was interviewed on <a href="https://changelog.com/jsparty/311" target="_blank" rel="nofollow noopener noreferrer">the Changelog’s JS Party podcast</a> about React Server Components</p>
</li>
<li>
<p><a href="https://react.dev/community/team#matt-carroll">Matt Carroll</a> was interviewed on the <a href="https://www.buzzsprout.com/2226499/14462424-interview-the-two-reacts-with-rachel-nabors-evan-bacon-and-matt-carroll" target="_blank" rel="nofollow noopener noreferrer">Front-End Fire podcast</a> where he discussed <a href="https://overreacted.io/the-two-reacts/" target="_blank" rel="nofollow noopener noreferrer">The Two Reacts</a></p>
</li>
</ul>
<p>Thanks <a href="https://twitter.com/potetotes" target="_blank" rel="nofollow noopener noreferrer">Lauren Tan</a>, <a href="https://twitter.com/sophiebits" target="_blank" rel="nofollow noopener noreferrer">Sophie Alpert</a>, <a href="https://threads.net/someextent" target="_blank" rel="nofollow noopener noreferrer">Jason Bonta</a>, <a href="https://twitter.com/Eli_White" target="_blank" rel="nofollow noopener noreferrer">Eli White</a>, and <a href="https://twitter.com/_gsathya" target="_blank" rel="nofollow noopener noreferrer">Sathya Gunasekaran</a> for reviewing this post.</p>
<p>Thanks for reading, and <a href="https://conf.react.dev/" target="_blank" rel="nofollow noopener noreferrer">see you at React Conf</a>!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Berlin TYPE: The official type for the city of Berlin (2020) (149 pts)]]></title>
            <link>https://www.hvdfonts.com/custom-cases/berlin-type</link>
            <guid>39521650</guid>
            <pubDate>Tue, 27 Feb 2024 08:42:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hvdfonts.com/custom-cases/berlin-type">https://www.hvdfonts.com/custom-cases/berlin-type</a>, See on <a href="https://news.ycombinator.com/item?id=39521650">Hacker News</a></p>
Couldn't get https://www.hvdfonts.com/custom-cases/berlin-type: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Submarine cables linking Africa-Asia-Europe severed (109 pts)]]></title>
            <link>https://en.globes.co.il/en/article-houthis-hit-underwater-communications-cables-1001472165</link>
            <guid>39521626</guid>
            <pubDate>Tue, 27 Feb 2024 08:39:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.globes.co.il/en/article-houthis-hit-underwater-communications-cables-1001472165">https://en.globes.co.il/en/article-houthis-hit-underwater-communications-cables-1001472165</a>, See on <a href="https://news.ycombinator.com/item?id=39521626">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="F_Content">
        <h2 id="coteret_SubCoteret">"Globes" has learned that four submarine communications cables have been damaged in the Red Sea between Jeddah in Saudi Arabia and Djibouti in East Africa.</h2>


        <article>
            <p>Three months after the Houthis began attacking merchant ships, the Yemenite rebels have carried out another one of their threats. "Globes" has learned that four submarine communication cables have been damaged in the Red Sea between Jeddah in Saudi Arabia and Djibouti in East Africa.</p>
<p>According to the reports, these are cables from the companies AAE-1, Seacom, EIG and TGN. This is causing serious disruption of Internet communications between Europe and Asia, with the main damage being felt in the Gulf countries and India.</p>



<section id="moreArticles"> 
    <h2>RELATED ARTICLES</h2>
    
    
    

    <p>
        <a href="https://en.globes.co.il/en/article-egypt-asks-houthis-to-attack-only-israeli-ships-report-1001468870">
            <h3>Egypt asks Houthis to attack only Israeli ships - report</h3>
        </a>
    </p>

</section>



<p>Estimates are that the damage to communications activities is significant but not critical because other cables pass through the same region linking Asia, Africa and Europe that have not been hit. The repair of such a large number of underwater cables may take at least eight weeks according to estimates and involve exposure to risk from the Houthi terror organization. The telecommunications companies will be forced to look for companies that will agree to carry out the repair work and probably pay them a high risk premium.</p>
<p>EIG (European India Gateway) connects Southern Europe with Egypt, Saudi Arabia, Djibouti, the UAE and India. The underwater cable was laid by Tyco arm Alcatel-Lucent at a cost of $700 million and was the first cable stretching from the UK to India. Shares in EIG are held by a consortium including AT&amp;T, Saudi Telecom, Verizon, and India's Bharat Sanchar.</p>
<p>TGN Atlantic was laid by Tyco International in 2001 and sold to Indian company Tata Communications in 2005 for $130 million. The AAE-1 cable which has also been cut links East Asia to Europe via Egypt. The cable, which has a 40 terabyte per second capacity, links China with the west via countries belonging to the Chinese-Iranian axis including those countries and Pakistan and Qatar.</p>
<p>The Seacom cable links Europe, Africa and India as well as South Africa.</p>
<p>Senior executives at international communications and underwater cable companies have posted reports about the damage on LinkedIn and X.</p>
<p><em>Published by Globes, Israel business news - <a href="https://en.globes.co.il/">en.globes.co.il</a> - on February 26, 2024.</em></p>
<p><em>© Copyright of Globes Publisher Itonut (1983) Ltd., 2024.</em></p>
			<!-- f67 -->
        </article>
        
        
            

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open Letter to Tim Cook, Sabotaging Web Apps Is Indefensible (118 pts)]]></title>
            <link>https://letter.open-web-advocacy.org/</link>
            <guid>39521200</guid>
            <pubDate>Tue, 27 Feb 2024 07:35:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://letter.open-web-advocacy.org/">https://letter.open-web-advocacy.org/</a>, See on <a href="https://news.ycombinator.com/item?id=39521200">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p><em>Dear Tim Cook,</em></p>
          <p>We write to express our concern at Apple’s decision to remove Web Apps (PWAs) from iOS and Safari in the European Union (EU), and to avail ourselves of our rights under the Digital Markets Act (DMA).</p>
          <p>Apple points to Web Apps as the open alternative to the App Store, and actions to remove them have created deep concern in the web community. iOS demoting Web Apps to shortcuts threaten data loss and undermine the web as a reliable platform for iOS users. These silently-introduced changes threaten critical features including integration with iOS, push notifications, unread count badging, and the ability to run full screen. Their removal will break Web Apps for students, governments, health care institutions, journalists, and startups.</p>
          <p>Entire categories of apps will no longer be viable on the web as a result. More troubling, we understand iOS will not include APIs for competing browsers to implement these features either. This will do vast, immediate, and ongoing harm to users, developers, and businesses, both inside and outside the EU.</p>
          <p>Apple’s justifications gesture toward security and privacy, but are at best unfounded. Web Apps provide safe computing that puts users in control through their browsers, and iOS opening up to competing browser engines will enhance, rather than erode, security and privacy. Web Apps powered by competing browsers can be safer and more capable than today’s apps, and removing support cannot be justified on security grounds. Apple’s arguments regarding the safety of competing browsers have been conclusively rejected by regulators worldwide, and this situation is no different.</p>
          <p>We, the undersigned “end users” and “business users”, avail ourselves of our rights under Articles 5 and 6 of the EU’s DMA. In particular, we assert our right under Article 6(7) ensuring businesses effective interoperability with the software features of the operating system. </p>
          <p>Pursuant to these rights, Apple is obligated to preserve the functionality to allow Safari and other iOS browsers to add Web Apps to the home screen, allow them to run in top-level activities (not in tabs), integrate with iOS settings and permissions, enable Push Notifications and homescreen icon badging, and to run fullscreen.</p>
          <p>Further, we assert that Apple’s proposed changes violate Article 13 of the DMA which prohibits anti-circumvention efforts by designated gatekeepers. Specifically, Article 13(6) which states:</p>

          <blockquote>
          <p>6. The gatekeeper <strong>shall not degrade the conditions or quality of any of the core platform services</strong> provided to <strong>business users</strong> or <strong>end users</strong> who <strong>avail themselves of the rights</strong> or choices laid down in Articles 5, 6 and 7, or ...</p>
          <cite><a rel="noreferrer" target="_blank" href="https://eur-lex.europa.eu/legal-content/EN/TXT/?toc=OJ%3AL%3A2022%3A265%3ATOC&amp;uri=uriserv%3AOJ.L_.2022.265.01.0001.01.ENG#:~:text=6.%C2%A0%C2%A0%C2%A0The%20gatekeeper%20shall%20not%20degrade,user%20interface%20or%20a%20part%20thereof.">EU Digital Markets Act, Article 13(6)</a> (emphasis added)</cite>
          </blockquote>

          <p>It is still possible for Apple to reverse course and preserve essential functionality iOS users and developers have relied on since 2007 when Steve Jobs introduced Web Apps for the iPhone. Degrading these features in iOS and Safari is not required by the DMA. We encourage Apple to engage with all stakeholders urgently, transparently, and in good faith to restore and enhance these essential capabilities.
          </p>
          
          <p>Preserving these features, making them available to competitors, and allowing browser choice worldwide is the only good-faith path forward, and we call on you to both comply with Apple’s legal obligations and to allow fair and effective competition on your platforms globally. Apple has the ability to compete on merit, rather than relying on lock-in and self-preferencing.</p>

          <p>Sincerely</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Netlify just sent me a $104K bill for a simple static site (1103 pts)]]></title>
            <link>https://old.reddit.com/r/webdev/comments/1b14bty/netlify_just_sent_me_a_104k_bill_for_a_simple/</link>
            <guid>39520776</guid>
            <pubDate>Tue, 27 Feb 2024 06:29:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/webdev/comments/1b14bty/netlify_just_sent_me_a_104k_bill_for_a_simple/">https://old.reddit.com/r/webdev/comments/1b14bty/netlify_just_sent_me_a_104k_bill_for_a_simple/</a>, See on <a href="https://news.ycombinator.com/item?id=39520776">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>So I received an email from Netlify last weekend saying that I have a $104,500.00 bill overdue. At first I thought this is a joke or some scam email but after checking my dashboard it seems like I am truly owing them 104K dollars:</p>

<p><a href="https://preview.redd.it/do33l577a2lc1.png?width=1091&amp;format=png&amp;auto=webp&amp;s=6718c5f3358e3df4c6e502fc7c7a558eec47a2cf">That's 190TB bandwidth in 4 days</a></p>

<p>So I was like 😅😅😅 and think okay maybe I got ddos attacked. Since Netlify charges 55$/100GB for the exceeding bandwith, the peak day Feb 16 has 33385/55 * 100GB = 60.7TB bandwidth in a day. I mean, it's not impossible but why attack a simple static site like mine? This site has been on Netlify for 4 years and is okay with the free tier. The monthly bandwith never exceeded even 10GB, and has only ~200 daily visitors.</p>

<p>I contacted their billing support and they responded me that they looked into it and the bandwidth came from some user agents, meaning it is a ddos attack. Then they say such cases happen and they usually charge their customer 20% on this. And since my amount is too large, they offer to discount to 5%, which means I still need to pay 5 thousand dollars.</p>

<p>This feels more like a scam to me. Why do serverless platforms like Netlify and Vercel not have ddos protection, or at least a spend limit? They should have alerted me if the spending skyrocketed. I checked my inbox and spam folder and found nothing. The only email is "Extra usage package purchased for bandwidth". It feels like they deliberately not support these features so that they can cash grab in situations like this.</p>

<p>The ddos attack was focused on a file on my site. Yes it's partly my fault to put a 3.44MB size sound file on my site rather than using a third-party platform like SoundCloud. But still this doesn't invalidate the point of having protection against such attacks, and limit the spending.</p>

<p>And yes I have migrated my site to Cloudflare. Learned my lesson and will never use Netlify (or even Vercel) again.</p>

<p>​</p>

<p>UPDATE: Thank you all for the suggestions I have posted this on <a href="https://news.ycombinator.com/item?id=39520776">HackerNews</a>.</p>

<p>​</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rapier is a set of 2D and 3D physics engines written in Rust (257 pts)]]></title>
            <link>https://rapier.rs/docs/</link>
            <guid>39519894</guid>
            <pubDate>Tue, 27 Feb 2024 03:39:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rapier.rs/docs/">https://rapier.rs/docs/</a>, See on <a href="https://news.ycombinator.com/item?id=39519894">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><header></header><div><p>Rapier is a set of 2D and 3D physics engines written using the <a href="https://www.rust-lang.org/" target="_blank" rel="noopener noreferrer">Rust programming language</a>.
It targets applications requiring real-time physics like video games, animation, and robotics. It is designed to be fast,
stable, and optionally cross-platform deterministic. Rapier features include:</p><ul><li>Rigid-body collisions and forces.</li><li>Joint constraints.</li><li>Contact events and sensors.</li><li>Snapshotting.</li><li>Optional cross-platform determinism.</li><li>JavaScript bindings.</li><li>And more…</li></ul><p>Rapier is free and open-source, released under the Apache 2.0 license. It is developed by the <a href="https://dimforge.com/" target="_blank" rel="noopener noreferrer">Dimforge</a>
open-source company. You can support us by sponsoring us on <a href="https://github.com/sponsors/dimforge" target="_blank" rel="noopener noreferrer">GitHub sponsor</a>.</p><p><img src="https://www.dimforge.com/img/logo/logo_dimforge_full" alt="dimforge_logo"></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: People who switched from GPT to their own models. How was it? (137 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39519692</link>
            <guid>39519692</guid>
            <pubDate>Tue, 27 Feb 2024 03:06:56 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39519692">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="39520739"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520739" href="https://news.ycombinator.com/vote?id=39520739&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>A couple of months ago I attended a presentation of an on-prem LLM. An audience member asked, if it was using OpenAI in any way.<p>The presenter, somewhat overeagerly, "Why not ask our new AI?" and went on to type: "Are you an independent  model or do you use OpenAI?"</p><p>To chat bot answered in flourish language that sure it was using ChatGPT as a backend. Which it was <i>not</i> and which was kind of the whole point of the presentation.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39521180"><td></td></tr>
                  <tr id="39520705"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520705" href="https://news.ycombinator.com/vote?id=39520705&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>Obviously talking my own book here, but we've helped dozens of customers make the transition from prompted GPT-4 or GPT-3.5 to their own fine-tuned models at OpenPipe.<p>The most common reaction I get is "wow, I didn't expect that to work so well with so little effort". For most tasks, a fine-tuned Mistral 7B will consistently outperform GPT-3.5 at a fraction of the cost, and for some use cases will even match or outperform GPT-4 (particularly for narrower tasks like classification, information extraction, summarization -- but a lot of folks have that kind of task). Some aggregate stats are in our blog: <a href="https://openpipe.ai/blog/mistral-7b-fine-tune-optimized">https://openpipe.ai/blog/mistral-7b-fine-tune-optimized</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39521055"><td></td></tr>
            <tr id="39520787"><td></td></tr>
                <tr id="39520819"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39520819" href="https://news.ycombinator.com/vote?id=39520819&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>I don't think they've released a fine-tuning API, but we'll definitely support it once they do!</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="39520078"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520078" href="https://news.ycombinator.com/vote?id=39520078&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>I fine-tuned an LLM to do technical stuff. It works pretty darn good. What I actually discovered is that when evaluating LLMs, it is surprisingly difficult to evaluate them. And, also, that GPT 4 isn't that great, in general.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39520227"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520227" href="https://news.ycombinator.com/vote?id=39520227&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>Could you provide more details on this matter? Specifically, I'm interested in knowing which base model you've utilized and the approach you've taken to fine-tune it. Your insights would be greatly appreciated and highly beneficial.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39520289"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39520289" href="https://news.ycombinator.com/vote?id=39520289&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>For narrow stuff you can do better job than base gpt4/mistral/etc model. You fine tune it with your very custom data, stuff that got didn’t seem to be trained on, it will generalize it well.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39520315"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39520315" href="https://news.ycombinator.com/vote?id=39520315&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>Have you done this? How did you do it?<p>I've been looking forward to someone providing a detailed guide on how to "fine tune it with your custom data" for ages!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39520567"><td></td></tr>
            <tr id="39520544"><td></td></tr>
            <tr id="39520486"><td></td></tr>
                <tr id="39521116"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_39521116" href="https://news.ycombinator.com/vote?id=39521116&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>You're not wrong. There's been a lot of drama over licensing and releasing datasets, and a lot of the LLM scene are just pitchmen and promoters with no better grasp over what they're doing than "trust me, it's better".<p>Like with "prompt engineering", a lot of people are just hiding how much of the heavy lifting is from base models and a fluke of the merge. The past few "secret" set leaks were low/no delta diffs to common releases.</p><p>I said it a year ago, but if we want to wowed, make this a job for MLIS holders and references librarians. Without thorough, thoughtful curation, these things are just toys in the wrong hands.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                                    <tr id="39520737"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520737" href="https://news.ycombinator.com/vote?id=39520737&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>Maybe the key to a good universal LLM is having multiple fine tuned models for various domains. The user thinks he's querying a single model but really there's some mechanism that selecting the best model for his query out of say like 300 different possibilities.<p>This also helps distributes traffic as a side effect.</p><p>I guess the problem is how the conversation would flow. If the user changes topics from say art to quantum physics then asks a question about quantum physics and art then I'm not sure what the algorithm should do.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39520877"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39520877" href="https://news.ycombinator.com/vote?id=39520877&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>That is actually the same idea as the (now) popular "Mixture of Experts" approach.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39520836"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39520836" href="https://news.ycombinator.com/vote?id=39520836&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>The user could talk to an "expert opinion aggregator" model which in turn makes a bunch of queries to specialized models.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39520578"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520578" href="https://news.ycombinator.com/vote?id=39520578&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>&gt; GPT 4 isn't that great, in general<p>same here, it doesn't adhere to explicit instructions, maybe one or two simple instructions are ok but not more complex ones
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39521179"><td></td></tr>
            <tr id="39520783"><td></td></tr>
                  <tr id="39520085"><td></td></tr>
            <tr id="39520096"><td></td></tr>
                <tr id="39520299"><td></td></tr>
                        <tr id="39520095"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520095" href="https://news.ycombinator.com/vote?id=39520095&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>Running Mistral-Instruct-0.1 for call/email summarization, Mixtral for contract mining &amp; OpenChat to augment agentic chatbot equipped with RAG tools(Instruct again).<p>Experience has been great, INT8 tradeoffs are acceptable until hardware FP8(FP4 anyone?) becomes more widely &amp; cheaply available. On-prem costs have been absorbed already for few boxes of A100s &amp; legacy V100s running millions of such interactions.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39520327"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520327" href="https://news.ycombinator.com/vote?id=39520327&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>What are the trade-offs with INT8? I thought even the INT4 loss of accuracy was small and the INT8 loss almost unmeasurable.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39520298"><td></td></tr>
                <tr id="39521331"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39521331" href="https://news.ycombinator.com/vote?id=39521331&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>Basically augmenting users parsing PDFs &amp; looking to prefill values into Excel instead of typing it all out. Ex. Liabilities, time-period/frequencies mentioned, owners of clauses etc.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="39520303"><td></td></tr>
                <tr id="39520528"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520528" href="https://news.ycombinator.com/vote?id=39520528&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>I've tried code-llama with Ollama, along with Continue.dev and found it to be pretty good. The only downside is that I couldn't "productively" run the 70B version, even on my MBP with M3 Max with 36GB of RAM (which interestingly should be enough to hold quantized model weights). It was simply painfully slow. 34B one works good enough for most of my use-cases, so I am happy.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39520887"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39520887" href="https://news.ycombinator.com/vote?id=39520887&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>I tried to use codellama 34B and I think it is pretty bad. For Example I asked it to convert a comment into a docstring and it would hallucinate a whole function around it.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39520554"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520554" href="https://news.ycombinator.com/vote?id=39520554&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>deepseek-coder 6.7b is seriously impressive for how quickly it runs on an M1 Max. There’s a few spots where it still doesn’t fare quite as well as ChatGPT but it’s a small tradeoff considering that it’s fully local and doesn’t even spin up my laptop’s fans.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39520687"><td></td></tr>
                  <tr id="39520729"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520729" href="https://news.ycombinator.com/vote?id=39520729&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>I prefer to use local models when running data extraction or processing over 10k or more records. Hosted services would be slow and brittle at this point.<p>Mistral 7B fine-tunes (OpenChat is my favorite) just chug through the data and get the job done.</p><p>Details: using vLLM to run the models. Using ChatGPT-4 to condense information for complex prompts (that the local models will execute).</p><p>I think, the situation will just keep on getting better with each month.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39521123"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39521123" href="https://news.ycombinator.com/vote?id=39521123&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>Im using mixtral 8x7b (q5) for my use cases, such as scripting, searching for ideas and or definitions that i allways need to factcheck.<p>Currently i use lmstudio on my m2 with 96gb ram. But i‘m looking into switchin to ollama or another oss solution.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39520895"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520895" href="https://news.ycombinator.com/vote?id=39520895&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>Mixed results. I think llama2 in general is pretty bad, especially at anything else than english. I've had very good results with Mixtral for Chat.<p>Of course all of them feel like a Frankenstein compared to actual ChatGPT. They feel similar and work just as well until, sometimes, they put out complete and utter garbage or artifacts and you wonder if they skimped on fine-tuning.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39520654"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520654" href="https://news.ycombinator.com/vote?id=39520654&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>I tested a bunch of models while building <a href="https://double.bot/">https://double.bot</a> but ended up back on gpt4. Other models are fun to play with but it gets frustrating even if they miss 1/100 questions that gpt4 gets. I find that right now I get more value implementing features around the model that fixes all the GitHub copilot papercuts (autocomplete that closes brackets properly, auto import upon accepting suggestions, disable suggestions when writing comments to be less distracting, midline completions, etc etc)<p>Hopefully os models can catch-up to gpt4 in the next six months when we fixed all the low hanging fruit outside of the model itself
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39520461"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520461" href="https://news.ycombinator.com/vote?id=39520461&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>We support both in our app and enterprise product. The APIs (OpenAI) vs libraries (i.e. llama.cpp for on-device) are so similar that the switch is basically transparent to the user. We're adding support for other platforms APIs soon, and everything we've looked so far is as easy to integrate as OpenAI - except Google that for some reason complicates everything on Google Cloud.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39520592"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520592" href="https://news.ycombinator.com/vote?id=39520592&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>To add to this question, are there LLMs that I can run on my own data, that also can provide citations similar to the way phind.com does for their results? Even better if they are multilingual.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39520665"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520665" href="https://news.ycombinator.com/vote?id=39520665&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>It’s been ok. I’m running llama2 7b and it’s … fine. The results I get from gpt4 aren’t much better. This for general tasks.<p>Mostly I think I need to use LLMs more effectively
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39520297"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520297" href="https://news.ycombinator.com/vote?id=39520297&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>My 2024 prediction is we will see far more people moving off of openai once they encounter its cost and latency compared to (less proven/scaled) competitors. It’s often a speed versus quality tradeoff, and I’ve seen multiple providers 3x faster than OpenAI with far more than 1/3 the quality</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39520598"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520598" href="https://news.ycombinator.com/vote?id=39520598&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>OpenAI currently leads the front on (commercial) AI, so I doubt people will switch. In fact most offerings become outdated pretty fast and everyone else tries to play catch up.<p>Imagine using a GPT-2 type model when everyone else is using GPT-4. Until the dust settles there's no point in investing in alt models imo, unless you're leading the research.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39520352"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520352" href="https://news.ycombinator.com/vote?id=39520352&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>I greatly prefer to use ChatGPT-4 instead of 3.5 despite the slowness. Really a good feature for them to have would be to easily re-run a prompt on 4. However, the glitchiness of the service is kind of annoying.</span></p></div></td></tr>
        </tbody></table></td></tr>
                            <tr id="39520466"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520466" href="https://news.ycombinator.com/vote?id=39520466&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>We have a first pass with our own model and then escalate to gpt if we aren't sure of our own model's results.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39520502"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520502" href="https://news.ycombinator.com/vote?id=39520502&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>Would be great if people could share their app demo, host &amp; model used/trained for better context.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39520497"><td></td></tr>
            <tr id="39520208"><td></td></tr>
                <tr id="39520457"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520457" href="https://news.ycombinator.com/vote?id=39520457&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>I think the llm utility[0] (the one from Simon, not Google) is probably the best quickstart experience you can find. Gives the option to connect to services via API or install/run local models.<p>As simple as</p><pre><code>  pip install llm
  # add the local plugin
  llm install llm-gpt4all
  # Download and run a prompt against the Orca Mini 7B model
  llm -m orca-mini-3b-gguf2-q4_0 'What is the capital of France?'
</code></pre>
Alternatively, you could use the llamafile[1] which is a tiny binary runner which gets packaged ontop of the multigigabyte models. Download the llamafile and you can launch it through your terminal or a web browser.<p>From the llamafile page, after you download the file, you can just launch it as</p><pre><code>  ./mistral-7b-instruct-v0.2.Q5_K_M.llamafile -ngl 9999 --temp 0.7 -p '[INST]Write a story about llamas[/INST]'
</code></pre>
[0] <a href="https://llm.datasette.io/en/stable/index.html" rel="nofollow">https://llm.datasette.io/en/stable/index.html</a><p>[1] <a href="https://github.com/Mozilla-Ocho/llamafile">https://github.com/Mozilla-Ocho/llamafile</a></p><p>Edit: added llm quickstart from the intro page
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39520551"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520551" href="https://news.ycombinator.com/vote?id=39520551&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>If you don't care about the details of how those model servers work, then something that abstracts out the whole process like LM Studio or Ollama is all you need.<p>However, if you want to get into the weeds of how this actually works, I recommend you look up model quantization and some libraries like ggml[1] that actually do that for you.</p><p>[1] <a href="https://github.com/ggerganov/ggml">https://github.com/ggerganov/ggml</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39520243"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520243" href="https://news.ycombinator.com/vote?id=39520243&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>You can try going get some pre-trained (sometimes, fine-tuned) models on HuggingFace, following their instructions. Good luck!</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39520444"><td></td></tr>
                <tr id="39520517"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39520517" href="https://news.ycombinator.com/vote?id=39520517&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>it's all pretty well put together nowadays honestly.<p>here's a dead simple way : (1) download LM Studio, install it[0] (2) download a model from within the client when prompted (3) have a ball.</p><p>the program is fairly intuitive, it takes care of finding the relevant files, and it can even accept addendum prompts and various ways to flavor or specialize answers.</p><p>Learn the basics there, take what you learn to a more 'industrial' playground later on.</p><p>[0]: <a href="https://lmstudio.ai/" rel="nofollow">https://lmstudio.ai/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39520280"><td></td></tr>
            <tr id="39520294"><td></td></tr>
            <tr id="39520241"><td></td></tr>
            <tr id="39520686"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520686" href="https://news.ycombinator.com/vote?id=39520686&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>The other answers are recommending paths which give you #1. less control and #2. projects with smaller eco-systems.<p>If you want a truly general purpose front-end for LLMs, the only good solution right now is oobabooga: <a href="https://github.com/oobabooga/text-generation-webui">https://github.com/oobabooga/text-generation-webui</a></p><p>All other alternatives have only small fractions of the features that oobabooga supports. All other alternatives only support a fraction of the LLM backends that oobabooga supports, etc.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39520069"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is Tableau Dead? (123 pts)]]></title>
            <link>https://www.mergeyourdata.com/blog/is-tableau-dead-the-future-of-tableau</link>
            <guid>39519145</guid>
            <pubDate>Tue, 27 Feb 2024 01:54:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mergeyourdata.com/blog/is-tableau-dead-the-future-of-tableau">https://www.mergeyourdata.com/blog/is-tableau-dead-the-future-of-tableau</a>, See on <a href="https://news.ycombinator.com/item?id=39519145">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Is Tableau Dead?</h2><p>No... and Yes.</p><p>The momentum and community around Tableau has been lost. But current and projected future revenue growth is solid. It's not the company that it used to be, but it'll be a major player as long as it's tied to Salesforce.</p><p>Similar to how no one celebrates Oracle products and shouts from the rooftops, Tableau will become a tool in the Salesforce toolbox that major players will continue to use.</p><hr noshade="" size="1"><p>I've been working with Tableau for more than 10 years. Back before there was Tableau Cloud (formerly Tableau Online). Before there was Tableau Prep. Before the Salesforce acquisition.</p><p>During this decade, you could feel the momentum. The product was up-and-coming. Developers, users, and executives were excited about the future possibilities of the tool. The insights it could and was uncovering.</p><p>The "koolaid" tasted good and was still nutritious.</p><p>But after the Salesforce acquisition, things started changing.</p><h3>Unofficial Leading and Lagging Indicators</h3><p>In 2019, Salesforce acquired Tableau. A heavyweight acquired an upcoming superstar. It would be like Michael Jordan bringing second year Lebron James onboard. Or maybe it is more like Darko Miličić getting drafted by the Detroit Pistons with Ben Wallace, Chauncey Billups, Richard Hamilton, and Tayshaun Prince on it?</p><blockquote><p>For non-NBA historians, Darko was considered a bust after getting brought on by the at-the-time NBA powerhouse, the Detroit Pistons. Lebron James meanwhile has had an incredibly successfuly career, living up to expectations.</p></blockquote><p>The 2019 acquisition itself was received with mixed emotions. Some were excited about the resources and market Tableau would get access to. Others were skeptical about the product's culture and vision with a new boss in place.</p><p>Both parties ended up being correct with their perspectives.</p><p>The leading indicators showed up quickly. Product updates that were long overdue started getting built. The community fragmented and lost previously active contributors.</p><p>The lagging indicators showed up after 2 to 3 years. People fed up with Tableau/Salesforce support (or lackthereof). Questions about the future of the company and momentum (like this post). Salesforce executives <a href="https://www.bnnbloomberg.ca/salesforce-guts-tableau-after-spending-15-7-billion-in-2019-deal-1.1866349" rel="noopener noreferrer nofollow" target="_blank">mentioning Tableau less than Slack or Mulesoft on their public calls</a>.</p><p>So depending on what you have focused on, Tableau is doing both great and heading in the wrong direction at the same time.</p><h3>Getting Past the Indicators</h3><p>When a company grows with koolaid, it also fails when there is a shortage of koolaid. But this is difficult to measure and see from a performance aspect until much later. <a href="https://www.geekwire.com/2023/salesforce-stock-spikes-14-after-beating-q4-estimates-tableau-revenue-grows-3-to-636m/" rel="noopener noreferrer nofollow" target="_blank">Revenue still looks great</a>. It's growing year over year.</p><p>But most Tableau executives have left as of now. We've personally had clients and prospects trading Tableau for Power BI, native data viz tools in their other software, or just simply dropping the software all together. </p><h2>The Future of Tableau</h2><p>The competition in the data visualization space is increasing. New companies that directly address the shortcomings of Tableau and similar data visualization tools are starting to take foot.</p><p>It'll take years for those companies to eat into any significant market share of Tableau. But like Salesforce's race against HubSpot; new incumbents have ample opportunity to succeed with better custom service and more targeted problems that it solves with data.</p><p>Some of these up and comers include more flexible solutions like <a href="https://omni.co/" rel="noopener noreferrer nofollow" target="_blank">Omni</a>. Others are more targeted toward simplicity like <a href="https://app.databox.com/datawall/0be921146ad72ee8c631430a5b9b39570625489f6?fp_ref=dan56" rel="noopener noreferrer nofollow" target="_blank">Databox</a>.</p><p>Meanwhile, Microsoft's Power BI is on a hot streak. It's already internally approved and accessible by companies on the Microsoft stack. It's "free" to start with and in Microsoft's customers' stack, so business users and developers alike can build POCs with real company data they have access to.</p><p>It's a real threat to the core business of Tableau. Many companies are reducing SaaS subscription costs and complexity after Covid made them run rampant. Microsoft already has relationships to nearly all of these large enterprise accounts of Tableau. And with the stickiness of Microsoft's products, they can take losses on Power BI to take Tableau market share (if they choose).</p><p>Obviously there are technical differences between the platforms, but that's not the real decider of whether someone chooses Tableau or Power BI. This is the only true short-term threat to Tableau's enterprise customers in my opinion.</p><h3>Where That Leaves Tableau</h3><p>Tableau seems like it's settling into an incumbent role similar to Qlik. I believe it'll be a consistent player at large institutions who typically go through complex project and procurement processes.</p><p>It'll be a necessary evil that people will view the same as Salesforce. The view that it's for the more complex use cases and expensive. But once you reach a certain growth point, it's the option you need to go with.</p><p>Companies that are already using Salesforce will expand their usage to Tableau as well. Similar to how Power BI is being adopted by companies who are already neck deep in the Microsoft stack. Similar to how Oracle sells multiple products under their umbrella once they have an inked relationship with a customer.</p><p>It will no longer be the hot new tool that will be embraced by SMBs. The community will not have the hope and excitement it had in the 2010s. Instead, they'll constantly voice frustration that no one in Salesforce cares about their customers and that they'll just be a line on a spreadsheet again. But it won't matter, because the Tableau revenue will continue to steadily grow as Salesforce digs its heels into current revenue streams.</p><p>Small players will snap up opportunities of frustrated former Tableau customers. They'lll grow until they're acquired by a bigger player wanting access to that market.</p><p>And the cycle will continue as it always does.</p><h3>Final Conclusion</h3><p>Tableau will continue to be used in Enterprise and in large government. Power BI will eat some of that market share (you might've witnessed this at your compay already). It'll be further moved into an "add-on" role for Salesforce instead of a standalone product.</p><p>SMBs will seek other solutions that are more flexible, easier to use, or cheaper. Salesforce won't care too much about this as they'll be focused on the larger accounts.</p><p>If you're a Tableau developer, your opportunities will be there, but will look more and more like a Qlik developer's opportunities.</p><p>The "good times" in the community have ended. #Datafam will become more like other incumbent communities. The magic is no longer there and that's ok. Nothing lasts forever.</p><hr noshade="" size="1"><p>This article is written by Dan Saavedra, Founder of <a href="http://mergeyourdata.com/" rel="noopener noreferrer nofollow" target="_blank">MergeYourData.com</a>. Identify your most profitable customer segment and double down on growing what works.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Font that Renders 14 types of Charts (142 pts)]]></title>
            <link>https://www.vectrotype.com/chartwell</link>
            <guid>39518964</guid>
            <pubDate>Tue, 27 Feb 2024 01:32:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vectrotype.com/chartwell">https://www.vectrotype.com/chartwell</a>, See on <a href="https://news.ycombinator.com/item?id=39518964">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Bpftop: Streamlining eBPF performance optimization (108 pts)]]></title>
            <link>https://netflixtechblog.com/announcing-bpftop-streamlining-ebpf-performance-optimization-6a727c1ae2e5?gi=223d75ac1771</link>
            <guid>39518791</guid>
            <pubDate>Tue, 27 Feb 2024 01:04:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://netflixtechblog.com/announcing-bpftop-streamlining-ebpf-performance-optimization-6a727c1ae2e5?gi=223d75ac1771">https://netflixtechblog.com/announcing-bpftop-streamlining-ebpf-performance-optimization-6a727c1ae2e5?gi=223d75ac1771</a>, See on <a href="https://news.ycombinator.com/item?id=39518791">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a href="https://netflixtechblog.medium.com/?source=post_page-----6a727c1ae2e5--------------------------------" rel="noopener follow"><div aria-hidden="false"><p><img alt="Netflix Technology Blog" src="https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a><a href="https://netflixtechblog.com/?source=post_page-----6a727c1ae2e5--------------------------------" rel="noopener  ugc nofollow"><div aria-hidden="false"><p><img alt="Netflix TechBlog" src="https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto"></p></div></a></div><p id="976a"><em>By </em><a href="https://www.linkedin.com/in/josefernandezmn/" rel="noopener ugc nofollow" target="_blank"><em>Jose Fernandez</em></a></p><p id="5c31">Today, we are thrilled to announce the release of <a href="https://github.com/Netflix/bpftop" rel="noopener ugc nofollow" target="_blank">bpftop</a>, a command-line tool designed to streamline the performance optimization and monitoring of eBPF programs. As Netflix increasingly adopts eBPF [<a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/extending-vector-with-ebpf-to-inspect-host-and-container-performance-5da3af4c584b">1</a>, <a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/how-netflix-uses-ebpf-flow-logs-at-scale-for-network-insight-e3ea997dca96">2</a>], applying the same rigor to these applications as we do to other managed services is imperative. Striking a balance between eBPF’s benefits and system load is crucial, ensuring it enhances rather than hinders our operational efficiency. This tool enables Netflix to embrace eBPF’s potential.</p><figure></figure><h2 id="9661">Introducing bpftop</h2><p id="a934">bpftop provides a dynamic real-time view of running eBPF programs. It displays the average execution runtime, events per second, and estimated total CPU % for each program. This tool minimizes overhead by enabling performance statistics only while it is active.</p><figure></figure><p id="0afb">bpftop simplifies the performance optimization process for eBPF programs by enabling an efficient cycle of benchmarking, code refinement, and immediate feedback. Without bpftop, optimization efforts would require manual calculations, adding unnecessary complexity to the process. With bpftop, users can quickly establish a baseline, implement improvements, and verify enhancements, streamlining the process.</p><p id="f37b">A standout feature of this tool is its ability to display the statistics in time series graphs. This approach can uncover patterns and trends that could be missed otherwise.</p><h2 id="a0b7">How it works</h2><p id="c629">bpftop uses the <a href="https://elixir.bootlin.com/linux/v6.6.16/source/include/uapi/linux/bpf.h#L792" rel="noopener ugc nofollow" target="_blank">BPF_ENABLE_STATS</a> syscall command to enable global eBPF runtime statistics gathering, which is disabled by default to reduce performance overhead. It collects these statistics every second, calculating the average runtime, events per second, and estimated CPU utilization for each eBPF program within that sample period. This information is displayed in a top-like tabular format or a time series graph over a 10s moving window. Once bpftop terminates, it turns off the statistics-gathering function. The tool is written in Rust, leveraging the <a href="https://github.com/libbpf/libbpf-rs" rel="noopener ugc nofollow" target="_blank">libbpf-rs</a> and <a href="https://github.com/ratatui-org/ratatui" rel="noopener ugc nofollow" target="_blank">ratatui</a> crates.</p><h2 id="24a2">Getting started</h2><p id="33df">Visit the project’s <a href="https://github.com/Netflix/bpftop" rel="noopener ugc nofollow" target="_blank">GitHub page</a> to learn more about using the tool. We’ve open-sourced bpftop under the Apache 2 license and look forward to contributions from the community.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Segmenting Comic book Frames (183 pts)]]></title>
            <link>https://vrroom.github.io/blog/2024/02/23/comic-frame-segmentation.html</link>
            <guid>39518202</guid>
            <pubDate>Mon, 26 Feb 2024 23:33:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vrroom.github.io/blog/2024/02/23/comic-frame-segmentation.html">https://vrroom.github.io/blog/2024/02/23/comic-frame-segmentation.html</a>, See on <a href="https://news.ycombinator.com/item?id=39518202">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><em>This post is based on my project in my Computer Vision class last semester</em></p>

<h2 id="introduction">Introduction</h2>

<p>As I was learning classical techniques in my Computer Vision class, I came across a <a href="https://maxhalford.github.io/blog/comic-book-panel-segmentation/">blog post</a> by Max Halford on extracting frames from comic books. He developed a very interesting algorithm where he applied <em>Canny</em> to detect the boundary of frames, filled holes and fit bounding boxes to contiguous regions.</p>

<p>This elegant algorithm did the job very well but had its shortcomings. For one, it didn’t handle arbitrary, un-aligned polygons and didn’t work on <em>negative frames</em>, which didn’t have a boundary of their own, but rather were defined by the boundaries of neighboring frames.</p>

<p>Given the hype around <em>foundation models</em> for segmentation such as <a href="https://github.com/facebookresearch/segment-anything">SAM</a>, I approached this problem by procedurally generating a synthetic dataset of comic books and finetuning SAM to detect the corner points of frames.</p>

<div>
<table>
<tbody><tr>
<td>
  <img src="https://vrroom.github.io/assets/comic_frame_seg/comic_panels.png" width="300px">
</td>
</tr>
</tbody><caption>Failure cases of heuristic approaches: (Top) Frames from Pepper and Carrot by David Revoy are polygons and not axis-aligned bounding boxes. (Bottom) Negative frames may not have a well defined border. </caption>
</table>
</div>

<h2 id="procedural-comic-generator">Procedural Comic Generator</h2>

<p>There isn’t abundant data available for this problem. But that doesn’t mean that we should hold our head in our hands. A common technique that is widely used (see <a href="https://errollw.com/">Erroll Wood’s</a> work) is to procedurally generate training data.</p>

<p>In our case, this means simulating comic books. Note, we don’t really need to make gripping animations and tell a story, we just need to generate panels that look like comics from 50,000 feet. In order to do this, I wrote a procedural generator of layouts and assigned random boxes on an empty image. I filled these boxes with images sampled from the <a href="https://danbooru.donmai.us/">Danbooru</a> dataset.</p>

<p>In order to ensure that the sampled images were atleast semi-coherant, I used <a href="https://github.com/openai/CLIP">CLIP L/14 image encoder</a> to create an image index. While choosing images for a particular page, I sampled one image at random from Danbooru and filled the rest of the boxes using it’s k-nearest neighbors.</p>

<p>With this procedural generator, I had complete control of the size, shape and boundary properties of the box, which I could set appropriately to simulate <em>negative</em> and <em>polygonal</em> frames.</p>



<h2 id="comic-segmentation">Comic Segmentation</h2>

<p>I used SAM as the backbone for my model. SAM is the state-of-the-art image segmentation model. It consists of a heavy, compute expensive image encoder and a light-weight decoder, which answers segmentation queries. The heavy encoder encodes an image only once, after which multiple segmentation queries are answered cheaply. This division of labor is particularly useful for deployment, where an enterprise serving a user can optimize for both speed and costs by keeping the heavy encoder inference on the cloud and using the user’s device for light-weight inference.</p>

<p>Since SAM predicts dense, per pixel mask, I modified it to predict points instead. An overview of the model can be seen below. The procedurally generated comic frame is fed to the image encoder (whose weights remain unchanged during training). A point is randomly sampled from a frame and given as a query/prompt. The light-weight decoder is trained to recover the corners of the frame.</p>

<div>
<table>
<tbody><tr>
<td>
  <img src="https://vrroom.github.io/assets/comic_frame_seg/architecture.png" width="500px">
</td>
</tr>
</tbody><caption>Model Overview</caption>
</table>
</div>

<p>I learned two lessons while training this model. Firstly, it was important to canonicalize the order in which the corners of the frame were predicted. Without this, the model got conflicting signals on the ordering of corner points and never converged. Secondly, it was important to use L1 instead of L2 loss since L2 optimized very quickly without improving the quality of predictions.</p>

<h2 id="evaluation">Evaluation</h2>

<p>I compared my method against original SAM and Halford’s method. Note that Halford’s method is a bit disadvantaged in this comparison since my method also uses a query (set to the center of the ground truth frame to be predicted). Despite this, it is evident that our model trained on our procedurally generated dataset, generalizes on “real-world” comics (Pepper and Carrot abbrev. as P&amp;C), coming close to Halford in the process. It beats Halford on procedurally generated dataset (abbrev. Pr), since this dataset is designed to expose the flaws in the method.</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>IoU (P&amp;C)</th>
      <th>PCK@0.1 (P&amp;C)</th>
      <th>L1 (P&amp;C)</th>
      <th>IoU (Pr)</th>
      <th>PCK@0.1 (Pr)</th>
      <th>L1 (Pr)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SAM</td>
      <td>0.42</td>
      <td>0.52</td>
      <td>0.37</td>
      <td>0.81</td>
      <td>0.94</td>
      <td>0.08</td>
    </tr>
    <tr>
      <td>Halford</td>
      <td><strong>0.93</strong></td>
      <td>0.96</td>
      <td><strong>0.04</strong></td>
      <td>0.47</td>
      <td>0.61</td>
      <td>0.47</td>
    </tr>
    <tr>
      <td>Ours</td>
      <td>0.88</td>
      <td><strong>0.98</strong></td>
      <td>0.05</td>
      <td><strong>0.88</strong></td>
      <td><strong>0.99</strong></td>
      <td><strong>0.03</strong></td>
    </tr>
  </tbody>
</table>

<p>Here, IoU simply measures the area of intersection over union of the ground truth and predicted frames. PCK@0.1 refers to the percentage of times, the predicted frame corner lies within certain radius of the ground truth frame corner (0.1 refers to the radius as a percentage of the diagonal of the comic page). L1 is simply the L1 distance between ground truth and predicted frames.</p>

<p>Below are some qualitative results which demonstrate that our method works on “real-world” comics. We run it in two modes. On the left, we interactively provide a query and the model produces the corners. On the right, we sample a bunch of query on the image, predict polygons and filter them using <em>non-maximal suppression</em> like the original SAM paper.</p>



<h2 id="final-thoughts">Final Thoughts</h2>

<p>There are still shortcomings to my method and it can often fail for complex, cluttered comic pages. But still, I like this approach to designing algorithm over composing OpenCV functions because it is often easier to see how to improve the dataset than to design new heuristics. Once you do that, you almost have a guarantee that the Neural Network machinery will get you the results.</p>

<p>The annotated Pepper and Carrot dataset that I used for evaluation can be found in my <a href="https://drive.google.com/file/d/1z8OE8TC8eupC6_ZNxUSVyfvk4rSkVIgE">drive link</a>. All my code and checkpoints are available in my <a href="https://github.com/Vrroom/segment-anything-comic">Github Repo</a>. If you think of any improvements to my approach, feel free to reach out!</p>

<hr>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cactus – A Modern Diablo II Version Switcher, Character Isolator, & Mod Manager (101 pts)]]></title>
            <link>https://github.com/fearedbliss/Cactus</link>
            <guid>39518161</guid>
            <pubDate>Mon, 26 Feb 2024 23:29:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/fearedbliss/Cactus">https://github.com/fearedbliss/Cactus</a>, See on <a href="https://news.ycombinator.com/item?id=39518161">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">Cactus <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/61bd41ec7f1d30871fe47ccfcf5f3e765f49c72ede959e1a4a912d5150a857ca/68747470733a2f2f692e696d6775722e636f6d2f6d53775a4a6d722e706e67"><img src="https://camo.githubusercontent.com/61bd41ec7f1d30871fe47ccfcf5f3e765f49c72ede959e1a4a912d5150a857ca/68747470733a2f2f692e696d6775722e636f6d2f6d53775a4a6d722e706e67" height="35" data-canonical-src="https://i.imgur.com/mSwZJmr.png"></a></h2><a id="user-content-cactus-" aria-label="Permalink: Cactus " href="#cactus-"></a></div>
<p dir="auto"><em><strong>A Modern Version Switcher, Character Isolator, and Mod Manager for Diablo II (Original, Not Resurrected)</strong></em></p>
<p dir="auto"><em><strong>By: Jonathan Vasquez (fearedbliss)</strong></em></p>
<p dir="auto"><em><strong>Build: 2024-02-26-2000</strong></em></p>
<ul dir="auto">
<li>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://xyinn.org/diablo/videos" rel="nofollow">Videos</a></h4><a id="user-content-videos" aria-label="Permalink: Videos" href="#videos"></a></p>
</li>
<li>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://github.com/fearedbliss/Cactus/blob/main/PORTS-COLLECTION.md">Ports Collection</a></h4><a id="user-content-ports-collection" aria-label="Permalink: Ports Collection" href="#ports-collection"></a></p>
</li>
<li>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://github.com/fearedbliss/Cactus/blob/main/PATCH-NOTES.md">Patch Notes</a></h4><a id="user-content-patch-notes" aria-label="Permalink: Patch Notes" href="#patch-notes"></a></p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Download</h2><a id="user-content-download" aria-label="Permalink: Download" href="#download"></a></p>
<p dir="auto">The Cactus (Core) source code can be downloaded at the
<a href="https://github.com/fearedbliss/Cactus-Core">Cactus (Core)</a> repo.</p>
<p dir="auto">The complete Cactus package is available as a direct download from my server
(uptime is not guaranteed). <a href="https://www.7-zip.org/download.html" rel="nofollow">7-Zip</a> must be
used to extract the archive since I'm using Ultra compression. All releases are
hashed and PGP signed with the key: <strong><code>34DA 858C 1447 509E C77A D49F FB85 90B7 C4CA 5279</code></strong>,
which can be found at the link below.</p>
<ul dir="auto">
<li>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://xyinn.org/diablo/Cactus.7z" rel="nofollow">Download Cactus</a></h3><a id="user-content-download-cactus" aria-label="Permalink: Download Cactus" href="#download-cactus"></a></p>
</li>
<li>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://keys.openpgp.org/search?q=34DA+858C+1447+509E+C77A+D49F+FB85+90B7+C4CA+5279" rel="nofollow">PGP Public Key</a></h3><a id="user-content-pgp-public-key" aria-label="Permalink: PGP Public Key" href="#pgp-public-key"></a></p>
</li>
<li>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://xyinn.org/diablo/Cactus.7z.SHA256.txt" rel="nofollow">Latest Release Hash</a></h3><a id="user-content-latest-release-hash" aria-label="Permalink: Latest Release Hash" href="#latest-release-hash"></a></p>
</li>
<li>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://xyinn.org/diablo/Cactus.7z.SHA256.txt.sig" rel="nofollow">Latest Release Hash Signature</a></h3><a id="user-content-latest-release-hash-signature" aria-label="Permalink: Latest Release Hash Signature" href="#latest-release-hash-signature"></a></p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Released under the <strong><a href="https://github.com/fearedbliss/Cactus/blob/main/LICENSE.txt">Simplified BSD License</a></strong>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li><em><strong>Windows 7 or 10 (11+ will not be supported, read below for why)</strong></em></li>
<li><em><strong>.NET Framework 4.6.2+ (Cactus)</strong></em></li>
<li><em><strong>Visual C++ 2015 - 2022 Redistributable (x86) (DSOAL w/ OpenAL Soft)</strong></em></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screenshots</h2><a id="user-content-screenshots" aria-label="Permalink: Screenshots" href="#screenshots"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Light Mode</h3><a id="user-content-light-mode" aria-label="Permalink: Light Mode" href="#light-mode"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2f251245ed1f4542ec5dcf5b745119e8b251a83fed79072062ca3153fb8c7982/68747470733a2f2f692e696d6775722e636f6d2f566e693333414b2e706e67"><img src="https://camo.githubusercontent.com/2f251245ed1f4542ec5dcf5b745119e8b251a83fed79072062ca3153fb8c7982/68747470733a2f2f692e696d6775722e636f6d2f566e693333414b2e706e67" alt="Cactus" data-canonical-src="https://i.imgur.com/Vni33AK.png"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dark Mode</h3><a id="user-content-dark-mode" aria-label="Permalink: Dark Mode" href="#dark-mode"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/0109a3b86ee3de3757ca29881ed7bd83ff7995f9a8f0d441f8c36d29e4167b81/68747470733a2f2f692e696d6775722e636f6d2f57457a6d4878432e706e67"><img src="https://camo.githubusercontent.com/0109a3b86ee3de3757ca29881ed7bd83ff7995f9a8f0d441f8c36d29e4167b81/68747470733a2f2f692e696d6775722e636f6d2f57457a6d4878432e706e67" alt="Cactus" data-canonical-src="https://i.imgur.com/WEzmHxC.png"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Out Of The Box Experience</h3><a id="user-content-out-of-the-box-experience" aria-label="Permalink: Out Of The Box Experience" href="#out-of-the-box-experience"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e6c498d0b0ccf490a4f0dc9d7eff9ab9c492e065e64c3be356cba3ecd763d52b/68747470733a2f2f692e696d6775722e636f6d2f384d57736b4a512e706e67"><img src="https://camo.githubusercontent.com/e6c498d0b0ccf490a4f0dc9d7eff9ab9c492e065e64c3be356cba3ecd763d52b/68747470733a2f2f692e696d6775722e636f6d2f384d57736b4a512e706e67" alt="Cactus" data-canonical-src="https://i.imgur.com/8MWskJQ.png"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Settings (19 Material Design Colors Available + Light/Dark Mode)</h3><a id="user-content-settings-19-material-design-colors-available--lightdark-mode" aria-label="Permalink: Settings (19 Material Design Colors Available + Light/Dark Mode)" href="#settings-19-material-design-colors-available--lightdark-mode"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/53903c6976f052f985ce10a899c8dfbfeba82e038b864a8473fcb36e0c6bd83a/68747470733a2f2f692e696d6775722e636f6d2f6d43614c666b762e706e67"><img src="https://camo.githubusercontent.com/53903c6976f052f985ce10a899c8dfbfeba82e038b864a8473fcb36e0c6bd83a/68747470733a2f2f692e696d6775722e636f6d2f6d43614c666b762e706e67" alt="Cactus" data-canonical-src="https://i.imgur.com/mCaLfkv.png"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Add Entry</h3><a id="user-content-add-entry" aria-label="Permalink: Add Entry" href="#add-entry"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1bda3411a7b88d62613a6e315d4d8f945077196023922cb73c41200bdeb3c6b6/68747470733a2f2f692e696d6775722e636f6d2f436959353844612e706e67"><img src="https://camo.githubusercontent.com/1bda3411a7b88d62613a6e315d4d8f945077196023922cb73c41200bdeb3c6b6/68747470733a2f2f692e696d6775722e636f6d2f436959353844612e706e67" alt="Cactus" data-canonical-src="https://i.imgur.com/CiY58Da.png"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Edit Entry</h3><a id="user-content-edit-entry" aria-label="Permalink: Edit Entry" href="#edit-entry"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/42cbc84e1f0b7e356bae148c2d543acf274cb393639fa8d72efe31579a131bb6/68747470733a2f2f692e696d6775722e636f6d2f6f6f6942576e482e706e67"><img src="https://camo.githubusercontent.com/42cbc84e1f0b7e356bae148c2d543acf274cb393639fa8d72efe31579a131bb6/68747470733a2f2f692e696d6775722e636f6d2f6f6f6942576e482e706e67" alt="Cactus" data-canonical-src="https://i.imgur.com/ooiBWnH.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">History</h2><a id="user-content-history" aria-label="Permalink: History" href="#history"></a></p>
<p dir="auto">Cactus started out as just a simple application that allowed you to
easily and efficiently Time Travel between every version of Diablo II
that ever came out, while maximizing disk space, and enabling full
character isolation between versions. However, even though Cactus itself
still is just that, the Cactus Repository has evolved to become a
centralized and historical archive, that aims to preserve every single
Diablo II version that exists (<em><strong>Official Retail</strong></em> and <em><strong>Official Beta</strong></em>
Releases), and has also become an ecosystem where mod developers can distribute
their mods on. Cactus is a complete rewrite from scratch of my previous
application called <em><strong>Bliss Version Switcher</strong></em>. However, since Cactus is
written in C#, it behaves as a native Windows application and allows it
to integrate natively with the system. On the other hand, Bliss Version
Switcher was written in Java and there were many limitations that lead to the
Cactus rewrite.</p>
<p dir="auto">This repository also includes several other utilities that I have either
created, or collected, which can help you play <em><strong>Vanilla</strong></em> Diablo II
better. All Cactus Platforms are <em><strong>Vanilla</strong></em> by default. The only fix
I made to all Platforms below 1.12 is to remove the CD requirement,
since modern computers no longer have a CD drive. Blizzard already did
this exact thing for all versions starting with 1.12. Other than that,
the only other modifications I provide are through
<a href="https://github.com/fearedbliss/Cactus/blob/main/README-SINGLING.md"><strong>Singling</strong></a>, which only contains
<em><strong>non-gameplay modifications</strong></em> and is <em><strong>completely opt-in</strong></em>.</p>
<p dir="auto">If you will be playing online, you should make a copy of the Vanilla Platform
and use that one to connect to Battle.net (for example, copying the <strong>1.14d</strong>
platform, and calling it something like <strong>1.14d BNET</strong>). You can use your other
platforms with the Singling changes for local play.</p>
<p dir="auto">The <strong>cnc-ddraw</strong> video renderer, and <strong>DSOAL w/ OpenAL Soft</strong> are included as
a <em><strong>shared resource</strong></em> for all platforms. cnc-ddraw is provided to improve
video compatibility for all versions between 1.00 - 1.13d. Blizzard removed
DirectDraw support starting with 1.14, and thus Cactus will only provide
video support for versions before that. DSOAL w/ OpenAL Soft is provided to
enable you to use the following lost in-game sound functionality: 3D Sound,
Environmental Effects, 3D Bias.</p>
<p dir="auto">Cactus can also be used for <em><strong>easily playing mods in an isolated and safe fashion.</strong></em>
Please check out the <em><strong><a href="https://github.com/fearedbliss/Cactus/blob/main/PORTS-COLLECTION.md">Cactus Ports Collection</a></strong></em> for a
list of mods that have either been ported to Cactus, attempted to be ported to
Cactus, or were made to run on Cactus natively. You'll also find the compatibility
statuses for each mod listed, and a dedicated page containing important info.</p>
<p dir="auto">Cactus requires a purchased copy of <em><strong>Diablo II (Original, Not Resurrected)</strong></em>
from Blizzard in order to have all of the game assets stored in the MPQs. Once
you have these, they will be reused for all platforms.</p>
<p dir="auto">For more information, please read the documentation below for anything you are
interested in exploring.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cactus Repository</h2><a id="user-content-cactus-repository" aria-label="Permalink: Cactus Repository" href="#cactus-repository"></a></p>
<p dir="auto">The following opt-in modifications and utilities are available in this repository:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/fearedbliss/Cactus/blob/main/README-SINGLING.md">Singling</a></h3><a id="user-content-singling" aria-label="Permalink: Singling" href="#singling"></a></p>
<p dir="auto">A collection of non-gameplay modifications and fixes in order to improve the
Vanilla Diablo II Single Player &amp; LAN Experience.</p>
<p dir="auto">To use Singling, simply copy the Singling files for the version you want to
play from the <strong><code>2. Singling/1. Files</code></strong> folder, and replace the ones for
the equivalent version in your Platforms directory. To revert, use the files
in <strong><code>2. Singling/2. Stock</code></strong> instead.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/fearedbliss/Cactus/blob/main/README-RENDERERS.md">Renderers</a></h3><a id="user-content-renderers" aria-label="Permalink: Renderers" href="#renderers"></a></p>
<p dir="auto">Cactus includes and promotes the <strong><code>cnc-ddraw</code></strong> video renderer for
Diablo II versions <strong><code>1.00 - 1.13d</code></strong>, which can help you run the game
on newer systems with a higher window resolution (not a higher internal
resolution), and the ability to use shaders to upscale the quality of
the graphics in the game. Since Blizzard removed <strong><code>DirectDraw</code></strong>
support in <strong><code>1.14+</code></strong>, you'll need to find an alternative video
renderer for those versions.</p>
<p dir="auto">Please read the <a href="https://github.com/fearedbliss/Cactus/blob/main/README-RENDERERS.md"><strong><code>README-RENDERERS</code></strong></a> for
further explanation on this, for information on how to set it up, or for
any known technical limitations. Definitely read the
<strong><code>Recommendations</code></strong> section at least, or you will most likely
encounter crashes if you've never played versions before <strong><code>1.14</code></strong>
before. Blizzard has done major changes with how video configuration
works starting in <strong><code>1.14</code></strong>.</p>
<ul dir="auto">
<li><a href="https://github.com/CnCNet/cnc-ddraw"><strong><code>cnc-ddraw</code></strong></a> - This renderer
reimplements the DirectDraw API for GDI, OpenGL, and Direct3D to improve
compatibility with Windows XP - 10, and Wine. This renderer also supports
the use of custom shaders - which will allow you to upscale the game so it
looks a lot better - and even provides hotkeys (such as <strong><code>[Alt] + [Enter]</code></strong>)
to switch between full screen and window mode.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/fearedbliss/Cactus/blob/main/README-3D-SOUND.md">3D Sound (DSOAL w/ OpenAL Soft)</a></h3><a id="user-content-3d-sound-dsoal-w-openal-soft" aria-label="Permalink: 3D Sound (DSOAL w/ OpenAL Soft)" href="#3d-sound-dsoal-w-openal-soft"></a></p>
<p dir="auto">Cactus includes the files required to allow you to enable the following
lost in-game sound functionality: <strong><code>3D Sound</code></strong>, <strong><code>Environmental Effects</code></strong>, and <strong><code>3D Bias</code></strong>.</p>
<p dir="auto">Please read the <a href="https://github.com/fearedbliss/Cactus/blob/main/README-3D-SOUND.md"><strong><code>README-3D-SOUND</code></strong></a> for more information.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation Instructions</h2><a id="user-content-installation-instructions" aria-label="Permalink: Installation Instructions" href="#installation-instructions"></a></p>
<ul dir="auto">
<li><a href="https://xyinn.org/diablo/videos/01.%20Cactus%20Installation%20Video.mp4" rel="nofollow"><strong>Cactus Setup &amp; Demo Video (2.6.2+) (37 Minutes)</strong></a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Required Files</h3><a id="user-content-required-files" aria-label="Permalink: Required Files" href="#required-files"></a></p>
<p dir="auto">After you finish installing Diablo II (or restoring the files from a backup),
you only need to keep the following files in your <strong><code>Diablo II Root Directory</code></strong>.
Everything else can be deleted since it will come from Cactus. Once you are done,
continue with the Cactus installation steps.</p>
<div data-snippet-clipboard-copy-content="- D2.LNG
- D2Char.mpq
- D2Data.mpq
- D2Exp.mpq      (1.07+)
- D2Music.mpq    (Not needed for 1.00)
- D2Sfx.mpq
- D2Speech.mpq
- D2Video.mpq
- D2XMusic.mpq   (1.07+)
- D2XTalk.mpq    (1.07+)
- D2XVideo.mpq   (1.07+)"><pre><code>- D2.LNG
- D2Char.mpq
- D2Data.mpq
- D2Exp.mpq      (1.07+)
- D2Music.mpq    (Not needed for 1.00)
- D2Sfx.mpq
- D2Speech.mpq
- D2Video.mpq
- D2XMusic.mpq   (1.07+)
- D2XTalk.mpq    (1.07+)
- D2XVideo.mpq   (1.07+)
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Install Cactus, Dependencies, and Prepare MPQs.</h3><a id="user-content-install-cactus-dependencies-and-prepare-mpqs" aria-label="Permalink: Install Cactus, Dependencies, and Prepare MPQs." href="#install-cactus-dependencies-and-prepare-mpqs"></a></p>
<p dir="auto">This section will help you install Cactus to the correct location, its dependencies,
and also help you fix your MPQs, so that they are compatible with the older versions
of Diablo II. I only test on and support <strong><code>Windows 7</code></strong> and <strong><code>Windows 10</code></strong>,
however these instructions will probably work on versions in between as well.</p>
<ol dir="auto">
<li>
<p dir="auto">Copy all of the files in the <strong><code>1. Files</code></strong> folder into your <strong><code>Diablo II Root Directory</code></strong>.</p>
<p dir="auto"><strong><code>Note:</code></strong> It's important that <strong><code>Cactus</code></strong> runs from inside your <strong><code>Diablo II Root Directory</code></strong>
or you will get weird behavior in various situations like running <strong><code>-direct -txt</code></strong> mods or
taking screenshots.</p>
</li>
<li>
<p dir="auto">If you need to fix your MPQs, then also copy the <strong><code>MpqFixer</code></strong> located in the <strong><code>3. Other</code></strong>
directory into your <strong><code>Diablo II Root Directory</code></strong>. This fix is only needed if you want to
play versions <strong><code>1.08 - 1.13d</code></strong> and you also happened to install Diablo II from the new
Blizzard Installer. If you are not planning to play those versions, or you installed
Diablo II from the original <strong><code>1.00, 1.03, 1.07</code></strong> discs, you don't need to fix your MPQs.</p>
<p dir="auto">You can then run the <strong><code>FIX_MPQS_RUN_AS_ADMIN.bat</code></strong> inside the <strong><code>MpqFixer</code></strong> directory
that you copied, as <strong><code>Administrator</code></strong>.</p>
</li>
<li>
<p dir="auto">Run the <strong><code>vc_redist.x86.exe</code></strong> file in the <strong><code>3. Other</code></strong> directory to install the
required libraries for <strong><code>3D Sound</code></strong>. If you run the game without these being installed,
you will get a <strong><code>VCRUNTIME140.dll</code></strong> error message.</p>
<p dir="auto"><strong><code>Note:</code></strong> If you don't want this functionality, just delete the <strong><code>dsoal-aldrv.dll</code></strong> and
<strong><code>dsound.dll</code></strong> from your <strong><code>Diablo II Root Directory</code></strong>.</p>
</li>
<li>
<p dir="auto"><strong><code>(Windows 7 Only)</code></strong> Cactus requires <strong><code>.NET Framework 4.6.2</code></strong> to function, but that version
does not come included in Windows 7 by default. You can run the
<strong><code>NET_Framework_4.6.2_Offline_Installer_for_Windows_7.exe</code></strong> file located in the <strong><code>3. Other</code></strong>
directory to install that dependency. Windows 10 provides this dependency by default.</p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adding/Running A Platform</h3><a id="user-content-addingrunning-a-platform" aria-label="Permalink: Adding/Running A Platform" href="#addingrunning-a-platform"></a></p>
<ol dir="auto">
<li>Open <strong><code>Cactus</code></strong>.</li>
<li>Click <strong><code>Settings</code></strong> and set your <strong><code>Diablo II Root Directory</code></strong> to your Diablo II folder. <strong><code>(Example: C:\Games\Diablo II)</code></strong></li>
<li>Click <strong><code>Add</code></strong>.</li>
<li>Type in the name of the <strong><code>Platform</code></strong> you want to run. This should match a folder in the <strong><code>Platforms</code></strong>
folder. (Example: If you want to run <strong><code>1.09b</code></strong>, type <strong><code>1.09b</code></strong>).</li>
<li><strong><code>Optional (Recommended)</code></strong>: Type in a <strong><code>Label</code></strong> for this platform. If you label your platform, it will have its own dedicated
save directory, allowing you to have multiple entries using the same platform but with different save locations.
If you don't use a label, all the characters with this platform name will be stored in the same location (flat structure).
You can have multiple entries using the same platform with and without labels as well. A label cannot be
removed from an entry once created, but it can be renamed. A label cannot be added to an entry once created.</li>
<li>Enter the name of your <strong><code>Launcher</code></strong>. <strong><code>(Example: Game.exe)</code></strong></li>
<li>Enter the Flags you want (Example: <strong><code>-ns</code></strong>).</li>
<li>Click <strong><code>Add</code></strong>.</li>
<li>Select your newly added Platform and press <strong><code>Launch</code></strong>.</li>
</ol>
<p dir="auto">The game should start. If you are having video issues, please make sure you
have read the <a href="https://github.com/fearedbliss/Cactus/blob/main/README-RENDERERS.md"><strong><code>README-RENDERERS</code></strong></a> and ensure that
it was configured properly.</p>
<p dir="auto"><strong><code>NOTE</code></strong>: Make sure to leave the Cactus application running throughout your
play sessions (you can minimize it). Cactus keeps track of the running Diablo II
processes it launches as to protect you from accidentally switching to a different
platform, and causing your <strong><code>Save Path</code></strong> to be updated to an incorrect location.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Windows 11+ will not be supported.</h2><a id="user-content-windows-11-will-not-be-supported" aria-label="Permalink: Windows 11+ will not be supported." href="#windows-11-will-not-be-supported"></a></p>
<p dir="auto">Due to Microsoft mandating people to have an online connection and a Microsoft
account for Windows 11 (at the OOBE stage), any version over Windows 10 will
not be supported. Given the Xbox One has the same type of requirement, I'm not
surprised Microsoft is going in that direction.</p>
<ul dir="auto">
<li><a href="https://blogs.windows.com/windows-insider/2022/02/16/announcing-windows-11-insider-preview-build-22557/" rel="nofollow"><strong>Source 1 - February 16, 2022 - Windows 11 Insider Preview Build 22557</strong></a></li>
</ul>
<div data-snippet-clipboard-copy-content="Similar to Windows 11 Home edition, Windows 11 Pro edition now requires internet
connectivity during the initial device setup (OOBE) only. If you choose to setup
device for personal use, MSA will be required for setup as well. You can expect
Microsoft Account to be required in subsequent WIP flights."><pre><code>Similar to Windows 11 Home edition, Windows 11 Pro edition now requires internet
connectivity during the initial device setup (OOBE) only. If you choose to setup
device for personal use, MSA will be required for setup as well. You can expect
Microsoft Account to be required in subsequent WIP flights.
</code></pre></div>
<ul dir="auto">
<li><a href="https://blogs.windows.com/windows-insider/2022/05/05/announcing-windows-11-insider-preview-build-22616/" rel="nofollow"><strong>Source 2 - May 5, 2022 - Windows 11 Insider Preview Build 22616</strong></a></li>
</ul>
<div data-snippet-clipboard-copy-content="Previously, we shared new requirements for internet and MSA on the Windows 11
Pro edition. Today, Windows Insiders on Windows 11 Pro edition will now require
MSA and internet connectivity during the initial device setup (OOBE) only when
setting up for personal use. If you choose to setup device for Work or School,
there is no change, and it will work the same way as before."><pre><code>Previously, we shared new requirements for internet and MSA on the Windows 11
Pro edition. Today, Windows Insiders on Windows 11 Pro edition will now require
MSA and internet connectivity during the initial device setup (OOBE) only when
setting up for personal use. If you choose to setup device for Work or School,
there is no change, and it will work the same way as before.
</code></pre></div>
<p dir="auto">Windows 10 Pro doesn't have this requirement at all, and is the OS I primarily
use on my gaming computer. Windows 10 Home does have this requirement though,
but can be bypassed by unplugging your internet connection before the OOBE.
<em><strong>I'm including Windows 10 support due to there still being a direct path
to use the OS with no workarounds through the Pro edition.</strong></em> Windows 10 reaches
EOL on <a href="https://learn.microsoft.com/en-us/lifecycle/products/windows-10-home-and-pro" rel="nofollow">October 14, 2025</a>.</p>
<p dir="auto">However, I have gone dark already using my <strong>Dark Island</strong> strategy, which
means that I'm using Windows exclusively in <strong>Offline Mode</strong>, with all forms
of communication to the public internet disabled. I never allowed it to reach
the internet from the very beginning, including during the installation stage.
I only play <strong>Offline Single Player DRM Free Games</strong>, so this strategy works
for me, and it also means that the EOL status of Windows 10 will have no
consequences for me. I'm basically using Windows like a N64 or Gameboy,
which is why I sometimes call this machine a Wintendo. I never needed
internet for those systems then, and still don't today, yet I can still use
and play those games decades later. Going dark years before the EOL, gives me
plenty of time to make sure that I backup everything that I need, to reproduce
my environment post EOL. If you are interested in my Dark Island strategy,
you can read the next section.</p>
<p dir="auto">Furthermore, I will be exiting Windows development completely for newer versions
of Windows. I will continue to maintain Cactus/Singling/Etc for Windows 7 and 10.</p>
<p dir="auto">Please do not file any bug reports if you are running my software on Windows 11+,
they will be promptly closed as <strong><code>NOT SUPPORTED</code></strong>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dark Island Strategy (Going Dark)</h3><a id="user-content-dark-island-strategy-going-dark" aria-label="Permalink: Dark Island Strategy (Going Dark)" href="#dark-island-strategy-going-dark"></a></p>
<p dir="auto">With this strategy, what we want to do is block Windows from accessing the
internet, but we still want LAN communication (for sideloading games and
other applications). If you don't need LAN communication, then you can
just disable all of your network interfaces, and use an external drive to
load apps into your airgapped machine. This would be the best approach, but
a bit less convenient for sideloading. The LAN-only approach gives you a
good balance of external isolation and internal convenience. You could
also do this at the firewall level, but for me it's just one computer,
isolated to just gaming. Security isn't the most important thing in this
environment. We can just change the Network Adapter settings in the OS.</p>
<p dir="auto">You can do this even without reinstalling, but if you want to have a fresh
installation of Windows 10 with no further updates being forced upon you
by Microsoft, I recommend re-installing Windows 10 and make sure that you
unplug your ethernet cable (and don't connect to wifi) before you start the
installer. You must use <strong>Windows 10 Pro</strong> (or a higher edition) since this
allows you to make a local offline account. Then follow the steps below:</p>
<ol dir="auto">
<li>Start -&gt; Settings</li>
<li>Network &amp; Internet -&gt; Advanced network settings -&gt; Change adapter options</li>
<li>Right click your ethernet adapter interface -&gt; Properties. (If you are on
wifi, then select your wireless interface). Basically anything that will be
connecting to your LAN and that you don't want internet access to work for.</li>
<li>Uncheck <strong>Internet Protocol Version 4 (TCP/IPv6)</strong> (or configure it if you
need it). I personally disabled IPv6 completely because I just need IPv4 for
this purpose, and I also don't want the machine to accidentally connect to
the internet, if I ever enable IPv6 on my modem (and/or if the ISP also has
working IPv6).</li>
<li>Right click <strong>"Internet Protocol Version 4 (TCP/IPv4)"</strong> -&gt; Properties</li>
<li>Check "Use the following IP address" (this should automatically also set
<strong>Use the following DNS server addresses</strong>).</li>
<li>Set the <strong>IP address</strong> to an IP on your local network that doesn't conflict
with your DHCP (maybe a static ip outside of the DHCP range). The
<strong>Subnet mask</strong> should automatically be set to <strong>255.255.255.0</strong>. Leave the
<strong>Default gateway</strong> and <strong>Preferred DNS Server</strong> empty.</li>
<li>Press OK and Close</li>
</ol>
<p dir="auto">That's basically it! Enjoy your life, be free, be happy. Let's play. Go dark!</p>
<p dir="auto">I've been using this approach using the
<strong><code>Win10_22H2_English_x64v1.iso (2023 Update)</code></strong> with hash
<strong><code>bbb1b234ea7f5397a1906ee59187087c78374f35</code></strong> and it works great. Another thing
to keep in mind is that since you are never allowing your OS to communicate to
the outside world, Windows won't be able to reach its activation servers, which
means that the activation timer won't start. You essentially have a free and
legal copy of Windows. Not only that, but you also don't need to deal with
forced updates, forced telemetry, and any other spying that's going on. With
this set up, you get a beautiful, quiet, stable, and privacy respecting version
of Windows 10, simply by disconnecting it from the internet completely.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Optional VM (Not Recommended)</h4><a id="user-content-optional-vm-not-recommended" aria-label="Permalink: Optional VM (Not Recommended)" href="#optional-vm-not-recommended"></a></p>
<p dir="auto">Now at this point, we have two options:</p>
<ol dir="auto">
<li><strong>(Recommended)</strong>: Use another computer as your main computer (or for
downloading games for your <strong>Wintendo</strong>).</li>
<li><strong>(Poses a security risk)</strong>: Setup a Virtual Machine in
<a href="https://www.virtualbox.org/" rel="nofollow">VirtualBox</a> for downloading games and any
other internet related tasks. Due to the potential security risk, if you do
go with this approach, I recommend restricting your usage of the VM to just
downloading games from GOG. You may be fine using this strategy for normal
computing though if you know what you are doing.</li>
</ol>
<p dir="auto">If you switch your virtual machine's network configuration from <strong>NAT</strong> to
<strong>Bridged Adapter</strong>, you can share your host's network adapter with the guest
directly. The Windows host will still have settings that prevent it from
accessing the internet, and the guest will have its own completely different
settings that allows it to access the internet. This means that we can download
all of our games from GOG in the guest, and transfer it with no issues to our
host via the LAN network at full speeds. I had issues with transferring files
between the Host and Guest via VirtualBox's <strong>Shared Folders</strong>, for any games
that required a large amount of disk space. But transferring over LAN was fine
(i.e transferring the files from the guest to the NAS and then back to the
host). Due to the security risk of attacks escaping from the VM, I don't
recommend this approach. Use a separate machine instead for your regular
computing and keep the gaming machine as isolated as possible.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Template &amp; Labeling System</h2><a id="user-content-template--labeling-system" aria-label="Permalink: Template &amp; Labeling System" href="#template--labeling-system"></a></p>
<p dir="auto">The Cactus Template &amp; Labeling System allows you to be able to easily
start using a few pretty cool and very interesting workflows, while
allowing you to re-use your existing platforms, thus minimizing the
amount of disk space used.</p>
<p dir="auto">For example, let's say we want to play <strong><code>Solo Self Found</code></strong> as defined
as <strong><code>Only using items that the character has found with their own hands</code></strong>,
this pretty much means untwinked play. However, let's say you
are also ok with using mules as a form of an extended stash for your
main character. Thus, any items your main character finds, can be placed
in storage, and will only be used by that main character specifically.
If you were to do this manually, for each particular main character you
made, it would quickly get out of hand since all the main characters and
each individual main character's set of mules, would all be in the same
folder. This is where the Templating &amp; Labeling System kicks in. Now, we
could simply make a new entry in Cactus pointing to an existing
platform, and give it a particular label (Say the name of that specific
main character) and play the game. A dedicated save folder with the given
label will be created under the Saves directory for this platform.</p>
<p dir="auto">For example, I want to make a character called <strong><code>Isaac</code></strong>. Isaac will
have their own set of mule characters as well and we'll call them
<strong><code>Mule_A</code></strong>, <strong><code>Mule_B</code></strong>, and <strong><code>Mule_C</code></strong> for simplicity. This
character will also be on <strong><code>1.09b</code></strong>. Thus, we can make a new entry for
our <strong><code>1.09b</code></strong> platform with the label <strong><code>Isaac</code></strong>. Once we start the
game, we will have the following structure in our Diablo II root
directory (let's assume I already made the characters in-game):</p>
<div data-snippet-clipboard-copy-content="/Platforms/1.09b/

/Saves/1.09b/

/Saves/1.09b/Isaac/
/Saves/1.09b/Isaac/Isaac.d2s
/Saves/1.09b/Isaac/Mule_A.d2s
/Saves/1.09b/Isaac/Mule_B.d2s
/Saves/1.09b/Isaac/Mule_C.d2s"><pre><code>/Platforms/1.09b/

/Saves/1.09b/

/Saves/1.09b/Isaac/
/Saves/1.09b/Isaac/Isaac.d2s
/Saves/1.09b/Isaac/Mule_A.d2s
/Saves/1.09b/Isaac/Mule_B.d2s
/Saves/1.09b/Isaac/Mule_C.d2s
</code></pre></div>
<p dir="auto">As you can see, this entry is fully isolated using its platform and
label combination. Now, let's say you and your friends want to have some
type of tournament on <strong><code>1.09b</code></strong>. No problem! You can quickly add
another entry for the <strong><code>1.09b</code></strong> platform with another label, such as
<strong><code>Tournament 2022</code></strong> and start it up. The same exact <strong><code>1.09b</code></strong>
platform files that we used before will be re-used, and we will have a
new save directory. Let's create a new character called <strong><code>Bethany</code></strong>
and give Bethany a few mules as well. We'll call the mules the same as
before, and since they are isolated, we can reuse the same muling naming
scheme with no conflicts. So now our structure looks like this:</p>
<div data-snippet-clipboard-copy-content="/Platforms/1.09b/

/Saves/1.09b/

/Saves/1.09b/Isaac/
/Saves/1.09b/Isaac/Isaac.d2s
/Saves/1.09b/Isaac/Mule_A.d2s
/Saves/1.09b/Isaac/Mule_B.d2s
/Saves/1.09b/Isaac/Mule_C.d2s

/Saves/1.09b/Bethany/
/Saves/1.09b/Bethany/Bethany.d2s
/Saves/1.09b/Bethany/Mule_A.d2s
/Saves/1.09b/Bethany/Mule_B.d2s
/Saves/1.09b/Bethany/Mule_C.d2s"><pre><code>/Platforms/1.09b/

/Saves/1.09b/

/Saves/1.09b/Isaac/
/Saves/1.09b/Isaac/Isaac.d2s
/Saves/1.09b/Isaac/Mule_A.d2s
/Saves/1.09b/Isaac/Mule_B.d2s
/Saves/1.09b/Isaac/Mule_C.d2s

/Saves/1.09b/Bethany/
/Saves/1.09b/Bethany/Bethany.d2s
/Saves/1.09b/Bethany/Mule_A.d2s
/Saves/1.09b/Bethany/Mule_B.d2s
/Saves/1.09b/Bethany/Mule_C.d2s
</code></pre></div>
<p dir="auto">This is just one of the new workflows that our Templating &amp; Labeling
System enables. This workflow requires a modified <strong><code>D2gfx.dll</code></strong> to
allow multiple instances of the game, allowing you to mule between
your main character and your mules via LAN. Singling provides this
feature for the versions it supports. For all other versions, you'll
need to find a copy of it online.</p>
<p dir="auto">Another workflow which I really like is using this labeling system to
separate my <strong><code>Classic</code></strong> and <strong><code>Expansion</code></strong> characters. By using two
separate labels to the same platform, we can have two separate save
paths for them. If we did this, we would have the following:</p>
<div data-snippet-clipboard-copy-content="/Platforms/1.09b/

/Saves/1.09b/

/Saves/1.09b/Classic/

/Saves/1.09b/Expansion/

/Saves/1.09b/Isaac/
/Saves/1.09b/Isaac/Isaac.d2s
/Saves/1.09b/Isaac/Mule_A.d2s
/Saves/1.09b/Isaac/Mule_B.d2s
/Saves/1.09b/Isaac/Mule_C.d2s

/Saves/1.09b/Bethany/
/Saves/1.09b/Bethany/Bethany.d2s
/Saves/1.09b/Bethany/Mule_A.d2s
/Saves/1.09b/Bethany/Mule_B.d2s
/Saves/1.09b/Bethany/Mule_C.d2s"><pre><code>/Platforms/1.09b/

/Saves/1.09b/

/Saves/1.09b/Classic/

/Saves/1.09b/Expansion/

/Saves/1.09b/Isaac/
/Saves/1.09b/Isaac/Isaac.d2s
/Saves/1.09b/Isaac/Mule_A.d2s
/Saves/1.09b/Isaac/Mule_B.d2s
/Saves/1.09b/Isaac/Mule_C.d2s

/Saves/1.09b/Bethany/
/Saves/1.09b/Bethany/Bethany.d2s
/Saves/1.09b/Bethany/Mule_A.d2s
/Saves/1.09b/Bethany/Mule_B.d2s
/Saves/1.09b/Bethany/Mule_C.d2s
</code></pre></div>
<p dir="auto">You can pretty much label your platforms whatever you want as long as
the name can be used for the folder on your hard drive. If you are using
some illegal symbols like <strong><code>/</code></strong>, then Cactus will properly detect that
and give you the appropriate message so that you can fix it. You can
also omit the label if you want and that works perfectly fine with the
above scenarios. Let's say you wanted to have a <strong><code>1.09b</code></strong> platform and
have everything in there without caring about labels (essentially a flat
layout, although you can also have a flat layout with a label, it just
depends on how you want to organize stuff), go ahead and create an entry
with the platform name <strong><code>1.09b</code></strong> and leave the label blank. Launching
the game will just point the save path to the <strong><code>/Saves/1.09b/</code></strong> folder
and your files will be placed in there. Assuming we then created a
character called <strong><code>Leslie</code></strong>, we would then have the following
structure:</p>
<div data-snippet-clipboard-copy-content="/Platforms/1.09b/

/Saves/1.09b/
/Saves/1.09b/Leslie.d2s

/Saves/1.09b/Classic/

/Saves/1.09b/Expansion/

/Saves/1.09b/Isaac/
/Saves/1.09b/Isaac/Isaac.d2s
/Saves/1.09b/Isaac/Mule_A.d2s
/Saves/1.09b/Isaac/Mule_B.d2s
/Saves/1.09b/Isaac/Mule_C.d2s

/Saves/1.09b/Bethany/
/Saves/1.09b/Bethany/Bethany.d2s
/Saves/1.09b/Bethany/Mule_A.d2s
/Saves/1.09b/Bethany/Mule_B.d2s
/Saves/1.09b/Bethany/Mule_C.d2s"><pre><code>/Platforms/1.09b/

/Saves/1.09b/
/Saves/1.09b/Leslie.d2s

/Saves/1.09b/Classic/

/Saves/1.09b/Expansion/

/Saves/1.09b/Isaac/
/Saves/1.09b/Isaac/Isaac.d2s
/Saves/1.09b/Isaac/Mule_A.d2s
/Saves/1.09b/Isaac/Mule_B.d2s
/Saves/1.09b/Isaac/Mule_C.d2s

/Saves/1.09b/Bethany/
/Saves/1.09b/Bethany/Bethany.d2s
/Saves/1.09b/Bethany/Mule_A.d2s
/Saves/1.09b/Bethany/Mule_B.d2s
/Saves/1.09b/Bethany/Mule_C.d2s
</code></pre></div>
<p dir="auto">It's just another folder after all ;). The nice thing about this is that
since all of these are sharing the same platform, switching between
these entries is extremely fast since no files need to change on your
hard drive, but rather we simply just update the registry save path, and
you are back in the action. Have fun!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Backup System</h2><a id="user-content-backup-system" aria-label="Permalink: Backup System" href="#backup-system"></a></p>
<p dir="auto">By clicking the <strong><code>Backup</code></strong> button, Cactus will automatically create a backup
of the following files in the <strong><code>Backups</code></strong> directory, inside your
<strong><code>Diablo II Root Directory</code></strong>:</p>
<ul dir="auto">
<li><strong><code>/Platforms/</code></strong></li>
<li><strong><code>/Saves/</code></strong></li>
<li><strong><code>/Entries.json</code></strong></li>
<li><strong><code>/LastRequiredFiles.json</code></strong></li>
<li><strong><code>/Settings.json</code></strong></li>
</ul>
<p dir="auto">The <strong><code>Backups</code></strong> directory is considered a <strong><code>Protected Directory</code></strong> by Cactus,
and will not be deleted.</p>
<p dir="auto">Lastly, you can change the backup location to any location you have write access
to, by modifying the <strong><code>Backups Directory</code></strong> setting in the <strong><code>Settings</code></strong>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Using Cactus like a Service</h2><a id="user-content-using-cactus-like-a-service" aria-label="Permalink: Using Cactus like a Service" href="#using-cactus-like-a-service"></a></p>
<p dir="auto">Cactus has some basic tracking of processes that it launches in order to
ensure that there are no accidental launches of other versions or
combinations of the game that would cause your Save Path location to be
changed mid-game. Thus it is essential for Cactus to remain running
while you are playing. If you are always going to be running Diablo II
via Cactus, you may want to go into the Cactus Settings and toggle the
<strong><code>Minimize to System Tray</code></strong> option so that it doesn't take up space in
your taskbar. I personally like having it show in the taskbar but that's
just preference ;D.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Moving Cactus To A New Computer</h2><a id="user-content-moving-cactus-to-a-new-computer" aria-label="Permalink: Moving Cactus To A New Computer" href="#moving-cactus-to-a-new-computer"></a></p>
<p dir="auto">If you want to move all of your Platforms, Characters, and Diablo II folder
to another machine, you will need to:</p>
<ol dir="auto">
<li>Copy your entire Diablo II folder to your new computer.</li>
<li>Open <strong><code>Cactus</code></strong>.</li>
<li>Click <strong><code>Settings</code></strong> and change your <strong><code>Diablo II Root Directory</code></strong> to match the location on your new computer.</li>
<li>Click <strong><code>Reset</code></strong>.</li>
<li>Now <strong><code>Launch</code></strong> whatever Platform you want.</li>
</ol>
<p dir="auto">Clicking <strong><code>Reset</code></strong> will cause Cactus to reconfigure itself by removing
some files from your <strong><code>Diablo II Root Directory</code></strong> and wiping the
<strong><code>Last Ran</code></strong> box on the entry that has it. Once you launch the game,
the registry will point to the appropriate save location, and your platform
files will be copied back to your Diablo II root folder. In some rare cases
(only on first install), you may need to do a little bit of manual organization
in your Diablo II root folder to get things aligned properly. Once aligned, it's
all tracked and automatic.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Unlocking All Cinematics</h2><a id="user-content-unlocking-all-cinematics" aria-label="Permalink: Unlocking All Cinematics" href="#unlocking-all-cinematics"></a></p>
<p dir="auto">If you moved Cactus to a new computer, or you did a fresh install, you can unlock
all of the cinematics by doing the following:</p>
<ol dir="auto">
<li>Launch Diablo II, and then close it.</li>
<li>Open the Registry Editor (<strong><code>regedit.exe</code></strong>).</li>
<li>Go to <strong><code>Computer\HKEY_CURRENT_USER\SOFTWARE\Blizzard Entertainment\Diablo II</code></strong>.</li>
<li>Set the <strong><code>Aux Battle.net</code></strong> key to <strong><code>216.148.246.35</code></strong>.</li>
<li>Congrats! All of your cinematics are now unlocked.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Updating Files In The Platforms folder</h2><a id="user-content-updating-files-in-the-platforms-folder" aria-label="Permalink: Updating Files In The Platforms folder" href="#updating-files-in-the-platforms-folder"></a></p>
<p dir="auto">If you update any files in your Platforms folder, then click the
<strong><code>Reset</code></strong> button, and run it again. This will cause Cactus to
re-install the files with the new ones.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">OMAHGOD! My Characters Are Gone! Cactus Deleted Them!!!</h2><a id="user-content-omahgod-my-characters-are-gone-cactus-deleted-them" aria-label="Permalink: OMAHGOD! My Characters Are Gone! Cactus Deleted Them!!!" href="#omahgod-my-characters-are-gone-cactus-deleted-them"></a></p>
<p dir="auto">Cactus comes with <a href="https://github.com/fearedbliss/Cactus-Core/blob/master/Cactus/FileGenerator.cs">built in safety features</a>
specifically designed to protect critical directories and files, which
includes the save directories. Thus it is impossible for Cactus to have
deleted them. Cactus also only operates within the Diablo II root
directory so it also wouldn't be possible for Cactus to delete saves
that are in <strong><code>1.14d+</code></strong>'s new save directory that is in your personal
folder.</p>
<p dir="auto">Since Cactus is <strong><code>A Modern Version Switcher, Character Isolator, and Mod Manager</code></strong>,
it will update the registry location of where the game should look for the
saves. For example, if you are playing a <strong><code>Platform</code></strong> called
<strong><code>1.05b</code></strong> with a <strong><code>Label</code></strong> called <strong><code>Chinchilla</code></strong>, the files for
this platform would logically be located under <strong><code>Platforms/1.05b/</code></strong>,
and the saves would be located under <strong><code>Saves/1.05b/Chinchilla/</code></strong>. Both
directories are located inside your Diablo II folder. Thus, when the
game starts, your characters are properly isolated and protected. If
this is the first time you launched a game with Cactus, and you
previously just had a regular Diablo II installation, then it would seem
as if all your characters got deleted, or magically dissapeared.
However, they are simply located in the original location that your
computer saved them to. If you were playing <strong><code>1.14d+</code></strong>, they most
likely are located at:</p>
<p dir="auto"><strong><code>%USERPROFILE%/Saved Games/Diablo II</code></strong></p>
<p dir="auto">If you were playing <strong><code>1.13d</code></strong> or below, they are inside the Diablo II
folder itself under a folder called <strong><code>save</code></strong>.</p>
<p dir="auto">Lastly, always remember to keep backups when running Third Party Tools
or Modifications.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cactus switches versions but I don't see the Diablo II window and there are no errors either.</h2><a id="user-content-cactus-switches-versions-but-i-dont-see-the-diablo-ii-window-and-there-are-no-errors-either" aria-label="Permalink: Cactus switches versions but I don't see the Diablo II window and there are no errors either." href="#cactus-switches-versions-but-i-dont-see-the-diablo-ii-window-and-there-are-no-errors-either"></a></p>
<p dir="auto">If switching versions with Cactus doesn't actually launch the game but you also
don't notice any errors, this could be an indication that either your Video Settings
are not correct, or that you may need to run Cactus in Admin Mode. I've noticed that
if I have Diablo II installed on the <strong><code>C:\</code></strong> drive (i.e <strong><code>C:\Games\Diablo II</code></strong>),
I would need to run Cactus at least once in Admin Mode for it to work properly, but
if I installed Diablo II on another drive (i.e <strong><code>D:\Games\Diablo II</code></strong>), it would
work fine without Admin privileges. I'm pretty sure this is due to the <strong><code>C:\</code></strong>
drive generally being a protected drive.</p>
<p dir="auto">Another thing to note is that I observed that I only needed to run Cactus once in Admin
Mode for this to "stick" and continue working even if I opened Cactus in the future without
Admin rights, although I haven't tested if this persists across reboots, but it possibly may.
I've also noticed that even when there was a problem, some versions would work,
and some wouldn't. Specifically versions <strong><code>1.00 - 1.06b</code></strong> worked, but <strong><code>1.07 - 1.13d</code></strong>
didn't.</p>
<p dir="auto">Lastly, make sure that there are no zombie Diablo II processes running in the background (Task Manager),
that can cause the version switcher to either not switch away or something else. I know that
the new telemetry executables added to <strong><code>1.14d</code></strong> (i.e <strong><code>SystemSurvey.exe</code></strong> and <strong><code>BlizzardError.exe</code></strong>)
are not needed for the game to actually function, and can lock the process for a bit after you close
the game. The Cactus platforms do not contain these files since I've deleted them, however, if you
are installing from a fresh copy of Diablo II from Blizzard, it will have them.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">File Read Error on Diablo II Start Up</h2><a id="user-content-file-read-error-on-diablo-ii-start-up" aria-label="Permalink: File Read Error on Diablo II Start Up" href="#file-read-error-on-diablo-ii-start-up"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9fcc249b278b9e2e694aa8faccbe1f57604a8139160791836b8d4f6d15347ea0/68747470733a2f2f692e696d6775722e636f6d2f44514d63486c4d2e706e67"><img src="https://camo.githubusercontent.com/9fcc249b278b9e2e694aa8faccbe1f57604a8139160791836b8d4f6d15347ea0/68747470733a2f2f692e696d6775722e636f6d2f44514d63486c4d2e706e67" alt="Error" data-canonical-src="https://i.imgur.com/DQMcHlM.png"></a></p>
<p dir="auto">If you are receiving the above error, it may be possible that some of
your MPQs are still hidden from Cactus' previous behavior before version
<strong><code>2.2.0</code></strong>. For newer versions of Cactus, Cactus will automatically
rename the following MPQs back to normal whenever you either
<strong><code>Launch</code></strong> a platform, or if you already have an entry that was
<strong><code>Last Ran</code></strong>, when you press the <strong><code>Reset</code></strong> button as well. If for
whatever reason that still doesn't work, you can go to your Diablo II
root directory and make sure that the following <strong><code>4</code></strong> Expansion MPQs
are properly named: <strong><code>d2exp.mpq</code></strong>, <strong><code>d2xvideo.mpq</code></strong>,
<strong><code>d2xmusic.mpq</code></strong>, and <strong><code>d2xtalk.mpq</code></strong>. If you see any of them with
the <strong><code>.bak</code></strong> extension, simply remove that extension and everything
should be good. If you are receiving this error but those files are in
place, then it is something else not related to Cactus.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Game Screenshots</h2><a id="user-content-game-screenshots" aria-label="Permalink: Game Screenshots" href="#game-screenshots"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2bcd27051aea669d8263a15495987d25d23905e23599f853d0b66576859bc5ab/68747470733a2f2f692e696d6775722e636f6d2f7575674d6e34382e6a7067"><img src="https://camo.githubusercontent.com/2bcd27051aea669d8263a15495987d25d23905e23599f853d0b66576859bc5ab/68747470733a2f2f692e696d6775722e636f6d2f7575674d6e34382e6a7067" alt="1.00" data-canonical-src="https://i.imgur.com/uugMn48.jpg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b67074c3e3461a80c50335a0568122279399990288cb7ccbe8b75ec315290225/68747470733a2f2f692e696d6775722e636f6d2f393031753456372e6a7067"><img src="https://camo.githubusercontent.com/b67074c3e3461a80c50335a0568122279399990288cb7ccbe8b75ec315290225/68747470733a2f2f692e696d6775722e636f6d2f393031753456372e6a7067" alt="1.05b" data-canonical-src="https://i.imgur.com/901u4V7.jpg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ab11b25948b1a162e3343f79e6d8684c5ecf6243977f5dd7da4beb9596f90b10/68747470733a2f2f692e696d6775722e636f6d2f7a48443973354c2e6a7067"><img src="https://camo.githubusercontent.com/ab11b25948b1a162e3343f79e6d8684c5ecf6243977f5dd7da4beb9596f90b10/68747470733a2f2f692e696d6775722e636f6d2f7a48443973354c2e6a7067" alt="1.07" data-canonical-src="https://i.imgur.com/zHD9s5L.jpg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ec17e02d14eea316670ba382bc53ef47b99457ba0ae865c60d24ab56524d3900/68747470733a2f2f692e696d6775722e636f6d2f735569464b714e2e6a7067"><img src="https://camo.githubusercontent.com/ec17e02d14eea316670ba382bc53ef47b99457ba0ae865c60d24ab56524d3900/68747470733a2f2f692e696d6775722e636f6d2f735569464b714e2e6a7067" alt="1.08" data-canonical-src="https://i.imgur.com/sUiFKqN.jpg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7e8fadc6127f9e29626aeb55efbb80bc7ddf5d103bf8c86c31bc8c77419fc248/68747470733a2f2f692e696d6775722e636f6d2f4a5a3962494f792e6a7067"><img src="https://camo.githubusercontent.com/7e8fadc6127f9e29626aeb55efbb80bc7ddf5d103bf8c86c31bc8c77419fc248/68747470733a2f2f692e696d6775722e636f6d2f4a5a3962494f792e6a7067" alt="1.09b" data-canonical-src="https://i.imgur.com/JZ9bIOy.jpg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4ed78562f5126584f8220ac99fa296c1c7d6376a204b670f53fa0da305394cf7/68747470733a2f2f692e696d6775722e636f6d2f5677347961444d2e6a7067"><img src="https://camo.githubusercontent.com/4ed78562f5126584f8220ac99fa296c1c7d6376a204b670f53fa0da305394cf7/68747470733a2f2f692e696d6775722e636f6d2f5677347961444d2e6a7067" alt="1.10" data-canonical-src="https://i.imgur.com/Vw4yaDM.jpg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ebd5045a12814db5f119bf833c358d294227de3ce920d1dd8479890338803b1f/68747470733a2f2f692e696d6775722e636f6d2f586951686558792e6a7067"><img src="https://camo.githubusercontent.com/ebd5045a12814db5f119bf833c358d294227de3ce920d1dd8479890338803b1f/68747470733a2f2f692e696d6775722e636f6d2f586951686558792e6a7067" alt="1.13d" data-canonical-src="https://i.imgur.com/XiQheXy.jpg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/581336626e892ea8db655cf3923a15d680ec730d7548005d61f3aca7fabe3aa3/68747470733a2f2f692e696d6775722e636f6d2f706f6a71334a772e6a7067"><img src="https://camo.githubusercontent.com/581336626e892ea8db655cf3923a15d680ec730d7548005d61f3aca7fabe3aa3/68747470733a2f2f692e696d6775722e636f6d2f706f6a71334a772e6a7067" alt="1.14d" data-canonical-src="https://i.imgur.com/pojq3Jw.jpg"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Jalapeño Peppers Are Less Spicy Than Ever (2023) (253 pts)]]></title>
            <link>https://www.dmagazine.com/food-drink/2023/05/why-jalapeno-peppers-less-spicy-blame-aggies/</link>
            <guid>39517145</guid>
            <pubDate>Mon, 26 Feb 2024 21:50:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dmagazine.com/food-drink/2023/05/why-jalapeno-peppers-less-spicy-blame-aggies/">https://www.dmagazine.com/food-drink/2023/05/why-jalapeno-peppers-less-spicy-blame-aggies/</a>, See on <a href="https://news.ycombinator.com/item?id=39517145">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>It’s not just you: jalapeño peppers are less spicy and less predictable than ever before. As heat-seekers chase ever-fiercer varieties of pepper—Carolina reapers, scorpions, ghosts—the classic jalapeño is going in the opposite direction. And the long-term “de-spicification” of the jalapeño is a deliberate choice, not the product of a bad season of weather.</p>
<p>This investigation began in my own kitchen. After months of buying heat-free jalapeños, I started texting chefs around Dallas to see if they were having the same experience. Many agreed. One prominent chef favors serranos instead. Regino Rojas of <a href="https://directory.dmagazine.com/restaurants/revolver-taco-lounge/" target="_blank" rel="noreferrer noopener">Revolver Taco Lounge</a> suggested jalapeños are now “more veggie-like than chile.” Luis Olvera, owner of Trompo, said that jalapeños now have so much less heat that “I tell my staff, ‘I think my hands are just too damn sweet,’ because I can’t make salsa spicy enough anymore.”</p>
<p>To be fair, not everyone agreed with these views. One restaurateur wondered if jalapeños seem less hot because diners have become infatuated with habaneros and serranos. Wayne White, general manager at Hutchins BBQ, offered a middle ground. “I noticed during covid, the quality got really bad, but now to me they’re beautiful,” he said. “We did have a season during covid, you could tell they were pulling them too soon, they weren’t that ripe. But I ate a whole jalapeño the other day, just to eat one, and it lit me up.”</p>
<p>I searched the internet to see whether jalapeños are really getting milder, but only found shopping tips. Gardening websites offered savvier advice: that peppers grow hotter under stress. If they’re well-watered, they won’t produce as much capsaicin, the chemical that generates the sensation we know as spiciness. But even this explanation leaves unanswered questions. My sunny backyard, which produces ferocious peppers, is one thing. What about all the peppers in the grocery store?</p>
<p>Clearly, a real investigation was required. So I called Stephanie Walker, extension vegetable specialist at New Mexico State University, advisory board member of that university’s <a href="https://cpi.nmsu.edu/">Chile Pepper Institute</a>, and chair of the 2023 New Mexico Chile Conference.</p>
<p>“Other complaints have come my way,” Walker said at the start of our phone call. This turned out to be a comedic understatement: she has a massive, existential complaint about the state of the chile pepper industry. I got on the phone expecting to hear a prosaic story of weather patterns shifting, unusual rains in pepper-growing regions, or the spread of greenhouses. I would not have been surprised if she validated Rojas’ theory: that jalapeños are now grown to look pretty, shiny, and big, regardless of flavor. “Pesticides and other enhancing farming elements make them look beautiful but not really spicy,” Rojas suggested to me. </p>
<blockquote>
<p>People lost a lot of interest in tomatoes for a long time until heirlooms came back. Now we have the same thing with peppers.</p>
<cite>Stephanie Walker, New Mexico State University Chile Pepper Institute</cite></blockquote>
<p>There’s truth to all these theories, but Walker says they are only secondary factors.</p>
<p>“As more growers have adopted drip irrigation, more high-tech farming tools to grow the peppers, they’ll tend to be milder,” Walker told me first, as a sort of throat-clearing exercise before the real explanation. “But there’s more to it than that.”</p>
<p>The truth is more like a vast industrial scheme to make the jalapeño more predictable—and less hot.</p>
<h2 id="h-the-vast-jalapeno-conspiracy">The Vast Jalapeño Conspiracy</h2>
<p>Most jalapeños go straight to factories, for canned peppers, pickled pepper rings, salsas, cream sauces, dressings, flavored chips and crackers, dips, sausages, and other prepared foods. For all those companies, consistency is key. Think about the salsa world’s “mild,” “medium,” and “hot” labels.</p>
<p>According to <em>The Mexican Chile Pepper Cookbook</em> by Dave DeWitt and José Marmolejo, 60 percent of jalapeños are sent to processing plants, 20 percent are smoke-dried into chipotles, and just 20 percent are sold fresh. Since big processors are the peppers’ main consumers, big processors get more sway over what the peppers taste like.</p>
<p>“It was a really big deal when breeders [told the industry], ‘hey, look, I have a low-heat jalapeño,’ and then a low-heat but high-flavor jalapeño,” Walker explained. “That kind of became the big demand for jalapeños—low heat jalapeños—because most of them are used for processing and cooking. [Producers] want to start with jalapeños and add oleoresin capsicum.”</p>
<div>
<figure>
<picture>
<img decoding="async" loading="lazy" srcset="https://assets.dmagstatic.com/wp-content/uploads/2015/09/texasfriedbaconjalapenobites.jpg 480w, https://assets.dmagstatic.com/wp-content/uploads/2015/09/texasfriedbaconjalapenobites.jpg 768w" src="https://assets.dmagstatic.com/wp-content/uploads/2015/09/texasfriedbaconjalapenobites.jpg" alt="Photo of jalapeño pepper fritters from the Texas State Fair.">
</picture>
<figcaption>
<span>Would this jalapeño fritter from the 2015 State Fair of Texas taste less spicy if it was made today?</span>
</figcaption>
</figure>
</div>
<p>Oleoresin capsicum is an extract from peppers, containing pure heat. It’s the active ingredient in pepper spray. It’s also the active ingredient, in a manner of speaking, for processed jalapeños. The salsa industry, Walker said, starts with a mild crop of peppers, then simply adds the heat extract necessary to reach medium and hot levels. She would know; she started her career working for a processed-food conglomerate.</p>
<p>“I’ve worked in peppers in my entire life,” she told me. “Jalapeños were originally prized as being a hot pepper grown in the field. When we were making hot sauce in my previous job, we had the same problem, that you couldn’t predict the heat. When you’re doing a huge run of salsa for shipment, and you want a hot label, medium label, mild label, it’s really important to predict what kind of heat you’ll get. We tried a statistical design from the fields, and it just didn’t work, because mother nature throws stressful events at you or, sometimes, does not bring stress.”</p>
<p>The standardization of the jalapeño was rapidly accelerated by the debut, about 20 years ago, of the TAM II jalapeño line, a reliably big, shiny, fleshy pepper that can grow up to six inches long—with little to no heat. TAM II peppers have become some of the most popular in the processing business. The <a href="https://journals.ashs.org/hortsci/view/journals/hortsci/37/6/article-p999.xml">2002 paper</a> in <em>HortScience</em> trumpeted TAM II’s benefits: virus resistance, absence of dark spots, longer fruit with thicker flesh, earlier maturation, and, compared to a variety of jalapeño called Grande, less than 10 percent of the spiciness. TAMs grown in one location measured in at 1620 Scoville units, while those at another came in at just 1080, which is <a href="https://www.alimentarium.org/en/story/scoville-scale">milder than a poblano</a>.</p>
<p>In conclusion, the paper’s authors wrote, “The large, low-pungency fruit of ‘TMJ II’ will make it equally suited for fresh-market and processing uses.”</p>
<p>DeWitt, writing in his solo book <em>Chile Peppers: A Global History</em>, says TAM became widespread in Texas after its introduction. “It was much milder and larger than the traditional jalapeños, and genes of this mild pepper entered the general jalapeño pool. Cross-breeding caused the gene pool to become overall larger and milder.”</p>
<p>Since I know you’re wondering who the inventors are: the clue is in the name TAM II. The hot (but also not hot) new jalapeño is an invention of Texas A&amp;M University. Yes, Aggies took the spice out of life.</p>
<div>
<figure>
<picture>
<img decoding="async" loading="lazy" srcset="https://assets.dmagstatic.com/wp-content/uploads/2021/09/Hutchins-bbq.jpg 480w, https://assets.dmagstatic.com/wp-content/uploads/2021/09/Hutchins-bbq.jpg 768w" src="https://assets.dmagstatic.com/wp-content/uploads/2021/09/Hutchins-bbq.jpg" alt="Picture of a barbecue tray from Hutchins BBQ, including a bacon-wrapped jalapeño pepper.">
</picture>
<figcaption>
<span>A tray from Hutchins BBQ, including a Texas Twinkie.</span>
<span>Bret Redman</span>
</figcaption>
</figure>
</div>
<p>And yes, “II” means it’s a sequel. The original TAM came out much earlier and was profiled in <a href="https://www.csmonitor.com/1983/0607/060702.html">a 1983 article</a> in the <em>Christian Science Monitor</em>. At the time, the A&amp;M scientists estimated 800 acres were being grown nationally, and they told reporter Daniel Benedict that there was plenty of room left on the market for spicier stuff. (“For the hot-pepper lover, there’s something for him already.”) </p>
<p>After 40 years of the milder pepper enjoying increased popularity, virus resistance, higher yields, and a shiny new sequel, hotter pre-TAM jalapeños appear to have lost substantial ground. Exact statistics on planting demand are hard to obtain because growers do not want to tip off seed suppliers on how to price their products.</p>
<p>As the invention of TAM I and II suggests, “jalapeño” as a name does not connote a single breed or genetic line. There are varieties of jalapeño as there are of tomatoes. Mitla peppers are at the opposite end of the scale from TAMs, sometimes reaching 8000 Scoville units. (The A&amp;M paper derides Mitlas since they are often wonkily curved, and need more culling.) </p>
<p>In my interviews around Dallas, I learned many restaurateurs don’t know what breed their supplier is offering, or even that various breeds exist. At Hutchins BBQ, which employs four people full-time preparing around 7,000 jalapeños a week for its iconic brisket-stuffed Texas Twinkies, suppliers drop off peppers and the barbecue joint sorts through, picking the specimens they want and returning the rest. Hutchins deseeds the peppers to reduce any remaining heat.</p>
<p>For heat seekers, Walker recommends Mitla and Early jalapeños; they’re called “Early” not because they were picked early but because, as a breed, they grow quickly and are well-adapted to cooler environments.</p>
<h2>First heirloom tomatoes, next heirloom peppers?</h2>
<p>Walker compares the current state of the pepper industry with the world of American tomatoes, which were bred for hardiness in shipping, firmness, and canning. Only recently has an heirloom tomato revolution tried to cater directly to home cooks and chefs with tomato breeds that emphasize flavor and juiciness first.</p>
<p>“People lost a lot of interest in tomatoes for a long time until heirlooms came back,” Walker said. “Now we have the same thing with peppers. There’s a place for people to embrace heirloom peppers, the way that we have with tomatoes.”</p>
<p>For gardeners and small growers, the <a href="https://chilepepperinstitute.ecwid.com/Seeds-c85441005">Chile Pepper Institute</a> sells seeds but results will always be complicated, since a hot, dry summer can turn even TAM jalapeños into weapons, and a cool, wet season will result in pampered plants. But how can you find hotter peppers if you are shopping, or looking to supply your restaurant? </p>
<p>Walker’s best advice is to lobby suppliers and grocers for specific pepper breeds. Ask a produce manager or a supplier if you can get Early or Mitla peppers, or if the store can label its pepper breeds. And ignore the bogus factoids spread by many online shopping guides. I found a <a href="https://www.rachaelrayshow.com/articles/the-trick-to-picking-a-really-spicy-or-less-spicy-jalapeno-pepper">Rachael Ray Show</a> article claiming that bigger peppers are always spicier than smaller ones—which contradicts everything I had just learned about TAMs being deliberately engineered for size. Walker called that tip “misinformation.”</p>
<p>If lobbying your grocery managers sounds like a futile effort, look at the changes that have rippled through the tomato industry as breeders re-embrace heirlooms. Or look at the widespread adoption of a <a href="https://www.npr.org/sections/thesalt/2019/10/30/773457637/from-culinary-dud-to-stud-how-dutch-plant-breeders-built-our-brussels-sprouts-bo">less stinky breed of Brussels sprouts</a>, scientifically developed through a similar selective breeding process, which turned that vegetable from a punchline into a favorite.</p>
<p>“I think it’s a great opportunity for growers who really want to get into specializing in some of these heirloom varieties,” Walker said. </p>
<p>Let’s hope some farmers are reading this and yearning for the days when a jalapeño was a reliable source of spice. Those days can return.</p>
<div>
<h3>
<span>Get the SideDish Newsletter</span>
</h3>
<p>
Dallas' hottest dining news, recipes, and reviews served up fresh to your inbox each week.
</p>
</div>
<div>
<h3>
<span>Author</span>
</h3>
<div>
<figure>
<img loading="lazy" src="https://assets.dmagstatic.com/wp-content/uploads/2022/04/brian-reinhart-headshot-150x150.png" alt="Brian Reinhart">
</figure>
<div>
<h3>
<span>Brian Reinhart</span>
</h3>
<p><a href="https://www.dmagazine.com/writers/brian-reinhart/" target="_self">
View Profile
<span aria-label="Arrow">
<svg width="32" height="16" viewBox="0 0 32 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M25 0.929688L23.5 2.42969L28.0703 7H0V9H28.0703L23.5 13.5703L25 15.0703L32.0703 8L25 0.929688Z"></path></svg> </span>
</a>
</p></div>
<p>
Brian Reinhart became D Magazine's dining critic in 2022 after six years of writing about restaurants for the <em>Dallas Observer</em> and the <em>Dallas Morning News.</em>
</p>
</div>
</div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral Remove "Committing to open models" from their website (180 pts)]]></title>
            <link>https://old.reddit.com/r/LocalLLaMA/comments/1b0o41v/top_10_betrayals_in_anime_history/</link>
            <guid>39517016</guid>
            <pubDate>Mon, 26 Feb 2024 21:36:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LocalLLaMA/comments/1b0o41v/top_10_betrayals_in_anime_history/">https://old.reddit.com/r/LocalLLaMA/comments/1b0o41v/top_10_betrayals_in_anime_history/</a>, See on <a href="https://news.ycombinator.com/item?id=39517016">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Hey all, here's my brief business analysis.</p>

<p>It's an interesting move. I see them pullling a 180 and reframing their go2market framework from open weights to GDPR-compliant and enterprise-friendly. That niche definitely exists right now - but surely quickly also will be a market to conquer for OpenAI aswell. All of OpenAI's efforts point in this direction.</p>

<p>Then Mistral will be in the situation to be much more dependent on Microsoft for their marketing via Azure, since it's their only major inbound channel, than vice-versa. And for Microsoft it will not make sense long-term to have multiple satellite enterprises competing for the same segment, overhead in infrastructure, etc. This is a small sized bet for Microsoft, but Microsoft is known to constantly seek for portfolio synergies and optimization. We might very well see Mistral become merged to OpenAI a year or two down the road.</p>

<p>Nevertheless, every actor in the industry knows the power and ingenuity of the Open Source and research community. I strongly believe that OSS nevertheless will have an extremely strong role to play.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New York medical school eliminates tuition after $1B gift (451 pts)]]></title>
            <link>https://www.bbc.com/news/world-us-canada-68407453</link>
            <guid>39516927</guid>
            <pubDate>Mon, 26 Feb 2024 21:25:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/world-us-canada-68407453">https://www.bbc.com/news/world-us-canada-68407453</a>, See on <a href="https://news.ycombinator.com/item?id=39516927">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="main-content" data-testid="main-content"><article><header></header><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 976w" type="image/webp"><img alt="Dr Ruth Gottesman in 2016" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 976w" src="https://ichef.bbci.co.uk/news/976/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg" width="976" height="549" loading="eager"></picture></span><span role="text"><span>Image source, </span>Getty Images</span></p><figcaption><span>Image caption, </span><p>Dr Ruth Gottesman studied learning disabilities and developed screening protocols at Albert Einstein College of Medicine.in the Bronx.</p></figcaption></figure></div><div data-component="text-block"><p><b>A New York City medical school will offer students free tuition following a $1bn donation from the 93-year-old widow of a major Wall Street investor. </b></p></div><div data-component="text-block"><p>The gift to Albert Einstein College of Medicine came from Dr Ruth Gottesman, a former professor at the Bronx school. </p></div><div data-component="text-block"><p>It is one of the largest ever donations made to a US school and is the largest ever made to a medical school. </p></div><div data-component="text-block"><p>The Bronx, New York City's poorest borough, is ranked as the unhealthiest of New York's 62 counties. </p></div><div data-component="text-block"><p>In a statement, university dean Dr Yaron Yomer said that the "transformational" gift "radically revolutionises our ability to continue attracting students who are committed to our mission, not just those who can afford it". </p></div><div data-component="text-block"><p>Tuition at the school is nearly $59,000 each year, leaving students with substantial debt.</p></div><div data-component="text-block"><p>The statement from Einstein noted students in their final year will be reimbursed for their spring 2024 tuition, and from August, all students, including those who are currently enrolled, will receive free tuition. </p></div><div data-component="text-block"><p>The donation "will free up and lift our students, enabling them to pursue projects and ideas that might otherwise be prohibitive", Dr Yomer added. </p></div><div data-component="text-block"><p>Dr Gottesman, now 93, began working at the school in 1968. She studied learning disabilities, ran literacy programmes and developed widely used screening and evaluation protocols.</p></div><div data-component="text-block"><p>Her late husband, David Gottesman, founded a prominent investment house and was an early investor in Berkshire Hathaway, Warren Buffet's multinational conglomerate. He died in September 2022 at the age of 96. </p></div><div data-component="text-block"><p>Dr Gottesman said in a statement that the doctors who train at Einstein go on to "provide the finest healthcare to communities here in the Bronx and all over the world".</p></div><div data-component="text-block"><p>"I am very thankful to my late husband, Sandy, for leaving these funds in my care, and l feel blessed to be given the great privilege of making this gift to such a worthy cause," she added.</p></div><div data-component="text-block"><p>About 50% of Einstein's first-year students are from New York, and approximately 60% are women. Statistics published by the school show that about 48% of its medical students are white, while 29% are Asian, 11% are Hispanic and 5% are black. </p></div><div data-component="text-block"><p>In an interview with the New York Times, she recalled that her late husband had left her a "whole portfolio of Berkshire Hathaway stock" when he died with the instructions to "do whatever you think is right with it". </p></div><div data-component="text-block"><p>"I wanted to fund students at Einstein so that they would receive free tuition," Dr Gottesman said she immediately realised. "There was enough money to do that in perpetuity." </p></div><div data-component="text-block"><p>She added that she occasionally wonders what her husband would have thought of the donation.</p></div><div data-component="text-block"><p>"I hope he's smiling and not frowning," she said. "He gave me the opportunity to do this, and I think he would be happy - I hope so." </p></div><section data-component="links-block"><p><h2>More on this story</h2></p></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Isn't Taxpayer-Funded U.S. Broadband Mapping Data Owned by the Public? (247 pts)]]></title>
            <link>https://www.techdirt.com/2024/02/26/why-isnt-taxpayer-funded-u-s-broadband-mapping-data-owned-by-the-public/</link>
            <guid>39516007</guid>
            <pubDate>Mon, 26 Feb 2024 19:49:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2024/02/26/why-isnt-taxpayer-funded-u-s-broadband-mapping-data-owned-by-the-public/">https://www.techdirt.com/2024/02/26/why-isnt-taxpayer-funded-u-s-broadband-mapping-data-owned-by-the-public/</a>, See on <a href="https://news.ycombinator.com/item?id=39516007">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-432775">


<h3>from the <i>this-is-why-we-can't-have-nice-things</i> dept</h3>

<p>We’ve&nbsp;<a href="https://www.techdirt.com/2022/01/06/shitty-us-broadband-maps-are-feature-not-bug/">noted for decades</a>&nbsp;how, despite all the political lip service paid toward “bridging the digital divide,” the U.S.&nbsp;doesn’t <strong>truly</strong> know where broadband is or isn’t available. The FCC’s past broadband maps, which cost $350 million to develop, have long been accused of all but hallucinating competitors, making up available speeds, and excluding a key metric of competitiveness: price.</p>
<p>You only need to spend a few minutes plugging your address into the FCC’s&nbsp;<a href="https://broadband477map.fcc.gov/#/">old map</a>&nbsp;to notice how the agency <strong>comically</strong> overstates broadband competition and available speeds. After being mandated by Congress in 2020 by the Broadband DATA Act, the FCC struck a new, $44 million contract with a company named Costquest to develop&nbsp;<a href="https://broadbandmap.fcc.gov/home">a new map</a>. </p>
<p>While an improvement, the new map still has problems with over-stating coverage and available speeds (<a href="https://broadbandmap.fcc.gov/home">try it for yourself</a>). And the FCC still refuses to collect and share pricing data, which industry opposes because it would only work to further highlight monopolization, consolidation, and muted competition. </p>
<p>But there’s another problem. As broadband industry consultant Doug Dawson notes, <a href="https://potsandpansbyccg.com/2024/02/14/shouldnt-broadband-mapping-data-belong-to-the-public/">the public doesn’t even own the finalized broadband mapping data</a>. Costquest does:</p>
<blockquote>
<p><em>“…the FCC gave CostQuest the ability to own the rights to the mapping fabric, which is the database that shows the location of every home and business in the country that is a potential broadband customer. This is a big deal because it means that CostQuest, a private company, controls the portal for data needed by the public to understand who has or doesn’t have broadband.”</em></p>
</blockquote>
<p>In addition to the $44.9 million the FCC paid Costquest to create the maps, Costquest received another <strong>$49.9 million</strong> from the NTIA to provide the databases and maps for the $42 billion broadband subsidy and grant program (included in the 2021 infrastructure bill). Third parties (like states trying to shore up access to affordable broadband) have to pay Costquest even more money to access the data. </p>
<p>So it’s all been incredibly profitable for Costquest. But <strong>taxpayers are closing in on paying nearly half a billion dollars for broadband maps that not only still aren’t fully accurate, but which they can’t transparently access and don’t own despite paying for. </strong></p>
<p>That’s fairly insane any way you slice it, and as Dawson notes, it’s a detriment to the cash-strapped folks who could be helping expand access to affordable broadband (and helping fact-check the data):</p>
<blockquote>
<p><em>“Our industry is full of data geeks who could work wonders if they had free access to the mapping fabric database. There are citizen broadband committees and retired folks in every community who are willing to sift through the mapping data to understand broadband trends and to identify locations where ISPs have exaggerated coverage claims. But citizens willing to do this research are not going to pay the fees to get access to the data – and shouldn’t have to.”</em></p>
</blockquote>
<p>For decades, feckless and corrupt state and federal regulators turned a blind eye as regional telecom monopolies dominated the market and crushed all competition underfoot, resulting in spotty access, high prices, and terrible customer service. Usually under the pretense that “deregulation” (read: very little real consumer protection oversight) had resulted in immense innovation. </p>
<p>Not only did government not address (or often even acknowledge) that problem, they’re still proving somewhat incapable when it comes to transparently mapping its impact. </p>
<p>The $42 billion in subsidies flowing to many states to shore up access <a href="https://www.techdirt.com/2023/06/27/biden-re-announces-42-billion-investment-in-broadband-because-apparently-people-didnt-notice-the-first-time/">is a good thing</a>, but its impact will most assuredly be corrupted by feckless bureaucrats who can’t stand up to industry giants, aren’t keen on the idea of data transparency, and will lack the courage necessary to ensure giant monopolies with a history of fraud (like Comcast and AT&amp;T) don’t <a href="https://communitynets.org/content/charter-comcast-continue-dominate-state-grant-awards">pocket most of the funds</a>. </p>
<p>
Filed Under: <a href="https://www.techdirt.com/tag/bead-grants/" rel="tag">BEAD grants</a>, <a href="https://www.techdirt.com/tag/broadband/" rel="tag">broadband</a>, <a href="https://www.techdirt.com/tag/broadband-mapping/" rel="tag">broadband mapping</a>, <a href="https://www.techdirt.com/tag/competition/" rel="tag">competition</a>, <a href="https://www.techdirt.com/tag/duopolies/" rel="tag">duopolies</a>, <a href="https://www.techdirt.com/tag/fcc/" rel="tag">fcc</a>, <a href="https://www.techdirt.com/tag/gigabit-fiber/" rel="tag">gigabit fiber</a>, <a href="https://www.techdirt.com/tag/high-speed-internet/" rel="tag">high speed internet</a>, <a href="https://www.techdirt.com/tag/mapping/" rel="tag">mapping</a>, <a href="https://www.techdirt.com/tag/subsidies/" rel="tag">subsidies</a>, <a href="https://www.techdirt.com/tag/telecom/" rel="tag">telecom</a>
<br>
Companies: <a href="https://www.techdirt.com/company/costquest/" rel="category tag">costquest</a>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[On Light, Colors, Mixing Paints, and Numerical Optimization (177 pts)]]></title>
            <link>https://github.com/miciwan/PaintMixing</link>
            <guid>39515478</guid>
            <pubDate>Mon, 26 Feb 2024 19:02:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/miciwan/PaintMixing">https://github.com/miciwan/PaintMixing</a>, See on <a href="https://news.ycombinator.com/item?id=39515478">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">On Light, Colors, Mixing Paints, and Numerical Optimization.</h2><a id="user-content-on-light-colors-mixing-paints-and-numerical-optimization" aria-label="Permalink: On Light, Colors, Mixing Paints, and Numerical Optimization." href="#on-light-colors-mixing-paints-and-numerical-optimization"></a></p>
<p dir="auto">This is a short write-up that is supposed to serve as a rough description of what's going on in the paint mixing tool in this depot.</p>
<p dir="auto">The tool is a virtual paint mixing tool and a solver that can generate recipes for creating a particular color out of existing paints. The tool comes with data for Kimera paints that I measured. The tool is a Python 3 program; it comes with all the sources, and if you have a Python distribution, you can just run it. There's also a Windows executable created with PyInstaller (see 'Releases', on the right). I can probably create a MacOS version too, if need be (edit: I actually added one; there's a .dmg file, and it does have something in it, and if you double-click it, it does show up, so it seems to work, but honestly, I barely use Mac, so it's hard for me to say if this is the right way, or is something more expected...)</p>
<p dir="auto">If you just want to grab the tool and play with it, that's about it! Have fun, and I hope you find it at least somewhat useful.</p>
<p dir="auto">But below, you'll find a more or less complete description of how it works (and when it doesn't). So, if you have a bit of time to spare, read on!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">Very recently, I discovered miniature painting. I was never really into WH40K or anything related, but I have some fond memories of playing pen &amp; paper RPGs years ago, and after watching a bunch of YouTube videos, I thought it looked easy enough to try. I still suck at it, but I somehow really enjoy the tranquilizing experience of putting thin layers of paint onto 3 cm tall figurines.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/1%20-%20figureJPG.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/1%20-%20figureJPG.JPG" width="70%"></a>
</p>
<p dir="auto">In my day job, I do real-time graphics engineering for video games, and it quickly turned out that a lot of the problems I deal with at work are very similar to those in miniature painting: you analyze how light behaves, how it interacts with different surfaces, how the eyes perceive it, etc. Of course, painting is not just engineering; it's <em>Art</em> after all (capital 'A'), but there seems to be a consensus that painters should understand these technical aspects, even just to know when they deliberately break them.</p>
<p dir="auto">There were a number of things that looked like fascinating research projects somewhere between miniatures and computer graphics, but one thing that sparked my particular interest was paints. Miniature paints usually come with these cryptic names: Skrag Brown, Tombstone Horror, or whatever. I don't really mind, but the producers never actually tell you what these colors actually are. And when you have limited experience, it's often hard to tell if a particular paint will work as some midtone or if it will be too dark. Many YouTube tutorial videos actually tell you which exact paints they use, but they most often come from different lines, some are immediately available, some are not, and for some, you need to wait - and I want to paint this very second! It seemed pretty clear that instead of buying all the possible paints, the more reasonable approach would be to pick some base paints and learn to mix them to get the colors that I need.</p>
<p dir="auto">For a beginner, there are, however, two problems. First: mixing paint is not a particularly intuitive process: sometimes you get something reasonable, sometimes you get muddy brown. Second: you need to know what color you actually want to get. Sure, there are some nice videos on how to color match, but if you don't have a good intuition of what skin tone you want to achieve, it's hard to tell if your mix needs more blue or red.</p>
<p dir="auto">Because of my engineering background, the solution seemed obvious: I would like to just pick a color on the screen (from, say, a photograph) and I want to know which paints, and how much of them, to mix to get it. I would also like to experiment with mixing paints without actually having to waste physical paint. For that, I need to somehow characterize the paints that I have, I need a model for simulating how they mix, and I need a numerical solver that will be able to minimize the error between the color that I want and a mix of some number of paints. These sorts of processes are something that I regularly go through and enjoy, so it looked like a perfect on-the-side project.</p>
<p dir="auto">Disclaimer here: yes, I know that in practice no one works that way. Especially if the solver gives you ratios like 88 parts of white, 3 parts of blue, and 2 parts of yellow - there's no way to mix something like this on a wet palette where you work with a minuscule amount of paint. But, at least to me, it's still useful to know that it's mostly white, with a touch of blue and yellow, so when I mix something on the palette, I'm not doing it completely blind. And yes, if you've been painting for some time, you learn these things, you get that intuition. But you need to get it somehow. Painting takes a lot of practice, so if I can do some experiments purely digitally, I'm totally up for it. And to be honest, it's all more of a cool side project rather than anything else.</p>
<p dir="auto">Just in case anyone else finds it useful, I thought I'll write up all the theoretical basis for a simple tool I developed for this and provide it together with a simple Python code. Since I just had a bit of free time (I got COVID), and I just got my set of Kimera paints (which are single pigment, so incredibly saturated colors, amazing for mixing), I spent a week on this, and you can read about the results here. As it might be read by people with a less technical background, I'm trying to keep it all pretty simple and self-contained, so all the information you need to understand it is here. I'm not sure how that worked out in the end, but if something is unclear, feel free to ping me and ask for details. None of it is actually any rocket science; it's mostly some high-school level math and physics (but if you're allergic, a warning: there is some math in there).</p>
<p dir="auto">So if you're curious about how it all works, details below.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Light</h2><a id="user-content-light" aria-label="Permalink: Light" href="#light"></a></p>
<p dir="auto">Light is an electromagnetic wave, oscillations of electrical and magnetic fields propagating in space. Human eyes are sensitive to wavelengths between roughly 400 and 700 nanometers, which we perceive as colors, from violet, through blue, green, yellow, orange, to red.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/2%20-%20spectrum.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/2%20-%20spectrum.JPG"></a>
</p>
<p dir="auto">Light that usually reaches our eyes is a mixture of many different wavelengths. Depending on the ratios between the amounts of particular wavelengths, we perceive the light as different colors. If it consists mostly of the shorter visible wavelengths, we'll see it as blue. If it's mostly longer wavelengths, it's going to be red. The more precise details are further down, but that's the general intuition.</p>
<p dir="auto">To reason about these characteristics in a more principled way, one useful tool is a so-called spectral power distribution (SPD for short). It's a function that, roughly speaking, describes how much of a particular wavelength is present in some radiation. It is usually plotted as a graph, with wavelength on the horizontal axis and some energy-related quantity on the vertical axis (so the stronger a particular wavelength is, the higher the plot).</p>
<p dir="auto">So the "generally blue" light might have an SPD like this:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/4%20-%20spectrum%20blue.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/4%20-%20spectrum%20blue.JPG" width="35%"></a>
</p>
<p dir="auto">and the "generally red" light might have it more like this:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/3%20-%20spectrum%20red.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/3%20-%20spectrum%20red.JPG" width="35%"></a>
</p>
<p dir="auto">One particularly interesting family of SPDs are those of different light sources. You can take any light source and measure how much of the light it produces comes from particular wavelengths. There's this thing in physics called black body radiation that describes the SPD of a perfectly black body (so that it doesn't reflect any light, just generates it) heated to a particular temperature (all that actually led straight to quantum mechanics and the world we know today; actually analyzing spectra of starlight led to the understanding that the distant stars produce energy just like the sun, the lines appearing in the spectra of excited gases was another catalyst in the evolution of quantum mechanics, and shift in the spectra of the light from different galaxies led to the discovery that the universe is expanding; it's all in the spectrum). If you've ever come across these "2700K light", "5000K light" markings, they are exactly that - they describe the light color as the color of a black body radiator of a given temperature, in Kelvin.</p>
<p dir="auto">The SPD of a typical ~2800K incandescent light looks like this:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/5%20-%202856K.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/5%20-%202856K.JPG" width="35%"></a>
</p>
<p dir="auto">and a ~4200K fluorescent light looks like this.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/6%20-%20fluorescent.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/6%20-%20fluorescent.JPG" width="35%"></a>
</p>
<p dir="auto">The SPD of what we consider sunlight is a fairly complex distribution that includes not only the actual SPD of the light generated by the sun but also the absorption and scattering of some of it occurring when the light passes through the atmosphere. Because the sunlight encounters different amounts of atmosphere on its way at different times of the day (less at noon, more in the evening), the sunlight SPD also depends on the time of day and atmospheric conditions. On a typical sunny day, it might look like this:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/7%20-%205000K.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/7%20-%205000K.JPG" width="35%"></a>
</p>
<p dir="auto">Because we're doing science here, everything needs to be standardized, measured, and quantified. For that reason, the CIE (International Commission on Illumination) introduced "standard illuminants" - a number of SPDs describing some very particular lights. Illuminant A represents a typical tungsten filament light bulb, a black-body radiator at 2856K. Illuminants B and C have become pretty much obsolete in favor of Illuminants D. There's a whole family of these; they describe daylight in different conditions - from more "warm" ones (D50, D55) to colder ones (D65, D70). The numbers 50/55/65/70 roughly correspond to a black-body temperature that would emit light of similar color (5000K, 5500K, 6500K, 7000K), but it's a longer topic and not particularly relevant here. There are also other illuminants (like E and F), but in most practical situations, the interesting ones are A, D50, and D65 (especially the last one).</p>
<p dir="auto">One last thing that seems fairly obvious, but is very important later on: light behavior is linear (in mathematical terms). If you take two lights with two SPDs and you turn them on at the same time, the resulting lighting will have an SPD that's the sum of the two components. If you make the light twice as bright, the resulting SPD will be two times greater.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Reflection</h2><a id="user-content-reflection" aria-label="Permalink: Reflection" href="#reflection"></a></p>
<p dir="auto">We rarely see light as it is generated by some source. Before it reaches our eyes, it usually bounces off things, and we register that indirect, reflected light.</p>
<p dir="auto">The way light interacts with surfaces is an incredibly complicated topic. The most basic principle is fairly simple and described by Fresnel's equations: light reaches the boundary between two mediums (say, air and an object) and some of it gets reflected off the boundary, and some of it gets refracted into the object. The angle between the reflected light and the normal to the surface (which is a direction perpendicular to the surface) is the same as the angle between the incident light and the normal (alpha on the figure below). How much of the light goes where, and the exact direction of the refracted light, depends on the index of refraction of both mediums (which describes how fast light travels in that particular medium compared to its speed in vacuum).</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/8%20-%20fresnel.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/8%20-%20fresnel.jpg"></a>
</p>
<p dir="auto">Unfortunately, this only describes reflection off a perfectly smooth, mirror surface - nothing like anything you see in reality. And it only describes the first reflection off a boundary. But light can bounce around off the microscopic roughnesses of the surface and go into the object in a different place. Or it can do it multiple times. Or it can go into the object, bounce around there, and go out (or not, there's a boundary when going outside to the air as well). And all this is ignoring any wave phenomena - diffraction, interference, etc.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/9%20-%20complex%20interactions.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/9%20-%20complex%20interactions.jpg"></a>
</p>
<p dir="auto">Physics, optics, and related fields have tried to simplify all these concepts and created multiple models for describing and quantifying these effects. Some are simpler, some are very complex. Computer graphics loves them because they allow us to render realistic-looking images on a computer.</p>
<p dir="auto">From the perspective of a miniature painter, the simplest way of looking at the light-material interaction is to split it into two components. I will call them "diffuse" and "specular" because these are the terms used in computer graphics, which I'm used to.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/10%20-%20diffuse%20spec.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/10%20-%20diffuse%20spec.jpg"></a>
</p>
<p dir="auto">The "specular" component of the reflection is everything that happens on the actual boundary between the air and the object. Some of the light bounces off it. Generally, it follows the law of reflection, so the angle between the direction the light falls onto the object and the normal is the same as the angle between the direction the light is reflected at and the normal (the normal is the direction perpendicular to the surface). I say "generally" because if the surface is not perfectly smooth, the light will be scattered in different directions - the rougher the surface, the more scattered it will be - but generally, it will be around that reflected direction. One very important bit: in the case of non-metal materials, light reflected this way does not change its color. The reflected light will have the same SPD as the one falling on the object. Interestingly, this behavior is very similar for most non-metals. To the extent that in computer graphics, we often just treat all non-metal surfaces the same way: they can be rough or smooth, but they reflect the same amount of light: no matter if it's plastic, skin, or concrete. It's a very decent approximation. Metals, due to their atomic structure, are different. When the light reflects off their surface in a specular reflection, it actually changes color. That's why gold is yellow and copper is orange.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/10.5%20-%20rough%20spec%20vs%20smooth%20spec.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/10.5%20-%20rough%20spec%20vs%20smooth%20spec.png" width="40%"></a>
</p>
<p dir="auto">The "diffuse" component is all the light that goes into the object and then some of it gets out and is, generally, scattered uniformly in all directions (or rather: let's just assume it for simplicity, that's a good enough approximation). It doesn't matter how the surface is viewed; its diffuse lighting is the same from all angles (unlike specular, which is strongly visible when viewed from that one particular direction, and not much when viewed from others). Not all the light that gets into the object gets scattered out. Some of it is absorbed and turned into heat. There's also an important difference between metals and non-metals when it comes to the diffuse component. For non-metals, the spectrum of the reflected light very much depends on the object: after all, the light goes into the surface, bounces around, and comes out - so it picks up some of the characteristics of the object. For metals, there's no diffuse component at all. The light doesn't go out; it's either absorbed or bounces off specularly. And even though, technically, the diffuse component is not "reflection", but rather a form of scattering occurring over short distances within the material, I will oftentimes just say "diffuse reflection" for simplicity.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/11%20-%20diffuse%20subsurface.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/11%20-%20diffuse%20subsurface.jpg"></a>
</p>
<p dir="auto">Side note: even though it's safe to treat the diffuse reflection as the same in all directions, its brightness still depends on the amount of light <em>falling</em> onto the given part of the object. And this amount is related to the angle between the normal of the surface and the direction towards the light: the bigger the angle, the less light the given part of the object receives (actually, the amount of light is the same; it's just distributed over a larger area, which makes "light per area" smaller, and this is what we consider "brightness"). In physics, this is called Lambert's law. In many miniature painting tutorials, people talk about shading basic shapes in a particular way: spheres have round shading towards the light, cylinders have highlights along the axis, and cubes/surfaces have generally flat lighting, depending on their orientation. This is a practical application of Lambert's law. Spheres have smoothly changing normals in all possible directions, so their brightest area is going to be in sections facing the light. Normals of a cylinder change as you go around the circle but are the same as you move along the axis, so the entire length of the cylinder is shaded the same. Flat planes have a constant normal, so every point gets the same lighting.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/12%20-%20NdotL.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/12%20-%20NdotL.jpg"></a>
</p>
<p dir="auto">(Side note to the side note: this is, of course, only true if we assume that the direction towards the light is the same on the entire surface! But is it? If we consider sunlight, the source is so far away that we can safely think that yes, every point gets the light from the same direction - it's a directional light. But for other sources, the position of the light matters too. And if the light is not a simple point but rather something larger, it all gets complicated even more).</p>
<p dir="auto">For paints, we can focus on the diffuse component only. Miniature paints dry pretty matte, so the surface of the dried paint is pretty rough. The specular component of the reflection is very faint and does brighten the surface a bit with light color, but the main characteristics of the appearance come from the diffuse part. I'll go into some theories describing what is going on with light in the paint layer later on, but for now, we can look at the macroscopic effect: light with some particular SPD falls onto the paint layer, and some of it gets scattered uniformly in all directions, and some get absorbed. We can compute the ratio of that scattered light to the incident light. This is called reflectance and, just like SPD, is a spectral characteristic: objects reflect different wavelengths differently. Some wavelengths are reflected more, some are absorbed more. This is what determines the color of the object. If an object absorbs short wavelengths and reflects and scatters long ones, it will appear reddish. If it reflects short wavelengths and absorbs long ones, it will be bluish. This is, of course, assuming that the illumination is uniform in all wavelengths. If there are no long wavelengths in the incoming lighting and the object reflects only long wavelengths, it won't look red; it will look black, as it will not reflect any light. The actual light that we see reflected diffusely off the object is a product of the object's reflectance and the SPD of the incoming light:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/13%20-%20reflection%20eq.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/13%20-%20reflection%20eq.png"></a>
</p>
<p dir="auto">  
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/14%20-%20D65%20times%20(%20yo%20+%20magenta)..JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/14%20-%20D65%20times%20(%20yo%20+%20magenta)..JPG"></a>
  <span>=</span>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/D65.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/D65.JPG"></a>  
  <span>*</span>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/3%20-%20spectrum%20red.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/3%20-%20spectrum%20red.JPG"></a>     
</p>
<p dir="auto">Not surprisingly, the same object can look very differently depending on the light used for illumination. And you can reason about the color of the object in different illumination using the above principles: if the object is blue in white light but is illuminated by yellowish-red candlelight, it will appear greenish.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The eye</h2><a id="user-content-the-eye" aria-label="Permalink: The eye" href="#the-eye"></a></p>
<p dir="auto">The next component of the whole system is the human eye: the light that gets reflected off objects reaches the eye, and we can see it. In the most simplistic view, the eye focuses the incoming light on the retina, which contains light-sensitive cells. There are two families of these cells: rods and cones. Rods respond strongest to light in the green part of the spectrum, they are responsible for seeing in low-light conditions, and they do not contribute to color vision - so we'll ignore them here. Our ability to see colors comes from cones. People with no form of color blindness have three types of cones, usually called L, M, and S - for long, medium, and short. They most strongly respond to the light with long-red wavelengths (L), medium-green (M), and short-blue (S).</p>
<p dir="auto">Just like SPD and object reflectance, we can define spectral sensitivity. It describes how strongly an eye (or any other sensor, camera, or similar) responds to light of a particular wavelength. It shows how excited the given type of cone is to see light of a particular frequency (pun intended) - the higher the sensitivity, the stronger the signal generated by the cone when it receives the same amount of energy. Here are the plots of sensitivity of three different types of human cones.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/16%20-%201920px-Cones_SMJ2_E.svg.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/16%20-%201920px-Cones_SMJ2_E.svg.png" width="35%"></a>
</p>
<p dir="auto">A cone excited by light produces a signal, but the actual wavelength that caused that excitation is irrelevant. The signal will be the same if, say, the M cone receives X amount of energy at 550nm, twice the X amount of energy at 500nm, or X energy at 500nm and X at 600nm. Technically, all the incoming energy at a particular wavelength is multiplied by the sensitivity of the sensor for that wavelength and added (integrated, but let's keep things simple) for all wavelengths to produce the cell signal. This actually leads to an interesting phenomenon called metamerism, where different distributions of incoming light can look the same to a human eye, just because they generate the same excitations in the cones.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/17%20-%20excitation.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/17%20-%20excitation.jpg"></a>
</p>
<p dir="auto">We now have a full picture: light is emitted by some source (source SPD), gets reflected by an object (source SPD multiplied by the object reflectance) and is registered by a cone in the eye (source SPD, multiplied by object reflectance, multiplied by the cone sensitivity, and added/integrated for all wavelengths). That last operation essentially involves calculating the area below the graph on the right-hand side (marked in red) - the larger the area, the greater the cone excitation, but the exact shape of that graph is irrelevant.</p>
<p dir="auto">Because our eyes contain three types of cones, the light generates three different signals that our brain interprets as color. Because there are three types of cones, the space of colors that people can see is intrinsically three-dimensional: visible colors can be described by three numbers, instead of having to deal with the entire spectral information (yes, this is where RGB comes from, but we'll get there...).</p>

<p dir="auto">All this was heavily investigated by researchers in the early 20th century. Measuring SPDs of different lights is pretty straightforward; you split the light with a prism or some diffraction grating and look at the rainbow of colors. Measuring reflectance is also relatively simple: you measure the SPD of some light, then shine it onto an object, measure the SPD of the reflected light, and divide one by the other. Measuring the sensitivity of the human eye is unfortunately really challenging. How would it work? It is very challenging to accurately tell if some particular light is 2 times or only 1.5 times brighter than some other light. Especially since the human vision system is (again) incredibly complex: rods and cones are one thing, but there's tons of processing going on later in the brain, that includes adaptation to light, color, and tons of other things (there are models for these effects too, Color Appearance Models if you want to read about some details, but it's way beyond the scope of anything here). But clever people figured out a way. They designed so-called "color matching experiments". (The actual sensitivities of the eye were actually measured in the 1950s, by Ragnar Granit, and he was awarded a Nobel Prize for this work).</p>
<p dir="auto">In the color matching experiments, a person is shown a color on one side of a screen. It's a pure color, generated by a light of a very narrow range of wavelengths. They then have to match that color by mixing three "base" lights on the opposite side of the screen - also pure colors of some particular wavelengths (they were chosen to be red, green, and blue, as it was already known that these are the colors that give peak response of the eye cells). Since not every pure color could be mixed from these three base lights, the researchers added the option to add some of the mixing light to the target color, which would effectively act as a negative mixing weight. By showing people the colors for every possible wavelength in the visible light, the researchers measured how much of the intensity of these three reference lights is needed to get the impression of the same color.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/19%20-%20color%20matching.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/19%20-%20color%20matching.jpg" width="35%"></a>
</p>
<p dir="auto">It's not quite the spectral sensitivity of a cell in the eye, but for all practical purposes, it's just as good: we can now quantify the color of any pure wavelength with three numbers. And because light is linear, any mixture made from different wavelengths can be described as some weighted sum of these triplets. So, <em>any visible color</em> can be precisely characterized by just three numbers.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/20%20-%20CIE1931_RGBCMF2.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/20%20-%20CIE1931_RGBCMF2.png" width="35%"></a>
</p>
<p dir="auto">The curves derived in the color matching experiments are called the r/g/b matching curves. The experiments were conducted in the 1920s, and back then, negative numbers were a big no-no when it came to numerical calculations. But because not all colors could be matched by additive mixing, some colors required adding light to the target color, resulting in some of the values in these curves being negative. So, the same CIE that standardized illuminants figured out that it would be good to transform these curves mathematically, so they are always positive. This would represent the mixing of some imaginary lights that cannot exist in practice. This would mean that some of the combinations of the numbers would represent colors that cannot exist, but they thought it's a good deal to get rid of these negatives. They also wanted one of the numbers to represent the general "brightness" of the color (that corresponds to the response of the rods). So, they did some mathematical magic and, based on the r/g/b matching curves, formulated the X/Y/Z matching curves.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/21%20-%202560px-CIE_1931_XYZ_Color_Matching_Functions.svg.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/21%20-%202560px-CIE_1931_XYZ_Color_Matching_Functions.svg.png" width="35%"></a>
</p>
<p dir="auto">And these X/Y/Z curves are the basis of modern colorimetry. They are still used today when describing color in any scientific or technical application. They form the basis for the XYZ color space, where every color has the three X/Y/Z values, and the Y value is the color brightness. The X/Y/Z matching curves behave exactly like the spectral sensitivities - but instead of being the sensitivities of cells in our eyes, they are just some mathematical abstract sensitivities, not describing anything in particular (well, the Y curve does represent the overall sensitivity of the eye to brightness). But since they are just some numbers, it doesn't really change anything; it's all just some abstract math anyway.</p>
<p dir="auto">We went from a continuous spectrum of visible light to three numbers we can do operations with. We can take light with the XYZ_1 color and add it to the light with XYZ_2 color, and we will get the (X_1 + X_2), (Y_1 + Y_2), (Z_1 + Z_2) color. We can take the light with XYZ_1 color and make it two times stronger, and we will get 2X_1, 2Y_1, 2*Z_1 color. This is called a color space. Technically, it's not really correct to multiply the components together - say, have the reflectance stored as XYZ and multiply it by the light XYZ, but it actually works pretty well (look at most of the modern CGI or video games; we all do this all the time. The only commercial renderer that actually works with spectral quantities and does it correctly is Manuka from Weta, used in "Avatar," "Planet of the Apes" series, and other movies).</p>
<p dir="auto">Fun fact: you might think that the matching curves were derived after massive studies involving thousands of volunteers. Quite far from reality, actually. The original experiments were performed by two people: W. David Wright with 10 (as in "ten") observers and John Guild with 7 ("seven") observers. Yes, the whole modern colorimetry is based on what 17 people saw in the 1920s, most likely friends and families of the two gentlemen.</p>
<p dir="auto">Later on, there were some revisions to these curves - most notably the 1964 10-degree standard observer (vs. the original 1931 2-degree standard observer): later researchers realized that due to all the adaptation in the visual system, it makes more sense for the test patches to take a larger part of the field of view, so they performed new experiments and created new curves that are slightly different. But long story short, the 2-degree, 1931 curves are still most commonly used today.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/22%20-%201931%20vs%201964%20xyz.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/22%20-%201931%20vs%201964%20xyz.jpg" width="35%"></a>
</p>
<p dir="auto">All the color spaces used in modern applications are derived from the XYZ. Some are just a linear transformation of the XYZ - so, just multiplying individual components by some constants and adding them in different combinations (like sRGB), while some involve some non-linear operations (computing powers of some expressions involving XYZ, or division) to make the values more "perceptually uniform" - so that differences in numbers describe similar changes in actual perceived colors (like Lab).</p>
<p dir="auto">Two important color spaces derived from XYZ are Yxy and sRGB. The first one uses Y to describe brightness and xy (which are X/(X+Y+Z) and Y/(X+Y+Z)) to describe the chromaticity, just the hue and saturation of the color. Oftentimes, all the visible colors are visualized in the xy plane as the visual locus, a space of all the colors that people can see. It is a convenient way of visualizing gamuts: subsets of the visible color that can be produced in some particular way, on screen or in print.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/23%20-%20YxyJPG.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/23%20-%20YxyJPG.JPG" width="35%"></a>
</p>
<p dir="auto">sRGB is a subset of XYZ, widely used in modern display technologies. It defines three primary colors (R, G, B) that are used to represent other colors and also specifies a "white point," which is considered the color white. The primaries were chosen so they can be physically realized in a technically feasible manner. However, the range of colors that can be represented in sRGB is somewhat limited because it relies on these base colors and negative weights cannot be used; monitors work by emitting light, not absorbing it. Ironically, the visual locus plots of all possible colors, when viewed on a typical screen, cannot fully display a wide range of these colors. Although sRGB is prevalent in the industry, it is gradually being replaced by standards with wider gamuts, such as Rec2020 in modern HDR displays. It's important to note that RGB values only make sense within the context of a specific color space and cannot be directly transferred between different spaces. For example, many cameras can capture photos in Adobe RGB, which has a slightly larger gamut than sRGB. To display these colors accurately on a standard monitor, they need to be converted to sRGB through additional calculations. This is the purpose of color profiles in software like Photoshop, which allow you to specify the color space you're working in and the color space of your input images, with the software handling all necessary conversions.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/24%20-%20sRGB%20gamut.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/24%20-%20sRGB%20gamut.JPG" width="35%"></a>
</p>
<p dir="auto">On the other hand, CMYK, commonly used in printing, cannot accurately describe general light color. It is designed to describe reflectance rather than light emission. CMYK colors, to be precise, require the definition of a specific illuminant and how individual components absorb and reflect light. It's not really a color space but more of a color model - a way to process color, rather than a precise mathematical method for defining and operating with colors.</p>
<p dir="auto">This is generally the case with any color system based on reflected light, including painted miniatures. To reason about their colors, we need to assume a certain illumination. Due to the widespread use of sRGB, the most sensible choice for an illuminant in such situations is D65, which is cold, white lighting and is the white point in sRGB, so (255, 255, 255) on a typical screen.</p>
<p dir="auto">To determine the color of paint and display it on a screen, the full process involves:</p>
<ul dir="auto">
<li>Taking spectral values for the D65 illuminant (this is publicly available, tabulated data).</li>
<li>Multiplying it by the spectral reflectivity of the paint.</li>
<li>Multiplying the result by the X, Y, and Z matching curves (again, publicly available).</li>
<li>Adding (integrating) the spectral values across all wavelengths to get X, Y, Z values of the light reflected off the paint layer.</li>
<li>Converting the XYZ values to sRGB with some simple math and displaying it on the screen.</li>
</ul>
<p dir="auto">You might notice that because of that integration across wavelengths, there's really no way of going back: many different spectral distributions can give the same XYZ values (the phenomenon of metamerism mentioned earlier). So, given the sRGB values, there's no straightforward way to retrieve the full spectrum back. However, we would like to overcome this limitation, so we'll try to work around the problem.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Pigments</h2><a id="user-content-pigments" aria-label="Permalink: Pigments" href="#pigments"></a></p>
<p dir="auto">The only missing bit to get the paint mixing software is the model for actually mixing the paints. You might think that maybe if we take 50% of paint A and 50% of paint B, the resulting mix will have the 50-50 mix of reflectance of paint A and paint B (or technically: that paint mixing is linear). Well, bad news, of course, it isn't linear. It's actually highly nonlinear, and to get something meaningful, we need to dive into radiative transport theory and differential equations. But no worries, we'll only skim the surface.</p>
<p dir="auto">We previously considered the diffuse reflection as light going into the object, bouncing around, scattering a bit, being absorbed a bit, and going out. Now, we will look into this process in just a bit more detail.</p>
<p dir="auto">The physical theory that describes processes like this is called radiative transport theory. It can describe phenomena like the blue color of the sky or the color of mixed paints, or more generally the effects of energy traveling through some form of medium. On the most basic level, it deals with two effects: absorption of radiation (light for us) and scattering. Absorption means that some of the light gets absorbed by particles of the medium and is converted to heat. As the light passes through the medium, there's less and less of it; it gets attenuated. Media that mostly absorb light appear translucent. If light goes through them, it can change color (because certain wavelengths can be absorbed more than others), but otherwise appears unchanged. If absorption is the main effect of the medium, it can be characterized by the Beer-Lambert law (which is a specific form of the radiative transfer equation), which states that the extinction of light is exponential with distance: the more light goes in, the more of it is absorbed on a unit distance. The absorption is fairly simple to model; after all, if the light is absorbed, it is gone, and we don't have to deal with it anymore.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/25%20-%20absorbtion%20and%20scattering.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/25%20-%20absorbtion%20and%20scattering.jpg" width="70%"></a>
</p>
<p dir="auto">The other effect is scattering. When light hits a particle of the medium, it can be scattered - bounce off in some other direction. And this is where the majority of the complexity comes from. Depending on the medium, the way its particles scatter light is different. Some media scatter predominantly forward (so the light direction is just slightly affected), some scatter uniformly (so whatever direction light comes from, it can get scattered in any other direction). On top of that, when light is scattered, it travels further and interacts with more particles, undergoing more scattering events. And of course, it all happens together with absorption, so scattered light might be scattered more, and then absorbed, and so on. This results in, once again, incredibly complex systems. Thousands of academic papers have been published on various ways of solving the radiative transfer equations for some particular setups of light, medium, and so on.</p>
<p dir="auto">Luckily for us, mixing colors is a pretty common problem in various industries, so it has been extensively studied too. To make things a bit simpler, usually, a number of simplifications are made: instead of dealing with some general objects, people analyze transport within very thin horizontal slabs of material. They are considered infinite, to avoid having to deal with any problems on the boundaries, but that's perfectly fine, as for our diffuse reflection, the light doesn't really go very far to the side within the material. They also assume that the scattering function is uniform, so when light is scattered, it bounces in any random direction with no preference. For regular, no-effect paints (so non-metallic paints), that's a totally valid assumption. Lastly, they assume circular symmetry along the vertical axis - which means that it all behaves the same way no matter how around the surface we look at it - the surface properties are the same (material is isotropic) and the lighting is the same too (for instance, the sample is illuminated in the same way from every direction). With all these assumptions, you can simplify the radiative transfer equations and reason about two separate "streams" ("fluxes") of lighting: one going down, into deeper layers, and one going up, and eventually leaving the paint layer.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/26%20-%20two%20diffuse%20fluxes.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/26%20-%20two%20diffuse%20fluxes.jpg" width="70%"></a>
</p>
<p dir="auto">Given a setup like this, the radiative transport equation actually simplifies to something reasonable that can be solved by hand (if you remember how to solve systems of differential equations, of course - don't look at me here, though). If, on top of that, you assume that the thickness of the object is big enough to not pass any light through (for instance: the paint fully covers the substrate below), you get a fairly simple formula that gives you the reflectance as a function of two parameters describing scattering and absorption.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/27%20-%20two%20flux%20reflectance.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/27%20-%20two%20flux%20reflectance.png" width="40%"></a>
</p>
<p dir="auto">The K coefficient describes the absorption, and the S coefficients describe scattering. Just like SPDs, reflectance, and sensitivities, they are spectrally varying: so they are different for every wavelength.</p>
<p dir="auto">This was done, although in a bit less principled fashion, in the 1920s by Kubelka and Munk, forming what is known nowadays as "Kubelka-Munk theory". It got revised later, in the framework of radiative transfer theory, in the form of two-flux theories (K-M is sort of two diffuse fluxes), and extended in three- and multi-flux theories. If you're curious, there's a great book by Georg Klein, "Industrial Color Physics," that dives into details. For practical purposes, when working with regular diffuse, non-effect paints, two-flux or Kubelka-Munk is generally considered enough. One thing that I don't cover here are the Saunderson corrections - these are additional factors that actually take the light specularly reflected on the surface of the paint into account - and reduce the amount of light that goes into the paint layer (and out of it).</p>
<p dir="auto">And this is one of those rare cases where things are actually simple. Mixing of media is linear with respect to K and S coefficients. The mixture of 50-50 paints with coefficients K1 and K2 will give 0.5K1 + 0.5K2, the same with S. In general:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/27%20-%20k%20sum.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/27%20-%20k%20sum.png" width="40%"></a>
</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/28%20-%20s%20sum.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/28%20-%20s%20sum.png" width="40%"></a>
</p>
<p dir="auto">(the w1 and w2 weights need to add up to 1.0)</p>
<p dir="auto">Now, we only need to get these K and S coefficients from somewhere.</p>
<p dir="auto">The equation for reflectance gives us reflectance as a function of K and S, but we can rearrange it to get, for instance, K based on S and reflectance. We know how to measure reflectance, so if we only figure out where to get S from, we could compute K. The trick is to take one of the paints - usually white - and just assume that it has S equal to 1.0 for all wavelengths. We then measure its reflectance and compute K from that.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/29%20-%20k%20from%20s.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/29%20-%20k%20from%20s.png" width="30%"></a>
</p>
<p dir="auto">Now for any other paint, we also need K and S, but we cannot arbitrarily set their S to be 1.0. However, we can mix them with that white which will act as our reference value. This gives us two measurements: one of the raw paint (so-called "masstone") and one mixed with white in some proportions (that we of course need to know, but that's easy, we can just weigh the components). This, together with the equations above, gives us two equations and two unknowns: K and S for the new paint (it's a separate system of equations for every wavelength). This is trivial to solve and gives us everything we need to know to model paint mixing digitally.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/30%20-%20k%20and%20s%20from%20mix.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/30%20-%20k%20and%20s%20from%20mix.png" width="110%"></a>
</p>
<p dir="auto">Big equation, sorry :( But it's pretty simple really. We know R(eflectance) for paint and for the mix of the paint with white (this is what we measure). We know K and S for white (we just calculated it above, setting S to 1.0), and we know how much of both white and paint we put into the mix (w(eight)_paint and w(eight)_white), so it's really just a system of two equations with two unknowns (K and S for the paint). I marked all the things that we know in blue and the unknowns in red.</p>
<p dir="auto">In the real world, usually more than one mix is done, usually around five, in different proportions, to get a better estimate of the K and S coefficients. This requires slightly more complex math, but these are technical details (although important if you're trying to do it on a budget, see below).</p>
<p dir="auto">Given all this theoretical background our course of action is:</p>
<ul dir="auto">
<li>take some number of paints, preferably saturated paints that will give us a wide range of mixed colors</li>
<li>measure the reflectance of a fully opaque white layer, assume the S for that paint is equal to 1.0 for all wavelengths and derive K (the reflectance of white paint will generally be fairly uniform and close to 1.0, but not quite, and we actually need to measure it properly)</li>
<li>for every other paint in the set prepare a fully covering patch of the paint straight from the pot and its mix with white in some measured proportions.</li>
<li>measure the reflectance of both patches, derive K and S coefficients for that paint</li>
<li>we can now take any paints, in any proportions, mix their K and S coefficients</li>
<li>from the mix, we can compute the reflectance, use D65 illuminant to virtually illuminate it, and convert it to XYZ and later to sRGB</li>
<li>given this machinery we can do numerical optimization to find the best combination of paints that gives us a given sRGB color</li>
</ul>
<p dir="auto">And that's pretty much what I did. Let's dig into practical details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Measurements</h2><a id="user-content-measurements" aria-label="Permalink: Measurements" href="#measurements"></a></p>
<p dir="auto">The crucial part of the whole above operation is the measurement of the spectral reflectance of the color samples. You need a spectrophotometer for that. There are some lab-grade ones, and there are handheld ones, aiming for industry (for validating if the plastic coming out from your injection moulder is the right color, or if your printing press prints the color that the customer ordered). They can do all sorts of interesting things, measure according to different standards (there's an ISO for that, 13655 if I remember correctly) compute color differences, some have filters etc. They have one thing in common: they are pretty expensive. The decent ones start at a few thousand dollars (though you can get the nix spectro - <a href="https://www.nixsensor.com/nix-spectro-2/" rel="nofollow">https://www.nixsensor.com/nix-spectro-2/</a> - that looks interesting and might be just enough - and it's only a bit over one grand - but I never tested it so it's hard for me to judge its accuracy) - so generally not the money you want to spend on a hobby project that you're going to forget about in a month. But luckily, a while ago, I actually constructed a handheld spectrophotometer at work. We needed some particular type of measurement, that's very rarely used in practice, so regular handheld devices do not support it. It was an interesting project on its own, but I'll spare you the details. The heart of the device is the Hamamatsu C12666MA sensor, some analog-to-digital converter and a microcontroller to drive all this. The reference lighting is provided by a wide spectrum LED, which is not really perfect - as the lighting below 410 nm and above 680nm is a bit weak, making readings in these ranges less reliable - but it was totally fine for us. During all the calibration procedures I went through when making these devices (I made something like 8 of them total) I got a set of calibrated color samples that came with full spectral reflectance data - so I was able to compare it with the ones produced by my device. They were all within 1-2% for each wavelength, so I generally trust the results. And I know the exact conditions used for measuring the calibration data, which happen to be the exact conditions used for typical industrial measurements of paint samples, so even though I know my device is not quite ideal for measurements like this, I know it's not too far off either.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/31%20-%20spectophotometer.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/31%20-%20spectophotometer.jpg" width="70%"></a>
</p>
<p dir="auto">For my set of paints, I picked the Kimera set (the one with 13 base colors). Kimera paints are great because they are single pigment, and the manufacturer actually tells you what pigment this is. And they are super heavy on pigment, so you don't need a lot to create a fully opaque layer. And because of all this simplicity, I trust they are consistent - after all, it's just some amount of given pigment and medium. That gives me some confidence that two bottles of white will actually behave the same, which is important if you want any reproducibility. Not that I want to complain here, but it's very much not the case with, for instance, Citadel or Vallejo paints. Even ignoring all these silly names, their consistency is absolutely random. Sometimes they are fine straight from the pot, sometimes they need tons of water to flow at all. If I'm working on some recipe for a color, I need to be certain that different samples of the same paint will behave the same. Neither Citadel nor Vallejo seems to guarantee that, which is a bit of a bummer because Kimeras are incredibly hard to get where I live (the land of the free and the home of the brave). I ended up getting them directly from Italy, which took over a month to get, and I'm definitely not a patient person (but, surprisingly, even with global shipping, it was still cheaper to get them from Italy than from the local distributors! so hey, order them from Pegaso World directly!).</p>
<p dir="auto">I took a piece of plywood, primed it black, and painted samples of all the colors for the set. They are around 3cm x 3cm, which corresponds to the measurement area of my spectrophotometer (it's actually smaller, but it illuminates an area of around that size, so I wanted it to be uniform). I took the paint straight out of the bottle, without diluting it at all, and painted an opaque layer. Or rather, tried to paint an opaque layer. Some of these pigments are just naturally translucent, especially two yellows, and getting an opaque layer was a nightmare. It took ages to put and dry multiple thick layers of paint, and honestly, in some spots, there's still a bit of substrate peeking through. Well, we'll have to live with that. In a professional setting, paint is applied on special draw-down charts with a draw-down bar that has a precisely controlled gap. You put a blob of paint and smear it down, leaving a layer of a particular thickness (see here: <a href="https://www.byk-instruments.com/en/Physical-Properties/Paint-Application/Manual-Film-Applicators/Film-Applicator%2C-1-Gap/c/p-5970" rel="nofollow">https://www.byk-instruments.com/en/Physical-Properties/Paint-Application/Manual-Film-Applicators/Film-Applicator%2C-1-Gap/c/p-5970</a>). I don't really care about the precise thickness, but it would be great to have it uniform. The draw-down bar, however, is around $400 - which seems a bit excessive for a piece of metal that I'm going to use once. So I used a brush (tbh, I did order some knock-off, no-name draw-down bar from China, it was $50, that's about as much as I'm willing to spend to test it - we'll see how well it does, but it's going to be a while until it gets here).</p>
<p dir="auto">Next, I mixed every paint with white, measuring the exact ratios. I measured by weight, just because it's much easier. I have a precise scale, with resolution up to one hundredth of a gram, but measuring small volumes of thick liquid is difficult. This means that the final recipes will be given by weight, but honestly I don't think these paints differ in density too much (and all this is so hand-wavy, that even if they do, it won't make much difference in practice anyway). You could probably weigh the paint bottles from a fresh set and figure out these densities, but I didn't, and now it's too late, since I only ordered one set. I only mixed each paint with white in one ratio. This is the minimum number of mixes you need to calculate the K and S coefficients, but having more would be better. But of course, to have more of these mixes, you need more paint, and, as I said, I only ordered one set, and I would actually want to paint something with them too, not only do some experiments. Of course, the biggest problem is white, which you need a lot for all these mixes, but just pure white is also out of stock everywhere around here, and I was too impatient to wait for it to ship from Italy again.</p>
<p dir="auto">This is what it all looked like:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/32%20-%20PXL_20240128_204751800.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/32%20-%20PXL_20240128_204751800.jpg" width="70%"></a>
</p>
<p dir="auto">Like a real scientist, I measured every patch three times and averaged the results. I then wrote a piece of Python code to do all the math for mixing K and S and calculating reflectance and then final color when illuminated by D65 from that.</p>
<p dir="auto">Here are the spectral reflectivities of two paints (green and red)</p>
<p dir="auto">  
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/33%20-%20green.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/33%20-%20green.JPG"></a>  
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/33%20-%20red.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/33%20-%20red.JPG"></a>
</p>

<p dir="auto">And just like you would expect, if you mix yellow and blue (and a bit of white, blues from Kimera set are dark as hell straight out of the bottle), you get green.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/34%20-%20blue%20and%20yelllow.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/34%20-%20blue%20and%20yelllow.JPG" width="70%"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Solver</h2><a id="user-content-solver" aria-label="Permalink: Solver" href="#solver"></a></p>
<p dir="auto">With all this, writing a solver that produces a given color from a set of paints is fairly straightforward. When you take some set of paints, the process of finding their relative amounts in the mix that give you color closest to some prescribed one is called optimization. We start with some mixing weights, compute the result and analyze how to change the weights to get a result somewhat closer to what we want. Rinse and repeat. This is a pretty common operation nowadays, particularly in the machine learning world. All the DALL-Es, Stable Diffusions, and other ChatGPTs go through a training phase, which is in fact a giant optimization problem. Here, we only have a couple of weights, so the problem is pretty trivial and can be quickly solved even using off-the-shelf solvers, for instance, ones from the Python scipy library.</p>
<p dir="auto">There are two details here. First is the so-called "loss function" (that's the machine learning lingo, but people call it error function, objective function, cost function, and probably a bunch of other names too). This is a function that tells us how different some intermediate result is from the target. It is usually assumed to be decreasing as two values get closer (so the process becomes "minimization" of the loss function, trying to find its minimum), but it's not always the case. It usually doesn't matter at all because even if it gets larger when the values are closer, you can always multiply it by -1. One of the most common loss functions is the L2 loss: you compute the differences between the values, square them (hence the '2' in the name; the squaring bit makes sure that if the difference becomes negative, it counts the same way as if it was positive), and if you have multiple components, you add them together. You get one number that describes some measure of the difference between two arbitrary sets of values. L2 loss has some nice properties (it differentiates to a set of linear functions, which can be solved trivially), but in general, it doesn't really represent anything in particular. Yes, a larger L2 loss means that things are more different, but it's not like when L2 loss is smaller, the results are actually better (for however you define "better"). That's why designing good loss functions in modern machine learning is an art on its own, involving combining different factors, sometimes in very creative ways. Our case is fairly simple, so I just went with L2 loss in sRGB: so after computing the color of a mix of paints, I compute the differences in the red, green, and blue channels, I square and add them, and I try to find a mix that gives me the smallest such number. It seems like the industry usually does it in Lab color space because it's more perceptually uniform: so even if the result is not perfect (if your method gets the loss to zero, it doesn't really matter what loss you use - you get the issues when your loss is not quite zero), it's "perceptually" closer to the desired color. What I noticed in practice is that if you're trying to match a color that's impossible to get with a set of base paints, the result is not good no matter which loss you use, so I don't bother and just do it in sRGB.</p>
<p dir="auto">The other issue is a bit of combinatorics in our problem. The same color can be obtained from different mixes of paints. Of course, ones that use fewer paints are generally better than the ones that use more. But sometimes using two paints can get you almost there, and while using three can get you all the way there, it doesn't really make sense to add that additional one just to fix this 1% error. And how do you even decide which actual two paints you would mix? If you have a blue and yellow paint, you can optimize for the ratios that give you something closest to the green that you want, but how would you know to mix yellow and blue in the first place? In big machine learning, there are strategies for such problems: for instance, incorporating L0 loss in your final loss function - which is the number of non-zero weights - so that solutions that have fewer weights would be preferable. Of course, then comes the issue of how much of the actual difference in color one extra paint is worth and you need to come up with some arbitrary weighting factors. Because we don't really have that many paints, we can do something simpler: just take all possible combinations of one, two, three, four paints, optimize all these mixes and find the best ones. And since there are also some other factors involved, I just decided to show all best three one, two, three, and four paint combinations. I don't filter the results in any way, so it might happen that the three paint combinations can be worse than two paint ones (I always ensure that if a paint is used for mixing, some of it, even a minuscule amount, ends up in the recipe) - when a color can be obtained with just two paints, and anything extra just makes it worse. The results are also presented just as they come out of the optimizer, with three decimal places - so you might totally get recipes like 0.8 yellow + 0.01 black. The 80:1 ratio is totally useless if you want to apply it precisely in painting, but again, it's more of a guideline that it's yellow with a tiny hint of black. Don't get too attached to these numbers, they are just some help, nothing more.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/35%20-%20single%20recipe.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/35%20-%20single%20recipe.JPG" width="70%"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The tool</h2><a id="user-content-the-tool" aria-label="Permalink: The tool" href="#the-tool"></a></p>
<p dir="auto">The tool is written in Python, with the UI done in PyQt. Most of the boilerplate code was actually generated by ChatGPT - as PyQt is something I have absolutely zero interest in learning. ChatGPT is brilliant in helping you navigate these sorts of libraries that you're not familiar with, saving tons of time. I cannot recommend it enough. It was actually able to produce setup code for the layout of the application based on a rough sketch from Paint I pasted into the conversation - how cool is that?</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/36%20-%20whole%20too.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/36%20-%20whole%20too.JPG" width="70%"></a>
</p>
<p dir="auto">The left list shows you list of all the paints in the database, with their names, colors as XYZ and sRGB. The checkboxes allow you to enable and disable individual paint for the recipe solver.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/37%20-%20base%20paints.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/37%20-%20base%20paints.JPG" width="30%"></a>
</p>
<p dir="auto">The next list is the mixing list. You can drag paints from main list onto it to add them to the mix, or the other way round to remove them. Paints on the mixing list have a slider that controls their amount in the mix.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/38%20-%20used%20paints.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/38%20-%20used%20paints.JPG" width="30%"></a>
</p>
<p dir="auto">Next pane shows you the spectral plot of reflectance of the mix. Each component is shown, as well as the reflectance of the mix. The top button show the color of the mix.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/39%20-%20reflectance.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/39%20-%20reflectance.JPG" width="30%"></a>
</p>
<p dir="auto">Below is the chromaticity plot with all the components and the mixed color marked as well.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/40%20-%20xy.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/40%20-%20xy.JPG" width="30%"></a>
</p>
<p dir="auto">Last, there's the recipe solver pane: you can pick a color with a picker (and PyQt picker lets you pick a screen color directly too) and then hit "solve," which will kick off the solver. It will solve for 1/2/3/4 paint mixes and add them all to the list below. Each recipe has a background of the color mixed according to the recipe, and the overall background is always the target color, so you can compare them side by side. Solve can take a while, even though it is parallelized, but the results are added to the list as they become available. You can double click on any recipe, and it will be put into the paint mixer.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/41%20-%20recipe%20list.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/41%20-%20recipe%20list.JPG" width="30%"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tests and conclusions</h2><a id="user-content-tests-and-conclusions" aria-label="Permalink: Tests and conclusions" href="#tests-and-conclusions"></a></p>
<p dir="auto">So! How does all this compare to reality? Actually decently well. I haven't tested it all super thourouglhly, but in all the test I did it behaved more-less like expected. Some examples (ignore anything odd below 400nm, the measurement device is not particularly reliable there):</p>
<p dir="auto">A 1 to 1 mix of yellow oxide and magenta (white: measured, colored: predicted)</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/mix_1.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/mix_1.JPG" width="30%"></a>
</p>
<p dir="auto">A 1 to 10 mix of blue red shade and orange (white: measured, colored: predicted)</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/mix_2.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/mix_2.JPG" width="30%"></a>
</p>
<p dir="auto">They are actually really close! Given how sloppy all this was, that's quite encouraging!</p>
<p dir="auto">Here are some not so accurate results:</p>
<p dir="auto">A 1 to 2 to 2 mix of blue red shade, cold yellow and white. Predicted reflectance in long wavelengths is overestimated, making the predicted result brighter</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/mix_3.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/mix_3.JPG" width="30%"></a>
</p>
<p dir="auto">A 1 to 1 mix of cold yellow and orange - similarly, the predicted result is slightly brigher than in reality.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/mix_4.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/mix_4.JPG" width="30%"></a>
</p>
<p dir="auto">The main offender seems to be yellow, mostly the cold variation. I think the problem comes from it's high translucency. I tried hard to get fully covering, opaque layer, but I'm not quite sure how well it worked in the end. Interestingly other people doing similar work (for instance Yoshio Okumura here: <a href="https://repository.rit.edu/cgi/viewcontent.cgi?article=5896&amp;context=theses" rel="nofollow">https://repository.rit.edu/cgi/viewcontent.cgi?article=5896&amp;context=theses</a>) ran into exact same problems with the same pigments. It looks like yellows just need more love. I might try playing with it a bit more, but tbh, the difference is not that huge in practice, the hue matches more-less and I'm not after exact recipes anyway.</p>
<p dir="auto">One thing missing in the model used for mixing are so-called Saunderson correction coefficients. They account for specular reflection on the surface of the measured sample. Testing how much this would improve the accuracy is another excersise for the future.</p>
<p dir="auto">If you got to that point, congratulations! Grab the tool, play with it, and if you have any questions, shoot me an email at <a href="mailto:miciwan@gmail.com">miciwan@gmail.com</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License and image credits</h2><a id="user-content-license-and-image-credits" aria-label="Permalink: License and image credits" href="#license-and-image-credits"></a></p>
<p dir="auto">All the rambling, all the images not specifically mentioned below, all the data in the depot are licensed under Creative Commons BY 4.0 (so you can do whatever with it, including using it for commercial purposed, you just need to say where you took it from).</p>
<p dir="auto">Image of the smooth vs rough specular reflection is from "Real-Time Rendering 4th edition". The publisher allowed to use all the figures from the book for non-commercial purposes under fair use. I <em>believe</em> that this actually counts as fair use, but tbh, I'm not 100 percent sure. But I'm one of the authors of this book, so maybe the publisher will be kind enough not to sue me if this is not the case.</p>
<p dir="auto">The image of the spectral sensitivities of human cones is by Vanessaezekowitz, from en.wikipedia, <a href="https://en.wikipedia.org/wiki/Trichromacy#/media/File:Cones_SMJ2_E.svg" rel="nofollow">https://en.wikipedia.org/wiki/Trichromacy#/media/File:Cones_SMJ2_E.svg</a> under CC BY 3.0</p>
<p dir="auto">The image of the rgb matching curved is by Marco Polo, from wikipedia, <a href="https://en.wikipedia.org/wiki/File:CIE1931_RGBCMF.svg" rel="nofollow">https://en.wikipedia.org/wiki/File:CIE1931_RGBCMF.svg</a> under Public Domain</p>
<p dir="auto">The image of the 1931 XYZ matching curves is by Acdx, from wikipedia, <a href="https://en.wikipedia.org/wiki/CIE_1931_color_space#/media/File:CIE_1931_XYZ_Color_Matching_Functions.svg" rel="nofollow">https://en.wikipedia.org/wiki/CIE_1931_color_space#/media/File:CIE_1931_XYZ_Color_Matching_Functions.svg</a> under CC BY-SA 4.0</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A new phase of matter: Physicists show non-Abelian anyons in quantum processor (111 pts)]]></title>
            <link>https://phys.org/news/2024-02-phase-physicists-abelian-anyons-quantum.html</link>
            <guid>39515007</guid>
            <pubDate>Mon, 26 Feb 2024 18:28:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phys.org/news/2024-02-phase-physicists-abelian-anyons-quantum.html">https://phys.org/news/2024-02-phase-physicists-abelian-anyons-quantum.html</a>, See on <a href="https://news.ycombinator.com/item?id=39515007">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
										
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/physicists-create-a-ne.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2024/physicists-create-a-ne.jpg" data-sub-html="Inside the chamber of the Quantinuum H2 quantum processor. Credit: Quantinuum">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/physicists-create-a-ne.jpg" alt="Physicists create a new phase of matter" title="Inside the chamber of the Quantinuum H2 quantum processor. Credit: Quantinuum" width="800" height="530">
             <figcaption>
                Inside the chamber of the Quantinuum H2 quantum processor. Credit: Quantinuum
            </figcaption>        </figure>
    </div><p>Our physical, 3D world consists of just two types of particles: bosons, which include light and the famous Higgs boson; and fermions—the protons, neutrons, and electrons that comprise all the "stuff," present company included.</p>


										      
																																	<p>Theoretical physicists like Ashvin Vishwanath, Harvard's George Vasmer Leverett Professor of Physics, don't like to limit themselves to just our world, though. In a 2D setting, for instance, all kinds of new particles and states of matter would become possible.</p>
<p>Vishwanath's team used a powerful machine called a <a href="https://phys.org/tags/quantum+processor/" rel="tag">quantum processor</a> to make, for the first time, a brand-new phase of matter called non-Abelian topological order. Previously recognized in theory only, the team demonstrated synthesis and control of exotic particles called non-Abelian anyons, which are neither bosons nor fermions, but something in between.</p>
<p>Their <a href="https://www.nature.com/articles/s41586-023-06934-4">results</a> are published in <i>Nature</i> in collaboration with researchers at the quantum computing company Quantinuum. Vishwanath's team included former Harvard Kenneth C. Griffin Graduate School of Arts and Sciences student Nat Tantivasadakarn '22, now at Caltech, and postdoctoral fellow Ruben Verresen.</p>
<p>Non-Abelian anyons, known to physicists as quasi-particles, are only mathematically possible in a 2D plane. The qualifier "quasi" refers to the fact that they are not exactly particles, but rather long-lived excitations through a specific phase of matter—think <a href="https://phys.org/tags/ocean+waves/" rel="tag">ocean waves</a>—and they have special memory-carrying capabilities.</p>

																																						
																																			<p>Besides the fact that creating a new phase of matter is exciting fundamental physics, non-Abelian anyons have been widely recognized as a potential platform for quantum computing—which infuses the research achievement with even more significance.</p>
<p>Non-Abelian anyons are inherently stable, unlike the flimsy and error-prone quantum bits, or qubits, on other quantum computing platforms. They can "remember" their pasts as they move around one another—like a magician shuffling cups with hidden balls. This property is also what makes them topological, or able to be bent and twisted without losing their core identity.</p>
<p>For all these reasons, non-Abelian anyons might someday make ideal qubits—units of computational power that extend well beyond the classical computers of today—if they can be created and controlled at larger scales.</p>
<p>"One very promising route to stable quantum computing is to use these kinds of exotic states of matter as the effective quantum bits and to do quantum computation with them," Tantivasadakarn said. "Then you have mitigated to a large extent the issues with noise."</p>
<p>The researchers employed some dogged creativity to realize their exotic matter state. Maxing out the capabilities of Quantinuum's newest H2 processor, the team started with a lattice of 27 trapped ions. They used partial, targeted measurements to sequentially increase the complexity of their quantum system, effectively ending up with an engineered quantum wave function with the exact properties and characteristics of the particles they were after.</p>

																																			<p>"Measurement is the most mysterious aspect of quantum mechanics, leading to famous paradoxes like Schrödinger's cat and numerous philosophical debates," Vishwanath said. "Here we used measurements as a tool to sculpt the quantum state of interest."</p>
<p>As a theorist, Vishwanath cherishes the ability to bounce between different ideas and applications of physics without being tethered to one platform or technology. But in the context of this work, he marvels at getting to not just explore a theory, but actually demonstrate it, particularly as the <a href="https://www.nature.com/articles/s42254-023-00600-4">field of quantum mechanics enters its 100th year</a>.</p>
<p>"At least for me, it was just amazing that it all works, and that we can do something very concrete," Vishwanath said. "It really connects many different aspects of physics over the years, from foundational quantum mechanics to more recent ideas of these new kinds of particles."</p>

																																																					
																				<div>
																						<p><strong>More information:</strong>
												Mohsin Iqbal et al, Non-Abelian topological order and anyons on a trapped-ion processor, <i>Nature</i> (2024). <a data-doi="1" href="https://dx.doi.org/10.1038/s41586-023-06934-4" target="_blank">DOI: 10.1038/s41586-023-06934-4</a>
																						
																						</p><p>
													<strong>Journal information:</strong>
																											<a href="https://phys.org/journals/nature/"><cite>Nature</cite></a>
														<a href="http://www.nature.com/nature/index.html" target="_blank" rel="nofollow">
															<svg>
																<use href="https://phys.b-cdn.net/tmpl/v6/img/svg/sprite.svg#icon_open" x="0" y="0"></use>
															</svg>
														</a> 
																									</p>
																					</div>
                               											
																					<p>
													Provided by
																											Harvard Gazette
																									</p>
                              																					 <p>
												  <i>This story is published courtesy of the <a href="http://news.harvard.edu/gazette/">Harvard Gazette</a>, Harvard University's official newspaper. For additional university news, visit <a href="http://www.harvard.edu/">Harvard.edu</a>.</i>
											 </p>
										                                        
										<!-- print only -->
										<div>
											 <p><strong>Citation</strong>:
												A new phase of matter: Physicists achieve first demonstration of non-Abelian anyons in a quantum processor (2024, February 21)
												retrieved 27 February 2024
												from https://phys.org/news/2024-02-phase-physicists-abelian-anyons-quantum.html
											 </p>
											 <p>
											 This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
											 part may be reproduced without the written permission. The content is provided for information purposes only.
											 </p>
										</div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Press Release: Future Software Should Be Memory Safe (126 pts)]]></title>
            <link>https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/press-release-technical-report/</link>
            <guid>39514844</guid>
            <pubDate>Mon, 26 Feb 2024 18:15:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/press-release-technical-report/">https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/press-release-technical-report/</a>, See on <a href="https://news.ycombinator.com/item?id=39514844">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
	


	<div>
								


<p><em><strong>Leaders in Industry Support White House Call to Address Root Cause of Many of the Worst Cyber Attacks</strong></em></p>



<p><em>Read the full report <a href="https://www.whitehouse.gov/wp-content/uploads/2024/02/Final-ONCD-Technical-Report.pdf">here</a></em></p>



<p>WASHINGTON – Today, the White House Office of the National Cyber Director (ONCD) released a report calling on the technical community to proactively reduce the attack surface in cyberspace. ONCD makes the case that technology manufacturers can prevent entire classes of vulnerabilities from entering the digital ecosystem by adopting memory safe programming languages. ONCD is also encouraging the research community to address the problem of software measurability to enable the development of better diagnostics that measure cybersecurity quality.</p>



<p>The report is titled <em><a href="https://www.whitehouse.gov/wp-content/uploads/2024/02/Final-ONCD-Technical-Report.pdf">“Back to the Building Blocks: A Path Toward Secure and Measurable Software.” </a></em></p>



<p>“We, as a nation, have the ability – and the responsibility – to reduce the attack surface in cyberspace and prevent entire classes of security bugs from entering the digital ecosystem but that means we need to tackle the hard problem of moving to memory safe programming languages,” said National Cyber Director Harry Coker.&nbsp; “Thanks to the work of our ONCD team and some tremendous collaboration from the technical community and our public and private sector partners, the report released today outlines the threat and opportunity available to us as we move toward a future where software is memory safe and secure by design. I’m also pleased that we are working with and calling on the academic community to help us solve another hard problem: how do we develop better diagnostics to measure cybersecurity quality? Addressing these challenges is imperative to ensuring we can secure our digital ecosystem long-term and protect the security of our Nation.”</p>



<p>By adopting an engineering-forward approach to policymaking, ONCD is ensuring that the technical community’s expertise is reflected in how the Federal Government approaches these problems. Creators of software and hardware can have an outsized impact on the Nation’s shared security by factoring cybersecurity outcomes into the manufacturing process.</p>



<p>“Some of the most infamous cyber events in history – the Morris worm of 1988, the Slammer worm of 2003, the Heartbleed vulnerability in 2014, the Trident exploit of 2016, the Blastpass exploit of 2023 – were headline-grabbing cyberattacks that caused real-world damage to the systems that society relies on every day. Underlying all of them is a common root cause: memory safety vulnerabilities. For thirty-five years, memory safety vulnerabilities have plagued the digital ecosystem, but it doesn’t have to be this way,” says Anjana Rajan, Assistant National Cyber Director for Technology Security. “This report was created for engineers by engineers because we know they can make the architecture and design decisions about the building blocks they consume – and this will have a tremendous effect on our ability to reduce the threat surface, protect the digital ecosystem and ultimately, the Nation.”</p>



<p>ONCD has engaged with a diverse group of stakeholders, rallying them to join the Administration’s effort. Statements of support from leaders across academia, civil society, and industry can be found <a href="https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/memory-safety-statements-of-support/">here</a>.</p>



<p>In line with two major themes of the President’s National Cybersecurity Strategy released nearly one year ago, the report released today takes an important step toward shifting the responsibility of cybersecurity away from individuals and small businesses and onto large organizations like technology companies and the Federal Government that are more capable of managing the ever-evolving threat. This work also aligns with and builds upon secure by design programs and research and development efforts from across the Federal Government, including those led by CISA, NSA, FBI, and NIST.</p>



<p>The work on memory safety in the report complements interest from Congress on this topic. This includes the efforts of the U.S. Senate and House Appropriations Committees, who included directive report language requiring a briefing from ONCD on this issue in Fiscal Year 2023 appropriations legislation. Additionally, U.S. Senate Homeland Security and Governmental Affairs Committee Chairman Gary Peters (D-MI) and U.S. Senator Ron Wyden (D-OR) have highlighted their legislative efforts on memory safety to ONCD.</p>



<p>Read the full report, <a href="https://www.whitehouse.gov/wp-content/uploads/2024/02/Final-ONCD-Technical-Report.pdf">“Back to the Building Blocks: A Path Toward Secure and Measurable Software.”</a> </p>



<p>Read our fact sheet <a href="https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/memory-safety-fact-sheet/">here</a>. </p>



<p>Read out statements of support from industry, academia, and civil society <a href="https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/memory-safety-statements-of-support/">here</a>.</p>



<p>Watch a video address from Director Coker and Assistant National Cyber Director for Technology Security Rajan outlining the challenges and solutions presented in the technical report <a href="https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/video-technical-report-launch/">here</a>. </p>











<p>###</p>
			</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ryzen Z1's Tiny iGPU (174 pts)]]></title>
            <link>https://chipsandcheese.com/2024/02/25/ryzen-z1s-tiny-igpu/</link>
            <guid>39514778</guid>
            <pubDate>Mon, 26 Feb 2024 18:11:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2024/02/25/ryzen-z1s-tiny-igpu/">https://chipsandcheese.com/2024/02/25/ryzen-z1s-tiny-igpu/</a>, See on <a href="https://news.ycombinator.com/item?id=39514778">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><strong>Editor’s Note</strong>: Just like <a href="https://chipsandcheese.com/2024/02/12/amds-mild-hybrid-strategy-ryzen-z1-in-asuss-rog-ally/">our prior Ryzen Z1 article</a>, the ROG Ally was kindly provided by Asus to let us test the Ryzen Z1.</p>
<p>ASUS ROG Ally comes in two configurations: AMD’s Ryzen Z1 Extreme and the Ryzen Z1. The Ryzen Z1 Extreme uses AMD’s high-end Zen 4 APU configuration, with eight Zen 4 cores and six RDNA 3 WGPs. Its non-extreme cousin uses a hybrid two Zen 4 + four Zen 4c CPU configuration and a much smaller iGPU with two RDNA 3 WGPs. We’ve covered the Ryzen Z1’s CPU side in a <a href="https://chipsandcheese.com/2024/02/12/amds-mild-hybrid-strategy-ryzen-z1-in-asuss-rog-ally/">prior article</a>. Here, we’ll be going over the iGPU.</p>
<p>Compared to Radeon 780M in the Ryzen Z1 Extreme, the Ryzen Z1’s Radeon 740M is much smaller. It’s also smaller than the Steam Deck’s iGPU, which uses four RDNA 2 WGPs. However, Ryzen Z1 does enjoy AMD’s newer RDNA 3 architecture. It’s also allowed to boost to very high clock speeds, while the Steam Deck’s iGPU is limited to 1.6 GHz.</p>
<h2>Compute Throughput</h2>
<p>WGPs, or Workgroup Processors, are the basic building blocks of AMD’s RDNA 3 graphics architecture. RDNA 3 introduces dual issue capability for a variety of common FP32 instructions, doubling the theoretical FP32 throughput. Shader programs can leverage its dual issue capability by using wave64 mode or special dual issue instructions in wave32 mode. On RDNA hardware, pixel shaders often use wave64 mode, in which 2048-bit vectors execute on the WGP’s 1024-bit execution units over 2 clock cycles, but achieve 1 instruction per cycle throughput on operations with dual issue support. Wave32 mode lets individual threads (waves) finish faster as 1 instruction per cycle throughput becomes the general case. However, taking advantage of RDNA 3’s extra FP32 units in wave32 mode requires the compiler to find dual issue pairs. That requires instruction-level parallelism within a basic block, and could be upended by register cache source port or result bus limitations.</p>
<p>I’ll be using Nemes’s Vulkan benchmark suite because the Steam Deck’s iGPU doesn’t support OpenCL, in which my own tests are written. AMD’s compiler uses wave64 for the instruction rate tests in this suite, making it a good showcase for RDNA 3’s increased FP32 throughput.</p>
<div>
<figure><a href="https://chipsandcheese.com/rz1_vk_fp/"><img decoding="async" width="682" height="1300" data-attachment-id="26148" data-permalink="https://chipsandcheese.com/rz1_vk_fp/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_fp.png?fit=682%2C1300&amp;ssl=1" data-orig-size="682,1300" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rz1_vk_fp" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_fp.png?fit=682%2C1300&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_fp.png?fit=682%2C1300&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_fp.png?resize=682%2C1300&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>Even though the Radeon 740M has only two WGPs, dual issue capability and much higher clock speeds give it more FP32 throughput than the Steam Deck’s iGPU. When RDNA 3’s dual issue ability doesn’t come into play, the two chips are much closer. That applies to special functions like inverse square roots. FP16 throughputs are nearly identical as well, because both RDNA 2 and RDNA 3 can pack two FP16 values into the low and high halves of a 32-bit register and do FP16 math at double rate.</p>
<p>AMD’s higher-end Ryzen Z1 Extreme dramatically outpaces the Ryzen Z1’s iGPU in all categories. The Z1 Extreme enjoys the same RDNA 3 advantages as the Z1, and also runs at high clock speeds.</p>
<div>
<figure><a href="https://chipsandcheese.com/rz1_vk_int/"><img decoding="async" width="675" height="778" data-attachment-id="26149" data-permalink="https://chipsandcheese.com/rz1_vk_int/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_int.png?fit=675%2C778&amp;ssl=1" data-orig-size="675,778" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rz1_vk_int" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_int.png?fit=675%2C778&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_int.png?fit=675%2C778&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_int.png?resize=675%2C778&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>Games primarily use floating point operations, but integer instructions often show up in shader code too. There, RDNA 3 largely behaves like RDNA 2. The Ryzen Z1’s iGPU again uses high clock speeds to catch up with the Steam Deck’s nominally larger iGPU.</p>
<h2>Cache and Memory Latency</h2>
<p>GPUs use massive thread-level parallelism to hide memory latency. RDNA 2 and 3’s SIMDs are capable of 16-way SMT as long as enough vector register and local data share capacity are available. A WGP with four SIMDs can thus track up to 64 independent instruction streams. But even with thread-level parallelism and a bit of instruction-level parallelism mixed in, memory latency often limits performance.</p>
<p>Therefore, cache is critical for performance. RDNA uses a sophisticated triple-level cache hierarchy. Each half of a WGP gets a L0 vector cache. A set of WGPs share a L1 mid-level cache, and a L2 cache is shared across the entire GPU. Discrete RDNA 2 and RDNA 3 cards additionally have a large Infinity Cache. For example, the RX 6900 XT has 128 MB of Infinity Cache.</p>
<div>
<figure><a href="https://chipsandcheese.com/rz1_vk_vec_latency/"><img decoding="async" width="688" height="297" data-attachment-id="26151" data-permalink="https://chipsandcheese.com/rz1_vk_vec_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_vec_latency.png?fit=1125%2C485&amp;ssl=1" data-orig-size="1125,485" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rz1_vk_vec_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_vec_latency.png?fit=1125%2C485&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_vec_latency.png?fit=688%2C297&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_vec_latency.png?resize=688%2C297&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_vec_latency.png?w=1125&amp;ssl=1 1125w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_vec_latency.png?resize=768%2C331&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Higher clock speeds and RDNA 3’s larger L0/L1 sizes give the Ryzen Z1 a latency advantage over the Steam Deck. Ryzen Z1’s lead continues as the test spills into DRAM. ROG Ally’s LPDDR5 memory configuration doesn’t suffer from high latency like the Steam Deck. We saw that on the CPU side already, and testing confirms the same on the GPU side.</p>
<p>AMD GPUs since GCN have a separate scalar memory path to load values constant across a wavefront. The scalar path helps take load off the vector caches, and is better optimized for latency. RDNA 3 has a 16 KB first-level scalar cache just like prior AMD GPUs, but the larger 256 KB L1 still helps.</p>
<div>
<figure><a href="https://chipsandcheese.com/rz1_vk_scalar_latency/"><img loading="lazy" decoding="async" width="688" height="297" data-attachment-id="26162" data-permalink="https://chipsandcheese.com/rz1_vk_scalar_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_scalar_latency.png?fit=1125%2C485&amp;ssl=1" data-orig-size="1125,485" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rz1_vk_scalar_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_scalar_latency.png?fit=1125%2C485&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_scalar_latency.png?fit=688%2C297&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_scalar_latency.png?resize=688%2C297&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_scalar_latency.png?w=1125&amp;ssl=1 1125w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_scalar_latency.png?resize=768%2C331&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>As with the vector path, the Ryzen Z1’s iGPU enjoys better scalar memory latency than the Steam Deck’s iGPU thanks to higher clocks. Compared to the Ryzen 780M in the Z1 Extreme, the Z1’s Ryzen 740M shows similar latency characteristics. However, AMD’s lower-end part does see L2 cache capacity cut from 2 MB to 1 MB. Valve’s Steam Deck also has 1 MB of L2 for the iGPU, as do older AMD iGPUs like the Vega iGPU in Renoir. The Z1 Extreme may need a larger L2 because its higher compute throughput requires more bandwidth. Higher L2 hitrate is a good way to achieve that higher bandwidth.</p>
<h2>Cache and Memory Bandwidth</h2>
<p>GPUs need high bandwidth to keep their wide vector execution units fed. The Ryzen Z1’s iGPU has first-level cache bandwidth similar to the Steam Deck’s iGPU, but gets there via high clocks instead of having more cache instances. As the test spills out into L1 and L2, the Radeon 740M maintains that high bandwidth because those lower level caches also run at higher clocks. In contrast, the Steam Deck’s iGPU and the Ryzen Z1 Extreme’s Radeon 780M have noticeably less L1 and L2 bandwidth per Compute Unit. Of course, the Z1 Extreme has higher bandwidth at each cache level and a larger L2 cache.</p>
<div>
<figure><a href="https://chipsandcheese.com/rz1_vk_bw/"><img loading="lazy" decoding="async" width="688" height="297" data-attachment-id="26164" data-permalink="https://chipsandcheese.com/rz1_vk_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_bw.png?fit=1122%2C484&amp;ssl=1" data-orig-size="1122,484" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rz1_vk_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_bw.png?fit=1122%2C484&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_bw.png?fit=688%2C297&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_bw.png?resize=688%2C297&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_bw.png?w=1122&amp;ssl=1 1122w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_bw.png?resize=768%2C331&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>iGPUs are traditionally limited by their DRAM configuration, which has to be shared with the CPU. LPDDR4 and LPDDR5 provide a large bandwidth increase compared to the DDR4 setups of years ago, enabling larger iGPU designs and PC gaming handhelds. The non-Extreme Ryzen Z1 gets similar LPDDR5 benefits without being a large iGPU, and thus gets a very good memory bandwidth to compute ratio. </p>
<div>
<figure><a href="https://chipsandcheese.com/rz1_bytes_per_flop/"><img loading="lazy" decoding="async" width="688" height="396" data-attachment-id="26245" data-permalink="https://chipsandcheese.com/rz1_bytes_per_flop/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_bytes_per_flop.png?fit=712%2C410&amp;ssl=1" data-orig-size="712,410" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rz1_bytes_per_flop" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_bytes_per_flop.png?fit=712%2C410&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_bytes_per_flop.png?fit=688%2C396&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_bytes_per_flop.png?resize=688%2C396&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>Measured bandwidth at 512 MB test size divided by measured FP32 FMA FLOPs. If we don’t consider RDNA 3’s FP dual issue capability, the 740M would have 0.06 DRAM bytes per FP32 FLOP.</figcaption></figure></div>
<p>In contrast, Ryzen Z1 Extreme’s very large and fast iGPU outpaces advances in memory bandwidth. DRAM bytes per FLOP is low compared to the Infinity Cache equipped RX 6900 XT, even if we factor out RDNA 3’s dual issue capability. That’s why the Radeon 780M gets a 2 MB L2 cache. The Steam Deck’s iGPU and the Ryzen Z1’s Radeon 740M in contrast make do with a 1 MB L2 because they have ample memory bandwidth relative to their compute throughput.</p>
<h2>CPU to GPU Link Bandwidth</h2>
<p>Integrated GPUs are often less powerful than their discrete cousins thanks to DRAM limitations. However, they do have an advantage when moving data between CPU and GPU memory spaces because they won’t be restricted by a relatively slow PCIe link. </p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=26195"><img loading="lazy" decoding="async" width="688" height="529" data-attachment-id="26195" data-permalink="https://chipsandcheese.com/2024/02/25/ryzen-z1s-tiny-igpu/rz1_vk_link-1/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_link-1.png?fit=758%2C583&amp;ssl=1" data-orig-size="758,583" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rz1_vk_link-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_link-1.png?fit=758%2C583&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_link-1.png?fit=688%2C529&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/rz1_vk_link-1.png?resize=688%2C529&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>All three AMD APUs tested here achieve similar performance, with the Steam Deck’s APU technically pulling ahead when the copy engine is in use. Achieved bandwidth is well above the 32 GB/s available with a PCIe 4.0 x16 link, so these integrated GPUs could have an advantage if the CPU and GPU need to communicate a lot. That’s unlikely to matter for gaming, but it could matter if a compute application has to process GPU-generated results on the CPU.</p>
<p>If we use a compute shader to move data between CPU and GPU memory, the Ryzen Z1 does fall behind a bit. That’s likely because its smaller shader array can’t keep as much work in flight to hide latency. Using a CPU-side <code>memcpy</code> to move data between host memory and a buffer mapped to GPU memory results in very low bandwidth. CPU cores are less latency tolerant than GPU ones, and there could be other inefficiencies when CPU cores directly access GPU memory.</p>
<h2>Final Words</h2>
<p><a href="https://www.youtube.com/watch?v=Lg624-NHqcw">Gamers Nexus</a> notes that ASUS’s non-Extreme ROG Ally gives up a lot of GPU performance for a $100 price drop, especially when the Ryzen Z1 Extreme’s iGPU can stretch its legs in docked mode. In that respect, it’s similar to the Steam Deck, which similarly <a href="https://www.eurogamer.net/digitalfoundry-2023-asus-rog-ally-vs-steam-deck-review?page=4">gets outpaced by the Z1 Extreme</a> when the latter is given enough power budget. On the flip side, the gap narrows on battery power. There, the Z1’s smaller iGPU can maintain high clocks even with a smaller power budget.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=26203"><img loading="lazy" decoding="async" width="688" height="388" data-attachment-id="26203" data-permalink="https://chipsandcheese.com/2024/02/25/ryzen-z1s-tiny-igpu/gn_z1_z1_extreme/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/gn_z1_z1_extreme.jpg?fit=1314%2C741&amp;ssl=1" data-orig-size="1314,741" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gn_z1_z1_extreme" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/gn_z1_z1_extreme.jpg?fit=1314%2C741&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/gn_z1_z1_extreme.jpg?fit=688%2C388&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/gn_z1_z1_extreme.jpg?resize=688%2C388&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/gn_z1_z1_extreme.jpg?w=1314&amp;ssl=1 1314w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/gn_z1_z1_extreme.jpg?resize=768%2C433&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/02/gn_z1_z1_extreme.jpg?resize=1200%2C677&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>That leaves me with mixed feelings about the Ryzen Z1 as a gaming chip. Its Radeon 740M is a demonstration of how high clocks can let a small GPU go far. On the other hand, this “speed demon” advantage only works when tight power budgets prevent larger GPUs from reaching similar clocks. Performance on battery is definitely important for a portable device like the ROG Ally or Steam Deck. But those devices can spend a lot of time plugged into the wall at the airport or coffee shops. Even car and airplane seats have power outlets available, so a handheld can run in turbo mode on the go.</p>

<p>For those situations, the iGPU in the Ryzen Z1 feels small for a gaming first device. The two-WGP Radeon 740M is only one step up from the minimal single WGP setup in Zen 4 desktop CPUs (Raphael). For sure, the Radeon 740M has a fully fleshed out cache setup instead of Raphael’s minimal 64 KB L1 and 128 KB L2. But even Renoir from a few years ago has a wider iGPU. Meanwhile, Ryzen Z1’s CPU is very strong for a low-power chip. Two Zen 4 cores provide excellent responsiveness, while the four Zen 4c cores maintain good multi-threaded performance.</p>
<p>Ryzen Z1’s priorities can be seen in die area allocated for the iGPU’s WGPs versus its CPU cores. TechPowerUp states the Ryzen Z1 occupies 137 mm<sup>2</sup>. Pixel counting indicates the two WGPs occupy about 5.1 mm<sup>2</sup> of area, while the Zen 4 and Zen 4c cores occupy 17.2 mm<sup>2</sup> (not counting shared cache). The Van Gogh APU in Valve’s Steam Deck in contrast uses 10.9 mm<sup>2</sup> to implement four Zen 2 cores and 17.7 mm<sup>2</sup> for four RDNA 2 WGPs. GPU performance is often more important than the CPU side, especially in handhelds that aren’t expected to hit high frame rates. Therefore, the Ryzen Z1’s die area allocation is strange for a handheld focused chip.</p>

<p>However, packing six Zen 4(c) cores worth of CPU power is great if you need a very low power, very small chip for mobile device that focuses on productivity first and gaming second. Ryzen Z1 can serve in a handheld console in a pinch, and I don’t think ASUS made the wrong decision to use it in the ROG Ally. But I think Ryzen Z1 would be more at home in a small ultrabook or convertible. And it’d be cool to see AMD’s small APU shine in such a device.</p>
<p>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JSTOR is Now Available in 1k Prisons (140 pts)]]></title>
            <link>https://about.jstor.org/news/jstor-available-in-1000-prisons/</link>
            <guid>39513126</guid>
            <pubDate>Mon, 26 Feb 2024 16:22:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://about.jstor.org/news/jstor-available-in-1000-prisons/">https://about.jstor.org/news/jstor-available-in-1000-prisons/</a>, See on <a href="https://news.ycombinator.com/item?id=39513126">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="tocontent">
		<article id="post-30129">
			<div data-sc="content:news">
					<h3>More than 500,000 incarcerated learners now have access to the digital library</h3>
<p>At the end of 2023, <a href="https://www.jstor.org/" rel="noopener" target="_blank">JSTOR</a>—a vast digital library of secondary and primary sources to support teaching and learning—reached a once unimaginable goal: providing JSTOR access in 1,000 prisons. Spread across four continents, the <a href="https://about.jstor.org/jstor-access-in-prison/" rel="noopener" target="_blank">JSTOR Access in Prison</a> initiative now supports the education and growth of more than 550,000 incarcerated people.  </p>
<p>Incarcerated learners have been left behind for decades. Limited access to the internet and scarce funding and support for higher education in prisons made access to digital libraries like JSTOR all but impossible. In October 2021, with funding from the Mellon Foundation, JSTOR <a href="https://www.ithaka.org/news/mellon-foundation-awards-ithaka-1-5-million-to-make-jstor-accessible-to-incarcerated-college-students/" rel="noopener" target="_blank">set an ambitious goal</a> to change that. The aspiration? For every incarcerated college student in the United States to have access to JSTOR, along with the research skills to use it and other digital resources. </p>
<p>Prior to 2021, JSTOR developed an offline index of its digital library. At the time, less than twenty prisons had access to it. Since then, developers have created an online version that meets the unique needs of carceral settings, most recently delivering online access on tablets. These changes—and the leadership of Stacy Burnett, a graduate of the Bard Prison Initiative who was hired to lead the JSTOR Access in Prison initiative—have enabled 1,000 prisons and more than 500,000 people to gain access to the digital equivalent of a college library. </p>
<p>“Creating more equitable learning environments inside prisons is the best way to pay forward my own prison-based education,” said Burnett. “We have proven that through understanding, collaboration, and creativity, we can create workable solutions that deliver meaningful digital equity and information literacy for incarcerated people.”</p>
<p>Evidence supports the fact that JSTOR use among the incarcerated is strong and growing. The students in Ohio prisons have reviewed nearly 30,000 unique articles over the past year, with 10% in the last month. Access there will be expanded to the prison general libraries. The students at Tennessee Higher Education Initiative regularly access 2,400 unique articles in a month—the highest per capita use in the country for incarcerated learners. Those pursuing higher education use JSTOR for research and coursework. Others use JSTOR to pursue a passion for learning and to gain skills they will need once they re-enter society. </p>
<p>L. Elizabeth Shatswell, a soon-to-be graduate of the University of Puget Sound, spent nearly two decades pursuing her high school and college education behind bars. She was able to conduct research on JSTOR that helped her advocate for an elderly incarcerated woman who required humane patient care. That experience changed Shatswell’s life. She said, “Prior to JSTOR, I knew full well there were disparities in access to healthcare for both women and incarcerated populations, but I did not know the scope. JSTOR facilitated the development of my voice in understanding these gaps and as a result I founded a program called the HOPE Team that uses evidence-based research to make changes to healthcare policy and practice.” </p>
<p>Prison access to JSTOR is free thanks to subsidies from ITHAKA, the nonprofit home of JSTOR, and generous grants from Ascendium and Mellon Foundation. Giving incarcerated learners access to JSTOR is impactful, but it hasn’t been easy. There are challenges in navigating different cultures and decision-making processes, and finding creative solutions to issues in technology infrastructure and support, which vary widely from one site to the next. </p>
<p>“It was really difficult in the beginning,” said Burnett. “Fortunately, we’ve worked with enough prisons now that when we encounter a question or issue, we’ve seen it before and have a solution. We’re making it easier and easier for them to say ‘yes’.”  </p>
<p>Burnett added, “After all, lowering barriers to access to knowledge and education is exactly what JSTOR and ITHAKA were founded to do.”</p>
<p>JSTOR hopes to reach the 400 American state and federal prisons without JSTOR and 1 million learners in 2024. </p>
<p>To learn more, visit <a href="https://about.jstor.org/jstor-access-in-prison/" rel="noopener" target="_blank">JSTOR Access in Prison</a>.</p>
<p><strong>Media contact:</strong><br>
Heidi McGregor<br>
VP, Communications<br>
ITHAKA<br>
<a href="mailto:heidi.mcgregor@ithaka.org" rel="noopener" target="_blank">heidi.mcgregor@ithaka.org</a>  </p>
<p><strong>About JSTOR</strong></p>
<p>JSTOR is a part of <a href="https://www.ithaka.org/" rel="noopener" target="_blank">ITHAKA</a>, a nonprofit organization with a mission to improve access to knowledge and education for people around the world. As a nonprofit that believes in the power of knowledge to change the world for the better, JSTOR partners with libraries, museums, and publishers to reduce costs, extend access, and preserve scholarship for the future as affordably and sustainably as possible. At JSTOR, we strengthen the depth and quality of research by bringing together journals, books, images, and primary sources on a platform with unique tools for teaching and exploration. We do this because we believe in the power of knowledge to  change the world for the better.</p>

				</div>

		</article>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What a major solar storm could do (150 pts)]]></title>
            <link>https://www.newyorker.com/magazine/2024/03/04/what-a-major-solar-storm-could-do-to-our-planet</link>
            <guid>39513051</guid>
            <pubDate>Mon, 26 Feb 2024 16:17:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newyorker.com/magazine/2024/03/04/what-a-major-solar-storm-could-do-to-our-planet">https://www.newyorker.com/magazine/2024/03/04/what-a-major-solar-storm-could-do-to-our-planet</a>, See on <a href="https://news.ycombinator.com/item?id=39513051">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Ken Tegnell’s first home was on Alcatraz. At the time—this was in the nineteen-fifties—there was, in addition to the federal penitentiary, a preschool, a post office, and housing for prison employees and their family members. That included Tegnell, who lived with his mother and grandfather, a guard, while his father was stationed in Korea. The whole of Alcatraz Island is less than a tenth of a square mile, so, despite all the security measures and “<em>DO NOT ENTER</em>” signs, the <a href="https://www.newyorker.com/news/afterword/one-of-the-last-men-who-served-time-at-alcatraz">inmates</a> and civilians were never very far apart. Yet even given the proximity to the likes of <a href="https://www.newyorker.com/magazine/2015/09/21/assets-and-liabilities">Whitey Bulger</a>, it was a peaceful place to live. The view was spectacular, almost none of the non-incarcerated residents locked their doors, and almost all of them knew one another and shared the camaraderie of an unusual identity. “We were an odd group of people,” Tegnell jokes, “and that’s why I’m strange the way I am.”</p><p>When Tegnell’s father returned from Korea, the family moved away, and then moved often. But eventually Tegnell returned to the Bay Area—this time to attend Berkeley, which, by the late nineteen-sixties, was another island of odd people. While taking an astronomy course there, he attended a lecture by a not yet famous scientist named Carl Sagan. Interested in things that happen in the sky and unmoved by the hippie culture around him, Tegnell joined the Air Force, in 1974. The military taught him to use telescopes and radio arrays, then sent him to the Learmonth Solar Observatory, at the northwestern tip of Australia, to gather data about the sun. He served two tours there, twelve hours from anything that could be called a city—a godforsaken place, as Tegnell recalls it, but gorgeous, with beautiful beaches, terrific fishing, and almost no rainfall year-round. Whether working or playing, he spent his days there looking at the sun.</p><p>That is still how Tegnell makes a living, although he hung up his wings in 1996. Today, his job is simultaneously so obscure that most people have never heard of it and so important that virtually every sector of the economy depends on it. His official title, one shared by no more than a few dozen Americans, is space-weather forecaster. Ever since leaving the Air Force, Tegnell has worked&nbsp;for the National Oceanic and Atmospheric Administration’s Space Weather Prediction Center, in Boulder, Colorado: ten hours a day, forty hours a week, three decades spent staring at real-time images of the sun. Eleven other forecasters work there as well. The remaining ones are employed by the only similar institution in the country: the Space Weather Operations Center, run by the Department of Defense on Offutt Air Force Base, in Sarpy County, Nebraska.</p><p>Regular, Earth-based weather is such a fundamental part of our lives that we are almost always aware of it and very often obsessed with it; it is the subject of everything from idle chitchat to impassioned political debate. By contrast, most people have no idea that there <em>is</em> weather in outer space, let alone what its fluctuations might mean for our planet. That’s because, unlike everyday weather, you can’t experience space weather directly. It doesn’t make you hot or cold, doesn’t flood your basement or take the roof off your home. In fact, until the nineteenth century, it had almost no appreciable effect whatsoever on human activity. Then came a series of scientific revolutions that made certain technologies, from electricity to telecommunications, central to our lives. Only later did we realize that those technologies are vulnerable to the effects of weather in outer space. The potential consequences are as sweeping as our technological dependence. In 2019, the Federal Emergency Management Agency, surveying the landscape of possible disasters, concluded that only two natural hazards have the capacity to simultaneously affect the entire nation. One is a <a href="https://www.newyorker.com/tag/coronavirus">pandemic</a>. The other is a severe solar storm.</p><p>That is why Tegnell’s job is so important. But “space-weather forecaster” is an optimistic misnomer; for the most part, he and his colleagues can’t predict what will happen in outer space. All they can do is try to figure out what’s happening there right now, preferably fast enough to limit the impact on our planet. Even that is difficult, because space weather is both an extremely challenging field—it is essentially applied astrophysics—and a relatively new one. As such, it is full of many lingering scientific questions and one looming practical question: What will happen here on Earth when the next huge space storm hits?</p><p>The first such storm to cause us trouble took place in 1859. In late August, the aurora borealis, which is normally visible only in polar latitudes, made a series of unusual appearances: in Havana, Panama, Rome, New York City. Then, in early September, the aurora returned with such brilliance that gold miners in the Rocky Mountains woke up at night and began making breakfast, and disoriented birds greeted the nonexistent morning.</p><p>This lovely if perplexing phenomenon had an unwelcome corollary: around the globe, telegraph systems went haywire. Many stopped working entirely, while others sent and received “fantastical and unreadable messages,” as the Philadelphia <em>Evening Bulletin</em> put it. At some telegraph stations, operators found that they could disconnect their batteries and send messages via the ambient current, as if the Earth itself had become an instant-messaging system.</p><p>Owing to a lucky coincidence, all these anomalies were soon linked to their likely cause. At around noon on September 1st, the British astronomer Richard Carrington was outside sketching a group of sunspots when he saw a burst of light on the surface of the sun: the first known observation of a solar flare. When accounts of the low-latitude auroras started rolling in, along with reports that magnetometers—devices that measure fluctuations in the Earth’s magnetic field—had surged so high they maxed out their recording capabilities, scientists began to suspect that the strange things happening on Earth were related to the strange thing Carrington had seen on the sun.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a28305&quot;}" href="https://www.newyorker.com/cartoon/a28305" rel="noopener" target="_blank"><picture></picture></a><p><span>“This recipe turned out awful despite my substituting every major ingredient.”</span></p><p><span>Cartoon by Mads Horwath</span></p></div></span></p></figure><p>Wonderment over the Carrington Event, as it is now known, faded almost as quickly as the auroras—but sixty years later it happened again. In May, 1921, dazzling lights filled the night sky in places as far from the poles as Texas and Samoa; this time, too, spectacle was followed by debacle. “Electric fluid” leaping from a telegraph switchboard set on fire a railroad station in Brewster, New York, while stray voltage on railway signal and switching systems halted trains in Manhattan and, farther north, started a fire at Albany’s Union Station.</p><p>Over the years, at odd intervals, this pattern kept repeating: brilliant night skies followed by troubling consequences, which changed in concert with evolving technologies. Teletype machines ceased to operate; or transatlantic cables stopped working; or worldwide radio circuits fell silent; or hundreds of thousands of miles of transmission lines used to send and receive wire stories all went down at the same time. In May, 1967, all three radar sites of the Ballistic Missile Early Warning Systems then maintained by the U.S. Air Force appeared to have been jammed; worried that the Soviet Union was on the verge of attacking, military officials nearly scrambled nuclear-equipped aircraft. Five years later, during the Vietnam War, the United States started sowing the waters outside North Vietnamese seaports with mines that had magnetic sensors, to trigger explosions when steel-hulled vessels passed overhead. Three months after that program began, many of those mines—four thousand of them, according to one contemporaneous source—detonated almost simultaneously. An investigation determined that the plan had been compromised not by Hanoi but by a newly discovered solar phenomenon called a coronal mass ejection.</p><p>In time, aided by each new technological difficulty, astrophysicists began to piece together a better understanding of the weather in outer space. But science can take a long time to make inroads into public awareness, let alone public policy, so space weather remained a mostly marginal subject until 2008, when the National Academy of Sciences convened a group of experts to assess the nation’s capacity to endure its terrestrial effects. Later that year, the N.A.S. published a <a href="https://nap.nationalacademies.org/catalog/12507/severe-space-weather-events-understanding-societal-and-economic-impacts-a">report</a> on the findings, “Severe Space Weather Events: Understanding Societal and Economic Impacts.”</p><p>The title was dry; the contents were not. The report noted that the Earth hadn’t experienced a Carrington-size storm during the space age, or, for that matter, during the age of widespread electrification, and that much of the country’s critical infrastructure seemed unlikely to withstand one. Extensive damage to satellites would compromise everything from communications to national security, while extensive damage to the power grid would compromise <em>everything:</em> health care, transportation, agriculture, emergency response, water and sanitation, the financial industry, the continuity of government. The report estimated that recovery from a Carrington-class storm could take up to a decade and cost many trillions of dollars.</p><p>That report made headlines, and also made its way to <a href="https://www.newyorker.com/tag/barack-obama">President Barack Obama</a>—who by then had appointed a new <em>FEMA</em> administrator, a man named Craig Fugate. At the time, very few people even within the emergency-response community knew much about space weather. But, by chance, Fugate had crossed paths with the Space Weather Prediction Center earlier in his career; interested in the center’s work, he had made himself into something of a space-weather expert.</p><p>As a result, when the White House came knocking to ask if it should be concerned about the N.A.S. report, Fugate was in a position to offer an emphatic yes. The question, for him, wasn’t whether a major solar storm posed a risk to the nation; it was how best to prepare for it beforehand and recover from it afterward. And so, as he began settling into his job, and getting to know the rest of the senior leaders at <em>FEMA</em>, he made a habit of presenting them with a hypothetical situation. “I asked them what they would do if there was a G5 storm,” Fugate told me, referring to the highest classification on the <em>NOAA</em> Space Weather Scale, akin to an F5 tornado or a Category 5 hurricane. “And they go, ‘What’s a G5 storm?’&nbsp;” <em>Hoo boy</em>, Fugate remembers thinking. <em>We got a problem</em>.</p><p>In space weather, every day is a sunny day. There is no interstellar rain, no interplanetary snow, no sleet spinning off the rings of Saturn; all the phenomena we call space weather originate on the sun. And so, to start, you must shed the idea—implicit in our meteorology and omnipresent in our metaphors—that the sun is a mild and beneficent force, a bestower of good moods and great tans.</p><p>In reality, the sun is an enormous thermonuclear bomb that has been exploding continuously for four and a half billion years. Its inner workings are imperfectly understood even by heliophysicists, who sometimes sound less like scientists than like nineteen-fifties comic-book heroes, enthusiastically invoking things like flux tubes and convection zones and galactic-cosmic-ray dropouts. Fortunately, for our purposes, the only two solar phenomena you need to understand are solar flares and coronal mass ejections, both of which stem from the same thing: a buildup of energy in the magnetic field of the sun.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>You are probably familiar with the Earth’s magnetic field, which makes all life here possible by deflecting dangerous radiation from outer space. If you could see that field, it would look like a relatively tidy series of rings surrounding our planet, flowing out at the South Pole and reëntering at the North. The solar magnetic field does not look like that. That’s largely because, although the sun is three hundred thousand times more massive than the Earth, no part of it is solid. Instead, it is made of plasma, that strange and mesmerizing fourth state of matter. (Heat up a liquid and it turns into a gas. Heat up a gas and it turns into a plasma, a glowing slurry of electrically charged particles.) As a result, the sun doesn’t have to rotate rigidly, as our planet must. One rotation of the Earth takes twenty-four hours in both Ecuador and Antarctica, but one rotation of the sun takes approximately twenty-five days at its equator and thirty-three days at its poles.</p><p>This uneven rotation wreaks havoc on the sun’s magnetic field. Imagine a race in which eight people are lined up on a track, holding on to the same long elastic ribbon. The starting gun fires and the people start running. The two in the middle are the fastest and the two on the ends are the slowest, so after a while the middle two are far ahead and the ribbon looks like this: &gt; . If the race kept going and the runners’ speeds remained constant, the two middle runners would eventually lap the others, and the ribbon would cross over itself. The longer the race lasted, the more tangled the ribbon would become.</p><p>That’s what happens to solar-magnetic-field lines. They twist and crisscross until clusters of them pop up from the sun’s surface, in huge loops that generate enormous amounts of energy. (Think of the energy stored in a rubber band when it is twisted and stretched. Now imagine that the rubber band is a hundred thousand miles long.) The ends of these loops are sunspots, the phenomenon that Carrington observed in 1859. He could see them readily enough for two reasons. The first is that they are darker than their surroundings, because they are a couple of thousand degrees cooler; the intensity of their magnetic fields hinders the flow of hot gas across the sun. The second is that they are large. An average sunspot is the size of the Earth, while the biggest ones can be ten times larger.</p><p>Forecasters like Ken Tegnell watch sunspots for the same reason that regular <a href="https://www.newyorker.com/magazine/2019/07/01/why-weather-forecasting-keeps-getting-better">meteorologists</a> watch low-pressure areas in the tropics: to see if a storm is forming. This happens when one of those twisted magnetic fields suddenly rips apart, then snaps back together again. That rearrangement returns the magnetic field to a more stable, lower-energy state, while releasing the excess energy into space in two different forms. The first is a solar flare: a burst of radiation that can range across the electromagnetic spectrum, from gamma rays and X rays to radio waves and visible light. Solar flares contain a colossal amount of energy—enough, in a large one, to meet our planet’s power needs for the next fifteen or twenty thousand years. The second is a coronal mass ejection: a billion-ton bubble of magnetized plasma that explodes off the surface of the sun. These two phenomena can occur separately, but when large ones occur together they mark the beginning of a major solar storm.</p><p>The forecasting room of the Space Weather Prediction Center is a dimly lit ground-floor office with no exterior windows. Nonetheless, in a sense, sunlight is everywhere. Banks of monitors run the length of one wall, filled with real-time images of the sun. Some show only the disk, others only the corona, others the entire star filtered through different wavelengths of light, turning it pale pink and brilliant yellow, electric blue and neon green. Two large images in the center show the sun as a writhing riot of orange and gold, the loops and filaments of its magnetic field lines rendered visible not by scientific instruments but by its own plasma, which is drawn to those field lines the way iron filings are drawn to bar magnets. Viewed this way, the sun does not make you want to grab a paperback and lie in a hammock. It looks like a volcanic eruption as seen from deep inside the caldera; it looks like a wildfire raging beneath forty billion hurricanes; it looks like, when it is over, there will be no survivors.</p><p>Surrounded by all of this, unfazed, Tegnell is logging in for his shift. In the hallway just outside, a mannequin stands upright in a <em>NASA</em> uniform. The uniform is the old-school, pale-blue kind, and the mannequin is pale and old school, too—crewcut, chisel-jawed, permanently twentysomething. Tegnell does not look like that. Bigger, bearded, older, he looks like the guy in the disaster movie who has the right combination of grit, experience, and indifference to authority to save the day. At present, he is eye level with a brace of computers, the screen of each one covered in flowing lines, as if the solar system were hooked up to half a dozen heart-rate monitors.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a22925&quot;}" href="https://www.newyorker.com/cartoon/a22925" rel="noopener" target="_blank"><picture></picture></a><p><span>“O.K., so if we share a ride and cut out all the singing, we just might be able to make it to the Emerald City in time for happy hour.”</span></p><p><span>Cartoon by Lonnie Millsap</span></p></div></span></p></figure><p>Some of the information filling those screens comes from terrestrial observatories, like the one where Tegnell used to work. The rest comes from space-based equipment on satellites, managed, variously, by <em>NASA</em>, <em>NOAA</em>, and the European Space Agency. Most of those satellites are in orbit twenty-two thousand miles above the Earth, a hundred times farther away than the International Space Station; a few are in orbit a million miles away, or about one per cent of the distance to the sun. From these outposts, they transmit data to the forecasting room, where it is Tegnell’s responsibility to interpret the contents, detect anything unusual, issue twice-daily forecasts, and, when necessary, activate a suite of watches and warnings.</p><p>Tegnell loves his job best when nothing is happening in the room—no groups of engineers trekking through, no stray journalists hanging around—but when many things are happening up in the sky. That makes some stretches of his professional life duller than others, because sunspots follow an eleven-year cycle, during which their activity goes from infrequent (solar minimum) to frequent (solar maximum). We are currently headed toward solar maximum, with activity on the sun expected to peak sometime between now and 2025. That cycle is not wholly determinative; a solar maximum can pass by uneventfully, while a powerful storm can happen during solar minimum.</p><p>Still, solar maximum does tend to make Tegnell’s job more interesting. As we talk, an automated voice keeps informing him that a flare has been detected, with the same impassive insistence of Siri saying, “Proceed to the route.” Tegnell ignores it, having already determined that the flare is too small to produce any effects on Earth, except possibly some auroras for people living near polar latitudes. (Auroras are the only pleasant by-product of charged particles entering our atmosphere, where they’re channelled north and south along magnetic-field lines and interact with nitrogen and oxygen molecules, causing them to produce interesting colors.) But then something else leaps off the edge of the sun: a fountain of plasma that looks, to my untrained eye, enormous. “It <em>is</em> enormous,” Tegnell affirms. “It’s just incredible.” It is not, however, headed toward the Earth.</p><p>“I know,” Tegnell’s colleague Bill Murtagh says as he watches me watching. “It’s stunning. I’ve been doing this for twenty-five years and I’ve never yet found it boring.” Like Tegnell, Murtagh arrived at the Space Weather Prediction Center via the U.S. Air Force, albeit more circuitously, as his Irish accent suggests. (He owes his American citizenship to the fact that he was born during a parental stint in the U.S., where his mother worked for Ogden Nash, taking care of his grandchildren.) Unlike Tegnell, he enjoys collaborating with other people. At <em>swpc</em>—which is pronounced “swipsy,” like “tipsy”—he coördinates space-weather-preparedness efforts with government officials, emergency managers, and the private sector, and he doesn’t mind being loaned out to the White House Office of Science and Technology Policy and working with the National Security Council. When a big storm starts materializing on one of the monitors in the forecasting room, it is Ken Tegnell’s job to notice. It is Bill Murtagh’s job to help minimize the storm’s impact on everything it might derail, damage, or destroy.</p><p>That is a long list, because solar storms affect a broad, strange swath of the human endeavor. For instance, outside the <em>swpc</em> forecasting room, in a glass case displaying old astronomical devices and a statue of a sun god, there is a life-size model of a homing pigeon. Pigeons navigate partly by tracking the Earth’s magnetic field; when it behaves in uncharacteristic ways, a pigeon race can end in a “smash,” the term of art for events in which many birds fail to return home. Since the most highly prized pigeons can be worth more than a million dollars, some pigeon racers have become dedicated subscribers to <em>swpc</em>’s space-weather alerts. Other constituents are interested for even more arcane reasons. One of Murtagh’s favorite phone calls came from a man who wanted to know if it was true that solar storms could interfere with G.P.S. signals. When Murtagh said yes, the man had a follow-up question: How did those storms affect electronic ankle bracelets? (“You know,” Murtagh told the caller, “I’m not too familiar with that technology.”) But the sectors that bear the brunt of bad space weather are anything but niche interest groups. They are the backbone of modern society: telecommunications, aviation, space-based technology, and the power grid.</p><p>Most solar storms do not hit the Earth, for the same reason that most baseballs don’t hit one particular person in the stands. But, when a storm does get here, it gets here fast. Some of the radiation from the solar flare arrives in a little more than eight minutes: the amount of time it takes anything travelling at the speed of light to cross the ninety-three million miles between us and the sun. All that energy smacking into our atmosphere further ionizes the ionosphere, its upper reaches. The result, in a severe storm, is a partial blackout of low-frequency radio wavelengths and a complete blackout of high-frequency wavelengths across the entire side of the Earth that’s facing the sun. Those blackouts, which can last up to several hours, disrupt ham radios, AM radio, ground-to-submarine communications (used by the Navy), backup ground-to-air communications (used by both military and civilian flights), and other backup communication, navigation, and timing systems used for military, government, and maritime purposes.</p><p>That is the first phase of a solar storm. Meanwhile, from the moment they formed, the flare and the coronal mass ejection began transferring energy to any protons and electrons in their path, accelerating them to relativistic or near-relativistic speeds. When those enhanced protons and electrons, known as solar energetic particles, reach our atmosphere, sometimes in just tens of minutes, they form the second phase, known as a solar-radiation storm.</p><p>As that name suggests, a solar-radiation storm can harm humans, although only if they happen to be up in the sky while such a storm is taking place. For people on airplanes flying routes over the poles (where energetic particles, following magnetic-field lines, tend to concentrate), that risk is minor; nonetheless, such flights get space-weather reports from <em>swpc</em> before takeoff, and will typically reroute if a big storm is expected. For astronauts, however, severe radiation storms are more of a concern. Those on the International Space Station benefit from the attenuated but still extant protection of the Earth’s magnetic field, and during extreme radiation events they can take cover in the better-shielded parts of the station. But for those beyond our atmosphere such a storm could be lethal, either immediately or because radiation sickness would render them unable to perform life-critical functions. One obstacle to some of the space exploration currently being contemplated is that the moon and Mars lack a magnetic field to deflect the sun’s radiation; as a result, absent adequate shelter, both are extremely dangerous in a solar storm. Only retroactively did it become apparent how lucky <em>NASA</em> was that no such storms happened during the Apollo missions.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>At the moment, though, the number of people in outer space—fewer than a dozen—pales in comparison with the number of satellites in outer space: more than eight thousand. Like us, those satellites are imperilled by solar-radiation storms. For one thing, solar energetic particles can pass straight into the satellites, physically damaging hardware and hijacking software by randomly changing ones to zeros or zeros to ones. For another, as those particles bombard a satellite, different parts of it can build up different levels of charge, and the electricity can arc from one area to another, attempting to neutralize itself and, in the process, damaging or disabling the onboard electronics.</p><p>Finally, enhanced solar radiation increases the density of certain regions of the Earth’s atmosphere, which increases the drag. This is particularly problematic in lower Earth orbit (up to about twelve hundred miles above the surface of our planet), where more than eighty per cent of all satellites are found. As drag increases, those satellites can shift out of place, leaving both their owners and the North American Aerospace Defense Command scrambling to find them in order to maintain functionality, prevent collisions, and avoid confusion about their identity: unidentified intruder or old friend in a new place? At best, satellites experiencing this drag must use more fuel to maintain orbit, thereby shortening their life spans; that’s why, back in 1979, Skylab crashed to Earth sooner than expected. At worst, they lose orbit entirely, burning up on reëntry. In February of 2022, SpaceX, the space-exploration company co-founded by <a href="https://www.newyorker.com/tag/elon-musk">Elon Musk</a>, launched forty-nine new satellites as part of its <a href="https://www.newyorker.com/magazine/2023/08/28/elon-musks-shadow-rule">Starlink system</a>, which aims to provide sky-based Internet access to paying customers anywhere on Earth. The company knew that a storm had started just before the launch date, but it was a mild one—a G2, the second-lowest category on <em>NOAA</em>’s geomagnetic storm scale—and internal modelling suggested that the satellites would be fine. One day after launch, thirty-eight of them lost orbit and suffered catastrophic failure.</p><p>SpaceX still plans to launch tens of thousands of satellites in the coming years, and other entities are likewise expanding their fleets, deploying space-based technology for everything from wildlife tracking to intelligence gathering. But, of all the satellites in the sky right now, none are more crucial than those which constitute our Global Positioning System—or, to use the more universal term, G.N.S.S., the Global Navigation Satellite System.</p><p>G.P.S. satellites are not endangered by drag, because they are not in lower Earth orbit; up where they hang out, there is not enough atmosphere left to affect them. But, to reach receivers on the ground, signals from those satellites must cross some twelve thousand miles of space. During a solar storm, when our ionosphere is disturbed, those signals get distorted, much the way light bends when it passes through water, leading to location inaccuracies of tens or, in rare cases, hundreds of metres. Those inaccuracies generally self-correct when the storm subsides, and they don’t really matter if you’re using G.P.S. just to remind yourself which exit to take for the airport. But an increasing number of processes require constant access to ultra-precise location data, including military operations, aviation, crop management, bridge building, and oil and natural-gas exploration, especially off deep-sea platforms, where exact positions must be maintained during underwater drilling operations regardless of wave action and drift.</p><p>The more important service provided by the Global Positioning System, however, is not about space but about time: every G.P.S. satellite carries multiple atomic clocks, normally accurate to within a billionth of a second, which transmit hyperaccurate temporal information known as G.P.S. timing signals. Those signals are one of our most essential pieces of invisible infrastructure. Cell-phone companies use them to manage the flow of data over their networks. Media companies use them to broadcast programs, chopping up large data streams into smaller packets to transmit them, then recombining them upon arrival based on the time stamp. Power companies use them to help regulate the flow of electricity from source to destination, protecting against surges and blackouts. Computer applications use them to coördinate any situation in which two or more users are working on the same project in different locations. The financial industry uses them to track mobile banking transactions and to time-stamp every trade—a crucial traffic-control system in a world where hundreds of thousands of financial messages are processed every second.</p><p>Like G.P.S. location accuracy, G.P.S. timing accuracy can suffer during a solar storm. The longer and more severe the storm, the more those errors compound, until the systems that depend on the signals no longer work correctly, or work at all. Backup programs are available; the Federal Aviation Administration, for instance, has alternative capabilities to keep planes flying safely when G.P.S. fails. Over all, though, incorporation of such alternatives remains limited, for a straightforward reason: G.P.S. is a service that our federal government provides free of charge. As the Department of Homeland Security dryly noted in a 2020 report, “Without regulatory requirements or positive benefit-cost equations, adoption of non-G.N.S.S. services is unlikely.”</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a22632&quot;}" href="https://www.newyorker.com/cartoon/a22632" rel="noopener" target="_blank"><picture></picture></a><p><span>Cartoon by Carolita Johnson</span></p></div></span></p></figure><p>In the meantime, our primary source of navigation and timing information remains vulnerable to the vicissitudes of weather on the sun. So do the thousands of other satellites that increasingly fill our skies, courtesy of a young, booming, and largely unregulated industry. This worries the generally unflappable Bill Murtagh. “It’s a Wild West out in space right now,” he says. His assessment of satellite companies is blunt: “I do not think they are ready for a major space-weather event.” If he is right, when that event happens, large portions of our life could be compromised: information, communication, entertainment, economic activity, national security. But all those are our vulnerabilities just in the sky. By most accounts, when the next extreme space storm hits, the real problems will be the ones on the ground.</p><p>If a solar flare is something like the muzzle flash of a cannon, a coronal mass ejection is the cannonball: slower, but more destructive. It takes anywhere from fifteen hours to several days to reach our planet, by which time it has expanded enormously in volume. Once it arrives, it smashes into our magnetosphere, flattening whichever side is facing the sun (that is, the daytime side) and sending the nighttime side streaming away from the Earth, like a wind sock in a gale. If you remember Faraday’s law, you know that moving a magnetic field around produces an electric current. And so it is ultimately the Earth’s own storm-tossed magnetosphere that induces excess electricity in our planet, thereby initiating the third and final phase of a space-weather event: the geomagnetic storm.</p><p>Although that storm can affect anything long and metal (pipelines, railroad tracks), it poses the gravest danger to power grids. In the United States, our grid is divided into three regions. The Eastern Interconnection runs from the East Coast to the Rocky Mountains; the Western Interconnection runs from the Rockies to the Pacific Ocean; Texas, in true Lone Star style, goes it alone. For the most part, power can’t flow from one region to another—which is why, when seventy-five per cent of Texas suffered blackouts during a winter storm in 2021, no outside energy providers could help. But, within each region, electricity flows freely—and so can electrical problems, as when, in 2003, a shorted power line in Ohio caused a blackout across much of the Midwest, the mid-Atlantic, and the Northeast, leaving fifty-five million people in the dark.</p><p>All this infrastructure, which continues across the border into Canada to form the North American Power Grid, is also known as the bulk-power system, because it handles energy transmission, not energy distribution. Distribution involves sending electricity from a local substation to everything nearby that needs it—schools, stoplights, factories, the toaster in your kitchen. Transmission gets power <em>to</em> that substation, from one of the more than six thousand generation facilities on the North American grid (nuclear plants, hydroelectric dams, solar farms, etc.), via more than half a million miles of line.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>The crucial nodes in this vast network are transformers. Power enters your home at a hundred and ten volts, but voltage that low can’t be sent from a coal plant in West Virginia to your laptop charger in Alexandria; too much energy (in the form of heat) would be lost in transit. Instead, a transformer at the power plant ramps up the electricity to hundreds of thousands of volts, so that it can be transferred efficiently over long distances; once it reaches a substation, another transformer ramps the voltage back down until it can safely enter your home. Whatever its voltage, all that power flows through the grid as alternating current, moving at a constant frequency of sixty hertz.</p><p>Hold that thought; here comes the coronal mass ejection. It smacks into our magnetic field, warping it—or, in severe storms, temporarily ripping part of it open—and setting in motion the chain of events that sends additional electric charge into the planet. Some of that charge, which is known as geomagnetically induced current, dissipates harmlessly, because it flows into a part of the Earth that excels at conducting electricity—salt water, say, or sedimentary rock. But, in places where the underlying rock is a poor conductor, the current must go elsewhere. Like all current, it follows the path of least resistance, and the least resistant path of all is the one designed to conduct electricity: the power grid.</p><p>By unfortunate chance, some of the least conductive bedrock in the United States is the very old metamorphic and igneous rock of the Appalachian Mountains and the New England Highlands—the geological substrates of Boston, New York, Philadelphia, Washington, D.C., and much of the rest of the Eastern Seaboard, home to half the country’s population. As detailed hazard maps recently created by the geophysicist Jeffrey Love and a team of his colleagues at the United States Geological Survey show, some other parts of the country, notably the Midwest, are likewise vulnerable to geomagnetically induced currents.</p><p>What makes these currents so disruptive is not their strength—they are actually quite weak—but their form. The power grid is built for alternating current, but geomagnetically induced currents are basically direct. The collision of these two currents can lead to the inability to transfer power efficiently, large temperature spikes inside transformers (which emit unholy groans and bangs under the strain), relays and other equipment tripping off-line, and, on a very bad day, voltage collapse. Mark Olson, a member of <em>NOAA</em>’s Space Weather Advisory Group and a manager of reliability assessments at the North American Electric Reliability Corporation—the nonprofit agency tasked by the Federal Energy Regulatory Commission and Canada’s provincial governments with keeping the continent’s power grid sound and secure—summed this up for me succinctly: “blackout.”</p><p>This can all happen almost instantly. On March 13, 1989, a coronal mass ejection struck the Earth; within ninety seconds, transformers on the Quebec power grid malfunctioned, dozens of safety mechanisms failed, and the entire grid shut down, leaving almost a quarter of the population of Canada in the dark. That geomagnetic storm—which also triggered outages in the U.K. and Sweden, destroyed a transformer at a nuclear power plant in New Jersey, and caused at least two hundred other issues on the North American grid alone—was strong, but not exceptionally so. Based on magnetometer readings, auroral latitudes, and other fingerprints left behind by solar storms, scientists now believe that at least three storms in the past hundred and fifty-odd years—the Carrington Event and others in 1872 and 1921—were roughly an order of magnitude more powerful.</p><p>All three of those storms took place before the power grid existed. The question that troubles space-weather experts—and divides them—is what will happen the next time a comparable one strikes. Some people think that the Quebec event was a wake-up call—the perfect-sized storm, really, large enough to teach a lesson without being large enough to cause a catastrophe. But, per the N.A.S. report, any gains following the Quebec storm were offset by trends in America’s bulk-power system, which came to rely on ever-larger amounts of power travelling through ever-longer transmission lines. A study commissioned by the federal government and summarized in the report found that a storm the size of the 1921 event would cause large regions of the grid to fail, with impacts that “would be of unprecedented scale and involve populations in excess of 130 million”—close to half of all Americans. The report estimated the cost of a storm like that as “$1 trillion to $2 trillion during the first year alone&nbsp;.&nbsp;.&nbsp;. with recovery times of four to ten years.”</p><p>Fifteen years later, some experts believe <em>that</em> was the wake-up call: that the 2008 report, in its sober-minded scariness, inspired reforms that will make the next severe solar storm more nuisance than nightmare. Bill Murtagh worries about satellite companies, but he thinks that most power companies take space weather seriously and are doing their best to prepare for it. Mark Olson, of the North American Electric Reliability Corporation, concedes that solar storms present “a very challenging risk” to the energy sector, not least because we still know relatively little about them. But, he says, when a major one happens, “the North American grid won’t be taken by surprise.” And he points to a federal directive that, as of this January, requires every provider of bulk power to have a plan in place to deal with a “benchmark geomagnetic disturbance event.”</p><p>That directive is important, but the benchmark itself is troubling. It was established by using thirty years of magnetic-field data to extrapolate the likely magnitude of a once-in-a-century storm. The resulting standard is clear, uniform, achievable, extremely useful during most solar storms, and wholly inadequate for severe ones. As Olson acknowledged, the federal benchmark is now widely believed to be weaker than the Carrington Event.</p><p>That wouldn’t matter if the Carrington storm were an outlier, likely to happen only once every several hundred years. But, in reality, it might not even have been the worst storm of the nineteenth century; the one in 1872 was at least as strong. We also know, from data collected by satellite, that a more powerful storm narrowly missed the Earth in 2012. As that suggests, an extreme geomagnetic storm—the <em>swpc</em> people call it a G5-Plus, at the upper threshold of the highest <em>NOAA</em> category of severity—could be a more common event than previously thought. Some scientists now believe there is an approximately twelve-per-cent chance of one striking the Earth in the next decade.</p><p>That scares some experts. One of the eminences in the field of space-weather studies is Daniel Baker, who was the head of space-plasma physics at Los Alamos National Laboratory and a division chief at <em>NASA</em>’s Goddard Space Flight Center before going to the University of Colorado to lead its Laboratory for Atmospheric and Space Physics. “I do not want to be unduly alarmist,” Baker told me. “But I <em>do</em> want to be duly alarmist.” Like so much American infrastructure, he notes, our bulk-power system is underfunded and aging, while demand on it keeps rising—not only from population growth but from an incommensurate increase in our energy use. As a result, he says, the grid is operating “closer and closer to its maximum stress level.” In that condition, it cannot easily absorb the additional stress of a solar storm.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a21843-rd&quot;}" href="https://www.newyorker.com/cartoon/a21843-rd" rel="noopener" target="_blank"><picture></picture></a><p><span>Cartoon by Emily Bernstein</span></p></div></span></p></figure><p>Our aging grid could be updated, but the factors that make doing so expensive and time-consuming will also dramatically compound the effects of a severe solar storm. “Transformers are not just something you can go to Home Depot and buy,” Baker points out; each one is idiosyncratic, a half-million-pound object designed specifically for one of the fifteen hundred-plus entities, from publicly traded companies to energy coöperatives, that together constitute the power grid. As a result, transformers can’t be stockpiled. They are almost always built to spec, and they are almost all made abroad, which increases shipping times and leaves them vulnerable to political conflict and supply-chain issues. Even under optimal circumstances, the typical lead time to replace a transformer is at least a year. If enough of them fail in a solar storm, the recovery will not be measured in days (the length of time it took to get the power back after the <a href="https://www.newyorker.com/news/dispatch/texans-in-the-midst-of-another-avoidable-catastrophe">Texas winter storms</a>) or weeks (the length of time it took after <a href="https://www.newyorker.com/tag/hurricane-katrina">Hurricane Katrina</a>). It will be measured, almost unthinkably, in months and years.</p><p>That’s one reason Craig Fugate, the former <em>FEMA</em> administrator, thinks the one-to-two-trillion-dollar figure in the N.A.S. report is “probably on the low side.” But he also raises a problem that extends beyond the power grid: because solar storms affect an unusually wide geographic area and an unusually broad range of technologies, they are more likely than other disasters to cause cascading failures. A malfunction in one part of the grid forces electricity to flow elsewhere, overburdening a second part, which is then more likely to malfunction as well; the more such problems you string together, the greater the burden on the remaining parts, and the more likely a catastrophic failure. And what is true of the disaster is also true of the disaster response. Unlike terrestrial hazards, solar storms are not, in <em>FEMA</em>-speak, “geofenced.” They can affect large areas of the world, which minimizes access to outside help in the aftermath. If an earthquake devastates Los Angeles, aid can pour in from neighboring regions. But, if a solar storm devastates New York, anywhere close enough to help will likely be devastated, too</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Above all, Fugate fears that, because space weather affects so many technologies, a severe storm could expose dependencies among them that we did not fully appreciate, or did not recognize at all. Our vast and interrelated technological infrastructure could turn out to harbor a single point of failure—a component, no matter how central or trivial, whose malfunction shuts the whole thing down. Many experts regard G.P.S. signals with alarm for this reason; as a 2021 report by the National Security Telecommunications Advisory Committee noted, the signals are used so ubiquitously in so many critical sectors that “their vulnerabilities pose a near-existential threat.” Alternatively, an individual system that seems robust in isolation might not respond as expected when other systems to which it is connected simultaneously experience powerful stressors—especially when those stressors involve, as Fugate put it, “more unknowns than knowns.” That is true not only of technology but also of the people who operate it; we do not always perform at our best when things around us start malfunctioning. In this kind of “system of systems,” even seemingly minor problems can concatenate in calamitous ways.</p><p>Baker worries about this as well. “We’ve built ourselves into a cyber-electric cocoon,” he says, “and a lot of risk analyses show that when you start to lose nodes in that kind of a connected system it can propagate in very unpredictable ways. And there’s nothing outside it.” In a closed loop like that, a disaster is disastrous not only because of the problems it causes but because of the solutions it eliminates. Post-disaster relief and recovery operations rely on functional transportation systems, but airports, railroads, gas pumps, stoplights, and an increasing number of vehicles all need electricity. Emergency dispatchers rely on sophisticated communication and mapping technologies, but those technologies rely on working computers and satellite transmissions. Power companies need water supplies, but water companies need electricity. Knock over the wrong domino and down goes, as the N.A.S. report put it, “just about every critical infrastructure including government services.” Baker, who led the team behind the report, suspects that we will see a devastating storm within a few decades, and that most of us alive today will suffer through those serial failures. “Maybe here in Colorado, we can go out and hunt elk or something,” he says. “But I’d be very concerned about the major metropolitan areas.”</p><p>All these problems have a meta problem. Radio blackouts, communication disruptions, power-grid problems: to an uncanny degree, solar storms mimic malicious actors trying to sabotage technology that is central to our economy and safety. Because of this, one of the most important functions of <em>swpc</em> and the Defense Department’s Space Weather Operations Center is attribution—determining whether a given anomaly was caused by bad weather in space rather than by a technical malfunction or deliberate interference. Such determinations must be accomplished quickly: if you have a radar system that’s jammed or a missile-defense system that’s malfunctioning, you can’t wait around for long to figure out why. “When we see something, we’ve got five to ten minutes or less to get this stuff out,” Tegnell says. Delay can be disastrous; in matters of national security, Murtagh notes, “a lot can happen in ten to fifteen minutes.”</p><p>In part to facilitate these assessments, <em>swpc</em> makes all its space-weather information publicly available. “We have no problem sharing information across the world,” Murtagh told me. The U.S. has a vested interest in the global community not mistaking natural hazards for foreign adversaries; for that matter, given international supply chains and international commerce, the United States has a vested interest in the global community minimizing disruptions from solar storms. Whether it can do so is impossible to say; we don’t even know how prepared the U.S. is, and the world is the ultimate system of systems, as we all learned at great cost from the pandemic. But it is difficult to be optimistic. For many nations, especially in the developing world, better space-weather preparedness is low on the list of priorities for infrastructure improvements.</p><p>And yet, precisely because solar storms can cause the same problems as enemy agents, better space-weather preparedness amounts to better preparedness over all. “I think of space weather as a stand-in for all those other disruptions,” Kathryn Draeger, an agronomist at the University of Minnesota who researches how to mitigate the impact of solar storms on agriculture, told me. “A terrorist attack on our grid, an electromagnetic pulse, a natural disaster, a pandemic—if we can figure it out for space weather, we will be better protected from all these other major disruptions.”</p><p>In theory, we’ve already figured out some of it. We could require backup navigation and timing systems; we could move away from ultra-long, ultra-high-voltage transmission lines. Certain new technologies could help, such as devices that block geomagnetically induced currents from entering the grid, as could a return to some old ones. The Army, concerned about overreliance on vulnerable technologies, has reinstated courses in orienteering, and the Navy has resumed teaching sailors how to use a sextant.</p><p>Still, persuading people to implement safety measures is difficult, because severe solar storms are what people in emergency management sometimes call low-frequency, high-consequence events. Such events are emotionally, ethically, and pragmatically vexing, and we respond to them in curious and inconsistent ways. In our private lives, we tend to focus on the high consequences: your nine-year-old will almost certainly not be kidnapped while playing alone at the local playground, but you don’t let him do so, because the potential cost is too devastating. By contrast, corporations and nations tend to focus on the low odds, and therefore wave away the possible consequences. “I’m working with people and they’ll say, ‘Why do I need to spend a cent on this issue? I’ve been here for forty years and I’ve never seen a problem,’&nbsp;” Murtagh told me. “And I look at them and say, ‘I don’t know what to say to you.’&nbsp;” As far as the sun is concerned, “the Carrington Event happened one second ago. And it will happen again.”</p><p>We don’t know when, of course; there is so much we do not know. Before Tegnell became a space-weather forecaster, he was a regular-weather forecaster, and he remains acutely aware of the difference between them. It’s not just that you have to go from thinking on the scale of cities and counties to thinking on the scale of millions of miles. It’s that with solar events “you have no idea what goes on in ninety per cent of them.” Space-weather forecasting, he believes, is where terrestrial meteorology was seventy-five years ago. Back then, we were farther from today’s reality, of minute-by-minute weather information on your phone, and closer to the reality of sixteenth-century mariners or third-century shepherds, for whom hurricanes and blizzards happened more or less out of nowhere, and for whom our vulnerability to severe weather seemed immutable and inevitable, laid down as our lot in life since that first Biblical flood.</p><p>Someday, Tegnell says, our current understanding of space weather will seem similarly sparse. We will put more and better instruments in space; we will learn more about the physical dynamics of the sun and their effects here on Earth. Whether infrastructure improvements will keep pace with that knowledge is beyond his job description, and beyond his ken. He is hoping to retire this year, after half a century of service to the United States. He is not worried about being bored. He has spent a lifetime studying solar activity and doesn’t figure that will change all that much. “I’m the kind of guy,” he told me, “who likes looking at sunsets.”&nbsp;♦</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to find the AWS account ID of any S3 bucket (554 pts)]]></title>
            <link>https://tracebit.com/blog/2024/02/finding-aws-account-id-of-any-s3-bucket/</link>
            <guid>39512896</guid>
            <pubDate>Mon, 26 Feb 2024 16:07:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tracebit.com/blog/2024/02/finding-aws-account-id-of-any-s3-bucket/">https://tracebit.com/blog/2024/02/finding-aws-account-id-of-any-s3-bucket/</a>, See on <a href="https://news.ycombinator.com/item?id=39512896">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>In 2021 <a href="https://twitter.com/benbridts">Ben Bridts</a> published a <a href="https://cloudar.be/awsblog/finding-the-account-id-of-any-public-s3-bucket/">highly inventive method</a> for finding the AWS Account ID of a public S3 bucket.</p><p><strong>This post describes a technique to find the Account ID of <em>any S3 bucket</em> (both private and public).</strong></p><p>I'd highly recommend reading Ben's technique first as we will re-use a lot of concepts.</p><h3>S3 Bucket to AWS Account ID</h3><p>Shell output can be worth a thousand words, here's what our technique enables - finding the previously unknown AWS Account ID for the bucket <code>bucket-alpha</code>:</p><pre><code>sh-5.2$ python3 find-s3-account.py bucket-alpha

VPC endpoint vpce-0e76855aadb0dafb5 policy already configured
Requesting bucket-alpha using session name 0-----------
Requesting bucket-alpha using session name 1-----------
Requesting bucket-alpha using session name 2-----------
Requesting bucket-alpha using session name 3-----------

SNIP

Requesting bucket-alpha using session name -----------7
Requesting bucket-alpha using session name -----------8
Requesting bucket-alpha using session name -----------9
Finding session names which passed the VPC endpoint in CloudTrail...
Found -----------1 for bucket-alpha in CloudTrail
Found ---------1-- for bucket-alpha in CloudTrail
Found --------9--- for bucket-alpha in CloudTrail
Found -----6------ for bucket-alpha in CloudTrail
Found --3--------- for bucket-alpha in CloudTrail
Found -2---------- for bucket-alpha in CloudTrail
Found 1----------- for bucket-alpha in CloudTrail
Found ----------0- for bucket-alpha in CloudTrail
Found -------8---- for bucket-alpha in CloudTrail
Found ------7----- for bucket-alpha in CloudTrail
Found ----5------- for bucket-alpha in CloudTrail
Found ---4-------- for bucket-alpha in CloudTrail
Bucket bucket-alpha: 123456789101</code></pre><h3>How exactly does this work?</h3><p>When exploring possibilities for this technique, I started by breaking down exactly why Ben's method works. There are three key elements which combine to make it work:</p><ol><li><strong>The ability to apply an IAM policy to the request</strong></li></ol><p>In the Ben's technique, this is achieved by applying a custom policy when assuming the role.</p><ol start="2"><li><strong>The ability to infer whether this IAM policy permitted the request or not</strong></li></ol><p>In the case of public buckets, this is quite simple. If our policy blocked the request, the request will fail with <code>AccessDenied</code>. Otherwise, the request will succeed as expected with requests to public buckets.</p><ol start="3"><li><strong>The ability to apply a wildcard match on the <code>s3:ResourceAccount</code> condition key</strong></li></ol><p>This allows us to discover the Account ID incrementally, one digit at a time, reducing the search space from trillions to hundreds.</p><h3>A solution</h3><p>After exploring a few different ideas, I found a solution which works. It involves using a VPC Endpoint for S3, and a difference of behaviour in CloudTrail when a request is denied by a VPC Endpoint policy.</p><p><a href="#img700414"><img src="https://tracebit.com/img/blog/2024/02/cloudtrail-and-vpc-endpoint-policy.png" alt="How VPC Endpoint policies interact with CloudTrail" title="How VPC Endpoint policies interact with CloudTrail"></a><br><a href="#void" id="img700414"><span></span></a></p><ol><li><strong>The ability to apply an IAM policy to the request</strong></li></ol><p>Creating a VPC Endpoint of type "Interface" for S3 will allow us to apply an IAM policy to the request. This policy intersects with the other policies which apply to the request (e.g. the bucket policy, the IAM policy of the principal making the request etc) when the request is made through the VPC Endpoint.</p><ol start="2"><li><strong>The ability to infer whether this IAM policy permitted the request or not</strong></li></ol><p>As the target bucket is owned by a third party and is a private bucket, we're (thankfully) going to receive an <code>AccessDenied</code> response, regardless of whichever policies we apply to the request. However, we can infer whether the VPC Endpoint policy blocked or permitted the request by whether it appears in <em>our own</em> CloudTrail logs.</p><ul><li>If the request <em>does</em> appear in our CloudTrail logs, it was permitted by our VPC Endpoint policy but blocked as expected by the bucket policy.</li><li>If the request <em>does not</em> appear in our CloudTrail logs, it was blocked by our VPC Endpoint policy.</li></ul><ol start="3"><li><strong>The ability to apply a wildcard match on the <code>s3:ResourceAccount</code> condition key</strong></li></ol><p>We can use the full power of IAM policy conditions, including <code>StringLike</code> wildcards and resource condition keys in a VPC Endpoint policy, so the same basic technique will work here.</p><h3>Step-by-step</h3><p>Let's say that we want to find the Account ID of the bucket <code>bucket-alpha</code>.</p><p><strong>Note that some of our activities here will be visible to the owner of the bucket in their own CloudTrail logs.</strong></p><h4>Determine the bucket region</h4><p>We need to find the region in which the bucket lives so that we can create a VPC in the same region. This can be done by curling the bucket's HTTP endpoint and examining the <code>x-amz-bucket-region</code> header (which is returned despite the request being forbidden).</p><pre><code><span>curl</span> <span>-v</span> bucket-alpha.s3.amazonaws.com

<span>..</span>.
x-amz-bucket-region: us-east-1</code></pre><h4>Deploy a VPC and VPC Endpoint in the same region</h4><p>We need to deploy a VPC and a VPC Endpoint for S3 in the same region as the target bucket. It's best to create a VPC specifically for this purpose as our VPC Endpoint will interfere with requests to S3 from the VPC. The VPC Endpoint should be of type "Interface" so we can apply a policy to the request.</p><h4>Launch an EC2 instance within the VPC and confirm that it's using the VPC Endpoint for S3</h4><p>We'll need to send requests to S3 from within the VPC so that the VPC Endpoint is used. An EC2 instance is a convenient way of doing so.</p><h4>Modify the VPC Endpoint policy to determine whether the account ID of the target bucket starts with "0"</h4><p>Apply a policy to the VPC Endpoint which performs a wildcard match on the <code>s3:ResourceAccount</code> condition key. This will only permit requests through the endpoint if the bucket's Account ID starts with "0".</p><pre><code><span>{</span>
    <span>"Version"</span><span>:</span> <span>"2012-10-17"</span><span>,</span>
    <span>"Statement"</span><span>:</span> <span>[</span>
        <span>{</span>
            <span>"Action"</span><span>:</span> <span>"s3:*"</span><span>,</span>
            <span>"Effect"</span><span>:</span> <span>"Allow"</span><span>,</span>
            <span>"Resource"</span><span>:</span> <span>"*"</span><span>,</span>
            <span>"Principal"</span><span>:</span> <span>"*"</span><span>,</span>
            <span>"Condition"</span><span>:</span> <span>{</span>
                <span>"StringLike"</span><span>:</span> <span>{</span>
                    <span>"s3:ResourceAccount"</span><span>:</span> <span>"0*"</span>
                <span>}</span>
            <span>}</span>
        <span>}</span>
    <span>]</span>
<span>}</span></code></pre><h4>Make a request to the target bucket</h4><p>Via the EC2 instance, make a request to the target bucket. This request will be denied as expected. It's best to use a "Management" request rather than a "Data" request so we don't need to do anything special with our CloudTrail setup. In this case I used <code>GetBucketAcl</code>.</p><pre><code>aws s3api get-bucket-acl <span>--bucket</span> bucket-alpha

An error occurred <span>(</span>AccessDenied<span>)</span> when calling the GetBucketAcl operation: Access Denied</code></pre><h4>Check whether the request appears in CloudTrail</h4><p>Now we want to check whether our request appears in CloudTrail.</p><pre><code>aws cloudtrail lookup-events --lookup-attributes <span>AttributeKey</span><span>=</span>EventName,AttributeValue<span>=</span>GetBucketAcl --start-time <span><span>$(</span><span>date</span> <span>-d</span> <span>"-10 minutes"</span> +%s<span>)</span></span></code></pre><p>If we find our request in CloudTrail, it means that the VPC Endpoint policy permitted the request - i.e. the Account ID of the bucket starts with <code>0</code>. If we don't find the request, then the VPC Endpoint policy blocked the request - i.e. the Account ID of the bucket does not start with <code>0</code>.</p><pre><code><span>{</span>
    <span>&lt;</span>SNIP<span>&gt;</span>
    <span>"eventSource"</span><span>:</span> <span>"s3.amazonaws.com"</span>,
    <span>"eventName"</span><span>:</span> <span>"GetBucketAcl"</span>,
    <span>"resources"</span><span>:</span> <span>[</span>
        <span>{</span>
            <span>"accountId"</span><span>:</span> <span>"HIDDEN_DUE_TO_SECURITY_REASONS"</span>,
            <span>"type"</span><span>:</span> <span>"AWS::S3::Bucket"</span>,
            <span>"ARN"</span><span>:</span> <span>"arn:aws:s3:::bucket-alpha"</span>
        <span>}</span>
    <span>]</span>,
    <span>"vpcEndpointId"</span><span>:</span> <span>"vpce-0e76855aadb0dafb5"</span>,
<span>}</span></code></pre><p>Bear in mind that it will take a few minutes for the request to appear in CloudTrail. To be safe, I'd recommend waiting a 10 minutes before deciding the event won't appear in CloudTrail.</p><h4>Rinse and repeat</h4><p>Depending on the result of the previous step, modify the VPC Endpoint policy to discover more information about the account ID. For instance, if the event <em>didn't</em> appear in CloudTrail, modify the condition to test whether the first digit is <code>1</code>:</p><pre><code><span>"Condition"</span><span>:</span> <span>{</span>
    <span>"StringLike"</span><span>:</span> <span>{</span>
        <span>"s3:ResourceAccount"</span><span>:</span> <span>"1*"</span>
    <span>}</span>
<span>}</span></code></pre><p>If it <em>did</em> appear in CloudTrail (so the first digit of the account ID is <code>0</code>), we can start work on the second digit:</p><pre><code><span>"Condition"</span><span>:</span> <span>{</span>
    <span>"StringLike"</span><span>:</span> <span>{</span>
        <span>"s3:ResourceAccount"</span><span>:</span> <span>"00*"</span>
    <span>}</span>
<span>}</span></code></pre><p>Bear it mind, it takes a few minutes for policy changes to fully propagate and take effect. I've found waiting 5 minutes after modifying the policy to work well.</p><h3>Results</h3><p>I wrote a script to automate this process and it could reliably find the Account ID of a bucket. As it's quite a slow process, I used a slightly modified technique of performing a binary search on each digit so fewer tests were needed, e.g:</p><pre><code><span>"Condition"</span><span>:</span> <span>{</span>
    <span>"StringLike"</span><span>:</span> <span>{</span>
        <span>"s3:ResourceAccount"</span><span>:</span> <span>[</span><span>"0*"</span><span>,</span> <span>"1*"</span><span>,</span> <span>"2*"</span><span>,</span> <span>"3*"</span><span>,</span> <span>"4*"</span><span>]</span>
    <span>}</span>
<span>}</span></code></pre><p>Leaving it for a few hours returned the account ID successfully:</p><pre><code>[ssm-user@ip-172-31-8-184 ~]$ python3 find-s3-account.py bucket-alpha
Searching for bucket bucket-alpha
Modifying VPC endpoint policy...
Modified VPC endpoint policy
Made S3 request to bucket: GPSWS2M4TH9ABX3C
Looking for event in CloudTrail...
Did not find event in CloudTrail (not permitted through VPC endpoint)
State: {'found': '', 'next_digits': [0, 1, 2, 3, 4]}
Modifying VPC endpoint policy...
Modified VPC endpoint policy
Made S3 request to bucket: 8T809NPVDGQSGB1N
Looking for event in CloudTrail...
Did not find event in CloudTrail (not permitted through VPC endpoint)
State: {'found': '', 'next_digits': [0, 1]}
Modifying VPC endpoint policy...
Modified VPC endpoint policy
Made S3 request to bucket: C9F0RTC7QK0G70TB
Looking for event in CloudTrail...
Found event in CloudTrail (permitted through VPC endpoint)
State: {'found': '1', 'next_digits': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}

...

State: {'found': '123456789101', 'next_digits': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}
Found account: 123456789101</code></pre><h3>Making it faster</h3><p>Waiting for the VPC Endpoint policy to take effect, and waiting for long enough to determine whether the request appears in CloudTrail is quite a slow process. Even using a binary search, it will take approximately (40 * 12 minutes = 8 hours) to find the Account ID.</p><p>To make this process faster, and eliminate the need to wait between each step, I modified the VPC endpoint policy like so:</p><pre><code><span>{</span>
    <span>"Version"</span><span>:</span> <span>"2012-10-17"</span><span>,</span>
    <span>"Statement"</span><span>:</span> <span>[</span>
      <span>{</span>
        <span>"Effect"</span><span>:</span> <span>"Allow"</span><span>,</span>
        <span>"Action"</span><span>:</span> <span>[</span>
          <span>"s3:*"</span>
        <span>]</span><span>,</span>
        <span>"Resource"</span><span>:</span> <span>"*"</span><span>,</span>
        <span>"Principal"</span><span>:</span> <span>"*"</span><span>,</span>
        <span>"Condition"</span><span>:</span> <span>{</span>
          <span>"StringLike"</span><span>:</span> <span>{</span>
            <span>"aws:userid"</span><span>:</span> <span>"*:0-----------"</span><span>,</span>
            <span>"s3:ResourceAccount"</span><span>:</span> <span>"0???????????"</span>
          <span>}</span>
        <span>}</span>
      <span>}</span><span>,</span>
      <span>{</span>
        <span>"Effect"</span><span>:</span> <span>"Allow"</span><span>,</span>
        <span>"Action"</span><span>:</span> <span>[</span>
          <span>"s3:*"</span>
        <span>]</span><span>,</span>
        <span>"Resource"</span><span>:</span> <span>"*"</span><span>,</span>
        <span>"Principal"</span><span>:</span> <span>"*"</span><span>,</span>
        <span>"Condition"</span><span>:</span> <span>{</span>
          <span>"StringLike"</span><span>:</span> <span>{</span>
            <span>"aws:userid"</span><span>:</span> <span>"*:1-----------"</span><span>,</span>
            <span>"s3:ResourceAccount"</span><span>:</span> <span>"1???????????"</span>
          <span>}</span>
        <span>}</span>
      <span>}</span><span>,</span>
      SNIP
      <span>{</span>
        <span>"Effect"</span><span>:</span> <span>"Allow"</span><span>,</span>
        <span>"Action"</span><span>:</span> <span>[</span>
          <span>"s3:*"</span>
        <span>]</span><span>,</span>
        <span>"Resource"</span><span>:</span> <span>"*"</span><span>,</span>
        <span>"Principal"</span><span>:</span> <span>"*"</span><span>,</span>
        <span>"Condition"</span><span>:</span> <span>{</span>
          <span>"StringLike"</span><span>:</span> <span>{</span>
            <span>"aws:userid"</span><span>:</span> <span>"*:-----------9"</span><span>,</span>
            <span>"s3:ResourceAccount"</span><span>:</span> <span>"???????????9"</span>
          <span>}</span>
        <span>}</span>
      <span>}</span>
    <span>]</span>
<span>}</span></code></pre><p>There are 120 statements in the policy - one for each possible digit in each possible position. The condition on <code>aws:userid</code> is used to match particular values of the <code>RoleSessionName</code> parameter (which we can freely specify) used in an STS <code>AssumeRole</code> call. In effect this means we can selectively choose which policy statement (i.e. a particular digit in a particular position) we want to test for each request, by assuming a role with a particular <code>RoleSessionName</code> before doing so.</p><p><a href="#img172768"><img src="https://tracebit.com/img/blog/2024/02/using-role-session-name-in-vpc-endpoint-policy.png" alt="Using RoleSessionName to selectively test digits" title="Using RoleSessionName to selectively test digits"></a><br><a href="#void" id="img172768"><span></span></a></p><p>As this policy (only just!) fits within the maximum character length of a VPC Endpoint policy, we can test all 120 possibilities in parallel, without modifying the policy or waiting for the results individually in CloudTrail.</p><p>This reduced the time taken to find the Account ID to less than 10 minutes:</p><pre><code>sh-5.2$ python3 find-s3-account.py bucket-alpha

VPC endpoint vpce-0e76855aadb0dafb5 policy already configured
Requesting bucket-alpha using session name 0-----------
Requesting bucket-alpha using session name 1-----------
Requesting bucket-alpha using session name 2-----------
Requesting bucket-alpha using session name 3-----------

SNIP

Requesting bucket-alpha using session name -----------7
Requesting bucket-alpha using session name -----------8
Requesting bucket-alpha using session name -----------9
Finding session names which passed the VPC endpoint in CloudTrail...
Found -----------1 for bucket-alpha in CloudTrail
Found ---------1-- for bucket-alpha in CloudTrail
Found --------9--- for bucket-alpha in CloudTrail
Found -----6------ for bucket-alpha in CloudTrail
Found --3--------- for bucket-alpha in CloudTrail
Found -2---------- for bucket-alpha in CloudTrail
Found 1----------- for bucket-alpha in CloudTrail
Found ----------0- for bucket-alpha in CloudTrail
Found -------8---- for bucket-alpha in CloudTrail
Found ------7----- for bucket-alpha in CloudTrail
Found ----5------- for bucket-alpha in CloudTrail
Found ---4-------- for bucket-alpha in CloudTrail
Bucket bucket-alpha: 123456789101</code></pre><h3>Remarks</h3><ul><li><p>I consulted the AWS Security team before publishing this blog post.</p></li><li><p>There has already been a lot of <a href="https://www.lastweekinaws.com/blog/are-aws-account-ids-sensitive-information/">interesting</a> <a href="https://blog.plerion.com/aws-account-ids-are-secrets/">discussion</a> about whether AWS account IDs should be considered sensitive. It's noteworthy that in the CloudTrail events themselves, AWS have chosen to leave the third-party account ID <code>HIDDEN_DUE_TO_SECURITY_REASONS</code>. You've probably also spotted that I've chosen to redact my own account ID from the examples in this post!</p></li><li><p>This technique should also work to uncover other resource condition keys (e.g. <code>aws:ResourceOrgID</code>, <code>aws:ResourceOrgPaths</code>, <code>aws:ResourceTag</code>) associated with the bucket - or indeed for services other than S3 to which this technique could be applied</p></li><li><p>It's possible that by creating mutually peered VPCs and VPC Endpoints in all regions you could create a setup which works regardless of the particular region of the target bucket.</p></li><li><p>These techniques are only feasible due to the ability to use <code>StringLike</code> conditions against <code>s3:ResourceAccount</code> in a policy. I can't think of a use case where a partial match against a randomly-generated identifier is necessary.</p></li><li><p>It seems like it might otherwise be beneficial for events which are denied by a VPC Endpoint policies to be logged in CloudTrail.</p></li></ul><h3>Acknowledgments</h3><ul><li><p>Ben Bridt's original technique inspired this work.</p></li><li><p>I'm very grateful to <a href="http://www.chrisfarris.com/">Chris Farris</a> for his invaluable help and advice.</p></li></ul><p><strong><em>Thanks for reading this far, we hope you got something from this article. Tracebit is building a new kind of security product to be the ‘easy button’ for adding detections to cloud environments using decoys. If you’re interested to learn more about what that looks like please <a href="https://tracebit.com/earlyaccess">reach out</a> (we currently support AWS, with Azure &amp; GCP coming soon).</em></strong></p><ul><li><a href="https://tracebit.com/blog/category/research">research</a></li><li><a href="https://tracebit.com/blog/category/aws">AWS</a></li><li><a href="https://tracebit.com/blog/category/cloudtrail">CloudTrail</a></li><li><a href="https://tracebit.com/blog/category/engineering">engineering</a></li><li><a href="https://tracebit.com/blog/category/security">security</a></li></ul></article></div>]]></description>
        </item>
    </channel>
</rss>